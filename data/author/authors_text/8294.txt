Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 889?896
Manchester, August 2008
Relational-Realizational Parsing
Reut Tsarfaty and Khalil Sima?an
Institute for Logic, Language and Computation, University of Amsterdam
Plantage Muidergracht 24, 1018TV, Amsterdam, The Netherlands
{rtsarfat,simaan}@science.uva.nl
Abstract
State-of-the-art statistical parsing models
applied to free word-order languages tend
to underperform compared to, e.g., pars-
ing English. Constituency-based mod-
els often fail to capture generalizations
that cannot be stated in structural terms,
and dependency-based models employ a
?single-head? assumption that often breaks
in the face of multiple exponence. In this
paper we suggest that the position of a con-
stituent is a form manifestation of its gram-
matical function, one among various pos-
sible means of realization. We develop the
Relational-Realizational approach to pars-
ing in which we untangle the projection
of grammatical functions and their means
of realization to allow for phrase-structure
variability and morphological-syntactic in-
teraction. We empirically demonstrate
the application of our approach to pars-
ing Modern Hebrew, obtaining 7% error
reduction from previously reported results.
1 Introduction
Many broad-coverage statistical parsers to date are
constituency-based with a Probabilistic Context-
Free Grammar (PCFG) or a Stochastic Tree Sub-
stitution Grammar (STSG) at their backbone. The
majority of such models belong to a Head-Driven
paradigm, in which a head constituent is gen-
erated first, providing a positional anchor for
subsequent (e.g., Markovian) sisters? generation.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Constituency-based models, lexicalized and un-
lexicalized alike, demonstrate state-of-the-art per-
formance for parsing English (Charniak, 1997;
Collins, 2003; Klein and Manning, 2003; Bod,
2003), yet a direct application of such models to
parsing less configurational languages often fails
to yield comparable results. The parameters of
such parsers capture generalizations that are eas-
ily stated in structural terms (e.g., subjects linearly
precede predicates, VPs dominate objects, etc.)
which may not be adequate for parsing languages
with less configurational character.
A different vein of research explores data-driven
dependency-based parsing methods (e.g., (Mc-
Donald et al, 2005)) which seem to be intuitively
more adequate for the task. It turns out, how-
ever, that even such models fail to provide the
desired remedy. Recent reports by (Nivre, 2007)
delineated a class of richly-inflected languages
with relatively free word-order (including Greek,
Basque, and Modern Standard Arabic) for which
the parsers performed poorly, regardless of the
parsing method used. The need for parsing meth-
ods that can effectively cope with such phenomena
doesn?t seem to have been eliminated by depen-
dency parsing ? perhaps quite the contrary.
The essential argument we promote here is that
in order to deal with the kind of variation that
is empirically observed cross-linguistically an al-
ternative view of the generation process is re-
quired. Our Relational-Realizational parsing pro-
posal, strongly inspired by Relational Grammar
(Perlmutter, 1982), takes grammatical relations
such as ?Subject? and ?Predicate? as central, primi-
tive notions of the syntactic representation, and re-
tains a distinction between the projection of such
relations and the means by which they are real-
ized. The grammar we develop here, formally
889
represented as a PCFG, articulates two alternating
generation phases: a Relational phase, in which a
clause-level category projects a monostratal Rela-
tional Network (RN) representation for the clause,
and a Realizational phase, in which the projected
relations are realized in a certain surface config-
uration. Paradigmatic morphosyntactic represen-
tations are constructed for all non-terminal nodes,
allowing for morphosyntactic interaction at vari-
ous levels of the syntactic parse tree.
We illustrate the application of our theoretical
reconstruction to the representation of clause-level
categories in Modern Hebrew (MH) and their in-
teraction with a handful of morphological features.
The treebank grammar resulting from our applica-
tion yields 13% error reduction relative to a tree-
bank PCFG which uses the same information in
the form of state-splits, and our best result shows
a 7% error reduction over the best parsing results
for MH so far. Through a quantitative and quali-
tative analysis we illustrate the advantages of the
Relational-Realizational approach and its poten-
tial promise for parsing other ?exotic? languages.
2 Background
Recent decades have seen a surge of interest in sta-
tistical models using a body of annotated text for
learning the distributions of grammatically mean-
ingful structures, in order to assign the most likely
ones to unseen sentences. Probabilistic Context
Free Grammars (PCFGs) have become popular in
the articulation of such models, and unlexicalized
treebank grammars (or representational variations
thereof) were shown to perform reasonably well on
English benchmark corpora (Johnson, 1998; Klein
and Manning, 2003).
A major leap in the performance of PCFG-based
statistical parsers has been introduced by the move
towards a Head-Driven paradigm (Collins, 2003;
Charniak, 1997), in which syntactic categories are
enriched with head information percolated up the
tree. The head-driven generation process allows
one to model the relation between the information
content of a constituent and the information con-
tent of its head-marked sister. At the same time,
such models introduce a bias with respect to the
positioning of a non-head constituent relative to its
head-marked sister. The vast improvement in pars-
ing results came about not without modeling costs,
e.g., additional ad-hoc modifications for capturing
complex structures such as conjunction.
An inherent difficulty with the application of
constituency-based parsing models is the implicit
assumption that the relation between the posi-
tion of a constituent and its grammatical func-
tion is fully predictable. For languages with
relatively free word-order, this assumption often
breaks down. Distinguishing, e.g., ?left? and
?right? distributions for constituents of the same
?sort? implicitly takes the position of a constituent
to be a primitive syntactic notion, and their gram-
matical function to be a secondary, derived one.
Theoretical accounts show that this may be insuf-
ficient (Perlmutter, 1982). A subsequent difficulty
with the head-driven paradigm, also shared by
dependency-based parsing methods, is the stipu-
lation that all grammatically relevant properties of
a phrase are recovered from a single head. In fact,
it is typologically established that grammatically
meaningful properties of a constituent may jointly
emerge from different surface forms dominated by
it (co-heads or multiple exponence (Zwicky, 1993;
Blevins, 2008)).1
The task we undertake here is to suggest a sta-
tistical generative parsing method which is linguis-
tically plausible as well as technologically viable
for parsing languages with relatively free word-
order and variable means of realization. In what
follows we remain within the computationally ef-
ficient framework of PCFGs, and propose a varia-
tion that draws on insights from syntactic and mor-
phological theories that have been explored cross-
linguistically.
3 Approach
3.1 Relational Grammars (RGs)
Relational Grammars (RGs) were introduced in the
early 80?s when attempts to find a universal def-
inition for notions such as a ?Subject? in terms
of various ?behavioral properties? seemed to have
failed (Perlmutter, 1982). The unsuccessful at-
tempts to recover an adequate definition of gram-
matical functions in structural terms led to a revival
of a view in which grammatical relations such as
?Subject? and ?Object? are primitive notions by
1We refrain here from referring to the increasingly popular
approach of discriminative parsing, firstly, because we are in-
terested in a generative parsing model that assigns a probabil-
ity distribution to all sentence-structure pairs in the language,
essentially allowing it to be used as a language model (e.g.,
in SR or SMT applications). Secondly, so far the features that
have been explored in these frameworks are mainly those eas-
ily stated in structural terms, with not much effort towards
modeling morphosyntactic interactions systematically.
890
which syntactic structures are defined (Postal and
Perlmutter, 1977). This view proved useful for de-
scriptive purposes, influencing the design of for-
malisms such as Arc-Pair grammars (Postal, 1982)
and LFG (Bresnan, 1979).
The two main primitive elements used in RGs
are (a) a set of nodes representing linguistic ele-
ments (which we refer to using upper case letters),
and (b) a set of names of grammatical relations
(which we refer to as gr
1
...gr
n
). RGs represent
the fact that a linguistic element bears a certain re-
lation to another element using a structure called
an ?Arc?, represented as [gr
i
(A,B)]. Arcs are rep-
resented as arrows, with A the head of the Arc and
B its tail, and a Relational Network (RN) is defined
to be a set of Arcs that share a single head.2
Now, a few theoretical observations are due.
Firstly, the essential difference between RGs and
dependency-based grammars is that RNs take the
linguistic element at the head of a network to be a
clause-level category, not a particular surface form.
The corresponding tails are then the various nom-
inals bearing the different grammatical relations
to the clause (including a ?Predicate?, a ?Subject?,
an ?Object?, etc.). In addition, RNs abstract away
from elements such as auxiliary verbs and particles
which do not have their own arc representation.
RGs also differ from phrase-structure grammars
in that their RNs are unordered. Therefore, linear
precedence need not play a role in stating general-
izations. RGs differ from both constituency- and
dependency-based formalisms in that they do not
weigh heavily the ?single-head? assumption ? RNs
may delineate a whole chunk as bearing a certain
grammatical relation to the clause.
The set-theoretic notion of RNs in RGs abstracts
away from surface phenomena crucial for generat-
ing phrase-structure trees. Thus, we next turn to
modeling how grammatical relations are realized.
3.2 Form, Function and Separation
Morphological phenomena such as suprasegmen-
tation, interdigitation, reduplication, subtractive
morphology, templatic morphology, and methathe-
sis demonstrate that it is sometimes impossible to
find a direct correspondence between a certain part
2RGs also define the notion of a stratum, a single level
of syntactic representation, and for the current discussion we
assume a monostratal representation. We do not claim that
our framework is capable of dealing with the full range of
phenomena multistratal RNs were shown to account for, yet
there is nothing in our proposal that excludes extending the
representation into a multistratal framework that does so.
of a word (a ?morpheme?) and the function it has
in altering the word?s meaning (Anderson, 1992).
Attempts to model such morphological phenom-
ena brought forward the hypothesis that ?form? and
?function? need not stand in one-to-one correspon-
dence, and that one is not necessarily immediately
predicted by the other. This hypothesis is known as
the ?Separation Hypothesis? (Beard, 1988). The
problem of modeling certain surface phenomena
then boils down to modeling form and function
correlations, bearing in mind that these may be
quite complex.
Bringing this general notion of separation into
the syntactic derivation, we propose to view the
position of a constituent in a phrase as its form and
the articulated grammatical relation as its function.
The task of learning the position of different con-
stituents realizing the grammatical relations in an
RN is now delegated to a statistical component. A
set of parameters which we refer to as ?configu-
ration? determines the syntactic position in which
each of the grammatical relations is to be realized.
3.3 Morphosyntactic Representations
In order to connect the abstract RN representation
with the constituents that syntactic parse trees are
?made of? we propose to view the internal nodes
of a tree as Morphosyntactic Paradigms. Our
morphosyntactic representation for constituents,
loosely inspired by (Anderson, 1992), is a struc-
tured representation of morphological and syntac-
tic properties for an internal node in the parse tree.
In our model, the morphological features asso-
ciated with a syntactic constituent are percolated
from its dominated surface forms, and we allow
the specification of head (PoS tag) information and
structural features such as vertical markovization.
Given the grammatical relation an element bears
to a clause, it is statistically feasible to learn the
morphosyntactic paradigm by which it is realized.
4 The Model
4.1 The Generative Model
Let S
p
? ?S
c
1
-gr
1
. . . S
c
n
-gr
n
? be a context-free
rule where S
p
is the morphosyntactic representa-
tion of a parent constituent, gr
1
...gr
n
are the gram-
matical relations forming its RN, and ?S
c
1
. . . S
c
n
?
are ordered morphosyntactic representations of the
child constituents bearing the respective relations
to the parent. Our grammar then conceptualizes
the generation of such a rule in three phases:
891
? Projection:
S
p
? {gr
i
}
n
i=1
@S
p
? Configuration:
{gr
i
}
n
i=1
@S
p
? ?gr
1
@S
p
. . . gr
n
@S
p
?
? Realization:
{gr
i
@S
p
? S
c
i
}
n
i=1
In the projection stage we generate the set of gram-
matical relations in the RN of a constituent. In
the configuration stage we order these grammat-
ical relations, and in realization we generate the
morphosyntactic representation of each child con-
stituent given the relation to its parent. Figure (1)
shows the application of this process to two clauses
bearing identical RNs that are in turn realized in
different possible configurations.
This three-step process does not generate func-
tional elements (such as auxiliary verbs and
special-purpose particles) that are outside of con-
stituents? RNs. We thus let the configuration stage
place obligatory or optional ?realizational slots?
between the ordered elements (marked gr
i
: gr
j
),
signalling periphrastic adpositions and/or modifi-
cation. Note that modification may introduce more
than one constituent, to be generated in realization.
? Projection:
S
p
? {gr
i
}
n
i=1
@S
p
? Configuration:
{gr
i
}
n
i=1
@S
p
?
?gr
0
: gr
1
@S
p
gr
1
@S
p
. . . gr
n
: gr
n+1
@S
p
?
? Realization:
{gr
i
@S
p
? S
c
i
}
n
i=1
{gr
i
: gr
i+1
@S
p
? ?S
c
i
1
...S
c
i
m
i
?}
n
i=0
In figure (2), the configuration stage reserves a slot
for an obligatory punctuation mark at the end of an
affirmative sentence. It further reserves a slot for
an optional adverbial modifier at a position com-
monly employed in MH for interjections.
In the current framework, grammatical rela-
tions may be realized in a certain surface position
via configuration, or using explicit morphological
marking per grammatical relation independently of
linear context. Figure (3) demonstrates how the
realization phase models the correlation between
grammatical relations and morphological informa-
tion percolated from dominated surface forms. In
particular, our model can capture the interaction
between marked features, e.g., the ?exclusive or?
relation between definiteness and accusativity in
marking direct objects in MH (Danon, 2001).
Finally, a conjunction structure in our model
is simply an RN representation of multiple mor-
phosyntactically equivalent conjuncts, as illus-
trated in figure (4). This modeling choice avoids
the need to stipulate a single head for such struc-
tures (cf. head-driven processes) and allows the
different conjuncts to share a realization distribu-
tion ? essentially implying homogeneity in the
assignment of heads and morphosyntactic features
across conjuncts.
4.2 The Probabilistic Model
Our probablistic model is a PCFG, where CFG
rules capture the three stages of generation. Ev-
ery time we apply our projection-configuration-
realization cycle we replace the rule probability
with the probabilities of the three stages, multi-
plied (n +?n
i=0
m
i
daugthers , gr
0
=gr
n+1
=null).
P (?S
c
i
1
, .., S
c
i
m
i
S
c
i
-gr
i
, S
c
i+1
1
, .., S
c
i+1
m
i+1
?
n
i=0
|S
p
) =
P ({gr
i
}
n
i=1
|S
p
)?
P (?gr
0
: gr
1
, g
1
, . . .?|{gr
i
}
n
i=1
, S
p
)?
?
n
i=1
P (S
c
i
|gr
i
, S
p
)?
P (?S
c
0
1
, ..., S
c
0
m
0
?|gr
0
: gr
1
, S
p
)?
?
n
i=1
P (?S
c
i
1
, ..., S
c
i
m
i
?|gr
i
: gr
i+1
, S
p
)
The multiplication implements the independence
assumption between form and function underlying
the Separation Hypothesis, and the conditioning
we articulate captures one possible way to model a
systematic many-to-many correspondence.
4.3 The Grammar
We use a probabilistic treebank grammar in which
the different parameters and their probability dis-
tributions are read off and estimated from the
treebank trees. Clause-level (or clause-like) con-
stituents such as S, SQ, FRAG, FRAGQ, inter-
nally complex VPs and a small number NPs can
head RNs. For the rest we use flat CFG rules.
We use a limited set of grammatical relations,
namely, ?Predicate?, ?Subject?, ?Object? and ?Com-
plement? ? making the distinction between a
nominal complement and a verbal (infinitival) one.
Our linguistic elements are morphosyntactic rep-
resentations of labeled non-terminal constituents,
where the morphosyntactic representations of con-
stituents incorporate morphological information
percolated from surface forms and syntactic infor-
mation about the constituent?s environment.
892
(a) S
NP-SBJ VP-PRD NP-OBJ
S
{PRD,OBJ,SBJ}@S
SBJ@S
NP
PRD@S
VP
OBJ@S
NP
(b) S
VP-PRD NP-SBJ NP-OBJ
S
{PRD,OBJ,SBJ}@S
PRD@S
VP
SBJ@S
NP
OBJ@S
NP
Figure 1: Generating Canonical and Non-Canonical Configurations: The CF depictions of the S level constituents at the
LHS of (a) and (b) are distinct, whereas the RR-CFG representations at the RHS of (a) and (b) share the projection of GRs and
differs in their configuration ? while (a) generates an SVO order, (b) generates a so-called Verb-Initial (VI) construction.
(c) S
VP-PRD ADVP NP-SBJ NP-OBJ DOT
S
{PRD,OBJ,SBJ}@S
PRD@S
VP
PRD:SBJ@S
ADVP
SBJ@S
NP
OBJ@S
NP
OBJ:@S
DOT
Figure 2: Generating Adjunction and Periphrastic Configurations: The CF depiction of S at the LHS of (c) generates
complements, adjuncts, and punctuation in one go, whereas the RR-CFG representation at the RHS generates first the projec-
tion of core grammatical elements and then the configuration of a modified affirmative sentence in which they are realized.
(Similarly, realising a question configuration using inversion in, e.g., English, naturally follows).
(d) S
VP-PRD
VB
NP
[Def+,Acc+]
-OBJ
AT
[Acc+]
NP
[Def+]
NNT NN
[Def+]
NP
[Def+]
-SBJ
NN
[Def+]
S
{PRD,OBJ,SBJ}@S
PRD@S
VP
VB
OBJ@S
NP
[Def+,Acc+]
AT
[Acc+]
NP
[Def+]
NNT NN
[Def+]
SBJ@S
NP
[Def+]
NN
[Def+]
Figure 3: Realizing Grammatical Relations with bounded and unbounded Morphemes: The CF depiction of the S level
constituent at the LHS of (d) shows a strong dependence between the position of syntactic constituents and the morphologically
realized features percolated from lower surface forms. In the RR-CFG representation at the RHS the feature distribution among
sub constituents is dependent on grammatical relations, independently of their positioning. The realization stage generates a
morphosyntactic paradigm in one go, allowing to capture meaningful collocations and idiosyncrasies, e.g., the Xor relation of
the Acc+ and def+ features when marking direct objects in MH (Danon, 2001).
S-CNJ
S S CC S DOT
S
{SCNJ,SCNJ,SCNJ}@S
SCNJ@S
S
SCNJ@S
S
SCNJ:SCNJ@S
CC
SCNJ@S
S
SCNJ:@S
DOT
Figure 4: Generating a Conjunction Structure: The conjunction structure in the LHS of (e) is generated by the RR-CFG
on the RHS in three stages. First, a relational network of finite number of conjuncts is generated, then a configuration for the
conjuncts and conjunction markers (in MH, a CC before the last conjunct) is proposed, and finally the different conjuncts are
generated conditioned on the same grammatical relation and the same parent. (Note that the possibility of different means for
realizing conjunction, e.g., using morphemes, punctuation or multiple adpositions, falls out naturally from this setup.)
893
5 Experiments
Data The data we use is taken from the Modern
Hebrew Treebank (MHTB) (Sima?an et al, 2001)
which consists of 6501 sentences from the newspa-
per ?haaretz? annotated with phrase-structure trees
and decorated with various morphological and
functional features. We use version 2.0 of the tree-
bank3 which we processed and head-annotated as
in (Tsarfaty and Sima?an, 2007). We experimented
with sentences 1?500 (development set) and sen-
tences 501?6001 (training set), and used sentences
6001-6501 (test set) for confirming our best result.
Models Our Plain models use the coarse-level
MHTB category-labels enriched with various mor-
phological features. Our morphological represen-
tation Base varies with respect to the use of the per-
colated features definiteness Def and accusativity
Acc. Constituents? morphosyntactic representa-
tions enriched with their head PoS tag are referred
to as Head and grand-parent encodings as Parent.
For each combination of morphological and syn-
tactic features we experimented with a state-split
PCFG and with our RR-PCFG implementation.
Procedure We read off our models? parame-
ters from the decorated phrase-structure trees in
the MHTB, and use relative frequency estimation
to instantiate their probability distributions. We
smooth lexical rules using a PoS-tags distribution
we learn for rare-words, where the ?rare? threshold
is set to 1. We then use BitPar, a general purpose
chart parser,4 to find the most likely structures,
and we extract the corresponding coarse-grained
tree-skeletons for the purpose of evaluation.5 We
use PARSEVAL measures to quantitatively evalu-
ate our models and perform a qualitative analysis
of the resulting parse trees.
Results Table 1 shows the average F-Measure
value for all sentences of length ?40 in our
development set with/without punctuation. The
na??ve baseline implementation for our experi-
ments, the BasePlain PCFG, performs at the level
of 67.61/68.67 (comparable to the baseline re-
ported in (Tsarfaty and Sima?an, 2007)). For all
3http://www.mila.cs.technion.ac.il/
english/resources/corpora/treebank/ver2.
0/index.html
4
:http://www.ims.uni-stuttgart.de/tcl/
SOFTWARE/BitPar.html
5Our setup is comparable to English, which means that our
surface forms are segmented per PoS tag without specifying
their respective PoS tags and morphological features.
Syntax Plain Head Parent ParentHead
Morphology
Base (PCFG) 67.61/68.77 71.01/72.48 73.56/73.79 73.44/73.61
(RR-PCFG) 65.86/66.86 71.84/72.76 74.06/74.28 75.13/75.29
BaseDef (PCFG) 67.68/68.86 71.17/72.47 74.13/74.39 72.54/72.79
(RR-PCFG) 66.65/67.86 73.09/74.13 74.59/74.59 76.05/76.34
BaseDefAcc (PCFG) 68.11/69.30 71.50/72.75 74.16/ 74.41 72.77/73.01
(RR-PCFG) 67.13/68.01 73.63/74.69 74.65/74.79 76.15/ 76.43
Table 1: Parsing Results for Sentences of Length < 40
in the Development Set: Averaged F-Measure With/Without
Punctuation. Base refers to coarse syntactic categories, Def
indicates percolating definiteness values, Acc indicated per-
colating accusativity marking. The underlined results repli-
cate previously reported results in similar settings.
models in the Plain column the simple PCFG out-
performs the RR-variety. Yet, the contribution of
percolated morphological features is higher with
the RR-PCFG than with the simple PCFG.
Moving to the Head column, we see that all RR-
models already outperform their enriched PCFG
counterparts. Again, morphological information
contributes more to the RR-variety. The best result
for this column, achieved by the BaseDefAccHead
RR-model (63.73/64.69), outperforms its PCFG
counterpart as well as all two-dimensional models
reported by (Tsarfaty and Sima?an, 2007). In the
Parent column), our RR-variety continues to out-
perform the PCFG albeit in an insignificant rate.
(Both results are at the same level as the best model
of (Tsarfaty and Sima?an, 2007).)
Finally, for all models in the ParentHead col-
umn the RR-models outperform their PCFG coun-
terparts to a significant degree. Similarly to the
Head column, the more morphological informa-
tion is added, the greater the improvement is. Our
best RR-model, BaseDefAccParentHead, scores
almost 10pt (25% error reduction) more than the
Plain PCFG, it is about 3.5pt better (13% error re-
duction) than a state-split PCFG using the same
information, and almost 2pt (7% error reduction)
more than the best results reported for MH so far.
We confirmed the results of our best model on
our test set, for which our baseline (BasePlain)
obtained 69.63/70.31. The enriched PCFG
of DaseDefAccHeadParent yields 73.66/73.86
whereas the RR-PCFG yields 75.83/75.89. The
overall performance for PCFGs is higher on this
set, yet the RR-model shows a notable improve-
ment (about 9% error reduction).
6 Analysis and Discussion
The trends in our quantitative analysis suggest that
the RR-models are more powerful in exploiting
different sorts of information encoded in parse
894
(a) S
NP
CDT
EFRWT
tens-of
NP
NN
ANFIM
people
VP
VB
MGIEIM
arrive
PP
IN
M
from
NP
NNP
TAILND
Thailand
PP
IN
L
to
NP
NNP
IFRAL
Israel
...
(b) S
NP
CDT
EFRWT
tens-of
NP
NN
ANFIM
people
VP
VB
MGIEIM
arrive
PP
IN
M
from
NP
NP
NNP
TAILND
Thailand
PP
IN
L
to
NP
NNP
IFRAL
Israel
....
Figure 5: Qualitative Analysis of Sentence (Fragment)
#1: (a) is the gold tree fragment, correctly predicted by our
best RR-PCFG model. (b) is the tree fragment predicted by
the PCFG corresponding to previously reported results.
trees, be it morphological information coming
from dominated surface forms or functional infor-
mation on top of syntactic categories.
We have shown that head information, which
has very little contribution to parsing accuracy as
a mere state-split, turns out to have crucial ef-
fects within the RR-models. For state-splits based
PCFGs, adding head information brings about
a category fragmentation and decreasing perfor-
mance. The separation between form and function
we articulate in the RR-approach allows us to cap-
ture generalizations concerning the distribution of
syntactic constituents under heads based on their
grammatical function, and use fine-grained fea-
tures to predict their morphosyntactic behaviour.
We have further shown that morphological in-
formation contributes a substantial improvement
when adopting the RR-approach, which is inline
with the linguistic insight that there is a correlation
between morphological marking on top of surface
forms and the grammatical function their domi-
nating constituents realize. Morphological infor-
mation is particularly useful in the presence of
heads. Taken together, head and percolated fea-
tures implement a rather complete conceptualiza-
tion of multiple exponence.
To wrap up the discussion, we leave numbers
aside and concentrate on the kind of structures pre-
dicted by our best model in comparison to the
ones suggested by previously reported unlexical-
ized PCFGs ((Tsarfaty and Sima?an, 2007), un-
derlined in our table). Due to lack of space we
(a) S
PP
MCD FNI
on the
other hand
VP
VB
MTIR
allows
NP
NNP
MSRD HEBWDH WHRWWXH
the ministry of...
VP
VB
LHESIK
to-
employ
NP
EWBDIM ZRIM
foreign
workers
PP
B..
in..
(b) S
PP
IN
M
from
NP
NNT
CD
side
NP
CDT
FNI
two
NP
NNT
MTIR
allows
NP
NNP
MSRD HEBWDH WHRWWXH
the ministry of...
VP
LHESIK
to-
employ
NP
NP
EWBDIM
workers
ADJP
ZRIM
foreigners
PP
B..
in..
Figure 6: Qualitative Analysis of Sentence (Fragment)
#4: (a) is the gold tree fragment, correctly predicted by our
best RR-PCFG model. (b) is the tree fragment predicted by
the PCFG corresponding to previously reported results.
only discuss errors found within the first 10 parsed
sentence, yet we note that the qualitative trend
we describe here persists throughout our develop-
ment set. Figures (5) and (6) show a gold tree
(a fragment of sentence #1) correctly predicted
by our best RR-model (a) in comparison with the
one predicted by the respective PCFG (b). The
tree fragment in figure (5) shows that the RR-
grammar bracketed and attached correctly all the
constituents that bear grammatical relations to the
S clause (5a). The corresponding PCFG conflated
the ?to? and ?from? phrases to a rather meaning-
less prepositional phrase (5b). For (a fragment of)
sentence #4 in our set (figure 6) the RR-model re-
covered all grammatically meaningful constituents
under the S clause and under the internal VP (6a).
Notably, the PCFG in (6b) recovered none of them.
Both grammars make attachment mistakes internal
to complex NPs, but the RR-model is better at iden-
tifying higher level constituents that correlate with
meaningful grammatical functions.
Our qualitative analysis suggests that our model
is even more powerful than our quantitative analy-
sis indicates, yet we leave the discussion of better
ways to quantify this for future research.
A Note on Related Work Studies on parsing
MH to date concentrate mostly on spelling out
the integration of a PCFG parser with a mor-
phological disambiguation component (e.g., (Tsar-
faty, 2006; Goldberg and Tsarfaty, 2008)). On a
setup identical to ours (gold segmentation, no PoS)
the latter obtained 70pt. (Tsarfaty and Sima?an,
895
2007) examined the contribution of horizontal
and vertical conditioning to an unlexicalized MH
parser and concluded that head-driven Markoviza-
tion performs below the level of vertical condi-
tioning enriched with percolated features. We do
not know of existing dependency-parsers applied
to parsing MH or mildly-context-sensitive broad-
coverage parsers applied to parsing a Semitic lan-
guage.6 To the best of our knowledge, this is the
first fully generative probabilistic framework that
models explicitly morpho-syntactic interaction to
enhance parsing for non-configrational languages.
7 Conclusion
Projection and Realization are two sides of the
same coin. Projection determines which gram-
matical relations appear in the syntactic represen-
tation, and Realization determines how such rela-
tions are realized. We suggest that the Relational-
Realizational (RR) approach is adequate for pars-
ing languages characteristically different from En-
glish, and we illustrate it with an application to
parsing MH. We show that our approach to mod-
eling the interaction between syntactic categories
and a handful of percolated features already yields
a notable improvement in parsing accuracy and
substantially improves the quality of suggested
parses. Incorporating additional functional and
morphological information, we expect, will help
bridging the gap in performance between Hebrew
and configurational languages such as English.
Acknowledgements We thank Remko Scha,
Jelle Zuidema, Yoav Goldberg, and three anony-
mous reviewers for comments on earlier drafts.
The first author wishes to thank Jim Blevins, Julia
Hockenmeir, Mark Johnson, Kevin Knight, Chris
Manning, Joakim Nivre and Gerald Penn for stim-
ulating discussion. Errors are our own. The work
of the first author is funded by the Dutch Science
Foundation (NWO), grant number 017.001.271.
References
Anderson, S. R. 1992. A-Morphus Morphology. Cam-
bridge University Press.
Beard, R. 1988. The Separation of Derivation and
Affixation: Toward a Lexeme-Morpheme Base Mor-
phology. Quarderni di semantica, pages 277?287.
6Parsing MSA has been explored with a treebank three
times as large as ours using a head-driven lexicalized parser
obtaining around 78% accuracy (http://papers.ldc.
upenn.edu/). The input setup assumes gold segmentation
as well as PoS tags information and some diacritization.
Blevins, J. P. 2008. Periphrasis as Syntactic Ex-
ponence. In Ackerman, F., J.P. Blevins, and G.S.
Stump, editors, Patterns in Paradigms. CSLI.
Bod, R. 2003. An Efficient Implementation of a New
Dop Model. In Proceedings of EACL.
Bresnan, Joan. 1979. A Theory of Grammatical Repre-
sentation. Duplicated Lecture Notes. Department of
Linguistics and Philosophy, MIT.
Charniak, E. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI.
Collins, M. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Comp. Linguistics.
Danon, G. 2001. Syntactic Definiteness in the Gram-
mar of Modern Hebrew. Linguistics, 6(39).
Goldberg, Y. and R. Tsarfaty. 2008. A Single Gener-
ative Framework for Joint Morphological Segmenta-
tion and Syntactic Parsing. In Proceedings of ACL.
Johnson, M. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
Klein, D. and C. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL.
McDonald, R., F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms. In Proceedings of HLT.
Nivre, J. 2007. Data-driven Dependency Parsing
Across Languages and Domains; Perspectives from
the CoNLL-2007 Shared Task. In Proceedings of
IWPT.
Perlmutter, D. M. 1982. Syntactic Representation,
Syntactic levels, and the Notion of Subject. In Ja-
cobson, Pauline and Geoffrey Pullum, editors, The
Nature of Syntactic Representation. Springer.
Postal, P. M. and D. M. Perlmutter. 1977. Toward
a Universal Characterization of Passivization. In
BLS3.
Postal, P. M. 1982. Some Arc-Pair Grammar Decrip-
tions. In Jacobson, P. and G. K. Pullum, editors, The
Nature of Syntactic Representation. Dordrecht.
Sima?an, K., A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a Tree-Bank for Modern Hebrew
Text. In Traitement Automatique des Langues.
Tsarfaty, R. and K. Sima?an. 2007. Three-Dimensional
Parametrization for Parsing Morphologically Rich
Languages. In Proceedings of IWPT.
Tsarfaty, R. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
Zwicky, A. M. 1993. Heads, Bases, and Functors. In
Corbett, G.G., N. Fraser, and S. McGlashan, editors,
Heads in Grammatical Theory. Cambridge.
896
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842?851,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Alternative to Head-Driven Approaches for
Parsing a (Relatively) Free Word-Order Language
Reut Tsarfaty Khalil Sima?an Remko Scha
Institute for Logic Language and Computation
University of Amsterdam
{r.tsarfaty,k.simaan,r.scha}@uva.nl
Abstract
Applying statistical parsers developed for
English to languages with freer word-
order has turned out to be harder than
expected. This paper investigates the
adequacy of different statistical parsing
models for dealing with a (relatively)
free word-order language. We show
that the recently proposed Relational-
Realizational (RR) model consistently
outperforms state-of-the-art Head-Driven
(HD) models on the Hebrew Treebank.
Our analysis reveals a weakness of HD
models: their intrinsic focus on configu-
rational information. We conclude that the
form-function separation ingrained in RR
models makes them better suited for pars-
ing nonconfigurational phenomena.
1 Introduction
Parsing technology has come a long way since
Charniak (1996) demonstrated that a simple tree-
bank PCFG performs better than any other parser
(with F
1
75 accuracy) on parsing the WSJ Penn
treebank (Marcus et al, 1993). Treebank Gram-
mars (Scha, 1990; Charniak, 1996) trained on
large corpora nowadays present the best available
means to parse natural language text.
The performance curve for parsing the WSJ was
a steep one at first, as the incorporation of no-
tions such as head, distance, subcategorization
(Charniak, 1997; Collins, 1999) brought about
a dramatic increase in parsing accuracy to the
level of F
1
88. Discriminative approaches, Data-
Oriented Parsing (?all-subtrees?) approaches, and
self-training techniques brought further improve-
ments, and recent results are starting to level off at
around F
1
92.1 (McClosky et al, 2008).
As the interest of the NLP community grows
to encompass more languages, we observe efforts
towards adapting an English parser for parsing
other languages (e.g., (Collins et al, 1999)), or
towards designing a language-independent frame-
work based on principles underlying the mod-
els for parsing English (Bikel, 2002). The per-
formance curve for parsing other languages with
these models looks rather different. A case in point
is Modern Standard Arabic. Since the initial ef-
fort of (Bikel, 2002) to parse the Arabic treebank
(Maamouri et al, 2004), which yielded F
1
75 ac-
curacy, four years and successive revisions have
led to no more than F
1
79 (Maamouri et al, 2008).
This pattern from Arabic is not peculiar. The
level of state-of-the-art results for other languages
still lags behind those for English, even after
putting considerable effort into the adaptation.1
Given that these languages are inherently differ-
ent from English and from one another, it appears
that we cannot avoid a question concerning the ad-
equacy of the models used to parse them. That is,
given the properties of a language, which model-
ing strategy would be appropriate for parsing it?
Until recently, there has been practically
no computationally affordable alternative to the
Head-Driven (HD) approach in the development
of phrase-structure based statistical parsing mod-
els. Recently, we proposed the Relational-
Realizational (RR) approach that rests upon differ-
ent premises (Tsarfaty and Sima?an, 2008). The
question of how the RR model fares against the
HD models that have so far been predominantly
used has never been tackled. Yet, it is precisely
such a comparison that can shed new light on the
question of adequacy we posed above.
Empirically quantifying the effects of differ-
ent modeling choices has been addressed for En-
glish by, e.g., (Johnson, 1998; Klein and Manning,
2003), and for German by, e.g., (Dubey, 2004;
1Consider, e.g., ?The PaGe shared task on parsing Ger-
man? (Kubler, 2008), reporting F
1
75, F
1
79, F
1
83 for the
participating parsers.
842
Rafferty and Manning, 2008). This paper provides
an empirical systematic comparison of conceptu-
ally different modeling strategies with respect to
parsing Hebrew. This comparison is intended to
provide a first answer to the question of parser ad-
equacy in the face of word-order freedom.
Our two empirical results are unequivocal.
Firstly, RR models significantly outperform HD
models (about 2 points absolute improvement in
F
1
) in parsing the Modern Hebrew treebank. In
particular, RR models show better performance
in identifying the constituents for which syntactic
positions are relatively free. Secondly, we show
a novel variation of the HD model, incorporating
the Relational notions of the RR model, on the hy-
pothesis that this might bridge the gap. The RR
model remains superior.
Our post-experimental analysis shows that HD
modeling is inherently problematic for parsing a
language with freer word-order because of the
hard-wiring of notions such as left, right and dis-
tance from the head. RR models, taking a prin-
cipled approach towards capturing variable form-
function correspondence patterns, are better suited
for parsing nonconfigurational phenomena.
2 The Data
This section describes some properties of Modern
Hebrew (henceforth, Hebrew) that make it signifi-
cantly different from English. These properties af-
fect the syntactic representations found in the He-
brew Treebank and the kind of syntactic phenom-
ena a parser for Hebrew has to cope with.
Modern Hebrew is a Semitic language with a
canonical SVO word-order pattern,2 yet it allows
considerable freedom in the placement of syntac-
tic constituents in a clause. For example, linguistic
elements of any kind may be fronted, triggering
an inversion familiar from Germanic languages
as in (1b) (Triggered Inversion (TI) in (Shlonsky,
1997)). Under some information structuring con-
ditions, Verb Initial (VI) constructions are also al-
lowed, as in (1c) (Melnik, 2002). All sentences
in (1) thus mean ?Dani gave the present to Dina?,
despite their different word-ordering.
(1) a. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
b. et hamatana natan dani ledina
ACC the-present gave Dani to-Dina
2SVO is an abbreviation for the Subject-Verb-Object type
in the basic word-order typology of (Greenberg, 1963).
Word Order Frequency Relative Frequency
SV 1612 41%
VS 1144 29%
No S 624 16%
No V 550 14%
Table 1: Modern Hebrew Predicative Clause-
Types in 3930 Predicative Matrix Clauses in the
Training Set of the Modern Hebrew Treebank.
c. natan dani et hamatana ledina
gave Dani ACC the-present to-Dina
A corpus study we conducted on a fragment of
the Modern Hebrew treebank reveals that although
there is a significant number of subjects preceding
verbs in simple (matrix) clauses (41%), there are
also a fair number of sentences for which this or-
der is reversed (29%), and there is evidence for
other configurations, such as empty realization of
subjects (16%) and non-verbal realization of pred-
icates (14%).
In the face of such lack of consistency in its
configurational position, the grammatical function
Object in Hebrew is indicated by Differential Ob-
ject Marking (DOM) (Aissen, 2003). NP objects
in Hebrew are marked for accusativity (using the
marker et) if they are also marked for definiteness
(indicated by the prefix ha). So, in contrast with
(2a)-(2b), the indefinite object renders (2c) un-
grammatical, and the missing accusativity renders
(2d) awkward. The fact that marking NP objects
involves the joint contribution of multiple surface
elements (et, ha) contributing features to the NP
constituent is referred to as extended exponence
(Matthews, 1993, p. 182).
(2) a. dani natan matana ledina
Dani gave present to-Dina
?Dani gave a present to Dina?
b. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
?Dani gave the present to Dina?
c. *dani natan et matana ledina
Dani gave ACC present to-Dina
d. ??dani natan hamatana ledina
Dani gave the-present to-Dina
These data pose a challenge to generative pars-
ing models, as they would be required to gener-
ate alternative word-order patterns while maintain-
ing a coherent pattern of object marking, encom-
843
passing the contribution of multiple surface expo-
nents. The question this paper addresses is there-
fore what kind of modeling approach would be ad-
equate for modeling the interplay between syntax
and morphology in marking grammatical relations
in Hebrew, as reflected by the sentence-pair (3).
They both mean, roughly, ?Dani gave the present
to Dina yesterday; their word-order vary, but the
pattern of object marking is retained.
(3) a. dani natan etmol et hamatana ledina
Dani gave yesterday ACC the-present
to-Dina
b. et hamatana natan etmol dani ledina
ACC the-present gave yesterday dani
to-dina
3 The Models
The different models we experiment with are all
trained on syntactic structures annotated in the
Modern Hebrew Treebank (Sima?an et al, 2001).
The native representation of clause-level cate-
gories in the Treebank employs flat structures.
This choice was made due to the lack of empirical
evidence in Hebrew for grouping freely positioned
syntactic elements to form a constituent.3 In order
to compensate for the ambiguity in the interpreta-
tion of flat structures, additional information such
as morphological marking and grammatical func-
tion labels is added to the phrase-structure trees.
3.1 The State-Splits Approach
The simplest way to encode grammatical func-
tions information on top of the phrase-structure
representation in the treebank is by decorating
non-terminal nodes with morphological or func-
tional features, similarly to the rich representation
format of syntactic categories in GPSG. This is
the approach taken by the annotators of the He-
brew treebank in which information about mor-
phological marking appears at multiple levels of
constituency (Guthmann et al, 2009), and func-
tional features (such as subject, object, etc.) deco-
rate phrase-level constituent labels (Sima?an et al,
2001). The S-level representation of our example
sentences (3a)?(3b) then would be as we depict
in figure 1, which can be read off as feature-rich
3Such clauses are defined formally as exocentric in for-
mal theories of syntax, and are used to describe syntactic
structures in, e.g., Tagalog, Hungarian and Warlpiri (Bres-
nan, 2001, page 110). This flat representation format is char-
acteristic of treebanks for other languages with relatively-free
word-order as well, such as German (cf. (Kubler, 2008)).
PCFG productions. We refer to this approach as
the State-Splits (SP) approach, which serves as the
baseline for the rest of our investigation.
3.2 The Head-Driven Approach
Following the linguistic wisdom that the inter-
nal organization of syntactic constituents revolves
around their heads, Head-Driven (HD) models
have been proposed by (Magerman, 1995; Char-
niak, 1997; Collins, 1999). In a generative HD
model, the head daughter is generated first, con-
ditioned on properties of the mother node. Then,
sisters of the head daughter are generated condi-
tioned on the head, typically by left and right gen-
eration processes. Overall, HD processes have the
modeling advantage that they capture structurally-
marked positions that characterize the argument
structure of the sentence. The simplest possible
process uses unigram probabilities, but (Klein and
Manning, 2003) show that using vertical and hori-
zontal Markovization improves parsing accuracy.4
An unlexicalized generative HD model will
generate our two example sentences as we illus-
trate in figure 2. The generation of the context-
free events in figure 1 is then broken down to
seven different context-free parameters each, en-
coding head-parent and head-sister structural rela-
tionships ? the latter mediated with a structurally-
marked delta function (?
i
). The rich morpho-
logical representation of phrase-level NP objects
(+def/acc), for instance, is conditioned on the
head sister, its direction, and the distance from the
head (check, e.g., nodes ?
L
1
,?
R
2
).
3.3 The Relational-Realizational Approach
The Relational-Realizational (RR) parsing model
of (Tsarfaty and Sima?an, 2008) similarly decom-
poses the generation of the context-free events in
figure 1 into multiple independent parameters, but
does so in a conceptually different way. Instead of
decomposing a context-free event to head and sis-
ters, the RR model is best viewed as a generative
grammar that decomposes it to form and function.
The RR grammar first generates a set of gram-
matical functions depicting the Relational Net-
work (RN) (Perlmutter, 1982) of the clause. This
4The success of Head-Driven models (Charniak, 1997;
Collins, 2003) was initially attributed to the fact that they
were fully lexicalized, but (Klein and Manning, 2003) show
that an unlexicalized model combining Head-Driven Marko-
vian processes with linguistically motivated state-splits can
approach the performance of fully lexicalized models.
844
(3a) S
NP-SBJ
Dani
VP-PRD
natan
gave
ADVP
etmol
yesterday
NP
+D+ACC
-OBJ
et-hamatana
the-present
PP-COM
le-dina
to-Dina
(3b) S
NP
+D+ACC
-OBJ
et-ha-matana
the-present
VP-PRD
natan
gave
ADVP
etmol
yesterday
NP-SBJ
Dani
Dani
PP-COM
le-dina
to-Dina
Figure 1: The State-Splits Approach for Ex. (3)
(3a) S
V P@S
L,?
L
1
, V P@S
NP
Dani
Dani
HEAD,V P@S
VP
natan
gave
R,?
R
1
, V P@S
ADVP
etmol
yesterday
R,?
R
2
, V P@S
NP
+D+ACC
et-ha-matana
the-Present
R,?
R
3
, V P@S
PP
le-dina
to-Dina
(3b) S
V P@S
L,?
L1
, V P@S
NP
+D+ACC
et-ha-matana
the-present
HEAD,V P@S
VP
natan
gave
R,?
R
1
, V P@S
ADVP
etmol
yesterday
R,?
R
2
, V P@S
NP
Dani
Dani
R,?
R
3
, V P@S
PP
le-dina
to-Dina
Figure 2: The Head-Driven Approach for Ex. (3)
(3a) S
{SBJ,PRD,OBJ,COM}@S
SBJ@S
NP
Dani
Dani
PRD@S
VP
natan
gave
PRD : OBJ@S
ADVP
etmol
yesterday
OBJ@S
NP
+D+ACC
et-hamatana
the-present
COM@S
PP
le-dina
to-Dina
(3b) S
{SBJ,PRD,OBJ,COM}@S
OBJ@S
NP
+D+ACC
et-ha-matana
the-Present
PRD@S
VP
natan
gave
PRD : SBJ@S
ADVP@S
etmol
yesterday
SBJ@S
NP
Dani
Dani
COM@S
PP
le-dina
to-Dina
Figure 3: The Relational-Realizational Approach
RN provides an abstract set-theoretic representa-
tion of the argument structure of the clause.5 This
is called the projection phase. Then an ordering
of the grammatical relations is generated, includ-
ing reserved contextual slots for adjunction and/or
punctuation marks. This is called the configura-
tion phase. Finally, each of the grammatical func-
tion labels and adjunction slots gets realized as a
morphosyntactic representation (a category label
plus dominated morphological features) of the re-
spective daughter constituent. This is called the
realization phase.6
Figure 3 shows the generation of sentences
(3a)?(3b) following the projection, configuration
and realization phases corresponding to the top-
down context-free layers of the tree. In both
cases, the same relational network is generated,
capturing the fact that they have the same argu-
ment structure. Then the different orderings of
the grammatical elements are generated, reserving
an adjunction slot for sentential modification (la-
beled by short context). Interestingly, the HD/RR
models for our sentences are of comparable size
(seven parameters) but the parameter types en-
code radically different notions. Illustrative of the
difference is the realization of a morphologically
marked NP object. In the RR model this is con-
ditioned on a grammatical relation (check, for in-
stance, node OBJ@S) and in the HD model it is
conditioned on linear ordering or configurational
notions such as left, right and distance.
4 Experiments
Goal We set out to compare the performance
of the different modeling approaches for pars-
ing Modern Hebrew. Considerable effort was de-
voted to making the models strictly comparable,
in terms of preparing the data, defining statistical
events, and unifying the rules determining cross-
cutting linguistic notions (e.g., heads and predi-
cates, grammatical functions and subcat sets). We
spell out some of the setup considerations below.
Data We use the Modern Hebrew treebank
(MHTB) (Sima?an et al, 2001) consisting of 6501
sentences from news-wire texts, morphologically
analyzed and syntactically annotated as phrase-
5Unlike in HD models or dependency grammars, the head
predicative element has no distinguished status here.
6Realization of adjunction slots (but not of function la-
bels) may generate multiple sisters adjoining at a single
position.
845
GF Description Applicable to. . .
PRD Predicative Elements VP, PREDP
SBJ Grammatical Subjects NP, SBAR
OBJ Direct Objects NP
COM Indirect Objects NP, PP
FInite Complements SBAR
IC Infinitival Complements VP
CNJ A Conjunct within
a Conjunction Structure All
Table 2: Grammatical Functions in the MHTB
SP-PCFG Expansion P(C
l
n
, . . . , C
h
, . . . , C
r
n
|P )
HD-PCFG Head P(C
h
|P )
Left Branch? P(L:?
l
1
, H:?
h
|C
h
, P )
Right Branch? P(C
h
, R:?
r
1
|?
h
, C
h
, P )
Left Arg/Mod P(C
l
i
,?
l
i+1
| L ,?
l
i
, C
h
, P )
Right Arg/Mod P(C
r
i
,?
r
i+1
| R ,?
r
i
, C
h
, P )
Left Final? P(C
1
| L ,?
l
n?1
, C
h
, P )
Right Final? P(C
n
| R ,?
r
n?1
, C
h
, P )
RR-PCFG Projection P({gr
1
, . . . , gr
m
}|P )
Configuration P(?gr
1
, . . . , gr
m
?|{gr
1
, . . . , gr
m
}P )
Realization P(C
j
|gr
j
, P )
Adjunction P(C
j
1
, . . . , C
j
n
|gr
j
: gr
j+1
, P )
Table 3: PCFG Parameter Classes for All Models
structure trees. In our version of the MHTB, def-
initeness and accusativity features are percolated
from the PoS-tags level to phrase-level categories,
extending the procedure of (Guthmann et al,
2009). For all models, we applied non-terminal
state-splits distinguishing finite from non-finite
verb forms and possessive from non-possessive
noun phrases. We head-annotated the treebank,
and based on the ?subject?, ?object?, ?complement?
and ?conjunction? labels in the MHTB we devised
an automatic procedure to annotate all the gram-
matical functions indicated in table 2.7
Procedure For all models, we learn a PCFG by
reading off the parameters described in table 3,
in accordance with the trees depicted in figures
1?3.8 For all models, we use relative frequency
estimates. For lexical parameters, we use a sim-
ple smoothing procedure assigning probability to
unknown words using the per-tag distribution of
rare words (?rare? threshold set to < 2). The in-
put to our parser consists of morphologically seg-
mented surface forms, and the parser has to as-
7The enhanced corpus will be available at www.
science.uva.nl/
?
rtsarfat/resources.htm.
8Our training procedure is strictly equivalent to the
transform-detransform methodology of (Johnson, 1998), but
we implement a tree-traverse procedure as in (Bikel, 2002)
collecting all parameters per event at once.
sign the syntactic as well as morphological anal-
ysis to the surface segments.9 We use the stan-
dard development/training/test split as in (Tsarfaty
and Sima?an, 2008). Since our goal is a detailed
comparison and fine-grained analysis of the results
we concentrate on the development set. We use
a general-purpose CKY parser (Schmid, 2004) to
exhaustively parse the sentences, and we strip off
all model-specific information prior to evaluation.
Evaluation We use standard Parseval measures
calculated for the original, flat, canonical repre-
sentation of the parse trees.10 We report Pre-
cision/Recall for the coarse-grained non-terminal
categories. In addition to overall Parseval scores
we report the accuracy results Per Syntactic Cate-
gory. We further report model size in terms of the
number of parameters. As is well known in Ma-
chine Learning, models with more parameters re-
quire more data to learn, and are more vulnerable
to sparseness. In our evaluation we thus follow the
rule of thumb that (all else being equal) for mod-
els of equal size the better performing model is
preferred, and for models with equal performance,
the smaller one is preferred.
5 Results and Analysis
5.1 Overall Results
Table 4 shows the parsing results for the State-
Split (SP) PCFG, the Head-Driven (HD) PCFG
and the Relational-Realizational (RR) PCFG
models on parsing the Modern Hebrew Treebank,
with definiteness and accusativity marked on PoS-
tags as well as phrase-level categories. For all
models, we experiment with grandparent encod-
ing. For non-HD models, we also examine the
utility of a head-category split.11
9This setup is more difficult than, e.g., the Arabic parsing
setup of (Bikel, 2002), as they assume gold-standard pos-tags
as input. Yet it is easier than the setup of (Tsarfaty, 2006;
Goldberg and Tsarfaty, 2008) which uses unsegmented sur-
face forms as input. The decision to use segmented and un-
tagged forms was made to retain a realistic scenario. Mor-
phological analysis is known to be ambiguous, and we do
not assume that morphological features are known up front.
Morphological segmentation is also ambiguous, but for our
purposes it is unavoidable. When comparing different mod-
els on an individual sentence they may propose segmenta-
tion to sequences of different lengths, for which accuracy re-
sults cannot be faithfully compared. See (Tsarfaty, 2006) for
discussion.
10The flat canonical representation also allows for a fair
comparison that is not biased by the differing branching fac-
tors of the different models.
11In HD models, a head-tag is already assumed in the con-
ditioning context for sister nodes (Klein and Manning, 2003).
846
SP-PCFG
Grand-Parent ? ? + +
Head-Tag ? + ? +
Prec/Rec 70.05/72.40 71.14/72.03 74.66/74.35 71.99/72.17
(#Params) (4995) (8366) (7385) (11633)
HD-PCFG
Grand-Parent ? ? + +
Markov 0 1 0 1
Prec/Rec 66.87/71.64 70.40/74.35 73.04/71.94 73.52/74.84
(#Params) (6678) (10015) (19066) (21399)
RR-PCFG
Grand-Parent ? ? + +
Head Tag ? + ? +
Prec/Rec 69.90/73.96 72.96/75.73 74.19/75.03 76.32/76.51
(#Params) (3791) (7546) (7611) (13618)
Table 4: The Performance of Different Models
in Parsing Hebrew: Parsing Results Prec/Recall
for Sentences of Length ? 40.
For all models, grandparent encoding is help-
ful. For HD models, a higher Markovian order im-
proves performance. This shows that even in He-
brew there are linear-precedence tendencies that
help steer the disambiguation in the right direc-
tion, which is in line with our observation that
word-order patterns in Modern Hebrew are not
completely free (cf. table 1).
The best SP model performs equally or better
than all HD models. This might be due to the
smaller size of SP grammars, resulting in more ro-
bust estimates. But it is remarkable that, given the
feature-rich representation, such a simple treebank
grammar provides better disambiguation capacity
than linguistically articulated HD models. We at-
tribute this to the fact that parent-daughter rela-
tions have a stronger association with grammati-
cal functions than relations between neighbouring
nodes. For Hebrew, such adjacency relations may
be arbitrary due to word-order variability.
Overall, RR models show the best performance
for the set of all models with parent encoding, and
for the set of all models without. Our best RR
model shows 6.6%/8.4% Prec/Rec error reduction
from the best SP model. The Recall improvement
shows that the RR model is much better in gener-
alizing, recovering successfully more of the con-
stituents found in the gold representation. The
best RR model also outperforms HD models with
8.7%/6.7% Prec/Rec error reduction from the best
In our SP or RR models, head-information is used as yet an-
other feature-value pair rather than an object with a distin-
guished status during generation.
Model / SP-PCFG HD-PCFG RR-PCFG
Category
NP 77.39 / 74.32 77.94 / 73.75 78.96 / 76.11
PP 71.78 / 71.14 71.83 / 69.24 74.4 / 72.02
SBAR 55.73 / 59.71 53.79 / 57.49 57.97 / 61.67
ADVP 71.37 / 77.01 72.52 / 73.56 73.57 / 77.59
ADJP 79.37 / 78.96 78.47 / 77.14 78.69 / 78.18
S 73.25 / 79.07 71.07 / 76.49 72.37 / 78.33
SQ 36.00 / 32.14 30.77 / 14.29 55.56 / 17.86
PREDP 36.31 / 39.63 44.74 / 39.63 44.51 / 46.95
VP 76.34 / 80.81 77.33 / 82.51 78.59 / 81.18
Table 5: Per-Category Evaluation of Parsing
Performance for Different Models: Prec/Rec
Per Category Calculated for All Sentences.
HD model. The resulting precision improvement
of the RR relative to HD is larger than the im-
provement relative to SP, and the Recall improve-
ment pattern is reversed. So it seems that the HD
model generalizes better than the SP model, but
also gets generalizations wrong more often than
the SP model.
The RR model combines the generalization
advantage of breaking down context-free events
while it maintains the coherence advantage of
learning flat trees (cf. (Johnson, 1998)). The best
RR model obtains the best performance among
all models: F
1
76.41. To put this result in con-
text, for the setting in which the Arabic parser of
(Maamouri et al, 2008) obtains F
1
78.1, ? i.e.,
with gold standard feature-rich tags ? the best
RR model obtains F
1
83.3 accuracy which is the
best parsing result reported for a Semitic language
so far. RR models also have the advantage of re-
sulting in more compact grammars, which makes
learning and parsing with them much more com-
putationally efficient.
5.2 Per-Category Break-Down Analysis
To understand better the merits of the different
models we conducted a break-down analysis of
performance-per-category for the best performing
models of each kind. The break-down results are
shown in table 5. We divided the table into three
sets of categories: those for which the RR model
gave the best performance, those for which the SP
model gave the best performance, and those for
which there is no clear trend.
The most striking outcome is that the RR model
identifies at higher accuracy precisely those syn-
tactic elements that are freely positioned with re-
847
spect to the head: NPs, PPs, ADVPs and SBARs.
Adjectives, in contrast, have clear ordering con-
straints ? they always appear after the noun. S
level elements, when embedded, always appear
immediately after a conjunction or a relativizer.
In particular, NPs and PPs realize arguments and
adjuncts that may occupy different positions rela-
tive to the head. The RR model is better than the
other models in identifying those elements partly
because morphological information helps to dis-
ambiguate syntactically relevant chunks and make
correct attachment decisions about them.
Remarkably, predicative (verb-less) phrases
(PREDP), which are characteristic of Semitic lan-
guages, are hard to parse, but here too the RR does
slightly better than the other two, as it allows for
variability in the means to realize a (verbal or verb-
less) predicate. Both RR and HD models outper-
form SP for VPs, which is due to the specific na-
ture of VPs in the MHTB ? they exist only for
complement phrases with strict linear ordering.
6 Distances, Functions and
Subcategorization Frames
Markovian processes to the left and to the right of
the head provide a first approximation of the pred-
icate?s argument structure, as they capture trends
in the co-occurrences of constituents reflected in
their pattern of positioning and adjacency. But
as our results so far show, such an approxima-
tion is empirically less rewarding for a language
in which grammatical relations are not tightly cor-
related with structural notions.12
Collins (2003) attempted a more abstract for-
mulation of argument-structure by articulating left
and right subcat-sets. Each set represents those
arguments that are expected to occur at each side
of the head. Argument sisters (?complements?)
are generated if and only if they are required, and
their generation ?cancels? the requirement in the
set. Adjuncts (?modifiers?) may be freely gener-
ated at any position.
At first glance, such a dissociation of configura-
tional positions and subcategorization sets seems
to be more adequate for parsing Hebrew, because
it allows for some variability in the order of gen-
eration. But here too, since the model uses sets of
12Conditioning based on adjacency and distance is also
common inside dependency parsing models, and we conjec-
ture that this is one of the reasons for their difficulty in coping
with freer word-order languages, a difficulty pointed out in
(Nivre et al, 2007).
(3a) S
V P@S
L,{SBJ}, V P@S
NP
Dani
Dani
H,V P@S
VP
natan
gave
R,{OBJ,COM}, V P@S
ADVP
etmol
yesterday
R,{OBJ,COM}, V P@S
NP
+D+ACC
et-ha-matana
the-Present
R,{COM}, V P@S
PP
le-dina
to-Dina
(3b) S
V P@S
L,{OBJ}, V P@S
NP
+D+ACC
et-ha-matana
the-present
H,V P@S
VP
natan
gave
R,{SBJ,COM}, V P@S
ADVP
etmol
yesterday
R,{SBJ,COM}, V P@S
NP
Dani
Dani
R,{COM}, V P@S
PP
le-dina
to-Dina
Figure 4: The Relational Head-Driven Approach
constituent labels, it disambiguates the grammati-
cal functions of an NP solely based on the direc-
tion of the head, which is adequate for English but
not for Hebrew. In order to relax this association
further, we propose to replace constituent labels
in the subcat-sets with grammatical relations iden-
tical to the functional elements in the relational
network of the RR. This provides means to medi-
ate the cancellation of constituents in the sets with
their functions and correlate it with morphology.
To get an idea of the implications of such a
modeling strategy, let us consider our example
sentences in such a Relational-HD model as de-
picted in figure 4. Both representations share
the event of generating the verbal head. Sisters
are generated conditioned on the head and the
functional elements remaining to be ?cancelled?.
Each of the two trees consists of an event real-
izing an ?object?, one for an NP to the right of
the head, and the other for an NP to its left. In
both cases, an object constituent will be generated
jointly with the morphological features associated
with it. Evidently, when using sets of grammatical
relations instead of constituent-labels, correlation
of morphology and grammatical functions is more
straight-forward to maintain.
848
Model SP-PCFG HD-PCFG HD-PCFG HD-PCFG HD-PCFG RR-PCFG
Type of Distance ? Phrase-Level Intervening Left and Right Left and Right Left and Right Subcat Sets
or Subcategorization State-Splits Verb/Punc #Constituents Constituent Labels Function Labels Configuration
Precision/Recall 70.95/70.32 72.39 / 71.97 72.70 / 74.46 72.42 / 74.29 72.84/74.62 76.32/76.51
(#Params) (13884) (11650) (18058) (16334) (16460) (13618)
Table 6: Incorporating Distance and Grammatical Functions into Head-Driven Parsing Models
Reporting Precison/Recall (#Parameters) for Sentences Length < 40.
6.1 Results and Analysis
Table 6 reports the results of experimenting with
HD models with different instantiations of a dis-
tance function, starting from the standard notion
of (Collins, 2003) and ending with our proposed,
relational, function sets. For all HD models, we
retain the head, left and right generation cycle and
only change the conditioning context (?
i
) for sis-
ter generation.
As a baseline, we show the results of adding
grammatical function information as state-splits
on top of an SP-PCFG.13 This SP model presents
much lower performance than the RR model al-
though they are almost of the same size and they
start off with the same information. This result
shows that sophisticated modeling can blunt the
claws of the sparseness problem. One may ob-
tain the same number of parameters for two dif-
ferent models, but correlate them with more pro-
found linguistic notions in one model than in the
other. In our case, there is more statistical evi-
dence in the data for, e.g., case marking patterns,
than for association of grammatical relations with
structurally-marked positions.
For all HD variations, the RR model contin-
ues to outperform HD models. The function-set
variation performs slightly (but not significantly)
better than the category-set. What seems to be
still standing in the way of getting useful dis-
ambiguation cues for HD models is the fact that
the left and right direction of realization is hard-
wired in their representation. This breaks down a
coherent distribution over morphosyntactic repre-
sentations realizing grammatical relations to arbi-
trary position-dependent fragments, which results
in larger grammars and inferior performance.14
13The startegy of adding grammatical functions as state-
splits is used in, e.g., German (Rafferty and Manning, 2008).
14Due to the difference in the size of the grammars, one
could argue that smoothing will bridge the gap between
the HD and RR modeling strategies. However, the better
size/accuracy trade-off shown here for RR models suggests
that they provide a good bias/variance balancing point, es-
pecially for feature-rich models characterizing morphologi-
7 A Typological Detour
Hebrew, Arabic and other Semitic Languages are
known to be substantially different from English
in that English is strongly configurational. In
configurational languages word-order is fixed, and
information about the grammatical functions of
constituents (e.g., subject or object) is often cor-
related with structurally-marked positions inside
highly-nested constituency structures. Nonconfig-
urational languages (Hale, 1983), in contrast, al-
low for freedom in their word-ordering and infor-
mation about grammatical relations between con-
stituents is often marked by means of morphology.
Configurationality is hardly a clear-cut notion.
The difference in the configurationality level of
different languages is often conceived as depicted
in figure 7. In linguistic typology, the branch
of linguistics that studies the differences between
languages (Song, 2001), the division of labor be-
tween linear ordering and morphological marking
in the realization of grammatical relations is of-
ten viewed as a continuum. Common wisdom has
it that the lower a language is on the configura-
tionality scale, the more morphological marking
we expect to be used (Bresnan, 2001, page 6).
For a statistical parser to cope with nonconfig-
urational phenomena as observed in, for instance,
Hebrew or German, it should allow for flexibil-
ity in the form of realization of the grammati-
cal functions within the phrase-structure represen-
tation of trees. Recent morphological theories
employ Form-Function separation as a widely-
accepted practice for enhancing the adequacy of
models describing variability in the realization of
grammatical properties. Our results suggest that
the adequacy of syntactic processing models is re-
lated to such typological insights as well, and is
enhanced by adopting a similar form-function sep-
aration for expressing grammatical relations.
cally rich languages. A promising strategy then would be to
smooth or split-and-merge (Petrov et al, 2006)) RR-based
models rather than to add an elaborate smoothing component
to configurationally-based HD models.
849
configurational ?????? nonconfigurational
Chinese>English>{German,Hebrew}>Warlpiri
Figure 5: The Configurationality Scale
The HD assumptions take the function of a con-
stituent to be transparently related to its formal
position, which entails word-order rigidity. Such
transparent relations between configurational po-
sitions and grammatical functions are assumed by
other kinds of parsing frameworks such as the ?all-
subtrees? approach of Data-Oriented Parsing, and
the distinction between left and right application
in CCG-based parsers.
The RR modeling strategy stipulates a strict
separation between form ? parametrizing explic-
itly basic word-order (Greenberg, 1963) and mor-
phological realization (Greenberg, 1954) ? and
function ? parametrizing relational networks bor-
rowed from (Perlmutter, 1982) ? which makes
it possible to statistically learn complex form-
function mapping reflected in the data. This is
an adequate means to capture, e.g., morphosyn-
tactic interactions, which characterize the less-
configurational languages on the scale.
8 Conclusion
In our comparison of the HD and RR modeling
approaches, the RR approach is shown to be em-
pirically superior and typologically more adequate
for parsing a language exhibiting word-order vari-
ation interleaved with extended morphology. HD
models are less accurate and more vulnerable to
sparseness as they assume transparent mappings
between form and function, based on left and right
decompositions hard-wired in the HD representa-
tion. RR models, in contrast, employ form and
function separation which allows the statistical
model to learn complex correspondance patterns
reflected in the data. In the future we plan to in-
vestigate how the different models fare against one
another in parsing different languages. In particu-
lar we wish to examine whether parsing different
languages should be pursued by different models,
or whether the RR strategy can effectively cope
with different languages types. Finally, we wish
to explore the implications of RR modeling for
applications that consider the form of expression
in multiple languages, for instance Statistical Ma-
chine Translation (SMT).
9 Acknowledgements
We thank Jelle Zuidema, Inbal Tsarfati, David
McCloskey and Yoav Golderg for excellent com-
ments on earlier versions. We also thank Miles
Osborne and Tikitu de Jager for comments on the
camera-ready draft. All errors are our own. The
work of the first author is funded by the Dutch Sci-
ence Foundation (NWO) grant 017.001.271.
References
J. Aissen. 2003. Differential Object Marking: Iconic-
ity vs. Economy. Natural Language and Linguistic
Theory, 21.
D. M. Bikel. 2002. Design of a Multi-lingual, Parallel-
processing Statistical Parsing Engine. In Proceed-
ings of HLT.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Textbooks in Linguistics. Blackwell.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI.
M. Collins, J. Hajic?, E. Brill, L. Ramshaw, and C. Till-
mann. 1999. A Statistical Parser of Czech. In Pro-
ceedings ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics.
A. Dubey. 2004. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Y. Goldberg and R. Tsarfaty. 2008. A Single Frame-
work for Joint Morphological Segmentation and
Syntactic Parsing. In Proceedings of ACL.
J.H. Greenberg. 1954. A Quantitative Approach to
the Morphological Typology of Language. In R. F.
Spencer, editor, Method and Perspective in Anthro-
pology. University of Minessota Press.
J. H. Greenberg. 1963. Some Universals of Grammar
with Particular Reference to the Order of Meaning-
ful Elements. In Joseph H. Greenberg, editor, Uni-
versals of Language. MIT Press.
N. Guthmann, Y. Krymolowski, A. Milea, and Y. Win-
ter. 2009. Automatic Annotation of Morpho-
Syntactic Dependencies in a Modern Hebrew Tree-
bank. In Proceedings of TLT.
850
K. L. Hale. 1983. Warlpiri and the Grammar of Non-
Configurational Languages. Natural Language and
Linguistic Theory, 1(1).
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL.
S. Kubler. 2008. The PaGe Shared task on Parsing
German. In ACL Workshop on Parsing German.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a Large-
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanced
Annotation and Parsing of the Arabic treebank. In
Proceedings of INFOS.
D. M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics.
P. H. Matthews. 1993. Morphology. Cambridge.
D. McClosky, E. Charniak, and M. Johnson. 2008.
When is self-training effective for parsing? In Pro-
ceedings of CoLing.
N. Melnik. 2002. Verb-Initial Constructions in Mod-
ern Hebrew. Ph.D. thesis, Berkeley, California.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The Shared Task on Dependency Pars-
ing. In Proceedings of the CoNLL Shared Task.
D. M. Perlmutter. 1982. Syntactic Representation,
Syntactic Levels, and the Notion of a Subject. In
Pauline Jacobson and Geoffrey Pullum, editors, The
Nature of Syntactic Representation. Springer.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL.
A. Rafferty and C. D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In ACL WorkShop on Parsing German.
R. Scha. 1990. Language Theory and Language Tech-
nology; Competence and Performance. In Q. A. M.
de Kort and G. L. J. Leerdam, editors, Computer-
toepassingen in de Neerlandistiek. Almere: LVVN.
H. Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit vectors.
In Proceedings of COLING.
U. Shlonsky. 1997. Clause Structure and Word Order
in Hebrew and Arabic. Oxford University Press.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a Tree-Bank for Modern He-
brew Text. In Traitement Automatique des Langues.
J. J. Song. 2001. Linguistic Typology: Morphology
and Syntax. Pearson Education Limited, Edinbrugh.
R. Tsarfaty and K. Sima?an. 2008. Relational-
Realizational Parsing. In Proceedings of CoLing.
R. Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
851
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327?335,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Enhancing Unlexicalized Parsing Performance
using a Wide Coverage Lexicon, Fuzzy Tag-set Mapping,
and EM-HMM-based Lexical Probabilities
Yoav Goldberg1? Reut Tsarfaty2? Meni Adler1? Michael Elhadad1
1Department of Computer Science, Ben Gurion University of the Negev
{yoavg|adlerm|elhadad}@cs.bgu.ac.il
2Institute for Logic, Language and Computation, University of Amsterdam
R.Tsarfaty@uva.nl
Abstract
We present a framework for interfacing
a PCFG parser with lexical information
from an external resource following a dif-
ferent tagging scheme than the treebank.
This is achieved by defining a stochas-
tic mapping layer between the two re-
sources. Lexical probabilities for rare
events are estimated in a semi-supervised
manner from a lexicon and large unanno-
tated corpora. We show that this solu-
tion greatly enhances the performance of
an unlexicalized Hebrew PCFG parser, re-
sulting in state-of-the-art Hebrew parsing
results both when a segmentation oracle is
assumed, and in a real-word parsing sce-
nario of parsing unsegmented tokens.
1 Introduction
The intuition behind unlexicalized parsers is that
the lexicon is mostly separated from the syntax:
specific lexical items are mostly irrelevant for ac-
curate parsing, and can be mediated through the
use of POS tags and morphological hints. This
same intuition also resonates in highly lexicalized
formalism such as CCG: while the lexicon cate-
gories are very fine grained and syntactic in na-
ture, once the lexical category for a lexical item is
determined, the specific lexical form is not taken
into any further consideration.
Despite this apparent separation between the
lexical and the syntactic levels, both are usually es-
timated solely from a single treebank. Thus, while
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
?Funded by the Dutch Science Foundation (NWO), grant
number 017.001.271.
?Post-doctoral fellow, Deutsche Telekom labs at Ben Gu-
rion University
PCFGs can be accurate, they suffer from vocabu-
lary coverage problems: treebanks are small and
lexicons induced from them are limited.
The reason for this treebank-centric view in
PCFG learning is 3-fold: the English treebank is
fairly large and English morphology is fairly sim-
ple, so that in English, the treebank does provide
mostly adequate lexical coverage1; Lexicons enu-
merate analyses, but don?t provide probabilities
for them; and, most importantly, the treebank and
the external lexicon are likely to follow different
annotation schemas, reflecting different linguistic
perspectives.
On a different vein of research, current POS tag-
ging technology deals with much larger quantities
of training data than treebanks can provide, and
lexicon-based unsupervised approaches to POS
tagging are practically unlimited in the amount
of training data they can use. POS taggers rely
on richer knowledge than lexical estimates de-
rived from the treebank, have evolved sophisti-
cated strategies to handle OOV and can provide
distributions p(t|w, context) instead of ?best tag?
only.
Can these two worlds be combined? We pro-
pose that parsing performance can be greatly im-
proved by using a wide coverage lexicon to sug-
gest analyses for unknown tokens, and estimating
the respective lexical probabilities using a semi-
supervised technique, based on the training pro-
cedure of a lexicon-based HMM POS tagger. For
many resources, this approach can be taken only
on the proviso that the annotation schemes of the
two resources can be aligned.
We take Modern Hebrew parsing as our case
study. Hebrew is a Semitic language with rich
1This is not the case with other languages, and also not
true for English when adaptation scenarios are considered.
327
morphological structure. This rich structure yields
a large number of distinct word forms, resulting in
a high OOV rate (Adler et al, 2008a). This poses
a serious problem for estimating lexical probabili-
ties from small annotated corpora, such as the He-
brew treebank (Sima?an et al, 2001).
Hebrew has a wide coverage lexicon /
morphological-analyzer (henceforth, KC Ana-
lyzer) available2, but its tagset is different than the
one used by the Hebrew Treebank. These are not
mere technical differences, but derive from dif-
ferent perspectives on the data. The Hebrew TB
tagset is syntactic in nature, while the KC tagset
is lexicographic. This difference in perspective
yields different performance for parsers induced
from tagged data, and a simple mapping between
the two schemes is impossible to define (Sec. 2).
A naive approach for combining the use of the
two resources would be to manually re-tag the
Treebank with the KC tagset, but we show this ap-
proach harms our parser?s performance. Instead,
we propose a novel, layered approach (Sec. 2.1),
in which syntactic (TB) tags are viewed as contex-
tual refinements of the lexicon (KC) tags, and con-
versely, KC tags are viewed as lexical clustering
of the syntactic ones. This layered representation
allows us to easily integrate the syntactic and the
lexicon-based tagsets, without explicitly requiring
the Treebank to be re-tagged.
Hebrew parsing is further complicated by the
fact that common prepositions, conjunctions and
articles are prefixed to the following word and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes can be am-
biguous and must be determined in a specific con-
text only. Thus, the leaves of the syntactic parse
trees do not correspond to space-delimited tokens,
and the yield of the tree is not known in advance.
We show that enhancing the parser with external
lexical information is greatly beneficial, both in an
artificial scenario where the token segmentation is
assumed to be known (Sec. 4), and in a more re-
alistic one in which parsing and segmentation are
handled jointly by the parser (Goldberg and Tsar-
faty, 2008) (Sec. 5). External lexical informa-
tion enhances unlexicalized parsing performance
by as much as 6.67 F-points, an error reduction
of 20% over a Treebank-only parser. Our results
are not only the best published results for pars-
ing Hebrew, but also on par with state-of-the-art
2http://mila.cs.technion.ac.il/hebrew/resources/lexicons/
lexicalized Arabic parsing results assuming gold-
standard fine-grained Part-of-Speech (Maamouri
et al, 2008).3
2 A Tale of Two Resources
Modern Hebrew has 2 major linguistic resources:
the Hebrew Treebank (TB), and a wide coverage
Lexicon-based morphological analyzer developed
and maintained by the Knowledge Center for Pro-
cessing Hebrew (KC Analyzer).
The Hebrew Treebank consists of sentences
manually annotated with constituent-based syn-
tactic information. The most recent version (V2)
(Guthmann et al, 2009) has 6,219 sentences, and
covers 28,349 unique tokens and 17,731 unique
segments4.
The KC Analyzer assigns morphological analy-
ses (prefixes, suffixes, POS, gender, person, etc.)
to Hebrew tokens. It is based on a lexicon of
roughly 25,000 word lemmas and their inflection
patterns. From these, 562,439 unique word forms
are derived. These are then prefixed (subject to
constraints) by 73 prepositional prefixes.
It is interesting to note that even with these
numbers, the Lexicon?s coverage is far from com-
plete. Roughly 1,500 unique tokens from the He-
brew Treebank cannot be assigned any analysis
by the KC Lexicon, and Adler et al(2008a) report
that roughly 4.5% of the tokens in a 42M tokens
corpus of news text are unknown to the Lexicon.
For roughly 400 unique cases in the Treebank, the
Lexicon provides some analyses, but not a correct
one. This goes to emphasize the productive nature
of Hebrew morphology, and stress that robust lex-
ical probability estimates cannot be derived from
an annotated resource as small as the Treebank.
Lexical vs. Syntactic POS Tags The analyses
produced by the KC Analyzer are not compatible
with the Hebrew TB.
The KC tagset (Adler et al, 2008b; Netzer et
al., 2007; Adler, 2007) takes a lexical approach to
POS tagging (?a word can assume only POS tags
that would be assigned to it in a dictionary?), while
the TB takes a syntactic one (?if the word in this
particular positions functions as an Adverb, tag it
as an Adverb, even though it is listed in the dictio-
nary only as a Noun?). We present 2 cases that em-
phasize the difference: Adjectives: the Treebank
3Our method is orthogonal to lexicalization and can be
used in addition to it if one so wishes.
4In these counts, all numbers are conflated to one canoni-
cal form
328
treats any word in an adjectivial position as an Ad-
jective. This includes also demonstrative pronouns
?? ??? (this boy). However, from the KC point of
view, the fact that a pronoun can be used to modify
a noun does not mean it should appear in a dictio-
nary as an adjective. The MOD tag: similarly,
the TB has a special POS-tag for words that per-
form syntactic modification. These are mostly ad-
verbs, but almost any Adjective can, in some cir-
cumstances, belong to that class as well. This cat-
egory is highly syntactic, and does not conform to
the lexicon based approach.
In addition, many adverbs and prepositions in
Hebrew are lexicalized instances of a preposition
followed by a noun (e.g., ?????, ?in+softness?,
softly). These can admit both the lexical-
ized and the compositional analyses. Indeed,
many words admit the lexicalized analyses in
one of the resource but not in the other (e.g.,
????? ?for+benefit? is Prep in the TB but only
Prep+Noun in the KC, while for ??? ?from+side?
it is the other way around).
2.1 A Unified Resource
While the syntactic POS tags annotation of the TB
is very useful for assigning the correct tree struc-
ture when the correct POS tag is known, there are
clear benefits to an annotation scheme that can be
easily backed by a dictionary.
We created a unified resource, in which every
word occurrence in the Hebrew treebank is as-
signed a KC-based analysis. This was done in a
semi-automatic manner ? for most cases the map-
ping could be defined deterministically. The rest
(less than a thousand instances) were manually as-
signed. Some Treebank tokens had no analyses
in the KC lexicon, and some others did not have
a correct analysis. These were marked as ?UN-
KNOWN? and ?MISSING? respectively.5
The result is a Treebank which is morpho-
logically annotated according to two different
schemas. On average, each of the 257 TB tags
is mapped to 2.46 of the 273 KC tags.6 While this
resource can serve as a basis for many linguisti-
cally motivated inquiries, the rest of this paper is
5Another solution would be to add these missing cases to
the KC Lexicon. In our view this act is harmful: we don?t
want our Lexicon to artificially overfit our annotated corpora.
6A ?tag? in this context means the complete morphologi-
cal information available for a morpheme in the Treebank: its
part of speech, inflectional features and possessive suffixes,
but not prefixes or nominative and accusative suffixes, which
are taken to be separate morphemes.
devoted to using it for constructing a better parser.
Tagsets Comparison In (Adler et al, 2008b),
we hypothesized that due to its syntax-based na-
ture, the Treebank morphological tagset is more
suitable than the KC one for syntax related tasks.
Is this really the case? To verify it, we simulate a
scenario in which the complete gold morpholog-
ical information is available. We train 2 PCFG
grammars, one on each tagged version of the Tree-
bank, and test them on the subset of the develop-
ment set in which every token is completely cov-
ered by the KC Analyzer (351 sentences).7 The
input to the parser is the yields and disambiguated
pre-terminals of the trees to be parsed. The parsing
results are presented in Table 1. Note that this sce-
nario does not reflect actual parsing performance,
as the gold information is never available in prac-
tice, and surface forms are highly ambiguous.
Tagging Scheme Precision Recall
TB / syntactic 82.94 83.59
KC / dictionary 81.39 81.20
Table 1: evalb results for parsing with Oracle
morphological information, for the two tagsets
With gold morphological information, the TB
tagging scheme is more informative for the parser.
The syntax-oriented annotation scheme of the
TB is more informative for parsing than the lexi-
cographic KC scheme. Hence, we would like our
parser to use this TB tagset whenever possible, and
the KC tagset only for rare or unseen words.
A Layered Representation It seems that learn-
ing a treebank PCFG assuming such a different
tagset would require a treebank tagged with the
alternative annotation scheme. Rather than assum-
ing the existence of such an alternative resource,
we present here a novel approach in which we
view the different tagsets as corresponding to dif-
ferent aspects of the morphosyntactic representa-
tion of pre-terminals in the parse trees. Each of
these layers captures subtleties and regularities in
the data, none of which we would want to (and
sometimes, cannot) reduce to the other. We, there-
fore, propose to retain both tagsets and learn a
fuzzy mapping between them.
In practice, we propose an integrated represen-
tation of the tree in which the bottommost layer
represents the yield of the tree, the surface forms
7For details of the train/dev splits as well as the grammar,
see Section 4.2.
329
are tagged with dictionary-based KC POS tags,
and syntactic TB POS tags are in turn mapped onto
the KC ones (see Figure 1).
TB: KC: Layered:
...
JJ-ZYTB
??
...
PRP-M-S-3-DEMKC
??
...
JJ-ZYTB
PRP-M-S-3-DEMKC
??
...
INTB
??????
...
INKC
?
...
NN-F-SKC
?????
...
INTB
INKC
?
NN-F-SKC
?????
Figure 1: Syntactic (TB), Lexical (KC) and
Layered representations
This representation helps to retain the informa-
tion both for the syntactic and the morphologi-
cal POS tagsets, and can be seen as capturing the
interaction between the morphological and syn-
tactic aspects, allowing for a seamless integra-
tion of the two levels of representation. We re-
fer to this intermediate layer of representation as
a morphosyntactic-transfer layer and we formally
depict it as p(tKC |tTB).
This layered representation naturally gives rise
to a generative model in which a phrase level con-
stituent first generates a syntactic POS tag (tTB),
and this in turn generates the lexical POS tag(s)
(tKC). The KC tag then ultimately generates the
terminal symbols (w). We assume that a morpho-
logical analyzer assigns all possible analyses to a
given terminal symbol. Our terminal symbols are,
therefore, pairs: ?w, t?, and our lexical rules are of
the form t? ?w, t?. This gives rise to the follow-
ing equivalence:
p(?w, tKC?|tTB) = p(tKC |tTB)p(?w, tKC?|tKC)
In Sections (4, 5) we use this layered gener-
ative process to enable a smooth integration of
a PCFG treebank-learned grammar, an external
wide-coverage lexicon, and lexical probabilities
learned in a semi-supervised manner.
3 Semi-supervised Lexical Probability
Estimations
A PCFG parser requires lexical probabilities
of the form p(w|t) (Charniak et al, 1996).
Such information is not readily available in
the lexicon. However, it can be estimated
from the lexicon and large unannotated cor-
pora, by using the well-known Baum-Welch
(EM) algorithm to learn a trigram HMM tagging
model of the form p(t1, . . . , tn, w1, . . . , wn) =
argmax
?
p(ti|ti?1, ti?2)p(wi|ti), and taking
the emission probabilities p(w|t) of that model.
In Hebrew, things are more complicated, as
each emission w is not a space delimited token, but
rather a smaller unit (a morphological segment,
henceforth a segment). Adler and Elhadad (2006)
present a lattice-based modification of the Baum-
Welch algorithm to handle this segmentation am-
biguity.
Traditionally, such unsupervised EM-trained
HMM taggers are thought to be inaccurate, but
(Goldberg et al, 2008) showed that by feeding the
EM process with sufficiently good initial proba-
bilities, accurate taggers (> 91% accuracy) can be
learned for both English and Hebrew, based on a
(possibly incomplete) lexicon and large amount of
raw text. They also present a method for automat-
ically obtaining these initial probabilities.
As stated in Section 2, the KC Analyzer (He-
brew Lexicon) coverage is incomplete. Adler
et al(2008a) use the lexicon to learn a Maximum
Entropy model for predicting possible analyses for
unknown tokens based on their orthography, thus
extending the lexicon to cover (even if noisily) any
unknown token. In what follows, we use KC Ana-
lyzer to refer to this extended version.
Finally, these 3 works are combined to create
a state-of-the-art POS-tagger and morphological
disambiguator for Hebrew (Adler, 2007): initial
lexical probabilities are computed based on the
MaxEnt-extended KC Lexicon, and are then fed
to the modified Baum-Welch algorithm, which is
used to fit a morpheme-based tagging model over
a very large corpora. Note that the emission prob-
abilities P (W |T ) of that model cover all the mor-
phemes seen in the unannotated training corpus,
even those not covered by the KC Analyzer.8
We hypothesize that such emission probabili-
ties are good estimators for the morpheme-based
P (T ? W ) lexical probabilities needed by a
PCFG parser. To test this hypothesis, we use it
to estimate p(tKC ? w) in some of our models.
4 Parsing with a Segmentation Oracle
We now turn to describing our first set of exper-
iments, in which we assume the correct segmen-
8P (W |T ) is defined also for words not seen during train-
ing, based on the initial probabilities calculation procedure.
For details, see (Adler, 2007).
330
tation for each input sentence is known. This is
a strong assumption, as the segmentation stage
is ambiguous, and segmentation information pro-
vides very useful morphological hints that greatly
constrain the search space of the parser. However,
the setting is simpler to understand than the one
in which the parser performs both segmentation
and POS tagging, and the results show some in-
teresting trends. Moreover, some recent studies on
parsing Hebrew, as well as all studies on parsing
Arabic, make this oracle assumption. As such, the
results serve as an interesting comparison. Note
that in real-world parsing situations, the parser is
faced with a stream of ambiguous unsegmented to-
kens, making results in this setting not indicative
of real-world parsing performance.
4.1 The Models
The main question we address is the incorporation
of an external lexical resource into the parsing pro-
cess. This is challenging as different resources fol-
low different tagging schemes. One way around
it is re-tagging the treebank according to the new
tagging scheme. This will serve as a baseline
in our experiment. The alternative method uses
the Layered Representation described above (Sec.
2.1). We compare the performance of the two ap-
proaches, and also compare them against the per-
formance of the original treebank without external
information.
We follow the intuition that external lexical re-
sources are needed only when the information
contained in the treebank is too sparse. There-
fore, we use treebank-derived estimates for reli-
able events, and resort to the external resources
only in the cases of rare or OOV words, for which
the treebank distribution is not reliable.
Grammar and Notation For all our experi-
ments, we use the same grammar, and change
only the way lexical probabilities are imple-
mented. The grammar is an unlexicalized
treebank-estimated PCFG with linguistically mo-
tivated state-splits.9
In what follows, a lexical event is a word seg-
ment which is assigned a single POS thereby func-
tioning as a leaf in a syntactic parse tree. A rare
9Details of the grammar: all functional information is re-
moved from the non-terminals, finite and non-finite verbs, as
well as possessive and other PPs are distinguished, definite-
ness structure of constituents is marked, and parent annota-
tion is employed. It is the same grammar as described in
(Goldberg and Tsarfaty, 2008).
(lexical) event is an event occurring less than K
times in the training data, and a reliable (lexical)
event is one occurring at least K times in the train-
ing data. We use OOV to denote lexical events ap-
pearing 0 times in the training data. count(?) is
a counting function over the training data, rare
stands for any rare event, and wrare is a specific
rare event. KCA(?) is the KC Analyzer function,
mapping a lexical event to a set of possible tags
(analyses) according to the lexicon.
Lexical Models
All our models use relative frequency estimated
probabilities for reliable lexical events: p(t ?
w|t) = count(w,t)count(t) . They differ only in their treat-
ment of rare (including OOV) events.
In our Baseline, no external resource is used.
We smooth for rare and OOV events using a per-
tag probability distribution over rare segments,
which we estimate using relative frequency over
rare segments in the training data: p(wrare|t) =
count(rare,t)
count(t) . This is the way lexical probabilities
in treebank grammars are usually estimated.
We experiment with two flavours of lexical
models. In the first, LexFilter, the KC Analyzer is
consulted for rare events. We estimate rare events
using the same per-tag distribution as in the base-
line, but use the KC Analyzer to filter out any in-
compatible cases, that is, we force to 0 the proba-
bility of any analysis not supported by the lexicon:
p(wrare|t) =
{
count(rare,t)
count(t) t ? KCA(wrare)
0 t /? KCA(wrare)
Our second flavour of lexical models, Lex-
Probs, the KC Analyzer is consulted to propose
analyses for rare events, and the probability of an
analysis is estimated via the HMM emission func-
tion described in Section 3, which we denote B:
p(wrare|t) = B(wrare, t)
In both LexFilter and LexProbs, we resort to
the relative frequency estimation in case the event
is not covered in the KC Analyzer.
Tagset Representations
In this work, we are comparing 3 different rep-
resentations: TB, which is the original Treebank,
KC which is the Treebank converted to use the KC
Analyzer tagset, and Layered, which is the layered
representation described above.
The details of the lexical models vary according
to the representation we choose to work with.
For the TB setting, our lexical rules are of the form
331
ttb ? w. Only the Baseline models are relevant
here, as the tagset is not compatible with that of
the external lexicon.
For the KC setting, our lexical rules are of the form
tkc ? w, and their probabilities are estimated as
described above. Note that this setting requires our
trees to be tagged with the new (KC) tagset, and
parsed sentences are also tagged with this tagset.
For the Layered setting, we use lexical rules of
the form ttb ? w. Reliable events are esti-
mated as usual, via relative frequency over the
original treebank. For rare events, we estimate
p(ttb ? w|ttb) = p(ttb ? tkc|ttb)p(tkc ? w|tkc),
where the transfer probabilities p(ttb ? tkc) are
estimated via relative frequencies over the layered
trees, and the emission probabilities are estimated
either based on other rare events (LexFilter) or
based on the semi-supervised method described in
Section 3 (LexProbs).
The layered setting has several advantages:
First, the resulting trees are all tagged with the
original TB tagset. Second, the training proce-
dure does not require a treebank tagged with the
KC tagset: Instead of learning the transfer layer
from the treebank we could alternatively base our
counts on a different parallel resource, estimate it
from unannotated data using EM, define it heuris-
tically, or use any other estimation procedure.
4.2 Experiments
We perform all our experiments on Version 2 of
the Hebrew Treebank, and follow the train/test/dev
split introduced in (Tsarfaty and Sima?an, 2007):
section 1 is used for development, sections 2-12
for training, and section 13 is the test set, which
we do not use in this work. All the reported re-
sults are on the development set.10 After removal
of empty sentences, we have 5241 sentences for
training, and 483 for testing. Due to some changes
in the Treebank11, our results are not directly com-
parable to earlier works. However, our baseline
models are very similar to the models presented
in, e.g. (Goldberg and Tsarfaty, 2008).
In order to compare the performance of the
model on the various tagset representations (TB
tags, KC tags, Layered), we remove from the test
set 51 sentences in which at least one token is
marked as not having any correct segmentation in
the KC Analyzer. This introduces a slight bias in
10This work is part of an ongoing work on a parser, and the
test set is reserved for final evaluation of the entire system.
11Normalization of numbers and percents, correcting of
some incorrect trees, etc.
favor of the KC-tags setting, and makes the test
somewhat easier for all the models. However, it
allows for a relatively fair comparison between the
various models.12
Results and Discussion
Results are presented in Table 2.13
Baseline
rare: < 2 rare: < 10
Prec Rec Prec Rec
TB 72.80 71.70 67.66 64.92
KC 72.23 70.30 67.22 64.31
LexFilter
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.18 76.31 77.34 76.20
Layered 76.69 76.40 76.66 75.74
LexProbs
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.29 76.65 77.22 76.36
Layered 76.81 76.49 76.85 76.08
Table 2: evalb results for parsing with a
segmentation Oracle.
As expected, all the results are much lower than
those with gold fine-grained POS (Table 1).
When not using any external knowledge (Base-
line), the TB tagset performs slightly better than
the converted treebank (KC). Note, however, that
the difference is less pronounced than in the gold
morphology case. When varying the rare words
threshold from 2 to 10, performance drops consid-
erably. Without external knowledge, the parser is
facing difficulties coping with unseen events.
The incorporation of an external lexical knowl-
edge in the form of pruning illegal tag assignments
for unseen words based on the KC lexicon (Lex-
Filter) substantially improves the results (? 72 to
? 77). The additional lexical knowledge clearly
improves the parser. Moreover, varying the rare
words threshold in this setting hardly affects the
parser performance: the external lexicon suffices
to guide the parser in the right direction. Keep-
ing the rare words threshold high is desirable, as it
reduces overfitting to the treebank vocabulary.
We expected the addition of the semi-
supervised p(t ? w) distribution (LexProbs) to
improve the parser, but found it to have an in-
significant effect. The correct segmentation seems
12We are forced to remove these sentences because of the
artificial setting in which the correct segmentation is given. In
the no-oracle setting (Sec. 5), we do include these sentences.
13The layered trees have an extra layer of bracketing
(tTB ? tKC ). We remove this layer prior to evaluation.
332
to remove enough ambiguity as to let the parser
base its decisions on the generic tag distribution
for rare events.
In all the settings with a Segmentation Oracle,
there is no significant difference between the KC
and the Layered representation. We prefer the lay-
ered representation as it provides more flexibility,
does not require trees tagged with the KC tagset,
and produces parse trees with the original TB POS
tags at the leaves.
5 Parsing without a Segmentation Oracle
When parsing real world data, correct token seg-
mentation is not known in advance. For method-
ological reasons, this issue has either been set-
aside (Tsarfaty and Sima?an, 2007), or dealt with
in a pipeline model in which a morphological dis-
ambiguator is run prior to parsing to determine the
correct segmentation. However, Tsarfaty (2006)
argues that there is a strong interaction between
syntax and morphological segmentation, and that
the two tasks should be modeled jointly, and not
in a pipeline model. Several studies followed this
line, (Cohen and Smith, 2007) the most recent of
which is Goldberg and Tsarfaty (2008), who pre-
sented a model based on unweighted lattice pars-
ing for performing the joint task.
This model uses a morphological analyzer to
construct a lattice over all possible morphologi-
cal analyses of an input sentence. The arcs of
the lattice are ?w, t? pairs, and a lattice parser
is used to build a parse over the lattice. The
Viterbi parse over the lattice chooses a lattice path,
which induces a segmentation over the input sen-
tence. Thus, parsing and segmentation are per-
formed jointly.
Lexical rules in the model are defined over the
lattice arcs (t? ?w, t?|t), and smoothed probabil-
ities for them are estimated from the treebank via
relative frequency over terminal/preterminal pairs.
The lattice paths themselves are unweighted, re-
flecting the intuition that all morphological anal-
yses are a-priori equally likely, and that their per-
spective strengths should come from the segments
they contain and their interaction with the syntax.
Goldberg and Tsarfaty (2008) use a data-driven
morphological analyzer derived from the treebank.
Their better models incorporated some external
lexical knowledge by use of an Hebrew spell
checker to prune some illegal segmentations.
In what follows, we use the layered represen-
tation to adapt this joint model to use as its mor-
phological analyzer the wide coverage KC Ana-
lyzer in enhancement of a data-driven one. Then,
we further enhance the model with the semi-
supervised lexical probabilities described in Sec 3.
5.1 Model
The model of Goldberg and Tsarfaty (2008) uses a
morphological analyzer to constructs a lattice for
each input token. Then, the sentence lattice is built
by concatenating the individual token lattices. The
morphological analyzer used in that work is data
driven based on treebank observations, and em-
ploys some well crafted heuristics for OOV tokens
(for details, see the original paper). Here, we use
instead a morphological analyzer which uses the
KC Lexicon for rare and OOV tokens.
We begin by adapting the rare vs. reliable events
distinction from Section 4 to cover unsegmented
tokens. We define a reliable token to be a token
from the training corpus, which each of its possi-
ble segments according to the training corpus was
seen in the training corpus at least K times.14 All
other tokens are considered to be rare.
Our morphological analyzer works as follows:
For reliable tokens, it returns the set of analyses
seen for this token in the treebank (each analysis
is a sequence of pairs of the form ?w, tTB?).
For rare tokens, it returns the set of analyses re-
turned by the KC analyzer (here, analyses are se-
quences of pairs of the form ?w, tKC?).
The lattice arcs, then, can take two possible
forms, either ?w, tTB? or ?w, tKC?.
Lexical rules of the form tTB ? ?w, tTB? are reli-
able, and their probabilities estimated via relative
frequency over events seen in training.
Lexical rules of the form tTB ? ?w, tKC?
are estimated in accordance with the transfer
layer introduced above: p(tTB ? ?w, tKC?) =
p(tKC |tTB)p(?w, tKC?|tKC).
The remaining question is how to estimate
p(?w, tKC?|tKC). Here, we use either the LexFil-
ter (estimated over all rare events) or LexProbs
(estimated via the semisupervised emission prob-
abilities)models, as defined in Section 4.1 above.
5.2 Experiments
As our Baseline, we take the best model of (Gold-
berg and Tsarfaty, 2008), run against the current
14Note that this is more inclusive than requiring that the
token itself is seen in the training corpus at least K times, as
some segments may be shared by several tokens.
333
version of the Treebank.15 This model uses the
same grammar as described in Section 4.1 above,
and use some external information in the form of a
spell-checker wordlist. We compare this Baseline
with the LexFilter and LexProbs models over the
Layered representation.
We use the same test/train splits as described in
Section 4. Contrary to the Oracle segmentation
setting, here we evaluate against all sentences, in-
cluding those containing tokens for which the KC
Analyzer does not contain any correct analyses.
Due to token segmentation ambiguity, the re-
sulting parse yields may be different than the gold
ones, and evalb can not be used. Instead, we use
the evaluation measure of (Tsarfaty, 2006), also
used in (Goldberg and Tsarfaty, 2008), which is
an adaptation of parseval to use characters instead
of space-delimited tokens as its basic units.
Results and Discussion
Results are presented in Table 3.
rare: < 2 rare: < 10
Prec Rec Prec Rec
Baseline 67.71 66.35 ? ?
LexFilter 68.25 69.45 57.72 59.17
LexProbs 73.40 73.99 70.09 73.01
Table 3: Parsing results for the joint parsing+seg
task, with varying external knowledge
The results are expectedly lower than with the
segmentation Oracle, as the joint task is much
harder, but the external lexical information greatly
benefits the parser also in the joint setting. While
significant, the improvement from the Baseline to
LexFilter is quite small, which is due to the Base-
line?s own rather strong illegal analyses filtering
heuristic. However, unlike the oracle segmenta-
tion case, here the semisupervised lexical prob-
abilities (LexProbs) have a major effect on the
parser performance (? 69 to ? 73.5 F-score), an
overall improvement of ? 6.6 F-points over the
Baseline, which is the previous state-of-the art for
this joint task. This supports our intuition that rare
lexical events are better estimated using a large
unannotated corpus, and not using a generic tree-
bank distribution, or sparse treebank based counts,
and that lexical probabilities have a crucial role in
resolving segmentation ambiguities.
15While we use the same software as (Goldberg and Tsar-
faty, 2008), the results reported here are significantly lower.
This is due to differences in annotation scheme between V1
and V2 of the Hebrew TB
The parsers with the extended lexicon were un-
able to assign a parse to about 10 of the 483 test
sentences. We count them as having 0-Fscore
in the table results.16 The Baseline parser could
not assign a parse to more than twice that many
sentences, suggesting its lexical pruning heuris-
tic is quite harsh. In fact, the unparsed sen-
tences amount to most of the difference between
the Baseline and LexFilter parsers.
Here, changing the rare tokens threshold has
a significant effect on parsing accuracy, which
suggests that the segmentation for rare tokens is
highly consistent within the corpus. When an un-
known token is encountered, a clear bias should
be taken toward segmentations that were previ-
ously seen in the same corpus. Given that that ef-
fect is remedied to some extent by introducing the
semi-supervised lexical probabilities, we believe
that segmentation accuracy for unseen tokens can
be further improved, perhaps using resources such
as (Gabay et al, 2008), and techniques for incor-
porating some document, as opposed to sentence
level information, into the parsing process.
6 Conclusions
We present a framework for interfacing a parser
with an external lexicon following a differ-
ent annotation scheme. Unlike other studies
(Yang Huang et al, 2005; Szolovits, 2003) in
which such interfacing is achieved by a restricted
heuristic mapping, we propose a novel, stochastic
approach, based on a layered representation. We
show that using an external lexicon for dealing
with rare lexical events greatly benefits a PCFG
parser for Hebrew, and that results can be further
improved by the incorporation of lexical probabil-
ities estimated in a semi-supervised manner using
a wide-coverage lexicon and a large unannotated
corpus. In the future, we plan to integrate this
framework with a parsing model that is specifi-
cally crafted to cope with morphologically rich,
free-word order languages, as proposed in (Tsar-
faty and Sima?an, 2008).
Apart from Hebrew, our method is applicable
in any setting in which there exist a small tree-
bank and a wide-coverage lexical resource. For
example parsing Arabic using the Arabic Tree-
bank and the Buckwalter analyzer, or parsing En-
glish biomedical text using a biomedical treebank
and the UMLS Specialist Lexicon.
16When discarding these sentences from the test set, result
on the better LexProbs model leap to 74.95P/75.56R.
334
References
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological
disambiguation. In Proc. of COLING/ACL2006.
Meni Adler, Yoav Goldberg, David Gabay, and
Michael Elhadad. 2008a. Unsupervised lexicon-
based resolution of unknown words for full morpho-
logical analysis. In Proc. of ACL 2008.
Meni Adler, Yael Netzer, David Gabay, Yoav Goldberg,
and Michael Elhadad. 2008b. Tagging a hebrew
corpus: The case of participles. In Proc. of LREC
2008.
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael Littman, and John McCann. 1996. Taggers
for parsers. Artif. Intell., 85(1-2):45?57.
Shay B. Cohen and Noah A. Smith. 2007. Joint mor-
phological and syntactic disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, pages 208?217.
David Gabay, Ziv Ben Eliahu, and Michael Elhadad.
2008. Using wikipedia links to construct word seg-
mentation corpora. In Proc. of the WIKIAI-08 Work-
shop, AAAI-2008 Conference.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proc. of ACL 2008.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start). In Proc. of ACL 2008.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc. of TLT.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008. Enhanced annotation and parsing of the ara-
bic treebank. In INFOS 2008, Cairo, Egypt, March
27-29, 2008.
Yael Netzer, Meni Adler, David Gabay, and Michael
Elhadad. 2007. Can you tag the modal? you should!
In ACL07 Workshop on Computational Approaches
to Semitic Languages, Prague, Czech.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
P. Szolovits. 2003. Adding a medical lexicon to an
english parser. In Proc. AMIA 2003 Annual Sympo-
sium.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morpholog-
ically rich languages. In Proc. of IWPT 2007.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proc. of CoLING, pages
889?896, Manchester, UK, August. Coling 2008.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. In
Proceedings of ACL-SRW-06.
MS Yang Huang, MD Henry J. Lowe, PhD Dan Klein,
and MS Russell J. Cucina, MD. 2005. Improved
identification of noun phrases in clinical radiology
reports using a high-performance statistical natural
language parser augmented with the umls specialist
lexicon. J Am Med Inform Assoc, 12(3), May.
335
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 49?54,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integrated Morphological and Syntactic Disambiguation
for Modern Hebrew
Reut Tsarfaty
Institute for Logic, Language and Computation, University of Amsterdam
Plantage Muidergratch 24, 1018 TV Amsterdam, The Netherlands
rtsarfat@science.uva.nl
Abstract
Current parsing models are not immedi-
ately applicable for languages that exhibit
strong interaction between morphology
and syntax, e.g., Modern Hebrew (MH),
Arabic and other Semitic languages. This
work represents a first attempt at model-
ing morphological-syntactic interaction in
a generative probabilistic framework to al-
low for MH parsing. We show that mor-
phological information selected in tandem
with syntactic categories is instrumental
for parsing Semitic languages. We further
show that redundant morphological infor-
mation helps syntactic disambiguation.
1 Introduction
Natural Language Processing is typically viewed
as consisting of different layers,1 each of which is
handled separately. The structure of Semitic lan-
guages poses clear challenges to this traditional
division of labor. Specifically, Semitic languages
demonstrate strong interaction between morpho-
logical and syntactic processing, which limits the
applicability of standard tools for, e.g., parsing.
This work focuses on MH and explores the
ways morphological and syntactic processing in-
teract. Using a morphological analyzer, a part-of-
speech tagger, and a PCFG-based general-purpose
parser, we segment and parse MH sentences based
on a small, annotated corpus. Our integrated
model shows that percolating morphological am-
biguity to the lowest level of non-terminals in the
syntactic parse tree improves parsing accuracy.
1E.g., phonological, morphological, syntactic, semantic
and pragmatic.
Moreover, we show that morphological cues facil-
itate syntactic disambiguation. A particular contri-
bution of this work is to demonstrate that MH sta-
tistical parsing is feasible. Yet, the results obtained
are not comparable to those of, e.g., state-of-the-
art models for English, due to remaining syntactic
ambiguity and limited morphological treatment.
We conjecture that adequate morphological and
syntactic processing of MH should be done in a
unified framework, in which both levels can inter-
act and share information in both directions.
Section 2 presents linguistic data that demon-
strate the strong interaction between morphology
and syntax in MH, thus motivating our choice to
treat both in the same framework. Section 3 sur-
veys previous work and demonstrates again the
unavoidable interaction between the two. Sec-
tion 4.1 puts forward the formal setting of an inte-
grated probabilistic language model, followed by
the evaluation metrics defined for the integrated
task in section 4.2. Sections 4.3 and 4.4 then
describe the experimental setup and preliminary
results for our baseline implementation, and sec-
tion 5 discusses more sophisticated models we in-
tend to investigate.
2 Linguistic Data
Phrases and sentences in MH, as well as Arabic
and other Semitic languages, have a relatively free
word order.2 In figure 1, for example, two distinct
syntactic structures express the same grammatical
relations. It is typically morphological informa-
tion rather than word order that provides cues for
structural dependencies (e.g., agreement on gen-
der and number in figure 1 reveals the subject-
predicate dependency).
2MH allows for both SV and VS, and in some circum-
stances also VSO, SOV and others.
49
SNP-SBJ
D
h
the
N
ild
child.MS
VP
V
ica
go.out.MS
PP
P
m
from
NP
D
h
the
N
bit
house
S
PP
P
m
from
NP
D
h
the
N
bit
house
VP
V
ica
go.out.MS
NP-SBJ
D
h
the
N
ild
child.MS
Figure 1: Word Order in MH Phrases (marking the
agreement features M(asculine), S(ingular))
S-CNJ
CC
?w
and
S
SBAR
REL
kf
when
S
PP
P
m
from
NP
D
h
the
N
bit?
house
VP
V
?ica?
go.out
NP
D
?h
the
N
ild?
boy
S
...
Figure 2: Syntactic Structures of MH Phrases
(marking word boundaries with ? ?)
Furthermore, boundaries of constituents in the
syntactic structure of MH sentences need not co-
incide with word boundaries, as illustrated in fig-
ure 2. A MH word may coincide with a single
constituent, as in ?ica?3 (go out), it may overlap
with an entire phrase, as in ?h ild ? (the boy), or it
may span across phrases as in ?w kf m h bit ? (and
when from the house). Therefore, we conclude
that in order to perform syntactic analysis (pars-
ing) of MH sentences, we must first identify the
morphological constituents that form MH words.
There are (at least) three distinct morphologi-
cal processes in Semitic languages that play a role
in word formation. Derivational morphology is a
non-concatenative process in which verbs, nouns,
and adjectives are derived from (tri-)consonantal
roots plugged into templates of consonant/vowel
skeletons. The word-forms in table 1, for example,
are all derived from the same root, [i][l][d] (child,
birth), plugged into different templates. In addi-
tion, MH has a rich array of agreement features,
such as gender, number and person, expressed in
the word?s inflectional morphology. Verbs, adjec-
tives, determiners and numerals must agree on the
inflectional features with the noun they comple-
3We adopt the transliteration of (Sima?an et al, 2001).
a. ?ild? b. ?iild? c. ?mwld?
[i]e[l]e[d] [i]i[l](l)e[d] mw[][l](l)a[d]
child deliver a child innate
Table 1: Derivational Morphology in MH ([..]
mark templates? slots for consonantal roots, (..)
mark obligatory doubling of roots? consonants.)
a. ild gdwl b. ildh gdwlh
child.MS big.MS child.FS big.FS
a big boy a big girl
Table 2: Inflectional Morphology in MH (marking
M(asculine)/F(eminine), S(ingular)/P(lural))
ment or modify. It can be seen in table 2 that the
suffix h alters the noun ?ild ? (child) as well as its
modifier ?gdwl ? (big) to feminine gender. Finally,
particles that are prefixed to the word may serve
different syntactic functions, yet a multiplicity of
them may be concatenated together with the stem
to form a single word. The word ?wkfmhbit ? in
figure 2, for instance, is formed from a conjunc-
tion w (and), a relativizer kf (when), a preposition
m (from), a definite article h (the) and a noun bit
(house). Identifying such particles is crucial for
analyzing syntactic structures as they reveal struc-
tural dependencies such as subordinate clauses,
adjuncts, and prepositional phrase attachments.
At the same time, MH exhibits a large-scale am-
biguity already at the word level, which means that
there are multiple ways in which a word can be
broken down to its constituent morphemes. This
is further complicated by the fact that most vo-
calization marks (diacritics) are omitted in MH
texts. To illustrate, table 3 lists two segmenta-
tion possibilities, four readings, and five mean-
ings of different morphological analyses for the
word-form ?fmnh?.4 Yet, the morphological anal-
ysis of a word-form, and in particular its mor-
phological segmentation, cannot be disambiguated
without reference to context, and various morpho-
logical features of syntactically related forms pro-
vide useful hints for morphological disambigua-
tion. Figure 3 shows the correct analyses of the
form ?fmnh ? in different syntactic contexts. Note
that the correct analyses maintain agreement on
gender and number between the noun and its mod-
ifier. In particular, the analysis ?that counted? (b)
4A statistical study on a MH corpus has shown that the
average number of possible analyses per word-form was 2.1,
while 55% of the word-forms were morphologically ambigu-
ous (Sima?an et al, 2001).
50
?fmnh? ?fmnh? ?fmnh? ?fmnh? ?f + mnh?
shmena shamna shimna shimna she + mana
fat.FS got-fat.FS put-oil.FS oil-of.FS that + counted
fat (adj) got fat (v) put-oil (v) her oil (n) that (rel) counted (v)
Table 3: Morphological Analyses of the Word-
form ?fmnh?
a. NP
N
ildh.FS
child.FS
A
fmnh.FS
fat.FS
b. NP
N
ild.MS
child.MS
CP
Rel
f
that
V
mnh.MS
counted.MS
Figure 3: Ambiguity Resolution in Different Syn-
tactic Contexts
is easily disambiguated, as it is the only one main-
taining agreement with the modified noun.
In light of the above, we would want to con-
clude that syntactic processing must precede mor-
phological analysis; however, this would contra-
dict our previous conclusion. For this reason,
independent morphological and syntactic analyz-
ers for MH will not suffice. We suggest per-
forming morphological and syntactic processing
of MH utterances in a single, integrated, frame-
work, thereby allowing shared information to sup-
port disambiguation in multiple tasks.
3 Related Work
As of yet there is no statistical parser for MH.
Parsing models have been developed for different
languages and state-of-the-art results have been
reported for, e.g., English (Collins, 1997; Char-
niak, 2000). However, these models show impov-
erished morphological treatment, and they have
not yet been successfully applied for MH parsing.
(Sima?an et al, 2001) present an attempt to parse
MH sentences based on a small, annotated corpus
by applying a general-purpose Tree-gram model.
However, their work presupposes correct morpho-
logical disambiguation prior to parsing.5
In order to treat morphological phenomena
a few stand-alone morphological analyzers have
been developed for MH.6 Most analyzers consider
words in isolation, and thus propose multiple anal-
yses for each word. Analyzers which also at-
tempt disambiguation require contextual informa-
tion from surrounding word-forms or a shallow
parser (e.g., (Adler and Gabai, 2005)).
5The same holds for current work on parsing Arabic.
6Available at mila.cs.technion.ac.il.
A related research agenda is the development of
part-of-speech taggers for MH and other Semitic
languages. Such taggers need to address the seg-
mentation of words into morphemes to which dis-
tinct morphosyntactic categories can be assigned
(cf. figure 2). It was illustrated for both MH (Bar-
Haim, 2005) and Arabic (Habash and Rambow,
2005) that an integrated approach towards mak-
ing morphological (segmentation) and syntactic
(POS tagging) decisions within the same architec-
ture yields excellent results. The present work fol-
lows up on insights gathered from such studies,
suggesting that an integrated framework is an ade-
quate solution for the apparent circularity in mor-
phological and syntactic processing of MH.
4 The Integrated Model
As a first attempt to model the interaction between
the morphological and the syntactic tasks, we in-
corporate an intermediate level of part-of-speech
(POS) tagging into our model. The key idea is that
POS tags that are assigned to morphological seg-
ments at the word level coincide with the lowest
level of non-terminals in the syntactic parse trees
(cf. (Charniak et al, 1996)). Thus, POS tags can
be used to pass information between the different
tasks yet ensuring agreement between the two.
4.1 Formal Setting
Let wm1 be a sequence of words from a fixed vo-cabulary, sn1 be a sequence of segments of wordsfrom a (different) vocabulary, tn1 a sequence ofmorphosyntactic categories from a finite tag-set,
and let pi be a syntactic parse tree.
We define segmentation as the task of identi-
fying the sequence of morphological constituents
that were concatenated to form a sequence of
words. Formally, we define the task as (1), where
seg(wm1 ) is the set of segmentations resultingfrom all possible morphological analyses of wn1 .
sn1 ? = argmax
sn1 ?seg(wm1 )
P (sn1 |wm1 ) (1)
Syntactic analysis, parsing, identifies the structure
of phrases and sentences. In MH, such tree struc-
tures combine segments of words that serve differ-
ent syntactic functions. We define it formally as
(2), where yield(pi?) is the ordered set of leaves of
a syntactic parse tree pi?.
pi? = argmax
pi?{pi?:yield(pi?)=sn1 }
P (pi|sn1 ) (2)
51
Similarly, we define POS tagging as (3), where
analysis(sn1 ) is the set of all possible POS tag as-signments for sn1 .
tn1 ? = argmax
tn1 ?analyses(sn1 )
P (tn1 |sn1 ) (3)
The task of the integrated model is to find the
most probable segmentation and syntactic parse
tree given a sentence in MH, as in (4).
?pi, sn1 ?? = argmax
?pi,sn1 ?
P (pi, sn1 |wm1 ) (4)
We reinterpret (4) to distinguish the morphological
and syntactic tasks, conditioning the latter on the
former, yet maximizing for both.
?pi, sn1 ?? = argmax
?pi,sn1 ?
P (pi|sn1 , wm1 )
? ?? ?
parsing
P (sn1 |wm1 )
? ?? ?
segmentation
(5)
Agreement between the tasks is implemented by
incorporating morphosyntactic categories (POS
tags) that are assigned to morphological segments
and constrain the possible trees, resulting in (7).
?pi, tn1 , sn1 ?? = argmax
?pi,tn1 ,sn1 ?
P (pi, tn1 , sn1 |wm1 ) (6)
= argmax
?pi,tn1 ,sn1 ?
P (pi|tn1 , sn1 , wm1 )
? ?? ?
parsing
P (tn1 |sn1 , wm1 )
? ?? ?
tagging
P (sn1 |wm1 )
? ?? ?
segmentation(7)
Finally, we employ the assumption that
P (wm1 |sn1 ) ? 1, since segments can only beconjoined in a certain order.7 So, instead of (5)
and (7) we end up with (8) and (9), respectively.
? argmax
?pi,sn1 ?
P (pi|sn1 )
? ?? ?
parsing
P (sn1 |wm1 )
? ?? ?
segmentation
(8)
? argmax
?pi,tn1 ,sn1 ?
P (pi|tn1 , sn1 )
? ?? ?
parsing
P (tn1 |sn1 )
? ?? ?
tagging
P (sn1 |wm1 )
? ?? ?
segmentation
(9)
4.2 Evaluation Metrics
The intertwined nature of morphology and syn-
tax in MH poses additional challenges to standard
parsing evaluation metrics. First, note that we can-
not use morphemes as the basic units for com-
parison, as the proposed segmentation need not
coincide with the gold segmentation for a given
sentence. Since words are complex entities that
7Since concatenated particles (conjunctions et al) appear
in front of the stem, pronominal and inflectional affixes at the
end of the stem, and derivational morphology inside the stem,
there is typically a unique way to restore word boundaries.
can span across phrases (see figure 2), we can-
not use them for comparison either. We propose
to redefine precision and recall by considering the
spans of syntactic categories based on the (space-
free) sequences of characters to which they corre-
spond. Formally, we define syntactic constituents
as ?i, A, j? where i, j mark the location of char-
acters. T = {?i, A, j?|A spans from i to j} and
G = {?i, A, j?|A spans from i to j} represent the
test/gold parses, respectively, and we calculate:8
Labeled Precision = #(G ? T )/#T (10)
Labeled Recall = #(G ? T )/#G (11)
4.3 Experimental Setup
Our departure point for the syntactic analysis of
MH is that the basic units for processing are not
words, but morphological segments that are con-
catenated together to form words. Therefore, we
obtain a segment-based probabilistic grammar by
training a Probabilistic Context Free Grammar
(PCFG) on a segmented and annotated MH cor-
pus (Sima?an et al, 2001). Then, we use exist-
ing tools ? i.e., a morphological analyzer (Segal,
2000), a part-of-speech tagger (Bar-Haim, 2005),
and a general-purpose parser (Schmid, 2000) ? to
find compatible morphological segmentations and
syntactic analyses for unseen sentences.
The Data The data set we use is taken from the
MH treebank which consists of 5001 sentences
from the daily newspaper ?ha?aretz? (Sima?an et
al., 2001). We employ the syntactic categories and
POS tag sets developed therein. Our data set in-
cludes 3257 sentences of length greater than 1 and
less than 21. The number of segments per sen-
tence is 60% higher than the number of words per
sentence.9 We conducted 8 experiments in which
the data is split to training and test sets and apply
cross-fold validation to obtain robust averages.
The Models Model I uses the morphological an-
alyzer and the POS tagger to find the most prob-
able segmentation for a given sentence. This is
done by providing the POS tagger with multiple
morphological analyses per word and maximizing
the sum ?tn1 P (tn1 , sn1 |wm1 ) (Bar-Haim, 2005, sec-tion 8.2). Then, the parser is used to find the most
8Covert definite article errors are counted only at the POS
tags level and discounted at the phrase-level.
9The average number of words per sentence in the com-
plete corpus is 17 while the average number of morphological
segments per sentence is 26.
52
probable parse tree for the selected sequence of
morphological segments. Formally, this model is
a first approximation of equation (8) using a step-
wise maximization instead of a joint one.10
In Model II we percolate the morphological am-
biguity further, to the lowest level of non-terminals
in the syntactic trees. Here we use the morpholog-
ical analyzer and the POS tagger to find the most
probable segmentation and POS tag assignment
by maximizing the joint probability P (tn1 , sn1 |wm1 )(Bar-Haim, 2005, section 5.2). Then, the parser
is used to parse the tagged segments. Formally,
this model attempts to approximate equation (9).
(Note that here we couple a morphological and
a syntactic decision, as we are looking to max-
imize P (tn1 , sn1 |wm1 ) ? P (tn1 |sn1 )P (sn1 |wm1 ) andconstrain the space of trees to those that agree with
the resulting analysis.)11
In both models, smoothing the estimated prob-
abilities is delegated to the relevant subcompo-
nents. Out of vocabulary (OOV) words are treated
by the morphological analyzer, which proposes
all possible segmentations assuming that the stem
is a proper noun. The Tri-gram model used for
POS tagging is smoothed using Good-Turing dis-
counting (see (Bar-Haim, 2005, section 6.1)), and
the parser uses absolute discounting with various
backoff strategies (Schmid, 2000, section 4.4).
The Tag-Sets To examine the usefulness of var-
ious morphological features shared with the pars-
ing task, we alter the set of morphosyntactic cate-
gories to include more fine-grained morphological
distinctions. We use three sets: Set A contains bare
POS categories, Set B identifies also definite nouns
marked for possession, and Set C adds the distinc-
tion between finite and non-finite verb forms.
Evaluation We use seven measures to evaluate
our models? performance on the integrated task.
10At the cost of incurring indepence assumptions, a step-
wise architecture is computationally cheaper than a joint one
and this is perhaps the simplest end-to-end architecture for
MH parsing imaginable. In the absence of previous MH pars-
ing results, this model is suitable to serve as a baseline against
which we compare more sophisticated models.
11We further developed a third model, Model III, which
is a more faithful approximation, yet computationally afford-
able, of equation (9). There we percolate the ambiguity all the
way through the integrated architecture by means of provid-
ing the parser with the n-best sequences of tagged morpho-
logical segments and selecting the analysis ?pi, tn1 , sn1 ? whichmaximizes the production P (pi|tn1 , sn1 )P (sn1 , tn1 |wm1 ). How-ever, we have not yet obtained robust results for this model
prior to the submission of this paper, and therefore we leave
it for future discussion.
String Labeled POS tags Segment.
Cover. Prec. / Rec. Prec. / Rec. Prec. / Rec.
Model I-A 99.2% 60.3% / 58.4% 82.4% / 82.6% 94.4% / 94.7 %
Model II-A 95.9% 60.7% / 60.5% 84.5% / 84.8% 91.3% / 91.6%
Model I-B 99.2 % 60.3% / 58.4% 81.6% / 82.3% 94.2% / 95.0%
Model II-B 95.7% 60.7% / 60.5% 82.8% / 83.5% 90.9% / 91.7%
Model I-C 99.2% 60.9% / 59.2% 80.4% / 81.1% 94.2% / 95.1%
Model II-C 95.9% 61.7% / 61.9% 81.6% / 82.3% 91.0% / 91.9%
Table 4: Evaluation Metrics, Models I and II
First, we present the percentage of sentences for
which the model could propose a pair of corre-
sponding morphological and syntactic analyses.
This measure is referred to as string coverage. To
indicate morphological disambiguation capabili-
ties we report segmentation precision and recall.
To capture tagging and parsing accuracy, we refer
to our redefined Parseval measures and separate
the evaluation of morphosyntactic categories, i.e.,
POS tags precision and recall, and phrase-level
syntactic categories, i.e., labeled precision and re-
call (where root nodes are discarded and empty
trees are counted as zero).12 The labeled cate-
gories are evaluated against the original tag set.
4.4 Results
Table 4 shows the evaluation scores for models I-A
to II-C. To the best of our knowledge, these are the
first parsing results for MH assuming no manual
interference for morphological disambiguation.
For all sets, parsing of tagged-segments (Model
II) shows improvement of up to 2% over pars-
ing bare segments? sequences (Model I). This indi-
cates that morphosyntactic information selected in
tandem with morphological segmentation is more
informative for syntactic analysis than segmenta-
tion alone. We also observe decreasing string cov-
erage for Model II, possibly since disambiguation
based on short context may result in a probable,
yet incorrect, POS tag assignment for which the
parser cannot recover a syntactic analysis. Cor-
rect disambiguation may depend on long-distance
cues, e.g., agreement, so we advocate percolating
the ambiguity further up to the parser.
Comparing the performance for the different tag
sets, parsing accuracy increases for models I-B/C
and II-B/C while POS tagging results decrease.
These results seem to contradict the common wis-
dom that performance on a ?complex? task de-
12Since we evaluate the models? performance on an inte-
grated task, sentences in which one of the subcomponents
failed to propose an analysis counts as zero for all subtasks.
53
pends on a ?simpler?, preceding one; yet, they sup-
port our thesis that morphological information or-
thogonal to syntactic categories facilitates syntac-
tic analysis and improves disambiguation capacity.
5 Discussion
Devising a baseline model for morphological and
syntactic processing is of great importance for the
development of a broad-coverage statistical parser
for MH. Here we provide a set of standardized
baseline results for later comparison while con-
solidating the formal and architectural underpin-
ning of an integrated model. However, our results
were obtained using a relatively small set of train-
ing data and a weak (unlexicalized) parser, due to
the size of the corpus and its annotated scheme.13
Training a PCFG on our treebank resulted in a
severely ambiguous grammar, mainly due to high
phrase structure variability.
To compensate for the flat, ambiguous phrase-
structures, in the future we intend to employ prob-
abilistic grammars in which all levels of non-
terminals are augmented with morphological in-
formation percolated up the tree. Furthermore,
the MH treebank annotation scheme features a set
of so-called functional features14 which express
grammatical relations. We propose to learn the
correlation between various morphological mark-
ings and functional features, thereby constraining
the space of syntactic structures to those which ex-
press meaningful predicate-argument structures.
Since our data set is relatively small,15 introduc-
ing orthogonal morphological information to syn-
tactic categories may result in severe data sparse-
ness. In the current architecture, smoothing is
handled separately by each of the subcomponents.
Enriched grammars would allow us to exploit mul-
tiple levels of information in smoothing the esti-
mated probabilities and to redistribute probability
mass to unattested events based on their similarity
to attested events in their integrated representation.
6 Conclusion
Traditional approaches for devising parsing mod-
els, smoothing techniques and evaluation metrics
are not well suited for MH, as they presuppose
13The lack of head marking, for instance, precludes the use
of lexicalized models a` la (Collins, 1997).
14SBJ for subject, OBJ for object, COM for complement,
etc. (Sima?an et al, 2001).
15The size of our treebank is less than 30% of the Arabic
Treebank, and less than 10% of the WSJ Penn Treebank.
separate levels of processing. Different languages
mark regularities in their surface structures in dif-
ferent ways ? English encodes regularities in word
order, while MH provides useful hints about gram-
matical relations in its derivational and inflectional
morphology. In the future we intend to develop
more sophisticated models implementing closer
interaction between morphology and syntax, by
means of which we hope to boost parsing accu-
racy and improve morphological disambiguation.
Acknowledgments I would like to thank Khalil
Sima?an for supervising this work, Remko Scha,
Rens Bod and Jelle Zuidema for helpful com-
ments, and Alon Itai, Yoad Winter and Shuly
Wintner for discussion. The Knowledge Cen-
ter for Hebrew Processing provided corpora and
tools, and Roy Bar-Haim provided knowledge and
technical support for which I am grateful. This
work is funded by the Netherlands Organization
for Scientific Research (NWO) grant 017.001.271.
References
Meni Adler and Dudi Gabai. 2005. MorphologicalAnalyzer and Disambiguator for Modern Hebrew.
Knowledge Center for Processing Hebrew.
Roy Bar-Haim. 2005. Part-of-Speech Tagging for He-brew and Other Semitic Languages. Master?s thesis,Technion, Haifa, Israel.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz,Michael L. Littman, and John McCann. 1996. Tag-gers for Parsers. AI, 85(1-2):45?57.
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL 2000.
Michael Collins. 1997. Three Generative, LexicalisedModels for Statistical Parsing. In Proceedings of
ACL-EACL 1997.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-cal Disambiguation in One Fell Swoop. In Proceed-
ings of ACL 2005.
Helmut Schmid, 2000. LoPar: Design and Implemen-
tation. Institute for Computational Linguistics, Uni-versity of Stuttgart.
Erel Segal. 2000. A Probabilistic Morphological An-alyzer for Hebrew Undotted Texts. Master?s thesis,
Computer Science Department, Technion, Isreal.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ. 2001. Building a Tree-Bank forModern Hebrew Text. In Traitement Automatique
des Langues, volume 42, pages 347?380.
54
Proceedings of ACL-08: HLT, pages 371?379,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Single Generative Model
for Joint Morphological Segmentation and Syntactic Parsing
Yoav Goldberg
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg@cs.bgu.ac.il
Reut Tsarfaty
Institute for Logic Language and Computation
University of Amsterdam
Plantage Muidergracht 24, Amsterdam, NL
rtsarfat@science.uva.nl
Abstract
Morphological processes in Semitic languages
deliver space-delimited words which intro-
duce multiple, distinct, syntactic units into the
structure of the input sentence. These words
are in turn highly ambiguous, breaking the
assumption underlying most parsers that the
yield of a tree for a given sentence is known in
advance. Here we propose a single joint model
for performing both morphological segmenta-
tion and syntactic disambiguation which by-
passes the associated circularity. Using a tree-
bank grammar, a data-driven lexicon, and a
linguistically motivated unknown-tokens han-
dling technique our model outperforms previ-
ous pipelined, integrated or factorized systems
for Hebrew morphological and syntactic pro-
cessing, yielding an error reduction of 12%
over the best published results so far.
1 Introduction
Current state-of-the-art broad-coverage parsers as-
sume a direct correspondence between the lexical
items ingrained in the proposed syntactic analyses
(the yields of syntactic parse-trees) and the space-
delimited tokens (henceforth, ?tokens?) that consti-
tute the unanalyzed surface forms (utterances). In
Semitic languages the situation is very different.
In Modern Hebrew (Hebrew), a Semitic language
with very rich morphology, particles marking con-
junctions, prepositions, complementizers and rela-
tivizers are bound elements prefixed to the word
(Glinert, 1989). The Hebrew token ?bcl?1, for ex-
ample, stands for the complete prepositional phrase
1We adopt here the transliteration of (Sima?an et al, 2001).
?in the shadow?. This token may further embed
into a larger utterance, e.g., ?bcl hneim? (literally
?in-the-shadow the-pleasant?, meaning roughly ?in
the pleasant shadow?) in which the dominated Noun
is modified by a proceeding space-delimited adjec-
tive. It should be clear from the onset that the parti-
cle b (?in?) in ?bcl? may then attach higher than the
bare noun cl (?shadow?). This leads to word- and
constituent-boundaries discrepancy, which breaks
the assumptions underlying current state-of-the-art
statistical parsers.
One way to approach this discrepancy is to as-
sume a preceding phase of morphological segmen-
tation for extracting the different lexical items that
exist at the token level (as is done, to the best of
our knowledge, in all parsing related work on Arabic
and its dialects (Chiang et al, 2006)). The input for
the segmentation task is however highly ambiguous
for Semitic languages, and surface forms (tokens)
may admit multiple possible analyses as in (Bar-
Haim et al, 2007; Adler and Elhadad, 2006). The
aforementioned surface form bcl, for example, may
also stand for the lexical item ?onion?, a Noun. The
implication of this ambiguity for a parser is that the
yield of syntactic trees no longer consists of space-
delimited tokens, and the expected number of leaves
in the syntactic analysis in not known in advance.
Tsarfaty (2006) argues that for Semitic languages
determining the correct morphological segmentation
is dependent on syntactic context and shows that in-
creasing information sharing between the morpho-
logical and the syntactic components leads to im-
proved performance on the joint task. Cohen and
Smith (2007) followed up on these results and pro-
371
posed a system for joint inference of morphological
and syntactic structures using factored models each
designed and trained on its own.
Here we push the single-framework conjecture
across the board and present a single model that
performs morphological segmentation and syntac-
tic disambiguation in a fully generative framework.
We claim that no particular morphological segmen-
tation is a-priory more likely for surface forms be-
fore exploring the compositional nature of syntac-
tic structures, including manifestations of various
long-distance dependencies. Morphological seg-
mentation decisions in our model are delegated to a
lexeme-based PCFG and we show that using a sim-
ple treebank grammar, a data-driven lexicon, and
a linguistically motivated unknown-tokens handling
our model outperforms (Tsarfaty, 2006) and (Co-
hen and Smith, 2007) on the joint task and achieves
state-of-the-art results on a par with current respec-
tive standalone models.2
2 Modern Hebrew Structure
Segmental morphology Hebrew consists of
seven particles m(?from?) f (?when?/?who?/?that?)
h(?the?) w(?and?) k(?like?) l(?to?) and b(?in?).
which may never appear in isolation and must
always attach as prefixes to the following open-class
category item we refer to as stem. Several such
particles may be prefixed onto a single stem, in
which case the affixation is subject to strict linear
precedence constraints. Co-occurrences among the
particles themselves are subject to further syntactic
and lexical constraints relative to the stem.
While the linear precedence of segmental mor-
phemes within a token is subject to constraints, the
dominance relations among their mother and sister
constituents is rather free. The relativizer f(?that?)
for example, may attach to an arbitrarily long rela-
tive clause that goes beyond token boundaries. The
attachment in such cases encompasses a long dis-
tance dependency that cannot be captured by Marko-
vian processes that are typically used for morpho-
logical disambiguation. The same argument holds
for resolving PP attachment of a prefixed preposition
or marking conjunction of elements of any kind.
A less canonical representation of segmental mor-
2Standalone parsing models assume a segmentation Oracle.
phology is triggered by a morpho-phonological pro-
cess of omitting the definite article h when occur-
ring after the particles b or l. This process triggers
ambiguity as for the definiteness status of Nouns
following these particles.We refer to such cases
in which the concatenation of elements does not
strictly correspond to the original surface form as
super-segmental morphology. An additional case of
super-segmental morphology is the case of Pronom-
inal Clitics. Inflectional features marking pronom-
inal elements may be attached to different kinds of
categories marking their pronominal complements.
The additional morphological material in such cases
appears after the stem and realizes the extended
meaning. The current work treats both segmental
and super-segmental phenomena, yet we note that
there may be more adequate ways to treat super-
segmental phenomena assuming Word-Based mor-
phology as we explore in (Tsarfaty and Goldberg,
2008).
Lexical and Morphological Ambiguity The rich
morphological processes for deriving Hebrew stems
give rise to a high degree of ambiguity for Hebrew
space-delimited tokens. The form fmnh, for exam-
ple, can be understood as the verb ?lubricated?, the
possessed noun ?her oil?, the adjective ?fat? or the
verb ?got fat?. Furthermore, the systematic way in
which particles are prefixed to one another and onto
an open-class category gives rise to a distinct sort
of morphological ambiguity: space-delimited tokens
may be ambiguous between several different seg-
mentation possibilities. The same form fmnh can be
segmented as f-mnh, f (?that?) functioning as a rele-
tivizer with the form mnh. The form mnh itself can
be read as at least three different verbs (?counted?,
?appointed?, ?was appointed?), a noun (?a portion?),
and a possessed noun (?her kind?).
Such ambiguities cause discrepancies between
token boundaries (indexed as white spaces) and
constituent boundaries (imposed by syntactic cate-
gories) with respect to a surface form. Such discrep-
ancies can be aligned via an intermediate level of
PoS tags. PoS tags impose a unique morphological
segmentation on surface tokens and present a unique
valid yield for syntactic trees. The correct ambigu-
ity resolution of the syntactic level therefore helps to
resolve the morphological one, and vice versa.
372
3 Previous Work on Hebrew Processing
Morphological analyzers for Hebrew that analyze a
surface form in isolation have been proposed by Se-
gal (2000), Yona and Wintner (2005), and recently
by the knowledge center for processing Hebrew (Itai
et al, 2006). Such analyzers propose multiple seg-
mentation possibilities and their corresponding anal-
yses for a token in isolation but have no means to
determine the most likely ones. Morphological dis-
ambiguators that consider a token in context (an ut-
terance) and propose the most likely morphologi-
cal analysis of an utterance (including segmentation)
were presented by Bar-Haim et al (2005), Adler
and Elhadad (2006), Shacham and Wintner (2007),
and achieved good results (the best segmentation re-
sult so far is around 98%).
The development of the very first Hebrew Tree-
bank (Sima?an et al, 2001) called for the exploration
of general statistical parsing methods, but the appli-
cation was at first limited. Sima?an et al (2001) pre-
sented parsing results for a DOP tree-gram model
using a small data set (500 sentences) and semi-
automatic morphological disambiguation. Tsarfaty
(2006) was the first to demonstrate that fully auto-
matic Hebrew parsing is feasible using the newly
available 5000 sentences treebank. Tsarfaty and
Sima?an (2007) have reported state-of-the-art results
on Hebrew unlexicalized parsing (74.41%) albeit as-
suming oracle morphological segmentation.
The joint morphological and syntactic hypothesis
was first discussed in (Tsarfaty, 2006; Tsarfaty and
Sima?an, 2004) and empirically explored in (Tsar-
faty, 2006). Tsarfaty (2006) used a morphological
analyzer (Segal, 2000), a PoS tagger (Bar-Haim et
al., 2005), and a general purpose parser (Schmid,
2000) in an integrated framework in which morpho-
logical and syntactic components interact to share
information, leading to improved performance on
the joint task. Cohen and Smith (2007) later on
based a system for joint inference on factored, inde-
pendent, morphological and syntactic components
of which scores are combined to cater for the joint
inference task. Both (Tsarfaty, 2006; Cohen and
Smith, 2007) have shown that a single integrated
framework outperforms a completely streamlined
implementation, yet neither has shown a single gen-
erative model which handles both tasks.
4 Model Preliminaries
4.1 The Status Space-Delimited Tokens
A Hebrew surface token may have several readings,
each of which corresponding to a sequence of seg-
ments and their corresponding PoS tags. We refer
to different readings as different analyses whereby
the segments are deterministic given the sequence of
PoS tags. We refer to a segment and its assigned PoS
tag as a lexeme, and so analyses are in fact sequences
of lexemes. For brevity we omit the segments from
the analysis, and so analysis of the form ?fmnh? as
f/REL mnh/VB is represented simply as REL VB.
Such tag sequences are often treated as ?complex
tags? (e.g. REL+VB) (cf. (Bar-Haim et al, 2007;
Habash and Rambow, 2005)) and probabilities are
assigned to different analyses in accordance with
the likelihood of their tags (e.g., ?fmnh is 30%
likely to be tagged NN and 70% likely to be tagged
REL+VB?). Here we do not submit to this view.
When a token fmnh is to be interpreted as the lex-
eme sequence f /REL mnh/VB, the analysis intro-
duces two distinct entities, the relativizer f (?that?)
and the verb mnh (?counted?), and not as the com-
plex entity ?that counted?. When the same token
is to be interpreted as a single lexeme fmnh, it may
function as a single adjective ?fat?. There is no re-
lation between these two interpretations other then
the fact that their surface forms coincide, and we ar-
gue that the only reason to prefer one analysis over
the other is compositional. A possible probabilistic
model for assigning probabilities to complex analy-
ses of a surface form may be
P (REL,VB|fmnh, context) =
P (REL|f)P (VB|mnh,REL)P (REL,VB| context)
and indeed recent sequential disambiguation models
for Hebrew (Adler and Elhadad, 2006) and Arabic
(Smith et al, 2005) present similar models.
We suggest that in unlexicalized PCFGs the syn-
tactic context may be explicitly modeled in the
derivation probabilities. Hence, we take the prob-
ability of the event fmnh analyzed as REL VB to be
P (REL? f|REL) ? P (VB? mnh|VB)
This means that we generate f and mnh indepen-
dently depending on their corresponding PoS tags,
373
and the context (as well as the syntactic relation be-
tween the two) is modeled via the derivation result-
ing in a sequence REL VB spanning the form fmnh.
4.2 Lattice Representation
We represent all morphological analyses of a given
utterance using a lattice structure. Each lattice arc
corresponds to a segment and its corresponding PoS
tag, and a path through the lattice corresponds to
a specific morphological segmentation of the utter-
ance. This is by now a fairly standard representa-
tion for multiple morphological segmentation of He-
brew utterances (Adler, 2001; Bar-Haim et al, 2005;
Smith et al, 2005; Cohen and Smith, 2007; Adler,
2007). Figure 1 depicts the lattice for a 2-words
sentence bclm hneim. We use double-circles to in-
dicate the space-delimited token boundaries. Note
that in our construction arcs can never cross token
boundaries. Every token is independent of the oth-
ers, and the sentence lattice is in fact a concatena-
tion of smaller lattices, one for each token. Fur-
thermore, some of the arcs represent lexemes not
present in the input tokens (e.g. h/DT, fl/POS), how-
ever these are parts of valid analyses of the token (cf.
super-segmental morphology section 2). Segments
with the same surface form but different PoS tags
are treated as different lexemes, and are represented
as separate arcs (e.g. the two arcs labeled neim from
node 6 to 7).
0
5
bclm/NNP
1b/IN
2
bcl/NN
7
hneim/VB
6
h/DT
clm/NN
clm/VB
cl/NN
3
h/DT
4
fl/POS
clm/NN
hm/PRP
neim/VB
neim/JJ
Figure 1: The Lattice for the Hebrew Phrase bclm hneim
A similar structure is used in speech recognition.
There, a lattice is used to represent the possible sen-
tences resulting from an interpretation of an acoustic
model. In speech recognition the arcs of the lattice
are typically weighted in order to indicate the prob-
ability of specific transitions. Given that weights on
all outgoing arcs sum up to one, weights induce a
probability distribution on the lattice paths. In se-
quential tagging models such as (Adler and Elhadad,
2006; Bar-Haim et al, 2007; Smith et al, 2005)
weights are assigned according to a language model
based on linear context. In our model, however, all
lattice paths are taken to be a-priori equally likely.
5 A Generative PCFG Model
The input for the joint task is a sequence W =
w1, . . . , wn of space-delimited tokens. Each tokenmay admit multiple analyses, each of which a se-
quence of one or more lexemes (we use li to denotea lexeme) belonging a presupposed Hebrew lexicon
LEX . The entries in such a lexicon may be thought
of as meaningful surface segments paired up with
their PoS tags li = ?si, pi?, but note that a surfacesegment s need not be a space-delimited token.
The Input The set of analyses for a token is thus
represented as a lattice in which every arc corre-
sponds to a specific lexeme l, as shown in Figure
1. A morphological analyzer M : W ? L is a
function mapping sentences in Hebrew (W ? W)
to their corresponding lattices (M(W ) = L ? L).
We define the lattice L to be the concatenation of the
lattices Li corresponding to the input words wi (s.t.
M(wi) = Li). Each connected path ?l1 . . . lk? ?
L corresponds to one morphological segmentation
possibility of W .
The Parser Given a sequence of input tokens
W = w1 . . . wn and a morphological analyzer, welook for the most probable parse tree pi s.t.
p?i = arg max
pi
P (pi|W,M)
Since the lattice L for a given sentence W is deter-
mined by the morphological analyzer M we have
p?i = arg max
pi
P (pi|W,M,L)
Hence, our parser searches for a parse tree pi over
lexemes ?l1 . . . lk? s.t. li = ?si, pi? ? LEX ,
?l1 . . . lk? ? L and M(W ) = L. So we remain with
p?i = arg max
pi
P (pi|L)
which is precisely the formula corresponding to the
so-called lattice parsing familiar from speech recog-
nition. Every parse pi selects a specific morphologi-
cal segmentation ?l1...lk? (a path through the lattice).This is akin to PoS tags sequences induced by dif-
ferent parses in the setup familiar from English and
explored in e.g. (Charniak et al, 1996).
374
Our use of an unweighted lattice reflects our be-
lief that all the segmentations of the given input sen-
tence are a-priori equally likely; the only reason to
prefer one segmentation over the another is due to
the overall syntactic context which is modeled via
the PCFG derivations. A compatible view is pre-
sented by Charniak et al (1996) who consider the
kind of probabilities a generative parser should get
from a PoS tagger, and concludes that these should
be P (w|t) ?and nothing fancier?.3 In our setting,
therefore, the Lattice is not used to induce a proba-
bility distribution on a linear context, but rather, it is
used as a common-denominator of state-indexation
of all segmentations possibilities of a surface form.
This is a unique object for which we are able to de-
fine a proper probability model. Thus our proposed
model is a proper model assigning probability mass
to all ?pi,L? pairs, where pi is a parse tree and L is
the one and only lattice that a sequence of characters
(and spaces) W over our alpha-beth gives rise to.
?
pi,L
P (pi,L) = 1; L uniquely index W
The Grammar Our parser looks for the most
likely tree spanning a single path through the lat-
tice of which the yield is a sequence of lexemes.
This is done using a simple PCFG which is lexeme-
based. This means that the rules in our grammar
are of two kinds: (a) syntactic rules relating non-
terminals to a sequence of non-terminals and/or PoS
tags, and (b) lexical rules relating PoS tags to lattice
arcs (lexemes). The possible analyses of a surface
token pose constraints on the analyses of specific
segments. In order to pass these constraints onto the
parser, the lexical rules in the grammar are of the
form pi ? ?si, pi?
Parameter Estimation The grammar probabili-
ties are estimated from the corpus using simple rela-
tive frequency estimates. Lexical rules are estimated
in a similar manner. We smooth Prf (p ? ?s, p?) forrare and OOV segments (s ? l, l ? L, s unseen) us-
ing a ?per-tag? probability distribution over rare seg-
ments which we estimate using relative frequency
estimates for once-occurring segments.
3An English sentence with ambiguous PoS assignment can
be trivially represented as a lattice similar to our own, where
every pair of consecutive nodes correspond to a word, and every
possible PoS assignment for this word is a connecting arc.
Handling Unknown tokens When handling un-
known tokens in a language such as Hebrew various
important aspects have to be borne in mind. Firstly,
Hebrew unknown tokens are doubly unknown: each
unknown token may correspond to several segmen-
tation possibilities, and each segment in such se-
quences may be able to admit multiple PoS tags.
Secondly, some segments in a proposed segment se-
quence may in fact be seen lexical events, i.e., for
some p tag Prf (p ? ?s, p?) > 0, while other seg-ments have never been observed as a lexical event
before. The latter arcs correspond to OOV words
in English. Finally, the assignments of PoS tags to
OOV segments is subject to language specific con-
straints relative to the token it was originated from.
Our smoothing procedure takes into account all
the aforementioned aspects and works as follows.
We first make use of our morphological analyzer to
find all segmentation possibilities by chopping off
all prefix sequence possibilities (including the empty
prefix) and construct a lattice off of them. The re-
maining arcs are marked OOV. At this stage the lat-
tice path corresponds to segments only, with no PoS
assigned to them. In turn we use two sorts of heuris-
tics, orthogonal to one another, to prune segmenta-
tion possibilities based on lexical and grammatical
constraints. We simulate lexical constraints by using
an external lexical resource against which we verify
whether OOV segments are in fact valid Hebrew lex-
emes. This heuristics is used to prune all segmenta-
tion possibilities involving ?lexically improper? seg-
ments. For the remaining arcs, if the segment is in
fact a known lexeme it is tagged as usual, but for the
OOV arcs which are valid Hebrew entries lacking
tags assignment, we assign all possible tags and then
simulate a grammatical constraint. Here, all token-
internal collocations of tags unseen in our training
data are pruned away. From now on all lattice arcs
are tagged segments and the assignment of probabil-
ity P (p ? ?s, p?) to lattice arcs proceeds as usual.4
A rather pathological case is when our lexical
heuristics prune away all segmentation possibilities
and we remain with an empty lattice. In such cases
we use the non-pruned lattice including all (possibly
ungrammatical) segmentation, and let the statistics
(including OOV) decide. We empirically control for
4Our heuristics may slightly alter Ppi,L P (pi, L) ? 1
375
the effect of our heuristics to make sure our pruning
does not undermine the objectives of our joint task.
6 Experimental Setup
Previous work on morphological and syntactic dis-
ambiguation in Hebrew used different sets of data,
different splits, differing annotation schemes, and
different evaluation measures. Our experimental
setup therefore is designed to serve two goals. Our
primary goal is to exploit the resources that are most
appropriate for the task at hand, and our secondary
goal is to allow for comparison of our models? per-
formance against previously reported results. When
a comparison against previous results requires addi-
tional pre-processing, we state it explicitly to allow
for the reader to replicate the reported results.
Data We use the Hebrew Treebank, (Sima?an
et al, 2001), provided by the knowledge center
for processing Hebrew, in which sentences from
the daily newspaper ?Ha?aretz? are morphologically
segmented and syntactically annotated. The tree-
bank has two versions, v1.0 and v2.0, containing
5001 and 6501 sentences respectively. We use v1.0
mainly because previous studies on joint inference
reported results w.r.t. v1.0 only.5 We expect that
using the same setup on v2.0 will allow a cross-
treebank comparison.6 We used the first 500 sen-
tences as our dev set and the rest 4500 for training
and report our main results on this split. To facili-
tate the comparison of our results to those reported
by (Cohen and Smith, 2007) we use their data set in
which 177 empty and ?malformed?7 were removed.
The first 3770 trees of the resulting set then were
used for training, and the last 418 are used testing.
(we ignored the 419 trees in their development set.)
Morphological Analyzer Ideally, we would use
an of-the-shelf morphological analyzer for mapping
each input token to its possible analyses. Such re-
sources exist for Hebrew (Itai et al, 2006), but un-
fortunately use a tagging scheme which is incom-
5The comparison to performance on version 2.0 is meaning-
less not only because of the change in size, but also conceptual
changes in the annotation scheme
6Unfortunatley running our setup on the v2.0 data set is cur-
rently not possible due to missing tokens-morphemes alignment
in the v2.0 treebank.
7We thank Shay Cohen for providing us with their data set
and evaluation Software.
patible with the one of the Hebrew Treebank.8 For
this reason, we use a data-driven morphological an-
alyzer derived from the training data similar to (Co-
hen and Smith, 2007). We construct a mapping from
all the space-delimited tokens seen in the training
sentences to their corresponding analyses.
Lexicon and OOV Handling Our data-driven
morphological-analyzer proposes analyses for un-
known tokens as described in Section 5. We use the
HSPELL9 (Har?el and Kenigsberg, 2004) wordlist
as a lexeme-based lexicon for pruning segmenta-
tions involving invalid segments. Models that em-
ploy this strategy are denoted hsp. To control for
the effect of the HSPELL-based pruning, we also ex-
perimented with a morphological analyzer that does
not perform this pruning. For these models we limit
the options provided for OOV words by not consid-
ering the entire token as a valid segmentation in case
at least some prefix segmentation exists. This ana-
lyzer setting is similar to that of (Cohen and Smith,
2007), and models using it are denoted nohsp,
Parser and Grammar We used BitPar (Schmid,
2004), an efficient general purpose parser,10 together
with various treebank grammars to parse the in-
put sentences and propose compatible morpholog-
ical segmentation and syntactic analysis.
We experimented with increasingly rich gram-
mars read off of the treebank. Our first model is
GTplain, a PCFG learned from the treebank afterremoving all functional features from the syntactic
categories. In our second model GTvpi we alsodistinguished finite and non-finite verbs and VPs as
8Mapping between the two schemes involves non-
deterministic many-to-many mappings, and in some cases re-
quire a change in the syntactic trees.
9An open-source Hebrew spell-checker.
10Lattice parsing can be performed by special initialization
of the chart in a CKY parser (Chappelier et al, 1999). We
currently simulate this by crafting a WCFG and feeding it to
BitPar. Given a PCFG grammar G and a lattice L with nodes
n1 . . . nk , we construct the weighted grammar GL as follows:for every arc (lexeme) l ? L from node ni to node nj , we add
to GL the rule [l ? tni , tni+1 , . . . , tnj?1 ] with a probability of1 (this indicates the lexeme l spans from node ni to node nj).
GL is then used to parse the string tn1 . . . tnk?1 , where tni isa terminal corresponding to the lattice span between node ni
and ni+1. Removing the leaves from the resulting tree yields a
parse for L under G, with the desired probabilities. We use a
patched version of BitPar allowing for direct input of probabili-
ties instead of counts. We thank Felix Hageloh (Hageloh, 2006)
for providing us with this version.
376
proposed in (Tsarfaty, 2006). In our third model
GTppp we also add the distinction between gen-eral PPs and possessive PPs following Goldberg and
Elhadad (2007). In our forth model GTnph weadd the definiteness status of constituents follow-
ing Tsarfaty and Sima?an (2007). Finally, model
GTv = 2 includes parent annotation on top of thevarious state-splits, as is done also in (Tsarfaty and
Sima?an, 2007; Cohen and Smith, 2007). For all
grammars, we use fine-grained PoS tags indicating
various morphological features annotated therein.
Evaluation We use 8 different measures to eval-
uate the performance of our system on the joint dis-
ambiguation task. To evaluate the performance on
the segmentation task, we report SEG, the stan-
dard harmonic means for segmentation Precision
and Recall F1 (as defined in Bar-Haim et al (2005);Tsarfaty (2006)) as well as the segmentation ac-
curacy SEGTok measure indicating the percentageof input tokens assigned the correct exact segmen-
tation (as reported by Cohen and Smith (2007)).
SEGTok(noH) is the segmentation accuracy ignor-ing mistakes involving the implicit definite article
h.11 To evaluate our performance on the tagging
task we report CPOS and FPOS corresponding
to coarse- and fine-grained PoS tagging results (F1)measure. Evaluating parsing results in our joint
framework, as argued by Tsarfaty (2006), is not triv-
ial under the joint disambiguation task, as the hy-
pothesized yield need not coincide with the correct
one. Our parsing performance measures (SY N )
thus report the PARSEVAL extension proposed in
Tsarfaty (2006). We further report SY NCS , the
parsing metric of Cohen and Smith (2007), to fa-
cilitate the comparison. We report the F1 value ofboth measures. Finally, our U (unparsed) measure
is used to report the number of sentences to which
our system could not propose a joint analysis.
7 Results and Analysis
The accuracy results for segmentation, tagging and
parsing using our different models and our standard
data split are summarized in Table 1. In addition
we report for each model its performance on gold-
segmented input (GS) to indicate the upper bound
11Overt definiteness errors may be seen as a wrong feature
rather than as wrong constituent and it is by now an accepted
standard to report accuracy with and without such errors.
for the grammars? performance on the parsing task.
The table makes clear that enriching our grammar
improves the syntactic performance as well as mor-
phological disambiguation (segmentation and POS
tagging) accuracy. This supports our main thesis that
decisions taken by single, improved, grammar are
beneficial for both tasks. When using the segmen-
tation pruning (using HSPELL) for unseen tokens,
performance improves for all tasks as well. Yet we
note that the better grammars without pruning out-
perform the poorer grammars using this technique,
indicating that the syntactic context aids, to some
extent, the disambiguation of unknown tokens.
Table 2 compares the performance of our system
on the setup of Cohen and Smith (2007) to the best
results reported by them for the same tasks.
Model SEGTok CPOS FPOS SY NCS
GTnohsp/pln 89.50 81.00 77.65 62.22
GTnohsp/???+nph 89.58 81.26 77.82 64.30
CSpln 91.10 80.40 75.60 64.00
CSv=2 90.90 80.50 75.40 64.40
GThsp/pln 93.13 83.12 79.12 64.46
GTnohsp/???+v=2 89.66 82.85 78.92 66.31Oracle CSpln 91.80 83.20 79.10 66.50Oracle CSv=2 91.70 83.00 78.70 67.40
GThsp/???+v=2 93.38 85.08 80.11 69.11
Table 2: Segmentation, Parsing and Tagging Results us-
ing the Setup of (Cohen and Smith, 2007) (sentence
length ? 40). The Models? are Ordered by Performance.
We first note that the accuracy results of our
system are overall higher on their setup, on all
measures, indicating that theirs may be an easier
dataset. Secondly, for all our models we provide
better fine- and coarse-grained POS-tagging accu-
racy, and all pruned models outperform the Ora-
cle results reported by them.12 In terms of syn-
tactic disambiguation, even the simplest grammar
pruned with HSPELL outperforms their non-Oracle
results. Without HSPELL-pruning, our simpler
grammars are somewhat lagging behind, but as the
grammars improve the gap is bridged. The addi-
tion of vertical markovization enables non-pruned
models to outperform all previously reported re-
12Cohen and Smith (2007) make use of a parameter (?)
which is tuned separately for each of the tasks. This essentially
means that their model does not result in a true joint inference,
as executions for different tasks involve tuning a parameter sep-
arately. In our model there are no such hyper-parameters, and
the performance is the result of truly joint disambiguation.
377
Model U SEGTok / no H SEGF CPOS FPOS SY N / SY NCS GS SY N
GTnohsp/pln 7 89.77 / 93.18 91.80 80.36 76.77 60.41 / 61.66 65.00
???+vpi 7 89.80 / 93.18 91.84 80.37 76.74 61.16 / 62.41 66.70
???+ppp 7 89.79 / 93.20 91.86 80.43 76.79 61.47 / 62.86 67.22
???+nph 7 89.78 / 93.20 91.86 80.43 76.87 61.85 / 63.06 68.23
???+v=2 9 89.12 / 92.45 91.77 82.02 77.86 64.53 / 66.02 70.82
GThsp/pln 11 92.00 / 94.81 94.52 82.35 78.11 62.10 / 64.17 65.00
???+vpi 11 92.03 / 94.82 94.58 82.39 78.23 63.00 / 65.06 66.70
???+ppp 11 92.02 / 94.85 94.58 82.48 78.33 63.26 / 65.42 67.22
???+nph 11 92.14 / 94.91 94.73 82.58 78.47 63.98 / 65.98 68.23
???+v=2 13 91.42 / 94.10 94.67 84.23 79.25 66.60 / 68.79 70.82
Table 1: Segmentation, tagging and parsing results on the Standard dev/train Split, for all Sentences
sults. Furthermore, the combination of pruning and
vertical markovization of the grammar outperforms
the Oracle results reported by Cohen and Smith.
This essentially means that a better grammar tunes
the joint model for optimized syntactic disambigua-
tion at least in as much as their hyper parameters
do. An interesting observation is that while vertical
markovization benefits all our models, its effect is
less evident in Cohen and Smith.
On the surface, our model may seem as a special
case of Cohen and Smith in which ? = 0. How-
ever, there is a crucial difference: the morphological
probabilities in their model come from discrimina-
tive models based on linear context. Many morpho-
logical decisions are based on long distance depen-
dencies, and when the global syntactic evidence dis-
agrees with evidence based on local linear context,
the two models compete with one another, despite
the fact that the PCFG takes also local context into
account. In addition, as the CRF and PCFG look at
similar sorts of information from within two inher-
ently different models, they are far from independent
and optimizing their product is meaningless. Cohen
and Smith approach this by introducing the ? hy-
perparameter, which performs best when optimized
independently for each sentence (cf. Oracle results).
In contrast, our morphological probabilities are
based on a unigram, lexeme-based model, and all
other (local and non-local) contextual considerations
are delegated to the PCFG. This fully generative
model caters for real interaction between the syn-
tactic and morphological levels as a part of a single
coherent process.
8 Discussion and Conclusion
Employing a PCFG-based generative framework to
make both syntactic and morphological disambigua-
tion decisions is not only theoretically clean and
linguistically justified and but also probabilistically
apropriate and empirically sound. The overall per-
formance of our joint framework demonstrates that
a probability distribution obtained over mere syn-
tactic contexts using a Treebank grammar and a
data-driven lexicon outperforms upper bounds pro-
posed by previous joint disambiguation systems and
achieves segmentation and parsing results on a par
with state-of-the-art standalone applications results.
Better grammars are shown here to improve per-
formance on both morphological and syntactic tasks,
providing support for the advantage of a joint frame-
work over pipelined or factorized ones. We conjec-
ture that this trend may continue by incorporating
additional information, e.g., three-dimensional mod-
els as proposed by Tsarfaty and Sima?an (2007). In
the current work morphological analyses and lexi-
cal probabilities are derived from a small Treebank,
which is by no means the best way to go. Using
a wide-coverage morphological analyzer based on
(Itai et al, 2006) should cater for a better cover-
age, and incorporating lexical probabilities learned
from a big (unannotated) corpus (cf. (Levinger et
al., 1995; Goldberg et al, ; Adler et al, 2008)) will
make the parser more robust and suitable for use in
more realistic scenarios.
Acknowledgments We thank Meni Adler and
Michael Elhadad (BGU) for helpful comments and
discussion. We further thank Khalil Simaan (ILLC-
UvA) for his careful advise concerning the formal
details of the proposal. The work of the first au-
thor was supported by the Lynn and William Frankel
Center for Computer Sciences. The work of the sec-
ond author as well as collaboration visits to Israel
was financed by NWO, grant number 017.001.271.
378
References
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceeding of COLING-
ACL-06, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised Lexicon-Based Reso-
lution of Unknown Words for Full Morpholological
Analysis. In Proceedings of ACL-08.
Meni Adler. 2001. Hidden Markov Model for Hebrew
Part-of-Speech Tagging. Master?s thesis, Ben-Gurion
University of the Negev.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos- tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2007.
Part-of-speech tagging of Modern Hebrew text. Natu-
ral Language Engineering, 14(02):223?251.
J. Chappelier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice Parsing for Speech Recognition.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony R. Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael L. Littman, and John McCann. 1996. Tag-
gers for Parsers. AI, 85(1-2):45?57.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic Di-
alects. In Proceedings of EACL-06.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL-07, pages 208?217.
Lewis Glinert. 1989. The Grammar of Modern Hebrew.
Cambridge University Press.
Yoav Goldberg and Michael Elhadad. 2007. SVM Model
Tampering and Anchored Learning: A Case Study
in Hebrew NP Chunking. In Proceeding of ACL-07,
Prague, Czech Republic.
Yoav Goldberg, Meni Adler, and Michael Elhadad. EM
Can Find Pretty G]ood HMM POS-Taggers (When
Given a Good Start), booktitle = Proceedings of ACL-
08, year = 2008,.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceeding of
ACL-05.
Felix Hageloh. 2006. Parsing Using Transforms over
Treebanks. Master?s thesis, University of Amsterdam.
Nadav Har?el and Dan Kenigsberg. 2004. HSpell - the
free Hebrew Spell Checker and Morphological Ana-
lyzer. Israeli Seminar on Computational Linguistics.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
Computational Lexicon of Contemporary Hebrew. In
Proceedings of LREC-06.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing Morpholexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Computa-
tional Linguistics, 21:383?404.
Helmut Schmid, 2000. LoPar: Design and Implementa-
tion. Institute for Computational Linguistics, Univer-
sity of Stuttgart.
Helmut Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit Vector. In
Proceedings of COLING-04.
Erel Segal. 2000. Hebrew Morphological Analyzer for
Hebrew Undotted Texts. Master?s thesis, Technion,
Haifa, Israel.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical Disambiguation of Hebrew: A Case Study in
Classifier Combination. In Proceedings of EMNLP-
CoNLL-07, pages 439?447.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues, volume 42.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of HLT-05, pages
475?482, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-Based or
Morpheme-Based? Annotation Strategies for Modern
Hebrew Clitics. In Proceedings of LREC-08.
Reut Tsarfaty and Khalil Sima?an. 2004. An Integrated
Model for Morphological and Syntactic Disambigua-
tion in Modern Hebrew. MOZAIEK detailed proposal,
NWO Mozaiek scheme.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
Dimensional Parametrization for Parsing Morphologi-
cally Rich Languages. In Proceedings of IWPT-07.
Reut Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceedings of ACL-SRW-06.
Shlomo Yona and Shuly Wintner. 2005. A Finite-
state Morphological Grammar of Hebrew. In Proceed-
ings of the ACL-05 Workshop on Computational Ap-
proaches to Semitic Languages.
379
Proceedings of the 10th Conference on Parsing Technologies, pages 156?167,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Three-Dimensional Parametrization for Parsing
Morphologically Rich Languages
Reut Tsarfaty and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands
{rtsarfat,simaan}@science.uva.nl
Abstract
Current parameters of accurate unlexical-
ized parsers based on Probabilistic Context-
Free Grammars (PCFGs) form a two-
dimensional grid in which rewrite events
are conditioned on both horizontal (head-
outward) and vertical (parental) histories.
In Semitic languages, where arguments
may move around rather freely and phrase-
structures are often shallow, there are ad-
ditional morphological factors that govern
the generation process. Here we pro-
pose that agreement features percolated up
the parse-tree form a third dimension of
parametrization that is orthogonal to the pre-
vious two. This dimension differs from
mere ?state-splits? as it applies to a whole
set of categories rather than to individual
ones and encodes linguistically motivated
co-occurrences between them. This paper
presents extensive experiments with exten-
sions of unlexicalized PCFGs for parsing
Modern Hebrew in which tuning the param-
eters in three dimensions gradually leads to
improved performance. Our best result in-
troduces a new, stronger, lower bound on the
performance of treebank grammars for pars-
ing Modern Hebrew, and is on a par with
current results for parsing Modern Standard
Arabic obtained by a fully lexicalized parser
trained on a much larger treebank.
1 Dimensions of Unlexicalized Parsing
Probabilistic Context Free Grammars (PCFGs) are
the formal backbone of most high-accuracy statisti-
cal parsers for English, and a variety of techniques
was developed to enhance their performance rela-
tive to the na??ve treebank implementation ? from
unlexicalized extensions exploiting simple category
splits (Johnson, 1998; Klein and Manning, 2003)
to fully lexicalized parsers that condition events be-
low a constituent upon the head and additional lexi-
cal content (Collins, 2003; Charniak, 1997). While
it is clear that conditioning on lexical content im-
proves the grammar?s disambiguation capabilities,
Klein and Manning (2003) demonstrate that a well-
crafted unlexicalized PCFG can close the gap, to a
large extent, with current state-of-the-art lexicalized
parsers for English.
The factor that sets apart vanilla PCFGs (Char-
niak, 1996) from their unlexicalized extensions pro-
posed by, e.g., (Johnson, 1998; Klein and Manning,
2003), is the choice for statistical parametrization
that weakens the independence assumptions implicit
in the treebank grammar. Studies on accurate unlex-
icalized parsing models outline two dimensions of
parametrization. The first, proposed by (Johnson,
1998), is the annotation of parental history, and the
second encodes a head-outward generation process
(Collins, 2003). Johnson (1998) augments node la-
bels with the label of their parent, thus incorporat-
ing a dependency on the node?s grandparent. Collins
(2003) proposes to generate the head of a phrase first
and then generate its sisters using Markovian pro-
cesses, thereby exploiting head/sister-dependencies.
156
Klein and Manning (2003) systematize the dis-
tinction between these two forms of parametrization
by drawing them on a horizontal-vertical grid: par-
ent encoding is vertical (external to the rule) whereas
head-outward generation is horizontal (internal to
the rule). By varying the value of the parame-
ters along the grid, Klein and Manning (2003) tune
their treebank grammar to achieve improved perfor-
mance. This two-dimensional parametrization has
been instrumental in devising parsing models that
improve disambiguation capabilities for English as
well as other languages, such as German (Dubey and
Keller, 2003) Czech (Collins et al, 1999) and Chi-
nese (Bikel and Chiang, 2000). However, accuracy
results for parsing languages other than English still
lag behind.1
We propose that for various languages includ-
ing the Semitic family, e.g. Modern Hebrew (MH)
and Modern Standard Arabic (MSA), a third di-
mension of parametrization is necessary for encod-
ing linguistic information relevant for breaking false
independence assumptions. In Semitic languages,
arguments may move around rather freely and the
phrase-structure of clause-level categories is often
shallow. For such languages agreement features play
a role in disambiguation at least as important as the
vertical and horizontal conditioning. We propose a
third dimension of parameterizations that encodes
morphological features such as those realizing syn-
tactic agreement. These features are percolated from
surface forms in a bottom-up fashion and express
information that is complementary to the horizon-
tal and vertical generation histories proposed before.
Such morphological information refines syntactic
categories based on their morpho-syntactic role, and
captures linguistically motivated co-occurrences and
dependencies manifested via, e.g., morpho-syntactic
agreement.
This work aims at parsing MH and explores the
empirical contribution of the three dimensions of
parameters specified above. We present extensive
experiments that gradually lead to improved perfor-
mance as we extend the degree to which the three
dimensions are exploited. Our best model uses all
three dimensions of parametrization, and our best re-
1The learning curves over increasing training data (e.g., for
German (Dubey and Keller, 2003)) show that treebank size can-
not be the sole factor to account for the inferior performance.
sult is on a par with those achieved for MSA using a
fully lexicalized parser and a much larger treebank.
The remainder of this document is organized as fol-
lows. In section 2 we review characteristic aspects
of MH (and other Semitic languages) and illustrate
the special role of morphology and dependencies
displayed by morpho-syntactic processes using the
case of syntactic definiteness in MH. In section 3 we
define our three-dimensional parametrization space.
In section 4 we spell out the method and procedure
for the empirical evaluation of one, two and three
parametrization dimensions, and in section 5 we re-
port and analyze results for different parametrization
choices. Finally, section 6 discusses related work
and in section 7 we summarize and conclude.
2 Dimensions of Modern Hebrew Syntax
Parsing MH is in its infancy. Although a syntacti-
cally annotated corpus has been available for quite
some time (Sima?an et al, 2001), we know of only
two studies attempting to parse MH using statistical
methods (see section 6). One reason for the sparse-
ness in this field is that the adaptation of existing
models to parsing MH is technically involved yet
does not guarantee to yield comparable results as
the processes that license grammatical structures of
phrases and sentences in MH differ from those as-
sumed for English. This section outlines differences
between English and MH and discusses their reflec-
tion in the MH treebank annotation scheme. We
argue that on top of syntactic processes exploited
by current parsers there is an orthogonal morpho-
syntactic dimension which is invaluable for syntac-
tic disambiguation, and it can be effectively learned
using simple treebank grammars.
2.1 Modern Hebrew Structure
Phrases and sentences in MH, as well as in Arabic
and other Semitic languages, have a relatively flexi-
ble phrase structure. Subjects, verbs and objects can
be inverted and prepositional phrases, adjuncts and
verbal modifiers can move around rather freely. The
factors that affect word-order in the language are not
exclusively syntactic and have to do with rhetorical
and pragmatic factors as well.2
2See, for instance, (Melnik, 2002) for an Information
Structure-syntactic account of verb initial sentences.
157
(a) S
NP.MP-SBJ
CD.MP
sni
two.MP
N.MP
hildim
the-children.MP
VP.MP
V.MP
aklw
ate.MP
NP.FS-OBJ
N.FS
ewgh
cake.FS
(b) S
NP.FS-OBJ
N.FS
ewgh
cake.FS
VP.MP
V.MP
aklw
ate.MP
NP.MP-SBJ
CD.MP
sni
two.MP
N.MP
hildim
the-children.MP
Figure 1: Word Order and Agreement Features in MH
Phrases: Agreement on MP features reveals the subject-
predicate dependency between surface forms and their dom-
inating constituents in a variable phrase-structure (marking
M(asculine), F(eminine), S(ingular), P(lural).)
It would be too strong a claim, however, to clas-
sify MH (and similar languages) as a free-word-
order language in the canonical sense. The level of
freedom in the order and number of internal con-
stituents varies between syntactic categories. Within
a verb phrase or a sentential clause, for instance,
the order of constituents obeys less strict rules than
within, e.g., a noun phrase.3 Figure 1 illustrates two
syntactic structures that express the same grammat-
ical relations yet vary in their internal order of con-
stituents. Within the noun phrase constituents, how-
ever, determiners always precede nouns.
Within the flexible phrase structure it is typically
morphological information that provides cues for the
grammatical relations between surface forms. In
figure 1, for example, it is agreement on gender
and number that reveals the subject-predicate depen-
dency between surface forms. Figure 1 also shows
that agreement features help to reveal such relations
between higher levels of constituents as well.
Determining the child constituents that contribute
each of the features is not a trivial matter either. To
illustrate the extent and the complexity of that matter
let us consider definiteness in MH, which is morpho-
logically marked (as an h prefix to the stem, glossed
here explicitly as ?the-?) and behaves as a syntactic
3See (Wintner, 2000) and (Goldberg et al, 2006) for formal
and statistical accounts (respectively) of noun phrases in MH.
(a) NP.FS.D
NP.FS.D
sganit hmnhl
deputy.FS the-manager.MS.D
ADJP.FS.D
hmswrh
the-dedicated.FS.D
(a) S
NP.FS.D
sganit hmnhl
deputy.FS the-manager.MS.D
PREDP.FS
mswrh
dedicated.FS
Figure 2: Definiteness in MH as a Phrase-Level Agreement
Feature: Agreement on definiteness helps to determine the in-
ternal structure of a higher level NP (a), and the absence thereof
helps to determine the attachment to a predicate in a verb-less
sentence (b) (marking D(efiniteness))
(a) S
NP.FS.D
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
VP.FS
V.FS
htpjrh
resigned.FS
(b) S?V?
NP?NNT?.FS.D
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
VP?V?).FS
V.FS
htpjrh
resigned.FS
Figure 3: Phrase-Level Agreement Features and Head-
Dependencies in MH: The direction of percolating definiteness
in MH is distinct of that of the head (marking ?head-tag?)
property (Danon, 2001). Definite noun-phrases ex-
hibit agreement with other modifying phrases, and
such agreement helps to determine the internal struc-
ture, labels, and the correct level of attachment as
illustrated in figure 2. The agreement on definite-
ness helps to determine the internal structure of noun
phrases 2(a), and the absence thereof helps in de-
termining the attachment to predicates in verb-less
sentences, as in 2(b). Finally, definiteness may be
percolated from a different form than the one deter-
mining the gender and number of a phrase. In figure
3(a), for instance, the definiteness feature (marked
as D) percolates from ?hmnhl ? (the-manager.MS.D)
while the gender and number are percolated from
?sganit ? (deputy.FS). The direction of percolation
of definiteness may be distinct of that of percolat-
ing head information, as can be seem in figure 3(b).
(The direction of head-dependencies in MH typi-
cally coincides with that of percolating gender.)
To summarize, agreement features are helpful in
analyzing and disambiguating syntactic structures in
MH, not only at the lexical level, but also at higher
levels of constituency. In MH, features percolated
from different surface forms jointly determine the
features of higher-level constituents, and such fea-
tures manifest multiple dependencies, which in turn
cannot be collapsed onto a single head.
158
2.2 The Modern Hebrew Treebank Scheme
The annotation scheme of version 2.0 of the MH
treebank (Sima?an et al, 2001)4 aims to capture the
morphological and syntactic properties of MH just
described. This results in several aspects that dis-
tinguish the MH treebank from, e.g., the WSJ Penn
treebank annotation scheme (Marcus et al, 1994).
The MH treebank is built over word segments.
This means that the yields of the syntactic trees do
not correspond to space delimited words but rather
to morphological segments that carry distinct syn-
tactic roles, i.e., each segment corresponds to a sin-
gle POS tag. (This in turn means that prefixes
marking determiners, relativizers, prepositions and
definite articles are segmented away and appear as
leaves in a syntactic parse tree.) The POS categories
assigned to segmented words are decorated with fea-
tures such as gender, number, person and tense, and
these features are percolated higher up the tree ac-
cording to pre-defined syntactic dependencies (Kry-
molowski et al, 2007). Since agreement features
of non-terminal constituents may be contributed by
more than one child, the annotation scheme defines
multiple dependency labels that guide the percola-
tion of the different features higher up the tree. Def-
initeness in the MH treebank is treated as a segment
at the POS tags level and as a feature at the level of
non-terminals. As any other feature, it is percolated
higher up the tree according to marked dependency
labels. Table 1 lists the features and values annotated
on top of syntactic categories and table 2 describes
the dependencies according to which these features
are percolated from child constituents to their par-
ents.
In order to comply with the flexible phrase struc-
ture in MH, clausal categories (S, SBAR and FRAG
and their corresponding interrogatives SQ, SQBAR
and FRAGQ) are annotated as flat structures. Verbs
(VB tags) always attach to a VP mother, however
only non-finite VBs can accept complements un-
der the same VP parent, meaning that all inflected
verb forms are represented as unary productions
under an inflected VP. NP and PP are annotated
4Version 2.0 of the MH treebank is publicly available
at http://mila.cs.technion.ac.il/english/
index.html along with a complete overview of the MH
annotation scheme and illustrative examples (Krymolowski et
al., 2007).
Feature:Value Value Encoded
gender:Z masculine
gender:N feminine
gender:B both
number:Y singular
number:R plural
number:B both
definiteness:H definite
definiteness:U underspecified
Table 1: Features and Values in the MH Treebank
Dependency Type Features Percolated
DEP HEAD all
DEP MAJOR at least gender
DEP NUMBER number
DEP DEFINITE definiteness
DEP ACCUSATIVE case
DEP MULTIPLE all (e.g., conjunction)
Table 2: Dependency Labels in the MH Treebank
as nested structures capturing the recursive struc-
ture of construct-state nouns, numerical expressions
and possession. An additional category, PREDP, is
added in the treebank scheme to account for sen-
tences in MH that lack a copular element, and it may
also be decorated with inflectional features agreeing
with the subject. The MH treebank scheme also fea-
tures null elements that mark traces and additional
labels that mark functional features (e.g., SBJ,OBJ)
which we strip off and ignore throughout this study.
Morphological features percolated up the tree
manifest dependencies that are marked locally yet
have a global effect. We propose to learn treebank
grammars in which the syntactic categories are aug-
mented with morphological features at all levels of
the hierarchy. This allows to learn finer-grained
categories with subtle differences in their syntactic
behavior and to capture non-independence between
certain parts of the syntactic parse-tree.
3 Refining the Parameter Space
(Klein and Manning, 2003) argue that parent en-
coding on top of syntactic categories and RHS
markovization of CFG productions are two instances
of the same idea, namely that of encoding the gener-
ation history of a node to a varying degree. They
subsequently describe two dimensions that define
their parameters? space. The vertical dimension (v),
capturing the history of the node?s ancestors in a top-
159
down generation process (e.g., its parent and grand-
parent), and the horizontal dimension (h), capturing
the previously generated horizontal ancestors of a
node (effectively, its sisters) in a head-outward gen-
eration process. By varying the value of h and v
along this two-dimensional grid they improve per-
formance of their induced treebank grammar.
Formally, the probability of a parse tree pi is cal-
culated as the probability of its derivation, the se-
quential application of rewrite rules. This in turn
is calculated as the product of rules? probabilities,
approximated by assuming independence between
them P (pi) = ?i P (ri|r1 ? ... ? ri?1) ?
?
i P (ri).
The vertical dimension v can be thought of as a func-
tion ?0 selecting features from the generation his-
tory of the constituent thus restoring selected depen-
dencies:
P (ri) = P (ri|?0(r1 ? .. ? ri?1))
The horizontal dimension h can be thought of as two
functions ?1,?2 over decomposed rules, where ?1
selects hidden internal features of the parent, and
?2 selects previously generated sisters in a head-
outward Markovian process (we retain here the as-
sumption that the head child H always matters).
P (ri) = Ph(H|?1(LHS(ri)))
?
?
C?RHS(ri)?H
PC(C|?2(RHS(ri)),H)
The fact that the default notion of a treebank
grammar takes v = 1 (i.e., ?0(r1 ? .. ? ri?1) = ?)
and h = ? (RHS cannot decompose) is, according
to Klein and Manning (2003), a historical accident.
We claim that languages with freeer word order
and richer morphology call for an additional dimen-
sion of parametrization. The additional parameter
shows to what extent morphological features en-
coded in a specialized structure back up the deriva-
tion of the tree. This dimension can be thought of
as a function ?3 selecting aspects of morphological
orthogonal analysis of the rules, where MA denotes
morphological analysis of the syntactic categories in
both LHS and RHS of the rule.
P (ri) = P (ri|?3(MA(ri)))
The fact that in current parsers ?3(MA(ri)) = ? is,
we claim, another historical accident. Parsing En-
glish is quite remarkable in that it can be done with
Figure 4: The Three-Dimensional Parametrization Space
impoverished morphological treatment, but for lan-
guages in which morphological processes are more
pertinent, we argue, bi-dimensional parametrization
shall not suffice.
The emerging picture is as follows. Bare-category
skeletons reside in a bi-dimensional parametrization
space (figure 3(a)) in which the vertical (figure 3(b))
and horizontal (figure 3(c)) parameter instantiations
elaborate the generation history of a non-terminal
node. Specialized structures enriched with (an in-
creasing amount of) morphological features reside
deeper along a third dimension we refer to as depth
(d). Figure 4 illustrates an instantiation of d = 1
with a single definiteness feature. Higher d values
would imply adding more (accumulating) features.
Klein and Manning (2003) view the vertical
and horizontal parametrization dimensions as im-
plementing external and internal annotation strate-
gies respectively. External parameters indicate fea-
tures of the external environment that influence the
node?s expansion possibilities, and internal parame-
ters mark aspects of hidden internal content which
influence constituents? external distribution. We
view the third dimension of parametrization as im-
plementing a relational strategy of annotation en-
coding the way different constituents may combine
to form phrases and sentences. In a bottom up pro-
cess this annotation strategy imposes soft constraints
on a the top-down head-outward generation process.
Figure 6(a) focuses on a selected NP node high-
lighted in figure 4 and shows its expansion possibil-
ities in three dimensions. Figure 6(b) illustrates how
the depth expansion interacts with both parent anno-
160
(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension
Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003)
tation and neighbor dependencies thereby affecting
both distributions.
3.1 A Note on State-Splits
Recent studies (Klein and Manning, 2003; Mat-
suzaki et al, 2005; Prescher, 2005; Petrov et al,
2006) suggest that category-splits help in enhanc-
ing the performance of treebank grammars, and a
previous study on MH (Tsarfaty, 2006) outlines spe-
cific POS-tags splits that improve MH parsing ac-
curacy. Yet, there is a major difference between
category-splits, whether manually or automatically
acquired, and the kind of state-splits that arise from
agreement features that refine phrasal categories.
While category-splits aim at each category in iso-
lation, agreement features apply to a whole set
of categories all at once, thereby capturing refine-
ment of the categories as well as linguistically mo-
tivated co-occurrences between them. Individual
category-splits are viewed as taking place in a two-
dimensional space and it is hard to analyze and em-
pirically evaluate their interaction with other annota-
tion strategies. Here we propose a principled way to
statistically model the interaction between different
linguistic processes that license grammatical struc-
tures and empirically contrast their contribution.
3.2 A Note on Stochastic AV grammars
The practice of having morphological features or-
thogonal to a constituency structure is not a new
one and is familiar from formal theories of syntax
such as HPSG (Sag et al, 2003) and LFG (Ka-
plan and Bresnan, 1982). Here we propose to re-
frame systematic morphological decoration of syn-
tactic categories at all levels of the hierarchy as
(a) (b)
Figure 6: The Expansion Possibilities of a Non-Terminal
Node: Expanding the NP from figure 4 in a three-dimensional
parameterization Space
an additional dimension of statistical estimation for
learning unlexicalized treebank PCFGs. Our pro-
posal deviates from various stochastic extensions of
such constraints-based grammatical formalisms (cf.
(Abney, 1997)) and has the advantage of elegantly
bypassing the issue of loosing probability mass to
failed derivations due to unification failures. To the
best of our knowledge, this proposal has not been
empirically explored before.
4 Experimental Setup
Our goal is to determine the optimal strategy for
learning treebank grammars for MH and to contrast
it with bi-dimensional strategies explored for En-
glish. The methodology we use is adopted from
(Klein and Manning, 2003) and our procedure is
identical to the one described in (Johnson, 1998).
We define transformations over the treebank that ac-
cept as input specific points in the (h, v, d) space de-
picted in figure 7. We use the transformed training
sets for learning different treebank PCFGs which we
then used to parse unseen sentences, and detrans-
form the parses for the purpose of evaluation.5
5Previous studied on MH used different portions of the tree-
bank and its annotation scheme due to its gradual development
161
Data We use version 2.0 of the MH treebank
which consists of 6501 sentences from the daily
newspaper ?Ha?aretz?. We employ the syntactic cat-
egories, POS categories and morphological features
annotated therein. The data set is split into 13 sec-
tions consisting of 500 sentences each. We use the
first section (section 0) as our development set and
the last section (section 12) as our test set. The re-
maining sentences (sections 1?11) are all used for
training. After removing empty sentences, sentences
with uneven bracketing and sentences that do not
match the annotation scheme6 we remain with a de-
vset of 483 sentences (average length in word seg-
ments 48), a trainset of 5241 sentences (53) and
a testset of 496 sentences (58). Since this work
is only the first step towards the development of a
broad-coverage statistical parser for MH (and other
Semitic languages) we use the development set for
parameter-tuning and error analysis and use the test
set only for confirming our best results.
Models The models we implement use one-, two-
or three-dimensional parametrization and different
instantiation of values thereof. (Due to the small
size of our data set we only use the values {0, 1}
as possible instantiations.)
The v dimension is implemented using a trans-
form as in (Johnson, 1998) where v = 0 corresponds
to bare syntactic categories and v = 1 augments
node labels with the label of their parent node.
The h dimension is peculiar in that it distinguishes
PCFGs (h = ?), where RHS cannot decompose,
from their head-driven unlexicalized variety. To im-
plement h 6= ? we use a PCFG transformation em-
ulating (Collins, 2003)?s first model, in which sisters
are generated conditioned on the head tag and a sim-
ple ?distance? function (Hageloh, 2007).7 The in-
process. As the MH treebank is approaching maturity we feel
that the time is ripe to standardize its use for MH statistical
parsing. The software we implemented will be made available
for non-commercial use upon request to the author(s) and the
feature percolation software by (Krymolowski et al, 2007) is
publicly available through the Knowledge Center for Process-
ing Hebrew. By this we hope to increase the interest in MH
within the parsing community and to facilitate the application
of more sophisticated models by cutting down on setup time.
6Marked as ?NO MATCH? in the treebank.
7A formal overview of the transformation and its corre-
spondence to (Collins, 2003)?s models is available at (Hageloh,
2007). We use the distance function defined therein, marking
the direction and whether it is the first node to be generated.
stantiated value of h then selects the number of pre-
viously generated (non-head) sisters to be taken into
account when generating the next sister in a Marko-
vian process (?2 in our formal exposition).
The d dimension we proposed is implemented us-
ing a transformation that augments syntactic cate-
gories with morphological features percolated up the
tree. We use d = 0 to select bare syntactic cate-
gories and instantiate d = 1 with the definiteness
feature. The decision to select definiteness (rather
than, e.g., gender or number) is rather pragmatic as
its direction of percolation may be distinct of head
information and the question remains whether the
combination of such non-overlapping dependencies
is instrumental for parsing MH.
Our baseline model is a vanilla treebank PCFG
as described in (Charniak, 1996) which we locate
on the (?, 0, 0) point of our coordinates-system.
In a first set of experiments we implement simple
PCFG extensions of the treebank trees based on se-
lected points on the (?, v, d) plain. In a second
set of experiments we use an unlexicalized head-
driven baseline a` la (Collins, 2003) located on the
(0, 0, 0) coordinate. We transform the treebank trees
in correspondence with different points in the three-
dimensional space defined by (h, v, d). The models
we implement are marked in the coordinate-system
depicted in figure 7. The implementation details of
the transformations we use are spelled out in tables
3?4.
Procedure We implement different models that
correspond to different instantiations of h, v and d.
For each instantiation we transform the training set
and learn a PCFG using Maximum Likelihood es-
timates, and we use BitPar (Schmidt, 2004), an ef-
ficient general-purpose parser, to parse unseen sen-
tences. The input to the parser is a sequence of word
segments where each segment corresponds to a sin-
gle POS tag, possibly decorated with morphologi-
cal features. This setup assumes partial morpholog-
ical disambiguation (namely, segmentation) but cru-
cially we do not disambiguate their respective POS
categories. This setup is more appropriate for us-
ing general-purpose parsing tools and it makes our
results comparable to studies in other languages.8
8Our working assumption is that better performance of a
parsing model in our setup will improve performance also
162
Transliterate The lexical items (leaves) in the MH treebank are written left-to-write and are encoded
in utf8. A transliteration software is used to convert the utf encoding into Latin characters and to reverse
their order, essentially allowing for standard left-to-right processing.
Correct The manual annotation resulted in unavoidable errors in the annotation scheme, such as typos
(e.g., SQBQR instead of SQBAR) wrong delimiters (e.g., ?-? instead of ? ?) or wrong feature order (e.g.,
number-gender instead of gender-number). We used an automatic script to detect these error, we manually
determine their correction. Then we created an automatic script to apply all fixes (57 errors in 1% sentences).
Re-attach VB elements are attached by convention to a VP which inherits its morphological features.
9 VB instances in the treebank are mistakenly attached to an S parent without an intermediate VP level.
Our software re-attaches those VB elements to a VP parent and percolates its morphological features.
Disjoint Due to recursive processes of generating noun phrases and numerical expression (smixut)
in MH the sets of POS and syntactic categories are not disjoint. This is a major concern for PCFG parsers
that assume disjoint sets of pre- and non-terminals. The overlap between the sets also introduces additional
infinite derivations to which we loose probability mass. Our software takes care to decorate POS categories
used as non-terminal with an additional ?P?, creating a new set of categories encoding partial derivations.
Lexicalize A pre-condition for applying horizontal parameterizations a` la Collins is the annotation of
heads of syntactic phrases. The treebank provided by the knowledge center does not define unique heads,
but rather, mark multiple dependencies for some categories and none for others. Our software uses rules
for choosing the syntactic head according to specified dependencies and a head table when none are specified.
Linearize In order to implement the head-outward constituents? generation process we use software made
available to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and right
markovized derivation processes. We used two versions of Markovization, one which conditions only on the
head and a distance function, and another which conditions also on immediately neighboring sister(s).
Decorate Our software implements an additional general transform which selects the features that are to be
annotated on top of syntactic categories to implement various parametrization decisions. This transform can be
used for, e.g., displaying parent information, selecting morphological features, etc.
Table 3: Transforms over the MH Treebank: We clean and correct the treebank using Transliterate, Correct, Re-attach and
Disjoint, and transform the training set according to certain parametrization decisions using Lexicalize, Linearize and Decorate.
Smoothing pre-terminal rules is done explicitly by
collecting statistics on ?rare word? occurrences and
providing the parser with possible open class cat-
egories and their corresponding frequency counts.
The frequency threshold defining ?rare words? was
tuned empirically and set to 1. The resulting test
parses are detransformed and to skeletal constituent
structures, and are compared against the gold parses
to evaluate parsing accuracy.
Evaluation We evaluate our models using EVALB
in accordance with standard PARSEVAL evaluation
metrics. The evaluation of all models focuses on
Labeled Precision and Recall considering bare syn-
tactic categories (stripping off all morphological or
parental features and removing intermediate nodes
for linearization). We report the average F-measure
for sentences of length up to 40 and for all sentences
(F?40 and FAll respectively). We report the results
within an integrated model for morphological and syntactic dis-
ambiguation in the spirit of (Tsarfaty, 2006). We conjecture
that the kind of models developed here which takes into account
morphological information is more appropriate for the morpho-
logical disambiguation task defined therein.
for two evaluation options, once including punctua-
tion marks (WP ) and once excluding them (WOP ).
5 Results
Our baseline for the first set of experiments is
a vanilla PCFG as described in (Charniak, 1996)
(without a preceding POS tagging phase and without
right branching corrections). We transform the tree-
bank trees based on various points in the (?, v, d)
two-dimensional space to evaluate the performance
of the resulting PCFG extensions.
Table 5 reports the accuracy results for all models
on section 0 (devset) of the treebank. The accuracy
results for the vanilla PCFG are approximately 10%
lower than reported by (Charniak, 1996) for English
demonstrating that parsing MH using the currently
available treebank is a harder task. For all unlexical-
ized extensions learned from the transfromed tree-
banks, the resulting grammars show enhanced dis-
ambiguation capabilities and improved parsing ac-
curacy. We observe that the vertical dimension con-
tributes the most from both one-dimensional mod-
163
Name Params Description Transforms used
DIST h = 0 0-order Markov process Lexicalize(category), Linearize(distance)
MRK h = 1 1-order Markov process Lexicalize(category), Linearize(distance, neighbor)
PA v = 1 Parent Annotation Decorate(parent)
DEF d = 1 Definiteness feature percolation Decorate(definiteness)
Table 4: Implementing Different Parametrization Options using Transforms
Implementation (h, v, d) FALL F?40 FALL F?40
WP WP WOP WOP
PCFG (?, 0, 0) 65.17 66.63 66.17 67.7
PA (?, 0, 1) 70.6 71.96 70.96 72.18
DEF (?, 1, 0) 67.53 68.78 68.82 70.06
PA+DEF (?, 1, 1) 72.63 73.89 73.01 74.11
Table 5: PCFG Two-Dimensional Extensions: Accuracy re-
sults for parsing the devest (section 0)
els. A qualitative error analysis reveals that parent
annotation strategy distinguishes effectively various
kinds of distributions clustered together under a sin-
gle category. For example, S categories that appear
under TOP tend to be more flat than S categories ap-
pearing under SBAR (SBAR clauses typically gen-
erate a non-finite VP node under which additional
PP modifiers can be attached).
Orthogonal morphological marking provide addi-
tional information that is indicative of the kind of
dependencies that exist between a category and its
various child constituents, and we see that the d di-
mension instantiated with definiteness not only con-
tribute more than 2% to the overall parsing accuracy
of a vanilla PCFG, but also contributes as much to
the improvement obtained from a treebank already
annotated with the vertical dimension. The contribu-
tions are thus additive providing preliminary empir-
ical support to our claim that these two dimensions
provide information that is complementary.
In our next set of experiments we evaluate the
contribution of the depth dimension to extensions of
the head-driven unlexicalized variety a` la (Collins,
2003). We set our baseline at the (0, 0, 0) coordi-
nate and evaluate models that combine one, two and
three dimensions of parametrization. Table 6 shows
the accuracy results for parsing section 0 using the
resulting models.
The first outcome of these experiments is that our
new baseline improves on the accuracy results of
a simple treebank PCFG. This result indicates that
head-dependencies which play a role in determin-
ing grammatical structures in English are also in-
strumental for parsing MH. However, the marginal
contribution of the head-driven variation is surpris-
ingly low. Next we observe that for one-dimensional
models the vertical dimension still contributes the
most to parsing accuracy. However, morphologi-
cal information represented by the depth dimension
contributes more to parsing accuracy than informa-
tion concerning immediately preceding sisters on
the horizontal dimension. This outcome is consis-
tent with our observation that the grammar of MH
puts less significance on the position of constituents
relative to one others and that morphological in-
formation is more indicative of the kind of syntac-
tic relations that appear between them. For two-
dimensional models, incorporating the depth dimen-
sion (orthogonal morphological marking) is better
than not doing so, and relying solely on horizon-
tal/vertical parameters performs slightly worse than
the vertical/depth combination. The best performing
model for two-dimensional head-driven extensions
is the one combining vertical history and morpho-
logical depth. This is again consistent with the prop-
erties of MH highlighted in section 2 ? parental in-
formation gives cues about the possible expansion
on the current node, and morphological information
indicates possible interrelation between child con-
stituents that may be generated in a flexible order.
Our second set of experiments shows that a three-
dimensional annotation strategy strikes the best bal-
ance between bias and variance and achieves the best
accuracy results among all models. Different dimen-
sions provide different sorts of information which
are complementary, resulting in a model that is ca-
pable of generalizing better. The total error reduc-
tion from a plain PCFG is more than 20%, and our
best result is on a par with those achieved for other
languages (e.g., 75% for MSA).
164
Implementation Params FALL F?40 FALL F?40
(h, v, d) WP WP WOP WOP
DIST (0, 0, 0) 66.56 68.20 67.59 69.24
MRK (1, 0, 0) 66.69 68.14 67.93 69.37
PA (0, 1, 0) 68.87 70.48 69.64 70.91
DEF (0, 0, 1) 68.85 69.92 70.42 71.45
PA+MRK (1, 1, 0) 69.97 71.48 70.69 71.98
MRK+DEF (1, 0, 1) 69.46 70.79 71.05 72.37
PA+DEF (0, 1, 1) 71.15 72.34 71.98 72.91
PA+MRK+DEF (1, 1, 1) 72.34 73.63 73.27 74.41
Table 6: Head-Driven Three-Dimensional Extensions: Ac-
curacy results for parsing the devest (section 0)
Implementation Params FALL F?40 FALL F?40
(h, v, d) WP WP WOP WOP
PCFG (?, 0, 0) 65.08 67.31 65.82 68.22
PCFG+PA+DEF (?, 1, 1) 72.26 74.46 72.42 74.52
DIST (0, 0, 0) 66.33 68.79 67.06 69.47
PA+MRK+DEF (1, 1, 1) 72.64 74.64 73.21 75.25
Table 7: PCFG and Head-Driven Unlexicalized Models:
Accuracy Results for parsing the testst (section 12)
Figure 8 shows the FAll(WOP ) results for all
models we implemented. In general, we see that for
parsing MH higher dimensionality is better. More-
over, we see that for all points on the (v, h, 0) plain
the corresponding models on the (v, h, 1) plain al-
ways perform better. We further see that the contri-
bution of the depth dimension to a parent annotated
PCFG can compensate, to a large extent on the lack
of head-dependency information. These accumula-
tive results, then, provide empirical evidence to the
importance of morphological and morpho-syntactic
processes such as definiteness for syntactic analysis
and disambiguation as argued for in section 2.
We confirm our results on the testset and report
in table 7 our results on section 12 of the treebank.
The performance has slightly increased and we ob-
tain better results for our best strategy. We retain the
high error-reduction rate and propose our best result,
75.25% for sentences of length ? 40, as an empiri-
cally established string baseline on the performance
of treebank grammars for MH.
6 Related Work
The MH treebank (Sima?an et al, 2001), a mor-
phologically and syntactically annotated corpus, has
been successfully used for various NLP tasks such as
morphological disambiguation, POS tagging (Bar-
Haim et al, 2007) and NP chunking (Goldberg et
al., 2006). However its use for statistical parsing has
been more scarce and less successful. The only pre-
vious studies attempting to parse MH we know of
are (Sima?an et al, 2001), applying a variation of the
DOP tree-gram model to 500 sentences, and (Tsar-
faty, 2006), using a treebank PCFG in an integrated
system for morphological and syntactic disambigua-
tion.9 The adaptation of state-of-the-art parsing
models to MH is not immediate as the flat variable
structures of phrases are hard to parse and a plen-
tiful of morphological features that would facilitate
disambiguation are not exploited by currently avail-
able parsers. Also, the MH treebank is much smaller
than the ones for, e.g., English (Marcus et al, 1994)
and Arabic (Maamouri and Bies, 2004), making it
hard to apply data-intensive methods such as the all-
subtrees approach (Bod, 1992) or full lexicalization
(Collins, 2003). Our best performing model incor-
porates three dimensions of parametrization and our
best result (75.25%) is similar to the one obtained
by the parser of (Bikel, 2004) for Modern Standard
Arabic (75%) using a fully lexicalized model and
a training corpus about three times as large as our
newest MH treebank.
This work has shown that devising an adequate
baseline for parsing MH requires more than sim-
ple category-splits and sophisticated head-driven ex-
tensions, and our results provide preliminary evi-
dence for the variation in performance of different
parametrization strategies relative to the properties
and structure of a given language. The compari-
son with parsing accuracy for MSA suggests that
parametrizing an orthogonal depth dimension may
be able to compensate, to some extent, on the lack
of sister-dependencies, lexical information, and per-
haps even the lack of annotated data, but establish-
ing empirically its contribution to parsing MSA is a
matter for further research. In the future we intend
to further investigate the significance of the depth di-
mension by extending our models to include more
morphological features, more variation in the pa-
9Both studies acheived between 60%?70% accuracy, how-
ever the results are not comparable to our study because of the
use of different training sets, different annotation conventions,
and different evaluation schemes.
165
Figure 7: All Models: Locating Unlexicalized Parsing Models
in a Three-Dimensional Parametrization Space
Figure 8: All Results: Parsing Results for Unlexicalized Mod-
els in a Three-Dimensional Parametrization Space
rameter space, and applications to more languages.
7 Conclusion
Morphologically rich languages introduce a new di-
mension into the expansion possibilities of a non-
terminal node in a syntactic parse tree. This di-
mension is orthogonal to the vertical (Collins, 2003)
and horizontal (Johnson, 1998) dimensions previ-
ously outlined by Klein and Manning (2003), and
it cannot be collapsed into any one of the previous
two. These additional dependencies exist alongside
the syntactic head dependency and are attested using
morphosyntactic phenomena such as long distance
agreement. We demonstrate using syntactic defi-
niteness in MH that incorporating morphologically
marked features as a third, orthogonal dimension
for annotating syntactic categories is invaluable for
weakening the independence assumptions implicit
in a treebank PCFG and increasing the model?s dis-
ambiguation capabilities. Using a three-dimensional
model we establish a new, stronger, lower bound on
the performance of unlexicalized parsing models for
Modern Hebrew, comparable to those achieved for
other languages (Czech, Chinese, German and Ara-
bic) with much larger corpora.
Tuning the dimensions and value of the parame-
ters for learning treebank grammars is largely an em-
pirical matter, and we do not wish to claim here that
a three-dimensional annotation strategy is the best
for any given language. Rather, we argue that for
different languages different optimal parametriza-
tion strategies may apply. MH is not a free-word-
order language in the canonical sense, and our qual-
itative analysis shows that all dimensions contribute
to the models? disambiguation capabilities. Orthog-
onal dimensions provide complementary informa-
tion that is invaluable for the parsing process to the
extent that the relevant linguistic phenomena license
grammatical structures in the language. Our results
point out a principled way to quantitatively charac-
terizing differences between languages, thus guid-
ing the selection of parameters for the development
of annotated resources, custom parsers and cross-
linguistic robust parsing engines.
Acknowledgments We thank the Knowledge
Center for Processing Hebrew and Dalia Bojan for
providing us with the newest version of the MH
treebank. We are particularly grateful to the devel-
opment team of version 2.0, Adi Mile?a and Yuval
Krymolowsky, supervised by Yoad Winter for con-
tinued collaboration and technical support. We fur-
ther thank Felix Hageloh for allowing us to use the
software resulting from his M.Sc. thesis work. We
also like to thank Remko Scha, Jelle Zuidema, Yoav
Seginer and three anonymous reviewers for helpful
comments on the text, and Noa Tsarfaty for techni-
cal help in the graphical display. The work of the
first author is funded by the Netherlands Organiza-
tion for Scientific Research (NWO), grant number
017.001.271, for which we are grateful.
166
References
S. Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23 (4):597?618.
R. Bar-Haim, K. Sima?an, and Y. Winter. 2007. Part-of-
Speech Tagging of Modern Hebrew Text. Journal of
Natural Language Engineering.
D. Bikel and D. Chiang. 2000. Two Statistical Parsing
Models Applied to the Chinese Treebank. In Second
Chinese Language Processing Workshop, Hong Kong.
D. Bikel. 2004. Intricacies of Collins? Parsing Model.
Computational Linguistics, 4(30).
R. Bod. 1992. Data Oriented Parsing. In Proceedings of
COLING.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI,
pages 598?603.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings of ACL,
College Park, Maryland.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4).
G. Danon. 2001. Syntactic Definiteness in the Grammar
of Modern Hebrew. Linguistics, 6(39):1071?1116.
A. Dubey and F. Keller. 2003. Probabilistic Parsing for
German using Sister-Head Dependencies. In Proceed-
ings of ACL.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
Phrase Chunking in Hebrew: Influence of Lexical and
Morphological Features. In Proceedings of COLING-
ACL.
F. Hageloh. 2007. Parsing using Transforms over Tree-
banks. Master?s thesis, University of Amsterdam.
M. Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632.
R. Kaplan and J. Bresnan. 1982. Lexical-Functional
Grammar: A formal system for grammatical represen-
tation. In J. Bresnan, editor, The Mental Representa-
tion of Grammatical Relations, Cambridge, MA. The
MIT Press.
D. Klein and C. Manning. 2003. Accurate Unlexicalized
Parsing. In Proceedings of ACL, pages 423?430.
Y. Krymolowski, Y. Adiel, N. Guthmann, S. Kenan,
A. Milea, N. Nativ, R. Tenzman, and P. Veisberg.
2007. Treebank Annotation Guide. MILA, Knowl-
edge Center for Hebrew Processing.
M. Maamouri and A. Bies. 2004. Developing an Ara-
bic Treebank: Methods, Guidelines, Procedures, and
Tools. In Proceedings of COLING.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating Predicate-
Argument Structure.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL?05.
N. Melnik. 2002. Verb-Initial Constructions in Modern
Hebrew. Ph.D. thesis, Berkeley University of Califor-
nia.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL-COLING, pages
433?440, Sydney, Australia, July.
D. Prescher. 2005. Head-Driven PCFGs with Latent-
Head Statistics. In In Proceedings of the International
Workshop on Parsing Technologies.
I. A. Sag, T. Wasow, and E. M. Bender. 2003. Syntactic
Theory: A Formal Introduction. CSLI Publications,
address, second edition.
H. Schmidt. 2004. Efficient Parsing of Highly Ambigu-
ous Context-Free Grammars with Bit Vectors. In Pro-
ceedings of COLING, Geneva, Switzerland.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a Tree-Bank of Modern Hebrew Text.
In Traitment Automatique des Langues.
R. Tsarfaty. 2006. Integrated Morphological and Syntac-
tic Disambiguation for Modern Hebrew. In Proceed-
ing of SRW COLING-ACL.
S. Wintner. 2000. Definiteness in the Hebrew Noun
Phrase. Journal of Linguistics, 36:319?363.
167
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 385?396,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Evaluating Dependency Parsing:
Robust and Heuristics-Free Cross-Annotation Evaluation
Reut Tsarfaty
Uppsala University
Sweden
Joakim Nivre
Uppsala University
Sweden
Evelina Andersson
Uppsala University
Sweden
Abstract
Methods for evaluating dependency parsing
using attachment scores are highly sensitive
to representational variation between depen-
dency treebanks, making cross-experimental
evaluation opaque. This paper develops a ro-
bust procedure for cross-experimental eval-
uation, based on deterministic unification-
based operations for harmonizing different
representations and a refined notion of tree
edit distance for evaluating parse hypothe-
ses relative to multiple gold standards. We
demonstrate that, for different conversions of
the Penn Treebank into dependencies, perfor-
mance trends that are observed for parsing
results in isolation change or dissolve com-
pletely when parse hypotheses are normalized
and brought into the same common ground.
1 Introduction
Data-driven dependency parsing has seen a consid-
erable surge of interest in recent years. Dependency
parsers have been tested on parsing sentences in En-
glish (Yamada and Matsumoto, 2003; Nivre and
Scholz, 2004; McDonald et al, 2005) as well as
many other languages (Nivre et al, 2007a). The
evaluation metric traditionally associated with de-
pendency parsing is based on scoring labeled or
unlabeled attachment decisions, whereby each cor-
rectly identified pair of head-dependent words is
counted towards the success of the parser (Buchholz
and Marsi, 2006). As it turns out, however, such
evaluation procedures are sensitive to the annotation
choices in the data on which the parser was trained.
Different annotation schemes often make differ-
ent assumptions with respect to how linguistic con-
tent is represented in a treebank (Rambow, 2010).
The consequence of such annotation discrepancies is
that when we compare parsing results across differ-
ent experiments, even ones that use the same parser
and the same set of sentences, the gap between re-
sults in different experiments may not reflect a true
gap in performance, but rather a difference in the an-
notation decisions made in the respective treebanks.
Different methods have been proposed for making
dependency parsing results comparable across ex-
periments. These methods include picking a single
gold standard for all experiments to which the parser
output should be converted (Carroll et al, 1998; Cer
et al, 2010), evaluating parsers by comparing their
performance in an embedding task (Miyao et al,
2008; Buyko and Hahn, 2010), or neutralizing the
arc direction in the native representation of depen-
dency trees (Schwartz et al, 2011).
Each of these methods has its own drawbacks.
Picking a single gold standard skews the results in
favor of parsers which were trained on it. Trans-
forming dependency trees to a set of pre-defined la-
beled dependencies, or into task-based features, re-
quires the use of heuristic rules that run the risk of
distorting correct information and introducing noise
of their own. Neutralizing the direction of arcs is
limited to unlabeled evaluation and local context,
and thus may not cover all possible discrepancies.
This paper proposes a new three-step protocol for
cross-experiment parser evaluation, and in particu-
lar for comparing parsing results across data sets
that adhere to different annotation schemes. In the
385
first step all structures are brought into a single for-
mal space of events that neutralizes representation
peculiarities (for instance, arc directionality). The
second step formally computes, for each sentence
in the data, the common denominator of the differ-
ent gold standards, containing all and only linguistic
content that is shared between the different schemes.
The last step computes the normalized distance from
this common denominator to parse hypotheses, mi-
nus the cost of distances that reflect mere annotation
idiosyncrasies. The procedure that implements this
protocol is fully deterministic and heuristics-free.
We use the proposed procedure to compare de-
pendency parsing results trained on Penn Treebank
trees converted into dependency trees according to
five different sets of linguistic assumptions. We
show that when starting off with the same set of
sentences and the same parser, training on differ-
ent conversion schemes yields apparently significant
performance gaps. When results across schemes are
normalized and compared against the shared linguis-
tic content, these performance gaps decrease or dis-
solve completely. This effect is robust across parsing
algorithms. We conclude that it is imperative that
cross-experiment parse evaluation be a well thought-
through endeavor, and suggest ways to extend the
protocol to additional evaluation scenarios.
2 The Challenge: Treebank Theories
Dependency treebanks contain information about
the grammatically meaningful elements in the utter-
ance and the grammatical relations between them.
Even if the formal representation in a dependency
treebank is well-defined according to current stan-
dards (Ku?bler et al, 2009), there are different ways
in which the trees can be used to express syntactic
content (Rambow, 2010). Consider, for instance, al-
gorithms for converting the phrase-structure trees in
the Penn Treebank (Marcus et al, 1993) into depen-
dency structures. Different conversion algorithms
implicitly make different assumptions about how to
represent linguistic content in the data. When mul-
tiple conversion algorithms are applied to the same
data, we end up with different dependency trees for
the same sentences (Johansson and Nugues, 2007;
Choi and Palmer, 2010; de Marneffe et al, 2006).
Some common cases of discrepancies are as follows.
Lexical vs. Functional Head Choice. In linguis-
tics, there is a distinction between lexical heads and
functional heads. A lexical head carries the seman-
tic gist of a phrase while a functional one marks its
relation to other parts of the sentence. The two kinds
of heads may or may not coincide in a single word
form (Zwicky, 1993). Common examples refer to
prepositional phrases, such as the phrase ?on Sun-
day?. This phrase has two possible analyses, one se-
lects a lexical head (1a) and the other selects a func-
tional one (1b), as depicted below.
(1a) Sunday
on
(1b) on
Sunday
Similar choices are found in phrases which contain
functional elements such as determiners, coordina-
tion markers, subordinating elements, and so on.
Multi-Headed Constructions. Some phrases are
considered to have multiple lexical heads, for in-
stance, coordinated structures. Since dependency-
based formalisms require us to represent all con-
tent as binary relations, there are different ways we
could represent such constructions. Let us consider
the coordination of nominals below. We can choose
between a functional head (1a) and a lexical head
(2b, 2c). We can further choose between a flat rep-
resentation in which the first conjunct is a single
head (2b), or a nested structure where each con-
junct/marker is the head of the following element
(2c). All three alternatives empirically exist. Exam-
ple (2a) reflects the structures in the CoNLL 2007
shared task data (Nivre et al, 2007a). Johansson
and Nugues (2007) use structures like (2b). Exam-
ple (2c) reflects the analysis of Mel?c?uk (1988).
(2a) and
earth wind fire
(2b) earth
wind and fire
(2c) earth
wind
and
fire
Periphrastic Marking. When a phrase includes
periphrastic marking ? such as the tense and modal
marking in the phrase ?would have worked? below
? there are different ways to consider its division
into phrases. One way to analyze this phrase would
be to choose auxiliaries as heads, as in (3a). Another
alternative would be to choose the final verb as the
prep pobj
con
j con
j
conj cc
coord
conj
coordconj
conj
386
Experiment Gold Parse
#1 arrive
on
Sunday
arrive
on
Sunday
#2 arrive
Sunday
on
arrive
Sunday
on
Gold: #1 # 2
Parse
#1 1.0 0.0
#2 0.0 1.0
Figure 1: Calculating cross-experiment LAS results
main head, and let the auxiliaries create a verb chain
with different levels of projection. Each annotation
decision dictates a different direction of the arcs and
imposes its own internal division into phrases.
(3a) would
have
worked
(3b) worked
have
would
In standard settings, an experiment that uses
a data set which adheres to a certain annotation
scheme reports results that are compared against the
annotation standard that the parser was trained on.
But if parsers were trained on different annotation
standards, the empirical results are not comparable
across experiments. Consider, for instance, the ex-
ample in Figure 1. If parse1 and parse2 are com-
pared against gold2 using labeled attachment scores
(LAS), then parse1 results are lower than the results
of parse2, even though both parsers produced lin-
guistically correct and perfectly useful output.
Existing methods for making parsing results com-
parable across experiments include heuristics for
converting outputs into dependency trees of a prede-
fined standard (Briscoe et al, 2002; Cer et al, 2010)
or evaluating the performance of a parser within an
embedding task (Miyao et al, 2008; Buyko and
Hahn, 2010). However, heuristic rules for cross-
annotation conversion are typically hand written and
error prone, and may not cover all possible discrep-
ancies. Task-based evaluation may be sensitive to
the particular implementation of the embedding task
and the procedures that extract specific task-related
features from the different parses. Beyond that,
conversion heuristics and task-based procedures are
currently developed almost exclusively for English.
Other languages typically lack such resources.
A recent study by Schwartz et al (2011) takes
a different approach towards cross-annotation eval-
uation. They consider different directions of
head-dependent relations (such as on?Sunday
and Sunday?on) and different parent-child and
grandparent-child relations in a chain (such as
arrive?on and arrive?sunday in ?arrive on sun-
day?) as equivalent. They then score arcs that fall
within corresponding equivalence sets. Using these
new scores Schwartz et al (2011) neutralize certain
annotation discrepancies that distort parse compar-
ison. However, their treatment is limited to local
context and does not treat structures larger than two
sequential arcs. Additionally, since arcs in differ-
ent directions are typically labeled differently, this
method only applies for unlabeled dependencies.
What we need is a fully deterministic and for-
mally precise procedure for comparing any set of la-
beled or unlabeled dependency trees, by consolidat-
ing the shared linguistic content of the complete de-
pendency trees in different annotation schemes, and
comparing parse hypotheses through sound metrics
that can take into account multiple gold standards.
3 The Proposal: Cross-Annotation
Evaluation in Three Simple Steps
We propose a new protocol for cross-experiment
parse evaluation, consisting of three fundamental
components: (i) abstracting away from annotation
peculiarities, (ii) generalizing theory-specific struc-
tures into a single linguistically coherent gold stan-
dard that contains all and only consistent informa-
tion from all sources, and (iii) defining a sound met-
ric that takes into account the different gold stan-
dards that are being considered in the experiments.
In this section we first define functional trees as
the common space of formal objects and define a de-
terministic conversion procedure from dependency
trees to functional trees. Next we define a set of for-
mal operations on functional trees that compute, for
every pair of corresponding trees of the same yield, a
single gold tree that resolves inconsistencies among
gold standard alternatives and combines the infor-
mation that they share. Finally, we define scores
based on tree edit distance, refined to consider the
distance from parses to the overall gold tree as well
as the different annotation alternatives.
vg vg
vgvg
tmod
pobj pobj
prepprep
tmod
tmod
tmod
387
Preliminaries. Let T be a finite set of terminal
symbols and let L be a set of grammatical relation
labels. A dependency graph d is a directed graph
which consists of nodes Vd and arcs Ad ? Vd ? Vd.
We assume that all nodes in Vd are labeled by ter-
minal symbols via a function labelV : Vd ? T . A
well-formed dependency graph d = (Vd, Ad) for a
sentence S = t1, t2, ..., tn is any dependency graph
that is a directed tree originating out of a node v0
labeled t0 = ROOT , and spans all terminals in
the sentence, that is, for every ti ? S there exists
vj ? Vd labeled labelV (vj) = ti. For simplicity we
assume that every node vj is indexed according to
the position of the terminal label, i.e., that for each
ti labeling vj , i always equals j. In a labeled de-
pendency tree, arcs in Ad are labeled by elements
of L via a function labelA : Ad ? L that encodes
the grammatical relation between the terminals la-
beling the connected nodes. We define two auxiliary
functions on nodes in dependency trees. The func-
tion subtree : Vd ? P(Vd) assigns to every node
v ? Vd the set of nodes accessible by it through
the reflexive transitive closure of the arc relation Ad.
The function span : Vd ? P(T ) assigns to every
node v ? Vd a set of terminals such that span(v) =
{t ? T |t = labelV (u) and u ? subtree(v)}.1
Step 1: Functional Representation Our first goal
is to define a representation format that keeps all
functional relationships that are represented in the
dependency trees intact, but remains neutral with
respect to the directionality of the head-dependent
relations. To do so we define functional trees
? linearly-ordered labeled trees which, instead of
head-to-head binary relations, represent the com-
plete functional structure of a sentence. Assuming
the same sets of terminal symbols T and grammat-
ical relation labels L, and assuming extended sets
of nodes V and arcs A ? V ? V , a functional tree
pi = (V,A) is a directed tree originating from a sin-
gle root v0 ? V where all non-terminal nodes in
pi are labeled with grammatical relation labels that
signify the grammatical function of the chunk they
dominate inside the tree via labelNT : V ? L. All
1If a dependency tree d is projective, than for all v ? Vd the
terminals in span(v) form a contiguous segment of S. The cur-
rent discussion assumes that all trees are projective. We com-
ment on non-projective dependencies in Section 4.
terminal nodes in pi are labeled with terminal sym-
bols via a labelT : V ? T function. The function
span : V ? P(V ) now picks out the set of ter-
minal labels of the terminal nodes accessible by a
node v ? V via A. We obtain functional trees from
dependency trees using the following procedure:
? Initialize the set of nodes and arcs in the tree.
V := Vd
A := Ad
? Label each node v ? V with the label of its
incoming arc.
labelNT (v) = labelA(u, v)
? In case |span(v)| > 1 add a new node u as a
daughter designating the lexical head, labeled
with the wildcard symbol *:
V := V ? {u}
A := A ? {(v, u)}
labelNT (u) = ?
? For each node v such that |span(v)| = 1, add a
new node u as a daughter, labeled with its own
terminal:
V := V ? {u}
A := A ? {(v, u)}
if (labelNT (v) = ?)
labelT (u) := labelV (v)
else
labelT (u) := labelV (parent(v))
That is to say, we label all nodes with spans
greater than 1 with the grammatical function of their
head, and for each node we add a new daughter u
designating the head word, labeled with its gram-
matical function. Wildcard labels are compatible
with any, more specific, grammatical function of the
word inside the phrase. This gives us a constituency-
like representation of dependency trees labeled with
functional information, which retains the linguis-
tic assumptions reflected in the dependency trees.
When applying this procedure, examples (1)?(3) get
transformed into (4)?(6) respectively.
(4a) ...
prep
on
*
Sunday
(4b) ...
*
on
pobj
Sunday
388
(5a) ...
conj
earth
conj
wind
*
and
conj
fire
(5b) ...
*
earth
conj
wind
cc
and
conj
fire
(5c) ...
*
earth
coord
*
wind
coord
*
and
conj
fire
(6a) ...
*
would
vg
*
have
vg
worked
(6b) ...
vg
vg
would
*
have
*
worked
Considering the functional trees resulting from
our procedure, it is easy to see that for tree pairs
(4a)?(4b) and (5a)?(5b) the respective functional
trees are identical modulo wildcards, while tree pairs
(5b)?(5c) and (6a)?(6b) end up with different tree
structures that realize different assumptions con-
cerning the internal structure of the tree. In order
to compare, combine or detect inconsistencies in the
information inherent in different functional trees, we
define a set of formal operations that are inspired by
familiar notions from unification-based formalisms
(Shieber (1986) and references therein).
Step 2: Formal Operations on Trees The intu-
ition behind the formal operations we define is sim-
ple. A completely flat tree over a span is the most
general structural description that can be given to it.
The more nodes dominate a span, the more linguis-
tic assumptions are made with respect to its struc-
ture. If an arc structure in one tree merely elaborates
an existing flat span in another tree, the theories un-
derlying the schemes are compatible, and their in-
formation can be combined. Otherwise, there exists
a conflict in the linguistic assumptions, and we need
to relax some of the assumptions, i.e., remove func-
tional nodes, in order to obtain a coherent structure
that contains the information on which they agree.
Let pi1, pi2 be functional trees over the same yield
t1, .., tn. Let the function span(v) pick out the ter-
minals labeling terminal nodes that are accessible
via a node v ? V in the functional tree through the
relation A. We define first the tree subsumption re-
lation for comparing the amount of information in-
herent in the arc-structure of two trees.2
T-Subsumption, denoted t, is a relation be-
tween trees which indicates that a tree pi1 is
consistent with and more general than tree
pi2. Formally: pi1 t pi2 iff for every node
n ? pi1 there exists a node m ? pi2 such
that span(n) = span(m) and label(n) =
label(m).
Looking at the functional trees of (4a)?(4b) we
see that their unlabeled skeletons mutually subsume
each other. In their labeled versions, however, each
tree contains labeling information that is lacking in
the other. In the functional trees (5b)?(5c) a flat
structure over a span in (5b) is more elaborated in
(5c). In order to combine information in trees with
compatible arc structures, we define tree unification.
T-Unification, denoted unionsqt, is the operation that
returns the most general tree structure pi3 that
is subsumed by both pi1, pi2 if such exists, and
fails otherwise. Formally: pi1 unionsqt pi2 = pi3 iff
pi1 t pi3 and pi2 t pi3, and for all pi4 such that
pi1 t pi4 and pi2 t pi4 it holds that pi3 t pi4.
Tree unification collects the information from two
trees into a single result if they are consistent, and
detects an inconsistency otherwise. In case of an
inconsistency, as is the case in the functional trees
(6a) and (6b), we cannot unify the structures due
to a conflict concerning the internal division of an
expression into phrases. However, we still want to
generalize these two trees into one tree that contains
all and only the information that they share. For that
we define the tree generalization operation.
T-Generalization, denoted t, is the operation
that returns the most specific tree that is more
general than both trees. Formally, pi1 t pi2 =
pi3 iff pi3 t pi1 and pi3 t pi2, and for every pi4
such that pi4 t pi1 and pi4 t pi2 it holds that
pi4 t pi3.
2Note that the wildcard symbol * is equal to any other sym-
bol. In case the node labels consist of complex feature structures
made of attribute-value lists, we replace label(n) = label(m)
in the subsumption definition with label(n)  label(m) in the
sense of (Shieber, 1986).
389
Unlike unification, generalization can never fail.
For every pair of trees there exists a tree that is more
general than both: in the extreme case, pick the com-
pletely flat structure over the yield, which is more
general than any other structure. For (6a)?(6b), for
instance, we get that (6a)t(6b) is a flat tree over
pre-terminals where ?would? and ?have? are labeled
with ?vg? and ?worked? is the head, labeled with ?*?.
The generalization of two functional trees pro-
vides us with one structure that reflects the common
and consistent content of the two trees. These struc-
tures thus provide us with a formally well-defined
gold standard for cross-treebank evaluation.
Step 3: Measuring Distances. Our functional
trees superficially look like constituency-based
trees, so a simple proposal would be to use Parse-
val measures (Black et al, 1991) for comparing the
parsed trees against the new generalized gold trees.
Parseval scores, however, have two significant draw-
backs. First, they are known to be too restrictive
with respect to some errors and too permissive with
respect to others (Carroll et al, 1998; Ku?bler and
Telljohann, 2002; Roark, 2002; Rehbein and van
Genabith, 2007). Secondly, F1 scores would still
penalize structures that are correct with respect to
the original gold, but are not there in the generalized
structure. Here we propose to adopt measures that
are based on tree edit distance (TED) instead. TED-
based measures are, in fact, an extension of attach-
ment scores for dependency trees. Consider, for in-
stance, the following operations on dependency arcs.
reattach-arc remove arc (u, v) ? Ad and add
an arc Ad ? {(w, v)}.
relabel-arc relabel arc l1(u, v) as l2(u, v)
Assuming that each operation is assigned a cost,
the attachment score of comparing two dependency
trees is simply the cost of all edit operations that are
required to turn a parse tree into its gold standard,
normalized with respect to the overall size of the de-
pendency tree and subtracted from a unity.3 Here
we apply the idea of defining scores by TED costs
normalized relative to the size of the tree and sub-
stracted from a unity, and extend it from fixed-size
dependency trees to ordered trees of arbitrary size.
3The size of a dependency tree, either parse or gold, is al-
ways fixed by the number of terminals.
Our formalization follows closely the formulation
of the T-Dice measure of Emms (2008), building on
his thorough investigation of the formal and empir-
ical differences between TED-based measures and
Parseval. We first define for any ordered and labeled
tree pi the following operations.
relabel-node change the label of node v in pi
delete-node delete a non-root node v in pi with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.
insert-node insert a node v as a child of u in
pi making it the parent of a consecutive subse-
quence of the children of u.
An edit script ES(pi1, pi2) = {e0, e1....ek} between
pi1 and pi2 is a set of edit operations required for turn-
ing pi1 into pi2. Now, assume that we are given a cost
function defined for each edit operation. The cost of
ES(pi1, pi2) is the sum of the costs of the operations
in the script. An optimal edit script is an edit script
between pi1 and pi2 of minimum cost.
ES?(pi1, pi2) = argminES(pi1,pi2)
?
e?ES(pi1,pi2)
cost(e)
The tree edit distance problem is defined to be the
problem of finding the optimal edit script and com-
puting the corresponding distance (Bille, 2005).
A simple way to calculate the error ? of a parse
would be to define it as the edit distance between
the parse hypothesis pi1 and the gold standard pi2.
?(pi1, pi2) = cost(ES?(pi1, pi2))
However, in such cases the parser may still get pe-
nalized for recovering nodes that are lacking in the
generalization. To solve this, we refine the distance
between a parse tree and the generalized gold tree
to discard edit operations on nodes that are there in
the native gold tree but are eliminated through gen-
eralization. We compute the intersection of the edit
script turning the parse tree into the generalize gold
with the edit script turning the native gold tree into
the generalized gold, and discard its cost. That is, if
parse1 and parse2 are compared against gold1 and
gold2 respectively, and if we set gold3 to be the re-
sult of gold1tgold2, then ?new is defined as:
390
Figure 2: The evaluation pipeline. Different versions of the treebank go into different experiments, resulting in
different parse and gold files. All trees are transformed into functional trees. All gold files enter generalization to
yield a new gold. The different ? arcs represent the different tree distances used for calculating the TED-based scores.
?new(parse1, gold1,gold3) =
?(parse1,gold3)
?cost(ES?(parse1,gold3)?ES?(gold1,gold3))
Now, if gold1 and gold3 are identi-
cal, then ES?(gold1,gold3)=? and we fall
back on the simple tree edit distance score
?new(parse1,gold1,gold3)=?(parse1, gold3).
When parse1 and gold1 are identical,
i.e., the parser produced perfect out-
put with respect to its own scheme, then
?new(parse1,gold1,gold3)=?new(gold1,gold1,gold3)
=?(gold1,gold3)? cost(ES?(gold1,gold3))=0, and
the parser does not get penalized for recovering a
correct structure in gold1 that is lacking in gold3.
In order to turn distances into accuracy measures
we have to normalize distances relative to the maxi-
mal number of operations that is conceivable. In the
worst case, we would have to remove all the internal
nodes in the parse tree and add all the internal nodes
of the generalized gold, so our normalization factor
? is defined as follows, where |pi| is the size4 of pi.
?(parse1,gold3) = |parse1| + |gold3|
We now define the score of parse1 as follows:5
1? ?new(parse1,gold1,gold3)?(parse1,gold3)
Figure 2 summarizes the steps in the evalu-
ation procedure we defined so far. We start
off with two versions of the treebank, TB1 and
TB2, which are parsed separately and provide their
own gold standards and parse hypotheses in a la-
beled dependencies format. All dependency trees
4Following common practice, we equate size |pi| with the
number of nodes in pi, discarding the terminals and root node.
5If the trees have only root and leaves, ? = 0, score := 1.
are then converted into functional trees, and we
compute the generalization of each pair of gold
trees for each sentence in the data. This pro-
vides the generalized gold standard for all exper-
iments, here marked as gold3.6 We finally com-
pute the distances ?new(parse1,gold1,gold3) and
?new(parse2,gold2,gold3) using the different tree
edit distances that are now available, and we repeat
the procedure for each sentence in the test set.
To normalize the scores for an entire test set of
size n we can take the arithmetic mean of the scores.
?|test-set|
i=1 score(parse1i,gold1i,gold3i)
|test-set|
Alternatively we can globally average of all edit dis-
tance costs, normalized by the maximally possible
edits on parse trees turned into generalized trees.
1?
?|test-set|
i=1 ?new(parse1i,gold1i,gold3i)?|test-set|
i=1 ?(parse1i,gold3i)
The latter score, global averaging over the entire test
set, is the metric we use in our evaluation procedure.
4 Experiments
We demonstrate the application of our procedure to
comparing dependency parsing results on different
versions of the Penn Treebank (Marcus et al, 1993).
The Data We use data from the PTB, converted
into dependency structures using the LTH soft-
ware, a general purpose tool for constituency-to-
dependency conversion (Johansson and Nugues,
2007). We use LTH to implement the five different
annotation standards detailed in Table 3.
6Generalization is an associative and commutative opera-
tion, so it can be extended for n experiments in any order.
TB1 parse1.dep
gold1.dep
parse2.dep
gold2.dep
parse1
gold3
gold1
parse2
gold2TB2
parse
parse
? (parse1,gold3)
? (gold1,gold3)
? (pars
e2,gold
3)
? (gold2,gold
3)
parse transform generalizeparse
391
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9142 0.6077 0.7772
LAS 0.8820 0.4801 0.6454
U-TED 0.9488 0.8926 0.9237
L-TED 0.9241 0.7811 0.8441
Old LTH UAS 0.6053 0.8955 0.6508
LAS 0.4816 0.8644 0.5771
U-TED 0.8931 0.9564 0.9092
L-TED 0.7811 0.9317 0.8197
CoNLL07 UAS 0.7734 0.6474 0.8917
LAS 0.6479 0.5722 0.8736
U-TED 0.9260 0.9097 0.9474
L-TED 0.8480 0.8204 0.9233
Default-OldLTH U-TED 0.9500 0.9543
L-TED 0.9278 0.9324
Default-CoNLL07 U-TED 0.9444? 0.9453?
L-TED 0.9266? 0.9260?
oldLTH-CoNLL07 U-TED 0.9519 0.9490
L-TED 0.9323 0.9283
default-oldLTH-CoNLL U-TED 0.9464? 0.9515 0.9471?
L-TED 0.9281? 0.9336 0.9280?
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8917 0.8054 0.6986
LAS 0.8736 0.7895 0.6831
U-TED 0.9474 0.9357 0.9237
L-TED 0.9233 0.8960 0.8606
Functional UAS 0.8040 0.8970 0.6110
LAS 0.7873 0.8793 0.5977
U-TED 0.9347 0.9466 0.9107
L-TED 0.8948 0.9239 0.8316
Lexical UAS 0.7013 0.6138 0.8823
LAS 0.6875 0.6022 0.8635
U-TED 0.9252 0.9132 0.9500
L-TED 0.8623 0.8345 0.9266
CoNLL07-Functional U-TED 0.9473? 0.9473?
L-TED 0.9233 0.9247
CoNLL07-Lexical U-TED 0.9490? 0.9500?
L-TED 0.9253? 0.9266?
Functional-Lexical U-TED 0.9489? 0.9501?
L-TED 0.9266? 0.9267?
CoNLL07-Functional-Lexical U-TED 0.9489? 0.9489? 0.9501?
L-TED 0.9254? 0.9266? 0.9267?
Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan-
dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported
in the same row. The ? sign marks pairwise results where the difference is not statistically significant.
Train Default Old LTH CoNLL07
Gold
Default UAS 0.9173 0.6085 0.7709
LAS 0.8833 0.4780 0.6414
U-TED 0.9513 0.8903 0.9236
L-TED 0.9249 0.7727 0.8424
Old LTH UAS 0.6078 0.8952 0.6415
LAS 0.4809 0.8471 0.5669
U-TED 0.8960 0.9550 0.9096
L-TED 0.7823 0.9224 0.8170
CoNLL07 UAS 0.7767 0.6517 0.8991
LAS 0.6504 0.5725 0.8709
U-TED 0.9289 0.9087 0.9479
L-TED 0.8502 0.8159 0.9208
Default-oldLTH U-TED 0.9533 0.9515
L-TED 0.9289 0.9224
Default-CoNLL U-TED 0.9474? 0.9460?
L-TED 0.9281 0.9238
OldLTH-CoNLL U-TED 0.9479 0.9493
L-TED 0.9234 0.9258
Default-OldLTH-CoNLL U-TED 0.9492? 0.9461 0.9480?
L-TED 0.9298 0.9241? 0.9258?
Train CoNLL07 Functional Lexical
Gold
CoNLL07 UAS 0.8991 0.8077 0.7018
LAS 0.8709 0.7902 0.6804
U-TED 0.9479 0.9373 0.9221
L-TED 0.9208 0.8955 0.8505
Functional UAS 0.8083 0.8978 0.6150
LAS 0.7895 0.8782 0.5975
U-TED 0.9356 0.9476 0.9092
L-TED 0.8929 0.9226 0.8218
Lexical UAS 0.6997 0.6161 0.8826
LAS 0.6835 0.6034 0.8491
U-TED 0.9259 0.9152 0.9483
L-TED 0.8593 0.8340 0.9160
CoNLL-Functional U-TED 0.9479? 0.9487?
L-TED 0.9209 0.9237
CoNLL-Lexical U-TED 0.9497 0.9483
L-TED 0.9228 0.9161
Functional-Lexical U-TED 0.9504 0.9483
L-TED 0.9258 0.9161
CoNLL-Functional-Lexical U-TED 0.9498 0.9504? 0.9483?
L-TED 0.9229 0.9258 0.9161
Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We
report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results
reported in the same row. The ? sign marks pairwise results where the difference is not statistically significant.
ID Description
Default The LTH conversion default settings
OldLTH The conversion used in Johansson and Nugues (2007)
CoNLL07 The conversion used in the CoNLL shared task (Nivre et al, 2007a)
Lexical Same as CoNLL, but selecting only lexical heads when a choice exists
Functional Same as CoNLL, but selecting only functional heads when a choice exists
Table 3: LTH conversion schemes used in the experiments. The LTH conversion settings in terms of the complete
feature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material.
392
The Default, OldLTH and CoNLL schemes
mainly differ in their coordination structure, and the
Functional and Lexical schemes differ in their selec-
tion of a functional and a lexical head, respectively.
All schemes use the same inventory of labels.7 The
LTH parameter settings for the different schemes are
elaborated in the supplementary material.
The Setup We use two different parsers: (i) Malt-
Parser (Nivre et al, 2007b) with the arc eager algo-
rithm as optimized for English in (Nivre et al, 2010)
and (ii) MSTParser with the second-order projec-
tive model of McDonald and Pereira (2006). Both
parsers were trained on the different instances of
sections 2-21 of the PTB obeying the different an-
notation schemes in Table 3. Each trained model
was used to parse section 23. All non-projective de-
pendencies in the training and gold sets were projec-
tivized prior to training and parsing using the algo-
rithm of Nivre and Nilsson (2005). A more princi-
pled treatment of non-projective dependency trees is
an important topic for future research. We evaluated
the parses using labeled and unlabeled attachment
scores, and using our TEDEVAL software package.
Evaluation Our TEDEVAL software package im-
plements the pipeline described in Section 3. We
convert all parse and gold trees into functional
trees using the algorithm defined in Section 3, and
for each pair of parsing experiments we calculate
a shared gold standard using generalization deter-
mined through a chart-based greedy algorithm.8 Our
scoring procedure uses the TED algorithm defined
by Zhang and Shasha (1989).9 The unlabeled score
is obtained by assigning cost(e) = 0 for every e re-
labeling operation. To calculate pairwise statistical
significance we use a shuffling test with 10,000 it-
erations (Cohen, 1995). A sample of all files in the
evaluation pipeline for a subset of 10 PTB sentences
is available in the supplementary materials.10
7In case the labels are not taken from the same inventory,
e.g., subjects in one scheme are marked as SUB and in the other
marked as SBJ, it is possible define a a set of zero-cost operation
types ? in such case, to the operation relabel(SUB,SBJ) ? in
order not to penalize string label discrepancies.
8Our algorithm has space and runtime complexity ofO(n2).
9Available via http://web.science.mq.edu.au/
?swan/howtos/treedistance/
10The TEDEVAL software package is available via http:
//stp.lingfil.uu.se/?tsarfaty/unipar
Results Table 1 reports the results for the inter-
and cross-experiment evaluation of parses produced
by MaltParser. The left hand side of the table
presents the parsing results for a set of experiments
in which we compare parsing results trained on the
Default, OldLTH and CoNLL07 schemes. In a sec-
ond set of experiments we compare the CoNLL07,
Lexical and Functional schemes. Table 2 reports the
evaluation of the parses produced by MSTParser for
the same experimental setup. Our goal here is not to
compare the parsers, but to verify that the effects of
switching from LAS to TEDEVAL are robust across
parsing algorithms.
In each of the tables, the top three groups of four
rows compare results of parsed dependency trees
trained on a particular scheme against gold trees of
the same and the other schemes. The next three
groups of two rows report the results for compar-
ing pairwise sets of experiments against a general-
ized gold using our proposed procedure. In the last
group of two rows we compare all parsing results
against a single gold obtained through a three-way
generalization.
As expected, every parser appears to perform at
its best when evaluated against the scheme it was
trained on. This is the case for both LAS and TEDE-
VAL measures and the performance gaps are statis-
tically significant. When moving to pairwise evalu-
ation against a single generalized gold, for instance,
when comparing CoNLL07 to the Default settings,
there is still a gap in performance, e.g., between
OldLTH and CoNLL07, and between OldLTH and
Default. This gap is however a lot smaller and is not
always statistically significant. In fact, when evalu-
ating the effect of linguistically disparate annotation
variations such as Lexical and Functional on the per-
formance of MaltParser, Table 1 shows that when
using TEDEVAL and a generalized gold the perfor-
mance gaps are small and statistically insignificant.
Moreover, observed performance trends when
evaluating individual experiments on their original
training scheme may change when compared against
a generalized gold. The Default scheme, for Malt-
Parser, appears better than OldLTH when both are
evaluated against their training schemes. But look-
ing at the pairwise-evaluated experiments, it is the
other way round (the difference is smaller, but statis-
tically significant). In evaluating against a three-way
393
generalization, all the results obtained for different
training schemes are on a par with one another, with
minor gaps in performance, rarely statistically sig-
nificant. This suggests that apparent performance
trends between experiments when evaluating with
respect to the training schemes may be misleading.
These observations are robust across parsing algo-
rithms. In each of the tables, results obtained against
the training schemes show significant differences
whereas applying our cross-experimental procedure
shows small to no gaps in performance across dif-
ferent schemes. Annotation variants which seem to
have crucial effects have a relatively small influence
when parsed structures are brought into the same
formal and theoretical common ground for compar-
ison. Of course, it may be the case that one parser is
better trained on one scheme while the other utilizes
better another scheme, but objective performance
gaps can only be observed when they are compared
against shared linguistic content.
5 Discussion and Extensions
This paper addresses the problem of cross-
experiment evaluation. As it turns out, this prob-
lem arises in NLP in different shapes and forms;
when evaluating a parser against different annota-
tion schemes, when evaluating parsing performance
across parsers and different formalisms, and when
comparing parser performance across languages.
We consider our contribution successful if after
reading it the reader develops a healthy suspicion to
blunt comparison of numbers across experiments, or
better yet, across different papers. Cross-experiment
comparison should be a careful and well thought-
through endeavor, in which we retain as much infor-
mation as we can from the parsed structures, avoid
lossy conversions, and focus on an object of evalua-
tion which is agreed upon by all variants.
Our proposal introduces one way of doing so in
a streamlined, efficient and formally worked out
way. While individual components may be further
refined or improved, the proposed setup and imple-
mentation can be straightforwardly applied to cross-
parser and cross-framework evaluation. In the fu-
ture we plan to use this procedure for comparing
constituency and dependency parsers. A conversion
from constituency-based trees into functional trees
is straightforward to define: simply replace the node
labels with the grammatical function of their domi-
nating arc ? and the rest of the pipeline follows.
A pre-condition for cross-framework evaluation
is that all representations encode the same set of
grammatical relations by, e.g., annotating arcs in de-
pendency trees or decorating nodes in constituency
trees. For some treebanks this is already the case
(Nivre and Megyesi, 2007; Skut et al, 1997; Hin-
richs et al, 2004) while for others this is still lack-
ing. Recent studies (Briscoe et al, 2002; de Marn-
effe et al, 2006) suggest that evaluation through a
single set of grammatical relations as the common
denominator is a linguistically sound and practically
useful way to go. To guarantee extensions for cross-
framework evaluation it would be fruitful to make
sure that resources use the same set of grammatical
relation labels across different formal representation
types. Moreover, we further aim to inquire whether
we can find a single set of grammatical relation la-
bels that can be used across treebanks for multiple
languages. This would then pave the way for the de-
velopment of cross-language evaluation procedures.
6 Conclusion
We propose an end-to-end procedure for compar-
ing dependency parsing results across experiments
based on three steps: (i) converting dependency trees
to functional trees, (ii) generalizing functional trees
to harmonize information from different sources,
and (iii) using distance-based metrics that take the
different sources into account. When applied to
parsing results of different dependency schemes,
dramatic gaps observed when comparing parsing re-
sults obtained in isolation decrease or dissolve com-
pletely when using our proposed pipeline.
Acknowledgments We thank the developers of
the LTH and TED software who made their code
available for our use. We thank Richard Johansson
for providing us with the LTH parameter settings of
existing dependency schemes. We thank Ari Rap-
poport, Omri Abend, Roy Schwartz and members of
the NLP lab at the Hebrew University of Jerusalem
for stimulating discussion. We finally thank three
anonymous reviewers for useful comments on an
earlier draft. The research reported in the paper was
partially funded by the Swedish Research Council.
394
References
Philip Bille. 2005. A survey on tree edit distance
and related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. Procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black, ed-
itor, Proceedings of the workshop on Speech and Nat-
ural Language, HLT, pages 306?311. Association for
Computational Linguistics.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes.
In Proceedings of LREC Workshop?Beyond Parseval
? Towards improved evaluation measures for parsing
systems?.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the impact of alternative dependency graph encodings
on solving event extraction tasks. In Proceedings of
EMNLP, pages 982?992.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In
Proceedings of LREC, pages 447?454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC.
Jinho D. Choi and Martha Palmer. 2010. Robust
constituent-to-dependency conversion for English. In
Proceedings of TLT.
Paul Cohen. 1995. Empirical Methods for Artificial In-
telligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, pages 449?454.
Martin Emms. 2008. Tree-distance and some other vari-
ants of evalb. In Proceedings of LREC.
Erhard Hinrichs, Sandra Ku?bler, Karin Naumann, Heike
Telljohan, and Julia Trushkina. 2004. Recent develop-
ment in linguistic annotations of the Tu?Ba-D/Z Tree-
bank. In Proceedings of TLT.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA.
Sandra Ku?bler and Heike Telljohann. 2002. Towards
a dependency-oriented evaluation for partial parsing.
In Proceedings of LREC Workshop?Beyond Parseval
? Towards improved evaluation measures for parsing
systems?.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of ACL, pages 46?54.
Joakim Nivre and Beata Megyesi. 2007. Bootstrapping
a Swedish Treebank using cross-corpus harmonization
and annotation projection. In Proceedings of TLT.
Joakim Nivre and Jens Nilsson. 2005. Pseudo projective
dependency parsing. In Proceeding of ACL, pages 99?
106.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings of
COLING, pages 64?70.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
JoakimNivre, Laura Rimell, RyanMcDonald, and Carlos
Go?mez-Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. pages 813?821.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In Proceedings of HLT-ACL, pages 337?
340.
Ines Rehbein and Josef van Genabith. 2007. Why is it so
difficult to compare treebanks? Tiger and Tu?Ba-D/Z
revisited. In Proceedings of TLT, pages 115?126.
395
Brian Roark. 2002. Evaluating parser accuracy us-
ing edit distance. In Proceedings of LREC Work-
shop?Beyond Parseval ? Towards improved evaluation
measures for parsing systems?.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing eval-
uation. In Proceedings of ACL, pages 663?672.
Stuart M. Shieber. 1986. An Introduction to Unification-
Based Grammars. Center for the Study of Language
and Information.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for free
word-order languages. In Proceedings of the fifth con-
ference on Applied natural language processing, pages
88?95.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceeding of IWPT, pages 195?206.
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees and
related problems. In SIAM Journal of Computing, vol-
ume 18, pages 1245?1262.
Arnold M. Zwicky. 1993. Heads, bases, and functors.
In G.G. Corbett, N. Fraser, and S. McGlashan, editors,
Heads in Grammatical Theory, pages 292?315. Cam-
bridge University Press.
396
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296?1307,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Parsing Using Content and Context:
A Case Study from Requirements Elicitation
Reut Tsarfaty
Weizmann Institute
Rehovot, Israel
Ilia Pogrebezky
Interdisciplinary Center
Herzliya, Israel
Guy Weiss
Weizmann Institute
Rehovot, Israel
Yaarit Natan
Weizmann Institute
Rehovot, Israel
Smadar Szekely
Weizmann Institute
Rehovot, Israel
David Harel
Weizmann Institute
Rehovot, Israel
Abstract
We present a model for the automatic se-
mantic analysis of requirements elicitation
documents. Our target semantic repre-
sentation employs live sequence charts, a
multi-modal visual language for scenario-
based programming, which can be directly
translated into executable code. The ar-
chitecture we propose integrates sentence-
level and discourse-level processing in a
generative probabilistic framework for the
analysis and disambiguation of individual
sentences in context. We show empiri-
cally that the discourse-based model con-
sistently outperforms the sentence-based
model when constructing a system that re-
flects all the static (entities, properties) and
dynamic (behavioral scenarios) require-
ments in the document.
1 Introduction
Requirements elicitation is a process whereby a
system analyst gathers information from a stake-
holder about a desired system (software or hard-
ware) to be implemented. The knowledge col-
lected by the analyst may be static, referring to
the conceptual model (the entities, their properties,
the possible values) or dynamic, referring to the
behavior that the system should follow (who does
what to whom, when, how, etc). A stakeholder in-
terested in the system typically has a specific static
and dynamic domain in mind, but he or she cannot
necessarily prescribe any formal models or code
artifacts. The term requirements elicitation we use
here refers to a piece of discourse in natural lan-
guage, by means of which a stakeholder commu-
nicates their desiderata to the system analyst.
The role of a system analyst is to understand
the different requirements and transform them into
formal constructs, formal diagrams or executable
code. Moreover, the analyst needs to consolidate
the different pieces of information to uncover a
single shared domain. Studies in software engi-
neering aim to develop intuitive symbolic systems
with which human agents can encode require-
ments that would then be unambiguously trans-
lated into a formal model (Fuchs and Schwitter,
1995; Bryant and Lee, 2002).
More recently, Gordon and Harel (2009) de-
fined a natural fragment of English that can be
used for specifying requirements which can be
effectively translated into live sequence charts
(LSC) (Damm and Harel, 2001; Harel and
Marelly, 2003), a formal language for specifying
the dynamic behavior of reactive systems. How-
ever, the grammar that underlies this language
fragment is highly ambiguous, and all disam-
biguation has to be conducted manually by a hu-
man agent. Indeed, it is commonly accepted that
the more natural a controlled language fragment
is, the harder it is to develop an unambiguous
translation mechanism (Kuhn, 2014).
In this paper we accept the ambiguity of re-
quirements descriptions as a premise, and aim to
answer the following question: can we automati-
cally recover a formal representation of the com-
plete system ? one that best reflects the human-
perceived interpretation of the entire document?
Recent advances in natural language processing,
with an eye to semantic parsing (Zettlemoyer and
Collins, 2005; Liang et al., 2011; Artzi and Zettle-
moyer, 2013; Liang and Potts, 2014), use differ-
ent formalisms and various kinds of learning sig-
nals for statistical semantic parsing. In particu-
lar, the model of Lei et al. (2013) induces input
parsers from format descriptions. However, rarely
do these models take into account the entire docu-
ment?s context.
The key idea we promote here is that discourse
context provides substantial disambiguating infor-
mation for sentence analysis. We suggest a novel
1296
Figure 1: An LSC scenario: ?When the user clicks
the button, the display color must change to red.?
model for integrated sentence-level and discourse-
level processing, in a joint generative probabilistic
framework. The input for the requirements elici-
tation task is given in a simplified, yet highly am-
biguous, fragment of English, as specified in Gor-
don and Harel (2009). The output, in contrast, is
a sequence of unambiguous and well-formed live
sequence charts (LSC) (Damm and Harel, 2001;
Harel and Marelly, 2003) describing the dynamic
behavior of the system, tied to a single shared
code-base called a system model (SM).
Our solution takes the form of a hidden markov
model (HMM) where emission probabilities re-
flect the grammaticality and interpretability of tex-
tual requirements via a probabilistic grammar and
transition probabilities model the overlap between
SM snapshots of a single, shared, domain. Using
efficient viterbi decoding, we search for the best
sequence of domain snapshots that has most likely
generated the entire requirements document. We
empirically show that such an integrated model
consistently outperforms a sentence-based model
learned from the same set of data.
The remainder of this document is organized as
follows. In Section 2 we describe the task, and
spell out our formal assumptions concerning the
input and the output. In Section 3 we present
our target semantic representation and a specially
tailored notion of grounding for anchoring the
requirements in a code-base. In Section 4 we
develop our sentence-based and discourse-based
models, and in Section 5 we evaluate the models
on various case studies. In Section 6 we discuss
applications and future extensions, and in Sec-
tion 7 we summarize and conclude.
2 Parsing Requirements Elicitation
Documents: Task Description
There is an inherent discrepancy between the in-
put and the output of the software engineering pro-
cess. The input, system requirements, is specified
in a natural, informal, language. The output, the
system, is ultimately implemented in a formal un-
ambiguous programming language. Can we auto-
matically recover such a formal representation of
a complete system from a set of requirements? In
this work we explore this challenge empirically.
The Input. We assume a scenario-based pro-
gramming paradigm (a.k.a behavioral program-
ming (BP) (Harel et al., 2012)) in which system
development is seen as a process whereby humans
describe the expected behavior of the system by
means of ?short-stories?, formally called scenar-
ios (Harel, 2001). We further assume that a given
requirements document describes exactly one sys-
tem, and that each sentence describes a single,
possibly complex, scenario. The requirements we
aim to parse are given in a simplified form of En-
glish (specifically, the English fragment described
in Gordon and Harel (2009)). Contrary to strictly
formal specification languages, which are closed
and unambiguous, this fragment of English em-
ploys an open-ended lexicon and exhibits exten-
sive syntactic and semantic ambiguity.
1
The Output. Our target semantic representation
employs live sequence charts (LSC), a diagram-
matic formal language for scenario-based pro-
gramming (Damm and Harel, 2001). Formally,
LSCs are an extension of the well-known UML
message sequence diagrams (Harel and Maoz,
2006), and they have a direct translation into ex-
ecutable code (Harel and Marelly, 2003).
2
Using
LSC diagrams for software modelling enjoys the
advantages of being easily learnable (Harel and
Gordon, 2009), intuitively interpretable (Eitan et
al., 2011) and straightforwardly amenable to exe-
cution (Harel et al., 2002) and verification (Harel
et al., 2013). The LSC language is particularly
suited for representing natural language require-
ments, since its basic formal constructs, scenar-
ios, nicely align with events, the primitive objects
of Neo-Davidsonian Semantics (Parsons, 1990).
1
Formally, this variant may be viewed as a CNL of degree
P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12).
2
It can be shown that the execution semantics of the LSC
language is embedded in a fragment of a branching temporal
logic called CTL* (Kugler et al., 2005).
1297
Live Sequence Charts and Code Artifacts. A
live sequence chart (LSC) is a diagram that de-
scribes a possible or necessary run of a specified
system. In a single LSC diagram, entities are rep-
resented as vertical lines called lifelines, and inter-
actions between entities are represented using hor-
izontal arrows between lifelines called messages,
connecting a sender to a receiver. Messages may
refer to other entities (or properties of entities) as
arguments. Time in LSCs proceeds from top to
bottom, imposing a partial order on the execution
of messages. LSC messages can be hot (red, ?must
happen?) or cold (blue, ?may happen?). A mes-
sage may have an execution status, which desig-
nates it as monitored (dashed arrow, ?wait for?)
or executed (full arrow, ?execute?). The LSC lan-
guage also encompasses conditions and control
structures, and it allows defining requirements in
terms of the negation of charts. Figure 1 illustrates
the LSC for the scenario ?When the user clicks
the button, the display color must change to red.?.
The respective system model (SM) is a code-base
hierarchy containing the classes USER, BUTTON,
DISPLAY, the method BUTTON.CLICK() and the
property DISPLAY.COLOR.
3 Formal Settings
In the text-to-code generation task, we aim to im-
plement a prediction function f : D ? M, such
thatD ? D is a piece of discourse consisting of an
ordered set of requirements D = d
1
, d
2
...d
n
, and
f(D) = M ? M is a code-base hierarchy that
grounds the semantic interpretation of D; we de-
note this by M . sem(d
1
, ..., d
n
). We now define
the objects D,M , and describe how to construct
the semantic interpretation function (sem(.)). We
then spell out the notion of grounding (.).
Surface Structures: Let ? be a finite lexicon
and let L
req
? ?
?
be a language for specifying
requirements. We assume the sentences in L
req
have been generated by a context-free grammar
G = ?N ,?, S ? N ,R?, where N is a set of non-
terminals, ? is the aforementioned lexicon, S ?
N is the start symbol andR is a set of context-free
rules {A ? ?|A ? N , ? ? (N ? ?)
?
}. For each
utterance u ? L
req
, we can find a sequential appli-
cation of rules that generates it: u = r
1
? ... ? r
k
;
?i : r
i
? R. We call such a sequence a deriva-
tion of u. These derivations may be graphically
depicted as parse trees, where the utterance u de-
fines the sequence of tree terminals in the leaves.
We define T
req
to be the set of trees strongly
generated by G, and utilize an auxiliary yield
function yield : T
req
? L
req
returning the leaves
of the given tree t ? .L
req
. Different parse-trees
can generate the same utterance, so the task of an-
alyzing the structure of an utterance u ? L
req
is
modeled via a function syn : L
req
? T
req
that
returns the correct, human-perceived, parse of u.
Semantic Structures: Our target semantic rep-
resentation of a requirement d ? L
req
is a dia-
grammatic structure called a live sequence chart
(LSC). The LSC formal definition we provide here
is based on the appendix of Harel and Marelly
(2003), but rephrased in set-theoretic, event-based,
terms. We defined this alternative formalization
in order to make LSCs compatible with Neo-
Davidsonian, event-based, semantic theories. As
a result, this form of LSC formalization is well-
suited for representing the semantics of natural
language utterances.
Let us assume that L is a dictionary of entities
(lifelines), A is a dictionary of actions, P is a dic-
tionary of attribute names and V a dictionary of
attribute values. The set of simple events in the
LSC formal system is defined as follows:
E
active
? L?A? L? (L? P ? V )
?
?{hot, cold} ? {executed,monitored}
where e = ?l
1
, a, l
2
, {l
i
: p
i
: v
i
}
k
i=3
, temp, exe?
and l
i
? L, a ? A, p
i
? P, temp ? {hot, cold},
exe ? {executed,monitored}. The event e is
called a message in which an action a is carried
over from a sender l
1
to a receiver l
2
.
3
The set
{l
i
: p
i
: v
i
}
k
i=3
depicts a set of attribute:value
pairs provided as arguments to action a. The tem-
perature temp marks the modality of the action
(may, must), and the status exe distinguishes ac-
tions to be taken from actions to be waited for.
An event e can also refer to a state, where a
logical expression is being evaluated over a set of
property:value pairs. We call such an event a con-
dition, and specify the set of possible conditions
as follows:
E
cond
? Exp? (L? P ? V )
?
?{hot, cold} ? {executed,monitored}
3
The LSC language also distinguishes static lifelines from
dynamically-bound lifelines. For brevity, we omit this from
the formal description of events, and simply assert that it may
be listed as one of the properties of the relevant lifeline.
1298
Specifically, e = ?exp, {l : p : v}
k
i=0
, temp, exe?
is a condition to be evaluated, where l
i
? L, p
i
?
P, v
i
? V, temp ? {hot, cold} and exe ?
{executed,monitored} are as specified above.
The condition exp ? Exp is a first-order logic for-
mula using the usual operators (?,?,?,?,?,?).
The set {l : p : v}
k
i=0
depicts a (possibly empty)
set of attribute:value pairs that participates as pred-
icates in exp. Executing a condition, that is, evalu-
ating the logical expression specified by exp, also
has a modality (may/must) and an execution status
(performed/waited for).
The LSC language further allows us to define
more complex events by combining partially or-
dered sets of events with control structures.
E
complex
? N ? E
cond
?
{?E
c
, <?|?E
c
, <? is a poset }
N is a set of non-negative integers, E
cond
is a set
of conditions as described above, and each ele-
ment ?E
c
, <? is a partially ordered set of events.
This structure allows us to derive three kinds of
control structures:
? e = ?#, ?, ?E,<?? is a loop in which ?E,<?
is executed # times.
? e = ?0, cond, ?E,<?? is a conditioned exe-
cution. If cond holds, ?E,<? is executed.
? e = ?#, {cond}
#
i=1
, {?E
c
, <?}
#
i=1
? is a
switch: in case i, if the condition i holds,
?E
c
, <?
i
is executed.
Definition 1 (LSC) An LSC c = ?E,<? is a
partially ordered set of events such that
?e ? E : e ? E
active
? e ? E
cond
? e ? E
complex
Grounded Semantics: The information repre-
sented in the LSC provides the recipe for a rig-
orous construction of the code-base that will im-
plement the program. This code-base is said to
ground the semantic representation. For exam-
ple, if our target programming language is an
Object-Oriented programming language such as
Java, then the code-base will include the objects,
the methods and the properties that are minimally
required for executing the scenario that is repre-
sented by the LSC. We refer to this code-base as
a system model (henceforth, SM), and define it as
follows.
Definition 2: (SM) Let L
m
be a set of imple-
mented objects, A
m
a set of implemented meth-
ods, P
m
a set of arguments and V
m
argument
values. We further define the auxiliary functions
methods : A
m
? L
m
, props : P
m
? L
m
and
values : V
m
? L
m
? P
m
, for identifying the
entity l ? L
m
that implements the method a ?
A
m
, the entity l ? L
m
that contains the property
p ? P
m
, and the entity property ?l, p? ? L
m
?P
m
that assumes that value v ? V
m
, respectively. A
system model (SM) is a tuple m representing the
implemented architecture.
m = ?L
m
, A
m
, P
m
, V
m
,methods, props, values?
Analogously to interpretation functions in logic
and natural language semantics, we assume here
an implementation function, denoted [[.]], which
maps each formal entity in the LSC semantic rep-
resentation to its instantiation in the code-base.
Using this function we define a notion of ground-
ing that captures the fact that a certain code-base
permits the execution of a given LSC c.
Definition 3(a): (Grounding) LetM be the set
of system models and let C be the set of LSC
charts. We say that m grounds c = ?E,<?, and
write m . c, if ?e ? E : m . e, where:
? if e ? E
active
then
m . e?
[[l
1
]], [[l
2
]] ? L &
[[a]] ? methods([[l
2
]]) &
?i : ?l : p : v?
i
? [[l]] ? L
m
&[[p]] ?
props[[l]]&v ? values([[l]], [[p]])
? if e ? E
cond
then
m . e?
?i : ?l : p : v?
i
? [[l]] ? L
m
&[[p]] ?
props[[l]]&v ? values([[l]], [[p]])
? if e = ?#, e
s
, ?E
c
, <? ? E
complex
then
m . e?m . e
s
& ?e
?
? E
c
: m . e
?
We have thus far defined how the semantics of
a single LSC can be grounded in a single SM. In
the real world, however, a requirements document
typically contains multiple different requirements,
but it is interpreted as a complete whole. The de-
sired SM is then one that represents a single do-
main shared by all the specified requirements. Let
us then assume a document d = d
1
, ..., d
n
con-
taining n requirements, where ?i : d
i
? L
req
, and
1299
let unionsq be a unification operation that returns the for-
mal unification of two SMs if such exists, and an
empty SM otherwise. We define a discourse in-
terpretation function sem(d) that returns a single
SM for the entire document, where different men-
tions across sentences may share the same refer-
ence. The discourse interpretation function sem
can be as simple as unifying all individual SMs
for d
i
, and asserting that all elements that have the
same name in different SMs refer to a single ele-
ment in the overall SM. Or, it can be as complex as
taking into account synonyms (?clicks the button?
and ?presses the button?), anaphora (?when the
user clicks the button, it changes colour?), bind-
ing (?when the user clicks any button, this button
is highlighted?), and so on. We can now define the
grounding of an entire requirements document.
Definition 3(b): (Grounding) Let d = d
1
...d
n
be a requirements document and let m = m
1
...m
n
be a sequence of system models. M = ?m,unionsq? is
a sequence of models and a unification operation,
and M . sem(d) if and only if ?i : m
i
. sem(d
i
)
and ((m
1
unionsqm
2
).... unionsqm
n
) . sem(d
1
, ...., d
n
).
In this work we assume that sem(d) is a simple
discourse interpretation function, where entities,
methods, properties, etc. that are referred to using
the same name in different local SMs refer to a sin-
gle element in the overall code-base. This simple
assumption already carries a substantial amount of
disambiguating information concerning individual
requirements. For example, assume that we have
seen a ?click? method over a ?button? object in
sentence i. This may help us disambiguate future
attachment ambiguity, favoring structures where
a ?button? is attached to ?click? over other at-
tachment alternatives. Our goal is then to model
discourse-level context for supporting the accurate
semantic analysis of individual requirements.
4 Probabilistic Modeling
In this section we set out to explicitly model
the requirement?s context, formally captured as a
document-level SM, in order to support the accu-
rate disambiguation of the requirements? content.
We first specify our probabilistic content model,
a sentence-level model which is based on a prob-
abilistic grammar augmented with compositional
semantic rules. We then specify our probabilistic
context model, a document-level sequence model
that takes into account the content as well as the
relation between SMs at different time points.
4.1 Sentence-Based Modeling
The task of our sentence-based model is to learn
a function that maps each requirement sentence
to its correct LSC diagram and SM snapshot.
In a nutshell, we do this via a (partially lexi-
calized) probabilistic context-free grammar aug-
mented with a semantic interpretation function.
More formally, given a discourse D = d
1
...d
n
we think of each d
i
as having been generated by
a probabilistic context-free grammar (PCFG) G.
The syntactic analysis of d
i
may be ambiguous,
so we first implement a syntactic analysis function
syn : L
req
? T
req
using a probabilistic model
that selects the most likely syntax tree t of each
d individually. We can simplify syn(d), with d
constant with respect to the maximization:
syn(d) = argmax
t?T
req
P (t|d)
= argmax
t?T
req
P (t,d)
p(d)
= argmax
t?T
req
P (t, d)
= argmax
t?{t|t?T
req
,yield(t)=d}
P (t)
Because of the context-freeness assumption, it
holds that P (t) =
?
r?der(t)
P (r), where der(t)
returns the rules that derive t. The resulting proba-
bility distribution P : T
req
? [0, 1] defines a prob-
abilistic language model over all requirements d ?
L
req
, i.e.,
?
d?L
req
?
t?T
req
,yield(t)=d
P (t) = 1.
We assume a function sem : T ? C mapping
syntactic parse trees to semantic constructs in the
LSC language. Syntactic parse trees are complex
entities, assigning structures to the flat sequences
of words. The principle of compositionality as-
serts that the meaning of a complex syntactic en-
tity is a function of the meaning of its parts and
their mode of combination. Here, the semantics of
a tree t ? T
req
is derived compositionally from the
interpretation of the rules in the grammar G. We
overload the sem notation to define sem : R ? C
as a function assigning rules to LSC constructs
(events or parts of events),
4
with
?
? merging the
resulting sets of events. Our sentence-based com-
positional semantics is summarized as:
sem(u) = sem(syn(u)) = sem(r
1
? ... ? r
n
) =
sem(r
1
)
?
?...
?
?sem(r
n
) = c
1
?
?...
?
?c
n
= c
4
Here, it suffices to say that sem maps edges in the
syntax tree to functions in the API of an existing LSC
editor. For example: sem(NP ? DET NN) =
fCreateObject(DET.sem,NN.sem). We specify the
function sem in the supplementary materials. The code of
sem is available as part of PlayGo (www.playgo.co).
1300
For a single chart c, one can easily construct an
implementation for every entity, action and prop-
erty in the chart. Then, by design, we get an
SM m such that m . c. To construct the SM of
the entire discourse in the sentence-based model
we simply return f(d
1
, ..., d
n
) = unionsq
n
i=1
m
i
where
?i : m
i
. sem(syn(d
i
)) and unionsq unifies different
mentions of the same string to a single element.
4.2 Discourse-Based Modeling
We assume a given document D ? D and aim to
find the most probable system modelM ?M that
satisfies the requirements. We assume that M re-
flects a single domain that the stakeholders have in
mind, and we are provided with an ambiguous nat-
ural language evidence, an elicited discourseD, in
which they convey it. We instantiate this view as a
noisy channel model (Shannon, 1948), which pro-
vides the foundation for many NLP applications,
such as speech recognition (Bahl et al., 1983) and
machine translation (Brown et al., 1993).
According to the noisy channel model, when a
signal is received it does not uniquely identify the
message being sent. A probabilistic model is then
used to decode the original message. In our case,
the signal is the discourse and the message is the
overall system model. In formal terms, we want to
find a model M that maximises the following:
f(D) = argmax
M?M
P (M |D)
We can simplify further, using Bayes law, where
D is constant with respect to the maximisation.
f(D) = argmax
M?M
P (M |D)
= argmax
M?M
P (D|M)?P (M)
P (D)
= argmax
M?M
P (D|M)? P (M)
We would thus like to estimate two types of prob-
ability distributions, P (M) over the source and
P (D|M) over the channel.
BothM andD are structured objects with com-
plex internal structure. In order to assign prob-
abilities to objects involving such complex struc-
tures it is customary to break them down into sim-
pler, more basic, events. We know that D =
d
1
, d
2
, ..., d
n
is composed of n individual sen-
tences, each representing a certain aspect of the
model M . We assume a sequence of snapshots of
M that correspond to the timestamps 1...n, that is:
m
1
,m
2
, ...,m
n
? M where ?i : m
i
. sem(d
i
).
The complete SM is given by the union of the
different snapshots reflected in different require-
ments, i.e., M =
?
i
m
i
. We then rephrase:
P (M) = P (m
1
, ...,m
n
)
P (D|M) = P (d
1
, ...., d
n
|m
1
, ...,m
n
)
These events may be seen as points in a high di-
mensional space. In actuality, they are too com-
plex and would be too hard to estimate directly.
We then define two independence assumptions.
First, we assume that a system model snapshot at
time i depends only on k previous snapshots (a
stationary distribution). Secondly, we assume that
each sentence i depends only on the SM snapshot
at time i. We now get:
P (m
1
...m
n
) ?
?
i
P (m
i
|m
i?1
...m
i?k
)
P (d
1
...d
n
|m
1
...m
n
) ?
?
i
P (d
i
|m
i
)
Furthermore, assuming bi-gram transitions, our
objective function is now represented as follows:
f(D) = argmax
M?M
n
?
i=1
P (m
i
|m
i?1
)P (d
i
|m
i
)
Note that m
0
may be empty if the system is im-
plemented from scratch, and non-empty if the re-
quirements assume an existing code-base, which
makes p(m
1
|m
0
) a non-trivial transition.
4.3 Training and Decoding
Our model is in essence a Hidden Markov Model
in which states capture SM snapshots, state-
transition probabilities model transitions between
SM snapshots, and emission probabilities model
the verbal description of each state. To implement
this, we need to implement a decoding algorithm
that searches through all possible state sequences,
and a training algorithm that can automatically
learn the values of the still rather complex param-
eters P (m
i
|m
i?1
), P (d
i
|m
i
) from data.
f(D) = argmax
M?M
? ?? ?
decoding
n
?
i=1
P (m
i
|m
i?1
)P (d
i
|m
i
)
? ?? ?
training
Training: We assume a supervised training set-
ting in which we are given a set of examples anno-
tated by a human expert. For instance, these can
be requirements an analyst has formulated and en-
coded using an LSC editor, while manually pro-
viding disambiguating information. We are pro-
vided with a set of pairs {D
i
,M
i
}
n
i=1
containing n
documents, where each of the pairs in i = 1..n is a
1301
tuple set {d
ij
, t
ij
, c
ij
,m
ij
}
n
i
j=1
. For all i, j, it holds
that t
ij
= syn(d
ij
), c
ij
= sem(t
ij
), and m
ij
.
sem(syn(d
ij
)). The union of the n
i
SM snapshots
yields the entire model unionsq
j
m
i
j
= M
i
, that satisfies
the set of requirements M
i
. sem(d
i1
, ..., d
in
i
).
(i) Emission Parameters Our emission parame-
ters P (d
i
|m
i
) represent the probability of a verbal
description of a requirement given an SM snap-
shot which grounds the semantics of the descrip-
tion. A single SM may result from different syn-
tactic derivations. We calculate this probability
by marginalizing over the syntactic trees that are
grounded in the same SM snapshot.
P (d,m)
P (m)
=
?
t?{t|yield(t)=d,m.sem(t)}
P (t)
?
t?{t|t?T
req
,m.sem(t)}
P (t)
The probability of P (t) is estimated using a tree-
bank PCFG (Charniak, 1996), based on all pairs
?d
ij
, t
ij
? in the annotated corpus. We estimate
rule probabilities using maximum-likelihood es-
timates, and use simple smoothing for unknown
lexical items, using rare-words distributions.
(ii) Transition Parameters Our transition pa-
rameters P (m
i
|m
i?1
) represent the amount of
overlap between the SM snapshots. We look at the
current and the previous system model, and aim
to estimate how likely the current SM is given the
previous one. There are different assumptions that
may underly this probability distribution, reflect-
ing different principles of human communication.
We first define a generic estimator as follows:
?
P (m
i
|m
j
) =
gap(m
i
,m
j
)
?
m
j
gap(m
i
,m
j
)
where gap(.) quantifies the information sharing
between SM snapshots. Regardless of the im-
plementation of gap, it can be easily shown that
?
P is a conditional probability distribution where
?
P : M ? M ? [0, 1] and, for all m
i
,m
j
, :
?
m
j
?
P (m
i
|m
j
) = 1. (For efficiency reasons, we
considerM to be a restricted universe that is con-
sidered be the decoder, as specified shortly.)
We define different gap implementations, re-
flecting different assumptions about the discourse.
Our first assumption here is that different SM
snapshots refer to the same conceptual world, so
there should be a large overlap between them. We
call this the max-overlap assumption. A second
assumption is that, in collaborative communica-
tion, a new requirement will only be stated if it
Transition: gap(m
curr
,m
prev
)
max-overlap
|set(m
curr
)?set(m
prev
)|
|set(m
curr
)|
max-expansion 1?
|set(m
curr
)?set(m
prev)
|
|set(m
prev
)?set(m
curr
)|
min-distance 1?
ted(m
prev
,m
curr
)
|set(m
prev
)|+|set(m
curr
)|
Table 1: Quantifying the gap between snapshots.
set(m
i
) is a set of nodes marked by path to root.
provides new information, akin to Grice (1975).
This is the max-expansion assumption. An addi-
tional assumption prefers ?easy? transitions over
?hard? ones, this is the min-distance assumption.
The different gap calculations are listed in Table 1.
Decoding An input document contains n re-
quirements. Our decoding algorithm considers the
N-best syntactic analyses for each requirement. At
each time step i = 1...n we assume N, states rep-
resenting the semantics of the N best syntax trees,
retrieved via a CKY chart parser. Thus, setting
N = 1 is equal to a sentence-based model, in
which for each sentence we simply select the most
likely tree according to a probabilistic grammar,
and construct a semantic representation for it.
For each document of length n, we assume that
our entire universe of system models M is com-
posed of N ? n SM snapshots, reflecting the N
most-likely analyses of n sentences, as provided
by the probabilistic syntactic model. (As shall be
seen shortly, even with this restricted
5
universe ap-
proximating M, our discourse-based model pro-
vides substantial improvements over a sentence-
based model).
Our discourse-based model is an HMM where
each requirement is an observed signal, and each
i = 1..N is a state representing the SM that
grounds the i th best tree. Because of the
Markov independence assumption our setup satis-
fies the optimal subproblem and overlapping prob-
lem properties, and we can use efficient viterbi de-
coding to exhaustively search through the differ-
ent state sequences, and find the most probable
sequence that has generated the sequence of re-
quirements according to our discourse-based prob-
abilistic model.
5
This restriction is akin to pseudo-likelihood estimation,
as in Arnold and Strauss (1991). In pseudo-likelihood estima-
tion, instead of normalizing over the entire set of elements,
one uses a subset that reflects only the possible outcomes.
Here, instead of summing SM probabilities over all possible
sentences in the language, we sum up the SM analyses of the
sentences observed in the document only. This estimation
could also be addressed via, e.g., sampling methods.
1302
The overall complexity decoding a document
with n sentences of which max length is l, using a
grammar G of size |G| and a fixed N , is given by:
O(n? l
3
? |G|
3
+ l
2
?N
2
? n+ n
3
?N
2
)
We can break this expression down as follows: (i)
In O(n ? l
3
? |G|
3
) we generate N best trees for
each one of the n requirements, using a CKY chart
(Younger, 1967). (ii) In O(l
2
?N
2
?n) we create
the universe M based on the N best trees of the
n requirements, and calculate N ? N transitions.
(iii) InO((N?n)
2
?n) = O(N
2
?n
3
) we decode
the n?N grid using Viterbi (1967) decoding.
5 Experiments
Goal. We set out to evaluate the accuracy of a se-
mantic parser for requirements documents, in the
two modes of analysis presented above. Our eval-
uation methodology is as standardly assumed in
machine learning and NLP: given a set of anno-
tated examples ? that is, given a set of require-
ments documents, where each requirement is an-
notated with its correct LSC representation and
each document is associated with a complete SM
? we partition this set into a training set and a test
set that are disjoint. We train our statistical model
on the examples in the training set and automati-
cally analyze the requirements in the test set. We
then compare the predicted semantic analyses of
the test set with the human-annotated (henceforth,
gold) semantic analyses of this test set, and empir-
ically quantify our prediction accuracy.
Metrics. Our semantic LSC objects have the
form of a tree (reflecting the sequence of nested
events in our scenarios). Therefore, we can use
standard tree evaluation metrics, such as ParseE-
val (Black et al., 1992), to evaluate the accuracy
of the prediction. Overall, we define three metrics
to evaluate the accuracy of the LSC trees:
POS: the POS metric is the percentage of
part-of-speech tags predicted correctly.
LSC-F1: F1 is the harmonic means of the
precision and recall of the predicted tree.
LSC-EM: EM is 1 if the predicted tree is an
exact match to the gold tree, and 0 otherwise.
In the case of SM trees, as opposed to the LSC
trees, we cannot assume identity of the yield be-
tween the gold and parse trees for the same sen-
System #Scenarios avg sentence length
Phone 21 24.33
WristWatch 15 29.8
Chess 18 15.83
Baby Monitor 14 20
Total 68 22.395
Table 2: Seed Gold-Annotated Requirements
N=1 POS LSC-F1 LSC-EM SM-TED SM-EM
Gen-Only 85.52 84.40 9.52 84.25 9.52
Gen+Seed 91.59 88.05 14.29 85.17 14.29
Table 3: Sentence-Based modeling: Accuracy re-
sults on the Phone development set.
tence,
6
so we cannot use ParseEval. Therefore, we
implement a distance-based metrics in the spirit of
Tsarfaty et al. (2012). Then, to evaluate the accu-
racy of the SM, we use two kinds of scores:
SM-TED: TED is the normalized edit dis-
tance between the predicted and gold SM
trees, subtracted from a unity.
SM-EM: EM is 1 if the predicted SM is an
exact match with the gold SM, 0 otherwise.
Data. We have a small seed of correctly anno-
tated requirements-specification case studies that
describe simple reactive systems in the LSC lan-
guage. Each document contains a sequence of
requirements, each of which is annotated with
the correct LSC diagram. The entire program is
grounded in a java implementation. As training
data, we use the case studies provided by Gordon
and Harel (2009). Table 2 lists the case studies and
basic statistics concerning these data.
As our annotated seed is quite small, it is hard to
generalize from it to unseen examples. In particu-
lar, we are not guaranteed to have observed all pos-
sible structures that are theoretically permitted by
the assumed grammar. To cope with this, we cre-
ate a synthetic set of examples using the grammar
of Gordon and Harel (2009) in generation mode,
and randomly generate trees t ? T
req
.
The grammar we use to generate the synthetic
examples clearly over-generates. That is to say,
it creates many trees that do not have a sound in-
terpretation. In fact, only 3000 our of 10000 gen-
erated examples turn out to have a sound seman-
tic interpretation grounded in an SM. Nonetheless,
these data allow us to smooth the syntactic distri-
butions that are observed in the seed, and increase
the coverage of the grammar learned from it.
6
This is because the LSC trees are predicted bottom up
and the SM trees are predicted top-down.
1303
Results. Table 3 presents the results for pars-
ing the Phone document, our development set,
with the sentence-based model, varying the train-
ing data. We see that despite the small size of the
seed, adding it to our set if synthetics examples
substantially improves results over a model trained
on synthetic examples only.
In our next experiment, we provide empirical
upper-bounds and lower-bounds for the discourse-
based model. Table 4 presents the results of
the discourse-based model for N > 1 on the
Phone example. Gen-Only presents the results of
the discourse-based model with a PCFG learned
from synthetic trees only, incorporating transitions
obeying the max-overlap assumption. Already
here, we see a mild improvement for N > 1 rel-
ative to the N = 1 results, indicating that even a
weak signal such as the overlap between neighbor-
ing sentences already improves sentence disam-
biguation in context. We next present the results of
an Oracle experiment, where every requirement is
assigned the highest scoring tree in terms of LSC-
F1 with respect to the gold tree, keeping the same
transitions. Again we see that results improve with
N , indicating that the syntactic model alone does
not provide optimal disambiguation. These re-
sults provides an upper bound on the parser perfor-
mance for each N . Gen+Seed presents results of
the discourse-based model where the PCFG inter-
polates the seed set and the synthetic train set, with
max-overlap transitions. Here, we see larger im-
provements over the synthetic-only PCFG. That is,
modeling grammaticality of individual sentences
improves the interpretation of the document.
Next we compare the performance for differ-
ent implementations of the gap(m
i
,m
j
) function.
We estimate probability distributions that reflect
each of the assumptions we discussed, and add
an additional method called hybrid, in which we
interpolate the max-expansion and max-overlap
estimates (equal weights). In Table 5, the trend
from the previous experiment persists. Notably,
the hybrid model provides a larger error reduc-
tion than its components used separately, indicat-
ing that in order to capture discourse context we
may need to balance possibly conflicting factors.
In no emissions we rely solely on the probability
of state transitions, and again increasing N leads
to improvement. This result confirms that con-
text is indispensable for sentence interpretation ?
even when probabilities for the sentence?s seman-
System N=2 4 8 16 32 64 128
Gen-Only
POS 85.52 86.30 87.67 88.45 88.85 88.85 88.85
LSC-F1 84.40 85.35 86.31 87.51 88.81 89.30 89.51
LSC-EM 9.52 9.52 14.29 14.29 14.29 14.29 14.29
SM-TED 84.25 85.94 89.14 91.90 92.81 93.31 92.70
SM-EM 9.52 19.05 33.33 33.33 33.33 38.10 33.33
Gen+Seed
POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73
LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86
SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76
SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38
Oracle
POS 91.98 93.54 94.91 95.30 96.09 96.67 96.87
LSC-F1 88.73 91.33 93.19 94.39 95.11 95.91 96.70
LSC-EM 23.81 42.86 61.90 61.90 66.67 76.19 76.19
SM-TED 86.54 91.28 94.28 94.88 96.24 97.51 98.80
SM-EM 23.81 42.86 66.67 71.43 76.19 76.19 76.19
Table 4: Discourse-Based Modeling: Accuracy re-
sults on the Phone dev set. The Oracle selects the
highest scoring LSC tree among the N-candidates,
providing an upper bound on accuracy. Gen-Only
selects the most probable tree, relying on synthetic
examples only, providing a lower bound.
tics (content) are entirely absent.
We finally perform a cross-fold experiment in
which we leave one document out as a test set
and take the rest as our seed. The results are pro-
vided in Table 6. The discourse-based model out-
performs the sentence-based model N = 1 in all
cases. Moreover, the drop in N = 128 for Phone
seems incidental to this set, and the other cases
level off beforehand. Despite our small seed, the
persistent improvement on all metrics is consistent
with our hypothesis that modeling the interpreta-
tion process within the discourse has substantial
benefits for automatic understanding of the text.
6 Applications and Discussion
The statistical models we present here are ap-
plied in the context of PlayGo,
7
a comprehensive
tool for behavioral, scenario-based, programming.
PlayGo now provides two modes of playing-in
natural language requirements: interactive play-in,
where a user manually disambiguates the analyses
of the requirements (Gordon and Harel, 2009), and
statistical play-in, where disambiguation decisions
are taken using our discourse-based model.
The fragment of English we use is very ex-
pressive. It covers not only entities and predi-
cates, but also temporal and aspectual information,
modalities, and program flow. Beyond that, we as-
sume an open-ended lexicon. Overall, we are not
7
www.playgo.co.
1304
Transitions N=2 4 8 16 32 64 128
Min Dist
POS 91.98 92.76 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.39 89.77 91.00 90.99 91.81 92.09 91.73
LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62
SM-TED 86.54 91.71 94.38 93.81 95.57 96.43 94.53
SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14
Max Overlap
POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73
LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86
SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76
SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38
Max Expand
POS 91.98 92.76 93.74 93.54 94.32 94.52 93.93
LSC-F1 88.39 89.71 91.00 90.99 91.68 91.96 91.60
LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62
SM-TED 86.54 91.93 93.75 93.18 94.79 95.66 93.75
SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14
Hybrid
POS 91.78 92.95 93.93 93.74 94.72 94.91 94.32
LSC-F1 88.11 90.18 91.34 91.33 92.15 92.42 92.07
LSC-EM 19.05 38.10 47.62 47.62 47.62 47.62 47.62
SM-TED 85.49 90.78 93.66 93.09 94.87 95.75 93.83
SM-EM 19.05 38.10 57.14 57.14 57.14 57.14 57.14
No Emissions
POS 91.78 91.98 92.37 92.37 92.17 92.76 93.15
LSC-F1 88.11 88.79 89.12 89.12 89.39 89.67 89.89
LSC-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81
SM-TED 85.49 85.74 85.82 85.82 85.87 86.85 86.92
SM-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81
Table 5: Discourse-Based modeling: Experiments
on the Phone development set. Estimation proce-
dure for transition probabilities. All experiments
use the Gen+Seed emission probablities.
only translating English sentences into executable
LSCs ? we provide a fully generative model for
translating a complete document (text) into a com-
plete system model (code).
This text-to-code problem may be thought of as
a machine translation (MT) problem, where one
aims to translate sentences in English to the formal
language of LSCs. However, standard statistical
MT techniques rely on the assumption that textual
requirements and code are aligned at a sentence
level. Creating a formal model that aligns text and
code on a sentence-by-sentence basis is precisely
our technical contribution in Section 3.
To our knowledge, modeling syntax and dis-
course processing via a fully joint generative
model, where a discourse level HMM is in-
terleaved with PCFG sentence-based emissions,
is novel. By plugging in different models for
p(d|m), different languages may be parsed. This
method may further be utilized for relating content
and context in other tasks: parsing and document-
level NER, parsing and document-level IE, etc. To
do so, one only needs to redefine the PCFG (emis-
sions) and state-overlap (transition) parameters, as
appropriate for their data.
8
8
Our code, annotated data, four case studies, and the LSC
Data Set N=1 32 64 128
Baby Monitor
POS 94.29 96.07 96.07 96.07
LSC-F1 91.50 94.96 94.96 94.96
LSC-EM 14.29 21.43 21.43 21.43
SM-TED 88.63 91.11 91.11 91.11
SM-EM 28.57 50.00 50.00 50.00
Chess
POS 92.63 93.68 93.68 93.68
LSC-F1 95.79 96.16 96.16 96.16
LSC-EM 5.56 11.11 11.11 11.11
SM-TED 94.90 97.10 97.10 97.10
SM-EM 61.11 66.67 66.67 66.67
Phone
POS 91.59 94.72 94.91 94.32
LSC-F1 88.05 92.15 92.42 92.07
LSC-EM 14.29 47.62 47.62 47.62
SM-TED 85.17 94.87 95.75 93.83
SM-EM 14.29 57.14 57.14 57.14
WristWatch
POS 34.23 34.45 34.45 34.45
LSC-F1 50.06 51.05 51.05 51.05
LSC-EM 26.67 26.67 26.67 26.67
SM-TED 71.15 72.73 72.73 72.73
SM-EM 26.67 33.33 33.33 33.33
Table 6: Cross-Fold Validation for N=1..128.
Seed+Generated emissions, Hybrid transitions.
7 Conclusion
The requirements understanding task presents an
exciting challenge for CL/NLP. We ought to au-
tomatically discover the entities in the discourse,
the actions they take, conditions, temporal con-
straints, and execution modalities. Furthermore, it
requires us to extract a single ontology that satis-
fies all individual requirements. The contributions
of this paper are three-fold: we formalize the text-
to-code prediction task, propose a semantic rep-
resentation with well-defined grounding, and em-
pirically evaluate models for this prediction. We
show consistent improvement of discourse-based
over sentence-based models, in all case studies.
In the future, we intend to extend this model for
interpreting requirements in un-restricted, or less-
restricted, English, endowed with a more sophisti-
cated discourse interpretation function.
Acknowledgements
We thank Shahar Maoz, Rami Marelly, Yoav
Goldberg and three anonymous reviewers for
their insightful comments on an earlier draft.
This research was supported by an Advanced
Research Grant to D. Harel from the Euro-
pean Research Council (ERC) under the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013), and by a grant to D.
Harel from the Israel Science Foundation (ISF).
visual editor are available via http://wiki.weizmann.
ac.il/playgo/index.php/Download_PlayGo.
1305
References
B. C. Arnold and D. Strauss. 1991. Pseudolikelihood
Estimation: Some Examples. Sankhy?a: The Indian
Journal of Statistics, Series B (1960-2002), 53(2).
Y. Artzi and L. Zettlemoyer. 2013. Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. TACL, 1:49?62.
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A
maximum likelihood approach to continuous speech
recognition. IEEE Trans. Pattern Anal. Mach. In-
tell., 5(2):179?190.
E. Black, J. D. Lafferty, and S. Roukos. 1992. De-
velopment and evaluation of a broad-coverage prob-
abilistic grammar of English-language computer
manuals. In Proceedings of ACL, pages 185?192.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Comput.
Linguist., 19(2):263?311, June.
B. Bryant and B.-S. Lee. 2002. Two-level gram-
mar as an object-oriented requirements specifica-
tion language. In Proceedings of the 35th Annual
Hawaii International Conference on System Sci-
ences (HICSS?02)-Volume 9 - Volume 9, HICSS ?02,
pages 280?, Washington, DC, USA. IEEE Computer
Society.
E. Charniak. 1996. Tree-bank grammars. In Proceed-
ings of the Thirteenth National Conference on Arti-
ficial Intelligence, pages 1031?1036.
W. Damm and D. Harel. 2001. LSCs: Breathing life
into message sequence charts. Form. Methods Syst.
Des., 19(1):45?80, July.
N. Eitan, M. Gordon, D. Harel, A. Marron, and
G. Weiss. 2011. On visualization and compre-
hension of scenario-based programs. In Proceed-
ings of the 2011 IEEE 19th International Conference
on Program Comprehension, ICPC ?11, pages 189?
192, Washington, DC, USA. IEEE Computer Soci-
ety.
N. E. Fuchs and R. Schwitter. 1995. Attempto: Con-
trolled natural language for requirements specifica-
tions. In Markus P. J. Fromherz, Marc Kirschen-
baum, and Anthony J. Kusalik, editors, LPE.
M. Gordon and D. Harel. 2009. Generating executable
scenarios from natural language. In Proceedings of
the 10th International Conference on Computational
Linguistics and Intelligent Text Processing, CICLing
?09, pages 456?467, Berlin, Heidelberg. Springer-
Verlag.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics:
Vol. 3: Speech Acts, pages 41?58. Academic Press,
San Diego, CA.
D. Harel and M. Gordon. 2009. On teaching visual
formalisms. IEEE Softw., 26(3):87?95, May.
D. Harel and S. Maoz. 2006. Assert and negate revis-
ited: Modal semantics for UML sequence diagrams.
In Proceedings of the 2006 International Workshop
on Scenarios and State Machines: Models, Algo-
rithms, and Tools, SCESM ?06, pages 13?20, New
York, NY, USA. ACM.
D. Harel and R. Marelly. 2003. Come, Let?s Play:
Scenario-Based Programming Using LSCs and the
Play-Engine. Springer-Verlag New York, Inc., Se-
caucus, NJ, USA.
D. Harel, H. Kugler, R. Marelly, and A. Pnueli. 2002.
Smart play-out of behavioral requirements. In Pro-
ceedings of the 4th International Conference on For-
mal Methods in Computer-Aided Design, FMCAD
?02, pages 378?398, London, UK. Springer-Verlag.
D. Harel, A. Marron, and G. Weiss. 2012. Behavioral
programming. Commun. ACM, 55(7):90?100, July.
D. Harel, A. Kantor, G. Katz, A. Marron, L. Mizrahi,
and G. Weiss. 2013. On composing and proving
the correctness of reactive behavior. In Embedded
Software (EMSOFT), 2013 Proceedings of the Inter-
national Conference on, pages 1?10, Sept.
D. Harel. 2001. From play-in scenarios to code: An
achievable dream. Computer, 34(1):53?60, January.
H. Kugler, D. Harel, A. Pnueli, Y. Lu, and Y. Bon-
temps. 2005. Temporal logic for scenario-based
specifications. In Proceedings of the 11th In-
ternational Conference on Tools and Algorithms
for the Construction and Analysis of Systems,
TACAS?05, pages 445?460, Berlin, Heidelberg.
Springer-Verlag.
T. Kuhn. 2014. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics, 40(1):121?170.
T. Lei, F. Long, R. Barzilay, and M. C. Rinard. 2013.
From natural language specifications to program in-
put parsers. In ACL (1), pages 1294?1303.
P. Liang and C. Potts. 2014. Bringing machine learn-
ing and compositional semantics together. Annual
Reviews of Linguistics (submitted), 0.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590?599.
T. Parsons. 1990. Events in the Semantics of English:
A study in subatomic semantics. MIT Press, Cam-
bridge, MA.
C. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379?
423, 623?656, July, October.
1306
R. Tsarfaty, J. Nivre, and E. Andersson. 2012. Cross-
framework evaluation for statistical parsing. In
W. Daelemans, M. Lapata, and L. M`arquez, editors,
Proceedings of EACL, pages 44?54. The Associa-
tion for Computer Linguistics.
A. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Trans. Inf. Theor.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI,
pages 658?666. AUAI Press.
1307
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 44?54,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Cross-Framework Evaluation for Statistical Parsing
Reut Tsarfaty Joakim Nivre Evelina Andersson
Uppsala University, Box 635, 75126 Uppsala, Sweden
tsarfaty@stp.lingfil.uu.se,{joakim.nivre,evelina.andersson}@lingfil.uu.se
Abstract
A serious bottleneck of comparative parser
evaluation is the fact that different parsers
subscribe to different formal frameworks
and theoretical assumptions. Converting
outputs from one framework to another is
less than optimal as it easily introduces
noise into the process. Here we present a
principled protocol for evaluating parsing
results across frameworks based on func-
tion trees, tree generalization and edit dis-
tance metrics. This extends a previously
proposed framework for cross-theory eval-
uation and allows us to compare a wider
class of parsers. We demonstrate the useful-
ness and language independence of our pro-
cedure by evaluating constituency and de-
pendency parsers on English and Swedish.
1 Introduction
The goal of statistical parsers is to recover a for-
mal representation of the grammatical relations
that constitute the argument structure of natural
language sentences. The argument structure en-
compasses grammatical relationships between el-
ements such as subject, predicate, object, etc.,
which are useful for further (e.g., semantic) pro-
cessing. The parses yielded by different parsing
frameworks typically obey different formal and
theoretical assumptions concerning how to rep-
resent the grammatical relationships in the data
(Rambow, 2010). For example, grammatical rela-
tions may be encoded on top of dependency arcs
in a dependency tree (Mel?c?uk, 1988), they may
decorate nodes in a phrase-structure tree (Marcus
et al 1993; Maamouri et al 2004; Sima?an et
al., 2001), or they may be read off of positions in
a phrase-structure tree using hard-coded conver-
sion procedures (de Marneffe et al 2006). This
diversity poses a challenge to cross-experimental
parser evaluation, namely: How can we evaluate
the performance of these different parsers relative
to one another?
Current evaluation practices assume a set of
correctly annotated test data (or gold standard)
for evaluation. Typically, every parser is eval-
uated with respect to its own formal representa-
tion type and the underlying theory which it was
trained to recover. Therefore, numerical scores
of parses across experiments are incomparable.
When comparing parses that belong to different
formal frameworks, the notion of a single gold
standard becomes problematic, and there are two
different questions we have to answer. First, what
is an appropriate gold standard for cross-parser
evaluation? And secondly, how can we alle-
viate the differences between formal representa-
tion types and theoretical assumptions in order to
make our comparison sound ? that is, to make sure
that we are not comparing apples and oranges?
A popular way to address this has been to
pick one of the frameworks and convert all
parser outputs to its formal type. When com-
paring constituency-based and dependency-based
parsers, for instance, the output of constituency
parsers has often been converted to dependency
structures prior to evaluation (Cer et al 2010;
Nivre et al 2010). This solution has vari-
ous drawbacks. First, it demands a conversion
script that maps one representation type to another
when some theoretical assumptions in one frame-
work may be incompatible with the other one.
In the constituency-to-dependency case, some
constituency-based structures (e.g., coordination
44
and ellipsis) do not comply with the single head
assumption of dependency treebanks. Secondly,
these scripts may be labor intensive to create, and
are available mostly for English. So the evalua-
tion protocol becomes language-dependent.
In Tsarfaty et al(2011) we proposed a gen-
eral protocol for handling annotation discrepan-
cies when comparing parses across different de-
pendency theories. The protocol consists of three
phases: converting all structures into function
trees, for each sentence, generalizing the different
gold standard function trees to get their common
denominator, and employing an evaluation mea-
sure based on tree edit distance (TED) which dis-
cards edit operations that recover theory-specific
structures. Although the protocol is potentially
applicable to a wide class of syntactic represen-
tation types, formal restrictions in the procedures
effectively limit its applicability only to represen-
tations that are isomorphic to dependency trees.
The present paper breaks new ground in the
ability to soundly compare the accuracy of differ-
ent parsers relative to one another given that they
employ different formal representation types and
obey different theoretical assumptions. Our solu-
tion generally confines with the protocol proposed
in Tsarfaty et al(2011) but is re-formalized to
allow for arbitrary linearly ordered labeled trees,
thus encompassing constituency-based as well as
dependency-based representations. The frame-
work in Tsarfaty et al(2011) assumes structures
that are isomorphic to dependency trees, bypass-
ing the problem of arbitrary branching. Here we
lift this restriction, and define a protocol which
is based on generalization and TED measures to
soundly compare the output of different parsers.
We demonstrate the utility of this protocol by
comparing the performance of different parsers
for English and Swedish. For English, our
parser evaluation across representation types al-
lows us to analyze and precisely quantify previ-
ously encountered performance tendencies. For
Swedish we show the first ever evaluation be-
tween dependency-based and constituency-based
parsing models, all trained on the Swedish tree-
bank data. All in all we show that our ex-
tended protocol, which can handle linearly-
ordered labeled trees with arbitrary branch-
ing, can soundly compare parsing results across
frameworks in a representation-independent and
language-independent fashion.
2 Preliminaries: Relational Schemes for
Cross-Framework Parse Evaluation
Traditionally, different statistical parsers have
been evaluated using specially designated evalu-
ation measures that are designed to fit their repre-
sentation types. Dependency trees are evaluated
using attachment scores (Buchholz and Marsi,
2006), phrase-structure trees are evaluated using
ParsEval (Black et al 1991), LFG-based parsers
postulate an evaluation procedure based on f-
structures (Cahill et al 2008), and so on. From a
downstream application point of view, there is no
significance as to which formalism was used for
generating the representation and which learning
methods have been utilized. The bottom line is
simply which parsing framework most accurately
recovers a useful representation that helps to un-
ravel the human-perceived interpretation.
Relational schemes, that is, schemes that en-
code the set of grammatical relations that con-
stitute the predicate-argument structures of sen-
tences, provide an interface to semantic interpre-
tation. They are more intuitively understood than,
say, phrase-structure trees, and thus they are also
more useful for practical applications. For these
reasons, relational schemes have been repeatedly
singled out as an appropriate level of representa-
tion for the evaluation of statistical parsers (Lin,
1995; Carroll et al 1998; Cer et al 2010).
The annotated data which statistical parsers are
trained on encode these grammatical relationships
in different ways. Dependency treebanks provide
a ready-made representation of grammatical rela-
tions on top of arcs connecting the words in the
sentence (Ku?bler et al 2009). The Penn Tree-
bank and phrase-structure annotated resources en-
code partial information about grammatical rela-
tions as dash-features decorating phrase structure
nodes (Marcus et al 1993). Treebanks like Tiger
for German (Brants et al 2002) and Talbanken
for Swedish (Nivre and Megyesi, 2007) explic-
itly map phrase structures onto grammatical rela-
tions using dedicated edge labels. The Relational-
Realizational structures of Tsarfaty and Sima?an
(2008) encode relational networks (sets of rela-
tions) projected and realized by syntactic cate-
gories on top of ordinary phrase-structure nodes.
Function trees, as defined in Tsarfaty et al
(2011), are linearly ordered labeled trees in which
every node is labeled with the grammatical func-
45
(a) -ROOT- John loves Marysbj objroot ? root
sbj
John
hd
loves
obj
Mary
(b) S-root
NP-sbj
NN-hd
John
VP-prd
V-hd
loves
NP-obj
NN-hd
Mary
? root
sbj
hd
John
prd
hd
loves
obj
hd
Mary
(c) S
{sbj,prd,obj}
sbj
NP
{hd}
hd
NN
John
prd
V
loves
obj
NP
{hd}
hd
NN
Mary
? root
sbj
hd
John
prd
loves
obj
hd
Mary
Figure 1: Deterministic conversion into function trees.
The algorithm for extracting a function tree from a de-
pendency tree as in (a) is provided in Tsarfaty et al
(2011). For a phrase-structure tree as in (b) we can re-
place each node label with its function (dash-feature).
In a relational-realizational structure like (c) we can re-
move the projection nodes (sets) and realization nodes
(phrase labels), which leaves the function nodes intact.
tion of the dominated span. Function trees ben-
efit from the same advantages as other relational
schemes, namely that they are intuitive to under-
stand, they provide the interface for semantic in-
terpretation, and thus may be useful for down-
stream applications. Yet they do not suffer from
formal restrictions inherent in dependency struc-
tures, for instance, the single head assumption.
For many formal representation types there ex-
ists a fully deterministic, heuristics-free, proce-
dure mapping them to function trees. In Figure 1
we illustrate some such procedures for a simple
transitive sentence. Now, while all the structures
at the right hand side of Figure 1 are of the same
formal type (function trees), they have different
tree structures due to different theoretical assump-
tions underlying the original formal frameworks.
(t1) root
f1
f2
w
(t2) root
f2
f1
w
(t3) root
{f1,f2}
w
Figure 2: Unary chains in function trees
Once we have converted framework-specific
representations into function trees, the problem of
cross-framework evaluation can potentially be re-
duced to a cross-theory evaluation following Tsar-
faty et al(2011). The main idea is that once
all structures have been converted into function
trees, one can perform a formal operation called
generalization in order to harmonize the differ-
ences between theories, and measure accurately
the distance of parse hypotheses from the gener-
alized gold. The generalization operation defined
in Tsarfaty et al(2011), however, cannot handle
trees that may contain unary chains, and therefore
cannot be used for arbitrary function trees.
Consider for instance (t1) and (t2) in Figure 2.
According to the definition of subsumption in
Tsarfaty et al(2011), (t1) is subsumed by (t2)
and vice versa, so the two trees should be identi-
cal ? but they are not. The interpretation we wish
to give to a function tree such as (t1) is that the
word w has both the grammatical function f1 and
the grammatical function f2. This can be graphi-
cally represented as a set of labels dominating w,
as in (t3). We call structures such as (t3) multi-
function trees. In the next section we formally de-
fine multi-function trees, and then use them to de-
velop our protocol for cross-framework and cross-
theory evaluation.
3 The Proposal: Cross-Framework
Evaluation with Multi-Function Trees
Our proposal is a three-phase evaluation proto-
col in the spirit of Tsarfaty et al(2011). First,
we obtain a formal common ground for all frame-
works in terms of multi-function trees. Then we
obtain a theoretical common ground by means
of tree-generalization on gold trees. Finally, we
calculate TED-based scores that discard the cost
of annotation-specific edits. In this section, we
define multi-function trees and update the tree-
generalization and TED-based metrics to handle
multi-function trees that reflect different theories.
46
Figure 3: The Evaluation Protocol. Different formal frameworks yield different parse and gold formal types.
All types are transformed into multi-function trees. All gold trees enter generalization to yield a new gold for
each sentence. The different ? arcs represent the different edit scripts used for calculating the TED-based scores.
3.1 Defining Multi-Function Trees
An ordinary function tree is a linearly ordered tree
T = (V,A) with yield w1, ..., wn, where internal
nodes are labeled with grammatical function la-
bels drawn from some set L. We use span(v)
and label(v) to denote the yield and label, respec-
tively, of an internal node v. A multi-function tree
is a linearly ordered tree T = (V,A) with yield
w1, ..., wn, where internal nodes are labeled with
sets of grammatical function labels drawn from L
and where v 6= v? implies span(v) 6= span(v?)
for all internal nodes v, v?. We use labels(v) to
denote the label set of an internal node v.
We interpret multi-function trees as encoding
sets of functional constraints over spans in func-
tion trees. Each node v in a multi-function tree
represents a constraint of the form: for each
l ? labels(v), there should be a node v? in the
function tree such that span(v) = span(v?) and
label(v?) = l. Whenever we have a conversion for
function trees, we can efficiently collapse them
into multi-function trees with no unary produc-
tions, and with label sets labeling their nodes.
Thus, trees (t1) and (t2) in Figure 2 would both
be mapped to tree (t3), which encodes the func-
tional constraints encoded in either of them.
For dependency trees, we assume the conver-
sion to function trees defined in Tsarfaty et al
(2011), where head daughters always get the la-
bel ?hd?. For PTB style phrase-structure trees, we
replace the phrase-structure labels with functional
dash-features. In relational-realization structures
we remove projection and realization nodes. De-
terministic conversions exist also for Tiger style
treebanks and frameworks such as LFG, but we
do not discuss them here.1
1All the conversions we use are deterministic and are
defined in graph-theoretic and language-independent terms.
We make them available at http://stp.lingfil.uu.
se/?tsarfaty/unipar/index.html.
3.2 Generalizing Multi-Function Trees
Once we obtain multi-function trees for all the
different gold standard representations in the sys-
tem, we feed them to a generalization operation
as shown in Figure 3. The goal of this opera-
tion is to provide a consensus gold standard that
captures the linguistic structure that the different
gold theories agree on. The generalization struc-
tures are later used as the basis for the TED-based
evaluation. Generalization is defined by means of
subsumption. A multi-function tree subsumes an-
other one if and only if all the constraints defined
by the first tree are also defined by the second tree.
So, instead of demanding equality of labels as in
Tsarfaty et al(2011), we demand set inclusion:
T-Subsumption, denoted vt, is a relation
between multi-function trees that indicates
that a tree pi1 is consistent with and more
general than tree pi2. Formally: pi1 vt pi2
iff for every node n ? pi1 there exists a node
m ? pi2 such that span(n) = span(m) and
labels(n) ? labels(m).
T-Unification, denoted unionsqt, is an operation
that returns the most general tree structure
that contains the information from both input
trees, and fails if such a tree does not exist.
Formally: pi1 unionsqt pi2 = pi3 iff pi1 vt pi3 and
pi2 vt pi3, and for all pi4 such that pi1 vt pi4
and pi2 vt pi4 it holds that pi3 vt pi4.
T-Generalization, denoted ut, is an opera-
tion that returns the most specific tree that
is more general than both trees. Formally,
pi1utpi2 = pi3 iff pi3 vt pi1 and pi3 vt pi2, and
for every pi4 such that pi4 vt pi1 and pi4 vt pi2
it holds that pi4 vt pi3.
The generalization tree contains all nodes that ex-
ist in both trees, and for each node it is labeled by
47
the intersection of the label sets dominating the
same span in both trees. The unification tree con-
tains nodes that exist in one tree or another, and
for each span it is labeled by the union of all label
sets for this span in either tree. If we generalize
two trees and one tree has no specification for la-
bels over a span, it does not share anything with
the label set dominating the same span in the other
tree, and the label set dominating this span in the
generalized tree is empty. If the trees do not agree
on any label for a particular span, the respective
node is similarly labeled with an empty set. When
we wish to unify theories, then an empty set over
a span is unified with any other set dominating the
same span in the other tree, without altering it.
Digression: Using Unification to Merge Infor-
mation From Different Treebanks In Tsarfaty
et al(2011), only the generalization operation
was used, providing the common denominator of
all the gold structures and serving as a common
ground for evaluation. The unification operation
is useful for other NLP tasks, for instance, com-
bining information from two different annotation
schemes or enriching one annotation scheme with
information from a different one. In particular,
we can take advantage of the new framework to
enrich the node structure reflected in one theory
with grammatical functions reflected in an anno-
tation scheme that follows a different theory. To
do so, we define the Tree-Labeling-Unification
operation on multi-function trees.
TL-Unification, denoted unionsqtl, is an opera-
tion that returns a tree that retains the struc-
ture of the first tree and adds labels that ex-
ist over its spans in the second tree. For-
mally: pi1 unionsqtl pi2 = pi3 iff for every node
n ? pi1 there exists a node m ? pi3 such
that span(m) = span(n) and labels(m) =
labels(n) ? labels(pi2, span(n)).
Where labels(pi2, span(n)) is the set of labels of
the node with yield span(n) in pi2 if such a node
exists and ? otherwise. We further discuss the TL-
Unification and its use for data preparation in ?4.
3.3 TED Measures for Multi-Function Trees
The result of the generalization operation pro-
vides us with multi-function trees for each of the
sentences in the test set representing sets of con-
straints on which the different gold theories agree.
We would now like to use distance-based met-
rics in order to measure the gap between the gold
and predicted theories. The idea behind distance-
based evaluation in Tsarfaty et al(2011) is that
recording the edit operations between the native
gold and the generalized gold allows one to dis-
card their cost when computing the cost of a parse
hypothesis turned into the generalized gold. This
makes sure that different parsers do not get penal-
ized, or favored, due to annotation specific deci-
sions that are not shared by other frameworks.
The problem is now that TED is undefined with
respect to multi-function trees because it cannot
handle complex labels. To overcome this, we
convert multi-function trees into sorted function
trees, which are simply function trees in which
any label set is represented as a unary chain of
single-labeled nodes, and the nodes are sorted ac-
cording to the canonical order of their labels.2 In
case of an empty set, a 0-length chain is created,
that is, no node is created over this span. Sorted
function trees prevent reordering nodes in a chain
in one tree to fit the order in another tree, since it
would violate the idea that the set of constraints
over a span in a multi-function tree is unordered.
The edit operations we assume are add-
node(l, i, j) and delete-node(l, i, j) where l ? L
is a grammatical function label and i < j define
the span of a node in the tree. Insertion into a
unary chain must confine with the canonical order
of the labels. Every operation is assigned a cost.
An edit script is a sequence of edit operations that
turns a function tree pi1 into pi2, that is:
ES(pi1, pi2) = ?e1, . . . , ek?
Since all operations are anchored in spans, the se-
quence can be determined to have a unique order
of traversing the tree (say, DFS). Different edit
scripts then only differ in their set of operations
on spans. The edit distance problem is finding the
minimal cost script, that is, one needs to solve:
ES?(pi1, pi2) = min
ES(pi1,pi2)
?
e?ES(pi1,pi2)
cost(e)
In the current setting, when using only add and
delete operations on spans, there is only one edit
script that corresponds to the minimal edit cost.
So, finding the minimal edit script entails finding
a single set of operations turning pi1 into pi2.
2The ordering can be alphabetic, thematic, etc.
48
We can now define ? for the ith framework, as
the error of parsei relative to its native gold stan-
dard goldi and to the generalized gold gen. This
is the edit cost minus the cost of the script turning
parsei into gen intersected with the script turning
goldi into gen. The underlying intuition is that
if an operation that was used to turn parsei into
gen is used to discard theory-specific information
from goldi, its cost should not be counted as error.
?(parsei, goldi, gen) = cost(ES
?(parsei, gen))
?cost(ES?(parsei, gen) ? ES
?(goldi, gen))
In order to turn distance measures into parse-
scores we now normalize the error relative to the
size of the trees and subtract it from a unity. So
the Sentence Score for parsing with framework i
is:
score(parsei, goldi, gen) =
1?
?(parsei, goldi,gen)
|parsei|+ |gen|
Finally, Test-Set Average is defined by macro-
avaraging over all sentences in the test-set:
1?
?|testset|
j=1 ?(parseij , goldij , genj)
?|testset|
j=1 |parseij |+ |genj |
This last formula represents the TEDEVAL metric
that we use in our experiments.
A Note on System Complexity Conversion of
a dependency or a constituency tree into a func-
tion tree is linear in the size of the tree. Our
implementation of the generalization and unifica-
tion operation is an exact, greedy, chart-based al-
gorithm that runs in polynomial time (O(n2) in
n the number of terminals). The TED software
that we utilize builds on the TED efficient algo-
rithm of Zhang and Shasha (1989) which runs in
O(|T1||T2|min(d1, n1)min(d2, n2)) time where
di is the tree degree (depth) and ni is the number
of terminals in the respective tree (Bille, 2005).
4 Experiments
We validate our cross-framework evaluation pro-
cedure on two languages, English and Swedish.
For English, we compare the performance of
two dependency parsers, MaltParser (Nivre et al
2006) and MSTParser (McDonald et al 2005),
and two constituency-based parsers, the Berkeley
parser (Petrov et al 2006) and the Brown parser
(Charniak and Johnson, 2005). All experiments
use Penn Treebank (PTB) data. For Swedish,
we compare MaltParser and MSTParser with two
variants of the Berkeley parser, one trained on
phrase structure trees, and one trained on a vari-
ant of the Relational-Realizational representation
of Tsarfaty and Sima?an (2008). All experiments
use the Talbanken Swedish Treebank (STB) data.
4.1 English Cross-Framework Evaluation
We use sections 02?21 of the WSJ Penn Tree-
bank for training and section 00 for evaluation and
analysis. We use two different native gold stan-
dards subscribing to different theories of encoding
grammatical relations in tree structures:
? THE DEPENDENCY-BASED THEORY is the
theory encoded in the basic Stanford Depen-
dencies (SD) scheme. We obtain the set of
basic stanford dependency trees using the
software of de Marneffe et al(2006) and
train the dependency parsers directly on it.
? THE CONSTITUENCY-BASED THEORY is
the theory reflected in the phrase-structure
representation of the PTB (Marcus et al
1993) enriched with function labels compat-
ible with the Stanford Dependencies (SD)
scheme. We obtain trees that reflect this
theory by TL-Unification of the PTB multi-
function trees with the SD multi-function
trees (PTBunionsqtlSD) as illustrated in Figure 4.
The theory encoded in the multi-function trees
corresponding to SD is different from the one
obtained by our TL-Unification, as may be seen
from the difference between the flat SD multi-
function tree and the result of the PTBunionsqtlSD in
Figure 4. Another difference concerns coordina-
tion structures, encoded as binary branching trees
in SD and as flat productions in the PTBunionsqtlSD.
Such differences are not only observable but also
quantifiable, and using our redefined TED metric
the cross-theory overlap is 0.8571.
The two dependency parsers were trained using
the same settings as in Tsarfaty et al(2011), using
SVMTool (Gime?nez and Ma`rquez, 2004) to pre-
dict part-of-speech tags at parsing time. The two
constituency parsers were used with default set-
tings and were allowed to predict their own part-
of-speech tags. We report three different evalua-
tion metrics for the different experiments:
49
(PTB) S
NP
NN
John
VP
V
loves
NP
NN
Mary
?
John loves
Mary
? ?
?
John
?
?
loves
?
Mary
(SD) -ROOT- John loves Marysbj objroot ? root
sbj
John
hd
loves
obj
Mary
? {root}
{sbj}
John
{hd}
loves
{obj}
Mary
(PTB) unionsqtl (SD) = {root}
{sbj}
John
?
{hd}
loves
{obj}
Mary
Figure 4: Conversion of PTB and SD tree to multi-
function trees, followed by TL-Unification of the trees.
Note that some PTB nodes remain without an SD label.
? LAS/UAS (Buchholz and Marsi, 2006)
? PARSEVAL (Black et al 1991)
? TEDEVAL as defined in Section 3
We use LAS/UAS for dependency parsers that
were trained on the same dependency theory. We
use ParseEval to evaluate phrase-structure parsers
that were trained on PTB trees in which dash-
features and empty traces are removed. We
use our implementation of TEDEVAL to evaluate
parsing results across all frameworks under two
different scenarios:3 TEDEVAL SINGLE evalu-
ates against the native gold multi-function trees.
TEDEVAL MULTIPLE evaluates against the gen-
eralized (cross-theory) multi-function trees. Un-
labeled TEDEVAL scores are obtained by sim-
ply removing all labels from the multi-function
nodes, and using unlabeled edit operations. We
calculate pairwise statistical significance using a
shuffling test with 10K iterations (Cohen, 1995).
Tables 1 and 2 present the results of our cross-
framework evaluation for English Parsing. In the
left column of Table 1 we report ParsEval scores
for constituency-based parsers. As expected, F-
Scores for the Brown parser are higher than the
F-Scores of the Berkeley parser. F-Scores are
however not applicable across frameworks. In
the rightmost column of Table 1 we report the
LAS/UAS results for all parsers. If a parser yields
3Our TedEval software can be downloaded at
http://stp.lingfil.uu.se/?tsarfaty/
unipar/download.html.
a constituency tree, it is converted to and evalu-
ated on SD. Here we see that MST outperforms
Malt, though the differences for labeled depen-
dencies are insignificant. We also observe here a
familiar pattern from Cer et al(2010) and others,
where the constituency parsers significantly out-
perform the dependency parsers after conversion
of their output into dependencies.
The conversion to SD allows one to compare
results across formal frameworks, but not with-
out a cost. The conversion introduces a set of an-
notation specific decisions which may introduce
a bias into the evaluation. In the middle column
of Table 1 we report the TEDEVAL metrics mea-
sured against the generalized gold standard for all
parsing frameworks. We can now confirm that
the constituency-based parsers significantly out-
perform the dependency parsers, and that this is
not due to specific theoretical decisions which are
seen to affect LAS/UAS metrics (Schwartz et al
2011). For the dependency parsers we now see
that Malt outperforms MST on labeled dependen-
cies slightly, but the difference is insignificant.
The fact that the discrepancy in theoretical as-
sumptions between different frameworks indeed
affects the conversion-based evaluation procedure
is reflected in the results we report in Table 2.
Here the leftmost and rightmost columns report
TEDEVAL scores against the own native gold
(SINGLE) and the middle column against the gen-
eralized gold (MULTIPLE). Had the theories
for SD and PTBunionsqtlSD been identical, TEDEVAL
SINGLE and TEDEVAL MULTIPLE would have
been equal in each line. Because of theoretical
discrepancies, we see small gaps in parser perfor-
mance between these cases. Our protocol ensures
that such discrepancies do not bias the results.
4.2 Cross-Framework Swedish Parsing
We use the standard training and test sets of the
Swedish Treebank (Nivre and Megyesi, 2007)
with two gold standards presupposing different
theories:
? THE DEPENDENCY-BASED THEORY is the
dependency version of the Swedish Tree-
bank. All trees are projectivized (STB-Dep).
? THE CONSTITUENCY-BASED THEORY is
the standard Swedish Treebank with gram-
matical function labels on the edges of con-
stituency structures (STB).
50
Formalism PS Trees MF Trees Dep Trees
Theory PTB unionsqlt SD (PTB unionsqlt SD) SD
ut SD
Metrics PARSEVAL TEDEVAL ATTSCORES
MALT N/A U: 0.9525L: 0.9088
U: 0.8962
L: 0.8772
MST N/A U: 0.9549L: 0.9049
U: 0.9059
L: 0.8795
BERKELEY F-Scores0.9096
U: 0.9677
L: 0.9227
U: 0.9254
L: 0.9031
BROWN F-Scores0.9129
U: 0.9702
L: 0.9264
U: 0.9289
L: 0.9057
Table 1: English cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are highest in their column. Italic scores
are the highest for dependency parsers in their column.
Formalism PS Trees MF Trees Dep Trees
Theory PTB unionsqlt SD (PTB unionsqlt SD) SD
ut SD
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9525L: 0.9088
U: 0.9524
L: 0.9186
MST N/A U: 0.9549L: 0.9049
U: 0.9548
L: 0.9149
BERKELEY U: 0.9645L: 0.9271
U: 0.9677
L: 0.9227
U: 0.9649
L: 0.9324
BROWN U: 0.9667L: 0.9301
U: 9702
L: 9264
U: 0.9679
L: 0.9362
Table 2: English cross-framework evaluation: TEDE-
VAL scores against gold and generalized gold. Bold-
face scores are highest in their column. Italic scores
are highest for dependency parsers in their column.
Because there are no parsers that can out-
put the complete STB representation including
edge labels, we experiment with two variants of
this theory, one which is obtained by simply re-
moving the edge labels and keeping only the
phrase-structure labels (STB-PS) and one which
is loosely based on the Relational-Realizational
scheme of Tsarfaty and Sima?an (2008) but ex-
cludes the projection set nodes (STB-RR). RR
trees only add function nodes to PS trees, and
it holds that STB-PSutSTB-RR=STB-PS. The
overlap between the theories expressed in multi-
function trees originating from STB-Dep and
STB-RR is 0.7559. Our evaluation protocol takes
into account such discrepancies while avoiding
biases that may be caused due to these differences.
We evaluate MaltParser, MSTParser and two
versions of the Berkeley parser, one trained on
STB-PS and one trained on STB-RR. We use
predicted part-of-speech tags for the dependency
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics PARSEVAL TEDEVAL ATTSCORE
MALT N/A U: 0.9266L: 0.8225
U: 0.8298
L: 0.7782
MST N/A U: 0.9275L: 0.8121
U: 0.8438
L: 0.7824
BKLY/STB-RR F-Score0.7914
U: 0.9281
L: 0.7861 N/A
BKLY/STB-PS F-Score0.7855 N/A N/A
Table 3: Swedish cross-framework evaluation: Three
measures as applicable to the different schemes. Bold-
face scores are the highest in their column.
Formalism PS Trees MF Trees Dep Trees
Theory STB STB ut Dep Dep
Metrics TEDEVAL TEDEVAL TEDEVAL
SINGLE MULTIPLE SINGLE
MALT N/A U: 0.9266L: 0.8225
U: 0.9264
L: 0.8372
MST N/A U: 0.9275L: 0.8121
U: 0.9272
L: 0.8275
BKLY-STB-RR U: 0.9239L: 0.7946
U: 0.9281
L: 0.7861 N/A
Table 4: Swedish cross-framework evaluation: TEDE-
VAL scores against the native gold and the generalized
gold. Boldface scores are the highest in their column.
parsers, using the HunPoS tagger (Megyesi,
2009), but let the Berkeley parser predict its own
tags. We use the same evaluation metrics and pro-
cedures as before. Prior to evaluating RR trees
using ParsEval we strip off the added function
nodes. Prior to evaluating them using TedEval we
strip off the phrase-structure nodes.
Tables 3 and 4 summarize the parsing results
for the different Swedish parsers. In the leftmost
column of table 3 we present the constituency-
based evaluation measures. Interestingly, the
Berkeley RR instantiation performs better than
when training the Berkeley parser on PS trees.
These constituency-based scores however have a
limited applicability, and we cannot use them to
compare constituency and dependency parsers. In
the rightmost column of Table 3 we report the
LAS/UAS results for the two dependency parsers.
Here we see higher performance demonstrated by
MST on both labeled and unlabeled dependen-
cies, but the differences on labeled dependencies
are insignificant. Since there is no automatic pro-
cedure for converting bare-bone phrase-structure
Swedish trees to dependency trees, we cannot use
51
LAS/UAS to compare across frameworks, and we
use TEDEVAL for cross-framework evaluation.
Training the Berkeley parser on RR trees which
encode a mapping of PS nodes to grammatical
functions allows us to compare parse results for
trees belonging to the STB theory with trees obey-
ing the STB-Dep theory. For unlabeled TEDE-
VAL scores, the dependency parsers perform at the
same level as the constituency parser, though the
difference is insignificant. For labeled TEDEVAL
the dependency parsers significantly outperform
the constituency parser. When considering only
the dependency parsers, there is a small advantage
for Malt on labeled dependencies, and an advan-
tage for MST on unlabeled dependencies, but the
latter is insignificant. This effect is replicated in
Table 4 where we evaluate dependency parsers us-
ing TEDEVAL against their own gold theories. Ta-
ble 4 further confirms that there is a gap between
the STB and the STB-Dep theories, reflected in
the scores against the native and generalized gold.
5 Discussion
We presented a formal protocol for evaluating
parsers across frameworks and used it to soundly
compare parsing results for English and Swedish.
Our approach follows the three-phase protocol of
Tsarfaty et al(2011), namely: (i) obtaining a for-
mal common ground for the different representa-
tion types, (ii) computing the theoretical common
ground for each test sentence, and (iii) counting
only what counts, that is, measuring the distance
between the common ground and the parse tree
while discarding annotation-specific edits.
A pre-condition for applying our protocol is the
availability of a relational interpretation of trees in
the different frameworks. For dependency frame-
works this is straightforward, as these relations
are encoded on top of dependency arcs. For con-
stituency trees with an inherent mapping of nodes
onto grammatical relations (Merlo and Musillo,
2005; Gabbard et al 2006; Tsarfaty and Sima?an,
2008), a procedure for reading relational schemes
off of the trees is trivial to implement.
For parsers that are trained on and parse into
bare-bones phrase-structure trees this is not so.
Reading off the relational structure may be more
costly and require interjection of additional theo-
retical assumptions via manually written scripts.
Scripts that read off grammatical relations based
on tree positions work well for configurational
languages such as English (de Marneffe et al
2006) but since grammatical relations are re-
flected differently in different languages (Postal
and Perlmutter, 1977; Bresnan, 2000), a proce-
dure to read off these relations in a language-
independent fashion from phrase-structure trees
does not, and should not, exist (Rambow, 2010).
The crucial point is that even when using ex-
ternal scripts for recovering a relational scheme
for phrase-structure trees, our protocol has a clear
advantage over simply scoring converted trees.
Manually created conversion scripts alter the the-
oretical assumptions inherent in the trees and thus
may bias the results. Our generalization operation
and three-way TED make sure that theory-specific
idiosyncrasies injected through such scripts do
not lead to over-penalizing or over-crediting
theory-specific structural variations.
Certain linguistic structures cannot yet be eval-
uated with our protocol because of the strict as-
sumption that the labeled spans in a parse form a
tree. In the future we plan to extend the protocol
for evaluating structures that go beyond linearly-
ordered trees in order to allow for non-projective
trees and directed acyclic graphs. In addition, we
plan to lift the restriction that the parse yield is
known in advance, in order to allow for evalua-
tion of joint parse-segmentation hypotheses.
6 Conclusion
We developed a protocol for comparing parsing
results across different theories and representa-
tion types which is framework-independent in the
sense that it can accommodate any formal syntac-
tic framework that encodes grammatical relations,
and it is language-independent in the sense that
there is no language specific knowledge encoded
in the procedure. As such, this protocol is ad-
equate for parser evaluation in cross-framework
and cross-language tasks and parsing competi-
tions, and using it across the board is expected
to open new horizons in our understanding of the
strengths and weaknesses of different parsers in
the face of different theories and different data.
Acknowledgments We thank David McClosky,
Marco Khulmann, Yoav Goldberg and three
anonymous reviewers for useful comments. We
further thank Jennifer Foster for the Brown parses
and parameter files. This research is partly funded
by the Swedish National Science Foundation.
52
References
Philip Bille. 2005. A survey on tree edit distance and
related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Clau-
dia Gdaniec, Ralph Grishman, P. Harrison, Don-
ald Hindle, Robert Ingria, Frederick Jelinek, Ju-
dith L. Klavans, Mark Liberman, Mitchell P. Mar-
cus, Salim Roukos, Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for quantitatively
comparing the syntactic coverage of English gram-
mars. In Proceedings of the DARPA Workshop on
Speech and Natural Language, pages 306?311.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The Tiger
treebank. In Proceedings of TLT.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81?124.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: A survey and a new pro-
posal. In Proceedings of LREC, pages 447?454.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL.
Paul Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the Penn treebank. In Proceed-
ing of HLT-NAACL, pages 184?191.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of LREC.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, pages 1420?1425.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference
on Arabic Language Resources and Tools.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT ?05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
Beata Megyesi. 2009. The open source tagger Hun-
PoS for Swedish. In Proceedings of the 17th Nordic
Conference of Computational Linguistics (NODAL-
IDA), pages 239?241.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of EMNLP, pages
620?627.
Joakim Nivre and Beata Megyesi. 2007. Bootstrap-
ping a Swedish Treebank using cross-corpus har-
monization and annotation projection. In Proceed-
ings of TLT.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Joakim Nivre, Laura Rimell, Ryan McDonald, and
Carlos Go?mez-Rodr??guez. 2010. Evaluation of de-
pendency parsers on unbounded dependencies. In
Proceedings of COLING, pages 813?821.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL.
Paul M. Postal and David M. Perlmutter. 1977. To-
ward a universal characterization of passivization.
In Proceedings of the 3rd Annual Meeting of the
Berkeley Linguistics Society, pages 394?417.
Owen Rambow. 2010. The simple truth about de-
pendency and phrase structure representations: An
opinion piece. In Proceedings of HLT-ACL, pages
337?340.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of ACL, pages
663?672.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
53
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Kaizhong Zhang and Dennis Shasha. 1989. Sim-
ple fast algorithms for the editing distance between
trees and related problems. In SIAM Journal of
Computing, volume 18, pages 1245?1262.
54
Parsing Morphologically Rich Languages:
Introduction to the Special Issue
Reut Tsarfaty?
Uppsala University
Djame? Seddah??
Universite? Paris-Sorbonne/INRIA
Sandra Ku?bler?
Indiana University
Joakim Nivre?
Uppsala University
Parsing is a key task in natural language processing. It involves predicting, for each natural
language sentence, an abstract representation of the grammatical entities in the sentence and
the relations between these entities. This representation provides an interface to compositional
semantics and to the notions of ?who did what to whom.? The last two decades have seen great
advances in parsing English, leading to major leaps also in the performance of applications that
use parsers as part of their backbone, such as systems for information extraction, sentiment
analysis, text summarization, and machine translation. Attempts to replicate the success of
parsing English for other languages have often yielded unsatisfactory results. In particular,
parsing languages with complex word structure and flexible word order has been shown to
require non-trivial adaptation. This special issue reports on methods that successfully address
the challenges involved in parsing a range of morphologically rich languages (MRLs). This
introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the
contributions of the articles in the special issue. These contributions present up-to-date research
efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs
addresses challenges that transcend particular representational and algorithmic choices.
1. Parsing MRLs
Parsing is a central task in natural language processing, where a system accepts a
sentence in a natural language as input and provides a syntactic representation of the
? Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: tsarfaty@stp.lingfil.uu.se.
?? Inria?s Alpage project & Universite? Paris Sorbonne, Maison de la Recherche, 28 rue Serpentes, 75006
Paris, France. E-mail: djame.seddah@paris-sorbonne.fr.
? Indiana University, Department of Linguistics, Memorial Hall 322, Bloomington IN-47405, USA.
E-mail: skuebler@indiana.edu.
? Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
entities and grammatical relations in the sentence as output. The input sentences to a
parser reflect language-specific properties (in terms of the order of words, the word
forms, the lexical items, and so on), whereas the output abstracts away from these
properties in order to yield a structured, formal representation that reflects the functions
of the different elements in the sentence.
The best broad-coverage parsing systems to date use statistical models, possibly
in combination with hand-crafted grammars. They use machine learning techniques
that allow the system to generalize the syntactic patterns characterizing the data. These
machine learning methods are trained on a treebank, that is, a collection of natural
language sentences which are annotated with their correct syntactic analyses. Based on
the patterns and frequencies observed in the treebank, parsing algorithms are designed
to suggest and score novel analyses for unseen sentences, and search for the most likely
analysis.
The release of a large-scale annotated corpus for English, the Wall Street Journal
Penn Treebank (PTB) (Marcus, Santorini, and Marcinkiewicz 1993), led to a significant
leap in the performance of statistical parsing for English (Magerman 1995; Collins 1997;
Charniak 2000; Charniak and Johnson 2005; Petrov et al 2006; Huang 2008; Finkel,
Kleeman, and Manning 2008; Carreras, Collins, and Koo 2008). At the time of their
publication, each of these models improved the state-of-the-art of English parsing,
bringing constituency-based parsing performance on the standard test set of the PTB
to the level of 92% F1-score using the PARSEVAL evaluation metrics (Black et al 1991).
The last decade has seen the development of large-scale annotated treebanks
for languages such as Arabic (Maamouri et al 2004), French (Abeille?, Cle?ment, and
Toussenel 2003), German (Uszkoreit 1987; Skut et al 1997), Hebrew (Sima?an et al
2001), Swedish (Nivre and Megyesi 2007), and others. The availability of syntactically
annotated corpora for these languages had initially raised the hope of attaining the
same level of parsing performance on these languages, by simply porting the existing
models to the newly available corpora.
Early attempts to apply the aforementioned constituency-based parsing models to
other languages have demonstrated that the success of these approaches was rather lim-
ited. This observation was confirmed for individual languages such as Czech (Collins
et al 1999), German (Dubey and Keller 2003), Italian (Corazza et al 2004), French (Arun
and Keller 2005), Modern Standard Arabic (Kulick, Gabbard, andMarcus 2006), Modern
Hebrew (Tsarfaty and Sima?an 2007), and many more (Tsarfaty et al 2010).
The same observation was independently confirmed by parallel research efforts on
data-driven dependency-based parsing (Ku?bler, McDonald, and Nivre 2009). Results
coming from multilingual parsing evaluation campaigns, such as the CoNLL shared
tasks on multilingual dependency parsing, showed significant variation in the results
of the same models applied to a range of typologically different languages. In partic-
ular, these results demonstrated that the morphologically rich nature of some of those
languages makes them inherently harder to parse, regardless of the parsing technique
used (Buchholz and Marsi 2006; Nivre et al 2007a).
Morphologically rich languages (MRLs) express multiple levels of information al-
ready at the word level. The lexical information for each word form in an MRL may
be augmented with information concerning the grammatical function of the word in
the sentence, its grammatical relations to other words, pronominal clitics, inflectional
affixes, and so on. In English, many of these notions are expressed implicitly by word
order and adjacency: The direct object, for example, is generally the first NP after the
verb and thus does not necessarily need an explicit marking. Expressing such functional
information morphologically allows for a high degree of word-order variation, since
16
Tsarfaty et al Parsing Morphologically Rich Languages:
grammatical functions need no longer be strongly associated with syntactic positions.
Furthermore, lexical items appearing in different syntactic contexts may be realized in
different forms. This leads to a high level of word-form variation and complicates lexical
acquisition from small sized corpora.
2. The Overarching Challenges
The complexity of the linguistic patterns found in MRLs was shown to challenge
parsing in many ways. For instance, standard models assume that a word always
corresponds to a unique terminal in the parse tree. In Arabic, Hebrew, Turkish, and other
languages, an input word-token may correspond to multiple terminals. Furthermore,
models developed primarily to parse English draw substantial inference based onword-
order patterns. Parsing non-configurational languages such as Hungarian may require
relying on morphological information to infer equivalent functions. Parsing Czech or
German is further complicated by case syncretism, which precludes a deterministic
correlation between morphological case and grammatical functions. In languages such
as Hungarian or Finnish, the diversity of word forms leads to a high rate of out-of-
vocabulary words unseen in the annotated data. MRL parsing is thus often associated
with increased lexical data sparseness. An MRL parser requires robust statistical meth-
ods for analyzing such phenomena.
Following Tsarfaty et al (2010) we distinguish three overarching challenges that are
associated with parsing MRLs.
(i) The Architectural Challenge. Contrary to English, where the input signal uniquely
determines the sequence of tree terminals, word forms in an MRLmay contain multiple
units of information (morphemes). These morphemes have to be segmented in order to
reveal the basic units of analysis. Furthermore, morphological analysis of MRL words
may be highly ambiguous, and morphological segmentation may be a non-trivial task
for certain languages. Therefore, a parsing architecture for an MRL must contain, at the
very least, a morphological component for segmentation and a syntactic component for
parsing. The challenge is thus to determine how these two models should be combined
in the overall parsing architecture: Should we assume a pipeline architecture, where the
morphological segmentation is disambiguated prior to parsing? Or should we construct
a joint architecture where the model picks out a parse tree and a segmentation at once?
(ii) The Modeling Challenge. The design of a statistical parsing model requires specifying
three formal elements: the formal output representation, the events that can be observed
in the data, and the independence assumptions between these events. For anMRL, com-
plex morphosyntactic interactions may impose constraints on the form of events and on
their possible combination. In such cases, we may need to incorporate morphological
information in the syntactic model explicitly. How should morphological information
be treated in the syntactic model: as explicit tree decoration, as hidden variables, or as
complex objects in their own right? Which morphological features should be explicitly
encoded? Where should we mark morphological features: at the part-of-speech level, at
phrase level, on dependency arcs? How domorphological and syntactic events interact,
and how can we exploit these interactions for inferring correct overall structures?
(iii) The Lexical Challenge. A parsing model for MRLs requires recognizing the morpho-
logical information in each word form. Due to the high level of morphological variation,
however, data-driven systems are not guaranteed to observe all morphological variants
17
Computational Linguistics Volume 39, Number 1
of a word form in a given annotated corpus. How can we assign correct morphological
signatures to the lexical items in the face of such extreme data spareseness? When
devising a model for parsing MRLs, one may want to make use of whatever additional
resources one has access to?morphological analyzers, unlabeled data, and lexica?in
order to extend the coverage of the parser and obtain robust and accurate predictions.
3. Contributions of this Special Issue
This special issue draws attention to the different ways in which researchers work-
ing on parsing MRLs address the challenges described herein. It contains six stud-
ies discussing parsing results for six languages, using both constituency-based and
dependency-based frameworks (cf. Table 1). The first three studies (Seeker and Kuhn;
Fraser et al; Kallmeyer and Maier) focus on parsing European languages and deal
with phenomena that lie within their flexible phrase ordering and rich morphology,
including problems posed by case syncretism. The next two papers (Goldberg and
Elhadad; Marton et al) focus on Semitic languages and study the application of general-
purpose parsing algorithms (constituency-based and dependency-based, respectively)
to parsing such data. They empirically show gaps in performance between different
architectures (pipeline vs. joint , gold vs. machine-predicted input), feature choices, and
techniques for increasing lexical coverage of the parser. The last paper (Green et al) is
a comparative study on multi-word expression (MWE) recognition via two specialized
parsing models applied to both French and Modern Standard Arabic. Let us briefly
outline the individual contributions made by each of the articles in this special issue.
Seeker and Kuhn present a comparative study of dependency parsing for three
European MRLs from different typological language families: German (Germanic),
Czech (Slavonic), and Hungarian (Finno-Ugric). Although all these languages possess
richer morphological marking than English, there is variation among these languages
in terms of the richness of the morphological information encoded in the word forms,
and the ambiguity of these morphological markers. Hungarian is agglutinating, that
is, morphological markers in Hungarian are non-ambiguous and easy to recognize.
German and Czech are fusional languages with different types of case syncretism.
Seeker and Kuhn use the Bohnet Parser (Bohnet 2010) to parse all these languages, and
show that not using morphological information in the statistical feature model is detri-
mental. Using gold morphology significantly improves results for all these languages,
whereas automatically predicted morphology leads to smaller improvements for the
fusional languages, relative to the agglutinating one. To combat this loss in performance,
they add linguistic constraints to the decoder, restricting the possible structures. They
show that a decoding algorithm which filters out dependency parses that do not obey
Table 1
Contributions to the CL special issue on parsing morphologically rich languages (CL-PMRL).
Constituency-Based Dependency-Based
Arabic Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013
Czech Seeker and Kuhn 2013
French Green, de Marneffe, and Manning 2013
German Kallmeyer and Maier 2013 Seeker and Kuhn 2013
Fraser et al 2013
Hebrew Goldberg and Elhadad 2013
Hungarian Seeker and Kuhn 2013
18
Tsarfaty et al Parsing Morphologically Rich Languages:
predicate-argument constraints allows the authors to obtain more substantial gains
from morphology.
Fraser et al also focus on parsing German, though in a constituency-based setting.
They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of
manual treebank annotations that bring the treebank grammar performance to the level
of automatically predicted states learned by Petrov et al (2006). As in the previous
study, syncretism is shown to cause ambiguity that hurts parsing performance. To
combat this added ambiguity, they use external information sources. In particular, they
show different ways of using information from monolingual and bilingual data sets
in a re-ranking framework for improving parsing accuracy. The bilingual approach
is inspired by machine translation studies and exploits the variation in marking the
same grammatical functions differently across languages for increasing the confidence
of a disambiguation decision in one language by observing a parallel non-ambiguous
structure in the other one.
These two studies use German corpora stripped of discontinuous constituents in
order to benchmark their parsers. In each of these cases, the discontinuities are con-
verted into pure tree structures, thus ignoring the implied long distance dependencies.
Kallmeyer and Maier propose an alternative approach for parsing such languages
by presenting an overall solution for parsing discontinuous structures directly. They
present a parsing model based on Probabilistic Linear Context-Free Rewriting Systems
(PLCFRS), which implements many of the technological advances that were developed
in the context of parsing with PCFGs. In particular, they present a decoding algorithm
based on weighted deductive CKY parsing, and use it in conjunction with PLCFRS
parameters directly estimated from treebank data. Because PLCFRS is a powerful for-
malism, the parser needs to be tuned for speed. The authors present several admissible
heuristics that facilitate faster A* parsing. The authors present parsing results that are
competitive with constituency-based parsing of German while providing invaluable
information concerning discontinuous constituents and long distance dependencies.
Goldberg and Elhadad investigate constituency parsing for Modern Hebrew
(Semitic), a language which is known to have a very rich and ambiguous morpho-
logical structure. They empirically show that an application of the split-and-merge
general-purpose model of Petrov et al (2006) for parsing Hebrew does not guarantee
accurate parsing in and of itself. In order to obtain competitive parsing performance,
they address all three challenges we have noted. In order to deal with the problem of
word segmentation (the architectural challenge), they extend the chart-based decoder
of Petrov et al with a lattice-based decoder. In order to handle morphological marking
patterns (the modeling challenge), they refine the initial treebank with particularly
targeted state-splits, and add a set of linguistic constraints that act as a filter ruling
out trees that violate agreement. Finally, they add information from an external wide-
coverage lexicon to combat lexical sparseness (the lexical challenge). They show that the
contribution of these different methods is cumulative, yielding state-of-the-art results
on constituency parsing of Hebrew.
Marton et al study dependency parsing of Modern Standard Arabic (Semitic)
and attend to the same challenges. They show that for two transition-based parsers,
MaltParser (Nivre et al 2007b) and EasyFirst (Goldberg and Elhadad 2010), controlling
the architectural and modeling choices leads to similar effects. For instance, when
comparing parsing performance on gold and machine-predicted input conditions, they
show that rich informative tag sets are preferred in gold conditions, but smaller tag
sets are preferred in machine-predicted conditions. They further isolate a set of mor-
phological features which leads to significant improvements in the machine-predicted
19
Computational Linguistics Volume 39, Number 1
condition, for both frameworks. They also show that function-based morphological
features are more informative than surface-based features, and that performance loss
that is due to errors in part-of-speech tagging may be restored by training the model
on a joint set of trees encoding gold tags and machine-predicted tags. At the same time,
undirected parsing of EasyFirst shows better accuracy, possibly due to the flexiblity in
phrase ordering. The emerging insight is that tuning morphological information inside
general-purpose parsing systems is of crucial importance for obtaining competitive
performance.
Focusing on Modern Standard Arabic (Semitic) and French (Romance), the last
article of this special issue, by Green et al, may be seen as an applications paper,
treating the task of MWE recognition as a side effect of a joint model for parsing and
MWE identification. The key problem here is knowing what to consider a minimal
unit for parsing, and how to handle parsing in realistic scenarios where MWEs have
not yet been identified. The authors present two parsing models for such a task: a
factored model including a factored lexicon that integrates morphological knowledge
into the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet Process
Tree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010). The
latter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, and
Sima?an 2003) in a Bayesian framework, extended to include specific features that ease
the extraction of tree fragments matching MWEs. Interestingly, those very different
models do provide the same range of performance when confronted with predicted
morphology input. Additional important challenges that are exposed in the context of
this study concern the design of experiments for cross-linguistic comparison in the face
of delicate asymmetries between the French and Arabic data sets.
4. Conclusion
This special issue highlights actively studied areas of research that address parsing
MRLs. Most approaches described in this issue rely on extending existing parsing
models to address three overarching challenges. The joint parsing and segmentation
architecture scenario can be addressed by extending a general-purpose CKY decoder
into a lattice-based decoder. The modeling challenge may be addressed by explicitly
marking morphological features as syntactic state-splits, by modeling discontinuities
in the formal syntactic representation directly, by incorporating hard-coded linguistic
constraints as filters, and so on. The lexical challenge can be addressed by using external
resources such as a wide-coverage lexicon for analyzing unknown words, and the use
of additional monolingual and bilingual data in order to obtain robust statistics in the
face of extreme sparseness.
An empirical observation reflected in the results presented here is that languages
which we refer to as MRLs exhibit their own cross-lingual variation and thus should not
be treated as a single, homogeneous class of languages. Some languages show richer
morphology than others; some languages possess more flexible word ordering than
others; some fusional languages show syncretism (coarse-grained underspecifiedmark-
ers) whereas others use a large set of fine-grained and unambiguous morphological
markers. The next challenge would then be to embrace these variations, and investigate
whether the typological properties of languages can inform us more directly concerning
the adequate methods that can be used to effectively parse them.
As the next research goal, we then set out to obtain a deeper understanding of
how annotation choices paired up with modeling choices systematically correlate with
parsing performance for different languages. Further work in the line of the studies
20
Tsarfaty et al Parsing Morphologically Rich Languages:
presented here is required in order to draw relevant generalizations. Furthermore,
the time is ripe for another multilingual parser evaluation campaign, which would
encourage the community to develop parsing systems that can easily be transferred
from one language type to another. By compiling these recent contributions, we hope
to encourage not only the development of novel systems for parsing individual MRLs,
but also to facilitate the search for more robust, generic cross-linguistic solutions.
Acknowledgments
As guest editors of this special issue, we
wish to thank the regular members and
the guest members of the Computational
Linguistics editorial board for their thorough
work, which allowed us to assemble this
special issue of high quality contributions in
the emerging field of parsing MRLs. We also
want to thank Marie Candito, Jennifer Foster,
Yoav Goldberg, Ines Rehbein, Lamia Tounsi,
and Yannick Versley for their contribution
to the initial proposal for this special issue.
Finally, we want to express our gratitude
to Robert Dale and Suzy Howlett for their
invaluable support throughout the editorial
process.
References
Abeille?, Anne, Lionel Cle?ment, and Franc?ois
Toussenel. 2003. Building a treebank for
French. In Anne Abeille?, editor, Treebanks.
Kluwer, Dordrecht, pages 165?188.
Arun, Abhishek and Frank Keller. 2005.
Lexicalization in crosslinguistic
probabilistic parsing: The case of French.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics,
pages 306?313, Ann Arbor, MI.
Black, Ezra, Steven Abney, Dan Flickinger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Frederick Jelinek, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of English grammars. Speech
Communication, 33(1,2):306?311.
Bod, Rens. 1992. A computational model
of language performance: Data oriented
parsing. In Proceedings of the 14th Conference
on Computational linguistics-Volume 3,
pages 855?859, Nantes.
Bod, Rens, Remko Scha, and Khalil Sima?an,
editors. 2003. Data-Oriented Parsing. CSLI,
Stanford, CA.
Bohnet, Bernd. 2010. Top accuracy and fast
dependency parsing is not a contradiction.
In Proceedings of CoLing, pages 89?97, Sydney.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the Tenth Conference on Computational
Language Learning (CoNLL),
pages 149?164, New York, NY.
Carreras, Xavier, Michael Collins, and
Terry Koo. 2008. TAG, dynamic
programming, and the perceptron
for efficient, feature-rich parsing.
In Proceedings of the Twelfth Conference on
Computational Natural Language Learning
(CoNLL), pages 9?16, Manchester.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings
of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 132?139, Seattle, WA.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and maxent discriminative reranking.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL 2005), pages 173?180,
Ann Arbor, MI.
Cohn, Trevor, Phil Blunsom, and Sharon
Goldwater. 2010. Inducing tree-
substitution grammars. The Journal of
Machine Learning Research, 11:3053?3096.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Collins, Michael, Jan Hajic?, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of the 37th
Annual Meeting of the ACL, pages 505?512,
College Park, MD.
Corazza, Anna, Alberto Lavelli, Giogio Satta,
and Roberto Zanoli. 2004. Analyzing an
Italian treebank with state-of-the-art
statistical parsers. In Proceedings of the
Third Workshop on Treebanks and Linguistic
Theories (TLT 2004), pages 39?50, Tu?bingen.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, pages 96?103,
Ann Arbor, MI.
21
Computational Linguistics Volume 39, Number 1
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random field
parsing. In Proceedings of ACL-08: HLT,
pages 959?967, Columbus, OH.
Goldberg, Yoav and Michael Elhadad. 2010.
An efficient algorithm for easy-first
non-directional dependency parsing.
In Human Language Technologies: The
2010 Annual Conference of the North
American Chapter of the Association
for Computational Linguistics,
pages 742?750, Los Angeles, CA.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics,
pages 423?430, Sapporo.
Ku?bler, Sandra, Ryan McDonald, and Joakim
Nivre. 2009. Dependency Parsing. Number 2
in Synthesis Lectures on Human Language
Technologies. Morgan & Claypool
Publishers.
Kulick, Seth, Ryan Gabbard, and Mitchell
Marcus. 2006. Parsing the Arabic treebank:
Analysis and improvements. In Proceedings
of the 5th International Workshop on
Treebanks and Linguistic Theories (TLT),
pages 31?42, Prague.
Maamouri, Mohamed, Anne Bies, Tim
Buckwalter, and Wigdan Mekki. 2004.
The Penn Arabic Treebank: Building a
large-scale annotated Arabic corpus. In
Proceedings of the NEMLAR Conference
on Arabic Language Resources and Tools,
pages 102?109, Cairo.
Magerman, David M. 1995. Statistical
decision-tree models for parsing.
In Proceedings of the 33rd Annual Meeting on
Association for Computational Linguistics,
pages 276?283, Cambridge, MA.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English:
The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007a. The
CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 915?932, Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser:
A language-independent system for
data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Nivre, Joakim and Beata Megyesi. 2007.
Bootstrapping a Swedish Treebank
using cross-corpus harmonization and
annotation projection. In Proceedings of the
Sixth International Workshop on Treebanks
and Linguistic Theories (TLT), pages 97?102,
Bergen.
Petrov, Slav, Leon Barrett, Romain Thibaux,
and Dan Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 433?440, Sydney.
Schmid, Helmut. 2004. Efficient parsing of
highly ambiguous context-free grammars
with bit vectors. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING 2004), pages 162?168,
Geneva.
Sima?an, Khalil, Alon Itai, Yoad Winter,
Alon Altmann, and Noa Nativ. 2001.
Building a tree-bank of Modern Hebrew
text. Traitement Automatique des Langues,
42:347?380.
Skut, Wojciech, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997.
An annotation scheme for free word
order languages. In Proceedings
of the Fifth Conference on Applied
Natural Language Processing (ANLP),
pages 88?95, Washington, D.C.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Ku?bler, Marie
Candito, Jennifer Foster, Yannick Versley,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL): What, how and
whither. In Proceedings of the NAACL
Workshop on Statistical Parsing of
Morphologically Rich Languages, pages 1?12,
Los Angeles, CA.
Tsarfaty, Reut and Khalil Sima?an. 2007.
Three-dimensional parametrization for
parsing morphologically rich languages.
In Proceedings of the Tenth International
Conference on Parsing Technologies,
pages 156?167, Prague.
Uszkoreit, Hans. 1987.Word Order and
Constituent Structure in German. CSLI,
Stanford, CA.
22
Book Review
Design Patterns in Fluid Construction Grammar
Luc Steels (editor)
Universitat Pompeu Fabra and Sony Computer Science Laboratory, Paris
Amsterdam: John Benjamins Publishing Company (Constructional Approaches to
Language series, edited by Mirjam Fried and Jan-Ola O?stman, volume 11), 2012,
xi+332 pp; hardbound, ISBN 978-90-272-0433-2, e99.00, $149.00
Reviewed by
Nathan Schneider, Carnegie Mellon University
and Reut Tsarfaty, Uppsala University
In computational modeling of natural language phenomena, there are at least three
modes of research. The currently dominant statistical paradigm typically prioritizes
instance coverage: Data-driven methods seek to use as much information observed in
data as possible in order to generalize linguistic analyses to unseen instances. A second
approach prioritizes detailed description of grammatical phenomena, that is, forming
and defending theories with a focus on a small number of instances. A third approach
might be called integrative: Rather than addressing phenomena in isolation, different
approaches are brought together to address multiple challenges in a unified framework,
and the behavior of the system is demonstrated with a small number of instances. Design
Patterns in Fluid Construction Grammar (DPFCG) exemplifies the third approach, intro-
ducing a linguistic formalism called Fluid Construction Grammar (FCG) that addresses
parsing, production, and learning in a single computational framework.
The book emphasizes grammar-engineering, following broad-coverage descrip-
tive paradigms that can be traced back to Generalized Phrase Structure Grammar
(GPSG) (Gazdar et al 1985), Lexical Functional Grammar (LFG) (Bresnan 2000), Head-
Driven Phrase-Structure Grammar (HPSG) (Sag and Wasow 1999), and Combinatory
Categorial Grammar (CCG) (Steedman 1996). In all of these cases, a formal meta-
framework allows computational linguists to formalize their hypotheses and intuitions
about a language?s grammatical behavior and then explore how these representational
choices affect the processing of natural language utterances. Many of the aforemen-
tioned approaches have engendered large-scale platforms that can be used and reused
to provide formal description of grammars for different languages, such as Par-Gram
for LFG (Butt et al 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger,
and Oepen 2002).
? 2013 Association for Computational Linguistics
FCG offers a similar grammar engineering framework that follows the principles
of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG
treats constructions as the basic units of grammatical organization in language. The con-
structions are viewed as learned associations between form (e.g., sounds, morphemes,
syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG
does not impose a strict separation between lexicon and grammar?indeed, it is per-
haps best known as treating semi-productive idioms like ?the X-er, the Y-er? and
?X let alne Y? on equal footing with lexemes and ?core? syntactic patterns (Fillmore,
doi:10.1162/COLI r 00154
Computational Linguistics Volume 39, Number 2
Kay, and O?Connor 1988; Kay and Fillmore 1999). FCG, like other CxG formalisms?
namely, Embodied Construction Grammar (Bergen and Chang 2005; Feldman, Dodge,
and Bryant 2009) and Sign-Based Construction Grammar (Boas and Sag 2012)?is
unification-based.1 The studies in this book describe constructions and how they can
be combined in order to model natural language interpretation or generation as feature
structure unification in a general search procedure.
The book has five parts, covering the groundwork, basic linguistic applications,
processing matters, advanced case studies, and, finally, features of FCG that make it
fluid and robust. Each chapter identifies general strategies (design patterns) that might
merit reuse in new FCG grammars, or perhaps in other computational frameworks.
Part I: Introduction lays the groundwork for the rest of the book. ?Introducing Fluid
Construction Grammar? (by Luc Steels) presents the aims of the FCG formalism. FCG
was designed as a framework for describing linguistic units (constructions?their form
and meaning), with an emphasis on language variation and evolution (?fluidity?). The
constructionist approach to language is described and the argument for applying it to
study language variation and change is defended. Psychological validity is explicitly
ruled out as a modeling goal (?The emphasis is on getting working systems, and this
is difficult enough?; page 4). The architects of FCG set out to include both sides of
the processing coin, however?parsing (interpretation) and production (generation).
The concept of search in processing is emphasized, though some of the explanations
of processing steps are too abstract for the reader to comprehend at this point. A
further desideratum?robustness to noisy input containing disfluencies, fragments, and
errors?is given as motivating a constructionist approach.
The next chapter, ?A First Encounter with Fluid Construction Grammar?
(by Steels), describes the mechanisms of FCG in detail. In FCG, a working analysis
hypothesized in processing is known as transient structure; the transduction of form to
meaning (and vice versa) selects a sequence of constructions that apply to the transient
structure to gradually expand it until reaching a final analysis. Identifying construc-
tions that may apply to a transient structure presents a non-trivial search problem,
also addressed by the architects of FCG. The sheer number of technical details make
this chapter somewhat overwhelming. Most of the chapter is devoted to the low-level
feature structures and the operations manipulating them. Templates?a practical means
of avoiding boilerplate code when defining constructions?are then introduced, and do
most of the heavy lifting in the rest of the book.
In Part II: Grammatical Structures, we begin to see how constructions are defined in
practice. ?A Design Pattern for Phrasal Constructions? (by Steels) illustrates how con-
structions are used to describe the combination of multiple units into higher-level, typed
phrases. Skeletal constructions compose with smaller units to form hierarchical struc-
tures (essentially similar to the Immediate Constituents Analysis of Bloomfield [1933]
and follow-up work in structuralist linguistics [Harris 1946]), and a range of additional
constructions impose form (e.g., ordering) constraints and add new meaning to the
newly created phrases. This chapter is of a tutorial nature, illustrating the step-by-step
application of four kinds of noun phrase constructions to expand transient structures
in processing. Over twenty templates are introduced in this chapter; they encapsulate
design patterns dealing with hierarchical structure, agreement, and feature percolation.
1 Less formal members of the Construction Grammar family include cognitive approaches in the
Berkeley tradition (Lakoff 1987; Goldberg 1995, 2006), Cognitive Grammar (Langacker 1987),
and Radical Construction Grammar (Croft 2001).
448
Book Review
An aspect of phrasal constructions that is not yet dealt with is the complex linking of se-
mantic arguments and the morphosyntactic categorizations of the composed elements.
?A Design Pattern for Argument Structure Constructions? (by Remi van Trijp)
then builds on the formal machinery presented in the previous chapter to explicitly
address the complex mappings between semantic arguments (agent, patient, etc.) and
syntactic arguments (subject, object, etc.). This mapping is a complex matter due to
language-specific conceptualization of semantic arguments and different means of
morphosyntactic realization used by different languages. In FCG, each lexical item
introduces its linking potential in terms of the different types of semantic and syntactic
arguments that it may take, with no particular mapping between them. Each argument
structure construction imposes a partial mapping between the syntactic and semantic
arguments to yield a particular argument structure instantiation, one of the multiple
alternatives that may be available for a single lexical item. This account stands in sharp
contrast to the lexicalist view of argument-structure (the view taken in LFG, HPSG, and
CCG) whereby each lexical entry dictates all the necessary linking information. The
construction-based approach is defended for its ability to deal with unknown words2
and constructional coercion3 (Goldberg 1995). The argument structure design pattern
allows FCG to crudely recover a partial specification of the form-meaning mapping of
these elements, which is important for robust processing (see subsequent discussion).
Part III: Managing Processing addresses how FCG transduces between a linguistic
string and a meaning representation, where the two directions (parsing and production)
share a common declarative representation of linguistic knowledge (the grammar). This
entails assembling an analysis incrementally on the basis of the grammar, the input, and
any partial analysis that has already been created. With FCG (and unification grammars
more broadly) this search is nontrivial, and streamlining search (i.e., minimizing non-
determinism and avoiding dead ends) is a key motivator of many of the grammar
design patterns suggested in the book.
?Search in Linguistic Processing? (by Joris Bleys, Kevin Stadler, and Joachim De
Beule) deals mainly with the problem of choosing which of multiple compatible con-
structions to apply next. Whereas the default heuristic search in FCG is a greedy, depth-
first search (which can backtrack if the user-defined end-goal has not yet been achieved)
the FCG framework allows for a guided search through scores that reflect the relative
tendency of a construction to apply next. The authors suggest that such scoring can be
informed by general principles, for instance: (i) specific constructions are preferred to
more general ones, and (ii) previously co-applied constructions are preferred. Choosing
appropriate constructions to apply early on dramatically reduces the time needed for
processing the utterance.
?Organizing Constructions in Networks? (by Pieter Wellens) takes this idea to
the next level, and proposes to organize the different constructions in networks of
conditional dependencies. A conditional dependency links two constructions where
one provides necessary information for the application of the other. These dependency
networks can be updated whenever an input is processed so that the system learns
to search more efficiently when the same constructions are encountered in the future.
Using these networks to guide the search thus significantly reduces the search for
compatible constructions. An empirical effort to quantify this effect indeed shows a
2 For example, ?He blicked the napkin off the table??a reasonable inference is that the subject caused the
napkin to leave the table.
3 ?He sneezed the napkin off the table??note that sneeze is not normally transitive.
449
Computational Linguistics Volume 39, Number 2
sharp reduction in search time; unlike the held-out experimental paradigm accepted in
statistical NLP, however, the parsed/produced sentence is assumed to have been seen
already by the system.
Part IV: Case Studies addresses three challenging linguistic phenomena in FCG.
?Feature Matrices and Agreement? (by van Trijp) on German case offers a new
unification-based solution to the problem of feature indeterminacy. For instance, in the
sentence Er findet und hilft Frauen ?He finds and helps women?, the first verb requires
an accusative object, whereas the second requires a dative object; the coordination
is allowed only because Frauen can be either accusative or dative. Kindern ?children?,
which can only be dative, is not licensed here. Encoding case in a single feature on
the Frauen construction wouldn?t work because the feature would have to unify with
contradictory values (from the verbs? inflectional features). Instead, case restrictions
specified lexically for a noun or verb can be expressed with a distinctive feature matrix,
with each matrix slot holding a variable or the value + or -. Unification then does
the right thing?allowing Frauen and forbidding Kindern?without resorting to type
hierarchies or disjunctive features.
?Construction Sets and Unmarked Forms? (by Katrien Beuls) on Hungarian verbal
agreement models a phenomenon whereby morphosyntactic, semantic, and phono-
logical factors affect the choice between poly- and mono-personal agreement?that is,
the decision whether a Hungarian transitive verb should agree with its object or just
with its subject. The case and definiteness of the object and the person hierarchy rela-
tionship between subject and object determine which kind of agreement obtains, and
phonological constraints determine its form. To make the different levels of structure
interact properly, constructions are grouped into sets (lexical, morphological, etc.) and
those sets are considered in a fixed order during processing. Construction sets also allow
for efficient handling of unmarked forms (null affixes)?they are considered only after
the overt affixes have had the opportunity to apply, thereby functioning as defaults.
?Syntactic Indeterminacy and Semantic Ambiguity? (by Michael Spranger and
Martin Loetzsch) on German spatial phrases models the German spatial terms for front,
back, left, and right. To model spatial language in situated interaction with robots, two
problems must be overcome. The first is syntactic indeterminacy: Any of these spatial
relations may be realized as an adjective, an adverb, or a preposition. The second is
semantic ambiguity, specifically when the perspective (e.g., whose ?left??) is implicit.
Both are forms of underspecification which could cause early splits in the search space
if handled na??vely. Much in the spirit of the argument structure constructions (see
above), the solutions (which are too technical to explain here) involve (a) disjunctive
representations of potential values of a feature, and (b) deferring decisions until a more
opportune stage.
Part V: Robustness and Fluidity (by Steels and van Trijp) surveys the different
features of the system that ensure robustness in the face of variation, disfluencies,
and noise. Natural language is fluid and open-ended. There is variation between
speakers, there are disfluencies and speech errors, and noise may corrupt the speech
signal. All of these may jeopardize the interpretability of the signal, but human
listeners are adept at processing such input. In the spirit of usage-based grammar
(Tomasello 2003), FCG emphasizes the attainment of a communicative goal, rather than
ensuring grammaticality of parsed/produced utterances. This is accomplished with
a diagnostic-repair process that runs in parallel to parsing/production. Diagnostics
can test for unknown words, unfamiliar meanings, missing constructions, and so on.
Diagnostic tests are implemented by reversing the direction of the transduction process:
a speaker may assume the hearer?s point of view to analyze what she has produced
450
Book Review
in order to see whether communicative success has been attained. Likewise, a hearer
may produce a phrase according to his own interpretation of the speaker?s form, and
check for a match. If a test fails, repair strategies such as proposing new constructions,
relaxing the matching process for construction application, and coercing constructions
to adapt to novel language use are considered.
Fluidity and robustness are the hallmarks of FCG, and the computational
framework has been used in experiments that assume embedded communication in
robotic agents. This research program is developed at length by Steels (2012b).
Discussion. Like the legacy of the GPSG book (Gazdar et al 1985), this book?s main merit
is not necessarily in its technical details or computational choices, but in demonstrating
the feasibility of implementing the constructional approach in a full-fledged computa-
tional framework. We suggest that the CxG perspective presents a formidable challenge
to the computational linguistics/natural language processing community. It posits a
different notion of modularity than is observed by most NLP systems: Rather than
treat different levels of linguistic structure independently, CxG recognizes that multiple
formal components (phonological, lexical, morphological, syntactic) may be tied by
convention to a specific meaning or function. Systematically describing these ?cross-
cutting? constructions and their processing, especially in a way that scales to large data
encompassing both form and meaning and accommodates both parsing and generation,
would in our view make for a more comprehensive account of language processing than
our field is able to offer today. Thus, we hope this book will be provocative even outside
of the grammar engineering community.
This book is not without its weaknesses. In parts the writing is quite technical and
terse, which can be daunting for readers new to FCG. Contextualization with respect to
other strands of computational linguistics and AI research is, for the most part, lacking,
though a second FCG book (Steels 2012a) picks up some of the slack on this front.4
DPFCG does not address the feasibility of learning constructions directly from data,5 nor
does it discuss the expressive power of the formalism in relation to learnability results
(such as that of Gold [1967]). As admitted by the authors, much more work would be
needed to build life-size grammars. Still, we hope that readers of DPFCG will appreciate
the authors? vision for a model of linguistic form and function that is at once formal,
computational, fluid, and robust.
References
Bender, Emily M., Dan Flickinger, and
Stephan Oepen. 2002. The grammar
matrix: An open-source starter-kit for the
rapid development of cross-linguistically
consistent broad-coverage precision
grammars. In Proceedings of the Workshop
on Grammar Engineering and Evaluation
at the 19th International Conference on
Computational Linguistics, pages 8?14,
Taipei.
Bergen, Benjamin K. and Nancy Chang.
2005. Embodied Construction Grammar in
simulation-based language understanding.
4 In particular, the Embodied Construction Grammar formalism noted earlier offers a similar
computational framework, though it reflects different research goals. The most important difference is
that FCG was designed to study language evolution, and ECG to study language acquisition and use
from a cognitive perspective. FCG is largely about processing (support for production as well as parsing
is considered essential); ECG places a greater premium on the meaning representation, formalizing a
number of theoretical constructs from cognitive semantics. Chang, De Beule, and Micelli (2012) compare
and contrast the two approaches in detail.
5 Ideas that may be of relevance here are discussed in the Data-Oriented Parsing literature (Scha 1990; Bod
2003) and the statistical learning of Stochastic Tree Substitution Grammars (O?Donnell 2011).
451
Computational Linguistics Volume 39, Number 2
In Jan-Ola O?stman and Mirjam Fried,
editors, Construction Grammars: Cognitive
Grounding and Theoretical Extensions. John
Benjamins, Amsterdam, pages 147?190.
Bloomfield, Leonard. 1933. Language. Holt,
Rinehart and Winston Inc.
Boas, Hans C. and Ivan A. Sag, editors.
2012. Sign-Based Construction Grammar.
Number 193 in CSLI Lecture Notes.
CSLI Publications, Stanford, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
10th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 19?26, Budapest.
Bresnan, Joan. 2000. Lexical-Functional Syntax.
Blackwell.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The parallel grammar
project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, Taipei.
Chang, Nancy, Joachim De Beule, and
Vanessa Micelli. 2012. Computational
construction grammar: Comparing
ECG and FCG. In Luc Steels, editor,
Computational Issues in Fluid Construction
Grammar. Springer Verlag, Berlin,
pages 259?288.
Croft, William. 2001. Radical Construction
Grammar: Syntactic Theory in Typological
Perspective. Oxford University Press,
Oxford.
Feldman, Jerome A., Ellen Dodge,
and John Bryant. 2009. Embodied
Construction Grammar. In Bernd Heine
and Heiko Narrog, editors, The
Oxford Handbook of Linguistic Analysis.
Oxford University Press, Oxford,
pages 111?138.
Fillmore, Charles J., Paul Kay, and
Mary Catherine O?Connor. 1988.
Regularity and idiomaticity in
grammatical constructions:
The case of ?let alne.? Language,
64(3):501?538.
Gazdar, Gerald, Ewan Klein, Geoffrey K.
Pullum, and Ivan A. Sag. 1985. Generalised
Phrase Structure Grammar. Blackwell,
Oxford, England.
Gold, E. Mark. 1967. Language identication
in the limit. Information and Control,
10(5):447?474.
Goldberg, Adele E. 1995. Constructions:
A construction grammar approach to
argument structure. University of
Chicago Press, Chicago.
Goldberg, Adele E. 2003. Constructions:
A new theoretical approach to language.
Trends in Cognitive Sciences, 7(5):219?224.
Goldberg, Adele E. 2006. Constructions at
work: the nature of generalization in language.
Oxford University Press, Oxford.
Harris, Zellig S. 1946. From morpheme to
utterance. Language, 22(3):161?183.
Hoffmann, Thomas and Graeme Trousdale,
editors. 2013. The Oxford Handbook
of Construction Grammar. Oxford
University Press, Oxford.
Kay, Paul and Charles J. Fillmore. 1999.
Grammatical constructions and linguistic
generalizations: The What?s X doing Y?
construction. Language, 75(1):1?33.
Lakoff, George. 1987. Women, Fire, and
Dangerous Things: What Categories Reveal
About the Mind. University of Chicago
Press, Chicago.
Langacker, Ronald W. 1987. Foundations of
Cognitive Grammar, volume 1. Stanford
University Press, Stanford, CA.
O?Donnell, Timothy J. 2011. Productivity
and reuse in language. Ph.D. dissertation,
Harvard University, Cambridge, MA.
Sag, Ivan A. and Thomas Wasow. 1999.
Syntactic Theory: A Formal Introduction.
CSLI Publications, Stanford, CA.
Scha, Remko. 1990. Language theory and
language technology: Competence and
performance. In R. de Kort and G. L. J.
Leerdam, editors, Computertoepassingen
in de Neerlandistiek. Almere, pages 7?22.
Schneider, Nathan. 2010. Computational
cognitive morphosemantics: Modeling
morphological compositionality
in Hebrew verbs with Embodied
Construction Grammar. In Proceedings
of the 36th Annual Meeting of the Berkeley
Linguistics Society, Berkeley, CA.
Steedman, Mark. 1996. Surface Structures
and Interpretation. Number 30 in
Linguistic Inquiry Monographs.
The MIT Press, Cambridge, MA.
Steels, Luc, editor. 2012a. Computational
Issues in Fluid Construction Grammar.
Number 7249 in Lecture Notes in
Computer Science. Springer, Berlin.
Steels, Luc, editor. 2012b. Experiments in
Cultural Language Evolution. Number 3
in Advances in Interaction Studies.
John Benjamins Publishing, Amsterdam.
Tomasello, Michael. 2003. Constructing a
Language: A Usage-Based Theory of Language
Acquisition. Harvard University Press,
Cambridge, MA.
Tsarfaty, Reut. 2010. Relational-Realization
Parsing. PhD thesis, University of
Amsterdam.
452
Book Review
Nathan Schneider is a doctoral student at Carnegie Mellon University. His research has touched
several areas of computational semantics, including an account of Semitic morphology in
Embodied Construction Grammar (Schneider 2010). E-mail: nschneid@cs.cmu.edu. Reut Tsarfaty
is a postdoctoral researcher at Uppsala University. In her research she developed the Relational-
Realization framework for statistical modeling of flexible form-function correspondence patterns
(Tsarfaty 2010). Tsarfaty is a guest editor of the recent Computational Linguistics special issue on
Parsing Morphologically Rich Languages (PMRL) and she is the author of a book on the topic to
be published by Morgan & Claypool Publishers. E-mail: tsarfaty@stp.lingfil.uu.se.
453

Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 6?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Evaluation of Morphological Segmentation and Syntactic Parsing
Reut Tsarfaty Joakim Nivre Evelina Andersson
Box 635, 751 26, Uppsala University, Uppsala, Sweden
tsarfaty@stp.lingfil.uu.se, {joakim.nivre, evelina.andersson}@lingfil.uu.se
Abstract
We present novel metrics for parse evalua-
tion in joint segmentation and parsing sce-
narios where the gold sequence of terminals
is not known in advance. The protocol uses
distance-based metrics defined for the space
of trees over lattices. Our metrics allow us
to precisely quantify the performance gap be-
tween non-realistic parsing scenarios (assum-
ing gold segmented and tagged input) and re-
alistic ones (not assuming gold segmentation
and tags). Our evaluation of segmentation and
parsing for Modern Hebrew sheds new light
on the performance of the best parsing systems
to date in the different scenarios.
1 Introduction
A parser takes a sentence in natural language as in-
put and returns a syntactic parse tree representing
the sentence?s human-perceived interpretation. Cur-
rent state-of-the-art parsers assume that the space-
delimited words in the input are the basic units of
syntactic analysis. Standard evaluation procedures
and metrics (Black et al, 1991; Buchholz and Marsi,
2006) accordingly assume that the yield of the parse
tree is known in advance. This assumption breaks
down when parsing morphologically rich languages
(Tsarfaty et al, 2010), where every space-delimited
word may be effectively composed of multiple mor-
phemes, each of which having a distinct role in the
syntactic parse tree. In order to parse such input the
text needs to undergo morphological segmentation,
that is, identifying the morphological segments of
each word and assigning the corresponding part-of-
speech (PoS) tags to them.
Morphologically complex words may be highly
ambiguous and in order to segment them correctly
their analysis has to be disambiguated. The multiple
morphological analyses of input words may be rep-
resented via a lattice that encodes the different seg-
mentation possibilities of the entire word sequence.
One can either select a segmentation path prior to
parsing, or, as has been recently argued, one can let
the parser pick a segmentation jointly with decoding
(Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg
and Tsarfaty, 2008; Green and Manning, 2010). If
the selected segmentation is different from the gold
segmentation, the gold and parse trees are rendered
incomparable and standard evaluation metrics break
down. Evaluation scenarios restricted to gold input
are often used to bypass this problem, but, as shall be
seen shortly, they present an overly optimistic upper-
bound on parser performance.
This paper presents a full treatment of evaluation
in different parsing scenarios, using distance-based
measures defined for trees over a shared common
denominator defined in terms of a lattice structure.
We demonstrate the informativeness of our metrics
by evaluating joint segmentation and parsing perfor-
mance for the Semitic language Modern Hebrew, us-
ing the best performing systems, both constituency-
based and dependency-based (Tsarfaty, 2010; Gold-
berg, 2011a). Our experiments demonstrate that, for
all parsers, significant performance gaps between re-
alistic and non-realistic scenarios crucially depend
on the kind of information initially provided to the
parser. The tool and metrics that we provide are
completely general and can straightforwardly apply
to other languages, treebanks and different tasks.
6
(tree1) TOP
PP
IN
0B1
?in?
NP
NP
DEF
1H2
?the?
NP
NN
2CL3
?shadow?
PP
POSS
3FL4
of
PRN
4HM5
?them?
ADJP
DEF
5H6
?the?
JJ
6NEIM7
?pleasant?
(tree2) TOP
PP
IN
0B1
?in?
NP
NP
NN
1CL2
?shadow?
PP
POSS
2FL3
?of?
PRN
3HM4
?them?
VB
4HNEIM5
?made-pleasant?
Figure 1: A correct tree (tree1) and an incorrect tree (tree2) for ?BCLM HNEIM?, indexed by terminal boundaries.
Erroneous nodes in the parse hypothesis are marked in italics. Missing nodes from the hypothesis are marked in bold.
2 The Challenge: Evaluation for MRLs
In morphologically rich languages (MRLs) substan-
tial information about the grammatical relations be-
tween entities is expressed at word level using in-
flectional affixes. In particular, in MRLs such as He-
brew, Arabic, Turkish or Maltese, elements such as
determiners, definite articles and conjunction mark-
ers appear as affixes that are appended to an open-
class word. Take, for example the Hebrew word-
token BCLM,1 which means ?in their shadow?. This
word corresponds to five distinctly tagged elements:
B (?in?/IN), H (?the?/DEF), CL (?shadow?/NN), FL
(?of?/POSS), HM (?they?/PRN). Note that morpho-
logical segmentation is not the inverse of concatena-
tion. For instance, the overt definite article H and
the possessor FL show up only in the analysis.
The correct parse for the Hebrew phrase ?BCLM
HNEIM? is shown in Figure 1 (tree1), and it pre-
supposes that these segments can be identified and
assigned the correct PoS tags. However, morpholog-
ical segmentation is non-trivial due to massive word-
level ambiguity. The word BCLM, for instance, can
be segmented into the noun BCL (?onion?) and M (a
genitive suffix, ?of them?), or into the prefix B (?in?)
followed by the noun CLM (?image?).2 The multi-
tude of morphological analyses may be encoded in a
lattice structure, as illustrated in Figure 2.
1We use the Hebrew transliteration in Sima?an et al (2001).
2The complete set of analyses for this word is provided in
Goldberg and Tsarfaty (2008). Examples for similar phenom-
ena in Arabic may be found in Green and Manning (2010).
Figure 2: The morphological segmentation possibilities
of BCLM HNEIM. Double-circles are word boundaries.
In practice, a statistical component is required to
decide on the correct morphological segmentation,
that is, to pick out the correct path through the lat-
tice. This may be done based on linear local context
(Adler and Elhadad, 2006; Shacham and Wintner,
2007; Bar-haim et al, 2008; Habash and Rambow,
2005), or jointly with parsing (Tsarfaty, 2006; Gold-
berg and Tsarfaty, 2008; Green and Manning, 2010).
Either way, an incorrect morphological segmenta-
tion hypothesis introduces errors into the parse hy-
pothesis, ultimately providing a parse tree which
spans a different yield than the gold terminals. In
such cases, existing evaluation metrics break down.
To understand why, consider the trees in Figure 1.
Metrics like PARSEVAL (Black et al, 1991) cal-
culate the harmonic means of precision and recall
on labeled spans ?i, label, j? where i, j are termi-
nal boundaries. Now, the NP dominating ?shadow
of them? has been identified and labeled correctly
in tree2, but in tree1 it spans ?2,NP, 5? and in tree2
it spans ?1,NP, 4?. This node will then be counted
as an error for tree2, along with its dominated and
dominating structure, and PARSEVAL will score 0.
7
A generalized version of PARSEVAL which con-
siders i, j character-based indices instead of termi-
nal boundaries (Tsarfaty, 2006) will fail here too,
since the missing overt definite article H will cause
similar misalignments. Metrics for dependency-
based evaluation such as ATTACHMENT SCORES
(Buchholz and Marsi, 2006) suffer from similar
problems, since they assume that both trees have the
same nodes ? an assumption that breaks down in
the case of incorrect morphological segmentation.
Although great advances have been made in pars-
ing MRLs in recent years, this evaluation challenge
remained unsolved.3 In this paper we present a solu-
tion to this challenge by extending TEDEVAL (Tsar-
faty et al, 2011) for handling trees over lattices.
3 The Proposal: Distance-Based Metrics
Input and Output Spaces We view the joint task
as a structured prediction function h : X ? Y from
input space X onto output space Y . Each element
x ? X is a sequence x = w1, . . . , wn of space-
delimited words from a setW . We assume a lexicon
LEX, distinct fromW , containing pairs of segments
drawn from a set T of terminals and PoS categories
drawn from a set N of nonterminals.
LEX = {?s, p?|s ? T , p ? N}
Each word wi in the input may admit multiple
morphological analyses, constrained by a language-
specific morphological analyzer MA. The morpho-
logical analysis of an input word MA(wi) can be
represented as a lattice Li in which every arc cor-
responds to a lexicon entry ?s, p?. The morpholog-
ical analysis of an input sentence x is then a lattice
L obtained through the concatenation of the lattices
L1, . . . , Ln where MA(w1) = L1, . . . , MA(wn) =
Ln. Now, let x = w1, . . . , wn be a sentence with
a morphological analysis lattice MA(x) = L. We
define the output space YMA(x)=L for h (abbreviated
YL), as the set of linearly-ordered labeled trees such
that the yield of LEX entries ?s1, p1?,. . . ,?sk, pk? in
each tree (where si ? T and pi ? N , and possibly
k 6= n) corresponds to a path through the lattice L.
3A tool that could potentially apply here is SParseval (Roark
et al, 2006). But since it does not respect word-boundaries, it
fails to apply to such lattices. Cohen and Smith (2007) aimed to
fix this, but in their implementation syntactic nodes internal to
word boundaries may be lost without scoring.
Edit Scripts and Edit Costs We assume a
set A={ADD(c, i, j),DEL(c, i, j),ADD(?s, p?, i, j),
DEL(?s, p?, i, j)} of edit operations which can add
or delete a labeled node c ? N or an entry ?s, p? ?
LEX which spans the states i, j in the lattice L. The
operations in A are properly constrained by the lat-
tice, that is, we can only add and delete lexemes that
belong to LEX, and we can only add and delete them
where they can occur in the lattice. We assume a
function C(a) = 1 assigning a unit cost to every op-
eration a ? A, and define the cost of a sequence
?a1, . . . , am? as the sum of the costs of all opera-
tions in the sequence C(?a1, ..., am?) =
?m
i=1 C(ai).
An edit script ES(y1, y2) = ?a1, . . . , am? is a se-
quence of operations that turns y1 into y2. The tree-
edit distance is the minimum cost of any edit script
that turns y1 into y2 (Bille, 2005).
TED(y1, y2) = min
ES(y1,y2)
C(ES(y1, y2))
Distance-Based Metrics The error of a predicted
structure p with respect to a gold structure g is now
taken to be the TED cost, and we can turn it into a
score by normalizing it and subtracting from a unity:
TEDEVAL(p, g) = 1?
TED(p, g)
|p|+ |g| ? 2
The term |p| + |g| ? 2 is a normalization factor de-
fined in terms of the worst-case scenario, in which
the parser has only made incorrect decisions. We
would need to delete all lexemes and nodes in p and
add all the lexemes and nodes of g, except for roots.
An Example Both trees in Figure 1 are contained
in YL for the lattice L in Figure 2. If we re-
place terminal boundaries with lattice indices from
Figure 2, we need 6 edit operations to turn tree2
into tree1 (deleting the nodes in italic, adding the
nodes in bold) and the evaluation score will be
TEDEVAL(tree2,tree1) = 1? 614+10?2 = 0.7273.
4 Experiments
We aim to evaluate state-of-the-art parsing architec-
tures on the morphosyntactic disambiguation of He-
brew texts in three different parsing scenarios: (i)
Gold: assuming gold segmentation and PoS-tags,
(ii) Predicted: assuming only gold segmentation,
and (iii) Raw: assuming unanalyzed input text.
8
SEGEVAL PARSEVAL TEDEVAL
Gold PS U: 100.00 U: 94.35
L: 100.00 L: 88.75 L: 93.39
Predicted PS U: 100.00 U: 92.92
L: 90.85 L: 82.30 L: 86:26
Raw PS U: 96.42 U: 88.47
L: 84.54 N/A L: 80.67
Gold RR U: 100.00 U: 94.34
L: 100.00 L: 83.93 L: 92.45
Predicted RR U: 100.00 U: 92.82
L: 91.69 L: 78.93 L: 85.83
Raw RR U: 96.03 U: 87.96
L: 86.10 N/A L: 79.46
Table 1: Phrase-Structure based results for the Berke-
ley Parser trained on bare-bone trees (PS) and relational-
realizational trees (RR). We parse all sentences in the dev
set. RR extra decoration is removed prior to evaluation.
SEGEVAL ATTSCORES TEDEVAL
Gold MP 100.00 U: 83.59 U: 91.76
Predicted MP 100.00 U: 82.00 U: 91.20
Raw MP 95.07 N/A U: 87.03
Gold EF 100.00 U: 84.68 U: 92.25
Predicted EF 100.00 U: 83.97 U: 92:02
Raw EF 95.07 N/A U: 87.75
Table 2: Dependency parsing results by MaltParser (MP)
and EasyFirst (EF), trained on the treebank converted into
unlabeled dependencies, and parsing the entire dev-set.
For constituency-based parsing we use two mod-
els trained by the Berkeley parser (Petrov et al,
2006) one on phrase-structure (PS) trees and one
on relational-realizational (RR) trees (Tsarfaty and
Sima?an, 2008). In the raw scenario we let a lattice-
based parser choose its own segmentation and tags
(Goldberg, 2011b). For dependency parsing we use
MaltParser (Nivre et al, 2007b) optimized for He-
brew by Ballesteros and Nivre (2012), and the Easy-
First parser of Goldberg and Elhadad (2010) with the
features therein. Since these parsers cannot choose
their own tags, automatically predicted segments
and tags are provided by Adler and Elhadad (2006).
We use the standard split of the Hebrew tree-
bank (Sima?an et al, 2001) and its conversion into
unlabeled dependencies (Goldberg, 2011a). We
use PARSEVAL for evaluating phrase-structure trees,
ATTACHSCORES for evaluating dependency trees,
and TEDEVAL for evaluating all trees in all scenar-
ios. We implement SEGEVAL for evaluating seg-
mentation based on our TEDEVAL implementation,
replacing the tree distance and size with string terms.
Table 1 shows the constituency-based parsing re-
sults for all scenarios. All of our results confirm
that gold information leads to much higher scores.
TEDEVAL allows us to precisely quantify the drop
in accuracy from gold to predicted (as in PARSE-
VAL) and than from predicted to raw on a single
scale. TEDEVAL further allows us to scrutinize the
contribution of different sorts of information. Unla-
beled TEDEVAL shows a greater drop when moving
from predicted to raw than from gold to predicted,
and for labeled TEDEVAL it is the other way round.
This demonstrates the great importance of gold tags
which provide morphologically disambiguated in-
formation for identifying phrase content.
Table 2 shows that dependency parsing results
confirm the same trends, but we see a much smaller
drop when moving from gold to predicted. This is
due to the fact that we train the parsers for predicted
on a treebank containing predicted tags. There is
however a great drop when moving from predicted
to raw, which confirms that evaluation benchmarks
on gold input as in Nivre et al (2007a) do not pro-
vide a realistic indication of parser performance.
For all tables, TEDEVAL results are on a simi-
lar scale. However, results are not yet comparable
across parsers. RR trees are flatter than bare-bone
PS trees. PS and DEP trees have different label
sets. Cross-framework evaluation may be conducted
by combining this metric with the cross-framework
protocol of Tsarfaty et al (2012).
5 Conclusion
We presented distance-based metrics defined for
trees over lattices and applied them to evaluating
parsers on joint morphological and syntactic dis-
ambiguation. Our contribution is both technical,
providing an evaluation tool that can be straight-
forwardly applied for parsing scenarios involving
trees over lattices,4 and methodological, suggesting
to evaluate parsers in all possible scenarios in order
to get a realistic indication of parser performance.
Acknowledgements
We thank Shay Cohen, Yoav Goldberg and Spence
Green for discussion of this challenge. This work
was supported by the Swedish Science Council.
4The tool can be downloaded http://stp.ling.uu.
se/?tsarfaty/unipar/index.html
9
References
Meni Adler and Michael Elhadad. 2006. An unsuper-
vised morpheme-based HMM for Hebrew morpholog-
ical disambiguation. In Proceedings of COLING-ACL.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: A system for MaltParser optimization. Istan-
bul.
Roy Bar-haim, Khalil Sima?an, and Yoad Winter. 2008.
Part-of-speech tagging of Modern Hebrew text. Natu-
ral Language Engineering, 14(2):223?251.
Philip Bille. 2005. A survey on tree-edit distance
and related. problems. Theoretical Computer Science,
337:217?239.
Ezra Black, Steven P. Abney, D. Flickenger, Claudia
Gdaniec, Ralph Grishman, P. Harrison, Donald Hin-
dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-
vans, Mark Liberman, Mitchell P. Marcus, Salim
Roukos, Beatrice Santorini, and Tomek Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proceed-
ings of the DARPA Workshop on Speech and Natural
Language.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL, pages 208?217.
Yoav Goldberg and Michael Elhadad. 2010. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of NAACL/HLT workshop on Statistical Parsing
of Morphologically Rich Languages.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg. 2011a. Automatic Syntactic Processing
of Modern Hebrew. Ph.D. thesis, Ben-Gurion Univer-
sity of the Negev.
Yoav Goldberg. 2011b. Joint morphological segmen-
tation and syntactic parsing using a PCFGLA lattice
parser. In Proceedings of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(1):1?41.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Brian Roark, Mary Harper, Eugene Charniak, Bon-
nie Dorr C, Mark Johnson D, Jeremy G. Kahn
E, Yang Liu F, Mari Ostendorf E, John Hale
H, Anna Krasnyanskaya I, Matthew Lease D,
Izhak Shafran J, Matthew Snover C, Robin Stewart K,
and Lisa Yung J. 2006. Sparseval: Evaluation metrics
for parsing speech. In Proceesings of LREC.
Danny Shacham and Shuly Wintner. 2007. Morpholog-
ical disambiguation of Hebrew: A case study in clas-
sifier combination. In Proceedings of the 2007 Joint
Conference of EMNLP-CoNLL, pages pages 439?447.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique des
Langues.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, San-
dra Kuebler, Marie Candito, Jennifer Foster, Yan-
nick Versley, Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing for morphologically rich language
(SPMRL): What, how and whither. In Proceedings of
the first workshop on Statistical Parsing of Morpho-
logically Rich Languages (SPMRL) at NA-ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In Proceedings of EACL.
Reut Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for Modern Hebrew. In Pro-
ceeding of ACL-SRW.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
10
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 578?584,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Unified Morpho-Syntactic Scheme of Stanford Dependencies
Reut Tsarfaty
Uppsala University, Sweden
tsarfaty@stp.lingfil.uu.se
Abstract
Stanford Dependencies (SD) provide a
functional characterization of the gram-
matical relations in syntactic parse-trees.
The SD representation is useful for parser
evaluation, for downstream applications,
and, ultimately, for natural language un-
derstanding, however, the design of SD fo-
cuses on structurally-marked relations and
under-represents morphosyntactic realiza-
tion patterns observed in Morphologically
Rich Languages (MRLs). We present a
novel extension of SD, called Unified-SD
(U-SD), which unifies the annotation of
structurally- and morphologically-marked
relations via an inheritance hierarchy. We
create a new resource composed of U-SD-
annotated constituency and dependency
treebanks for the MRL Modern Hebrew,
and present two systems that can automat-
ically predict U-SD annotations, for gold
segmented input as well as raw texts, with
high baseline accuracy.
1 Introduction
Stanford Dependencies (SD) provide a functional
characterization of the grammatical relations in
syntactic trees, capturing the predicate-argument
structure of natural language sentences (de Marn-
effe et al, 2006). The SD representation proved
useful in a range of downstream tasks, includ-
ing Textual Entailments (Dagan et al, 2006) and
BioNLP (Fundel and Zimmer., 2007), and in re-
cent years SD structures have also become a de-
facto standard for parser evaluation in English (de
Marneffe and Manning, 2008a; Cer et al, 2010;
Nivre et al, 2010). Efforts now commence to-
wards extending SD for cross-lingual annotation
and evaluation (McDonald et al, 2013; Che et al,
2012; Haverinen et al, 2011). By and large, these
efforts aim to remain as close as possible to the
original SD scheme. However, the original SD de-
sign emphasizes word-tokens and configurational
structures, and consequently, these schemes over-
look properties and realization patterns observed
in a range of languages known as Morphologically
Rich Languages (MRLs) (Tsarfaty et al, 2010).
MRLs use word-level affixes to express gram-
matical relations that are typically indicated by
structural positions in English. By virtue of
word-level morphological marking, word-order in
MRLs may be flexible. MRLs have been a fo-
cal point for the parsing community due to the
challenges that these phenomena pose for systems
originally developed for English.1 Here we argue
that the SD hierarchy and design principles simi-
larly emphasize English-like structures and under-
represent morphosyntactic argument-marking al-
ternatives. We define an extension of SD, called
Unified-SD (U-SD), which unifies the annotation
of structurally and morphologically marked rela-
tions via an inheritance hierarchy. We extend SD
with a functional branch, and provide a principled
treatment of morpho-syntactic argument marking.
Based on the U-SD scheme we create a new
parallel resource for the MRL Modern Hebrew,
whereby aligned constituency and dependency
trees reflect equivalent U-SD annotations (cf.
Rambow (2010)) for the same set of sentences. We
present two systems that can automatically learn
U-SD annotations, from the dependency and the
constituency versions respectively, delivering high
baseline accuracy on the prediction task.
1See also the SPMRL line of workshops https://
sites.google.com/site/spsemmrl2012/ and the
MT-MRL workshop http://cl.haifa.ac.il/MT/.
578
2 The Challenge: SD for MRLs
Stanford Dependencies (SD) (de Marneffe et al,
2006; de Marneffe and Manning, 2008b) deliver a
functional representation of natural language sen-
tences, inspired by theoretical linguistic work such
as studies on Relational Grammars (Postal and
Perlmutter, 1977), Lexical Functional Grammars
(LFG) (Bresnan, 2000) and the PARC dependency
scheme (King et al, 2003). At the same time, the
scheme is designed with end-users in mind, allow-
ing them to utilize parser output in a form which
is intuitively interpretable and easily processed.
SD basic trees represent sentences as binary
relations between word tokens. These relations
are labeled using traditional grammatical concepts
(subject, object, modifier) that are arranged into an
inheritance hierarchy (de Marneffe and Manning,
2008a, Sec. 3). There are different versions of SD
annotations: the basic SD scheme, which anno-
tates surface dependency relations as a tree span-
ning all word tokens in the sentence, and the col-
lapsed SD version, in which function words (such
as prepositions) are collapsed and used for speci-
fying a direct relation between content words.
The SD scheme defines a core set of labels
and principles which are assumed to be useful
for different languages. However, a close exam-
ination of the SD label-set and inheritance hier-
archy reveals that some of its design principles
are geared towards English-like (that is, configu-
rational) phenomena, and conflict with basic prop-
erties of MRLs. Let us list three such design prin-
ciples and outline the challenges that they pose.
2.1. SD relate input-tokens. In MRLs, substan-
tial information is expressed as word affixes. One
or more morphemes may be appended to a content
word, and several morphemes may be contained in
a single space-delimited token. For example, the
Hebrew token wkfraiti2 in (1) includes the mor-
phemes w (and), kf (when) and raiti (saw); the lat-
ter segment is a content word, and the former two
are functional morphemes.
(1) wkfraiti
and-when-saw.1st.Singular
at
acc
hsrj
the-movie
hifn
the-old
w/and-1.1 kf/when-1.2 raiti/saw-1.3
at/acc-2 h/the-3.1 srj/movie-3.2 h/the-4.1
ifn/old-4.2
2We use the transliteration of Sima?an et al (2001).
(a) S
NP-sbj
?John?
VP
V-prd
?loves?
NP-obj
?Mary?
(b) S
NP-sbj
?dan?
Dan
V-prd
?ohev?
loves
NP-obj
?et-dana?
ACC-Dana
Figure 1: English (a) and Hebrew (b) PS trees dec-
orated with function labels as dash features.
Na??vely taking input tokens as words fails to cap-
ture meaningful relations between morphological
segments internal to space-delimited tokens.
2.2. SD label structurally-marked relations.
Configurational languages like English use func-
tion words such as prepositions and auxiliaries
to indicate relations between content words and
to mark properties of complete structures. In
MRLs, such relations and properties may be indi-
cated by word-level morphological marking such
as case (Blake, 1994) and agreement (Corbett,
2006). In (1), for instance, the case marker at indi-
cates an accusative object relation between ?see?
and ?movie?, to be distinguished from, e.g, a da-
tive object. Moreover, the agreement in (1) on
the definite morpheme signals that ?old? modifies
?movie?. While the original SD scheme label-set
covers function words (e.g. auxpass, expl, prep),
it misses labels for bound morphemes that mark
grammatical relations across languages (such as
accusative, dative or genitive). Explicit labeling of
such relational morphemes will allow us to benefit
from the information that they provide.
2.3. SD relations may be inferred using struc-
tural cues. SD relations are extracted from dif-
ferent types of trees for the purpose of, e.g., cross-
framework evaluation (Cer et al, 2010). Inso-
far, recovering SD relations from phrase-structure
(PS) trees have used a range of structural cues
such as positions and phrase-labels (see, for in-
stance, the software of de Marneffe and Manning
(2008a)). In MRLs, positions and phrase types
may not suffice for recovering SD relations: an
NP under S in Hebrew, for instance, may be a
subject or an object, as shown in Figure 1, and
morphological information then determines the
function of these constituents. Automatically in-
ferring predicate-argument structures across tree-
banks thus must rely on both structural and mor-
phological marking, calling for a single annotation
scheme that inter-relate the marking alternatives.
579
gf
root hdprdexist nhdghd depargsbj obj comp mod conj
func
markerprep caseacc gen detdef
auxpassaux cop subrel complmn cc
punct
Figure 3: The Unified SD (U-SD) Ontology. The architectural changes from the original SD scheme: (i)
added a hd branch, for implicit head labels; (ii) added a func branch where all functional elements (prep,
aux, cc, rel) as well as morphological markers are moved under; (iii) there is a clear separation between
open-class categories (which fall under hd, dep), closed class elements (under func) and non-words (root
and punct). Boldface elements are new to U-SD. Italic branches spell out further as in the original SD.
(a) ROOT
S-root
V-prd
?raiti?
NP-obj
ACC-acc
?at?
NP-hd
NP-hd
H-def
?h?
NN-hd
?srj?
ADJP-mod
H-def
?h?
ADJ-hd
?ifn?
(b) ROOT
root
?raiti?/V
obj
?at?/ACC
hd
?srj?/NN
def
h/H
mod
?ifn?/ADJ
def
h/H
(c) ROOT
root
?raiti?/V
ACC-obj
?srj?/NN.DEF
mod
?ifn?/ADJ.DEF
Figure 2: Sample U-SD Trees for sentence (1).
(a) a phrase-structure tree decorated with U-SD la-
bels, (b) a basic U-SD tree, and (c) a collapsed U-
SD tree, where functional nodes are consumed.
3 The Proposal: Unified-SD (U-SD)
To address these challenges, we propose an exten-
sion of SD called Unified-SD (U-SD) which an-
notates relations between morphological segments
and reflects different types of argument-marking
patterns. The SD ontology is re-organized and ex-
tended to allow us to annotate morphologically-
and structurally-marked relations alike.
Preliminaries. We assume that M(w1...wn) =
s1....sm is a morphological analysis function that
identifies all morphological segments of a sen-
tence S = w1...wn. The U-SD scheme provides
the syntactic representation of S by means of a set
of triplets (l, si, sj) consisting of a label l, a head
si and a dependent sj (i 6= j). The segments are
assumed to be numbered x.y where x is the posi-
tion of the input token, and y is the position of the
segment inside the token. The segmentation num-
bering is demonstrated in Example (1).
The U-SD Hierarchy. Figure 3 shows our pro-
posed U-SD hierarchy. Everything in the ontol-
ogy is of type gf (grammatical function). We
define five ontological sub-types: root, hd, dep,
func, punct. The root marks a special root de-
pendency. The dep branch is used for depen-
dent types, and it retains much of the structure in
the original SD scheme (separating sbj types, obj
types, mod types, etc.). The new func branch con-
tains argument-marking elements, that is, function
words and morphemes that play a role in indicat-
ing properties or grammatical relations in the syn-
tactic representation. These functional elements
may be of types marker (prepositions and case),
aux (auxiliary verbs and copular elements) and sub
(subordination/conjunction markers). All inher-
ited func elements may be consumed (henceforth,
collapsed) in order to infer grammatical proper-
ties and relations between content words. Head
types are implicit in dependency triplets, however,
when decorating PS trees with dependency labels
as dash features or edge features (as in TigerXML
formats (Brants et al, 2002) or via unification-
based formalisms) both heads and dependents are
labeled with their grammatical types (see Fig-
ure 2(a)). The hd branch extends the scheme with
an inventory of argument-taking elements, to be
used when employing SD inside constituency tree-
banks. The punct branch is reserved for punctu-
ation, prosody and other non-verbal speech acts.
The complete ontology is given in the appendix.
Annotation Guidelines. Anderson (1992) de-
lineates three kinds of properties that are realized
by morphology: structural, inherent, and agree-
ment properties. Structural properties (e.g., case)
are marked on a content word to indicate its rela-
580
Gold:
Segments Functions
DEP 1.00 0.8475RR 1.00 0.8984 Predicted:
Segments Functions
DEP 1.00 0.8349RR 1.00 0.8559 Raw:
Segments Functions
DEP 0.9506 0.7817RR 0.9603 0.8130
Table 1: Inferring U-SD trees using different frameworks. All numbers report labeled TedEval accuracy.
tion to other parts of the sentence. Inherent prop-
erties (gender, number, etc.) indicate inherent se-
mantic properties of nominals. Agreement prop-
erties indicate the semantic properties of nominals
on top of other elements (verbs, adjectives, etc.),
in order to indicate their relation to the nominals.
We define annotation guidelines that reflect
these different properties. Structural morphemes
(case) connect words in the arc-structure, linking
a head to its semantic dependent, like the case
marker ?at?-ACC in Figure 2(b). Inherent / agree-
ment properties are annotated as dependents of the
content word they add properties to, for instance,
the prefixes def in Figure 2(b) hang under the mod-
ified noun and adjective.
Collapsed U-SD structures interpret func ele-
ments in order to refine the representation of re-
lations between content words. Case markers can
be used for refining the relation between the con-
tent words they connect by labeling their direct re-
lation, much like prep in the original SD scheme
(see, e.g., the ACC-obj in Figure 2c). Inher-
ent/agreement features are in fact features of their
respective head word (as the X.DEF nodes in Fig-
ure 2c).3 Auxiliaries may further be used to add
tense/aspect to the main predicate, and subordina-
tors may propagate information inside the struc-
ture (much like conjunction is propagated in SD).
Universal Aspects of U-SD. The revised U-
SD ontology provides a typological inventory
of labels that describe different types of argu-
ments (dep), argument-taking elements (hd), and
argument-marking elements (func) in the grammar
of different languages. Abstract (universal) con-
cepts reside high in the hierarchy, and more spe-
cific distinctions, e.g., morphological markers of
particular types, are daughters within more spe-
cific branches. Using U-SD for evaluating mono-
lingual parsers is best done with the complete label
set relevant for that language. For cross-language
evaluation, we can limit the depth of the hierar-
chy, and convert the more specific notions to their
most-specific ancestor in the evaluation set.
3Technically, this is done by deleting a line adding a prop-
erty to the morphology column in the CoNLL format.
4 Automatic Annotation of U-SD Trees
Can U-SD structures be automatically predicted?
For MRLs, this requires disambiguating both mor-
phological and syntactic information. Here we
employ the U-SD scheme for annotating mor-
phosyntactic structures in Modern Hebrew, and
use these resources to train two systems that pre-
dict U-SD annotations for raw texts.4
Data. We use the Modern Hebrew treebank
(Sima?an et al, 2001), a corpus of 6220 sentences
morphologically segmented and syntactically an-
alyzed as PS trees. We infer the function label
of each node in the PS trees based on the mor-
phological features, syntactic environment, and
dash-feature (if exist), using deterministic gram-
mar rules (Glinert, 1989). Specifically, we com-
pare each edge with a set of templates, and, once
finding a template that fits the morphological and
syntactic profile of an edge, we assign functions
to all daughters. This delivers PS trees where each
node is annotated with a U-SD label (Figure 2a).
At a second stage we project the inferred labels
onto the arcs of the unlabeled dependency trees of
Goldberg (2011), using the tree unification opera-
tion of Tsarfaty et al (2012a). The result is a de-
pendency tree aligned with the constituency tree
where dependency arcs are labeled with the same
function as the respective span in the PS tree.5
Systems. We present two systems that predict
U-SD labels along with morphological and syn-
tactic information, using [DEP], a dependency
parser (Nivre et al, 2007), and [RR], a Relational-
Realizational (RR) constituency parser (Tsarfaty
and Sima?an, 2008). DEP is trained directly on the
dependency version of the U-SD resource. Since
it cannot predict its own segmentation, automatic
segments and tags are predicted using the system
of Adler and Elhadad (2006). The constituency-
4Despite significant advances in parsing Hebrew, as of yet
there has been no functional evaluation of Hebrew parsers.
E.g., Goldberg and Elhadad (2010) evaluate on unlabeled de-
pendencies, Tsarfaty (2010) evaluate on constituents. This is
largely due to the lack of standard resources and guidelines
for annotating functional structures in such a language.
5The resources can be downloaded at http://www.
tsarfaty.com/heb-sd/.
581
based model is trained on U-SD-labeled RR trees
using Petrov et al (2006). We use the lattice-based
extension of Goldberg and Elhadad (2011) to per-
form joint segmentation and parsing. We evalu-
ate three input scenarios: [Gold] gold segmen-
tation and gold tags, [Predicted] gold segments,
and [Raw] raw words. We evaluate parsing results
with respect to basic U-SD trees, for 42 labels. We
use TedEval for joint segmentation-tree evaluation
(Tsarfaty et al, 2012b) and follow the cross-parser
evaluation protocol of Tsarfaty et al (2012a).
Results. Since this work focuses on creating a
new resource, we report results on the standard
devset (Table 1). The gold input scenarios ob-
tain higher accuracy on function labels in all cases,
since gold morphological analysis delivers disam-
biguated functions almost for free. Constituency-
based RR structures obtain better accuracy on U-
SD annotations than the respective dependency
parser. All in all, the U-SD seed we created allows
us to infer rich interpretable annotations automati-
cally for raw text, using either a dependency parser
or a constituency parser, in good accuracy.
5 Conclusion
The contribution of this paper is three-fold. We of-
fer a principled treatment of annotating MRLs via
a Unified-SD scheme, which we design to be ap-
plicable to many languages. We deliver new U-SD
annotated resources for the MRL Modern Hebrew,
in different formal types. We finally present two
systems that automatically predict U-SD annota-
tions for raw texts. These structures are intended
to serve semantic applications. We further intend
to use this scheme and computational frameworks
to serve a wide cross-parser investigation on infer-
ring functional structures across languages.
Appendix: The U-SD Ontology
The list in (2) presents the complete U-SD ontol-
ogy. The hierarchy employs and extends the SD
label set of de Marneffe et al (2006). For read-
ability, we omit here various compound types un-
der mod, including nn, mwe, predet and preconj.
Acknowledgements
We thank Joakim Nivre, Yoav Goldberg, Djame?
Seddah and anonymous reviewers for comments
and discussion. This research was partially funded
by the Swedish Research Council. The author is
now a researcher at the Weizmann Institute.
(2) gf root - root
hd - head (governor, argument-taking)
prd - verbal predicate
exist - head of an existential phrase
nhd - head of a nominal phrase
ghd - genitive head of a nominal phrase
dep - dependent (governed, or an argument)
arg - argument
agent - agent
comp - complement
acomp - adjectival complement
ccomp - comp clause with internal sbj
xcomp - comp clause with external sbj
pcomp - comp clause of a preposition
obj - object
dobj - direct object
gobj - genitive object
iobj - indirect object
pobj - object of a preposition
subj - subject
expl - expletive subject
nsubj - nominal subject
? nsubjpass - passive nominal sbj
csubj - clausal subject
? csubjpass - passive clausal sbj
mod - modifier
appos - apposition/parenthetical
abbrev - abbreviation
amod - adjectival modifier
advmod - adverbial modifier
? neg - negative modifier
prepmod - prepositional modifier
? possmod - possession modifier
? tmod - temporal modifier
rcmod - relative clause modifier
infmod - infinitival modifier
nummod - numerical modifier
parataxis - ?side-by-side?, interjection
conj - conjuct
func - functional (argument marking)
marker - nominal-marking elements
prep - preposition
case - case marker
? acc - accusative case
? dat - dative case
? gen - genitive case
? nom - nominative case
det - determiner
? def - definite marker
? dem - demonstrative
sub - phrase-marking elements
complm - introducing comp phrase
rel - introducing relative phrase
cc - introducing conjunction
mark - introducing an advb phrase
aux - auxiliary verb or a feature-bundle
auxpass - passive auxiliary
cop - copular element
modal - modal verb
qaux - question auxiliary
punct - punctuation
582
References
Meni Adler and Michael Elhadad. 2006. An
unsupervised morpheme-based HMM for Hebrew
morphological disambiguation. In Proceedings of
COLING-ACL.
Stephen R. Anderson. 1992. A-Morphous Morphol-
ogy. Cambridge University Press.
Barry J. Blake. 1994. Case. Cambridge University
Press, Cambridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of TLT.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Parsing
to stanford dependencies: Trade-offs between speed
and accuracy. In Proceedings of LREC.
Wanxiang Che, Valentin I. Spitkovsky, and Ting Liu.
2012. A comparison of chinese parsers for stanford
dependencies. In Proceedings of ACL, pages 11?16.
Greville G. Corbett. 2006. Agreement. Cam-
bridge Textbooks in Linguistics. Cambridge Univer-
sity Press.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In MLCW 2005, LNAI Volume.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008a. Stanford dependencies manual. Tech-
nical Report.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008b. The stanford typed dependencies
representation. In Proceedings of the workshop on
Cross-Framework and Cross-Domain Parser Evalu-
ation.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Robert Kuffner Fundel, Katrin and Ralf Zimmer. 2007.
RelEx relation extraction using dependency parse
trees. Bioinformatics, (23).
Lewis Glinert. 1989. The Grammar of Modern He-
brew. Cambridge University Press.
Yoav Goldberg and Michael Elhadad. 2010. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of NAACL/HLT workshop on Statistical Parsing
of Morphologically Rich Languages.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFGLA lat-
tice parser. In Proceedings of ACL.
Yoav Goldberg. 2011. Automatic syntactic processing
of Modern Hebrew. Ph.D. thesis, Ben Gurion Uni-
versity of the Negev.
Katri Haverinen, Filip Ginter, Samuel Kohonen, Timo
Viljanen, Jenna Nyblom, and Tapio Salakoski.
2011. A dependency-based analysis of treebank an-
notation errors. In Proceedings of DepLing.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald Kaplan. 2003. The PARC
700 dependency bank. In The 4th International
Workshop on Linguistically Interpreted Corpora.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car Ta?ckstro?m, Claudia Bedini, Nu?ria Bertomeu
Castello?, and Jungmee Lee. 2013. Universal depen-
dency annotation for multilingual parsing. In Pro-
ceedings of ACL.
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(1):1?41.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-
los Go?mez-Rodr??guez. 2010. Evaluation of depen-
dency parsers on unbounded dependencies. In Pro-
ceedings of COLING, pages 813?821.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Paul M. Postal and David M. Perlmutter. 1977. Toward
a universal characterization of passivization. In BLS
3.
Owen Rambow. 2010. The Simple Truth about De-
pendency and Phrase Structure Representations: An
Opinion Piece. In Proceedings of HLT-ACL.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
Kuebler, Marie Candito, Jennifer Foster, Yannick
Versley, Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing for morphologically rich language
(SPMRL): What, how and whither. In Proceedings
of the first workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL) at NA-ACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical
parsing. In Proceeding of EACL.
583
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and pars-
ing. In Proceedings of ACL.
Reut Tsarfaty. 2010. Relational-Realizational Pars-
ing. Ph.D. thesis, University of Amsterdam.
584
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 40?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Modeling Morphosyntactic Agreement
in Constituency-Based Parsing of Modern Hebrew
Reut Tsarfaty?and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
{r.tsarfaty,k.simaan}@uva.nl
Abstract
We show that na??ve modeling of morphosyn-
tactic agreement in a Constituency-Based
(CB) statistical parsing model is worse than
none, whereas a linguistically adequate way
of modeling inflectional morphology in CB
parsing leads to improved performance. In
particular, we show that an extension of the
Relational-Realizational (RR) model that in-
corporates agreement features is superior to
CB models that treat morphosyntax as state-
splits (SP), and that the RR model benefits
more from inflectional features. We focus on
parsing Hebrew and report the best result to
date, F184.13 for parsing off of gold-tagged
text, 5% error reduction from previous results.
1 Introduction
Agreement is defined by linguists as the system-
atic covariance of the grammatical properties of one
linguistic element to reflect the semantic or formal
properties of another (Corbett, 2001). Morpholog-
ically marked agreement features such as gender,
number and person are used to realize grammat-
ical relations between syntactic constituents, and
such patterns are abundantly found in (less- or) non-
configurational languages (Hale, 1983) where the
order of words is known to be (relatively) free.
Agreement features encompass information con-
cerning the functional relations between constituents
in the syntactic structure, but whether incorporat-
ing agreement features in a statistical parsing model
leads to improved performance has so far remained
an open question and saw contradictory results.
?The first author is currently a researcher at the department
of Linguistics and Philology at Uppsala University.
Taking Semitic languages as an example, it was
shown that an SVM-based shallow parser (Gold-
berg et al, 2006) does not benefit from includ-
ing agreement features for NP chunking in Hebrew.
Phrase-structure based parsers for Arabic system-
atically discard morphological features from their
label-set and never parametrize agreement explic-
itly (Maamouri et al, 2008). Models based on deep
grammars such as CCG (Hockenmaier and Steed-
man, 2003) and HPSG (Miyao and Tsujii, 2008)
could in principle use inflectional morphology, but
they currently rely on functional information mainly.
For formalisms that do incorporate morphology,
generative models are may leak probability due to
unification failures (Abney, 1997). Even results
from dependency parsing remain inconclusive. It
was shown for dependency parsing that case, defi-
niteness and animacy features are useful to enhance
parsing (e.g., (?vrelid and Nivre, 2007)), agreement
patterns are often excluded. When agreement fea-
tures were included as features in dependency parser
for Hebrew in (Goldberg and Elhadad, 2009) for He-
brew they obtained tiny-to-no improvement.
A question thus emerges whether there are any
benefits in explicitly incorporating morphosyntactic
agreement patterns into our models. This question is
a manifestation of a greater issue, namely, whether
it is beneficial to represent complex patterns of mor-
phology in the statistical parsing model, or whether
configurational information subsume the relevant
patterns, as it is commonly assumed in constituency-
based parsing. Here we claim that agreement fea-
tures are useful for statistical parsing provided that
they are represented and parametrized in a way that
reflects their linguistic substance; to express func-
tional information orthogonal to configuration.
40
We do so by extending the Relational-
Realizational (RR) model we presented in (Tsarfaty
and Sima?an, 2008) to explicitly encode agreement
features in its native representation (RR-AGR). In
the RR model, a joint distribution over grammatical
relations is firstly articulated in the projection phase.
The grammatical relations may be spelled out by
positioning them with respect to one another in the
configuration phase, through the use of morphology
in the realization phase, or both. This paper shows
that, for Hebrew, this RR-AGR strategy signifi-
cantly outperforms a constituency-based model that
treats agreement features as internally structured
non-terminal state-splits (SP-AGR). As we accumu-
late morphological features, the performance gap
between the RR and SP models becomes larger.
The best result we report for the RR-AGR model,
F184.13, is the best result reported for Hebrew to
date for parsing gold PoS-tagged segments, with
5% error reduction from previous results. This
result is also significantly higher than all parsing
results reported so far for Arabic, a Semitic lan-
guage with similar morphosyntactic phenomena.1
The RR approach is shown to be an adequate way
to model complex morphosyntactic patterns for im-
proving constituency-based parsing of a morpholog-
ically rich, free word order language. Because the
RR model is also proper and generative, it may also
embed as a language model to enhance more com-
plex NLP tasks, e.g., statistical Machine Translation.
2 The Data
The grammar of nonconfigurational languages al-
lows for freedom in word ordering and discontinu-
ities of syntactic constituents (Hale, 1983). Such
languages do not rely on configurational information
such as position and adjacency in marking grammat-
ical relations such as subject and object, but instead
they use word-level morphology. One way to encode
grammatical relations in the form of words is by us-
ing morphological case, that is, explicitly marking
an argument (e.g. nominative, accusative) with re-
spect to its grammatical function. In (Tsarfaty et
al., 2009) we showed that incorporating case indeed
leads to improved performance for constituency-
based, Relational-Realizational parsing of Hebrew.
1In (Maamouri et al, 2008), F178.1 for gold standard input.
A more involved way to morphologically encode
grammatical relations is by making explicit refer-
ence to the properties of multiple linguistic ele-
ments. This is the general pattern of agreement, i.e.,
?[A] systematic covariance between a se-
mantic or a formal property of one ele-
ment and a formal property of another.?
(Steele, adapted from (Corbett, 2001))
Describing agreement patterns involves explicit
reference to the following four components; the el-
ement which determines the agreement properties
is the Controller of the agreement, the element
whose properties are determined by agreement is
the Target, the syntactic environment in which the
agreement occurs is the Domain of agreement, and
the properties with respect to which they agree are
agreement Features (Corbett, 2001). Agreement is
an inherently asymmetrical relation. Combination
of features displayed by controllers has to be ac-
commodated by the inflectional features of the tar-
get, but there is no opposite requirement. Let us il-
lustrate the formal description of agreement through
Subject-Verb agreement familiar from English (1).
(1) a. Subject-Verb Agreement in English:
Controller: NP
Target: V
Domain: S
Features: number, person
b. Example:
i. They like the paper
ii. *They likes the paper
The agreement target (the verb) in English has a
rich enough inflectional paradigm that reflects the
person and number features inherent in controllers
? the nouns that realize subjects. (But nouns in En-
glish need not reflect, say, tense.) Had the subject
been an NP, e.g., the phrase ?the committee?, the
agreement pattern would have had to be determined
by the features of the entire NP, and in English the
features of the phrase would be determined by the
lexical head ?committee?. The controller of the
agreement (noun) does not coincide with the head of
the lexical dependency (the verb), which means that
the direction of morphological dependencies need
not coincide with that of lexical dependencies.
41
The Semitic LanguageModernHebrew Modern
Hebrew, (henceforth, Hebrew) is a Semitic language
with a flexible word order and rich morphological
structure. Hebrew nouns morphologically reflect
their inherent gender and number. Pronouns also
reflect person features. Hebrew verbs are inflected
to reflect gender, number, person and tense. Adjec-
tives are inflected to reflect the inherent properties of
nouns, and both nouns and adjectives are inflected
for definiteness. The Hebrew grammar uses this ar-
senal of properties to implement a wide variety of
agreement patterns realizing grammatical relations.
Agreement in Hebrew S Domains Hebrew man-
ifests different patterns of agreement in its S do-
main. Verbal predicates (the target) in matrix sen-
tences (the domain) agree with their nominal sub-
jects (the controller) on the agreement features gen-
der, number and person. This occurs regardless of
their configurational positions, as illustrated in (2b).
(2) a. Agreement in Verbal Sentences:
Controller: NP
Target: V
Domain: S
Features: number, person, gender
b. i. ????? ???? ??? ???
dani
Dani.3MS
natan
gave.3MS
matana
present
ledina
to-Dina
Dani gave a present to Dina (SVO)
ii. ????? ??? ??? ????
matana
present
natan
gave.3MS
dani
Dani.3MS
ledina
to-Dina
Dani gave a present to Dina (VI)
Subject-Predicate agreement relations are not
only independent of surface positions, but are also
orthogonal to the syntactic distributional type of
the constituent realizing the predicate. Semitic lan-
guages allow for predicates to be realized as an
NP, an ADJP or a PP clause (3b) lacking a verb
altogether. (In the Hebrew treebank, such pred-
icates are marked as PREDP). In all such cases,
agreement feature-bundles realized as pronominals,
which (Doron, 1986) calls Pron, are optionally
placed after the subject. The position of Pron el-
ement with respect to the subject and predicate is
fixed.2 The role of these Pron elements is to indicate
the argument-structure of a nominal sentence that is
not projected by a verb. In the Hebrew treebank they
are subsumed under predicative phrases (PREDPs).
If a PREDP head is of type NP or ADJP it must be
inflected to reflect the features of the subject con-
troller, as is illustrated in examples (3b-i)?(3b-ii).
(3) a. Agreement in Nominal Sentences:
Controller: NP
Target: Pron
Domain: S
Features: number, gender,person
b. i. ????? (???) ????
dina
Dina.FS
(hi)
(Pron.3FS)
cayeret
painter.FS
Dina is a painter
ii. ?????? (???) ????
Dina
Dina.FS
(hi)
(Pron.3FS)
muchsheret
talented.FS
Dina is talented
iii. ???? (???) ????
Dina
Dina.FS
(hi)
(Pron.3FS)
babayit
in-the-house
Dina is at home
c. i. ????? ???? *(???) (hi)* dina cayeret
(Pron.3FS)* Dina.FS painter.FS
The pronominal features gender, number, person
are also a part the inflectional paradigm of the verb
??? (be), which is extended to include tense features.
These inflected elements are used as AUX which
function as co-heads together with the main (nom-
inal or verbal) predicate. AUX elements that take a
nominal predicate as in (4b) agree with their subject,
and so do auxiliaries that take a verbal complement,
e.g., the modal verb in (4c). The nominal predicate
in (4b) also agrees with the subject ? and so does
the modal verb in (4c). Agreement of AUX with the
2Doron (1986) shows that these Pron elements can not be
considered the present tense supplements of AUX elements in
Hebrew since their position with respect to the subject and pred-
icate is fixed, whereas AUX can change position, see (4) below.
42
verbal or nominal predicates is again independent of
their surface positions.
(4) a. Subject-AUX Agreement in Hebrew:
Controller: NP
Target: AUX
Domain: S
Features: number, person, gender
b. i. ????? ??? ???? ???
hi
she.3FS
hayta
was.3FS
be?avar
in-past
cayeret
painter.FS
She was a painter in the past
ii. ????? ??? ???? ???
be?avar
in-past
hayta
was.3FS
hi
she.3FS
cayeret
painter.FS
She was a painter in the past?
c. i. ??? ????? ???? ???
hi
She.3FS
hayta
was.3FS
amura
supposed.FS
lehagi?a
to-arrive
She was supposed to arrive
ii. ??? ???? ????? ???
hi
She.3FS
amura
supposed.FS
hayta
was.3FS
lehagi?a
to-arrive
She was supposed to arrive
Agreement in Construct State Nouns Semitic
languages allow for the creation of noun compounds
by phonologically marking their lexical head and
adding a genitive complement. These constructions
are called Construct-State Nouns (CSN) (Danon,
2008) and an example of a CSN is provided in (5a).3
(5) a. ????? ??
bat
child.FS.CSN
ha-cayar
Def-painter.MS
The painter?s daughter
3Also known as iDaFa constructions in Arabic.
In such cases, all the agreement features are taken
from the head of the CSN, the noun ?daughter? in (5).
Since CSNs may be embedded in other CSNs, the
constructions may be arbitrarily long. When short
or long, CSNs themselves may be modified by ad-
jectives that agree with the CSN as a whole. This
gives rise to multiple patterns of agreement within
a single complex CSN. Consider, for instance, the
modified CSN in (6a).
(6) a. ??????? ????? ??
bat
child.FS.CSN
ha-cayar
Def-painter.MS
ha-muchsheret
Def-talented.FS
The talented daughter of the painter
The features Def, F, S of the adjective ?talented?
agree with the inherent properties of the CSN head
?child.FS? and with the definiteness status of the em-
bedded genitive Def-painter. This phenomenon is
called by Danon (2008) definiteness-spreading, and
what is important about such spreading is to observe
that it is not always the case that all agreement fea-
tures of a phrase are contributed by its lexical head.4
Interim Summary The challenges of model-
ing agreement inside constituency-based statistical
models can be summarized as follows. The models
are required to assign probability mass to alternating
sequences of constituents while retaining equivalent
feature distributions that capture agreement. Agree-
ment is (i) orthogonal to the position of constituents
(ii), orthogonal to their distributional types, and (iii)
orthogonal to features? distributions among domi-
nated subconstituents. Yet, from a functional point
of view their contribution is entirely systematic.
3 The Models
The strong version of the well-known Lexicalist
Hypothesis (LH) states that ?syntactic rules cannot
make reference to any aspect of word internal struc-
ture? (Chomsky, 1970). Anderson (1982) argues
that syntactic processes operating within configura-
tional structures can often manipulate, or have ac-
cess to, formal and inherent properties of individ-
ual words. Anderson (1982) argues that a model
4Examples for non-overlapping contribution of features by
multiple dependencies can be found in (Guthmann et al, 2009).
43
that is well-equipped to capture such phenomena is
one that retains a relaxed version of the LH, that is,
one in which syntactic processes do not make refer-
ence to aspects of word-internal structure other than
morphologically marked inflectional features. What
kind of parsing model would allow us to implement
this relaxed version of the Lexicalist Hypothesis?
The Morphosyntatctic State-Splits (SP) Model
One way to maintain a relaxed version of the LH
in syntax is to assume a constituency-based rep-
resentation in which the morphological features of
words are percolated to the level of constituency
in which they are syntactically relevant. This ap-
proach is characteristic of feature-based grammars
(e.g., GPSG (Gazdar et al, 1985) and follow-up
studies). These grammars assume a feature geom-
etry that defines the internal structure of node labels
in phrase-structure trees.5
Category-label state-splits can reflect the different
morphosyntactic behavior of different non-terminals
of the same type. Using such supervised, linguis-
tically motivated, state-splits, based on the phrase-
level marking of morphological information is one
may build an efficient implementation of a PCFG-
based parsing model that takes into account mor-
phological features. State-split models were shown
to obtain state-of-the-art performance with little
computational effort. Supervised state-splits for
constituency-based unlexicalized parsing in (Klein
and Manning, 2003) in an accurate English parser.
For the pair of Hebrew sentences (2b), the morpho-
logical state-split context-free representation of the
domain S is as described at the top of figure 1.6
The Relational-Realizational (RR) Model A dif-
ferent way to implement a syntactic model that con-
form to the relaxed LH is by separating the inflec-
tional features of surface words from their grammat-
ical functions in the syntactic representation and let-
5While agreement patterns in feature-rich grammars give
rise to re-entrancies that break context-freeness, GPSG shows
that using feature-percolation we can get quite far in modeling
morphosyntactic dependencies and retaining context-freeness.
6Horizontal markovization a` la (Klein and Manning, 2003)
would be self-defeating here. Markovization of constituents
conditions inflectional features on configurational positions,
which is inadequate for free word-order languages as Hebrew.
This is already conjectured in the PhD thesis of Collins, and it
is verified empirically for Hebrew in (Tsarfaty et al, 2009).
ting the model learn systematic form-function corre-
spondence patterns between them.
The Relational-Realizational (RR) model (Tsar-
faty and Sima?an, 2008) takes such a ?separational-
ist? approach which is constituent-based. Grammat-
ical relations are separated from their morphologi-
cal or syntactic means of realization, which are in
turn also distinguished. The easiest way to describe
the RR model is via a three-phase generative process
encompassing the projection, configuration and re-
alization phases. In the projection phase, a clause-
level syntactic category generates a Relational Net-
work (RN), i.e., a set of grammatical function-labels
representing the argument-structure of the clause. In
the configuration phase, linear ordering is generated
for the function-labels and optional realization slots
are reserved for elements such as punctuation, auxil-
iaries and adjuncts. The realization phase spells out
a rich morphosyntactic representation (MSR) ? a
syntactic label plus morphological features ? real-
izing each grammatical function and each of the re-
served slots. The process repeats as necessary until
MSRs of pre-terminals are mapped to lexical items.
In (Tsarfaty et al, 2009) we have shown that
the RR model makes beneficial use of morpholog-
ical patterns involving case marking, but did not
study the incorporation of inflectional agreement
features such as gender. Since agreement features
such as gender, number and case-related informa-
tion such accusativity, definiteness are determined
by non-overlapping subconstituents, it remains an
open question whether an addition of agreement fea-
tures into the model can be down in a linguistically
adequate and statistically sound way, and whether or
not they further improve performance.
We claim that the Relational-Realizational model
of (Tsarfaty et al, 2009) has all the necessary ingre-
dients to seamlessly migrate RR representations to
ones that encode agreement explicitely. In order to
explain how we do so let us recapitulate the empir-
ical facts. Agreement is an asymmetric relation de-
fined for a certain domain, in which the agreement
properties of a target co-vary with the inherent prop-
erties of the controller. Consider the two sentences
in (2b) in which the formal means to differentiate the
subject from the object is by the pattern of an agree-
ing predicate. The RR representations of the domain
S are given at the bottom of figure 1.
44
The agreement targets and agreement controllers
are easy to recognize; controllers are the syntac-
tic constituents that realize subjects, parametrized
as Prealization(V B|PRD@S), and targets are the
ones that realize predicates, parametrized as
Prealization(NP |SBJ@S). Now, if we take the
predicted labels of controllers and targets to in-
clude reference to inflectional features, we get
the following parameterization of the realization
parameters Prealization(V B?FEATSi?|PRD@S) and
Prealization(NP ?FEATSj?|SBJ@S) with ?FEATSi?,
?FEATSj? the inflectional features indicated in their
morphosyntactic representation. Now, we only need
to make sure that ?FEATSi?, ?FEATSj? indeed agree,
regardless of their position under S.
We do so by explicitly marking the domain
of agreement, the S category, with the features
of the syntactically most prominent participant in
the situation, the subject (this is where the non-
symmetrical nature of agreement comes into play).
The realization distributions take the following
forms Prealization(V B?FEATSj?|PRD@S?FEATSi?)
and Prealization(NP ?FEATSi?|SBJ@S?FEATSi?). In
the former, NP ?FEATSi? reflects the inherent fea-
tures of the SBJ and in the latter V B?FEATSj? re-
flects the agreement features of the PRD. Now, re-
gardless of word order, and regardless of the inter-
nal structure of NPs, the parameters capturing agree-
ment would be the same for examples (2b i-ii). The
only parameters that differ are the configuration pa-
rameters (boxed), reflecting word-order alternation.
For the sake of completeness we include here also
the SP vs. RR representation of S domains involv-
ing auxiliaries in figure 2. Here the sentences vary
only in the position of the AUX element relative to
the subject with which it agrees. Subjects, predi-
cates, and slots that have been reserved for AUX
elements, all reflect the same pattern of agreement
through their conditioning on the rich representa-
tion of the domain.7 More parameters that vary here
(boxed) are AUX placement and realization param-
eters. Since Pron elements endow PREDPs with
agreement features, agreement with verbless (nomi-
nal) predicates under S analogously follows.
7In Hebrew, even some adverbial modifiers reflect pat-
terns of agreement, e.g., ????? (literally, ?I am still?, glossed
?still.1S?). This solution caters for all such patterns in which
non-obligatory elements exhibit agreement.
4 Experiments
We aim to examine whether the explicit incorpora-
tion of agreement features helps Hebrew parsing,
and if so, which of the two modeling strategies is
better for utilizing the disambiguation cues provided
by morphosyntactic agreement.
Data We use the Hebrew treebank v2.0 with the
extended annotation of (Guthmann et al, 2009),
which adds inflectional properties to non-terminal
categories such as NP and VP. We head-annotate
the corpus and systematically add the agreement fea-
tures of Domains throughout the treebank. We fur-
ther distinguish finite from non-finite verb forms,
and cliticized from non-cliticized nouns, as in
(Goldberg and Tsarfaty, 2008; Tsarfaty et al, 2009).
On top of the treebank labels SBJ subject, OBJ ob-
ject, COM complement and CNJ conjunction we
add PRD predicates and IC infinitival complements.
Procedure We devised a procedure to read-off
treebank grammars based on (i) GPSG-like, states-
plit context-free parameters (SP-AGR), and (ii) RR-
AGR parameters in which context-free rules capture
the projection, configuration and realization phases.
In each model the multiplication provides the prob-
ability of the generation. We use relative frequency
estimates and exhaustively parse gold pos-tagged in-
put8 using a general-purpose CKY parser. We use
the same data split as in (Goldberg and Tsarfaty,
2008; Tsarfaty et al, 2009) (training on sentences
501-6000 and parsing sentences 1-500) and we con-
vert all trees to the flat, coarse-grained, original tree-
bank representation for the purpose of evaluation.
Setup We experiment with bare constituent labels,
grand-parent decorated labels (gp), and labels deco-
rated with grand-parent and head-tag labels (gp,hd).
We use increasingly richer subsets of the {gender,
definiteness, accusativity} set.9
8This choice to parse gold-tagged sentences is meant to alle-
viate the differences in the model?s morphological disambigua-
tion capacity. We want to evaluate the contribution of morpho-
logical features for syntactic disambiguation, and if the models
will disambiguate the morphological analyses differently, the
syntactic analysis will be assigned to different yields and the
accuracy results would be strictly incomparable. But see (Gold-
berg and Tsarfaty, 2008) for a way to combine the two.
9We deliberately choose features that have non-overlapping
behavior, to see whether their contribution is accumulative.
45
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
n tan
gave
NP-OBJ
m tana
present
P-COM
ledian
to-dina
SMS
NP-OBJ
m tana
present
VPMS-PRD
n tan
gave
NPMS-SBJ
dani
Dani
P-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ, P-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ, P-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
n tan
gave
OBJ@SMS
NP-OBJ
m tana
present
COM@SMS
P-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
m tana
present
PRD@SMS
VPMS
n tan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
P-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization( P | COM@ SMS ) Prealization( P | COM@ SMS )
Figure 1: The SP-AGR (top) and R-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPF -SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPF -SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, P, PREDPFS-PRD? | SFS ) P(? P, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and R-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
projection({SBJ,PRD,OBJ,COM} | SMS ) projection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P ,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR repres tations of s n ences (2b- (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) (?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@ FS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
mura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD S+FS
PP
mura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
projection({SBJ,PRD,COM} | SFS ) projection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUX | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of s n ences (4c- (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
atan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
atan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | S S ) Pprojection({SBJ,PRD,OBJ,COM} | S S )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
S S
NP S-SBJ
dani
Dani
VP S-PRD
natan
gave
NP-OBJ
matana
present
PP-CO
ledian
to-dina
S S
NP-OBJ
matana
present
VP S-PRD
natan
gave
NP S-SBJ
dani
Dani
PP-CO
ledian
to-dina
P( P S-SBJ, P S-PR , P- BJ,PP-C | S S ) P( P- BJ, P S-PR , P S-SBJ,PP-C | S S )
S S
{SBJ,PRD,OBJ,CO } S S
SBJ S S
NP S
dani
Dani
PRD S S
VP S
natan
gave
OBJ S S
NP-OBJ
matana
present
CO S S
PP-CO
ledian
to-dina
S S
{SBJ,PRD,OBJ,CO } S S
OBJ S S
NP-OBJ
matana
present
PRD S S
VP S
natan
gave
SBJ S S
NP S
dani
Dani
CO S S
PP-CO
ledian
to-dina
Pprojection({SBJ,PR , BJ,C } | S S ) Pprojection({SBJ,PR , BJ,C } | S S )
Pconfiguration(?S,P, ,C? | {SBJ,PR , BJ,C } S S) Pconfiguration(? ,P,S,C? | {SBJ,PR , BJ,C } S S)
Prealization( P S | SBJ S S ) Prealization( P S | SBJ S S )
Prealization( B S | PR S S ) Prealization( B S | PR S S )
Prealization( P | BJ S S ) Prealization( P | BJ S S )
Prealization(PP | C S S ) Prealization(PP | C S S )
igure 1: The SP- (top) and - representations of sentences (2b-i) (left) and (2b-ii).
FS
NPFS-SBJ
hi
she
AUXFS
hayta
was
DFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
FS
NPFS-SBJ
hi
she
DFS-PRD
amura
supposed
AUXFS
hayt
was
VPINF-COM
lehagia
to-arrive
P(? PFS-SBJ, FS, PP, PRE PFS-PR ? | SFS ) P(?PP, FS, PFS-SBJ, PRE PFS-PR ? | SFS )
FS
{SBJ,PRD,CO } FS
SBJ FS
NPFS
hi
she
SBJ:PRD FS
AUXFS
hayta
was
PRD S+FS
PP
amura
supposed
CO SINF
PREDPFS
lehagia
to-arrive
FS
{SBJ,PRD,CO } FS
SBJ FS
NPFS
hi
she
PRD S+FS
PP
amura
supposed
PRD:CO FS
AUXFS
hayta
was
CO SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PR ,C } | SFS ) Pprojection({SBJ,PR ,C } | SFS )
Pconfiguration( ?SBJ, SBJ:PR , PR , C ? | {SBJ,PR ,C } SFS) Pconfiguration( ?SBJ, PR , PR :C , C ? | {SBJ,PR ,C } SFS)
Prealization( PFS | SBJ SFS ) Prealization( PFS | SBJ SFS )
Prealization( FS | SBJ:PR SFS ) Prealization( FS | PR :C SFS )
Prealization( FS | PR SFS ) Prealization( FS | PR SFS )
Prealization( P | C SFS) Prealization( P | C SFS)
igure 2: The SP- (top) and - representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-C
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
P @SMS
VPMS
natan
gave
BJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-CO
ledian
to-dina
SM
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
p esent
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SM
PP-COM
ledian
to-dina
Pprojection({S J,PRD,OBJ,COM} | SMS ) Pproject on({SBJ,PRD, J,CO } | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfigur ti (?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealiz tion(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,P , }@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | BJ@SFS ) Prealizat on(NPFS | BJ@ FS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@ FS )
realization(VP | COM@SFS) Prealization(VP | COM@ FS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
V -PRD
n tan
gave
NP-OBJ
matana
pres nt
P COM
ledian
to-di a
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
P-COM
ledian
to-dina
P(NPMS-SBJ,V -PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS PRD,NP S-SBJ,PP-CO | SMS )
S S
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@ MS
VPMS
natan
gave
OBJ@SMS
N -OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ S S
NPMS
d i
Dani
COM@SMS
PP-COM
ledian
to-d na
Pprojection({SBJ,PRD,OBJ,CO } | S ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | { BJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii .
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
FS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
le gi
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | S ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | S )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
ayta
was
PRD@S+
PP
mura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@ +FS
PP
amura
upposed
P :COM@SFS
AUXFS
hayta
was
SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, J:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM COM? | {SBJ,PRD,COM}@SFS)
lization(NPFS | SBJ@S ) Prealization(N FS | SBJ@S )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii .
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
g ve
N -OBJ
matana
prese t
PP-COM
ledian
to- na
SMS
NP-OBJ
matana
prese t
VPMS- RD
tan
g ve
NPMS-SBJ
dani
Dani
PP-COM
ledian
to- na
P(NPMS-SBJ,VPMS-PRD,N OBJ,PP-COM | SMS ) P(N -OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,O COM}@SMS
BJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
g ve
OBJ@S S
NP-OBJ
matana
prese t
COM@SMS
PP-COM
ledian
to- na
SMS
{SBJ,PRD,O OM}@SMS
OBJ@SMS
NP-OBJ
matana
prese t
PRD@SMS
VPMS
tan
g ve
BJ@ S
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to- na
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfigurati (?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfigurati (?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@ MS ) Pre lization(NPMS | SBJ@ MS )
Prealization(VB | PRD S ) Prealization(VB | PR S )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM ) Prealization(PP | CO )
Figur 1: The SP-AGR (top) and R-AGR representatio of sentence (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayt
was
MD -PRD
amura
supposed
VPINF-COM
lehagia
t -arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amur
supposed
AUXFS
hayta
was
VPIN -COM
leh gia
to-arrive
P(?NPFS-SBJ, AUXFS, P , PREDPFS-PR ? | FS ) P(?PP, AUXFS, NPFS- BJ, PREDPFS-PR ? | FS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@ FS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
REDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@ INF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfigurati ( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfigurati ( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | BJ@ FS ) Prealization(NPFS | BJ@ FS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PR @SFS ) Prealization(MDFS | PR @SFS )
Prealization(VP | COM@SFS) realization(VP | COM@SFS)
Figur 2: The SP-AGR (top) and R-AGR representation of sentence (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
o-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@ FS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPF
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PR ,COM} | SFS )
Pc nfiguration( ?SBJ, S J:PRD, PRD, C M? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
realization(NPFS | SBJ@SFS ) Pre lization(NPFS | SBJ@SFS )
Prealization(AUXFS | BJ:PRD SFS ) Prealization(AUXFS | PRD:C @SFS )
Prealization(MDFS | F ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS realization(VP | COM@ FS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
46
Model { gender def+acc gender+def+acc
SP-AGR 79.77 79.55 80.13 80.26
(3942) (7594) (4980) (8933)
RR-AGR 80.23 81.09 81.48 82.64
(3292) (5686) (3772) (6516)
SP-AGR (gp) 83.06 82.18 79.53 80.89
(5914) (10765) (12700) (11028)
RR-AGR (gp) 83.49 83.70 83.66 84.13
(6688) (10063) (12383) (12497)
SP-AGR (gp,hd) 76.61 64.07 75.12 61.69
(10081) (16721) (11681) (18428)
RR-AGR (gp,hd) 83.40 81.19 83.33 80.45
(12497) (22979) (13828) (24934)
Table 1: F-score (#params) measure for all models on
the Hebrew treebank dev-set for Sentences Length < 40
5 Results and Discussion
Table 1 shows the standard F1 scores (and #param-
eters) for all models. Throughout, the RR-AGR
model outperforms the SP-AGR models that use the
same category set and the same morphological fea-
tures as state splits. For RR-AGR and RR-AGR (gp)
models, adding agreement features to case features
improves performance. The accumulative contribu-
tion is significant. For SP-AGR and SP-AGR (gp)
models, adding more features either remains at the
same level of performance or becomes detrimental.
Since the SP/RR-AGR and SP/RR-AGR (gp)
models are of comparable size for each feature-set,
it is unlikely that the differences in performance are
due to the lack of training data. A more reason-
able explanation if that the RR parameters repre-
sent functional generalizations orthogonal to config-
uration for which statistical evidence is more easily
found in the data. The robust realization distribu-
tions which cut across ordering alternatives can steer
the disambiguation in the right direction.
The RR-AGR (gp) +gen+def+acc model yields
the best result for parsing Hebrew to date (F1 84.13),
improving upon our best model in (Tsarfaty et al,
2009) (F1 83.33, underlined) in a pos-tagged set-
ting. For this setting, Arabic parsing results are F1
78.1. Given the similar morphosyntactic phenomena
(agreement, MaSDaR, iDaFa) it would be interest-
ing to see if the model enhances parsing for Arabic.
For (gp,hd) models (a configuration which was
shown to give the best results in (Tsarfaty et al,
2009)) there is a significant decrease in accuracy
with the gender feature, but there is a lesson to be
learned. Firstly, while the RR-AGR (gp,hd) model
shows moderate decrease with gender, the decrease
in performance of SP-AGR (gp,hd) for the same
feature-set is rather dramatic, which is consistent
with the observation that the RR model is less vul-
nerable to sparseness and that it makes better use of
the statistics of functional relations in the data.
Consulting the size of the different grammars, the
combination of RR-AGR (gp, hd) with gender fea-
tures indeed results in substantially larger grammars,
and it is possible that at this point we indeed need to
incorporate smoothing. At the same time there may
be an alternative explanation for the decreased per-
formance. It might be that the head-tag does not add
informative cues beyond the contribution of the fea-
tures which are spread inside the constituent, and are
already specified. This is a reasonable hypothesis
since gender in Hebrew always percolates through
the head as opposed to def/acc that percolate from
other forms. Incorporating head-tag in (Tsarfaty et
al., 2009) might have led to improvement only due
to the lack of agreement features which subsume
the relevant pattern. This suggests that incorporat-
ing all co-heads and functional elements that con-
tribute morphological features spread inside the con-
stituent, is more adequate for modeling morphosyn-
tax than focusing on the features of a single head.
6 Conclusion
We show that morphologically marked agreement
features can significantly improve parsing perfor-
mance if they are represented and parametrized in
a way that reflects their linguistic substance: relat-
ing form-and-function in a non-linear fashion. We
have so far dealt with the adequacy of representa-
tion and we plan to test whether more sophisticated
estimation (e.g., split-merge-smooth estimation as in
(Petrov et al, 2006)) can obtain further improve-
ments from the explicit representation of agreement.
At the same time, the state-of-the-art results we
present render the RR model promising for further
exploration with morphologically rich languages.
Acknowledgements The work of the first author
has been funded by NWO, grant 017.001.271. We
wish to thank Joakim Nivre and three anonymous
reviewers for helpful comments on earlier drafts.
47
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Stephen R. Anderson. 1982. Where?s morphology? Lin-
guistic Inquiry.
Noam Chomsky. 1970. Remarks on nominalization. In
R. Jacobs and P. Rosenbaum, editors, Reading in En-
glish Transformational Grammar. Waltham: Ginn.
Greville G. Corbett. 2001. Agreement: Terms and
boundaries. In SMG conference papers.
Gabi Danon. 2008. Definiteness spreading in the hebrew
construct-state. Lingua, 118(7):872?906.
Edit Doron. 1986. The pronominal ?copula? as agree-
ment clitic. Syntax and Semantics, (19):313?332.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan A. Sag. 1985. Generalised phrase structure
grammar. Blackwell, Oxford, England.
Yoav Goldberg and Michael Elhadad. 2009. Hebrew de-
pendency parsing: Initial results. In Proceedings of
IWPT.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006.
Noun phrase chunking in hebrew: Influence of lex-
ical and morphological features. In Proceedings of
COLING-ACL.
Nomie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew treebank.
In Frank Van Eynde, Anette Frank, Koenraad De
Smedt, and Gertjan van Noord, editors, Proceedings
of TLT.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Julia Hockenmaier and Mark Steedman. 2003. Parsing
with generative models of predicate-argument struc-
ture. In Proceedings of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the arabic tree-
bank. In Proceedings of INFOS.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature-forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Lilja ?vrelid and Joakim Nivre. 2007. Swedish depen-
dency parsing with rich linguistic features. In Pro-
ceeding of RANLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Khalil Sima?an, and Remko Scha. 2009.
An alternative to head-driven approaches for parsing a
(relatively) free word order language. In Proceedings
of EMNLP.
48
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58?67,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
Generating Subjective Responses to Opinionated Articles in Social Media:
An Agenda-Driven Architecture and a Turing-Like Test
Tomer Cagan
School of Computer Science
The Interdisciplinary Center
Herzeliya, Israel
cagan.tomer@idc.ac.il
Stefan L. Frank
Centre for Language Studies
Radboud University
Nijmegen, The Netherlands
s.frank@let.ru.nl
Reut Tsarfaty
Mathematics and Computer Science
Weizmann Institute of Science
Rehovot, Israel
tsarfaty@weizmann.ac.il
Abstract
Natural language traffic in social media
(blogs, microblogs, talkbacks) enjoys vast
monitoring and analysis efforts. How-
ever, the question whether computer sys-
tems can generate such content in order
to effectively interact with humans has
been only sparsely attended to. This pa-
per presents an architecture for generat-
ing subjective responses to opinionated
articles based on users? agenda, docu-
ments? topics, sentiments and a knowledge
graph. We present an empirical evalua-
tion method for quantifying the human-
likeness and relevance of the generated re-
sponses. We show that responses gen-
erated using world knowledge in the in-
put are regarded as more human-like than
those that rely on topic, sentiment and
agenda only, whereas the use of world
knowledge does not affect perceived rel-
evance.
1 Introduction
Digital media, user-generated content and social
networks enable effective human interaction; so
much so that much of our day-to-day interaction
is conducted online (Viswanath et al., 2009). In-
teraction in social media fundamentally changes
the way businesses and consumers behave (Qual-
man, 2012), can be instrumental to the success
of individuals and businesses (Haenlein and Ka-
plan, 2009), and even affects the stability of polit-
ical regimes (Howard et al., 2011; Lamer, 2012).
These facts force organizations (businesses, gov-
ernments, and non-profit organizations) to be con-
stantly involved in the monitoring of, and the inter-
action with, human agents in digital environments
(Langheinrich and Karjoth, 2011).
Automatic analysis of user-generated online
content benefits from extensive research and com-
mercial opportunities. In natural language pro-
cessing, there is ample research on the analysis
of subjectivity and sentiment of content in social
media. The development of tools for sentiment
analysis (Davidov et al., 2010), mood aggregation
(Agichtein et al., 2008), opinion mining (Mishne,
2006), and many more, now enjoys wide inter-
est and exposure, as is also evident by the many
workshops and dedicated tracks at ACL venues.
1
Methods are also developed for the analysis of po-
litical texts (O?Connor et al., 2010; O?Connor et
al., 2013) and for text-driven forecasting based on
these data (Yano et al., 2009). A related strand
of research uses computational methods to find
out what kind of published utterances are influ-
ential, and how they affect linguistic communi-
ties (Danescu-Niculescu-Mizil et al., 2009). Such
work complements, and contributes to, studies
from sociology and sociolinguistics that aim to de-
lineate the process of generating meaningful re-
sponses (e.g., Amabile (1981)).
In contrast to these analysis efforts, the topic
of generating responses to content in social me-
dia is only sparsely explored. Commercially, there
is movement towards online response automation
(Owyang, 2012; Mah, 2012).
2
Research on user
interfaces is trying to move away from script-
based interaction towards the development of chat
bots that attempt natural human-like interaction
(Mori et al., 2003; Feng et al., 2006). However,
these chat bots are typically designed to provide
an automated one-size-fits-all type of interaction.
A study by Ritter et al. (2011) addresses
the generation of responses to natural language
tweets in a data-driven setup. It applies a
machine-translation approach to response gener-
ation, where moods and sentiments already ex-
1
E.g., the ACL series LASM http://tinyurl.com/
ludyrkz; WASSA http://tinyurl.com/kjjdhax.
2
There is a general debate on the efficiency of automated
tools (Nall, 2013) and whether such tools are desirable in so-
cial media (McConnell (2012); responses to Owyang (2012)).
58
pressed in the past are replicated or reused. A re-
cent study by Hasegawa et al. (2013) modifies Rit-
ter?s approach to produce responses that elicit an
emotion from the addressee. Yet, these responses
do not target particular topics and are not driven
by a user agenda.
The present paper addresses the problem of
generating novel, subjective, responses to on-
line opinionated articles. We formally define the
document-to-response mapping problem and sug-
gest an end-to-end system to solve it. Our sys-
tem integrates a range of NLP and NLG technolo-
gies (including topic models, sentiment analysis,
and the integration of a knowledge graph) to de-
sign a flexible generation mechanism that allows
us to vary the information in the input to the gen-
eration procedure. We then use a Turing-inspired
test to study the different factors that contribute to
the perceived human-likeness and relevance of the
generated responses, and show how the perception
of responses depends on external knowledge and
the expressed sentiment.
The remainder of this paper is organized as fol-
lows. The next section presents our proposal: Sec-
tion 2.1 describes our approach, Section 2.2 for-
malizes the proposal, and Section 2.3 presents our
end-to-end architecture. This is followed by our
evaluation method and empirical results in Sec-
tion 3. We discuss related and future work in Sec-
tion 4, and in Section 5 we conclude.
2 The Proposal: Generating Subjective
Responses
2.1 Our Approach
Natural language is, above all, a communicative
device that we employ to achieve certain goals.
In social media, the driving force behind generat-
ing responses is a responder?s disposition towards
some topic. This topic could be a political cam-
paign or a candidate, a product, or some abstract
idea, which the responder has a motive to promote.
Let us call this goal our user?s agenda.
User response generation, like any other natu-
ral language utterance generation, is triggered by
a certain event that is related to the communica-
tive goal. In a social media setting, this event
is often a new online document. The document
and the agenda thus form the input to our gener-
ation system. Each document and each agenda
contain (possibly many) topics, each of which is
associated with a (positive or negative) sentiment.
Document sentiments are attributed to the author,
whereas agenda sentiments are attributed to the
user (henceforth: the responder).
For each non-empty intersection of the topics
in the document and in the agenda, our response-
generation system aims to generate utterances that
are fluent, human-like, and effectively engage
readers. The generation is based on three assump-
tions, roughly reflecting the Gricean maxims of
cooperative interaction (Grice, 1967). Online user
responses should then be:
? Economic (Maxim of Quantity): Responses
are brief and concise;
? Relevant (Maxim of Relation): Responses di-
rectly address the documents? content.
? Opinionated (Maxim of Quality): Responses
express responders beliefs, sentiments, or
dispositions towards the topic(s).
2.2 The Formal Model
Let D be a set of documents and let A be a set
of user agendas as we define shortly. Let S be a
set of English sentences over a finite vocabulary
S = ?
?
. Our system implements a function that
maps each ?document, agenda? pair to a natural
language response sentence s ? S.
f
response
: D ?A? S
Response generation takes place in two phases,
roughly corresponding to macro and micro plan-
ning in Reiter and Dale (1997):
? Macro Planning (below, the analysis phase):
What are we going to talk about?
? Micro Planning (below, the generation
phase): How are we going to say it?
The analysis function p : D ? C maps a docu-
ment to a subjective representation of its content.
3
The generation function g : C ? A ? S inter-
sects the content elements in the document and in
the user agenda, and generates a response based
on the content of the intersection. All in all, our
system implements a composition of the analysis
and the generation functions:
f
response
(d, a) = g(p(d), a) = s
3
A content element may conceivably encompass a topic,
its sentiment, its objectivity, its evidentiality, its perceived
truthfulness, and so on. In this paper we focus on topic and
sentiment, and leave the rest for future research.
59
Each content element c ? C or an agenda item
a ? A is composed of a topic t associated with a
sentiment value sentiment
t
? [?n..n] that sig-
nifies the (negative or positive) disposition of the
document?s author (if c ? C) or the user?s agenda
(if a ? A) towards the topic. We assume here that
a topic is simply a bag of words from our vocabu-
lary ?. Thus, we have the following:
A,C ? P(?)? [?n..n]
Our generation component accepts the result of
the intersection as input and relies on a template-
based grammar and a set of functions for generat-
ing referring expressions in order to construct the
output. To make the responses economic, we limit
the content of a response to one statement about
the document or its author, followed by a state-
ment on the relevant topic. To make the response
relevant, the templates that generate the response
make use of topics in the intersection of the docu-
ment and the agenda. To make the response opin-
ionated, the sentiment of the response depends on
the (mis)match between the sentiment values for
the topic in the document and in the agenda. Con-
cretely, the response is positive if the sentiments
for the topic in the document and agenda are the
same (both positive or both negative) and it is neg-
ative otherwise.
We suggest two variants of the generation func-
tion g. The basic variant implements the baseline
function defined above:
g
base
(c, a) = s
c ? C, a ? A, s ? ?
?
For the other variant we define a knowledge
base (KB) as a directed graph in which words
w ? ? from the topic models correspond to nodes
in the graph, and relations r ? R between the
words are predicates that hold in the real world.
Our second generation function now becomes:
g
kb
(c, a,KB) = s
KB ? {(w
i
, r, w
j
)|w
i
, w
j
? ?, r ? R}
with c ? C, a ? A, s ? ?
?
as defined in g
base
above.
2.3 The Architecture
The system architecture from a bird?s eye view
is presented in Figure 1. In a nutshell, a docu-
ment enters the analysis phase, where topic infer-
ence and sentiment scoring take place, resulting
in ?topic, sentiment?-pairs. During the subsequent
generation phase, these are intersected with the
?topic, sentiment?-pairs in the user agenda. This
intersection, possibly augmented with a knowl-
edge graph, forms the input for a template-based
generation component.
Analysis phase For the task of inferring the top-
ics of the document we use topic modeling: a
probabilistic generative modeling technique that
allows for the discovery of abstract topics over
a large body of documents (Papadimitriou et al.,
1998; Hofmann, 1999; Blei et al., 2003). Specif-
ically, we use topic modeling based on Latent
Dirichlet Allocation (LDA) (Blei et al., 2003; Blei,
2012). Given a new document and a trained
model, the inference method provides a weighted
mix of topics for that document, where each topic
is represented as a vector containing keywords as-
sociated with probabilities. For training the topic
model and inferring the topics in new documents
we use Gensim (Rehurek and Sojka, 2010), a fast
and easy-to-use implementation of LDA.
Next, we wish to infer the sentiment that is ex-
pressed in the text with relation to the topic(s)
identified in the document. We use the seman-
tic/lexical method as implemented in Kathuria
(2012). We rely on a WSD sentiment classifier
that uses the SentiWordNet (Baccianella et al.,
2010) database and calculates the positivity and
negativity scores of a document based on the pos-
itivity and negativity of individual words. The re-
sult of the sentiment analysis is a pair of values,
indicating the positive and negative sentiments of
the document-based scores for individual words.
We use the larger of these two values as the senti-
ment value for the whole document.
4
Generation phase Our generation function first
intersects the set of topics in the document and the
set of topics in the agenda in order to discover rel-
evant topics to which the system would generate
responses. A response may in principle integrate
content from a range of topics in the topic model
distribution, but, for the sake of generating concise
responses, in the current implementation we focus
on the single most prevalent, topic. We pick the
highest scoring word of the highest scoring topic,
and intersect it with topics in the agenda. The sys-
tem generates a response based on the identified
4
Clearly, this is a simplifying assumption. We discuss this
assumption further in Section 4.
60
Figure 1: The system architecture from a bird?s eye view. Components on gray background are executed
offline.
topic, the sentiment for the topic in the document,
and the sentiment for that topic in the user agenda.
The generation component relies on a template-
based approach similar to Reiter and Dale (1997)
and Van Deemter et al. (2005). Templates are
essentially subtrees with leaves that are place-
holders for other templates or for functions gener-
ating referring expressions (Theune et al., 2001).
These functions receive (relevant parts of) the in-
put and emit the sequence of fine-grained part-of-
speech (POS) tags that realizes the relevant refer-
ring expression. The POS tags in the resulting
sequences are ultimately place holders for words
from a lexicon ?. In order to generate a variety of
expression forms ? nouns, adjectives and verbs
? these items are selected randomly from a fine-
grained lexicon we defined. The sentiment (posi-
tive or negative) is expressed in a similar fashion
via templates and randomly selected lexical en-
tries for the POS slots, after calculating the over-
all sentiment for the intersection as stated above.
Our generation implementation is based on Sim-
pleNLG (Gatt and Reiter, 2009) which is a surface
realizer API that allows us to create the desired
templates and functions, and aggregates content
into coherent sentences. The templates and func-
tions that we defined are depicted in Figure 2.
In addition, we handcrafted a simple knowledge
graph (termed here KB) containing the words in a
set of pre-defined user agendas. Table 1 shows a
snippet of the constructed knowledge graph. The
knowledge graph can be used to expand the re-
sponse in the following fashion: The topic of the
response is a node in the KB. We randomly se-
lect one of its outgoing edges for creating a related
Source Relation Target
Apple CompetesWith Samsung
Apple CompetesWith Google
Apple Creates iOS
Table 1: A knowledge graph snippet.
statement that has the target node of this relation
as its subject. The related sentence generation uses
the same template-based mechanism as before. In
principle, this process may be repeated any num-
ber of times and express larger parts of the KB.
Here we only add one single knowledge-base re-
lation per response, to keep the responses concise.
3 Evaluation
We set out to evaluate how computer-generated re-
sponses compare to human responses in their per-
ceived human-likeness and relevance. More in
particular, we compare different system variants
in order to investigate what makes responses seem
more human-like or relevant.
3.1 Materials
Our empirical evaluation is restricted to topics re-
lated to mobile telephones, specifically Apple?s
iPhone and devices based on the Android operat-
ing system. We collected 300 articles from lead-
ing technology sites in the domain to train the
topic models on, settling on 10 topics models.
Next, we generated a set of user agendas refer-
ring to the same 10 topics. Each agenda is rep-
resented by a single keyword from a topic model
distribution and a sentiment value sentiment
t
?
{?8,?4, 0, 4, 8}. Finally, we selected 10 new ar-
ticles from similar sites and generated a pool of
61
Sresponse
NP
I
VP
V?
belief
SBAR
that (S
response
)
S
response
S
article
S
item
(S
relation
)
S
item
NP?
itemRef
VP?
senti
i
S
article
NP?
articleRef
VP?
senti
a
S
relation
NP?
relationRef
VP?
senti
r
articleRef ? ExpressArticle(...)
itemRef ? ExpressItem(...)
relationRef ? ExpressRelation(...)
sentiment
a
? ExpressArticleSentiment(...)
sentiment
i
? ExpressItemSentiment(...)
sentiment
r
? ExpressRelationSentiment(...)
belief ? ExpressBelief(...)
Figure 2: Template-based response generation. The templates are on the left. The Express* functions on
the right uses regular expressions over the arguments and vocabulary items from a closed lexicon.
1000 responses for each, comprising 100 unique
responses for each combination of sentiment
t
and system variant (i.e., with or without a knowl-
edge base). Table 2 presents an example response
for each such combination. In addition, we ran-
domly collected 5 to 10 real, short or medium-
length, online human responses for each article.
3.2 Surveys
We collected evaluation data via two online
surveys on Amazon Mechanical Turk (www.
mturk.com). In Survey 1, participants judged
whether responses to articles were written by hu-
man or computer, akin to (a simplified version of)
the Turing test (Turing, 1950). In Survey 2, re-
sponses were rated on their relevance to the ar-
ticle, in effect testing whether they abide by the
Gricean Maxim of Relation. This is comparable
to the study by Ritter et al. (2011) where people
judged which of two responses was ?best?.
Each survey comprises 10 randomly ordered tri-
als, corresponding to the 10 selected articles. First,
the participant was presented with a snippet from
the article. When clicking a button, the text was
removed and its presentation duration recorded.
Next, a multiple-choice question asked about the
snippet?s topic. Data on a trial was discarded from
analysis if the participant answered incorrectly or
if the snippet was presented for less than 10 msec
per character; we took these to be cases where the
snippet was not properly read. Next, the partic-
ipant was shown a randomly ordered list of re-
sponses to the article.
In Survey 1, four responses were presented for
each article: three randomly selected from the
pool of human responses to that article and one
generated by our system. The task was to cate-
gorize each response on a 7-point scale with la-
bels ?Certainly human/computer?, ?Probably hu-
man/computer?, ?Maybe human/computer? and
?Unsure?. In Survey 2, five responses were pre-
sented: three human responses and two computer-
generated. The task was to rate the responses?
relevance on a 7-point scale labeled ?Completely
(not) relevant?, ?Mostly (not) relevant?, ?Some-
what (not) relevant?, and ?Unsure?. As a con-
trol condition, one of the human responses and
one of the computer responses were actually taken
from another article than the one just presented.
In both surveys, the computer-generated responses
presented to each participant were balanced across
sentiment levels and generation functions (g
base
and g
kb
). After completing the 10 trials, partic-
ipants provided basic demographic information,
including native language. Data from non-native
English speakers was discarded. Surveys 1 and 2
were completed by 62 and 60 native speakers, re-
spectively.
3.3 Analysis and Results
Survey 1: Computer-Likeness Rating. Table 3
shows the mean ?computer-likeness?-ratings from
1 (?Certainly human?) to 7 (?Certainly computer?)
for each response category. Clearly, the human
responses are rated as more human-like than the
computer-generated ones: our model did not gen-
erally mislead the participants. This may be due
to the template-based response structure: over the
course of the survey, human raters are likely to
notice this structure and infer that such responses
are computer-generated. To investigate whether
such learning indeed occurs, a linear mixed-
effects model was fitted, with predictor variables
IS COMP (+1:computer-generated, ?1:human re-
sponses), POS (position of the trial in the survey, 0
to 9), and the interaction between the two. Table 4
62
Sent. KB Response
?8
No Android is horrendous so I think that the writer is completely correct!!!
Yes Apple is horrendous so I feel that the author is not really right!!! iOS is horrendous as well.
?4
No I think that the writer is mistaken because apple actually is unexceptional.
Yes I think that the author is wrong because Nokia is mediocre. Apple on the other hand is pretty good ...
0
No The text is accurate. Apple is okay.
Yes Galaxy is okay so I think that the content is accurate. All-in-all samsung makes fantastic gadgets.
4
No Android is pretty good so I feel that the author is right.
Yes Nokia is nice. The article is precise. Samsung on the other hand is fabulous...
8
No Galaxy is great!!! The text is completely precise.
Yes Galaxy is awesome!!! The author is not completely correct. In fact I think that samsung makes
awesome products.
Table 2: Responses generated by the system with or without a knowledge-base (KB), with different
sentiment levels.
Response Type Mean and CI
Human 3.33 ? 0.08
Computer (all) 4.49 ? 0.15
Computer (?KB) 4.66 ? 0.20
Computer (+KB) 4.32 ? 0.22
Table 3: Mean and 95% confidence interval of
computer-likeness rating per response category.
?KB indicates whether g
base
or g
kb
was used.
Factor b t P (b < 0)
(intercept) 3.590
IS COMP 0.193 2.11 0.015
POS 0.069 4.76 0.000
IS COMP ? POS 0.085 6.27 0.000
Table 4: Computer-likeness rating regression re-
sults, comparing human to computer responses.
presents, for each factor in the regression analysis,
the coefficient b and its t-statistic. The coefficient
equals the increase in computer-likeness rating for
each unit increase in the predictor variable. The t-
statistic is indicative of how much variance in the
ratings is accounted for by the predictor. We also
obtained a probability distribution over each co-
efficient by Markov Chain Monte Carlo sampling
using the R package lme4 version 0.99 (Bates,
2005). From each coefficient?s distribution, we es-
timate the posterior probability that b is negative,
which quantifies the reliability of the effect.
The positive b value for POS shows that re-
sponses drift towards the ?computer?-end of the
scale. More importantly, a positive interaction
with IS COMP indicates that the difference be-
tween human and computer responses becomes
more noticeable as the survey progresses ?
the participants did learn to identify computer-
generated responses. However, the positive coef-
ficient for IS COMP means that even at the very
first trial, computer responses are considered to be
more computer-like than human responses.
Factors Affecting Human-Likeness. Our find-
ing that the identifiability of computer-generated
responses cannot be fully attributed to their repet-
itiveness, raises the question: What makes a such
a response more human-like? The results provide
several insights into this matter.
First, the mean scores in Table 3 suggest that in-
cluding a knowledge base increases the responses?
human-likeness. To further investigate this, we
performed a separate regression analysis, using
only the data on computer-generated responses.
This analysis also included predictors KB (+1:
knowledge base included, ?1: otherwise), SENT
(sentiment
t
, from ?8 to +8), absolute value of
SENT, and the interaction between KB and POS.
As can be seen in Table 5, there is no reliable in-
teraction between KB and POS: the effect of in-
cluding the KB on the human-likeness of responses
remained constant over the course of the survey.
Furthermore, we see evidence that responses
with a more positive sentiment are considered
more computer-like. The (only weakly reliable)
negative effect of the absolute value of senti-
ment suggests that more extreme sentiments are
considered more human-like. Apparently, people
count on computer responses to be mildly positive,
whereas human responses are expected to be more
extreme, and extremely negative in particular.
Survey 2: Relevance Rating. The mean rele-
vance scores in Table 6 reveal that a response is
rated as more relevant to a snippet if it was actu-
ally a response to that snippet, rather than to a dif-
ferent snippet. This reinforces our design choice
63
Factor b t P (b < 0)
(intercept) 4.022
KB ?0.240 ?2.13 0.987
POS 0.144 5.82 0.000
SENT 0.035 2.98 0.002
abs(SENT) ?0.041 ?1.97 0.967
KB ? POS 0.023 1.03 0.121
Table 5: Computer-likeness rating regression re-
sults, comparing systems with and without KB.
Response Type Source Mean and CI
Human
this 4.85 ? 0.11
other 3.56 ? 0.18
Computer (all)
this 4.52 ? 0.16
other 2.52 ? 0.15
Computer (?KB)
this 4.53 ? 0.23
other 2.46 ? 0.21
Computer (+KB)
this 4.51 ? 0.23
other 2.58 ? 0.22
Table 6: Mean and 95% confidence interval of
relevance rating per response category. ?Source?
indicates whether the response is from the pre-
sented text snippet or a random other snippet.
?KB indicates whether g
base
or g
kb
was used.
Factor b t P (b < 0)
(intercept) 3.861
IS COMP ?0.339 ?7.10 1.000
SOURCE 0.824 16.80 0.000
IS COMP ? PRES 0.179 5.03 0.000
Table 7: Relevance ratings regression results,
comparing human to computer responses.
Factor b t P (b < 0)
(intercept) 3.603
KB 0.026 0.49 0.322
SOURCE 1.003 15.90 0.000
SENT 0.023 1.94 0.029
abs(SENT) ?0.017 ?0.93 0.819
KB ? SOURCE ?0.032 ?0.61 0.731
Table 8: Relevance ratings regression results,
comparing systems with and without KB.
to include input items referring specifically to the
topic and sentiment of the author. However, hu-
man responses are considered more relevant than
the computer-generated ones. This is confirmed
by a reliably negative regression coefficient for
IS COMP (see regression results in Table 7).
The analysis included the binary factor SOURCE
(+1 if the response came from the presented snip-
pet, ?1 if it came from a random article). We
see a positive interaction between SOURCE and
IS COMP, indicating that presenting a response
from a random article is more detrimental to rel-
evance of computer-generated responses than that
of the human responses. This is not surprising, as
the computer-generated responses (unlike the hu-
man responses) always includes the article?s topic.
When analyzing only data on computer-
generated responses, and including predictors for
agenda sentiment and for presence of the knowl-
edge base, we see that including the KB does not
affect response relevance (see Table 8). Also, there
is no interaction between KB and SOURCE, that
is, the effect of presenting a response from a dif-
ferent article does not differ between the models
with and without the knowledge base. Possibly,
responses are considered as more relevant if they
have more positive sentiment, but the evidence for
this is fairly weak.
4 Related and Future Work
In contrast to the vast amount of research on sen-
timent and topic analysis, as well as generation
tasks in which the input is artificial or pre-defined,
our system implements a full end-to-end cycle
from natural language analysis to natural language
generation with applications in social media and
automated interaction in real-world settings.
The only two other studies on response gener-
ation in social media we know of are Ritter et al.
(2011) and Hasegawa et al. (2013). Ritter?s and
Hasegawa?s approaches differ from ours in their
objective and their approach to generation. Specif-
ically, Ritter?s approach is based on machine trans-
lation, creating responses by directly re-using pre-
vious content. Their data-driven approach gener-
ates relevant, but not opinionated responses. In
addition, both Ritter?s and Hasegawa?s systems re-
spond to tweets, while our system analyzes and re-
sponds to complete articles. Hasegawa?s approach
is closer to ours in that it generates responses that
are intended to elicit a specific emotion from the
addressee. However, it still differs considerably in
settings (dialogues versus online posting) and in
the goal itself (eliciting emotion versus expressing
opinion). Thus, we see these studies as comple-
mentary to ours in the realm of response genera-
tion in social media.
64
A natural contact point of our work with exist-
ing work in social media analysis is the investiga-
tion of how a change in the implementation of in-
dividual components (e.g., topic inference or sen-
timent scoring) would affect the result of the over-
all generation. In particular, it would be interesting
to test whether a novel mechanism for joint infer-
ence of topic/sentiment distributions could lead to
improvement in the human-likeness of the gener-
ated responses.
The syntactic and semantic means of expres-
sion that we use are based on bare bone templates
and fine-grained POS tags (Theune et al., 2001).
These may potentially be expanded with different
ways to express subject/object relations, relations
between phrases, polarity of sentences, and so on.
Additional approaches to generation can factor in
such aspects, e.g., the template-based methods in
Becker (2002) and Narayan et al. (2011), or gram-
mar based methods, as in DeVault et al. (2008).
Using more sophisticated generation methods with
a rich grammatical backbone may combat the sen-
sitivity to computer-generated response patterns as
acquired by our human raters over time.
Furthermore, our result concerning the human-
likeness of g
kb
clearly demonstrates that semantic
knowledge must be brought in to support better,
and more human-like, response generation. Large-
scale knowledge graphs such as Freebase support
many semantic tasks (Jacobs, 1985), and can be
used for providing richer context for automatically
generating human-like responses.
From a theoretical viewpoint, the system will
clearly benefit from rigorous analysis of human
interaction in online media. Responses to user-
generated content on the Internet share some
linguistic characteristics in structure, length and
manner of expression. Studying these features the-
oretically and then examining them empirically
using a Turing-like evaluation as presented here
can take us a big step in the direction of better gen-
eration, and also better understanding of the pro-
cesses underlying human response generation.
This latter understanding may be complemented
with insights into the causes, motivations and in-
tricacies of human interaction in such environ-
ments, as studied by sociologists and psychol-
ogists. In particular, our preliminary interac-
tion with colleagues from communication stud-
ies suggests that the present endeavor nicely com-
plements that of ?persuasive computing? (Fogg,
1998; Fogg, 2002), and we hope that this collabo-
ration will lead to valuable synergies.
Finally, bridging the gap between the technical
and the theoretical, it would be fascinating to test
the responses in the context for which they are
generated ? social media. Generated texts may
be posted as a response to the original article, or
shared with a link of the original article, followed
by measuring the responses to, and shares of, that
response. Such real-world evaluation could indi-
cate that generated responses are indeed believable
and engaging, and may better simulate a Turing-
like test in which machine-generated responses
cannot be distinguished from human responses.
5 Conclusion
We presented a system for generating responses
that are directly tied to responders? agendas and
document content. To the best of our knowledge,
this is the first system to generate subjective re-
sponses directly reflecting users? agendas. Our re-
sponse generation architecture provides an easy-
to-use and easy-to-extend solution encompassing
a range of NLP and NLG techniques. We evalu-
ated both the human-likeness and the relevance of
the generated content, thereby empirically quan-
tifying the efficacy of computer-generated re-
sponses compared head-to-head against human re-
sponses.
Generating concise, relevant, and opinionated
responses that are also human-like is hard ? it
requires the integration of text-understanding and
sentiment analysis, and it is also contingent on the
expression of the agents? prior knowledge, reasons
and motives. We suggest our architecture and eval-
uation method as a baseline for future research
on generated content that would effectively pass
a Turing-like test, and successfully convince hu-
mans of the authenticity of generated responses.
5
Acknowledgments
We thank Yoav Francis for his contribution in the
early stages of this research. We further thank
our anonymous reviewers for their insightful com-
ments on an earlier draft.
5
Our code, training data, experimental data (computer and
human responses) and analysis scripts are publicly available
via www.tsarfaty.com/nlg-sd/.
65
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceed-
ings of the international conference on Web search
and web data mining, pages 183?194. ACM.
Teresa M. Amabile. 1981. Brilliant but Cruel: Per-
ceptions of Negative Evaluators. Washington, DC:
ERIC Clearinghouse.
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association (ELRA).
Douglas M. Bates. 2005. Fitting linear mixed models
in R. R News, 5:27?30.
Tilman Becker. 2002. Practical, template-based natu-
ral language generation with TAG. In Proceedings
of the 6th International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993?1022.
David M. Blei. 2012. Probabilistic topic models.
Commun. ACM, 55(4):77?84.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study
on amazon.com helpfulness votes. In Proceedings
of the 18th International Conference on World Wide
Web, WWW ?09, pages 141?150, New York, NY,
USA. ACM.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 241?249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, INLG ?08, pages 77?
85, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard
Hovy. 2006. An intelligent discussion-bot for
answering student queries in threaded discussions.
In Proceedings of Intelligent User Interface (IUI-
2006), pages 171?177.
B. J. Fogg. 1998. Persuasive computers: Perspec-
tives and research directions. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ?98, pages 225?232, New York,
NY, USA. ACM Press/Addison-Wesley Publishing
Co.
B. J. Fogg. 2002. Persuasive technology: Using com-
puters to change what we think and do. Ubiquity,
December.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A
realisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ?09, pages 90?93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
H. P. Grice. 1967. Logic and conversation. In H. P.
Grice, editor, Studies in the ways of words, pages
22?40. Harvard University Press.
Michael Haenlein and Andreas M. Kaplan. 2009.
Flagship brand stores within virtual worlds: The im-
pact of virtual store exposure on real-life attitude
toward the brand and purchase intent. Recherche
et Applications en Marketing (English Edition),
24(3):57?79.
Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,
and Masashi Toyoda. 2013. Predicting and eliciting
addressee?s emotion in online dialogue. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 964?972, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
?99, pages 50?57, New York, NY, USA. ACM.
Philip N. Howard, Aiden Duffy, Deen Freelon, Muza-
mmil Hussain, Will Mari, and Marwa Mazaid.
2011. Opening closed regimes: What was the role
of social media during the Arab spring? Project on
Information Technology and Political Islam.
Paul S Jacobs. 1985. A knowledge-based approach to
language production. Technical report, University
of California at Berkeley, Berkeley, CA, USA.
Pulkit Kathuria. 2012. Sentiment Clas-
sification using WSD, Maximum En-
tropy and Naive Bayes Classifiers.
https://github.com/kevincobain2000/sentiment classifier.
Visited March 2014.
Wiebke Lamer. 2012. Twitter and tyrants: New me-
dia and its effects on sovereignty in the Middle East.
Arab Media and Society.
Marc Langheinrich and G?unter Karjoth. 2011. Social
networking and the risk to companies and institu-
tions. Information Security Technical Report. Spe-
cial Issue: Identity Reconstruction and Theft, pages
51?56.
66
Paul Mah. 2012. Tools to automate your
customer service response on social me-
dia. http://www.itbusinessedge.com/blogs/smb-
tech/tools-to-automate-your-customer-service-
response-on-social-media.html. Visited August
2013.
Chris McConnell. 2012. When brands auto-
mate Twitter and Facebook responses I?ll re-
volt. http://dailytekk.com/2012/06/07/brands-
automating-social-media/. Visited August 2013.
Gilad Mishne. 2006. Multiple ranking strategies for
opinion retrieval in blogs. In Proceedings of the 15th
Text Retrieval Conference.
Kyoshi Mori, Adam Jatowt, and Mitsuru Ishizuka.
2003. Enhancing conversational flexibility in multi-
modal interactions with embodied lifelike agent. In
Proceedings of the 8th International Conference on
Intelligent User Interfaces, IUI ?03, pages 270?272,
New York, NY, USA. ACM.
Mickey Nall. 2013. You can?t automate so-
cial media engagement, argues PRSA?s Mickey
Nall. http://www.prmoment.com/1359/you-cant-
automate-social-media-engagement-argues-prsas-
mickey-nall.aspx. Visited August 2013.
Karthik Sankaran Narayan, Charles Lee Isbell Jr., and
David L. Roberts. 2011. Dextor: Reduced effort
authoring for template-based natural language gen-
eration. In Vadim Bulitko and Mark O. Riedl, ed-
itors, Proceedings of the Seventh Artificial Intelli-
gence and Interactive Digital Entertainment Confer-
ence. The AAAI Press.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen
and Samuel Gosling, editors, ICWSM. The AAAI
Press.
Brendan O?Connor, Brandon M. Stewart, and Noah A.
Smith. 2013. Learning to extract international rela-
tions from political context. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1094?1104. The Association for Computer Linguis-
tics.
Jeremiah Owyang. 2012. Brands Start Automating
Social Media Responses on Facebook and Twitter.
http://techcrunch.com/2012/06/07/brands-start-
automating-social-media-responses-on-facebook-
and-twitter/. Visited August 2013.
Christos H. Papadimitriou, Hisao Tamaki, Prabhakar
Raghavan, and Santosh Vempala. 1998. La-
tent Semantic Indexing: A probabilistic analy-
sis. In Proceedings of the Seventeenth ACM
SIGACT-SIGMOD-SIGART Symposium on Princi-
ples of Database Systems, PODS ?98, pages 159?
168, New York, NY, USA. ACM.
Erik Qualman. 2012. Socialnomics: How social media
transforms the way we live and do business. John
Wiley & Sons, Hoboken, NJ, USA, 2nd edition.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Nat. Lang.
Eng., 3(1):57?87.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
M. Theune, E. Klabbers, J. R. De Pijper, E. Krahmer,
and J. Odijk. 2001. From data to speech: A general
approach. Nat. Lang. Eng., 7(1):47?86.
Alan M. Turing. 1950. Computing machinery and in-
telligence. Mind, LIX:433?460.
Kees Van Deemter, Emiel Krahmer, and Mari?et The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Comput. Lin-
guist., 31(1):15?24.
Bimal Viswanath, Alan Mislove, Meeyoung Cha, and
Krishna P. Gummadi. 2009. On the evolution of
user interaction in Facebook. In Proceedings of
the 2nd ACM Workshop on Online Social Networks,
WOSN ?09, pages 37?42, New York, NY, USA.
ACM.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
477?485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
67
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 103?109 Dublin, Ireland, August 23-29 2014.
Introducing the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages
Djame? Seddah
INRIA & Univ. Paris Sorbonne
Paris, France
djame.seddah@paris-sorbonne.fr
Sandra Ku?bler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Reut Tsarfaty
Weizman Institute
Rehovot, Israel
reut.tsarfaty@weizmann.ac.il
1 Introduction
This first joint meeting on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis
of Non-Canonical English (SPMRL-SANCL) featured a shared task on statistical parsing of morpholog-
ically rich languages (SPMRL). The goal of the shared task is to allow to train and test different partic-
ipating systems on comparable data sets, thus providing an objective measure of comparison between
state-of-the-art parsing systems on data data sets from a range of different languages. This 2014 SPMRL
shared task is a continuation and extension of the SPMRL shared task, which was co-located with the
SPMRL meeting at EMNLP 2013 (Seddah et al., 2013).
This paper provides a short overview of the 2014 SPMRL shared task goals, data sets, and evaluation
setup. Since the SPMRL 2014 largely builds on the infrastructure established for the SPMRL 2013
shared task, we start by reviewing the previous shared task (?2) and then proceed to the 2014 SPMRL
evaluation settings (?3), data sets (?4), and a task summary (?5). Due to organizational constraints,
this overview is published prior to the submission of all system test runs, and a more detailed overview
including the description of participating systems and the analysis of their results will follow as part of
(Seddah et al., 2014), once the shared task is completed.
2 The SPMRL Shared Task 2013
The SPMRL Shared Task 2013 (Seddah et al., 2013) was organized with the goal of providing standard
data sets, streamlined evaluation metrics, and a set of strong baselines for parsing morphologically rich
languages (MRLs). The goals were both to provide a focal point for researchers interested in parsing
MRLs and consequently to advance the state of the art in this area of research.
The shared task focused on parsing nine morphologically rich languages, from different typological
language families, in both a constituent-based and a dependency-based format. The set of nine typolog-
ically diverse languages comprised data sets for Arabic, Basque, French, German, Hebrew, Hungarian,
Korean, Polish, and Swedish. Compared to previous multilingual shared tasks (Buchholz and Marsi,
2006; Nivre et al., 2007), the SPMRL shared task targeted parsing in realistic evaluation scenarios, in
which the analysis of morphologically ambiguous input tokens is not known in advance. An additional
novelty of the SPMRL shared task is that it allowed for both a dependency-based and a constituent-
based parse representation. This setting relied on an intricate and careful data preparation process which
ensured consistency between the constituent and the dependency version by aligning the two representa-
tion types at the token level and at the level of part-of-speech tags. For all languages, we provided two
versions of the data sets: an all data set, identical in size to the one made available by the individual
treebank providers, and a small data set, with a training set of 5,000 sentences, and a test set of about 500
sentences. Controlling the set sizes across languages allows us to level the playing field across languages
and treebanks.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
103
The shared task also advanced the state of the art by introducing different levels of complexity in
parsing. In general, parsing is reduced to the parsing proper step, assuming gold segmentation of the
text into sentences and words as well as gold POS tags and morphological analyses. This is a serious
simplification of the task since especially in Semitic languages, the segmentation into input tokens is a
task that is best performed in combination with parsing because of the ambiguities involved.
The shared task deviated from this standard configuration by adding conditions in which more realistic
settings were given: In the gold setting, unambiguous gold morphological segmentation, POS tags, and
morphological features for each input token were given. In the predicted setting, disambiguated morpho-
logical segmentation was provided, but the POS tags and morphological features for each input segment
were not. In the raw setting, there was no gold information, i.e., morphological segmentation, POS tags
and morphological features for each input token had to be predicted as part of the parsing task. To lower
the entry cost, participants were provided with reasonable baseline (if not state-of-the-art) morphological
predictions (either disambiguated ? in most cases? or ambiguous prediction in lattice forms).
As a consequence of the raw scenario, it was not possible to (only) rely on the accepted parsing met-
rics, labeled bracket evaluation via EVALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy,
2003) for constituents and CONLL X?s Labeled/Unlabeled Attachment Score for dependencies (Buch-
holz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be
discrepancies on the lexical levels, which neither EVALB and LEAF-ANCESTOR nor LAS/UAS are pre-
pared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic
structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the par-
ticipants, we did not try to enforce function label evaluation for constituent parsing. We hope that further
shared tasks will try to generalize such an evaluation. Indeed, having predicted function labels would
ease labeled TEDEVAL evaluation and favor a full parsing chain evaluation. Nevertheless, the choice of
TEDEVAL allowed us to go beyond the standard cross-parser evaluation within one setting and approach
cross-framework (constituent vs. dependency (Tsarfaty et al., 2012a)) and cross-language evaluation,
thus pushing the envelope on parsing evaluation. Additionally, we performed a specialized evaluation of
multi-word expressions in the French treebank.
The SPMRL Shared Task 2013 featured seven teams who approached the dependency parsing task
and one team that approached constituent parsing. The best performing system (Bjo?rkelund et al., 2013)
in either framework consisted of an ensemble system, combining several dependency parsers or sev-
eral instantiations of a PCFG-LA parser by a (re-)ranker, both on top of state-of-the-art morphological
analyses. The results show that parser combination helps to reach a robust performance across lan-
guages. However, the integration of morphological analysis into the parsing needs to be investigated
thoroughly, and new, morphologically aware approaches are needed. The cross-parser, cross-scenario,
and cross-framework evaluation protocols show that performance on gold morphological input is signif-
icantly higher than that in more realistic scenarios, and more training data is beneficial. Additionally,
differences between dependency and constituents are smaller than previously assumed, and languages
which are typologically farthest from English, such as Semitic and Asian languages, are still amongst
the hardest to parse, regardless of the parsing method used.
3 SPMRL 2014 Parsing Scenarios
As in the previous edition, this year, we consider three parsing scenarios, depending on how much of the
morphological information is provided. The scenarios are listed below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided with unambiguous gold morphological segmentation,
POS tags, and morphological features for each input token.
? Predicted: In this scenario, the parser is provided with disambiguated morphological segmentation.
However, the POS tags and morphological features for each input segment are unknown.
? Raw: In this scenario, the parser is provided with morphologically ambiguous input. The morpho-
logical segmentation, POS tags, and morphological features for each input token are unknown.
1We extended the usualEVALB to penalize unparsed sentences.
104
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation scenarios. X depicts gold information, ? depicts
unknown information, to be predicted by the system.
The Predicted and Raw scenarios require predicting morphological analyses. This may be done using
a language-specific morphological analyzer, or it may be done jointly with parsing. We provide inputs
that support these different scenarios:
? Predicted: Gold treebank segmentation is given to the parser. The POS tags assignment and mor-
phological features are automatically predicted by the parser or by an external resource.
? Raw (1-best): The 1-best segmentation and POS tags assignment is predicted by an external re-
source and given to the parser.
? Raw (all): All possible segmentations and POS tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all scenarios is shown in table 1. For languages in which terminals equal tokens, only
Gold and Predicted scenarios are considered. For the Semitic languages, we further provide input for
both Raw (1-best) and Raw (all) scenarios.2
4 SPMRL 2014 Data Sets
The main innovation of the SPMRL 2014 shared task with respect to the previous edition is the availabil-
ity of additional, unannotated data, for the purpose of semi-supervised training. This section provides
a description of the unlabeled-data preparation that is required in the context of parsing MRLs, and the
core labeled data that is used in conjunction with it.
4.1 SPMRL Unlabeled Data Set
One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data
sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c).
The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL
data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown
et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito
and Crabbe?, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for
Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing
German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results
for some MRLs (Cirik and S?ensoy, 2013).
By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly
compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development
of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table
2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations will
be provided in (Seddah et al., 2014). Note that we could not ensure the same volume of data for all
languages, nor we could run the same parser, or morphology prediction, on all data. Potential future work
could focus on ensuring a stricter level of comparability of these data or on investigating the feasibility
of such a normalization of procedures.
2The raw Arabic lattices were made available later than the other data. They are now included in the shared task release.
105
Language Source (main) type size (tree tokens) morph parsed
Arabic news domain news 120M X* X*
Basque web balanced 150M X X
French news domain newswire 120M X+mwe X*
German Wikipedia wiki (edited) 205M X X
Hebrew Wikipedia wiki (edited) 160M X X
Hungarian news domain newswire 100M X X
Korean news domain newswire 40M X X*
Polish Wikipedia wiki (edited) 100M X X
Swedish PAROLE balanced 24M X X
Table 2: Unlabeled data set properties.*: made available mid-july
4.2 SPMRL Core Labeled Data Set
In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used
the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic
data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the
Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance,
following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the
SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et
al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both
instances could be aligned at the token level. Regarding French, we used a new instance of the French
Treebank (Abeille? et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the
morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as
Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted
to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the
Modern Hebrew Treebank (Sima?an et al., 2001), with the Goldberg (2011) dependency version, in turn
aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in
order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8.
The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010),
while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to
dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in
(Wolin?ski et al., 2011; S?widzin?ski and Wolin?ski, 2010; Wro?blewska, 2012). Compared to the last year?s
edition, we added explicit feature names in the relevant data fields. The Swedish data originate from
(Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note
that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are
also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of
discontinuous structures in their phrase-based structures.
5 Conclusion
At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final
submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared
task edition, indicating an increased awareness of and continued interest in the topic of the shared task.
Results, cross-parser and cross-data analysis, and shared task description papers will be made available
at http://www.spmrl.org/spmrl2014-sharedtask.html.
Acknowledgments
We would like to express our gratitude to the original treebank labeled and unlabeled data contribu-
tors for the considerable time they devoted to our shared task. Namely, Arabic: Nizar Habash, Ryan
Roth (Columbia University); Spence Green (Stanford University) , Ann Bies, Seth Kulick, Mohamed
Maamouri (the Linguistic Data Consortium) ; Basque: Koldo Gojenola, Iakes Goenaga (University of
the Basque Country) ; French: Marie Candito (Univ. Paris 7 & Inria), Djame? Seddah (Univ. Paris
Sorbonne & Inria) , Matthieu Constant (Univ. Marne la Valle?e) ; German: Wolfgang Seeker (IMS
106
Stuttgart), Wolfgang Maier (Univ. of Dusseldorf), Yannick Versley (Univ. of Tuebingen) ; Hebrew:
Yoav Goldberg (Bar Ilan Univ.), Reut Tsarfaty (Weizmann Institute of Science) ; Hungarian: Richa`rd
Farkas, Veronika Vincze (Univ. of Szeged) ; Korean: Jinho D. Choi (Univ. of Massachusetts Amherst),
Jungyeul Park (Kaist); Polish: Adam Przepio?rkowski, Marcin Wolin?ski, Alina Wro?blewska (Institute
of Computer Science, Polish Academy of Sciences) ; Swedish: Joakim Nivre (Uppsala Univ.), Marco
Kuhlmann (Linko?ping University).
We gratefully acknowledge the contribution of Spra?kbanken and the University of Gothenburg for
providing the PAROLE corpus. We are also very grateful to the Philosophical Faculty of the Heinrich-
Heine Universita?t Du?sseldorf for hosting the shared task data via their dokuwiki.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel. 2003. Building a treebank for French. In Anne Abeille?,
editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. D??az de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In Proceedings of the Second Workshop on Treebanks and
Linguistic Theories, pages 201?204, Va?xjo?, Sweden.
Anders Bjo?rkelund, Ozlem Cetinoglu, Richa?rd Farkas, Thomas Mueller, and Wolfgang Seeker. 2013. (Re)ranking
meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 134?144, Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia Gdaniec, Ralph Grishman, Philip Harrison, Donald Hindle,
Robert Ingria, Frederick Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice
Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA Speech and Natural Language Workshop 1991, pages
306?311, Pacific Grove, CA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of CoNLL, pages 149?164, New York, NY.
Marie Candito and Beno??t Crabbe?. 2009. Improving generative statistical parsing with semi-supervised word
clustering. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT?09), pages
138?141, Paris, France.
Marie Candito and Djame? Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL/HLT Workshop on
Statistical Parsing of Morphologically Rich Languages (SPMRL 2010), Los Angeles, CA.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W. Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings of the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Volkan Cirik and Hu?snu? S?ensoy. 2013. The AI-KU system at the SPMRL 2013 shared task: Unsupervised features
for dependency parsing. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich
Languages, pages 68?75, Seattle, WA.
Matthieu Constant, Marie Candito, and Djame? Seddah. 2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and dependency parsing. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically-Rich Languages, pages 46?52, Seattle, WA.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and Andra?s Kocsor. 2005. The Szeged treebank. In Proceedings
of the 8th International Conference on Text, Speech and Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
107
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities.
In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL), pages 327?335, Athens,
Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of Modern Hebrew. Ph.D. thesis, Ben Gurion University of
the Negev.
Spence Green and Christopher D. Manning. 2010. Better Arabic parsing: Baselines, evaluations, and analysis. In
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,
Beijing, China.
Nizar Habash and Ryan Roth. 2009. CATiB: The Columbia Arabic Treebank. In Proceedings of ACL-IJCNLP,
pages 221?224, Suntec, Singapore.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical
machine translation. In Proceedings of ACL-08: HLT, Short Papers, pages 57?60, Columbus, OH.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic treebank: Build-
ing a large-scale annotated Arabic corpus. In Proceedings of NEMLAR International Conference on Arabic
Language Resources and Tools, pages 102?109, Cairo, Egypt.
Andreas Mengel and Wolfgang Lezius. 2000. An XML-based encoding format for syntactically annotated cor-
pora. In Proceedings of the Second International Conference on Language Resources and Engineering (LREC
2000), pages 121?126, Athens, Greece.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of LREC, pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Republic.
Ines Rehbein. 2011. Data point selection for self-training. In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?67, Dublin, Ireland.
Geoffrey Sampson and Anna Babarczy. 2003. A test of the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolf-
gang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Clergerie. 2013. Overview of the
SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182,
Seattle, WA.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho Choi, Matthieu Constant, Richa?rd Farkas,
Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze,
Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Cle?rgerie. 2014. Overview of the spmrl 2014
shared task on parsing morphologically rich languages. In Notes of the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages, Dublin, Ireland.
Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German
Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
3132?3139, Istanbul, Turkey.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann, and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues, 42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards a bank of constituent parse trees for Polish. In Proceed-
ings of Text, Speech and Dialogue, pages 197?204, Brno, Czech Republic.
108
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra Ku?bler, Marie Candito, Jennifer Foster, Yannick Versley,
Ines Rehbein, and Lamia Tounsi. 2010. Statistical parsing for morphologically rich language (SPMRL): What,
how and whither. In Proceedings of the First workshop on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012a. Cross-framework evaluation for statistical parsing.
In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012b. Joint evaluation for segmentation and parsing. In
Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djame? Seddah, Sandra Ku?bler, and Joakim Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Linguistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing. Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme of Stanford dependencies. In Proceedings of ACL, Sofia,
Bulgaria.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungarian
Dependency Treebank. In Proceedings of LREC, Valletta, Malta.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek S?widzin?ski. 2011. A preliminary version of Sk?adnica?a
treebank of Polish. In Proceedings of the 5th Language & Technology Conference, pages 299?303, Poznan?,
Poland.
Alina Wro?blewska. 2012. Polish Dependency Bank. Linguistic Issues in Language Technology, 7(1):1?15.
109
