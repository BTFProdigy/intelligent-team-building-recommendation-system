First Joint Conference on Lexical and Computational Semantics (*SEM), pages 673?678,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UOW: Semantically Informed Text Similarity
Miguel Rios and Wilker Aziz
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton,
WV1 1SB, UK
{M.Rios, W.Aziz}@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello,
Sheffield, S1 4DP, UK
L.Specia@sheffield.ac.uk
Abstract
The UOW submissions to the Semantic Tex-
tual Similarity task at SemEval-2012 use a
supervised machine learning algorithm along
with features based on lexical, syntactic and
semantic similarity metrics to predict the se-
mantic equivalence between a pair of sen-
tences. The lexical metrics are based on word-
overlap. A shallow syntactic metric is based
on the overlap of base-phrase labels. The
semantically informed metrics are based on
the preservation of named entities and on the
alignment of verb predicates and the overlap
of argument roles using inexact matching. Our
submissions outperformed the official base-
line, with our best system ranked above aver-
age, but the contribution of the semantic met-
rics was not conclusive.
1 Introduction
We describe the UOW submissions to the Semantic
Textual Similarity (STS) task at SemEval-2012. Our
systems are based on combining similarity scores as
features using a regression algorithm to predict the
degree of semantic equivalence between a pair of
sentences. We train the regression algorithm with
different classes of similarity metrics: i) lexical,
ii) syntactic and iii) semantic. The lexical similar-
ity metrics are: i) cosine similarity using a bag-of-
words representation, and ii) precision, recall and
F-measure of content words. The syntactic metric
computes BLEU (Papineni et al, 2002), a machine
translation evaluation metric, over a labels of base-
phrases (chunks). Two semantic metrics are used: a
metric based on the preservation of Named Entities
and TINE (Rios et al, 2011). Named entities are
matched by type and content: while the type has to
match exactly, the content is compared with the as-
sistance of a distributional thesaurus. TINE is a met-
ric proposed to measure adequacy in machine trans-
lation and favors similar semantic frames. TINE
attempts to align verb predicates, assuming a one-
to-one correspondence between semantic roles, and
considering ontologies for inexact alignment. The
surface realization of the arguments is compared us-
ing a distributional thesaurus and the cosine similar-
ity metric. Finally, we use METEOR (Denkowski
and Lavie, 2010), also a common metric for ma-
chine translation evaluation, that also computes in-
exact word overlap as at way of measuring the im-
pact of our semantic metrics.
The lexical and syntactic metrics complement the
semantic metrics in dealing with the phenomena ob-
served in the task?s dataset. For instance, from the
MSRvid dataset:
S1 Two men are playing football.
S2 Two men are practicing football.
In this case, as typical of paraphrasing, the situa-
tion and participants are the same while the surface
realization differs, but playing can be considered
similar to practicing. From the SMT-eur dataset:
S3 The Council of Europe, along with the Court of
Human Rights, has a wealth of experience of
such forms of supervision, and we can build on
these.
673
S4 Just as the European Court of Human Rights, the
Council of Europe has also considerable expe-
rience with regard to these forms of control; we
can take as a basis.
Similarly, here although with different realiza-
tions, the Court of Human Rights and the European
Court of Human Rights represent the same entity.
Semantic metrics based on predicate-argument
structure can play a role in cases when different re-
alization have similar semantic roles:
S5 The right of a government arbitrarily to set aside
its own constitution is the defining characteris-
tic of a tyranny.
S6 The right for a government to draw aside its con-
stitution arbitrarily is the definition character-
istic of a tyranny.
In this work we attempt to exploit the fact that su-
perficial variations such the ones in these examples
should still render very similarity scores.
In Section 2 we describe the similarity metrics in
more detail. In Section 3 we show the results of our
three systems. In Section 4 we discuss these results
and in Section 5 we present some conclusions.
2 Similarity Metrics
The metrics used in this work are as follows:
2.1 Lexical metrics
All our lexical metrics use the same surface repre-
sentation: words. However, the cosine metric uses
bag-of-words, while all the other metrics use only
content words. We thus first represent the sentences
as bag-of-words. For example, given the pair of sen-
tences S7 and S8:
S7 A man is riding a bicycle.
S8 A man is riding a bike.
the bag-of-words are S7 = {A, man, is, riding, a,
bicycle,.} and S8 = {A, man, is, riding, a, bike, .},
and the bag-of-content-words are S7 = {man, riding,
bicycle} and S8 = {man, riding, bike}.
We compute similarity scores using the following
metrics between a pair of sentencesA andB: cosine
distance (Equation 1), precision (Equation 2), recall
(Equation 3) and F-measure (Equation 4).
cosine(A,B) =
|A
?
B|
?
|A| ? |B|
(1)
precision(A,B) =
|A
?
B|
|B|
(2)
recall(A,B) =
|A
?
B|
|A|
(3)
F (A,B) = 2 ?
precision(A,B) ? recall(A,B)
precision(A,B) + recall(A,B)
(4)
2.2 BLEU over base-phrases
The BLEU metric is used for the automatic evalua-
tion of Machine Translation. The metric computes
the precision of exact matching of n-grams between
a hypothesis and reference translations. This sim-
ple procedure has limitations such as: the matching
of non-content words mixed with the counts of con-
tent words affects in a perfect matching that can hap-
pen even if the order of sequences of n-grams in the
hypothesis and reference translation are very differ-
ent, changing completely the meaning of the trans-
lation. To account for similarity in word order we
use BLEU over base-phrase labels instead of words,
leaving the lexical matching for other lexical and se-
mantic metrics. We compute the matchings of 1-
4-grams of base-phrase labels. This metric favors
similar syntactic order.
2.3 Named Entities metric
The goal of the metric is to deal with synonym enti-
ties. First, named entities are grouped by class (e.g.
Organization), and then the content of the named en-
tities within the same classes is compared through
cosine similarity. If the surface realization is differ-
ent, we retrieve words that share the same context
with the named entity using Dekang Lin?s distribu-
tional thesaurus (Lin, 1998). Therefore, the cosine
similarity will have more information than just the
named entities themselves. For example, from the
sentence pair S9 and S10:
S9 Companies include IBM Corp. ...
674
S10 Companies include International Business Ma-
chines ...
The entity from S9: IBM Corp. and the entity
from S10: International Business Machines have
the same tag Organization. The metric groups
them and adds words from the thesaurus result-
ing in the following bag-of-words. S9: {IBM
Corp.,... Microsoft, Intel, Sun Microsystems, Mo-
torola/Motorola, Hewlett-Packard/Hewlett-Packard,
Novell, Apple Computer...} and S10: {International
Business Machines,... Apple Computer, Yahoo, Mi-
crosoft, Alcoa...}. The metric then computes the co-
sine similarity between this expanded pair of bag-of-
words.
2.4 METEOR
This metric is also a lexical metric based on uni-
gram matching between two sentences. However,
matches can be exact, using stems, synonyms, or
paraphrases of unigrams. The synonym matching is
computed using WordNet (Fellbaum, 1998) and the
paraphrase matching is computed using paraphrase
tables (Callison-Burch et al, 2010). The structure of
the sentences is not not directly considered, but sim-
ilar word orders are rewarded through higher scores
for the matching of longer fragments.
2.5 Semantic Role Label metric
Rios et al (2011) propose TINE, an automatic met-
ric based on the use semantic roles to align predi-
cates and their respective arguments in a pair of sen-
tences. The metric complements lexical matching
with a shallow semantic component to better address
adequacy in machine translation evaluation. The
main contribution of such a metric is to provide a
more flexible way of measuring the overlap between
shallow semantic representations (semantic role la-
bels) that considers both the semantic structure of
the sentence and the content of the semantic compo-
nents.
This metric allows to match synonym predicates
by using verb ontologies such as VerbNet (Schuler,
2006) and VerbOcean (Chklovski and Pantel, 2004)
and distributional semantics similarity metrics, such
as Dekang Lin?s thesaurus (Lin, 1998), where pre-
vious semantic metrics only perform exact match of
predicate structures and arguments. For example, in
VerbNet the verbs spook and terrify share the same
class amuse-31.1, and in VerbOcean the verb dress
is related to the verb wear, so these are considered
matches in TINE.
The main sources of errors in this metric are the
matching of unrelated verbs and the lack of coverage
of the ontologies. For example, for S11 and S12,
remain and say are (incorrectly) related as given by
VerbOcean.
S11 If snow falls on the slopes this week, Christmas
will sell out too, says Schiefert.
S12 If the roads remain snowfall during the week,
the dates of Christmas will dry up, said
Schiefert.
For this work the matching of unrelated verbs is
a particularly crucial issue, since the sentences to be
compared are not necessarily similar, as it is the gen-
eral case in machine translation. We have thus mod-
ified the metric with a preliminary optimization step
which aligns the verb predicates by measuring two
degrees of similarity: i) how similar their arguments
are, and ii) how related the predicates? realizations
are. Both scores are combined as shown in Equation
5 to score the similarity between the two predicates
(Av, Bv) from a pair of sentences (A,B).
sim(Av,Bv) = (wlex ? lexScore(Av, Bv))
+(warg ? argScore(Aarg, Barg))
(5)
where wlex and warg are the weights for each
component, argScore(Aarg, Barg) is the similarity,
which is computed as in Equation 7, of the argu-
ments between the predicates being compared and
lexScore(Av, Bv) is the similarity score extracted
from the Dekang Lin?s thesaurus between the predi-
cates being compared. The Dekang Lin?s thesaurus
is an automatically built thesaurus, and for each
word it has an entry with the most similar words and
their similarity scores. If the verbs are related in the
thesaurus we use their similarity score as lexScore
otherwise lexScore = 0. The pair of predicates
with the maximum sim score is aligned. The align-
ment is an optimization problem where predicates
are aligned 1-1: we search for all 1-1 alignments that
lead to the maximum average sim for the pair of sen-
tences. For example, S13 and S14 have the follow-
ing list of predicates: S13 = {loaded, rose, ending}
675
and S14 = {laced, climbed}. The metric compares
each pair of predicates and it aligns the predicates
rose and climbed because they are related in the the-
saurus with a similarity score lexScore = 0.796
and a argScore = 0.185 given that the weights are
set to 0.5 and sum up to 1 the predicates reach the
maximum sim = 0.429 score. The output of this
step results in a set of aligned verbs between a pair
of sentences.
S13 The tech - loaded Nasdaq composite rose 0
points to 0 , ending at its highest level for 0
months.
S14 The technology - laced Nasdaq Composite In-
dex IXIC climbed 0 points , or 0 percent , to
0.
The SRL similarity metric semanticRole be-
tween two sentences A and B is then defined as:
semanticRole(A,B) =
?
v?V verbScore(Av, Bv)
|VB |
(6)
The verbScore in Equation 6 is computed over
the set of aligned predicates from the previous opti-
mization step and for each aligned predicate the ar-
gument similarity is computed by Equation 7.
verbScore(Av, Bv) =
?
arg?ArgA?ArgB
argScore(Aarg, Barg)
|ArgB |
(7)
In Equation 6, V is the set of verbs aligned between
the two sentences A and B, and |VB| is the num-
ber of verbs in one of the sentences.1 The similar-
ity between the arguments of a verb pair (Av, Bv)
in V is measured as defined in Equation 7, where
ArgA and ArgB are the sets of labeled arguments
of the first and the second sentences and |ArgB| is
the number of arguments of the verb in B.2 The
argScore(Aarg, Barg) computation is based on the
cosine similarity as in Equation 1. We treat the to-
kens in the argument as a bag-of-words.
1This is inherited from the use of the metric focusing on re-
call in machine translation, where the B is the reference trans-
lation. In this work a better approach could be to compute this
metric twice, in both directions.
2Again, from the analogy of a recall metric for machine
translation.
3 Experiments and Results
We use the following state-of-the-art tools to pre-
process the data for feature extraction: i) Tree-
Tagger3 for lemmas and ii) SENNA (Collobert et
al., 2011)4 for Part-of-Speech tagging, Chunking,
Named Entity Recognition and Semantic Role La-
beling. SENNA has been reported to achieve an F-
measure of 75.79% for tagging semantic roles on the
CoNLL-2005 2 benchmark. The final feature set in-
cludes:
? Lexical metrics
? Cosine metric over bag-of-words
? Precision over content words
? Recall over content words
? F-measure over content words
? BLEU metric over chunks
? METEOR metric over words (with stems, syn-
onyms and paraphrases)
? Named Entity metric
? Semantic Role Labeling metric
The Machine Learning algorithm used for re-
gression is the LIBSVM5 Support Vector Machine
(SVM) implementation using the radial basis kernel
function. We used a simple genetic algorithm (Back
et al, 1999) to tune the parameters of the SVM. The
configuration of the genetic algorithm is as follows:
? Fitness function: minimize the mean squared
error found by cross-validation
? Chromosome: real numbers for SVM parame-
ters ?, cost and 
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
5http://www.csie.ntu.edu.tw/ cjlin/libsvm/
676
? Mutation probability: 0.01
We submitted three system runs, each is a varia-
tion of the above feature set. For the official submis-
sion we used the systems with optimized SVM pa-
rameters. We trained SVM models with each of the
following task datasets: MSRpar, MSRvid, SMT-
eur and the combination of MSRpar+MSRvid. For
each test dataset we applied their respective training
models, except for the new test sets, not covered by
any training set: for On-WN we used the combina-
tion MSRpar+MSRvid, and for SMT-news we used
SMT-eur.
Tables 1 to 3 focus on the Pearson correlation
of our three systems/runs for individual datasets of
the predicted scores against human annotation, com-
pared against the official baseline, which uses a sim-
ple word overlap metric. Table 4 shows the aver-
age results over all five datasets, where ALL stands
for the Pearson correlation with the gold standard
for the five dataset, Rank is the absolute rank among
all submissions, ALLnrm is the Pearson correlation
when each dataset is fitted to the gold standard us-
ing least squares, RankNrm is the corresponding
rank and Mean is the weighted mean across the five
datasets, where the weight depends on the number
of sentence pairs in the dataset.
3.1 Run 1: All except SRL features
Our first run uses the lexical, BLEU, METEOR and
Named Entities features, without the SRL feature.
Table 1 shows the results over the test set, where
Run 1-A is the version without SVM parameter op-
timization and Run 1-B are the official results with
optimized parameters for SVM.
Task Run 1-A Run 1-B Baseline
MSRpar 0.455 0.455 0.433
MSRvid 0.706 0.362 0.300
SMT-eur 0.461 0.307 0.454
On-WN 0.514 0.281 0.586
SMT-news 0.386 0.208 0.390
Table 1: Results for Run 1 using lexical, chunking,
named entities and METEOR as features. A is the non-
optimized version, B are the official results
3.2 Run 2: SRL feature
In this run we use only the SRL feature in order to
analyze whether this feature on its own could be suf-
ficient or lexical and other simpler features are im-
portant. Table 2 shows the results over the test set
without parameter optimization (Run 2-A) and the
official results with optimized parameters for SVM
(Run 2-B).
Task Run 2-A Run 2-B Baseline
MSRpar 0.335 0.300 0.433
MSRvid 0.264 0.291 0.300
SMT-eur 0.264 0.161 0.454
On-WN 0.281 0.257 0.586
SMT-news 0.189 0.221 0.390
Table 2: Results for Run 2 using the SRL feature only. A
is the non-optimized version, B are the official results
3.3 Run 3: All features
In the last run we use all features. Table 3 shows
the results over the test set without parameter opti-
mization (Run 3-A) and the official results with op-
timized parameters for SVM (Run 3-B).
Task Run 3-A Run 3-B Baseline
MSRpar 0.472 0.353 0.433
MSRvid 0.705 0.572 0.300
SMT-eur 0.471 0.307 0.454
On-WN 0.511 0.264 0.586
SMT-news 0.410 0.116 0.390
Table 3: Results for Run 3 using all features. A is the
non-optimized version, B are the official results
4 Discussion
Table 4 shows the ranking and normalized offi-
cial scores of our submissions compared against the
baseline. Our submissions outperform the official
baseline but significantly underperform the top sys-
tems in the shared task. The best system (Run 1)
achieved an above average ranking, but disappoint-
ingly the performance of our most complete system
(Run 3) using the semantic metric is poorer. Sur-
prisingly, the results of the non-optimized versions
outperform the optimized versions used in our offi-
cial submission. One possible reason for that is the
overfitting of the optimized models to the training
sets.
Run 1 and Run 3 have very similar results: the
overall correlation between all datasets of these two
systems is 0.98. One of the reasons for these results
is that the SRL metric is compromised by the length
677
System ALL Rank ALLnrm RankNrm Mean RankMean
Run 1 0.640 36 0.719 71 0.382 80
Run 2 0.536 59 0.629 88 0.257 88
Run 3 0.598 49 0.696 82 0.347 84
Baseline 0.311 87 0.673 85 0.436 70
Table 4: Official results and ranking over the test set for Runs 1-3 with SVM parameters optimized
of the sentences. In the MSRvid dataset, where the
sentences are simple such as ?Someone is drawing?,
resulting in a good semantic parsing, a high per-
formance for this metric is achieved. However, in
the SMT datasets, sentences are much longer (and
often ungrammatical, since they are produced by a
machine translation system) and the performance of
the metric drops. In addition, the SRL metric makes
mistakes such as judging as highly similar sentences
such as ?A man is peeling a potato? and ?A man is
slicing a potato?, where the arguments are the same
but the situations are different.
5 Conclusions
We have presented our systems based on similar-
ity scores as features to train a regression algorithm
to predict the semantic similarity between a pair
of sentences. Our official submissions outperform
the baseline method, but have lower performance
than most participants, and a simpler version of the
systems without any parameter optimization proved
more robust. Disappointingly, our main contribu-
tion, the addition of a metric based on Semantic Role
Labels shows no improvement as compared to sim-
pler metrics.
Acknowledgments
This work was supported by the Mexican National
Council for Science and Technology (CONACYT),
scholarship reference 309261.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. Cambridge, MA ; London,
May.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011. Tine:
A metric to assess mt adequacy. Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
678
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 779?784,
Dublin, Ireland, August 23-24, 2014.
UoW: Multi-task Learning Gaussian Process
for Semantic Textual Similarity
Miguel Rios
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton,
WV1 1SB, UK
M.Rios@wlv.ac.uk
Lucia Specia
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello,
Sheffield, S1 4DP, UK
L.Specia@sheffield.ac.uk
Abstract
We report results obtained by the UoW
method in SemEval-2014?s Task 10 ? Mul-
tilingual Semantic Textual Similarity. We
propose to model Semantic Textual Simi-
larity in the context of Multi-task Learning
in order to deal with inherent challenges of
the task such as unbalanced performance
across domains and the lack of training
data for some domains (i.e. unknown
domains). We show that the Multi-task
Learning approach outperforms previous
work on the 2012 dataset, achieves a ro-
bust performance on the 2013 dataset and
competitive results on the 2014 dataset.
We highlight the importance of the chal-
lenge of unknown domains, as it affects
overall performance substantially.
1 Introduction
The task of Semantic Textual Similarity (STS)
(Agirre et al., 2012) is aimed at measuring the
degree of semantic equivalence between a pair of
texts. Natural Language Processing (NLP) ap-
plications such as Question Answering (Lin and
Pantel, 2001), Text Summarisation (Lin and Hovy,
2003) and Information Retrieval (Park et al., 2005)
rely heavily on the ability to measure semantic
similarity between pairs of texts. The STS eval-
uation campaign provides datasets that consist of
pairs of sentences from different NLP domains
such as paraphrasing, video paraphrasing, and ma-
chine translation (MT) evaluation. The participat-
ing systems are required to predict a graded simi-
larity score from 0 to 5, where a score of 0 means
that the two sentences are on different topics and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
a score of 5 means that the two sentences have ex-
actly the same meaning.
Methods for STS are commonly based on com-
puting various types of similarity metrics between
the pair of sentences, where the similarity scores
are used as features to train regression algorithms.
B?ar et al. (2012) use similarity metrics of vary-
ing complexity. The range of features goes from
simple string similarity metrics to complex vector
space models. The method yielded the best av-
erage results based on the official evaluation met-
rics, despite not having achieved the best results
in all individual domains.
?
Sari?c et al. (2012) use a
similar set up, extracting features from similarity
metrics, where these features are based on word-
overlap and syntax similarity. The method was
among the best for domains related to paraphras-
ing. It also achieved a high correlation between
the training and test data. In contrast, for the ma-
chine translation data the performance in the test
set was lower than the one over the training data.
A possible reason for the poor results on this do-
main is the difference in length between the train-
ing and test sentences, as in the test data the pairs
tend to be short and share similar words.
?
Sari?c et
al. (2012) claim that these differences show that
the MT training data is not representative of the
test set given their choice of features.
Most of the participating systems in the STS
challenges achieve good results on certain do-
mains (i.e. STS datasets), but poor results on oth-
ers. Even the most robust methods still show a big
gap in performances for different datasets. In the
second evaluation campaign of STS a new chal-
lenge was proposed: domains for which no train-
ing sets are provided, but only test sets. Heilman
and Madnani (2013) propose to incorporate do-
main adaptation techniques (Daum?e et al., 2010)
for STS to generalise models to new domains.
They add new features into the model, where the
feature set contains domain specific features plus
779
general task features. The machine learning al-
gorithm infers the extra weights of each specific
domain and of the general domain. When an in-
stance of a specific domain is to be predicted, only
the copy of the features of that domain will be ac-
tive; if the domain is unknown, the general fea-
tures will be active. Severyn et al. (2013) pro-
pose to use meta-classification to cope with do-
main adaptation. They merge each pair into a sin-
gle text and extract meta-features such as bag-of-
words and syntactic similarity scores. The meta-
classification model predicts, for each instance, its
most likely domain based on these features.
A possible solution to alleviate unbalanced per-
formances on different domains is to model STS
in the context of Multi-task Learning (MTL). The
motivation behind MTL is that by learning multi-
ple related tasks simultaneously the model perfor-
mance may improve compared to the case where
the tasks are learnt separately. MTL is based on
the assumption that related tasks can be clustered
and inter-task correlations between tasks within
the same cluster can be transferred.
We propose to model STS using MTL based
on a state-of-the-art STS feature set (
?
Sari?c et al.,
2012). As algorithm we use a non-parametric
Bayesian approach, namely Gaussian Processes
(GP) (Rasmussen, 2006). We show that the MTL
model outperforms previous work on the 2012
datasets and leads to robust performance on the
2013 datasets. On the STS 2014 challenge, our
method shows competitive results.
2 Experimental Setting
We apply MTL to cope with the challenge of un-
balanced performances across domains and un-
known domains present in the STS datasets.
2.1 TakeLab Features
We use the features from one the top perform-
ing system in STS 2012: the TakeLab
1
system,
which is publicly available. It extracts the follow-
ing types of features:
N-gram overlap is the harmonic mean of the de-
gree of matching between the first and second
texts, and vice-versa. The overlap is com-
puted for unigrams, bigrams, and trigrams.
WordNet-augmented word overlap is the par-
tial WordNet path length similarity score as-
1
http://takelab.fer.hr/sts/
signed to words that are not common to both
texts.
Vector space sentence similarity is the repre-
sentation of each text as a distributional vec-
tor by summing the distributional (i.e., LSA)
vectors of each word in the text and taking the
cosine distance between these texts vectors.
Shallow NE similarity is the matching between
Named Entities (NE) that indicates whether
they were found in both texts.
Numbers overlap is an heuristic that penalises
differences between numbers in texts.
Altogether, these features make up a vector of 21
similarity scores.
2.2 Multi-task Gaussian Processes
Gaussian Processes (Rasmussen, 2006) is a
Bayesian non-parametric machine learning frame-
work based on kernels for regression and classifi-
cation. In GP regression, for the inputs x we want
to learn a function f that is inferred from a GP
prior:
f(x) ? GP (m(x), k(x, x?)), (1)
where m(x) defines a 0 mean and k(x, x?) defines
the covariance or kernel functions. In the single
output case, the random variables are associated
to a process f evaluated at different values of the
input x. In the multiple output case, the random
variables are associated to different processes and
evaluated at different values of x.
We are interested in the intrinsic coregionaliza-
tion model for GP. A coregionalization model is
a heterotopic MTL model in which each output is
associated with a different set of inputs. In our
case the different set of inputs are the STS do-
mains (i.e. datasets). The intrinsic coregionaliza-
tion model (i.e. MTL-GP) is based on a separable
multi-task kernel (
?
Alvarez et al., 2012) of the form
K(X,X) = B ? k(X,X), (2)
where k(X,X) is a standard kernel over the in-
put points and B is a positive semi-definite ma-
trix encoding task covariances, called coregion-
alization matrix. B is built from other matrices
B = WW
>
+ diag(k), where W is a matrix that
determines the correlations between the different
outputs and k is a matrix which allows the outputs
780
(i.e. tasks) to behave independently. The repre-
sentation of data points is augmented with task ids
and given the id of a pair of data points the co-
variance from the standard kernel between them
is multiplied by a corresponding covariance from
B, which modifies the data points? covariance de-
pending on whether they belong to the same task
or different tasks.
The coregionalization matrix B allows us to
control the amount of inter and intra task transfer
of learning among tasks. Cohn and Specia (2013)
propose different types of B matrices to model
the problem of predicting the quality of machine
translations. They developed B matrices that rep-
resent an explicit intra-task transfer to be a part of
the parameterised kernel function. We use a de-
fault B where the weights of the matrix are learnt
along with the hyper-parameters by the GP tool.
For training our method we use the GPy toolkit
2
with a combination of RBF and coregionalization
kernels. The parameters used to build the core-
gionalization matrix are the number of outputs to
coregionalize and the rank of W . For example,
in the 2012 training set, the number of outputs
to coregionalize is 3, given that we have three
tasks/domains. The B matrix and the RBF kernel
hyper-parameters are jointly optimised. Each in-
stance of the training data is then augmented with
the id of their corresponding task. During test-
ing a new instance has to be matched to a specific
task/domain id from the training data. In the case
of an unknown test domain, we match it to a train-
ing domain which is similar, given the description
of the test dataset.
For the STS 2014 dataset, given the large num-
ber of training instances, we train a sparse GP
model within GPy. The main limitation of the GP
model is the that memory demands grow O(n
2
),
and the computational demands grow O(n
3
), with
n equals the number of training instances. Sparse
methods (e.g. (Titsias, 2009)) try to overcome this
limitation by constructing an approximation of the
full model on a smaller set of m support or induc-
ing instances that allow the reduction of compu-
tational demands to O(nm
2
). For the sparse GP
we use the same combination of kernels as the full
model, where we chose empirically the number of
inducing instances m and the GP tool randomly
selects the instances from the training data.
2
https://github.com/SheffieldML/GPy
3 Results and Discussion
In what follows we show a comparison with previ-
ous work on the STS 2012 and 2013 datasets, and
the official results for English and Spanish STS
2014 datasets.
3.1 STS 2012 and STS 2013
For training we use the STS 2012 training datasets
and we compare the results on the STS 2012 with
publicly available systems and with the official
Baseline, which is based on the cosine metric com-
puted over word overlaps. The official evaluation
metric is Pearson?s correlation. We match the un-
known domain OnWN to MSRpar given that the
domain of paraphrasing is that of news from the
web, which potentially contains a broad enough
vocabulary to cover OnWN.
Table 3.1 shows a comparison of the MTL-GP
with previous work on the STS 2012 data, where
our method outperforms them for most of the do-
mains. Our method improves the results of Take-
Lab with the same feature set. In other words,
the transfer learning improves over (
?
Sari?c et al.,
2012), which is trained with a separate Support
Vector Regression model for each domain. We
note that we can only compare our method against
the simpler version of TakeLab that is available.
A different version using syntactic features was
also proposed, where most results do not show a
significant variation, except for an improvement
of r=0.4683 in the SMTnews dataset. For the
complete alternative results we refer the reader to
(
?
Sari?c et al., 2012).
On the STS 2013 dataset, we compare our
method with work based on domain adaptation
and the official baseline. We use the 2012 data for
training as no additional training data is provided
in 2013. Table 3.1 shows all the possible match-
ing combinations between the STS 2013 test sets
and STS 2012 training sets. The best results are
given by matching the STS 2013 test sets with the
MSRvid domain, where all 2013 sets achieve their
best results.
In Table 3.1, we show the comparison with
previous work on the 2013 datasets, where we
use the best matching result from Table 3.1
(MSRvid). Our method shows very competitive
results but only with the correct matching of do-
mains, whereas the worst performed matching
(SMTeuroparl, Table 3.1) shows results that are
closer to the official Baseline. In previous work
781
Method MSRpar MSRvid SMTeuroparl SMTnews OnWN
?
Sari?c et al. (2012) 0.7343 0.8803 0.4771 0.3989 0.6797
B?ar et al. (2012) 0.68 0.8739 0.5280 0.4937 0.6641
MTL-GP 0.7324 0.8877 0.5615 0.6053 0.7256
Baseline 0.4334 0.2996 0.4542 0.3908 0.5864
Table 1: Comparison with previous work on the STS 2012 test datasets.
(Heilman and Madnani, 2013), domain adaptation
is performed with the addition of extra features
and the subsequent extra parameters to the model,
where in the MTL-GP the transfer learning is done
with the coregionalization matrix and does not de-
pend on large amounts of data.
3.2 English STS 2014
The training dataset consists of the combination of
each English training and test STS datasets from
2012 and 2013, which results in 7 domains. For
testing, in our first run we matched similar do-
mains with each other and the unknown domain
with MSRpar. For our second run, we matched
the unknown domains with a similar one. The
domain matching (test/training) was done as fol-
lows: deft-forum/MSRpar, deft-news/SMTnews,
tweet-news/SMTnews and images/MSRvid. For
our third run, the difference in matching is for deft-
news/headlines and tweet-news/headlines, where
the other domains remain with the same match-
ing. Table 3.2 shows the official STS 2014 results
where our best method (i.e. run3) achieves rank
10.
In Table 3.2, we show the comparison of the
MTL-GP and the sparse MTL-GP with the best
2014 system (DLSCU run2). For both MTL meth-
ods we match the 2014 domains with the train-
ing domain headlines. For the sparse MTL-GP,
we chose empirically a number m of 500 ran-
domly induced points. For reference, the corre-
lation of sparse MTL-GP with 50 points on deft-
forum is r=0.4691 obtained in 0.23 hours, with
100 points, r=0.4895, with 500 points, r=0.4912,
and with 1000 points, r=0.4911. The sparse MTL-
Test
Train
MSRvid MSRpar SMTeuroparl
Headlines
0.6666 0.6595 0.5693
OnWN
0.6516 0.4635 0.4113
FNWN
0.4062 0.3217 0.2344
Table 2: Matching of new 2013 domains with
2012 training data.
GP with 500 points runs in 1.38 hours, compared
to 2.39 hours for the full MTL-GP
3
. Addition-
ally, the sparse version achieves similar results to
the full model and very competitive performance
compared to the best STS 2014 system. However,
the result for OnWN is substantially lower than the
best system. This result can be highly improved
(r=0.7990) if the test set is matched with the cor-
respondent training domain.
3.3 Spanish STS 2014
For the Spanish STS subtask we use both sim-
ple and state-of-the-art (SoA) features to train the
MTL-GP. The simple features are similarity scores
from string metrics such as Levenshtein, Gotoh,
Jaro, etc.
4
The SoA similarity features come again
from TakeLab. The training dataset consists of the
combination of each English STS domains from
2012 and 2013 and the Spanish trial dataset with
task-id matching each instance to a given domain.
We represent the feature vectors with sparse fea-
tures for the English and Spanish training datasets,
where in English the pairs have simple and SoA
features, and for Spanish, only the simple features.
In other words, the feature vectors have the same
number of features (34): 13 simple features and 21
SoA features. However, for Spanish the SoA fea-
tures are set to 0 in training and testing. The moti-
vation to use SoA and simple features in English is
that the extra information will improve the transfer
learning on the English domains and discriminate
between the English domains and the Spanish do-
main, which only contains simple features. For
testing we only extracted the simple features; the
SoA features were set to 0. For the coregionaliza-
tion matrix we set the number of domains to be the
English STS domains from 2012 and 2013, plus
the Spanish trial, where the Spanish is treated as an
additional domain, which results in 8 domains. In
the first run of testing, we matched the test datasets
to the Spanish domain, and in the second run we
matched the datasets to the English MSRpar do-
3
Intel Xeon(R) at 2.67GHz with 24 cores
4
https://github.com/Simmetrics/simmetrics
782
Method Headlines OnWN FNWN
Heilman and Madnani (2013) 0.7601 0.4631 0.3516
Severyn et al. (2013) 0.7465 0.5572 0.3875
MTL-GP 0.6666 0.6516 0.4062
Baseline 0.5399 0.2828 0.2146
Table 3: Comparison between best matching MTL-GP (MSRvid) and previous work on the STS 2013
test datasets.
Run deft-forum deft-news headlines images OnWN tweet-news Weighted mean rank
UoW run1 0.3419 0.7512 0.7535 0.7763 0.7990 0.7368 0.7143 11
UoW run2 0.3419 0.5875 0.7535 0.7877 0.7990 0.6281 0.6817 17
UoW run3 0.3419 0.7634 0.7535 0.7877 0.7990 0.7529 0.7207 10
Table 4: Official English STS 2014 results.
main. Table 3.3 shows the official results for the
Spanish subtask, where our method achieves com-
petitive performance, placed 7 in the systems rank-
ing. We only show the results for the first run as
both runs achieved the same performance.
Run Wikipedia News Weighted
mean
rank
UoW 0.7483 0.8001 0.7792 7
Table 6: Official Spanish STS 2014 results.
Table 3.3 shows the comparison of the best
Spanish STS 2014 system (UMCC DLSI run2)
against two different sparse MTL-GP matched
with the Spanish trial with 500 inducing points.
Sparse MTL-GP run1 uses the sparse features de-
scribed above, while run2 uses a modification of
the feature set consisting in specific features for
each type of domain. For the English domains
the simple features are set to 0, and for Spanish
the SoA are still set to 0. The difference between
sparse MTL-GP models is very small, where the
use of all the features on the English domains im-
proves the results. However, the performance of
both models is still substantially lower than that of
the best system.
Run Wikipedia News
UMCC DLSI run2 0.7802 0.8254
Sparse MTL-GP run1 0.7468 0.7959
Sparse MTL-GP run2 0.7380 0.7878
Table 7: Comparison of best system against sparse
MTL-GP STS 2014 results.
4 Conclusions
We propose the use of MTL for STS. We show
that MTL improves the results of one of the best
STS systems, TakeLab. However, the match-
ing of an unknown domain during testing proved
a key challenge that affects performance signifi-
cantly. Given the results of STS 2013 and 2014,
our method tends to achieve best results when
known/unknown domains are matched to the same
training domains (i.e. MSRpar for 2013 and head-
lines for 2014). The sparse MTL-GP shows sim-
ilar performance to the full GP model, but takes
half the time to be trained. In the Spanish subtask,
we train our method with English datasets and the
Spanish trial data as an additional domain. For
this subtask our method also shows competitive re-
sults. Future work involves the automatic match-
ing of unknown domains at test time via meta-
classification (Severyn et al., 2013).
Acknowledgments
This work was supported by the Mexican National
Council for Science and Technology (CONA-
CYT), scholarship reference 309261, and by the
QTLaunchPad (EU FP7 CSA No. 296347)
project.
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics, SemEval ?12, pages 385?393,
Stroudsburg, PA, USA.
Mauricio A.
?
Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for vector-valued func-
tions: A review. Found. Trends Mach. Learn.,
4(3):195?266, March.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
783
Run deft-forum deft-news headlines images OnWN tweet-news
DLSCU run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639
Best matching MTL-GP 0.4903 0.7633 0.7535 0.8063 0.7222 0.7528
Sparse MTL-GP 0.4910 0.7642 0.7540 0.8057 0.7276 0.7539
Table 5: Comparison between best matching MTL-GP (headlines), Sparse MTL-GP and best STS 2014
system.
Joint Conference on Lexical and Computational Se-
mantics, SemEval ?12, pages 435?440, Stroudsburg,
PA, USA.
Trevor Cohn and Lucia Specia. 2013. Modelling an-
notator bias with multi-task gaussian processes: An
application to machine translation quality estima-
tion. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL-2013, pages 32?
42, Sofia, Bulgaria.
Hal Daum?e, III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Pro-
cessing, DANLP 2010, pages 53?59, Stroudsburg,
PA, USA.
Michael Heilman and Nitin Madnani. 2013. Henry-
core: Domain adaptation and stacking for text simi-
larity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), pages 96?102,
Atlanta, Georgia, USA, June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, NAACL ?03,
pages 71?78, Stroudsburg, PA, USA.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for question-answering. Nat. Lang.
Eng., 7(4):343?360.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang.
2005. Techniques for improving web retrieval effec-
tiveness. Inf. Process. Manage., 41(5):1207?1223.
Carl Edward Rasmussen. 2006. Gaussian processes
for machine learning. MIT Press.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. ikernels-core: Tree kernel learning
for textual similarity. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 53?58, Atlanta, Georgia, USA, June.
Michalis Titsias. 2009. Variational Learning of Induc-
ing Variables in Sparse Gaussian Processes. In the
12th International Conference on Artificial Intelli-
gence and Statistics (AISTATS).
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, SemEval ?12, pages 441?
448, Stroudsburg, PA, USA.
784
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116?122,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TINE: A Metric to Assess MT Adequacy
Miguel Rios, Wilker Aziz and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{m.rios, w.aziz, l.specia}@wlv.ac.uk
Abstract
We describe TINE, a new automatic evalua-
tion metric for Machine Translation that aims
at assessing segment-level adequacy. Lexical
similarity and shallow-semantics are used as
indicators of adequacy between machine and
reference translations. The metric is based on
the combination of a lexical matching com-
ponent and an adequacy component. Lexi-
cal matching is performed comparing bags-
of-words without any linguistic annotation.
The adequacy component consists in: i) us-
ing ontologies to align predicates (verbs), ii)
using semantic roles to align predicate argu-
ments (core arguments and modifiers), and
iii) matching predicate arguments using dis-
tributional semantics. TINE?s performance
is comparable to that of previous metrics
at segment level for several language pairs,
with average Kendall?s tau correlation from
0.26 to 0.29. We show that the addition of
the shallow-semantic component improves the
performance of simple lexical matching strate-
gies and metrics such as BLEU.
1 Introduction
The automatic evaluation of Machine Translation
(MT) is a long-standing problem. A number of met-
rics have been proposed in the last two decades,
mostly measuring some form of matching between
the MT output (hypothesis) and one or more human
(reference) translations. However, most of these
metrics focus on fluency aspects, as opposed to ad-
equacy. Therefore, measuring whether the meaning
of the hypothesis and reference translation are the
same or similar is still an understudied problem.
The most commonly used metrics, BLEU (Pap-
ineni et al, 2002) and alike, perform simple exact
matching of n-grams between hypothesis and refer-
ence translations. Such a simple matching proce-
dure has well known limitations, including that the
matching of non-content words counts as much as
the matching of content words, that variations of
words with the same meaning are disregarded, and
that a perfect matching can happen even if the order
of sequences of n-grams in the hypothesis and ref-
erence translation are very different, changing com-
pletely the meaning of the translation.
A number of other metrics have been proposed
to address these limitations, for example, by allow-
ing for the matching of synonyms or paraphrases
of content words, such as in METEOR (Denkowski
and Lavie, 2010). Other attempts have been made
to capture whether the reference translation and hy-
pothesis translations share the same meaning us-
ing shallow semantics, i.e., Semantic Role Labeling
(Gime?nez and Ma?rquez, 2007). However, these are
limited to the exact matching of semantic roles and
their fillers.
We propose TINE, a new metric that comple-
ments lexical matching with a shallow semantic
component to better address adequacy. The main
contribution of such a metric is to provide a more
flexible way of measuring the overlap between shal-
low semantic representations that considers both the
semantic structure of the sentence and the content
of the semantic elements. The metric uses SRLs
such as in (Gime?nez and Ma?rquez, 2007). However,
it analyses the content of predicates and arguments
seeking for either exact or ?similar? matches. The
116
inexact matching is based on the use of ontologies
such as VerbNet (Schuler, 2006) and distributional
semantics similarity metrics, such as Dekang Lin?s
thesaurus (Lin, 1998) .
In the remainder of this paper we describe some
related work (Section 2), present our metric - TINE
- (Section 3) and its performance compared to pre-
vious work (Section 4) as well as some further im-
provements. We then provide an analysis of these
results and discuss the limitations of the metric (Sec-
tion 5) and present conclusions and future work
(Section 6).
2 Related Work
A few metrics have been proposed in recent years
to address the problem of measuring whether a hy-
pothesis and a reference translation share the same
meaning. The most well-know metric is probably
METEOR (Banerjee and Lavie, 2005; Denkowski
and Lavie, 2010). METEOR is based on a general-
ized concept of unigram matching between the hy-
pothesis and the reference translation. Alignments
are based on exact, stem, synonym, and paraphrase
matches between words and phrases. However, the
structure of the sentences is not considered.
Wong and Kit (2010) measure word choice and
word order by the matching of words based on
surface forms, stems, senses and semantic similar-
ity. The informativeness of matched and unmatched
words is also weighted.
Liu et al (2010) propose to match bags of uni-
grams, bigrams and trigrams considering both recall
and precision and F-measure giving more impor-
tance to recall, but also using WordNet synonyms.
Tratz and Hovy (2008) use transformations in or-
der to match short syntactic units defined as Ba-
sic Elements (BE). The BE are minimal-length
syntactically well defined units. For example,
nouns, verbs, adjectives and adverbs can be con-
sidered BE-Unigrams, while a BE-Bigram could be
formed from a syntactic relation (e.g. subject+verb,
verb+object). BEs can be lexically different, but se-
mantically similar.
Pado? et al (2009) uses Textual Entailment fea-
tures extracted from the Standford Entailment Rec-
ognizer (MacCartney et al, 2006). The Textual En-
tailment Recognizer computes matching and mis-
matching features over dependency parses. The met-
ric then predicts the MT quality with a regression
model. The alignment is improved using ontologies.
He et al (2010) measure the similarity between
hypothesis and reference translation in terms of
the Lexical Functional Grammar (LFG) represen-
tation. The representation uses dependency graphs
to generate unordered sets of dependency triples.
Calculating precision, recall, and F-score on the
sets of triples corresponding to the hypothesis and
reference segments allows measuring similarity at
the lexical and syntactic levels. The measure also
matches WordNet synonyms.
The closest related metric to the one proposed in
this paper is that by Gime?nez and Ma?rquez (2007)
and Gime?nez et al (2010), which also uses shallow
semantic representations. Such a metric combines a
number of components, including lexical matching
metrics like BLEU and METEOR, as well as com-
ponents that compute the matching of constituent
and dependency parses, named entities, discourse
representations and semantic roles. However, the se-
mantic role matching is based on exact matching of
roles and role fillers. Moreover, it is not clear what
the contribution of this specific information is for the
overall performance of the metric.
We propose a metric that uses a lexical similar-
ity component and a semantic component in order
to deal with both word choice and semantic struc-
ture. The semantic component is based on seman-
tic roles, but instead of simply matching the surface
forms (i.e. arguments and predicates) it is able to
match similar words.
3 Metric Description
The rationale behind TINE is that an adequacy-
oriented metric should go beyond measuring the
matching of lexical items to incorporate information
about the semantic structure of the sentence, as in
(Gime?nez et al, 2010). However, the metric should
also be flexible to consider inexact matches of se-
mantic components, similar to what is done with lex-
ical metrics like METEOR (Denkowski and Lavie,
2010). We experiment with TINE having English
as target language because of the availability of lin-
guistic processing tools for this language. The met-
ric is particularly dependent on semantic role label-
117
ing systems, which have reached satisfactory perfor-
mance for English (Carreras and Ma?rquez, 2005).
TINE uses semantic role labels (SRL) and lexical se-
mantics to fulfill two requirements by: (i) compare
both the semantic structure and its content across
matching arguments in the hypothesis and refer-
ence translations; and (ii) propose alternative ways
of measuring inexact matches for both predicates
and role fillers. Additionally, it uses an exact lexi-
cal matching component to reward hypotheses that
present the same lexical choices as the reference
translation. The overall score s is defined using the
simple weighted average model in Equation (1):
s(H,R) = max
{
?L(H,R) + ?A(H,R)
?+ ?
}
R?R
(1)
where H represents the hypothesis translation, R
represents a reference translation contained in the set
of available references R; L defines the (exact) lex-
ical match component in Equation (2), A defines the
adequacy component in Equation (3); and ? and ?
are tunable weights for these two components. If
multiple references are provided, the score of the
segment is the maximum score achieved by compar-
ing the segment to each available reference.
L(H,R) =
|H
?
R|
?
|H| ? |R|
(2)
The lexical match component measures the over-
lap between the two representations in terms of the
cosine similarity metric. A segment, either a hypoth-
esis or a reference, is represented as a bag of tokens
extracted from an unstructured representation, that
is, bag of unigrams (words or stems). Cosine sim-
ilarity was chosen, as opposed to simply checking
the percentage of overlapping words (POW) because
cosine does not penalize differences in the length of
the hypothesis and reference translation as much as
POW. Cosine similarity normalizes the cardinality
of the intersection |H?R| using the geometric mean?
|H| ? |R| instead of the union |H?R|. This is par-
ticularly important for the matching of arguments -
which is also based on cosine similarity. If an hy-
pothesized argument has the same meaning as its
reference translation, but differs from it in length,
cosine will penalize less the matching than POW.
That is specially interesting when core arguments
get merged with modifiers due to bad semantic role
labeling (e.g. [A0 I] [T bought] [A1 something to eat
yesterday] instead of [A0 I] [T bought] [A1 some-
thing to eat] [AM-TMP yesterday]).
A(H,R) =
?
v?V verb score(Hv, Rv)
|Vr|
(3)
In the adequacy component, V is the set of verbs
aligned between H and R, and |Vr| is the number of
verbs in R. Hereafter the indexes h and r stand for
hypothesis and reference translations, respectively.
Verbs are aligned using VerbNet (Schuler, 2006) and
VerbOcean (Chklovski and Pantel, 2004). A verb in
the hypothesis vh is aligned to a verb in the refer-
ence vr if they are related according to the follow-
ing heuristics: (i) the pair of verbs share at least one
class in VerbNet; or (ii) the pair of verbs holds a re-
lation in VerbOcean.
For example, in VerbNet the verbs spook and ter-
rify share the same class amuse-31.1, and in VerbO-
cean the verb dress is related to the verb wear.
verb score(Hv, Rv) =
?
a?Ar?At
arg score(Ha, Ra)
|Ar|
(4)
The similarity between the arguments of a verb
pair (vh, vr) in V is measured as defined in Equa-
tion (4), where Ah and At are the sets of labeled
arguments of the hypothesis and the reference re-
spectively and |Ar| is the number of arguments of
the verb in R. In other words, we only measure the
similarity of arguments in a pair of sentences that are
annotated with the same role. This ensures that the
structure of the sentence is taken into account (for
example, an argument in the role of agent would not
be compared against an argument in a role of experi-
encer). Additionally, by restricting the comparison
to arguments of a given verb pair, we avoid argument
confusion in sentences with multiple verbs.
The arg score(Ha, Ra) computation is based on
the cosine similarity as in Equation (2). We treat
the tokens in the argument as a bag-of-words. How-
ever, in this case we change the representation of
the segments. If the two sets do not match exactly,
we expand both of them by adding similar words.
For every mismatch in a segment, we retrieve the
118
20-most similar words from Dekang Lin?s distribu-
tional thesaurus (Lin, 1998), resulting in sets with
richer lexical variety.
The following example shows how the computa-
tion of A(H,R) is performed, considering the fol-
lowing hypothesis and reference translations:
H: The lack of snow discourages people from ordering
ski stays in hotels and boarding houses.
R: The lack of snow is putting people off booking ski
holidays in hotels and guest houses.
1. extract verbs from H: Vh = {discourages, ordering}
2. extract verbs from R: Vr = {putting, booking}
3. similar verbs aligned with VerbNet (shared class
get-13.5.1): V = {(vh = order,vr = book)}
4. compare arguments of (vh = order,vr = book):
Ah = {A0, A1, AM-LOC}
Ar = {A0, A1, AM-LOC}
5. Ah ?Ar = {A0, A1, AM-LOC}
6. exact matches:
HA0 = {people} and RA0 = {people}
argument score = 1
7. different word forms: expand the representation:
HA1 = {ski, stays} and RA1 = {ski, holidays}
expand to:
HA1 = {{ski},{stays, remain... journey...}}
RA1 = {{ski},{holidays, vacations, trips... jour-
ney...}}
argument score = 0.5
8. similarly to HAM?LOC and RAM?LOC
argument score = 0.72
9. verb score (order, book) = 1+0.5+0.723 = 0.74
10. A(H,R) = 0.742 = 0.37
Different from previous work, we have not used
WordNet to measure lexical similarity for two main
reasons: problems with lexical ambiguity and lim-
ited coverage in WordNet (instances of named enti-
ties are not in WordNet, e.g. Barack Obama). For
example, in WordNet the aligned verbs (order/book)
from the previous hypothesis and reference trans-
lations have: 9 senses - order (e.g. give instruc-
tions to or direct somebody to do something with
authority, make a request for something, etc.) - and
4 senses - book (engage for a performance, arrange
for and reserve (something for someone else) in ad-
vance, etc.). Thus, a WordNet-based similarity mea-
sure would require disambiguating segments, an ad-
ditional step and a possible source of errors. Second,
a thresholds would need to be set to determine when
a pair of verbs is aligned. In contrast, the structure of
VerbNet (i.e. clusters of verbs) allows a binary deci-
sion, although the VerbNet heuristic results in some
errors, as we discuss in Section 5.
4 Results
We set the weights ? and ? by experimental test-
ing to ? = 1 and ? = 0.25. The lexical component
weight is prioritized because it has shown a good av-
erage Kendall?s tau correlation (0.23) on a develop-
ment dataset (Callison-Burch et al, 2010). Table 1
shows the correlation of the lexical component with
human judgments for a number of language pairs.
Table 1: Kendall?s tau segment-level correlation of the
lexical component with human judgments
Metric cz-en fr-en de-en es-en avg
Lexical 0.27 0.21 0.26 0.19 0.23
We use the SENNA1 SRL system to tag the
dataset with semantic roles. SENNA has shown to
have achieved an F-measure of 75.79% for tagging
semantic roles over the CoNLL 2005 2 benchmark.
We compare our metric against standard BLEU
(Papineni et al, 2002), METEOR (Denkowski and
Lavie, 2010) and other previous metrics reported in
(Callison-Burch et al, 2010) which also claim to use
some form of semantic information (see Section 2
for their description). The comparison is made in
terms of Kendall?s tau correlation against the human
judgments at a segment-level. For our submission to
the shared evaluation task, system-level scores are
obtained by averaging the segment-level scores.
TINE achieves the same average correlation with
BLUE, but outperforms it for some language pairs.
Additionally, TINE outperforms some of the previ-
ous which use WordNet to deal with synonyms as
part of the lexical matching.
The closest metric to TINE (Gime?nez et al,
2010), which also uses semantic roles as one of its
1http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
119
Table 2: Comparison with previous semantically-
oriented metrics using segment-level Kendall?s tau cor-
relation with human judgments
Metric cz-en fr-en de-en es-en avg
(Liu et al,
2010)
0.34 0.34 0.38 0.34 0.35
(Gime?nez
et al, 2010)
0.34 0.33 0.34 0.33 0.33
(Wong and
Kit, 2010)
0.33 0.27 0.37 0.32 0.32
METEOR 0.33 0.27 0.36 0.33 0.32
TINE 0.28 0.25 0.30 0.22 0.26
BLEU 0.26 0.22 0.27 0.28 0.26
(He et al,
2010)
0.15 0.14 0.17 0.21 0.17
(Tratz
and Hovy,
2008)
0.05 0.0 0.12 0.05 0.05
components, achieves better performance. However,
this metric is a rather complex combination of a
number of other metrics to deal with different lin-
guistic phenomena.
4.1 Further Improvements
As an additional experiment, we use BLEU as the
lexical component L(H,R) in order to test if the
shallow-semantic component can contribute to the
performance of this standard evaluation metric. Ta-
ble 3 shows the results of the combination of BLEU
and the shallow-semantic component using the same
parameter configuration as in Section 4. The addi-
tion of the shallow-semantic component increased
the average correlation of BLEU from 0.26 to 0.28.
Table 3: TINE-B: Combination of BLEU and the
shallow-semantic component
Metric cz-en fr-en de-en es-en avg
TINE-B 0.27 0.25 0.30 0.30 0.28
Finally, we improve the tuning of the weights of
the components (? and ? parameters) by using a
simple genetic algorithm (Back et al, 1999) to se-
lect the weights that maximize the correlation with
human scores on a development set (we use the de-
velopment sets from WMT10 (Callison-Burch et al,
2010)). The configuration of the genetic algorithm
is as follows:
? Fitness function: Kendall?s tau correlation
? Chromosome: two real numbers, ? and ?
? Number of individuals: 80
? Number of generations: 100
? Selection method: roulette
? Crossover probability: 0.9
? Mutation probability: 0.01
Table 4 shows the parameter values obtaining
from tuning for each language pair and the corre-
lation achieved by the metric with such parameters.
With such an optimization step the average correla-
tion of the metric increases to 0.29.
Table 4: Optimized values of the parameters using a ge-
netic algorithm and Kendall?s tau and final correlation of
the metric on the test sets
Language pair Correlation ? ?
cz-en 0.28 0.62 0.02
fr-en 0.25 0.91 0.03
de-en 0.30 0.72 0.1
es-en 0.31 0.57 0.02
avg 0.29 ? ?
5 Discussion
In what follows we discuss with a few examples
some of the common errors made by TINE. Over-
all, we consider the following categories of errors:
1. Lack of coverage of the ontologies.
R: This year, women were awarded the Nobel Prize in all
fields except physics
H: This year the women received the Nobel prizes in all
categories less physical
The lack of coverage in VerbNet prevented the
detection of the similarity between receive and
award.
2. Matching of unrelated verbs.
R: If snow falls on the slopes this week, Christmas will
sell out too, says Schiefert.
H: If the roads remain snowfall during the week, the dates
of Christmas will dry up, said Schiefert.
In VerbOcean remain and say are incorrectly
120
said to be related. VerbOcean was cre-
ated by a semi-automatic extraction algorithm
(Chklovski and Pantel, 2004) with an average
accuracy of 65.5%.
3. Incorrect tagging of the semantic roles by
SENNA.
R: Colder weather is forecast for Thursday, so if anything
falls, it should be snow.
H: On Thursday , must fall temperatures and, if there is
rain, in the mountains should.
The position of the predicates affects the SRL
tagging. The predicate fall has the following
roles (A1, V, and S-A1) in the reference, and
the following roles (AM-ADV, A0, AM-MOD,
and AM-DIS) in the hypothesis. As a con-
sequence, the metric cannot attempt to match
the fillers. Also, SRL systems do not detect
phrasal verbs such as in the example of Section
3, where the action putting people off is similar
to discourages.
6 Conclusions and Future Work
We have presented an MT evaluation metric based
on the alignment of semantic roles and flexible
matching of role fillers between hypothesis and ref-
erence translations. To deal with inexact matches,
the metric uses ontologies and distributional seman-
tics, as opposed to lexical databases like WordNet,
in order to minimize ambiguity and lack of cover-
age. The metric also uses an exact lexical matching
component to reward hypotheses that present lexical
choices similar to those of the reference translation.
Given the simplicity of the metric, it has achieved
competitive results. We have shown that the addition
of the shallow-semantic component into a lexical
component yields absolute improvements in the cor-
relation of 3%-6% on average, depending on the lex-
ical component used (cosine similarity or BLEU).
In future work, in order to improve the perfor-
mance of the metric we plan to add components to
address a few other linguistic phenomena such as
in (Gime?nez and Ma?rquez, 2007; Gime?nez et al,
2010). In order to deal with the coverage problem
of an ontology, we plan to use distributional seman-
tics (i.e. word space models) also to align the pred-
icates. We consider using a backoff model for the
shallow-semantic component to deal with the very
frequent cases where there are no comparable pred-
icates between the reference and hypothesis transla-
tions, which result in a 0 score from the semantic
component. Finally, we plan to improve the lexical
component to better tackle fluency, for example, by
adding information about the word order.
References
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, Michigan, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July.
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of the 9th Conference on Natural Lan-
guage Learning, CoNLL-2005, Ann Arbor, MI USA.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33?40, Barcelona,
Spain, July.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339?342, July.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 256?
264, Stroudsburg, PA, USA.
Jesu?s Gime?nez, Llu??s Ma?rquez, Elisabet Comelles, Irene
Castello?n, and Victoria Arranz. 2010. Document-
level automatic mt evaluation based on discourse rep-
resentations. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, WMT ?10, pages 333?338, Stroudsburg, PA,
USA.
121
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 349?353, Strouds-
burg, PA, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ?98, pages 768?
774, Stroudsburg, PA, USA.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, WMT ?10, pages 354?359,
Stroudsburg, PA, USA.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the NAACL, pages 41?48,
New York City, USA, June.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23:181?193, September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Stephen Tratz and Eduard Hovy. 2008. Summarisation
evaluation using transformed basic elements. In Pro-
ceedings TAC 2008.
Billy T.-M. Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT ?10, pages 360?
364, Stroudsburg, PA, USA.
122
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 316?322,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Shallow Semantic Trees for SMT
Wilker Aziz, Miguel Rios and Lucia Specia
Research Group in Computational Linguistics
University of Wolverhampton
Stafford Street, Wolverhampton, WV1 1SB, UK
{w.aziz, m.rios, l.specia}@wlv.ac.uk
Abstract
We present a translation model enriched with
shallow syntactic and semantic information
about the source language. Base-phrase la-
bels and semantic role labels are incorporated
into an hierarchical model by creating shal-
low semantic ?trees?. Results show an in-
crease in performance of up to 6% in BLEU
scores for English-Spanish translation over a
standard phrase-based SMT baseline.
1 Introduction
The use of semantic information to improve Statis-
tical Machine Translation (SMT) is a very recent re-
search topic that has been attracting significant at-
tention. In this paper we describe our participation
in the shared translation task of the 6th Workshop on
Statistical Machine Translation (WMT) with a sys-
tem that incorporates shallow syntactic and semantic
information into hierarchical SMT models.
The system is based on the Moses toolkit (Hoang
et al, 2009; Koehn et al, 2007) using hierarchi-
cal models informed with shallow syntactic (chunks)
and semantic (semantic role labels) information for
the source language. The toolkit SENNA (Collobert
et al, 2011) is used to provide base-phrases (chunks)
and semantic role labels.
Experiments with English-Spanish and English-
German news datasets show promising results and
highlight important issues about the use of seman-
tic information in hierarchical models as well as a
number of possible directions for further research.
The remaining of the paper is organized as fol-
lows: Section 2 presents related work; Section 3 de-
scribes the method; Section 4 presents the results ob-
tained for the English-Spanish and English-German
translation tasks; and Section 5 brings some conclu-
sions and directions for further research.
2 Related Work
In hierarchical SMT (Chiang, 2005), a Synchronous
Context Free Grammar (SCFG) is learned from a
parallel corpus.The model capitalizes on the recur-
sive nature of language replacing sub-phrases by
an unlabeled nonterminal. Hierarchical models are
known to produce high coverage rules, once they are
only constrained by the word alignment. Neverthe-
less the lack of specialized vocabulary also leads to
spurious ambiguity (Chiang, 2005).
Syntax-based models are hierarchical models
whose rules are constrained by syntactic informa-
tion.The syntactic constraints have an impact in
the rule extraction process, reducing drastically the
number of rules available to the system. While this
may be helpful to reduce ambiguity, it can lead to
poorer performance (Ambati and Lavie, 2008).
Motivated by the fact that syntactically constrain-
ing a hierarchical model can decrease translation
quality, some attempts to overcome the problems
at rule extraction time have been made. Venugopal
and Zollmann (2006) propose a heuristic method to
relax parse trees known as Syntax Augmented Ma-
chine Translation (SAMT). Significant gains are ob-
tained by grouping nonterminals under categories
when they do not span across syntactic constituents.
Hoang and Koehn (2010) propose a soft syntax-
based model which combines the precision of a
syntax-constrained model with the coverage of an
316
unconstrained hierarchical model. Instead of hav-
ing heuristic strategies to combine nonterminals in a
parse tree, whenever a rule cannot be retrieved be-
cause it does not span a constituent, the extraction
procedure falls back to the hierarchical approach, re-
trieving a rule with unlabeled nonterminals. Perfor-
mance gains are reported over standard hierarchical
models using both full parse trees and shallow syn-
tax.
Moving beyond syntactic information, some at-
tempts have recently been made to add semantic an-
notations to SMT. Wu and Fung (2009) present a
two-pass model to incorporate semantic information
to the phrase-based SMT pipeline. The method per-
forms conventional translation in a first step, fol-
lowed by a constituent reordering step seeking to
maximize the cross-lingual match of the semantic
role labels of the translation and source sentences.
Liu and Gildea (2010) add features extracted from
the source sentences annotated with semantic role
labels in a tree-to-string SMT model. They mod-
ify a syntax-based SMT system in order to penal-
ize/reward role reordering and role deletion. The
input sentence is parsed for semantic roles and the
roles are then projected onto the target side using
word alignment information at decoding time. They
assume that a one-to-one mapping between source
and target roles is desirable.
Baker et al (2010) propose to graft semantic in-
formation, namely named entities and modalities, to
syntactic tags in a syntax-based model. The vocab-
ulary of nonterminals is specialized using the se-
mantic categories, for instance, a noun phrase (NP)
whose head is a geopolitical entity (GPE) will be
tagged as NPGPE, making the rule table less am-
biguous.
Similar to (Baker et al, 2010) we specialize a vo-
cabulary of syntactic nonterminals with semantic in-
formation, however we use shallow syntax (base-
phrases) and semantic role labels instead of con-
stituent parse and named entities. The resulting shal-
low trees are relaxed following SAMT (Venugopal
and Zollmann, 2006). Different from previous work
we add the semantic knowledge at the level of the
corpus annotation. As a consequence, instead of bi-
asing deletion and reordering through additional fea-
tures (Liu and Gildea, 2010), we learn hierarchical
rules that encode those phenomena, taking also into
account the semantic role of base-phrases.
3 Proposed Method
The proposed method is based on an extension of the
hierarchical models in Moses using source language
information. Our submission included systems for
two language pairs: English-Spanish (en-es) and
English-German (en-de) and was constrained to us-
ing data provided by WMT11. Phrase and rule ex-
traction were performed using the entire en-es and
en-de portions of Europarl. Model parameters were
tuned using the news-test2008 dataset. Three 5-
gram Spanish and German language models were
trained using SRILM1 with the News Commentaries
(? 160K sentences), Europarl (? 2M sentences)
and News (? 5M sentences) corpora. These models
were interpolated using scripts provided in Moses
(Koehn and Schroeder, 2007).
At pre-processing stage, sentences longer than 80
tokens were filtered from the training/development
corpus. The parallel corpus was then tokenized and
truecased. Additionally, for en-de, compound split-
ting of the German side of the corpus was performed
using a frequency based method described in (Koehn
and Knight, 2003). This method helps alleviate spar-
sity, reducing the size of the vocabulary by decom-
posing compounds into their base words. Recas-
ing and detokenization, along with compound merg-
ing of the translations into German, were handled
at post-processing stage. Compound merging was
performed by finding the most likely sequences of
words to be merged into previously seen compounds
(Stymne, 2009).
3.1 Source Language Annotation
For rule extraction, training and test, the English side
of the corpus was annotated with Semantic Role La-
bels (SRL) using the toolkit SENNA2, which also
outputs POS and base-phrase (without prepositional
attachment) tags. The resulting source language an-
notation was used to produce trees in order to build
a tree-to-string model in Moses.
1http://www.speech.sri.com/projects/
srilm/
2http://ml.nec-labs.com/senna/
317
S
NP VP NP PP NP O O NP VP NP ADVP
PRP VBZ TO VB DT NN TO NN PUNC CC PRP VBZ RB VBD WDT RB
he intends to donate this money to charity , but he has not decided which yet
Figure 1: Example of POS tags and base-phrase annotation. Base-phrases: noun-phrase (NP), verb-phrase
(VP), prepositional-phrase (PP), adverbial-phrase (ADVP), outside-of-a-phrase (O)
In order to derive trees for the source side of the
corpus from this annotation, a new level is created to
add the POS tags for each word form. Syntactic tags
are then added by grouping words and POS tags into
base phrases using linguistic information as given
by SENNA. Figure 1 shows an example of an input
sentence annotated with POS and base-phrase infor-
mation. Additionally, SRLs are used to enrich the
POS and base-phrase annotation levels. Semantic
roles are assigned to each predicate independently.
As a consequence, the resulting annotation cannot
be considered a tree and there is not an obvious hi-
erarchy of predicates in a sentence. For example,
Figure 2 shows the SRL annotation for the example
in Figure 1.
[A0 He] [T intends] [A1 to donate this money to charity],
but he has not decided which yet
[A0 He] intends to [T donate] [A1 this money] [A2 to
charity], but he has not decided which yet
He intends to donate this money to charity, but [A0 he]
has [AM-NEG not] [T decided] [A1 which] [AM-TMP
yet]
Figure 2: SRL for sentence in Figure 1
Arguments of a single predicate never overlap,
however in longer sentences, the occurrence of mul-
tiple verbs increases the chances that arguments of
different predicates overlap, that is, the argument of
a verb might contain or even coincide with the argu-
ment of another verb and depending on the verb the
argument role might change. For example, in Fig-
ure 2: i) He is both the agent of intend and donate;
ii) this money is the donated thing and also part of
the chunk which express the intention (to donate this
money to charity). In a different example we can see
that arguments might overlap and their roles change
completely depending on their target predicates (e.g
in I gave you something to eat, you is the recipient
of the verb give and the agent of the verb eat). For
this reason, why semantic role labels are usually an-
notated individually in different structures, as shown
in Figure 2, each annotation focusing on a single tar-
get verb. In order to convert the predicates and argu-
ments of a sentence into a single tree, we enrich the
POS-tags and base-phrase annotation as follows:
? Semantic labels are directly grafted to the base-
phrase annotation whenever possible, that is,
if a predicate argument coincides with a sin-
gle base-phrase, the base-phrase type is spe-
cialized with the argument role. In Figure 3,
the noun-phrase (NP) the money is specialized
into NP:A1:donate, since that single NP is the
argument A1 of donate.
? If a predicate argument groups multiple base-
phrases, the semantic label applies to a node in
a new level of the tree subsuming all these base-
phrases. In Figure 3, the base-phrases to (PP)
and charity (NP) are grouped by A2:donate.
? We add the labels sequentially from the short-
est chunks to the largest ones. If two la-
bels spanning the same number of tokens: i)
overlap completely, we merge them so that
no hierarchy is imposed between their targets
(e.g. in Figure 3, the noun-phrase He is spe-
cialized into NP:A0:donate,intend); ii) over-
lap partially, we merge them so that the re-
sulting label will compete against other labels
in a different length category. If a label span-
ning a larger chunk overlaps partially with a
label spanning a shorter chunk, or contains it,
we stack them in a way that the first subsumes
the second (e.g in Figure 3, A1:intend sub-
sumes VP:T:donate, NP:A1:donate,intend and
A2:donate).
? Verb phrases might get split if they contain
multiple target predicates (e.g. in Figure 3,
the VP intends to donate is split into two verb-
318
phrases, each specialized with its own role la-
bel).
? Finally, tags are lexicalized, that is, semantic
labels are composed by their type (e.g. A0) and
target predicate lemma (verb).
Figure 3 shows and example of how semantic la-
bels are combined with shallow syntax in order to
produce the input tree for the sentence in Figure
1. The argument A1 of intend subsumes the target
verb donate and its arguments A1 and A2; A2:donate
groups base-phrases so as to attach the preposition to
the noun phrase.
Finally, following the method for syntactic trees
by Venugopal and Zollmann (2006), the input trees
are relaxed in order to alleviate the impact of the
linguistic constraints on rule extraction. We relax
trees3 by combining any pairs of neighboring nodes.
For example, NP:A0:donate,intend+VP:T:intend
and NP:A1:donate+A2:donate are created for the
tree in Figure 3.
4 Results
As a baseline to compare against our proposed ap-
proach (srl), we took a phrase-based SMT system
(pb) built using the Moses toolkit with the same
datasets and training conditions described in Sec-
tion 3. The results are reported in terms of standard
BLEU (Papineni et al, 2002) (and its case sensitive
version, BLEU-c) and tested for statistical signifi-
cance using an approximate randomization test (Rie-
zler and Maxwell, 2005) with 100 iterations.
In addition, we included an intermediate model
between these two: a hierarchical model in-
formed with source-language base-phrase informa-
tion (chunk). For the English-Spanish task we also
built a purely hierarchical model (hier) using Moses
and the same datasets and training conditions. For
the English-German task, hierarchical models have
not been shown to outperform standard phrase-based
models in previous work (Koehn et al, 2010).
Table 1 shows the performance achieved for the
English-Spanish translation task test set, where (srl)
is our official submission. One can notice a signifi-
cant gain in performance (up to 6% BLEU) in using
tree-based models (with or without source language
3Using the Moses implementation relax-parse for SAMT 2
annotation) as opposed to using standard phrase-
based models.
Model BLEU BLEU-c
pb 0.2429 0.2340
srl 0.2901 0.2805
hier 0.3029 0.2933
chunk 0.3034 0.2935
Table 1: English-Spanish experiments - differences
between all pairs of models are statistically signifi-
cant with 99% confidence, except for the pair (hier,
chunk)
The purely hierarchical approach performs as
well as our linguistically informed tree-based mod-
els (chunk and srl). On the one hand this finding
is somewhat disappointing as we expected that tree-
based models would benefit from linguistic annota-
tion. On the other hand it shows that the linguistic
annotation yields a significant reduction in the num-
ber of unnecessary productions: the linguistically in-
formed models are much smaller than hier (Table
5), but perform just as well. Whether the linguistic
annotation significantly helps make the productions
less ambiguous or not is still a question to be ad-
dressed in further experimentation.
Table 2 shows the performance achieved for the
English-German translation task test set. These re-
sults indicate that the linguistic information did not
lead to any significant gains in terms of automatic
metrics. An in-depth comparative analysis based on
a manual inspection of the translations remains to be
done.
Model BLEU BLEU-c
pb 0.1398 0.1360
srl 0.1381 0.1344
chunk 0.1403 0.1367
Table 2: English-German experiments - differences
between pairs of models are not statistically signifi-
cant
In Table 3 we also show the impact of three com-
pound merging strategies as post-processing for en-
de: i) no compound merging (nm), ii) frequency-
based compound merging (fb), and iii) frequency-
319
SNP:A0:donate,intend
PRP
He
VP:T:intend
VBZ
intends
A1:intend
VP:T:donate
TO
to
VB
donate
NP:A1:donate
DT
this
NN
money
A2:donate
PP
TO
to
NP
NN
charity
...
Figure 3: Tree for example in Figure 1
based compound merging constrained by POS4
(cfb). Applying both frequency-based compound
merging strategies (Stymne, 2009) resulted in sig-
nificant improvements of nearly 0.5% in BLEU.
Model BLEU BLEU-c
nm 0.1334 0.1298
fb 0.1369 0.1332
cfb 0.1381 0.1344
Table 3: English-German compound merging - dif-
ferences between all pairs of models are statistically
significant with 99% confidence
Another somewhat disappoint result is the perfor-
mance of srl when compared to chunk. We believe
the main reason why the chunk models outperform
the srl models is data sparsity. The semantic infor-
mation, and particularly the way it was used in this
paper, with lexicalized roles, led to a very sparse
model. As an attempt to make the srl model less
sparse, we tested a version of this model without
lexicalizing the semantic tags, in other words, us-
ing the semantic role labels only, for example, A1
instead of A1:intend in Figure 3. Table 4 shows that
models with lexicalized semantic roles (lex) consis-
tently outperform the alternative version (non lex),
although the differences were only statistically sig-
nificant for the en-de dataset. One reason for that
may be that non-lexicalized rules do not help mak-
4POS tagging was performed using the TreeTagger toolkit:
http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
ing the chunk rules less ambiguous.
Model BLEU BLEU-c
en-esnon lex 0.2891 0.2795
en-eslex 0.2901 0.2805
en-denon lex 0.1319 0.1284
en-delex 0.1381 0.1344
Table 4: Alternative model with non-lexicalized tags
- differences are statistically significant with 99%
confidence for en-de only
Table 5 shows how the additional annotation con-
strains the rule extraction (for the en-es dataset). The
unconstrained model hier presents the largest rule
table, followed by the chunk model, which is only
constrained by syntactic information. The models
enriched with semantic labels, both the lexicalized
or non-lexicalized versions, contain a comparable
number of rules. They are at least half the size of
the chunk model and about 9 times smaller than the
hier model. However, the number of nonterminals
in the lexicalized models highlights the sparsity of
such models.
Model Rules Nonterminals
hier 962,996,167 1
chunk 235,910,731 3,390
srlnon lex 92,512,493 44,095
srllex 117,563,878 3,350,145
Table 5: Statistics from the rule table
In order to exemplify the importance of having
320
some form of lexicalized information as part of the
semantic models, Figure 4 shows two predicates
which present different semantic roles, even though
they have nearly the same shallow syntactic struc-
ture. In this case, unless lexicalized, rules map-
ping semantic roles into base-phrases become am-
biguous. Besides, the same role might appear sev-
eral times in the same sentence (Figure 2). In this
case, if the semantic roles are not annotated with
their target lemma, they bring additional confusion.
Therefore, the model needs the lexical information
to distinguish role deletion and reordering phenom-
ena across predicates.
Figure 4: Different SRL for similar chunks
[NP:A0 I] [VP:T gave] [NP:A2 you] [NP:A1 a car]
[NP:A0 I] [VP:T dropped] [NP:A1 the glass] [AM-LOC
[PP on] [NP the floor]]
In WMT11?s official manual evaluation, our sys-
tem submissions (srl) were ranked 10th out of 15
systems in the English-Spanish task, and 18th out
of 22 systems participating in the English-German
task. For detailed results refer to the overview paper
of the Shared Translation Task of the Sixth Work-
shop on Machine Translation (WMT11).
5 Conclusions
We have presented an effort towards using shal-
low syntactic and semantic information for SMT.
The model based on shallow syntactic information
(chunk annotation) has significantly outperformed a
baseline phrase-based model and performed as well
as a hierarchical phrase-based model with a signifi-
cantly smaller number of translation rules.
While annotating base-phrases with semantic la-
bels is intuitively a promising research direction, the
current model suffers from sparsity and representa-
tion issues resulting from the fact that multiple pred-
icates share arguments within a given sentence. As
a consequence, shallow semantics has not yet shown
improvements with respect to the chunk-based mod-
els.
In future work, we will address the sparsity is-
sues in the lexicalized semantic models by cluster-
ing predicates in a way that semantic roles can be
specialized with semantic categories, instead of the
verb lemmas.
References
Vamshi Ambati and Alon Lavie. 2008. Improving syntax
driven translation models by re-structuring divergent
and non-isomorphic parse tree structures. In The Eight
Conference of the Association for Machine Translation
in the Americas (AMTA).
Kathryn Baker, Michael Bloodgood, Chris Callison-
burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-informed syntactic machine translation:
A tree-grafting approach.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceeding of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
arXiv:1103.0398v1.
Hieu Hoang and Philipp Koehn. 2010. Improved trans-
lation with source syntax labels. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 409?417.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A
unified framework for phrase-based, hierarchical, and
syntax-based statistical machine translation. In Pro-
ceedings of International Workshop on Spoken Lan-
guage Translation, pages 152 ? 159.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
tenth conference on European chapter of the Associ-
ation for Computational Linguistics - Volume 1, pages
187?193.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 224?227.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In 45th An-
nual Meeting of the Association for Computational
Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu
Hoang. 2010. More linguistic annotation for statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 115?120.
321
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 716?724.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for mt. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, Work-
shop in Intrinsic and Extrinsic Evaluation Measures
for MT and Summarization.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of german compounds. In Proceed-
ings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics: Stu-
dent Research Workshop, pages 61?69.
Ashish Venugopal and Andreas Zollmann. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: a hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 13?16.
322
