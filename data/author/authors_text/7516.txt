Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
Dialogue Act Modeling for 
Automatic Tagging and Recognition 
of Conversational Speech 
Andreas Stolcke* 
SRI International 
Noah Coccaro 
University of Colorado at Boulder 
Rebecca Bates 
University of Washington 
Paul Taylor 
University of Edinburgh 
Carol Van Ess-Dykema 
U.S. Department of Defense 
Klaus Ries 
Carnegie Mellon University and 
University of Karlsruhe 
Elizabeth Shriberg 
SRI International 
Daniel Jurafsky 
University of Colorado at Boulder 
Rachel Martin 
Johns Hopkins University 
Marie Meteer 
BBN Technologies 
We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- 
act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREE- 
MENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, colloca- 
tional, and prosodic ues, as well as on the discourse coherence of the dialogue act sequence. 
The dialogue model is based on treating the discourse structure of a conversation as a hidden 
Markov model and the individual dialogue acts as observations emanating from the model states. 
Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The 
statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks 
modeling the idiosyncratic lexical and prosodic manifestations ofeach dialogue act. We develop 
a probabilistic ntegration of speech recognition with dialogue modeling, to improve both speech 
recognition and dialogue act classification accuracy. Models are trained and evaluated using a 
large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous 
human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based 
on errorful, automatically recognized words and prosody, and 71% based on word transcripts, 
compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction 
in word recognition error. 
? Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 
94025, 1-650-859-2544. E-mail: stolcke@speech.sri.com. 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 3 
Table 1 
Fragment of a labeled conversation (from the Switchboard corpus). 
Speaker Dialogue Act Utterance 
A YEs-No-QuESTION So do you go to college right now? 
A ABANDONED Are yo-, 
B YES- ANSWER Yeah, 
B STATEMENT it's my last year \[laughter\]. 
A DECLARATIVE-QUESTION You're a, so you're a senior now. 
B YEs-ANSWER Yeah, 
B STATEMENT I'm working on my projects trying to graduate 
\[laughter\]. 
A APPRECIATION Oh, good for you. 
B BACKCHANNEL Yeah. 
A APPRECIATION That's great, 
A YEs-No-QUESTION um, is, is N C University is that, uh, State, 
B STATEMENT N C State. 
A SIGNAL-NoN-UNDERSTANDING What did you say? 
B STATEMENT N C State. 
1. Introduction 
The ability to model and automatically detect discourse structure is an important 
step toward understanding spontaneous dialogue. While there is hardly consensus 
on exactly how discourse structure should be described, some agreement exists that 
a useful first level of analysis involves the identification of dialogue acts (DAs). A 
DA represents he meaning of an utterance at the level of illocutionary force (Austin 
1962). Thus, a DA is approximately the equivalent of the speech act of Searle (1969), 
the conversational game move of Power (1979), or the adjacency pair part of Schegloff 
(1968) and Saks, Schegloff, and Jefferson (1974). 
Table 1 shows a sample of the kind of discourse structure in which we are inter- 
ested. Each utterance is assigned a unique DA label (shown in column 2), drawn from 
a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that 
classifies utterances according to a combination of pragmatic, semantic, and syntactic 
criteria. The computational community has usually defined these DA categories so as 
to be relevant to a particular application, although efforts are under way to develop 
DA labeling systems that are domain-independent, such as the Discourse Resource 
Initiative's DAMSL architecture (Core and Allen 1997). 
While not constituting dialogue understanding in any deep sense, DA tagging 
seems clearly useful to a range of applications. For example, a meeting summarizer 
needs to keep track of who said what to whom, and a conversational agent needs to 
know whether it was asked a question or ordered to do something. In related work 
DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin 
and Moore 1977; Levin et al 1999), a slightly higher level unit that comprises a small 
number of DAs. Interactional dominance (Linell 1990) might be measured more ac- 
curately using DA distributions than with simpler techniques, and could serve as an 
indicator of the type or genre of discourse at hand. In all these cases, DA labels would 
enrich the available input for higher-level processing of the spoken words. Another im- 
portant role of DA information could be feedback to lower-level processing. For exam- 
ple, a speech recognizer could be constrained by expectations of likely DAs in a given 
context, constraining the potential recognition hypotheses so as to improve accuracy. 
340 
Stolcke et al Dialogue Act Modeling 
Table 2 
The 42 dialogue act labels. DA frequencies are given as percentages of the total 
number of utterances in the overall corpus. 
Tag Example % 
STATEMENT 
BACKCHANNEL/ACKNOWLEDGE 
OPINION 
ABANDONED/UNINTERPRETABLE 
AGREEMENT/ACCEPT 
APPRECIATION 
YEs-No-QUESTION 
NON-VERBAL 
YES ANSWERS 
CONVENTIONAL-CLOSING 
WH-QUESTION 
NO ANSWERS 
RESPONSE ACKNOWLEDGMENT 
HEDGE 
DECLARATIVE YES-No-QuESTION 
OTHER 
BACKCHANNEL-QUESTION 
QUOTATION 
SUMMARIZE/REFORMULATE 
AFFIRMATIVE NON-YES ANSWERS 
ACTION-DIRECTIVE 
COLLABORATIVE COMPLETION 
REPEAT-PHRASE 
OPEN-QUESTION 
RHETORICAL-QUESTIONS 
HOLD BEFORE ANSWER/AGREEMENT 
REJECT 
NEGATIVE NON-NO ANSWERS 
SIGNAL-NON-UNDERSTANDING 
OTHER ANSWERS 
CONVENTIONAL-OPENING 
OR-CLAUSE 
DISPREFERRED ANSWERS 
3RD-PARTY-TALK 
OFFERS, OPTIONS ~ COMMITS 
SELF-TALK 
D OWNPLAYER 
MAYBE/AcCEPT-PART 
TAG-QUESTION 
DECLARATIVE WH-QUESTION 
APOLOGY 
THANKING 
Me, I'm in the legal department. 36% 
Uh-huh. 19% 
I think it's great 13% 
So, -/ 6% 
That's exactly it. 5% 
I can imagine. 2% 
Do you have to have any special training? 2% 
<Laughter>, < Throat_clearing> 2% 
Yes. 1% 
Well, it's been nice talking to you. 1% 
What did you wear to work today? 1% 
No. 1% 
Oh, okay. 1% 
I don't know if I'm making any sense or not. 1% 
So you can afford to get a house? 1% 
Well give me a break, you know. 1% 
Is that right? 1% 
You can't be pregnant and have cats .5% 
Oh, you mean you switched schools for the kids. .5% 
It is. .4% 
Why don't you go first .4% 
Who aren't contributing. .4% 
Oh, fajitas .3% 
How about you ? .3% 
Who would steal a newspaper? .2% 
I'm drawing a blank. .3% 
Well, no .2% 
Uh, not a whole lot. .1% 
Excuse me? .1% 
I don't know .1% 
How are you? .1% 
or is it more of a company? .1% 
Well, not so much that. .1% 
My goodness, Diane, get down from there. .1% 
I'I1 have to check that out .1% 
What's the word I'm looking for .1% 
That's all right. .1% 
Something like that <.1% 
Right? <.1% 
You are what kind of buff? <.1% 
I'm sorry. <.1% 
Hey thanks a lot <.1% 
The goal of this article is twofold: On the one hand, we aim to present a com- 
prehensive f ramework for model ing and automatic lassification of DAs, founded on 
wel l -known statistical methods. In doing so, we will pull together previous approaches 
as well as new ideas. For example, our model draws on the use of DA n-grams and the 
hidden Markov models of conversation present in earlier work, such as Nagata and 
Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However,  
our f ramework generalizes earlier models, giving us a clean probabilistic approach for 
performing DA classification from unreliable words and nonlexical evidence. For the 
341 
Computational Linguistics Volume 26, Number 3 
speech recognition task, our framework provides a mathematically principled way to 
condition the speech recognizer on conversation context through dialogue structure, as 
well as on nonlexical information correlated with DA identity. We will present meth- 
ods in a domain-independent framework that for the most part treats DA labels as an 
arbitrary formal tag set. Throughout the presentation, we will highlight he simplifi- 
cations and assumptions made to achieve tractable models, and point out how they 
might fall short of reality. 
Second, we present results obtained with this approach on a large, widely available 
corpus of spontaneous conversational speech. These results, besides validating the 
methods described, are of interest for several reasons. For example, unlike in most 
previous work on DA labeling, the corpus is not task-oriented in nature, and the 
amount of data used (198,000 utterances) exceeds that in previous tudies by at least 
an order of magnitude (see Table 14). 
To keep the presentation i teresting and concrete, we will alternate between the 
description of general methods and empirical results. Section 2 describes the task 
and our data in detail. Section 3 presents the probabilistic modeling framework; a
central component of this framework, the discourse grammar, is further discussed in 
Section 4. In Section 5 we describe xperiments for DA classification. Section 6 shows 
how DA models can be used to benefit speech recognition. Prior and related work is 
summarized in Section 7. Further issues and open problems are addressed in Section 8, 
followed by concluding remarks in Section 9. 
2. The Dialogue Act Labeling Task 
The domain we chose to model is the Switchboard corpus of human-human con- 
versational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by 
the Linguistic Data Consortium. Each conversation i volved two randomly selected 
strangers who had been charged with talking informally about one of several, self- 
selected general-interest topics. To train our statistical models on this corpus, we com- 
bined an extensive ffort in human hand-coding of DAs for each utterance, with a 
variety of automatic and semiautomatic tools. Our data consisted of a substantial 
portion of the Switchboard waveforms and corresponding transcripts, totaling 1,155 
conversations. 
2.1 Utterance Segmentation 
Before hand-labeling each utterance in the corpus with a DA, we needed to choose an 
utterance segmentation, as the raw Switchboard ata is not segmented in a linguis- 
tically consistent way. To expedite the DA labeling task and remain consistent with 
other Switchboard-based research efforts, we made use of a version of the corpus that 
had been hand-segmented into sentence-level units prior to our own work and in- 
dependently of our DA labeling system (Meteer et al 1995). We refer to the units of 
this segmentation as utterances. The relation between utterances and speaker turns 
is not one-to-one: a single turn can contain multiple utterances, and utterances can 
span more than one turn (e.g., in the case of backchanneling by the other speaker in 
midutterance). Each utterance unit was identified with one DA, and was annotated 
with a single DA label. The DA labeling system had special provisions for rare cases 
where utterances seemed to combine aspects of several DA types. 
Automatic segmentation f spontaneous speech is an open research problem in its 
own right (Mast et al 1996; Stolcke and Shriberg 1996). A rough idea of the difficulty 
of the segmentation problem on this corpus and using the same definition of utterance 
units can be derived from a recent study (Shriberg et al 2000). In an automatic labeling 
342 
Stolcke t al. Dialogue Act Modeling 
of word boundaries as either utterance or nonboundaries u ing a combination oflexical 
and prosodic ues, we obtained 96% accuracy based on correct word transcripts, and 
78% accuracy with automatically recognized words. The fact that the segmentation 
and labeling tasks are interdependent (Warnke et al 1997; Finke et al 1998) further 
complicates the problem. 
Based on these considerations, we decided not to confound the DA classification 
task with the additional problems introduced by automatic segmentation a d assumed 
the utterance-level s gmentations as given. An important consequence of this decision 
is that we can expect utterance l ngth and acoustic properties at utterance boundaries 
to be accurate, both of which turn out to be important features of DAs (Shriberg et al 
1998; see also Section 5.2.1). 
2.2 Tag Set 
We chose to follow a recent standard for shallow discourse structure annotation, the 
Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the 
natural language processing community under the auspices of the Discourse Resource 
Initiative (Core and Allen 1997). We began with the DAMSL markup system, but modi- 
fied it in several ways to make it more relevant to our corpus and task. DAMSL aims to 
provide a domain-independent framework for dialogue annotation, as reflected by the 
fact that our tag set can be mapped back to DAMSL categories (Jurafsky, Shriberg, and 
Biasca 1997). However, our labeling effort also showed that content- and task-related 
distinctions will always play an important role in effective DA labeling. 
The Switchboard omain itself is essentially "task-free," thus giving few external 
constraints on the definition of DA categories. Our primary purpose in adapting the 
tag set was to enable computational DA modeling for conversational speech, with 
possible improvements to conversational speech recognition. Because of the lack of a 
specific task, we decided to label categories that seemed inherently interesting linguis- 
tically and that could be identified reliably. Also, the focus on conversational speech 
recognition led to a certain bias toward categories that were lexically or syntactically 
distinct (recognition accuracy is traditionally measured including all lexical elements 
in an utterance). 
While the modeling techniques described in this paper are formally independent of 
the corpus and the choice of tag set, their success on any particular task will of course 
crucially depend on these factors. For different asks, not all the techniques used in 
this study might prove useful and others could be of greater importance. However, 
we believe that this study represents a fairly comprehensive application of technology 
in this area and can serve as a point of departure and reference for other work. 
The resulting SWBD-DAMSL tag set was multidimensional; pproximately 50ba- 
sic tags (e.g., QUESTION, STATEMENT) could each be combined with diacritics indicat- 
ing orthogonal information, for example, about whether or not the dialogue function 
of the utterance was related to Task-Management and Communication-Management. 
Approximately 220 of the many possible unique combinations ofthese codes were used 
by the coders (Jurafsky, Shriberg, and Biasca 1997). To obtain a system with somewhat 
higher interlabeler agreement, as well as enough data per class for statistical mod- 
eling purposes, a less fine-grained tag set was devised. This tag set distinguishes 42 
mutually exclusive utterance types and was used for the experiments reported here. 
Table 2 shows the 42 categories with examples and relative frequencies. 1 While some 
1 For the study focusing on prosodic modeling ofDAs reported elsewhere (Shriberg et al 1998), the tag 
set was further reduced to six categories. 
343 
Computational Linguistics Volume 26, Number 3 
of the original infrequent classes were collapsed, the resulting DA type distribution 
is still highly skewed. This occurs largely because there was no basis for subdividing 
the dominant DA categories according to task-independent a d reliable criteria. 
The tag set incorporates both traditional sociolinguistic and discourse-theoretic 
notions, such as rhetorical relations and adjacency pairs, as well as some more form- 
based labels. Furthermore, the tag set is structured so as to allow labelers to annotate 
a Switchboard conversation from transcripts alone (i.e., without listening) in about 
30 minutes. Without hese constraints he DA labels might have included some finer 
distinctions, but we felt that this drawback was balanced by the ability to cover a large 
amount of data. 2
Labeling was carried out in a three-month period in 1997 by eight linguistics 
graduate students at CU Boulder. Interlabeler agreement for the 42-1abel tag set used 
here was 84%, resulting in a Kappa statistic of 0.80. The Kappa statistic measures 
agreement ormalized for chance (Siegel and Castellan, Jr. 1988). As argued in Carletta 
(1996), Kappa values of 0.8 or higher are desirable for detecting associations between 
several coded variables; we were thus satisfied with the level of agreement achieved. 
(Note that, even though only a single variable, DA type, was coded for the present 
study, our goal is, among other things, to model associations between several instances 
of that variable, e.g., between adjacent DAs.) 
A total of 1,155 Switchboard conversations were labeled, comprising 205,000 ut- 
terances and 1.4 million words. The data was partitioned into a training set of 1,115 
conversations (1.4M words, 198K utterances), used for estimating the various compo- 
nents of our model, and a test set of 19 conversations (29K words, 4K utterances). 
Remaining conversations were set aside for future use (e.g., as a test set uncompro- 
mised of tuning effects). 
2.3 Major Dialogue Act Types 
The more frequent DA types are briefly characterized below. As discussed above, the 
focus of this paper is not on the nature of DAs, but on the computational framework 
for their recognition; full details of the DA tag set and numerous motivating examples 
can be found in a separate report (Jurafsky, Shriberg, and Biasca 1997). 
Statements and Opinions. The most common types of utterances were STATEMENTS 
and OPINIONS. This split distinguishes "descriptive, narrative, or personal" statements 
(STATEMENT) from "other-directed opinion statements" (OPINION). The distinction was 
designed to capture the different kinds of responses we saw to opinions (which are 
often countered or disagreed with via further opinions) and to statements (which more 
often elicit continuers or backchannels): 
Dialogue Act 
STATEMENT 
STATEMENT 
STATEMENT 
OPINION 
OPINION 
Example Utterance 
Well, we have a cat, um, 
He's probably, oh, a good two years old, 
big, old, fat and sassy tabby. 
He's about five months old 
Well, rabbits are darling. 
I think it would be kind of stressful. 
2 The effect of lacking acoustic information on labeling accuracy was assessed by relabeling a subset of 
the data with listening, and was found to be fairly small (Shriberg et al 1998). A conservative estimate 
based on the relabeling study is that, for most DA types, at most 2% of the labels might have changed 
based on listening. The only DA types with higher uncertainty were BACKCHANNELS and 
AGREEMENTS, which are easily confused with each other without acoustic ues; here the rate of change 
was no more than 10%. 
344 
Stolcke t al. Dialogue Act Modeling 
OPINIONS often include such hedges as I think, I believe, it seems, and I mean. We 
combined the STATEMENT and OPINION classes for other studies on dimensions in 
which they did not differ (Shriberg et al 1998). 
Questions. Questions were of several types. The YES-No-QUESTION label includes only 
utterances having both the pragmatic force of a yes-no-question and the syntactic mark- 
ings of a yes-no-question (i.e., subject-inversion r sentence-final t gs). DECLARATIVE- 
QUESTIONS are utterances that function pragmatically as questions but do not have 
"question form." By this we mean that declarative questions normally have no wh- 
word as the argument of the verb (except in "echo-question" format), and have "declar- 
ative" word order in which the subject precedes the verb. See Weber (1993) for a survey 
of declarative questions and their various realizations. 
Dialogue Act Example Utterance 
YEs-No-QUESTION 
YEs-No-QUESTION 
YEs-No-QuESTION 
DECLARATIVE- QUESTION 
WH-QUESTION 
Do you have to have any special training? 
But that doesn't eliminate it, does it? 
Uh, I guess a year ago you're probably 
watching C N N a lot, right? 
So you're taking a government course? 
Well, how old are you? 
Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, 
e.g., indicating that the speaker should go on talking. These are usually referred to in 
the conversation analysis literature as "continuers" and have been studied extensively 
(Jefferson 1984; Schegloff 1982; Yngve 1970). We expect recognition of backchannels to
be useful because of their discourse-structuring role (knowing that the hearer expects 
the speaker to go on talking tells us something about the course of the narrative) 
and because they seem to occur at certain kinds of syntactic boundaries; detecting a 
backchannel may thus help in predicting utterance boundaries and surrounding lexical 
material. 
For an intuition about what backchannels look like, Table 3 shows the most com- 
mon realizations of the approximately 300 types (35,827 tokens) of backchannel in 
our Switchboard subset. The following table shows examples of backchannels in the 
context of a Switchboard conversation: 
Speaker Dialogue Act Utterance 
B STATEMENT 
A BACKCHANNEL 
B STATEMENT 
B STATEMENT 
A BACKCHANNEL 
B STATEMENT 
B STATEMENT 
A APPRECIATION 
but, uh, we're to the point now where our 
financial income is enough that we can consider 
putting some away - 
Uh-huh. / 
- for college, / 
so we are going to be starting a regular payroll 
deduction - 
Urn. / 
- -  in the fall / 
and then the money that I will be making this 
summer we'll be putting away for the college 
fund. 
Urn. Sounds good. 
Turn Exits and Abandoned Utterances. Abandoned utterances are those that the speaker 
breaks off without finishing, and are followed by a restart. Turn exits resemble aban- 
doned utterances in that they are often syntactically broken off, but they are used 
345 
Computational Linguistics Volume 26, Number 3 
Table 3 
Most common realizations of backchannels in Switchboard. 
Frequency Form Frequency Form Frequency Form 
38% uh-huh 2% yes 1% sure 
34% yeah 2% okay 1.% um 
9% right 2% oh yeah 1% huh-uh 
3% oh 1% huh 1% uh 
mainly as a way of passing speakership to the other speaker. Turn exits tend to be 
single words, often so or or. 
Speaker Dialogue Act Utterance 
A STATEMENT we're from, uh, I 'm from Ohio / 
A STATEMENT and my wife's from Florida / 
A TURN-ExIT SO, -/  
B BACKCHANNEL Uh-huh./ 
A HEDGE 
A ABANDONED 
A STATEMENT 
so, I don't know, / 
it's Klipsmack>, - / 
I 'm glad it's not the kind of problem I have to 
come up with an answer to because it's not - 
Answers and Agreements. YES-ANSWERS include yes, yeah, yep, uh-huh, and other varia- 
tions on yes, when they are acting as an answer to a YES-NO-QUESTION or DECLARA- 
TWE-0UESTION. Similarly, we also coded NO-ANSWERS. Detecting ANSWERS can help 
tell us that the previous utterance was a YES-NO-QUESTION. Answers are also seman- 
tically significant since they are likely to contain new information. 
AGREEMENT/ACCEPT, REJECT, and MAYBE/ACCEPT-PART all mark the degree 
to which a speaker accepts ome previous proposal, plan, opinion, or statement. The 
most common of these are the AGREEMENT/AccEPTS. These are very often yes or yeah, 
so they look a lot like ANSWERS. But where ANSWERS follow questions, AGREEMENTS 
often follow opinions or proposals, so distinguishing these can be important for the 
discourse. 
3. Hidden Markov Modeling of Dialogue 
We will now describe the mathematical  nd computational  f ramework used in our 
study. Our goal is to perform DA classification and other tasks using a probabilis- 
tic formulation, giving us a principled approach for combining multiple knowledge 
sources (using the laws of probability), as well as the ability to derive model  parame- 
ters automatical ly from a corpus, using statistical inference techniques. 
Given all available evidence E about a conversation, the goal is to find the DA 
sequence U that has the highest posterior probabil ity P(UIE ) given that evidence. 
Apply ing Bayes' rule we get 
U* = argmaxP(UIE ) 
U 
P(U)P(ElU) = argmax u P(E) 
= argmaxP(U)P(ElU) (1) 
U 
Here P(U) represents the prior probabil ity of a DA sequence, and P(EIU ) is the like- 
346 
Stolcke t al. Dialogue Act Modeling 
Table 4 
Summary of random variables used in dialogue modeling. 
(Speaker labels are introduced in Section 4.) 
Symbol Meaning 
U 
E 
F 
A 
W 
T 
sequence of DA labels 
evidence (complete speech signal) 
prosodic evidence 
acoustic evidence (spectral features used in ASR) 
sequence of words 
speakers labels 
lihood of U given the evidence. The likelihood is usually much more straightforward 
to model than the posterior itself. This has to do with the fact that our models are 
generative or causal in nature, i.e., they describe how the evidence is produced by the 
underlying DA sequence U. 
Estimating P (U) requires building a probabilistic discourse grammar, i.e., a statisti- 
cal model of DA sequences. This can be done using familiar techniques from language 
modeling for speech recognition, although the sequenced objects in this case are DA 
labels rather than words; discourse grammars will be discussed in detail in Section 4. 
3.1 Dialogue Act Likelihoods 
The computation of likelihoods P(EIU ) depends on the types of evidence used. In our 
experiments we used the following sources of evidence, ither alone or in combination: 
Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W 
refers to the true (hand-transcribed) words spoken in a conversation. 
Recognized words: The evidence consists of recognizer acoustics A, and we seek 
to compute P(A I U). As described later, this involves considering multiple 
alternative recognized word sequences. 
Prosodic features- Evidence is given by the acoustic features F capturing various 
aspects of pitch, duration, energy, etc., of the speech signal; the associated 
likelihoods are P(F I U). 
For ease of reference, all random variables used here are summarized in Table 4. 
The same variables are used with subscripts to refer to individual utterances. For 
example, Wi is the word transcription of the ith utterance within a conversation ( ot 
the ith word). 
To make both the modeling and the search for the best DA sequence feasible, we 
further equire that our likelihood models are decomposable byutterance. This means 
that the likelihood given a complete conversation can be factored into likelihoods 
given the individual utterances. We use Ui for the ith DA label in the sequence U, 
i.e., U = (U1 . . . . .  Ui,..., Un), where n is the number of utterances in a conversation. 
In addition, we use Ei for that portion of the evidence that corresponds to the ith 
utterance, .g., the words or the prosody of the ith utterance. Decomposability of the 
likelihood means that 
P(EIU) = P(E11 U1).... .  P(En \[Un) (2) 
Applied separately to the three types of evidence Ai, Wi, and Fi mentioned above, 
it is clear that this assumption is not strictly true. For example, speakers tend to reuse 
347 
Computational Linguistics Volume 26, Number 3 
E1 Ei E.  
T T T 
<start> , U1 , . . .  ~ Ui ) . . . - - - *  Un 
Figure 1 
The discourse HMM as Bayes network. 
<end> 
words found earlier in the conversation (Fowler and Housum 1987) and an answer 
might actually be relevant o the question before it, violating the independence of the 
P(WilUi). Similarly, speakers adjust their pitch or volume over time, e.g., to the con- 
versation partner or because of the structure of the discourse (Menn and Boyce 1982), 
violating the independence of the P(FilUi). As in other areas of statistical modeling, 
we count on the fact that these violations are small compared to the properties actually 
modeled, namely, the dependence of Ei on Ui. 
3.2 Markov  Mode l ing  
Returning to the prior distribution of DA sequences P(U), it is convenient to make 
certain independence assumptions here, too. In particular, we assume that the prior 
distribution of U is Markovian, i.e., that each Ui depends only on a fixed number k of 
preceding DA labels: 
P(U i lU l ,  . . . ,  U i -1 )  ~- P (U i lU i -k  . . . . .  Ui -1 )  (3) 
(k is the order of the Markov process describing U). The n-gram-based discourse gram- 
mars we used have this property. As described later, k = 1 is a very good choice, i.e., 
conditioning on the DA types more than one removed from the current one does not 
improve the quality of the model by much, at least with the amount of data available 
in our experiments. 
The importance of the Markov assumption for the discourse grammar is that 
we can now view the whole system of discourse grammar and local utterance-based 
likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). 
The HMM states correspond to DAs, observations correspond to utterances, transition 
probabilities are given by the discourse grammar (see Section 4), and observation 
probabilities are given by the local likelihoods P(Eil Ui). 
We can represent the dependency structure (as well as the implied conditional 
independences) as a special case of Bayesian belief network (Pearl 1988). Figure 1 
shows the variables in the resulting HMM with directed edges representing conditional 
dependence. To keep things simple, a first-order HMM (bigram discourse grammar) 
is assumed. 
3.3 D ia logue  Act  Decod ing  
The HMM representation allows us to use efficient dynamic programming algorithms 
to compute relevant aspects of the model, such as 
? the most probable DA sequence (the Viterbi algorithm) 
? the posterior probability of various DAs for a given utterance, after 
considering all the evidence (the forward-backward algorithm) 
The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable 
state sequence. When applied to a discourse model with locally decomposable ike- 
lihoods and Markovian discourse grammar, it will therefore find precisely the DA 
348 
Stolcke et al Dialogue Act Modeling 
sequence with the highest posterior probability: 
U* = argmaxP(UIE ) (4) 
u 
The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is 
fundamentally the same as the standard probabilistic approaches to speech recognition 
(Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the prob- 
ability of getting the entire DA sequence correct, but it does not necessarily find the 
DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To 
minimize the total number of utterance labeling errors, we need to maximize the prob- 
ability of getting each DA label correct individually, i.e., we need to maximize P(UilE) 
for each i = 1 . . . . .  n. We can compute the per-utterance posterior DA probabilities by 
summing: 
P(u\[E) = E P(UIE) (5) 
U: Ui=u 
where the summation is over all sequences U whose ith element matches the label in 
question. The summation is efficiently carried out by the forward-backward algorithm 
for HMMs (Baum et al 1970). 3
For zeroth-order (unigram) discourse grammars, Viterbi decoding and forward- 
backward decoding necessarily ield the same results. However, for higher-order 
discourse grammars we found that forward-backward decoding consistently gives 
slightly (up to 1% absolute) better accuracies, as expected. Therefore, we used this 
method throughout. 
The formulation presented here, as well as all our experiments, uses the entire 
conversation as evidence for DA classification. Obviously, this is possible only during 
off-line processing, when the full conversation is available. Our paradigm thus follows 
historical practice in the Switchboard omain, where the goal is typically the off-line 
processing (e.g., automatic transcription, speaker identification, indexing, archival) of 
entire previously recorded conversations. However, the HMM formulation used here 
also supports computing posterior DA probabilities based on partial evidence, e.g., 
using only the utterances preceding the current one, as would be required for on-line 
processing. 
4. Discourse Grammars 
The statistical discourse grammar models the prior probabilities P(U) of DA sequences. 
In the case of conversations for which the identities of the speakers are known (as 
in Switchboard), the discourse grammar should also model turn-taking behavior. A 
straightforward approach is to model sequences of pairs (Ui, Ti) where Ui is the DA 
label and Ti represents he speaker. We are not trying to model speaker idiosyncrasies, 
so conversants are arbitrarily identified as A or B, and the model is made symmetric 
with respect o the choice of sides (e.g., by replicating the training sequences with 
sides switched). Our discourse grammars thus had a vocabulary of 42 x 2 = 84 labels, 
plus tags for the beginning and end of conversations. For example, the second DA tag 
in Table 1 would be predicted by a trigram discourse grammar using the fact that the 
same speaker previously uttered a YES-NO-QUESTION, which in turn was preceded by 
the start-of-conversation. 
3 We note in passing that he Viterbi and Baum algorithms have quivalent formulations in the Bayes 
network framework (Pearl 1988). The HMM terminology was chosen here mainly for historical reasons. 
349 
Computational Linguistics Volume 26, Number 3 
Table 5 
Perplexities of DAs with and without urn 
information. 
Discourse Grammar P(U) P(U, T) P(UIT )
None 42 84 42 
Unigram 11.0 18.5 9.0 
Bigram 7.9 10.4 5.1 
Trigram 7.5 9.8 4.8 
4.1 N-gram Discourse Mode ls  
A computationally convenient type of discourse grammar is an n-gram model based on 
DA tags, as it allows efficient decoding in the HMM framework. We trained standard 
backoff n-gram models (Katz 1987), using the frequency smoothing approach of Witten 
and Bell (1991). Models of various orders were compared by their perplexities, i.e., 
the average number of choices the model predicts for each tag, conditioned on the 
preceding tags. 
Table 5 shows perplexities for three types of models: P(U), the DAs alone; P(U, T), 
the combined DA/speaker ID sequence; and P(UIT ), the DAs conditioned on known 
speaker IDs (appropriate for the Switchboard task). As expected, we see an improve- 
ment (decreasing perplexities) for increasing n-gram order. However, the incremental 
gain of a trigram is small, and higher-order models did not prove useful. (This ob- 
servation, initially based on perplexity, is confirmed by the DA tagging experiments 
reported in Section 5.) Comparing P(U) and P(U\[T), we see that speaker identity adds 
substantial information, especially for higher-order models. 
The relatively small improvements from higher-order models could be a result of 
lack of training data, or of an inherent independence of DAs from DAs further re- 
moved. The near-optimality of the bigram discourse grammar is plausible given con- 
versation analysis accounts of discourse structure in terms of adjacency pairs (Schegloff 
1968; Sacks, Schegloff, and Jefferson 1974). Inspection of bigram probabilities estimated 
from our data revealed that conventional djacency pairs receive high probabilities, as 
expected. For example, 30% of YES-NO-QUESTIONS are followed by YES-ANSWERS, 
14% by NO-ANSWERS (confirming that the latter are dispreferred). COMMANDS are fol- 
lowed by AGREEMENTS in 23% of the cases, and STATEMENTS elicit BACKCHANNELS 
in 26% of all cases. 
4.2 Other Discourse Mode ls  
We also investigated non-n-gram discourse models, based on various language model- 
ing techniques known from speech recognition. One motivation for alternative models 
is that n-grams enforce a one-dimensional representation  DA sequences, whereas 
we saw above that the event space is really multidimensional (DA label and speaker 
labels). Another motivation is that n-grams fail to model long-distance dependencies, 
such as the fact that speakers may tend to repeat certain DAs or patterns throughout 
the conversation. 
The first alternative approach was a standard cache model (Kuhn and de Mori 
1990), which boosts the probabilities of previously observed unigrams and bigrams, on 
the theory that tokens tend to repeat hemselves over longer distances. However, this 
does not seem to be true for DA sequences in our corpus, as the cache model showed 
no improvement over the standard N-gram. This result is somewhat surprising since 
unigram dialogue grammars are able to detect speaker gender with 63% accuracy (over 
350 
Stolcke t al. Dialogue Act Modeling 
a 50% baseline) on Switchboard (Ries 1999b), indicating that there are global variables 
in the DA distribution that could potentially be exploited by a cache dialogue grammar. 
Clearly, dialogue grammar adaptation needs further esearch. 
Second, we built a discourse grammar that incorporated constraints on DA se- 
quences in a nonhierarchical way, using maximum entropy (ME) estimation (Berger, 
Della Pietra, and Della Pietra 1996). The choice of features was informed by similar 
ones commonly used in statistical language models, as well our general intuitions 
about potentially information-bearing elements in the discourse context. Thus, the 
model was designed so that the current DA label was constrained by features uch as 
unigram statistics, the previous DA and the DA once removed, DAs occurring within a 
window in the past, and whether the previous utterance was by the same speaker. We 
found, however, that an ME model using n-gram constraints performed only slightly 
better than a corresponding backoff n-gram. 
Additional constraints such as DA triggers, distance-1 bigrams, separate ncoding 
of speaker change and bigrams to the last DA on the same/other channel did not 
improve relative to the trigram model. The ME model thus confirms the adequacy of 
the backoff n-gram approach, and leads us to conclude that DA sequences, at least 
in the Switchboard omain, are mostly characterized by local interactions, and thus 
modeled well by low-order n-gram statistics for this task. For more structured tasks this 
situation might be different. However, we have found no further exploitable structure. 
5. Dialogue Act Classification 
We now describe in more detail how the knowledge sources of words and prosody 
are modeled, and what automatic DA labeling results were obtained using each of the 
knowledge sources in turn. Finally, we present results for a combination of all knowl- 
edge sources. DA labeling accuracy results hould be compared to a baseline (chance) 
accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) in 
our test set. 4 
5.1 Dialogue Act Classification Using Words 
DA classification using words is based on the observation that different DAs use 
distinctive word strings. It is known that certain cue words and phrases (Hirschberg 
and Litman 1993) can serve as explicit indicators of discourse structure. Similarly, 
we find distinctive correlations between certain phrases and DA types. For example, 
92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams "<start> 
do you" occur in YES-NO-QUESTIONS. To leverage this information source, without 
hand-coding knowledge about which words are indicative of which DAs, we will use 
statistical language models that model the full word sequences associated with each 
DA type. 
5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words 
of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) 
in a straightforward way, by building a statistical language model for each of the 42 
DAs. All DAs of a particular type found in the training corpus were pooled, and 
a DA-specific trigram model was estimated using standard techniques (Katz backoff 
\[Katz 1987\] with Witten-Bell discounting \[Witten and Bell 1991\]). 
4 The frequency of STATEMENTS across all labeled ata was slightly different, cf. Table 2. 
351 
Computational Linguistics Volume 26, Number 3 
A1 Ai An 
T T 
wl wi w.  
T T T 
<start> ~ U1 ~ . . . .  ~ Ui ~ . . . .  Un ~ <end> 
Figure 2 
Modified Bayes network including word hypotheses and recognizer acoustics. 
5.1.2 Classification from Recognized Words. For fully automatic DA classification, 
the above approach is only a partial solution, since we are not yet able to recognize 
words in spontaneous speech with perfect accuracy. A standard approach is to use 
the 1-best hypothesis from the speech recognizer in place of the true word transcripts. 
While conceptually simple and convenient, his method will not make optimal use of 
all the information in the recognizer, which in fact maintains multiple hypotheses as 
well as their relative plausibilities. 
A more thorough use of recognized speech can be derived as follows. The classifi- 
cation framework is modified such that the recognizer's acoustic information (spectral 
features) A appear as the evidence. We compute P(A\[U) by decomposing it into an 
acoustic likelihood P(A\]W) and a word-based likelihood P(W\[ U), and summing over 
all word sequences: 
P(AlU) = ~-~ P(AIW, U)P(WIU) 
w 
= ~P(A IW)P(W\ [U  ) 
w 
(6) 
The second line is justified under the assumption that the recognizer acoustics (typ- 
ically, cepstral coefficients) are invariant o DA type once the words are fixed. Note 
that this is another approximation i our modeling. For example, different DAs with 
common words may be realized by different word pronunciations. Figure 2 shows the 
Bayes network resulting from modeling recognizer acoustics through word hypothe- 
ses under this independence assumption; note the added Wi variables (that have to 
be summed over) in comparison to Figure 1. 
The acoustic likelihoods P(A\[W) correspond to the acoustic scores the recognizer 
outputs for every hypothesized word sequence W. The summation over all W must 
be approximated; in our experiments we summed over the (up to) 2,500 best hypothe- 
ses generated by the recognizer for each utterance. Care must be taken to scale the 
recognizer acoustic scores properly, i.e., to exponentiate he recognizer acoustic scores 
by 1/~, where A is the language model weight of the recognizer, s 
5 In a standard recognizer the total og score of a hypothesis Wi is computed as 
logP(AdWi ) + )~ logP(Wi) - I~\]Wi\], 
where \[Wi\] is the number of words in the hypothesis, and both A and/~ are parameters optimized to 
minimize the word error rate. The word insertion penalty/~ represents a correction to the language 
model that allows balancing insertion and deletion errors. The language model weight ,~ compensates 
for acoustic score variances that are effectively too large due to severe independence assumptions in 
the recognizer acoustic model. According to this rationale, it is more appropriate o divide all score 
components by ),. Thus, in all our experiments, we computed a summand in Equation 6whose 
352 
Stolcke t al. Dialogue Act Modeling 
Table 6 
DA classification accuracies (in %) from transcribed and recognized 
words (chance = 35%). 
Discourse Grammar True Recognized Relative Error Increase 
None 54.3 42.8 25.2% 
Unigram 68.2 61.8 20.1% 
Bigram 70.6 64.3 21.4% 
Trigram 71.0 64.8 21.4% 
5.1.3 Results. Table 6 shows DA classification accuracies obtained by combining the 
word- and recognizer-based likelihoods with the n-gram discourse grammars de- 
scribed earlier. The best accuracy obtained from transcribed words, 71%, is encour- 
aging given a comparable human performance of84% (the interlabeler agreement, see 
Section 2.2). We observe about a 21% relative increase in classification error when us- 
ing recognizer words; this is remarkably small considering that the speech recognizer 
used had a word error rate of 41% on the test set. 
We also compared the n-best DA classification approach to the more straightfor- 
ward 1-best approach. In this experiment, only the single best recognizer hypothesis 
is used, effectively treating it as the true word string. The 1-best method increased 
classification error by about 7% relative to the n-best algorithm (61.5% accuracy with 
a bigram discourse grammar). 
5.2 Dialogue Act Classification Using Prosody 
We also investigated prosodic information, i.e., information i dependent of the words 
as well as the standard recognizer acoustics. Prosody is important for DA recogni- 
tion for two reasons. First, as we saw earlier, word-based classification suffers from 
recognition errors. Second, some utterances are inherently ambiguous based on words 
alone. For example, some YES-NO-QUESTiONS have word sequences identical to those 
of STATEMENTS, but can often be distinguished by their final F0 rise. 
A detailed study aimed at automatic prosodic lassification of DAs in the Switch- 
board domain is available in a companion paper (Shriberg et al 1998). Here we investi- 
gate the interaction of prosodic models with the dialogue grammar and the word-based 
DA models discussed above. We also touch briefly on alternative machine learning 
models for prosodic features. 
5.2.1 Prosodic Features. Prosodic DA classification was based on a large set of fea- 
tures computed automatically from the waveform, without reference to word or phone 
information. The features can be broadly grouped as referring to duration (e.g., utter- 
ance duration, with and without pauses), pauses (e.g., total and mean of nonspeech 
regions exceeding 100 ms), pitch (e.g., mean and range of F0 over utterance, slope of 
F0 regression line), energy (e.g., mean and range of RMS energy, same for signal-to- 
logarithm was 
1 logP(Ai\]Wi) + logP(WilUi) - ~lWil. -d 
We found this approach to give better esults than the standard multiplication of logP(W) by ,L Note 
that for selecting the best hypothesis in a recognizer only the relative magnitudes of the score weights 
matter; however, for the summation in Equation 6 the absolute values become important. The 
parameter values for )~ and # were those used by the standard recognizer; they were not specifically 
optimized for the DA classification task. 
353 
Computational Linguistics Volume 26, Number 3 
~ 23.403 
~ an utt < 0 .3" /~U >= 0.3"/17 
Figure 3 
Decision tree for the classification of BACKCHANNELS (B) and AGREEMENTS (A). Each node is 
labeled with the majority class for that node, as well as the posterior probabilities of the two 
classes. The following features are queried in the tree: number of frames in continuous (> 1 s) 
speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration 
excluding pauses > 100 ms (ling_dur_minus_minlOpause), andmean signal-to-noise ratio 
(snr_mean_utt ). 
noise ratio \[SNR\]), speaking rate (based on the "enrate" measure of Morgan, Fosler, 
and Mirghafori \[1997\]), and gender (of both speaker and listener). In the case of ut- 
terance duration, the measure correlates both with length in words and with overall 
speaking rate. The gender feature that classified speakers as either male or female was 
used to test for potential inadequacies in F0 normalizations. Where appropriate, we 
included both raw features and values normalized by utterance and/or  conversation. 
We also included features that are the output of the pitch accent and boundary tone 
event detector of Taylor (2000) (e.g., the number of pitch accents in the utterance). A 
complete description of prosodic features and an analysis of their usage in our models 
can be found in Shriberg et al (1998). 
5.2.2 Prosodic Decision Trees. For our Prosodic classifiers, we used CART-style deci- 
sion trees (Breiman et al 1984). Decision trees allow the combination of discrete and 
continuous features, and can be inspected to help in understanding the role of different 
features and feature combinations. 
To illustrate one area in which prosody could aid our classification task, we applied 
trees to DA classifications known to be ambiguous from words alone. One frequent 
example in our corpus was the distinction between BACKCHANNELS and AGREEMENTS 
(see Table 2), which share terms such as right and yeah. As shown in Figure 3, a prosodic 
tree trained on this task revealed that agreements have consistently longer durations 
and greater energy (as reflected by the SNR measure) than do backchannels. 
354 
Stolcke t al. Dialogue Act Modeling 
Table 7 
DA classification using prosodic 
decision trees (chance = 35%). 
Discourse Grammar Accuracy (%) 
None 38.9 
Unigram 48.3 
Bigram 49.7 
The HMM framework requires that we compute prosodic likelihoods of the form 
P(FilUi) for each utterance Ui and associated prosodic feature values Fi. We have 
the apparent difficulty that decision trees (as well as other classifiers, such as neural 
networks) give estimates for the posterior probabilities, P(Ui\[Fi). The problem can be 
overcome by applying Bayes' rule locally: 
P(Ui) t rue)  
(7) 
Note that P(Fi) does not depend on Ui and can be treated as a constant for the purpose 
of DA classification. A quantity proportional to the required likelihood can therefore 
be obtained either by dividing the posterior tree probability by the prior P(Ui), 6 or by 
training the tree on a uniform prior distribution of DA types. We chose the second 
approach, downsampling our training data to equate DA proportions. This also coun- 
teracts a common problem with tree classifiers trained on very skewed distributions 
of target classes, i.e., that low-frequency classes are not modeled in sufficient detail 
because the majority class dominates the tree-growing objective hznction. 
5.2.3 Results with Dec is ion Trees. As a preliminary experiment to test the integra- 
tion of prosody with other knowledge sources, we trained a single tree to discriminate 
among the five most frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABAN- 
DONED, and AGREEMENT, totaling 79% of the data) and an Other category comprising 
all remaining DA types. The decision tree was trained on a downsampled training 
subset containing equal proportions of these six DA classes. The tree achieved a clas- 
sification accuracy of 45.4% on an independent test set with the same uniform six-class 
distribution. The chance accuracy on this set is 16.6%, so the tree clearly extracts useful 
information from the prosodic features. 
We then used the decision tree posteriors as scaled DA likelihoods in the dialogue 
model HMM, combining it with various n-gram dialogue grammars for testing on our 
full standard test set. For the purpose of model integration, the likelihoods of the Other 
class were assigned to all DA types comprised by that class. As shown in Table 7, the 
tree with dialogue grammar performs ignificantly better than chance on the raw DA 
distribution, although not as well as the word-based methods (cf. Table 6). 
5.2.4 Neural  Network  Classifiers. Although we chose to use decision trees as prosodic 
classifiers for their relative ase of inspection, we might have used any suitable proba- 
bilistic classifier, i.e., any model that estimates the posterior probabilities of DAs given 
the prosodic features. We conducted preliminary experiments o assess how neural 
6 Bourlard and Morgan (1993) use this approach tointegrate neural network phonetic models in a 
speech recognizer. 
355 
Computational Linguistics Volume 26, Number 3 
Table 8 
Performance ofvarious prosodic neural network classifiers on 
an equal-priors, ix-class DA set (chance = 16.6%). 
Network Architecture Accuracy (%) 
Decision tree 45.4 
No hidden layer, linear output function 44.6 
No hidden layer, softmax output function 46.0 
40-unit hidden layer, softmax output function 46.0 
networks compare to decision trees for the type of data studied here. Neural networks 
are worth investigating since they offer potential advantages over decision trees. They 
can learn decision surfaces that lie at an angle to the axes of the input feature space, 
unlike standard CART trees, which always split continuous features on one dimen- 
sion at a time. The response function of neural networks is continuous (smooth) at 
the decision boundaries, allowing them to avoid hard decisions and the complete 
fragmentation f data associated with decision tree questions. 
Most important, however, related work (Ries 1999a) indicated that similarly struc- 
tured networks are superior classifiers if the input features are words and are therefore 
a plug-in replacement for the language model classifiers described in this paper. Neural 
networks are therefore a good candidate for a jointly optimized classifier of prosodic 
and word-level information since one can show that they are a generalization of the 
integration approach used here. 
We tested various neural network models on the same six-class downsampled 
data used for decision tree training, using a variety of network architectures and out- 
put layer functions. The results are summarized in Table 8, along with the baseline 
result obtained with the decision tree model. Based on these experiments, a softmax 
network (Bridle 1990) without hidden units resulted in only a slight improvement 
over the decision tree. A network with hidden units did not afford any additional 
advantage, ven after we optimized the number of hidden units, indicating that com- 
plex combinations of features (as far as the network could learn them) do not predict 
DAs better than linear combinations of input features. While we believe alternative 
classifier architectures should be investigated further as prosodic models, the results 
so far seem to confirm our choice of decision trees as a model class that gives close to 
optimal performance for this task. 
5.2.5 Intonation Event Likel ihoods. An alternative way to compute prosodically based 
DA likelihoods uses pitch accents and boundary phrases (Taylor et al 1997). The ap- 
proach relies on the intuition that different utterance types are characterized by dif- 
ferent intonational "tunes" (Kowtko 1996), and has been successfully applied to the 
classification ofmove types in the DCIEM Map Task corpus (Wright and Taylor 1997). 
The system detects equences ofdistinctive pitch patterns by training one continuous- 
density HMM for each DA type. Unfortunately, the event classification accuracy on 
the Switchboard corpus was considerably poorer than in the Map Task domain, and 
DA recognition results when coupled with a discourse grammar were substantially 
worse than with decision trees. The approach could prove valuable in the future, 
however, if the intonation event detector can be made more robust to corpora like 
OURS. 
356 
Stolcke et al Dialogue Act Modeling 
A1 Ai An 
T 1 1 
Wl Wi W,, 
T T t 
<start> - , 0"1 , . . .---* U/ , ... ~ Un , <end> 
1 1 ,t 
F1 Fi G 
Figure 4 
Bayes network for discourse HMM incorporating both word recognition and prosodic features. 
5.3 Using Multiple Knowledge Sources 
As mentioned earlier, we expect improved performance from combining word and 
prosodic information. Combining these knowledge sources requires estimating a com- 
bined likelihood P(Ai, Fi\[Ui) for each utterance. The simplest approach is to assume 
that the two types of acoustic observations (recognizer acoustics and prosodic features) 
are approximately conditionally independent once Ui is given: 
P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) 
~, P(ai, Wi\[Ui)P(FilUi) (8) 
Since the recognizer acoustics are modeled by way of their dependence on words, it 
is particularly important o avoid using prosodic features that are directly correlated 
with word identities, or features that are also modeled by the discourse grammars, 
such as utterance position relative to turn changes. Figure 4 depicts the Bayes network 
incorporating evidence from both word recognition and prosodic features. 
One important respect in which the independence assumption is violated is in the 
modeling of utterance length. While utterance length itself is not a prosodic feature, 
it is an important feature to condition on when examining prosodic characteristics 
of utterances, and is thus best included in the decision tree. Utterance length is cap- 
tured directly by the tree using various duration measures, while the DA-specific 
LMs encode the average number of words per utterance indirectly through n-gram 
parameters, but still accurately enough to violate independence in a significant way 
(Finke et al 1998). As discussed in Section 8, this problem is best addressed by joint 
lexical-prosodic models. 
We need to allow for the fact that the models combined in Equation 8 give es- 
timates of differing qualities. Therefore, we introduce an exponential weight a on 
P(Fi\[Ui) that controls the contribution of the prosodic likelihood to the overall likeli- 
hood. Finally, a second exponential weight fl on the combined likelihood controls its 
dynamic range relative to the discourse grammar scores, partially compensating for 
any correlation between the two likelihoods. The revised combined likelihood estimate 
thus becomes: 
P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi\[Ui)~}  (9) 
In our experiments, the parameters a and fl were optimized using twofold jackknifing. 
The test data was split roughly in half (without speaker overlap), each half was used 
to separately optimize the parameters, and the best values were then tested on the 
respective other half. The reported results are from the aggregate outcome on the two 
test set halves. 
357 
Computational Linguistics Volume 26, Number 3 
Table 9 
Combined utterance classification accuracies (chance = 
35%). The first two columns correspond to Tables 7 
and 6, respectively. 
Discourse Grammar Accuracy (%) 
Prosody Recognizer Combined 
None 38.9 42.8 56.5 
Unigram 48.3 61.8 62.4 
Bigram 49.7 64.3 65.0 
Table 10 
Accuracy (in %) for individual 
subtasks, using uniform priors 
and combined models for two 
(chance = 50%). 
Classification Task True Words Recognized Words 
Knowledge Source 
QUESTIONS/STATEMENTS 
prosody only 76.0 76.0 
words only 85.9 75.4 
words+prosody 87.6 79.8 
AGREEMENTS / BACKCHANNELS 
prosody only 72.9 72.9 
words only 81.0 78.2 
words+prosody 84.7 81.7 
5.3.1 Results. In this experiment we combined the acoustic n-best likelihoods based 
on recognized words with the Top-5 tree classifier mentioned in Section 5.2.3. Results 
are summarized in Table 9. 
As shown, the combined classifier presents a slight improvement over the rec- 
ognizer-based classifier, The experiment without discourse grammar indicates that 
the combined evidence is considerably stronger than either knowledge source alone, 
yet this improvement seems to be made largely redundant by the use of priors and 
the discourse grammar. For example, by definition DECLARATIVE-QUESTIONS are not 
marked by syntax (e.g., by subject-auxiliary inversion) and are thus confusable with 
STATEMENTS and OPINIONS. While prosody is expected to help disambiguate hese 
cases, the ambiguity can also be removed by examining the context of the utterance, 
e.g., by noticing that the following utterance is a YEs-ANswER or NO-ANSWER. 
5.3.2 Focused Classifications. To gain a better understanding of the potential for 
prosodic DA classification i dependent of the effects of discourse grammar and the 
skewed DA distribution i  Switchboard, we examined several binary DA classification 
tasks. The choice of tasks was motivated by an analysis of confusions committed by a 
purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, 
and BACKCHANNELS for AGREEMENTS (and vice versa). We tested aprosodic lassifier, 
a word-based classifier (with both transcribed and recognized words), and a combined 
classifier on these two tasks, downsampling the DA distribution to equate the class 
sizes in each case. Chance performance in all experiments i  therefore 50%. Results 
are summarized in Table 10. 
358 
Stolcke et al Dialogue Act Modeling 
As shown, the combined classifier was consistently more accurate than the classi- 
fier using words alone. Although the gain in accuracy was not statistically significant 
for the small recognizer test set because of a lack of power, replication for a larger 
hand-transcribed test set showed the gain to be highly significant for both subtasks 
by a Sign test, p < .001 and p < .0001 (one-tailed), respectively. Across these, as well 
as additional subtasks, the relative advantage of adding prosody was larger for recog- 
nized than for true words, suggesting that prosody is particularly helpful when word 
information is not perfect. 
6. Speech Recognition 
We now consider ways to use DA modeling to enhance automatic speech recognition 
(ASR). The intuition behind this approach is that discourse context constrains the 
choice of DAs for a given utterance, and the DA type in turn constrains the choice of 
words. The latter can then be leveraged for more accurate speech recognition. 
6.1 Integrating DA Modeling and ASR 
Constraints on the word sequences hypothesized by a recognizer are expressed prob- 
abilistically in the recognizer language model (LM). It provides the prior distribution 
P(Wi) for finding the a posteriori most probable hypothesized words for an utterance, 
given the acoustic evidence Ai (Bahl, Jelinek, and Mercer 1983): 7
W 7 = argmaxP(WilAi) 
wi 
P(Wi)P(AilWi) = argmax 
wi P(Ai) 
= argmaxP(Wi)P(AilWi) (10) 
wi 
The likelihoods P(AilWi) are estimated by the recognizer's acoustic model. In a stan- 
dard recognizer the language model P(Wi) is the same for all utterances; the idea here 
is to obtain better-quality LMs by conditioning on the DA type Ui, since presumably 
the word distributions differ depending on DA type. 
W7 -- argmaxP(WilAi, Ui) 
wi 
P( WilUi)P(AilWi, Ui) = argmax 
Wi P(AiIUi) 
argmaxP(WilUi)P(AirWi) (11) 
wi 
As before in the DA classification model, we tacitly assume that the words Wi depend 
only on the DA of the current utterance, and also that the acoustics are independent of
the DA type if the words are fixed. The DA-conditioned language models P(Wil Ui) are 
readily trained from DA-specific training data, much as we did for DA classification 
from words. 8 
7 Note the similarity of Equations 10 and 1. They are identical except for the fact that we are now 
operating at the level of an individual utterance, the evidence isgiven by the acoustics, and the targets 
are word hypotheses instead of DA hypotheses. 
8 In Equation 11 and elsewhere in this section we gloss over the issue of proper weighting of model 
probabilities, which is extremely important in practice. The approach explained in detail in footnote 5
applies here as well. 
359 
Computational Linguistics Volume 26, Number 3 
The problem with applying Equation 11, of course, is that the DA type Ui is 
generally not known (except maybe in applications where the user interface can be 
engineered to allow only one kind of DA for a given utterance). Therefore, we need 
to infer the likely DA types for each utterance, using available evidence E from the 
entire conversation. This leads to the following formulation: 
W~ = argmaxP(WilAi, E)
wi 
---- argmax ~-~ P(WilAi, Ui, E)P(UilE) 
Wi Ui 
argmax ~\[\] P( WiiAi, Ui)P( Ui\[E) 
W~ U~ 
(12) 
The last step in Equation 12 is justified because, as shown in Figures 1 and 4, the 
evidence E (acoustics, prosody, words) pertaining to utterances other than i can affect 
the current utterance only through its DA type Ui. 
We call this the mixture-of-posteriors approach, because it amounts to a mixture of 
the posterior distributions obtained from DA-specific speech recognizers (Equation 11), 
using the DA posteriors as weights. This approach is quite expensive, however, as it 
requires multiple full recognizer or rescoring passes of the input, one for each DA 
type. 
A more efficient, though mathematically ess accurate, solution can be obtained 
by combining uesses about the correct DA types directly at the level of the LM. We 
estimate the distribution of likely DA types for a given utterance using the entire 
conversation E as evidence, and then use a sentence-level mixture (Iyer, Ostendorf, 
and Rohlicek 1994) of DA-specific LMs in a single recognizer run. In other words, we 
replace P(WilUi) in Equation 11 with 
~_~ P(WilUi)P(Ui\]E), 
ui 
a weighted mixture of all DA-specific LMs. We call this the mixture-of-LMs ap- 
proach. In practice, we would first estimate DA posteriors for each utterance, us- 
ing the forward-backward algorithm and the models described in Section 5, and then 
rerecognize the conversation or rescore the recognizer output, using the new posterior- 
weighted mixture LM. Fortunately, as shown in the next section, the mixture-of-LMs 
approach seems to give results that are almost identical to (and as good as) the mixture- 
of-posteriors approach. 
6.2 Computational Structure of Mixture Modeling 
It is instructive to compare the expanded scoring formulas for the two DA mixture 
modeling approaches for ASK The mixture-of-posteriors approach yields 
P(WilAi, E) = ~ P(ailui) 
ui 
(13) 
whereas the mixture-of-LMs approach gives 
) P(A,Iw,) 
P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) 
360 
Stolcke t al. Dialogue Act Modeling 
Table 11 
Switchboard word recognition error rates and 
LM perplexities. 
Model WER (%) Perplexity 
Baseline 41.2 76.8 
1-best LM 41.0 69.3 
Mixture-of-posteriors 41.0 n/a 
Mixture-of-LMs 40.9 66.9 
Oracle LM 40.3 66.8 
We see that the second equation reduces to the first under the crude approximation 
P(Ai\] Ui) ~ P(Ai). In practice, the denominators are computed by summing the numer- 
ators over a finite number of word hypotheses Wi, so this difference translates into 
normalizing either after or before summing over DAs. When the normalization takes 
place as the final step it can be omitted for score maximization purposes; this shows 
why the mixture-of-LMs approach is less computationally expensive. 
6.3 Experiments and Results 
We tested both the mixture-of-posteriors and the mixture-of-LMs approaches on our 
Switchboard test set of 19 conversations. Instead of decoding the data from scratch 
using the modified models, we manipulated n-best lists consisting of up to 2,500 best 
hypotheses for each utterance. This approach is also convenient since both approaches 
require access to the full word string for hypothesis scoring; the overall model is no 
longer Markovian, and is therefore inconvenient to use in the first decoding stage, or 
even in lattice rescoring. 
The baseline for our experiments was obtained with a standard backoff trigram 
language model estimated from all available training data. The DA-specific language 
models were trained on word transcripts of all the training utterances of a given type, 
and then smoothed further by interpolating them with the baseline LM. Each DA- 
specific LM used its own interpolation weight, obtained by minimizing the perplexity 
of the interpolated model on held-out DA-specific training data. Note that this smooth- 
ing step is helpful when using the DA-specific LMs for word recognition, but not for 
DA classification, since it renders the DA-specific LMs less discriminative. 9 
Table 11 summarizes both the word error rates achieved with the various models 
and the perplexities of the corresponding LMs used in the rescoring (note that per- 
plexity is not meaningful in the mixture-of-posteriors approach). For comparison, we 
also included two additional models: the 'q-best LM" refers to always using the DA- 
specific LM corresponding to the most probable DA type for each utterance. It is thus 
an approximation to both mixture approaches where only the top DA is considered. 
Second, we included an "oracle LM," i.e., always using the LM that corresponds to 
the hand-labeled DA for each utterance. The purpose of this experiment was to give us 
an upper bound on the effectiveness of the mixture approaches, by assuming perfect 
DA recognition. 
It was somewhat disappointing that the word error rate (WER) improvement in
the oracle experiment was small (2.2% relative), even though statistically highly sig- 
nificant (p < .0001, one-tailed, according to a Sign test on matched utterance pairs). 
9 Indeed, during our DA classification experiments, wehad observed that smoothed DA-specific LMs 
yield lower classification accuracy. 
361 
Computational Linguistics Volume 26, Number 3 
Table 12 
Word error reductions through DA oracle, by DA type. 
Dialogue Act Baseline WER Oracle WER WER Reduction 
NO-ANSWER 29.4 11.8 -17.6 
BACKCHANNEL 25.9 18.6 -7.3 
BACKCHANNEL-QUESTION 15.2 9.1 -6.1 
ABANDONED/UNINTERPRETABLE 48.9 45.2 -3.7 
WH-QUESTION 38.4 34.9 -3.5 
YES-No-QUESTION 55.5 52.3 --3.2 
STATEMENT 42.0 41.5 --0.5 
OPINION 40.8 40.4 --0.4 
Other 8% 
onded/Uninterpretable 3% 
kchannel 3% 
as-No-Question 3% 
Statement 53% 
Dpinion 30% 
Figure 5 
Relative contributions to test set word counts by DA type. 
The WER reduction achieved with the mixture-of-LMs approach did not achieve sta- 
tistical significance (0.25 > p > 0.20). The 1-best DA and the two mixture models 
also did not differ significantly on this test set. In interpreting these results one must 
realize, however, that WER results depend on a complex combination of factors, most 
notably interaction between language models and the acoustic models. Since the ex- 
periments only varied the language models used in rescoring, it is also informative to 
compare the quality of these models as reflected by perplexity. On this measure, we 
see a substantial 13% (relative) reduction, which is achieved by both the oracle and 
the mixture-of-LMs. The perplexity reduction for the 1-best LM is only 9.8%, showing 
the advantage of the mixture approach. 
To better understand the lack of a more substantial reduction in word error, we an- 
alyzed the effect of the DA-conditioned rescoring on the individual DAs, i.e., grouping 
the test utterances by their true DA types. Table 12 shows the WER improvements for 
a few DA types, ordered by the magnitude of improvement achieved. As shown, all 
frequent DA types saw improvement, but the highest wins were observed for typically 
short DAs, such as ANSWERS and BACKCHANNELS. This is to be expected, as such DAs 
tend to be syntactically and lexically highly constrained. Furthermore, the distribution 
of number of words across DA types is very uneven (Figure 5). STATEMENTS and 
OPINIONS, the DA types dominating in both frequency and number of words (83% of 
total), see no more than 0.5% absolute improvement, thus explaining the small overall 
improvement. In hindsight, this is also not surprising, since the bulk of the training 
data for the baseline LM consists of these DAs, allowing only little improvement in
362 
Stolcke et al Dialogue Act Modeling 
the DA-specific LMs. A more detailed analysis of the effect of DA modeling on speech 
recognition errors can be found elsewhere (Van Ess-Dykema nd Ries 1998). 
In summary, our experiments confirmed that DA modeling can improve word 
recognition accuracy quite substantially in principle, at least for certain DA types, 
but that the skewed distribution of DAs (especially in terms of number of words per 
type) limits the usefulness of the approach on the Switchboard corpus. The benefits 
of DA modeling might therefore be more pronounced on corpora with more even 
DA distribution, as is typically the case for task-oriented ialogues. Task-oriented 
dialogues might also feature specific subtypes of general DA categories that might 
be constrained by discourse. Prior research on task-oriented dialogues ummarized in 
the next section, however, has also found only small reductions in WER (on the order 
of 1%). This suggests that even in task-oriented domains more research is needed to 
realize the potential of DA modeling for ASR. 
7. Prior and Related Work 
As indicated in the introduction, our work builds on a number of previous efforts 
in computational discourse modeling and automatic discourse processing, most of 
which occurred over the last half-decade. It is generally not possible to directly com- 
pare quantitative results because of vast differences in methodology, tag set, type and 
amount of training data, and, principally, assumptions made about what information 
is available for "free" (e.g., hand-transcribed versus automatically recognized words, 
or segmented versus unsegmented utterances). Thus, we will focus on the conceptual 
aspects of previous research efforts, and while we do offer a summary of previous 
quantitative results, these should be interpreted as informative datapoints only, and 
not as fair comparisons between algorithms. 
Previous research on DA modeling has generally focused on task-oriented ia- 
logue, with three tasks in particular garnering much of the research effort. The Map 
Task corpus (Anderson et al 1991; Bard et al 1995) consists of conversations between 
two speakers with slightly different maps of an imaginary territory. Their task is to 
help one speaker eproduce a route drawn only on the other speaker's map, all with- 
out being able to see each other's maps. Of the DA modeling algorithms described 
below, Taylor et al (1998) and Wright (1998) were based on Map Task. The VERBMO- 
BIL corpus consists of two-party scheduling dialogues. A number of the DA m6deling 
algorithms described below were developed for VERBMOBIL, including those of Mast 
et al (1996), Warnke et al (1997), Reithinger et al (1996), Reithinger and Klesen (1997), 
and Samuel, Carberry, and Vijay-Shanker (1998). The ATR Conference corpus is a sub- 
set of a larger ATR Dialogue database consisting of simulated dialogues between a
secretary and a questioner at international conferences. Researchers using this corpus 
include Nagata (1992), Nagata and Morimoto (1993, 1994), and Kita et al (1996). Ta- 
ble 13 shows the most commonly used versions of the tag sets from those three tasks. 
As discussed earlier, these domains differ from the Switchboard corpus in being 
task-oriented. Their tag sets are also generally smaller, but some of the same problems 
of balance occur. For example, in the Map Task domain, 33% of the words occur in 1 
of the 12 DAs 0NSTRUCT). Table 14 shows the approximate size of the corpora, the tag 
set, and tag estimation accuracy rates for various recent models of DA prediction. The 
results summarized in the table also illustrate the differences in inherent difficulty of 
the tasks. For example, the task of Warnke et al (1997) was to simultaneously segment 
and tag DAs, whereas the other results rely on a prior manual segmentation. Similarly, 
the task in Wright (1998) and in our study was to determine DA types from speech 
input, whereas work by others is based on hand-transcribed textual input. 
363 
Computational Linguistics Volume 26, Number 3 
Table 13 
Dialogue act tag sets used in three other extensively studied corpora. 
VERBMOBIL. These 18 high-level DAs used in VERBMOBIL-1 are 
abstracted over a total of 43 more specific DAs; most experiments on 
VERBMOBIL DAs use the set of 18 rather than 43. Examples are from 
Jekat et al (1995). 
Tag Example 
THANK 
GREET 
INTRODUCE 
BYE 
REQUEST~COMMENT 
SUGGEST 
REJECT 
ACCEPT 
REQUEST-SUGGEST 
INIT 
GIVE_REASON 
FEEDBACK 
DELIBERATE 
CONFIRM 
CLARIFY 
DIGRESS 
MOTIVATE 
GARBAGE 
Thanks 
Hello Dan 
It's me again 
Alright bye 
How does that look? 
from thirteenth through seventeenth June 
No Friday I'm booked all day 
Saturday sounds fine, 
What is a good day of the week for you? 
I wanted to make an appointment with you 
Because I have meetings all afternoon 
Okay 
Let me check my calendar here 
Okay, that would be wonderful 
Okay, do you mean Tuesday the 23rd? 
\[we could meet for lunch\] and eat lots of ice cream 
We should go to visit our subsidiary in Munich 
Oops, I- 
Maptask. The 12 DAs or "move types" used in Map Task. Examples are 
from Taylor et al (1998). 
Tag Example 
INSTRUCT 
EXPLAIN 
ALIGN 
CHECK 
QUERY-YN 
QUERY-W 
ACKNOWLEDGE 
CLARIFY 
REPLY-Y 
REPLY-N 
REPLY-W 
READY 
Go round, ehm horizontally underneath diamond mine 
I don't have a ravine 
Okay? 
So going down to Indian Country? 
Have you got the graveyard written down ? 
In where? 
Okay 
{you want to go. . .  diagonally} Diagonally down 
I do. 
No, I don't 
{And across to?} The pyramid. 
Okay 
ATR. The 9 DAs ("illocutionary force types") used in the ATR Dialogue 
database task; some later models used an extended set of 15 DAs. 
Examples are from the English translations given by Nagata (1992). 
Tag Example 
PHATIC 
EXPRESSIVE 
RESPONSE 
PROMISE 
REQUEST 
INFORM 
QUESTIONIP 
QUESTIONREF 
QUESTIONCONF 
Hello 
Thank you 
That's right 
I will send you a registration form 
Please go to Kitaooji station by subway 
We are not giving any discount his time 
Do you have the announcement of the conference ? 
What should I do? 
You have already transferred the registration fee, right ? 
364 
Stolcke et al D ia logue Act  Mode l ing  
,.0 
C 
~'~ o 
% > 
.~  to 
.~~ ~ 
g,..l 
m m ~ 
c~8 ,~.~ ~ 
c ~ Z 
~m 
.~  K 
, -~  
NS~ 
~ ~.~ 
?? l l  ~ ~  
v 
r--~ ~ , O', ?~ 
365 
Computational Linguistics Volume 26, Number 3 
The use of n-grams to model the probabilities of DA sequences, or to predict 
upcoming DAs on-line, has been proposed by many authors. It seems to have been 
first employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto 
(1993, 1994) on the ATR Dialogue database. The model predicted upcoming DAs by 
using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 
2,722 DAs. Many others subsequently relied on and enhanced this n-grams-of-DAs 
approach, often by applying standard techniques from statistical language modeling. 
Reithinger et al (1996), for example, used deleted interpolation tosmooth the dialogue 
n-grams. Chu-Carroll (1998) uses knowledge of subdialogue structure to selectively 
skip previous DAs in choosing conditioning for DA prediction. 
Nagata and Morimoto (1993, 1994) may also have been the first to use word n- 
grams as a miniature grammar for DAs, to be used in improving speech recognition. 
The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke 
et al (1997), Reithinger and Klesen (1997), and Taylor et al (1998) all use variants of 
backoff, interpolated, orclass n-gram language models to estimate DA likelihoods. Any 
kind of sufficiently powerful, trainable language model could perform this function, of 
course, and indeed Alexandersson and Reithinger (1997) propose using automatically 
learned stochastic context-free grammars. Jurafsky, Shriberg, Fox, and Curl (1998) show 
that the grammar of some DAs, such as appreciations, can be captured by finite-state 
automata over part-of-speech tags. 
N-gram models are likelihood models for DAs, i.e., they compute the conditional 
probabilities of the word sequence given the DA type. Word-based posterior probability 
estimators are also possible, although less common. Mast et al (1996) propose the use 
of semantic lassification trees, a kind of decision tree conditioned on word patterns 
as features. Finally, Ries (1999a) shows that neural networks using only unigram fea- 
tures can be superior to higher-order n-gram DA models. Warnke et al (1999) and 
Ohler, Harbeck, and Niemann (1999) use related discriminative training algorithms 
for language models. 
Woszczyna nd Waibel (1994) and Suhm and Waibel (1994), followed by Chu- 
Carroll (1998), seem to have been the first to note that such a combination of word 
and dialogue n-grams could be viewed as a dialogue HMM with word strings as 
the observations. (Indeed, with the exception of Samuel, Carberry, and Vijay-Shanker 
(1998), all models listed in Table 14 rely on some version of this HMM metaphor.) 
Some researchers explicitly used HMM induction techniques to infer dialogue gram- 
mars. Woszczyna nd Waibel (1994), for example, trained an ergodic HMM using 
expectation-maximization o model speech act sequencing. Kita et al (1996) made 
one of the few attempts at unsupervised iscovery of dialogue structure, where a 
finite-state grammar induction algorithm is used to find the topology of the dialogue 
grammar. 
Computational pproaches to prosodic modeling of DAs have aimed to auto- 
matically extract various prosodic parameters--such as duration, pitch, and energy 
patterns--from the speech signal (Yoshimura et al \[1996\]; Taylor et al \[1997\]; Kompe 
\[1997\], among others). Some approaches model F0 patterns with techniques such as 
vector quantization and Gaussian classifiers to help disambiguate utterance types. An 
extensive comparison of the prosodic DA modeling literature with our work can be 
found in Shriberg et al (1998). 
DA modeling has mostly been geared toward automatic DA classification, and 
much less work has been done on applying DA models to automatic speech recog- 
nition. Nagata and Morimoto (1994) suggest conditioning word language models on 
DAs to lower perplexity. Suhm and Waibel (1994) and Eckert, Gallwitz, and Niemann 
(1996) each condition a recognizer LM on left-to-right DA predictions and are able to 
366 
Stolcke et al Dialogue Act Modeling 
show reductions in word error rate of 1% on task-oriented corpora. Most similar to 
our own work, but still in a task-oriented omain, the work by Taylor et al (1998) 
combines DA likelihoods from prosodic models with those from 1-best recognition 
output o condition the recognizer LM, again achieving an absolute reduction in word 
error rate of 1%, as disappointing as the 0.3% improvement in our experiments. 
Related computational tasks beyond DA classification and speech recognition have 
received even less attention to date. We already mentioned Warnke et al (1997) and 
Finke et al (1998), who both showed that utterance segmentation a d classification can 
be integrated into a single search process. Fukada et al (1998) investigate augmenting 
DA tagging with more detailed semantic "concept" tags, as a preliminary step toward 
an interlingua-based dialogue translation system. Levin et al (1999) couple DA clas- 
sification with dialogue game classification; dialogue games are units above the DA 
level, i.e., short DA sequences such as question-answer pairs. 
All the work mentioned so far uses statistical models of various kinds. As we have 
shown here, such models offer some fundamental dvantages, uch as modularity and 
composability (e.g., of discourse grammars with DA models) and the ability to deal 
with noisy input (e.g., from a speech recognizer) in a principled way. However, many 
other classifier architectures are applicable to the tasks discussed, in particular to DA 
classification. A nonprobabilistic approach for DA labeling proposed by Samuel, Car- 
berry, and Vijay-Shanker (1998) is transformation-based l arning (Brill 1993). Finally 
it should be noted that there are other tasks with a mathematical structure similar to 
that of DA tagging, such as shallow parsing for natural anguage processing (Munk 
1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which 
further techniques could be borrowed. 
How does the approach presented here differ from these various earlier models, 
particularly those based on HMMs? Apart from corpus and tag set differences, our 
approach differs primarily in that it generalizes the simple HMM approach to cope 
with new kinds of problems, based on the Bayes network representations depicted in 
Figures 2 and 4. For the DA classification task, our framework allows us to do classifi- 
cation given unreliable words (by marginalizing over the possible word strings corre- 
sponding to the acoustic input) and given nonlexical (e.g., prosodic) evidence. For the 
speech recognition task, the generalized model gives a clean probabilistic framework 
for conditioning word probabilities on the conversation context via the underlying DA 
structure. Unlike previous models that did not address peech recognition or relied 
only on an intuitive 1-best approximation, our model allows computation of the opti- 
mum word sequence by effectively summing over all possible DA sequences as well 
as all recognition hypotheses throughout the conversation, using evidence from both 
past and future. 
8. D iscuss ion and Issues for Future Research 
Our approach to dialogue modeling has two major components: tatistical dialogue 
grammars modeling the sequencing of DAs, and DA likelihood models expressing 
the local cues (both lexical and prosodic) for DAs. We made a number of significant 
simplifications to arrive at a computationally and statistically tractable formulation. 
In this formulation, DAs serve as the hinges that join the various model components, 
but also decouple these components through statistical independence assumptions. 
Conditional on the DAs, the observations across utterances are assumed to be inde- 
pendent, and evidence of different kinds from the same utterance (e.g., lexical and 
prosodic) is assumed to be independent. Finally, DA types themselves are assumed 
to be independent beyond a short span (corresponding to the order of the dialogue 
367 
Computational Linguistics Volume 26, Number 3 
n-gram). Further research within this framework can be characterized by which of 
these simplifications are addressed. 
Dialogue grammars for conversational speech need to be made more aware of the 
temporal properties of utterances. For example, we are currently not modeling the fact 
that utterances by the conversants may actually overlap (e.g., backchannels interrupt- 
ing an ongoing utterance). In addition, we should model more of the nonlocal aspects 
of discourse structure, despite our negative results so far. For example, a context-free 
discourse grammar could potentially account for the nested structures proposed in 
Grosz and Sidner (1986). 1? 
The standard n-gram models for DA discrimination with lexical cues are probably 
suboptimal for this task, simply because they are trained in the maximum likelihood 
framework, without explicitly optimizing discrimination between DA types. This may 
be overcome by using discriminative training procedures (Warnke et al 1999; Ohler, 
Harbeck, and Niemann 1999). Training neural networks directly with posterior prob- 
ability (Ries 1999a) seems to be a more principled approach and it also offers much 
easier integration with other knowledge sources. Prosodic features, for example, can 
simply be added to the lexical features, allowing the model to capture dependencies 
and redundancies across knowledge sources. Keyword-based techniques from the field 
of message classification should also be applicable here (Rose, Chang, and Lippmann 
1991). Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic 
cues into a single model, e.g., one that predicts the next DA based on DA history and 
all the local evidence. 
The study of automatically extracted prosodic features for DA modeling is likewise 
only in its infancy. Our preliminary experiments with neural networks have shown that 
small gains are obtainable with improved statistical modeling techniques. However, 
we believe that more progress can be made by improving the underlying features 
themselves, in terms of both better understanding of how speakers use them, and 
ways to reliably extract hem from data. 
Regarding the data itself, we saw that the distribution of DAs in our corpus limits 
the benefit of DA modeling for lower-level processing, in particular speech recognition. 
The reason for the skewed distribution was in the nature of the task (or lack thereof) in 
Switchboard. It remains to be seen if more fine-grained DA distinctions can be made 
reliably in this corpus. However, it should be noted that the DA definitions are really 
arbitrary as far as tasks other than DA labeling are concerned. This suggests using 
unsupervised, self-organizing learning schemes that choose their own DA definitions 
in the process of optimizing the primary task, whatever it may be. Hand-labeled DA 
categories may still serve an important role in initializing such an algorithm. 
We believe that dialogue-related tasks have much to benefit from corpus-driven, 
automatic learning techniques. To enable such research, we need fairly large, stan- 
dardized corpora that allow comparisons over time and across approaches. Despite 
its shortcomings, the Switchboard omain could serve this purpose. 
9. Conclusions 
We have developed an integrated probabilistic approach to dialogue act modeling for 
conversational speech, and tested it on a large speech corpus. The approach combines 
models for lexical and prosodic realizations of DAs, as well as a statistical discourse 
10 The inadequacy of n-gram models for nested discourse structures i  pointed out by Chu-Carroll (1998), 
although the suggested solution is a modified n-gram approach. 
368 
Stolcke et al Dialogue Act Modeling 
grammar. All components of the model are automatically trained, and are thus appli- 
cable to other domains for which labeled data is available. Classification accuracies 
achieved so far are highly encouraging, relative to the inherent difficulty of the task as 
measured by human labeler performance. We investigated several modeling alterna- 
tives for the components of the model (backoff n-grams and maximum entropy models 
for discourse grammars, decision trees and neural networks for prosodic lassification) 
and found performance largely independent of these choices. Finally, we developed a
principled way of incorporating DA modeling into the probability model of a contin- 
uous speech recognizer, by constraining word hypotheses using the discourse context. 
However, the approach gives only a small reduction in word error on our corpus, 
which can be attributed to a preponderance of a single dialogue act type (statements). 
Note 
The research described here is based on a 
project at the 1997 Workshop on Innovative 
Techniques in LVCSR at the Center for Speech 
and Language Processing at Johns Hopkins 
University (Jurafsky et al 1997; Jurafsky et 
al. 1998). The DA-labeled Switchboard tran- 
scripts as well as other project-related publi- 
cations are available at http://www.colorado. 
edu/ling/jurafsky/ws97/. 
Acknowledgments 
We thank the funders, researchers, and 
support staff of the 1997 Johns Hopkins 
Summer Workshop, especially Bill Byrne, 
Fred Jelinek, Harriet Nock, Joe Picone, 
Kimberly Shiring, and Chuck Wooters. 
Additional support came from the NSF via 
grants IRI-9619921 and IRI-9314967, and 
from the UK Engineering and Physical 
Science Research Council (grant 
GR/J55106). Thanks to Mitch Weintraub, to 
Susann LuperFoy, Nigel Ward, James Allen, 
Julia Hirschberg, and Marilyn Walker for 
advice on the design of the SWBD-DAMSL 
tag set, to the discourse labelers at CU 
Boulder (Debra Biasca, Marion Bond, Traci 
Curl, Anu Erringer, Michelle Gregory, Lori 
Heintzelman, Taimi Metzler, and Amma 
Oduro) and the intonation labelers at the 
University of Edinburgh (Helen Wright, 
Kurt Dusterhoff, Rob Clark, Cassie Mayo, 
and Matthew Bull). We also thank Andy 
Kehler and the anonymous reviewers for 
valuable comments on a draft of this paper. 
References 
Alexandersson, Jan and Norbert Reithinger. 
1997. Learning dialogue structures from a 
corpus. In G. Kokkinakis, N. Fakotakis, 
and E. Dermatas, editors, Proceedings ofthe 
5th European Conference on Speech 
Communication a d Technology, volume 4, 
pages 2,231-2,234. Rhodes, Greece, 
September. 
Anderson, Anne H., Miles Bader, Ellen G. 
Bard, Elizabeth H. Boyle, Gwyneth M. 
Doherty, Simon C. Garrod, Stephen D. 
Isard, Jacqueline C. Kowtko, Jan M. 
McAllister, Jim Miller, Catherine F. Sotillo, 
Henry S. Thompson, and Regina Weinert. 
1991. The HCRC Map Task corpus. 
Language and Speech, 34(4):351-366. 
Austin, J. L. 1962. How to do Things with 
Words. Clarendon Press, Oxford. 
Bahl, Lalit R., Frederick Jelinek, and 
Robert L. Mercer. 1983. A maximum 
likelihood approach to continuous speech 
recognition. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 
5(2):179-190, March. 
Bard, Ellen G., Catherine Sotillo, Anne H. 
Anderson, and M. M. Taylor. 1995. The 
DCIEM Map Task corpus: Spontaneous 
dialogues under sleep deprivation and 
drug treatment. In Isabel Trancoso and 
Roger Moore, editors, Proceedings ofthe 
ESCA-NATO Tutorial and Workshop on 
Speech under Stress, pages 25-28, Lisbon, 
September. 
Baum, Leonard E., Ted Petrie, George 
Soules, and Norman Weiss. 1970. A 
maximization technique occurring in the 
statistical analysis of probabilistic 
functions in Markov chains. The Annals of 
Mathematical Statistics, 41(1):164-171. 
Berger, Adam L., Stephen A. Della Pietra, 
and Vincent J. Della Pietra. 1996. A 
maximum entropy approach to natural 
language processing. Computational 
Linguistics, 22(1):39-71. 
Bourlard, Herv6 and Nelson Morgan. 1993. 
Connectionist Speech Recognition. A Hybrid 
Approach. Kluwer Academic Publishers, 
Boston, MA. 
Breiman, L., J. H. Friedman, R. A. Olshen, 
and C. J. Stone. 1984. Classification and 
Regression Trees. Wadsworth and Brooks, 
Pacific Grove, CA. 
369 
Computational Linguistics Volume 26, Number 3 
Bridle, J. S. 1990. Probabilistic interpretation 
of feedforward classification etwork 
outputs, with relationships to statistical 
pattern recognition. In F. Fogleman Soulie 
and J. Herault, editors, Neurocomputing: 
Algorithms, Architectures and Applications. 
Springer, Berlin, pages 227-236. 
Brill, Eric. 1993. Automatic grammar 
induction and parsing free text: A 
transformation-based approach. In 
Proceedings ofthe ARPA Workshop on Human 
Language Technology, Plainsboro, NJ, 
March. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: The Kappa statistic. 
Computational Linguistics, 22(2):249-254. 
Carlson, Lari. 1983. Dialogue Games: An 
Approach to Discourse Analysis. D. Reidel. 
Chu-Carroll, Jennifer. 1998. A statistical 
model for discourse act recognition in 
dialogue interactions. In Jennifer 
Chu-Carroll and Nancy Green, editors, 
Applying Machine Learning to Discourse 
Processing. Papers from the 1998 AAAI 
Spring Symposium. Technical Report 
SS-98-01, pages 12-17. AAAI Press, Menlo 
Park, CA. 
Church, Kenneth Ward. 1988. A stochastic 
parts program and noun phrase parser 
for unrestricted text. In Second Conference 
on Applied Natural Language Processing, 
pages 136-143, Austin, TX. 
Core, Mark and James Allen. 1997. Coding 
dialogs with the DAMSL annotation 
scheme. In Working Notes of the AAAI Fall 
Symposium on Communicative Action in 
Humans and Machines, pages 28-35, 
Cambridge, MA, November. 
Dermatas, Evangelos and George 
Kokkinakis. 1995. Automatic stochastic 
tagging of natural anguage texts. 
Computational Linguistics, 21(2):137-163. 
Eckert, Wieland, Florian Gallwitz, and 
Heinrich Niemann. 1996. Combining 
stochastic and linguistic language models 
for recognition of spontaneous speech. In 
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 423-426, Atlanta, GA, 
May. 
Finke, Michael, Maria Lapata, Alon Lavie, 
Lori Levin, Laura Mayfield Tomokiyo, 
Thomas Polzin, Klaus Ries, Alex Waibel, 
and Klaus Zechner. 1998. Clarity: 
Inferring discourse structure from speech. 
In Jennifer Chu-Carroll and Nancy Green, 
editors, Applying Machine Learning to 
Discourse Processing. Papers from the 1998 
AAAI Spring Symposium. Technical Report 
SS-98-01, pages 25-32. AAAI Press, Menlo 
Park, CA. 
Fowler, Carol A. and Jonathan Housum. 
1987. Talkers' signaling of "new" and 
"old" words in speech and listeners' 
perception and use of the distinction. 
Journal of Memory and Language, 26:489-504. 
Fukada, Toshiaki, Detlef Koll, Alex Waibel, 
and Kouichi Tanigaki. 1998. Probabilistic 
dialogue act extraction for concept based 
multilingual translation systems. In 
Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 6, pages 2,771-2,774, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Godfrey, J. J., E. C. Holliman, and 
J. McDaniel. 1992. SWITCHBOARD: 
Telephone speech corpus for research and 
development. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 517-520, San 
Francisco, CA, March. 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intention, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Hirschberg, Julia B. and Diane J. Litman. 
1993. Empirical studies on the 
disambiguation of cue phrases. 
Computational Linguistics, 19(3):501-530. 
Iyer, Rukmini, Mari Ostendorf, and J. Robin 
Rohlicek. 1994. Language modeling with 
sentence-level mixtures. In Proceedings of
the ARPA Workshop on Human Language 
Technology, pages 82-86, Plainsboro, NJ, 
March. 
Jefferson, Gail. 1984. Notes on a systematic 
deployment of the acknowledgement 
tokens 'yeah' and 'mm hm'. Papers in 
Linguistics, 17:197-216. 
Jekat, Susanne, Alexandra Klein, Elisabeth 
Maier, Ilona Maleck, Marion Mast, and 
Joachim Quantz. 1995. Dialogue acts in 
VERBMOBIL. Verbmobil-Report 65, 
Universit~it Hamburg, DFKI GmbH, 
Universit~it Erlangen, and TU Berlin, 
April. 
Jurafsky, Dan, Rebecca Bates, Noah Coccaro, 
Rachel Martin, Marie Meteer, Klaus Ries, 
Elizabeth Shriberg, Andreas Stolcke, Paul 
Taylor, and Carol Van Ess-Dykema. 1997. 
Automatic detection of discourse 
structure for speech recognition and 
understanding. In Proceedings ofthe IEEE 
Workshop on Speech Recognition and 
Understanding, pages 88-95, Santa 
Barbara, CA, December. 
Jurafsky, Daniel, Rebecca Bates, Noah 
Coccaro, Rachel Martin, Marie Meteer, 
Klaus Ries, Elizabeth Shriberg, Andreas 
Stolcke, Paul Taylor, and Carol Van 
370 
Stolcke et al Dialogue Act Modeling 
Ess-Dykema. 1998. Switchboard iscourse 
language modeling project final report. 
Research Note 30, Center for Language 
and Speech Processing, Johns Hopkins 
University, Baltimore, MD, January. 
Jurafsky, Daniel, Elizabeth Shriberg, and 
Debra Biasca. 1997. Switchboard-DAMSL 
Labeling Project Coder's Manual. 
Technical Report 97-02, University of 
Colorado, Institute of Cognitive Science, 
Boulder, CO. http://www.colorado.edu/ 
ling/jurafsky/manual.augustl.html. 
Jurafsky, Daniel, Elizabeth E. Shriberg, 
Barbara Fox, and Traci Curl. 1998. Lexical, 
prosodic, and syntactic ues for dialog 
acts. In Proceedings ofACL/COLING-98 
Workshop on Discourse Relations and 
Discourse Markers, pages 114-120. 
Association for Computational 
Linguistics. 
Katz, Slava M. 1987. Estimation of 
probabilities from sparse data for the 
language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech, and Signal Processing, 35(3):400-401, 
March. 
Kita, Kenji, Yoshikazu Fukui, Masaaki 
Nagata, and Tsuyoshi Morimoto. 1996. 
Automatic acquisition of probabilistic 
dialogue models. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 1, 
pages 196-199, Philadelphia, PA, October. 
Kompe, Ralf. 1997. Prosody in speech 
understanding systems. Springer, Berlin. 
Kowtko, Jacqueline C. 1996. The Function of 
Intonation in Task Oriented Dialogue. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh. 
Kuhn, Roland and Renato de Mori. 1990. A 
cache-base natural anguage model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6):570-583, June. 
Levin, Joan A. and Johanna A. Moore. 1977. 
Dialogue games: Metacommunication 
structures for natural anguage 
interaction. Cognitive Science, 1(4):395-420. 
Levin, Lori, Klaus Ries, Ann Thym~-Gobbel, 
and Alon Lavie. 1999. Tagging of speech 
acts and dialogue games in Spanish 
CallHome. In Towards Standards and Tools 
for Discourse Tagging (Proceedings ofthe ACL 
Workshop at ACL'99), pages 42-47, College 
Park, MD, June. 
Linell, Per. 1990. The power of dialogue 
dynamics. In Ivana Markov~ and Klaus 
Foppa, editors, The Dynamics of Dialogue. 
Harvester, Wheatsheaf, New York, 
London, pages 147-177. 
Mast, M., R. Kompe, S. Harbeck, 
A. Kiel~ling, H. Niemann, E. NOth, E. G. 
Schukat-Talamazzini, and V. Warnke. 
1996. Dialog act classification with the 
help of prosody. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 3, 
pages 1,732-1,735, Philadelphia, PA, 
October. 
Menn, Lise and Suzanne E. Boyce. 1982. 
Fundamental frequency and discourse 
structure. Language and Speech, 25:341-383. 
Meteer, Marie, Ann Taylor, Robert 
MacIntyre, and Rukmini Iyer. 1995. 
Dysfluency annotation stylebook for the 
Switchboard corpus. Distributed by LDC, 
ftp://ftp.cis.upenn.edu/pub/treebank/ 
swbd/doc/DFL-book.ps, February. 
Revised June 1995 by Ann Taylor. 
Morgan, Nelson, Eric Fosler, and Nikki 
Mirghafori. 1997. Speech recognition 
using on-line estimation of speaking rate. 
In G. Kokkinakis, N. Fakotakis, and E. 
Dermatas, editors, Proceedings ofthe 5th 
European Conference on Speech 
Communication a d Technology, volume 4, 
pages 2,079-2,082, Rhodes, Greece, 
September. 
Munk, Marcus. 1999. Shallow Statistical 
Parsing for Machine Translation. Diploma 
thesis, Carnegie Mellon University. 
Nagata, Masaaki. 1992. Using pragmatics to 
rule out recognition errors in cooperative 
task-oriented dialogues. In John J. Ohala, 
Terrance M. Nearey, Bruce L. Derwing, 
Megan M. Hodge, and Grace E. Wiebe, 
editors, Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 1, pages 647-650, Banff, Canada, 
October. 
Nagata, Masaaki and Tsuyoshi Morimoto. 
1993. An experimental statistical dialogue 
model to predict he speech act type of 
the next utterance. In Katsuhiko Shirai, 
Tetsunori Kobayashi, and Yasunari 
Harada, editors, Proceedings ofthe 
International Symposium on Spoken Dialogue, 
pages 83-86, Tokyo, November. 
Nagata, Masaaki and Tsuyoshi Morimoto. 
1994. First steps toward statistical 
modeling of dialogue to predict he 
speech act type of the next utterance. 
Speech Communication, 15:193-203. 
Ohler, Uwe, Stefan Harbeck, and Heinrich 
Niemann. 1999. Discriminative training of 
language model classifiers. In Proceedings 
of the 6th European Conference on Speech 
Communication a d Technology, volume 4, 
pages 1607-1610, Budapest, September. 
371 
Computational Linguistics Volume 26, Number 3 
Pearl, Judea. 1988. Probabilistic Reasoning in 
Intelligent Systems: Networks of Plausible 
Inference. Morgan Kaufmann, San Mateo, 
CA. 
Power, Richard J. D. 1979. The organization 
of purposeful dialogues. Linguistics, 
17:107-152. 
Rabiner, L. R. and B. H. Juang. 1986. An 
introduction to hidden Markov models. 
IEEE ASSP Magazine, 3(1):4-16, January. 
Reithinger, Norbert, Ralf Engel, Michael 
Kipp, and Martin Klesen. 1996. Predicting 
dialogue acts for a speech-to-speech 
translation system. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 2, 
pages 654-657, Philadelphia, PA, October. 
Reithinger, Norbert and Martin Klesen. 
1997. Dialogue act classification using 
language models. In G. Kokkinakis, N. 
Fakotakis, and E. Dermatas, editors, 
Proceedings ofthe 5th European Conference on 
Speech Communication a d Technology, 
volume 4, pages 2,235-2,238, Rhodes, 
Greece, September. 
Ries, Klaus. 1999a. HMM and neural 
network based speech act classification. In
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 497-500, Phoenix, AZ, 
March. 
Ries, Klaus. 1999b. Towards the detection 
and description of textual meaning 
indicators in spontaneous conversations. 
In Proceedings ofthe 6th European Conference 
on Speech Communication a d Technology, 
volume 3, pages 1,415--1,418, Budapest, 
September. 
Rose, R. C., E. I. Chang, and R. P. 
Lippmann. 1991. Techniques for 
information retrieval from voice 
messages. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 317-320, 
Toronto, May. 
Sacks, H., E. A. Schegloff, and G. Jefferson. 
1974. A simplest semantics for the 
organization of turn-taking in 
conversation. Language, 50(4):696-735. 
Samuel, Ken, Sandra Carberry, and 
K. Vijay-Shanker. 1998. Dialogue act 
tagging with transformation-based 
learning. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics, volume 2, 
pages 1,150-1,156, Montreal. 
Schegloff, Emanuel A. 1968. Sequencing in
conversational openings. American 
Anthropologist, 70:1,075-1,095. 
Schegloff, Emanuel A. 1982. Discourse as an 
interactional chievement: Some uses of 
'uh huh' and other things that come 
between sentences. In Deborah Tannen, 
editor, Analyzing Discourse: Text and Talk. 
Georgetown University Press, 
Washington, D.C., pages 71-93. 
Searle, J. R. 1969. Speech Acts. Cambridge 
University Press, London-New York. 
Shriberg, Elizabeth, Rebecca Bates, Andreas 
Stolcke, Paul Taylor, Daniel Jurafsky, 
Klaus Ries, Noah Coccaro, Rachel Martin, 
Marie Meteer, and Carol Van 
Ess-Dykema. 1998. Can prosody aid the 
automatic lassification of dialog acts in 
conversational speech? Language and 
Speech, 41(3-4):439--487. 
Shriberg, Elizabeth, Andreas Stolcke, Dilek 
Hakkani-Ti~r, and GOkhan Tiir. 2000. 
Prosody-based automatic segmentation f 
speech into sentences and topics. Speech 
Communication, 32(1-2). Special Issue on 
Accessing Information in Spoken Audio. 
To appear. 
Siegel, Sidney and N. John Castellan, Jr. 
1988. Nonparametric Statistics for the 
Behavioral Sciences. Second edition. 
McGraw-Hill, New York. 
Stolcke, Andreas and Elizabeth Shriberg. 
1996. Automatic linguistic segmentation 
of conversational speech. In H. Timothy 
Bunnell and William Idsardi, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 2, 
pages 1,005-1,008, Philadelphia, PA, 
October. 
Suhm, B. and A. Waibel. 1994. Toward better 
language models for spontaneous speech. 
In Proceedings ofthe International Conference 
on Spoken Language Processing, volume 2, 
pages 831-834, Yokohama, September. 
Taylor, Paul A. 2000. Analysis and synthesis 
of intonation using the tilt model. Journal 
of the Acoustical Society of America, 
107(3):1,697-1,714. 
Taylor, Paul A., Simon King, Stephen Isard, 
and Helen Wright. 1998. Intonation and 
dialog context as constraints for speech 
recognition. Language and Speech, 
41(3-4):489-508. 
Taylor, Paul A., Simon King, Stephen Isard, 
Helen Wright, and Jacqueline Kowtko. 
1997. Using intonation to constrain 
language models in speech recognition. In 
G. Kokkinakis, N. Fakotakis, and E. 
Dermatas, editors, Proceedings ofthe 5th 
European Conference on Speech 
Communication a d Technology, volume 5, 
pages 2,763-2,766, Rhodes, Greece, 
September. 
372 
Stolcke et al Dialogue Act Modeling 
Van Ess-Dykema, Carol and Klaus Ries. 
1998. Linguistically engineered tools for 
speech recognition error analysis. In 
Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 5, pages 2,091-2,094, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Viterbi, A. 1967. Error bounds for 
convolutional codes and an 
asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 13:260-269. 
Warnke, Volker, Stefan Harbeck, Elmar 
N0th, Heinrich Niemann, and Michael 
Levit. 1999. Discriminative estimation of 
interpolation parameters for language 
model classifiers. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 525-528, 
Phoenix, AZ, March. 
Warnke, Volker, R. Kompe, Heinrich 
Niemann, and Elmar NOth. 1997. 
Integrated ialog act segmentation a d 
classification using prosodic features and 
language models. In G. Kokkinakis, N. 
Fakotakis, and E. Dermatas, editors, 
Proceedings ofthe 5th European Conference on 
Speech Communication a d Technology, 
volume 1, pages 207-210, Rhodes, Greece, 
September. 
Weber, Elizabeth G. 1993. Varieties of 
Questions in English Conversation. John 
Benjamins, Amsterdam. 
Witten, Ian H. and Timothy C. Bell. 1991. 
The zero-frequency problem: Estimating 
the probabilities of novel events in 
adaptive text compression. IEEE 
Transations on Information Theory, 
37(4):1,085-1,094, July. 
Woszczyna, M. and A. Waibel. 1994. 
Inferring linguistic structure in spoken 
language. In Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 2, pages 847-850, Yokohama, 
September. 
Wright, Helen. 1998. Automatic utterance 
type detection using suprasegmental 
features. In Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 4, pages 1,403-1,406, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Wright, Helen and Paul A. Taylor. 1997. 
Modelling intonational structure using 
hidden Markov models. In Intonation: 
Theory, Models and Applications. Proceedings 
of an ESCA Workshop, pages 333-336, 
Athens, September. 
Yngve, Victor H. 1970. On getting a word in 
edgewise. In Papers from the Sixth Regional 
Meeting of the Chicago Linguistic Society, 
pages 567-577, Chicago, April. University 
of Chicago. 
Yoshimura, Takashi, Satoru Hayamizu, 
Hiroshi Ohmura, and Kazuyo Tanaka. 
1996. Pitch pattern clustering of user 
utterances in human-machine dialogue. In 
H. Timothy Bunnell and William Idsardi, 
editors, Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 2, pages 837-840, Philadelphia, 
PA, October. 
373 

Integrating Prosodic and Lexical Cues for 
Automatic Topic Segmentation 
G6khan Ttir* 
Bilkent University 
Andreas  Stolcke t 
SRI International 
Di lek Hakkani -Tt i r*  
Bilkent University 
E l izabeth Shr iberg t 
SRI International 
We present aprobabilistic model that uses both prosodic and lexical cues for the automatic seg- 
mentation of speech into topically coherent units. We propose two methods for combining lexical 
and prosodic information using hidden Markov models and decision trees. Lexical information is 
obtained from a speech recognizer, and prosodic features are extracted automatically from speech 
waveforms. We evaluate our approach on the Broadcast News corpus, using the DARPA-TDT 
evaluation metrics. Results how that the prosodic model alone is competitive with word-based 
segmentation methods. Furthermore, we achieve a significant reduction in error by combining 
the prosodic and word-based knowledge sources. 
1. Introduction 
Topic segmentation is the task of automatically dividing a stream of text or speech into 
topically homogeneous blocks. That is, given a sequence of (written or spoken) words, 
the aim of topic segmentation is to find the boundaries where topics change. Figure 1 
gives an example of a topic change boundary from a broadcast news transcript. Topic 
segmentation is an important task for various language understanding applications, 
such as information extraction and retrieval, and text summarization. In this paper, 
we present our work on automatic detection of topic boundaries from speech input 
using both prosodic and lexical information. 
Other automatic topic segmentation systems have focused on written text and 
have depended mostly on lexical information. This is problematic when segmenting 
speech. First, relying on word identities can propagate automatic speech recognizer 
errors to the topic segmenter. Second, speech lacks typographic ues, as shown in 
Figure 1: there are no headers, paragraphs, entence punctuation marks, or capitalized 
letters. Speech itself, on the other hand, provides an additional, nonlexical knowledge 
source through its durational, intonational, and energy characteristics, i.e., its prosody. 
Prosodic cues are known to be relevant o discourse structure in spontaneous 
speech (cf. Section 2.3) and can therefore be expected to play a role in indicating topic 
transitions. Furthermore, prosodic cues, by their nature, are relatively unaffected by 
word identity, and should therefore improve the robustness of lexical topic segmenta- 
tion methods based on automatic speech recognition. 
? Department of Computer Engineering, Bilkent University, Ankara, 06533, Turkey. E-mail: {tur, 
hakkani}@cs.bilkent.edu.tr. The research reported here was carried out while the authors were 
International Fellows at SRI International. 
t Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 
94025. E-maih {stolcke,ees}@speech.sri.com. 
Computational Linguistics Volume 27, Number 1 
? .. tens of thousands of people are homeless in northern china tonight after a powerful 
earthquake hit an earthquake r gistering six point two on the richter scale at least forty 
seven people are dead few pictures available from the region but we do know tem- 
peratures there will be very cold tonight minus seven degrees <TOPIC_CHANGE> 
peace talks expected to resume on monday in belfast northern ireland former u. s. sen- 
ator george mitchell is representing u. s. interests in the talks but it is another american 
center senator ather who was the focus of attention in northern ireland today here's 
a. b. c.'s richard gizbert he senator from america's best known irish catholic family 
is in northern ireland today to talk about peace and reconciliation a peace process 
does not mean asking unionists or nationalists to change or discard their identity or 
aspirations... 
Figure 1 
An example of a topic boundary in a broadcast news transcript. 
Topic segmentation research based on prosodic information has generally relied 
on hand-coded cues (with the notable exception of Hirschberg and Nakatani \[1998\]), 
or has not combined prosodic information with lexical cues (Litman and Passon- 
neau \[1995\] is one example where lexical information was combined with hand-coded 
prosodic features for a related task). Therefore, the work presented here is the first 
that combines automatic extraction of both lexical and prosodic information for topic 
segmentation. 
The general framework for combining lexical and prosodic cues for tagging speech 
with various kinds of "hidden" structural information is a further development of 
our earlier work on sentence segmentation and disfluency detection for spontaneous 
speech (Shriberg, Bates, and Stolcke 1997; Stolcke and Shriberg 1996; Stolcke et al 
1998), conversational dialogue tagging (Stolcke et al 2000), and information extraction 
from broadcast news (Hakkani-T~ir et al 1999). 
In the next section, we review previous work on topic segmentation. In Section 3, 
we describe our prosodic and language models as well as methods for combining 
them. Section 4 reports our experimental procedures and results. We close with some 
general discussion (Section 5) and conclusions (Section 6). 
2. Previous Work 
Work on topic segmentation is generally based on two broad classes of cues. On the 
one hand, one can exploit the fact that topics are correlated with topical content-word 
usage, and that global shifts in word usage are indicative of changes in topic. Quite 
independently, discourse cues, or linguistic devices such as discourse markers, cue 
phrases, syntactic onstructions, and prosodic signals are employed by speakers (or 
writers) as generic indicators of endings or beginnings of topical segments. Interest- 
ingly, most previous work has explored either one or the other type of cue, but only 
rarely both. In automatic segmentation systems, word usage cues are often captured 
by statistical language modeling and information retrieval techniques. Discourse cues, 
on the other hand, are typically modeled with rule-based approaches or classifiers 
derived by machine learning techniques (such as decision trees). 
2.1 Approaches Based on Word Usage 
Most automatic topic segmentation work based on text sources has explored topical 
word usage cues in one form or other. Kozima (1993) used mutual similarity of words 
in a sequence of text as an indicator of text structure. Reynar (1994) presented a method 
that finds topically similar regions in the text by graphically modeling the distribution 
32 
Ttir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
of word repetitions. The method of Hearst (1994, 1997) uses cosine similarity in a word 
vector space as an indicator of topic similarity. 
More recently, the U.S. Defense Advanced Research Projects Agency (DARPA) 
initiated the Topic Detection and Tracking (TDT) program to further the state of the 
art in finding and following new topics in a stream of broadcast news stories. One 
of the tasks in the TDT effort is segmenting a news stream into individual stories. 
Several of the participating systems rely essentially on word usage: Yamron et al 
(1998) model topics with unigram language models and their sequential structure with 
hidden Markov models (HMMs). Ponte and Croft (1997) extract related word sets for 
topic segments with the information retrieval technique of local context analysis, and 
then compare the expanded word sets. 
2.2 Approaches Based on Discourse and Combined Cues 
Previous work on both text and speech as found that cue phrases or discourse parti- 
cles (items uch as now or by the way), as well as other lexical cues, can provide valuable 
indicators of structural units in discourse (Grosz and Sidner 1986; Passonneau and Lit- 
man 1997, among others). 
In the TDT framework, the UMass HMM approach described in Allan et al (1998) 
uses an HMM that models the initial, middle, and final sentences of a topic segment, 
capitalizing on discourse cue words that indicate beginnings and ends of segments. 
Aligning the HMM to the data amounts to segmenting it.
Beeferman, Berger, and Lafferty (1999) combined a large set of automatically se- 
lected lexical discourse cues in a maximum entropy model. They also incorporated 
topical word usage into the model by building two statistical language models: one 
static (topic independent) and one that adapts its word predictions based on past 
words. They showed that the log likelihood ratio of the two predictors behaves as an 
indicator of topic boundaries, and can thus be used as an additional feature in the 
exponential model classifier. 
2.3 Approaches Using Prosodic Cues 
Prosodic cues form a subset of discourse cues in speech, reflecting systematic dura- 
tion, pitch, and energy patterns at topic changes and related locations of interest. A 
large literature in linguistics and related fields has shown that topic boundaries (as 
well as similar entities uch as paragraph boundaries in read speech, or discourse- 
level boundaries in spontaneous speech) are indicated prosodically in a manner that 
is similar to sentence or utterance boundaries--only stronger. Major shifts in topic 
typically show longer pauses, an extra-high F0 onset or "reset," a higher maximum 
accent peak, greater ange in F0 and intensity (Brown, Currie, and Kenworthy 1980; 
Grosz and Hirschberg 1992; Nakajima and Allen 1993; Geluykens and Swerts 1993; 
Ayers 1994; Hirschberg and Nakatani 1996; Nakajima nd Tsukada 1997; Swerts 1997) 
and shifts in speaking rate (Brubaker 1972; Koopmans-van geinum and van Donzel 
1996; Hirschberg and Nakatani 1996). Such cues are known to be salient for human 
listeners; in fact, subjects can perceive major discourse boundaries even if the speech 
itself is made unintelligible via spectral filtering (Swerts, Geluykens, and Terken 1992). 
Work in automatic extraction and computational modeling of these characteristics 
has been more limited, with most of the work in computational prosody modeling 
dealing with boundaries at the sentence level or below. However, there have been 
some studies of discourse-level boundaries in a computational framework. They differ 
in various ways, such as type of data (monologue or dialogue, human-human or
human-computer), type of features (prosodic and lexical versus prosodic only), which 
features are considered available (e.g., utterance boundaries or no boundaries), to 
33 
Computational Linguistics Volume 27, Number 1 
what extent features are automatically extractable and normalizable, and the machine 
learning approach used. Because of these vast difference, the overall results cannot be 
compared irectly to each other or to our work, but we describe three of the approaches 
briefly here. 
An early study by Litman and Passonneau (1995) used hand-labeled prosodic 
boundaries and lexical information, but applied machine learning to a training corpus 
and tested on unseen data. The researchers combined pause, duration, and hand-coded 
intonational boundary information with lexical information from cue phrases (such as 
and and so). Additional knowledge sources included complex relations, such as coref- 
erence of noun phrases. Work by Swerts and Ostendorf (1997) used prosodic features 
that in principle could be extracted automatically, such as pitch range, to classify ut- 
terances from human-computer task-oriented dialogue into two categories: initial or 
noninitial in the discourse segment. The approach used CART-style decision trees to 
model the prosodic features, as well as various lexical features that, in principle, could 
also be estimated automatically. In this case, utterances were presegmented, so the task 
was to classify segments rather than find boundaries in continuous peech; some of the 
features included, such as type of boundary tone, may not be easy to extract robustly 
across speaking styles. Finally, Hirschberg and Nakatani (1998) proposed a prosody- 
only front end for tasks such as audio browsing and playback, which could segment 
continuous audio input into meaningful information units. They used automatically 
extracted pitch, energy, and "other" features (such as the cross-correlation value used 
by the pitch tracker in determining the estimate of F0) as inputs to CART-style trees, 
and aimed to predict major discourse-level boundaries. They found various effects of 
frame window length and speakers, but concluded overall that prosodic cues could 
be useful for audio browsing applications. 
3. The Approach 
Topic segmentation i the paradigm used in this study and others (Allan et al 1998) 
proceeds in two phases. In the first phase, the input is divided into contiguous trings 
of words assumed to belong to the same topic. We refer to this step as chopping. For ex- 
ample, in textual input, the natural units for chopping are sentences (as can be inferred 
from punctuation and capitalization), since we can assume that topics do not change in 
mid sentence. 1 For continuous speech input, the choice of chopping criteria is less obvi- 
ous; we compare several possibilities in our experimental evaluation. Here, for simplic- 
ity, we will use "sentence" to refer to units of chopping, regardless of the criterion used. 
In the second phase, the sentences are further grouped into contiguous tretches 
belonging to one topic, i.e., the sentence boundaries are classified into topic bound- 
aries and nontopic boundaries. 2 Topic segmentation is thus reduced to a boundary 
classification problem. We will use B to denote the string of binary boundary classi- 
fications. Furthermore, our two knowledge sources are the (chopped) word sequence 
W and the stream of prosodic features F. Our approach aims to find the segmentation 
B with highest probability given the information in W and F 
argmax P( BI W, F ) (1) 
B 
using statistical modeling techniques. 
1 Similarly, it is sometimes assumed for topic segmentation purposes that topics change only at 
paragraph boundaries (Hearst 1997). 
2 We do not consider the problem of detecting recurring, discontinuous instances of the same topic, a 
task known as topic tracking in the TDT paradigm (Doddington 1998). 
34 
Ttir, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
In the following subsections, we first describe the prosodic model of the depen- 
dency between prosody F and topic segmentation B; then, the language model relating 
words W and B; and finally, two approaches for combining the models. 
3.1 Prosodic Modeling 
The job of the prosodic model is to estimate the posterior probability (or, alternatively, 
likelihood) of a topic change at a given word boundary, based on prosodic features ex- 
tracted from the data. For the prosodic model to be effective, one must devise suitable, 
automatically extractable f atures. Feature values extracted from a corpus can then be 
used in training probability estimators and to select a parsimonious subset of features 
for modeling purposes. We discuss each of these steps in turn in the following sections. 
3.1.1 Features. We started with a large collection of features capturing two major 
aspects of speech prosody, similar to our previous work (Shriberg, Bates, and Stolcke 
1997): 
Duration features: duration of pauses, duration of final vowels and final 
rhymes, and versions of these features normalized both for phone 
durations and speaker statistics. 3 
Pitch features: fundamental frequency (F0) patterns preceding and 
following the boundary, F0 patterns across the boundary, and pitch 
range relative to the speaker's baseline. We processed the raw F0 
estimates (obtained with ESPS signal processing software from Entropic 
Research Laboratory \[1993\]), with robustness-enhancing techniques 
developed by S6nmez et al (1998). 
We did not use amplitude- or energy-based features ince exploratory work showed 
these to be much less reliable than duration and pitch and largely redundant given the 
above features. One reason for omitting energy features is that, unlike duration and 
pitch, energy-related measurements vary with channel characteristics. Since channel 
properties vary widely in broadcast news, features based on energy measures can 
correlate with shows, speakers, and so forth, rather than with the structural locations 
in which we were interested. 
We included features that, based on the descriptive literature, should reflect breaks 
in the temporal and intonational contour. We developed versions of such features that 
could be defined at each interword boundary, and that could be extracted by com- 
pletely automatic means (no human labeling). Furthermore, the features were designed 
to be as independent of word identities as possible, for robustness to imperfect recog- 
nizer output. A brief characterization f the informative features for the segmentation 
task is given with our results in Section 4.6. Since the focus here is on computational 
modeling we refer the reader to a companion paper (Shriberg et al 2000) for a detailed 
description of the acoustic processing and prosodic feature extraction. 
3.1.2 Decision Trees. Any of a number of probabilistic lassifiers (such as neural net- 
works, exponential models, or naive Bayes networks) could be used as posterior prob- 
ability estimators. As in past prosodic modeling work (Shriberg, Bates, and Stolcke 
1997), we chose CART-style decision trees (Breiman et al 1984), as implemented by 
3 The rhyme is the part of a syllable that comprises the nuclear phone (typically a vowel) and any 
following phones. This is the part of the syllable most ypically affected by lengthening. 
35 
Computational Linguistics Volume 27, Number 1 
the IND package (Buntine and Caruana 1992), because of their ability to model feature 
interactions, to deal with missing features, and to handle large amounts of training 
data. The foremost reason for our preference for decision trees, however, is that the 
learned models can be inspected and diagnosed by human investigators. This ability 
is crucial for understanding what features are used and how, and for debugging the 
feature xtraction process itself. 4
Let Fi be the features extracted from a window around the ith potential topic 
boundary (chopping boundary), and let Bi be the boundary type (boundary/no-bound- 
ary) at that position. We trained ecision trees to predict he ith boundary type, i.e., to 
estimate P(\]3ilFi, W). The decision is only weakly conditioned on the word sequence 
W, insofar as some of the prosodic features depend on the phonetic alignment of the 
word models (which we will denote with Wt). We can thus expect he prosodic model 
estimates to be robust o recognition errors. The decision tree paradigm also allows us 
to add, and automatically select, other (nonprosodic) features that might be relevant 
to the task. 
3.1.3 Feature Selection. The greedy nature of the decision tree learning algorithm 
implies that larger initial feature sets can give worse results than smaller subsets. Fur- 
thermore, it is desirable to remove redundant features for computational efficiency 
and to simplify the interpretation f results. For this purpose we developed an itera- 
rive feature selection "wrapper" algorithm (John, Kohavi, and Pfleger 1994) that finds 
useful, task-specific feature subsets. The algorithm combines elements of a brute-force 
search with previously determined heuristics about good groupings of features. The 
algorithm proceeds in two phases: In the first phase, the number of features i  reduced 
by leaving out one feature at a time during tree construction. A feature whose removal 
increases performance is marked as to be avoided. The second phase then starts with 
the reduced feature set and performs a beam search over all possible subsets to max- 
imize tree performance. 
We used entropy reduction in the overall tree (after cross-validation pruning) as a 
metric for comparing alternative f ature subsets. Entropy reduction is the difference in 
entropy between the prior class distribution and the posterior distribution estimated 
by the tree, as measured on a held-out set; it is a more fine-grained metric than 
classification accuracy, and is also more relevant to the model combination approach 
described later. 
3.1.4 Training Data. To train the prosodic model, we automatically aligned and ex- 
tracted features from 70 hours (about 700,000 words) of the Linguistic Data Consortium 
(LDC) 1997 Broadcast News (BN) corpus. Topic boundary information determined by 
human labelers was extracted from the SGML markup that accompanies the word 
transcripts of this corpus. The word transcripts were aligned automatically with the 
acoustic waveforms to obtain pause and duration information, using the SRI Broadcast 
News recognizer (Sankar et al 1998). 
3.2 Lexical Modeling 
Lexical information i  our topic segmenter is captured by statistical language models 
(LMs) embedded in an HMM. The approach is an extension of the topic segmenter 
4 Interpreting large trees can be a daunting task. However, the decision questions ear the tree root are 
usually interpretable, or, when nonsensical, usually indicate problems with the data. Furthermore, as 
explained inSection 4.6, we have developed simple statistics that give an overview of feature usage 
throughout the tree. 
36 
Tfir, Hakkani-Tfir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
Figure 2 
Structure of the basic HMM developed by Dragon for the TDT Pilot Project. The labels on the 
arrows indicate the transition probabilities. TSP represents he topic switch penalty. 
developed by Dragon Systems for the TDT2 effort (Yamron et al 1998), which was 
based purely on topical word distributions. We extend it to also capture lexical and 
(as described in Section 3.3) prosodic discourse cues. 
3.2.1 Model Structure. The overall structure of the model is that of an HMM (Rabiner 
and Juang 1986) in which the states correspond to topic clusters Tj, and the obser- 
vations are sentences (or chopped units) W1 . . . . .  WN. The resulting HMM, depicted 
in Figure 2, forms a complete graph, allowing for transitions between any two topic 
clusters. Note that it is not necessary that the topic clusters correspond exactly to the 
actual topics to be located; for segmentation purposes, it is sufficient that two adjacent 
actual topics are unlikely to be mapped to the same induced cluster. The observation 
likelihoods for the HMM states, P(WilTj), represent the probability of generating a 
given sentence Wi in a particular topic cluster Tj. 
We automatically constructed 100 topic cluster LMs, using the multipass k-means 
algorithm described in Yamron et al (1998). Since the HMM emissions are meant o 
model the topical usage of words, but not topic-specific syntactic structures, the LMs 
37 
Computational Linguistics Volume 27, Number 1 
consist of unigram distributions that exclude stopwords (high-frequency function and 
closed-class words). To account for unobserved words, we interpolate the topic-cluster- 
specific LMs with the global unigram LM obtained from the entire training data. The 
observation likelihoods of the HMM states are then computed from these smoothed 
unigram LMs. 
All HMM transitions within the same topic cluster are given probability one, 
whereas all transitions between topics are set to a global topic switch penalty (TSP) 
that is optimized on held-out training data. The TSP parameter allows trading off 
between false alarms and misses. Once the HMM is trained, we use the Viterbi al- 
gorithm (Viterbi 1967; Rabiner and Juang 1986) to search for the best state sequence 
and corresponding segmentation. Note that the transition probabilities in the model 
are not normalized to sum to one; this is convenient and permissible since the out- 
put of the Viterbi algorithm depends only on the relative weight of the transition 
weights. 
We augmented the Dragon segmenter with additional states and transitions to 
also capture lexical discourse cues. In particular, we wanted to model the initial and 
final sentences in each topic segment, as these often contain formulaic phrases and 
keywords used by broadcast speakers (From Washington, this is . . . .  And  now . . .  ). We 
added two additional states, BEGIN and END, to the HMM (Figure 3) to model these 
sentences. Likelihoods for the BEGIN and END states are obtained as the unigram 
language model probabilities of the initial and final sentences, respectively, of the 
topic segments in the training data. Note that a single BEGIN and END state are 
shared for all topics. Best results were obtained by making traversal of these states 
optional in the HMM topology, presumably because some initial and final sentences 
are better modeled by the topic-specific LMs. 
The resulting model thus effectively combines the Dragon and UMass HMM topic 
segmentation approaches described in Allan et al (1998). In preliminary experiments, 
we observed a 5% relative reduction in segmentation error with initial and final states 
over the baseline HMM topology of Figure 2. Therefore, all results reported later use an 
HMM topology with initial and final states. Note that, since the topic-initial and topic- 
final states are optional, our training of the model is suboptimal. Instead of labeling all 
topic-initial and topic-final training sentences as data for the corresponding state, we 
would expect further improvements by training the HMM in unsupervised fashion 
using the Baum-Welch algorithm (Baum et al 1970; Rabiner and Juang 1986). 
3.2.2 Training Data. Topic unigram language models were trained from the pooled 
TDT Pilot and TDT2 training data (Cieri et al 1999), covering transcriptions of broad- 
cast news from January 1992 through June 1994 and from January 1998 through Febru- 
ary 1998, respectively. These corpora are similar in style, but do not overlap with the 
1997 LDC BN corpus from which we selected our prosodic training data and the eval- 
uaton test set. For training the language models, we removed stories with fewer than 
300 and more than 3,000 words, leaving 19,916 stories with an average length of 538 
words (including stopwords). 
3.3 Model Combination 
We are now in a position to describe how lexical and prosodic information can be 
combined for topic segmentation. As discussed before, the LMs in the HMM capture 
topical word usage as well as lexical discourse cues at topic transitions, whereas a 
decision tree models prosodic discourse cues. We expect hat these knowledge sources 
are largely independent, so their combination should yield significantly improved 
performance. 
38 
T~r, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
TSI 
TSP 
1 
? 
TSP 
TSP 
TSP 
TSP 
1 
TSP 
TSP 
Figure 3 
Structure of an HMM with topic BEGIN and END states. TSP represents he topic switch 
penalty. 
Below we present wo approaches for building a combined statistical model that 
performs topic segmentation using all available knowledge sources. For both ap- 
proaches it is convenient to associate a "boundary" pseudotoken with each potential 
topic boundary (i.e., with each sentence boundary). Correspondingly, we introduce into 
the HMM new states that emit these boundary tokens. No other states emit boundary 
tokens; therefore ach sentence boundary must align with one of the boundary states 
in the HMM. As shown in Figure 4, there are two boundary states for each topic 
cluster, one representing a topic transition and the other representing a topic-internal 
transition between sentences. Unless otherwise noted, the observation likelihoods for 
the boundary states are set to unity. 
The addition of boundary states allows us to compute the model's prediction of 
topic changes as follows: Let B1,.. ?, Bc denote the topic boundary states and, similarly, 
let N1,... ,  Nc denote the nontopic boundary states, where C is the number of topic 
clusters. Using the forward-backward algorithm for HMMs (Rabiner and Juang 1986), 
we can compute P(qi = BflW) and P(qi = NjlW), the posterior probabilities that one 
of these states is occupied at boundary i. The model's prediction of a topic boundary 
39 
Computational Linguistics Volume 27, Number i
TSP 
TSP 
TSP 
Figure 4 
Structure of the final HMM with fictitious boundary states used for combining language and 
prosodic models. In the figure, states B1, B2, . . . ,  B100 represent the presence of a topic 
boundary, whereas tates N1, N2, . . . ,  N100 represent topic-internal sentence boundaries. TSP 
is the topic switch penalty. 
is simply the sum over the corresponding state posteriors: 
c 
PHMM(Bi ~- yes\]W) = ~P(q i  = Bj lW) (2) 
j=l 
c 
PHMM(Bi = no lW ) = ~_~P(qi = N j lW)  
j=l 
= 1 - PHMM(Bi = yes\[W) (3) 
40 
Tiir, Hakkani-T(ir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
3.3.1 Model  Combination in the Decis ion Tree. Decision trees allow the training of 
a single classifier that takes both lexical and prosodic features as input, provided we 
can compactly encode the lexical information for the decision tree. We compute the 
posterior probability PHMM(Bi = yeslW) as shown above, to summarize the HMM's 
belief in a topic boundary based on all available lexical information W. The posterior 
value is then used as an additional input feature to the prosodic decision tree, which is 
trained in the usual manner. During testing, we declare a topic boundary whenever the 
tree's overall posterior estimate PDT(BilFi, W) exceeds ome threshold. The threshold 
may be varied to trade off false alarms for miss errors, or to optimize an overall cost 
function. 
Using HMM posteriors as decision tree features is similar in spirit to the knowl- 
edge source combination approaches used by Beeferman, Berger, and Lafferty (1999) 
and Reynar (1999), who also used the output of a topical word usage model as in- 
put to an overall classifier. In previous work (Stolcke et al 1998) we used the present 
approach as one of the knowledge source combination strategies for sentence and 
disfluency detection in spontaneous speech. 
3.3.2 Model  Combinat ion in the HMM. An alternative approach to knowledge source 
combination uses the HMM as the top-level model. In this approach, the prosodic 
decision tree is used to estimate likelihoods for the boundary states of the HMM, thus 
integrating the prosodic evidence into the HMM's segmentation decisions. 
More formally, let Q =- (rl, ql . . . . .  ri, qi,...,rN, qN) be a state sequence through 
the HMM. The model is constructed such that the states ri representing topic (or 
BEGIN/END) clusters alternate with the states qi representing boundary decisions. 
As in the baseline model, the likelihoods of the topic cluster states Tj account for the 
lexical observations: 
P(Wi\]ri ~- Tj) = P(WilTj) (4) 
as estimated by the unigram LMs. Now, in addition, we let the likelihood of the 
boundary state at position i reflect he prosodic observation Fi. Recall that, like Wi, Fi 
refers to complete sentence units; specifically, Fi denotes the prosodic features of the 
ith boundary between such units. 
P(Fi\]qi = Bj, W) = P(Fi\]Bi = yes, W) 
P(Fi\[qi = Nj, W) = P(FiIBi = no, W) j for all j = 1 . . . .  , C (5) 
Using this construction, the product of all state likelihoods will give the overall ike- 
lihood, accounting for both lexical and prosodic observations: 
N N 
1-\[ P(Wilri) I I  P(Filqi, W) = P(W, FIQ ) (6) 
i=1 i=1 
Applying the Viterbi algorithm to the HMM will thus return the most likely segmen- 
tation conditioned on both words and prosody, which is our goal. 
Although decomposing the likelihoods as shown allows prosodic observations to
be conditioned on the words W, we use only the phonetic alignment information Wt 
from the word sequence W in our prosodic models, ignoring the word identities, so 
as to make them more robust o recognition errors. 
The likelihoods P(FilBi, Wt) for the boundary states can now be obtained from the 
prosodic decision tree. Note that the decision tree estimates posteriors PDT(Bil\]2i, Wt). 
41 
Computational Linguistics Volume 27, Number 1 
These can be converted to likelihoods using Bayes rule as in 
P(Fi\[Bi, Wt) = P(FilWt)PDT (BilEG Wt) 
P(BiIWt) (7) 
The term P(FilWt) is a constant for all decisions Bi and can thus be ignored when 
applying the Viterbi algorithm. Next, we approximate P(BilWt) ,~ P(Bi), justified by the 
fact that the Wt contains information about start and end times of phones and words, 
but not directly about word identities. Instead of explicitly dividing the posteriors, 
1 we prefer to downsample the training set to make P(Bi = yes) = P(Bi = no) = ~. 
A beneficial side effect of this approach is that the decision tree models the lower- 
frequency events (topic boundaries) in greater detail than if presented with the raw, 
highly skewed class distribution. 
As is often the case when combining probabilistic models of different ypes, it is 
advantageous to weight the contributions of the language models and the prosodic 
trees relative to each other. We do so by introducing a tunable model combination 
weight (MCW), and by using PDT(FilBi, Wt) MCW as the effective prosodic likelihoods. 
The value of MCW is optimized on held-out data. 
4. Experiments and Results 
To evaluate our topic segmentation models, we carried out experiments in the TDT 
paradigm. We first describe our test data and the evaluation metrics used to compare 
model performance, then give the results we obtained with individual knowledge 
sources, followed by the results of the combined models. 
4.1 Test Data 
We evaluated our system on three hours (6 shows, about 53,000 words) of the 1997 
LDC BN corpus. The threshold for the model combination in the decision tree and 
the topic switch penalty were optimized on the larger development training set of 
104 shows, which includes the prosodic model training data. The MCW for the model 
combination in the HMM was optimized using a smaller held-out set of 10 shows of 
about 85,000 words total size, separate from the prosodic model training data. 
We used two test conditions: forced alignments using the true words, and recog- 
nized words as obtained by a simplified version of the SRI Broadcast News recognizer 
(Sankar et al 1998), with a word error rate of 30.5%. 
Our aim in these experiments was to use fully automatic recognition and pro- 
cessing wherever possible. For practical reasons, we departed from this strategy in 
two areas. First, for word recognition, we used the acoustic waveform segmentations 
provided with the corpus (which also included the location of non_news material, such 
as commercials and music). Since current BN recognition systems perform this seg- 
mentation automatically with very good accuracy and with only a few percentage 
points penalty in word error rate (Sankar et al 1998), we felt the added complication 
in experimental setup and evaluation was not justified. 
Second, for prosodic modeling, we used information from the corpus markup 
concerning speaker changes and the identity of frequent speakers (e.g., news anchors). 
Automatic speaker segmentation and labeling is possible, although not without errors 
(Przybocki and Martin 1999). Our use of speaker labels was motivated by the fact 
that meaningful prosodic features may require careful normalization by speaker, and 
unreliable speaker information would have made the analysis of prosodic feature 
usage much less meaningful. 
42 
Tfir, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
4.2 Evaluation Metrics 
We have adopted the evaluation paradigm used by the TDT2--Topic Detection and 
Tracking Phase 2 (Doddington 1998) program, allowing fair comparisons of various 
approaches both within this study and with respect to other ecent work. Segmentation 
accuracy was measured using TDT evaluation software from NIST, which implements 
a variant of an evaluation metric suggested by Beeferman, Berger, and Lafferty (1999). 
The TDT segmentation metric is different from those used in most previous topic 
segmentation work, and therefore merits some discussion. It is designed to work on 
data streams without any potential topic boundaries, uch as paragraph or sentence 
boundaries, being given a priori. It also gives proper partial credit to segmentation 
decisions that are close to actual boundaries; for example, placing a boundary one 
word from an actual boundary is considered a lesser error than if the hypothesized 
boundary is off by, say, 100 words. 
The evaluation metric reflects the probability that two positions in the corpus 
probed at random and separated by a distance of k words are correctly classified as 
belonging to the same story or not. If the two words belong to the same topic segment, 
but are erroneously claimed to be in different opic segments by the segmenter, then 
this will increase the system's false alarm probability. Conversely, if the two words are 
in different opic segments, but are erroneously marked to be in the same segment, 
this will contribute to the miss probability. The false alarm and miss rates are defined 
as averages over all possible probe positions with distance k. 
Formally, miss and false alarm rates are computed as 5 
PMis, = Y~s ~N--~lk d~yp (i, i + k) x (1 - dS~?f (i, i + k)) (8) 
E E Ns -k l ' '  ( i , i+k)) s i=1 , l -d~? i  
v"N~-k  (1 ? s ? ? PFalseAla~m -- Y'~s A..M=I \ -- dShyp( l" i + k)) x d~ef(Z, ~ + k) (9) 
v-,N~-k as (i i + k) Es  Z.~i=l ~ref  \ " 
where the summation is over all broadcast shows s and word positions i in the test 
corpus and where 
d sli,,={i if words i and j in show s are deemed by sys to be within the same story 
otherwise 
Here sys can be ref to denote the reference (correct) segmentation, or hyp to denote the 
segmenter's decision. 
An analogous metric is defined for audio sources, where segmentation decisions 
(same or different topic) are probed at a time-based istance A: 
-T~ -A  -s - 
PMiss = GsJt=o ahyp(t,t+A) x (1-d~?f(t,t+A))dt (10) 
~'  ~G-A(1  s Jr=0 -dScd( t , t+A) )dt  
f  -a( .  PFa,,?A,a .... = Gsat=O ,l -d~yp( t , t+A) )  x d~?/(t , t+A)dt  (11) 
-G -A  -s  - ~s J,=o a~z(t,t+n)dt 
5 The definitions are those from Doddington (1998), but have been simplified and edited for clarity. 
43 
Computational Linguistics Volume 27, Number 1 
Table 1 
Segmentation error rates for various chopping criteria, using true words of the larger 
development data set. 
Chopping Criterion P Miss P FalseA larm C Seg 
FIXED 0.5688 0.0639 0.2153 
TURN 0.6737 0.0436 0.2326 
SENTENCE 0.5469 0.0557 0.2030 
PAUSE 0.5111 0.0688 0.2002 
where the integration is over the entire duration of all stories of the shows in the test 
corpus, and where 
if times tl and t2 in show s are deemed by sys to 
be within the same story 
otherwise 
We used the same parameters as used in the official TDT2 evaluation: k = 50 
and A = 15 seconds. Furthermore, again following NIST's evaluation procedure, we 
combine miss and false alarm rates into a single segmentation cost metric 
Cseg : CMiss X PMiss X P~eg + CFalseAlarm X PFalseAlarm ? (1 - P~?9) (12) 
where the CMis~ = 1 is the cost of a miss, CFalseAlarm : 1 is the cost of a false alarm, 
and Pseg = 0.3 is the a priori probability of a segment being within an interval of k 
words or A seconds on the TDT2 training corpus. 6
4.3 Chopping 
Unlike written text, the output of the automatic speech recognizer contains no sentence 
boundaries. Therefore, chopping text into (pseudo)sentences is a nontrivial problem 
when processing speech. Some presegmentation into roughly sentence-length units is 
necessary since otherwise the observations associated with HMM states would com- 
prise too few words to give robust likelihoods of topic choice, causing poor perfor- 
mance. 
We investigated chopping criteria based on a fixed number of words (FIXED), at 
speaker changes (TURN), at pauses (PAUSE), and, for reference, at actual sentence 
boundaries (SENTENCE) obtained from the transcripts. Table 1 gives the error rates 
for the four conditions, using the true word transcripts of the larger development 
data set. For the PAUSE condition, we empirically determined an optimal minimum 
pause duration threshold to use. Specifically, we considered pauses exceeding 0.575 
of a second as potential topic boundaries in this (and all later) experiments. For the 
FIXED condition, a block length of 10 words was found to work best. 
We conclude that a simple prosodic feature, pause duration, is an excellent criterion 
for the chopping step, giving comparable or better performance than standard sentence 
boundaries. Therefore, we used pause duration as the chopping criterion in all further 
experiments. 
6 Another parameter in the NIST evaluation is the deferral period, i.e., the amount of look-ahead before 
a segmentation decision is made. In all our experiments, we allowed unlimited deferral, effectively 
until the end of the news show being processed. 
44 
Tfir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
Table 2 
Summary of error rates with the language model only (LM), the prosody model only (PM), the 
combined ecision tree (CM-DT), and the combined HMM (CM-HMM). (a) shows word-based 
error metrics, 00) shows time-based error metrics. In both cases a "chance" classifier that labels 
all potential boundaries as nontopic would achieve 0.3 weighted segmentation cost. 
(a) Error Rates on Forced Alignments Error Rates on Recognized Words 
Model PMiss PFalseAlarm Cseg PMiss PFalseAlarm Cseg 
Chance 1.0 0.0 0.3 1.0 0.0 0.3 
LM 0.4847 0.0630 0.1895 0.4978 0.0577 0.1897 
PM 0.4130 0.0596 0.1657 0.4125 0.0705 0.1731 
CM-DT 0.4677 0.0260 0.1585 0.4891 0.0146 0.1569 
CM-HMM 0.3339 0.0536 0.1377 0.3748 0.0450 0.1438 
(b) Error Rates on Forced Alignments Error Rates on Recognized Words 
Model  PMiss PFalseAlarm Cseg PMiss PFalseAlarm Cseg 
Chance 1.0 0.0 0.3 1.0 0.0 0.3 
LM 0.5260 0.0490 0.1921 0.5361 0.0415 0.1899 
PM 0.3503 0.0892 0.1675 0.3846 0.0737 0.1669 
CM-DT 0.5136 0.0210 0.1688 0.5426 0.0125 0.1715 
CM-HMM 0.3426 0.0496 0.1375 0.3746 0.0475 0.1456 
4.4 Source-Specific Model Tuning 
As mentioned earlier, the segmentation models contain global parameters (the topic 
transition penalty of the HMM and the posterior threshold for the combined ecision 
tree) to trade false alarms for miss errors. Optimal settings for these parameters depend 
on characteristics of the source, in particular on the relative frequency of topic changes. 
Since broadcast news programs come from identified sources, it is useful and legitimate 
to optimize these parameters for each show type .  7 We therefore optimized the global 
parameter for each model to minimize the segmentation cost on the training corpus 
(after training all other model parameters in a source-independent fashion). 
Compared to a baseline using source-independent global TSP and threshold, the 
source-dependent models showed between 5% and 10% relative error reduction. All 
results reported below use the source-dependent approach. 
4.5 Segmentation Results 
Table 2 shows the results for both individual knowledge sources (words and prosody), 
as well as for the combined models (decision tree and HMM). It is worth noting 
that the prosody-only results were obtained by running the combined HMM without 
language model likelihoods; this approach gave better performance than using the 
prosodic decision trees directly as classifiers. 
Both word- and time-based metrics are given; they exhibit generally very similar 
results. Another dimension of the evaluation is the use of correct word transcripts 
(forced alignments) versus automatically recognized words. Again, results along this 
dimension are very similar, with some exceptions noted below. 
Comparing the individual knowledge sources, we observe that prosody alone does 
somewhat better than the word-based HMM alone. The types of errors made differ 
7 Shows in the 1997 BN corpus come from eight sources: ABC World News Tonight, CNN Headline 
News, CNN Early Prime, PRI The World, CNN Prime News, CNN The World Today, C-SPAN Public 
Policy, and C-SPAN Washington Journal. Six of these occurred in the test set. 
45 
Computational Linguistics Volume 27, Number 1 
consistently: the prosodic model has a higher false alarm rate, while the word-LMs 
have more miss errors. The prosodic model shows more false alarms because regular 
sentence boundaries often show characteristics similar to those of topic boundaries. It
also suggests that both models could be combined by letting the prosodic model select 
candidate topic boundaries that would then be filtered using lexical information. 
The combined models generally improve on the individual knowledge sources,  
In the word-based evaluation, the combined decision tree (DT) reduced overall seg- 
mentation cost by 19% over the language model on true words (17% on recognized 
words). The combined HMM gave even better esults: 27% and 24% improvement in
the error rate over the language model for true and recognized words, respectively. 
Looking again at the breakdown of errors, we can see that the two model combina- 
tion approaches work quite differently: the combined DT has about the same miss rate 
as the LM, but a lower false alarms rate. The combined HMM, by contrast, combines 
a miss rate as low as (or lower than) that of the prosodic model with the lower false 
alarm rate of the LM, suggesting that the functions of the two knowledge sources are 
complementary, as discussed above. Furthermore, the different error patterns of the 
two combination approaches suggest hat further error reductions could be achieved 
by combining the two hybrid models. 9
The trade-off between false alarms and miss probabilities is shown in more de- 
tail in Figure 5, which plots the two error metrics against each other. Note that the 
false alarm rate does not reach one because the segmenter is constrained by the chop- 
ping algorithm: the pause criterion prevents the segmenter f om hypothesizing topic 
boundaries everywhere. 
4.6 Decision Tree for the Prosody-Only Model 
Feature subset selection was run with an initial set of 73 potential features, which the 
algorithm reduced to a set of 7 nonredundant features helpful for the topic segmen- 
tation task. The full decision tree learned is shown in Figure 6. We can identify four 
different kinds of features used in the tree, listed below. For each feature type, we give 
the feature names found in the tree and the relative feature usage, an approximate 
measure of feature importance (Shriberg, Bates, and Stolcke 1997). Relative feature 
usage is computed as the relative frequency with which features of a given type are 
queried in the tree, over a held-out est set. 
. 
. 
Pause duration (PhU_DUR, 42.7% usage). This feature is the duration of 
the nonspeech interval occurring at the boundary. The importance of 
pause duration is underestimated here because, as explained earlier, 
pause durations are already used during the chopping process, so that 
the decision tree is applied only to boundaries exceeding a certain 
duration. Separate xperiments using boundaries below our chopping 
threshold show that the tree also distinguishes shorter pause durations 
for segmentation decisions. 
FO differences across the boundary (FOK_LRd~EAN_KBASELN and 
FOK_WRD_DIFF_MNMI~_NG, 35.9% usage). These features compare the mean 
8 The exception is the time-based evaluation of the combined ecision tree. We found that the posterior 
probability threshold optimized on the training set works poorly on the test set for this model 
architecture and the time-based evaluation. The threshold that is optimal on the test set achieves 
Csea = 0.1651. Section 4.7 gives a possible xplanation for this result. 
9 Such a combination ofcombined models was suggested by one of the reviewers; we hope to pursue it 
in future research. 
46 
Ti.ir, Hakkani-Ttir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
.~..~ 
,.Q 
< 
r.,g3 
0.6 
0.4 
0.2 
0.0 
* * LM 
\[\] \[\] PM 
a ~ CM-DT 
: CM-HMM 
0.2 0.4 0.6 0.8 1.0 
M iss  P robab i l i ty  
Figure 5 
False alarm versus miss probabilities (word-based metrics) for automatic topic segmentation 
from known words (forced alignments). The segmenters used were a words-only HMM (LM), 
a prosody-only HMM (PM), a combined ecision tree (CM-DT), and a combined HMM 
(CM-HMM). 
. 
F0 of the word preceding the boundary (measured from voiced regions 
within that word) to either the speaker's estimated baseline F0 
(FOK_LR_MEAN_KBASELN) or to the mean F0 of the word following the 
boundary (FOK_WRD_DIFF_.MNMN_N). Both features were computed based on 
a log-normal scaling of F0. Other measures (such as minimum or 
maximum F0 in the word or preceding window) as well as other 
normalizations (based on F0 toplines, or non-log-based scalings) were 
included in the initial feature set, but were not selected in the 
best-performing tree. The baseline feature captures a pitch range effect, 
and is useful at boundaries where the speaker changes (since range here 
is compared only within-speaker). The second feature captures the 
relative size of the pitch change at the boundary, but of course is not 
meaningful at speaker boundaries. 
Turn features (TURN_F and TURN_TIME, 14.6% usage). These features 
reflect he change of speakers. TURN_F indicates whether a speaker 
47 
Computational Linguistics Volume 27, Number 1 
/ 
z 
v 
~a 
z 
z z 
v 
i 
O 
48 
Tar, Hakkani-Ti~r, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
. 
change occurred at the boundary, while TURN_TIME measures the time 
passed since the start of the current urn. 
Gender (GEN, 6.8% usage). This feature indicates the speaker gender 
right before a potential boundary. 
Inspection of the tree reveals that the purely prosodic features (pause duration and 
F0 differences) are used as the prosody literature suggests. The longer the observed 
pause, the more likely a boundary corresponds to a topic change. Also, the closer a 
speaker comes to his or her F0 baseline, or the larger the difference to the F0 following 
a boundary, the more likely a topic change occurs. These features thus correspond 
to the well-known phenomena of boundary tones and pitch reset that are generally 
associated with sentence boundaries (Vaissi6re 1983). We found these indicators of 
sentences boundaries to be particularly pronounced at topic boundaries. 
While turn and gender features are not prosodic features per se, they do interact 
closely with them since prosodic measurements must be informed by and carefully 
normalized for speaker identity and gender, and it is therefore natural to include them 
in a prosodic lassifier. 1?Not surprisingly, we find that turn boundaries are positively 
correlated with topic boundaries, and that topic changes become more likely the longer 
a turn has been going on. 
Interestingly, speaker gender is used by the decision tree for several reasons. One 
reason is stylistic differences between males and females in the use of F0 at topic 
boundaries. This is true even after proper normalization, e.g., equatIng the gender- 
specific nontopic boundary distributions. In addition, we found that nontopic pauses 
(i.e., chopping boundaries) are more likely to occur in male speech. It could be that 
male speakers in BN are assigned longer topic segments on average, or that male 
speakers are more prone to pausing in general, or that male speakers dominate the 
spontaneous speech portions, where pausing is naturally more frequent. The details 
of this gender effect await further study. 
4.7 Decision Tree for the Combined Model 
Figure 7 depicts the decision tree that combines the HMM language model topic deci- 
sions with prosodic features (see Section 3.3.1). Again, we list the features used with 
their relative feature usages. 
1. Language model posterior (POST_TOPIC, 49.3% usage). This is the 
posterior probability P(Bi = yeslW) computed from the HMM. 
2. Pause duration (PAU_DUR, 49.3% usage). This feature is the same as 
described for the prosody-only model. 
3. FO differences across the boundary (FOK_WRD_DIFF_HILO_N and 
FOK_LR_MEAN_KBASELN, 1.4% usage). These features are similar to those 
found for the prosody-only tree. The only difference is that for the first 
feature, the comparison of FO values across the boundary is done by 
taking the max imum FO of the previous word and the min imum FO of 
the following word, rather than the mean for both cases. 
10 For example, the features that measure F0 differences across boundaries do not make sense if the 
speaker changes atthe boundary. Accordingly, we made such features undefined for the decision tree 
at turn boundaries. 
49 
Computational Linguistics Volume 27, Number 1 
POST_TOPIC >= -0.083984 
r PAU_DUR >= 1058.3 PAU_DUR <82.5 ~PAU_DUR >= 82.5 
KBASELN < 0 .1977~R MEAN KBASELN >= 0.19777 
~_WRD_DIFF_HILO_N <-0 .02498~RD DIFF HILO N >=-0.024989 
Figure 7 
The decision tree of the combination model. 
The decision tree found for the combined task is smaller and uses fewer features 
than the one trained with prosodic features only, for two reasons. First, the LM poste- 
rior feature is found to be highly informative, superseding the selection of many of the 
low-frequency features previously found. Furthermore, as explained in Section 3.3.2, 
the prosody-only tree was trained on a downsampled dataset that equalizes the priors 
for topic and nontopic boundaries, as required for integration i to the HMM. A wel- 
come side effect of this procedure is that it forces the tree to model the less frequent 
class (topic boundaries) in much greater detail than if the tree were trained on the raw 
class distribution, as is the case here. 
Because of its small size, the tree in Figure 7 is particularly easy to interpret. The 
top-level split is based on the LM posterior. The right branch handles cases where 
words are highly indicative of a topic boundary. However, for short pauses, the tree 
queries further prosodic features to prevent false alarms. Specifically, short pauses 
must be accompanied both by an F0 close to the speaker's baseline and by a large 
F0 reset o be deemed topic boundaries. Conversely, if the LM posteriors are low (left 
top-level branch), but the pause is very long, the tree still outputs a topic boundary. 
4.8 Comparison of Model Combination Approaches 
Results indicate that the model combination approach using an HMM as the top-level 
model works better than the combined ecision tree. While this result deserves more 
investigation, we can offer some preliminary insights. 
We found it difficult to set the posterior probability thresholds for the combined 
decision tree in a robust way. As shown by the CM-DT curve in Figure 5, there is a 
large jump in the false alarm/miss trade-off for the combined tree, in contrast to the 
combined HMM approach, which controls the trade-off by a changing topic switch 
penalty. This occurs because posterior probabilities from the decision tree do not vary 
smoothly; rather, they vary in steps corresponding to the leaves of the tree. The dis- 
50 
Tfir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
Table 3 
Segmentation error rates with the language model only (LM), the combined HMM using all 
prosodic features (CM-HMM-all), the combined HMM using only pause duration and turn 
features (CM-HMM-pause-turn), and using only pause duration, turn, and gender features 
(CM-HMM-pause-turn-gender). 
Model C~ex 
LM 0.1895 
CM-HMM-pause-turn 0.1519 
CM-HMM-pause-turn-gender 0.1511 
CM-HMM-all 0.1377 
continuous character of the thresholded variable makes it hard to estimate a threshold 
on the training data that performs robustly on the test data. This could account for 
the poor result on the time-based metrics for the combined tree (where the threshold 
optimized on the training data was far from optimal on the test set; see footnote 8). 
The same phenomenon is reflected in the fact that the prosody-only tree gave better 
results when embedded in an HMM without LM likelihoods than when used by itself 
with a posterior threshold. 
4.9 Contributions of Different Feature Types 
We saw in Section 4.6 that pause duration is by far the single most important feature in 
the prosodic decision tree. Furthermore, speaker changes are queried almost as often 
as the F0-related features. Pause durations can be obtained using standard speech rec- 
ognizers, and are in fact used by many current TDT systems (see Section 4.10). Speaker 
changes are not prosodic features per se, and would be detected independently from 
the prosodic features proper. To determine if prosodic measurements beyond pause 
and speaker information improve topic segmentation accuracy, we tested systems that 
consisted of the HMM with the usual topic LMs, plus a decision tree that had ac- 
cess only to various subsets of pause- and speaker-related features, without using 
any of the F0-based features. Decision tree and HMM were combined as described in 
Section 3.3.2. 
Table 3 shows the results of the system using only topic language models (LM) 
as well as combined systems using all prosodic features (CM-HMM-all), only pause 
duration and turn features (CM-HMM-pause-turn), and using only pause duration, 
turn, and gender features (CM-HMM-pause-turn-gender). These results how that by 
using only pause duration, turn, and gender features, it is indeed possible to obtain 
better esults (20% reduced segmentation cost) than with the lexical model alone, with 
gender making only a minor contribution. However, we also see that a substantial 
further improvement (9% relative) is obtained by adding F0 features to the prosodic 
model. 
4.10 Results Compared to Other Approaches 
Because our work focused on the use of prosodic information and required detailed 
linguistic annotations ( uch as sentence punctuation, turn boundaries, and speaker 
labels), we used data from the LDC 1997 BN corpus to form the training set for the 
prosodic models and the (separate) test set used for evaluation. This choice was crucial 
for the research, but unfortunately complicates a quantitative comparison ofour results 
to other TDT segmentation systems. The recent TDT2 evaluation used a different set 
of broadcast news data that postdated the material we used, and was generated by 
a different speech recognizer (although with a similar word error rate) (Cieri et al 
51 
Computational Linguistics Volume 27, Number 1 
Table 4 
Word-based segmentation error rates for different corpora. Note that a hand-transcribed 
(forced alignment) version of the TDT2 test set was not available. 
Error Rates on Forced Alignments Error Rates on Recognized Words 
Test Set PMis~ PFalseAlarm Cseg PMiss PFalseAlarrn Cseg 
TDT2 NA NA NA 0.5509 0.0694 0.2139 
BN'97 0.4685 0.0817 0.1978 0.5128 0.0683 0.2017 
1999). Nevertheless we have attempted to calibrate our results with respect o these 
TDT2 results, n We have not tried to compare our results to research outside the TDT 
evaluation framework. In fact, other evaluation methodologies differ too much to allow 
meaningful quantitative comparisons across publications. 
We wanted to ensure that the TDT2 evaluation test set was comparable in seg- 
mentation difficulty to our test set drawn from the 1997 BN corpus, and that the TDT2 
metrics behaved similarly on both sets. To this end, we ran an early version of our 
words-only segmenter on both test sets. As shown in Table 4, not only are the results 
on recognized words quite close, but the optimal false alarm/miss trade-off is similar 
as well, indicating that the two corpora have roughly similar topic granularities. 
While the full prosodic omponent of our topic segmenter was not applied to the 
TDT2 test corpus, we can compare the performance of a simplified version of SRI's 
segmenter to other evaluation systems (Fiscus et al 1999). The two best-performing 
systems in the evaluation were those of CMU (Beeferman, Berger, and Lafferty 1999) 
with Cse9 = 0.1463, and Dragon (Yamron et al 1998; van Mulbregt et al 1999) with 
Cse9 = 0.1579. The SRI system achieved Cs~g = 0.1895. All systems in the evaluation, 
including ours, used only information from words and pause durations determined 
by a speech recognizer. 
A good reference to calibrate our performance is the Dragon system, from which 
we borrowed the lexical HMM segmentation framework. Dragon made adjustments 
in its lexical modeling that account for the improvements relative to the basic HMM 
structure on which our system is based. As described by van Mulbregt et al (1999), 
a significant segmentation error reduction was obtained from optimizing the number 
of topic clusters (kept fixed at 100 in our system). Second, Dragon introduced more 
supervision into the model training by building separate LMs for segments that had 
been hand-labeled asnot related to news (such as sports and commercials) in the TDT2 
training corpus, which also resulted in substantial improvements. Finally, Dragon used 
some of the TDT2 training data for tuning the model to the specifics of the TDT2 
corpus. 
In summary, the performance ofour combined lexical-prosodic system with Cs?9 = 
0.1438 is competitive with the best word-based systems reported to date. More impor- 
tantly, since we found the prosodic and lexical knowledge sources to complement each 
other, and since Dragon's improvements for TDT2 were confined to a better modeling 
of the lexical information, we would expect hat adding these improvements o our 
combined segmenter would lead to a significant improvement in the state of the art. 
11 Since our study was conducted, a third round of TDT benchmarks (TDT3) has taken place (NIST 1999). 
However, for TDT3, the topic segmentation evaluation metric was modified and the most recent results 
are thus not directly comparable with those from TDT2 or the present study, 
52 
Tiir, Hakkani-Ti.ir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
5. Discussion 
Results so far indicate that prosodic information provides an excellent source of in- 
formation for automatic topic segmentation, both by itself and in conjunction with 
lexical information. Pause duration, a simple prosodic feature that is readily available 
as a by-product of speech recognition, proved highly effective in the initial chopping 
phase, and was the most important feature used by prosodic decision trees. Additional, 
pitch-based prosodic features are also effective as features in the decision tree. 
The results obtained with recognized words (at 30% word error rate) did not differ 
greatly from those obtained with correct word transcripts. No significant degradation 
was found with the words-only segmentation model, while the best combined model 
exhibited about a 5% error increase with recognized words. The lack of degradation 
on the words-only model may be partly due to the fact that the recognizer generally 
outputs fewer words than contained in the correct ranscripts, biasing the segmenter 
toward a lower false alarm rate. Still, part of the appeal of prosodic segmentation is 
that it is inherently robust o recognition errors. This characteristic makes it even more 
attractive for use in domains with higher error rates due to poor acoustic onditions or 
more conversational speaking styles. It is especially encouraging that the prosody-only 
segmenter achieved competitive performance. 
It was fairly straightforward tomodify the original Dragon HMM segmenter (Yam- 
ron et al 1998), which is based purely on topical word usage, to incorporate discourse 
cues, both lexical and prosodic. The addition of these discourse cues proved highly 
effective, especially in the case of prosody. The alternative knowledge source combi- 
nation approach, using HMM posterior probabilities as decision tree inputs, was also 
effective, although less so than the HMM-based approach. Note that the HMM-based 
integration, as implemented here, makes more stringent assumptions about the in- 
dependence of lexical and prosodic cues. The combined decision tree, on the other 
hand, has some ability to model dependencies between lexical and prosodic cues. The 
fact that the HMM-based combination approach gave the best results is thus indirect 
evidence that lexical and prosodic knowledge sources are indeed largely independent. 
Apart from the question of probabilistic independence, it seems that lexical and 
prosodic models are also complementary in the errors they make. This is manifested 
in the different distributions of miss and false alarm errors discussed in Section 4.5. 
It is also easy to find examples where the two models make complementary errors. 
Figure 8 shows two topic boundaries that are missed by one model but not the other. 
Several aspects of our model are preliminary or suboptimal in nature and can be 
improved. Even when testing on recognized words, we used parameters optimized 
on forced alignments. This is suboptimal but convenient, since it avoids the need to 
run word recognition on the relatively large training set. Since results on recognized 
words are very similar to those on true words, we can conclude that not much was lost 
with this expedient. Also, we have not yet optimized the chopping stage relative to 
the combined model (only relative to the words-only segmenter). The use of prosodic 
features other than pause duration for chopping should further improve the overall 
performance. 
The improvement obtained with source-dependent topic switch penalties and 
posterior thresholds uggests that more comprehensive source-dependent modeling 
would be beneficial. In particular, both prosodic and lexical discourse cues are likely 
to be somewhat source specific (e.g., because of different show formats and different 
speakers). Given enough training data, it is straightforward to train source-dependent 
models. 
53 
Computational Linguistics Volume 27, Number 1 
(a) ?.. we have a severe thunderstorm watch two severe thunderstorm watches 
and a tornado watch in effect he tornado watch in effect back here in eastern 
colorado the two severe thunderstorm watches here indiana over into ohio 
those obviously associated with this line which is already been producing 
some hail i'll be back in a moment we'll take a look at our forecast weather 
map see if we can cool it off in the east will be very cold tonight minus seven 
degrees <TOPIC_CHANGE> 
LM probability: 0.018713 
PM probability: 0.937276 
karen just walked in was in the computer and found out for me that national 
airport in washington d. c. did hit one hundred degrees today it's a record 
high for them it's going to be uh hot again tomorrow but it will begin to 
cool off the que question is what time of day is this cold front going to move 
by your house if you want to know how warm it's going to be tomorrow 
comes through early in the day won't be that hot at all midday it'll still be 
into the nineties but not as hot as it was today comes through late in the day 
you'll still be in the upper nineties but some relief is on the way ... 
(b) ... you know the if if the president has been unfaithful to his wife and at 
this point you know i simply don't know any of the facts other than the 
bits and pieces that we hear and they're simply allegations at this point but 
being unfaithful to your wife isn't necessarily a crime lying in an affidavit is 
a crime inducing someone to lie in an affidavit is a crime but that occurred 
after this apparent aping so i'll tell you there are going to be extremely 
thorny legal issues that will have to be sorted out white house spokesman 
mike mccurry says the administration will cooperate in starr's investigation 
<TOPIC_CHANGE> 
LM probability: 1?000000 
PM probability: 0.134409 
cubans have been waiting for this day for a long time after months of plan- 
ning and preparation pope john paul the second will make his first visit to 
the island nation this afternoon it is the first pilgrimage ver by a pope to 
cuba judy fortin joins us now from havana with more . . . .  
Figure 8 
Examples of true topic boundaries where lexical and prosodic models make opposite 
decisions? (a) The prosodic model correctly predicts a topic change, the LM does not. (b) The 
LM predicts a topic change, the prosodic model does not. 
6. Conc lus ion  
We have presented a probabil istic approach to topic segmentat ion of speech, combin ing 
both lexical and prosodic cues. Topical word  usage and lexical discourse cues are 
represented by language models  embedded in an HMM.  Prosodic discourse cues, 
such as pause durat ions and pitch resets, are mode led  by a decision tree based on 
automatical ly extracted acoustic features and al ignments. Lexical and prosodic features 
can be combined either in the HMM or in the decision tree f ramework.  
Our  topic segmentat ion model  was evaluated on broadcast  news speech, and 
found to give competit ive per formance (around 14% error according to the weighted 
TDT2 segmentat ion cost metric). Notably, the segmentat ion accuracy of the prosodic  
54 
Tier, Hakkani-T(ir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
model alone is competit ive with a word-based segmenter, and a combined prosodic /  
lexical HMM achieves a substantial error reduction over the individual knowledge 
sources. 
Acknowledgments 
We thank Becky Bates, Madelaine PlauchG 
Ze'ev Rivlin, Ananth Sankar, and Kemal 
S6nmez for invaluable assistance in
preparing the data for this study. The paper 
was greatly improved as a result of 
comments by Andy Kehler, Madelaine 
Plauch6, and the anonymous reviewers. 
This research was supported by DARPA 
and NSF under NSF grant IRI-9619921 and 
DARPA contract no. N66001-97-C-8544. The 
views herein are those of the authors and 
should not be interpreted as representing 
the policies of the funding agencies. 
References 
Allan, J., J. Carbonell, G. Doddington, 
J. Yamron, and Y. Yang. 1998. Topic 
detection and tracking pilot study: Final 
report. In Proceedings ofDARPA Broadcast 
News Transcription and Understanding 
Workshop, pages 194-218, Lansdowne, VA, 
February. Morgan Kaufmann. 
Ayers, Gayle M. 1994. Discourse functions 
of pitch range in spontaneous and read 
speech. In Working Papers in Linguistics 
No. 44. Ohio State University, pages 1-49. 
Baum, Leonard E., Ted Petrie, George 
Soules, and Norman Weiss. 1970. A 
maximization technique occurring in the 
statistical analysis of probabilistic 
functions in Markov chains. The Annals of 
Mathematical Statistics, 41(1):164-171. 
Beeferman, Doug, Adam Berger, and John 
Lafferty. 1999. Statistical models for text 
segmentation. Machine Learning, 
34(1-3):177-210. Special Issue on Natural 
Language Learning. 
Breiman, L., J. H. Friedman, R. A. Olshen, 
and C. J. Stone. 1984. Classification and 
Regression Trees. Wadsworth and Brooks, 
Pacific Grove, CA. 
Brown, G., K. L. Currie, and J. Kenworthy. 
1980. Questions of Intonation. University 
Park Press, Baltimore. 
Brubaker, R. S. 1972. Rate and pause 
characteristics of oral reading. Journal of 
Psycholinguistic Research, 1:141-147. 
Buntine, Wray and Rich Caruana, 1992. 
Introduction to IND Version 2.1 and Recursive 
Partitioning. NASA Ames Research 
Center, Moffett Field, CA, December. 
Cieri, Chris, David Graft, Mark Liberman, 
Nii Martey, and Stephanie Strassell. 1999. 
The TDT-2 text and speech corpus. In 
Proceedings ofDARPA Broadcast News 
Workshop, pages 57-60, Herndon, VA, 
February. Morgan Kaufmann. 
Doddington, George. 1998. The Topic 
Detection and Tracking Phase 2 (TDT2) 
evaluation plan. In Proceedings ofDARPA 
Broadcast News Transcription and 
Understanding Workshop, pages 223-229, 
Lansdowne, VA, February. Morgan 
Kaufmann. Revised version available 
from http://www.nist.gov/speech/ 
tests/tdt/tdt98/. 
Entropic Research Laboratory, 1993. ESPS 
Version 5.0 Programs Manual. Washington, 
D.C. August. 
Fiscus, Jon, George Doddington, John 
Garofolo, and Alvin Martin. 1999. NIST's 
1998 Topic Detection and Tracking 
evaluation (TDT2). In Proceedings of
DARPA Broadcast News Workshop, 
pages 19-24, Herndon, VA, February. 
Morgan Kaufmann. 
Geluykens, R. and M. Swerts. 1993. Local 
and global prosodic ues to discourse 
organization i  dialogues. In Working 
Papers 41, Proceedings ofESCA Workshop on 
Prosody, pages 108-111, Lurid, Sweden. 
Grosz, B. and J. Hirschberg. 1992. Some 
intonational characteristics of discourse 
structure. In John J. Ohala, Terrance M. 
Nearey, Bruce L. Derwing, Megan M. 
Hodge, and Grace E. Wiebe, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 1, 
pages 429432, Banff, Canada, October. 
Grosz, B. and C. Sidner. 1986. Attention, 
intention, and the structure of discourse. 
Computational Linguistics, 12(3):175-204. 
Hakkani-Tiir, Dilek, GOkhan Tiir, Andreas 
Stolcke, and Elizabeth Shriberg. 1999. 
Combining words and prosody for 
information extraction from speech. In 
Proceedings ofthe 6th European Conference on 
Speech Communication a d Technology, 
volume 5, pages 1991-1994, Budapest, 
September. 
Hearst, Marti A. 1994. Multi-paragraph 
segmentation f expository text. In 
Proceedings ofthe 32nd Annual Meeting, 
pages 9-16, New Mexico State University, 
Las Cruces, NM, June. Association for 
Computational Linguistics. 
Hearst, Marti A. 1997. TextTiling: 
Segmenting text info multi-paragraph 
subtopic passages. Computational 
Linguistics, 23(1):33-64. 
55 
Computational Linguistics Volume 27, Number 1 
Hirschberg, Julia and Christine Nakatani. 
1996. A prosodic analysis of discourse 
segments in direction-giving monologues. 
In Proceedings ofthe 34th Annual Meeting, 
pages 286-293, Santa Cruz, CA, June. 
Association for Computational 
Linguistics. 
Hirschberg, Julia and Christine Nakatani. 
1998. Acoustic indicators of topic 
segmentation. I  Robert H. Mannell and 
Jordi Robert-Ribes, editors, Proceedings of
the International Conference on Spoken 
Language Processing, pages 976-979, 
Sydney, December. Australian Speech 
Science and Technology Association. 
John, George H., Ron Kohavi, and Karl 
Pfleger. 1994. Irrelevant features and the 
subset selection problem. In William W. 
Cohen and Haym Hirsh, editors, Machine 
Learning: Proceedings ofthe llth International 
Conference, pages 121-129, San Francisco. 
Morgan Kaufmann. 
Koopmans-van Beinum, Florien J. and 
Monique E. van Donzel. 1996. 
Relationship between discourse structure 
and dynamic speech rate. In H. Timothy 
Bunnell and William Idsardi, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 3, 
pages 1724-1727, Philadelphia, October. 
Kozima, H. 1993. Text segmentation based 
on similarity between words. In 
Proceedings ofthe 31st Annual Meeting, 
pages 286-288, Ohio State University, 
Columbus, Ohio, June. Association for 
Computational Linguistics. 
Litman, Diane J. and Rebecca J. Passonneau. 
1995. Combining multiple knowledge 
sources for discourse segmentation. I  
Proceedings ofthe 33rd Annual Meeting, 
pages 108-115, MIT, Cambridge, MA, 
June. Association for Computational 
Linguistics. 
Nakajima, Shin'ya and J. F. Allen. 1993. A 
study on prosody and discourse structure 
in cooperative dialogues. Phonetica, 
50:197-210. 
Nakajima, Shin'ya and Hajime Tsukada. 
1997. Prosodic features of utterances in
task-oriented dialogues. In Yoshinori 
Sagisaka, Nick Campbell, and Norio 
Higuchi, editors, Computing Prosody: 
Computational Models for Processing 
Spontaneous Speech. Springer, New York, 
chapter 7, pages 81-94. 
NIST. 1999. 1999 Topic Detection and 
Tracking Evaluation Project (TDT-3) 
Evaluation Project. Speech Group, 
National Institute for Standards and 
Technology, Gaithersburg, MD. http:// 
www.nist.gov/speech/tests/tdt/tdt99/. 
Passonneau, Rebecca J. and Diane J. Litman. 
1997. Discourse segmentation byhuman 
and automated means. Computational 
Linguistics, 23(1):103-139. 
Ponte, J. M. and W. B. Croft. 1997. Text 
segmentation bytopic. In Proceedings ofthe 
First European Conference on Research and 
Advanced Technology for Digital Libraries, 
pages 120-129, Pisa, Italy. 
Przybocki, M. A. and A. F. Martin. 1999. 
The 1999 NIST speaker ecognition 
evaluation, using summed two-channel 
telephone data for speaker detection and 
speaker tracking. In Proceedings ofthe 6th 
European Conference on Speech 
Communication a d Technology, volume 5, 
pages 2215-2218, Budapest, September. 
Rabiner, L. R. and B. H. Juang. 1986. An 
introduction to hidden Markov models. 
IEEE ASSP Magazine, 3(1):4-16, January. 
Reynar, Jeffrey C. 1994. An automatic 
method of finding topic boundaries. In 
Proceedings ofthe 32nd Annual Meeting, 
pages 331-333, New Mexico State 
University, Las Cruces, NM, June. 
Association for Computational 
Linguistics. 
Reynar, Jeffrey C. 1999. Statistical models 
for topic segmentation. I  Proceedings of
the 37th Annual Meeting, pages 357-364, 
University of Maryland, College Park, 
MD, June. Association for Computational 
Linguistics. 
Sankar, Ananth, Fuliang Weng, Ze'ev Rivlin, 
Andreas Stolcke, and Ramana Rao Gadde. 
1998. The development of SRI's 1997 
Broadcast News transcription system. In 
Proceedings DARPA Broadcast News 
Transcription and Understanding Workshop, 
pages 91-96, Lansdowne, VA, February. 
Morgan Kaufmann. 
Shriberg, Elizabeth, Rebecca Bates, and 
Andreas Stolcke. 1997. A prosody-only 
decision-tree model for disfluency 
detection. In G. Kokkinakis, N. Fakotakis, 
and E. Dermatas, editors, Proceedings ofthe 
5th European Conference on Speech 
Communication a d Technology, volume 5, 
pages 2383-2386, Rhodes, Greece, 
September. 
Shriberg, Elizabeth, Andreas Stolcke, Dilek 
Hakkani-Ti~r, and G6khan Tiir. 2000. 
Prosody-based automatic segmentation f 
speech into sentences and topics. Speech 
Communication, 32(1-2), pages 127-154. 
Special Issue on Accessing Information in 
Spoken Audio. 
S6nmez, Kemal, Elizabeth Shriberg, Larry 
Heck, and Mitchel Weintraub. 1998. 
Modeling dynamic prosodic variation for 
speaker verification. In Robert H. Mannell 
56 
Tiir, Hakkani-Tiir, Stolcke, and Shriberg Integrating Prosodic and Lexical Cues 
and Jordi Robert?Ribes, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 7, 
pages 3189-3192, Sydney, December. 
Australian Speech Science and 
Technology Association. 
Stolcke, Andreas, Klaus Ries, Noah Coccaro, 
Elizabeth Shriberg, Dan Jurafsky, Paul 
Taylor, Rachel Martin, Carol Van 
Ess-Dykema, nd Marie Meteer. 2000. 
Dialogue act modeling for automatic 
tagging and recognition of conversational 
speech. Computational Linguistics, 
26(3):339-373. 
Stolcke, Andreas and Elizabeth Shriberg. 
1996. Automatic linguistic segmentation 
of conversational speech. In H. Timothy 
Bunnell and William Idsardi, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 2, 
pages 1005-1008, Philadelphia, October. 
Stolcke, Andreas, Elizabeth Shriberg, 
Rebecca Bates, Mari Ostendorf, Dilek 
Hakkani, Madelaine PlauchG GOkhan 
Tier, and Yu Lu. 1998. Automatic 
detection of sentence boundaries and 
disfluencies based on recognized words. 
In Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 5, pages 2247-2250, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Swerts, M. 1997. Prosodic features at 
discourse boundaries of different 
strength. Journal of the Acoustical Society of 
America, 101:514-521. 
Swerts, M., R. Geluykens, and J. Terken. 
1992. Prosodic correlates of discourse 
units in spontaneous speech. In John J. 
Ohala, Terrance M. Nearey, Bruce L. 
Derwing, Megan M. Hodge, and Grace E. 
Wiebe, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 1, pages 421-424, 
Banff, Canada, October. 
Swerts, M. and M. Ostendorf. 1997. Prosodic 
and lexical indications of discourse 
structure in human-machine interactions. 
Speech Communication, 22(1):25--41. 
Vaissi6re, Jacqueline. 1983. 
Language-independent prosodic features. 
In A. Cutler and D. R. Ladd, editors, 
Prosody: Models and Measurements. 
Springer, Berlin, chapter 5, pages 53-66. 
van Mulbregt, P., I. Carp, L. Gillick, S. Lowe, 
and J. Yamron. 1999. Segmentation f 
automatically transcribed broadcast news 
text. In Proceedings ofDARPA Broadcast 
News Workshop, ages 77-80, Herndon, 
VA, February. Morgan Kaufmann. 
Viterbi, A. 1967. Error bounds for 
convolutional codes and an 
asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 13:260-269. 
Yamron, J. P., I. Carp, L. Gillick, S. Lowe, 
and P. van Mulbregt. 1998. A hidden 
Markov model approach to text 
segmentation a d event racking. In 
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 333-336, Seattle, WA, 
May. 
57 

Identifying Agreement and Disagreement in Conversational Speech:
Use of Bayesian Networks to Model Pragmatic Dependencies
Michel Galley   , Kathleen McKeown   , Julia Hirschberg   ,
  Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA

galley,kathy,julia  @cs.columbia.edu
and Elizabeth Shriberg 
 SRI International
Speech Technology and Research Laboratory
333 Ravenswood Avenue
Menlo Park, CA 94025, USA
ees@speech.sri.com
Abstract
We describe a statistical approach for modeling
agreements and disagreements in conversational in-
teraction. Our approach first identifies adjacency
pairs using maximum entropy ranking based on a
set of lexical, durational, and structural features that
look both forward and backward in the discourse.
We then classify utterances as agreement or dis-
agreement using these adjacency pairs and features
that represent various pragmatic influences of pre-
vious agreement or disagreement on the current ut-
terance. Our approach achieves 86.9% accuracy, a
4.9% increase over previous work.
1 Introduction
One of the main features of meetings is the occur-
rence of agreement and disagreement among par-
ticipants. Often meetings include long stretches
of controversial discussion before some consensus
decision is reached. Our ultimate goal is auto-
mated summarization of multi-participant meetings
and we hypothesize that the ability to automatically
identify agreement and disagreement between par-
ticipants will help us in the summarization task.
For example, a summary might resemble minutes of
meetings with major decisions reached (consensus)
along with highlighted points of the pros and cons
for each decision. In this paper, we present a method
to automatically classify utterances as agreement,
disagreement, or neither.
Previous work in automatic identification of
agreement/disagreement (Hillard et al, 2003)
demonstrates that this is a feasible task when var-
ious textual, durational, and acoustic features are
available. We build on their approach and show
that we can get an improvement in accuracy when
contextual information is taken into account. Our
approach first identifies adjacency pairs using maxi-
mum entropy ranking based on a set of lexical, dura-
tional and structural features that look both forward
and backward in the discourse. This allows us to ac-
quire, and subsequently process, knowledge about
who speaks to whom. We hypothesize that prag-
matic features that center around previous agree-
ment between speakers in the dialog will influence
the determination of agreement/disagreement. For
example, if a speaker disagrees with another per-
son once in the conversation, is he more likely to
disagree with him again? We model context using
Bayesian networks that allows capturing of these
pragmatic dependencies. Our accuracy for classify-
ing agreements and disagreements is 86.9%, which
is a 4.9% improvement over (Hillard et al, 2003).
In the following sections, we begin by describ-
ing the annotated corpus that we used for our ex-
periments. We then turn to our work on identify-
ing adjacency pairs. In the section on identification
of agreement/disagreement, we describe the contex-
tual features that we model and the implementation
of the classifier. We close with a discussion of future
work.
2 Corpus
The ICSI Meeting corpus (Janin et al, 2003) is
a collection of 75 meetings collected at the In-
ternational Computer Science Institute (ICSI), one
among the growing number of corpora of human-
to-human multi-party conversations. These are nat-
urally occurring, regular weekly meetings of vari-
ous ICSI research teams. Meetings in general run
just under an hour each; they have an average of 6.5
participants.
These meetings have been labeled with adja-
cency pairs (AP), which provide information about
speaker interaction. They reflect the structure of
conversations as paired utterances such as question-
answer and offer-acceptance, and their labeling is
used in our work to determine who are the ad-
dressees in agreements and disagreements. The an-
notation of the corpus with adjacency pairs is de-
scribed in (Shriberg et al, 2004; Dhillon et al,
2004).
Seven of those meetings were segmented into
spurts, defined as periods of speech that have no
pauses greater than .5 second, and each spurt was
labeled with one of the four categories: agreement,
disagreement, backchannel, and other.1 We used
spurt segmentation as our unit of analysis instead of
sentence segmentation, because our ultimate goal is
to build a system that can be fully automated, and
in that respect, spurt segmentation is easy to ob-
tain. Backchannels (e.g. ?uhhuh? and ?okay?) were
treated as a separate category, since they are gener-
ally used by listeners to indicate they are following
along, while not necessarily indicating agreement.
The proportion of classes is the following: 11.9%
are agreements, 6.8% are disagreements, 23.2% are
backchannels, and 58.1% are others. Inter-labeler
reliability estimated on 500 spurts with 2 labelers
was considered quite acceptable, since the kappa
coefficient was .63 (Cohen, 1960).
3 Adjacency Pairs
3.1 Overview
Adjacency pairs (AP) are considered fundamental
units of conversational organization (Schegloff and
Sacks, 1973). Their identification is central to our
problem, since we need to know the identity of
addressees in agreements and disagreements, and
adjacency pairs provide a means of acquiring this
knowledge. An adjacency pair is said to consist of
two parts (later referred to as A and B) that are or-
dered, adjacent, and produced by different speakers.
The first part makes the second one immediately rel-
evant, as a question does with an answer, or an offer
does with an acceptance. Extensive work in con-
versational analysis uses a less restrictive definition
of adjacency pair that does not impose any actual
adjacency requirement; this requirement is prob-
lematic in many respects (Levinson, 1983). Even
when APs are not directly adjacent, the same con-
straints between pairs and mechanisms for select-
ing the next speaker remain in place (e.g. the case
of embedded question and answer pairs). This re-
laxation on a strict adjacency requirement is partic-
ularly important in interactions of multiple speak-
ers since other speakers have more opportunities to
insert utterances between the two elements of the
AP construction (e.g. interrupted, abandoned or ig-
nored utterances; backchannels; APs with multiple
second elements, e.g. a question followed by an-
swers of multiple speakers).2
Information provided by adjacency pairs can be
used to identify the target of an agreeing or dis-
agreeing utterance. We define the problem of AP
1Part of these annotated meetings were provided by the au-
thors of (Hillard et al, 2003).
2The percentage of APs labeled in our data that have non-
contiguous parts is about 21%.
identification as follows: given the second element
(B) of an adjacency pair, determine who is the
speaker of the first element (A). A quite effective
baseline algorithm is to select as speaker of utter-
ance A the most recent speaker before the occur-
rence of utterance B. This strategy selects the right
speaker in 79.8% of the cases in the 50 meetings that
were annotated with adjacency pairs. The next sub-
section describes the machine learning framework
used to significantly outperform this already quite
effective baseline algorithm.
3.2 Maximum Entropy Ranking
We view the problem as an instance of statisti-
cal ranking, a general machine learning paradigm
used for example in statistical parsing (Collins,
2000) and question answering (Ravichandran et al,
2003).3 The problem is to select, given a set of  
possible candidates 			
 (in our case, po-
tential A speakers), the one candidate  that maxi-
mizes a given conditional probability distribution.
We use maximum entropy modeling (Berger et
al., 1996) to directly model the conditional proba-
bility  DETECTION OF AGREEMENT vs. DISAGREEMENT IN MEETINGS:
TRAINING WITH UNLABELED DATA
Dustin Hillard and Mari Ostendorf
University of Washington, EE
{hillard,mo}@ee.washington.edu
Elizabeth Shriberg
SRI International and ICSI
ees@speech.sri.com
Abstract
To support summarization of automatically
transcribed meetings, we introduce a classifier
to recognize agreement or disagreement utter-
ances, utilizing both word-based and prosodic
cues. We show that hand-labeling efforts can
be minimized by using unsupervised training
on a large unlabeled data set combined with
supervised training on a small amount of data.
For ASR transcripts with over 45% WER, the
system recovers nearly 80% of agree/disagree
utterances with a confusion rate of only 3%.
1 Introduction
Meetings are an integral component of life in most or-
ganizations, and records of meetings are important for
helping people recall (or learn for the first time) what
took place in a meeting. Audio (or audio-visual) record-
ings of meetings offer a complete record of the inter-
actions, but listening to the complete recording is im-
practical. To facilitate browsing and summarization of
meeting recordings, it is useful to automatically annotate
topic and participant interaction characteristics. Here, we
focus on interactions, specifically identifying agreement
and disagreement. These categories are particularly im-
portant for identifying decisions in meetings and infer-
ring whether the decisions are controversial, which can be
useful for automatic summarization. In addition, detect-
ing agreement is important for associating action items
with meeting participants and for understanding social
dynamics. In this study, we focus on detection using both
prosodic and language cues, contrasting results for hand-
transcribed and automatically transcribed data.
The agreement/disagreement labels can be thought of
as a sort of speech act categorization. Automatic classifi-
cation of speech acts has been the subject of several stud-
ies. Our work builds on (Shriberg et al, 1998), which
showed that prosodic features are useful for classifying
speech acts and lead to increased accuracy when com-
bined with word based cues. Other studies look at predic-
tion of speech acts primarily from word-based cues, using
language models or syntactic structure and discourse his-
tory (Chu-Carroll, 1998; Reithinger and Klesen, 1997).
Our work is informed by these studies, but departs signif-
icantly by exploring unsupervised training techniques.
2 Approach
Our experiments are based on a subset of meeting record-
ings collected and transcribed by ICSI (Morgan et al,
2001). Seven meetings were segmented (automatically,
but with human adjustment) into 9854 total spurts. We
define a ?spurt? as a period of speech by one speaker that
has no pauses of greater than one half second (Shriberg et
al., 2001). Spurts are used here, rather than sentences,
because our goal is to use ASR outputs and unsuper-
vised training paradigms, where hand-labeled sentence
segmentations are not available.
We define four categories: positive, backchannel, neg-
ative, and other. Frequent single-word spurts (specifi-
cally, yeah, right, yep, uh-huh, and ok) are separated out
from the ?positive? category as backchannels because of
the trivial nature of their detection and because they may
reflect encouragement for the speaker to continue more
than actual agreement. Examples include:
Neg: (6%) ?This doesn?t answer the question.?
Pos: (9%) ?Yeah, that sounds great.?
Back: (23%) ?Uh-huh.?
Other: (62%) ?Let?s move on to the next topic.?
The first 450 spurts in each of four meetings were
hand-labeled with these four categories based on listening
to speech while viewing transcripts (so a sarcastic ?yeah,
right? is labeled as a disagreement despite the positive
wording). Comparing tags on 250 spurts from two label-
ers produced a kappa coefficient (Siegel and Castellan,
1988) of .6, which is generally considered acceptable.
Additionally, unlabeled spurts from six hand-transcribed
training meetings are used in unsupervised training ex-
periments, as described later. The total number of au-
tomatically labeled spurts (8094) is about five times the
amount of hand-labeled data.
For system development and as a control, we use hand-
transcripts in learning word-based cues and in training.
We then evaluate the model with both hand-transcribed
words and ASR output. The category labels from the
hand transcriptions are mapped to the ASR transcripts,
assigning an ASR spurt to a hand-labeled reference if
more than half (time wise) of the ASR spurt overlaps the
reference spurt.
Feature Extraction. The features used in classification
include heuristic word types and counts, word-based fea-
tures derived from n-gram scores, and prosodic features.
Simple word-based features include: the total num-
ber of words in a spurt, the number of ?positive? and
?negative? keywords, and the class (positive, negative,
backchannel, discourse marker, other) of the first word
based on the keywords. The keywords were chosen based
on an ?effectiveness ratio,? defined as the frequency of a
word (or word pair) in the desired class divided by the fre-
quency over all dissimilar classes combined. A minimum
of five occurrences was required and then all instances
with a ratio greater than .6 were selected as keywords.
Other word-based features are found by computing the
perplexity (average log probability) of the sequence of
words in a spurt using a bigram language model (LM)
for each of the four classes. The perplexity indicates the
goodness of fit of a spurt to each class. We used both
word and class LMs (with part-of-speech classes for all
words except keywords). In addition, the word-based LM
is used to score the first two words of the spurt, which
often contain the most information about agreement and
disagreement. The label of the most likely class for each
type of LM is a categorical feature, and we also compute
the posterior probability for each class.
Prosodic features include pause, fundamental fre-
quency (F0), and duration (Baron et al, 2002). Features
are derived for the first word alone and for the entire
spurt. Average, maximum and initial pause duration fea-
tures are used. The F0 average and maximum features
are computed using different methods for normalizing F0
relative to a speaker-dependent baseline, mean and max.
For duration, the average and maximum vowel duration
from a forced alignment are used, both unnormalized and
normalized for vowel identity and phone context. Spurt
length in terms of number of words is also used.
Classifier design and feature selection. The overall
approach to classifying spurts uses a decision tree clas-
sifier (Breiman et al, 1984) to combine the word based
and prosodic cues. In order to facilitate learning of cues
for the less frequent classes, the data was upsampled (du-
plicated) so that there were the same number of training
points per class. The decision tree size was determined
using error-based cost-complexity pruning with 4-fold
cross validation. To reduce our initial candidate feature
set, we used an iterative feature selection algorithm that
involved running multiple decision trees (Shriberg et al,
2000). The algorithm combines elements of brute-force
search (in a leave-one-out paradigm) with previously de-
termined heuristics for narrowing the search space. We
used entropy reduction of the tree after cross-validation
as a criterion for selecting the best subtree.
Unsupervised training. In order to train the models
with as much data as possible, we used an unsupervised
clustering strategy for incorporating unlabeled data. Four
bigram models, one for each class, were initialized by
dividing the hand transcribed training data into the four
classes based upon keywords. First, all spurts which con-
tain the negative keywords are assigned to the negative
class. Backchannels are then pulled out when a spurt con-
tains only one word and it falls in the backchannel word
list. Next, spurts are selected as agreements if they con-
tain positive keywords. Finally, the remaining spurts are
associated with the ?other? class.
The keyword separation gives an initial grouping; fur-
ther regrouping involves unsupervised clustering using a
maximum likelihood criterion. A preliminary language
model is trained for each of the initial groups. Then, by
evaluating each spurt in the corpus against each of the
four language models, new groups are formed by asso-
ciating spurts with the language model that produces the
lowest perplexity. New language models are then trained
for the reorganized groups and the process is iterated un-
til there is no movement between groups. The final class
assignments are used as ?truth? for unsupervised training
of language and prosodic models, as well as contributing
features to decision trees.
3 Results and Discussion
Hand-labeled data from one meeting is held out for test
data, and the hand-labeled subset of three other meet-
ings are used for training decision trees. Unlabeled spurts
taken from six meetings, different from the test meeting,
are used for unsupervised training. Performance is mea-
sured in terms of overall 3-way classification accuracy,
merging the backchannel and agreement classes. The
overall accuracy results can be compared to the ?chance?
rate of 50%, since testing is on 4-way upsampled data.
In addition, we report the confusion rate between agree-
ments and disagreements and their recovery (recall) rate,
since these two classes are most important for our appli-
cation.
Results are presented in Table 1 for models using only
word-based cues. The simple keyword indicators used
in a decision tree give the best performance on hand-
transcribed speech, but performance degrades dramati-
cally on ASR output (with WER > 45%). For all other
training conditions, the degradation in performance for
the system based on ASR transcripts is not as large,
though still significant. The system using unsupervised
training clearly outperforms the system trained only on a
small amount of hand-labeled data. Interestingly, when
Hand Transcriptions ASR Transcriptions
Overall A/D A/D Overall A/D A/D
Features Accuracy Confusion Recovery Accuracy Confusion Recovery
Keywords 82% 2% 87% 61% 7% 53%
Hand Trained LM 71% 13% 74% 64% 10% 67%
Unsupervised LM 78% 10% 81% 67% 14% 70%
All word based 79% 8% 83% 71% 3% 78%
Table 1: Results for detection with different classifiers using word based features.
the keywords are used in combination with the language
model, they do provide some benefit in the case where
the system uses ASR transcripts.
The results in Table 2 correspond to models using only
prosodic cues. When these models are trained on only a
small amount of hand-labeled data, the overall accuracy
is similar to the system using keywords when operating
on the ASR transcript. Performance is somewhat better
than chance, and use of hand vs. ASR transcripts (and as-
sociated word alignments) has little impact. There is a
small gain in accuracy but a large gain in agree/disagree
recovery from using the data that was labeled via the un-
supervised language model clustering technique. Unfor-
tunately, when the prosody features are combined with
the word-based features, there is no performance gain,
even for the case of errorful ASR transcripts.
Transcripts Overall A/D A/D
Train/Test Accuracy Confusion Recovery
Hand/Hand 62% 17% 62%
Unsup./Hand 66% 13% 72%
Hand/ASR 62% 16% 61%
Unsup./ASR 64% 14% 75%
Table 2: Results for classifiers using prosodic features.
4 Conclusion
In summary, we have described an approach for au-
tomatic recognition of agreement and disagreement in
meeting data, using both prosodic and word-based fea-
tures. The methods can be implemented with a small
amount of hand-labeled data by using unsupervised LM
clustering to label additional data, which leads to signifi-
cant gains in both word-based and prosody-based classi-
fiers. The approach is extensible to other types of speech
acts, and is especially important for domains in which
very little annotated data exists. Even operating on ASR
transcripts with high WERs (45%), we obtain a 78% rate
of recovery of agreements and disagreements, with a very
low rate of confusion between these classes. Prosodic
features alone provide results almost as good as the word-
based models on ASR transcripts, but no additional ben-
efit when used with word-based features. However, the
good performance from prosody alone offers hope for
performance gains given a richer set of speech acts with
more lexically ambiguous cases (Bhagat et al, 2003).
Acknowledgments
This work is supported in part by the NSF under grants 0121396
and 0619921, DARPA grant N660019928924, and NASA grant
NCC 2-1256. Any opinions, conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of these agencies.
References
D. Baron et al 2002. Automatic punctuation and disfluency
detection in multi-party meetings using prosodic and lexical
cues. In Proc. ICSLP, pages 949?952.
S. Bhagat, H. Carvey, and E. Shriberg. 2003. Automatically
generated prosodic cues to lexically ambiguous dialog acts
in multi-party meetings. In ICPhS.
L. Breiman et al 1984. Classification And Regression Trees.
Wadsworth International Group, Belmont, CA.
J. Chu-Carroll. 1998. A statistical model for discourse act
recognition in dialogue interactions. In Applying Machine
Learning to Discourse Processing. Papers from the 1998
AAAI Spring Symposium, pages 12?17.
N. Morgan et al 2001. The meeting project at ICSI. In
Proc. Conf. on Human Language Technology, pages 246?
252, March.
N. Reithinger and M. Klesen. 1997. Dialogue act classification
using language models. In Proc. Eurospeech, pages 2235?
2238, September.
E. Shriberg et al 1998. Can prosody aid the automatic classi-
fication of dialog acts in conversational speech? Language
and Speech, 41(3?4), pages 439?487.
E. Shriberg et al 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32(1-2):127?154, September.
E. Shriberg et al 2001. Observations on overlap: Findings and
implications for automatic processing of multi-party conver-
sation. In Proc. Eurospeech, pages 1359?1362.
S. Siegel and J. Castellan. 1988. Nonparametric Statistics For
the Behavioral Sciences. McGraw-Hill Inc., New York, NY,
second edition edition.
Proceedings of the 43rd Annual Meeting of the ACL, pages 451?458,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Conditional Random Fields For Sentence Boundary Detection In
Speech
Yang Liu
ICSI, Berkeley
yangl@icsi.berkeley.edu
Andreas Stolcke Elizabeth Shriberg
SRI and ICSI
stolcke,ees@speech.sri.com
Mary Harper
Purdue University
harper@ecn.purdue.edu
Abstract
Sentence boundary detection in speech is
important for enriching speech recogni-
tion output, making it easier for humans to
read and downstream modules to process.
In previous work, we have developed hid-
den Markov model (HMM) and maximum
entropy (Maxent) classifiers that integrate
textual and prosodic knowledge sources
for detecting sentence boundaries. In this
paper, we evaluate the use of a condi-
tional random field (CRF) for this task
and relate results with this model to our
prior work. We evaluate across two cor-
pora (conversational telephone speech and
broadcast news speech) on both human
transcriptions and speech recognition out-
put. In general, our CRF model yields a
lower error rate than the HMM and Max-
ent models on the NIST sentence bound-
ary detection task in speech, although it
is interesting to note that the best results
are achieved by three-way voting among
the classifiers. This probably occurs be-
cause each model has different strengths
and weaknesses for modeling the knowl-
edge sources.
1 Introduction
Standard speech recognizers output an unstructured
stream of words, in which the important structural
features such as sentence boundaries are missing.
Sentence segmentation information is crucial and as-
sumed in most of the further processing steps that
one would want to apply to such output: tagging
and parsing, information extraction, summarization,
among others.
1.1 Sentence Segmentation Using HMM
Most prior work on sentence segmentation (Shriberg
et al, 2000; Gotoh and Renals, 2000; Christensen
et al, 2001; Kim and Woodland, 2001; NIST-
RT03F, 2003) have used an HMM approach, in
which the word/tag sequences are modeled by N-
gram language models (LMs) (Stolcke and Shriberg,
1996). Additional features (mostly related to speech
prosody) are modeled as observation likelihoods at-
tached to the N-gram states of the HMM (Shriberg
et al, 2000). Figure 1 shows the graphical model
representation of the variables involved in the HMM
for this task. Note that the words appear in both
the states1 and the observations, such that the
word stream constrains the possible hidden states
to matching words; the ambiguity in the task stems
entirely from the choice of events. This architec-
ture differs from the one typically used for sequence
tagging (e.g., part-of-speech tagging), in which the
?hidden? states represent only the events or tags.
Empirical investigations have shown that omitting
words in the states significantly degrades system
performance for sentence boundary detection (Liu,
2004). The observation probabilities in the HMM,
implemented using a decision tree classifier, capture
the probabilities of generating the prosodic features
1In this sense, the states are only partially ?hidden?.
451
P (F
i
jE
i
;W
i
).
2 An N-gram LM is used to calculate
the transition probabilities:
P (W
i
E
i
jW
1
E
1
: : :W
i 1
E
i 1
) =
P (W
i
jW
1
E
1
: : :W
i 1
E
i 1
)
P (E
i
jW
1
E
1
: : :W
i 1
E
i 1
E
i
)
In the HMM, the forward-backward algorithm is
used to determine the event with the highest poste-
rior probability for each interword boundary:
^
E
i
= argmax
E
i
P (E
i
jW;F ) (1)
The HMM is a generative modeling approach since
it describes a stochastic process with hidden vari-
ables (sentence boundary) that produces the observ-
able data. This HMM approach has two main draw-
backs. First, standard training methods maximize
the joint probability of observed and hidden events,
as opposed to the posterior probability of the correct
hidden variable assignment given the observations,
which would be a criterion more closely related to
classification performance. Second, the N-gram LM
underlying the HMM transition model makes it dif-
ficult to use features that are highly correlated (such
as words and POS labels) without greatly increas-
ing the number of model parameters, which in turn
would make robust estimation difficult. More details
about using textual information in the HMM system
are provided in Section 3.
1.2 Sentence Segmentation Using Maxent
A maximum entropy (Maxent) posterior classifica-
tion method has been evaluated in an attempt to
overcome some of the shortcomings of the HMM
approach (Liu et al, 2004; Huang and Zweig, 2002).
For a boundary position i, the Maxent model takes
the exponential form:
P (E
i
jT
i
; F
i
) =
1
Z

(T
i
; F
i
)
e
P
k

k
g
k
(E
i
;T
i
;F
i
) (2)
where Z

(T
i
; F
i
) is a normalization term and T
i
represents textual information. The indicator func-
tions g
k
(E
i
; T
i
; F
i
) correspond to features defined
over events, words, and prosody. The parameters in
2In the prosody model implementation, we ignore the word
identity in the conditions, only using the timing or word align-
ment information.
Wi Ei
Fi
Oi
Wi+1 Ei+1
Oi+1
Wi Fi+1Wi+1
Figure 1: A graphical model of HMM for the
sentence boundary detection problem. Only one
word+event pair is depicted in each state, but in
a model based on N-grams, the previous N   1
tokens would condition the transition to the next
state. O are observations consisting of words W and
prosodic features F , and E are sentence boundary
events.
Maxent are chosen to maximize the conditional like-
lihood
Q
i
P (E
i
jT
i
; F
i
) over the training data, bet-
ter matching the classification accuracy metric. The
Maxent framework provides a more principled way
to combine the largely correlated textual features, as
confirmed by the results of (Liu et al, 2004); how-
ever, it does not model the state sequence.
A simple combination of the results from the
Maxent and HMM was found to improve upon the
performance of either model alone (Liu et al, 2004)
because of the complementary strengths and weak-
nesses of the two models. An HMM is a generative
model, yet it is able to model the sequence via the
forward-backward algorithm. Maxent is a discrimi-
native model; however, it attempts to make decisions
locally, without using sequential information.
A conditional random field (CRF) model (Laf-
ferty et al, 2001) combines the benefits of the HMM
and Maxent approaches. Hence, in this paper we
will evaluate the performance of the CRF model and
relate the results to those using the HMM and Max-
ent approaches on the sentence boundary detection
task. The rest of the paper is organized as follows.
Section 2 describes the CRF model and discusses
how it differs from the HMM and Maxent models.
Section 3 describes the data and features used in the
models to be compared. Section 4 summarizes the
experimental results for the sentence boundary de-
tection task. Conclusions and future work appear in
Section 5.
452
2 CRF Model Description
A CRF is a random field that is globally conditioned
on an observation sequence O. CRFs have been suc-
cessfully used for a variety of text processing tasks
(Lafferty et al, 2001; Sha and Pereira, 2003; McCal-
lum and Li, 2003), but they have not been widely ap-
plied to a speech-related task with both acoustic and
textual knowledge sources. The top graph in Figure
2 is a general CRF model. The states of the model
correspond to event labels E. The observations O
are composed of the textual features, as well as the
prosodic features. The most likely event sequence ^E
for the given input sequence (observations) O is
^
E = argmax
E
e
P
k

k
G
k
(E;O)
Z

(O)
(3)
where the functions G are potential functions over
the events and the observations, and Z

is the nor-
malization term:
Z

(O) =
X
E
e
P
k

k
G
k
(E;O) (4)
Even though a CRF itself has no restriction on
the potential functions G
k
(E;O), to simplify the
model (considering computational cost and the lim-
ited training set size), we use a first-order CRF in
this investigation, as at the bottom of Figure 2. In
this model, an observation O
i
(consisting of textual
features T
i
and prosodic features F
i
) is associated
with a state E
i
.
The model is trained to maximize the conditional
log-likelihood of a given training set. Similar to the
Maxent model, the conditional likelihood is closely
related to the individual event posteriors used for
classification, enabling this type of model to explic-
itly optimize discrimination of correct from incor-
rect labels. The most likely sequence is found using
the Viterbi algorithm.3
A CRF differs from an HMM with respect to its
training objective function (joint versus conditional
likelihood) and its handling of dependent word fea-
tures. Traditional HMM training does not maxi-
mize the posterior probabilities of the correct la-
bels; whereas, the CRF directly estimates posterior
3The forward-backward algorithm would most likely be bet-
ter here, but it is not implemented in the software we used (Mc-
Callum, 2002).
E 1 E 2 E i E N
O
E i
Oi
E i-1
O i-1
E i+1
O i+1
Figure 2: Graphical representations of a general
CRF and the first-order CRF used for the sentence
boundary detection problem. E represent the state
tags (i.e., sentence boundary or not). O are observa-
tions consisting of words W or derived textual fea-
tures T and prosodic features F .
boundary label probabilities P (EjO). The under-
lying N-gram sequence model of an HMM does
not cope well with multiple representations (fea-
tures) of the word sequence (e.g., words, POS), es-
pecially when the training set is small; however, the
CRF model supports simultaneous correlated fea-
tures, and therefore gives greater freedom for incor-
porating a variety of knowledge sources. A CRF
differs from the Maxent method with respect to its
ability to model sequence information. The primary
advantage of the CRF over the Maxent approach is
that the model is optimized globally over the entire
sequence; whereas, the Maxent model makes a local
decision, as shown in Equation (2), without utilizing
any state dependency information.
We use the Mallet package (McCallum, 2002) to
implement the CRF model. To avoid overfitting, we
employ a Gaussian prior with a zero mean on the
parameters (Chen and Rosenfeld, 1999), similar to
what is used for training Maxent models (Liu et al,
2004).
3 Experimental Setup
3.1 Data and Task Description
The sentence-like units in speech are different from
those in written text. In conversational speech,
these units can be well-formed sentences, phrases,
or even a single word. These units are called SUs
in the DARPA EARS program. SU boundaries, as
453
well as other structural metadata events, were an-
notated by LDC according to an annotation guide-
line (Strassel, 2003). Both the transcription and the
recorded speech were used by the annotators when
labeling the boundaries.
The SU detection task is conducted on two cor-
pora: Broadcast News (BN) and Conversational
Telephone Speech (CTS). BN and CTS differ in
genre and speaking style. The average length of SUs
is longer in BN than in CTS, that is, 12.35 words
(standard deviation 8.42) in BN compared to 7.37
words (standard deviation 8.72) in CTS. This dif-
ference is reflected in the frequency of SU bound-
aries: about 14% of interword boundaries are SUs in
CTS compared to roughly 8% in BN. Training and
test data for the SU detection task are those used in
the NIST Rich Transcription 2003 Fall evaluation.
We use both the development set and the evalua-
tion set as the test set in this paper in order to ob-
tain more meaningful results. For CTS, there are
about 40 hours of conversational data (around 480K
words) from the Switchboard corpus for training
and 6 hours (72 conversations) for testing. The BN
data has about 20 hours of Broadcast News shows
(about 178K words) in the training set and 3 hours
(6 shows) in the test set. Note that the SU-annotated
training data is only a subset of the data used for
the speech recognition task because more effort is
required to annotate the boundaries.
For testing, the system determines the locations
of sentence boundaries given the word sequence W
and the speech. The SU detection task is evaluated
on both the reference human transcriptions (REF)
and speech recognition outputs (STT). Evaluation
across transcription types allows us to obtain the per-
formance for the best-case scenario when the tran-
scriptions are correct; thus factoring out the con-
founding effect of speech recognition errors on the
SU detection task. We use the speech recognition
output obtained from the SRI recognizer (Stolcke et
al., 2003).
System performance is evaluated using the offi-
cial NIST evaluation tools.4 System output is scored
by first finding a minimum edit distance alignment
between the hypothesized word string and the refer-
4See http://www.nist.gov/speech/tests/rt/rt2003/fall/ for
more details about scoring.
ence transcriptions, and then comparing the aligned
event labels. The SU error rate is defined as the total
number of deleted or inserted SU boundary events,
divided by the number of true SU boundaries. In
addition to this NIST SU error metric, we use the
total number of interword boundaries as the denomi-
nator, and thus obtain results for the per-boundary-
based metric.
3.2 Feature Extraction and Modeling
To obtain a good-quality estimation of the condi-
tional probability of the event tag given the obser-
vations P (E
i
jO
i
), the observations should be based
on features that are discriminative of the two events
(SU versus not). As in (Liu et al, 2004), we utilize
both textual and prosodic information.
We extract prosodic features that capture duration,
pitch, and energy patterns associated with the word
boundaries (Shriberg et al, 2000). For all the model-
ing methods, we adopt a modular approach to model
the prosodic features, that is, a decision tree classi-
fier is used to model them. During testing, the de-
cision tree prosody model estimates posterior prob-
abilities of the events given the associated prosodic
features for a word boundary. The posterior prob-
ability estimates are then used in various modeling
approaches in different ways as described later.
Since words and sentence boundaries are mu-
tually constraining, the word identities themselves
(from automatic recognition or human transcrip-
tions) constitute a primary knowledge source for
sentence segmentation. We also make use of vari-
ous automatic taggers that map the word sequence to
other representations. Tagged versions of the word
stream are provided to support various generaliza-
tions of the words and to smooth out possibly un-
dertrained word-based probability estimates. These
tags include part-of-speech tags, syntactic chunk
tags, and automatically induced word classes. In ad-
dition, we use extra text corpora, which were not an-
notated according to the guideline used for the train-
ing and test data (Strassel, 2003). For BN, we use
the training corpus for the LM for speech recogni-
tion. For CTS, we use the Penn Treebank Switch-
board data. There is punctuation information in
both, which we use to approximate SUs as defined
in the annotation guideline (Strassel, 2003).
As explained in Section 1, the prosody model and
454
Table 1: Knowledge sources and their representations in different modeling approaches: HMM, Maxent,
and CRF.
HMM Maxent CRF
generative model conditional approach
Sequence information yes no yes
LDC data set (words or tags) LM N-grams as indicator functions
Probability from prosody model real-valued cumulatively binned
Additional text corpus N-gram LM binned posteriors
Speaker turn change in prosodic features a separate feature,
in addition to being in the prosodic feature set
Compound feature no POS tags and decisions from prosody model
the N-gram LM can be integrated in an HMM. When
various textual information is used, jointly modeling
words and tags may be an effective way to model the
richer feature set; however, a joint model requires
more parameters. Since the training set for the SU
detection task in the EARS program is quite limited,
we use a loosely coupled approach:
 Linearly combine three LMs: the word-based
LM from the LDC training data, the automatic-
class-based LMs, and the word-based LM
trained from the additional corpus.
 These interpolated LMs are then combined
with the prosody model via the HMM. The
posterior probabilities of events at each bound-
ary are obtained from this step, denoted as
P
HMM
(E
i
jW;C;F ).
 Apply the POS-based LM alone to the POS
sequence (obtained by running the POS tag-
ger on the word sequence W ) and generate the
posterior probabilities for each word boundary
P
posLM
(E
i
jPOS), which are then combined
from the posteriors from the previous step,
i.e., P
final
(E
i
jT; F ) = P
HMM
(E
i
jW;C;F )+
P
posLM
(E
i
jP ).
The features used for the CRF are the same as
those used for the Maxent model devised for the SU
detection task (Liu et al, 2004), briefly listed below.
 N-grams of words or various tags (POS tags,
automatically induced classes). Different Ns
and different position information are used (N
varies from one through four).
 The cumulative binned posterior probabilities
from the decision tree prosody model.
 The N-gram LM trained from the extra cor-
pus is used to estimate posterior event proba-
bilities for the LDC-annotated training and test
sets, and these posteriors are then thresholded
to yield binary features.
 Other features: speaker or turn change, and
compound features of POS tags and decisions
from the prosody model.
Table 1 summarizes the features and their repre-
sentations used in the three modeling approaches.
The same knowledge sources are used in these ap-
proaches, but with different representations. The
goal of this paper is to evaluate the ability of these
three modeling approaches to combine prosodic and
textual knowledge sources, not in a rigidly parallel
fashion, but by exploiting the inherent capabilities
of each approach. We attempt to compare the mod-
els in as parallel a fashion as possible; however, it
should be noted that the two discriminative methods
better model the textual sources and the HMM bet-
ter models prosody given its representation in this
study.
4 Experimental Results and Discussion
SU detection results using the CRF, HMM, and
Maxent approaches individually, on the reference
transcriptions or speech recognition output, are
shown in Tables 2 and 3 for CTS and BN data, re-
spectively. We present results when different knowl-
edge sources are used: word N-gram only, word N-
gram and prosodic information, and using all the
455
Table 2: Conversational telephone speech SU detection results reported using the NIST SU error rate (%)
and the boundary-based error rate (% in parentheses) using the HMM, Maxent, and CRF individually and in
combination. Note that the ?all features? condition uses all the knowledge sources described in Section 3.2.
?Vote? is the result of the majority vote over the three modeling approaches, each of which uses all the
features. The baseline error rate when assuming there is no SU boundary at each word boundary is 100%
for the NIST SU error rate and 15.7% for the boundary-based metric.
Conversational Telephone Speech
HMM Maxent CRF
word N-gram 42.02 (6.56) 43.70 (6.82) 37.71 (5.88)
REF word N-gram + prosody 33.72 (5.26) 35.09 (5.47) 30.88 (4.82)
all features 31.51 (4.92) 30.66 (4.78) 29.47 (4.60)
Vote: 29.30 (4.57)
word N-gram 53.25 (8.31) 53.92 (8.41) 50.20 (7.83)
STT word N-gram + prosody 44.93 (7.01) 45.50 (7.10) 43.12 (6.73)
all features 43.05 (6.72) 43.02 (6.71) 42.00 (6.55)
Vote: 41.88 (6.53)
features described in Section 3.2. The word N-
grams are from the LDC training data and the extra
text corpora. ?All the features? means adding textual
information based on tags, and the ?other features? in
the Maxent and CRF models as well. The detection
error rate is reported using the NIST SU error rate,
as well as the per-boundary-based classification er-
ror rate (in parentheses in the table) in order to factor
out the effect of the different SU priors. Also shown
in the tables are the majority vote results over the
three modeling approaches when all the features are
used.
4.1 CTS Results
For CTS, we find from Table 2 that the CRF is supe-
rior to both the HMM and the Maxent model across
all conditions (the differences are significant at p <
0:05). When using only the word N-gram informa-
tion, the gain of the CRF is the greatest, with the dif-
ferences among the models diminishing as more fea-
tures are added. This may be due to the impact of the
sparse data problem on the CRF or simply due to the
fact that differences among modeling approaches are
less when features become stronger, that is, the good
features compensate for the weaknesses in models.
Notice that with fewer knowledge sources (e.g., us-
ing only word N-gram and prosodic information),
the CRF is able to achieve performance similar to or
even better than other methods using all the knowl-
edges sources. This may be useful when feature ex-
traction is computationally expensive.
We observe from Table 2 that there is a large
increase in error rate when evaluating on speech
recognition output. This happens in part because
word information is inaccurate in the recognition
output, thus impacting the effectiveness of the LMs
and lexical features. The prosody model is also af-
fected, since the alignment of incorrect words to the
speech is imperfect, thereby degrading prosodic fea-
ture extraction. However, the prosody model is more
robust to recognition errors than textual knowledge,
because of its lesser dependence on word identity.
The results show that the CRF suffers most from the
recognition errors. By focusing on the results when
only word N-gram information is used, we can see
the effect of word errors on the models. The SU
detection error rate increases more in the STT con-
dition for the CRF model than for the other models,
suggesting that the discriminative CRF model suf-
fers more from the mismatch between the training
(using the reference transcription) and the test con-
dition (features obtained from the errorful words).
We also notice from the CTS results that when
only word N-gram information is used (with or
without combining with prosodic information), the
HMM is superior to the Maxent; only when various
additional textual features are included in the fea-
ture set does Maxent show its strength compared to
456
Table 3: Broadcast news SU detection results reported using the NIST SU error rate (%) and the boundary-
based error rate (% in parentheses) using the HMM, Maxent, and CRF individually and in combination. The
baseline error rate is 100% for the NIST SU error rate and 7.2% for the boundary-based metric.
Broadcast News
HMM Maxent CRF
word N-gram 80.44 (5.83) 81.30 (5.89) 74.99 (5.43)
REF word N-gram + prosody 59.81 (4.33) 59.69 (4.33) 54.92 (3.98)
all features 48.72 (3.53) 48.61 (3.52) 47.92 (3.47)
Vote: 46.28 (3.35)
word N-gram 84.71 (6.14) 86.13 (6.24) 80.50 (5.83)
STT word N-gram + prosody 64.58 (4.68) 63.16 (4.58) 59.52 (4.31)
all features 55.37 (4.01) 56.51 (4.10) 55.37 (4.01)
Vote: 54.29 (3.93)
the HMM, highlighting the benefit of Maxent?s han-
dling of the textual features.
The combined result (using majority vote) of the
three approaches in Table 2 is superior to any model
alone (the improvement is not significant though).
Previously, it was found that the Maxent and HMM
posteriors combine well because the two approaches
have different error patterns (Liu et al, 2004). For
example, Maxent yields fewer insertion errors than
HMM because of its reliance on different knowledge
sources. The toolkit we use for the implementation
of the CRF does not generate a posterior probabil-
ity for a sequence; therefore, we do not combine
the system output via posterior probability interpola-
tion, which is expected to yield better performance.
4.2 BN Results
Table 3 shows the SU detection results for BN. Sim-
ilar to the patterns found for the CTS data, the CRF
consistently outperforms the HMM and Maxent, ex-
cept on the STT condition when all the features are
used. The CRF yields relatively less gain over the
other approaches on BN than on CTS. One possible
reason for this difference is that there is more train-
ing data for the CTS task, and both the CRF and
Maxent approaches require a relatively larger train-
ing set than the HMM. Overall the degradation on
the STT condition for BN is smaller than on CTS.
This can be easily explained by the difference in
word error rates, 22.9% on CTS and 12.1% on BN.
Finally, the vote among the three approaches outper-
forms any model on both the REF and STT condi-
tions, and the gain from voting is larger for BN than
CTS.
Comparing Table 2 and Table 3, we find that the
NIST SU error rate on BN is generally higher than
on CTS. This is partly because the NIST error rate
is measured as the percentage of errors per refer-
ence SU, and the number of SUs in CTS is much
larger than for BN, giving a large denominator and
a relatively lower error rate for the same number of
boundary detection errors. Another reason is that the
training set is smaller for BN than for CTS. Finally,
the two genres differ significantly: CTS has the ad-
vantage of the frequent backchannels and first per-
son pronouns that provide good cues for SU detec-
tion. When the boundary-based classification metric
is used (results in parentheses), the SU error rate is
lower on BN than on CTS; however, it should also
be noted that the baseline error rate (i.e., the priors
of the SUs) is lower on BN than CTS.
5 Conclusion and Future Work
Finding sentence boundaries in speech transcrip-
tions is important for improving readability and aid-
ing downstream language processing modules. In
this paper, prosodic and textual knowledge sources
are integrated for detecting sentence boundaries in
speech. We have shown that a discriminatively
trained CRF model is a competitive approach for
the sentence boundary detection task. The CRF
combines the advantages of being discriminatively
trained and able to model the entire sequence, and
so it outperforms the HMM and Maxent approaches
457
consistently across various testing conditions. The
CRF takes longer to train than the HMM and Max-
ent models, especially when the number of features
becomes large; the HMM requires the least training
time of all approaches. We also find that as more fea-
tures are used, the differences among the modeling
approaches decrease. We have explored different ap-
proaches to modeling various knowledge sources in
an attempt to achieve good performance for sentence
boundary detection. Note that we have not fully op-
timized each modeling approach. For example, for
the HMM, using discriminative training methods is
likely to improve system performance, but possibly
at a cost of reducing the accuracy of the combined
system.
In future work, we will examine the effect of
Viterbi decoding versus forward-backward decoding
for the CRF approach, since the latter better matches
the classification accuracy metric. To improve SU
detection results on the STT condition, we plan to
investigate approaches that model recognition un-
certainty in order to mitigate the effect of word er-
rors. Another future direction is to investigate how
to effectively incorporate prosodic features more di-
rectly in the Maxent or CRF framework, rather than
using a separate prosody model and then binning the
resulting posterior probabilities.
Important ongoing work includes investigating
the impact of SU detection on downstream language
processing modules, such as parsing. For these ap-
plications, generating probabilistic SU decisions is
crucial since that information can be more effec-
tively used by subsequent modules.
6 Acknowledgments
The authors thank the anonymous reviewers for their valu-
able comments, and Andrew McCallum and Aron Culotta at
the University of Massachusetts and Fernando Pereira at the
University of Pennsylvania for their assistance with their CRF
toolkit. This work has been supported by DARPA under
contract MDA972-02-C-0038, NSF-STIMULATE under IRI-
9619921, NSF KDI BCS-9980054, and ARDA under contract
MDA904-03-C-1788. Distribution is unlimited. Any opinions
expressed in this paper are those of the authors and do not reflect
the funding agencies. Part of the work was carried out while the
last author was on leave from Purdue University and at NSF.
References
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for smooth-
ing maximum entropy models. Technical report, Carnegie
Mellon University.
H. Christensen, Y. Gotoh, and S. Renal. 2001. Punctuation an-
notation using statistical prosody models. In ISCA Workshop
on Prosody in Speech Recognition and Understanding.
Y. Gotoh and S. Renals. 2000. Sentence boundary detection in
broadcast speech transcripts. In Proceedings of ISCA Work-
shop: Automatic Speech Recognition: Challenges for the
New Millennium ASR-2000, pages 228?235.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proceedings of the
International Conference on Spoken Language Processing,
pages 917?920.
J. Kim and P. C. Woodland. 2001. The use of prosody in a com-
bined system for punctuation generation and speech recogni-
tion. In Proceedings of the European Conference on Speech
Communication and Technology, pages 2757?2760.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random field: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2004. Com-
paring and combining generative and posterior probability
models: Some advances in sentence boundary detection in
speech. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich Transcription
of Speech. Ph.D. thesis, Purdue University.
A. McCallum and W. Li. 2003. Early results for named en-
tity recognition with conditional random fields. In Proceed-
ings of the Conference on Computational Natural Language
Learning.
A. McCallum. 2002. Mallet: A machine learning for language
toolkit. http://mallet.cs.umass.edu.
NIST-RT03F. 2003. RT-03F workshop agenda and
presentations. http://www.nist.gov/speech/tests/rt/rt2003/
fall/presentations/, November.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the Association
for Computational Linguistics annual meeting.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, pages 127?154.
A. Stolcke and E. Shriberg. 1996. Automatic linguistic seg-
mentation of conversational speech. In Proceedings of the
International Conference on Spoken Language Processing,
pages 1005?1008.
A. Stolcke, H. Franco, R. Gadde, M. Graciarena, K. Pre-
coda, A. Venkataraman, D. Vergyri, W. Wang, and
J. Zheng. 2003. Speech-to-text research at SRI-
ICSI-UW. http://www.nist.gov/speech/tests/rt/rt2003/
spring/presentations/index.htm.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
458
Comparing and Combining Generative and Posterior Probability Models:
Some Advances in Sentence Boundary Detection in Speech
Yang Liu
ICSI and Purdue University
yangl@icsi.berkeley.edu
Andreas Stolcke Elizabeth Shriberg
SRI and ICSI
stolcke,ees@speech.sri.com
Mary Harper
Purdue University
harper@ecn.purdue.edu
Abstract
We compare and contrast two different models for
detecting sentence-like units in continuous speech.
The first approach uses hidden Markov sequence
models based on N-grams and maximum likeli-
hood estimation, and employs model interpolation
to combine different representations of the data.
The second approach models the posterior proba-
bilities of the target classes; it is discriminative and
integrates multiple knowledge sources in the max-
imum entropy (maxent) framework. Both models
combine lexical, syntactic, and prosodic informa-
tion. We develop a technique for integrating pre-
trained probability models into the maxent frame-
work, and show that this approach can improve
on an HMM-based state-of-the-art system for the
sentence-boundary detection task. An even more
substantial improvement is obtained by combining
the posterior probabilities of the two systems.
1 Introduction
Sentence boundary detection is a problem that has
received limited attention in the text-based com-
putational linguistics community (Schmid, 2000;
Palmer and Hearst, 1994; Reynar and Ratnaparkhi,
1997), but which has recently acquired renewed im-
portance through an effort by the DARPA EARS
program (DARPA Information Processing Technol-
ogy Office, 2003) to improve automatic speech tran-
scription technology. Since standard speech recog-
nizers output an unstructured stream of words, im-
proving transcription means not only that word ac-
curacy must be improved, but also that commonly
used structural features such as sentence boundaries
need to be recognized. The task is thus fundamen-
tally based on both acoustic and textual (via auto-
matic word recognition) information. From a com-
putational linguistics point of view, sentence units
are crucial and assumed in most of the further pro-
cessing steps that one would want to apply to such
output: tagging and parsing, information extraction,
and summarization, among others.
Sentence segmentation from speech is a difficult
problem. The best systems benchmarked in a re-
cent government-administered evaluation yield er-
ror rates between 30% and 50%, depending on the
genre of speech processed (measured as the num-
ber of missed and inserted sentence boundaries as
a percentage of true sentence boundaries). Because
of the difficulty of the task, which leaves plenty of
room for improvement, its relevance to real-world
applications, and the range of potential knowledge
sources to be modeled (acoustics and text-based,
lower- and higher-level), this is an interesting chal-
lenge problem for statistical and computational ap-
proaches.
All of the systems participating in the recent
DARPA RT-03F Metadata Extraction evaluation
(National Institute of Standards and Technology,
2003) were based on a hidden Markov model frame-
work, in which word/tag sequences are modeled by
N-gram language models (LMs). Additional fea-
tures (mostly reflecting speech prosody) are mod-
eled as observation likelihoods attached to the N-
gram states of the HMM (Shriberg et al, 2000). The
HMM is a generative modeling approach, since it
describes a stochastic process with hidden variables
(the locations of sentence boundaries) that produces
the observable data. The segmentation is inferred
by comparing the likelihoods of different boundary
hypotheses.
While the HMM approach is computationally ef-
ficient and (as described later) provides a convenient
way for modularizing the knowledge sources, it has
two main drawbacks: First, the standard training
methods for HMMs maximize the joint probability
of observed and hidden events, as opposed to the
posterior probability of the correct hidden variable
assignment given the observations. The latter is a
criterion more closely related to classification error.
Second, the N-gram LM underlying the HMM tran-
sition model makes it difficult to use features that
are highly correlated (such as word and POS labels)
without greatly increasing the number of model pa-
rameters; this in turn would make robust estimation
channelword string
prosody
idea
syntax, semantics,
word selection,
puntuation
impose prosody
signal
prosodic feature
extraction
prosodic
features
textual feature
speech
recognizer
word string
word, POS,
classes
fusion of
knowledge
souces
sentence boundary
hypothesis
Figure 1: Diagram of the sentence segmentation task.
difficult.
In this paper, we describe our effort to overcome
these shortcomings by 1) replacing the generative
model with one that estimates the posterior proba-
bilities directly, and 2) using the maximum entropy
(maxent) framework to estimate conditional distri-
butions, giving us a more principled way to com-
bine a large number of overlapping features. Both
techniques have been used previously for traditional
NLP tasks, but they are not straightforward to ap-
ply in our case because of the diverse nature of the
knowledge sources used in sentence segmentation.
We describe the techniques we developed to work
around these difficulties, and compare classification
accuracy of the old and new approach on different
genres of speech. We also investigate how word
recognition error affects that comparison. Finally,
we show that a simple combination of the two ap-
proaches turns out to be highly effective in improv-
ing the best previous results obtained on a bench-
mark task.
2 The Sentence Segmentation Task
The sentence boundary detection problem is de-
picted in Figure 1 in the source-channel framework.
The speaker intends to say something, chooses the
word string, and imposes prosodic cues (duration,
emphasis, intonation, etc). This signal goes through
the speech production channel to generate an acous-
tic signal. A speech recognizer determines the most
likely word string given this signal. To detect pos-
sible sentence boundaries in the recognized word
string, prosodic features are extracted from the sig-
nal, and combined with textual cues obtained from
the word string. At issue in this paper is the final
box in the diagram: how to model and combine the
available knowledge sources to find the most accu-
rate hypotheses.
Note that this problem differs from the sen-
tence boundary detection problem for written text in
the natural language processing literature (Schmid,
2000; Palmer and Hearst, 1994; Reynar and Rat-
naparkhi, 1997). Here we are dealing with spo-
ken language, therefore there is no punctuation in-
formation, the words are not capitalized, and the
transcripts from the recognition output are errorful.
This lack of textual cues is partly compensated by
prosodic information (timing, pitch, and energy pat-
terns) conveyed by speech. Also note that in spon-
taneous conversational speech ?sentence? is not al-
ways a straightforward notion. For our purposes we
use the definition of a ?sentence-like unit?, or SU,
as defined by the LDC for labeling and evaluation
purposes (Strassel, 2003).
The training data has SU boundaries marked by
annotators, based on both the recorded speech and
its transcription. In testing, a system has to recover
both the words and the locations of sentence bound-
aries, denoted by (W;E) = w
1
e
1
w
2
: : : w
i
e
i
: : : w
n
where W represents the strings of word tokens and
E the inter-word boundary events (sentence bound-
ary or no boundary).
The system output is scored by first finding a min-
imum edit distance alignment between the hypothe-
sized word string and the reference, and then com-
paring the aligned event labels. The SU error rate is
defined as the total number of deleted or inserted SU
boundary events, divided by the number of true SU
boundaries.1 For diagnostic purposes a secondary
evaluation condition allows use of the correct word
transcripts. This condition allows us to study the
segmentation task without the confounding effect of
speech recognition errors, using perfect lexical in-
formation.
3 Features and Knowledge Sources
Words and sentence boundaries are mutually con-
strained via syntactic structure. Therefore, the word
identities themselves (from automatic recognition
or human transcripts) constitute a primary knowl-
edge source for the sentence segmentation task. We
also make use of various automatic taggers that map
the word sequence to other representations. The
TnT tagger (Brants, 2000) is used to obtain part-of-
speech (POS) tags. A TBL chunker trained on Wall
Street Journal corpus (Ngai and Florian, 2001) maps
each word to an associated chunk tag, encoding
chunk type and relative word position (beginning of
an NP, inside a VP, etc.). The tagged versions of
the word stream are provided to allow generaliza-
tions based on syntactic structure and to smooth out
possibly undertrained word-based probability esti-
1This is the same as simple per-event classification accu-
racy, except that the denominator counts only the ?marked?
events, thereby yielding error rates that are much higher than
if one uses all potential boundary locations.
mates. For the same reasons we also generate word
class labels that are automatically induced from bi-
gram word distributions (Brown et al, 1992).
To model the prosodic structure of sentence
boundaries, we extract several hundred features
around each word boundary. These are based on the
acoustic alignments produced by a speech recog-
nizer (or forced alignments of the true words when
given). The features capture duration, pitch, and
energy patterns associated with the word bound-
aries. Informative features include the pause du-
ration at the boundary, the difference in pitch be-
fore and after the boundary, and so on. A cru-
cial aspect of many of these features is that they
are highly correlated (e.g., by being derived from
the same raw measurements via different normaliza-
tions), real-valued (not discrete), and possibly unde-
fined (e.g., unvoiced speech regions have no pitch).
These properties make prosodic features difficult to
model directly in either of the approaches we are ex-
amining in the paper. Hence, we have resorted to a
modular approach: the information from prosodic
features is modeled separately by a decision tree
classifier that outputs posterior probability estimates
P (e
i
jf
i
), where e
i
is the boundary event after w
i
,
and f
i
is the prosodic feature vector associated with
the word boundary. Conveniently, this approach
also permits us to include some non-prosodic fea-
tures that are highly relevant for the task, but not
otherwise represented, such as whether a speaker
(turn) change occurred at the location in question.2
A practical issue that greatly influences model de-
sign is that not all information sources are avail-
able uniformly for all training data. For example,
prosodic modeling assumes acoustic data; whereas,
word-based models can be trained on text-only data,
which is usually available in much larger quantities.
This poses a problem for approaches that model all
relevant information jointly and is another strong
motivation for modular approaches.
4 The Models
4.1 Hidden Markov Model for Segmentation
Our baseline model, and the one that forms the ba-
sis of much of the prior work on acoustic sentence
segmentation (Shriberg et al, 2000; Gotoh and Re-
nals, 2000; Christensen, 2001; Kim and Woodland,
2001), is a hidden Markov model. The states of
the model correspond to words w
i
and following
2Here we are glossing over some details on prosodic mod-
eling that are orthogonal to the discussion in this paper. For
example, instead of simple decision trees we actually use en-
semble bagging to reduce the variance of the classifier (Liu et
al., 2004).
Wi Ei
Fi
Oi
Wi+1 Ei+1
Oi+1
Wi Fi+1Wi+1
Figure 2: The graphical model for the SU detection
problem. Only one word+event is depicted in each state,
but in a model based on N-grams the previous N   1
tokens would condition the transition to the next state.
event labels e
i
. The observations associated with
the states are the words, as well as other (mainly
prosodic) features f
i
. Figure 2 shows a graphi-
cal model representation of the variables involved.
Note that the words appear in both the states and the
observations, such that the word stream constrains
the possible hidden states to matching words; the
ambiguity in the task stems entirely from the choice
of events.
4.1.1 Classification
Standard algorithms are available to extract the most
probable state (and thus event) sequence given a set
of observations. The error metric is based on clas-
sification of individual word boundaries. Therefore,
rather than finding the highest probability sequence
of events, we identify the events with highest poste-
rior individually at each boundary i:
e^
i
= arg max
e
i
P (e
i
jW;F ) (1)
where W and F are the words and features for
the entire test sequence, respectively. The individ-
ual event posteriors are obtained by applying the
forward-backward algorithm for HMMs (Rabiner
and Juang, 1986).
4.1.2 Model Estimation
Training of the HMM is supervised since event-
labeled data is available. There are two sets of pa-
rameters to estimate. The state transition proba-
bilities are estimated using a hidden event N-gram
LM (Stolcke and Shriberg, 1996). The LM is
obtained with standard N-gram estimation meth-
ods from data that contains the word+event tags in
sequence: w
1
; e
1
; w
2
; : : : e
n 1
; w
n
. The resulting
LM can then compute the required HMM transition
probabilities as3
P (w
i
e
i
jw
1
e
1
: : : w
i 1
e
i 1
) =
P (w
i
jw
1
e
1
: : : w
i 1
e
i 1
) 
P (e
i
jw
1
e
1
: : : w
i 1
e
i 1
w
i
)
The N-gram estimator maximizes the joint
word+event sequence likelihood P (W;E) on the
training data (modulo smoothing), and does not
guarantee that the correct event posteriors needed
for classification according to Equation (1) are
maximized.
The second set of HMM parameters are the ob-
servation likelihoods P (f
i
je
i
; w
i
). Instead of train-
ing a likelihood model we make use of the prosodic
classifiers described in Section 3. We have at our
disposal decision trees that estimate P (e
i
jf
i
). If
we further assume that prosodic features are inde-
pendent of words given the event type (a reasonable
simplification if features are chosen appropriately),
observation likelihoods may be obtained by
P (f
i
jw
i
; e
i
) =
P (e
i
jf
i
)
P (e
i
)
P (f
i
) (2)
Since P (f
i
) is constant we can ignore it when car-
rying out the maximization (1).
4.1.3 Knowledge Combination
The HMM structure makes strong independence as-
sumptions: (1) that features depend only on the cur-
rent state (and in practice, as we saw, only on the
event label) and (2) that each word+event label de-
pends only on the last N   1 tokens. In return, we
get a computationally efficient structure that allows
information from the entire sequence W;F to in-
form the posterior probabilities needed for classifi-
cation, via the forward-backward algorithm.
More problematic in practice is the integration
of multiple word-level features, such as POS tags
and chunker output. Theoretically, all tags could
simply be included in the hidden state representa-
tion to allow joint modeling of words, tags, and
events. However, this would drastically increase the
size of the state space, making robust model estima-
tion with standard N-gram techniques difficult. A
method that works well in practice is linear inter-
polation, whereby the conditional probability esti-
mates of various models are simply averaged, thus
reducing variance. In our case, we obtain good re-
sults by interpolating a word-N-gram model with
3To utilize word+event contexts of length greater than one
we have to employ HMMs of order 2 or greater, or equivalently,
make the entire word+event N-gram be the state.
one based on automatically induced word classes
(Brown et al, 1992).
Similarly, we can interpolate LMs trained from
different corpora. This is usually more effective
than pooling the training data because it allows con-
trol over the contributions of the different sources.
For example, we have a small corpus of training data
labeled precisely to the LDC?s SU specifications,
but a much larger (130M word) corpus of standard
broadcast new transcripts with punctuation, from
which an approximate version of SUs could be in-
ferred. The larger corpus should get a larger weight
on account of its size, but a lower weight given the
mismatch of the SU labels. By tuning the interpola-
tion weight of the two LMs empirically (using held-
out data) the right compromise was found.
4.2 Maxent Posterior Probability Model
As observed, HMM training does not maximize the
posterior probabilities of the correct labels. This
mismatch between training and use of the model
as a classifier would not arise if the model directly
estimated the posterior boundary label probabilities
P (e
i
jW;F ). A second problem with HMMs is that
the underlying N-gram sequence model does not
cope well with multiple representations (features) of
the word sequence (words, POS, etc.) short of build-
ing a joint model of all variables. This type of sit-
uation is well-suited to a maximum entropy formu-
lation (Berger et al, 1996), which allows condition-
ing features to apply simultaneously, and therefore
gives greater freedom in choosing representations.
Another desirable characteristic of maxent models
is that they do not split the data recursively to condi-
tion their probability estimates, which makes them
more robust than decision trees when training data
is limited.
4.2.1 Model Formulation and Training
We built a posterior probability model for sentence
boundary classification in the maxent framework.
Such a model takes the familiar exponential form4
P (ejW;F ) =
1
Z

(W;F )
e
P
k

k
g
k
(e;W;F ) (3)
where Z

(W;F ) is the normalization term:
Z

(W;F ) =
X
e
0
e
P
k

k
g
k
(e
0
;W;F ) (4)
The functions g
k
(e;W;F ) are indicator functions
corresponding to (complex) features defined over
4We omit the index i from e here since the ?current? event
is meant in all cases.
events, words, and prosodic features. For example,
one such feature function might be:
g(e;W;F ) =
The Meeting Project at ICSI
Nelson Morgan1;4 Don Baron1;4 Jane Edwards1;4 Dan Ellis1;2 David Gelbart1;4
Adam Janin1;4 Thilo Pfau1 Elizabeth Shriberg1;3 Andreas Stolcke1;3
1International Computer Science Institute, Berkeley, CA
2Columbia University, New York, NY
3SRI International, Menlo Park, CA
4University of California at Berkeley, Berkeley, CA
fmorgan,dbaron,edwards,dpwe,gelbart,janin,tpfau,ees,stolckeg@icsi.berkeley.edu
ABSTRACT
In collaboration with colleagues at UW, OGI, IBM, and SRI, we are
developing technology to process spoken language from informal
meetings. The work includes a substantial data collection and tran-
scription effort, and has required a nontrivial degree of infrastruc-
ture development. We are undertaking this because the new task
area provides a significant challenge to current HLT capabilities,
while offering the promise of a wide range of potential applica-
tions. In this paper, we give our vision of the task, the challenges it
represents, and the current state of our development, with particular
attention to automatic transcription.
1. THE TASK
We are primarily interested in the processing (transcription,
query, search, and structural representation) of audio recorded from
informal, natural, and even impromptu meetings. By ?informal? we
mean conversations between friends and acquaintances that do not
have a strict protocol for the exchanges. By ?natural? we mean
meetings that would have taken place regardless of the recording
process, and in acoustic circumstances that are typical for such
meetings. By ?impromptu? we mean that the conversation may
take place without any preparation, so that we cannot require spe-
cial instrumentation to facilitate later speech processing (such as
close-talking or array microphones). A plausible image for such
situations is a handheld device (PDA, cell phone, digital recorder)
that is used when conversational partners agree that their discussion
should be recorded for later reference.
Given these interests, we have been recording and transcrib-
ing a series of meetings at ICSI. The recording room is one of
ICSI?s standard meeting rooms, and is instrumented with both
close-talking and distant microphones. Close-mic?d recordings
will support research on acoustic modeling, language modeling,
dialog modeling, etc., without having to immediately solve the
difficulties of far-field microphone speech recognition. The dis-
tant microphones are included to facilitate the study of these deep
acoustic problems, and to provide a closer match to the operating
conditions ultimately envisaged. These ambient signals are col-
.
lected by 4 omnidirectional PZM table-mount microphones, plus
a ?dummy? PDA that has two inexpensive microphone elements.
In addition to these 6 distant microphones, the audio setup permits
a maximum of 9 close-talking microphones to be simultaneously
recorded. A meeting recording infrastructure is also being put in
place at Columbia University, at SRI International, and by our col-
leagues at the University of Washington. Recordings from all sites
will be transcribed using standards evolved in discussions that also
involved IBM (who also have committed to assist in the transcrip-
tion task). Colleagues at NIST have been in contact with us to fur-
ther standardize these choices, since they intend to conduct related
collection efforts.
A segment from a typical discussion recorded at ICSI is included
below in order to give the reader a more concrete sense of the task.
Utterances on the same line separated by a slash indicate some de-
gree of overlapped speech.
A: Ok. So that means that for each utterance, .. we?ll need
the time marks.
E: Right. / A: the start and end of each utterance.
[a few turns omitted]
E: So we - maybe we should look at the um .. the tools that
Mississippi State has.
D: Yeah.
E: Because, I - I - I know that they published .. um .. annota-
tion tools.
A: Well, X-waves have some as well, .. but they?re pretty
low level .. They?re designed for uh - / D: phoneme / A: for
phoneme-level / D: transcriptions. Yeah.
J: I should -
A: Although, they also have a nice tool for - .. that could be
used for speaker change marking.
D: There?s a - there are - there?s a whole bunch of tools
J: Yes. / D: web page, where they have a listing. D: like 10
of them or something.
J: Are you speaking about Mississippi State per se? or
D: No no no, there?s some .. I mean, there just - there are -
there are a lot of / J: Yeah.
J: Actually, I wanted to mention - / D: (??)
J: There are two projects, which are .. international .. huge
projects focused on this kind of thing, actually .. one of
them?s MATE, one of them?s EAGLES .. and um.
D: Oh, EAGLES.
D: (??) / J: And both of them have
J: You know, I shou-, I know you know about the big book.
E: Yeah.
J: I think you got it as a prize or something.
E: Yeah. / D: Mhm.
J: Got a surprise. flaughg fJ. thought ?as a prize? sounded
like ?surprise?g
Note that interruptions are quite frequent; this is, in our expe-
rience, quite common in informal meetings, as is acoustic overlap
between speakers (see the section on error rates in overlap regions).
2. THE CHALLENGES
While having a searchable, annotatable record of impromptu
meetings would open a wide range of applications, there are sig-
nificant technical challenges to be met; it would not be far from the
truth to say that the problem of generating a full representation of a
meeting is ?AI complete?, as well as ?ASR complete?. We believe,
however, that our community can make useful progress on a range
of associated problems, including:
 ASR for very informal conversational speech, including the
common overlap problem.
 ASR from far-field microphones - handling the reverberation
and background noise that typically bedevil distant mics, as
well as the acoustic overlap that is more of a problem for
microphones that pick up several speakers at approximately
the same level.
 Segmentation and turn detection - recovering the different
speakers and turns, which also is more difficult with overlaps
and with distant microphones (although inter-microphone
timing cues can help here).
 Extracting nonlexical information such as speaker identifi-
cation and characterization, voice quality variation, prosody,
laughter, etc.
 Dialog abstraction - making high-level models of meet-
ing ?state?; identifying roles among participants, classifying
meeting types, etc. [2].
 Dialog analysis - identification and characterization of fine-
scale linguistic and discourse phenomena [3][10].
 Information retrieval from errorful meeting transcriptions -
topic change detection, topic classification, and query match-
ing.
 Summarization of meeting content [14] - representation of
the meeting structure from various perspectives and at vari-
ous scales, and issues of navigation in thes representations.
 Energy and memory resource limitation issues that arise in
the robust processing of speech using portable devices [7].
Clearly we and others working in this area (e.g., [15]) are at an
early stage in this research. However, the remainder of this pa-
per will show that even a preliminary effort in recording, manually
transcribing, and recognizing data from natural meetings has pro-
vided some insight into at least a few of these problems.
3. DATA COLLECTION AND HUMAN
TRANSCRIPTION
Using the data collection setup described previously, we have
been recording technical meetings at ICSI. As of this writing we
have recorded 38 meetings for a total of 39 hours. Note that there
are separate microphones for each participant in addition to the 6
far-field microphones, and there can be as many as 15 open chan-
nels. Consequently the sound files comprise hundreds of hours of
recorded audio. The total number of participants in all meetings is
237, and there were 49 unique speakers. The majority of the meet-
ings recorded so far have either had a focus on ?Meeting Recorder?
(that is, meetings by the group working on this technology) or ?Ro-
bustness? (primarily concerned with ASR robustness to acoustic
effects such as additive noise). A smaller number of other meeting
types at ICSI were also included.
In addition to the spontaneous recordings, we asked meeting par-
ticipants to read digit strings taken from a TI digits test set. This
was done to facilitate research in far-field microphone ASR, since
we expect this to be quite challenging for the more unconstrained
case. At the start or end of each meeting, each participant read 20
digit strings.
Once the data collection was in progress, we developed a set of
procedures for our initial transcription. The transcripts are word-
level transcripts, with speaker identifier, and some additional in-
formation: overlaps, interrupted words, restarts, vocalized pauses,
backchannels, and contextual comments, and nonverbal events
(which are further subdivided into vocal types such as cough and
laugh, and nonvocal types such as door slams and clicks). Each
event is tied to the time line through use of a modified version of the
?Transcriber? interface (described below). This Transcriber win-
dow provides an editing space at the top of the screen (for adding
utterances, etc), and the wave form at the bottom, with mechanisms
for flexibly navigating through the audio recording, and listening
and re-listening to chunks of virtually any size the user wishes.
The typical process involves listening to a stretch of speech until
a natural break is found (e.g., a long pause when no one is speak-
ing). The transcriber separates that chunk from what precedes and
follows it by pressing the Return key. Then he or she enters the
speaker identifier and utterance in the top section of the screen.
The interface is efficient and easy to use, and results in an XML
representation of utterances (and other events) tied to time tags for
further processing.
The ?Transcriber? interface [13] is a well-known tool for tran-
scription, which enables the user to link acoustic events to the wave
form. However, the official version is designed only for single-
channel audio. As noted previously, our application records up to
15 parallel sound tracks generated by as many as 9 speakers, and we
wanted to capture the start and end times of events on each channel
as precisely as possible and independently of one another across
channels. The need to switch between multiple audio channels to
clarify overlaps, and the need to display the time course of events
on independent channels required extending the ?Transcriber? in-
terface in two ways. First, we added a menu that allows the user to
switch the playback between a number of audio files (which are all
assumed to be time synchronized). Secondly, we split the time-
linked display band into as many independent display bands as
there are channels (and/or independent layers of time-synchronized
annotation). Speech and other events on each of the bands can now
be time-linked to the wave form with complete freedom and totally
independently of the other bands. This enables much more precise
start and end times for acoustic events.
See [8] for links to screenshots of these extensions to Transcriber
(as well as to other updates about our project).
In the interests of maximal speed, accuracy and consistency, the
transcription conventions were chosen so as to be: quick to type,
related to standard literary conventions where possible (e.g., - for
interrupted word or thought, .. for pause, using standard orthogra-
phy rather than IPA), and minimalist (requiring no more decisions
by transcribers than absolutely necessary).
After practice with the conventions and the interface, transcribers
achieved a 12:1 ratio of transcription time to speech time. The
amount of time required for transcription of spoken language is
known to vary widely as a function of properties of the discourse
(amount of overlap, etc.), and amount of detailed encoding (prosod-
ics, etc.), with estimates ranging from 10:1 for word-level with
minimal added information to 20:1, for highly detailed discourse
transcriptions (see [4] for details).
In our case, transcribers encoded minimal added detail, but had
two additional demands: marking boundaries of time bins, and
switching between audio channels to clarify the many instances of
overlapping speech in our data. We speeded the marking of time
bins by providing them with an automatically segmented version
(described below) in which the segmenter provided a preliminary
set of speech/nonspeech labels. Transcribers indicated that the pre-
segmentation was correct sufficiently often that it saved them time.
After the transcribers finished, their work was edited for consis-
tency and completeness by a senior researcher. Editing involved
checking exhaustive listings of forms in the data, spell check-
ing, and use of scripts to identify and automatically encode cer-
tain distinctions (e.g., the distinction between vocalized nonverbal
events, such as cough, and nonvocalized nonverbal events, like door
slams). This step requires on average about 1:1 - one minute of
editing for each minute of speech.
Using these methods and tools, we have currently transcribed
about 12 hours out of our 39 hours of data. Other data have
been sent to IBM for a rough transcription using commercial tran-
scribers, to be followed by a more detailed process at ICSI. Once
this becomes a routine component of our process, we expect it to
significantly reduce the time requirements for transcription at ICSI.
4. AUTOMATIC TRANSCRIPTION
As a preliminary report on automatic word transcription, we
present results for six example meetings, totalling nearly 7 hours
of speech, 36 total speakers, and 15 unique speakers (since many
speakers participated in multiple meetings). Note that these re-
sults are preliminary only; we have not yet had a chance to address
the many obvious approaches that could improve performance. In
particular, in order to facilitate efforts in alignment, pronuncia-
tion modeling, language modeling, etc., we worked only with the
close-mic?d data. In most common applications of meeting tran-
scription (including those that are our chief targets in this research)
such a microphone arrangement may not be practical. Nevertheless
we hope the results using the close microphone data will illustrate
some basic observations we have made about meeting data and its
automatic transcription.
4.1 Recognition system
The recognizer was a stripped-down version of the large-
vocabulary conversational speech recognition system fielded by
SRI in the March 2000 Hub-5 evaluation [11]. The system per-
forms vocal-tract length normalization, feature normalization, and
speaker adaptation using all the speech collected on each chan-
nel (i.e., from one speaker, modulo cross-talk). The acous-
tic model consisted of gender-dependent, bottom-up clustered
(genonic) Gaussian mixtures. The Gaussian means are adapted by
a linear transform so as to maximize the likelihood of a phone-loop
model, an approach that is fast and does not require recognition
prior to adaptation. The adapted models are combined with a bi-
gram language model for decoding. We omitted more elaborate
adaptation, cross-word triphone modeling, and higher-order lan-
guage and duration models from the full SRI recognition system
as an expedient in our initial recognition experiments (the omitted
steps yield about a 20% relative error rate reduction on Hub-5 data).
It should be noted that both the acoustic models and the lan-
guage model of the recognizer were identical to those used in the
Hub-5 domain. In particular, the acoustic front-end assumes a tele-
phone channel, requiring us to downsample the wide-band signals
of the meeting recordings. The language model contained about
30,000 words and was trained on a combination of Switchboard,
CallHome English and Broadcast News data, but was not tuned for
or augmented by meeting data.
4.2 Speech segmentation
As noted above, we are initially focusing on recognition of the
individual channel data. Such data provide an upper bound on
recognition accuracy if speaker segmentation were perfect, and
constitute a logical first step for obtaining high quality forced align-
ments against which to evaluate performance for both near- and far-
field microphones. Individual channel recordings were partitioned
into ?segments? of speech, based on a ?mixed? signal (addition
of the individual channel data, after an overall energy equalization
factor per channel). Segment boundary times were determined ei-
ther by an automatic segmentation of the mixed signal followed by
hand-correction, or by hand-correction alone. For the automatic
case, the data was segmented with a speech/nonspeech detector
consisting of an extension of an approach using an ergodic hidden
Markov model (HMM) [1]. In this approach, the HMM consists
of two main states, one representing ?speech? and one represent-
ing ?nonspeech? and a number of intermediate states that are used
to model the time constraints of the transitions between the two
main states. In our extension, we are incorporating mixture den-
sities rather than single Gaussians. This appears to be useful for
the separation of foreground from background speech, which is a
serious problem in these data.
The algorithm described above was trained on the
speech/nonspeech segmentation provided manually for the
first meeting that was transcribed. It was used to provide segments
of speech for the manual transcribers, and later for the recognition
experiments. Currently, for simplicity and to debug the various
processing steps, these segments are synchronous across chan-
nels. However, we plan to move to segments based on separate
speech/nonspeech detection in each individual channel. The latter
approach should provide better recognition performance, since it
will eliminate cross-talk in segments in which one speaker may
say only a backchannel (e.g. ?uhhuh?) while another speaker is
talking continuously.
Performance was scored for the spontaneous conversational por-
tions of the meetings only (i.e., the read digit strings referred to
earlier were excluded). Also, for this study we ran recognition only
on those segments during which a transcription was produced for
the particular speaker. This overestimates the accuracy of word
recognition, since any speech recognized in the ?empty? segments
would constitute an error not counted here. However, adding the
empty regions would increase data load by a factor of about ten?
which was impractical for us at this stage. Note that the current
NIST Hub-5 (Switchboard) task is similar in this respect: data are
recorded on separated channels and only the speech regions of a
speaker are run, not the regions in which they are essentially silent.
We plan to run all speech (including these ?empty? segments) in
future experiments, to better assess actual performance in a real
meeting task.
4.3 Recognition results and discussion
Overall error rates. Table 1 lists word error rates for the six
meetings, by speaker. The data are organized into two groups: na-
tive speakers and nonnative speakers. Since our recognition system
is not trained on nonnative speakers, we provide results only for the
native speakers; however the word counts are listed for all partici-
Table 1: Recognition performance by speaker and meeting (MRM = ?Meeting Recorder meeting?; ROB = ?Robustness meeting?).
Speaker gender is indicated by ?M? or ?F? in the speaker labels. ?* : : : *? marks speakers using a lapel microphone; all other cases
used close-talking head-mounted microphones. ??? indicates speakers with severely degraded or missing signals due to incorrect
microphone usage. Word error rates are in boldface, total number of words in Roman, and out-of-vocabulary (OOV) rates in italics.
OOV rate is by token, relative to a Hub-5 language model. WER is for conversational speech sections of meetings only, and are not
reported for nonnative speakers.
Meeting MRM002 MRM003 MRM004 MRM005 ROB005 ROB004
Duration (minutes) 45 78 60 68 81 70
Native speakers
M 004 42.4 48.1 44.3 48.4 45.1
4550 3087 3432 4912 5512
2.07 2.75 1.60 2.12 1.61
M 001 42.4 50.6 37.6 38.6
2311 2488 1904 3400
1.82 2.09 2.78 1.56
F 001 45.2 43.2 42.9 41.9
3008 3360 2714 2705
2.59 3.18 4.05 2.14
M 009 *100.1* *115.8* 38.2 *68.7*
1122 367 1066 696
1.59 2.45 1.88 2.01
F 002 45.2 43.7 *46.0*
1549 1481 2480
2.26 2.64 1.63
M 002 *55.6*
990
2.12
Speakers with low word counts
M 007 55.6 ?
198 69
2.97 2.90
M 008 72.7 59.5
55 121
5.45 5.79
M 015 ?
59
6.56
Non-native speakers (total words only)
M 003 (British) 2189
M 011 (Spanish) 2653 1239 663
F 003 (Spanish) 620 220
M 010 (German) 28
M 012 (German) 639
M 006 (French) 3524 2648
pants for completeness.1
The main result to note from Table 1 is that overall word error
rates are not dramatically worse than for Switchboard-style data.
This is particularly impressive since, as described earlier, no meet-
ing data were used in training, and no modifications of the acoustic
or language models were made. The overall WER for native speak-
ers was 46.5%, or only about a 7% relative increase over a compa-
rable recognition system on Hub-5 telephone conversations. This
suggests that from the point of view of pronunciation and language
(as opposed to acoustic robustness, e.g., for distant microphones),
Switchboard may also be ?ASR-complete?. That is, talkers may not
really speak in a more ?sloppy? manner in meetings than they do in
casual phone conversation. We further investigate this claim in the
next section, by breaking down results by overlap versus nonover-
lap regions, by microphone type and by speaker.
Note that in some cases there were very few contributions from
a speaker (e.g., speakers M 007, M 008, and M 015), and such
speakers also tended to have higher word error rates. We initially
suspected the problem was a lack of sufficient data for speaker
adaptation; indeed the improvement from adaptation was less than
for other speakers. Thus for such speakers it would make sense to
pool data across meetings for repeat participants. However, in look-
ing at their word transcripts we noted that their utterances, while
few, tended to be dense with information content. That is, these
were not the speakers uttering ?uhhuh? or short common phrases
(which are generally well modeled in the Switchboard recognizer)
but rather high-perplexity utterances that are generally harder to
recognize. Such speakers also tend to have a generally higher over-
all OOV rate than other speakers.
Error rates in overlap versus nonoverlap regions. As noted
in the previous section, the overall word error rate in our sam-
ple meetings was slightly higher than in Switchboard. An obvious
question to ask here is: what is the effect on recognition of over-
lapping speech? To address this question, we defined a crude mea-
sure of overlap. Since segments were channel-synchronous in these
meetings, a segment was either non-overlapping (only one speaker
was talking during that time segment), or overlapping (two or more
speakers were talking during the segment). Note that this does not
measure amount of overlap or number of overlapping speakers;
more sophisticated measures based on the phone backtrace from
forced alignment would provide a better measure for more detailed
analyses. Nevertheless, the crude measure provides a clear first
answer to our question. Since we were also interested in the inter-
action if any between overlap and microphone type, we computed
results separately for the head-mounted and lapel microphones. Re-
sults were also computed by speaker, since as shown earlier in Ta-
ble 1, speakers varied in word error rates, total words, and words by
microphone type. Note that speakers M 009 and F 002 have data
from both conditions.
As shown, our measure of overlap (albeit crude), clearly shows
that overlapping speech is a major problem for the recognition of
speech from meetings. If overlap regions are removed, the recog-
nition accuracy overall is actually better than that for Switchboard.
It is premature to make absolute comparisons here, but the fact that
the same pattern is observed for all speakers and across microphone
1Given the limitations of these pilot experiments (e.g., no on-task
training material and general pronunciation models), recognition
on nonnative speakers is essentially not working at present. In the
case of one nonnative speaker, we achieved a 200% word error rate,
surpassing a previous ICSI record. Word error results presented
here are based on meeting transcripts as of March 7, 2000, and are
subject to small changes as a result of ongoing transcription error
checking.
Table 2: Word error rates broken down by whether or not seg-
ment is in a region of overlapping speech.
Speaker No overlap With overlap
Headset Lapel Headset Lapel
M 004 41.0 - 50.3 -
M 001 34.2 - 47.6 -
F 001 40.5 - 45.8 -
M 009 30.7 41.0 40.7 117.8
F 002 37.7 29.8 50.5 56.3
M 002 - 48.6 - 71.3
M 007 52.2 - 81.3 -
M 008 50.9 - 69.9
Overall 39.9 38.5 48.7 85.2
conditions suggests that it is not the inherent speech properties of
participants that makes meetings difficult to recognize, but rather
the presence of overlapping speech.
Furthermore, one can note from Table 2 that there is a large inter-
action between microphone type and the effect of overlap. Overlap
is certainly a problem even for the close-talking head-mounted mi-
crophones. However, the degradation due to overlap is far greater
for the lapel microphone, which picks up a greater degree of back-
ground speech. As demonstrated by speaker F 002, it is possible
to have a comparatively good word error rate (29.8%) on the lapel
microphone in regions of no overlap (in this case 964/2480 words
were in nonoverlapping segments). Nevertheless, since the rate of
overlaps is so high in the data overall, we are avoiding the use
of the lapel microphone where possible in the future, preferring
head-mounted microphones for obtaining ground truth for research
purposes. We further note that for tests of acoustic robustness for
distant microphones, we tend to prefer microphones mounted on
the meeting table (or on a mock PDA frame), since they provide a
more realistic representation of the ultimate target application that
is a central interest to us - recognition via portable devices. In other
words, we are finding lapel mics to be too ?bad? for near-field mi-
crophone tests, and too ?good? for far-field tests.
Error rates by error type. The effect of overlapping speech on
error rates is due almost entirely to insertion errors, as shown in
Figure 1. Rates of other error types are nearly identical to those ob-
served for Switchboard (modulo a a slight increase in substitutions
associated with the lapel condition). This result is not surprising,
since background speech obviously adds false words in the hypoth-
esis. However, it is interesting that there is little increase in the
other error types, suggesting that a closer segmentation based on
individual channel data (as noted earlier) could greatly improve
recognition accuracy (by removing the surrounding background
speech).
Error rates by meeting type. Different types of meetings
should give rise to differences in speaking style and social interac-
tion, and we may be interested in whether such effects are realized
as differences in word error rates. The best way to measure such
effects is within speaker. The collection of regular, ongoing meet-
ings at ICSI offers the possibility of such within-speaker compar-
isons, since multiple speakers participate in more than one type of
regular meeting. Of the speakers shown in the data set used for this
study, speaker M 004 is a good case in point, since he has data from
three ?Meeting Recorder? meetings and two ?Robustness? meet-
ings. These two meeting types differ in social interaction; in the
first, there is a fairly open exchange between many of the partici-
Substitutions Deletions Insertions
Error Type
0
10
20
30
40
50
R
at
e 
 (%
)
Switchboard
Head?Mic, Overlap 
Head?Mic, Nonoverlap 
Lapel?Mic, Overlap 
Lapel?Mic, Nonoverlap 
Figure 1: Word error rates by error type and micro-
phone/overlap condition. Switchboard scores refer to an in-
ternal SRI development testset that is a representative subset
of the development data for the 2001 hub-5 evals. It contains
41 speakers (5-minute conversation sides), from Switchboard-
1, Switchboard-2 and Cellular Switchboard in roughly equal
proportions, and is also balanced for gender and ASR diffi-
culty. The other scores are evaluated for the data described
in the text.
pants, while in the second, speaker M 004 directs the flow of the
meeting. It can also be seen from the table that speaker M 004 con-
tributes a much higher rate of words relative to overall words in the
latter meeting type. Interestingly however, his recognition rate and
OOV rates are quite similar across the meeting types. Study of ad-
ditional speakers across meetings will allow us to further examine
this issue.
5. FUTURE WORK
The areas mentioned in the earlier section on ?Challenges? will
require much more work in the future. We and our colleagues at
collaborating institutions will be working in all of these. Here, we
briefly mention some of the work in our current plans for the study
of speech from meetings.
Far-field microphone ASR. Starting with the read digits and
proceeding to spontaneous speech, we will have a major focus on
improving recognition on the far-field channels. In earlier work we
have had some success in recognizing artificially degraded speech
[6][5], and will be adapting and more fully developing these ap-
proaches for the new data and task. Our current focus in these
methods is on the designing of multiple acoustic representations
and the combination of the resulting probability streams, but we
will also compare these to methods that are more standard (but im-
practical for the general case) such as echo cancellation using both
the close and distant microphones.
Overlap type modeling. One of the distinctive characteristics
of naturalistic conversation (in contrast to monolog situations) is
the presence of overlapping speech. Overlapping speech may be
of several types, and affects the flow of discourse in various ways.
An overlap may help to usurp the floor from another speaker (e.g.,
interruptions), or to encourage a speaker to continue (e.g., back
channels). Also, some overlaps may be accidental, or a part of joint
action (as when a group tries to help a speaker to recall a person?s
name when he is in mid-sentence). In addition, different speakers
may differ in the amount and kinds of overlap in which they engage
(speaker style). In future work we will explore types of overlaps
and their physical parameters, including prosodic aspects.
Language modeling. Meetings are also especially challenging
for the language model, since they tend to comprise a diverse range
of topics and styles, and matched training data is hard to come
by (at least in this initial phase of the project). Therefore, we ex-
pect meeting recognition to necessitate investigation into novel lan-
guage model adaptation and robustness techniques.
Prosodic modeling. Finally, we plan to study the potential con-
tribution of prosodic (temporal and intonational) features to auto-
matic processing of meeting data. A project just underway is con-
structing a database of prosodic features for meeting data, extend-
ing earlier work [10, 9]. Goals include using prosody combined
with language model information to help segment speech into co-
herent semantic units, to classify dialog acts [12], and to aid speaker
segmentation.
6. ACKNOWLEDGMENTS
The current work has been funded under the DARPA Communi-
cator project (in a subcontract from the University of Washington),
supplemented by an award from IBM. In addition to the authors of
this abstract, the project involves colleagues at a number of other
institutions, most notably: Mari Ostendorf, Jeff Bilmes, and Katrin
Kirchhoff from the University of Washington; and Hynek Herman-
sky from the Oregon Graduate Institute.
7. REFERENCES
[1] M. Beham and G. Ruske, Adaptiver stochastischer
Sprache/Pause-Detektor. Proc. DAGM Symposium
Mustererkennung, pp. 60?67, Bielefeld, May 1995, Springer.
[2] D. Biber, Variation across speech and writing. 1st pbk. ed.
Cambridge [England]; New York: Cambridge University
Press, 1991.
[3] W. Chafe, Cognitive constraints on information flow. In R. S.
Tomlin (ed.) Coherence and grounding in discourse.
Philadelphia: John Benjamins, pp. 21?51, 1987.
[4] J. Edwards, The transcription of Discourse. In D. Tannen, D.
Schiffrin, and H. Hamilton (eds). The Handbook of
Discourse Analysis. NY: Blackwell (in press).
[5] H. Hermansky, D. Ellis, and S. Sharma, Tandem
connectionist feature stream extraction for conventional
HMM systems, Proc. ICASSP, pp. III-1635?1638, Istanbul,
2000.
[6] H. Hermansky and N. Morgan, RASTA Processing of
Speech, IEEE Trans. Speech and Audio Processing 2(4),
578?589, 1994.
[7] A. Janin and N. Morgan, SpeechCorder, the Portable
Meeting Recorder, Workshop on hands-free speech
communication, Kyoto, April 9-11, 2001.
[8] http://www.icsi.berkeley.edu/speech/mtgrcdr.html
[9] E. Shriberg, R. Bates, A. Stolcke, P. Taylor, D. Jurafsky, K.
Ries, N. Coccaro, R. Martin, M. Meteer, and C. Van
Ess-Dykema. Can prosody aid the automatic classification of
dialog acts in conversational speech? Language and Speech,
41(3-4):439?487, 1998.
[10] E. Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r.
Prosody-based automatic segmentation of speech into
sentences and topics. Speech Communication,
32(1-2):127?154, 2000.
[11] A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao
Gadde, M. Plauche?, C. Richey, E. Shriberg, K. So?nmez, F.
Weng, and J. Zheng. The SRI March 2000 Hub-5
conversational speech transcription system. Proc. NIST
Speech Transcription Workshop, College Park, MD, May
2000.
[12] A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M.
Meteer, Dialogue Act Modeling for Automatic Tagging and
Recognition of Conversational Speech, Computational
Linguistics 26(3), 339?373, 2000.
[13] http://www.etca.fr/CTA/gip/Projets/Transcriber/
[14] A. Waibel, M. Bett, M. Finke, and R. Stiefelhagen, Meeting
Browser: Tracking and Summarizing Meetings, Proc.
DARPA Broadcast News Transcription and Understanding
Workshop, Lansdowne, VA, 1998.
[15] H. Yu, C. Clark, R. Malkin, and A. Waibel, Experiments in
Automatic Meeting Transcription Using JRTK, Proc.
ICASSP, pp. 921?924, Seattle, 1998.
Improving Automatic Sentence Boundary Detection
with Confusion Networks
D. Hillard
 
M. Ostendorf
 
University of Washington, EE
 

hillard,mo  @ee.washington.edu
A. Stolcke  Y. Liu  E. Shriberg 
ICSI  and SRI International 

stolcke,ees  @speech.sri.com
yangl@icsi.berkeley.edu
Abstract
We extend existing methods for automatic sen-
tence boundary detection by leveraging multi-
ple recognizer hypotheses in order to provide
robustness to speech recognition errors. For
each hypothesized word sequence, an HMM
is used to estimate the posterior probability of
a sentence boundary at each word boundary.
The hypotheses are combined using confusion
networks to determine the overall most likely
events. Experiments show improved detec-
tion of sentences for conversational telephone
speech, though results are mixed for broadcast
news.
1 Introduction
The output of most current automatic speech recognition
systems is an unstructured sequence of words. Additional
information such as sentence boundaries and speaker la-
bels are useful to improve readability and can provide
structure relevant to subsequent language processing, in-
cluding parsing, topic segmentation and summarization.
In this study, we focus on identifying sentence boundaries
using word-based and prosodic cues, and in particular we
develop a method that leverages additional information
available from multiple recognizer hypotheses.
Multiple hypotheses are helpful because the single best
recognizer output still has many errors even for state-
of-the-art systems. For conversational telephone speech
(CTS) word error rates can be from 20-30%, and for
broadcast news (BN) word error rates are 10-15%. These
errors limit the effectiveness of sentence boundary pre-
diction, because they introduce incorrect words to the
word stream. Sentence boundary detection error rates
on a baseline system increased by 50% relative for CTS
when moving from the reference to the automatic speech
condition, while for BN error rates increased by about
20% relative (Liu et al, 2003). Including additional rec-
ognizer hypotheses allows for alternative word choices to
inform sentence boundary prediction.
To integrate the information from different alterna-
tives, we first predict sentence boundaries in each hypoth-
esized word sequence, using an HMM structure that in-
tegrates prosodic features in a decision tree with hidden
event language modeling. To facilitate merging predic-
tions from multiple hypotheses, we represent each hy-
pothesis as a confusion network, with confidences for
sentence predictions from a baseline system. The final
prediction is based on a combination of predictions from
individual hypotheses, each weighted by the recognizer
posterior for that hypothesis.
Our methods build on related work in sentence bound-
ary detection and confusion networks, as described in
Section 2, and a baseline system and task domain re-
viewed in Section 3. Our approach integrates prediction
on multiple recognizer hypotheses using confusion net-
works, as outlined in Section 4. Experimental results are
detailed in Section 5, and the main conclusions of this
work are summarized in Section 6.
2 Related Work
2.1 Sentence Boundary Detection
Previous work on sentence boundary detection for auto-
matically recognized words has focused on the prosodic
features and words of the single best recognizer output
(Shriberg et al, 2000). That system had an HMM struc-
ture that integrates hidden event language modeling with
prosodic decision tree outputs (Breiman et al, 1984). The
HMM states predicted at each word boundary consisted
of either a sentence or non-sentence boundary classifica-
tion, each of which received a confidence score. Improve-
ments to the hidden event framework have included inter-
polation of multiple language models (Liu et al, 2003).
A related model has been used to investigate punc-
tuation prediction for multiple hypotheses in a speech
recognition system (Kim and Woodland, 2001). That sys-
tem found improvement in punctuation prediction when
rescoring using the classification tree prosodic feature
model, but it also introduced a small increase in word
error rate. More recent work has also implemented a sim-
ilar model, but used prosodic features in a neural net in-
stead of a decision tree (Srivastava and Kubala, 2003).
A maximum entropy model that included pause informa-
tion was used in (Huang and Zweig, 2002). Both finite-
state models and neural nets have been investigated for
prosodic and lexical feature combination in (Christensen
et al, 2001).
2.2 Confusion Networks
Confusion networks are a compacted representation of
word lattices that have strictly ordered word hypothesis
slots (Mangu et al, 2000). The complexity of lattice rep-
resentations is reduced to a simpler form that maintains
all possible paths from the lattice (and more), but trans-
forms the space to a series of slots which each have word
hypotheses (and null arcs) derived from the lattice and as-
sociated posterior probabilities. Confusion networks may
also be constructed from an N-best list, which is the case
for these experiments. Confusion networks are used to
optimize word error rate (WER) by selecting the word
with the highest probability in each particular slot.
3 Tasks & Baseline
This work specifically detects boundaries of sentence-
like units called SUs. An SU roughly corresponds to a
sentence, except that SUs are for the most part defined as
units that include only one independent main clause, and
they may sometimes be incomplete as when a speaker
is interrupted and does not complete their sentence. A
more specific annotation guideline for SUs is available
(Strassel, 2003), which we refer to as the ?V5? standard.
In this work, we focus only on detecting SUs and do not
differentiate among the different types (e.g. statement,
question, etc.) that were used for annotation. We work
with a relatively new corpus and set of evaluation tools,
which are described below.
3.1 Corpora
The system is evaluated for both conversational telephone
speech (CTS) and broadcast news (BN), in both cases us-
ing training, development and test data annotated accord-
ing to the V5 standard. The test data is that used in the
DARPA Rich Transcription (RT) Fall 2003 evaluations;
the development and evaluation test sets together com-
prise the Spring 2003 RT evaluation test sets.
For CTS, there are 40 hours of conversations available
for training from the Switchboard corpus, and 3 hours
(72 conversation sides) each of development and evalua-
tion test data drawn from both the Switchboard and Fisher
corpora. The development and evaluation set each have
roughly 6000 SUs.
The BN data consists of a set of 20 hours of news
shows for training, and 3 hours (6 shows) for testing. The
development and evaluation test data contains 1.5 hours
(3 shows) each for development and evaluation, each with
roughly 1000 SUs. Test data comes from the month of
February in 2001; training data is taken from a previous
time period.
3.2 Baseline System
The automatic speech recognition systems used were up-
dated versions of those used by SRI in the Spring 2003
RT evaluations (NIST, 2003), with a WER of 12.1%
on BN data and 22.9% on CTS data. Both systems
perform multiple recognition and adaptation passes, and
eventually produce up to 2000-best hypotheses per wave-
form segment, which are then rescored with a number of
knowledge sources, such as higher-order language mod-
els, pronunciation scores, and duration models (for CTS).
For best results, the systems combine decoding output
from multiple front ends, each producing a separate N-
best list. All N-best lists for the same waveform segment
are then combined into a single word confusion network
(Mangu et al, 2000) from which the hypothesis with low-
est expected word error is extracted. In our baseline SU
system, the single best word stream thus obtained is then
used as the basis for SU recognition.
Our baseline SU system builds on previous work on
sentence boundary detection using lexical and prosodic
features (Shriberg et al, 2000). The system takes as in-
put alignments from either reference or recognized (1-
best) words, and combines lexical and prosodic infor-
mation using an HMM. Prosodic features include about
100 features reflecting pause, duration, F0, energy, and
speaker change information. The prosody model is a de-
cision tree classifier that generates the posterior probabil-
ity of an SU boundary at each interword boundary given
the prosodic features. Trees are trained from sampled
training data in order to make the model sensitive to fea-
tures of the minority SU class. Recent prosody model im-
provements include the use of bagging techniques in deci-
sion tree training to reduce the variability due to a single
tree (Liu et al, 2003). Language model improvements
include adding information from a POS-based model, a
model using automatically-induced word classes, and a
model trained on separate data.
3.3 Evaluation
Errors are measured by a slot error rate similar to the
WER metric utilized by the speech recognition commu-
nity, i.e. dividing the total number of inserted and deleted
SUs by the total number of reference SUs. (There are
no substitution errors because there is only one sentence
class.) When recognition output is used, the words will
generally not align perfectly with the reference transcrip-
tion and hence the SU boundary predictions will require
some alignment procedure to match to the reference lo-
cation. Here, the alignment is based on the minimum
word error alignment of the reference and hypothesized
word strings, and the minimum SU error alignment if the
WER is equal for multiple alignments. We report num-
bers computed with the su-eval scoring tool from NIST.
SU error rates for the reference words condition of our
baseline system are 49.04% for BN, and 30.13% for CTS,
as reported at the NIST RT03F evaluation (Liu et al,
2003). Results for the automatic speech recognition con-
dition are described in Section 5.
4 Using N-Best Sentence Hypotheses
The large increase in SU detection error rate in mov-
ing from reference to recognizer transcripts motivates an
approach that reduces the mistakes introduced by word
recognition errors. Although the best recognizer output is
optimized to reduce word error rate, alternative hypothe-
ses may together reinforce alternative (more accurate) SU
predictions. The oracle WER for the confusion networks
is much lower than for the single best hypothesis, in the
range of 13-16% WER for the CTS test sets.
4.1 Feature Extraction and SU Detection
Prediction of SUs using multiple hypotheses requires
prosodic feature extraction for each hypothesis, which
in turn requires a forced alignment of each hypothesis.
Thousands of hypotheses are output by the recognizer,
but we prune to a smaller set to reduce the cost of run-
ning forced alignments and prosodic feature extraction.
The recognizer outputs an N-best list of hypotheses and
assigns a posterior probability to each hypothesis, which
is normalized to sum to 1 over all hypotheses. We collect
hypotheses from the N-best list for each acoustic segment
up to 90% of the posterior mass (or to a maximum count
of 1000).
Next, forced alignment and prosodic feature extraction
are run for all segments in this pruned set of hypothe-
ses. Statistics for prosodic feature normalization (such as
speaker and turn F0 mean) are collected from the single
best hypothesis. After obtaining the prosodic features,
the HMM predicts sentence boundaries for each word se-
quence hypothesis independently. For each hypothesis,
an SU prediction is made at all word boundaries, result-
ing in a posterior probability for SU and no SU at each
boundary. The same models are used as in the 1-best pre-
dictions ? no parameters were re-optimized for the N-best
framework. Given independent predictions for the indi-
vidual hypotheses, we then build a system to incorporate
the multiple predictions into a single hypothesis, as de-
scribed next.
4.2 Combining Hypotheses
The prediction results for an individual hypothesis are
represented in a confusion network that consists of a
series of word slots, each followed by a slot with SU and
no SU, as shown in Figure 1 with hypothetical confi-
dences for the between-word events. (This representation
is a somewhat unusual form because the word slots have
only a single hypothesis.) The words in the individual
hypotheses have probability one, and each arc with an
SU or no SU token has a confidence (posterior prob-
ability) assigned from the HMM. The overall network
has a score associated with its N-best hypothesis-level
posterior probability, scaled by a weight corresponding to
the goodness of the system that generated that hypothesis.
president
SU
no_SU at
SU
no_SU war
SU
no_SU
1 1 1
.2
.8
.1
.9
.7
.3
Figure 1: Confusion network for a single hypothesis.
The confusion networks for each hypothesis are
then merged with the SRI Language Modeling Toolkit
(Stolcke, 2002) to create a single confusion network
for an overall hypothesis. This confusion network is
derived from an alignment of the confusion networks
of each individual hypothesis. The resulting network
contains slots with the word hypotheses from the N-best
list and slots with the combined SU/no SU probability,
as shown in Figure 2. The confidences assigned to each
token in the new confusion network are a weighted
linear combination of the probabilities from individual
hypotheses that align to each other, compiled from
the entire hypothesis list, where the weights are the
hypothesis-level scores from the recognizer.
president
SU
no_SU at
SU
no_SU war SU
no_SU
1 .2 .4
.2
.8
.15
.85 .3
or
of
.6
.2
 - .6  - .6
.1
Figure 2: Confusion network for a merged hypothesis.
Finally, the best decision at each point is selected by
choosing the words and boundaries with the highest prob-
ability. Here, the words and SUs are selected indepen-
dently, so that we obtain the same words as would be
selected without inserting the SU tokens and guarantee
no degradation in WER. The key improvement is that the
SU detection is now a result of detection across all recog-
nizer hypotheses, which reduces the effect of word errors
in the top hypothesis.
5 Experiments
Table 1 shows the results in terms of slot error rate on
the four test sets. The middle column indicates the per-
formance on a single hypothesis, with the words derived
from the pruned set of N-best hypotheses. The right col-
umn indicates the performance of the system using mul-
tiple hypotheses merged with confusion networks.
Multiple hypotheses provide a reduction of error for
both test sets of CTS (significant at p   .02 using the Mc-
Nemar test), but give insignificant (and mixed) results for
BN. The small increase in error for the BN evaluation set
WER SU error rate
Single Best Confusion Nets
BN Dev 12.2 55.79% 54.45%
BN Eval 12.0 57.78% 58.42%
CTS Dev 23.6 44.14% 42.72%
CTS Eval 22.2 44.95% 44.01%
Table 1: Word and SU error rates for single best vs. con-
fusion nets.
may be due to the fact that the 1-best parameters were
tuned on different news shows than were represented in
the evaluation data.
We expected a greater gain from the use of confusion
networks in CTS than BN, given the previously shown
impact of WER on 1-best SU detection. Additionally,
incorporating a larger number of N-best hypotheses has
improved results in all experiments so far, so we would
expect this trend to continue for additional increases, but
time constraints limited our ability to run these larger ex-
periments. One possible explanation for the relatively
small performance gains is that we constrained the con-
fusion network topology so that there was no change in
the word recognition results. We imposed this constraint
in our initial investigations to allow us to compare per-
formance using the same words. It it possible that better
performance could be obtained by using confusion net-
work topologies that link words and metadata.
A more specific breakout of error improvement for the
CTS development set is given in Table 2, showing that
both recall and precision benefit from using the N-best
framework. Including multiple hypotheses reduces the
number of SU deletions (improves recall), but the pri-
mary gain is in reducing insertion errors (higher preci-
sion). The same effect holds for the CTS evaluation set.
Single Best Confusion Nets Change
Deletions 1623 1597 -1.6%
Insertions 872 818 -6.2%
Total 2495 2415 -3.2%
Table 2: Errors for CTS development set
6 Conclusion
Detecting sentence structure in automatic speech recog-
nition provides important information for language pro-
cessing or human understanding. Incorporating multiple
hypotheses from word recognition output can improve
overall detection of SUs in comparison to prediction on a
single hypothesis. This is especially true for CTS, which
suffers more from word errors and can therefore benefit
from considering alternative hypotheses.
Future work will involve a tighter integration of SU de-
tection and word recognition by including SU events di-
rectly in the recognition lattice. This will provide oppor-
tunities to investigate the interaction of automatic word
recognition and structural metadata, hopefully resulting
in reduced WER. We also plan to extend these methods
to additional tasks such as disfluency detection.
Acknowledgments
This work is supported in part by DARPA contract no.
MDA972-02-C-0038, and made use of prosodic feature extrac-
tion and modeling tools developed under NSF-STIMULATE
grant IRI-9619921. Any opinions, conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of these agencies.
References
L. Breiman et al 1984. Classification And Regression Trees.
Wadsworth International Group, Belmont, CA.
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctua-
tion annotation using statistical prosody models. In Proc.
ISCA Workshop on Prosody in Speech Recognition and Un-
derstanding.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proc. Eurospeech.
J.-H. Kim and P. Woodland. 2001. The use of prosody in
a combined system for punctuation generation and speech
recognition. In Proc. Eurospeech, pages 2757?2760.
Y. Liu, E. Shriberg, and A. Stolcke. 2003. Automatic disflu-
ency identification in conversational speech using multiple
knowledge sources. In Proc. Eurospeech, volume 1, pages
957?960.
Y. Liu et al 2003. MDE Research at
ICSI+SRI+UW, NIST RT-03F Workshop.
http://www.nist.gov/speech/tests/rt/rt2003/fall/presentations/.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: word error minimization and other
applications of confusion networks. Computer Speech and
Language, pages 373?400.
NIST. 2003. RT-03S Workshop Agenda and Presentations.
http://www.nist.gov/speech/tests/rt/rt2003/spring/presentations/.
E. Shriberg et al 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32(1-2):127?154, September.
A. Srivastava and F. Kubala. 2003. Sentence boundary detec-
tion in arabic speech. In Proc. Eurospeech, pages 949?952.
A. Stolcke. 2002. SRILM?an extensible language modeling
toolkit. In Proc. ICSLP, volume 2, pages 901?904.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus 
 
 
Elizabeth Shriberg1,2,  Raj Dhillon1,  Sonali Bhagat1,  
Jeremy Ang1,  Hannah Carvey1,3 
 
1International Computer Science Institute 
2SRI International 
3 CSU Hayward 
{ees,rdhillon,sonalivb,jca,hmcarvey}@icsi.berkeley.edu 
 
 
 
 
Abstract 
We describe a new corpus of over 180,000 hand-
annotated dialog act tags and accompanying adjacency 
pair annotations for roughly 72 hours of speech from 75 
naturally-occurring meetings. We provide a brief sum-
mary of the annotation system and labeling procedure, 
inter-annotator reliability statistics, overall distributional 
statistics, a description of auxiliary files distributed with 
the corpus, and information on how to obtain the data.  
1 Introduction 
Natural meetings offer rich opportunities for studying a 
variety of complex discourse phenomena. Meetings 
contain regions of high speaker overlap, affective varia-
tion, complicated interaction structures, abandoned or 
interrupted utterances, and other interesting turn-taking 
and discourse-level phenomena. In addition, meetings 
that occur naturally involve real topics, debates, issues, 
and social dynamics that should generalize more readily 
to other real meetings than might data collected using 
artificial scenarios. Thus meetings pose interesting chal-
lenges to descriptive and theoretical models of dis-
course, as well as to researchers in the speech 
recognition community [4,7,9,13,14,15]. 
 
We describe a new corpus of hand-annotated dialog acts 
and adjacency pairs for roughly 72 hours of naturally 
occurring multi-party meetings. The meetings were re-
corded at the International Computer Science Institute 
(ICSI) as part of the ICSI Meeting Recorder Project [9]. 
Word transcripts and audio files from that corpus are 
available through the Linguistic Data Consortium 
(LDC). In this paper, we provide a first description of 
the meeting recorder dialog act (MRDA) corpus, a 
companion set of annotations that augment the word 
transcriptions with discourse-level segmentations, dia-
log act (DA) information, and adjacency pair informa-
tion. The corpus is currently available online for 
research purposes [16], and we plan a future release 
through the LDC. 
2 Data 
The ICSI Meeting Corpus data is described in detail in 
[9]. It consists of 75 meetings, each roughly an hour in 
length. There are 53 unique speakers in the corpus, and 
an average of about 6 speakers per meeting.  Reflecting 
the makeup of the Institute, there are more male than 
female speakers (40 and 13, respectively).  There are 
a28 native English speakers, although many of the 
nonnative English speakers are quite fluent. Of the 75 
meetings, 29 are meetings of the ICSI meeting recorder 
project itself, 23 are meetings of a research group 
focused on robustness in automatic speech recognition, 
15 involve a group discussing natural language 
processing and neural theories of language, and 8 are 
miscellaneous meeting types.  The last set includes 2 
very interesting meetings involving the corpus 
transcribers as participants (example included in [16]). 
3 Annotation 
Annotation involved three types of information: 
marking of DA segment boundaries, marking of DAs 
themselves, and marking of correspondences between 
DAs (adjacency pairs, [12]).  Each type of annotation is 
described in detail in [7].  Segmentation methods were 
developed based on separating out speech regions 
having different discourse functions, but also paying 
attention to pauses and intonational grouping. To 
distinguish utterances that are prosodically one unit but 
which contain multiple DAs, we use a pipe bar ( | ) in 
the annotations. This allows the researcher to either split 
or not split at the bar, depending on the research goals. 
 
We examined existing annotation systems, including  
[1,2,5,6,8,10,11], for similarity to the style of interaction 
in the ICSI meetings. We found that SWBD-DAMSL 
[11], a system adapted from DAMSL [6], provided a 
fairly good fit. Although our meetings were natural, and 
thus had real agenda items, the dialog was less like 
human-human or human-machine task-oriented dialog 
(e.g., [1,2,10]) and more like human-human casual 
conversation  ([5,6,8,11]). Since we were working with 
English rather than Spanish, and did not view a large tag 
set as a problem, we preferred [6,11] over [5,8] for this 
work. We modified the system in [11] a number of 
ways, as indicated in Figure 1 and as explained further 
in [7]. The MRDA system requires one ?general tag? 
per DA, and attaches a variable number of following 
?specific tags?. Excluding nonlabelable cases, there are 
11 general tags and 39 specific tags. There are two dis-
ruption forms (%-, %--), two types of indecipherable 
utterances (x, %) and a non-DA tag to denote rising tone 
(rt).   
 
An interface allowed annotators to play regions of 
speech, modify transcripts, and enter DA and adjacency 
pair information, as well as other comments. Meetings 
were divided into 10 minute chunks; labeling time aver-
aged about 3 hours per chunk, although this varied con-
siderably depending on the complexity of the dialog.  
 
4 Annotated Example  
An example from one of the meetings is shown in Fig-
ure 2 as an illustration of some of the types of interac-
tions we observe in the corpus. Audio files and 
additional sample excerpts are available from [16]. In 
addition to the obvious high degree of overlap?roughly 
one third of all words are overlapped?note the explicit 
struggle for the floor indicated by the two failed floor 
grabbers (fg) by speakers c5 and c6. Furthermore, 6 of 
the 19 total utterances express some form of agreement 
or disagreement (arp, aa, and nd) with previous utter-
ances. Also, of the 19 utterances within the excerpt, 9 
are incomplete due to interruption by another talker, as 
is typical of many regions in the corpus showing high 
speaker overlap. We find in related work that regions of 
high overlap correlate with high speaker involvement, 
or ?hot spots? [15].  The example also provides a taste 
of the frequency and complexity of adjacency pair in-
formation. For example, within only half a minute, 
speaker c5 has interacted with speakers c3 and c6, and 
speaker c6 has interacted with speakers c2 and c5.   
 
5 Reliability 
We computed interlabeler reliability among the three 
labelers for both segmentation (into DA units) and DA 
labeling, using randomly selected excerpts from the 75 
labeled meetings.  Since agreement on DA segmentation 
does not appear to have standard associated metrics in 
the literature, we developed our own approach. The 
philosophy is that any difference in words at the 
beginning and/or end of a DA could result in a different 
label for that DA, and the more words that are 
mismatched, the more likely the difference in label. As 
a very strict measure of reliability, we used the 
 
 
 
TAG TITLE 
  
SWBD - 
DAMSL 
  
MRDA 
  
   TAG TITLE 
  
SWBD - 
DAMSL 
  
MRDA 
  
   TAG TITLE 
  
SWBD - 
DAMSL 
  
MRDA
 
Indecipherable 
  
% 
  
% 
  
   
Conventional - Opening 
  
fp 
       
Reformulation 
  
bf 
  
bs 
  Abandoned 
  
% - 
  
% -- 
     
Conventional - Closing 
  
fc 
       
Appreciation 
  
ba 
  
ba 
  Interruption 
    
% - 
     
Topic Cha nge 
    
tc 
     
Sympathy 
  
by 
  
by 
  Nonspeech 
  
x 
  
x 
     
Explicit - Performative 
  
fx 
       
Downplayer 
  
bd 
  
bd 
  Self - Talk 
  
t1 
  
t1 
     
Exclamation 
  
fe 
  
fe 
     
Misspeak Correction 
  
bc 
  
bc 
  3 rd - Party Talk 
  
t3 
  
t3 
     
Other - Forward - Function 
  
fo 
       
Rhetorical - Question Backchannel 
  
bh 
  
bh 
  
Task - Managem ent 
  
t 
  
t 
     
Thanks 
  
ft 
  
ft 
     
Signal Non 
 
understanding 
  
br 
  
br 
  Communication -Management 
  
c 
       
Welcome 
  
fw 
  
fw 
     
Understanding Check 
    
bu 
  Statement 
  
sd 
  
s 
  
   
Apology 
  
fa 
  
fa 
     
Defending/Explanation 
    
df 
  Subjective Statement 
  
sv 
  
s 
  
   
Floor - Holder 
    
fh 
     
Misspeak Self -Correct ion 
    
bsc 
  Wh - Question 
  
qw 
  
qw 
     
Floor - Grabber 
    
fg 
     
"Follow Me" 
    
f 
  Y/N Question 
  
qy 
  
qy 
  
   
Accept, Yes Answers 
  
ny, aa 
  
aa 
     
Expansion/Supporting addition 
  
e 
  
e 
  
Open - Ended Question 
  
qo 
  
qo 
     
Partial Accept 
  
aap 
  
aap 
     
Narrative - affirmative answers 
  
na 
  
na 
  Or Question 
  
qr 
  
qr 
     
Partial Reject 
  
arp 
  
arp 
     
Narrative - negative answers 
  
ng 
  
ng 
  Or Clause After Y/N Question 
  
qrr 
  
qrr 
     
Maybe 
  
am 
  
am 
     
No knowledge answers 
  
no 
  
no 
  
Rhetorical Question 
  
qh 
  
qh 
  
   
Reject, No Answers 
  
nn, ar 
  
ar 
  
   
Dispreferred answers 
  
nd 
  
nd 
  Declarative - Question 
  
d 
  
d 
     
Hold 
  
h 
  
h 
     
Quoted Material 
  
q 
    Tag Question 
  
g 
  
g 
     
Collaborative -Completion 
  
2 
  
2 
     
Humorous Material 
    
j 
  Open - Option 
  
oo 
       
Backchannel 
  
b 
  
b 
     
Continued from previous line 
  
+ 
    Command 
  
ad 
  
co 
     
Acknowledgment 
  
bk 
  
bk 
     
Hedge 
  
h 
    Suggestion 
  
co 
  
cs 
  
   
Mimic 
  
m 
  
m 
     
Nonlabeled 
    
z 
  Commit (self - inclusive) 
  
cc 
  
cc 
     
Repeat 
    
r 
          
 
Figure 1: Mapping of MRDA tags to SWBD-DAMSL tags. Tags in boldface are not present in SWBD-DAMSL and were 
added in MRDA. Tags in italics are based on the SWBD-DAMSL version but have had meanings modified for MRDA. The 
ordering of tags in the table is explained as follows: In the mapping of DAMSL tags to SWBD-DAMSL tags in the SWBD-
DAMSL manual, tags were ordered in categories such as ?Communication Status?, ?Information Requests?, and so on.  In 
the mapping of MRDA tags to SWBD-DAMSL tags here, we have retained the same overall ordering of tags within the table, 
but we do not explicitly mark the higher-level SWBD-DAMSL categories in order to avoid confusion, since categorical 
structure differs in the two systems (see [7]). 
following approach: (1) Take one labeler?s transcript as 
a reference. (2) Look at each other labeler?s words. For 
each word, look at the utterance it comes from and see if 
the reference has the exact same utterance. (3) If it does, 
there is a match. Match every word in the utterance, and 
then mark the matched utterance in the reference so it 
cannot be matched again (this prevents felicitous 
matches due to identical repeated words). (4) Repeat 
this process for each word in each reference-labeler 
pair, and rotate to the next labeler as the reference. Note 
that this metric requires perfect matching of the full 
utterance a word is in for that word to be matched.  For 
example in the following case, labelers agree on 3 seg-
mentation locations, but the agreement on our metric is 
only 0.14, since only 1 of 7 words is matched: 
 
.  yeah  .  I agree     it?s a hard decision . 
.  yeah  .  I agree  .  it?s a hard decision . 
 
Overall segmentation results on this metric are provided 
by labeler pair in Table 1. 
 
We examined agreement on DA labels using the Kappa  
statistic [3], which adjusts for chance agreement. 
Because of the large number of unique full label 
combinations, we report Kappa values in Table 2 using 
various class mappings distributed with the corpus. 
Values are shown by labeler pair.  
 Table 1: Results for strict segmentation agreement metric 
 
Reference 
Labeler 
Comparison 
Labeler 
Agree Total Agree 
% 
1 2 3004 4915 61.1 
1 3 2797 3820 73.2 
2 1 3004 4908 61.2 
2 3 5253 7906 66.4 
3 1 2797 3808 73.5 
3 2 5253 7889 66.6 
Overall 22108 33246 66.5 
 
 
 
Table 2: Kappa values for DAs using different class mappings. 
Map 1: Disruptions vs. backchannels vs. fillers vs. statements 
vs. questions vs. unlabelable; does not break at the ?|?. Map 2: 
Same as Map 1 but breaks at the ?|?.  Map 3: Same as Map 2 
but breaks down fillers and questions into further subclasses. 
See [16] for further details. 
 
Labeler Labeler Map 1 Map 2 Map 3 
1 2 .75 .73 .72 
1 3 .82 .81 .80 
2 3 .82 .77 .75 
  
The overall value of Kappa for our basic, six-way 
classmap (Map1) is 0.80, representing good agreement 
for this type of task. 
Time Chan DA AP Transcript 
2804-2810 c3 s^df^e.%- 34a i mean you can't just like print the - the vaues out in ascii and you know look at 
them to see if they're == 
2810-2811 c6 fg  well == 
2810-2811 c5 s^arp^j 34b not unless you had a lot of time . 
2811-2812 c5 %-  and == 
2811-2814 c6 s^bu 35a uh and also they're not - i mean as i understand it you ? you don't have a  way to 
optimize the features for the final word error . 
2814-2817 c6 qy^d^g^rt 35a+ right ? 
2818-2818 c2 s^aa 35b right . 
2818-2820 c6 s^bd  i mean these are just discriminative . 
2820-2823 c6 s.%- 36a but they're not um optimized for the final == 
2822-2823 c2 s^nd 36b they're optimized for phone discrimination . 
2823-2825 c2 s^e.%-  not for == 
2823-2835 c6 s^bk|s.%- 37a right | so it - there's always this question of  whether you might do better with 
those features if there was a way to train it for the word error metric that you're 
actually - that you're actually == 
2824-2825 c5 s^aa  that's right . 
2829-2830 c5 s.%-  well the other == 
2831-2832 c5 fg|%-  yeah | th- - the == 
2833-2835 c2 %-  huh- - huh == 
2834-2835 c5 s^nd 37b.38a well you actually are . 
2835-2837 c5 s^e 37b+.38a+ but ? but it ? but in an indirect way . 
2837-2840 c6 s^aa|s^df.%-  well right | it?s indirect so you don?t know == 
Figure 2: Example from meeting Bmr023.  Time marks are truncated here; actual resolution is 10 msec.  ?Chan?: channel 
(speaker);  ?DA?: full dialog act label (multiple tags are separated by ?^?); ?==?: incomplete DA;  ?xx  -  xx?: disfluency inter-
ruption point between words; ?xx-?: incomplete word;  ?AP?:  adjacency pairs (use arbitrary identifiers).  For purposes of illus-
tration, overlapped speech regions are indicated in the figure by reverse font color. Audio and other samples available from [16]. 
6 Distributional Statistics 
We provide basic statistics based on the dialog act 
labels for the 75 meetings. If we ignore the tag marking 
rising intonation (rt), since this is not a DA tag, we find 
180,218 total tags. Table 3 shows the distribution of the 
tags in more detail.   
    
Table 3: Distribution of tags. Tags are listed in order of 
descending frequency; values are percentages of  the 180,218 
total tags. 
 
s 42.85 b 8.42 fh 4.65 %-- 4.39 bk 4.05 
aa 3.38 %- 3.33 qy 3.10 df 2.29 e 2.02 
d 1.74 fg 1.73 cs 1.69 ba 1.37 z 1.36 
bu 1.28 qw 1.15 na 0.97 g 0.89 % 0.69 
no 0.57 ar 0.53 j 0.49 2 0.48 co 0.46 
h 0.44 f 0.41 m 0.40 nd 0.39 tc 0.38 
r 0.34 t 0.33 fe 0.29 ng 0.28 bd 0.25 
cc 0.24 qh 0.23 qrr 0.22 am 0.21 t3 0.20 
x 0.18 t1 0.16 fa 0.16 aap 0.15 br 0.14 
qr 0.12 qo 0.11 arp 0.10 bsc 0.09 bs 0.09 
bh 0.09 ft 0.08 bc 0.03 by 0.01 
 
If instead we look at only the 11 obligatory general tags, 
for which there is one per DA, and if we split labels at 
the pipe bar, the total is 113,560 (excluding tags that 
only include a disruption label).  The distribution of 
general tags is shown in Table 4. 
 
Table 4: Distribution of general tags; values are percentages of 
113,560 total general tags. 
 
s 68.00 b 13.37 fh 7.38 qy 4.91 
fg 2.74 qw 1.82 h 0.70 qh 0.36 
qrr 0.35 qr 0.20 qo 0.17 
7 Auxiliary Information 
We include other useful information with the corpus.  
Word-level time information is available, based on 
alignments from an automatic speech recognizer. 
Annotator comments are also provided. We suggest 
various ways to group the large set of labels into a 
smaller set of classes, depending on the research focus. 
Finally, the corpus contains information that may be 
useful in for developing automatic modeling of prosody, 
such as hand-marked annotation of rising intonation. 
8 Acknowledgments 
We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas 
Stolcke for software assistance, Ashley Krupski for contribu-
tions to the annotation scheme, Andrei Popescu-Belis for 
analysis and comments on a release of the 50 meetings, and 
Barbara Peskin and Jane Edwards for general advice and feed-
back. This work was supported by an ICSI subcontract to the 
University of Washington on a DARPA Communicator pro-
ject, ICSI NSF ITR Award IIS-0121396, SRI NASA Award 
NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR 
project, an ICSI award from the Swiss National Science Foun-
dation through the research network IM2, and by the EU 
Framework 6 project on Augmented Multi-party Interaction 
(AMI).  The views are those of the authors and do not repre-
sent the views of the funding agencies.  
References 
[1] Alexandersson, J., Buschbeck-Wolf, B., Fujinami, T., et al  Dia  
logue Acts in VERBMOBIL-2 Second Edition.  VM-Report 
226, DFKI Saarbr?cken, Germany, July 1998. 
[2] Anderson, A. H., Bader, M., Bard, E. G., et al (1991).  The 
HCRC Map Task Corpus.  Language and Speech, 34(4), 351-
366. 
[3] Carletta, J., 1996. Assessing agreement on classification tasks: 
The Kappa Statistic.  Computational Linguistics, 22:2, 249-254. 
[4]  Cieri, C., Miller, D. & Walker, K., 2002. Research methodolo-
gies, observations, and outcomes in conversational speech data 
collection. Proc. HLT 2002. 
[5]   Clark, A. & Popescu-Belis, A., 2004.  Multi-level Dialogue Act 
Tags.  In Proceedings of SIGDIAL ?04 (5th SIGDIAL Workshop 
on Discourse and Dialog).  Cambridge, MA. 
[6] Core, M. & Allen, J., 1997. Coding dialogs with the DAMSL 
annotation scheme. Working Notes: AAAI Fall Symposium, 
AAAI, Menlo Park, CA, pp. 28-35. 
[7]  Dhillon, R., Bhagat, S., Carvey, H., & Shriberg, E., 2004. Meet-
ing Recorder Project: Dialog Act Labeling Guide. ICSI Techni-
cal Report TR-04-002, International Computer Science Institute. 
[8]  Finke, M., Lapata, M., Lavie, A., et al, 1998.  CLARITY: 
Inferring discourse structure from speech.  AAAI ?98 Spring 
Symposium Series, March 23-25, 1998, Stanford University, 
California. 
[9] Janin, A. et al, 2003.  The ICSI Meeting Corpus. Proc. ICASSP-
2003. 
[10] Jekat, S., Klein, A., Maier, E., et al  Dialogue Acts in Verbmo-
bil, Verbmobil-Report No. 65, April 1995. 
[11] Jurafsky, D., Shriberg, E., & Biasca, D., 1997. Switchboard 
SWBD-DAMSL Labeling Project Coder?s Manual, Draft 13. 
Technical Report 97-02, Univ. of Colorado Institute of Cogni-
tive Science. 
[12] Levinson, S., 1983.  Pragmatics. Cambridge: Cambridge   Uni-
versity Press. 
[13] NIST meeting transcription project, www.nist.gov/speech/test_beds 
[14] Waibel, A., et al, 2001. Advances in automatic meeting record 
creation and access. Proc. ICASSP-2001. 
[15] Wrede, B. & Shriberg, E., 2003.  The relationship between dia-
logue acts and hot spots in meetings. Proc. IEEE Speech Rec-
ognition and Understanding Workshop, St. Thomas. 
[16] www.icsi.berkeley.edu/~ees/dadb contains the annotation 
corpus and sample (audio + annotations)  excerpts. 
Proceedings of NAACL-HLT 2013, pages 221?229,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Out-of-Domain Data for Lexical Addressee Detection in
Human-Human-Computer Dialog
Heeyoung Lee1? Andreas Stolcke2 Elizabeth Shriberg2
1Dept. of Electrical Engineering, Stanford University, Stanford, California, USA
2Microsoft Research, Mountain View, California, USA
heeyoung@stanford.edu, {anstolck,elshribe}@microsoft.com
Abstract
Addressee detection (AD) is an important
problem for dialog systems in human-human-
computer scenarios (contexts involving mul-
tiple people and a system) because system-
directed speech must be distinguished from
human-directed speech. Recent work on AD
(Shriberg et al, 2012) showed good results
using prosodic and lexical features trained on
in-domain data. In-domain data, however, is
expensive to collect for each new domain. In
this study we focus on lexical models and in-
vestigate how well out-of-domain data (either
outside the domain, or from single-user sce-
narios) can fill in for matched in-domain data.
We find that human-addressed speech can be
modeled using out-of-domain conversational
speech transcripts, and that human-computer
utterances can be modeled using single-user
data: the resulting AD system outperforms
a system trained only on matched in-domain
data. Further gains (up to a 4% reduction in
equal error rate) are obtained when in-domain
and out-of-domain models are interpolated.
Finally, we examine which parts of an utter-
ance are most useful. We find that the first
1.5 seconds of an utterance contain most of
the lexical information for AD, and analyze
which lexical items convey this. Overall, we
conclude that the H-H-C scenario can be ap-
proximated by combining data from H-C and
H-H scenarios only.
?Work done while first author was an intern with Microsoft.
1 Introduction
Before a spoken dialog system can recognize and in-
terpret a user?s speech, it should ideally determine
if speech was even meant to be interpreted by the
system. We refer to this task as addressee detec-
tion (AD). AD is often overlooked, especially in tra-
ditional single-user scenarios, because with the ex-
ception of self-talk, side-talk or background speech,
the majority of speech is usually system-directed.
As dialog systems expand to more natural contexts
and multiperson environments, however, AD can be-
come a crucial part of the system?s operational re-
quirements. This is particularly true for systems in
which explicit system addressing (e.g., push-to-talk
or required keyword addressing) is undesirable.
Past research on addressee detection has focused
on human-human (H-H) settings, such as meetings,
sometimes with multimodal cues (op den Akker and
Traum, 2009). Early systems relied primarily on re-
jection of H-H utterances either because they could
not be interpreted (Paek et al, 2000), or because they
yielded low speech recognition confidence (Dowd-
ing et al, 2006). Some systems combine gaze
with lexical and syntactic cues to detect H-H speech
(Katzenmaier et al, 2004). Others use relatively
simple prosodic features based on pitch and energy
in addition to those derived from automatic speech
recognition (ASR) (Reich et al, 2011).
With some exceptions (Bohus and Horvitz, 2011;
Shriberg et al, 2012), relatively little work has
looked at the human-human-computer (H-H-C) sce-
nario, i.e. at contexts involving two or more people
who interact both with a system and with each other.
221
Shriberg et al (2012) found that novel prosodic
features were more accurate than lexical or seman-
tic features based on speech recognition for the ad-
dressee task. The corpus, also used herein, is com-
prised of H-H-C dialog in which roughly half of the
computer-addressed speech consisted of a small set
of fixed commands. While the word-based features
map directly to the commands, they had trouble
distinguishing all other (noncommand) computer-
directed speech from human-directed speech. This
is because addressee detection in the H-H-C sce-
nario becomes even more challenging when the sys-
tem is designed for natural speech, i.e., utterances
that are conversational in form and not limited to
command phrases with restricted syntax. Further-
more, H-H utterances can be about the domain of
the system (e.g., discussing the dialog task), mak-
ing AD based on language content more difficult.
The prosodic features were good at both types of
distinctions?even improving performance signifi-
cantly when combined with true-word (cheating)
lexical features that have 100% accuracy on the
commands. Nevertheless, the prior work showed
that lexical n-grams are useful for addressee detec-
tion in the H-H-C scenario.
A problem with lexical features is that they are
highly task- and domain-dependent. As with other
language modeling tasks, one usually has to collect
matched training data in significant quantities. Data
collection is made more cumbersome and expensive
by the multi-user aspect of the scenario. Thus, for
practical reasons alone, it would be much better if
the language models for AD could be trained on
out-of-domain data, and if whatever in-domain data
is needed could be limited to single-user interac-
tion. We show in this paper that precisely this train-
ing scenario is feasible and achieves results that are
comparable or better than using completely matched
H-H-C training data.
In addition to studying the role of out-of-domain
data for lexical AD models, we also examine which
words are useful, and how soon in elapsed time they
are available. Whereas most prior work in AD has
looked at processing of entire utterances, we con-
sider an online processing version where AD deci-
sions are to be made as soon as possible after an
utterance was initiated. We find that most of the
addressee-relevant lexical information can be found
Figure 1: Conversational Browser dialog system en-
vironment with multi-human scenario
in the first 1.5 seconds, and analyze which words
convey this information.
2 Data
We use in-domain and out-of-domain data from var-
ious sources. The corpora used in this work differ in
size, domain, and scenario.
2.1 In-domain data
In-domain data is collected from interactions be-
tween two users and a ?Conversational Browser?
(CB) spoken dialog system. We used the same
methodology as Shriberg et al (2012), but using ad-
ditional data. As depicted in Figure 1, the system
shows a browser on a large TV screen and users
are asked to use natural language for a variety of
information-seeking tasks. For more details about
the dialog system and language understanding ap-
proach, see Hakkani-Tu?r et al (2011a; 2011b).
We split the in-domain data into training, devel-
opment, and test sets, preserving sessions. Each ses-
sion is about 5 to 40 minutes long. Even though
the whole conversation is recorded, only the seg-
ments captured by the speech recognition system
are used in our experiments. Each utterance seg-
ment belongs to one of four types: computer-
command (C-command), comprising navigational
commands to the system, computer-noncommand
(C-noncommand), which are computer-directed ut-
terances other than commands, human-directed (H),
and mixed (M) utterances, which contain a combina-
222
Table 1: In-domain corpus
(a) Sizes, distribution, and ASR word error rates of in-
domain utterance types
Data set Train Dev Test WER
Transcribed words 6,490 11,298 9,486
ASR words 4,649 6,360 5,514 59.3%
H (%) 19.1 48.6 37.0 87.6%
C-noncomm. (%) 38.3 27.8 32.2 32.6%
C-command (%) 39.9 18.7 27.2 19.7%
M (%) 2.7 4.9 3.6 69.6%
(b) Example utterances by type
Type Example
H Do you want to watch a
movie?
C-noncommand How is the weather today?
C-command Scroll down, Go back.
M Show me sandwich shops.
Oh, are you vegetarian?
tion of human- and computer-directed speech. The
sizes and distribution of all utterance types, as well
as sample utterances are shown in Table 1.
The ASR system used in the system was based on
off-the-shelf acoustic models and had only the lan-
guage model adapted to the domain, using very lim-
ited data. Consequently, as shown in the right-most
column of Table 1(a), the word error rates (WERs)
are quite high, especially for human-directed utter-
ances. While these could be improved with tar-
geted effort, we consider this a realistic application
scenario, where in-domain training data is typically
scarce, at least early in the development process.
Therefore, any lexically based AD methods need to
be robust to poor ASR accuracy.
2.2 Out-of-domain data
To replace the hard-to-obtain in-domain H-H-C data
for training, we use the four out-of-domain corpora
(two H-C and two H-H) shown in Table 2.
Single-user CB data comes from the same Con-
versational Browser system as the in-domain data,
but with only one user present. This data can there-
fore be used for modeling H-C speech. Bing anchor
text (Huang et al, 2010) is a large n-gram corpus of
anchor text associated with links on web pages en-
Table 2: Out-of-domain corpora. ?Single-user CB?
is a corpus collected in same environment as the H-
H-C in-domain data, except that only a single user
was present.
Corpus Addressee Size
Single-user CB H-C 21.9k words
Bing anchor text H-C 1.3B bigrams
Fisher H-H 21M words
ICSI meetings H-H 0.7M words
Single user CB, Bing ?  out-of-domain 
in-domain  (HC) 
in-domain (HH) 
Fisher, ICSI meeting ?  out-of-domain 
Language model for human directed utterances (H)  
Language model for computer directed utterances (C)  
?(?|?) 
1? ??? ?(?|?)?(?|?) 
?(?|?) 
Figure 2: Language model-based score computation
for addressee detection
countered by the Bing search engine. When users
want to follow a link displayed on screen, they usu-
ally speak a variant of the anchor text for the link.
We hypothesized that this corpus might aid the mod-
eling of computer-noncommand type utterances in
which such ?verbal clicks? are frequent. Fisher tele-
phone conversations and ICSI meetings are both cor-
pora of human-directed speech. The Fisher corpus
(Cieri et al, 2004) comprises two-person telephone
conversations between strangers on prescribed top-
ics. The ICSI meeting corpus (Janin et al, 2003)
contains multiparty face-to-face technical discus-
sions among colleagues.
3 Method
3.1 Language modeling for addressee detection
We use a lexical AD system that is based on mod-
eling word n-grams in the two addressee-based ut-
terance classes, H (for H-H) and C (for H-C utter-
ances). This approach is similar to language model-
based approaches to speaker and language recogni-
tion, and was shown to be quite effective for this
task (Shriberg et al, 2012). Instead of making
hard decisions, the system outputs a score that is
223
the length-normalized likelihood ratio of the two
classes:
1
|w|
log
P (w|C)
P (w|H)
, (1)
where |w| is the number of words in the recognition
output w for an utterance. P (w|C) and P (w|H) are
obtained from class-specific language models. Fig-
ure 2 gives a flow-chart of the score computation.
Class likelihoods are obtained from standard tri-
gram backoff language models, using Witten-Bell
discounting for smoothing (Witten and Bell, 1991).
For combining various training data sources, we use
language model adaptation by interpolation (Bel-
legarda, 2004). First, a separate model is trained
from each source. The probability estimates from
in-domain and out-of-domain models are then aver-
aged in a weighted fashion:
P (wk|hk) = ?Pin(wk|hk) + (1? ?)Pout(wk|hk)
(2)
where wk is the k-th word, hk is the (n ? 1)-gram
history for the wordwk. ? is the interpolation weight
and is obtained by tuning a task-related metric on the
development set. We investigated optimizing ? for
either model perplexity or classification accuracy, as
discussed below.
3.2 Part-of-speech-based modeling
So far we have only been modeling the lexical forms
of words in utterances. If we encounter a word never
before seen, it would appear as an out-of-vocabulary
item in all class-specific language models, and not
contribute much to the decision. More generally, if
a word is rare, its n-gram statistics will be unreliable
and poorly modeled by the system. (The sparseness
issue is exacerbated by small amounts of training
data as in our scenario.)
One common approach to deal with data sparse-
ness in language modeling is to model n-grams over
word classes rather than raw words (Brown et al,
1992). For example, if we have an utterance How
is the weather in Paris?, the addressee probabilities
are likely to be similar had we seen London instead
of Paris. Therefore, replacing words with properly
chosen word class labels can give better generaliza-
tion from the observed training data. Among the
many methods proposed to class words for language
modeling purposes we chose part-of-speech (POS)
tagging over other, purely data-derived classing al-
gorithms (Brown et al, 1992), for two reasons. First,
our goal here is not to minimize the perplexity of the
data, but to enhance discrimination among utterance
classes. Second, a data-driven class inference algo-
rithm would suffer from the same sparseness issues
when it comes to unseen and rare words (as no ro-
bust statistics are available to infer an unseen word?s
best class in the class induction step). A POS tag-
ger, on the other hand, can do quite well on unseen
words, using context and morphological cues.
A hidden Markov model tagger using POS-
trigram statistics and context-independent class
membership probabilities was used for tagging all
LM training data. The tagger itself had been
trained on the Switchboard (conversational tele-
phone speech) transcripts of the Penn Treebank-
3 corpus (Marcus et al, 1999), and used the 39
Treebank POS labels. To strike a compromise be-
tween generalization and discriminative power in
the language model, we retained the topN most fre-
quent word types from the in-domain training data
as distinct tokens, and varied N as a metaparam-
eter. Barzilay and Lee (2003) used a similar idea
to generalize patterns by substituting words with
slots. This strategy will tend to preserve words that
are either generally frequent function and domain-
independent words, capturing stylistic and syntac-
tic patterns, or which are frequent domain-specific
words, and can thus help characterize computer-
directed utterances.
Here is a sample sentence and its transformed ver-
sion:
Original: Let?s find an Italian restaurant
around this area.
POS-tagged: Let?s find an JJ NN around this
area.
The words except Italian and restaurant are un-
changed because they are in the list of N most fre-
quent words. We transformed all training and test
data in this fashion and then modeled n-gram statis-
tics as before. The one exception was the Bing
anchor-text data, which was only available in the
form of word n-grams (the sentence context required
for accurate POS tagging was missing).
224
Table 3: Addressee detection performance (EER) with different training sets
ASR Transcript
Baseline (in-domain only) 31.1 17.3
Fisher+ICSI, Single-user CB+Bing (out-of-domain only) 27.8 14.2
Baseline + Fisher+ICSI, Single CB + Bing (both-all) 26.9 14.0
Baseline + ICSI, Single-user CB (both-small) 26.6 13.0
3.3 Evaluation metrics
Typically, an application-dependent threshold would
be applied to the decision score to convert it into a
binary decision. The optimal threshold is a func-
tion of prior class probabilities and error costs. As
in Shriberg et al (2012), we used equal error rate
(EER) to compare systems, since we are interested
in the discriminative power of the decision score in-
dependent of priors and costs. EER is the probability
of false detections and misses at the operating point
at which the two types of errors are equally proba-
ble. A prior-free metric such as EER is more mean-
ingful than classification accuracy because the utter-
ance type distribution is heavily skewed (Table 1),
and because the rate of human- versus computer-
directed speech can vary widely depending on the
particular people, domain, and context. We also use
classification accuracy (based on data priors) in one
analysis below, because EERs are not comparable
for different test data subdivisions.
3.4 Online model
The actual dialog system used in this work pro-
cesses utterances after receiving an entire segment
of speech from the recognition subsystem. How-
ever, we envision that a future version of the sys-
tem would perform addressee detection in an online
manner, making a decision as soon as enough evi-
dence is gathered. This raises the question how soon
the addressee can be detected once the user starts
speaking. We simulate this processing mode using a
windowed AD model.
As shown in Figure 3, we define windows start-
ing at the beginning of the utterance and investigate
how AD performance changes as a function of win-
dow size. We use only the words and n-grams falling
completely within a given window. For example, the
word find would be excluded from Window 1 in Fig-
  >????         find       an     Italian    restaurant   around  this         area 
Window 1  
Window 2  ? 
Figure 3: The window model
ure 3.
The benefit of early detection in this case is that
once speech is classified as human-directed, it does
not need to be sent to the speech recognizer and sub-
sequent semantic processing. This saves processing
time, especially if processing happens on a server.
Based on the window model performance, we can
assess the feasibility of an online AD model, which
can be approached by shifting the detection window
through time and finding addressee changes.
4 Results and Discussion
Table 3 compares the performance of our system us-
ing various training data sources. For diagnostic pur-
poses we also compare performance based on recog-
nized words (the realistic scenario) to that based on
human transcripts (idealized, best-case word recog-
nition).
Somewhat surprisingly, the system trained on out-
of-domain data alone performs better by 3.3 EER
points on ASR output and 3.1 points on transcripts
compared to the in-domain baseline. Combining
in-domain and out-of-domain data (both-all, both-
small) gives about 1 point additional EER gain. Note
that training on in-domain data plus the smaller-size
out-of-domain corpora (both-small) is better than
using all available data (both-all).
Figure 4 shows the detection error trade-off
(DET) between false alarm and miss errors for the
225
8  7  6  
5  
4  
3  2  1  
Figure 4: Detection error trade-off (DET) curves for
the systems in Table 3. Thin lines at the top right
corner use ASR output (1-4); thick lines at the bot-
tom left corner use reference transcripts (5-8). Each
line number represents one of the systems in Table 3:
1,5 = in-domain only, 2,6 = out-of-domain only, 4,7
= both-all, 3,8 = both-small.
systems in Table 3. The DET plot depicts perfor-
mance not only at the EER operating point (which
lies on the diagonal), but over the range of possible
trade-offs between false alarm and miss error rates.
As can be seen, replacing or combining in-domain
data with out-of-domain data gives clear perfor-
mance gains, regardless of operating point (score
threshold), and for both reference and recognized
words.
Figure 5 shows H-H vs. H-C classification accu-
racies on each of the four utterance subtypes listed
in Table 1. It is clear that computer-command ut-
terances are the easiest to classify; the accuracy is
more than 90% using transcripts, and more than 85%
using ASR output. This is not surprising, since
commands are from a fixed small set of phrases.
The biggest gain from use of out-of-domain data
is found for computer-directed noncommand utter-
ances. This is helpful, since in general it is the
noncommand computer-directed utterances (rather
than the commands) that are highly confusable with
human-directed utterances: both use unconstrained
natural language. We note that H-H utterance are
very poorly recognized in the ASR condition when
only out-of-domain data is used. This may be be-
20 30
40 50
60 70
80 90
100
baseline(in-domain only)out-of-domainonlyboth-all
both-small
ASR  REF  
Figure 5: AD accuracies by utterance type
Table 4: Perplexities (computed on dev set ASR
words) by utterance type, for different training cor-
pora. Interpolation refers to the combination of the
three models listed in each case.
Test class
Training set H-C H-H
In-domain H-C (ASR) 257 1856
Single-user CB 104 1237
Bing anchor text 356 789
Interpolation 58 370
In-domain H-H (ASR) 887 1483
Fisher 995 795
ICSI meeting 2007 1583
Interpolation 355 442
cause the human-human corpora used in training
consist of transcripts, whereas the ASR output for
human-directed utterances is very errorful, creating
a severe train-test mismatch.
As for the optimization of the mixing weight ?,
we found that minimizing perplexity on the devel-
opment set of each class is effective. This is a
standard optimization approach for interpolated lan-
guage models, and can be carried out efficiently us-
ing an expectation maximization algorithm. We also
tried search-based optimization using the classifica-
tion metric (EER) as the criterion. While this ap-
proach could theoretically give better results (since
perplexity is not a discriminative criterion) we found
no significant improvement in our experiments.
226
Table 4 shows the perplexities by class of lan-
guage models trained on different corpora. We can
take these as an indication of training/test mismatch
(lower perplexity indicating better match). We also
find substantial perplexity reductions from interpo-
lating models. In order to make perplexities compa-
rable, we trained all models using the union of the
vocabularies from the different sources.
In spite of perplexity being a good way to opti-
mize the weighting of sources, it is not clear that it
is a good criterion for selecting data sources. For
example, we see that the Fisher model has a much
lower perplexity on H-H utterances than the ICSI
meeting model. However, as reflected in Table 3,
the H language model that leaves out the Fisher data
actually performed better. The most likely expla-
nation is that the Fisher corpus is an order of mag-
nitude larger than the ICSI corpus, and that sheer
data size, not stylistic similarity, may account for the
lower perplexity of the Fisher model. Further inves-
tigation is needed regarding good criteria for corpus
selection for classification tasks such as AD.
Table 5 shows the EER performance of the POS-
based model, for various sizes N of the most-
frequent word list. We observe that the partial re-
placement of words with POS tags indeed improves
over the baseline model performance, by 1.5 points
on ASR output and by 1.1 points on transcripts.
We also see that the gain over the corresponding
word-only model is largest for the in-domain base-
line model, and less or non-existent for the out-of-
domain model. This is consistent with the notion
that the in-domain model suffers the most from data
sparseness, and therefore has the most to gain from
better generalization.
Interpolating with out-of-domain data still helps
here. The optimal N differs for ASR output versus
transcripts. The POS-based model with N = 300
improves the EER by 0.5 points on ASR output,
and N = 1000 improves the EER by 0.8 points on
transcripts. Here we use relatively large amounts of
training data, thus the performance gain is smaller,
though still meaningful.
Figure 6 shows the performance of the system
using time windows anchored at the beginnings of
utterances. We incrementally increase the window
width from 0.5 seconds to 3 seconds and compare
results to using full utterances. The leveling off of
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.5 1 1.5 2 3 full
Equ
al er
ror r
ate 
Window width (seconds) 
ASR baselineASR outsideASR both-allASR both-smallREF baselineREF outsideREF both-allREF both-small
Figure 6: Simulated online performance on incre-
mental windows
Table 6: The top 15 first words in utterances
ASR H-C Transcript H-C ASR H-H Transcript H-H
go go play I
scroll scroll go ohh
start start is so
show stop it yeah
stop show what it?s
bing find this you
search Bing show uh
find search how okay
play pause bing what
pause play select it
look look okay and
what uh does that?s
select what start is
how how so no
the ohh I we
the error plots indicates that most addressee infor-
mation is contained in the first 1 to 1.5 seconds,
although some additional information is found in
the later part of utterances (the plots never level off
completely). This pattern holds for both in-domain
and out-of-domain training, as well as for combined
models.
To give an intuitive understanding of where this
early addressee-relevant information comes from,
we tabulated the top 15 word unigrams in each ut-
terance class, are shown in Table 6. Note that
the substantial differences between the third and
fourth columns in the table reflect the high ASR
error rate for human-directed utterances, whereas
227
Table 5: Performance of POS-based model with various top-N word lists (EER)
Training data top100 top200 top300 top400 top500 top1000 top2000 Original
ASR baseline 31.6 31.0 29.6 30.1 30.2 31.4 31.5 31.1
out-of-domain only 36.5 37.0 37.2 36.9 36.8 36.6 37.3 27.8
both-all 28.2 26.6 26.1 26.7 27.4 26.9 27.6 26.9
both-small 28.0 26.5 26.2 26.6 26.4 26.3 26.5 26.6
REF baseline 17.1 16.2 16.6 17.1 16.7 17.0 17.2 17.3
out-of-domain only 17.6 17.6 17.5 17.2 17.1 17.2 18.1 14.2
both-all 12.5 12.5 12.5 12.7 12.8 13.2 13.5 14.0
both-small 13.0 13.2 12.8 13.2 12.8 12.2 12.7 13.0
for computer-directed utterances, the frequent first
words are mostly recognized correctly.
In computer-directed utterances we see mostly
command verbs, which, due to the imperative syn-
tax of these commands occur in utterance-initial po-
sition. Human-directed utterances are characterized
by subject pronouns such as I and it, or answer parti-
cles such as yeah and okay, which likewise occur in
initial position. Based on word frequency and syn-
tax alone it is thus clear why the beginnings of utter-
ances contain strong lexical cues.
5 Conclusion
We explored the use of outside data for training
lexical addressee detection systems for the human-
human-computer scenario. Advantages include sav-
ing the time and expense of an in-domain data col-
lection, as well as performance gains even when
some in-domain data is available. We show that H-
C training data can be obtained from a single-user
H-C collection, and that H-H speech can be mod-
eled using general conversational speech. Using the
outside training data, we obtain results that are even
better than results using matched (but smaller) H-
H-C training data. Results can be improved consid-
erably by adapting H-C and H-H language models
with small amounts of matched H-H-C data, via in-
terpolation. The main reason for the improvement is
better detection of computer-directed noncommand
utterances, which tend to be confusable with human-
directed utterances. Another effective way to over-
come scarce training data is to replace the less fre-
quent words with part-of-speech labels. In both
baseline and interpolated model, we found that POS-
based models that keep an appropriate number of the
topN most frequent word types can further improve
the system?s performance.
In a second study we found that the most salient
phrases for lexical addressee detection occur within
the first 1 to 1.5 seconds of speech in each utter-
ance. It reflects a syntactic tendency of class-specific
words to occur utterance-initially, which shows the
feasibility of the online AD system.
Acknowledgments
We thank our Microsoft colleagues Madhu
Chinthakunta, Dilek Hakkani-Tu?r, Larry Heck,
Lisa Stiefelman, and Gokhan Tu?r for developing
the dialog system used in this work, as well as for
many valuable discussions. Ashley Fidler was in
charge of much of the data collection and annotation
required for this study. We also thank Dan Jurafsky
for useful feedback.
228
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings HLT-NAACL
2003, pages 16?23, Edmonton, Canada.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42:93?108.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn tak-
ing in situated dialog: Study, lessons, and directions.
In Proceedings ACL SIGDIAL, pages 98?109, Port-
land, OR.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings 4th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 69?71, Lisbon.
John Dowding, Richard Alena, William J. Clancey,
Maarten Sierhuis, and Jeffrey Graham. 2006. Are
you talking to me? dialogue systems supporting mixed
teams of humans and robots. In Proccedings AAAI
Fall Symposium: Aurally Informed Performance: Inte-
grating Machine Listening and Auditory Presentation
in Robotic Systems, Washington, DC.
Dilek Hakkani-Tu?r, Gokhan Tur, and Larry Heck. 2011a.
Research challenges and opportunities in mobile appli-
cations [dsp education]. IEEE Signal Processing Mag-
azine, 28(4):108 ?110.
Dilek Z. Hakkani-Tu?r, Go?khan Tu?r, Larry P. Heck, and
Elizabeth Shriberg. 2011b. Bootstrapping domain de-
tection using query click logs for new domains. In
Proceedings Interspeech, pages 709?712.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansang Wang, and Fritz Behr. 2010. Exploring web
scale language models for search query processing. In
Proceedings 19th International Conference on World
Wide Web, pages 451?460, Raleigh, NC.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The ICSI meeting corpus. In Pro-
ceedings IEEE ICASSP, volume 1, pages 364?367,
Hong Kong.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings 6th International Conference
on Multimodal Interfaces, ICMI, pages 144?151, New
York, NY, USA. ACM.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, catalog item LDC99T42.
Rieks op den Akker and David Traum. 2009. A com-
parison of addressee detection methods for multiparty
conversations. In Proceedings of Diaholmia, pages
99?106.
Tim Paek, Eric Horvitz, and Eric Ringger. 2000. Con-
tinuous listening for unconstrained spoken dialog. In
Proceedings ICSLP, volume 1, pages 138?141, Bei-
jing.
Daniel Reich, Felix Putze, Dominic Heger, Joris Ijssel-
muiden, Rainer Stiefelhagen, and Tanja Schultz. 2011.
A real-time speech command detector for a smart con-
trol room. In Proceedings Interspeech, pages 2641?
2644, Florence.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r,
and Larry Heck. 2012. Learning when to listen:
Detecting system-addressed speech in human-human-
computer dialog. In Proceedings Interspeech, Port-
land, OR.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
229
Proceedings of the SIGDIAL 2014 Conference, pages 238?242,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Detecting Inappropriate Clarification Requests in Spoken Dialogue
Systems
Alex Liu
1
, Rose Sloan
2
, Mei-Vern Then
1
, Svetlana Stoyanchev
3
,
Julia Hirschberg
1
, Elizabeth Shriberg
4
Columbia University
1
, Yale University
2
, AT&T Labs Research
3
, SRI International
4
{al3037@columbia.edu, rose.sloan@yale.edu,
mt2837@columbia.edu, sveta@research.att.com,
julia@cs.columbia.edu, elizabeth.shriberg@sri.com}
Abstract
Spoken Dialogue Systems ask for clarifi-
cation when they think they have misun-
derstood users. Such requests may dif-
fer depending on the information the sys-
tem believes it needs to clarify. However,
when the error type or location is misiden-
tified, clarification requests appear confus-
ing or inappropriate. We describe a clas-
sifier that identifies inappropriate requests,
trained on features extracted from user re-
sponses in laboratory studies. This classi-
fier achieves 88.5% accuracy and .885 F-
measure in detecting such requests.
1 Introduction
When Spoken Dialogue Systems (SDS) believe
they have not understood a user, they generate re-
quests for clarification. For example, in the fol-
lowing exchange, the System believes it has mis-
understood the word Washington in the user?s ut-
terance and asks a clarification question, prompt-
ing the user to repeat the misrecognized word.
User: I?d like a ticket to Washington.
System: A ticket to where?
User: Washington.
Clarification requests may be generic or specific
to the type and location of the information the sys-
tem believes it has not recognized. Targeted clar-
ifications focus on a specific part of an utterance,
as in the system?s question above. They use under-
stood portions of an utterance (?I?d like a ticket
to?) to query a misunderstood portion (?Wash-
ington?). Targeted clarification is a type of task-
related request, which has been shown to be more
effective and prevalent in human-human dialogues
than more general clarification requests (Skantze,
2005). Such generic clarifications signal mis-
understanding without identifying the type or lo-
cation of the misunderstanding. They often take
the form of a request to repeat or rephrase, e.g.
?please repeat?, ?please rephrase?, ?what did
you say??.
Questions that address a particular type of mis-
recognition come in several varieties. Systems
may ask reprise clarification questions, by repeat-
ing a recognized portion of an utterance (Ginzburg
and Cooper, 2004; Purver, 2004). Systems may
also request that users spell a word if they be-
lieve the misrecognized word is a proper name,
especially one that is not in its vocabulary (OOV).
They may ask the user to provide a synonym for
OOV terms that are not proper names. Systems
may also ask users to disambiguate homophones
(e.g. ?Did you mean ?right? as in correct or ?rite? as
in a ritual??). They may request confirmation ex-
plicitly (e.g. ?I heard you say Washington. Is that
correct??), or implicitly, by repeating the recog-
nized information while asking a follow-up query
(e.g. ?When do you want to go to Washington??).
Each request type may be appropriate in different
circumstances. However, when systems make in-
appropriate requests to users, such as to rephrase
a proper name or to confirm a statement that con-
tains a misrecognized word, dialogues often go
awry. Therefore, it is extremely important for sys-
tems to know when a request is inappropriate, so
that they can provide a different clarification re-
quest or fall back to a more generic strategy.
In this work, we develop a data-driven method
for detecting inappropriate clarification requests.
We have defined a list of inappropriate request
types and have collected a corpus of speaker re-
sponses to both appropriate and inappropriate re-
quests under laboratory conditions. We use this
corpus to train an inappropriate clarification clas-
sifier to be used by a system after a user responds
to a system request, in order to determine whether
the question was appropriate or not. In Section 2,
we describe previous research on error handling in
dialogue. We describe our data set in Section 3 and
238
our approach in Section 4. We present our evalua-
tion results in Section 5. We conclude in Section 6
and discuss future directions.
2 Related Work
Today?s SDS use generic approaches to clarifica-
tion, asking the user to repeat or rephrase an en-
tire utterance when the system believes it has not
been understood correctly. They use confidence
scores on the ASR hypothesis to decide whether
to accept, reject, or ask for clarification (Bohus
and Rudnicky, 2005). Hypotheses with low scores
may be confirmed and those with lower scores will
trigger a generic request for repetition or rephras-
ing. Researchers have found that the formulation
of system prompts has a significant effect on the
success of SDS interaction. Goldberg et al. (2003)
find that form of a clarification question affects
user frustration and the consequent success of clar-
ification subdialogue. In previous work, we ex-
plored the use of targeted reprise clarifications to
improve naturalness (Stoyanchev et al., 2014).
Lendvai et al. (2002) apply machine learning
methods to detect errors in human-machine di-
alogue, focusing on predicting when a user ut-
terance causes a misunderstanding. Litman et
al. (2006) identify user corrections of the system?s
recognition errors from speech prosody, ASR con-
fidence scores, and the dialogue history. In con-
trast, we focus here on detecting when a system
clarification request is the cause of dialogue prob-
lems. We employ only lexical features here, as
well as the type of system request, to investigate
user responses to a wide variety of system re-
quests, and to identify system errors in request for-
mulation from user reactions. In future work we
will include acoustic and prosodic features as well.
3 Data
Our data consists of spoken answers to clarifica-
tion requests collected at Columbia University us-
ing a simulated dialogue system in order to control
recognition results and type of system response.
The system displays a sentence and asks the user
to read it. The system then issues a pre-prepared
clarification request, which may be appropriate or
inappropriate, to which the user responds. For ex-
ample, in the following exchange, the system sim-
ulates a misunderstanding of the word furor by
asking a targeted reprise clarification question.
User: We hope this won?t create a furor.
System: Create a what?
User: A furor, an uproar.
The system issued six different types of clari-
fication requests: confirmation; rephrase, spell, or
disambiguate part of the utterance; targeted reprise
clarification; and a targeted-reprise-rephrase com-
bination. These request types were chosen based
on the types of requests made by the SRI Thunder-
BOLT speech-to-speech translation system (Ayan
and others, 2013). Confirmation questions sim-
ply ask the user to confirm an ASR hypothesis.
Rephrase-part requests ask users to rephrase a spe-
cific part of an utterance which is played back
to the user. Spell questions ask users to spell a
word or phrase using the NATO alphabet. Disam-
biguate questions clarify ambiguous terms. Tar-
geted reprise clarification questions make use of
the recognized portion of an utterance to query the
part that has been misrecognized based on the sys-
tem?s assessment. Targeted-reprise-rephrase re-
quests are similar, with the additional request for
the user to rephrase a portion of the utterance
believed to have been misrecognized, which is
played to the user.
Inappropriate requests in this study were de-
fined as those that resulted from the Thunder-
BOLT system?s incorrect identification of an er-
ror segment or an error type. For example, the
clarification request ?Please say a different word
for Afdhal? is inappropriate since it asks for a
rephrasal of a proper name. A request to spell
a very long phrase is also identified as inappro-
priate since users have found this difficult, espe-
cially when using the NATO alphabet. Requests
to disambiguate in the system provide two possi-
ble senses of the ambiguous word and are inap-
propriate when the correct sense is not one of the
two provided. Targeted reprise clarification ques-
tions are inappropriate when the error segment is
not correctly recognized and an errorful segment
is included in the question (e.g. ?The okay I zoo
would like what??). An appropriate question cor-
rectly identifies the error segment or ambiguous
term and the error type. For example, the ques-
tion ?I think ?Afdhal? is a name. Please spell it?,
would be appropriate when ?Afdhal? is OOV be-
cause it correctly targets the error and its type.
For each clarification request type, except for
confirmation questions, which are always appro-
priate, we created one or more types of inappro-
priate requests for each of the conditions we ob-
239
served in dialogues collected with the Thunder-
BOLT system. For example, when the system
asks the user to rephrase a part of their utter-
ance which the system believes to be a misrecog-
nized non-proper-name, the question is appropri-
ate when indeed that non-proper-name has been
misrecognized. However, the request will be in-
appropriate when the hypothesized error segment
played back to the user is a partial word, a proper
name, an extended segment including a name, or
a function word. We created instances of each
of these conditions for our users to respond to in
our experiment. A full list of the system question
types and their appropriate and inappropriate con-
ditions is provided in Table 3, in the Appendix.
We prepared 228 clarification requests (84 appro-
priate and 144 inappropriate), 12 for each of the 19
categories listed in Table 3 in the Appendix, based
on data in the TRANSTAC dataset (Akbacak and
others, 2009). Our subjects were 17 native Ameri-
can English speakers, each of whom answered 114
requests. We recorded speakers? answers to 714
appropriate and 1224 inappropriate requests. As
most request types have more than one inappro-
priate version, 63% of the requests in the data set
are inappropriate.
4 Experiment
We used the Weka machine learning library (Wit-
ten and Eibe, 2005) to train classifiers to predict
whether a clarification request was appropriate or
inappropriate. Our features were extracted from
transcripts of user utterances, and included lexical,
syntactic, numeric, and features from the output of
Linguistic Inquiry and Word Count (LIWC) (Pen-
nebaker et al., 2007) as described in Table 1.
We included unigram and bigram features, ex-
cluding unigrams that appeared fewer than 3 times
in the dataset (11% of the unigrams), and bi-
grams that appeared fewer than 2 times (25%),
with thresholds set empirically. LIWC features
were extracted using the LIWC 2007 software,
which includes lexical categories, such as articles
and negations, and psychological constructs, such
as affect and cognition. In one version of the
corpus, we replaced sequences of user spellings
with the tag ?SPELL? and disfluencies with the
symbol ?DISF?. We used the Stanford POS tag-
ger (Toutanova and others, 2003) to tag both
the original corpus as well as the modified ver-
sion. In the latter, we replaced the ?SPELL? and
Feature Description
word unigrams
(Lexical)
Count of unigrams
word bigrams
(Lexical)
Count of bigrams
pos bigrams
(Syntactic)
Bigrams of POS assigned by Stanford
tagger
liwc LIWC Output
func ratio Proportion of function words in re-
sponse
len spell Total length of spelling sequences in
response
request type Type of request preceding response
Table 1: Features used in Classification.
?DISF? tags with the symbols themselves. We
also mapped nine of the most frequent unigrams
to their own POS classes, such as ?no?, ?not?,
and ?neither? to ?NO? and ?word? to ?WORD?.
We then used counts of POS bigrams as a syn-
tactic feature. Additionally, as we observed that
responses to inappropriate requests contained a
higher proportion of function words, we added this
as a numeric feature. We also observed that aver-
age length of responses to inappropriate requests
was greater than responses to appropriate ones,
and we hypothesized this was in part due to in-
appropriate requests to spell long phrases. There-
fore, we also used the length of the total spelling
sequences, or the count of letters spelled out, as a
numeric feature. We also added type of clarifica-
tion request as a feature since some requests are
less likely to be inappropriate than others. For ex-
ample, we consider confirmation questions (?Did
you say . . . ??) to always be appropriate.
5 Results
We report classification results using Weka?s J48
decision tree classifier with 10-fold cross valida-
tion in Table 2, which outperformed JRip and
LibSVM in our experiments. Compared to the
majority baseline of 63.2% accuracy and .489 F-
measure, our classifier which uses all of the fea-
tures in Table 1 achieves a significant improve-
ment, with an accuracy of 88.5% and an F-
measure of .885. A baseline method that uses
only system request type feature (Req. type base-
line) achieves accuracy of 73.7% and F-measure
of .686, which is significantly below the perfor-
mance of the trained classifier. To identify the
most important features in predicting inappropri-
ate requests, we iteratively removed a single fea-
ture from the full feature set and re-evaluated pre-
diction accuracy. Table 2 shows absolute decrease
240
Features Acc (%) P/R/F-Measure
Majority baseline 63.2 * 0.399/0.632/0.489
Req. type baseline 73.7 * 0.814/0.737/0.686
All Features 88.5 0.885/0.885/0.885
less request type ?7.6 * ?0.076
less liwc ?2.3 ?0.023
less pos bigrams ?2.0 ?0.020
less word unigrams ?0.4 ?0.004
less func ratio ?0.1 ?0.001
less len spell ?0.05 ?0.0005
less word bigrams +0.05 +0.0007
Table 2: Classifying Inappropriate Requests: All
Features vs. Baseline vs. Leave-One-Out Classi-
fiers, where * indicates statistically significant dif-
ference from All Features (p < 0.01)
in percentage points and in F-measure when each
feature is removed in turn compared to the clas-
sifier trained on the full features set. We found
that system request type was the most important
feature, as performance decreased by 7.6 percent-
age points without it. This makes sense in light of
the fact that the ratio of inappropriate to appropri-
ate requests varied for the different request types
represented in our dataset. The next most useful
features were the output of LIWC and the POS
bigrams. We had hypothesized that, since LIWC
captures the presence of negations and assents, it
could capture negative user responses to the sys-
tem such as yes or no. As for the POS bigrams, we
modified the POS tags to mark common words and
included start and end markers in the bigrams be-
cause we hypothesized that the first words and last
words in the responses might be particularly infor-
mative. Looking at the decision tree created with
all our features, we find that the first five branches
involve decisions regarding the unigrams ?name?
and ?SPELL? (a collapsed spelling sequence), the
?START, ?neither?? bigram, the LIWC ingestion-
word feature, and the type of request, in that order.
Not only do these findings confirm our hypothe-
ses, they also confirm that the unigrams ?name?,
?SPELL?, and ?neither? which we had mapped to
special POS classes are particularly useful.
After training our model, we used it to classify
our entire dataset to see which responses it per-
formed well on and which it tended to misclassify.
Responses to targeted reprise and targeted-reprise-
rephrase questions together accounted for around
half of the misclassified instances. Many easily
identifiable responses to inappropriate requests in-
volved the user correcting the system, as in the fol-
lowing example:
User: You are going to need to dole out
punishment.
System: I think this is a name: ?dole out
punishment?. Please spell that name.
User: It is not a name, it is a phrase, dole
out punishment.
However, when the users did not correct the sys-
tem after an inappropriate request, their responses
appeared no different from answers to appropri-
ate requests. In the following example, the system
misrecognizes ?hyperbaric? and interprets it as the
word ?hyper? followed by an unknown phrase, but
the user simply ignores the request and repeats.
User: We are going to put you in a
hyperbaric chamber.
System: Put you in a high what? Please
give me another word or phrase
for ?perbaric?.
User: Hyperbaric chamber.
Many cases in which appropriate requests were
misclassified as inappropriate involved users re-
sponding correctly to targeted or targeted-rephrase
questions. We hypothesize that these are also due
primarily to users ignoring the inappropriate sys-
tem request and providing the information the sys-
tem should have asked for. As a result, those cases
make it difficult to distinguish between responses
to appropriate and inappropriate targeted ques-
tions. Of course, users may be giving prosodic
cues to indicate confusion or uncertainty or hyper-
articulating in their responses. We will address the
use of prosodic features in predicting inappropri-
ate requests in future work.
6 Conclusions
In this work, we have addressed a novel task of
identifying inappropriate clarification requests us-
ing features extracted from user responses. We
collected responses to inappropriate clarification
requests based on six request types in a simulated
SDS environment. The classifier trained on this
dataset detects inappropriate requests with accu-
racy of 88.5%, which is 25.3 percentage points
above the majority baseline, and an F-measure of
.885, which is .396 points above the majority F-
measure. In future work, we will include acoustic
and prosodic features as well as lexical features
and we will evaluate the use of an inappropriate
clarification request component in our speech-to-
speech translation system.
241
References
M. Akbacak et al. 2009. Recent advances in SRI?s
IraqComm
tm
Iraqi Arabic-English speech-to-speech
translation system. In ICASSP, pages 4809?4812.
N. F. Ayan et al. 2013. ?Can you give me another word
for hyperbaric??: Improving speech translation using
targeted clarification questions. In Acoustics, Speech
and Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8391?8395. IEEE.
D. Bohus and A. I. Rudnicky. 2005. A principled ap-
proach for rejection threshold optimization in spoken
dialog systems. In INTERSPEECH, pages 2781?2784.
J. Ginzburg and R. Cooper. 2004. Clarification, ellip-
sism and the nature of contextual updates. Linguistics
and Philosophy, 27(3).
J. Goldberg, M. Ostendorf, and K. Kirchhoff. 2003.
The impact of response wording in error correction
subdialogs. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
P. Lendvai, A. van den Bosch, E. Krahmer, and
M. Swerts. 2002. Improving machine-learned de-
tection of miscommunications in human-machine di-
alogues through informed data splitting. In Proceed-
ings of the ESSLLI Workshop on Machine Learning Ap-
proaches in Computational Linguistics, pages 1?15.
D. Litman, J. Hirschberg, and M. Swerts. 2006. Char-
acterizing and predicting corrections in spoken dia-
logue systems. Computational linguistics, 32(3):417?
438.
J. W. Pennebaker, C. K. Chung, M. Ireland, A. Gon-
zales, and R. J. Booth, 2007. The development and
psychometric properties of LIWC2007. Austin, TX.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
G. Skantze. 2005. Exploring human error recovery
strategies: Implications for spoken dialogue systems.
Speech Communication, 45(2-3):325?341.
S. Stoyanchev, A. Liu, and J. Hirschberg. 2014. To-
wards natural clarification questions in dialogue sys-
tems. In Proceedings of AISB2014.
K. Toutanova et al. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1.
Association for Computational Linguistics.
I. Witten and F. Eibe. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, 2nd edition.
Appendix
ID Simulation Appro. Example
1. Confirmation
1 Correctly recognized utterance yes Did you say ?place this on the pane??
2 Misrecognized utterance yes Did you say ?these are in um searches will cause the insur-
gents to priest buyer??
2. Rephrase-part
1 Full non-name word or phrase yes Please say a different word for ?surmise?.
2 Partial word no Please say a different word for ?nouncing?.
3 Name no Please say a different word for ?Afdhal?.
4 Extended segment including name no Please say a different word for ?checkpoint at Betirma?.
5 Function word no Please say a different word for ?off over?.
3. Disambiguate
1 One choice is correct yes Did you mean fliers as in handouts or fliers as in pilots?
2 Neither choice is correct no Did you mean plane as in aircraft or plain as in simple?
3 Word being disambiguated was not said no Did you mean sight as in vision or site as in location?
4. Spell
1 Name yes Please spell ?Hadi Al Hemdani?.
2 Non-name no I think this is a name: ?eluding?. Please spell that name.
3 Extended segment no Please spell ?staff are stealing themselves?.
5. Reprise
1 Error segment correctly recognized and
no other errors
yes We will search some of the what?
2 Recognition error right before ?what?
word
no Supplies of I see them what?
3 Recognition error which is not the last
word before ?what?
no Ask if they are for eating for what?
6. Reprise rephrase
1 No errors outside of the error segment yes Use a what? Please say another word for ?bristled?.
2 Error segment is a partial word no Are there any my what? Please say another word for ?nors?.
3 Error outside the targeted segment no Be a right is what? Please say another word for ?rain?.
Table 3: Clarification Requests and Contexts in which they are Appropriate and Inappropriate.
242
