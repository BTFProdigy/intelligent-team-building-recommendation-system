Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 79?87,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
A Dependency Based Statistical Translation Model 
 
Giuseppe Attardi 
 
Universit? di Pisa 
Dipartimento di Informatica 
attardi@di.unipi.it 
Atanas Chanev 
 
Universit? di Pisa 
Dipartimento di Informatica 
chanev@di.unipi.it 
Antonio Valerio Miceli Barone 
 
Universit? di Pisa 
Dipartimento di Informatica 
miceli@di.unipi.it 
 
 
Abstract 
We present a translation model based on 
dependency trees. The model adopts a tree-
to-string approach and extends Phrase-
Based translation (PBT) by using the de-
pendency tree of the source sentence for 
selecting translation options and for reor-
dering them. Decoding is done by translat-
ing each node in the tree and combining its 
translations with those of its head in alter-
native orders with respect to its siblings. 
Reordering of the siblings exploits a heu-
ristic based on the syntactic information 
from the parse tree which is learned from 
the corpus. The decoder uses the same 
phrase tables produced by a PBT system 
for looking up translations of single words 
or of partial sub-trees. A mathematical 
model is presented and experimental re-
sults are discussed.  
1 Introduction 
Several efforts are being made to incorporate syn-
tactic analysis into phrase-base statistical transla-
tion (PBT) (Och 2002; Koehn et. al. 2003), which 
represents the state of the art in terms of robust-
ness in modeling local word reordering and effi-
ciency in decoding. Syntactic analysis is meant to 
improve some of the pitfalls of PBT: 
 Translation options selection: candidate phrases 
for translation are selected as consecutive n-
grams. This may miss to consider certain syn-
tactic phrases if their component words are far 
apart. 
 Phrase reordering: especially for languages 
with different word order, e.g. subject-verb-
object (SVO) and subject-object-verb (SVO) 
languages, long distance reordering is a prob-
lem. This has been addressed with a distance 
based distortion model (Och 2002; Koehn et al 
2003), lexicalized phrase reordering (Tillmann, 
2004; Koehn, et.al., 2005; Al-Onaizan and Pa-
pineni, 2006), by hierarchical phrase reordering 
model (Galley and Manning, 2008) or by reor-
dering the nodes in a dependency tree (Xu et 
al., 2009) 
 Movement of translations of fertile words: a 
word with fertility higher than one can be trans-
lated into several words that do not occur con-
secutively. For example, the Italian sentence 
?Lui partir? domani? translates into German as 
?Er wird morgen abreisen?. The Italian word 
?partir?? (meaning ?will leave?) translates into 
?wird gehen? in German, but the infinite ?ab-
reisen? goes to the end of the sentence with a 
movement that might be quite long. 
Reordering of phrases is necessary because of dif-
ferent word order typologies of languages: consti-
tuent word order like SOV for Hindi vs. SVO for 
English; order of modifiers like noun?adjective for 
French, Italian vs. adjective-noun in English. Xu et 
al. (2009) tackle this issue by introducing a reor-
dering approach based on manual rules that are 
applied to the parse tree produced by a dependen-
cy parser. 
However the splitting phenomenon mentioned 
above requires more elaborate solutions than sim-
ple reordering grammatical rules. 
Several schemes have been proposed for im-
proving PBMT systems based on dependency 
trees.  Our approach extends basic PBT as de-
79
scribed in (Koehn et. al., 2003) with the following 
differences: 
 we perform tree-to-string translation. The de-
pendency tree of the source language sentence 
allows identifying syntactically meaningful 
phrases as translation options, instead of n-
grams. However these phrases are then still 
looked up in a Phrase Translation Table (PT) 
quite similarly to PBT. Thus we avoid the 
sparseness problem that other methods based 
on treelets suffer (Quirk et al, 2005). 
 reordering of phrases is carried out traversing 
the dependency tree and selecting as options 
phrases that are children of each head. Hence a 
far away but logically connected portion of a 
phrase can be included in the reordering. 
 phrase combination is performed by combining 
the translations of a node with those of its head. 
Hence only phrases that have a syntactic rela-
tion are connected. The Language Model (LM) 
is still consulted to ensure that the combination 
is proper, and the overall score of each transla-
tion is carried along.  
 when all the links in the parse tree have been 
reduced, the root node contains candidate trans-
lations for the whole sentences 
 alternative visit orderings of the tree may pro-
duce different translations so the final transla-
tion is the one with the highest score. 
Some of the benefits of our approach include: 
1) reordering is based on syntactic phrases rather 
than arbitrary chunks 
2) computing the future cost estimation can be 
avoided, since the risk of choosing an easier n-
gram is mitigated by the fact that phrases are 
chosen according to the dependency tree 
3) since we are translating from tree to string, we 
can directly exploit the standard phrase tables 
produced by PBT tools such as giza++ (Och 
and Ney, 2000) and Moses (Koehn, 2007) 
4) integration with the parser: decoding can be 
performed incrementally while a dependency 
Shift/Reduce parser builds the parse tree (At-
tardi, 2006). 
2 The  Dependency Based Decoder 
We describe in more detail the approach by pre-
senting a simple example. 
The translation of an input sentence is generated 
by reducing the dependency tree one link at a time, 
i.e. merging one node with its parent and combin-
ing their translations, until a single node remains. 
Links must be chosen in an order that preserves 
the connectivity of the dependency tree. Since 
there is a one-to-one correspondence between 
links and nodes (i.e. the link between a node and 
its head), we can use any ordering that corres-
ponds to a topological ordering of the nodes of the 
tree. 
A sentence is a sequence of words (w1, ? , wn), 
so we can use their index to identify words and 
hence each ordering is a permutation of those in-
dexes. 
Consider for example the dependency tree for 
the Italian sentence: Il ragazzo alto (?The tall 
boy?). 
 
There are only two possible topological orderings 
for this tree: 1-3-2 and 3-1-2.  
In principle the decoding process should ex-
plore all possible topological orderings for gene-
rating translations, but their number is too big, 
being proportional to the factorial of the number of 
words, so we will introduce later a criterion for 
selecting a subset of these, which conform best 
with the rules of the languages. 
Given a permutation we obtain a translation by 
merging in that order each node with its parent. 
The initialization step of the decoder creates 
nodes corresponding to the parse tree and collects 
translations for each individual word from the PT. 
 
ragazzo 
boy 
 
alto 
tall 
high 
Il 
The 
Il   ragazzo   alto 
80
Case 1: Permutation 1-3-2 
The first merge step is applied to the nodes for w1 
and its head w2, performing the concatenation of 
the translations of nodes il (the) and ragazzo (boy), 
both in normal and reverse order. Hence expansion 
of this hypothesis reduces the tree to the follow-
ing, where we show also the partial translations 
associated to each node. Each translation has asso-
ciated weights (i.e. the LM weight, the translation 
model weight, etc.) and a cumulative score. The 
score is the dot product of the weights for the sen-
tence and the vector of tuning parameters for the 
model. The score is used to rank the sentences and 
also to limit how many of them are kept according 
to the beam size parameter of the algorithm. 
 
The second step merges the node for word w3 (?al-
to?) with that of its head w2 (?ragazzo?) producing 
a single node with four translations: ?the boy tall?, 
?boy the tall?, ?tall the boy? and ?tall boy the?. 
 
Case 2: Permutation 3-1-2 
The first merge between w3 and w2 generates two 
translation fragments: ?boy tall? and ?tall boy?. 
The second one creates four translations: ?the boy 
tall?, ?boy tall the?, ?the tall boy?, ?tall boy the?. 
 
When the tree has been reduced to a single root 
node and the results of both permutations are col-
lected, the node will contain all eight alternative 
translations ranked according to the language 
model, so that the best one, possibly ?the tall boy?, 
can be selected as overall sentence translation. 
3 Node Merge 
The operation of node merge consists of taking all 
possible translations for the two nodes and conca-
tenating them in either sequential or reverse order, 
adding them to the translation of the parent node 
and dropping the child. 
In certain cases though, for example idiomatic 
phrases, the best translation is not obtained by 
combining the individual translations of each 
word, but instead a proper translation might be 
found in the Phrase Translation Table (PT). Hence 
besides performing combination of translations, 
we also consider the sub-tree rooted at the head 
node hri of node ri. We consider the phrase corres-
ponding to the leaves of the sub-tree rooted at hri 
and all children already merged into it, including 
ri: if this phrase is present in the PT, then its trans-
lations are also added to the node. 
This is sometimes useful, since it allows the de-
coder to exploit phrases that only correspond to 
partial sub-trees that it will otherwise miss. 
4 Reordering Rules 
In order to restrict the number of permutations to 
consider, we introduce a reordering step based on 
rules that examine the dependency tree of the 
source sentence. 
The rules are dependent on the language pair 
and they can be learned automatically from the 
corpus. 
We report first a simple set of hand crafted rules 
devised for the pair Italian-English that we used as 
a baseline. 
The default ordering is to start numbering the 
left children of a node backwards, i.e. the node 
closer to the head comes first, then continuing 
with the right children in sequential order. 
Special rules handle these cases: 
1) The head is a verb: move an adverb child to 
first position.  This lets a sequence of VA VM 
V R be turned into VA VM R V, where VA is 
the POS for auxiliary verbs, VM for modals, 
V for main verb and R for adverbs. 
2) The head is a noun: move adjectives or prepo-
sitions immediately following the head to the 
beginning. 
Il ragazzo alto 
the boy tall 
boy the tall 
tall the boy 
tall boy the 
Il ragazzo 
the boy 
boy the 
alto 
tall 
high 
81
4.1 Learning Reordering Rules 
In order to learn the reordering rules we created a 
word-aligned parallel corpus from 1.3 million 
source sentences selected from the parallel corpus. 
The corpus is parsed and each parse tree is ana-
lyzed using the giza++ word alignments of its 
translation to figure out node movements. 
For each source-language word, we estimate a 
unique alignment to a target-language word. If the 
source word is aligned to more than one target 
word we select the first one appearing in the 
alignment file. If a source word is not aligned to 
any word, we choose the first alignment in its des-
cendants in the dependency tree. If no alignment 
can be found in the descendants, we assume that 
the word stays in its original position. 
We reorder the source sentence according to 
this alignment, putting it in target-language order. 
We produce a training event consisting of a pair 
(context, offset) for each non-root word. The con-
text of the event consists of a set of features (the 
POS tag of a word, its dependency tag and the 
POS of its head) extracted for the word and its 
children. The outcome of the event is the offset of 
the word relative to its parent (negative for words 
that appear on the left of their parent in target-
language order, positive otherwise). 
We calculate the relative frequency of each 
event conditioned on the context, deriving rules of 
the form: 
(context, offset, Pr[Offset = offset | Context = 
context]). 
During decoding, we compute a reordering posi-
tion for each source word by adding to the word 
position to the offset predicted by the most likely 
reordering rule matching the word context (or 0 if 
no matching context is found). 
The reordering position drives the children 
combination procedure in the decoder. 
Our reordering rules are similar to those pro-
posed by Xu at al. (2009), except that we derive 
them automatically from the training set, rather 
than being hand-coded. 
4.2 Beam Search 
Search through the space of hypotheses generated 
is performed using beam search that keeps in each 
node the list of the top best translations for the 
node. The score for the translation is computed 
using the weights of the individual phrases that 
make up the translation and the overall LM proba-
bility of the combination. 
The scores are computed querying the standard 
Moses Phrase Table and the LM for the target lan-
guage; other weights uses by moses such as the 
reordering weights or the future cost estimates are 
discarded or not computed. 
5 The Model 
A mathematical model of the dependency based 
translation process can be formulated as follows. 
Consider the parse of a sentence f of length n. 
Let R denote all topological ordering of the nodes 
according to the dependency tree. 
Let fr denote the parse tree along with a consis-
tent node ordering r. Each ordering gives rise to 
several different translations. Let Er denote the set 
of translations corresponding to fr. We assign to 
each translation er  Er a probability according to 
the formula below. The final translation is the best 
result obtained through combinations over all or-
derings. 
Error! Objects cannot be created from editing field 
codes. 
Where er denotes any of the translations of f ob-
tained when nodes are combined according to 
node ordering r.  
The probability of a translation er corresponding 
to a node ordering r for a phrase f, p(er | f ) is de-
fined as: 
Error! Objects cannot be created from editing field 
codes. 
where 
Error! Objects cannot be created from editing 
field codes. andError! Objects cannot be 
created from editing field codes.denote the leaf 
words from node ri and those of its head node hri,  
respectively. 
Error! Objects cannot be created from edit-
ing field codes.is either Error! Objects cannot 
82
be created from editing field codes.or Error! 
Objects cannot be created from editing field 
codes. 
p(f, e) = pPT(str(f), e) if str(f)  PT 
str(f) is the sentence at the leaves of node ri 
pLM is the Language Model probability 
pPT is the Phrase Table probability 
6 Related Work 
Yamada and Knight (2001) introduced a syntax-
based translation model that incorporated source-
language syntactic knowledge within statistical 
translation. Many similar approaches are based on 
constituent grammars, among which we mention 
(Chiang, 2005) who introduced hierarchical trans-
lation models. 
The earliest approach based on dependency 
grammars is the work by Ashlawi et al (2000), 
who developed a tree-to-tree translation model, 
based on middle-out string transduction capable of 
phrase reordering. It translated transcribed spoken 
utterances from English to Spanish and from Eng-
lish to Japanese. Improvements were reported over 
a word-for-word baseline. 
Ambati (2008) presents a survey of other ap-
proaches based on dependency trees. 
Quirk et. al. (2005) explore a tree-to-tree ap-
proach, called treelet translation, that extracts tree-
lets, i.e. sub-trees, from both source and target 
language by means of a dependency parser. A 
word aligner is used to align the parallel corpus. 
The source dependency is projected onto the target 
language sentence in order to extract treelet trans-
lation pairs. Given a foreign input sentence, their 
system first generates its dependency tree made of 
treelets. These treelets are translated into treelets 
of the target language, according to the dependen-
cy treelet translation model. Translated treelets are 
then reordered according to a reorder model. 
The ordering model is trained on the parallel 
corpus. Treelet translation pairs are used for de-
coding. The reordering is done at the treelet level 
where all the child nodes of a node are allowed all 
possible orders. The results show marginal im-
provements in the BLEU score (40.66) in compar-
ison with Pharaoh and MSR-MT.  But the treelet 
translation algorithm is more than an order of 
magnitude slower. 
Shen et. al. (2008) present a hierarchical ma-
chine translation method from string to trees. The 
scheme uses the dependency structure of the target 
language to use transfer rules while generating a 
translation. The scheme uses well-formed depen-
dency structure which involves fixed and floating 
type structures. The floating structures allow the 
translation scheme to perform different concatena-
tion, adjoining and unification operations still be-
ing within the definition of well-formed structures. 
While decoding the scheme uses the probability of 
a word being the root, and also the left-side, right-
side generative probabilities. The number of rules 
used varies from 27 M (for a string to dependency 
system) to 140 M (baseline system). The perfor-
mance reached 37.25% for the system with 3-
grams, 39.47% for 5-grams. 
Marcu and Wong (2002) propose a joint- prob-
ability model. The model establishes a correspon-
dence between a source phrase and a target phrase 
through some concept. The reordering is inte-
grated into the joint probability model with the 
help of: 
3) Phrase translation probabilities Error! Ob-
jects cannot be created from editing field 
codes. denoting the probability that concept ci 
generates the translation Error! Objects can-
not be created from editing field codes. for 
the English and Error! Objects cannot be 
created from editing field codes. for the for-
eign language inputs. 
4) Distortion probabilities based on absolute po-
sitions of the phrases.  
Decoding uses a hill-climbing algorithm.  Perfor-
mance wise the approach records an average 
BLEU score of 23.25%, with about 2% of im-
provement over the baseline IBM system. 
Zhang et. al. (2007) present a reordering model 
that uses linguistic knowledge to guide both 
phrase reordering and translation between linguis-
tically correct phrases by means of rules. Rules are 
encoded in the form of weighted synchronous 
grammar and express transformations on the parse 
trees. They experiment also mixing constituency 
and dependency trees achieving some improve-
83
ments in BLEU score (27.37%) over a baseline 
system (26.16%). 
Cherry (2008) introduces a cohesion feature in-
to a traditional phrase based decoder. It is imple-
mented as a soft constraint which is based on the 
dependency syntax of the source language. He 
reports a BLEU score improvement on French-
English translation. 
The work by Xu et al (2009) is the closest to 
our approach. They perform preprocessing of the 
foreign sentences by parsing them with a depen-
dency parser and applying a set of hand written 
rules to reorder the children of certain nodes. The 
preprocessing is applied to both the training cor-
pus and to the sentences to translate, hence after 
reordering a regular hierarchical system can be 
applied. Translation experiments between English 
and five non SVO Asian languages show signifi-
cant improvements in accuracy in 4 out of 5 lan-
guages. With respect to our approach the solution 
by Xu et al does not require any intervention on 
the translation tools, since the sentences are rewrit-
ten before being passed to the processing chain: on 
the other hand the whole collection has to undergo 
full parsing with higher performance costs and 
higher dependency on the accuracy of the parser. 
Dyer and Resnik (2010) introduce a translation 
model based on a Synchronous Context Free 
Grammar (SCFG). In their model, translation 
examples are stored as a context-free forest. The 
process of translation comprise two steps: tree-
based reordering and phrase transduction. While 
reordering is modeled with the context-free forest, 
the reordered source is transduced into the target 
language by a Finite State Transducer (FST). The 
implemented model is trained on those portions of 
the data which it is able to generate. An increase 
of BLEU score is achieved for Chinese-English 
when compared to the phrase based baseline. 
Our approach is a true tree-to-string model and 
differs from (Xu et al, 2009), which uses trees 
only as an intermediate representation to rearrange 
the original sentences. We perform parsing and 
reordering only on the phrases to be translated. 
The training collection is kept in the original form, 
and this has two benefits: training is not subject to 
parsing errors and our system can share the same 
model of a regular hierarchical system. 
Another difference is in the selection of transla-
tion options: our method exploits the parse tree to 
select grammatical phrases as translation options. 
7 Implementation 
The prototype decoder consists of the following 
components: 
1) A specialized table lookup server, providing 
an XML-RPC interface for querying both the 
phrase table and the LM 
2) A parser engine based on DeSR (DeSR, 2009) 
3) A reordering algorithm that adds ordering 
numbers to the output produced by DeSR in 
CoNLL-X format. Before reordering, this step 
also performs a restructuring of the parse tree, 
converting from the conventions of the Italian 
Tanl Treebank to a structure that helps the 
analysis. In particular it converts conjunctions, 
which are represented as chains, where each 
conjunct connects to the previous, to a tree 
where they are all dependent of the same head 
word. Compound verbs are also revised: in the 
dependency tree each auxiliary of a verb is a 
direct child of the main verb. For example in 
?avrebbe potuto vedere?, both the auxiliary 
?avrebbe? and the modal ?potuto? depend on 
the verb ?vedere?.  This steps groups all aux-
iliaries of a verb under the first one, i.e. ?potu-
to?. This helps so that the full auxiliary can be 
looked up separately from the verb in the 
phrase table. 
4) A decoder that uses the output produced by 
the reordering algorithm, queries the phrase 
table and performs a beam search on the hypo-
theses produced according to the suggested 
reordering. 
8 Experimental Setup and Results 
Moses (Koehn et al, 2007) is used as a baseline 
phrase-based SMT system. The following tools 
and data were used in our experiments:  
1) the IRSTLM toolkit (Marcello and Cettolo, 
2007) is used to train a 5-gram language mod-
84
el with Kneser-Ney smoothing on a set of 4.5 
million sentences from the Italian Wikipedia. 
2) the Europarl version 6 corpus, consisting of 
1,703,886 sentence pairs, is used for training. 
A tuning set of 2000 sentences from ACL 
WMT 2007 is used to tune the parameters.  
3) the model is trained with lexical reordering. 
4) the model is tuned with mert (Bertoldi, et al ) 
5) the official test set from ACL WMT 2008 
(Callison-Burch et al, 2008), consisting of 
2000 sentences, is used as test set. 
6) the open-source parser DeSR (DeSR, 2009) is 
used to parse Italian sentences, trained on the 
Evalita 2009 corpus (Bosco et al, 2009). Pars-
er domain adaptation is obtained by adding to 
this corpus a set of 1200 sentences from the 
ACL WMT 2005 test set, parsed by DeSR and 
then corrected by hand. 
Both the training corpora and the test set had to be 
cleaned in order to normalize tokens: for example 
the English versions contained possessives split 
like this ?Florence' s?. We applied the same toke-
nizer used by the parser which conforms to the 
PTB standard. 
DeSR achieved a Labeled Accuracy Score of 
88.67% at Evalita 2009, but for the purpose of 
translation, just the Unlabeled Accuracy is rele-
vant, which was 92.72%. 
The table below shows the results of our decod-
er (Desrt) in the translation from Italian to English, 
compared to a baseline Moses system trained on 
the same corpora and to the online version of 
Google translate. 
Desrt was run with a beam size of 10, since ex-
periments showed no improvements with a larger 
beam size. 
We show two versions of Desrt, one with parse 
trees as obtained by the parser and one (Desrt 
gold) where the trees were corrected by hand. The 
difference is minor and this confirms that the de-
coder is robust and not much affected by parsing 
errors. 
System BLEU NIST 
Moses 29.43 7.22 
Moses tree phrases 28.55 7.10 
Desrt gold 26.26 6.88 
Desrt 26.08 6.86 
Google Translate 24.96 6.86 
Desrt learned 24.37 6.76 
Table 1. Results of the experiments. 
Since we used the same phrase table produced by 
Moses also for Desrt, Moses has an advantage, 
because it can look up n-grams that do not corres-
pond to grammatical phrases, which Desrt never 
considers. In order to determine how this affects 
the results, we tested Moses restricting its choice 
to phrases corresponding to treelets form the parse 
tree. The result is shown in the row in the table 
labeled as ?Moses tree phrases?. The score is low-
er, as expected, but this confirms that Desrt makes 
quite good use of the portion of the phrase table it 
uses. 
Since the version of the reordering algorithm we 
used produces a single reordering, the Desrt de-
coder has linear complexity on the length of the 
sentence. Indeed, despite being written in Python 
and having to query the PT as a network service, it 
is quite faster than Moses.  
9 Error Analysis 
Despite that fact that Desrt is driven by the parse 
tree, it is capable of selecting fairly good and even 
long sentences for look up in the phrase table. 
How close is the Desrt translation from those of 
the Moses baseline can be seen from this table: 
 1-gram 2-gram 3-gram 4-gram 5-gram 
NIST 7.28 3.05 1.0 0.27 0.09 
BLEU 84.73 67.69 56.94 48.59 41.78 
Sometimes Desrt fails to select a better translation 
for a verb, since it looks up prepositional phrases 
separately from the verb, while Moses often con-
nects the preposition to the verb. 
This could be improved by performing a check 
and scoring higher translations which include the 
translation of the preposition dependent on the 
verb. 
Another improvement could come from creating 
phrase tables limited to treelet phrases, i.e. phrases 
corresponding to treelets from the parser. 
85
10 Enhancements 
The current algorithm needs to be improved to 
fully deal with certain aspects of long distance 
dependencies. Consider for example the sentence 
?The grass around the house is wet?. The depen-
dency tree of the sentence contains the non-
contiguous phrases ?The grass? and ?wet?, whose 
Italian translation must obey a morphological 
gender agreement between the subject ?grass? 
(?erba?, feminine), and the adjective ?wet? (?bag-
nata?). 
However, the current combination algorithm 
does not exploit this dependence, because the last 
phases of node merge will occur when the tree has 
been reduced to this: 
The PT however could tell us that ?erba bagnata? 
is more likely than ?erba bagnato? and allow us to 
score the former higher. 
11 Conclusions 
We have described a decoding algorithm guided 
by the dependency tree of the source sentence. By 
exploiting the dependency tree and deterministic 
reordering rules among the children of a node, the 
decoder is fast and can be kept simple by avoiding 
to consider multiple reorderings, to use reordering 
weights and to estimate future costs. 
There is still potential for improving the algo-
rithm exploiting information implicit in the PT in 
terms of morphological constraints, while main-
taining a simple decoding algorithm that does not 
involve complex grammatical transformation 
rules. 
The experiments show encouraging results with 
respect to state of the art PBT systems. We plan to 
test the system on other language pairs to see how 
it generalizes to other situations where phrase 
reordering is relevant. 
Acknowledgments 
Zauhrul Islam helped setting up our baseline sys-
tem and Niladri Chatterjie participated in the early 
design of the model.  
References 
G. Attardi. 2006. Experiments with a Multilanguage 
Non-Projective Dependency Parser. Proc. of the 
Tenth Conference on Natural Language Learning, 
New York, (NY). 
H. Alshawi, S. Douglas and S. Bangalore. 2000. 
Learning Dependency Translation Models as 
Collections of Finite State Head Transducers. 
Computational Linguistics 26(1), 45?60. 
N. Bertoldi, B. Haddow, J-B. Fouet. 2009. Improved 
Minimum Error Rate Training in Moses. In Proc. of 
3rd MT Marathon, Prague, Czech Republic. 
V. Ambati. 2008. Dependency Structure Trees in Syn-
tax Based Machine Translation. Adv. MT Seminar 
Course Report. 
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo, F. 
Dell?Orletta and A. Lenci. 2009. Evalita?09 Parsing 
Task: comparing dependency parsers and treebanks. 
Proc. of Evalita 2009. 
P. F. Brown, V. J. Della Pietra, S. A. and  R. L. Mercer. 
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Lin-
guistics, 19(2), 263?311. 
Callison-Burch et al 2008. Further Meta-Evaluation of 
Machine Translation. Proc. of ACL WMT 2008. 
C. Cherry. 2008. Cohesive phrase-based decoding for 
statistical machine translation. Proc. of ACL 2008: 
HLT. 
D. Chiang. 2005. A hierarchical phrase-based model for 
statistical machine translation. In Proc. of ACL 2005.  
DeSR. Dependency Shift Reduce parser. 
http://sourceforge.net/projects/desr/ 
Y. Ding, and M. Palmer. 2005.  Machine Translation 
using Probabilistic Synchronous Dependency Inser-
tion Grammar.  Proc. of ACL?05, 541?548. 
C. Dyer and P. Resnik. 2010. Context-free reordering, 
finite-state translation. Proc. of HLT: The 2010 
Annual Conference of the North American Chapter 
of the ACL, 858?866. 
grass 
L? erba intorno alla casa 
is 
? 
wet 
bagnata 
bagnato 
 
 
86
F. Marcello, M. Cettolo. 2007. Efficient Handling of N-
gram Language Models for Statistical Machine 
Translation. Workshop on Statistical Machine Trans-
lation 2007. 
M. Galley and C. D. Manning. 2008. A Simple and 
Effective Hierarchical Phrase Reordering Model. In 
Proc. of EMNLP 2008. 
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas and O. 
Kolak, 2005.  Bootstrapping Parsers via Syntactic 
Projection across Parallel texts. Natural Language 
Engineering 11(3), 311-325. 
P. Koehn, F. J. Och and D. Marcu.  2003. Statistical 
Phrase-Based Translation. Proc. of Human Lan-
guage Technology and North American Association 
for Computational Linguistics Conference 
(HLT/NAACL), 127?133. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and 
E. Herbst. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proc. of the 45th An-
nual Meeting of the ACL, demonstration session, 
177?180, Prague, Czech Republic. 
P. Koehn. 2009. Statistical Machine Translation.  
Cambridge University Press. 
Y. Liu, Q. Liu and S. Lin. 2006. Tree-to-string Align-
ment Template for Statistical Machine Translation, 
In Proc. of COLING-ACL. 
D. Marcu and W. Wong. 2002. A Phrase-Based Joint 
Probability Model for Statistical Machine Transla-
tion. Proc. Empirical Methods in Natural Language 
Processing (EMNLP), 133?139. 
C. Quirk, A. Menzes and C. Cherry. 2005. Dependency 
Treelet Translation: Syntactically Informed Phrasal 
SMT. Proc. 43rd Annual Meeting of the ACL, 217?
279. 
S. Libin, J. Xu and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with 
a Target Dependency Language Model. Proc. ACL-
08, 577?585. 
F. J. Och 2002. Statistical Machine Translation: From 
Single Word Models to Alignment Template. Ph.D. 
Thesis, RWTH Aachen, Germany. 
F.J. Och, H. Ney. 2000. Improved Statistical Alignment 
Models. Proc. of the 38th Annual Meeting of the 
ACL.  Hong Kong, China. 440-447. 
K. Yamada and K. Knight. 2001. A Syntax-Based Sta-
tistical Translation Model. Proc. 39th Annual Meet-
ing of ACL (ACL-01), 6?11. 
P. Xu, J. Kang, M. Ringgaard and F. Och. 2009. Using 
a Dependency Parser to Improve SMT for Subject-
Object-Verb Languages. Proc. of NAACL 2009, 245?
253, Boulder, Colorado. 
D. Zhang, Mu Li, Chi-Ho Li and M. Zhou.  2007. 
Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT.  Proc. Joint Conference on 
Empirical Methods in Natural Language Processing 
and Computational  Natural Language Processing: 
533?540. 
 
 
87
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55?59,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency Parsing domain adaptation using transductive SVM
Antonio Valerio Miceli-Barone
University of Pisa, Italy /
Largo B. Pontecorvo, 3, Pisa, Italy
miceli@di.unipi.it
Giuseppe Attardi
University of Pisa, Italy /
Largo B. Pontecorvo, 3, Pisa, Italy
attardi@di.unipi.it
Abstract
Dependency Parsing domain adaptation
involves adapting a dependency parser,
trained on an annotated corpus from a given
domain (e.g., newspaper articles), to work
on a different target domain (e.g., legal doc-
uments), given only an unannotated corpus
from the target domain.
We present a shift/reduce dependency
parser that can handle unlabeled sentences
in its training set using a transductive SVM
as its action selection classifier.
We illustrate the the experiments we per-
formed with this parser on a domain adap-
tation task for the Italian language.
1 Introduction
Dependency parsing is the task of identifying syn-
tactic relationships between words of a sentence
and labeling them according to their type. Typ-
ically, the dependency relationships are not de-
fined by an explicit grammar, rather implicitly
through a human-annotated corpus which is then
processed by a machine learning procedure, yield-
ing a parser trained on that corpus.
Shift-reduce parsers (Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; Attardi, 2006) are
an accurate and efficient (linear complexity) ap-
proach to this task: They scan the words of a sen-
tence while updating an internal state by means of
shift-reduce actions selected by a classifier trained
on the annotated corpus.
Since the training corpora are made by human an-
notators, they are expensive to produce and are
typically only available for few domains that don?t
adequately cover the whole spectrum of the lan-
guage. Parsers typically lose significant accuracy
when applied on text from domains not covered
by their training corpus. Several techniques have
been proposed to adapt a parser to a new domain,
even when only unannotated samples from it are
available (Attardi et al, 2007a; Sagae and Tsujii,
2007).
In this work we present a domain adaptation based
on the semi-supervised training of the classifier of
a shift-reduce parser. We implement the classifier
as a multi-class SVM and train it with a transduc-
tive SVM algorithm that handles both labeled ex-
amples (generated from the source-domain anno-
tated corpus) and unlabeled examples (generated
from the the target-domain unannotated corpus).
2 Background
2.1 Shift-Reduce Parsing
A shift-reduce dependency parser is essentially a
pushdown automaton that scans the sentence one
token at a time in a fixed direction, while updat-
ing a stack of tokens and also updating a set of
directed, labeled edges that is eventually returned
as the dependency parse graph of the sentence.
Let T be the set of input token instances of
the sentence and D be the set of dependency
labels. The state of the parser is defined by
the tuple ?s, q, p?, where s ? T ? is the stack,
q ? T ? is the current token sequence and p ?
{
E|E ? 2T?T?D, E is a forest
}
is the current
parse graph.
The parser starts in the state ?[], q0, {}?, where q0
is the input sentence, and terminates whenever it
reaches a state in the form ?s, [], p?. At each step,
55
it performs one of the following actions:
shift :
?s, [t|q], p?
?[t|s], q, p?
rightreduced :
?[u|s], [t|q], p?
?s, [t|q], p ? {(u, t, d)}?
leftreduced :
?[u|s], [t|q], p?
?s, [u|q], p ? {(t, u, d)}?
note that there are rightreduced and
leftreduced actions for each label d ? D.
Action selection is done by the combination
of two functions f ? c : a feature extraction
function f : States ? Rn that computes a
(typically sparse) vector of numeric features of
the current state and the multi-class classifier
c : R ? Actions. Alternatively, the classifier
could score each available action, allowing a
search procedure such as best-first (Sagae and
Tsujii, 2007) or beam search to be used.
In our experiments we used an extension of
this approach that has an additional stack and
additional actions to handle non-projective de-
pendency relationships (Attardi, 2006). Training
is performed by computing, for each sentence
in the annotated training corpus, a sequence of
states and actions that generates its correct parse,
yielding, for each transition, a training example
(x, y) ? Rn ?Actions for the classifier.
Various classification algorithms have been
successfully used, including maximum entropy,
multi-layer perceptron, averaged perceptron,
SVM, etc. In our approach, the classifier is
always a multi-class SVM composed of multiple
(one-per-parsing-action) two-class SVMs in
one-versus-all configuration.
2.2 Parse Graph Revision
Attardi and Ciaramita (2007b) developed a
method for improving parsing accuracy using
parse graph revision: the output of the parser is
fed to a procedure that scans the parsed sentence
in a fixed direction and, at each step, possibly re-
vises the current node (rerouting or relabeling its
unique outgoing edge) based on the classifier?s
output.
Training is performed by parsing the training cor-
pus and comparing the outcome against the anno-
tation: for each sentence, a sequence of actions
necessary to transform the machine-generated
parse into the reference parse is computed and it
is used to train the classifier. (Usually, a lower-
quality parser is used during training, assuming
that it will generate more errors and hence more
revision opportunities).
This method tends to produce robust parsers: er-
rors in the first stage have the opportunity to be
corrected in the revision stage, thus, even if it
does not learn from unlabeled data, it neverthe-
less performs well in domain adaptation tasks (At-
tardi et al, 2007a). In our experiments we used
parse graph revision both as a baseline for accu-
racy comparison, and in conjunction with our ap-
proach (using a transductive SVM classifier in the
revision stage).
2.3 Transductive SVM
Transductive SVM (Vapnik, 1998) is a framework
for the semi-supervised training of SVM classi-
fiers.
Consider the inductive (completely supervised)
two-class SVM training problem: given a training
set {(xi, yi) |xi ? Rn, yi ? {?1, 1}}
L
i=1, find
the maximum margin separation hypersurface w ?
? (x) + b = 0 by solving the following optimiza-
tion problem:
arg min
w, b, ?
1
2
?w?22 + C
L?
i=1
?i (1)
?i : yiw ? ? (x) + b ? 1? ?i
?i : ?i ? 0
w ? Rm, b ? R
where C ? 0 is a regularization parameter and
?(?) is defined such that k (x, x?) ? ?(x) ? ? (x?)
is the SVM kernel function. This is a convex
quadratic programming problem that can be
solved efficiently by specialized algorithms.
Including an unlabeled example set
{
x?j |x
?
j ? R
n
}L?
j=1
we obtain the transduc-
tive SVM training problem:
arg min
w, b, ?, y?, xi?
1
2
?w?22 + C
L?
i=1
?i + C
?
L??
j=1
??j
(2)
56
?i : yiw ? ? (xi) + b ? 1? ?i
?j : y?j w ? ?
(
x?j
)
+ b ? 1? ??j
?i : ?i ? 0
?j : ??j ? 0
?j : y?j ? {?1, 1}
w ? Rm, b ? R
This formulation essentially models the unlabeled
examples the same way the labeled examples
are modeled, with the key difference that the
y?j (the unknown labels of the unlabeled exam-
ples) are optimization variables rather than pa-
rameters. Optimizing over these discrete variables
makes the problem non-convex and in fact NP-
hard. Nevertheless, algorithms that feasibly find
a local minimum that is typically good enough
for practical purposes do exist. In our exper-
iments we used the iterative transductive SVM
algorithm implemented in the SvmLight library
(Joachims, 1999). This algorithm tends to be-
come impractical when the number of unlabeled
examples is greater than a few thousands, hence
we were forced to use only a small portion on the
available target domain corpus. We also tried the
concave-convex procedure (CCCP) TSVM algo-
rithm (Collobert et al, 2006) as implemented by
the the Universvm package, and the multi-switch
and deterministic annealing algorithms for linear
TSVM (Sindhwani and Keerthi, 2007) as imple-
mented by the Svmlin package. These methods
are considerably faster but appear to be substan-
tially less accurate than SvmLight on our training
data.
3 Proposed approach
We present a semi-supervised training procedure
for shift/reduce SVM parsers that allows to in-
clude unannotated sentences in the training cor-
pus.
We randomly sample a small number (approx.
100) of sentences from the unannotated corpus
(the target domain corpus in a domain adaptation
task). For each of these sentences, we generate a
sequence of states that the parser may encounter
while scanning the sentence. For each state we
extract the features to generate an unlabeled train-
ing example for the SVM classifier which is in-
cluded in the training set alng with the labeled
examples generated from the annotated corpus.
There is a caveat here: the parser state at any given
point during the parsing of a sentence generally
depends on the actions taken before, but when we
are training on an unannotated sentence, we have
no way of knowing what actions the parser should
have taken, and thus the state we generate can be
generally incorrect. For this reason we evaluated
pre-parsing the unannotated sentences with a non-
transductively trained parser in order to generate
plausible state transitions while still adding unla-
beled examples. However, it turned out that this
pre-parsing does not seem to improve accuracy.
We conjecture that, because the classifier does not
see actual states but only features derived from
them, and many of these features are independent
of previous states and actions (features such as the
lemma and POS tag of the current token and its
neighbors have this property), these features con-
tain enough information to perform parsing.
The classifier is trained using the SvmLight trans-
ductive algorithm. Since SvmLight supports only
two-class SVMs while our classifier is multi-class
(one class for each possible parsing action), we
implement it in terms of two-class classifiers. We
chose the one-versus-all strategy:
We train a number of sub-classifiers equal to the
number of original classes. Each labeled training
example (x, y) is converted to the example (x, 1)
for the sub-classifier number y and to the example
(x, ?1) for the rest of sub-classifiers. Unlabeled
examples are just replicated to all sub-classifiers.
During classification the input example is eval-
uated by all the sub-classifiers and the one re-
turning the maximum SVM score determines the
class.
Our approach has been also applied to the second
stage of the revision parser, by presenting the fea-
tures of the unannotated sentences to the revision
classifier as unlabeled training examples.
4 Experiments
4.1 Experimental setup
We performed our experiments using the DeSR
parser (Attardi, 2006) on the data sets for the
Evalita 2011 dependency parsing domain adapta-
tion task for the Italian language (Evalita, 2011).
The data set consists in an annotated source-
domain corpus (newspaper articles) and an unan-
notated target-domain corpus (legal documents),
57
plus a small annotated development corpus also
from the target domain, which we used to evalu-
ate the performance.
We performed a number of runs of the DeSR
parser in various configurations, which differed
in the number and type of features extracted, the
sentence scanning direction, and whether or not
parse tree revision was enabled. The SVM clas-
sifiers always used a quadratic kernel. In order to
keep the running time of transductive SVM train-
ing acceptable, we limited the number of unanno-
tated sentences to one hundred, which resulted in
about 3200 unlabeled training examples fed to the
classifiers. The annotated sentences were 3275.
We performed one run with 500 unannotated sen-
tences and, at the cost of a greatly increased run-
ning time, the accuracy improvement was about
1%. We conjecture that a faster semi-supervised
training algorithm could allow greater perfor-
mance improvements by increasing the size of the
unannotated corpus that can be processed. All
the experiments were performed on a machine
equipped with an quad-core Intel Xeon X3440
processor (8M Cache, 2.53 GHz) and 12 Giga-
bytes of RAM.
4.2 Discussion
As it is evidenced from the table in figure
1, our approach typically outperforms the non-
transductive parser by about 1% of all the three
score measures we considered. While the im-
provement is small, it is consistent with differ-
ent configurations of the parser that don?t use
parse tree revision. Accuracy remained essen-
tially equal or became slightly worse in the two
configurations that use parse tree revision. This is
possibly due to the fact that the first stage parser of
the revision configurations uses a maximum en-
tropy classifier during training that does not learn
from the unlabeled examples.
These results suggest that unlabeled examples
contain information that can exploited to improve
the parser accuracy on a domain different than the
labeled set domain. However, the computational
cost of transductive learning algorithm we used
limits the amount of unlabeled data we can ex-
ploit.
This is consistent with the results obtained by
the self-training approaches, where a first parser
is trained on a the labeled set, which is used to
parse the unlabeled set which is then included into
the training set of a second parser. (In fact, self-
training is performed in the first step of the Svm-
Light TSVM algorithm).
Despite earlier negative results, (Sagae, 2010)
showed that even naive self-training can provide
accuracy benefits (about 2%) in domain adapta-
tion, although these results are not directly com-
parable to ours because they refer to constituency
parsing rather than dependency parsing. (Mc-
Closky et al, 2006) obtain even better results (5%
f-score gain) using a more sophisticated form of
self-training, involving n-best generative parsing
and discriminative reranking. (Sagae and Tsujii,
2007) obtain similar gains (about 3 %) for de-
pendency parsing domain adaptation, using self-
training on a subset of the target-domain instances
selected on the basis of agreement between two
different parsers. (the results are not directly com-
parable to ours because they were obtained on a
different corpus in a different language).
5 Conclusions and future work
We presented a semi-supervised training ap-
proach for shift/reduce SVM parsers and we illus-
trated an application to domain adaptation, with
small but mostly consistent accuracy gains. While
these gains may not be worthy enough to justify
the extra computational cost of the transductive
SVM algorithm (at least in the SvmLight imple-
mentation), they do point out that there exist a
significant amount of information in an unanno-
tated corpus that can be exploited for increasing
parser accuracy and performing domain adapta-
tion. We plan to further investigate this method by
exploring classifier algorithms other than trans-
ductive SVM and combinations with other semi-
supervised parsing approaches. We also plan to
test our method on standardized English-language
corpora to obtain results that are directly compa-
rable to those in the literature.
References
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
Proceedings of the 9th International Workshop on
Parsing Technologies.
J. Nivre and M. Scholz. 2004. Deterministic De-
pendency Parsing of English Text. Proceedings of
COLING 2004.
G. Attardi. 2006. Experiments with a Multilanguage
58
Figure 1: Experimental results
Accuracy (-R: right-to-left, -rev: left-to-right with revision, -rev2: right-to-left with revision):
Transductive Normal
Parser configuration LAS UAS Label only LAS UAS Label only
6 74.3 77.0 87.5 73.1 75.5 86.7
6-R 75.7 78.6 88.7 74.6 77.6 87.8
6-rev 75.2 78.2 88.6 75.1 78.0 88.3
6-rev2 75.0 77.8 88.7 75.8 78.6 88.7
8 74.3 77.0 87.3 73.4 76.0 85.9
8-R 75.7 78.6 88.7 75.3 78.3 88.1
2 74.7 77.4 87.4 73.1 75.8 86.5
Figure 2: Typical features (configuration 6).
Numbers denote offsets.
?FEATS? denotes rich morphological features (grammatical number, gender, etc).
LEMMA -2 -1 0 1 2 3 prev(0) leftChild(-1) leftChild(0) rightChild(-1) rightChild(0)
POSTAG -2 -1 0 1 2 3 next(-1) leftChild(-1) leftChild(0) rightChild(-1) rightChild(0)
CPOSTAG -1 0 1
FEATS -1 0 1
DEPREL leftChild(-1) leftChild(0) rightChild(-1)
Non-Projective Dependency Parser. Proceedings of
CoNNL-X 2006.
G. Attardi, A. Chanev, M. Ciaramita, F. Dell?Orletta
and M. Simi. 2007. Multilingual Dependency Pars-
ing and domain adaptation using DeSR. Proceed-
ings the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, Prague, 2007.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. CoNLL Shared Task.
G. Attardi, M. Ciaramita. 2007. Tree Revision Learn-
ing for Dependency Parsing. Proc. of the Human
Language Technology Conference 2007.
V. Vapnik. 1998. Statistical Learning Theory. Wiley.
Ronan Collobert and Fabian Sinz and Jason Weston
and Lon Bottou and Thorsten Joachims. 2006.
Large Scale Transductive SVMs. Journal of Ma-
chine Learning Research
Thorsten Joachims. 1999. Transductive Infer-
ence for Text Classification using Support Vector
Machines. International Conference on Machine
Learning (ICML), 1999.
Vikas Sindhwani and S. Sathiya Keerthi 2007. New-
ton Methods for Fast Solution of Semisupervised
Linear SVMs. Large Scale Kernel Machines. MIT
Press (Book Chapter), 2007
Kenji Sagae 2010. Self-Training without Reranking
for Parser Domain Adaptation and Its Impact on
Semantic Role Labeling. Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing. Uppsala, Sweden: Association
for Computational Linguistics. p. 37-44
David McClosky, Eugene Charniak and Mark John-
son 2006. Reranking and self-training for parser
adaptation. Proceeding ACL-44. Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics
Evalita. 2011. Domain Adaptation for Dependency
Parsing. .
59
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 164?169,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Pre-reordering for machine translation using transition-based walks on
dependency parse trees
Antonio Valerio Miceli-Barone
Dipartimento di Informatica
Largo B. Pontecorvo, 3
56127 Pisa, Italy
miceli@di.unipi.it
Giuseppe Attardi
Dipartimento di Informatica
Largo B. Pontecorvo, 3
56127 Pisa, Italy
attardi@di.unipi.it
Abstract
We propose a pre-reordering scheme to
improve the quality of machine translation
by permuting the words of a source sen-
tence to a target-like order. This is accom-
plished as a transition-based system that
walks on the dependency parse tree of the
sentence and emits words in target-like or-
der, driven by a classifier trained on a par-
allel corpus. Our system is capable of gen-
erating arbitrary permutations up to flexi-
ble constraints determined by the choice of
the classifier algorithm and input features.
1 Introduction
The dominant paradigm in statistical machine
translation consists mainly of phrase-based sys-
tem such as Moses (Koehn et.al.,2007). Differ-
ent languages, however, often express the same
concepts in different idiomatic word orders, and
while phrase-based system can deal to some ex-
tent with short-distance word swaps that are cap-
tured by short segments, they typically perform
poorly on long-distance (more than four or five
words apart) reordering. In fact, according to
(Birch et.al., 2008), the amount of reordering be-
tween two languages is the most predictive feature
of phrase-based translation accuracy.
A number of approaches to deal with long-
distance reordering have been proposed. Since an
extuasive search of the permutation space is un-
feasible, these approaches typically constrain the
search space by leveraging syntactical structure of
natural languages.
In this work we consider approaches which in-
volve reordering the words of a source sentence
in a target-like order as a preprocessing step, be-
fore feeding it to a phrase-based decoder which
has itself been trained with a reordered training
set. These methods also try to leverage syntax,
typically by applying hand-coded or automatically
induced reordering rules to a constituency or de-
pendency parse of the source sentence. (Gal-
ley and Manning, 2008; Xu et.al., 2009; Genzel,
2010; Isozaki et.al., 2010) or by treating reorder-
ing as a global optimization problem (Tromble and
Eisner, 2009; Visweswariah et.al., 2011). In or-
der to keep the training and execution processes
tractable, these methods impose hard constrains
on the class of permutations they can generate.
We propose a pre-reordering method based on
a walk on the dependency parse tree of the source
sentence driven by a classifier trained on a parallel
corpus.
In principle, our system is capable of generat-
ing arbitrary permutations of the source sentence.
Practical implementations will necessarily limit
the available permutations, but these constraints
are not intrinsic to the model, rather they depend
on the specific choice of the classifier algorithm,
its hyper-parameters and input features.
2 Reordering as a walk on a dependency
tree
2.1 Dependency parse trees
Let a sentence be a list of words s ?
(w1, w2, . . . , wn) and its dependency parse tree
be a rooted tree whose nodes are the words of the
sentence. An edge of the tree represents a syntac-
tical dependency relation between a head (parent)
word and a modifier (child) word. Typical depen-
dency relations include verb-subject, verb-object,
noun-adjective, and so on.
We assume that in addition to its head hi and
dependency relation type di each word is also an-
notated with a part-of-speech pi and optionally a
lemma li and a morphology mi (e.g. grammatical
case, gender, number, tense).
Some definitions require dependency parse
trees to be projective, meaning that any complete
164
subtree must correspond to a contiguous span of
words in the sentence, however, we don?t place
such a requirement. In practice, languages with a
substantially strict word ordering like English typ-
ically have largely projective dependencies, while
languages with a more free word ordering like
Czech can have substantial non-projectivity.
2.2 Reordering model
Given a sentence s ? S with its dependency parse
tree and additional annotations, we incrementally
construct a reordered sentence s? by emitting its
words in a sequence of steps. We model the re-
ordering process as a non-deterministic transition
system which traverses the parse tree:
Let the state of the system be a tuple x ?
(i, r, a, , . . . ) containing at least the index of the
current node i (initialized at the root), the list of
emitted nodes r (initialized as empty) and the last
transition action a (initialized as null). Additional
information can be included in the state x, such as
the list of the last K nodes that have been visited,
the last K actions and a visit count for each node.
At each step we choose one of the following ac-
tions:
? EMIT : emit the current node. Enabled onlyif the current node hasn?t already been emit-ted
i /? r
(i, r, a, , . . . ) EMIT? (i, (r | i) , EMIT, , . . . )
? UP : move to the parent of the current node
hi 6= null, ?j a 6= DOWNj
(i, r, a, , . . . ) UP? (hi, r, UP, , . . . )
? DOWNj : move to the child j of the currentnode. Enabled if the subtree of j (including
j) contains nodes that have not been emittedyet.
hj = i, a 6= UP, ?k ? subtree(i) : k /? r
(i, r, a, , . . . ) DOWNj? (j, r, DOWNj , , . . . )
The pre-conditions on the UP and DOWN actions
prevent them from canceling each other, ensuring
that progress is made at each step. The additional
precondition on DOWN actions ensures that the
process always halts at a final state where all the
nodes have been emitted.
Let T (s) be the set of legal traces of the transi-
tion system for sentence s. Each trace ? ? T (s)
defines a permutation s? of s as the list of emitted
nodes r of its final state.
We define the reordering problem as finding the
trace ?? that maximizes a scoring function ?
?? ? arg max
??T (s)
? (s, ?) (1)
Note that since the parse tree is connected, in
principle any arbitrary permutation can be gen-
erated for a suitable choice of ?, though the
maximization problem (1) is NP-hard and APX-
complete in the general case, by trivial reduction
from the traveling salesman problem.
The intuition behind this model is to leverage
the syntactical information provided by the de-
pendency parse tree, as successfully done by (Xu
et.al., 2009; Genzel, 2010; Isozaki et.al., 2010)
without being strictly constrained by a specific
type reordering rules.
2.3 Trace scores
We wish to design a scoring function ? that cap-
tures good reorderings for machine translation and
admits an efficient optimization scheme.
We chose a function that additively decomposes
into local scoring functions, each depending only
on a single state of the trace and the following tran-
sition action
? (s, ?) ?
|? |?1?
t=1
? (s, x (?, t) , xa (?, t+ 1))
(2)
We further restrict our choice to a function
which is linear w.r.t. a set of elementary local fea-
ture functions {fk}
? (s, x, a) ?
|F |?
k=1
vkfk (s, x, a) (3)
where {vk} ? R|F | is a vector of parameters
derived from a training procedure.
While in principle each feature function could
depend on the whole sentence and the whole se-
quence of nodes emitted so far, in practice we re-
strict the dependence to a fixed neighborhood of
the current node and the last few emitted nodes.
This reduces the space of possible permutations.
2.4 Classifier-driven action selection
Even when the permutation space has been re-
stricted by an appropriate choice of the feature
functions, computing an exact solution of the opti-
mization problem (1) remains non-trivial, because
165
at each step of the reordering generation process,
the set of enabled actions depends in general on
nodes emitted at any previous step, and this pre-
vents us from applying typical dynamic program-
ming techniques. Therefore, we need to apply an
heuristic procedure.
In our experiments, we apply a simple greedy
procedure: at each step we choose an action ac-
cording to the output a two-stage classifier:
1. A three-class one-vs-all logistic classifier
chooses an action among EMIT, UP or
DOWN based on a vector of features ex-
tracted from a fixed neighborhood of the cur-
rent node i, the last emitted nodes and addi-
tional content of the state.
2. If a DOWN action was chosen, then a one-
vs-one voting scheme is used to choose
which child to descend to: For each pair
(j, j?) : j < j? of children of i, a binary lo-
gistic classifier assigns a vote either to j or
j?. The child that receives most votes is cho-
sen. This is similar to the max-wins approach
used in packages such as LIBSVM (Chang
and Lin, 2011) to construct a M -class clas-
sifier from M (M ? 1) /2 binary classifiers,
except that we use a single binary classifier
acting on a vector of features extracted from
the pair of children (j, j?) and the node i,
with their respective neighborhoods.
We also experimented with different classification
schemes, but we found that this one yields the best
performance.
Note that we are not strictly maximizing a
global linear scoring function as as defined by
equations (2) and (3), although this approach is
closely related to that framework.
This approach is related to transition-based de-
pendency parsing such as (Nivre and Scholz,
2004; Attardi, 2006) or dependency tree revi-
sion(Attardi and Ciaramita, 2007).
3 Training
3.1 Dataset preparation
Following (Al-Onaizan and Papineni, 2006;
Tromble and Eisner, 2009; Visweswariah et.al.,
2011), we generate a source-side reference re-
ordering of a parallel training corpus. For each
sentence pair, we generate a bidirectional word
alignment using GIZA++ (Och and Ney, 2000)
and the ?grow-diag-final-and? heuristic imple-
mented in Moses (Koehn et.al.,2007), then we as-
sign to each source-side word a integer index cor-
responding to the position of the leftmost target-
side word it is aligned to (attaching unaligned
words to the following aligned word) and finally
we perform a stable sort of source-side words ac-
cording to this index.
On language pairs where GIZA++ produces
substantially accurate alignments (generally all
European languages) this scheme generates a
target-like reference reordering of the corpus.
In order to tune the parameters of the down-
stream phrase-based translation system and to test
the overall translation accuracy, we need two addi-
tional small parallel corpora. We don?t need a ref-
erence reordering for the tuning corpus since it is
not used for training the reordering system, how-
ever we generate a reference reordering for the test
corpus in order to evaluate the accuracy of the re-
ordering system in isolation. We obtain an align-
ment of this corpus by appending it to the train-
ing corpus, and processing it with GIZA++ and
the heuristic described above.
3.2 Reference traces generation and classifier
training
For each source sentence s in the training set
and its reference reordering s?, we generate a
minimum-length trace ? of the reordering transi-
tion system, and for each state and action pair in it
we generate the following training examples:
? For the first-stage classifier we generate a sin-
gle training examples mapping the local fea-
tures to an EMIT, UP or DOWN action label
? For the second-stage classifier, if the action is
DOWNj , for each pair of children (k, k?) :
k < k? of the current node i, we generate a
positive example if j = k or a negative ex-
ample if j = k?.
Both classifiers are trained with the LIBLIN-
EAR package (Fan et.al., 2008), using the L2-
regularized logistic regression method. The reg-
ularization parameter C is chosen by two-fold
cross-validation. In practice, subsampling of the
training set might be required in order to keep
memory usage and training time manageable.
3.3 Translation system training and testing
Once the classifiers have been trained, we run
the reordering system on the source side of the
166
whole (non-subsampled) training corpus and the
tuning corpus. For instance, if the parallel cor-
pora are German-to-English, after the reorder-
ing step we obtain German?-to-English corpora,
where German? is German in an English-like
word order. These reordered corpora are used to
train a standard phrase-based translation system.
Finally, the reordering system is applied to source
side of the test corpus, which is then translated
with the downstream phrase-based system and the
resulting translation is compared to the reference
translation in order to obtain an accuracy measure.
We also evaluate the ?monolingual? reordering ac-
curacy of upstream reordering system by compar-
ing its output on the source side of the test cor-
pus to the reference reordering obtained from the
alignment.
4 Experiments
We performed German-to-English and Italian-to-
English reordering and translation experiments.
4.1 Data
The German-to-English corpus is Europarl v7
(Koehn, 2005). We split it in a 1,881,531 sentence
pairs training set, a 2,000 sentence pairs develop-
ment set (used for tuning) and a 2,000 sentence
pairs test set. We also used a 3,000 sentence pairs
?challenge? set of newspaper articles provided by
the WMT 2013 translation task organizers.
The Italian-to-English corpus has been assem-
bled by merging Europarl v7, JRC-ACQUIS v2.2
(Steinberger et.al., 2006) and bilingual newspaper
articles crawled from news websites such as Cor-
riere.it and Asianews.it. It consists of a 3,075,777
sentence pairs training set, a 3,923 sentence pairs
development set and a 2,000 sentence pairs test
set.
The source sides of these corpora have been
parsed with Desr (Attardi, 2006). For both lan-
guage pairs, we trained a baseline Moses phrase-
based translation system with the default configu-
ration (including lexicalized reordering).
In order to keep the memory requirements and
duration of classifier training manageable, we sub-
sampled each training set to 40,000 sentences,
while both the baseline and reordered Moses sys-
tem are trained on the full training sets.
4.2 Features
After various experiments with feature selection,
we settled for the following configuration for both
German-to-English and Italian-to-English:
? First stage classifier: current node i state-
ful features (emitted?, left/right subtree emit-
ted?, visit count), curent node lexical and
syntactical features (surface form wi, lemma
li, POS pi, morphology mi, DEPREL di, and
pairwise combinations between lemma, POS
and DEPREL), last two actions, last two vis-
ited nodes POS, DEPREL and visit count,
last two emitted nodes POS and DEPREL, bi-
gram and syntactical trigram features for the
last two emitted nodes and the current node,
all lexical, syntactical and stateful features
for the neighborhood of the current node
(left, right, parent, parent-left, parent-right,
grandparent, left-child, right-child) and pair-
wise combination between syntactical fea-
tures of these nodes.
? Second stage classifier: stateful features for
the current node i and the the children pair
(j, j?), lexical and syntactical features for
each of the children and pairwise combina-
tions of these features, visit count differences
and signed distances between the two chil-
dren and the current node, syntactical trigram
features between all combinations of the two
children, the current node, the parent hi and
the two last emitted nodes and the two last
visited nodes, lexical and syntactical features
for the two children left and right neighbors.
All features are encoded as binary one-of-n indi-
cator functions.
4.3 Results
For both German-to-English and Italian-to-
English experiments, we prepared the data as
described above and we trained the classifiers on
their subsampled training sets. In order to evaluate
the classifiers accuracy in isolation from the rest
of the system, we performed two-fold cross vali-
dation on the same training sets, which revealed
an high accuracy: The first stage classifier obtains
approximately 92% accuracy on both German and
Italian, while the second stage classifier obtains
approximately 89% accuracy on German and 92%
on Italian.
167
BLEU NIST
German 57.35 13.2553
Italian 68.78 15.3441
Table 1: Monolingual reordering scores
BLEU NIST
de-en baseline 33.78 7.9664
de-en reordered 32.42 7.8202
it-en baseline 29.17 7.1352
it-en reordered 28.84 7.1443
Table 2: Translation scores
We applied the reordering preprocessing system
to the source side of the corpora and evaluated the
monolingual BLEU and NIST score of the test sets
(extracted from Europarl) against their reference
reordering computed from the alignment
To evaluate translation performance, we trained
a Moses phrase-based system on the reordered
training and tuning corpora, and evaluated the
BLEU and NIST of the (Europarl) test sets. As
a baseline, we also trained and evaluated Moses
system on the original unreordered corpora.
We also applied our baseline and reordered
German-to-English systems to the WMT2013
translation task dataset.
5 Discussion
Unfortunately we were generally unable to im-
prove the translation scores over the baseline, even
though our monolingual BLEU for German-to-
English reordering is higher than the score re-
ported by (Tromble and Eisner, 2009) for a com-
parable dataset.
Accuracy on the WMT 2013 set is very low. We
attribute this to the fact that it comes form a differ-
ent domain than the training set.
Since classifier training set cross-validation ac-
curacy is high, we speculate that the main problem
lies with the training example generation process:
training examples are generated only from opti-
mal reordering traces. This means that once the
classifiers produce an error and the system strays
away from an optimal trace, it may enter in a fea-
ture space that is not well-represented in the train-
ing set, and thus suffer from unrecoverable per-
formance degradation. Moreover, errors occurring
on nodes high in the parse tree may cause incor-
rect placement of whole spans of words, yielding
a poor BLEU score (although a cursory exami-
nation of the reordered sentences doesn?t reveal
this problem to be prevalent). Both these issues
could be possibly addressed by switching from
a classifier-based system to a structured predic-
tion system, such as averaged structured percep-
tron (Collins, 2002) or MIRA (Crammer, 2003;
McDonald et.al., 2005).
Another possible cause of error is the purely
greedy action selection policy. This could be ad-
dressed using a search approach such as beam
search.
We reserve to investigate these approaches in
future work.
References
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 2 - Vol-
ume 2 (EMNLP ?09), Vol. 2. Association for Com-
putational Linguistics, Stroudsburg, PA, USA, 1007-
1016.
G. Attardi, M. Ciaramita. 2007. Tree Revision Learn-
ing for Dependency Parsing. In Proc. of the Human
Language Technology Conference 2007.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL ?09). Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 245-253.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING ?10). Association for Computational Linguis-
tics, Stroudsburg, PA, USA, 376-384.
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (ACL-44). Association for Computational
Linguistics, Stroudsburg, PA, USA, 529-536.
Alexandra , Miles Osborne, and Philipp Koehn. 2008.
Predicting success in machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP ?08). Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, 745-754.
168
BLEU BLEU (11b) BLEU-cased BLEU-cased (11b) TER
de-en baseline 18.8 18.8 17.8 17.8 0.722
de-en reordered 18.1 18.1 17.3 17.3 0.739
Table 3: WMT2013 de-en translation scores
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions
(ACL ?07). Association for Computational Linguis-
tics, Stroudsburg, PA, USA, 177-180.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head finalization: a simple re-
ordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR (WMT ?10). Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, 244-251.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP ?08). Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 848-856.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?11). Association for
Computational Linguistics, Stroudsburg, PA, USA,
486-496.
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X ?06). Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, 166-170.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of English text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics (COLING ?04). Association for
Computational Linguistics, Stroudsburg, PA, USA, ,
Article 64 .
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics (ACL ?00). Association for Com-
putational Linguistics, Stroudsburg, PA, USA, 440-
447.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Trans. Intell. Syst. Technol. 2, 3, Article 27 (May
2011), 27 pages.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. J. Mach.
Learn. Res. 9 (June 2008), 1871-1874.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. MT Summit 2005.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis?. 2006. The
JRC-Acquis: A multilingual aligned parallel corpus
with 20+ languages. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC?2006).
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10 (EMNLP
?02), Vol. 10. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 1-8.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics (ACL ?05). Association for Computational
Linguistics, Stroudsburg, PA, USA, 91-98.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res. 3 (March 2003), 951-991.
169
