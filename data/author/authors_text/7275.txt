Parsing Schemata for Grammars with 
Variable Number  and Order of Const i tuents  
Kar l -M ichae l  Schne ider  
Depart inent  of General Linguistics 
University of Passau 
Imlstr. 40, 94032 Passau, Germany 
schneide(@l)hil.uni-t)assau.de 
Abst ract  
We define state transition grammars (STG) as 
an intermediate tbrmalism between grammars 
and parsing algorithms which is intended to 
separate the description of a parsing strategy 
from the grammar tbrmalism. This allows to de- 
fine more general parsing algorithms for larger 
classes of grammars, including gramnmrs where 
the nunfl)er and order of subconstituents de- 
tined by a production may not be tlxed. Various 
grammar formalisms are characterized in terms 
of prol)erties of STG's. We define an Earley 
parsing schema tbr S'rC's and characterize the 
wflid l)arse items. We also discuss the usabil- 
ity of STG's tbr head-(:orner parsing and direct 
1)arsing of sets of tree constraints. 
1 I n t roduct ion  
This t)aper addresses the qllestion of how l;o de- 
fine (talmlar) parsing algorithms on a greater 
level of al)straction, in order to apply them 
to larger (:lasses of grammars (as compared 
to parsing algorithms tbr context-Dee gram- 
lllars). SllCtl an abstraction is useflll beCallSe 
it; allows to study l)rot)erties of parsing algo- 
rithms, and to compare different parsing algo- 
rithms, independently of tile prot)erties of an 
mtderlying rammar formalism. While previ- 
ous atteml)ts to define more general parsers 
have only aimed at expanding the domain of 
the nontenninal symbols of a grammar (Pereira 
and Warren, 1983), this paper aims at a gen- 
eralization of parsing in a difl'erent dimension, 
namely to include grammars with a flexible con- 
stituent sI;ructure, i.e., where tile sequence of 
subconstituents specified by a grammar produc- 
tion is not fixed. We consider two grammar 
tbrmalisms: Extended context-ii'ee grammars 
(ECFG) and ID/LP granllllars. 
ECFG's (sometimes called r(~.q'ular righ, t part 
grammars) are a generalization of context-free 
grammars (CFG) in which a grammar produc- 
tion specifies a regular set of sequences of sub- 
constituents of its left-haM side instead of a 
fixed sequence of subconstituents. The right- 
hand side of a production can 1)e represented 
as a regular set, or a regular expression, or a 
finite automaton, which are all equivalent con- 
cepts (Hopcroft and Ulhnan, 1979). ECFG's 
are often used by linguistic and programming 
language grammar writers to represent a (pos- 
sibly infinite) set of context-free productions as 
a single production rule (Kaplan and Bresnan, 
1982; Woods, 1973). Parsing of ECFG's has 
been studied t br example ill Purdom, Jr. and 
Brown (1981)and l;~','r,nakers (1989). 'rab,ll~r 
parsing teclmiques tbr CFG's can be generalized 
1;o ECFG's in a natural way by using the con> 
putations of the tinite automata in the grammar 
productions to guide the recognition of new sub- 
constituents. 
ID/LP grammars are a variant of CFG's that 
were introduced into linguistic tbrmalisms to en- 
code word order generalizations (Gazdar et al, 
1985). Her(',, the number of snbconstituents of 
the left-hand side of a production is fixed, but 
their order can w~ry. ID rules (immediate dom- 
inance rules) speci(y the subconstituents of a 
constituent but leave their order unspeeitied. 
The adnfissible order|rigs of subeonstituents are 
specified separate, ly by a set of LP constraints 
(linear precedence constraints). 
A simple approach to ID/LP parsing (called 
indirect parsing) is to tully expand a gram- 
mar into a CFG, but this increases the nmnber 
of productions ignificantly. Therefore, direct; 
parsing algorithms for ID/LP grammars were 
proposed (Shieber, 1984). It is also possible to 
encode an ID/LP grammar as an ECFG by in- 
terleaving the ID rules with LP checking with- 
733 
out increasing the number of productions. How- 
ever, tbr unification ID/LP grammars, expan- 
sion into a CFG or encoding as an ECFG is 
ruled out because the information contained in 
the ID rules is only partial and has to be instan- 
tiated, which can result in an infinite number 
of productions. Moreover, Seiffert (1991) has 
observed that, during the recognition of sub- 
constituents, a subconstituent recognized in one 
step can instantiate t~atures on another subcon- 
stituent recognized in a previous step. There- 
tbre, all recognized subconstituents must remain 
accessible fbr LP checking (Morawietz, 1995). 
We define an intermediate tbrmalism be- 
tween grammars and parsers (called state tran- 
sition 9rammars, STG) in which different gram- 
mar fbrmalisms, including CFG's, ECFG's, and 
ID/LP grammars can be tel)resented. More- 
over, admissible sequences of subconstituents 
are defined in a way that allows a parser to 
access subconstituents that were recognized in 
previous parsing steps. Next, we describe an 
Earley algorithm tbr STG's, using the parsing 
schemata ibrmalism of Sikkel (1993). This gives 
us a very high level description of Earley's algo- 
rithm, in which the definition of parsing steps 
is separated from the properties of the grammar 
tbrmalism. An Earley algorithm for a grammar 
may be obtained tiom this description by rep- 
resenting the grammar as an STG. 
The paper is organized as tbllows. In Sec- 
tion 2, we define STG's and give a characteri- 
zation of various grammar tbrmalisms in terms 
of properties of STG's. In Section 3 we present 
an Earley parsing schema for STG's and give a 
characterization f the wflid parse items. In Sec- 
tion 4, we introduce a variant; of STG's tbr head- 
corner parsing. In Section 5, we discuss the us- 
ability of STG's to define parsers for grammars 
that define constituent structures by means of 
local tree constraints, i.e., formulae of a (re- 
stricted) logical language. Section 6 presents 
final conclusions. 
2 State  Trans i t ion  Grammars  
Wc denote nonterminal symbols with A, B, ter- 
minal symbols with a, terminal and nonterminal 
symbols with X, states with F, strings of sym- 
bols with/3, % and the empty string with c. An 
STG is defined as tbllows: 
Def in i t ion  1 (ST( \ ] ) .  Art STG G is a tuple 
( N, E, A~, AJ l;', He, P, S) where 
? N is a finite set of nonterminal symbols, 
? E is a finite set of terminal symbols, 
? A/I is a finite set of states, 
,, A.4\];, c_ .A4 is a set of final states, 
,, Ha c (.A4 x V) 2 is a binary relation of the 
form (r,/3) ~-a (F',/3X), where V = NUE,  
? P C_ N ? AJ \ .A41,~ is a set of productions 
written as A -+ F, and 
? S E N is a start symbol. 
Note thai; we do not allow final states in the 
right-hand side of a production. A pair (F,/3) is 
called a configuration. If F is a fnal  state then 
(P,/3) is called a final configuration. The reflex- 
ive and transitive closure of \[-c, is denoted with 
H~. The state projection of Hc is the binary 
relation 
 (Ho) = {(r, r')l  /3x: (p,/3) (p',/3x)}. 
Ha is called context:free iff a transition from 
(P,/3) does not del)end on fl, tbrmally: for all 
/3, fl', r ,  r ' ,  x :  (r,/3) Ha (r',  fiX) iff (r,/3') He; 
(F',/3'X). The set of terminal states of G is the 
set 
w(C) = {PlVP' : (1 ~, P') ~ ~(Ha)}. 
The language defined by a state P is the set 
of strings in the final configurations reachable 
t'rom (r, e): 
L(r) = {/313 r' My :  (r, (r',/3)}. 
Note that if A --> F is a production then e 
L(P) (i.e., there are no ~-productions). The 
derivation relation is defined by 7A5 ==> 7fl5 
itf for some production A ~ P: /3 C L(P). The 
language defined by G is the set of strings in E* 
that are derivable fi'om the start symbol. 
We denote a CFG as a tuple (N ,E ,P ,S )  
where N, E, S are as betbre and P C_ N x V + is 
a finite set of productions A -+/~. We assume 
that there are no e-productions. 
An ECFO can be represented asan extension 
of a CFO with productions of the tbrm A -+ A, 
where .A = (V, Q, qo, 5, Of) is a nondeterministic 
finite automaton (NFA) without e-transitions, 
734 
54 
ECFG Q 
 D/LP {MI A+M'cP: MC_M'} 
{c} 
Qf 
F = XF'  
(r ,  X, r ' )  ~ 
r = r 'u  {x} , /~x  < LP 
Tnble 1: Encoding of grmnmars in STG's. 
with input alphalmt V, state set Q, initial state 
q0, final (or accepting) states Q f, m~(t r~msi- 
tion relation 5 C_ Q x V x Q (I{opcroft and Ull- 
man, 1979). A accepts ~ string fl ill tbr some 
final st;;~l;e q C Q f, (qo,/'-\], q) ~ 5". Furl;hermore, 
we assume that q0 ~ Q f, i.e., ..4 does nol; ac- 
(:ept the emi)l;y word. We can assmne wit;hour 
loss of generalizal;ion thai, the mfl;omal;a in the 
right- lmnd sides of a grammar are nll disjoint. 
Then we cml rel)resent ml ECFG as a tul)le 
(N, E, Q, Q f, 5,1 ), S) where N, E, Q, Q f, 5, S m'e 
as befbre and P C N x (2 is ~t finite set of produc- 
tions A -> q0 (q0 is ml initial st~te.). For rely pro- 
duct ion p = A ~ q0 let A p = (17, Q, q0, (t, Oj.) 
l)e the NFA with initiM state q0. The, deriwd;ion 
relation is detined by 7A5 ~ 7/35 itf fbr some 
1)roduction p = A ---> q0, A p accet)ts fl. 
An ID /LP  grnmm~tr is represented as a l;u- 
pie (N~ E, \] , LP, S) whoa'e. N, E, S are as before 
nnd P is a finite set of productions (ID rules) 
A --+ M,  where. A C N ;uid ~4 is ~ multiset 
over V, and LP is a set ()f line~r l)re(:edence 
constraints. We are not concerned with de.tails 
of the LP constra.ints here. We write fl ~ LP 
to denote that the sl;ring fi s~d;isties all the con- 
straints in l,P. The derivation r(;l~|;ion is de- 
fined by 7A5 ~ 7\[3d i1\[ fl = X~. . .X~ and 
a > {X~, . . . ,Xk}  ~ 1" mM fl ~ LI'. 
CFG's,  ECFG's  and ID /LP  grmnlnars (:;m 
t)e chara(:l;erized by al)t)rol)ri~te restrictions on 
the transit ion relation and the fired st;~l;es of an 
STG: ~ 
? CFG: \]-o is context-free and deterministic, 
cy(t-6,) is acyelic, 2~4F = T(G).  
? ECFG: t-a is context-free. 
? ID /LP :  or(t-(;) is aeyclic: J~41,' = T(G),  for 
all F: iffl, 7 C L(F) then 7 is ~t permutal,iolt 
These conditions define normal-forms ofSTG's; that 
is, for STG's that do not, satist~y the conditions for some 
type there can nevertheless lmstrongly equivalent gram- 
mars of that; t;ype. These STG's are regarded as degen- 
erate mM are not fllrther considered. 
of ft. 
For instance, if G is an STG that satisfies the 
conditions tbr CFG's, then a CFG G / can be 
constructed as follows: l,br every product ion 
A -~ q0 in G, let A -~ fl be a product ion in 
G' whe.re L(qo) = {/3}. Then the deriw~tion re- 
lations of G mid G' coincide. Similarly tbr the 
other grammar tyl)es. Conversely, if ~t grammar 
is of a given type, l;hen it (:ml be rel)resented as 
ml STG satist~ying the conditions tbr that  type, 
by spe(:it~ying the states and transit ion relation, 
as shown in Table 1 (tO denotes nmltiset lnlion). 
3 Ear ley  Pars ing  
Parsing schemat~ were proposed by Sikkel 
(1.993) as a framework for the specific~tion 
0rod comparison) of tabular parsing algorithms. 
Parsing schemata provide n well-detined level of 
abstra(:l;ion by al)stra(:ting fi'om (:ontrol struc- 
tures (i.e., or(lering of operations) and (later 
structures. A parsing schem;t cmJ \])e imple- 
mented as n tabulm: parsing ;flgorithm in ~ 
em~onical w;~y (Sikkel, 1998). 
A \])re:sing schema for n gr;tllllll;~r cla,ss is & 
function that assigns ('.~mh grmnmar and each 
input string a deduction system, called a parsing 
sy.ste.m. A parsing schema is usmdly defined by 
pre.senting a parsing system. A parsing system 
consists of ~ finite set Z of pars(; items, a finite 
set "H of hyt)otheses , whi(:h ell(:()(\](; the input 
string, mxd ~ finite set 29 of deduction stel)s of 
the fbrm x~, . . . , x ,  t- a: where xi C 2; U ~ and 
x E Z. The hypotheses can be represented as 
deduction steps with empty prenfises, so we can 
assume that, all xi m'e it;eros, and represent a 
parsing system as a pair (Z, 29). 
Correctness of a l)~rsing system is defined 
with respect to some item senmntics. Every 
item denotes a particub~r deriw~tion of some 
substring of the input string. A parsing sys- 
te.m is correct if an item is deducible precisely if 
it denotes an admissible deriw~tion. Items that 
denote admissible derivations are called coffee/,. 
735 
Z={\ [A~/3 .F , i , j \ ] IAEN,  f lEV* ,  ?EM,  1/31 <.,, O<i<j<n} 
D Init =- S - -~ P E P 
\[S ~ .r, 0,0\] 
Dpredi~t = \[A --+ ft. P, i, j\] 
T)Comp I =_ 
\[B --+ .P0, j , j \ ]  
\[A + fl.P,i , j \] 
\[A -+ \[3aj+l. F', i, j + 1\] 
r':  (r,/3) No (r',/~B), B -~ ro e p 
(r,/5) \[-G (r',/3aj+l) 
\[A ~ /3. r , i , j \ ]  
\[B ~ ,>r:, j, k\] 
\[A -+ fiB. ?', i, t~\] r: E M, ,  (r,/3) >a (r',/3~) 
Figure 1: The Earley parsing schema for an STG G and input string w = al . . .  an. 
STG's constitute a level of abstraction be- 
tween grammars and parsing schemata because 
they can be used to encode various classes of 
grammars, whereas the mechanism for recog- 
nizing admissible sequences of subconstituents 
by a parsing algorithm is built into the gram- 
mar. Thereibre, STG's allow to define the pars- 
ing steps separately f iom the mechanism in a 
grmnmar that specifies admissible sequences of 
subconstituents. 
A generalization of Earley's algorithm ibr 
CFG's (Earley, 1970) to STG's is described by 
the parsing schema shown in Fig. 1. An item 
\[A -~/3.P, i, j\] denotes an A-constituent that is 
partially recognized fi'om position i through j
in tile input string, where/3 is the sequence of 
recognized subconstituents of A, and a sequence 
of transitions that recognizes ~ can lead to state 
F. Note that the length of/5 can be restricted 
to the length of the int)ut string because there 
are no g-productions. 
In order to give a precise definition of the se- 
mantics of the items, we define a derivation re- 
lation which is capable of describing the partial 
recognition of constituents. This relation is de- 
fined on pairs (7, A) where 7 E V* and A is a 
finite sequence of states (a pair (% A) could be 
called a super configuration). 7 represents the 
fi'ont (or yield) of a partial derivation, while A 
contains one state for every partially recognized 
constituent. 
Def in i t ion  2. The Earley derivation relation 
is defined by th, e clauses: 
? (TA, A) ~ (7/5, FA) iff 3A --+ P' E P: 
(r', e) e5 (r,/3). 
? (TAa, A) p (7/3a, A) /ff 7Aa ~ 798. 
The first clause describes the I)artial recog- 
nition of an A-constituent, where/3 is the rec- 
ognized part and tile state P is reached when 
/3 is recognized. The second clause describes 
~he complete recognition of an A-constituent; 
in this case, the final state is discarded. Each 
step ill the derivation of a super configuration 
(% A) corresponds to a sequence of deduction 
steps in the parsing schema. As a consequence 
of the second clause we have that w E L(G) iff 
(S, c) ~* (w, c). Note that ~-, is too weak to de-- 
scribe the recognition of the next subconstituent 
of a partially recognized constituent, but it is 
sufficient o define the semantics of the items in 
Fig. 1. The fbllowing theorem is a generaliza- 
tion of the definition of the semantics of Earley 
items for CFG's (Sikkel, 1993) (a l . . .  an is the 
input string): 
Theorem 1 (Cor rectness ) .  
F* \[A --+/3.F, i,j\] iff the conditions are satisfied: 
? for some A, (S, c) \]'--,* (al . . .  aiA, A). 
? (A, e) b" (/3, F). 
? /3 ::==>* a i+ 1 . . .  a j .  
The first and third condition are sometimes 
called top-down and bottom-up condition, re- 
spectively. The second condition refers to the 
partial recognition of the A-constituent. 
736 
\[~ -~. ~,, 0, 0\] (re, ~) >* (r,, ~), (E, ~) \[~ (~, q~) 
IT -~ .q3,0,0\] (m~) ~ (T,<~), (T,~) p (~, ?~) 
(m ~) ~ (T, ~) 
\[F -+ .q,~, 0, 0\] (E,E) t..,(T,q,jI--,(F, q4q2), (F,~) ~ (E, qs) 
(Z, ~) \[- (T, q2) b (F, ~t2) 
(z, ~) > (T, ~) b (F, <s4) 
(z, ~) b (m, ~) b (F, ~) 
\[F --> a.qu, 0, 1\] (E,e) b(T,q,jb(F,q,lq,2), (F,e)~,,(a, qo) 
(m ~) b (T, w) b (r, w) 
(m ~) P (T, ~) b (r, q4) 
(E, ~) b (T, ~) b (F, ~) 
\[T -+ S~.~s4,O,q (E,~)b(T,  q2), (T,~)p(S<,s4), F~*a  
(E, ~) b (T, ~) 
\[\]'~ --+ a. q6, 2, 3\] (E,m) b(T, q2) b(17*F, qaq2) b(a*F, q4q2), (/P, ?) b (a, q6) 
(m ~) b (T, q~) b (~ * s< v',) b (o,. F, w) 
(z, ~) \[~ (T, ~) \[~ (F ? F, q4) \[~ (o,. ~, w) 
(m ~) I--. (T, ~) I--' (F ? sV, ~) t-' (<, * S< ~) 
\[E--+T*T.q,2,0,3\] (E,g) V*(E,e), (E,g)~,,(T*T, q2), T*T==>*a*a  
~ihble 2: Valid parse items and derbable super configurations for a * a. 
Example  1. Consider the following STG: 
G = ({z, T, r}, to,, +, .}, {m,.. . ,  <~6}, 
{q~, q4, q~}, FG, P, E), 
P = {E  --> q~, T --> qa, F -+ q,~} 
with the following transitions (for all fi): 
(m, l~) i-(~ (<s~, I~T), (,s~, i~) i-c~ (m, i~+), 
(qa, i3) t-c (q4,/~S~), (<S4, h ~) i-c; (<S:~, iJ*), 
(q,~, f~) i-c (<so,/Ja). 
Table 2 shows soule valid parse items fbr the 
recognition of the string a * a, together with the 
conditions according to Theorem 1. 
4 Bid i rec t iona l  Pars ing  
STG's describe the recognition of admissi- 
ble sequences of subconstituents in unidirec- 
tional parsing algorithms, like Earley's algo- 
rithm. Bidirectional parsing strategies, e.g., 
head-conic< strategies, start the recognition of 
a sequence of subconstituents at sonic position 
in the middle of the sequence and proceed to 
both sides. We can define appropriate STG's 
for 1)idirectional parsing strategies as follows. 
Def in i t ion  3. A h, eaded, bidirectional STG G 
is like an STG excq~t hat P is a finite set of 
productions of the form A --+ (P,X, A), 'where 
A c N and X E V and F, A c .M. 
The two states in a production accOullt for the 
bidirectional expansion of a constituent. The 
derivation relation for a headed, bidirectional 
STG is defined by 7A6 ~ 7fllXfl"6 if\[ for some 
production A -+ (P, X, A): (fit)-* c L(P) and 
fi' C L(A) ((S) -1 denotes the inversion of fit). 
Note that P defines the left part of an adnfissible 
sequence Doul right to left,. 
A t)ottom-up head-conmr parsing schema 
uses items of the tbrm \[A -+ F. fl. A, i, j\] (Schnei- 
der, 2000). The semantics of these items is given 
by the tbllowing clauses: 
? tbr some production A ~ (P0, X, A0), 
some fll,fl,.: fl = flZXflr and (P0,e) t-G 
(r, (/~)-~) dud (A0,~)~o (a,/~"). 
,, /3 ~*  a i+ l . . ,  aj. 
5 Loca l  T ree  Const ra in ts  
In this section we discuss the usability of STG's 
for the design of direct parsing algorithms for 
grammars that use a set of well-fonnedness 
conditions, or constraints, expressed in a logi- 
cal language, to define the admissible syntac- 
tic structures (i.e., trees), in contrast o gram- 
mars that are based on a derivation mechanism 
737 
(i.e., production rules). Declarative characteri- 
zations of syntactic structures provide a nlealiS 
to tbrmalize grammatical frameworks, and thus 
to compare theories expressed in different for- 
malisms. There are also applications in the- 
oretical explorations of the complexity of lin- 
guistic theories, based on results which relate 
language classes to definability of structures in 
certain logical languages (Rogers, 2000). 
From a model-theoretic point of view, such 
a grammar is an axiomatization of a class of 
structures, and a well-formed syntactic struc- 
ture is a model of the grammar (Blackt)urn et 
al., 1993). The connection between models and 
strings is established via a yield function, which 
assigns each syntactic structure a string of ter- 
minal symbols. The parsing problem can then 
be stated as the problem: Given a string w and 
a grammar G, find the models .A4 with A.4 ~ G 
and yieId(./V4) = w. 
In many cases, there are eft~ctive methods to 
translate logical fornmlae into equivalent ree 
automata (Rogers, 2000) or rule-based gram- 
mars (Pahn, 1997). Thus, a possible way to 
approach the parsing problem is to translate a 
set of tree constraints into a grammar and use 
standard parsing methods. However, depending 
on the expressive power of the logical language, 
the complexity of the translation often limits 
this approach in practice. 
In this section, we consider the possibility to 
apply tabular parsing methods directly to gram- 
mars that consist of sets of tree constraints. The 
idea is to interleave the translation of tbrmu- 
lae into production rules with the recognition 
of subconstituents. It should be noted that this 
approach suffers from the same complexity lim- 
itations as the pure translation. 
In Schneider (1999), we used a fragment of 
a propositional bimodal anguage to express lo- 
cal constraints on syntactic structures. The two 
modal operators ($} and (-~) refer to the left- 
most child and the right sibling, respectively, of 
a node in a tree. Furthermore, the nesting of 
($) is limited to depth one. A so-called modal 
grammar consists of a formula that represents 
the conjunction of a set of constraints that must 
be satisfied at every node of a tree. In addition, 
a second formula represents a condition tbr the 
root of a tree. 
In Schneider (1999), we have also shown 
how an extension of a standard nlethod tbr 
automatic proof search in modal logic (so- 
called analytic labelled tableauz) in conjmm- 
tion with dynamic progrmnming techniques can 
be employed to parse input strings according 
to a modal grammar. Basically, a labelled 
tableau procedure is used to construct a la- 
belled tableau, i.e., a tree labelled with tbrmn- 
lae, by breaking tbrmulae up into subtbrmulae; 
this tableau may then be used to construct a 
model tbr the original formula. The extended 
tableau procedure constructs an infinite tableau 
that allows to obtain all admissible trees (i.e., 
models of the grammar). 
The approach can be described as tbllows: An 
STG is defined by using certain formulae that 
appear on the tableau as states, and by defining 
the transition relation in terms of the tableau 
rules (i.e., the operations that are used to con- 
struct a tableau). The states are formulae of 
the form 
x A A<,>o A AI. \]o' A A A\[q ' 
where X is a propositional variable and \[$\], \[-->\] 
are the dnal operators to (.\[), (~) .  X is used 
as a node \]abe\] in a tree model. The t rans i t ion 
relation can be regarded as a silnnlation of the 
application of tableau rules to fbrmulae, and a 
tabular parser tbr this STG can be viewed as a 
tabulation of the (infinite) tal)leau construction. 
In particular, it should be noted that this con- 
struction makes no reference to any particular 
parsing strategy. 
6 Conc lus ion  
We have defined state transition grammars 
(STG) as an intermediate formalism between 
grammars and parsing algorithnls. They com- 
plement the parsing schemata formalism of 
Sikkel (1993). A parsing schema abstracts 
from unimportant algorithmic details and thus, 
like STG's, represents a well-defined level of 
abstraction between grammars and parsers. 
STG's add another abstraction to parsing 
schemata, namely on the grammar side. There- 
fore, we argued, a t)arsing schenla defined over a 
STG represents a very high level description of 
a tabular parsing algorithm that can be applied 
to various gralnlnar tbrmalisms. In this paper 
we concentrated on grammar formalisms with 
a flexible constituent structure, i.e., where the 
738 
mmfl)er and order of subconstituents st)e(:ified 
by a grammar i)roduction may not \[)e fixed. In 
particular, we have discussed extended context- 
free grammars (ECFG), I I ) /LP grammars, and 
grammars in which admissible trees are delined 
by means of local tree ('onstraints cxI)resscd in 
a simple logical language. 
References  
Patrick Blackl)urn, Clair(,' Gar(t(mt, and Wil- 
fi'ied Meyer-Viol. 1993. Talking about trees. 
In P~ve. 5th Con:fcrenee of th.c European 
Chapter of the Association for Computational 
Linguistics (EA CL'93), pages 21 29. 
.lay Earley. 1970. An efli(:icnt context-free pars- 
ing algorithm. Communication.~' of the A CM, 
13:2:94--102. 
Gerald Gazdar, Ewm H. Klein, Geoffrey K. Pul- 
turn, and Iwm A. S~g. 1985. Generalized 
Ph, rase Structure Gramntar. Bla(:kwell, Ox- 
\[brd. 
John E. IIot)croft and Jetfrcy D. Ulhmm. 1979. 
lnbvduetion to Automata Theory, Languages 
and Computation: Addison-Wesley, Amster- 
daIIl. 
Ronald M. Kaplan and Joan Bresnan. 1982. 
Lexical-flmctiollal grammar: A ibrmal sys- 
I;em tbr gramlnatical rel)r(;sentation. In Joan 
13resnan, editor, The Mental l~.eprcsentation 
of Grammatical H, clation.~, (:hal)t(;r 4:, t)ages 
175 281. MIT Press, Calnbridge, MA. 
Rend Leermakers. 1989. How to cover a gram- 
mar. In P~wc, 27th Annual Meetin9 of the 
Association for Computational Linguistics 
(ACL '89), pages 1.35 -142. 
Frank Morawietz. 1995. A ratification- 
based ID/LP parsing s('h(',ma. In 1)roe. 
.4th Int. Workshop on Parsing Technologies 
(IWP T'95), Prague. 
Adi PMm. 1997. Tran.~:fo'rming Tree Con- 
straints into Formal Grammar. Infix, Sankt 
Augustin. 
Fernando C. N. Pereira and David It. D. War- 
ren. 1983. Parsing as deduction. In Prec. 
21st Annual Meeting of the Association for 
Computational Linguistics (14CL'83), pages 
137-144. 
Paul Walton Purdom, Jr. and Cynthia A. 
Brown. 1981. Parsing extended LR(k) gram- 
mars. Acta IKformatica, 15:115-127. 
James Rogers. 2000. wMSO theories as 
grammar fi)rmalisms. In Pwc. of 16th 
Twentc Workshop on Language Technology: 
Al.qcbraic Methods in Language PTvccssing 
(TWLT I6/AMiL1 ~ 2000), pages 201. 222, 
Iowa City, Iowa. 
Karl-Michael Sclmeider. 1999. An ~pplication 
of lab(filed tableaux to parsing. In Neil Mur- 
ray, editor, Automa, tic Reasoning with An- 
alytic "l},blcaux and Related Methods, pages 
117-131. ~lbch. Report 99-1, SUNY, N.Y. 
Karl-Michael Schneider. 2000. Alget)raic con- 
struction of t)arsing schemata. In Pwc. 
6th Int. Workshop on Parsin9 ~chnologics 
(IWPT 2000), pages 242-253, Trent(). 
l/,oland Seiflb, rt. 1.991. Unification-ID/LP 
grammars: Formalization and I)arsing. In Ot- 
thein Herzog and Clmls-Rainer Rollinger, ed- 
itors, Text Understanding in LILOG, LNAI 
546, pages 63 7a. Springer, Berlin. 
Stuart M. Shiebcr. 1984. Direct parsing of 
ID/LI ) grammars. Linguistics and Ph, iloso- 
phy, 7(2):135 154. 
Klaas Sikkel. 1993. Parsing Schemata. Proe5 
schrift, Universiteit Twente, CIP-Gegevens 
Koninklijke Bibliotheek, Den Haag. 
Klaas Sikkel. 1998. Parsing schemata nd (:or- 
rectness of parsing algorithms. 'l'heoretical 
Computer Science, 199(1--2):87 -103. 
William A. Woods. 1973. An exi)(;rimenta,1 
parsing system tbr transition n(',l;work gram- 
mars. In Randall t/,ustin, (,'(titor~ Nat'm'al Lau- 
g'aagc 1)~vcessing, pages 111- 154. Algorith- 
mic Press, New York. 
739 
307
308
309
310
311
312
313
314
A New Feature Selection Score for Multinomial Naive
Bayes Text Classification Based on KL-Divergence
Karl-Michael Schneider
Department of General Linguistics
University of Passau
94032 Passau, Germany
schneide@phil.uni-passau.de
Abstract
We define a new feature selection score for text
classification based on the KL-divergence between
the distribution of words in training documents and
their classes. The score favors words that have a
similar distribution in documents of the same class
but different distributions in documents of different
classes. Experiments on two standard data sets in-
dicate that the new method outperforms mutual in-
formation, especially for smaller categories.
1 Introduction
Text classification is the assignment of predefined
categories to text documents. Text classification
has many applications in natural language process-
ing tasks such as E-mail filtering, prediction of user
preferences and organization of web content.
The Naive Bayes classifier is a popular machine
learning technique for text classification because it
performs well in many domains, despite its simplic-
ity (Domingos and Pazzani, 1997). Naive Bayes as-
sumes a stochastic model of document generation.
Using Bayes? rule, the model is inverted in order to
predict the most likely class for a new document.
We assume that documents are generated accord-
ing to a multinomial event model (McCallum and
Nigam, 1998). Thus a document is represented as
a vector di = (xi1 . . . xi|V |) of word counts where
V is the vocabulary and each xit ? {0, 1, 2, . . . } in-
dicates how often wt occurs in di. Given model pa-
rameters p(wt|cj) and class prior probabilities p(cj)
and assuming independence of the words, the most
likely class for a document di is computed as
c?(di) = argmax
j
p(cj)p(d|cj)
= argmax
j
p(cj)
|V |?
t=1
p(wt|cj)n(wt,di)
(1)
where n(wt, di) is the number of occurrences of wt
in di. p(wt|cj) and p(cj) are estimated from train-
ing documents with known classes, using maximum
likelihood estimation with a Laplacean prior:
p(wt|cj) =
1 + ?di?cj n(wt, di)
|V | + ?|V |t=1
?
di?cj n(wt, di)
(2)
p(cj) =
|cj |?|C|
j?=1 |cj? |
(3)
It is common practice to use only a subset of
the words in the training documents for classifi-
cation to avoid overfitting and make classification
more efficient. This is usually done by assigning
each word a score f(wt) that measures its useful-
ness for classification and selecting the N highest
scored words. One of the best performing scoring
functions for feature selection in text classification
is mutual information (Yang and Pedersen, 1997).
The mutual information between two random vari-
ables, MI(X;Y ), measures the amount of informa-
tion that the value of one variable gives about the
value of the other (Cover and Thomas, 1991).
Note that in the multinomial model, the word
variable W takes on values from the vocabulary V .
In order to use mutual information with a multi-
nomial model, one defines new random variables
Wt ? {0, 1} with p(Wt = 1) = p(W = wt) (Mc-
Callum and Nigam, 1998; Rennie, 2001). Then the
mutual information between a word wt and the class
variable C is
MI(Wt;C) =
|C|?
j=1
?
x=0,1
p(x, cj) log
p(x, cj)
p(x)p(cj)
(4)
where p(x, cj) and p(x) are short for p(Wt = x, cj)
and p(Wt = x). p(x, cj), p(x) and p(cj) are esti-
mated from the training documents by counting how
often wt occurs in each class.
2 Naive Bayes and KL-Divergence
There is a strong connection between Naive Bayes
and KL-divergence (Kullback-Leibler divergence,
relative entropy). KL-divergence measures how
much one probability distribution is different from
another (Cover and Thomas, 1991). It is defined (for
discrete distributions) by
KL(p, q) =
?
x
p(x) log p(x)q(x) . (5)
By viewing a document as a probability distribu-
tion over words, Naive Bayes can be interpreted in
an information-theoretic framework (Dhillon et al,
2002). Let p(wt|d) = n(wt, d)/|d|. Taking loga-
rithms and dividing by the length of d, (1) can be
rewritten as
c?(d)
=argmax
j
log p(cj) +
|V |?
t=1
n(wt, d) log p(wt|cj)
=argmax
j
1
|d| log p(cj) +
|V |?
t=1
p(wt|d) log p(wt|cj)
(6)
Adding the entropy of p(W |d), we get
c?(d)
= argmax
j
1
|d| log p(cj) ?
|V |?
t=1
p(wt|d) log
p(wt|d)
p(wt|cj)
= argmin
j
KL(p(W |d), p(W |cj)) ?
1
|d| log p(cj)
(7)
This means that Naive Bayes assigns to a document
d the class which is ?most similar? to d in terms
of the distribution of words. Note also that the
prior probabilities are usually dominated by docu-
ment probabilities except for very short documents.
3 Feature Selection using KL-Divergence
We define a new scoring function for feature selec-
tion based on the following considerations. In the
previous section we have seen that Naive Bayes as-
signs a document d the class c? such that the ?dis-
tance? between d and c? is minimized. A classifi-
cation error occurs when a test document is closer
to some other class than to its true class, in terms of
KL-divergence.
We seek to define a scoring function such that
words whose distribution in the individual training
documents of a class is much different from the dis-
tribution in the class (according to (2)) receive a
lower score, while words with a similar distribution
in all training documents of the same class receive
a higher score. By removing words with a lower
score from the vocabulary, the training documents
of each class become more similar to each other,
and therefore, also to the class, in terms of word dis-
tribution. This leads to more homogeneous classes.
Assuming that the test documents and training doc-
uments come from the same distribution, the simi-
larity between the test documents and their respec-
tive classes will be increased as well, thus resulting
in higher classification accuracy.
We now make this more precise. Let S =
{d1, . . . , d|S|} be the set of training documents, and
denote the class of di with c(di). The average KL-
divergence for a word wt between the training doc-
uments and their classes is given by
KLt(S) =
1
|S|
?
di?S
KL(p(wt|di), p(wt|c(di))).
(8)
One problem with (8) is that in addition to the con-
ditional probabilities p(wt|cj) for each word and
each class, the computation considers each individ-
ual document, thus resulting in a time requirement
of O(|S|).1 In order to avoid this additional com-
plexity, instead of KLt(S) we use an approxima-
tion K?Lt(S), which is based on the following two
assumptions: (i) the number of occurrences of wt
is the same in all documents that contain wt, (ii)
all documents in the same class cj have the same
length. Let Njt be the number of documents in cj
that contain wt, and let
p?d(wt|cj) = p(wt|cj)
|cj |
Njt
(9)
be the average probability of wt in those documents
in cj that contain wt (if wt does not occur in cj , set
p?d(wt|cj) = 0). Then KLt(S) reduces to
K?Lt(S) =
1
|S|
|C|?
j=1
Njtp?d(wt|cj) log
p?d(wt|cj)
p(wt|cj)
.
(10)
Plugging in (9) and (3) and defining q(wt|cj) =
Njt/|cj |, we get
K?Lt(S) = ?
|C|?
j=1
p(cj)p(wt|cj) log q(wt|cj). (11)
Note that computing K?Lt(S) only requires a statis-
tics of the number of words and documents for each
1Note that KLt(S) cannot be computed simultaneously with
p(wt|cj) in one pass over the documents in (2): KLt(S) re-
quires p(wt|cj) when each document is considered, but com-
puting the latter needs iterating over all documents itself.
class, not per document. Thus K?Lt(S) can be com-
puted in O(|C|). Typically, |C| is much smaller
than |S|.
Another important thing to note is the following.
By removing words with an uneven distribution in
the documents of the same class, not only the doc-
uments in the class, but also the classes themselves
may become more similar, which reduces the ability
to distinguish between different classes. Let p(wt)
be the number of occurrences of wt in all training
documents, divided by the total number of words,
q(wt) =
?|C|
j=1 Njt/|S| and define
K?t(S) = ?p(wt) log q(wt). (12)
K?t(S) can be interpreted as an approximation of the
average divergence of the distribution of wt in the
individual training documents from the global dis-
tribution (averaged over all training documents in
all classes). If wt is independent of the class, then
K?t(S) = K?Lt(S). The difference between the two
is a measure of the increase in homogeneity of the
training documents, in terms of the distribution of
wt, when the documents are clustered in their true
classes. It is large if the distribution of wt is similar
in the training documents of the same class but dis-
similar in documents of different classes. In analogy
to mutual information, we define our new scoring
function as the difference
KL(wt) = K?t(S) ? K?Lt(S). (13)
We also use a variant of KL, denoted dKL, where
p(wt) is estimated according to (14):
p?(wt) =
|C|?
j=1
p(cj)p(wt|cj) (14)
and p(wt|cj) is estimated as in (2).
4 Experiments
We compare KL and dKL to mutual information,
using two standard data sets: 20 Newsgroups2 and
Reuters 21578.3 In tokenizing the data, only words
consisting of alphabetic characters are used after
conversion to lower case. In addition, all numbers
are mapped to a special token NUM. For 20 News-
groups we remove the newsgroup headers and use a
stoplist consisting of the 100 most frequent words of
2http://www.ai.mit.edu/?jrennie/20Newsgroups/
3http://www.daviddlewis.com/resources/testcollections/
reuters21578/
dKL
KL
MI
Vocabulary Size
Cl
as
sifi
ca
tio
n
Ac
cu
ra
cy
10000010000100010010
1
0.8
0.6
0.4
0.2
0
Figure 1: Classification accuracy for 20 News-
groups. The curves have small error bars.
the British National Corpus.4 We use the ModApte
split of Reuters 21578 (Apte? et al, 1994) and use
only the 10 largest classes. The vocabulary size is
111868 words for 20 Newsgroups and 22430 words
for Reuters.
Experiments with 20 Newsgroups are performed
with 5-fold cross-validation, using 80% of the data
for training and 20% for testing. We build a sin-
gle classifier for the 20 classes and vary the num-
ber of selected words from 20 to 20000. Figure 1
compares classification accuracy for the three scor-
ing functions. dKL slightly outperforms mutual in-
formation, especially for smaller vocabulary sizes.
The difference is statistically significant for 20 to
200 words at the 99% confidence level, and for 20
to 2000 words at the 95% confidence level, using a
one-tailed paired t-test.
For the Reuters dataset we build a binary classi-
fier for each of the ten topics and set the number of
positively classified documents such that precision
equals recall. Precision is the percentage of posi-
tive documents among all positively classified doc-
uments. Recall is the percentage of positive docu-
ments that are classified as positive.
In Figures 2 and 3 we report microaveraged and
macroaveraged recall for each number of selected
words. Microaveraged recall is the percentage of all
positive documents (in all topics) that are classified
as positive. Macroaveraged recall is the average of
the recall values of the individual topics. Microav-
eraged recall gives equal weight to the documents
and thus emphasizes larger topics, while macroav-
eraged recall gives equal weight to the topics and
thus emphasizes smaller topics more than microav-
4http://www.itri.brighton.ac.uk/?Adam.Kilgarriff/bnc-
readme.html
dKL
KL
MI
Vocabulary Size
Pr
ec
is
io
n/
Re
ca
llB
re
ak
ev
e
n
Po
in
t
10000010000100010010
1
0.95
0.9
0.85
0.8
0.75
0.7
Figure 2: Microaveraged recall on Reuters at break-
even point.
dKL
KL
MI
Vocabulary Size
Pr
ec
is
io
n/
Re
ca
llB
re
ak
ev
e
n
Po
in
t
10000010000100010010
1
0.95
0.9
0.85
0.8
0.75
0.7
Figure 3: Macroaveraged recall on Reuters at break-
even point.
eraged recall.
Both KL and dKL achieve slightly higher values
for microaveraged recall than mutual information,
for most vocabulary sizes (Fig. 2). KL performs best
at 20000 words with 90.1% microaveraged recall,
compared to 89.3% for mutual information. The
largest improvement is found for dKL at 100 words
with 88.0%, compared to 86.5% for mutual infor-
mation.
For smaller categories, the difference between
the KL-divergence based scores and mutual infor-
mation is larger, as indicated by the curves for
macroaveraged recall (Fig. 3). KL yields the high-
est recall at 20000 words with 82.2%, an increase of
3.9% compared to mutual information with 78.3%,
whereas dKL has its largest value at 100 words with
78.8%, compared to 76.1% for mutual information.
We find the largest improvement at 5000 words with
5.6% for KL and 2.9% for dKL, compared to mutual
information.
5 Conclusion
By interpreting Naive Bayes in an information the-
oretic framework, we derive a new scoring method
for feature selection in text classification, based on
the KL-divergence between training documents and
their classes. Our experiments show that it out-
performs mutual information, which was one of
the best performing methods in previous studies
(Yang and Pedersen, 1997). The KL-divergence
based scores are especially effective for smaller cat-
egories, but additional experiments are certainly re-
quired.
In order to keep the computational cost low,
we use an approximation instead of the exact KL-
divergence. Assessing the error introduced by this
approximation is a topic for future work.
References
Chidanand Apte?, Fred Damerau, and Sholom M.
Weiss. 1994. Towards language independent au-
tomated learning of text categorization models.
In Proc. 17th ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR ?94), pages 23?30.
Thomas M. Cover and Joy A. Thomas. 1991. El-
ements of Information Theory. John Wiley, New
York.
Inderjit S. Dhillon, Subramanyam Mallela, and Ra-
hul Kumar. 2002. Enhanced word clustering for
hierarchical text classification. In Proc. 8th ACM
SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 191?
200.
Pedro Domingos and Michael Pazzani. 1997. On
the optimality of the simple bayesian classifier
under zero-one loss. Machine Learning, 29:103?
130.
Andrew McCallum and Kamal Nigam. 1998. A
comparison of event models for Naive Bayes text
classification. In Learning for Text Categoriza-
tion: Papers from the AAAI Workshop, pages 41?
48. AAAI Press. Technical Report WS-98-05.
Jason D. M. Rennie. 2001. Improving multi-class
text classification with Naive Bayes. Master?s
thesis, Massachusetts Institute of Technology.
Yiming Yang and Jan O. Pedersen. 1997. A com-
parative study on feature selection in text catego-
rization. In Proc. 14th International Conference
on Machine Learning (ICML-97), pages 412?
420.
