Proceedings of the Third Workshop on Statistical Machine Translation, pages 151?154,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Word Alignment with Language Model Based Confidence Scores
Nguyen Bach, Qin Gao, Stephan Vogel
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, qing, vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine trans-
lation systems submitted to the ACL-WMT 2008
shared translation task. Systems were submitted for
two translation directions: English?Spanish and
Spanish?English. Using sentence pair confidence
scores estimated with source and target language
models, improvements are observed on the News-
Commentary test sets. Genre-dependent sentence
pair confidence score and integration of sentence
pair confidence score into phrase table are also in-
vestigated.
1 Introduction
Word alignment models are a crucial component in sta-
tistical machine translation systems. When estimating
the parameters of the word alignment models, the sen-
tence pair probability is an important factor in the objec-
tive function and is approximated by the empirical prob-
ability. The empirical probability for each sentence pair
is estimated by maximum likelihood estimation over the
training data (Brown et al, 1993). Due to the limitation of
training data, most sentence pairs occur only once, which
makes the empirical probability almost uniform. This is
a rather weak approximation of the true distribution.
In this paper, we investigate the methods of weighting
sentence pairs using language models, and extended the
general weighting method to genre-dependent weight. A
method of integrating the weight directly into the phrase
table is also explored.
2 The Baseline Phrase-Based MT System
The ACL-WMT08 organizers provided Europarl and
News-Commentary parallel corpora for English ? Span-
ish. Detailed corpus statistics is given in Table 1. Follow-
ing the guidelines of the workshop we built baseline sys-
tems, using the lower-cased Europarl parallel corpus (re-
stricting sentence length to 40 words), GIZA++ (Och and
Ney, 2003), Moses (Koehn et al, 2007), and the SRI LM
toolkit (Stolcke, 2002) to build 5-gram LMs. Since no
News development sets were available we chose News-
Commentary sets as replacements. We used test-2006
(E06) and nc-devtest2007 (NCd) as development sets for
Europarl and News-Commentary; test-2007 (E07) and
nc-test2007 (NCt) as held-out evaluation sets.
English Spanish
Europarl (E)
sentence pairs 1,258,778
unique sent. pairs 1,235,134
avg. sentence length 27.9 29.0
# words 35.14 M 36.54 M
vocabulary 108.7 K 164.8 K
News-Commentary (NC)
sentence pairs 64,308
unique sent. pairs 64,205
avg. sentence length 24.0 27.4
# words 1.54 M 1.76 M
vocabulary 44.2 K 56.9 K
Table 1: Statistics of English?Spanish Europarl and News-
Commentary corpora
To improve the baseline performance we trained sys-
tems on all true-cased training data with sentence length
up to 100. We used two language models, a 5-gram LM
build from the Europarl corpus and a 3-gram LM build
from the News-Commentary data. Instead of interpolat-
ing the two language models, we explicitly used them in
the decoder and optimized their weights via minimum-
error-rate (MER) training (Och, 2003). To shorten the
training time, a multi-threaded GIZA++ version was used
to utilize multi-processor servers (Gao and Vogel, 2008).
Other parameters were the same as the baseline sys-
tem. Table 2 shows results in lowercase BLEU (Pap-
ineni et al, 2002) for both the baseline (B) and the im-
proved baseline systems (B5) on development and held-
151
out evaluation sets. We observed significant gains for the
News-Commentary test sets. Our improved baseline sys-
tems obtained a comparable performance with the best
English?Spanish systems in 2007 (Callison-Burch et al,
2007).
Pairs Europarl NC
E06 E07 NCd NCt
En?Es B 33.00 32.21 31.84 30.56B5 33.33 32.25 35.10 34.08
Es?En B 33.08 33.23 31.18 31.34B5 33.26 33.23 36.06 35.56
Table 2: NIST-BLEU scores of baseline and improved baseline
systems experiments on English?Spanish
3 Weighting Sentence Pairs
3.1 Problem Definition
The quality of word alignment is crucial for the perfor-
mance of the machine translation system.
In the well-known so-called IBM word alignment
models (Brown et al, 1993), re-estimating the model pa-
rameters depends on the empirical probability P? (ek, fk)
for each sentence pair (ek, fk). During the EM train-
ing, all counts of events, e.g. word pair counts, distortion
model counts, etc., are weighted by P? (ek, fk). For ex-
ample, in IBM Model 1 the lexicon probability of source
word f given target word e is calculated as (Och and Ney,
2003):
p(f |e) =
?
k c(f |e; ek, fk)?
k,f c(f |e; ek, fk)
(1)
c(f |e; ek, fk) =
?
ek,fk
P? (ek, fk)
?
a
P (a|ek, fk) ? (2)
?
j
?(f , fkj )?(e, ekaj )
Therefore, the distribution of P? (ek, fk) will affect the
alignment results. In Eqn. 2, P? (ek, fk) determines
how much the alignments of sentence pair (ek, fk) con-
tribute to the model parameters. It will be helpful if
the P? (ek, fk) can approximate the true distribution of
P (ek, fk).
Consider that we are drawing sentence pairs from a
given data source, and each unique sentence pair (ek, fk)
has a probability P (ek, fk) to be observed. If the training
corpora size is infinite, the normalized frequency of each
unique sentence pair will converge to P (ek, fk). In that
case, equally assigning a number to each occurrence of
(ek, fk) and normalizing it will be valid. However, the
assumption is invalid if the data source is finite. As we
can observe in the training corpora, most sentences occur
only one time, and thus P? (ek, fk) will be uniform.
To get a more informative P? (ek, fk), we explored
methods of weighting sentence pairs. We investigated
three sets of features: sentence pair confidence (sc),
genre-dependent sentence pair confidence (gdsc) and
phrase alignment confidence (pc) scores. These features
were calculated over an entire training corpus and could
be easily integrated into the phrase-based machine trans-
lation system.
3.2 Sentence Pair Confidence
We can hardly compute the joint probability of P (ek, fk)
without knowing the conditional probability P (ek|fk)
which is estimated during the alignment process. There-
fore, to estimate P (ek, fk) before alignment, we make an
assumption that P? (ek, fk) = P (ek)P (fk), which means
the two sides of sentence pair are independent of each
other. P (ek) and P (fk) can be obtained by using lan-
guage models. P (ek) or P (fk), however, can be small
when the sentence is long. Consequently, long sentence
pairs will be assigned low scores and have negligible ef-
fect on the training process. Given limited training data,
ignoring these long sentences may hurt the alignment re-
sult. To compensate this, we normalize the probability by
the sentence length. We propose the following method
to weighting sentence pairs in the corpora. We trained
language models for source and target language, and the
average log likelihood (AVG-LL) of each sentence pair
was calculated by applying the corresponding language
model. For each sentence pair (ek, fk), the AVG-LL
L(ek, fk) is
L(ek) = 1|ek|
?
eki ?ek logP (e
k
i |h)
L(fk) = 1|fk|
?
fkj ?fk logP (f
k
j |h)
L(ek, fk) = [L(ek) + L(fk)]/2
(3)
where P (eki |h) and P (fkj |h) are ngram probabilities.
The sentence pair confidence score is then given by:
sc(ek, fk) = exp(L(ek, fk)). (4)
3.3 Genre-Dependent Sentence Pair Confidence
Genre adaptation is one of the major challenges in statis-
tical machine translation since translation models suffer
from data sparseness (Koehn and Schroeder, 2007). To
overcome these problems previous works have focused
on explicitly modeling topics and on using multiple lan-
guage and translation models. Using a mixture of topic-
dependent Viterbi alignments was proposed in (Civera
and Juan, 2007). Language and translation model adap-
tation to Europarl and News-Commentary have been ex-
plored in (Paulik et al, 2007).
Given the sentence pair weighting method, it is pos-
sible to adopt genre-specific language models into the
152
weighting process. The genre-dependent sentence pair
confidence gdsc simulates weighting the training sen-
tences again from different data sources, thus, given
genre g, it can be formulated as:
gdsc(ek, fk) = sc(ek, fk|g) (5)
where P (eki |h) and P (fkj |h) are estimated by genre-
specific language models.
The score generally represents the likelihood of the
sentence pair to be in a specific genre. Thus, if both sides
of the sentence pair show a high probability according
to the genre-specific language models, alignments in the
pair should be more possible to occur in that particular
domain, and put more weight may contribute to a better
alignment for that genre.
3.4 Phrase Alignment Confidence
So far the confidence scores are used only in the train-
ing of the word alignment models. Tracking from which
sentence pairs each phrase pair was extracted, we can use
the sentence level confidence scores to assign confidence
scores to the phrase pairs. Let S(e?, f?) denote the set of
sentences pairs from which the phrase pair (e?, f?) was ex-
tracted. We calculate then a phrase alignment confidence
score pc as:
pc(e?, f?) = exp
?
(ek,fk)?S(e?,f?) log sc(ek, fk)
|S(e?, f?)| (6)
This score is used as an additional feature of the phrase
pair. The feature weight is estimated in MER training.
4 Experimental Results
The first step in validating the proposed approach was
to check if the different language models do assign dif-
ferent weights to the sentence pairs in the training cor-
pora. Using the different language models NC (News-
Commentary), EP (Europarl), NC+EP (both NC and EP)
the genre-specific sentence pair confidence scores were
calculated. Figure 1 shows the distributions of the dif-
ferences in these scores across the two corpora. As ex-
pected, the language model build from the NC corpus as-
signs - on average - higher weights to sentence pairs in the
NC corpus and lower weights to sentence pairs in the EP
corpus (Figure 1a). The opposite is true for the EP LM.
When comparing the scores calculated from the NC LM
and the combined NC+EP LM we still see a clear sep-
aration (Figure 1b). No marked difference can be seen
between using the EP LM and the NC+EP LM (Figure
1c), which again is expected, as the NC corpus is very
small compared to the EP corpus.
The next step was to retrain the word alignment mod-
els using sentences weights according to the various con-
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
(a) Difference in Weight (NC?EP)
Pro
port
ion 
in C
orpo
ra
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
0.02
(b) Difference in Weight (NC?NE)
Pro
port
ion 
in C
orpo
ra
?0.06 ?0.04 ?0.02 0 0.02 0.04 0.060
0.005
0.01
0.015
0.02
(c) Difference in Weight (NE?EP)
Pro
port
ion 
in C
orpo
ra
Europal DataNews Commentary Data
Europal DataNews Commentary Data
Europal DataNews Commentary Data
Figure 1: Histogram of weight differences genre specific con-
fidence scores on NC and EP training corpora
fidence scores. Table 3 shows training and test set per-
plexities for IBM model 4 for both training directions.
Not only do we see a drop in training set perplexities,
but also in test set perplexities. Using the genre specific
confidence scores leads to lower perplexities on the cor-
responding test set, which means that using the proposed
method does lead to small, but consistent adjustments in
the alignment models.
Uniform NC+EP NC EP
train En?Es 46.76 42.36 42.97 44.47Es?En 70.18 62.81 62.95 65.86
test
NC(En?Es) 53.04 53.44 51.09 55.94
EP(En?Es) 91.13 90.89 91.84 90.77
NC(Es?En) 81.39 81.28 78.23 80.33
EP(Es?En) 126.56 125.96 123.23 122.11
Table 3: IBM model 4 training and test set perplexities using
genre specific sentence pair confidence scores.
In the final step the specific alignment models were
used to generate various phrase tables, which were then
used in translation experiments. Results are shown in Ta-
ble 4. We report lower-cased Bleu scores. We used nc-
dev2007 (NCt1) as an additional held-out evaluation set.
Bold cells indicate highest scores.
As we can see from the results, improvements are ob-
tained by using sentence pair confidence scores. Us-
ing confidence scores calculated from the EP LM gave
overall the best performance. While we observe only a
small improvement on Europarl sets, improvements on
News-Commentary sets are more pronounced, especially
on held-out evaluation sets NCt and NCt1. The exper-
iments do not give evidence that genre-dependent con-
fidence can improve over using the general confidence
153
Test Set
E06 E07 NCd NCt NCt1
Es?En
B5 33.26 33.23 36.06 35.56 35.64
NC+EP 33.23 32.29 36.12 35.47 35.97
NC 33.43 33.39 36.14 35.27 35.68
EP 33.36 33.39 36.16 35.63 36.17
En?Es
B5 33.33 32.25 35.10 34.08 34.43
NC+EP 33.23 32.29 35.12 34.56 34.89
NC 33.30 32.27 34.91 34.07 34.29
EP 33.08 32.29 35.05 34.52 35.03
Table 4: Translation results (NIST-BLEU) using gdsc with dif-
ferent genre-specific language models for Es?En systems
score. As the News-Commentary language model was
trained on a very small amount of data further work is
required to study this in more detail.
Test Set
E06 E07 NCd NCt NCt1
Es?En
B5 33.26 33.23 36.06 35.56 35.64
NC+EP+pc 33.54 33.39 36.07 35.38 35.85
NC+pc 33.17 33.31 35.96 35.74 36.04
EP+pc 33.44 32.87 36.22 35.63 36.09
En?Es
B5 33.33 32.25 35.10 34.08 34.43
NC+EP+pc 33.28 32.45 34.82 33.68 33.86
NC+pc 33.13 32.47 34.01 34.34 34.98
EP+pc 32.97 32.20 34.26 33.99 34.34
Table 5: Translation results (NIST-BLEU) using pc with differ-
ent genre-specific language models for Es?En systems
Table 5 shows experiments results in NIST-BLEU us-
ing pc score as an additional feature on phrase tables
in Es?En systems. We observed that across develop-
ment and held-out sets the gains from pc are inconsistent,
therefore our submissions are selected from the B5+EP
system.
5 Conclusion
In the ACL-WMT 2008, our major innovations are meth-
ods to estimate sentence pair confidence via language
models. We proposed to use source and target language
models to weight the sentence pairs. We developed sen-
tence pair confidence (sc), genre-dependent sentence pair
confidence (gdsc) and phrase alignment confidence (pc)
scores. Our experimental results shown that we had a bet-
ter word alignment and translation performance by using
gdsc. We did not observe consistent improvements by
using phrase pair confidence scores in our systems.
Acknowledgments
This work is in part supported by the US DARPA under the
GALE program. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of DARPA.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statisti-
cal machine translation: Parameter estimation. In Computa-
tional Linguistics, volume 19(2), pages 263?331.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-) evalua-
tion of machine translation. In Proc. of the ACL 2007 Second
Workshop on Statistical Machine Translation, Prague, Czech
Republic.
Jorge Civera and Alfons Juan. 2007. Domain adaptation in sta-
tistical translation with mixture modelling. In Proc. of the
ACL 2007 Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.
Qin Gao and Stephan Vogel. 2008. Parallel implementations
of word alignment tool. In Proc. of the ACL 2008 Soft-
ware Engineering, Testing, and Quality Assurance Work-
shop, Columbus, Ohio, USA.
Philipp Koehn and Josh Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation. In Proc.
of the ACL 2007 Second Workshop on Statistical Machine
Translation, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics, demo sessions, pages 177?
180, Prague, Czech Republic, June.
Franz J. Och and Hermann Ney. 2003. A systematic compar-
ison of various statistical alignment models. In Computa-
tional Linguistics, volume 1:29, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Erhard Hinrichs and Dan Roth,
editors, Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th Annual Conf. of the
Association for Computational Linguistics (ACL 02), pages
311?318, Philadelphia, PA, July.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja Hildebrand,
and Stephan Vogel. 2007. The ISL phrase-based mt system
for the 2007 ACL workshop on statistical machine transla-
tion. In In Proc. of the ACL 2007 Second Workshop on Sta-
tistical Machine Translation, Prague, Czech Republic.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing, volume 2, pages 901?904, Denver.
154
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49?57,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Parallel Implementations of Word Alignment Tool
Qin Gao and Stephan Vogel
Language Technology Institution
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{qing, stephan.vogel}@cs.cmu.edu
Abstract
Training word alignment models on large cor-
pora is a very time-consuming processes. This
paper describes two parallel implementations
of GIZA++ that accelerate this word align-
ment process. One of the implementations
runs on computer clusters, the other runs on
multi-processor system using multi-threading
technology. Results show a near-linear speed-
up according to the number of CPUs used, and
alignment quality is preserved.
1 Introduction
Training state-of-the-art phrase-based statistical ma-
chine translation (SMT) systems requires several
steps. First, word alignment models are trained on
the bilingual parallel training corpora. The most
widely used tool to perform this training step is the
well-known GIZA++(Och and Ney, 2003). The re-
sulting word alignment is then used to extract phrase
pairs and perhaps other information to be used in
translation systems, such as block reordering mod-
els. Among the procedures, more than 2/3 of the
time is consumed by word alignment (Koehn et al,
2007). Speeding up the word alignment step can
dramatically reduces the overall training time, and in
turn accelerates the development of SMT systems.
With the rapid development of computing hard-
ware, multi-processor servers and clusters become
widely available. With parallel computing, process-
ing time (wall time) can often be cut down by one
or two orders of magnitude. Tasks, which require
several weeks on a single CPU machine may take
only a few hours on a cluster. However, GIZA++
was designed to be single-process and single-thread.
To make more efficient use of available computing
resources and thereby speed up the training of our
SMT system, we decided to modify GIZA++ so that
it can run in parallel on multiple CPUs.
The word alignment models implemented in
GIZA++, the so-called IBM (Brown et al, 1993) and
HMM alignment models (Vogel et al, 1996) are typ-
ical implementation of the EM algorithm (Dempster
et al, 1977). That is to say that each of these mod-
els run for a number of iterations. In each iteration
it first calculates the best word alignment for each
sentence pairs in the corpus, accumulating various
counts, and then normalizes the counts to generate
the model parameters for the next iteration. The
word alignment stage is the most time-consuming
part, especially when the size of training corpus is
large. During the aligning stage, all sentences can
be aligned independently of each other, as model
parameters are only updated after all sentence pairs
have been aligned. Making use of this property, the
alignment procedure can be parallelized. The basic
idea is to have multiple processes or threads aligning
portions of corpus independently and then merge the
counts and perform normalization.
The paper implements two parallelization meth-
ods. The PGIZA++ implementation, which is based
on (Lin et al 2006), uses multiple aligning pro-
cesses. When all the processes finish, a master pro-
cess starts to collect the counts and normalizes them
to produce updated models. Child processes are then
restarted for the new iteration. The PGIZA++ does
not limit the number of CPUs being used, whereas
it needs to transfer (in some cases) large amounts
49
of data between processes. Therefore its perfor-
mance also depends on the speed of the network in-
frastructure. The MGIZA++ implementation, on the
other hand, starts multiple threads on a common ad-
dress space, and uses a mutual locking mechanism
to synchronize the access to the memory. Although
MGIZA++ can only utilize a single multi-processor
computer, which limits the number of CPUs it can
use, it avoids the overhead of slow network I/O. That
makes it an equally efficient solution for many tasks.
The two versions of alignment tools are available on-
line at http://www.cs.cmu.edu/q?ing/giza.
The paper will be organized as follows, section 2
provides the basic algorithm of GIZA++, and sec-
tion 3 describes the PGIZA++ implementation. Sec-
tion 4 presents the MGIZA++ implementation, fol-
lowed by the profile and evaluation results of both
systems in section 5. Finally, conclusion and future
work are presented in section 6.
2 Outline of GIZA++
2.1 Statistical Word Alignment Models
GIZA++ aligns words based on statistical models.
Given a source string fJ1 = f1, ? ? ? , fj , ? ? ? , fJ and a
target string eI1 = e1, ? ? ? , ei, ? ? ? , eI , an alignment A
of the two strings is defined as(Och and Ney, 2003):
A ? {(j, i) : j = 1, ? ? ? , J ; i = 0, ? ? ? , I} (1)
in case that i = 0 in some (j, i) ? A, it represents
that the source word j aligns to an ?empty? target
word e0.
In statistical world alignment, the probability of a
source sentence given target sentence is written as:
P (fJ1 |eI1) =
?
aJ1
P (fJ1 , aJ1 |eI1) (2)
in which aJ1 denotes the alignment on the sen-
tence pair. In order to express the probability in
statistical way, several different parametric forms of
P (fJ1 , aJ1 |eI1) = p?(fJ1 , aJ1 |eI1) have been proposed,
and the parameters ? can be estimated using maxi-
mum likelihood estimation(MLE) on a training cor-
pus(Och and Ney, 2003).
?? = arg max
?
S
?
s=1
?
a
p?(fs, a|es) (3)
The best alignment of the sentence pair,
a?J1 = arg max
aJ1
p??(f
J
1 , aJ1 |eI1) (4)
is called Viterbi alignment.
2.2 Implementation of GIZA++
GIZA++ is an implementation of ML estimators for
several statistical alignment models, including IBM
Model 1 through 5 (Brown et al, 1993), HMM (Vo-
gel et al, 1996) and Model 6 (Och and Ney, 2003).
Although IBM Model 5 and Model 6 are sophisti-
cated, they do not give much improvement to align-
ment quality. IBM Model 2 has been shown to be
inferior to the HMM alignment model in the sense
of providing a good starting point for more complex
models. (Och and Ney, 2003) So in this paper we
focus on Model 1, HMM, Model 3 and 4.
When estimating the parameters, the EM (Demp-
ster et al, 1977) algorithm is employed. In the
E-step the counts for all the parameters are col-
lected, and the counts are normalized in M-step.
Figure 1 shows a high-level view of the procedure
in GIZA++. Theoretically the E-step requires sum-
ming over all the alignments of one sentence pair,
which could be (I + 1)J alignments in total. While
(Och and Ney, 2003) presents algorithm to imple-
ment counting over all the alignments for Model 1,2
and HMM, it is prohibitive to do that for Models 3
through 6. Therefore, the counts are only collected
for a subset of alignments. For example, (Brown
et al, 1993) suggested two different methods: us-
ing only the alignment with the maximum probabil-
ity, the so-called Viterbi alignment, or generating a
set of alignments by starting from the Viterbi align-
ment and making changes, which keep the align-
ment probability high. The later is called ?pegging?.
(Al-Onaizan et al, 1999) proposed to use the neigh-
bor alignments of the Viterbi alignment, and it yields
good results with a minor speed overhead.
During training we starts from simple models use
the simple models to bootstrap the more complex
ones. Usually people use the following sequence:
Model 1, HMM, Model 3 and finally Model 4. Table
1 lists all the parameter tables needed in each stage
and their data structures1. Among these models, the
1In filename, prefix is a user specified parameter, and n is
the number of the iteration.
50
Figure 1: High-level algorithm of GIZA++
lexicon probability table (TTable) is the largest. It
should contain all the p(fi, ej) entries, which means
the table will have an entry for every distinct source
and target word pair fi, ej that co-occurs in at least
one sentence pair in the corpus. However, to keep
the size of this table manageable, low probability en-
tries are pruned. Still, when training the alignment
models on large corpora this statistical lexicon often
consumes several giga bytes of memory.
The computation time of aligning a sentence pair
obviously depends on the sentence length. E.g. for
IBM 1 that alignment is O(J ? I), for the HMM
alignment it is O(J + I2), with J the number of
words in the source sentence and I the number of
words in the target sentence. However, given that
the maximum sentence length is fixed, the time com-
plexity of the E-step grows linearly with the num-
ber of sentence pairs. The time needed to perform
the M-step is dominated by re-normalizing the lexi-
con probabilities. The worst case time complexity is
O(|VF | ? |VE |), where |VF | is the size of the source
vocabulary and |VE | is the size of the target vocabu-
lary. Therefore, the time complexity of the M-step is
polynomial in the vocabulary size, which typically
grows logarithmic in corpus size. As a result, the
alignment stage consumes most of the overall pro-
cessing time when the number of sentences is large.
Because the parameters are only updated during
the M-step, it will be no difference in the result
whether we perform the word alignment in the E-
step sequentially or in parallel2. These character-
2However, the rounding problem will make a small differ-
istics make it possible to build parallel versions of
GIZA++. Figure 2 shows the basic idea of parallel
GIZA++.
Figure 2: Basic idea of Parallel GIZA++
While working on the required modification to
GIZA++ to run the alignment step in parallel we
identified a bug, which needed to be fixed. When
training the HMM model, the matrix for the HMM
trellis will not be initialized if the target sentence has
only one word. Therefore some random numbers
are added to the counts. This bug will also crash
the system when linking against pthread library. We
observe different alignment and slightly lower per-
plexity after fixing the bug 3.
3 Multi-process version - PGIZA++
3.1 Overview
A natural idea of parallelizing GIZA++ is to sep-
arate the alignment and normalization procedures,
and spawn multiple alignment processes. Each pro-
cess aligns a chunk of the pre-partitioned corpus and
outputs partial counts. A master process takes these
counts and combines them, and produces the nor-
malized model parameters for the next iteration. The
architecture of PGIZA++ is shown in Figure 3.
ence in the results even when processing the sentences sequen-
tially, but in different order.
3The details of the bug can be found in: http://www.mail-
archive.com/moses-support@mit.edu/msg00292.html
51
Model Parameter tables Filename Description Data structure
Model 1 TTable prefix.t1.n Lexicon Probability Array of Array
HMM TTable prefix.thmm.n
ATable prefix.ahmm.n Align Table 4-D Array
HMMTable prefix.hhmm.n HMM Jump Map
Model 3/4 TTable prefix.t3.n
ATable prefix.a3.n Align Table
NTable prefix.n3.n Fertility Table 2-D Array
DTable prefix.d3.n Distortion Table 4-D Array
pz prefix.p0 3.n Probability for null words p0 Scalar
(Model 4 only) D4Table prefix.d4.n prefix.D4.n Distortion Table for Model 4 Map
Table 1: Model tables created during training
Figure 3: Architecture of PGIZA++
3.2 Implementation
3.2.1 I/O of the Parameter Tables
In order to ensure that the next iteration has the
correct model, all the information that may affect the
alignment needs to be stored and shared. It includes
model files and statistics over the training corpus.
Table 1 is a summary of tables used in each model.
Step Without With
Pruning(MB) Pruning(MB)
Model 1, Step 1 1,273 494
HMM , Step 5 1,275 293
Model 4 , Step 3 1,280 129
Table 2: Comparison of the size of count tables for the
lexicon probabilities
In addition to these models, the summation of
?sentence weight? of the whole corpus should be
stored. GIZA++ allows assigning a weight wi for
each sentence pair si sto indicate the number of oc-
currence of the sentence pair. The weight is normal-
ized by pi = wi/
?
i wi, so that
?
i pi = 1. Then
the pi serves as a prior probability in the objective
function. As each child processes only see a portion
of training data, it is required to calculate and share
the
?
i wi among the children so the values can be
consistent.
The tables and count tables of the lexicon proba-
bilities (TTable) can be extremely large if not pruned
before being written out. Pruning the count tables
when writing them into a file will make the result
slightly different. However, as we will see in Sec-
tion 5, the difference does not hurt translation per-
formance significantly. Table 2 shows the size of
count tables written by each child process in an ex-
periment with 10 million sentence pairs, remember
there are more than 10 children writing the the count
tables, and the master would have to read all these
tables, the amount of I/O is significantly reduced by
pruning the count tables.
3.2.2 Master Control Script
The other issue is the master control script. The
script should be able to start processes in other
nodes. Therefore the implementation varies accord-
ing to the software environment. We implemented
three versions of scripts based on secure shell, Con-
dor (Thain et al, 2005) and Maui.
Also, the master must be notified when a child
process finishes. In our implementation, we use sig-
nal files in the network file system. When the child
process finishes, it will touch a predefined file in a
shared folder. The script keeps watching the folder
and when all the children have finished, the script
runs the normalization process and then starts the
next iteration.
52
3.3 Advantages and Disadvantages
One of the advantages of PGIZA++ is its scalability,
it is not limited by the number of CPUs of a sin-
gle machine. By adding more nodes, the alignment
speed can be arbitrarily fast4. Also, by splitting the
corpora into multiple segments, each child process
only needs part of the lexicon, which saves mem-
ory. The other advantage is that it can adopt differ-
ent resource management systems, such as Condor
and Maui/Torque. By splitting the corpus into very
small segments, and submitting them to a scheduler,
we can get most out of clusters.
However, PGIZA++ also has significant draw-
backs. First of all, each process needs to load the
models of the previous iteration, and store the counts
of the current step on shared storage. Therefore,
I/O becomes a bottleneck, especially when the num-
ber of child processes is large. Also, the normal-
ization procedure needs to read all the count files
from network storage. As the number of child pro-
cesses increases, the time spent on reading/writing
will also increase. Given the fact that the I/O de-
mand will not increase as fast as the size of corpus
grows, PGIZA++ can only provide significant speed
up when the size of each training corpus chunk is
large enough so that the alignment time is signifi-
cantly longer than normalization time.
Also, one obvious drawback of PGIZA++ is its
complexity in setting up the environment. One has
to write scripts specially for the scheduler/resource
management software.
Balancing the load of each child process is an-
other issue. If any one of the corpus chunks takes
longer to complete, the master has to wait for it. In
other words, the speed of PGIZA++ is actually de-
termined by the slowest child process.
4 Multi-thread version - MGIZA++
4.1 Overview
Another implementation of parallelism is to run sev-
eral alignment threads in a single process. The
threads share the same address space, which means
it can access the model parameters concurrently
without any I/O overhead.
4The normalization process will be slower when the number
of nodes increases
The architecture of MGIZA++ is shown in Figure
4.
Data Sentence 
Provider
Thread 1 Thread 2 Thread n
Synchronized Assignment of 
Sentence Pairs
Model
Synchronized 
Count Storage
Main Thread
Normalization
Figure 4: Architecture of MGIZA++
4.2 Implementation
The main thread spawns a number of threads, us-
ing the same entry function. Each thread will ask
a provider for the next sentence pair. The sentence
provider is synchronized. The request of sentences
are queued, and each sentence pair is guaranteed to
be assigned to only one thread.
The threads do alignment in their own stacks, and
read required probabilities from global parameter ta-
bles, such as the TTable, which reside on the heap.
Because no update on these global tables will be per-
formed during this stage, the reading can be concur-
rent. After aligning the sentence pairs, the counts
need to be collected. For HMMTable and D4Table,
which use maps as their data structure, we cannot
allow concurrent read/write to the table, because the
map structure may be changed when inserting a new
entry. So we must either put mutual locks to post-
pone reading until writing is complete, or dupli-
cate the tables for each thread and merge them af-
terwards. Locking can be very inefficient because
it may block other threads, so the duplicate/merge
method is a much better solution. However, for the
TTable the size is too large to have multiple copies.
Instead, we put a lock on every target word, so only
when two thread try to write counts for the same tar-
get word will a collisions happen. We also have to
put mutual locks on the accumulators used to calcu-
late the alignment perplexity.
53
Table Synchronizations Method
TTable Write lock on every target words
ATable Duplicate/Merge
HMMTable Duplicate/Merge
DTable Duplicate/Merge
NTable Duplicate/Merge
D4Table Duplicate /Merge
Perplexity Mutual lock
Table 3: Synchronizations for tables in MGIZA++
Each thread outputs the alignment into its own
output file. Sentences in these files are not in sequen-
tial order. Therefore, we cannot simply concatenate
them but rather have to merge them according to the
sentence id.
4.3 Advantages and Disadvantages
Because all the threads within a process share the
same address space, no data needs to be transferred,
which saves the I/O time significantly. MGIZA++ is
more resource-thrifty comparing to PGIZA++, it do
not need to load copies of models into memory.
In contrast to PGIZA++, MGIZA++ has a much
simpler interface and can be treated as a drop-in
replacement for GIZA++, except that one needs
to run a script to merge the final alignment files.
This property makes it very simple to integrate
MGIZA++ into machine translation packages, such
as Moses(Koehn et al, 2007).
One major disadvantage of MGIZA++ is also ob-
vious: lack of scalability. Accelerating is limited
by the number of CPUs the node has. Compared
to PGIZA++ on the speed-up factor by each addi-
tional CPU, MGIZA++ also shows some deficiency.
Due to the need for synchronization, there are al-
ways some CPU time wasted in waiting.
5 Experiments
5.1 Experiments on PGIZA++
For PGIZA++ we performed training on an Chinese-
English translation task. The dataset consists of ap-
proximately 10 million sentence pairs with 231 mil-
lion Chinese words and 258 million English words.
We ran both GIZA++ and PGIZA++ on the same
training corpus with the same parameters, then ran
Pharaoh phrase extraction on the resulting align-
ments. Finally, we tuned our translation systems on
the NIST MT03 test set and evaluate them on NIST
MT06 test set. The experiment was performed on
a cluster of several Xeon CPUs, the storage of cor-
pora and models are on a central NFS server. The
PGIZA++ uses Condor as its scheduler, splitting the
training data into 30 fragments, and ran training in
both direction (Ch-En, En-Ch) concurrently. The
scheduler assigns 11 CPUs on average to the tasks.
We ran 5 iterations of Model 1 training, 5 iteration
of HMM, 3 Model 3 iterations and 3 Model 4 iter-
ations. To compare the performance of system, we
recorded the total training time and the BLEU score,
which is a standard automatic measurement of the
translation quality(Papineni et al, 2002). The train-
ing time and BLEU scores are shown in Table 4: 5
Running (TUNE) (TEST)
Time MT03 MT06 CPUs
GIZA++ 169h 32.34 29.43 2
PGIZA++ 39h 32.20 30.14 11
Table 4: Comparison of GIZA++ and PGIZA++
The results show similar BLEU scores when us-
ing GIZA++ and PGIZA++, and a 4 times speed up.
Also, we calculated the time used in normaliza-
tion. The average time of each normalization step is
shown in Table 5.
Per-iteration (Avg) Total
Model 1 47.0min 235min (3.9h)
HMM 31.8min 159min (2.6h)
Model 3/4 25.2 min 151min (2.5h)
Table 5: Normalization time in each stage
As we can see, if we rule out the time spent in
normalization, the speed up is almost linear. Higher
order models require less time in the normalization
step mainly due to the fact that the lexicon becomes
smaller and smaller with each models (see Table 2.
PGIZA++, in small amount of data,
5.2 Experiment on MGIZA++
Because MGIZA++ is more convenient to integrate
into other packages, we modified the Moses sys-
tem to use MGIZA++. We use the Europal English-
Spanish dataset as training data, which contains 900
thousand sentence pairs, 20 million English words
and 20 million Spanish words. We trained the
5All the BLEU scores in the paper are case insensitive.
54
English-to-Spanish system, and tuned the system
on two datasets, the WSMT 2006 Europal test set
(TUNE1) and the WSMT news commentary dev-
test set 2007 (TUNE2). Then we used the first pa-
rameter set to decode WSMT 2006 Europal test set
(TEST1) and used the second on WSMT news com-
mentary test set 2007 (TEST2)6. Table 6 shows the
comparison of BLEU scores of both systems. listed
in Table 6:
TUNE1 TEST1 TUNE2 TEST2
GIZA++ 33.00 32.21 31.84 30.56
MGIZA++ 32.74 32.26 31.35 30.63
Table 6: BLEU Score of GIZA++ and MGIZA++
Note that when decoding using the phrase table
resulting from training with MGIZA++, we used
the parameter tuned for a phrase table generated
from GIZA++ alignment, which may be the cause
of lower BLEU score in the tuning set. However,
the major difference in the training comes from fix-
ing the HMM bug in GIZA++, as mentioned before.
To profile the speed of the system according to
the number of CPUs it use, we ran MGIZA++ on
1, 2 and 4 CPUs of the same speed. When it runs
on 1 CPU, the speed is the same as for the original
GIZA++. Table 7 and Figure 5 show the running
time of each stage:
4000
5000
6000
7000
8000
m
e(
s)
Model 1
HMM
Model3/4
0
1000
2000
3000
1 2 3 4
Ti
m
CPUS
Figure 5: Speed up of MGIZA++
When using 4 CPUs, the system uses only 41%
time comparing to one thread. Comparing to
PGIZA++, MGIZA++ does not have as high an ac-
6http://www.statmt.org/wmt08/shared-task.html
CPUs M1(s) HMM(s) M3,M4(s) Total(s)
1 2167 5101 7615 14913
2 1352 3049 4418 8854
(62%) (59%) (58%) (59%)
4 928 2240 2947 6140
(43%) (44%) (38%) (41%)
Table 7: Speed of MGIZA++
celeration rate. That is mainly because of the re-
quired locking mechanism. However the accelera-
tion is also significant, especially for small training
corpora, as we will see in next experiment.
5.3 Comparison of MGIZA++ and PGIZA++
In order to compare the acceleration rate of
PGIZA++ and MGIZA++, we also ran PGIZA++ in
the same dataset as described in the previous section
with 4 children. To avoid the delay of starting the
children processes, we chose to use ssh to start re-
mote tasks directly, instead of using schedulers. The
results are listed in Table 8.
M1(s) HMM(s) M3,M4(s) Total(s)
MGIZA+1CPU 2167 5101 7615 14913
MGIZA+4CPUs 928 2240 2947 6140
PGIZA+4Nodes 3719 4324 4920 12963
Table 8: Speed of PGIZA++ on Small Corpus
There is nearly no speed-up observed, and in
Model 1 training, we observe a loss in the speed.
Again, by investigating the time spent in normaliza-
tion, the phenomenon can be explained (Table 9):
Even after ruling out the normalization time, the
speed up factor is smaller than MGIZA++. That
is because of reading models when child processes
start and writing models when child processes finish.
From the experiment we can conclude that
PGIZA++ is more suited to train on large corpora
than on small or moderate size corpora. It is also im-
portant to determine whether to use PGIZA++ rather
than MGIZA++ according to the speed of network
storage infrastructure.
5.4 Difference in Alignment
To compare the difference in final Viterbi alignment
output, we counted the number of sentences that
have different alignments in these systems. We use
55
Per-iteration (Avg) Total
Model 1 8.4min 41min (0.68h)
HMM 7.2min 36min (0.60h)
Model 3/4 5.7 min 34min (0.57h)
Total 111min (1.85h)
Table 9: Normalization time in each stage : small data
GIZA++ with the bug fixed as the reference. The
results of all other systems are listed in Table 10:
Diff Lines Diff Percent
GIZA++(origin) 100,848 10.19%
MGIZA++(4CPU) 189 0.019%
PGIZA++(4Nodes) 18,453 1.86%
Table 10: Difference in Viterbi alignment (GIZA++ with
the bug fixed as reference)
From the comparison we can see that PGIZA++
has larger difference in the generated alignment.
That is partially because of the pruning on count ta-
bles.
To also compare the alignment score in the differ-
ent systems. For each sentence pair i = 1, 2, ? ? ? , N ,
assume two systems b and c have Viterbi alignment
scores Sbi , Sci . We define the residual R as:
R = 2
?
i
( |Sbi ? Sci |
(Sbi + Sci )
)
/N (5)
The residuals of the three systems are listed in Table
11. The residual result shows that the MGIZA++ has
a very small (less than 0.2%) difference in alignment
scores, while PGIZA++ has a larger residual.
The results of experiments show the efficiency
and also the fidelity of the alignment generated by
the two versions of parallel GIZA++. However,
there are still small differences in the final align-
ment result, especially for PGIZA++. Therefore,
one should consider which version to choose when
building systems. Generally speaking, MGIZA++
provides smoother integration into other packages:
easy to set up and also more precise. PGIZA++ will
not perform as good as MGIZA++ on small-size cor-
pora. However, PGIZA++ has good performance on
large data, and should be considered when building
very large scale systems.
6 Conclusion
The paper describes two parallel implementations
of the well-known and widely used word alignment
R
GIZA++(origin) 0.6503
MGIZA++(4CPU) 0.0017
PGIZA++(4Nodes) 0.0371
Table 11: Residual in Viterbi alignment scores (GIZA++
with the bug fixed as reference)
tool GIZA++. PGIZA++ does alignment on a num-
ber of independent processes, uses network file sys-
tem to collect counts, and performs normalization by
a master process. MGIZA++ uses a multi-threading
mechanism to utilize multiple cores and avoid net-
work transportation. The experiments show that the
two implementation produces similar results with
original GIZA++, but lead to a significant speed-up
in the training process.
With compatible interface, MGIZA++ is suit-
able for a drop-in replacement for GIZA++, while
PGIZA++ can utilize huge computation resources,
which is suitable for building large scale systems
that cannot be built using a single machine.
However, improvements can be made on both
versions. First, a combination of the two imple-
mentation is reasonable, i.e. running multi-threaded
child processes inside PGIZA++?s architecture. This
could reduce the I/O significantly when using the
same number of CPUs. Secondly, the mechanism
of assigning sentence pairs to the child processes can
be improved in PGIZA++. A server can take respon-
sibility to assign sentence pairs to available child
processes dynamically. This would avoid wasting
any computation resource by waiting for other pro-
cesses to finish. Finally, the huge model files, which
are responsible for a high I/O volume can be reduced
by using binary formats. A first implementation of a
simple binary format for the TTable resulted in files
only about 1/3 in size on disk compared to the plain
text format.
The recent development of MapReduce frame-
work shows its capability to parallelize a variety of
machine learning algorithms, and we are attempting
to port word alignment tools to this framework. Cur-
rently, the problems to be addressed is the I/O bot-
tlenecks and memory usage, and an attempt to use
distributed structured storage such as HyperTable to
enable fast access to large tables and also performing
filtering on the tables to alleviate the memory issue.
56
References
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum Likelihood From Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):138
Douglas Thain, Todd Tannenbaum, and Miron Livny.
2005. Distributed Computing in Practice: The Con-
dor Experience. Concurrency and Computation: Prac-
tice and Experience, 17(2-4):323-356
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19-51
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, Demonstration Session, Prague, Czech Repub-
lic
Peter F. Brown, Stephan A. Della Pietra, Vincent J. Della
Pietra, Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263-311
Stephan Vogel, Hermann Ney and Christoph Tillmann.
1996. HMM-based Word Alignment in Statistical
Translation. In COLING ?96: The 16th International
Conference on Computational Linguistics, pp. 836-
841, Copenhagen, Denmark.
Xiaojun Lin, Xinhao Wang and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 Machine Translation Evalu-
ation
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John D. Lafferty, I. Dan Melamed, David
Purdy, Franz J. Och, Noah A. Smith and David
Yarowsky. 1999. Statistical Machine Trans-
lation. Final Report JHU Workshop, Available at
http://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-
final-reports.ps
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu 2002. BLEU: a Method for Automatic Eval-
uation of machine translation. Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pp. 311-318, Philadelphia, PA
57
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 349?357,
Beijing, August 2010
EMDC: A Semi-supervised Approach for Word Alignment
Qin Gao
Language Technologies Institute
Carnegie Mellon University
qing@cs.cmu.edu
Francisco Guzman
Centro de Sistemas Inteligentes
Tecnolo?gico de Monterrey
guzmanhe@gmail.com
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
stephan.vogel@cs.cmu.edu
Abstract
This paper proposes a novel semi-
supervised word alignment technique
called EMDC that integrates discrimina-
tive and generative methods. A discrim-
inative aligner is used to find high preci-
sion partial alignments that serve as con-
straints for a generative aligner which
implements a constrained version of the
EM algorithm. Experiments on small-size
Chinese and Arabic tasks show consistent
improvements on AER. We also experi-
mented with moderate-size Chinese ma-
chine translation tasks and got an aver-
age of 0.5 point improvement on BLEU
scores across five standard NIST test sets
and four other test sets.
1 Introduction
Word alignment is a crucial component in sta-
tistical machine translation (SMT). From a Ma-
chine Learning perspective, the models for word
alignment can be roughly categorized as gener-
ative models and discriminative models. The
widely used word alignment tool, i.e. GIZA++
(Och and Ney, 2003), implements the well-known
IBM models (Brown et al, 1993) and the HMM
model (Vogel et al, 1996), which are genera-
tive models. For language pairs such as Chinese-
English, the word alignment quality is often un-
satisfactory. There has been increasing interest on
using manual alignments in word alignment tasks,
which has resulted in several discriminative mod-
els. Ittycheriah and Roukos (2005) proposed to
use only manual alignment links in a maximum
entropy model, which is considered supervised.
Also, a number of semi-supervised word align-
ers have been proposed (Taskar et al, 2005; Liu
et al, 2005; Moore, 2005; Blunsom and Cohn,
2006; Niehues and Vogel, 2008). These methods
use held-out manual alignments to tune weights
for discriminative models, while using the model
parameters, model scores or alignment links from
unsupervised word aligners as features. Callison-
Burch et. al. (2004) proposed a method to interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Also, there are recent
attempts to combine multiple alignment sources
using alignment confidence measures so as to im-
prove the alignment quality (Huang, 2009).
In this paper, the question we address is
whether we can jointly improve discriminative
models and generative models by feeding the in-
formation we get from the discriminative aligner
back into the generative aligner. Examples of
this line of research include Model 6 (Och and
Ney, 2003) and the EMD training approach pro-
posed by Fraser and Marcu (2006) and its ex-
tension called LEAF aligner (Fraser and Marcu,
2007). These approaches use labeled data to tune
additional parameters to weight different compo-
nents of the IBM models such as the lexical trans-
lation model, the distortion model and the fertility
model. These methods are proven to be effective
in improving the quality of alignments. However,
the discriminative training in these methods is re-
stricted in using the model components of gener-
ative models, in other words, incorporating new
features is difficult.
Instead of using discriminative training meth-
ods to tune the weights of generative models,
in this paper we propose to use a discrimina-
tive word aligner to produce reliable constraints
for the EM algorithm. We call this new train-
ing scheme EMDC (Expectation-Maximization-
Discrimination-Constraint). The methodology
can be viewed as a variation of bootstrapping. It
enables the generative models to interact with dis-
criminative models at the data level instead of the
model level. Furthermore, with a discriminative
349
word aligner that uses generative word aligner?s
output as features, we create a feedback loop that
can iteratively improve the quality of both align-
ers. The major contributions of this paper are: 1)
The EMDC training scheme, which ties the gen-
erative and discriminative aligners together and
enables future research on integrating other dis-
criminative aligners. 2) An extended generative
aligner based on GIZA++ that allows to perform
constrained EM training.
In Section 2, we present the EMDC training
scheme. Section 3 provides details of the con-
strained EM algorithm. In Section 4, we intro-
duce the discriminative aligner and link filtering.
Section 5 provides the experiment set-up and the
results. Section 6 concludes the paper.
2 EMDC Training Scheme
The EMDC training scheme consists of
three parts, namely EM, Discrimination, and
Constraints. As illustrated in Figure 1, a large
unlabeled training set is first aligned with a gen-
erative aligner (GIZA++ for the purpose of this
paper). The generative aligner outputs the model
parameters and the Viterbi alignments for both
source-to-target and target-to-source directions.
Afterwards, a discriminative aligner (we use the
one described in (Niehues and Vogel, 2008)),
takes the lexical translation model, fertility model
and Viterbi alignments from both directions as
features, and is tuned to optimize the AER on a
small manually aligned tuning set. Afterwards,
the alignment links generated by the discrimina-
tive aligner are filtered according to their likeli-
hood, resulting in a subset of links that has high
precision and low recall. The next step is to put
these high precision alignment links back into the
generative aligner as constraints. A conventional
generative word aligner does not support this type
of constraints. Thus we developed a constrained
EM algorithm that can use the links from a partial
alignment as constraints and estimate the model
parameters by marginalizing likelihoods.
After the constrained EM training is performed,
we repeat the procedure and put the updated gen-
erative models and Viterbi alignment back into the
discriminative aligner. We can either fix the num-
ber of iterations, or stop the procedure when the
gain on AER of a small held-out test set drops be-
Figure 1: Illustration of EMDC training scheme
low a threshold.
The key components for the system are:
1. A generative aligner that can make use of re-
liable alignment links as constraints and im-
prove the models/alignments.
2. A discriminative aligner that outputs con-
fidence scores for alignment links, which
allows to obtain high-precision-low-recall
alignments.
While in this paper we derive the reliable links
by filtering the alignment generated by a discrimi-
native aligner, such partial alignments may be ob-
tained from other sources as well: manual align-
ments, specific named entity aligner, noun-phrase
aligner, etc.
As we mentioned in Section 1, the discrimina-
tive aligner is not restricted to use features param-
eters of generative models and Viterbi alignments.
However, including the features from generative
models is required for iterative training, because
the improvement on the quality of these features
can in turn improve the discriminative aligner. In
our experiments, the discriminative aligner makes
heavy use of the Viterbi alignment and the model
parameters from the generative aligner. Nonethe-
less, one can easily replace the discriminative
aligner or add new features to it without modify-
ing the training scheme. The open-ended prop-
erty of the training scheme makes it a promising
method to integrate different aligners.
In the next two sections, we will describe the
key components of this framework in detail.
3 Constrained EM algorithm
In this section we will briefly introduce the con-
strained EM algorithm we used in the experiment,
350
further details of the algorithm can be found in
(Gao et al, 2010).
The IBM Models (Brown et al, 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003), the most widely
used implementation of IBM models and HMM
(Vogel et al, 1996), employs EM algorithm to es-
timate the model parameters. For simpler models
such as Model 1 and Model 2, it is possible to
obtain sufficient statistics from all possible align-
ments in the E-step. However, for fertility-based
models such as Models 3, 4, and 5, enumerating
all possible alignments is NP-complete. To over-
come this limitation, GIZA++ adopts a greedy
hill-climbing algorithm, which uses simpler mod-
els such as HMM or Model 2 to generate a ?center
alignment? and then tries to find better alignments
among its neighbors. The neighbors of an align-
ment aJ1 = [a1, a2, ? ? ? , aJ ] with aj ? [0, I] are
defined as alignments that can be generated from
aJ1 by one of the following two operators:
1. The move operator m[i,j], that changes aj :=
i, i.e. arbitrarily sets word fj in the target
sentence to align to the word ei in source sen-
tence;
2. The swap operator s[j1,j2] that exchanges aj1
and aj2 .
The algorithm will update the center alignment
as long as a better alignment can be found, and
finally outputs a local optimal alignment. The
neighbor alignments of the final center alignment
are then used in collecting the counts for the M-
Step. Och and Ney (2003) proposed a fast imple-
mentation of the hill-climbing algorithm that em-
ploys two matrices, i.e. Moving MatrixMI?J and
Swapping Matrix SJ?J . Each cell of the matrices
stores the value of likelihood difference after ap-
plying the corresponding operator.
We define a partial alignment constraint of a
sentence pair (fJ1 , eI1) as a set of links: ?JI =
{(i, j)|0 ? i < I, 0 ? j < J}. Given a set of
constraints, an alignment aJ1 = [a1, a2, ? ? ? , aj ]
on the sentence pair fJ1 , eI1, the translation proba-
bility of Pr(fJ1 |eI1) will be zero if the alignment
is inconsistent with the constraints. Constraints
(0, j) or (i, 0) are used to explicitly represent that
word fj or ei is aligned to the empty word.
Under the assumptions of the IBM models,
there are two situations that aJ1 is inconsistent with
?JI :
1. Target word misalignment: The IBM mod-
els assume that one target word can only be
aligned to one source word. Therefore, if the
target word fj aligns to a source word ei,
while the constraint ?JI suggests fj should be
aligned to ei? , the alignment violates the con-
straint and thus is considered inconsistent.
2. Source word to empty word misalignment: if
a source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.
However, the partial alignments, which allow
n-to-n alignments, may already violate the 1-to-n
alignment restriction of the IBM models. In these
cases, we relax the condition in situation 1 that if
the alignment link aj? is consistent with any one
of the conflicting target-to-source constraints, it
will be considered consistent. Also, we arbitrarily
assign the source word to empty word constraints
higher priorities than other constraints, because
unlike situation 1, it does not have the problem
of conflicting with other constraints.
3.1 Constrained hill-climbing algorithm
To ensure that resulting center alignment be
consistent with the constraints, we need to split
the hill-climbing algorithm into two stages: 1) op-
timize towards the constraints and 2) optimize to-
wards the optimal alignment under the constraints.
From a seed alignment, we first move the align-
ment towards the constraints by choosing a move
or swap operator that:
1. produces the alignment that has the highest
likelihood among alignments generated by
other operators,
2. eliminates at least one inconsistent link.
We iteratively update the alignment until no
other inconsistent link can be removed. The algo-
rithm implies that we force the seed alignment to
be closer to the constraints while trying to find the
best consistent alignment. Figure 2 demonstrates
the idea, given the constraints shown in (a), and
the seed alignment shown as solid links in (b), we
351
2005?
?
??
the
summer
of
2005Manual Alignment Link
(a)
2005?
?
??
the
summer
Of
2005Seed Alignment Consistent Alignment Center Alignment
(b)                    (c)
2005?
?
??
the
summer
of
2005
Figure 2: Illustration of Algorithm 1
move the inconsistent link to the dashed link by a
move operation.
After we find the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
under the constraints. The algorithm sets the value
of the cells in moving/swapping matrices to nega-
tive if the corresponding operators will lead to an
inconsistent alignment. The moving matrix needs
to be processed only once, whereas the swapping
matrix needs to be updated every iteration, since
once the alignment is updated, the possible viola-
tions will also change.
If a source word ei is aligned to the empty word,
we set Mi,j = ?1,?j. The swapping matrix does
not need to be modified in this case because the
swapping operator will not introduce new links.
Because the cells that can lead to violations are
set to negative, the operators will never be picked
when updating the center alignments. This en-
sures the consistency of the final center alignment.
3.2 Count Collection
After finding the center alignment, we need to
collect counts from neighbor alignments so that
the M-step can normalize the counts to produce
the model parameters for the next step. In this
stage, we want to make sure all the inconsistent
alignments in the neighbor set of the center align-
ment be ruled out from the sufficient statistics, i.e.
have zero probability. Similar to the constrained
hill climbing algorithm, we can manipulate the
moving/swapping matrices to effectively exclude
inconsistent alignments. Since the original count
collection algorithm depends only on moving and
swapping matrices, we just need to bypass all the
cells which hold negative values, i.e. represent in-
consistent alignments.
We can also view the algorithm as forcing
the posteriors of inconsistent alignments to zero,
and therefore increase the posteriors of consistent
alignments. When no constraint is given, the algo-
rithm falls back to conventional EM, and when all
the alignments are known, the algorithm becomes
fully supervised. And if the alignment quality
can be improved if high-precision partial align-
ment links is given as constraints. In (Gao et al,
2010) we experimented with using a dictionary to
generate such constraints, and in (Gao and Vogel,
2010) we experimented with manual word align-
ments from Mechanical Turk. And in this paper
we try to use an alternative method that uses a dis-
criminative aligner and link filtering to generate
such constraints.
4 Discriminative Aligner and Link
Filtering
We employ the CRF-based discriminative word
aligner described in (Niehues and Vogel, 2008).
The aligner can use a variety of knowledge
sources as features, such as: the fertility and lex-
ical translation model parameters from GIZA++,
the Viterbi alignment from both source-to-target
and target-to-source directions. It can also make
use of first-order features which model the depen-
dency between different links, the Parts-of-Speech
tagging features, the word form similarity feature
and the phrase features. In this paper we use all
the features mentioned above except the POS and
phrase features.
The aligner is trained using a belief-
propagation (BP) algorithm, and can be optimized
to maximize likelihood or directly optimize to-
wards AER on a tuning set. The aligner outputs
confidence scores for alignment links, which
allows us to control the precision and recall
rate of the resulting alignment. Guzman et al
(2009) experimented with different alignments
produced by adjusting the filtering threshold for
the alignment links and showed that they could
get high-precision-low-recall alignments by hav-
ing a higher threshold. Therefore, we replicated
the confidence filtering procedures to produce
the partial alignment constraints. Afterwards
we iterate by putting the partial alignments back
to the constrained word alignment algorithm
described in section 3.
Although the discriminative aligner performs
well in supplying high precision constraints, it
does not model the null alignment explicitly.
352
Num. of
Sentences
Num. of Words Num. of
LinksSource Target
Ch-En 21,863 424,683 524,882 687,247
Ar-En 29,876 630,101 821,938 830,349
Table 1: Corpus statistics of the manual aligned
corpora
Threshold P R AER
Ch-En
0.6 71.30 58.12 35.96
0.7 75.24 54.03 37.11
0.8 85.66 44.19 41.70
0.9 93.70 37.95 45.98
Ar-En
0.6 72.35 59.87 34.48
0.7 77.55 55.58 35.25
0.8 80.07 50.89 37.77
0.9 83.74 44.16 42.17
Table 2: The qualities of the constraints
Hence we are currently not able to provide source
word to empty word alignment constraints which
have been proven to be effective in improving the
alignment quality in (Gao et al, 2010). Due to
space limitation, please refer to: (Niehues and Vo-
gel, 2008; Guzman et al, 2009) for further details
of the aligner and link filtering, respectively.
5 Experiments
To validate the proposed training scheme, we
performed two sets of experiments. First of all,
we experimented with a small manually aligned
corpus to evaluate the ability of the algorithm to
improve the AER. The experiment was performed
on Chinese to English and Arabic to English tasks.
Secondly, we experimented with a moderate size
corpus and performed translation tasks to observe
the effects in translation quality.
5.1 Effects on AER
In order to measure the effects of EMDC in
alignment quality, we experimented with Chinese-
English and Arabic-English manually aligned cor-
pora. The statistics of these sets are shown in Ta-
ble 1. We split the data into two fragments, the
first 100 sentences (Set A) and the remaining (Set
B). We trained generative IBM models using the
Set B, and tuned the discriminative aligner using
the Set A. We evaluated the AER on Set B, but in
any of the training steps the manual alignments of
Set B were not used.
In each iteration of EDMC, we load the model
parameters from the previous step and continue
training using the new constraints. Therefore, it is
important to compare the performance of contin-
uous training against an unconstrained baseline,
because variation in alignment quality could be
attributed to either the effect of more training it-
erations or to the effect of semi-supervised train-
ing scheme. In Figures 3 and 4 we show the
alignment quality for each iteration. Iteration 0 is
the baseline, which comes from standard GIZA++
training1. The grey dash curves represent uncon-
strained Model 4 training, and the curves with
start, circle, cross and diamond markers are con-
strained EM alignments with 0.6, 0.7, 0.8 and
0.9 filtering thresholds respectively. As we can
see from the results, when comparing only the
mono-directional trainings, the alignment quali-
ties improve over the unconstrained training in all
the metrics (precision, recall and AER). From Ta-
ble 2, we observe that the quality of discrimina-
tive aligner also improved. Nonetheless, when
we consider the heuristically symmetrized align-
ment2, we observe mixed results. For instance,
for the Chinese-English case we observe that AER
improves over iterations, but this is the result of
a increasingly higher recall rate in detriment of
precision. Ayan and Dorr (2006) pointed out
that grow-diag-final symmetrization tends to out-
put alignments with high recall and low precision.
However this does not fully explain the tendency
we observed between iterations. The character-
istics of the alignment modified by EDMC that
lead to larger improvements in mono-directional
trainings but a precision drop with symmetrization
heuristics needs to be addressed in future work.
Another observation is how the filtering thresh-
olds affect the results. As we can see in Table 3,
for Chinese to English word alignment, the largest
gain on the alignment quality is observed when
the threshold was set to 0.8, while for Arabic to
English, the threshold of 0.7 or 0.6 works better.
Table 2 shows the precision, recall, and AER of
the constraint links used in the constrained EM al-
1We run 5, 5, 3, 3 iterations of Model 1, HMM, Model 3
and Model 4 respectively.
2We used grow-diag-final-and
353
0 2 4 6 860
6264
66
%
Precision
 
 
0 2 4 6 85052
5456
5860
Recall
0 2 4 6 838
4042
4446
AER
UnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9
(a) Arabic-English
0 2 4 6 859
6061
62
%
Precision
 
 
0 2 4 6 864
6668
7072
Recall
0 2 4 6 83334
3536
3738
39 AER
(b) English-Arabic
0 2 4 6 860.5
6161.5
6262.5
63
%
Precision
 
 
0 2 4 6 866
68
70
72 Recall
0 2 4 6 83233
3435
3637
AER
(c) Heuristically-symmetrized
Figure 3: Alignment qualities of each iteration for Arabic-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.
Source-Target Target-Source Heuristic Discriminative
P R AER P R AER P R AER P R AER
Ch
BL 68.22 46.88 44.43 65.35 55.05 40.25 69.15 57.47 37.23 67.45 59.77 36.62
NC +0.73 +0.71 -0.74 +1.14 +1.14 -1.15 +0.06 +1.07 -0.66 +0.15 +0.64 -0.42
0.6 +2.17 +2.28 -2.32 +1.17 +2.51 -1.97 -0.64 +2.65 -1.27 -0.39 +1.89 -0.87
0.7 +2.57 +2.32 -2.48 +1.94 +2.34 -2.19 -0.34 +2.30 -1.20 -0.28 +1.60 -0.76
0.8 +3.78 +3.27 -3.55 +2.94 +3.32 -3.18 -0.52 +3.32 -1.70 +0.69 +0.14 -0.89
0.9 +0.98 +1.13 -1.11 +1.48 +1.85 -1.71 -0.55 +1.94 -0.90 -0.58 +1.45 -0.54
Ar
BL 58.41 50.42 45.88 59.08 64.84 38.17 60.35 66.99 36.50 68.93 63.94 33.66
NC +2.98 +2.92 -2.96 +1.40 +2.06 -1.70 +0.97 +2.14 -1.49 -0.87 +2.37 -0.83
0.6 +6.69 +8.02 -7.47 +3.45 +6.70 -4.90 +2.62 +4.71 -3.55 +0.58 -0.55 +0.03
0.7 +8.38 +7.93 -8.16 +3.65 +5.26 -4.38 +2.83 +4.70 -3.67 +2.46 -0.42 -0.88
0.8 +6.48 +6.27 -6.39 +2.18 +3.54 -2.80 +1.81 +3.81 -2.70 +1.67 +2.30 -2.01
0.9 +4.02 +4.07 -4.07 +1.70 +3.10 -2.33 +0.62 +3.82 -2.03 +1.33 +2.70 -2.06
Table 3: Improvement on word alignment quality on small corpus after 8 iterations. BL stands for
baseline, and NC represents unconstrained Model 4 training, and 0.9, 0.8, 0.7, 0.6 are the thresholds
used in alignment link filtering.
gorithm, the numbers are averaged across all iter-
ations, the actual numbers of each iteration only
have small differences. Although one might ex-
pect that the quality of resulting alignment from
constrained EM be proportional to the quality of
constraints, from the numbers in Table 2 and 3,
we are not able to induce a clear relationship be-
tween them, and it could be language- or corpus-
dependent. However, in practice we nonetheless
use a held-out test set to tune this parameter. The
354
0 2 4 6 869
7071
72
%
Precision
 
 
0 2 4 6 84647
4849
5051
Recall
0 2 4 6 84041
4243
4445
AER
UnconstrainedFiltered 0.6Filtered 0.7Filtered 0.8Filtered 0.9
(a) Chinese-English
0 2 4 6 865.5
6666.5
6767.5
68
%
Precision
 
 
0 2 4 6 855
5657
5859
Recall
0 2 4 6 837
3839
4041
AER
(b) English-Chinese
0 2 4 6 868.6
68.869
69.2
%
Precision
 
 
0 2 4 6 857
5859
6061
Recall
0 2 4 6 835
36
37
38 AER
(c) Heuristically-symmetrized
Figure 4: Alignment qualities of each iteration for Chinese-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 filtering thresholds respectively.
Ch-En En-Ch Heuristic Discriminative
P R AER P R AER P R AER P R AER
BL 73.51 50.14 40.38 68.82 57.66 37.31 72.98 60.23 34.01 72.10 61.63 33.55
NC 73.23 50.38 40.30 68.30 58.00 37.27 72.39 60.99 33.80 72.07 61.81 33.45
0.8 76.27 52.90 37.53 70.26 60.26 35.11 72.75 63.49 32.19 72.64 63.29 32.35
Table 4: Improvement on word alignment quality on moderate-size corpus, where BL and NC represents
baseline and non-constrained Model 4 training
relationship between quality of constraints and
alignment results is an interesting topic for future
research.
5.2 Effects on translation quality
In this experiment we run the whole machine
translation pipeline and evaluate the system on
BLEU score. We used the corpus LDC2006G05
which contains 25 million words as training set,
the same discriminative tuning set as previously
used (100 sentence pairs) and the remaining
21,763 sentence pairs from the hand-aligned cor-
pus of the previous experiment are held-out test
set for alignment qualities. A 4-gram language
model trained from English GigaWord V1 and V2
corpus was used. The AER scores on the held-
out test set are also provided for every iteration.
Based on the observation in last experiment, we
adopt the filtering threshold of 0.8.
Similar to previous experiment, the heuristi-
cally symmetrized alignments have lower preci-
sions than their EMDC counterparts, however the
gaps are smaller as shown in Table 4. We observe
2.85 and 2.21 absolute AER reduction on two di-
rections, after symmetrization the gain on AER
is 1.82. Continuing Model 4 training appears to
have minimal effect on AER, and the improve-
355
I M NIST GALE
mt06 mt02 mt03 mt04 mt05 mt08 ain db-nw db-wb dd-nw dd-wb aia
0 G 31.00 31.80 29.89 32.63 29.33 24.24 26.92 24.48 28.44 24.26
1 D 30.65 31.60 30.04 32.89 29.34 24.52 0.12 27.43 24.72 28.32 24.30 0.14G 31.35 31.91 30.35 32.75 29.40 24.16 0.15 27.39 24.50 28.22 24.60 0.15
2 D 31.61 32.31 30.40 33.06 29.49 24.11 0.33 28.17 24.42 28.58 24.36 0.34G 31.14 31.94 30.42 32.86 29.49 24.15 0.20 27.31 24.51 27.50 24.02 0.03
3 D 31.29 32.39 30.28 33.19 29.60 24.41 0.43 27.64 25.32 28.55 24.71 0.47G 30.94 31.95 30.15 32.71 29.38 24.22 0.12 27.63 24.61 28.80 25.05 0.29
4 D 30.80 32.04 30.51 33.24 29.49 24.61 0.46 27.61 25.27 28.72 24.98 0.53G 30.68 31.81 30.33 33.05 29.28 24.41 0.26 27.20 24.79 28.43 24.50 0.24
5 D 30.93 31.89 29.96 32.89 29.37 24.50 0.17 27.75 24.50 29.05 24.90 0.33G 31.16 32.28 30.72 33.30 29.83 24.30 0.51 27.32 25.05 28.60 25.44 0.54
Table 5: Improvement on translation alignment quality on moderate-size corpus, The column ain shows
the average improvement of BLEU scores for all NIST test sets (excluding the tuning set MT06), and
column aia is the average improvement on all unseen test sets. The column M indicates the alignment
source, G means the alignment comes from generative aligner, and D means discriminative aligner
respectively. The number of iterations is shown in column I.
ment mainly comes from the constraints.
In the experiment, we use the Moses toolkit to
extract phrases, tune parameters and decode. We
use the NIST MT06 test set as the tuning set,
NIST MT02-05 and MT08 as unseen test sets.
We also include results for four additional unseen
test sets used in GALE evaluations: DEV07-Dev
newswire part (dd-nw, 278 sentences) and We-
blog part (dd-wb, 345 sentences), Dev07-Blind
newswire part (db-nw, 276 sentences and Weblog
part (db-wb, 312 sentences). Table 5 presents the
average improvement on BLEU scores in each it-
eration. As we can see from the results, in all iter-
ations we got improvement on BLEU scores, and
the largest gain we have gotten is on the fifth it-
eration, which has 0.51 average improvement on
five NIST test sets, and 0.54 average improvement
across all nine test sets.
6 Conclusion
In this paper we presented a novel training
scheme for word alignment task called EMDC.
We also presented an extension of GIZA++ that
can perform constrained EM training. By inte-
grating it with a CRF-based discriminative word
aligner and alignment link filtering, we can im-
prove the alignment quality of both aligners itera-
tively. We experimented with small-size Chinese-
English and Arabic English and moderate-size
Chinese-English word alignment tasks, and ob-
served in all four mono-directional alignments
more than 3% absolute reduction on AER, with
the largest improvement being 8.16% absolute on
Arabic-to-English comparing to the baseline, and
5.90% comparing to Model 4 training with the
same numbers of iterations. On a moderate-size
Chinese-to-English tasks we also evaluated the
impact of the improved alignment on translation
quality across nine test sets. The 2% absolute
AER reduction resulted in 0.5 average improve-
ment on BLEU score.
Observations on the results raise several inter-
esting questions for future research, such as 1)
What is the relationship between the precision of
the constraints and the quality of resulting align-
ments after iterations, 2) The effect of using dif-
ferent discriminative aligners, 3) Using aligners
that explicitly model empty words and null align-
ments to provide additional constraints. We will
continue exploration on these directions.
The extended GIZA++ is released to the re-
search community as a branch of MGIZA++ (Gao
and Vogel, 2008), which is available online3.
Acknowledgement
This work is supported by NSF CluE Project
(NSF 08-560) and DARPA GALE project.
3Accessible on Source Forge, with the URL:
http://sourceforge.net/projects/mgizapp/
356
References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 9?16.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65?72.
Brown, Peter F., Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. In Computational Linguistics,
volume 19(2), pages 263?331.
Callison-Burch, C., D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-
and sentence-aligned parallel corpora. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 175?183.
Fraser, Alexander and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 769?776.
Fraser, Alexander and Daniel Marcu. 2007. Get-
ting the structure right for word alignment: LEAF.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 51?60.
Gao, Qin and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the ACL 2008 Software Engineering, Testing, and
Quality Assurance Workshop, pages 49?57.
Gao, Qin and Stephan Vogel. 2010. Consensus ver-
sus expertise : A case study of word alignment with
mechanical turk. In NAACL 2010 Workshop on Cre-
ating Speech and Language Data With Mechanical
Turk, pages 30?34.
Gao, Qin, Nguyen Bach, and Stephan Vogel. 2010.
A semi-supervised word alignment algorithm with
partial manual alignments. In In Proceedings of
the ACL 2010 joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR (ACL-2010
WMT).
Guzman, Francisco, Qin Gao, and Stephan Vogel.
2009. Reassessment of the role of phrase extrac-
tion in pbsmt. In The twelfth Machine Translation
Summit.
Huang, Fei. 2009. Confidence measure for word
alignment. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 932?940.
Ittycheriah, Abraham and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 89?96.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 459?466.
Moore, Robert C. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 81?88.
Niehues, Jan. and Stephan. Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25.
Och, Franz Joseph and Hermann Ney. 2003. A
systematic comparison of various statistical align-
ment models. In Computational Linguistics, vol-
ume 1:29, pages 19?51.
Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 73?80.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM based word alignment in statis-
tical machine translation. In Proceedings of 16th In-
ternational Conference on Computational Linguis-
tics), pages 836?841.
357
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Minimum Translation Modeling with Recurrent Neural Networks
Yuening Hu
Department of Computer Science
University of Maryland, College Park
ynhu@cs.umd.edu
Michael Auli, Qin Gao, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,qigao,jfgao}@microsoft.com
Abstract
We introduce recurrent neural network-
based Minimum Translation Unit (MTU)
models which make predictions based on
an unbounded history of previous bilin-
gual contexts. Traditional back-off n-gram
models suffer under the sparse nature of
MTUs which makes estimation of high-
order sequence models challenging. We
tackle the sparsity problem by modeling
MTUs both as bags-of-words and as a
sequence of individual source and target
words. Our best results improve the out-
put of a phrase-based statistical machine
translation system trained on WMT 2012
French-English data by up to 1.5 BLEU,
and we outperform the traditional n-gram
based MTU approach by up to 0.8 BLEU.
1 Introduction
Classical phrase-based translation models rely
heavily on the language model and the re-
ordering model to capture dependencies between
phrases. Sequence models over Minimum Trans-
lation Units (MTUs) have been shown to com-
plement both syntax-based (Quirk and Menezes,
2006) as well as phrase-based (Zhang et al., 2013)
models by explicitly modeling relationships be-
tween phrases. MTU models have been tradi-
tionally estimated using standard back-off n-gram
techniques (Quirk and Menezes, 2006; Crego and
Yvon, 2010; Zhang et al., 2013), similar to word-
based language models (?2).
However, the estimation of higher-order n-gram
models becomes increasingly difficult due to data
sparsity issues associated with large n-grams, even
when training on over one hundred billion words
(Heafield et al., 2013); bilingual units are much
sparser than words and are therefore even harder
to estimate. Another drawback of n-gram mod-
els is that future predictions are based on a limited
amount of previous context that is often not suf-
ficient to capture important aspects of human lan-
guage (Rastrow et al., 2012).
Recently, several feed-forward neural network-
based models have achieved impressive improve-
ments over traditional back-off n-gram models in
language modeling (Bengio et al., 2003; Schwenk
et al., 2007; Schwenk et al., 2012; Vaswani et al.,
2013), as well as translation modeling (Allauzen et
al., 2011; Le et al., 2012; Gao et al., 2013). These
models tackle the data sparsity problem by rep-
resenting words in continuous space rather than
as discrete units. Similar words are grouped in
the same sub-space rather than being treated as
separate entities. Neural network models can be
seen as functions over continuous representations
exploiting the similarity between words, thereby
making the estimation of probabilities over higher-
order n-grams easier.
However, feed-forward networks do not directly
address the limited context issue either, since pre-
dictions are based on a fixed-size context, similar
to back-off n-gram models. We therefore focus
in this paper on recurrent neural network architec-
tures, which address the limited context issue by
basing predictions on an unbounded history of pre-
vious events which allows to capture long-span de-
pendencies. Recurrent architectures have recently
advanced the state of the art in language model-
ing (Mikolov et al., 2010; Mikolov et al., 2011a;
Mikolov, 2012) outperforming multi-layer feed-
forward based networks in perplexity and word er-
ror rate for speech recognition (Arisoy et al., 2012;
Sundermeyer et al., 2013). Recent work has also
shown successful applications to machine transla-
tion (Mikolov, 2012; Auli et al., 2013; Kalchbren-
ner and Blunsom, 2013). We extend this work by
modeling Minimum Translation Units with recur-
rent neural networks.
Specifically, we introduce two recurrent neu-
ral network-based MTU models to address the is-
20
M1 M2 M3 M4 M5
Yu        ZuoTian JuXing Le HuiTan
held
=> null
=> Yesterday
=> held
=> the
=> meeting
? ?? ?? ? ??
Yu
ZuoTian
JuXing_Le
null
HuiTan
null        
the meeting null yesterday
M1: 
M2: 
M3: 
M4: 
M5: 
Figure 1: Example Minimum Translation Unit
partitioning based on Zhang et al. (2013).
sues regarding data sparsity and limited context
sizes by leveraging continuous representations and
the unbounded history of the recurrent architec-
ture. Our first approach frames the problem as a
sequence modeling task over minimal units (?3).
The second model improves over the first by mod-
eling an MTU as a bag-of-words, thereby allow-
ing us to learn representations over sub-structures
of minimal units that are shared across MTUs
(?4). Our models significantly outperform the tra-
ditional back-off n-gram based approach and we
show that they act complementary to a very strong
recurrent neural network-based language model
based solely on target words (?5).
2 Minimum Translation Units
Banchs et al. (2005) introduced the idea of framing
translation as a sequence modeling problem where
a sentence pair is generated in left-to-right order as
a sequence of bilingual n-grams. Minimum Trans-
lation Units (Quirk and Menezes, 2006; Zhang
et al., 2013) are an extension which additionally
permit tuples with empty source or target sides,
thereby allowing insertion or deletion phrase pairs.
The two basic requirements for MTUs are that
there are no overlapping word alignment links be-
tween phrase pairs and it should not be possible to
extract smaller phrase pairs without violating the
word alignment constraints. Informally, we can
think of MTUs as small phrase pairs that cannot
be broken down any further without violating the
two requirements.
Minimum Translation Units partition a sentence
pair into a set of minimal bilingual units or tu-
Words MTUs
Tokens 34,769,416 14,853,062
Types 143,524 1,315,512
Singleton types 34.9% 80.1%
Table 1: Token and type counts for both source
and target words as well as MTUs based on the
WMT 2006 German to English data set (cf. ?5).
ples obtained by an algorithm similar to phrase-
extraction (Koehn et al., 2003). Figure 1 illus-
trates such a partitioning. Modeling minimal units
has two advantages over considering larger phrase
pairs that are effectively composed of MTUs:
First, minimal units result in a unique partition-
ing of a sentence pair. This has the advantage that
we avoid modeling spurious derivations, that is,
multiple derivations generating the same sentence
pair. Second, minimal units result in smaller mod-
els with a smoother distribution than models based
on composed units (Zhang et al., 2013).
Sentence pairs can be generated in multiple or-
ders, such as left-to-right or right-to-left, either in
source or target order. For example, the source
left-to-right order of the sentence pair in Figure 1
is simply M1, M2, M3, M4, M5, while the tar-
get left-to-right order is M3, M4, M5, M1, M2.
We deal with inserted or deleted words similar to
Zhang et al. (2013): The source side null token of
an inserted target phrase is placed next to the last
source word aligned to the closest preceding non-
null aligned target phrase; a similar rule is applied
to null tokens on the target side. For example, in
Figure 1 we place M4 straight after M3 because
?the?, the aligned target phrase, is after ?held?, the
previous non-null aligned target phrase.
We can straightforwardly estimate an n-gram
model over MTUs to estimate the probability
of a sentence pair using standard back-off tech-
niques commonly employed in language mod-
eling. For example, a trigram model in tar-
get left-to-right order factors the sentence pair in
Figure 1 as p(M3) p(M4|M3) p(M5|M3,M4)
p(M1|M4,M5)p(M2|M5,M1).
If we would like to model larger contexts, then
we quickly run into data sparsity issues. To illus-
trate this point, consider the parameter growth of
an n-gram model which is driven by the vocabu-
lary size |V | and the n-gram order n: O(|V |
n
).
Clearly, the exact estimation of higher-order n-
21
gram probabilities becomes more difficult with
large n, leading to the estimation of events with
increasingly sparse statistics, or having to rely
on statistics from lower-order events with back-
off models, which is less desirable. Even word-
based language models rarely ventured so far
much beyond 5-gram statistics as demonstrated
by Heafield et al. (2013) who trained a, by to-
day?s standards, very large 5-gram model on 130B
words. Data sparsity is therefore an even more sig-
nificant issue for MTU models relying on much
larger vocabularies. In our setting, the MTU vo-
cabulary is an order of magnitude larger than a
word vocabulary obtained from the same data (Ta-
ble 1). Furthermore, most MTUs are observed
only once making the reliable estimation of prob-
abilities very challenging.
Neural network-based sequence models tackle
the data sparsity problem by learning continuous
word representations, that group similar words to-
gether in continuous space. For example, the
distributional representations induced by recurrent
neural networks have been found to have interest-
ing syntactic and semantic regularities (Mikolov
et al., 2013). Furthermore, these representations
can be exploited to estimate more reliable statis-
tics over higher-order n-grams than with discrete
word units. Recurrent neural networks go beyond
fixed-size contexts and allow the model to keep
track of long-span dependencies that are important
for future predictions. In the next sections we will
present Minimum Translation Unit models based
on recurrent architectures.
3 Atomic MTU RNN Model
The first model we introduce is based on the recur-
rent neural network language model of Mikolov
et al. (2010). We frame the problem as a tradi-
tional sequence modeling task which treats MTUs
as atomic units, similar to the approach taken by
the traditional back-off n-gram models.
The model is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 2). The input layer encodes the
MTU at time t as a 1-of-N vector m
t
with all val-
ues being zero except for the entry representing
the MTU. The output layer y
t
represents a proba-
bility distribution over possible next MTUs; both
the input and output layers are of size |V |, the size
of the MTU vocabulary. The hidden layer state h
t
encodes the history of all MTUs observed in the
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
Figure 2: Structure of the atomic recurrent neu-
ral network MTU model following the word-based
RNN model of Mikolov (2012).
sequence up to time step t.
The state of the hidden layer is determined by
the input layer and the hidden layer configuration
of the previous time step h
t?1
. The weights of the
connections between the layers are summarized in
a number of matrices: U represents weights from
the input layer to the hidden layer, and W repre-
sents connections from the previous hidden layer
to the current hidden layer. Matrix V contains
weights between the current hidden layer and the
output layer.
The hidden and output layers are computed
via a series of matrix-vector products and non-
linearities:
h
t
= s(Um
t
+Wh
t?1
)
y
t
= g(Vh
t
)
where
s(z) =
1
1 + exp {?z}
, g(z
m
) =
exp {z
m
}
?
k
exp {z
k
}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram fea-
tures over input MTUs (Mikolov et al., 2011a).
The maximum entropy weights D are added to
the output activations before applying the softmax
function and are estimated jointly with all other
parameters (Figure 3).
1
1
While these features depend on multiple input MTUs, we
22
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
T
D
c t
Figure 3: Structure of atomic recurrent neural net-
work MTU model with classing layer c
t
and direct
connections D between the input and output lay-
ers (cf. Figure 2).
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the truncated back
propagation through time algorithm, which unrolls
the network and then computes error gradients
over multiple time steps (Rumelhart et al., 1986);
we use a cross entropy criterion to obtain the error
vector with respect to the output activations and
the desired prediction. After training, the output
layer represents posteriors p(m
t+1
|m
t
t?n+1
,h
t
),
the probability of the next MTU given the previ-
ous n input MTUs m
t
t?n+1
= m
t
, . . . ,m
t?n+1
and the current hidden layer configuration h
t
.
Na??ve computation of the probability distribu-
tion over the next MTU is very expensive for large
vocabularies, such as commonly encountered for
MTU models (Table 1). A well established ef-
ficiency trick assigns each possible output to a
unique class and then uses a two-step process to
find the probability of an MTU, instead of comput-
ing the probability of all possible outputs (Good-
man, 2001; Emami and Jelinek, 2005; Mikolov et
al., 2011b). Under this scheme we compute the
probability of an MTU by multiplying the prob-
ability of its class c
i
t
with the probability of the
depicted them for simplicity as a connection between the
current input vectorm
t
and the output layer.
minimal unit conditioned on the class:
p(m
t+1
|m
t
t?n+1
,h
t
) =
p(c
i
t
|m
t
t?n+1
,h
t
) p(m
t+1
|c
i
t
,m
t
t?n+1
,h
t
)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + max
i
|c
i
|) where |C| is the number of
classes and |c
i
| is the number of minimal units
in class c
i
. The best case complexity O(
?
|V |)
requires the number of classes and MTUs to be
evenly balanced, i.e., each class contains exactly
as many minimal units as there are classes.
Figure 3 illustrates how classing changes the
structure of the network by adding an additional
output layer for the class probabilities.
4 Bag-of-words MTU RNN Model
The previous model treats MTUs as atomic sym-
bols which leads to large vocabularies requir-
ing large parameter sets and expensive inference.
However, similar MTUs may share the same
words, or words which are related in continuous
space. The atomic MTU model does not exploit
this since it cannot access the internal structure of
a minimal unit.
The approach we pursue next is to break MTUs
into individual source and target words (Le et al.,
2012) in order to exploit structural similarities be-
tween infrequently observed minimal units. Sin-
gletons represent the vast majority of our MTU
vocabulary (Table 1). This resembles the word-
hashing trick of Huang et al. (2013) who repre-
sented individual words as a bag-of-character n-
grams to reduce the vocabulary size of a neural
network-based model in an information retrieval
setting.
2
We first describe a theoretically appealing but
computationally expensive model and then discuss
a more practical variation. The input layer of this
model accepts the current minimal unit as a K-of-
N vector representing K source and target words
as opposed to the 1-of-N encoding of entire MTUs
in the previous model (Figure 4). Larger MTUs
may contain the same word more than once and we
simply adjust their count to one.
3
Different to the
2
Applying the same technique would likely result in too many
collisions since we are dealing with multi-word units instead
of single words.
3
We found no effect on accuracy when using the unmodified
count in initial experiments.
23
x t
ht-1
ht
w t
V
W
U
1
0
1
1
0
1
0
0
D yt
C
. . .
. . .
. . .
sr c
tgt
MT U
Figure 4: Structure of MTU bag-of-words recur-
rent neural network model. The input layer rep-
resents a minimal unit as a bag-of-words and the
output layer y
t
is a probability distribution over
possible next MTUs depending on the activations
of the word layer w
t
representing source and tar-
get words of minimal units.
previous model, the input vector has now multiple
active entries whose signals are absorbed into the
new hidden layer configuration.
This bag-of-words encoding of minimal units
dramatically reduces the vocabulary size but it in-
evitably maps different MTUs to the same encod-
ing. On our data set, we observe less than 0.2% of
minimal units that are involved in collisions, a rate
that is similar to Huang et al. (2013). In practice
collisions are unlikely to affect accuracy in our set-
ting because MTUs that are mapped to the same
encoding usually do not differ much in semantic
meaning as illustrated by the following examples:
erfolg haben ? succeed collides with haben er-
folg? succeed, or damit ,? to and , damit? to;
in both examples either the auxiliary verb haben or
the comma changes position, neither of which sig-
nificantly changes the meaning for this particular
pair of MTUs.
The structure of the bag-of-words MTU RNN
models is shown in Figure 4. Similar to the atomic
MTU RNN model (?3), the hidden layer combines
the signal from the input layer and the previous
hidden layer configuration. The hidden layer acti-
vations feed into a word layer w
t
representing the
source and target words that part of all possible
MTUs; it is of the same size as the input layer. The
word layer is connected to a convolutional out-
put layer y
t
by weights summarized in the sparse
matrix C. The output layer represents all possi-
ble next minimal units, where each MTU entry is
only connected to neurons in the word layer repre-
senting its source and target words. The word and
MTU layers are then computed as follows:
w
t
= s(Vh
t
)
y
t
= g(Cw
t
)
However, there are a number of computational
issues with this model: First, we cannot efficiently
factor the word layer w
t
into classes such as for
the atomic MTU RNN model because we require
all its activations to compute the MTU output
layer y
t
. This reduces the best case complex-
ity of computing the word layer from O(
?
|V |)
back to linear in the number of source and tar-
get words |V |. In practice this results in between
200-1000 more activations that need to be com-
puted, depending on the word vocabulary size.
Second, turning the MTU output layer into a con-
volutional layer is not enough to sufficiently re-
duce the computational effort to compute the out-
put activations since the number of connections
between the word and MTU layers is very imbal-
anced. This is because frequent words, such as
function words, are part of many MTUs and there-
fore have a very high out-degree, e.g., the neuron
representing ?the? has over 82K outgoing edges.
On the other hand, infrequent words, have a very
low out-degree. This imbalance makes it hard
to efficiently compute activations and error gradi-
ents, even on a GPU, since some neurons require
substantially more work than others.
4
For these reasons we decided to design a sim-
pler, more tractable version of this model (Fig-
ure 5). The simplified model still represents an
input MTU as a bag-of-words but minimal units
are generated word-by-word, first emitting source
words and then target words. This is in contrast
to the original model which predicted an MTU as
a single unit. Decomposing the next MTU into
individual words dramatically reduces the size of
the output layer, thereby resulting in faster com-
putation of the outputs and making normalization
4
In initial experiments we found this model to be over twenty
times slower than the atomic MTU RNN model with esti-
mated training times of over 6 weeks. This was despite us-
ing a vastly smaller vocabulary and by computing the word
layer on a, by current standards, high-end GPU (NVIDIA
Tesla K20c) using sparse matrix optimizations (cuSPARSE)
for the convolutional layer.
24
mt
ht-1
ht
yt
V
W
U
1
0
1
1
0
1
0
0
T
D
c t
mt+ 1
sr c
tgt
MT U
Figure 5: Simplified MTU bag-of-words recurrent
neural network model (cf. Figure 4). An MTU is
input as bag-of-words and the next MTU is pre-
dicted as a sequence of both source and target
words.
into probabilities easier. Furthermore, the output
layer can be factorized into classes requiring only
a fraction of the neurons to be computed, a much
more efficient solution compared to the original
model which required calculation of the entire out-
put layer.
The simplified model computes the probability
of the next MTU m
t+1
as a product of individual
word probabilities:
p(m
t+1
|m
t
t?n+1
,h
t
) = (1)
?
a
1
,...,a
u
?m
t+1
p(c
k
|m
t
t?n+1
,h
t
)
p(a
k
|c
k
,m
t
t?n+1
,h
t
)
where we predict a sequence of source and target
words a
1
, . . . , a
u
? m
t+1
with a class-structured
output layer, similar to the atomic model (?3).
Training still uses a cross entropy criterion and
back propagation through time, however, error
vectors are computed on a per-word basis, instead
of a per-MTU basis. Direct connections between
the input and output layers are based on source and
target words which is less sparse than basing direct
features on entire MTUs such as for the original
bag-of-words model.
Overall, the simplified model retains the bag-of-
words input representation of the original model,
while permitting the efficient factorization of the
word-output layer into classes.
5 Experiments
We evaluate the effectiveness of both the atomic
MTU RNN model (?3) and the simplified bag-of-
words MTU RNN model (?4) in an n-best rescor-
ing setting, comparing against a trigram back-off
MTU model as well as the phrasal decoder 1-best
output which we denote as the baseline.
5.1 Experimental Setup
Baselines. We experiment with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007), scoring translations by a set of common
features including maximum likelihood estimates
of source given target mappings p
MLE
(e|f) and
vice versa p
MLE
(f |e), as well as lexical weight-
ing estimates p
LW
(e|f) and p
LW
(f |e), word and
phrase-penalties, a linear distortion feature and
a lexicalized reordering feature. The baseline
includes a standard modified Kneser-Ney word-
based language model trained on the target-side of
the parallel corpora described below. Log-linear
weights are estimated with minimum error rate
training (MERT; Och, 2003).
The 1-best output by the phrase-based decoder
is the baseline accuracy. As a second baseline we
experiment with a trigram back-off MTU model
trained on all extracted MTUs, denoted as n-gram
MTU. The trigram MTU model is estimated with
the same modified Kneser-Ney framework as the
target side language model. All MTU models are
trained in target left-to-right MTU order which
performed well in initial experiments.
Evaluation. We test our approach on two differ-
ent data sets. First, we train a German to English
system based on the data of the WMT 2006 shared
task (Koehn and Monz, 2006). The parallel corpus
includes about 35M words of parliamentary pro-
ceedings for training, a development set and two
test sets with 2000 sentences each.
Second, we experiment with a French to En-
glish system based on 102M words of training data
from the WMT 2012 campaign. The majority of
the training data set is parliamentary proceedings
except for about 5m words which are newswire; all
MTU models are trained on the newswire subset
since we found similar accuracy to using all data in
initial experiments. We evaluate on four newswire
domain test sets from 2008, 2010 and 2011 as well
as the 2010 system combination test set contain-
ing between 2034 to 3003 sentences. Log-linear
weights are estimated on the 2009 data set com-
25
prising 2525 sentences. We evaluate all systems
in a single reference BLEU setting.
Rescoring Setup. We rescore the 1000-best out-
put of the baseline phrase-based decoder by ei-
ther the trigram back-off MTU model or the
RNN models. The baseline accuracy is obtained
by choosing the 1-best decoder output. We re-
estimate the log-linear weights for rescoring by
running a further iteration of MERT with the ad-
ditional feature values; we initialize the rescoring
feature weight to zero and try 20 random restarts.
At test time we use the new set of log-linear
weights to rescore the test set n-best list.
Neural Network Setup. We trained the recur-
rent neural network models on between 88% and
93% of each data set and used the remainder as
validation data. The vocabulary of the atomic
MTU RNN model is comprised of all MTU types
which were observed more than once in the train-
ing data.
5
Similarly, we modeled all non-singleton
words for the bag-of-words MTU RNN model.
We obtain classes for words or MTUs using a
version of Brown-Clustering with an additional
regularization term to optimize the runtime of
the language model (Brown et al., 1992; Zweig
and Makarychev, 2013). Direct connections use
features over unigrams, bigrams and trigrams of
words or MTUs, depending on the model. Fea-
tures are hashed to a table with at most 500 million
values following Mikolov et al. (2011a). We use
the standard settings for the model with the default
learning rate ? = 0.1 that decays exponentially if
the validation set entropy does not decrease. Back
propagation through time computes error gradi-
ents over the past twenty time steps. Training
is stopped after 20 epochs or when the valida-
tion entropy does not decrease over two epochs.
Throughout, we use a hidden layer size of 100
which provided a good trade-off between time and
accuracy in initial experiments.
5.2 Results
We first report the decoder 1-best output as the
first baseline and then rescore our two data sets
(Table 2 and Table 3) with the n-gram back-off
MTU model to establish a second baseline (n-
gram MTU). The n-gram model improves by 0.4
BLEU over the decoder 1-best on all test sets for
German to English. On French-English accuracy
5
We tried modeling all MTUs which did not contain a single-
ton word but observed no significant effect on accuracy.
dev test1 test2
Baseline 25.8 26.0 26.0
n-gram MTU 26.3 26.6 26.4
atomic MTU RNN 26.5 26.8 26.5
BoW MTU RNN 26.5 27.0 26.9
word RNNLM 26.5 27.1 26.8
Combined 26.8 27.3 27.1
Table 2: German to English BLEU results for
the decoder 1-best output (Baseline) compared to
rescoring with a target left-to-right trigram MTU
model (n-gram MTU), our two recurrent neural
network-based MTU models, a word-based RNN-
based language model (word RNNLM), as well
as a combination of the three RNN-based models
(Combined).
improves on three out of five sets by up to 0.7
BLEU.
Next, we evaluate the accuracy of the MTU
RNN models. The atomic MTU RNN model im-
proves over the n-gram MTU model on all test sets
for German to English, however, for French to En-
glish the back-off model performs better on two
out of four test sets.
The next question we answer is if breaking
MTUs into individual units to leverage similarities
in the internal structure can help accuracy. The re-
sults (Table 2 and Table 3) for the bag-of-words
model (BoW MTU RNN) clearly show that this is
the case for both language pairs. We significantly
improve over the n-gram MTU model as well as
the atomic RNN model on all test sets. We observe
gains of up to 0.5 BLEU over the n-gram MTU
model for German to English as well as French to
English; improvements over the decoder baseline
are up to 1.2 BLEU for French to English.
How do our models compare to other neural net-
work approaches that rely only on target side in-
formation? To answer this question we compare
to the strong language model of Mikolov (2012;
RNNLM) which has recently improved the state-
of-the-art in language modeling perplexity. The
results (Table 2 and Table 3) show that RNNLM
performs competitively. However, our approaches
model translation since we use both source and tar-
get information as opposed to scoring only the flu-
ency of the target side, such as done by RNNLM.
Can our models act complementary to a strong
RNN language model? Our final experiment com-
bines the atomic MTU RNN model, the BoW
26
dev news2008 news2010 news2011 newssyscomb2010
Baseline 24.3 20.5 24.4 25.1 24.3
n-gram MTU 24.6 20.8 24.4 25.8 24.3
atomic MTU RNN 24.6 20.7 24.4 25.5 24.3
BoW MTU RNN 25.2 21.2 24.8 26.3 24.6
word RNNLM 25.1 21.4 25.1 26.4 24.9
Combined 25.4 21.4 25.1 26.6 24.9
Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to various
MTU models (cf. Table 2).
MTU RNN model, and the RNNLM (Combined).
The results (Table 2 and Table 3) confirm that this
is the case. For German to English translation
accuracy improves by 0.2 to 0.3 BLEU over the
RNNLM alone, with gains of up to 1.3 BLEU over
the baseline and up to 0.7 BLEU over the n-gram
MTU model. Improvements for French to English
are lower but we can see some gains on news2011
and on the dev set. Overall, we improve accuracy
on the French to English task by up to 1.5 BLEU
over the decoder 1-best, and by up to 0.8 BLEU
over the n-gram MTU model.
6 Related Work
Our approach of modeling Minimum Translation
Units is very much in line with recent work on n-
gram-based translation models (Crego and Yvon,
2010), and more recently, continuous space-based
translation models (Le et al., 2012). The mod-
els presented in this paper differ in a number of
key aspects: We use a recurrent architecture repre-
senting an unbounded history of MTUs rather than
a feed-forward style network. Feed-forward net-
works as well as back-off n-gram models rely on a
finite history which results in predictions indepen-
dent of anything but a short context of words. A
recent side-by-side comparison between recurrent
and feed-forward style neural networks (Sunder-
meyer et al., 2013) has shown that recurrent ar-
chitectures outperform feed-forward networks in
a language modeling task, a similar problem to
modeling sequences over Minimum Translation
Units.
Furthermore, the input of our best model is a
bag-of-words representation of an MTU, unlike
the ordered source and target word n-grams used
by Crego and Yvon (2010) as well as Le et al.
(2012). Finally, we model both source and target
words in a single recurrent neural network. The
approach of Le et al. (2012) factorizes the joint
probability over an MTU sequence in a way that
suggests the use of separate neural network mod-
els for the source and the target sides, where each
model generates words on the respective side only.
Other work on applying recurrent neural net-
works to machine translation (Mikolov, 2012; Auli
et al., 2013; Kalchbrenner and Blunsom, 2013)
concentrated on word-based language and transla-
tion models, whereas we model Minimum Trans-
lation Units.
7 Conclusion and Future Work
Minimum Translation Unit models based on recur-
rent neural networks lead to substantial gains over
their classical n-gram back-off models. We intro-
duced two models of which the best improves ac-
curacy by up to 1.5 BLEU over the 1-best decoder
output, and by 0.8 BLEU over a trigram MTU
model in an n-best rescoring setting.
Our experiments have shown that representing
MTUs as bags-of-words leads to better accuracy
since this exploits similarities in the internal struc-
ture of Minimum Translation Units, which is not
possible when modeling them as atomic symbols.
We have also shown that our models are comple-
mentary to a very strong RNN language model
(Mikolov, 2012).
In future work, we would like to make the initial
version of the bag-of-words model computation-
ally more tractable using a better GPU implemen-
tation. This model combines the efficient bag-of-
words input representation with the ability to pre-
dict MTUs as single units while explicitly model-
ing the constituent words in an intermediate layer.
8 Acknowledgements
We would like to thank Kristina Toutanova for
providing a dataset and for helpful discussions re-
lated to this work. We also thank the four anony-
mous reviewers for their comments.
27
References
Alexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-
Son Le, Aur?elien Max, Guillaume Wisniewski,
Franc?ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309?315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20?28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert,
Patrik Lambert, and Jos?e B. Mari?no. 2005. Statis-
tical Machine Translation of Euparl Data by Using
bilingual n-grams. In Proc. of ACL Workshop on
Building and Using Parallel Texts, pages 133?136,
Jun.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Josep Crego and Franc?ois Yvon. 2010. Factored bilin-
gual n-gram language models for statistical machine
translation. Machine Translation, 24(2):159?175.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine Learning,
60(1-3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning Semantic Representations
for the Phrase Translation Model. Technical Report
MSR-TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum
Entropy Training. In Proc. of ICASSP.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL, August.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning Deep
Structured Semantic Models for Web Search using
Clickthrough Data. In Proc. of CIKM, October.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700?1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. of NAACL Workshop
on Statistical Machine Translation, pages 102?121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39?48, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045?1048.
Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?s
Burget, and Jan
?
Cernock?y. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196?201.
Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proc. of ICASSP, pages 5528?5531.
Tom?a?s Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Tom?a?s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proc. of NAACL,
pages 8?16, New York, Jun.
28
Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze.
2012. Revisiting the Case for Explicit Syntactic
Information in Language Models. In NAACL-HLT
Workshop on the Future of Language Modeling for
HLT, pages 50?58. Association for Computational
Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Marta R. Costa-juss`a, and Jos?e A. R.
Fonollosa. 2007. Smooth Bilingual N -Gram Trans-
lation. In Proc. of EMNLP, pages 430?438, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11?19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing, pages 8430?8434, Vancouver,
Canada, May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proc. of NAACL,
pages 12?21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Geoff Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proc. of ICASSP.
29
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 294?298,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Corpus Expansion for Statistical Machine Translation with
Semantic Role Label Substitution Rules
Qin Gao and Stephan Vogel
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
{qing, stephan.vogel}@cs.cmu.edu
Abstract
We present an approach of expanding paral-
lel corpora for machine translation. By uti-
lizing Semantic role labeling (SRL) on one
side of the language pair, we extract SRL sub-
stitution rules from existing parallel corpus.
The rules are then used for generating new
sentence pairs. An SVM classifier is built to
filter the generated sentence pairs. The fil-
tered corpus is used for training phrase-based
translation models, which can be used directly
in translation tasks or combined with base-
line models. Experimental results on Chinese-
English machine translation tasks show an av-
erage improvement of 0.45 BLEU and 1.22
TER points across 5 different NIST test sets.
1 Introduction
Statistical machine translation (SMT) relies on par-
allel corpus. Aside from collecting parallel cor-
pus, we have seen interesting research on automat-
ically generating corpus from existing resources.
Typical examples are paraphrasing using bilingual
(Callison-Burch et al, 2006) or monolingual (Quirk
et al, 2004) data. In this paper, we propose a dif-
ferent methodology of generating additional parallel
corpus. The basic idea of paraphrasing is to find al-
ternative ways that convey the same information.
In contrast, we propose to build new parallel sen-
tences that convey different information, yet retain
correct grammatical and semantic structures.
The basic idea of the proposed method is to sub-
stitute source and target phrase pairs in a sentence
pair with phrase pairs from other sentences. The
problem is how to identify where a substitution
should happen and which phrase pairs are valid can-
didates for the substitution. While syntactical con-
straints have been proven to helpful in identifying
good paraphrases (Callison-Burch, 2008), it is in-
sufficient in our task because it cannot properly filter
the candidates for the replacement. If we allow all
the NPs to be replaced with other NPs, each sen-
tence pair can generate huge number of new sen-
tences. Instead, we resort to Semantic Role Labeling
(Palmer et al, 2005) to provide more lexicalized and
semantic constraints to select the candidates. The
method only requires running SRL labeling on ei-
ther side of the language pair, and that enables ap-
plications on low resource languages. Even with the
SRL constraints, the generated corpus may still be
large and noisy. Hence, we apply an additional fil-
tering stage on the generated corpus. We used an
SVM classifier with features derived from standard
phrase based translation models and bilingual lan-
guage models to identify high quality sentence pairs,
and use these sentence pairs in the SMT training. In
the remaining part of the paper, we introduce the ap-
proach and present experimental results on Chinese-
to-English translation tasks, which showed improve-
ments across 5 NIST test sets.
2 The Proposed Approach
The objective of the method is to generate new syn-
tactically and semantically well-formed parallel sen-
tences from existing corpus. To achieve this, we first
collect a set of rules as the candidates for the substi-
tution. We also need to know where we should put in
the replacements and whether the resulting sentence
pairs are grammatical.
First, standard word alignment and phrase extrac-
tion are performed on existing corpus. Afterwards,
we apply an SRL labeler on either the source or tar-
get language, whichever has a better SRL labeler.
Third, we extract SRL substitution rules (SSRs)
from the corpus. The rules carry information of se-
mantic frames, semantic roles, and corresponding
294
.! !   &
1

2
	 

 ,+,2+
% ,
+ ",  &
 ,+,1+
% ,+ ",.! %(" #
'!% ")"Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 30?34,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Consensus versus Expertise : A Case Study of Word Alignment with
Mechanical Turk
Qin Gao and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA, 15213
{qing,stephan.vogel}@cs.cmu.edu
Abstract
Word alignment is an important preprocessing step
for machine translation. The project aims at incorpo-
rating manual alignments from Amazon Mechanical
Turk (MTurk) to help improve word alignment qual-
ity. As a global crowdsourcing service, MTurk can
provide flexible and abundant labor force and there-
fore reduce the cost of obtaining labels. An easy-
to-use interface is developed to simplify the labeling
process. We compare the alignment results by Turk-
ers to that by experts, and incorporate the alignments
in a semi-supervised word alignment tool to improve
the quality of the labels. We also compared two pric-
ing strategies for word alignment task. Experimental
results show high precision of the alignments pro-
vided by Turkers and the semi-supervised approach
achieved 0.5% absolute reduction on alignment error
rate.
1 Introduction
Word alignment is used in various natural language
processing tasks. Most state-of-the-art statistical machine
translation systems rely on word alignment as a prepro-
cessing step. The quality of word alignment is usually
measured by AER, which is loosely related to BLEU
score (Lopez and Resnik, 2006). There has been re-
search on utilizing manually aligned corpus to assist auto-
matic word alignment, and obtains encouraging results on
alignment error rate. (Callison-Burch et al, 2004; Blun-
som and Cohn, 2006; Fraser and Marcu, 2006; Niehues
and Vogel, 2008; Taskar et al, 2005; Liu et al, 2005;
Moore, 2005). However, how to obtain large amount of
alignments with good quality is problematic. Labeling
word-aligned parallel corpora requires significant amount
of labor. In this paper we explore the possibility of us-
ing Amazon Mechanical Turk (MTurk) to obtain manual
word alignment faster, cheaper, with high quality.
Crowdsourcing is a way of getting random labor force
on-line with low cost. MTurk is one of the leading
providers for crowdsourcing marketplace. There have
been several research papers on using MTurk to help nat-
ural language processing tasks, Callison-Burch (2009)
used MTurk to evaluate machine translation results. Kit-
tur et al (2008) showed the importance of validation
data set, the task is evaluating quality of Wikipedia arti-
cles. There are also experiments use the annotation from
MTurk in place of training data. For example (Kaisser et
al., 2008) and (Kaisser and Lowe, 2008) used MTurk to
build question answering datasets and choose summary
lengths that suite the need of the users.
Word alignment is a relatively complicate task for in-
experienced workers. The fact puts us in a dilemma,
we can either provide lengthy instructions and train the
workers, or we must face the problem that workers may
have their own standards. The former solution is im-
practical in the context of crowdsourcing because heavily
trained workers will expect higher payment, which de-
feats economical nature of crowdsourcing. Therefore we
are forced to face the uncertainty, and ask ourselves the
following questions: First, how consistent would the la-
bels from random labelers be, given minimal or no in-
structions? Second, how consistent would these intuitive
labels be consistent with the labels from expert labelers?
Third, if there is certain level of consistency between the
intuitive labels and the labels from experts, can we extract
most reliable links from the former? Last but not least,
given the alignment links, can we utilize them to help au-
tomatic word alignment without further human efforts?
The statistics on the data we get shows the internal
consistency among multiple MTurk alignments is greater
than 70%, and the precision is greater than 84% when
consider all the links. By applying majority vote and
consensus strategies, we can select links that have greater
than 95% accuracy. When applying the alignment links
on a new aligner that can perform constrained EM al-
gorithm for IBM models we observe 0.5% absolute im-
provements on alignment error rate. The average per-
word cost is about 2 cent per word.
The paper will be organized as follows, first we will
discuss the design principle of the task and the implemen-
tation of the application for word alignment in section
2. Section 3 describes the algorithm used in utilizing the
manual alignments. Section 4 presents the analysis on the
harvested data and the expert labels, and the the experi-
ment results of semi-supervised word alignment. Section
5 concludes the paper.
30
2 Design of the task
In this task, we want to collect manual word alignment
data from MTurk workers, Figure 2 shows an example of
word alignment. There are two sentences which are trans-
lation of each other. There are links between words in two
sentences, indicating the words are translation pairs. No-
tice that one word can be aligned to zero or more words,
if a word is aligned to zero word, we can assume it is
aligned to a virtual empty word. Therefore, given a sen-
tence pair, we want workers to link words in source sen-
tence to one or more target words or the empty word.
In our experiment, we use a Chinese-English parallel
corpus and ask workers to alignment the words in Chi-
nese sentence to the words in English sentence. We do
not provide any alignment links from automatic aligner.
2.1 Guidelines of design
MTurk represents a new pattern of market that has
yet be thoroughly studied. Mason and Watts (2009)
shows that higher payment does not guarantee results
with higher quality. Also, one should be aware that the
web-based interface is vulnerable to automatic scripts
that generate highly consistent yet meaningless results.
To ensure a better result, several measures must be com-
bined: 1) Require workers to take qualifications before
they can accept the tasks. 2) Implement an interface less
vulnerable to automatic scripts. 3) Build quality control
mechanism that filters inaccurate results, and finally 4)
Redesign the interface so that the time spent by careful
and careless workers does not differ too much, so there
is less incentives for workers to submit random results.
With these guidelines in mind, we put together several
elements into the HIT.
Qualifications
We require the workers to take qualifications, which
requires them to pick correct translation of five Chinese
words. The Chinese word is rendered in bitmap.
Interface implementation
We implemented the word alignment interface on
top of Google Web Toolkit, which enables developing
Javascript based Web application in Java. Because
all the content of the interface, including the content in
the final result, is generated dynamically in the run time,
it is much more difficult to hack than plain HTML forms.
Figure 1 shows a snapshot of the interface1. The labeling
procedure requires only mouse click. The worker need
to label all the words with a golden background2. To
complete the task, the worker needs to: 1) Click on the
1A demo of the latest version can be found at http://
alt-aligner.appspot.com, the source code of the
aligner is distributed under Apache License 2.0 on http://
code.google.com/p/alt-aligner/
2If the document is printed in greyscale, the lightest background (ex-
cept the white one) is actually golden, the second lightest one is red and
the darkest one is dark blue.
Chinese word he want to label. 2) Click on the English
words he want the Chinese word to be linked, or click on
the empty word to the end of the sentence. 3) If he want to
delete a link, he need to click on the English word again,
otherwise he can move on to next unlabeled word, or to
modify links on another labeled word. 4) Only when all
required words are labeled, the user would be allowed to
click on submit button.
The interface has two more functionalities, first, it al-
lows to specify a subset of words in the sentence for user
to label, as shown in the snapshot, words with white back-
ground are not required to label. Secondly it supports
providing initial alignment on the sentence.
Quality control
Quality control is a crucial component of the system.
For problems that have clear gold standard answers to a
portion of data, the quality control can be done by min-
gling the known into the unknown, and rejecting the sub-
missions with low qualities on known samples. However
in our situation it is not easy to do so because although
we have fully manual aligned sentences, we do not have
corpus in which the sentences are partially aligned, there-
fore if we want to use the method we have to let worker
label an additional sentence, which may double the effort
for the workers. Also we do not provide thorough stan-
dard for users, therefore before we know the divergence
of the alignments, we actually do not know how to set the
threshold, even with given gold standard labels. In addi-
tion, if the method will be applied on languages with low
resource, we cannot assume availability of gold standard
answers. Therefore, we only try to filter out answers base
on the consensus. The quality control works as follows.
Firstly we assign an alignment task to 2n + 1 workers.
For these submissions, we first try to build a majority an-
swer from these assignments. For each alignment link,
if it appears in more than n submissions. Then every in-
dividual assignments will be compared to the majority
alignment, so we can get the precision and recall rates.
If either precision or recall rate is lower than a threshold,
we will reject the submission.
Figure 1: A snapshot of the labeling interface.
2.2 Pricing and worker base
We tried two pricing strategies. The first one fixes the
number of words that a worker need to label for each
HIT, and fix the rate for each HIT. The second one always
31
asks workers to label every word in the sentence, in the
mean time we vary the rate for each HIT according to the
lengths of source sentences. For each strategy we tried
different rates, starting from 10 words per cent. However
we did not get enough workers even after the price raised
to 2 words per cent. The result indicates a limited worker
base of Chinese speakers.
3 Utilizing the manual alignments
As we can expect, given no explicit guideline for word
alignments, the variance of different assignments can be
fairly large, a question will raise what can we do with
the disagreements? As we will see later in the experi-
ment part, the labels are more likely to be consistent with
expert labels if more workers agree on it. Therefore, a
simple strategy is to use only the links that more workers
have consensus on them.
2005? ? ??
The   summer   of    2005
Figure 2: Partial and full alignments
However the method instantly gives rise to a prob-
lem. Now the alignment is not ?full alignments?, instead,
they are ?partial?. The claim seems to be trivial but they
have completely different underlying assumptions. Fig-
ure 2 shows the comparison of partial alignments (the
bold link) and full alignments (the dashed and the bold
links). In the example, if full alignment is given, we can
assert 2005 is only aligned to 2005#, not to {or ,
but we cannot do that if only partial alignment is given.
In this paper we experiment with a novel method which
uses the partial alignment to constraint the EM algorithm
in the parameter estimation of IBM models.
IBM Models (Brown et. al., 1993) are a series of gen-
erative models for word alignment. GIZA++ (Och and
Ney, 2003) is the most widely used implementation of
IBM models and HMM (Vogel et al, 1996) where EM
algorithm is employed to estimate the model parameters.
In the E-step, it is possible to obtain sufficient statistics
from all possible alignments for simple models such as
Model 1 and Model 2. Meanwhile for fertility-based
models such as Model 3, 4, 5, enumerating all possible
alignments is NP-complete. In practice, we use sim-
pler models such as HMM or Model 2 to generate a
?center alignment? and then try to find better alignments
among the neighbors of it. The neighbors of an alignment
aJ1 = [a1, a2, ? ? ? , aJ ], aj ? [0, I] is defined as align-
ments that can be generated from aJ1 by one of the oper-
ators: 1) Move operator m[i,j], that changes aj := i, i.e.
arbitrarily set word fj in source sentence to align to word
ei in target sentence; 2) Swap operator s[j1,j2] that ex-
changes aj1 and aj2 . The algorithm will update the center
alignment as long as a better alignment can be found, and
finally outputs a local optimal alignment. The neighbor
alignments of the alignment are then used in collecting
the counts for the M Step.
In order to use partial manual alignments to constrain
the search space, we separate the algorithm into two
stages, first the seed alignment will be optimized towards
the constraints. Each iteration we only pick a new center
alignment with less inconsistent links than the original
one, until the alignment is consistent with all constraints.
After that, in each iteration we pick the alignment with
highest likelihood but does not introduce any inconsistent
links. The algorithm will output a local optimal align-
ment consistent with the partial alignment. When col-
lecting the counts for M-step, we also need to exclude all
alignments that are not consistent with the partial man-
ual alignment. The task can also be done by skipping the
inconsistent alignments in the neighborhood of the local
optimal alignment.
4 Experiment and analysis
In this section we will show the analysis of the har-
vested MTurk alignments and the results of the semi-
supervised word alignment experiments.
4.1 Consistency of the manual alignments
We first examine the internal consistency of the MTurk
alignments. We calculate the internal consistency rate
in both results. Because we requested three assignments
for every question, we classify the links in two different
ways. First, if a link appear in all three submissions, we
classify it as ?consensus link?. Second, if a link appear in
more than one submissions, we classify it as ?majority?,
otherwise it is classified as ?minority?. Table 1 presents
the statistics of partial alignment and full alignment tasks.
Note that by spending the same amount of money, we get
more sentences aligned because for fixed rate partial sen-
tence alignment tasks, sometimes we may have overlaps
between tasks. Therefore we also calculate a subset of
full alignment tasks that consists of all the sentences in
partial alignment tasks. The statistics shows that although
generally full alignment tasks generates more links, the
partial alignment tasks gives denser alignments. It is in-
teresting to know whether the denser alignments lead to
higher recall rate or lower precision.
4.2 Comparing MTurk and expert alignments
To exam the quality of alignments, we compared them
with expert alignments. Table 2 lists the precision, recall
and F-1 scores for partial and full alignment tasks. We
compare the consistency of all links, the links in majority
group and the consensus links.
As we can observe from the results, the Turkers tend
to label less links than the experts, Interestingly, the over-
all quality of partial alignment tasks is significantly better
than full alignment tasks. Despite the lower recall rate, it
is encouraging that the majority vote and consensus links
32
Partial Full Full-Int
Number of sentences 135 239 135
Number of words 2,008 3,241 2,008
Consensus words 13,03 2,299 1,426
Consensus rate(%) 64.89 70.93 71.02
Total Links 7,508 9,767 6,114
Consensus Links 5,625 7,755 4,854
Consensus Rate(%) 74.92 79.40 79.39
Total Unique Links 3,186 3,989 2,506
Consensus Links 1,875 2,585 1,618
Consensus Rate(%) 58.85 64.80 64.54
In majority group 2,447 3,193 1,426
Majority rate(%) 76.80 80.04 71.06
Table 1: Internal consistency of manual alignments, here
Full-Int means statistics of full alignment tasks on the
sentences that also aligned using partial alignment task
All Links Majority Links Consensus Links
P. R. F. P. R. F. P. R. F.
P 0.84 0.88 0.86 0.95 0.76 0.84 0.98 0.60 0.74
F 0.88 0.70 0.78 0.96 0.61 0.75 0.99 0.51 0.68
I 0.87 0.71 0.79 0.95 0.62 0.75 0.98 0.52 0.68
Table 2: Consistency of MTurk alignments with expert
alignments, showing precision (P), recall (R) and F1 (F)
between MTurk and expert alignments. P, F, and I corre-
spond to Partial, Full and Full-Int in Table 1
yield very high precisions against expert alignments. Ta-
ble 3 lists the words with most errors. Most errors occur
on function words. A manual review shows that more
than 85% errors have function words on either Chinese
side or English side. The result, however, is as expected
because these words are hard to label and we did not pro-
vide clear rule for function words.
4.3 Results of semi-supervised word alignment
In this experiment we try to use the alignment links in
the semi-supervised word alignment algorithm. We use
Chinese-English manually aligned corpus in the exper-
iments, which contains 21,863 sentence pairs, 424,683
Chinese words and 524,882 English words. First, we use
the parallel corpus to train IBM models without any man-
ual alignments, we run 5 iterations of model 1 and HMM,
Chinese English
FN FP FN FP
64 { 16 , 122 the 15 ,
26 4 11 ? 67 NULL 11 a
19 , 9 4 43 of 6 the
17 ? 3 ? 36 to 6 is
16 ?? 3 ? 24 a 4 to
Table 3: Words that most errors occur, FN means a false
negative error occurred on the word, i.e. a link to this
word or from this word is missing. FP means false pos-
itive, accordingly. The manual alignment links comes
from majority vote.
3 iterations of model 3 and 6 iterations of model 4. Then
we resume the training procedure from the third itera-
tions of model 4. This time we load the manual alignment
links and perform 3 iterations of constrained EM. We also
experiment with 3 different sets of alignments. Table 4
presents the improvements on the alignment quality.
Unsupervised
Ch-En En-Ch
Prec. Recall AER Prec. Recall AER
68.22 46.88 44.43 65.35 55.05 40.24
All Links
Partial 68.28 47.09 44.26 65.86 55.63 39.68
Full-Int 68.28 47.09 44.26 65.85 55.63 39.69
Full 68.37 47.15 44.19 65.90 55.67 39.65
Majority Links
Partial 68.28 47.08 44.27 65.84 55.62 39.70
Full-Int 68.28 47.08 44.27 65.84 55.61 39.71
Full 68.37 47.13 44.20 65.88 55.65 39.67
Consensus Links
Partial 68.24 47.06 44.30 65.83 55.60 39.71
Full-Int 68.25 47.06 44.29 65.83 55.60 39.72
Full 68.31 47.10 44.25 65.86 55.63 39.68
Table 4: The performance of using manual alignments in
semi-supervised word alignment
From the result we can see that given the same amount
of links the improvement of alignment error rate is gen-
erally the same for partial and full alignment tasks, how-
ever, if we consider the amount of money spent on the
task, the full alignment task collect much more data than
partial alignments, we consider full sentence alignment
more cost efficient in this sense.
5 Conclusion
In this pilot experiment, we explore the possibility of
using Amazon Mechanical Turk (MTurk) to collect bilin-
gual word alignment data to assist automatic word align-
ment. We develop a system including a word align-
ment interface based on Javascript and a quality control
scheme. To utilize the manual alignments, we develop a
semi-supervised word alignment algorithm that can per-
form constrained EM with partial alignments. The algo-
rithm enables us to use only the most reliable links by
majority vote or consensus. The effectiveness of these
methods is proven by small-scale experiments. The re-
sults show the manual alignments from MTurk have high
precision with expert word alignment, especially when
filtered by majority vote or consensus. We get small im-
provement on semi-supervised word alignment. Given
the promising results, it is interesting to see if the ten-
dency will carry on when we scale up the experiments.
However the experiment also shows some problems,
first the coverage of worker base on MTurk is limited.
Given small worker base for specific languages, the cost
efficiency for NLP tasks in those languages is question-
able.
33
References
P. Blunsom and T. Cohn. 2006. Discriminative word align-
ment with conditional random fields. In Proceedings of the
21st International Conference on Computational Linguistics
and the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 65?72.
P. F. Brown et. al. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. In Computational
Linguistics, volume 19(2), pages 263?331.
C. Callison-Burch, D. Talbot, and D. Osborne. 2004. Statistical
machine translation with word- and sentence-aligned parallel
corpora. In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics (ACL-2004).
C. Callison-Burch. 2009. Fast, cheap, and creative: Evaluat-
ing translation quality using Amazon?s Mechanical Turk. In
Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 286?295. Associa-
tion for Computational Linguistics.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In ACL-44: Proceedings of the
21st International Conference on Computational Linguistics
and the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 769?776.
M. Kaisser and J. B. Lowe. 2008. A research collection of
question answer sentence pairs. In Proceedings of The 6th
Language Resources and Evaluation Conference.
M. Kaisser, M. Hearst, and J.B. Lowe. 2008. Evidence for
varying search results summary lengths. In Proceedings of
the 46th Annual Meeting of the Association for Computa-
tional Linguistics.
A. Kittur, E. H. Chi, and B Suh. 2008. Crowdsourcing user
studies with mechanical turk. In CHI ?08: Proceeding of the
twenty-sixth annual SIGCHI conference on Human factors in
computing systems, pages 453?456.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models for word
alignment. In ACL ?05: Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics, pages
459?466.
A. Lopez and P. Resnik. 2006. Word-based alignment, phrase-
based translation: What?s the link? with philip resnik. In
Proceedings of the 7th Biennial Conference of the Associa-
tion for Machine Translation in the Americas (AMTA-2006).
W. Mason and D. J. Watts. 2009. Financial incentives and the
?performance of crowds?. In HCOMP ?09: Proceedings of
the ACM SIGKDD Workshop on Human Computation, pages
77?85.
R. C Moore. 2005. A discriminative framework for bilingual
word alignment. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in Natu-
ral Language Processing, pages 81?88.
J. Niehues and S. Vogel. 2008. Discriminative word alignment
via alignment matrix modeling. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages 18?25.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. In Computational Linguis-
tics, volume 1:29, pages 19?51.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discrim-
inative matching approach to word alignment. In Proceed-
ings of the conference on Human Language Technology and
Empirical Methods in Natural Language Processing, pages
73?80.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based word
alignment in statistical machine translation. In Proceedings
of 16th International Conference on Computational Linguis-
tics), pages 836?841.
34
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-supervised Word Alignment Algorithm
with Partial Manual Alignments
Qin Gao, Nguyen Bach and Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA, 15213
{qing, nbach, stephan.vogel}@cs.cmu.edu
Abstract
We present a word alignment framework
that can incorporate partial manual align-
ments. The core of the approach is a
novel semi-supervised algorithm extend-
ing the widely used IBM Models with
a constrained EM algorithm. The par-
tial manual alignments can be obtained
by human labelling or automatically by
high-precision-low-recall heuristics. We
demonstrate the usages of both methods
by selecting alignment links from manu-
ally aligned corpus and apply links gen-
erated from bilingual dictionary on unla-
belled data. For the first method, we con-
duct controlled experiments on Chinese-
English and Arabic-English translation
tasks to compare the quality of word align-
ment, and to measure effects of two differ-
ent methods in selecting alignment links
from manually aligned corpus. For the
second method, we experimented with
moderate-scale Chinese-English transla-
tion task. The experiment results show an
average improvement of 0.33 BLEU point
across 8 test sets.
1 Introduction
Word alignment is used in various natural lan-
guage processing applications, and most statistical
machine translation systems rely on word align-
ment as a preprocessing step. Traditionally the
word alignment model is trained in an unsuper-
vised manner, e.g. the most widely used tool
GIZA++ (Och and Ney, 2003), which implements
the IBM Models (Brown et. al., 1993) and the
HMM model (Vogel et al, 1996). However, for
language pairs such as Chinese-English, the word
alignment quality is often unsatisfactory (Guzman
et al, 2009). There has been increasing interest on
using manual alignments in word alignment tasks.
Ittycheriah and Roukos (2005) proposed to use
only manual alignment links in a maximum en-
tropy model. A number of semi-supervised word
aligners are proposed (Blunsom and Cohn, 2006;
Niehues and Vogel, 2008; Taskar et al, 2005; Liu
et al, 2005; Moore, 2005). These approaches use
held-out manual alignments to tune the weights
for discriminative models, with the model param-
eters, model scores or alignment links from un-
supervised word aligners as features. Also, sev-
eral models are proposed to address the prob-
lem of improving generative models with small
amount of manual data, including Model 6 (Och
and Ney, 2003) and the model proposed by Fraser
and Marcu (2006) and its extension called LEAF
aligner (Fraser and Marcu, 2007). The approaches
use labelled data to tune parameters to combine
different components of the IBM Models.
2005? ? ??
2005nian     de      xiatian
The   summer   of    2005
Figure 1: Partial and full alignments
An interesting question is, if we only have par-
tial alignments of sentences, can we make use of
them? Figure 1 shows the comparison of par-
tial alignments (the bold link) and full alignments
(both of the dashed and the bold links). A partial
alignment of a sentence only provides a portion of
links of the full alignment. Although it seems to be
trivial, they actually convey different information.
In the example, if the full alignment is given, we
can assert 2005 is only aligned to 2005nian, not to
de or xiatian, but if only the partial alignment is
given we cannot make such assertion.
Partial alignments can be obtained from vari-
ous sources, for example, we can fetch them by
manually correcting unsupervised alignments, by
simple heuristics such as dictionaries of technical
1
terms, by rule-based alignment systems that have
high accuracy but low recall rate. The function-
ality is considered useful in many scenarios. For
example, the researchers can analyse the align-
ments generated by GIZA++ and fix common
error patterns, and perform training again. On
another way, an application can combine active
learning (Arora et al, 2009) and crowdsourcing,
asking non-expertise such as workers of Amazon
Mechanical Turk to label crucial alignment links
that can improve the system with low cost, which
is now a promising methodology in NLP areas
(Callison-Burch, 2009).
In this paper, we propose a semi-supervised ex-
tension of the IBM Models that can utilize partial
alignment links. More specifically, we are seeking
answers for the following questions:
? Given the partial alignment of a sentence,
how to find the most probable alignment that
is consistent with the partial alignment.
? Given a set of partially aligned sentences,
how to get the parameters that maximize the
likelihood of the sentence pairs with align-
ments consistent with the partial alignments
? Given a set of partially aligned sentences,
with conflicting partial alignments, how to
answer the two questions above.
In the proposed approach, the manual partial
alignment links are treated as ground truth, there-
fore, they will be fixed. However, for all other
links we make no additional assumption. When
using manual alignments, there can be links con-
flicting with each other. These conflicting evi-
dences are treated as options and the generative
model will choose the most probable alignment
from them. An efficient training algorithm for
fertility-based models is proposed. The algorithm
manipulates the Moving and Swapping matrices
used in the hill-climbing algorithm (Och and Ney,
2003) to rule out inconsistent alignments in both
E-step and M-step of the training.
A similar attempt has been made by Callison-
Burch et al (2004), where the authors interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Our approach is differ-
ent from their method that we do not require fully
aligned data and we do not need to interpolate two
parameter sets. All the training is done within a
unified framework. Our approach is also different
from LEAF (Fraser and Marcu, 2007) and Model
6 (Och and Ney, 2003) that we do not use these
additional links to tune additional parameters to
combine model components, as a result, it is not
limited to fully aligned corpus.
A question may raise why the proposed method
is superior over using the partial alignment links
as features in discriminative aligners? There are
three possible explanations. First, the method pre-
serves the power of the generative model in which
the algorithm utilizes large amount of unlabeled
data. More importantly, the additional information
can propagate over the whole corpus through bet-
ter estimation of model parameters. In contrast, if
we use the alignment links in discriminative align-
ers as a feature, one link can only affect the par-
ticular word, or at most the sentence. Second, al-
though the discriminative word alignment meth-
ods provide flexibility to utilize labeled data, most
of them still rely on generative aligners. Some
rely on the model parameters of the IBM Mod-
els (Liu et al, 2005; Blunsom and Cohn, 2006),
others rely on the alignment links from GIZA++
as features or as training data (Taskar et al, 2005),
or use both the model parameters and the align-
ment links (Niehues and Vogel, 2008). Therefore,
improving the generative aligner is still important
even when using discriminative aligners. Third,
these methods require full alignment of sentences
to provide positive (aligned) and negative (non-
aligned) information, which limits the availability
of data (Niehues and Vogel, 2008).
The proposed method has been successfully ap-
plied on various tasks, such as utilizing manual
alignments harvested from Amazon Mechanical
Turk (Gao and Vogel, 2010), and active learning
methods for improving word alignment (Ambati
et al, 2010). This paper provides the detailed al-
gorithm of the method and controlled experiments
to demonstrate its behavior.
The paper is organized as follows, in section
2 we describe the proposed model as well as the
modified training algorithm. Section 3 presents
two approaches of obtaining manual alignment
links, The experimental results will be shown in
section 4. We conclude the paper in section 5.
2 Semi-supervised word alignment
2.1 Problem Setup
The IBM Models (Brown et. al., 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003) is the most widely
used implementation of the IBM Models and the
2
HMM model (Vogel et al, 1996). Given two
strings from target and source languages fJ1 =
f1, ? ? ? , fj , ? ? ? fJ and eI1 = e1, ? ? ? , ei, ? ? ? eI , an
alignment of the sentence pair is defined as aJ1 =
[a1, a2, ? ? ? , aJ ], aj ? [0, I]. The IBM Models
assume all the target words must be covered ex-
actly once (Brown et. al., 1993). We try to model
P (fJ1 |e
I
1), which is the probability of observing
source sentence given target sentence eI1. In sta-
tistical models a hidden alignment variable is in-
troduced, so that we can write the probability as
P (fJ1 |e
I
1) =
?
aJ1
Pr(fJ1 , a
J
1 |e
J
1 , ?), where Pr(?)
is the estimated probability given the parameter set
?. The IBM Models define several different set of
parameters, from Model 1 to Model 5. Starting
from Model 3, the fertility model is introduced.
EM algorithm is employed to estimate the
model parameters of the IBM Models. In E-step,
it is possible to obtain sufficient statistics from
all possible alignments with simplified formulas
for simple models such as Model 1 and Model 2.
Meanwhile for fertility-based models, enumerat-
ing all possibilities is NP-complete and hence it
cannot be carried out for long sentences. A solu-
tion is to explore only the ?neighbors? of Viterbi
alignments. However, obtaining Viterbi align-
ments itself is NP-complete for these models. In
practice, a greedy algorithm is employed to find
a local optimal alignments based on Viterbi align-
ments generated by simpler models.
First, we define the neighbor alignments of a as
the set of alignments that differ by one of the two
operators from the original ?center alignment?.
? Move operator m[i,j], that changes aj := i,
i.e. arbitrarily set word fj in source sentence
to align to word fi in target sentence.
? Swap operator s[j1,j2] that exchanges aj1 and
aj2 .
We denote the neighbor alignments set of
current center alignment a as nb(a). In
each step of hill-climbing algorithm, we find
the alignment b(a) in nb(a), s.t. b(a) =
argmaxa??nb(a) p(a
?|e, f), and update the current
center alignment. The algorithm iterates until
there is no update could be made. The statistics of
the neighbor alignments of the final center align-
ment will be collected for normalization step (M-
step). The algorithm is greedy, so a reasonable
start point is important. In practice GIZA++ uses
Model 2 or HMM to generate the seed alignment.
To improve the speed of hill climbing, GIZA++
caches the cost of all possible move and swap op-
erations in two matrices. In the so called Moving
Matrix M , the element Mij stores the likelihood
difference of a move operator aj = i:
Mij =
Pr(m[i,j](a)|e, f)
Pr(a|e, f)
? (1? ?(aj , i)) (1)
and in the Swapping Matrix S, the element Sjj?
stores the likelihood difference of a swap operator
between aj and aj? :
Sjj? =
{
Pr(S[j,j?](a)|e,f)
Pr(a|e,f) ? (1? ?(aj , aj?)) if j < j
?
0 otherwise
(2)
The matrices will be updated whenever an oper-
ator is made, but the update is limited to the rows
and columns involved in the operator.
We define a partial alignment of a sentence
pair (fJ1 , e
I
1) as ?
J
I = {(i, j), 0 ? i < I, 0 ?
j < J}, note that the partial alignment does not
assume 1-to-N restriction on either side, and the
word from neither source nor target side need to be
covered with links. If an index is missing, it does
not mean the word is aligned to the empty word.
Instead it just means no information is provided.
We use a link (0, j) or (i, 0) to explicitly represent
the information that word fj or ei is aligned to the
empty word.
In order to find the most probable align-
ment that is consistent the partial alignments,
we treat the partial alignment as constraints, i.e.
for an alignment aJ1 = [a1, a2, ? ? ? , aj ] on the
sentence pair fJ1 , e
I
1, the translation probability
Pr(fJ1 , a
J
1 |e
I
1, ?
J
I ) will be zero if the alignment is
inconsistent with the partial alignments.
Pr(fJ1 |e
I
1, a
J
1 , ?
J
I ) =
{
0, aJ1 is inconsistent with?
J
I
Pr(fJ1 |e
I
1, a
J
1 , ?), otherwise
(3)
Under the constraints of the IBM Models, there
are two situations that aJ1 is inconsistent with ?
J
I :
1. Target word misalignment: The IBM Models
assume one target word can only be aligned
to one source word. Therefore, if the target
word fj aligns to a source word ei, while the
constraint ?JI suggests fj should be aligned
to ei? , the alignment violates the constraint
and thus is considered inconsistent.
3
2005?
?
??
the
summer
of
2005Manual Alignment Link
(a)
2005?
?
??
the
summer
Of
2005Seed Alignment Consistent Alignment Center Alignment
(b)                    (c)
2005?
?
??
the
summer
of
2005
Figure 2: Illustration of Algorithm 1
2. Source word to empty word misalignment:
Since one source word can be aligned to mul-
tiple target words, it is hard to constrain the
alignments of source words. However, if a
source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.
However, we are facing the problem of con-
flicting evidences. The problem is not necessar-
ily caused by errors in manual alignments, but
the assumption of the IBM Models that one tar-
get word can only be aligned to one source word.
This assumption causes multiple alignment links
from one target word conflict with each other. In
this case, we relax the constraints of situation 1
that if the alignment link aj? is consistent with any
target-to-source links (i, j) that j = j?, it will be
considered consistent. Also, we arbitrarily assign
the source word to empty word constraints higher
priorities than other constraints.
In EM algorithm, to ensure the final model be
marginalized on the fixed alignment links, and
the final Viterbi alignment is consistent with the
fixed alignment links, we need to guarantee that
no statistics from inconsistent alignments be col-
lected into the sufficient statistics. On fertility-
based models, we have to make sure:
1. The hill-climbing algorithm outputs align-
ment links consistent with the fixed align-
ment links.
2. The count collection algorithm rules out all
the inconsistent statistics.
With the constrained hill-climbing algorithm
and count collection algorithm which will be de-
scribed below, the above two criteria are satisfied.
2.2 Constrained hill-climbing algorithm
Algorithm 1 shows the algorithm outline of con-
strained hill-climbing. First, similar to the original
hill-climbing algorithm described above, HMM
(or Model 2) is used to obtain a seed alignment.
To ensure the resulting center alignment be con-
sistent with manual alignment, we need to split the
Algorithm 1 Constrained Hill-Climbing
1: Calculate the seed alignment a0 using HMM model
2: while ic(a0) > 0 do
3: if {a : ic(a) < ic(a0)} = ? then
4: break
5: end if
6: a0 := argmaxa?nb(a0),ic(a)<ic(a0) Pr(f |e, a)
7: end while
8: Mij := ?1 if (i, j) 6? ?JI or (i, 0) ? ?
J
I
9: loop
10: Sjj? := ?1 if (j, aj?) 6? ?
J
I or (j
?, aj) 6? ?JI
11: Mi1j1 = argmaxMij ; Sj1j?1 = argmaxSij
12: if Mi1j1 ? 1 and Sj1j?1 ? 1 then
13: Break
14: end if
15: if Mi1j1 > Sj1j?1 then
16: Update Mi1?,Mj1?,M?i1 ,M?j1
and Si1?, Sj1?, S?i1 , S?j1 , set a0 := Mi1j1(a0)
17: else
18: Update Mj1?,Mj?1?,M?j1 ,M?j?1
and Sj?1?, Sj1?, S?j?1 , S?j1 , set a0 := Sj1j?1(a0)
19: end if
20: end loop
21: Return a0
hill-climbing algorithm into two stages, i.e. opti-
mize towards the constraints and towards the opti-
mal alignment under the constraints.
From a seed alignment, we first try to move the
alignment towards the constraints by choosing a
move or swap operator that:
1. has highest likelihood among alignments
generated by other operators, excluding the
original alignment,
2. eliminates at least one inconsistent link.
The first step reflects in line 2 through 7 in the
algorithm, where we use ic(?) to denote the total
number of inconsistent links in the alignment, and
nb(?) to denote the neighbor alignments.
We iteratively update the alignment until no ad-
ditional inconsistent link can be removed. The al-
gorithm implies that we force the seed alignment
to become closer to the constraints while trying
to find the best consistent alignment. Figure 2
demonstrates the idea, given the manual alignment
link shown in (a), and the seed alignment shown as
solid links in (b), we move the inconsistent link to
the dashed link by a move operation.
After we find the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
within the constraints. The algorithm sets the cells
to negative if the corresponding operations are not
allowed. The Moving matrix only need to be up-
dated once, as in line 8 of the algorithm. Whereas
the swapping matrix need to be updated every it-
4
eration, Since once the alignment is updated, the
possible violations will also change. This is done
in line 10.
If source words ik are aligned to the empty
word, we set Mik,j = ?1,?j, as shown in line 8.
The swapping matrix does not need to be modified
in this case because the swapping operator will not
introduce new links. Again, Figure 2 demonstrates
the optimization step in (c), two move operators
or one swap operator can move the link marked
with cross to the dashed line, which can be a bet-
ter alignment.
Because the cells that can lead to violations are
set to negative, the operators will never be picked
in line 11, therefore we effectively ensure the con-
sistency of the final center alignment.
The algorithm will end when no better update
can be made (line 12 through 14), otherwise, we
pick the new update with highest likelihood as new
center alignment and update the cells in the Mov-
ing and Swapping matrices that will be affected
by the update. Line 15 through line 19 perform
the operation.
2.3 Count Collection
After finding the center alignment, we collect
counts from the neighbor alignments so that the
M-step can normalize the counts to produce the
model parameters for the next step. All statis-
tics from inconsistent alignments are ruled out to
ensure the final sufficient statistics marginalized
on the fixed alignment links. Similar to the con-
strained hill climbing algorithm, we can manipu-
late the Moving/Swapping matrices to effectively
exclude inconsistent alignments. We just need to
bypass all the cells whose values are negative, i.e.
represent inconsistent alignments.
By combining the constrained EM algorithm
and the count collection, the Viterbi alignment is
guaranteed to be consistent with the fixed align-
ment links, and the sufficient statistics is guar-
anteed to contain no statistics from inconsistent
alignments.
2.4 Training scheme
We extend the multi-thread GIZA++ (Gao and
Vogel, 2008) to load the alignments from a mod-
ified corpus file. The links are appended to the
end of each sentence in the corpus file in the form
of indices pairs, which will be read by the aligner
during training. In practice, we first training un-
constrained models up to Model 4, and then switch
to constrained Model 4 and continue training for
several iterations, the actual number of training
order is: 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, 3 iterations of
unconstrained Model 4 and 3 iterations of con-
strained Model 4. Because here we actually have
more Model 4 iterations, to make the comparison
fair, in all the experiments below we perform 6 it-
erations of Model 4 in the baseline systems.
3 Obtaining alignment links
Given the algorithm described in the Section 2,
we still face the problem of obtaining alignment
links to constrain the system. In this section, we
describe two approaches to obtain the links, the
first is to resort to human labels, while the second
applies high-precision-low-recall heuristic-based
aligner on large unsupervised corpus.
3.1 Using manual alignment links
Using manual alignment links is simple and
straight-forward, however the problem is how to
select links for human to label given that labelling
the whole corpus is impossible. We propose two
link selectors, the first is the random selector in
which every links in the manual alignment has
equal probability of being selected. Obviously,
the random selecting method is far from optimal
because it pays no attention on the quality of ex-
isting links. In order to demonstrate that by select-
ing links carefully we can achieve better alignment
quality with less manual alignment links, we pro-
pose the second selector based on disagreements
of alignments from two directions. We first clas-
sify the source and target words fj and ei into
three categories. Use fj as an example, the cat-
egories are:
? C1: fj aligns to ei, i > 0 in e ? f ,1 but in
reversed direction ei does not align to fj but
to another word.
? C2: fj aligns to ei, i > 0, in f ? e, but in
reversed direction (e ? f ), fj aligns to the
empty word.
? C3: no word aligns to fj , in f ? e, but in
reversed direction fj aligns to ei, i > 0.2
The criteria of ei are the same as fj after swap-
ping the definitions of ?source? and ?target?.
We prioritize the links ?JI = (i, j) by looking at
the classes of the source/target words. The order of
1Recall that fj can align to only one word.
2This class is different from C1 that whether ei aligns to
concrete words or the empty word.
5
Order Criterion Order Criterion
1 fj ? C1 5 ei ? C2
2 fj ? C2 4 ei ? C1
3 fj ? C3 6 ei ? C3
Table 1: The priorities of alignment links
priorities is shown in Table 1. All the links not in
the six classes will have the lowest priorities. The
links with higher priorities will be selected first,
but the order of two links in a same priority class
is not defined and they will be selected randomly.
3.2 Using heuristics on unlabelled data
Another possible way of getting alignment links
is to make use of heuristics to generate high-
precision-low-recall links and feed them into the
aligner. The heuristics can be number map-
ping, person name translator or more sophisticated
methods such as alignment confidence measure
(Huang, 2009). In this paper we propose to use
manual dictionaries to generate alignment links.
First we filter out from the dictionary the en-
tries with high frequency in the source side, and
then build an aligner based on it. The aligner out-
put links between words if them match an entry
in the dictionary. The method can be applied on
large unlabelled corpus and generate large num-
ber of links, after that we use the links as manual
alignment links in proposed method.
The readers may notice that GIZA++ supports
utilizing manual dictionary as well, however it is
different from our method. The dictionary is used
in GIZA++ only in the initialization step of Model
1, where only the statistics of the word pairs ap-
peared in the dictionary will be collected and nor-
malized. Given the fact that Model 1 converges to
global optimal, the effect will fade out after sev-
eral iterations. In contrast, our method impose
a hard constraint on the alignments. Also, our
method can be used side-by-side with the method
in GIZA++.
4 Experiments
4.1 Experiments on manual link selectors
We designed a set of controlled experiments to
show that the algorithm acts as desired. Particu-
larly, with a number of manual alignment links fed
into the aligner, we should be able to correct more
misaligned alignment links than the manual align-
ment links through better alignment models. Also,
carefully selected alignment links should outper-
form randomly selected alignment links.
We used Chinese-English and Arabic-English
manually aligned corpus in the experiments. Ta-
ble 2 shows the statistics of the corpora:
Number of Num. of Words Alignment
Sentences Source Target Links
Ch-En 21,863 424,683 524,882 687,247
Ar-En 29,876 630,101 821,938 830,349
Table 2: Corpus statistics of the corpora
First the corpora is trained as unlabelled data
to serve as baselines, and then we feed a portion
of alignment links into the proposed aligner. We
experimented with different methods of choosing
alignment links and adjust the number of links vis-
ible to the aligner. Because of the limitations of
the IBM Models, such as no N-to-1 alignments,
the manual alignment is not reachable from ei-
ther direction. We then define the best align-
ment that the IBM Models can express ?oracle
alignment?, which can be obtained by dropping
all N-to-1 links from manual alignment. Also, to
show the upper-bound performance, we feed all
the manual alignment links to our aligner, and call
the alignment ?force alignment?. Table 3 shows
the alignment qualities of oracle alignments and
force alignments of both systems. For force align-
ments, we show the scores with and without im-
plicit empty links derived from the manual align-
ment.3 The oracle alignments are the performance
upper-bounds of all aligners under IBM Model?s
1-to-N assumption. The result from Table 3 shows
that, if we include the derived empty links, the
force alignments are close to the oracle results.
Then the question is how fast we can approach the
upper-bound.
To answer the question, we gradually increase
the number of links being fed into the aligner. In
these experiments the seeds for random number
generator are fixed so that the links selected in
later experiments are always superset of that of
earlier experiments. The comparison of the align-
ment quality is shown in Figure 3 and 4. To show
the actual improvement brought in by the algo-
rithm instead of the manual alignment links them-
selves, we compare the alignment results of the
proposed method with directly fixing the align-
ments from original GIZA++ training. By fix-
ing alignments we mean that first the conventional
3We can derive empty links if one word has no alignment
link from the full alignment we have access to.
6
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
100
Number of Links
Pre
cisio
n
Precision?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
48
50
52
54
56
58
60
62
Number of Links
Rec
all
Recall?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
40
Number of Links
AER
AER?Number of links (Chinese?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
100
Number of Links
Pre
cisio
n
Precision?Number of links (English?Chinese)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
60
65
70
75
80
Number of Links
Rec
all
Recall?Number of links (English?Chinese)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
40
Number of Links
AER
AER?Number of links (English?Chinese)
 
 NNWNDFFRFD
0 1 2 3 4 5 6
x 105
70
75
80
85
90
95
Number of Links
Pre
cisio
n
Precision?Number of links (Combined Ch/En)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
60
65
70
75
80
85
90
Number of Links
Rec
all
Recall?Number of links (Combined Ch/En)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (Combined Ch/En)
 
 NNWNDFFRFD
Figure 3: Alignment qualities of Chinese-English word alignment, NN: Random selector without empty
links, WN: Random seletor with empty links, DF: Disagreement selector, FR: Directly fixing the align-
ments with random selector, FD: Directly fixing the alignments with disagreement selector. Each row
shows the precision, recall and AER when applying different number of manual alignment links. The
three rows are for Chinese-English, English-Chinese and heuristically symmetrized alignments (grow-
diag-final-and) accordingly.
GIZA++ training is performed and then we add the
manual alignment links to the resulting alignment.
In case that the 1-to-N restriction of the IBM Mod-
els is violated, we keep the manual alignment links
and remove the links from GIZA++.
We show the results as FR (dashed curves with
diamond markers) and FD (dashed curves with
square markers) in the plots, corresponding to
alignments selected from the random link selector
and the disagreement-based link selector. These
two curves serve as baseline, and the gaps between
the FR curves and the WN curves (dotted curves
with cross markers) and the gaps between the FD
curves and the DF curves (solid curves) show the
amount of improvement we achieved using the
method in addition to the manual alignment links.
Therefore, they represent the effectiveness of the
proposed alignment approach. Also the gaps be-
tween DF and WN curves indicate the differences
in the performance of two link selectors.
The plots illustrate that when the number of
links is small, the WN and DF curves are al-
ways higher than the FR/FD curves. It proves
that our system does not just fix the links pro-
vided by manual alignments, instead the informa-
tion propagates to other links. The largest gap
between FD and DF is 8% absolute in com-
bined alignment of Chinese-English system with
200,000 manual alignment links. Also, we can
see that the disagreement-based link selector (DF)
always outperform the random selector (WN). It
suggest that, if we want to harvest manual align-
ment links, it is possible to apply active learning
method to minimize the user labelling effort while
maximizing the improvement on word alignment
qualities. Especially, notice that in the lower parts
7
0 1 2 3 4 5 6 7 8
x 105
60
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
55
60
65
70
Number of Links
Rec
all
Recall?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
40
45
Number of Links
AER
AER?Number of links (Arabic?English)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
60
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (English?Arabic)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
70
75
80
85
90
Number of Links
Rec
all
Recall?Number of links (English?Arabic)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (English?Arabic)
 
 NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
65
70
75
80
85
Number of Links
Pre
cisio
n
Precision?Number of links (Combined En/Ar)
 
 NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
70
75
80
85
90
95
100
Number of Links
Rec
all
Recall?Number of links (Combined En/Ar)
 
 
NNWNDFFRFD
0 1 2 3 4 5 6 7 8
x 105
5
10
15
20
25
30
35
Number of Links
AER
AER?Number of links (Combined En/Ar)
 
 
NNWNDFFRFD
Figure 4: Alignment qualities of Arabic-English word alignment, NN: Random selector without empty
links, WN: Random selector with empty links, DF: Disagreement selector, FR: Directly fixing the align-
ments with random selector, FD: Directly fixing the alignments with disagreement selector. Each row
shows the precision, recall and AER when applying different number of manual alignment links. The
three rows are for Arabic-English, English-Arabic and heuristically symmetrized alignments (grow-diag-
final-and) accordingly.
of the curves, with a small number of manual
alignment links, we can already improve the align-
ment quality by a large gap. This observation can
benefit low-resource word alignment tasks.
4.2 Experiment on using heuristics
The previous experiment shows the potential of
using the method on manual aligned corpus, here
we demonstrate another possible usage of the pro-
posed method that uses heuristics to generate high-
precision-low-recall links. We use LDC Chinese-
English dictionary as an example. The entries with
single Chinese character and more than six En-
glish words are filtered out. The heuristic-based
aligner yields alignment that has 79.48% preci-
sion and 17.36% recall rate on the test set we used
in 4.1. By applying the links as manual links,
we run proposed method on the same Chinese-
English test data presented in 4.1, and the results
of alignment qualities are shown in 5. As we can
see, the AER reduced by 1.64 from 37.23 to 35.61
on symmetrized alignment.
We also experimented with translation tasks
with moderate-size corpus. We used the corpus
LDC2006G05 with 25 million words. The train-
ing scheme is the same as previous experiments,
where the filtered LDC dictionary is used. After
word alignment, standard Moses phrase extraction
tool (Och and Ney, 2004) is used to build the trans-
lation models and finally Moses (Koehn et. al.,
2007) is used to tune and decode.
We tune the system on the NIST MT06 test
set (1664 sentences), and test on the MT08 (1357
sentences) and the DEV075 (1211 sentences) test
sets, which are further divided into two sources
(newswire and web data). A trigram language
5It is a test set used by GALE Rosseta Team
8
MT02 MT03 MT04 MT05 MT08-NW MT08-WB Dev07NW Dev07WB
Baseline 28.87 27.82 30.08 26.77 25.09 17.72 24.88 21.76
Dict-Link 29.59 27.67 31.01 27.13 25.14 17.96 25.51 21.88
Table 4: Comparison of the performance of baseline and the alignment generated by new aligner with
dictionary links in BLEU scores
Precision Recall AER
ORL 100.00 62.61 23.00
Ch-En F/NE 89.25 62.47 26.50
F/WE 99.59 62.47 23.22
ORL 100.00 80.98 10.51
En-Ch F/NE 93.49 80.79 13.32
F/WE 99.82 80.79 10.70
F/NE 90.79 87.49 10.89Comb F/WE 99.78 87.23 6.92
ORL 100.00 72.07 16.23
Ar-En F/NE 82.46 72.00 23.13
F/WE 94.25 72.00 18.36
ORL 100.00 90.14 5.18
En-Ar F/NE 79.81 90.06 15.37
F/WE 93.27 90.10 8.34
F/NE 78.91 93.07 14.59Comb F/WE 94.64 93.21 6.08
Table 3: Alignment quality of oracle alignment
and force alignment, the rows with ?ORL? in the
second column are oracle alignments, ?F/NE? and
?F/WE? represent force alignments with empty
links and without empty links correspondingly.
For ?F/NE? and ?F/WE? we also listed the
scores of heuristically symmetrized alignment4.
(?Comb?)
model trained from GigaWord V1 and V2 cor-
pora is used. Table 4 shows the comparison of
the performances on BLEU metric (Papineni et
al., 2002). As we can observe from the results,
the proposed method outperforms the baseline on
all test sets except MT03, and has significant6
improvement on MT02 (+0.72), MT04 (+0.93),
and Dev07NW(+0.63). The average improvement
across all test sets is 0.35 BLEU points.
As a summary, the purpose of the this experi-
ment is to demonstrate an important characteris-
tic of the proposed method. Even with imperfect
manual alignment links, we can get better align-
ment by applying our method. This characteristic
opens a possibility to integrate other more sophis-
ticated aligners.
5 Conclusion
In this study, our major contribution is a novel
generative model extended from IBM Model 4 to
6We used the confidence measurement described in
(Zhang and Vogel, 2004)
Chinese-English
Precision Recall AER
Baseline 68.22 46.88 44.43
Dict-Link 69.93 48.28 42.88
English-Chinese
Precision Recall AER
Baseline 65.35 55.05 40.24
Dict-Link 66.70 56.45 38.85
grow-diag-final-and
Precision Recall AER
Baseline 69.15 57.47 37.23
Dict-Link 70.11 59.54 35.61
Table 5: Comparison on alignment error rate by
using alignment links generated by dictionaries
utilize partial manual alignments. The proposed
method enables us to efficiently enforce subtle
alignment constraints into the EM training. We
performed experiments on manually aligned cor-
pora to prove the validity. We also demonstrated
using the method with simple heuristics to boost
the translation quality on moderate size unlabelled
corpus. The results show that our method is ef-
fective in promoting the word alignment quali-
ties with small amounts of partial alignments and
with high-precision-low-recall heuristics. Also the
method of using dictionary to generate manual
alignment links showed an average improvement
of 0.35 BLEU points across 8 test sets.
The algorithm has small impact on the speed of
GIZA++, and can easily be added to current multi-
thread implementation of GIZA++. Therefore it is
suitable for large scale training.
Future work includes applying the proposed ap-
proach on low resource language pairs and in-
tegrating the algorithm with other rule-based or
discriminative aligners that can generate high-
precision-low-recall partial alignments.
Acknowledgement
This work is supported by DARPA GALE
project and NSF CluE project.
9
References
V. Ambati, S. Vogel, and J. Carbonell. 2010. Ac-
tive semi-supervised learning for improving word
alignment. In Proceedings of the NAACL HLT 2010
Workshop on Active Learning for Natural Language
Processing.
S. Arora, E. Nyberg, and C. P. Rose?. 2009. Estimat-
ing annotation cost for active learning in a multi-
annotator environment. In HLT ?09: Proceedings of
the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 18?26.
P. Blunsom and T. Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65?72.
P. F. Brown et. al. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation.
In Computational Linguistics, volume 19(2), pages
263?331.
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-and
sentence-aligned parallel corpora. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, pages 175?183.
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons me-
chanical turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 286?295.
A. Fraser and D. Marcu. 2006. Semi-supervised
training for statistical word alignment. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 769?776.
A. Fraser and D. Marcu. 2007. Getting the struc-
ture right for word alignment: LEAF. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 51?60.
Q. Gao and S. Vogel. 2008. Parallel implementations
of word alignment tool. In Proceedings of the ACL
2008 Software Engineering, Testing, and Quality As-
surance Workshop, pages 49?57.
Q. Gao and S. Vogel. 2010. Consensus versus exper-
tise : A case study of word alignment with mechan-
ical turk. In NAACL 2010 Workshop on Creating
Speech and Language Data With Mechanical Turk,
pages 30?34.
F. Guzman, Q. Gao, and S. Vogel. 2009. Reassessment
of the role of phrase extraction in pbsmt. In The
twelfth Machine Translation Summit.
F. Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 932?940. Association
for Computational Linguistics.
A. Ittycheriah and S. Roukos. 2005. A maximum en-
tropy word aligner for arabic-english machine trans-
lation. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 89?96.
P. Koehn et. al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180.
Y. Liu, Q. Liu, and S. Lin. 2005. Log-linear models
for word alignment. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 459?466.
R. C Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88.
J. Niehues and S. Vogel. 2008. Discriminative word
alignment via alignment matrix modeling. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18?25.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. In Compu-
tational Linguistics, volume 1:29, pages 19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. In Com-
putational Linguistics, volume 30, pages 417?449.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL?02, pages
311?318, Philadelphia, PA, July.
B. Taskar, S. Lacoste-Julien, and Klein D. 2005. A dis-
criminative matching approach to word alignment.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 73?80.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM
based word alignment in statistical machine trans-
lation. In Proceedings of 16th International Confer-
ence on Computational Linguistics), pages 836?841.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. In Proceedings of The 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation, October.
10
A Multi-layer Chinese Word Segmentation System Optimized for
Out-of-domain Tasks
Qin Gao
Language Technologies Institute
Carnegie Mellon University
qing@cs.cmu.edu
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
stephan.vogel@cs.cmu.edu
Abstract
State-of-the-art Chinese word segmenta-
tion systems have achieved high perfor-
mance when training data and testing data
are from the same domain. However, they
suffer from the generalizability problem
when applied on test data from different
domains. We introduce a multi-layer Chi-
nese word segmentation system which can
integrate the outputs from multiple hetero-
geneous segmentation systems. By train-
ing a second layer of large margin clas-
sifier on top of the outputs from several
Conditional Random Fields classifiers, it
can utilize a small amount of in-domain
training data to improve the performance.
Experimental results show consistent im-
provement on F1 scores and OOV recall
rates by applying the approach.
1 Introduction
The Chinese word segmentation problem has been
intensively investigated in the past two decades.
From lexicon-based methods such as Bi-Directed
Maximum Match (BDMM) (Chen et al, 2005) to
statistical models such as Hidden Markove Model
(HMM) (Zhang et al, 2003), a broad spectrum
of approaches have been experimented. By cast-
ing the problem as a character labeling task, se-
quence labeling models such as Conditional Ran-
dom Fields can be applied on the problem (Xue
and Shen, 2003). State-of-the-art CRF-based sys-
tems have achieved good performance. However,
like many machine learning problems, generaliz-
ability is crucial for a domain-independent seg-
mentation system. Because the training data usu-
ally come from limited domains, when the domain
of test data is different from the training data, the
results are still not satisfactory.
A straight-forward solution is to obtain more la-
beled data in the domain we want to test. However
this is not easily achievable because the amount
of data needed to train a segmentation system are
large. In this paper, we focus on improving the
system performance by using a relatively small
amount of manually labeled in-domain data to-
gether with larger out-of-domain corpus1. The
effect of mingling the small in-domain data into
large out-of-domain data may be neglectable due
to the difference in data size. Hence, we try to
explore an alternative way that put a second layer
of classifier on top of the segmentation systems
built on out-of-domain corpus (we will call them
sub-systems). The classifier should be able to uti-
lize the information from the sub-systems and op-
timize the performance with a small amount of in-
domain data.
The basic idea of our method is to integrate
a number of different sub-systems whose per-
formance varies on the new domain. Figure 1
demonstrates the system architecture. There are
two layers in the system. In the lower layer,
the out-of-domain corpora are used, together with
other resources to produce heterogeneous sub-
systems. In the second layer the outputs of the
sub-systems in the first layer are treated as input
to the classifier. We train the classifier with small
in-domain data. All the sub-systems should have
1From this point, we use the term out-of-domain corpus
to refer to the general and large training data that are not
related to the test domain, and the term in-domain corpus
to refer to small amount of data that comes from the same
domain of the test data
reasonable performance on all domains, but their
performance on different domains may vary. The
job of the second layer is to find the best decision
boundary on the target domain, in presence of all
the decisions made by the sub-systems.
Num
ber
?
Tag
?Fea
ture
Clas
sifie
r?1
Trai
nin
g?da
ta?
Cha
ract
er?
Typ
e?Fe
atu
re
g
(in?
dom
ain)
Ent
rop
y?
Fea
ture
Clas
sifie
r?2
Trai
nin
gda
ta
Clas
sifie
r3
Inte
gra
ted
?
Trai
nin
g?da
ta?
(ou
t?of
?
dom
ain)
Clas
sifie
r?3
clas
sifie
r
Wo
rd?l
ist?1
Clas
sifie
r?4
Wo
rd?l
ist?2
Clas
sifie
r5
Wo
rd?l
ist?2
Clas
sifie
r?5
Figure 1: The architecture of the system, the first
layer (sub-systems) is trained on general out-of-
domain corpus and various resources, while the
second layer of the classifier is trained on in-
domain corpus.
Conditional Random Fields (CRF) (Lafferty et
al., 2001) has been applied on Chinese word seg-
mentation and achieved high performance. How-
ever, because of its conditional nature the small
amount of in-domain corpus will not significantly
change the distributions of the model parame-
ters trained on out-of-domain corpus, it is more
suitable to be used in the sub-systems than in
the second-layer classifier. Large margin models
such as Support Vector Machine (SVM) (Vapnik,
1995) can be trained on small corpus and gener-
alize well. Therefore we chose to use CRF in
building sub-systems and SVM in building the
second-layer. We built multiple CRF-based Chi-
nese word segmentation systems using different
features, and then use the marginal probability of
each tag of all the systems as features in SVM.
The SVM is then trained on small in-domain cor-
pus, results in a decision hyperplane that mini-
mizes the loss in the small training data. To in-
tegrate the dependencies of output tags, we use
SVM-HMM (Altun et al, 2003) to capture the in-
teractions between tags and features. By apply-
ing SVM-HMM we can bias our decision towards
most informative CRF-based system w.r.t. the tar-
get domain. Our methodology is similar to (Co-
hen and Carvalho, 2005), who applied a cross-
validation-like method to train sequential stacking
models, while we directly use small amount of in-
domain data to train the second-layer classifiers.
The paper is organized as follows, first we will
discuss the CRF-based sub-systems we used in
section 2, and then the SVM-based system com-
bination method in section 3. Finally, in section 4
the experimental results are presented.
2 CRF-based sub-systems
In this section we describe the sub-systems we
used in system. All of the sub-systems are based
on CRF with different features. The tag set we
use is the 6-tag (B1, B2, B3, M, E, S) set pro-
posed by Zhao et al(2006). All of the sub-systems
use the same tag set, however as we will see later,
the second-layer classifier in our system does not
require the sub-systems to have a common tag
set. Also, all of the sub-systems include a com-
mon set of character features proposed in (Zhao
and Kit, 2008). The offsets and concatenations
of the six n-gram features (the feature template)
are: C?1, C0, C1, C?1C0, C0C1, C?1C1. In the
remaining part of the section we will introduce
other features that we employed in different sub-
systems.
2.1 Character type features
By simply classify the characters into four types:
Punctuation (P), Digits (D), Roman Letters (L)
and Chinese characters (C), we can assign char-
acter type tags to every character. The idea is
straight-forward. We denote the feature as CTF .
Similar to character feature, we also use differ-
ent offsets and concatenations for character type
features. The feature template is identical to
character feature, i.e. CTF?1, CTF0, CTF1,
CTF?1CTF0, CTF0CTF1, CTF?1CTF1 are
used as features in CRF training.
2.2 Number tag feature
Numbers take a large portion of the OOV words,
which can easily be detected by regular expres-
sions or Finite State Automata. However there
are often ambiguities on the boundary of numbers.
Therefore, instead of using detected numbers as
final answers, we use them as features. The num-
ber detector we developed finds the longest sub-
strings in a sentence that are:
? Chinese Numbers (N)
? Chinese Ordinals (O)
? Chinese Dates (D)
For each character of the detected num-
bers/ordinal/date, we assign a tag that reflects the
position of the character in the detected num-
ber/ordinal/date. We adopt the four-tag set (B, M,
E, S). The position tags are appended to end of
the number/ordinal/date tags to form the number
tag feature of that character. I.e. there are totally
13 possible values for the number tag feature, as
listed in Table 1.2
Number Ordinal Date Other
Begin NB OB DB
Middle NM OM DM
End NE OE DE XX
Single NS OS? DS?
Table 1: The feature values used in the number tag
feature, note that OS and DS are never observed
because there is no single character ordinal/date
by our definition.
Similar to character feature and character type
feature, the feature template mention before is
also applied on the number tag feature. We de-
note the number tag features as NF .
2.3 Conditional Entropy Feature
We define the Forward Conditional Entropy of
a character C by the entropy of all the charac-
ters that follow C in a given corpus, and the
Backward Conditional Entropy as the entropy
of all the characters that precede C in a given
corpus. The conditional entropy can be com-
puted easily from a character bigram list gener-
ated from the corpus. Assume we have a bigram
2Two of the tags, OS and DS are never observed.
list B = {B1, B2, ? ? ? , BN}, where every bigram
entry Bk = {cik , cjk , nk} is a triplet of the two
consecutive characters cik and cjk and the count of
the bigram in the corpus, nk. The Forward Condi-
tional Entropy of the character C is defined by:
Hf (C) :=
?
cik=C
nk
Z
log
nk
Z
where Z =
?
cik=C
nk is the normalization fac-
tor.
And the Backward Conditional Entropy can be
computed similarly.
We assign labels to every character based on
the conditional entropy of it. If the conditional
entropy value is less than 1.0, we assign fea-
ture value 0 to the character, and for region
[1.0, 2.0), we assign feature value 1. Similarly we
define the region-to-value mappings as follows:
[2.0, 3.5) ? 2, [3.5, 5.0) ? 4, [5.0, 7.0) ? 5,
[7.0,+?) ? 6. The forward and backward con-
ditional entropy forms two features. We will refer
to these features as EF .
2.4 Lexical Features
Lexical features are the most important features to
make sub-systems output different results on dif-
ferent domains. We adopt the definition of the fea-
tures partially from (Shi and Wang, 2007). In our
system we use only the Lbegin(C0) and Lend(C0)
features, omitting the LmidC0 feature. The two
features represent the maximum length of words
found in the lexicon that contain the current char-
acter as the first or last character, correspondingly.
For feature values equal or greater than 6, we
group them into one value.
Although we can find a number of Chinese lex-
icons available, they may or may not be gener-
ated according to the same standard as the train-
ing data. Concatenating them into one may bring
in noise and undermine the performance. There-
fore, every lexicon will generate its own lexical
features.
3 SVM-based System Combination
Generalization is a fundamental problem of Chi-
nese word segmentation. Since the training data
may come from different domains than the test
data, the vocabulary and the distribution can also
be different. Ideally, if we can have labeled data
from the same domain, we can train segmenters
specific to the domain. However obtaining suffi-
cient amount of labeled data in the target domain
is time-consuming and expensive. In the mean
time, if we only label a small amount of data in the
target domain and put them into the training data,
the effect may be too small because the size of
out-of-domain data can overwhelm the in-domain
data.
In this paper we propose a different way of
utilizing small amount of in-domain corpus. We
put a second-layer classifier on top of the CRF-
based sub-systems, the output of CRF-based sub-
systems are treated as features in an SVM-HMM
(Altun et al, 2003) classifier. We can train the
SVM-HMM classifier on a small amount of in-
domain data. The training procedure can be
viewed as finding the optimal decision boundary
that minimize the hinge loss on the in-domain
data. Because the number of features for SVM-
HMM is significantly smaller than CRF, we can
train the model with as few as several hundred
sentences.
Similar to CRF, the SVM-HMM classifier still
treats the Chinese word segmentation problem as
character tagging. However, because of the limi-
tation of training data size, we try to minimize the
number of classes. We chose to adopt the two-tag
set, i.e. class 1 indicates the character is the end of
a word and class 2 means otherwise. Also, due to
limited amount of training data, we do not use any
character features, instead, the features comes di-
rectly from the output of sub-systems. The SVM-
HMM can use any real value features, which en-
ables integration of a wide range of segmenters.
In this paper we use only the CRF-based seg-
menters, and the features are the marginal prob-
abilities (Sutton and McCallum, 2006) of all the
tags in the tag set for each character. As an ex-
ample, for a CRF-based sub-system that outputs
six tags, it will output six features for each char-
acter for the SVM-HMM classifier, corresponding
to the marginal probability of the character given
the CRF model. The marginal probabilities for
the same tag (e.g. B1, S, etc) come from differ-
ent CRF-based sub-systems are treated as distinct
features.
Features Lexicons
S1 CF, CTF None
S2 CF, NF ADSO, CTB6
S3 CF, CTF, NF ADSO
S4 CF, CTF, NF, EF ADSO, CTB6
S5 CF, EF None
S6 CF, NF None
S7 CF, CTF ADSO
S8 CF, CTF CTB6
Table 2: The configurations of CRF-based sub-
systems. S1 to S4 are used in the final submission
of the Bake-off, S5 through S8 are also presented
to show the effects of individual features.
When we encounter data from a new domain,
we first use one of the CRF-based sub-system to
segment a portion of the data, and manually cor-
rect obvious segmentation errors. The manually
labeled data are then processed by all the CRF-
based sub-systems, so as to obtain features of ev-
ery character. After that, we train the SVM-HMM
model using these features.
During decoding, the Chinese input will also be
processed by all of the CRF-based sub-systems,
and the outputs will be fed into the SVM-HMM
classifier. The final decisions of word boundaries
are based solely on the classified labels of SVM-
HMM model.
For the Bake-off system, we labeled two hun-
dred sentences in each of the unsegmented train-
ing set (A and B). Since only one submission is
allowed, the SVM-HMM model of the final sys-
tem was trained on the concatenation of the two
training sets, i.e. four hundred sentences.
The CRF-based sub-systems are trained using
CRF++ toolkit (Kudo, 2003), and the SVM-HMM
trained by the SVMstruct toolkit (Joachims et al,
2009).
4 Experiments
To evaluate the effectiveness of the proposed sys-
tem combination method, we performed two ex-
periments. First, we evaluate the system combina-
tion method on provided training data in the way
that is similar to cross-validation. Second, we ex-
perimented with training the SVM-HMM model
with the manually labeled data come from cor-
Micro-Average Macro-Average
P R F1 OOV-R P R F1 OOV-R
S1 0.962 0.960 0.961 0.722 0.962 0.960 0.960 0.720
S2 0.965 0.966 0.966 0.725 0.965 0.966 0.966 0.723
S3 0.966 0.967 0.967 0.731 0.966 0.967 0.967 0.729
S4 0.968 0.969 0.968 0.731 0.967 0.969 0.969 0.729
S5 0.962 0.960 0.961 0.720 0.962 0.960 0.960 0.718
S6 0.963 0.961 0.962 0.730 0.963 0.961 0.961 0.729
S7 0.966 0.967 0.966 0.723 0.966 0.967 0.967 0.720
S8 0.963 0.960 0.962 0.727 0.963 0.960 0.960 0.726
CB 0.969 0.969 0.969 0.741 0.969 0.969 0.969 0.739
Table 3: The performance of individual sub-systems and combined system. The Micro-Average results
come from concatenating all the outputs of the ten-fold systems and then compute the scores, and the
Macro-Average results are calculated by first compute the scores in every of the ten-fold systems and
then average the scores.
Set A Set B
P R F1 OOV-R P R F1 OOV-R
S1 0.925 0.920 0.923 0.625 0.936 0.938 0.937 0.805
S2 0.934 0.934 0.934 0.641 0.941 0.930 0.935 0.751
S3 0.940 0.937 0.938 0.677 0.938 0.926 0.932 0.752
S4 0.942 0.940 0.941 0.688 0.944 0.929 0.936 0.776
CB1 0.943 0.941 0.942 0.688 0.948 0.936 0.942 0.794
CB2 0.941 0.940 0.941 0.692 0.939 0.949 0.944 0.821
CB3 0.943 0.939 0.941 0.699 0.950 0.950 0.950 0.820
Table 4: The performance of individual systems and system combination on Bake-off test data, CB1,
CB2, and CB3 are system combination trained on labeled data from domain A, B, and the concatenation
of the data from both domains.
responding domains, and tested the resulting sys-
tems on the Bake-off test data.
For experiment 1, We divide the training set
into 11 segments, segment 0 through 9 contains
1733 sentences, and segment 10 has 1724 sen-
tence. We perform 10-fold cross-validation on
segment 0 to 9. Every time we pick one segment
from segment 0 to 9 as test set and the remain-
ing 9 segments are used to train CRF-based sub-
systems. Segment 10 is used as the training set for
SVM-HMM model. The sub-systems we used is
listed in Table 2.
In Table 3 we provide the micro-level and
macro-level average of performance the ten-fold
evaluation, including both the combined system
and all the individual sub-systems. Because
the system combination uses more data than its
sub-systems (segment 10), in order to have a
fair comparison, when evaluating individual sub-
systems, segment 10 is appended to the training
data of CRF model. Therefore, the individual sub-
systems and system combination have exactly the
same set of training data.
As we can see in the results in Table 3, the sys-
tem combination method (Row CB) has improve-
ment over the best sub-system (S4) on both F1
and OOV recall rate, and the OOV recall rate im-
proved by 1%. We should notice that in this exper-
iment we actually did not deal with any data from
different domains, the advantage of the proposed
method is therefore not prominent.
We continue to present the experiment results
of the second experiment. In the experiment
we labeled 200 sentences from each of the unla-
beled bake-off training set A and B, and trained
the SVM-HMM model on the labeled data. We
compare the performance of the four sub-systems
and the performance of the system combination
method trained on: 1) 200 sentences from A, 2)
200 sentences from B, and 3) the concatenation
of the 400 sentences from both A and B. We show
the scores on the bake-off test set A and B in Table
4.
As we can see from the results in Table 4, the
system combination method outperforms all the
individual systems, and the best performance is
observed when using both of the labeled data from
domain A and B, which indicates the potential of
further improvement by increasing the amount of
in-domain training data. Also, the individual sub-
systems with the best performance on the two do-
mains are different. System 1 performs well on
Set B but not on Set A, so does System 4, which
tops on Set A but not as good as System 1 on Set
B. The system combination results appear to be
much more stable on the two domains, which is a
preferable characteristic if the segmentation sys-
tem needs to deal with data from various domains.
5 Conclusion
In this paper we discussed a system combina-
tion method based on SVM-HMM for the Chinese
word segmentation problem. The method can uti-
lize small amount of training data in target do-
mains to improve the performance over individ-
ual sub-systems trained on data from different do-
mains. Experimental results show that the method
is effective in improving the performance with a
small amount of in-domain training data.
Future work includes adding more heteroge-
neous sub-systems other than CRF-based ones
into the system and investigate the effects on the
performance. Automatic domain adaptation for
Chinese word segmentation can also be an out-
come of the method, which may be an interesting
research topic in the future.
References
Altun, Yasemin, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector
machines. In Proceedings of International Confer-
ence on Machine Learning (ICML).
Chen, Yaodong, Ting Wang, and Huowang Chen.
2005. Using directed graph based bdmm algorithm
for chinese word segmentation. pages 214?217.
Cohen, William W. and Vitor Carvalho. 2005. Stacked
sequential learning. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJ-CAI).
Joachims, Thorsten, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural svms. Machine Learning, 77(1):27?59.
Kudo, Taku. 2003. CRF++: Yet another crf toolkit.
Web page: http://crfpp.sourceforge.net/.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning (ICML).
Shi, Yanxin and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence (IJ-CAI).
Sutton, Charles and Andrew McCallum, 2006. Intro-
duction to Statistical Relational Learning, chapter
An Introduction to Conditional Random Fields for
Relational Learning. MIT Press.
Vapnik, Vladimir N. 1995. The Nature of Statistical
Learning Theory. Springer.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
the second SIGHAN workshop on Chinese language
processing, pages 176?179.
Zhang, Huaping, Qun Liu, Xueqi Cheng, Hao Zhang,
and Hongkui Yu. 2003. Chinese lexical analysis us-
ing hierarchical hidden markov model. In Proceed-
ings of the second SIGHAN workshop on Chinese
language processing, pages 63?70.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In The Sixth SIGHAN Workshop on
Chinese Language Processing (SIGHAN-6), pages
106?111.
Zhao, Hai, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in chinese
word segmentation via conditional random field
modeling. In Proceedings of the 20th Pacific Asia
Conference on Language, Information and Compu-
tation (PACLIC-20), pages 87?94.
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 107?115,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Utilizing Target-Side Semantic Role Labels to Assist Hierarchical
Phrase-based Machine Translation
Qin Gao and Stephan Vogel
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
{qing, stephan.vogel}@cs.cmu.edu
Abstract
In this paper we present a novel approach
of utilizing Semantic Role Labeling (SRL)
information to improve Hierarchical Phrase-
based Machine Translation. We propose an
algorithm to extract SRL-aware Synchronous
Context-Free Grammar (SCFG) rules. Con-
ventional Hiero-style SCFG rules will also be
extracted in the same framework. Special con-
version rules are applied to ensure that when
SRL-aware SCFG rules are used in deriva-
tion, the decoder only generates hypotheses
with complete semantic structures. We per-
form machine translation experiments using 9
different Chinese-English test-sets. Our ap-
proach achieved an average BLEU score im-
provement of 0.49 as well as 1.21 point reduc-
tion in TER.
1 Introduction
Syntax-based Machine Translation methods have
achieved comparable performance to Phrase-based
systems. Hierarchical Phrase-based Machine Trans-
lation, proposed by Chiang (Chiang, 2007), uses a
general non-terminal label X but does not use lin-
guistic information from the source or the target lan-
guage. There have been efforts to include linguis-
tic information into machine translation. Liu et al
(2006) experimented with tree-to-string translation
models that utilize source side parse trees, and later
improved the method by using the Packed Forest
data structure to reduce the impact of parsing errors
(Liu and Huang, 2010). The string-to-tree (Galley
et al 2006) and tree-to-tree (Chiang, 2010) meth-
ods have also been the subject of experimentation, as
well as other formalisms such as Dependency Trees
(Shen et al, 2008).
One problem that arises by using full syntactic la-
bels is that they require an exact match of the con-
stituents in extracted phrases, so it faces the risk
of losing coverage of the rules. SAMT (Zollmann
and Venugopal, 2006) and Tree Sequence Align-
ment (Zhang et al, 2008) are proposed to amend this
problem by allowing non-constituent phrases to be
extracted. The reported results show that while uti-
lizing linguistic information helps, the coverage is
more important (Chiang, 2010). When dealing with
formalisms such as semantic role labeling, the cov-
erage problem is also critical. In this paper we fol-
low Chiang?s observation and use SRL labels to aug-
ment the extraction of SCFG rules. I.e., the formal-
ism provides additional information and more rules
instead of restrictions that remove existing rules.
This preserves the coverage of rules.
Recently there has been increased attention to use
semantic information in machine translation. Liu
and Gildea (2008; 2010) proposed using Semantic
Role Labels (SRL) in their tree-to-string machine
translation system and demonstrated improvement
over conventional tree-to-string methods. Wu and
Fung (2009) developed a framework to reorder the
output using information from both the source and
the target SRL labels. In this paper, we explore an
approach of using the target side SRL information
in addition to a Hierarchical Phrase-based Machine
Translation framework. The proposed method ex-
tracts initial phrases with two different heuristics:
The first heuristic is used to extract rules that have
a general left-hand-side (LHS) non-terminal tag X ,
107
Second we must build a flood prevention system , strengthen pre-flood inspections and implement flood prevention measures 
arg0 mod 
arg0 mod 
arg0 mod 
pred 
pred 
arg1 
arg1 
pred 
arg1 
Figure 1: Example of predicate-argument structure in a sentence
i.e., Hiero rules. The second will extract phrases that
contain information of SRL structures. The pred-
icate and arguments that the phrase covers will be
represented in the LHS non-terminal tags. After
that, we obtain rules from the initial phrases in the
same way as the Hiero extraction algorithm, which
replaces nesting phrases with their corresponding
non-terminals.
By applying this scheme, we will obtain rules that
contain SRL information, without sacrificing the
coverage of rules. In this paper, we call such rules
SRL-aware SCFG rules. During decoding, both the
conventional Hiero-style SCFG rules with general
tag X and SRL-aware SCFG rules are used in a syn-
chronous Chart Parsing algorithm. Special conver-
sion rules are introduced to ensure that whenever
SRL-aware SCFG rules are used in the derivation,
a complete predicate-argument structure is built.
The main contributions are:
1. an algorithm to extract SRL-aware SCFG rules
using target side SRL information.
2. an approach to use Hiero rules side-by-side
with information-rich SRL-aware SCFG rules,
which improves the quality of translation re-
sults.
In section 2 we briefly review SCFG-based ma-
chine translation and SRL. In section 3, we describe
the SRL-aware SCFG rules. Section 4 provides
the detail of the rule extraction algorithm. Section
5 presents two alternative methods how to utilize
the SRL information. The experimental results are
given in Section 6, followed by analysis and conclu-
sion in Section 7.
2 Background
2.1 Hierarchical Phrase-based Machine
Translation
Proposed by Chiang (2005), the Hierarchical
Phrase-based Machine Translation model (com-
monly known as the Hiero model) has achieved re-
sults comparable, if not superior, to conventional
Phrase-based approaches. The basic idea is to treat
the translation as a synchronous parsing problem.
Using the source side terminals as input, the decoder
tries to build a parse tree and synchronously generate
target side terminals. The rules that generates such
synchronous parse trees are in the following form:
X ? (f1 X1 f2 X2 f3, e1 X2 e2 X1 e3)
where X1 and X2 are non-terminals, and the sub-
scripts represents the correspondence between the
non-terminals. In Chiang?s Hiero model all non-
terminals will have the same tag, i.e. X . The formal-
ism, known as Synchronous Context-Free Grammar
(SCFG) does not require the non-terminals to have a
unique tag name. Instead, they may have tags with
syntactic or semantic meanings, such as NP or V P .
2.2 Semantic Role Labeling and Machine
Translation
The task of semantic role labeling is to label the se-
mantic relationships between predicates and argu-
ments. This relationship can be treated as a depen-
dency structure called ?Predicate-Argument Struc-
ture? (PA structure for short). Figure 1 depicts ex-
amples of multiple PA structures in a sentence. The
lines indicate the span of the predicates and argu-
ments of each PA structure, and the tags attached to
these lines show their role labels.
Despite the similarity between PA structure and
dependency trees, SRL offers a structure that posses
better granularity. Instead of trying to analyze all
links between words in the sentences, PA structure
only deals with the relationships between verbs and
constituents that are arguments of the predicates.
This information is useful in preserving the mean-
ing of the sentence during the translation process.
However, using semantic role representation in
machine translation has its own set of problems.
108
First, we face the coverage problem. Some sen-
tences might not have semantic structure at all, if,
for instance they consist of single noun phrases or
contain only rare predicates that are not covered by
the semantic role labeler. Moreover, the PA struc-
tures are not guaranteed to cover the whole sentence.
This is especially true when two or more predicates
are presented in a coordinated structure. In this case,
the arguments of other predicates will not be covered
in the PA structure of the predicate.
The second problem is that the SRL labels are
only on the constituents of predicate and arguments.
There is no analysis conducted inside the augments.
That is different from syntactic parsing or depen-
dency parsing, which both provide a complete tree
from the sentence to every individual word. As
we can see in Figure 1, words such as ?Second?
and ?and? are not covered. Inside the NPs such
as ?a flood prevention system?, SRL will not pro-
vide more information. Therefore it is hard to build
a self-contained formalization based only on SRL
labels. Most work on SRL labels is built upon
or assisted by other formalisms. For instance, Liu
and Gildea (2010) integrated SRL label into a tree-
to-string translation system. Wu and Fung (2009)
used SRL labels for reordering the n-best output of
phrase-based translation systems. Similarly, in our
work we also adopt the methodology of using SRL
information to assist existing formalism. The dif-
ference of our method from Wu and Fung is that
we embed the SRL information directly into the de-
code, instead of doing two-pass decoding. Also, our
method is different from Liu and Gildea (2010) that
we utilize target side SRL information instead of the
source side.
As we will see in section 3, we define a mapping
function from the SRL structures that a phrase cov-
ers to a non-terminal tag before extracting the SCFG
rules. The tags will restrict the derivation of the tar-
get side parse tree to accept only SRL structures we
have seen in the training corpus. The mapping from
SRL structures to non-terminal tags can be defined
according to the SRL annotation set.
In this paper we adopt the PropBank (Palmer et
al., 2005) annotation set of semantic labels, because
the annotation set is relatively simple and easy to
parse. The small set of argument tags also makes
the number of LHS non-terminal tags small, which
alleviates the problem of data scarcity. However the
methodology of this paper is not limited to Prop-
Bank tags. By defining appropriate mapping, it is
also possible to use other annotation sets, such as
FrameNet (Baker et al, 2002).
3 SRL-aware SCFG Rules
The SRL-aware SCFG rules are SCFG rules. They
contain at least one non-terminal label with infor-
mation about the PA structure that is covered by the
non-terminal. The labels are called SRL-aware la-
bels, and the non-terminal itself is called SRL-aware
non-terminal. The non-terminal can be on the left
hand side or right hand side or the rule, and we do
not require all the non-terminals in the rules be SRL-
aware, thus, the general tag X can also be used. In
this paper, we assign SRL-aware labels based on the
SRL structure they cover. The label contains the fol-
lowing components:
1. The predicate frame; that is the predicate whose
predicate argument structure belongs to the
SRL-aware non-terminal.
2. The set of complete arguments the SRL-aware
non-terminal covers.
In practice, the predicates are stemmed. For ex-
ample, if we have a target side phrase: She beats
eggs today, where She will be labeled as ARG0 of the
predicate beat, and eggs will be labeled as ARG1, to-
day will be labeled as ARG-TMP, respectively. The
SRL-aware label that covers this phrase is:
#beat/0 1 TMP
There are two notes for the definition. Firstly,
the order of arguments is not important in the la-
bel. #beat/0 1 TMP is treated identically to
#beat/0 TMP 1. Secondly, as we always require
the predicate to be represented, an SRL-aware non-
terminal should always cover the predicate. This
property will be re-emphasized when we discuss
the rule extraction algorithm in Section 3. Figure
2 shows some examples of the SRL-aware SCFG
rules.
When the RHS non-terminal is an SRL-aware
non-terminal, we define the rule as a conversion rule.
A conversion rule is only generated when the right
109
Xin
jia
ng
?s
Yil
i
hol
ds
pro
pag
and
a
dri
ve
??
??
???
??
???
??
??
[#Ho
ld/1]
[#Ho
ld/0_
1]
[#Ho
ld/0][
#Hol
d]
[X]
[X]
[X]
[X][X]
[X]
Some
 SRL-aw
are Ru
les :  
[#Hold/0_1] 
?( [#Ho
ld/0] 
???
??
??,
 [#Hold/0] p
ropag
anda 
drive)
[#Hold/0_1] 
?( ?
??
??
??
[#Hold/1], Xin
jiang?s
 Yili[#Hold
/1])
[#Hold/0_1] 
?( ?
?[X 1
]??
?[#Hol
d/1], Xinjia
ng?s [
X 1] [#Hol
d/1])
[#Hold/0_1] 
?( [
X 1]ho
ld [X 2
], [X 1]
hold [X
2])
[#Hold/1]
?([#Hold
] 
??
??
??
?, [#Ho
ld] pro
pagan
da dr
ive)
[#Hold/0] 
?(?
??
??
??
[#Hold], Xin
jiang?s
 Yili[#Hold
])
[#Hold] 
?(?
?, h
olds)
Spec
ial SRL-
aware
 conv
ersion
 rule:  
[X
] ?[#Ho
ld/0_1]
Figure 2: Example SRL structure with word alignment
hand side is a complete SRL structure. For exam-
ple, #hold/0 is not a complete SRL structure in
Figure 2, because it lacks of a required argument,
while #hold/0 1 is a complete SRL structure. In
this case, the conversion rule X ? #hold/0 1
will be extracted from the example shown in Fig-
ure 2, but not the other. Together with the glue
rules that commonly used in Hiero decoder, i.e.
S ? (S X1, S X1) and S ? (X1, X1), the conver-
sion rules ensures that whenever SRL-aware SCFG
rules are used in parsing, the output parse tree con-
tains only complete SRL structures. This is because
only complete SRL structures that we have observed
in the training data can be converted back to the gen-
eral tag X .
After we have extracted the SRL-aware SCFG
rules, derivation can be done on the input of source
sentence. For example, the sentence ?? ???
????????? 1 can generate the parse tree
and translation in Figure 3a) using the rules shown
in Figure 2. Also, we can see in Figure 3b) that in-
complete SRL structures cannot be generated due to
the absence of a proper conversion rule.
1The translation is Xinjiang?s Yili holds propaganda drive
and the Pinyin transliteration is Xinjiang daguimo kaizhan
mianduimian xuanjiang huodong

  
	
	
 

	 

	   

 
	
 

	 
  
	
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 386?392,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
CMU Haitian Creole-English Translation System for WMT 2011
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi Ambati, Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sanjika,nbach,qing,vamshi,vogel+}@cs.cmu.edu
Abstract
This paper describes the statistical machine
translation system submitted to the WMT11
Featured Translation Task, which involves
translating Haitian Creole SMS messages into
English. In our experiments we try to ad-
dress the issue of noise in the training data,
as well as the lack of parallel training data.
Spelling normalization is applied to reduce
out-of-vocabulary words in the corpus. Us-
ing Semantic Role Labeling rules we expand
the available training corpus. Additionally we
investigate extracting parallel sentences from
comparable data to enhance the available par-
allel data.
1 Introduction
In this paper we describe the CMU-SMT Haitian
Creole-English translation system that was built as
part of the Featured Translation Task of the WMT11.
The task involved translating text (SMS) messages
that were collected during the humanitarian opera-
tions in the aftermath of the earthquake in Haiti in
2010.
Due to the circumstances of this situation, the
SMS messages were often noisy, and contained in-
complete information. Additionally they sometimes
contained text from other languages (e.g. French).
As is typical in SMS messages, abbreviated text (as
well as misspelled words) were present. Further,
since the Haitian Creole orthography is not fully
standardized (Allen, 1998), the text inherently con-
tained several different spelling variants.
These messages were translated into English by
a group of volunteers during the disaster response.
The background and the details of this crowdsourc-
ing translation effort is discussed in Munro (2010).
Some translations contain additional annotations
which are not part of the original SMS, possibly
added by the translators to clarify certain issues with
the original message. Along with the noise, spelling
variants, and fragmented nature of the SMS mes-
sages, the annotations contribute to the overall diffi-
culty in building a machine translation system with
this type of data. We aim to address some of these
issues in out effort.
Another challenge with building a Haitian Creole-
English translation system is the lack of parallel
data. As Haitian Creole is a less commonly spo-
ken language, the available resources are limited.
Other than the manually translated SMS messages,
the available Haitian Creole-English parallel data
is about 2 million tokens, which is considerably
smaller than the parallel data available for the Stan-
dard Translation Task of the WMT11.
Lewis (2010) details the effort quickly put
forth by the Microsoft Translator team in building
a Haitian Creole-English translation system from
scratch, as part of the relief effort in Haiti. We took
a similar approach to this shared task: rapidly build-
ing a translation system to a new language pair uti-
lizing available resources. Within a short span (of
about one week), we built a baseline translation sys-
tem, identified the problems with the system, and
exploited several approaches to rectify them and im-
prove its overall performance. We addressed the is-
sues above (namely: noise in the data and sparsity of
parallel data) when building our translation system
for Haitian Creole-English task. We also normalized
386
different spelling variations to reduce the number of
out-of-vocabulary (OOV) tokens in the corpus. We
used Semantic Role Labeling to expand the available
training corpus. Additionally we exploited other re-
sources, such as comparable corpora, to extract par-
allel data to enhance the limited amount of available
parallel data.
The paper is organized as follows: Section 2
presents the baseline system used, along with a de-
scription of training and testing data used. Section 3
explains different preprocessing schemes that were
tested for SMS data, and their effect on the trans-
lation performance. Corpus expansion approach is
given in Section 4. Parallel data extraction from
comparable corpora is presented in section 5. We
present our concluding remarks in Section 6.
2 System Architecture
The WMT11 has provided a collection of Haitian
Creole-English parallel data from a variety of
sources, including data from CMU1. A summary
of the data is given in Table 1. The primary in-
domain data comprises the translated (noisy) SMS
messages. The additional data contains newswire
text, medical dialogs, the Bible, several bilingual
dictionaries, and parallel sentences from Wikipedia.
Corpus Sentences Tokens (HT/EN)
SMS messages 16,676 351K / 324K
Newswire text 13,517 336K / 292K
Medical dialog 1,619 10K / 10K
Dictionaries 42,178 97K / 92K
Other 41,872 939K / 865K
Wikipedia 8,476 77K / 90K
Total 124,338 1.81M / 1.67M
Table 1: Haitian Creole (HT) and English (EN) parallel
data provide by WMT11
We preprocessed the data by separating the punc-
tuations, and converting both sides into lower case.
SMS data was further processed to normalize quo-
tations and other punctuation marks, and to remove
all markups.
To build a baseline translation system we fol-
lowed the recommended steps: generate word align-
1www.speech.cs.cmu.edu/haitian/
ments using GIZA++ (Och and Ney, 2003) and
phrase extraction using Moses (Koehn et al, 2007).
We built a 4-gram language model with the SRI
LM toolkit (Stolcke, 2002) using English side of
the training corpus. Model parameters for the lan-
guage model, phrase table, and lexicalized reorder-
ing model were optimized via minimum error-rate
(MER) training (Och, 2003).
The SMS test sets were provided in two formats:
raw (r) and cleaned (cl), where the latter had been
manually cleaned. We used the SMS dev clean to op-
timize the decoder parameters and the SMS devtest
clean and SMS devtest raw as held-out evaluation sets.
Each set contains 900 sentences. A separate SMS
test, with 1274 sentences, was used as the unseen
test set in the final evaluation. For each experiment
we report the case-insensitive BLEU (Papineni et
al., 2002) score.
Using the available training data we built several
baseline systems: The first system (Parallel-OOD),
uses all the out-of-domain parallel data except the
Wikipedia sentences. The second system, in addi-
tion, includes Wikipedia data. The third system uses
all available parallel training data (including both the
out-of-domain data as well as in-domain SMS data).
We used the third system as the baseline for later
experiments.
dev (cl) devtest (cl) devtest (r)
Parallel-OOD 23.84 22.28 17.32
+Wikipedia 23.89 22.42 17.37
+SMS 32.28 33.49 29.95
Table 2: Translation results in BLEU for different corpora
Translation results for different test sets using the
three systems are presented in Table 2. No signifi-
cant difference in BLEU was observed with the ad-
dition of Wikipedia data. However, a significant
improvement in performance can be seen when in-
domain SMS data is added, despite the fact that this
is noisy data. Because of this, we paid special atten-
tion to clean the noisy SMS data.
3 Preprocessing of SMS Data
In this section we explain two approaches that we
explored to reduce the noise in the SMS data.
387
3.1 Lexicon-based Collapsing of OOV Words
We observed that a number of words in the raw SMS
data consisted of asterisks or special character sym-
bols. This seems to occur because either users had
to type with a phone-based keyboard or simply due
to processing errors in the pipeline. Our aim, there-
fore, was to collapse these incorrectly spelled words
to their closest vocabulary entires from the rest of
the data.
We first built a lexicon of words using the entire
data provided for the Featured Task. We then built
a second probabilistic lexicon by cross-referencing
SMS dev raw with the cleaned-up SMS dev clean.
The first resource can be treated as a dictionary
while the second is a look-up table. We processed
incoming text by first selecting all the words with
special characters in the text, and then computing
an edit distance with each of the words in the first
lexicon. We return the most frequent word that is
the closest match as a substitute. For all words that
don?t have a closest match, we looked them up in the
probabilistic dictionary and return a potential substi-
tution if it exists. As the probabilistic dictionary is
constructed using a very small amount of data, the
two-level lookup helps to place less trust in it and
use it only as a back-off option for a missing match
in the larger lexicon.
This approach only collapses words with special
characters to their closest in-vocabulary words. It
does not make a significant difference to the OOV
ratios, but reduces the number of tokens in the
dataset. Using this approach we were able to col-
lapse about 80% of the words with special characters
to existing vocabulary entries.
3.2 Spelling Normalization
One of the most problematic issues in Haitian Cre-
ole SMS translation system is misspelled words.
When training data contains misspelled words, the
translation system performance will be affected at
several levels, such as word alignment, phrase/rule
extractions, and tuning parameters (Bertoldi et al,
2010). Therefore, it is desirable to perform spelling
correction on the data. Spelling correction based
on the noisy channel model has been explored in
(Kernighan et al, 1990; Brill and Moore, 2000;
Toutanova and Moore, 2002). The model is gener-
ally presented in the following form:
p(c?|h) = argmax
?c
p(h|c)p(c) (1)
where h is the Haitian Creole word, and c is a pos-
sible correction. p(c) is a source model which is a
prior of word probabilities. p(h|c) is an error model
or noisy channel model that accounts for spelling
transformations on letter sequences.
Unfortunately, in the case of Haitian Creole SMS
we do not have sufficient data to estimate p(h|c)
and p(c). However, we can assume p(c|h) ? p(c)
and c is in the French vocabulary and is not an En-
glish word. The rationale for this, from linguistic
point of view, is that Haitian Creole developed from
the 18th century French. As a result, an important
part of the Haitian Creole lexicon is directly derived
from French. Furthermore, SMS messages some-
times were mixed with English words. Therefore,
we ignore c if it appears in an English dictionary.
Given h, how do we get a list of possible normal-
ization c and estimate p(c)? We use edit distance
of 1 between h and c. An edit can be a deletion,
transposition, substitution, or insertion. If a word
has l characters, there will be 66l+31 possible cor-
rections2. It may result in a large list. However,
we only keep possible normalizations which appear
in a French dictionary and do not appear in an En-
glish dictionary3. To approximate p(c), we use the
French parallel Giga training data from the Shared
Task of the WMT11. p(c) is estimated by MLE. Fi-
nally, our system chooses the French word with the
highest probability.
dev (cl) devtest (cl) test (cl)
Before 2.6 ; 16 2.7 ; 16 2.6 ; 16
After 2.2 ; 13.63 2.3 ; 13.95 2.2 ; 14.3
Table 3: Percentage of OOV tokens and types in test sets
before and after performing spelling normalization.
Table 3 shows that spelling normalization helps
to bring down the percentage of OOV tokens and
types by 0.4% and 2% respectively on the three test
2l deletions, l-1 transpositions, 32l substitutions, and 32(l+1)
insertions; Haitian Creole orthography has 32 forms.
3The English dictionary was created from the English Gigaword
corpus.
388
sets. Some examples of Haitian Creole words and
their French normalization are (tropikal:tropical),
(economiques:economique), (irjan:iran), (idanti-
fie:identifie).
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
S1 32.18 30.22 25.45
S2 28.9 31.06 27.69
Table 4: Translation results in BLEU with/without
spelling correction
Given the encouraging OOV reductions, we ap-
plied the spelling normalization for the full corpus,
and built new translation systems. Our baseline sys-
tem has no spelling correction (for the training cor-
pus or the test sets); in S1, the spelling corrections
is applied to all words; in S2, the spelling correc-
tion is only applied to Haitian Creole words that oc-
cur only once or twice in the data. In S1, 11.5% of
Haitian Creole words had been mapped to French,
including high frequency words. Meanwhile, 4.5%
Haitian Creole words on training data were mapped
to French words in S2. Table 4 presents a compar-
ison of translation performance of the baseline, S1
and S2 for the SMS test sets. Unfortunately, none of
systems with spelling normalization outperformed
the system trained on the original data. Restricting
the spelling correction only to infrequent words (S2)
performed better for the devtest sets, but not for the
dev set, although all the test sets come from the same
domain.
4 Corpus Expansion using Semantic Role
Labeling
To address the problem of limited resources, we
tried to expand the training corpus by applying the
corpus expansion method described in (Gao and Vo-
gel, 2011). First, we parsed and labeled the semantic
roles of the English side of the corpus, using the AS-
SERT labeler (Pradhan et al, 2004). Next, using the
word alignment models of the parallel corpus, we
extracted Semantic Role Label (SRL) substitution
rules. SRL rules consist of source and target phrases
that cover whole constituents of semantic roles, the
verb frames they belong to, and the role labels of
the constituents. The source and target phrases must
comply with the restrictions detailed in (Gao and Vo-
gel, 2011). Third, for each sentence, we replaced
one of embedded SRL substitution rules with equiv-
alent rules that have the same verb frame and the
same role label.
The original method includes an additional but
crucial step of filtering out the grammatically incor-
rect sentences using an SVM classifier, trained with
labeled samples. However, we were unable to find
Haitian Creole speakers who could manually label
training data for the filtering step. Therefore, we
were forced to skip this filtering step. We expanded
the full training corpus which contained 124K sen-
tence pairs, resulting in an expanded corpus with
505K sentences. The expanded corpus was force-
aligned using the word alignment models trained
on the original unexpanded corpus. A new trans-
lation system was built using the original plus the
expanded corpus. As seen in Table 5, we observed
a small improvement with the expanded corpus for
the raw devtest. This method did not improve per-
formance for the other two test sets.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Expanded 31.79 32.98 30.1
Table 5: Translation results in BLEU with/without corpus
expansion
A possible explanation for this, in addition to
the missing component of filtering, is the low qual-
ity of SRL parsing on the SMS corpus. We ob-
served a very small ratio of expansions in the
Haitian Creole-English data, when compared to the
Chinese-English experiment shown in (Gao and Vo-
gel, 2011). The latter used a high quality corpus for
the expansion and the expanded corpus was 20 times
larger than the original one. Due to the noisy nature
of the available parallel data, only 61K of the 124K
sentences were successfully parsed and SRL-labeled
by the labeler.
389
5 Extracting Parallel Data from
Comparable Data
As we only have a limited amount of parallel data,
we focused on automatically extracting additional
parallel data from other available resources, such as
comparable corpora. We were not able to find com-
parable news articles in Haitian Creole and English.
However, we found several hundred Haitian Creole
medical articles on the Web which were linked to
comparable English articles4. Although some of the
medical articles seemed to be direct translations of
each other, converting the original pdf formats into
text did not produce sentence aligned parallel arti-
cles. Rather, it produced sentence fragments (some-
times in different orders) due to the structural dif-
ferences in the article pair. Hence a parallel sen-
tence detection technique was necessary to process
the data. Because the SMS messages are related to
the disaster relief effort, which may include many
words in the medical domain, we believe the newly
extracted data may help improve translation perfor-
mance.
Following Munteanu and Marcu (2005), we used
a Maximum Entropy classifier to identify compara-
ble sentence. To avoid the problem of having dif-
ferent sentence orderings in the article pair, we take
every source-target sentence pair in the two articles,
and apply the classifier to detect if they are paral-
lel. The classifier approach is appealing to a low-
resource language such as Haitian Creole, because
the features for the classifier can be generated with
minimal translation resources (i.e. a translation lex-
icon).
5.1 Maximum Entropy Classifier
The classifier probability can be defined as:
Pr(ci|S, T ) =
exp
(?n
j=1 ?jfij(ci, S, T )
)
Z(S, T )
(2)
where (S, T ) is a sentence pair, ci is the class, fij
are feature functions and Z(S) is a normalizing fac-
tor. The parameters ?i are the weights for the feature
functions and are estimated by optimizing on a train-
ing data set. For the task of classifying a sentence
pair, there are two classes, c0 = non ? parallel
4Two main sources were: www.rhin.org and www.nlm.nih.gov
and c1 = parallel . A value closer to one for
Pr(c1|S, T ) indicates that (S, T ) are parallel.
The features are defined primarily based on trans-
lation lexicon probabilities. Rather than computing
word alignment between the two sentences, we use
lexical probabilities to determine alignment points
as follows: a source word s is aligned to a tar-
get word t if p(s|t) > 0.5. Target word align-
ment is computed similarly. We defined a feature set
which includes: length ratio and length difference
between source and target sentences, lexical proba-
bility scores similar to IBM model 1 (Brown et al,
1993), number of aligned/unaligned words and the
length of the longest aligned word sequence. Lexi-
cal probability score, and alignment features gener-
ate two sets of features based on translation lexica
obtained by training in both directions. Features are
normalized with respect to the sentence length.
5.2 Training and Testing the Classifier
To train the model we need training examples that
belong to each of the two classes: parallel and non-
parallel. Initially we used a subset of the available
parallel data as training examples for the classifier.
This data was primarily sourced from medical con-
versations and newswire text, whereas the compa-
rable data was found in medical articles. This mis-
match in domain resulted in poor classification per-
formance. Therefore we manually aligned a set of
250 Haitian Creole-English sentence pairs from the
medical articles and divided them in to a training set
(175 sentences) and a test set (100 sentences).
The parallel sentence pairs were directly used as
positive examples. In selecting negative examples,
we followed the same approach as in (Munteanu
and Marcu, 2005): pairing all source phrases with
all target phrases, but filter out the parallel pairs and
those that have high length difference or a low lex-
ical overlap, and then randomly select a subset of
phrase pairs as the negative training set. The test
set was generated in a similar manner. The model
parameters were estimated using the GIS algorithm.
We used the trained ME model to classify the sen-
tences in the test set into the two classes, and notice
how many instances are classified correctly.
Classification results are as given in Table 6. We
notice that even with a smaller training set, the clas-
sifier produces results with high precision. Using
390
Precision Recall F-1 Score
Training Set 93.90 77.00 84.61
Test Set 85.53 74.29 79.52
Table 6: Performance of the Classifier
the trained classifier, we processed 220 article pairs
which contained a total of 20K source sentences
and 18K target sentences. The classifier selected
about 10K sentences as parallel. From these, we se-
lected sentences where pr(c1|S, T ) > 0.7 for trans-
lation experiments. The extracted data expanded the
source vocabulary by about 5%.
We built a second translation system by combin-
ing the baseline parallel corpus and the extracted
corpus. Table 7 shows the translation results for this
system.
dev (cl) devtest (cl) devtest (r)
Baseline 32.28 33.49 29.95
+Extracted 32.29 33.29 29.89
Table 7: Translation results in BLEU with/without ex-
tracted data
The results indicate that there is no significant per-
formance difference in using the extracted data. This
may be due to the relatively small size of the com-
parable corpus we used when extract the data.
6 Conclusion
Building an MT system to translate Haitian Creole
SMS messages involved several challenges. There
was only a limited amount of parallel data to train
the models. The SMS messages tend to be quite
noisy. After building a baseline MT system, we
investigated several approaches to improve its per-
formance. In particular, we tried collapsing OOV
words using a lexicon generated with clean data, and
normalize different variations in spelling. However,
these methods did not results in improved translation
performance.
We tried to address the data sparseness problem
with two approaches: expanding the corpus using
SRL rules, and extracting parallel sentences from
a collection of comparable documents. Corpus ex-
pansion showed a small improvement for the raw
devtest. Both corpus expansion and parallel data
extraction did not have a positive impact on other
test sets. Both these methods have shown significant
performance improvement in the past in large data
scenarios (for Chinese-English and Arabic-English),
but failed to show improvements in the current low-
data scenario. Thus, we need further investigations
in handling noisy data, especially in low-resource
scenarios.
Acknowledgment
We thank Julianne Mentzer for assisting with editing
and proofreading the final version of the paper. We
also thank the anonymous reviewers for their valu-
able comments.
References
Jeff Allen. 1998. Lexical variation in haitian cre-
ole and orthographic issues for machine translation
(MT) and optical character recognition (OCR) appli-
cations. In Proceedings of the First Workshop on Em-
bedded Machine Translation systems of AMTA confer-
ence, Philadelphia, Pennsylvania, USA, October.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.
2010. Statistical machine translation of texts with mis-
spelled words. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Los Angeles, California, June.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics (ACL 2000), pages
286?293.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Portland,
Oregon, USA, June.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics - Volume 2,
COLING ?90, pages 205?210.
391
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
June.
William Lewis. 2010. Haitian Creole: How to build and
ship an mt engine from scratch in 4 days, 17 hours, &
30 minutes. In Proceedings of the 14th Annual confer-
ence of the European Association for Machine Trans-
lation (EAMT), Saint-Raphae?l, France, May.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collab-
orative Crowdsourcing for Translation, Denver, Col-
orado, USA, October-November.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL-2004).
Andreas Stolcke. 2002. An extensible language model-
ing toolkit. In Proc. of International Conference on
Spoken Language Processing, volume 2, pages 901?
904, Denver, CO, September.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2002).
392
