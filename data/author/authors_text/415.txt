Proceedings of NAACL HLT 2009: Short Papers, pages 177?180,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
 
Towards Effective Sentence Simplification for  
Automatic Processing of Biomedical Text  
 
Siddhartha Jonnalagadda*, Luis Tari**, J?rg Hakenberg**, Chitta Baral**, Graciela Gonzalez* 
*Department of Biomedical Informatics, Arizona State University, Phoenix, AZ 85004, USA. 
**Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA. 
 Corresponding author: ggonzalez@asu.edu 
 
 
 
 
Abstract 
The complexity of sentences characteristic to 
biomedical articles poses a challenge to natu-
ral language parsers, which are typically 
trained on large-scale corpora of non-technical 
text. We propose a text simplification process, 
bioSimplify, that seeks to reduce the complex-
ity of sentences in biomedical abstracts in or-
der to improve the performance of syntactic 
parsers on the processed sentences. Syntactic 
parsing is typically one of the first steps in a 
text mining pipeline. Thus, any improvement 
in performance would have a ripple effect 
over all processing steps. We evaluated our 
method using a corpus of biomedical sen-
tences annotated with syntactic links. Our em-
pirical results show an improvement of 2.90% 
for the Charniak-McClosky parser and of 
4.23% for the Link Grammar parser when 
processing simplified sentences rather than the 
original sentences in the corpus. 
1 Introduction 
It is typical that applications for biomedical text 
involve the use of natural language syntactic pars-
ers as one of the first steps in processing. Thus, the 
performance of the system as a whole is largely 
dependent on how well the natural language syn-
tactic parsers perform.  
One of the challenges in parsing biomedical text is 
that it is significantly more complex than articles in 
typical English text. Different analysis show other 
problematic characteristics, including inconsistent 
use of nouns and partial words (Tateisi & Tsujii, 
2004), higher perplexity measures (Elhadad, 2006), 
greater lexical density, plus increased number of 
relative clauses and prepositional phrases (Ge-
moets, 2004), all of which correlate with dimi-
nished comprehension and higher text difficulty.  
These characteristics also lead to performance 
problems in terms of computation time and accura-
cy for parsers that are trained on common English 
text corpus. 
  We identified three categories of sentences: 1) 
normal English sentences, like in Newswire text, 
2) normal biomedical English sentences ? those 
sentences which can be parsed without a problem 
by Link Grammar-, and 3) complex biomedical 
English sentences ? those sentences which can?t be 
parsed by Link Grammar. Aside from the known 
characteristics mentioned before, sentences in the 
third group tended to be longer (18% of them had 
more than 50 words, while only 8% of those in 
group 2 and 2% of those in group 1 did). It has 
been observed that parsers perform well with sen-
tences of reduced length (Chandrasekar & Srini-
vas, 1997; Siddharthan, 2006).  
  In this paper, we explore the use of text simplifi-
cation as a preprocessing step for general parsing 
to reduce length and complexity of biomedical sen-
tences in order to enhance the performance of the 
parsers.  
2 Methods  
There are currently many publicly available corpo-
ra of biomedical texts, the most popular among 
them being BioInfer, Genia, AImed, HPRD 50, 
IEPA, LLL and BioCreative1-PPI. Among these 
corpora, BioInfer includes the most comprehensive 
collection of sentences and careful annotation for 
links of natural parser, in both the Stanford and 
Link Grammar schemes. Therefore, we chose the 
BioInfer corpus, version 1.1.0 (Pyysalo et al, 
2007), containing 1100 sentences for evaluating 
the effectiveness of our simplification method on 
177
  
the performance of syntactic parsers. The method 
includes syntactic and non-syntactic transforma-
tions, detailed next. 
2.1 Non-syntactic transformation 
We group here three steps of our approach: 1. pre-
processing through removal of spurious phrases; 2. 
replacement of gene names; 3. replacement of 
noun phrases. 
  To improve the correctness of the parsing, each 
biomedical sentence is first preprocessed to re-
move phrases that are not essential to the sentence. 
This includes removal of section indicators, which 
are phrases that specify the name of the section at 
the beginning of the sentence, plus the removal of 
phrases in parentheses (such as citations and num-
bering in lists). Also, partially hyphenated words 
are transformed by combining with the nearest 
word that follows or precedes the partial hyphe-
nated word to make a meaningful word. For in-
stance, the phrase ?alpha- and beta-catenin? is 
transformed into ?alpha-catenin and beta-catenin?. 
  Occurrences of multi-word technical terms and 
entity names involved in biomedical processes are 
common in biomedical text. Such terms are not 
likely to appear in the dictionary of a parser (per-
plexity is high), and will force it to use morpho-
guessing and unknown word guessing. This is time 
consuming and prone to error. Thus, unlike typical 
text simplification that emphasizes syntactic trans-
formation of sentences, our approach utilizes a 
named entity recognition engine, BANNER  
(Leaman & Gonzalez, 2008), to replace multi-word 
gene names with single-word placeholders.  
 Replacement of gene names with single elements 
is not enough, however, and grammatical category 
(i.e. singular or plural) of the element has to be 
considered. Lingpipe (Alias-i, 2006), a shallow 
parser for biomedical text, identifies noun phrases 
and replaces them with single elements. A single 
element is considered singular when the following 
verb indicates a third-person singular verb or the 
determiner preceded by the element is either ?a? or 
?an?. Otherwise it is considered as plural and an 
?s? is attached to the end of the element. 
2.2 Syntactic transformation 
The problem of simplifying long sentences in 
common English text has been studied before, not-
ably by Chandrasekar & Srinivas (1997) and Sidd-
harthan (2006). However, the techniques used in 
these studies might not totally solve the issue of 
parsing biomedical sentences. For example, using 
Siddharthan?s approach, the biological finding 
?The Huntington's disease protein interacts with 
p53 and CREB-binding protein and represses tran-
scription?, and assuming multi-word nouns such as 
?CREB-binding protein? do not present a problem, 
would be simplified to: 
?The Huntington's disease protein interacts with 
p53. The Huntington's disease protein interacts 
with CREB-binding protein. The Huntington's 
disease protein represses transcription.?  
 Our method transforms it to ?GENE1 interacts 
with GENE2 and GENE3 and represses transcrip-
tion.? Both decrease the average sentence length, 
but the earlier distorts the biological meaning 
(since the Huntington?s disease protein might not 
repress transcription on its own), while the latter 
signifies it. 
  While replacement of gene names and noun 
phrases can reduce the sentence length, there are 
Figure 1 ? Linkages after simplification of the original sentence 
? GENE1: human CREB binding protein 
? GENE2: CBP 
? GENE3s: CBP 
? REPNP1s: RTS patients 
Original sentence ST: The gene for the human CREB binding protein, the transcriptional coactivator CBP,   
is included in the RT1 cosmid, and mutations in CBP have recently been identified in RTS patients. 
ST1: 
ST2: 
c1 
c3 c4 
c2 
178
  
cases when the sentences are still too complex to 
be parsed efficiently. We developed a simple algo-
rithm that utilizes linkages (specific grammatical 
relationships between pairs of words in a sentence) 
of the Link Grammar parser (Sleator, 1998) and 
punctuations for splitting sentences into clauses. 
An example in Figure 1 illustrates the main part of 
the algorithm. Each linkage has a primary link type 
in CAPITAL followed by secondary link type in 
short. The intuition behind the algorithm is to try to 
identify independent clauses from complex sen-
tences. The first step is to split the sentence ST into 
clauses c1, c2, c3 and c4 based on commas. c1 is 
parsed using the Link Grammar parser, but c1 can-
not be a sentence as there is no ?S? link in the lin-
kage of c1. c2 is then attached to c1 and the linkage 
of ?c1, c2? does not contain a ?S? link as well. ?c1, 
c2, c3.? is recognized as a sentence, since the lin-
kage contains an ?S? link, indicating that it is a 
sentence, as well as the linkage of c4. So the algo-
rithm returns two sentences ST1 and ST2 for ST. 
3 Results 
Our method has the greatest impact on the perfor-
mance of Link Grammar (LG), which lies at the 
core of BioLG (Pyysalo et al, 2006). However, it 
also has a significant impact on the self-training 
biomedical parser by McClosky & Charniak (CM), 
which is currently the best parser available for 
biomedical text.  
3.1 Rudimentary statistics of the results of sim-
plification: After the simplification algorithm was 
tested on the 1100 annotated sentences of the Bio-
Infer corpus, there were 1159 simplified sentences 
because of syntactic transformation (section 2.2). 
The number of words per sentence showed a sharp 
drop of 20.4% from 27.0 to 21.5. The Flesh-
Kincaid score for readability dropped from 17.4 to 
14.2. The Gunning Fog index also dropped by 
18.3% from 19.7 to 16.1. 
Pre-
processing 
Replacement 
of gene names 
Replacement of 
noun phrases 
Syntactic  
Simplification 
359  1082  915  91 
Table 1: Sentences processed in each stage 
 
3.2 Impact of simplification on the efficiency of 
parsing: We inputted the BioInfer corpus to LG 
and CM. If LG cannot find a complete linkage, it 
invokes its panic mode, where sentences are re-
turned with considerably low accuracy. Out of the 
1100 original sentences in the corpus, 219 went 
into panic mode. After processing, only 39 out of 
1159 simplified sentences triggered panic mode (a 
16.4% improvement in efficiency). The average 
time for parsing a sentence also dropped from 7.36 
secs to 1.70 secs after simplification.  
 
3.3 Impact of simplification on the accuracy of 
parsing: Let ?g, ?o and ?s, respectively be the 
sets containing the links of the gold standard, the 
output generated by the parser on original sen-
tences and the output generated by the parser on 
simplified sentences. We denote a link of type ? 
between the tokens ?1 and ?2 by (?,?1,?2). In the 
case of the original sentences, the tokens ?1and ?2 
are single-worded. So, (?,?1,?2) is a true positive 
iff (?,?1,?2) belongs to both ?g and ?o, false posi-
tive iff it only belongs to ?o and false negative iff 
it only belongs to ?g. In the case of simplified sen-
tences, the tokens ?1and ?2 can have multiple 
words. So, (?,?1,?2) which belongs to ?s is a true 
positive iff (?,??1,??2) belongs to ?g where ??1 
and ??2 are respectively one of the words in ?1 and 
?2. Additionally, (?,?1,?2) which belongs to ?g is 
not a false negative if ?1 and ?2 are parts of a sin-
gle token of a simplified sentence. For measuring 
the performance of a parser, the nature of linkage 
is most relevant in the context of the sentence in 
consideration. So, we calculate precision and recall 
for each sentence and average them over all sen-
tences to get the respective precision and recall for 
the collection.  
 
 Precision Recall f-measure 
CM 77.94% 74.08% 75.96% 
BioSimplify + 
CM 
82.51% 75.51% 78.86% 
Improvement   4.57% 1.43% 2.90% 
    
LG 72.36% 71.65% 72.00% 
BioSimplify + 
LG 
78.30% 74.27% 76.23% 
Improvement   5.94% 2.62% 4.23% 
Table 2: Accuracy of McClosky & Charniak (CM) and 
Link Grammar (LG) parsers based on Stanford depen-
dencies, with and without simplified sentences.  
 
  In order to compare the effect of BioSimplify on 
the two parsers, a converter from Link Grammar to 
Stanford scheme was used (Pyysalo et al 2007: 
precision and recall of 98% and 96%). Results of 
179
  
this comparision are shown in Table 2. On CM and 
LG, we were able to achieve a considerable im-
provement in the f-measures by 2.90% and 4.23% 
respectively. CM demonstrated an absolute error 
reduction of 4.1% over its previous best on a dif-
ferent test set. Overall, bioSimplify leverages pars-
ing of biomedical sentences, increasing both the 
efficiency and accuracy. 
4 Related work 
During the creation of BioInfer, noun phrase ma-
cro-dependencies were determined using a simple 
rule set without parsing. Some of the problems re-
lated to parsing noun phrases were removed by 
reducing the number of words by more than 20%. 
BioLG enhances LG by expansion of lexicons and 
the addition of morphological rules for biomedical 
domain. Our work differs from BioLG not only in 
utilizing a gene name recognizer, a specialized 
shallow parser and syntactic transformation, but 
also in creating a preprocessor that can improve the 
performance of any parser on biomedical text. 
  The idea of improving the performance of deep 
parsers through the integration of shallow and deep 
parsers has been reported in (Crysmann et al, 
2002; Daum et al, 2003; Frank et al, 2003) for 
non-biomedical text. In BioNLP, extraction sys-
tems (Jang et al, 2006; Yakushiji et al, 2001) used 
shallow parsers to enhance the performance of 
deep parsers. However, there is a lack of evalua-
tion of the correctness of the dependency parses, 
which is crucial to the correctness of the extracted 
systems. We not only evaluate the correctness of 
the links, but also go beyond the problem of rela-
tionship extraction and empower future researchers 
in leveraging their parsers (and other extraction 
systems) to get better results. 
5 Conclusion and Future work 
We achieved an f-measure of 78.86% using CM on 
BioInfer Corpus which is a 2.90% absolute reduc-
tion in error. We achieved a 4.23% absolute reduc-
tion in error using LG. According to the measures 
described in section 3.1, the simplified sentences 
of BioInfer outperform the original ones by more 
than 18%. Our method can also be used with other 
parsers. As future work, we will demonstrate the 
impact of our simplification method on other text 
mining tasks, such as relationship extraction. 
Acknowledgments 
We thank Science Foundation Arizona (award 
CAA 0277-08 Gonzalez) for partly supporting this 
research. SJ also thanks Bob Leaman and Anoop 
Grewal for their guidance. 
References  
Chandrasekar, R., & Srinivas, B. (1997). Automatic 
induction of rules for text simplification. Knowledge-
Based Systems, 10, 183-190.  
Crysmann, B., Frank, A., Kiefer, B., Muller, S., Neu-
mann, G., et al (2002). An integrated architecture for 
shallow and deep processing. ACL'02. 
Daum, M., Foth, K., & Menzel, W. (2003). Constraint 
based integration of deep and shallow parsing tech-
niques. EACL'03.  
Elhadad, No?mie (2006) User-Sensitive Text Summari-
zation: Application to the Medical Domain. 
Ph.D. Thesis, Columbia University. Available at 
www.dbmi.columbia.edu/noemie/papers/thesis.pdf 
Frank, A., Becker M, et al,. (2003). Integrated shallow 
and deep parsing: TopP meets HPSG. ACL'03. 
Gemoets, D., Rosemblat, G., Tse, T., Logan, R., Assess-
ing Readability of Consumer Health Information: An 
Exploratory Study. MEDINFO 2004. 
Jang, H., Lim, J., Lim, J.-H., Park, S.-J., Lee, K.-C. and 
Park, S.-H. (2006) Finding the evidence for protein-
protein interactions from PubMed abstracts, Bioin-
formatics, 22, e220-226. 
Leaman, R., & Gonzalez, G. (2008). BANNER: An 
executable survery of advances in biomedical named 
entity recognition. 652-663.  
Manning, C. D., & Sch?tze, H. (1999). Foundations of 
statistical natural language processing MIT Press. 
McClosky, D., & Charniak, E. (2008) Self-training for 
biomedical parsing. ACL?08. 
Pyysalo, S., Ginter, F., Haverinen, K., Heimonen, J., 
Salakoski, T., & Laippala, V. (2007) On the unifica-
tion of syntactic annotations under the stanford de-
pendency scheme.. ACL?07. 
Pyysalo, S., Salakoski, T., Aubin, S., & Nazarenko, A. 
(2006). Lexical adaptation of link grammar to the 
biomedical sublanguage: A comparative evaluation 
of three approaches. BMC Bioinformatics, 7, S2.  
Siddharthan, A. (2006). Syntactic simplification and 
text cohesion. Res  Lang  Comput, 4(1), 77-109.  
Sleator, D. (1998) Link Grammar Documentation 
Tateisi, Y., & Tsujii, J. (2004). Part-of-speech annota-
tion of biology research abstracts. LREC, 1267-1270.  
Yakushiji, A., Tateisi, Y., Miyao, Y., & Tsujii, J. 
(2001). Event extraction from biomedical papers using a 
full parser. PSB'01, 6, 408-419. 
180
Using answer set programming to answer complex queries
Chitta Baral
Dept. of Computer Sc. & Eng.
Arizona State University
Tempe, AZ 85287
chitta@asu.edu
Michael Gelfond
Dept. of Computer Sc.
Texas Tech University
Lubbock, TX 79409
mgelfond@cs.ttu.edu
Richard Scherl
Computer Science Dept.
Monmouth University
West Long Branch, NJ 07764
rscherl@monmouth.edu
Abstract
In this paper we discuss the applicability of the
knowledge representation and reasoning lan-
guage AnsProlog for the design and implemen-
tation of query answering systems. We con-
sider a motivating example, and illustrate how
AnsProlog can be used to represent defaults,
causal relations, and other types of common-
sense knowledge needed to properly answer
non-trivial questions about the example?s do-
main.
1 Introduction and Motivation
Let us envision a query answering system (QAS) con-
sisting of a search engine which searches diverse sources
for information relevant to the given query, Q; a natural
language processing module (NLPM), which translates
this information (including the query) into a theory, F ,
of some knowledge representation language L; a gen-
eral knowledge base, KB, containing common-senses
and expert knowledge about various domains; and an in-
ference engine which takes F and KB as an input and
returns an answer to Q. Even though the choice of the
KR language L is irrelevant for the first component of
the system it plays an increasingly important role in the
design of its other components. In this paper we hypothe-
size that AnsProlog - a language of logic programs under
the answer set semantics (Gelfond and Lifschitz, 1988;
Gelfond and Lifschitz, 1991) - is a good candidate for the
KR language of QAS. This is especially true if answer-
ing a query Q requires sophisticated kinds of reasoning
including default, causal, and counterfactual reasoning,
reasoning about narratives, etc.
The list of attractive properties of AnsProlog include
its simplicity and expressive power, ability to reason
with incomplete information, existence of a well devel-
oped mathematical theory and programming methodol-
ogy (Baral, 2003), and the availability of rather efficient
reasoning systems such as SMODELS(Niemela and Si-
mons, 1997) and others as well(Eiter et al, 1997; Yu and
Maratea, 2004). AnsProlog allows its users to encode de-
faults, causal relations, inheritance hierarchies, and other
types of knowledge not readily available in other KR lan-
guages. In addition it supports construction of elabora-
tion tolerant knowledge bases, i.e., ability to accommo-
date new knowledge without doing large scale surgery.
The main drawback of the language is the inability of its
current inference engines to effectively deal with num-
bers and numerical computations1.
In this paper we illustrate the use of AnsProlog for query
answering via a simple example. Of course substantially
more work is needed to to prove (or disprove) our main
hypothesis.
Consider an analyst who would like to use his QAS to
answer simple queries Q1 and Q2 about two people, John
and Bob:
? Q1 ? Was John in the Middle East in mid-
December?
? Q2 ? If so, did he meet Bob in the Middle East in
mid-December?
Let us assume that the search engine of QAS extracted
the following simple text relevant to Q1 and Q2:
1The current answer set solvers start their computation with
grounding the program, i.e. replacing its variables by possible
ground instantiations. The grounding algorithms are smart and
capable of eliminating many useless rules; answer sets can be
effectively computed even if the resulting program consists of
hundreds of thousands of rules. However, if several integer vari-
ables are used by the program rules, the size of the grounded
program becomes unmanageable. We hope however that this
problem will be remedied by the development of new reasoning
algorithms and systems.
John spent Dec 10 in Paris and took a plane to
Baghdad the next morning. He was planning to
meet Bob who was waiting for him there.
We will also assume that the NLP module of the QAS re-
alizes that to answer our queries it needs general knowl-
edge of geography, calendar, and human activities in-
cluding travel, meetings, and plans. In the next section
we outline how such knowledge can be represented in
AnsProlog.
2 Representing general knowledge
2.1 The ?geography? module M1
The geography module M1 will contain a list
is(baghdad,city).
is(iraq,country).
...
of places and the definition of relation
in(P1, P2) - ?P1 is located in P2?
The definition is given by a collection of facts:
in(baghdad, iraq).
in(iraq,middle_east).
in(paris,france).
in(france,western_europe).
in(western_europe,europe).
...
and the rule
in(P1,P3) :- in(P1,P2),
in(P2,P3).
For simplicity we assume that our information about re-
lation in is complete, i.e., if in(p1, p2) is not known to be
true then it is false. This statement can be expressed by
the rule
-in(P1,P2) :- not in(P1,P2)
often referred to as the CWA- Closed World Assumption
(Reiter, 1978) (for in). Here ?p stands for ?p is false?
while not p says that ?there is no reason to belief p?.
Similar assumption can be written for is. The program
has unique answer set containing in(iraq,middle east),
-in(iraq, europe), etc. This answer set, (or its relevant
parts) can be computed by answer set solvers. Some-
times this will require small additions to the program. For
instance SMODELS, which require typing of variables,
will not be able to compile this program. This problem
can be remedied by adding a rule
position(P) :- is(P,C).
defining the type position and a statement
#domain position(P;P1;P2;P3)
declaring the type of the corresponding variables. Now
SMODELS will be able complete the computation.
2.2 The ?travelling? module M2
This module describes the effects of a person travelling
from one place to another. We are mainly interested in
locations of people and in various travelling events which
change these locations. Construction of M2 is based on
the theory of dynamic systems () which views the world
as a transition diagram whose states are labelled by flu-
ents (propositions whose values depend on time) and arcs
are labelled by actions. For instance, states of the di-
agram, D, can contain locations of different people; a
transition ??0, {a1, a2}, ?1? ? D iff ?1 is a possible state
of the domain after the concurrent execution of actions
a1 and a2 in ?0. There is a well developed methodol-
ogy of representing dynamic domain in AnsProlog (Baral
and Gelfond, 2000; Turner, 1997) which, in its simplified
form, will be used in the construction of M2.
The language of M2 will contain time-steps from [0, n],
fluent loc(P,X, T ) - ?place P is a location of person X
at step T ?. Various types of travelling events - fly, drive,
etc., will be recorded by the list:
instance_of(fly,travel).
instance_of(drive,travel).
...
Description of an event type will contain the event?s name
and attributes. The following is a generic description of
John flying to Baghdad.
event(a1).
type(a1,fly).
actor(a1,john).
destination(a1,baghdad).
An actual event of this type will be recorded by a state-
ment
occurs(a1, i).
(where i is a time-step in the history of the world) pos-
sibly accompanied by the actual time of i. In addition,
M2 will import relation in(P1, P2) from the geography
module M1.
The transition diagram, D, of M2 will be described by ??
groups of axioms.
?. The first group consists of state constraints establish-
ing the relationship between the domain fluents. In our
case it is sufficient to have the rules:
loc(P2,X,T) :- loc(P1,X,T),
in(P2,P1).
disjoint(P1,P2) :- -in(P1,P2),
-in(P2,P1),
neq(P1,P2).
-loc(P2,X,T) :- loc(P1,X,T),
disjoint(P1,P2).
Here neq stands for the inequality. The first rule allows
us to conclude that if at step T of the domain history X
is in Iraq then he is also in the Middle East. The second
two rules guarantee that X is not in Europe.
?. The second group contains causal laws describing di-
rect effects of actions. For our example it suffices to have
the rules
loc(P,X,T+1) :- occurs(E,T),
type(E,travel),
actor(E,X),
destination(E,P),
-interference(E,T).
-interference(E,T) :-
not interference(E,T).
The first rule says that, in the absence of interference, a
traveller will arrive at his destination. The second - the
CWA for interference - states that the interference is
an unusual event which normally does not happen.
?. The third group consists of executability conditions for
actions, which have the form
-occurs(E,T) :- cond(T).
which says that it is impossible for an event E occur at
time step T if at that time step the domain is in a state
satisfying condition cond.
Causal laws and state constraints determine changes
caused by execution of an action. To complete the def-
inition of the transition diagram of the domain we need
to specify what fluents do not change as the results of ac-
tions. This is a famous Frame Problem from (McCarthy
and Hayes, 1969) where the authors suggested to solve it
by formalizing the Inertia Axiom which says that ?things
tend to stay as they are?. This is a typical default which
can be easily represented in AnsProlog. In our particular
case it will have a form:
loc(P,X,T+1) :- loc(P,X,T),
not -loc(P,X,T+1).
-loc(P,X,T+1) :- -loc(P,X,T),
not loc(P,X,T+1).
The above representation is a slightly simplified version
of AnsProlog theory of dynamic domains which gives no-
tation for causal relations of the domain, includes general
(fluent independent) formulation of the inertia, explains
how the set of causal relations define the corresponding
transition diagram, etc. We used this version to simple
save space. Given the following history of the domain
loc(paris,john,0).
loc(baghdad,bob,0).
occurs(a1,0).
information contained in M1 and M2 is sufficient to
conclude loc(baghdad, john, 1), loc(baghdad, bob, 1),
loc(middle east, john, 1), -loc(paris, john, 1), etc. To
answer the original queries we now need to deal with tim-
ing our actions. Let us assume, for instance, that the tim-
ing of John?s departure from Paris is recorded by state-
ments:
time(0,day,11).
time(0,month,12).
time(0,year,03).
Here day, month, and year are the basic time measuring
units.
Finally we may need to specify typical durations of ac-
tions, e.g.
time(T+1,day,D) :- occurs(E,T),
type(E,fly),
time(T,day,D),
not -time(T+1,day,D).
where 1 ? D ? 31.
To reason about the time relation we need to include a
new module, M3, which will allow us to change granu-
larity of our time measure.
2.3 M3 - measuring time
The module contains types for basic measuring units, e.g.
day(1..31).
month(1..12).
part(start).
part(end).
part(middle).
...
and rules translating from one granularity measure to an-
other, e.g.
time(T,part,middle) :- time(T,d,D),
10 < D < 20.
time(T,season,summer):- time(T,month,M),
5 < M < 9.
...
M3 presented in this paper is deliberately short. It
includes very little knowledge beyond that needed to
answer our query. Ideally it should be much bigger
and include a formalization of the calendar. Among
other things the module should allow us to prove state-
ments like next(date(10, 12, 03), date(11, 12, 03) and
next(date(31, 12, 03), date(1, 1, 04).
Now let us assume that NLP module of our QAS trans-
lated
(a) information about John?s flight to Baghdad by a his-
tory
loc(paris,john,0).
loc(baghdad,bob,0).
occurs(a1,0).
time(0,day,11).
time(0,month,12).
(b) the query Q1 by
? loc(middle_east,john,T),
time(T,month,12),
time(T,part,middle).
Modules M1, M2 and M3 have enough information to
correctly answer Q1.
2.4 Planning the meeting - M4
To answer the second question we need an additional
module about the event meet. The event type for meet
will be similar to the previously discussed flying event
a1. It may look like:
event(a2).
type(a2,meet).
actor(a2,john).
actor(a2,bob).
place(a2,baghdad).
Notice however that the story contains no information
about actual occurrence of this event. All we know is
that a2 is planned to occur at time step one. We encode
this by simply stating:
planned(a2,1).
Note that to give a positive answer to the question
Q2 ??Did John meet Bob in the Middle East in mid-
December? ? we need to reason about planned events. It
seems that our positive answer to this question is obtain
by using a default: ?people normally follow their plans?.
Again this is a typical default statement which, according
to the general knowledge representation methodology of
AnsProlog could be expressed by the rule:
occurs(E,T) :- planned(E,T),
not -occurs(E).
In a slightly more complex situation we may need to as-
sume that people take their plans seriously ? they persist
with their plans until the planned event actually happen.
This is encoded as follows:
planned(E,T+1) :- planned(E,T),
-occurs(E,T).
Unlike traveling, the meeting event does not seem to have
any obvious causal effects. It, however, has the following
executability condition relevant to our story.
-occurs(E,T) :- type(E,meet),
actor(E,X),
place(E,P),
-loc(P,X,T).
Now we have enough information to answer our second
query, which can be encoded as
? occurs(E,T),
type(E,meet),
actor(E,john),
actor(E,bob),
loc(middle_east,john,T),
time(T,month,12),
time(T,part,middle).
As expected the answer will be positive. There are sev-
eral ways to obtain this answer. It can of course be ex-
tracted from the unique answer set of our program. With
small additions of types and declaration of variables sim-
ilar to that we used to define position in M1 this answer
set can be found by SMODELS or any other answer set
solver. This method however may not scale. The problem
is caused the calendar. Its integer variables for months,
days, etc, in conjunction with a longer history (and there-
fore a larger number of time steps) may cause an unman-
ageable increase in the number of ground rules of the pro-
gram. It seems however that in many interesting cases
(including ours), the computation can be made substan-
tially more efficient by properly combining answer set
finding algorithms with the traditional resolution of Pro-
log. The way of doing this will be illustrated in the full
paper. We also plan to expand our modules especially
those dealing with time and reasoning about plans.
3 FrameNet and Events
Our vision of the NLPM is that it will translate both our
short text and also the queries into AnsProlog sentences.
There is a body of literature on translating or parsing
English sentences into a semantic representation such as
First Order Logic. See (Blackburn and Bos, 2003) for a
recent survey of such techniques. The semantic represen-
tation makes use of symbols based upon the lexicon of
English.
The success of our endeavor requires that there be an
axiomatization of the relationship between the symbols
representing functions and predicate symbols in our vari-
ous AnsProlog theories (e.g., M1 ? M4) and the symbols
(based upon the lexicon of English) used in the seman-
tic representation of the English queries and the narrative
texts. The online lexical database, FrameNet(Baker et al,
1998) provides such a connection, especially for events.
This is done through the notion of frame semantics that
underlies FrameNet.
Frame semantics assumes that lexical items draw their
meaning from conceptual structures or frames that pro-
vide an abstract or scematic description of particular
types of events. The frames are structured into an inheri-
tance hierarchy. Each frame includes a number of frame
elements (FEs) or roles that make up the conceptual struc-
ture.
For example, our ?travelling? module M2 closely corre-
sponds to the related FrameNet frames Travel, Move,
and Ride Vehicle. The frames relate the various frame
elements of Area (where the travelling takes place), Goal
(where the travellers end up), Path (route of the travel),
Source (starting point of the trip), and the Traveller (the
living being which travels).
Consider the phrase took a plane used to express the trav-
elling activity. The verb take is associated with the frame
Ride Vehicle. This information allows the connection
with the axiomatization of flying events in M2. On the
other hand FrameNet does not have entries for the verb
spend as in spent Dec 10. But WordNet(Fellbaum, 1998)
has 3 senses for the verb spend. Sense 1 is ?pass ? (pass
(time) in a specific way. ?How are you spending your
summer vacation??). ? Unfortunately, neither pass nor
time allows us to index a useful frame for just being in a
place. The coverage of FrameNet is not sufficient. It will
be necessary to augment our use of FrameNet with other
online sources such as WordNet and to also increase the
number of frames within FrameNet.
There has been some related work on using the frame of
FrameNet for reasoning (Chang et al, 2002) and also on
the automatic annotation of English texts with regard to
the relevant frames (Gildea and Jurafsky, 2000) and frame
elements.
4 Syntax and Semantics of AnsProlog
An AnsProlog knowledge base consists of rules of the
form:
l0 ? l1, . . . , lm, not lm+1, . . . , not ln (4.1)
where each of the lis is a literal, i.e. an atom, a, or its clas-
sical negation, -a and not is a logical connective called
negation as failure or default negation. While -a states
that a is false, an expression not l says that there is no
reason to believe in l.
The answer set semantics of a logic program ? assigns to
? a collection of answer sets ? consistent sets of ground
literals corresponding to beliefs which can be built by a
rational reasoner on the basis of rules of ?. In the con-
struction of these beliefs the reasoner is guided by the
following informal principles:
? He should satisfy the rules of ?, understood as con-
straints of the form: If one believes in the body of a
rule one must belief in its head.
? He should adhere to the rationality principle which
says that one shall not believe anything he is not
forced to believe.
The precise definition of answer sets is first given for pro-
grams whose rules do not contain default negation. Let ?
be such a program and X a consistent set of ground liter-
als. Set X is closed under ? if, for every rule (4.1) of ?,
l0 ? X whenever for every 1 ? i ? m, li ? X and for
every m+ 1 ? j ? n, lj 6? X .
Definition 1 (Answer set ? part one)
A state X of ?(?) is an answer set for ? if X is minimal
(in the sense of set-theoretic inclusion) among the sets
closed under ?.
To extend this definition to arbitrary programs, take any
program ?, and consistent set X of ground literals. The
reduct, ?X , of ? relative to X is the set of rules
l0 ? l1, . . . , lm
for all rules (4.1) in ? such that lm+1, . . . , ln 6? X . Thus
?X is a program without default negation.
Definition 2 (Answer set ? part two)
X is an answer set for ? if X is an answer set for ?X .
Definition 3 (Entailment)
A program ? entails a literal l (? |= l) if l belongs to all
answer sets of ?.
The ??s answer to a query l is yes if ? |= l, no if ? |= l,
and unknown otherwise.
5 Summary
In conclusion, we feel that the features of AnsProlog are
well suited to form the foundations for an inference en-
gine supporting a QAS. Our future work will develop the
support tools and implementation needed to demonstrate
this hypothesis.
References
C. Baker, C. Fillmore, and J. Lowe. The Berkeley
FrameNet project. In Proceedings of the COLING-
ACL, Montreal, Canada, 1998.
C. Baral. Knowledge representation, reasoning and
declarative problem solving. Cambridge University
Press, 2003.
C. Baral and M. Gelfond. Reasoning agents in dynamic
domains. In J Minker, editor, Logic Based AI. pp. 257?
279, Kluwer, 2000.
P. Blackburn and J. Bos. Computational Semantics.
Theoria, 18(1), pages 365?387, 2003.
N. Chang, S. Narayanan, R. Miriam, and L. Petruck.
From frames to inference. In Proceedings of the First
International Workshop on Scalable Natural Language
Understanding, Heidelberg, Germany, 2002.
C. Fellbaum (ed). WordNet: An Electronic Lexical
Database. MIT Press, 1998.
M. Gelfond and V. Lifschitz. The stable model semantics
for logic programming. In R. Kowalski and K. Bowen,
editors, Logic Programming: Proc. of the Fifth Int?l
Conf. and Symp., pages 1070?1080. MIT Press, 1988.
M. Gelfond and V. Lifschitz. Classical negation in logic
programs and disjunctive databases. New Generation
Computing, pages 365?387, 1991.
D. Gildea and D. Jurafsky. Automatic Labeling of Se-
mantic Roles. In Proceedings of ACL 2000, Hong
Kong, China, 2000.
Lierler Yu., and Maratea M. Cmodels-2: SAT-based An-
swer Sets Solver Enhanced to Non-tight Programs, In
Proc. of LPNMR-7, pp. 346, 2004.
J. McCarthy and P. Hayes. Some philosophical prob-
lems from the standpoint of artificial intelligence. In
B. Meltzer and D. Michie, editors, Machine Intelli-
gence, volume 4, pages 463?502. Edinburgh Univer-
sity Press, Edinburgh, 1969.
T. Eiter, N. Leone, C. Mateis., G. Pfeifer and F. Scar-
cello. A deductive system for nonmonotonic rea-
soning, Proceedings of the 4rd Logic Programming
and Non-Monotonic Reasoning Conference ? LPNMR
?97, LNAI 1265, Springer-Verlag, Dagstuhl, Germa-
nia, Luglio 1997, pp. 363?374.
I. Niemela and P. Simons. Smodels ? an implementa-
tion of the stable model and well-founded semantics
for normal logic programs. In Proc. 4th international
conference on Logic programming and non-monotonic
reasoning, pages 420?429, 1997.
R. Reiter. On closed world data bases. In H. Gallaire
and J. Minker, editors, Logic and Data Bases, pages
119?140. Plenum Press, New York, 1978.
H. Turner. Representing actions in logic programs and
default theories. Journal of Logic Programming, 31(1-
3):245?298, May 1997.
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 54?61, Detroit, June 2005. c?2005 Association for Computational Linguistics
IntEx: A Syntactic Role Driven Protein-Protein Interaction Extractor 
for Bio-Medical Text 
 Syed Toufeeq Ahmed, Deepthi Chidambaram, Hasan Davulcu*, Chitta Baral. 
Department of Computer Science and Engineering, 
 Arizona State University,  
Tempe, AZ 85287.  
{toufeeq, deepthi, hdavulcu, chitta}@asu.edu 
  
 
 
Abstract 
In this paper, we present a fully auto-
mated extraction system, named IntEx, to 
identify gene and protein interactions in 
biomedical text. Our approach is based on 
first splitting complex sentences into sim-
ple clausal structures made up of syntactic 
roles. Then, tagging biological entities 
with the help of biomedical and linguistic 
ontologies. Finally, extracting complete 
interactions by analyzing the matching 
contents of syntactic roles and their lin-
guistically significant combinations. Our 
extraction system handles complex sen-
tences and extracts multiple and nested in-
teractions specified in a sentence. 
Experimental evaluations with two other 
state of the art extraction systems indicate 
that the IntEx system achieves better per-
formance without the labor intensive pat-
tern engineering requirement. ?  
1 Introduction 
Genomic research in the last decade has resulted in 
the production of a large amount of data in the 
form of micro-array experiments, sequence infor-
mation and publications discussing the discoveries. 
The data generated by these experiments is highly 
                                                          
? To whom correspondence should be addressed 
connected; the results from sequence analysis and 
micro-arrays depend on functional information and 
signal transduction pathways cited in peer-
reviewed publications for evidence. Though scien-
tists in the field are aided by many online data-
bases of biochemical interactions, currently a 
majority of these are curated labor intensively by 
domain experts. Information extraction from text 
has therefore been pursued actively as an attempt 
to extract knowledge from published material and 
to speed up the curation process significantly.  
In the biomedical context, the first step towards 
information extraction is to recognize the names of 
proteins (Fukuda, Tsunoda et al 1998), genes, 
drugs and other molecules. The next step is to rec-
ognize interaction events between such entities  
(Blaschke, Andrade et al 1999; Blaschke, Andrade 
et al 1999; Hunter 2000; Thomas, Milward et al 
2000; Thomas, Rajah et al 2000; Ono, Hishigaki 
et al 2001; Hahn and Romacker 2002) and then to 
finally recognize the relationship between interac-
tion events. However, several issues make extract-
ing such interactions and relationships difficult 
since (Seymore, McCallum et al1999) (i) the task 
involves free text ? hence there are many ways of 
stating the same fact (ii) the genre of text is not 
grammatically simple (iii) the text includes a lot of 
technical terminology unfamiliar to existing natu-
ral language processing systems (iv) information 
may need to be combined across several sentences, 
and (v) there are many sentences from which noth-
ing should be extracted. 
In this paper, we present a fully automated extrac-
tion approach to identify gene and protein interact- 
54
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
tions in natural language text with the help of bio-
medical and linguistic ontologies. Our approach 
works in three main stages: 
1. Complex Sentence Processor (CSP): First, is 
splitting complex sentences into simple clausal 
structures made of up syntactic roles. 
2. Tagging: Then, tagging biological entities 
with the help of biomedical and linguistic on-
tologies.  
3. Interaction Extractor: Finally, extracting 
complete interactions by analyzing the match-
ing contents of syntactic roles and their lin-
guistically significant combinations.  
The novel aspects of our system are its ability to 
handle complex sentence structures using the 
Complex Sentence Processor (CSP) and to extract 
multiple and nested interactions specified in a sen-
tence using the Interaction Extractor without the 
labor intensive pattern engineering requirement. 
Our approach is based on identification of syntac-
tic roles, such as subject, objects, verb and modifi-
ers, by using the word dependencies. We have used 
a dependency based English grammar parser, the 
Link Grammar (Sleator and Temperley 1993), to 
identify the roles. Syntactic roles are utilized to 
transform complex sentences into their multiple 
clauses each containing a single event. This clausal 
structure enables us to engineer an automated algo-
rithm for the extraction of events thus overcoming 
the burden of labor intensive pattern engineering 
for complex and compound sentences. Pronoun 
resolution module assists Interaction Extractor in 
identifying interactions spread across multiple sen-
tences using pronominal references. We performed 
comparative experimental evaluations with two 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
state of the art systems.  Our experimental results 
show that the IntEx system presented here achieves 
better performance without the labor intensive rule 
engineering step which is required for these state 
of the art systems.  
 
The rest of the paper is organized as follows. In 
Section 2 we survey the related work. In Section 3 
we present an architectural overview of the IntEx 
system. Sections 4 and 5 explain and illustrate the 
individual modules of the IntEx system. A detailed 
evaluation of our system with the BioRAT 
(Corney, Buxton et al 2004) and GeneWays 
(Rzhetsky, Iossifov et al 2004) is presented in Sec-
tion 6. Section 7 concludes the paper.  
  
2 Related Work 
Information extraction is the extraction of salient 
facts about pre-specified types of events, entities 
(Bunescu, Ge et al 2003) or relationships from 
free text. Information extraction from free-text util-
izes shallow-parsing techniques (Daelemans, 
Buchholz et al 1999), Parts-of-Speech tag-
ging(Brill 1992), noun and verb phrase chunking 
(Mikheev and Finch 1997), verb subject and object 
relationships (Daelemans, Buchholz et al 1999), 
and learned (Califf and Mooney 1998; Craven and 
Kumlein 1999; Seymore, McCallum et al 1999) or 
hand-build patterns to automate the creation of 
specialized databases. 
Manual pattern engineering approaches employ 
shallow parsing with patterns to extract the interac-
tions. In the (Ono, Hishigaki et al 2001) system, 
55
sentences are first tagged using a dictionary based 
protein name identifier and then processed by a 
module which extracts interactions directly from 
complex and compound sentences using regular 
expressions based on part of speech tags. 
 
The SUISEKI system of Blaschke (Blaschke, 
Andrade et al 1999) also uses regular expressions, 
with probabilities that reflect the experimental ac-
curacy of each pattern to extract interactions into 
predefined frame structures.  
 
GENIES (Friedman, Kra et al 2001) utilizes a 
grammar based NLP engine for information extrac-
tion. Recently, it has been extended as GeneWays 
(Rzhetsky, Iossifov et al 2004), which also pro-
vides a Web interface that allows  users to search 
and submit papers of interest for analysis. The 
BioRAT system (Corney, Buxton et al 2004) uses 
manually engineered  templates that combine lexi-
cal and semantic information to identify protein 
interactions. The GeneScene system(Leroy, Chen 
et al 2003) extracts interactions using frequent 
preposition-based templates.  
 
Grammar engineering approaches, on the other 
hand use manually generated specialized grammar 
rules (Rinaldi, Schneider et al 2004) that perform 
a deep parse of the sentences. Temkin (Temkin and 
Gilder 2003) addresses the problem of extracting 
protein interactions by using an extendable but 
manually built Context Free Grammar (CFG) that 
is designed specifically for parsing  biological text. 
The PathwayAssist system uses an NLP system, 
MedScan (Novichkova, Egorov et al 2003), for the 
biomedical domain that tags the entities in text and 
produces a semantic tree. Slot filler type rules are 
engineered based on the semantic tree representa-
tion to extract relationships from text. Recently, 
extraction systems have also used link grammar 
(Grinberg, Lafferty et al 1995) to identify interac-
tions between proteins (Ding, Berleant et al 2003). 
Their approach relies on various linkage paths be-
tween named entities such as gene and protein 
names. Such manual pattern engineering ap-
proaches for information extraction are very hard 
to scale up to large document collections since they 
require labor-intensive and skill-dependent pattern 
engineering. 
Machine learning approaches have also been used 
to learn extraction rules from user tagged training 
data. These approaches represent the rules learnt in 
various formats such as decision trees (Chiang, Yu 
et al 2004) or grammar rules (Phuong, Lee et al 
2003). Craven et al(Craven and Kumlien 1999) 
explored an automatic rule-learning approach that 
uses a combination of FOIL (Quinlan 1990) and 
Na?ve Bayes Classifier to learn extraction rules.  
  
3 System Architecture 
The sentences in English are classified as either 
simple, complex, compound or complex-
compound based on the number and types of 
clauses present in them. Our extraction system re-
solves the complex, compound and complex-
compound sentence structures (collectively re-
ferred to as complex sentence structures in this 
document) into simple sentence clauses which con-
tain a subject and a predicate. These simple sen-
tence clauses are then processed to obtain the 
interactions between proteins. The architecture of 
the IntEx system is shown in Figure 1, and the fol-
lowing Sections 4 and 5 explain the workings of its 
modules. 
4 Complex Sentence Processing  
4.1 Pronoun Resolution 
Interactions are often specified through pronominal 
references to entities in the discourse, or through 
co references where, a number of phrases are used 
to refer to the same entity. Hence, a complete ap-
proach to extracting information from text should 
also take into account the resolution of these refer-
ences. References to entities are generally catego-
rized as co-references or anaphora and has been 
investigated using various approaches (Casta?o, 
Zhang et al 2002). IntEx anaphora resolution sub-
system currently focuses on third person pronouns 
and reflexives since the first and second person 
pronouns are frequently used to refer to the authors 
of the papers.  
Our pronoun resolution module uses a heuristic 
approach to identify the noun phrases referred by 
the pronouns in a sentence. The heuristic is based 
on the number of the pronoun (singular or plural) 
and the proximity of the noun phrase. The first 
noun phrase that matches the number of the pro-
noun is considered as the referred phrase.   
56
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4.2 Entity Tagger 
The entity tagging module marks the names of 
genes, and proteins in text. The process of tagging 
is a combination of dictionary look up and heuris-
tics. Regular expressions are also used to mark the 
names that do not have a match in the dictionaries. 
The protein name dictionaries for the entity tagger  
are derived from various biological sources such as 
UMLS1, Gene Ontology2 and Locuslink3 database  
 
 
 
                                                          
1 http://www.nlm.nih.gov/research/umls/ 
2 http://www.geneontology.org/ 
3 http://www.ncbi.nlm.nih.gov/LocusLink/ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
.  
 
 Fig. 3 Example - a) A Sentence from an abstract (PMID: 1956405). b) Pronoun ?it?s? is resolved 
with ?The SAC6 gene?. c) Each row represents a simple sentence, d) for each constituent, role type is 
resolved and interaction words are tagged, e) Protein-Protein interaction is extracted. 
4.3 Preprocessor 
The tagged sentences need to be pre-processed to 
replace syntactic constructs, such as parenthesized 
nouns and domain specific terminology that cause 
the Link Grammar Parser to produce an incorrect 
output. This problem is overcome by replacing 
such elements with alternative formats that is rec-
ognizable by the parser.  
 
4.4 Link Grammar and the Link grammar 
parser 
Link grammar (LG)  introduced by  Sleator and 
Temperley (Sleator and Temperley 1991) is a de-
pendency based grammatical system. The basic 
idea of link grammar is to connect pairs of words 
57
in a sentence with various syntactically significant 
links.  The LG consists of set of words, each of 
which has various alternative linking requirements.   
 
A linking requirement can be seen as a block with 
connectors above each word. A connector is satis-
fied by matching it with compatible connector. 
Fig.2 below shows how linking requirements can 
be satisfied to produce a parse for the example sen-
tence "The dog chased a cat". 
  
 
 
 
 
 
 
 
Even though LG has no explicit notion of constitu-
ents or categories (Sleator and Temperley 1993), 
they emerge as contiguous connected sequence of 
words attached to the rest of sentence by a particu-
lar types of links, as in the above example where 
?the dog?  and ?a cat? are connected to the main 
verb via ?S? and ?O? links respectively. Our algo-
rithms utilize this property of LG where certain 
link types allow us to extract the constituents of 
sentences irrespective of the tense. The LG 
parser?s ability to detect multiple verbs and their 
constituent linkage in complex sentences makes it 
particularly well suited for our approach during 
resolving of complex sentences into their multiple 
clauses. The LG parsers? dictionary can also be 
easily enhanced to produce better parses for bio-
medical text (Szolovits 2003). 
4.5  Complex Sentence Processor Algorithm 
The complex sentence processor (CSP) component 
splitsthe complex sentences into a collection of 
simple sentence clauses which contain a subject 
and a predicate. The CSP follows a verb-based ap-
proach to extract the simple clauses. A sentence is 
identified to be complex it contains more than one 
verb. A simple sentence is identified to be one with 
a subject, a verb, objects and their modifying 
phrases. The example in Figure 3 illustrates the 
major steps involved during complex sentence 
processing. The following schema is used as the 
format to represent simple clauses: 
    Subject | Verb | Object | Modifying phrase to the 
verb 
5 Interaction Extraction 
Interaction Extractor (IE) extracts interactions 
from simple sentence clauses produced by the 
complex sentence processor. The highly technical 
terminology and the complex grammatical con-
structs that are present in the biomedical abstracts 
make the extraction task difficult, Even a simple 
sentence with a single verb can contain multiple 
and/or nested interactions. That?s why our IE sys-
tem is based on a deep parse tree structure pre-
sented by the LG and it considers a thorough case 
based analysis of contents of various syntactic 
roles of the sentences like their subjects (S), verbs 
(V), objects (O) and modifying phrases (M) as well 
as their linguistically significant and meaningful 
combinations like S-V-O, S-O, S-V-M or S-M, il-
lustrated in Figure 4, for finding and extracting 
protein-protein interactions.  
Figure 2: Link grammar representation of a sentence 
 
 
 
 
 
 
 
 
 
 
 
5.1 Role Type Matcher  
For each syntactic constituent of the sentence, the 
role type matcher identifies the type of each role as 
either ?Elementary?, ?Partial? or ?Complete? based 
on its matching content, as presented  in Table 1.  
Table 1: Role Type Matcher 
Role  Type Description 
Elementary If the role contains a Protein name or an 
interaction word. 
Partial  If the role has a Protein name and an interac-
tion word.  
Complete If the role has at least two Protein names and 
an interaction word. 
Figure 4: Interaction Extraction: Composition and analysis 
of various syntactic roles.  
S O M
S-O S-M 
Subject (S) Modifying Phrase (M)Object (O) 
S-V-O S-V-M 
58
5.2 Interaction Word Tagger 
 The words that match a biologically significant 
action between two gene/protein names are labeled 
as ?interaction words?. Our gazetteer for interaction 
words is derived from UMLS and WordNet4. Por-
ter Stemmer (Porter 1997) was also used for stem-
ming such words before matching.  
 
5.3 Interaction Extractor (IE) 
IntEx interaction extractor works as follows. The 
input to IE is the preprocessed and typed simple 
clause structures. The IE algorithm progresses bot-
tom up, starting from each syntactic role S, V or 
M, and expanding them using the lattice provided 
in Figure 4 until all ?Complete? singleton or com-
posite role types are obtained.  
 
Consider the example shown in Figure 3, for the 
third sentence, the boundaries of the subject and 
the modifying phrase are identified and both are 
role typed as ?Elementary? using Table 1. Since the 
main verb is tagged as an interaction word, IE uses 
the S-V-M composite role from Figure 4 to find 
and extract the following complete interaction:    
          
{?The SAC 6 gene Protein?, ?colocalizes?, ?actin?}.  
 
?Complete? roles also need to be analyzed in order 
to determine their voice as ?active? or ?passive?. 
Since there are only a small number of preposition 
combinations, such as of-by, from-to etc., that oc-
cur frequently within the clauses, they can be used 
to distinguish the agent and the theme of the inter-
actions.  
 
For example, in the sentence ?The kinase phos-
phorylation of pRb by c-Abl in the gland could 
inhibit ku70?, the subject role is ?The kinase phos-
phorylation of pRb by c-Abl in the gland?. Since 
the subject has at least two protein names and an 
interaction word it is ?complete?. By using the ?of-
by? pattern (?<Interaction-Word (action)>... of  
...<theme>?by  ...<agent>?) the IE is able to 
extract the correct interaction {c-Abl, phosphoryla-
tion, pRb} from the subject role alone. 
                                                          
                                                          
4 http://www.cogsci.princeton.edu/~wn/ 
6 Evaluation & discussion  
We have evaluated the performance of our system 
with two state of the art systems - BioRAT 
(Corney, Buxton et al 2004) and GeneWays 
(Rzhetsky, Iossifov et al 2004).  
 
Blaschke and Valencia (Valencia 2001) recom-
mend DIP (Xenarios, Rice et al 2000) dataset  as a 
benchmark for evaluating biomedical Information 
Extraction systems. The first evaluation for IntEx 
system was performed on the same dataset 5 that 
was used for the BioRAT evaluation. For BioRAT 
evaluation, authors identified 389 interactions from 
the DIP database such that both proteins participat-
ing in the interaction had SwissProt entries. These 
interactions correspond to 229 abstracts from the 
PubMed. The BioRAT system was evaluated using 
these 229 abstracts. The interactions extracted by 
the system were then manually examined by a do-
main expert for precision and recall. Precision is a 
measure of correctness of the system, and is calcu-
lated as the ratio of true positives to the sum of true 
positives and false positives. The sensitivity of the 
system is given by the recall measure, calculated as 
the ratio of true positives to the sum of true posi-
tives and false negatives.  
 
Table 2: Recall comparison of IntEx and BioRAT from  229 ab-
stracts when compared with DIP database. 
IntEx BioRAT Recall 
Results Cases Percent (%) Cases Percent(%) 
Match 142 26.94 79 20.31 
No 
Match 385 73.06 310 79.67 
Totals 527 100.00 389 100.00 
 
We have also limited our protein name dictionary 
to the SwissProt entries. Tables 2 and 3 present the 
evaluation results as compared with the BioRAT 
system. A detailed analysis of the sources of all 
types of errors is shown in Figure 6.  
5 Dataset was obtained from Dr. David Corney by personal 
communication. 
59
Table 3: Precision comparison of IntEx and BioRAT  from  229 
abstracts.  
 
DIP contains protein interactions from both ab-
stracts and full text. Since our extraction system 
was tested only on the abstracts, the system missed 
out on some interactions that were only present in 
the full text of the abstract.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Second evaluation for the IntEx system was done 
to test its recall performance using an article6 that 
was also used by the GeneWays (Rzhetsky, Iossi-
fov et al 2004) system. Both systems performance 
was tested using the full text of the article 
(Friedman, Kra et al 2001). GeneWays system 
achieves a recall of 65% where as IntEx extracted 
a total of 44 interactions corresponding to a recall 
measure of 66 %.  
Conclusion 
In this paper, we present a fully automated extrac-
tion system for identifying gene and protein inter-
                                                          
6 Dataset was obtained from Dr. Andrew Rzhetsky by personal 
communication. 
actions from biomedical text. The source code and 
documentation of the IntEx system, as well as all 
experimental documents and extracted interactions 
are available online at our Web site at 
http://cips.eas.asu.edu/textmining.htm. Our extrac-
tion system handles complex sentences and ex-
tracts multiple nested interactions specified in a 
sentence. Experimental evaluations of the IntEx 
system with the state of the art semi-automated 
systems -- the BioRAT and GeneWays datasets 
indicates that our system performs better without 
the labor intensive rule engineering requirement. 
We have shown that a syntactic role-based ap-
proach compounded with linguistically sound in-
terpretation rules applied on the full sentence?s 
parse can achieve better performance than existing 
systems which are based on manually engineered 
patterns which are both costly to develop and are 
not as scalable as the automated mechanisms pre-
sented in this paper.  
IntEx BioRAT Precision  
Results Cases Percent (%) Cases 
Percent 
(%) 
Correct 262 65.66 239 55.07 
Incorrect 137 34.33 195 44.93 
Totals 399 100.00 434 100.00 
Acknowledgements 
We would like to thank to both Dr. David Corney 
and Dr. Andrew Rzhetsky for sharing their evalua-
tion datasets and results. 
References 
Blaschke, C., M. A. Andrade, et al (1999). "Automatic extrac-
tion of biological information from scientific text: Protein-
protein interactions." Proceedings of International Sympo-
sium on Molecular Biology: 60-67. 
 
Brill, E. (1992). "A simple rule-based part-of-speech tagger." 
Proceedings of ANLP 92, 3rd Conference on Applied 
Natural Language Processing: 152-155. Figure 6: Analysis of types of errors encountered
 
Bunescu, R., R. Ge, et al (2003). Comparative Experiments 
on Learning Information Extractors for Proteins and   their 
Interactions. Artificial Intelligence in Medicine. 
 
Califf, M. E. and R. J. Mooney (1998). "Relational learning of 
pattern-match rules for information extraction." Working 
Notes of AAAI Spring Symposium on Applying Machine 
Learning to Discourse Processing: 6-11. 
 
Casta?o, J., J. Zhang, et al (2002). Anaphora Resolution in 
Biomedical Literature. International Symposium on Refer-
ence Resolution. Alicante, Spain. 
 
Chiang, J.-H., H.-C. Yu, et al (2004). "GIS: a biomedical text-
mining system for gene information discovery." Bioinfor-
matics 20(1): 120-121. 
 
60
Corney, D. P. A., B. F. Buxton, et al (2004). "BioRAT: ex-
tracting biological information from full-length papers." 
Bioinformatics 20(17): 3206-3213. 
 
Craven, M. and J. Kumlien (1999). Constructing Biological 
Knowledge Bases by Extracting Information from Text   
Sources. Proceedings of the Seventh International Confer-
ence on Intelligent Systems   for Molecular Biology: 77--
86. 
 
Daelemans, W., S. Buchholz, et al (1999). "Memory-based 
shallow parsing." Proceedings of CoNLL. 
 
Ding, J., D. Berleant, et al (2003). Extracting Biochemical 
Interactions from MEDLINE Using a Link Grammar 
Parser. Proceedings of the 15th IEEE International Confer-
ence on Tools with   Artificial Intelligence (ICTAI'03): 
467. 
 
Friedman, C., P. Kra, et al (2001). GENIES: a natural-
language processing system for the extraction of molecular   
pathways from journal articles. Proceedings of the Interna-
tional Confernce on Intelligent Systems for   Molecular Bi-
ology: 574-82. 
 
Fukuda, K., T. Tsunoda, et al (1998). "Toward information 
extraction: Identifying protein names from biological pa-
pers." PSB 1998,: 705-716. 
 
Grinberg, D., J. Lafferty, et al (1995). "A Robust Parsing 
Algorithm For LINK Grammars." (CMU-CS-TR-95-125). 
 
Hahn, U. and M. Romacker (2002). "Creating knowledge 
repositories from biomedical reports: The medsyndikate 
text mining system." Pacific Symposium on Biocomputing 
2002: 338-349. 
 
Hunter, R. T. a. C. R. a. J. (2000). "Extracting Molecular 
Binding Relationships from Biomedical Text." In Proceed-
ings of  the ANLP-NAACL 000,Association for Computa-
tional Linguistics: pages 188-195. 
 
Leroy, G., H. Chen, et al (2003). Genescene: biomedical text 
and data mining. Proceedings of the third ACM/IEEE-CS 
joint conference on Digital libraries: 116--118. 
 
Mikheev, A. and S. Finch (1997). "A workbench for finding 
structure in texts." Proceedings of Applied Natural Lan-
guage Processing (ANLP-97). 
 
Novichkova, S., S. Egorov, et al (2003). "MedScan, a natural 
language processing engine for MEDLINE abstracts." Bio-
informatics 19(13): 1699-1706. 
 
Ono, T., H. Hishigaki, et al (2001). "Automatic Extraction of 
Information on protein-protein interactions from the bio-
logical literature." Bioinformatics 17(2): 155-161. 
 
Phuong, T. M., D. Lee, et al (2003). "Learning Rules to Ex-
tract Protein Interactions from Biomedical Text." PAKDD 
2003: 148-158. 
 
Porter, M. F. (1997). "An algorithm for suffix stripping." Pro-
gam, vol. 14, no. 3, July 1980: 313--316. 
 
Quinlan, J. R. (1990). "Learning Logical Definitions from 
Relations." Mach. Learn. 5(3): 239--266. 
 
Rinaldi, F., G. Schneider, et al (2004). Mining relations in the 
GENIA corpus. Proceedings of the Second European 
Workshop on Data Mining and Text Mining   for Bioin-
formatics: 61 - 68. 
 
Rzhetsky, A., I. Iossifov, et al (2004). "GeneWays: a system 
for extracting, analyzing, visualizing, and integrating   mo-
lecular pathway data." J. of Biomedical Informatics 37(1): 
43--53. 
 
Seymore, K., A. McCallum, et al (1999). Learning hidden 
markov model structure for information extraction. AAAI 
99 Workshop on Machine Learning for Information Extrac-
tion. 
 
Sleator, D. and D. Temperley (1991). Parsing English with a 
Link Grammar.Carnegie Mellon University Computer Sci-
ence technical report CMU-CS-91-196, Carnegie Mellon 
University. 
 
Sleator, D. and D. Temperley (1993). Parsing English with a 
Link Grammar. Third International Workshop on Parsing 
Technologies. 
 
Szolovits, P. (2003). "Adding a Medical Lexicon to an English 
Parser." Proc. AMIA 2003 Annual Symposium.: 639-643. 
 
Temkin, J. M. and M. R. Gilder (2003). "Extraction of protein 
interaction information from unstructured text using   a 
context-free grammar." Bioinformatics 19(16): 2046-2053. 
 
Thomas, J., D. Milward, et al (2000). "Automatic extraction 
of protein interactions from scientific abstracts." Proceed-
ings of the Pacific Symposium on Biocomputing 5: 502-
513. 
 
Thomas, R., C. Rajah, et al (2000). "Extracting molecular 
binding relationships from biomedical text." Proceedings of 
the ANLP-NAACL 2000: 188-195. 
 
Valencia, B. a. (2001). "Can bibliographic pointers for known 
biological data be found automatically? Protein interactions 
as a case study." Comp. Funct. Genom. 2: 196-206. 
 
Xenarios, I., D. W. Rice, et al (2000). "DIP: the Database of 
Interacting Proteins." Nucl. Acids Res. 28(1): 289-291. 
 
 
61
Using Inverse ? and Generalization to Translate English to
Formal Languages
Chitta Baral
Arizona State University
chitta@asu.edu
Juraj Dzifcak
Arizona State University
juraj.dzifcak@asu.edu
Marcos Alvarez Gonzalez
Arizona State University
malvar@asu.edu
Jiayu Zhou
Arizona State University
Jiayu.Zhou@asu.edu
Abstract
We present a system to translate natural language sentences to formulas in a formal or a knowl-
edge representation language. Our system uses two inverse ?-calculus operators and using them can
take as input the semantic representation of some words, phrases and sentences and from that de-
rive the semantic representation of other words and phrases. Our inverse ? operator works on many
formal languages including first order logic, database query languages and answer set programming.
Our system uses a syntactic combinatorial categorial parser to parse natural language sentences and
also to construct the semantic meaning of the sentences as directed by their parsing. The same parser
is used for both. In addition to the inverse ?-calculus operators, our system uses a notion of gener-
alization to learn semantic representation of words from the semantic representation of other words
that are of the same category. Together with this, we use an existing statistical learning approach to
assign weights to deal with multiple meanings of words. Our system produces improved results on
standard corpora on natural language interfaces for robot command and control and database queries.
1 Introduction
Our long term goal is to develop general methodologies to translate natural language text into a formal
knowledge representation (KR) language. In the absence of a single KR language that is appropriate
for expressing all the nuances of a natural language, currently, depending on the need different KR
languages are used. For example, while first-order logic is appropriate for mathematical knowledge, one
of its subset Description logic is considered appropriate for expressing ontologies, temporal logics are
considered appropriate for expressing goals of agents and robots, and various non-monotonic logics have
been proposed to express common-sense knowledge. Thus, one of of our goals in this paper is to develop
general methodologies that can be used in translating natural language to a desired KR language.
There have been several learning based approaches, mainly from two groups at MIT and Austin.
These include the following works: Zettlemoyer and Collins (2005), Kate and Mooney (2006), Wong
and Mooney (2006), Wong and Mooney (2007), Lu et al (2008), Zettlemoyer and Collins (2007) and Ge
and Mooney (2009). Given a training corpus of natural language sentences coupled with their desired
representations, these approaches learn a model capable of translating sentences to a desired meaning
representation. For example, in the work by Zettlemoyer and Collins (2005), a set of hand crafted
rules is used to learn syntactic categories and semantic representations of words based on combinatorial
categorial grammar (CCG), as described by Steedman (2000), and ?-calculus formulas, as discussed
by Gamut (1991). The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. The
Austin group has several papers over the years. Many of their works including the one by Ge andMooney
(2009) use a word alignment method to learn semantic lexicon and learn rules for composing meaning
representation.
35
Similar to the work by Ge and Mooney (2009), we use an existing syntactic parser to parse natural
language. However we use a CCG parser, as described by Clark and Curran (2007), to parse sentences,
use lambda calculus for meaning representation, use the CCG parsing to compose meaning and have an
initial dictionary. Note that unlike the work by Ge and Mooney (2009), we do not need to learn rules
for composing meaning representation. We use a novel method to learn semantic lexicon which is based
on two inverse lambda operators that allow us to compute F given G and H such that F@G = H
or G@F = H . Compared to the work by Zettlemoyer and Collins (2005), we use the same learning
approach but use a completely different approach in lexical generation. Our inverse ? operator has been
tested to work for many languages including first order logic, database query language, CLANG by
Chen et al (2003), answer set programming (ASP) as described by Baral (2003), and temporal logic.
Thus our approach is not dependent on the language used to represent the semantics, nor limited by a
fixed set of rules. Rather, the new ?-calculus formulas and their semantic models, corresponding to the
semantic or meaning representations, are directly obtained from known semantic representations which
were provided with the data or learned before. The richness of ? calculus allows us to rely only on the
syntactic parse itself without the need to have separate rules for composing the semantics. The provided
method yields improved experimental results on existing corpora on robot command and control and
database queries.
2 Motivation and Background
We now illustrate how one can use CCG parsing and ?-calculus applications to obtain database query
representation of sentences. We then motivate and explain the role of our ?inverse ?? operator. A
syntactic and semantic parse tree for the sentence ?Give me the largest state.? is given in Table 1.
Give me the largest state.
S/NP NP/N N/N N
S/NP NP/N N
S/NP NP
S
Give me the largest state.
?x.answer(A, x@A) ?x.x ?x.?y.largest(y, x@y) ?z.state(z)
?x.answer(A, x@A) ?x.x ?y.largest(y, state(y))
?x.answer(A, x@A) ?y.largest(y, state(y))
answer(A, largest(A, state(A)))
Table 1: CCG and ?-calculus derivation for ?Give me the largest state.?
The upper portion of the figure lists the nodes corresponding to the CCG categories which are used to
syntactically parse the sentence. These are assigned to each word and then combined using combinatorial
rules, as described by Steedman (2000), to obtain the categories corresponding to parts of the sentence
and finally the complete sentence itself. For example, the category for ?largest?, N/N is combined with
the category of ?state.?,N , to obtain the category of ?largest state.?, which isN . In a similar manner, each
word is assigned a semantic meaning in the form of a ?-calculus formula, as indicated by the lower por-
tion of the figure. The language used to represent the semantics of words and the sentence is the database
query language used in the robocup domain. The formulas corresponding to words are combined by ap-
plying one to another, as dictated by the syntactic parse tree to obtain the semantic representation of the
whole sentence. For example, the semantics of ?the largest state.?, ?y.largest(y, state(y)) is applied
to the semantics of ?Give me?, ?x.answer(A, x@A), to obtain the semantics of ?Give me the largest
state.?, answer(A, largest(A, state(A))).
The given example illustrates how to obtain the semantics of the sentence given the semantics of
words. However, what happens if the semantics of the word ?largest? is not given? It might be either
missing completely, or the current semantics of ?largest? in the dictionary might simply not be applicable
36
for the sentence ?Give me the largest state.?.
Let us assume that the semantic representation of ?largest? is not known, while the semantic repre-
sentation of the rest of the sentence is known. We can then obtain the semantic representation of ?largest?
as follows. Given the formula answer(A, largest(A, state(A))) for the whole sentence ?Give me the
largest state.? and the formula ?x.answer(A, x@A) for ?Give me?, we can perform some kind of an in-
verse application 1 to obtain the semantics representation of ?the largest state?, ?y.largest(y, state(y)).
Similarly, we can then use the known semantics of ?the?, to obtain the semantic representation of ?largest
state.? as ?y.largest(y, state(y)). Finally, using the known semantics of state, ?z.state(z) we can ob-
tain the the semantics of ?largest? as ?x.?y.largest(y, x@y).
It is important to note that using @ we are able to construct relatively complex semantic representa-
tions that are properly mapped to the required syntax.
Given a set of training sentences with their desired semantic representations, a syntactic parser, such
as the one by Clark and Curran (2007), and an initial dictionary, we can apply the above idea on each
of the sentences to learn the missing semantic representations of words. We can then apply a learning
model, such as the one used by Zettlemoyer and Collins (2005), on these new semantic representations
and assign weights to different semantic representations. These can then be used to parse and represent
the semantics of new sentences. This briefly sums up our approach to learn and compute new semantic
representations. It is easy to see that this approach can be applied with respect to any language that can
be handled by ?inverse ?? operators and is not limited in the set of new representations it provides.
We will consider two domains to evaluate our approach. The fist one is the GEOQUERY domain used
by Zelle and Mooney (1996), which uses a Prolog based language to query a database with geographical
information about the U.S. It should be noted that this language uses higher-order predicates. An example
query is provided in Table 1. The second domain is the ROBOCUP domain of Chen et al (2003). This is
a multi-agent domain where agents compete against each other in a simulated soccer game. The language
CLANG of Chen et al (2003) is a formal language used to provide instructions to the agents. An example
query with the corresponding natural language sentence is given below.
? If the ball is in our midfield, position player 3 at (-5, -23).
? ((bpos (midfield our)) (do (player our 3) (pos (pt -5 -23))))
3 Learning Approach
We adopt the learning model given by Zettlemoyer and Collins (2005, 2007, 2009) and use it to assign
weights to the semantic representations of words. Since a word can have multiple possible syntac-
tic and semantic representations assigned to it, such as John may be represented as John as well as
?x.x@John, we use the probabilistic model to assign weights to these representations.
The main differences between our algorithm and the one given by Zettlemoyer and Collins (2005)
are the way in which new semantic representations are obtained. While Zettlemoyer and Collins (2005)
uses a predefined table to obtain these, we obtain the new semantic representations by using inverse ?
operators and generalization.
3.1 Learning model and parsing
We assume that complete syntactic parses are available2. The parsing uses a probabilistic combinatorial
categorial grammar framework similar to the one given by Zettlemoyer and Collins (2005). We assume a
probabilistic categorial grammar (PCCG) based on a log linear model. Let S denote a sentence, L denote
the semantic representation of the sentence, and T denote it?s parse tree. We assume a mapping f? of a
triple (L, T, S) to feature vectors Rd and a vector of parameters ?? ? Rd representing the weights. Then
the probability of a particular syntactic and semantic parse is given as:
1Thus instead of applying G to F to obtain H , G@F = H , we try to find an F such that G@F = H given G and H .
2A sentence can have several different parses.
37
P (L, T |S; ??) = ef?(L,T,S).???
(L,T ) e
f?(L,T,S).??
We use only lexical features. Each feature fj counts the number of times that the lexical entry is used
in T .
Parsing a sentence under PCCG includes finding L such that P (L|S; ??) is maximized.
argmaxLP (L|S; ??) =
argmaxL
?
T P (L, T |S; ??)
We use dynamic programming techniques to calculate the most probable parse for a sentence.
3.2 The inverse ? operators
For lack of space, we present only one of the two Inverse ? operators, InverseL and InverseR of
Gonzalez (2010). The objective of these two algorithms is that given typed ?-calculus formulas H and
G, we want to compute the formula F such that F@G = H and G@F = H . First, we introduce the
different symbols used in the algorithm and their meaning :
? Let G, H represent typed ?-calculus formulas, J1,J2,...,Jn represent typed terms, v1 to vn, v and
w represent variables and ?1,...,?n represent typed atomic terms.
? Let f() represent a typed atomic formula. Atomic formulas may have a different arity than the one
specified and still satisfy the conditions of the algorithm if they contain the necessary typed atomic
terms.
? Typed terms that are sub terms of a typed term J are denoted as Ji.
? If the formulas we are processing within the algorithm do not satisfy any of the if conditions then
the algorithm returns null.
Definition 1 (operator :) Consider two lists of typed ?-elements A and B, (ai, ..., an) and (bj , ..., bn)
respectively and a formula H . The result of the operation H(A : B) is obtained by replacing ai by bi,
for each appearance of A in H.
Next, we present the definition of an inverse operators3 InverseR(H,G):
Definition 2 (InverseR(H,G)) The function InverseR(H,G), is defined as:
Given G and H:
1. If G is ?v.v@J , set F = InverseL(H, J)
2. If J is a sub term of H and G is ?v.H(J : v) then F = J .
3. If G is not ?v.v@J , J is a sub term of H and G is ?w.H(J(J1, ..., Jm) : w@Jp, ...,@Jq) with 1
? p,q,s ? m. then F = ?v1, ..., vs.J(J1, ..., Jm : vp, ..., vq).
The function InverseL(H,G) is defined similarly.
Illustration: InverseR - Case 3:
Suppose H = in(river, Texas) and G = ?v.v@Texas@river
G is not of the form ?v.v@J since J = Texas@river is not a formula. Thus the first condition is not
satisfied. Similarly, there is no J that satisfies the second condition. Thus let us try to find a suitable J
that satisfies third condition. If we take J1 = river and J2 = Texas, then the third condition is satisfied
by G = ?x.H((J(J1, J2) : x@J2@J1), which in this case corresponds to G = ?x.H(in(river, Texas) :
x@Texas@river). Thus, F = ?v1, v2.J(J1, J2 : v2, v1) and so F = ?v1, v2.in(v2, v1).
It is easy to see that G @ F = H .
3This is the operator that was used in this implementation. In a companion work we develop an enhancement of this operator
which is proven sound and complete.
38
3.3 Generalization
Using INV ERSE L and INV ERSE R, we are able to obtain new semantic representations of par-
ticular words in the sentence. However, without any form of generalization, we are not able to extend
these to words beyond the ones actually contained in the training data. Since our goal is to go beyond
that, we strive to generalize the new semantic representations beyond those words.
To extend our coverage, a function that will take any new learned semantic expressions and the cur-
rent lexicon and will try to use them to obtain new semantic expressions for words of the same category
has to be designed. It will use the following idea. Consider the non-transitive verb ?fly? of category
S\NP . Lets assume we obtain a new semantic expression for ?fly? as ?x.fly(x) using INV ERSE L
and INV ERSE R. The GENERALIZE function looks up all the words of the same syntactic cat-
egory, S\NP . It then identifies the part of the semantic expression in which ?fly? is involved. In our
particular case, it?s the subexpression fly. It then proceeds to search the dictionary for all the words of
category S\NP . For each such wordw, it will add a new semantic expression ?x.w(x) to the dictionary.
For example for the verb ?swim?, it would add ?x.swim(x).
However, the above idea also comes with a drawback. It can produce a vast amount of new se-
mantics representations that are not necessary for most of the sentences, and thus have a negative
impact on performance. Thus instead of applying the above idea on the whole dictionary, we per-
form generalization ?on demand?. That is, if a sentence contains words with unknown semantics, we
look for words of the same category and use the same idea to find their semantics. Let us assume
IDENTIFY (word, semantics) identifies the parts of semantics in which word is involved and
REPLACE(s, a, b) replaces a with b in s. We assume that each lexical entry is a triple (w, cat, sem)
where w is the actual word, cat is the syntactic category and sem is the semantic expression correspond-
ing to w and cat.
GENERALIZED(L,?)
? For each lj ? L
? If lj(cat) = ?(cat)
? I = IDENTIFY (lj(w), lj(sem))
? S = REPLACE(lj(sem), I, ?(w))
? L = L ? (?(w), ?(cat), S)
As an example, consider the sentence ?Give me the largest state.? from Table 1. Let us assume that
the semantics of the word ?largest? as well as ?the? is not known, however the semantics of ?longest?
is given by the dictionary as ?x.?y.longest(y, x@y). Normally, the system would be unable to parse
this sentence and would continue on. However, upon calling GENERALIZED(L,?largest?), the
word longest is found in the dictionary with the same syntactic category. Thus this function takes the
semantic representation of ?longest? ?x.?y.longest(y, x@y), modifies it accordingly for largest, giving
?x.?y.largest(y, x@y) and stores it in the lexicon. After that, the INV ERSEL and INV ERSER can
be applied to obtain the semantics of ?the?.
3.4 Trivial inverse solutions
Even with on demand generalization, we might still be missing large amounts of semantics information
to be able to use INV ERSEL and INV ERSER. To make up for this, we allow trivial solutions
under certain conditions. A trivial solution is a solution, where one of the formulas is assigned a ?x.x
representation. For example, given H , we are looking for F such that H = G@F . If we set G to be
?x.x, then trivially F = H . Thus we can try to carefully set some unknown semantics of words as
?x.x which will allow us to compute the semantics of the remaining words using INV ERSEL and
INV ERSER. The question then becomes, when do we allow these? In our approach, we allow these
for words that do not seem to have any contribution to the final semantic meaning of the text. In some
39
cases, articles such as ?the?, while having a specific place in the English language, might not contribute
anything to the actual meaning representation of the sentence. In general, any word not present in the
final semantics is a potential candidate to be assigned the trivial semantic representation ?x.x. These are
added with very low weights compared to the semantics found using INV ERSEL and INV ERSER,
so that if at one point a non-trivial semantic representation is found, the system will attempt to use it over
the trivial one.
As an example, consider again the sentence ?Give me the largest state.? from Table 1 with the se-
mantics answer(A, largest(A, state(A))). Let us assume the semantic representations of ?the? and
?largest? are not known. Under normal circumstances the algorithm would be unable to find the seman-
tics of ?largest? using INV ERSEL and INV ERSER as it is missing the semantics of ?the?. However,
as ?the? is not present in the desired semantics, the system will attempt to assign ?x.x as its semantic
representation. After doing that, INV ERSEL and INV ERSER can be used to compute the semantic
representation of ?largest? as ?x.?y.largest(y, x@y).
3.5 The overall learning algorithm.
The complete learning algorithm used within our approach is shown below. The input to the algorithm
is an initial lexicon L0 and a set of pairs (Si, Li), i = 1, ..., n, where Si is a sentence and Li its corre-
sponding logical form. The output of the algorithm is a PCCG defined by the lexicon LT and a parameter
vector ?T .
The parameter vector ?i is updated at each iteration of the algorithm. It stores a real number for each
item in the dictionary. The initial values were set to 0.1. The algorithm is divided into two major steps,
lexical generation and parameters update. The goal of the algorithm is to extract as much information as
possible given the provided training data.
In the first step, the algorithm iterates over all the sentences n times and for each sentence constructs a
syntactic and (potentially incomplete) semantic parse tree. Using the semantic parse tree, it then attempts
to obtain new ?-calculus formulas by traversing the tree and performing regular applications and inverse
computations where possible. Any new semantics are then generalized and stored in the lexicon.
The main reason to iterate over all the sentences n times is to extract all the possible information
given the current parameter vector. There may be cases where the information learned from the last
sentence can be used to learn additional information from the third sentence, which can then be used to
learn new semantics from the second sentence etc. By looping over all sentences n times, we ensure we
capture and learn as much information as possible.
Note that the semantic parse trees of the sentences may change once the parameters of words change.
Thus even though we are looping over all the sentences T times, the semantic parse tree of a sentence
might change as a result of a change in the parameter vector. This change can be very minor, such as
change in the semantics of a single word, or in a rare case a major one where most of the semantic
expressions present in the tree change. Thus we might learn different semantics of words given different
parameter vectors.
In the second step, the parameter vector ?i is updated using stochastic gradient descent. Steps one
and two are performed T times. In our experiments, the value of T ranged from 50 to 100.
Overall, steps one and two form an exhaustive search which optimizes the log-likelihood of the
training model.
? Input:
A set of training sentences with their corresponding desired representations S = {(Si, Li) : i =
1...n} where Si are sentences and Li are desired expressions. Weights are given an initial value of
0.1.
An initial lexicon L0. An initial feature vector ?0.
? Output:
An updated lexicon LT+1. An updated feature vector ?T+1.
40
? Algorithm:
? For t = 1 . . . T
? Step 1: (Lexical generation)
? For i = 1...n.
? For j = 1...n.
? Parse sentence Sj to obtain Tj
? Traverse Tj
? apply INV ERSE L, INV ERSE R andGENERALIZED to find new ?-calculus
expressions of words and phrases ?.
? Set Lt+1 = Lt ? ?
? Step 2: (Parameter Estimation)
? Set ?t+1 = UPDATE(?t, Lt+1)4
? return GENERALIZE(LT , LT ),?(T )
4 Experimental Evaluation
4.1 The data
To evaluate our algorithm, we used the standard corpus in GEOQUERY and CLANG. The GEOQUERY
corpus contained 880 English sentences with respective database queries. The CLANG corpus contained
300 entries specifying rules, conditions and definitions in CLANG. The GEOQUERY corpus contained
relatively short sentences with the sentences ranging from four to seventeen words of quite similar syn-
tactic structure. The sentences in CLANG are much longer, with more complex structure with length
ranging from five to thirty eight words.
For our experiments, we used the C&C parser of Clark and Curran (2007) to provide syntactic
parses for sentences. For CLANG corpus, the position vectors and compound nouns with numbers were
pre-processed and consequently treated as single noun.
Our experiments were done using a 10 fold cross validation and were conducted as follows. A set of
training and testing examples was generated from the respective corpus. These were parsed by the C&C
parser to obtain the syntactic tree structure. These together with the training sets containing the training
sentences with their corresponding semantic representations (SRs) and an initial dictionary was used to
train a new dictionary with corresponding parameters. This dictionary was generalized with respect of
all the words in the test sentences. Note that it is possible that many of the words were still missing their
SRs. This dictionary was then used to parse the test sentences and highest scoring parse was used to
determine precision and recall. Since many words might have been missing their SRs, the system might
not have returned a proper complete semantic parse.
To measure precision and recall, we adopted the measures given by Ge andMooney (2009). Precision
denotes the percentage of of returned SRs that were correct, while Recall denotes the percentage of test
examples with pre-specified SRs returned. F-measure is the standard harmonic mean of precision and
recall. For database querying, an SRwas considered correct if it retrieved the same answer as the standard
query. For CLANG, an SR was correct if it was an exact match of the desired SR, except for argument
ordering of conjunctions and other commutative predicates. Additionally, a set of additional experiments
was run with ?(definec? and ?(definer? treated as being equal.
We evaluated two different version of our system. The first one, INV ERSE, uses INV ERSEL
and INV ERSER and regular generalization which is applied after each step. The second version,
INV ERSE+, uses trivial inverse solutions as well as on demand generalization. Both systems were
4For details on ? computation, please see the work by Zettlemoyer and Collins (2005)
41
evaluated on the same data sets using 10 fold cross validation and theC&C parser using an equal number
of train and test sentences, randomly chosen from their respective corpus. The initial dictionary contained
a few nouns, with the addition of one randomly selected word from the set {what, where, which} in
case of GEOQUERY. For CLANG, the initial dictionary also contained a few nouns, together with the
addition of one randomly selected word from the set {if, when, during}. The learning parameters were
set to the values used by Zettlemoyer and Collins (2005).
4.2 Results
We compared our systems with the performance results of several alternative systems for which the
performance data is available in the literature. In particular, we used the performance data given by
Ge and Mooney (2009). The systems that we compared with are: The SYN0, SYN20 and GOLDSYN
systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based
system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and
Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al
(2008). Please note that many of these approaches require different parsers, human supervision or other
additional tools, while our approach requires a syntactic parse of the sentences and an initial dictionary.
Our and their reported results for the respective corpora are given in the Tables 2 and 3.
Precision Recall F-measure
INVERSE+ 93.41 89.04 91.17
INVERSE 91.12 85.78 88.37
GOLDSYN 91.94 88.18 90.02
WASP 91.95 86.59 89.19
Z&C 91.63 86.07 88.76
SCISSOR 95.50 77.20 85.38
KRISP 93.34 71.70 81.10
Lu at al. 89.30 81.50 85.20
Table 2: Performance on GEOQUERY.
Precision Recall F-measure
INVERSE+(i) 87.67 79.08 83.15
INVERSE+ 85.74 76.63 80.92
GOLDSYN 84.73 74.00 79.00
SYN20 85.37 70.00 76.92
SYN0 87.01 67.00 75.71
WASP 88.85 61.93 72.99
KRISP 85.20 61.85 71.67
SCISSOR 89.50 73.70 80.80
Lu at al. 82.50 67.70 74.40
Table 3: Performance on CLANG.
The INV ERSE + (i) denotes training where ?(definec? and ?(definer? at the start of SRs were
treated as being equal. The main reason for this was that there seems to be no way to distinguish in
between them. Even as a human, we found it hard to be able to distinguish between them.
4.3 Analysis
Our testing showed that our method is capable of outperforming all of the existing parsers in F-measure.
However, there are parsers which can produce greater precision, such as WASP and SCISSOR on
CLANG corpus, however they do at the cost in recall. As discussed by Ge and Mooney (2009), the
GEOQUERY results for SCISSOR, KRISP and Lu?s work use a different, less accurate representation
language FUNSQL which may skew the results. Also, SCISSOR outperforms our system on GEO-
QUERY corpus in terms of precision, but at the cost of additional human supervision.
Our system is particularly accurate for shorter sentences, or a corpus where many sentences have
similar general structure, such as GEOQUERY. However, it is also capable of handling longer sentences,
in particular if they in fact consists of several shorter sentences, such as for example ?If the ball is in
our midfield, position player 3 at (-5,-23).?, which can be looked at as ?IF A, B? where ?A? and ?B?
are smaller complete sentences themselves. The system is capable of learning the semantics of several
basic categories such as verbs, after which most of the training sentences are easily parsed and missing
semantics is learned quickly. The inability to parse other sentences mostly comes from two sources. First
one is if the test sentence contains a syntactic category not seen in the training data. Our generalization
model is not capable of generalizing these and thus fails to produce a semantic parse. The second problem
comes from ambiguity of SRs. During training, many words will be assigned several SRs based on the
42
training data. The parses are then ranked and in several cases, the correct SR might not be on the top.
Re-ranking might help alleviate the second issue.
Unlike the other systems, we do not make use of a grammar for the semantics of the sentence. The
reason it is not required is that the actual semantics is analyzed in computing the inverse lambdas, and
the richness of ?-calculus allows us to compute relatively complex formulas to represent the semantic of
words.
We also run examples with increased size of training data. These produced larger dictionaries and in
general did not significantly affect the results. The main reason is that as discussed before, once the most
common categories of words have their semantics assigned, most of the sentences can be properly parsed.
Increasing the amount of training data increases the coverage in terms of the rare syntactic categories,
but these are also rarely present in the testing data. The used training sample was in all cases sufficient to
learn almost all of the categories. This might not be the case in general, for example if we had a corpus
with all of the sentences of a particular length and structure, our method might not be capable of learning
any new semantics. In such cases, additional words would have to be added to the initial dictionary, or
additional sentences of varying lengths would have to be added.
The C&C parser of Clark and Curran (2007) was primarily trained on news paper text and thus
did have some problems with these different domains and in some cases resulted in complex semantic
representations of words. This could be improved by using a different parser, or by simply adjusting
some of the parse trees. In addition, our system can be gradually improved by increasing the size of
initial dictionary.
5 Conclusions and Discussion
We presented a new approach to map natural language sentences to their semantic representations. We
used an existing syntactic parser, a novel inverse ? operator and several generalization techniques to learn
the semantic representations of words. Our method is largely independent of the target representation
language and directly computes the semantic representations based on the syntactic structure of the
syntactic parse tree and known semantic representations. We used statistical learning methods to assign
weights to different semantic representation of words and sentences.
Our results indicate that our approach outperforms many of the existing systems on the standard
corpora of database querying and robot command and control.
We envision several directions of future work. One direction is to experiment our system with cor-
pora where the natural language semantics is given through other Knowledge Representation languages
such as answer set programming (ASP)5 and temporal logic. We are currently building such corpora.
Another direction is to improve the statistical learning part of the system. An initial experimentation
with a different learning algorithm shows significant decrease in training time with slight reduction in
performance. Finally, since our system uses an initial dictionary, which we tried to minimize by only hav-
ing a few nouns and one of the query words, exploring how to reduce it further and possibly completely
eliminating it is a future direction of research.
References
Baral, C. (2003). Knowledge Representation, Reasoning, and Declarative Problem Solving. Cambridge
University Press.
Chen, M., E. Foroughi, F. Heintz, S. Kapetanakis, K. Kostadis, J. Kummeneje, I. Noda, O. Obst, P. Riley,
T. Steffens, and Y. W. X. Yin (2003). Users manual: Robocup soccer server manula for soccer server
version 7.07 and later. In Avaliable at http://sourceforge.net/projects/sserver/.
5A preliminary evaluation with respect to a corpus with newspaper text translated into ASP resulted in a precision of 77%,
recall of 82% with F-measure at 80 using a much smaller training set.
43
Clark, S. and J. R. Curran (2007). Wide-coverage efficient statistical parsing with ccg and log-linear
models. Computational Linguistics 33.
Gamut, L. (1991). Logic, Language, and Meaning. The University of Chicago Press.
Ge, R. and R. J. Mooney (2005). A statistical semantic parser that integrates syntax and semantics. In In
Proceedings of the Ninth Conference on Computational Natural Language Learning., pp. 9?16.
Ge, R. and R. J. Mooney (2009). Learning a compositional semantic parser using an existing syntactic
parser. In In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2009)., pp. 611?619.
Gonzalez, M. A. (2010). An inverse lambda calculus algorithm for natural language processing. Master?s
thesis, Arizona State University.
Kate, R. J. and R. J. Mooney (2006). Using string-kernels for learning semantic parsers. In In Proceed-
ings of the 21st Intl. Conference on Computational Linguistics., pp. 439?446.
Lu, W., H. T. Ng, W. S. Lee, and L. S. Zettlemoyer (2008). A generative model for parsing natural
language to meaning representations. In In Proceedings of the Conference on Empirical Methods in
Natural Language Pricessing (EMNLP-08).
Steedman, M. (2000). The syntactic process. MIT Press.
Wong, Y.W. and R. J. Mooney (2006). Learning for semantic parsing with statistical machine translation.
In In Proceedings of the Human Language Technology Conference of the North American Chapter of
the Association for Computational Linguistics (HLT/NAACL-2006)., pp. 439?446.
Wong, Y. W. and R. J. Mooney (2007). Learning synchronous grammars for semantic parsing with
lambda calculus. In In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics (ACL-07)., pp. 960?967.
Zelle, J. M. and R. J. Mooney (1996). Learning to parse database queries using inductive logic program-
ming. In 14th National Conference on Artificial Intelligence.
Zettlemoyer, L. and M. Collins (2005). Learning to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. In 21th Annual Conference on Uncertainty in Artificial
Intelligence, pp. 658?666.
Zettlemoyer, L. and M. Collins (2007). Online learning of relaxed ccg grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pp. 678?687.
Zettlemoyer, L. and M. Collins (2009). Learning context-dependent mappings from sentences to logical
form. In ACL.
44
