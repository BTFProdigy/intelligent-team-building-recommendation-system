Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 306?314,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Automatically Evaluating Content Selection in Summarization without
Human Models
Annie Louis
University of Pennsylvania
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
We present a fully automatic method for
content selection evaluation in summariza-
tion that does not require the creation of
human model summaries. Our work capi-
talizes on the assumption that the distribu-
tion of words in the input and an informa-
tive summary of that input should be sim-
ilar to each other. Results on a large scale
evaluation from the Text Analysis Con-
ference show that input-summary compar-
isons are very effective for the evaluation
of content selection. Our automatic meth-
ods rank participating systems similarly
to manual model-based pyramid evalua-
tion and to manual human judgments of
responsiveness. The best feature, Jensen-
Shannon divergence, leads to a correlation
as high as 0.88 with manual pyramid and
0.73 with responsiveness evaluations.
1 Introduction
The most commonly used evaluation method for
summarization during system development and
for reporting results in publications is the auto-
matic evaluation metric ROUGE (Lin, 2004; Lin
and Hovy, 2003). ROUGE compares system sum-
maries against one or more model summaries
by computing n-gram word overlaps between the
two. The wide adoption of such automatic mea-
sures is understandable because they are conve-
nient and greatly reduce the complexity of eval-
uations. ROUGE scores also correlate well with
manual evaluations of content based on compar-
ison with a single model summary, as used in
the early editions of the Document Understanding
Conferences (Over et al, 2007).
In our work, we take the idea of automatic
evaluation to an extreme and explore the feasi-
bility of developing a fully automatic evaluation
method for content selection that does not make
use of human model summaries at all. To this end,
we show that evaluating summaries by comparing
them with the input obtains good correlations with
manual evaluations for both query focused and up-
date summarization tasks.
Our results have important implications for fu-
ture development of summarization systems and
their evaluation.
High correlations between system ranking pro-
duced with the fully automatic method and
manual evaluations show that the new eval-
uation measures can be used during system
development when human model summaries
are not available.
Our results provide validation of several features
that can be optimized in the development of
new summarization systems when the objec-
tive is to improve content selection on aver-
age, over a collection of test inputs. However,
none of the features is consistently predictive
of good summary content for individual in-
puts.
We find that content selection performance on
standard test collections can be approximated
well by the proposed fully automatic method.
This result greatly underlines the need to re-
quire linguistic quality evaluations alongside
content selection ones in future evaluations
and research.
2 Model-free methods for evaluation
Proposals for developing fully automatic methods
for summary evaluation have been put forward
in the past. Their attractiveness is obvious for
large scale evaluations, or for evaluation on non-
standard test sets for which human models are not
available.
306
For example in Radev et al (2003), a large
scale fully automatic evaluation of eight summa-
rization systems on 18,000 documents was per-
formed without any human effort. A search engine
was used to rank documents according to their rel-
evance to a given query. The summaries for each
document were also ranked for relevance with re-
spect to the same query. For good summariza-
tion systems, the relevance ranking of summaries
is expected to be similar to that of the full docu-
ments. Based on this intuition, the correlation be-
tween relevance rankings of summaries and orig-
inal documents was used to compare the different
systems. The approach was motivated by the as-
sumption that the distribution of terms in a good
summary is similar to the distribution of terms in
the original document.
Even earlier, Donaway et al (2000) suggested
that there are considerable benefits to be had in
adopting model-free methods of evaluation involv-
ing direct comparisons between the original docu-
ment and its summary. The motivation for their
work was the considerable variation in content se-
lection choices in model summaries (Rath et al,
1961). The identity of the model writer signifi-
cantly affects summary evaluations (also noted by
McKeown et al (2001), Jing et al (1998)) and
evaluations of the same systems can be rather dif-
ferent when different models are used. In their
experiments, Donaway et al (2000) demonstrated
that the correlations between manual evaluation
using a model summary and
a) manual evaluation using a different model
summary
b) automatic evaluation by directly comparing
input and summary1,
are the same. Their conclusion was that such au-
tomatic methods should be seriously considered as
an alternative to model based evaluation.
In this paper, we present a comprehensive study
of fully automatic summary evaluation without
any human models. A summary?s content is
judged for quality by directly estimating its close-
ness to the input. We compare several probabilistic
and information-theoretic approaches for charac-
terizing the similarity and differences between in-
put and summary content. A simple information-
theoretic measure, Jensen Shannon divergence be-
tween input and summary, emerges as the best fea-
1They used cosine similarity to perform the input-
summary comparison.
ture. System rankings produced using this mea-
sure lead to correlations as high as 0.88 with hu-
man judgements.
3 TAC summarization track
3.1 Query-focused and Update Summaries
Two types of summaries, query-focused and up-
date summaries, were evaluated in the summariza-
tion track of the 2008 Text Analysis Conference
(TAC)2. Query-focused summaries were produced
from input documents in response to a stated user
information need. The update summaries require
more sophistication: two sets of articles on the
same topic are provided. The first set of articles
represents the background of a story and users are
assumed to be already familiar with the informa-
tion contained in them. The update task is to pro-
duce a multi-document summary from the second
set of articles that can serve as an update to the
user. This task is reminiscent of the novelty de-
tection task explored at TREC (Soboroff and Har-
man, 2005).
3.2 Data
The test set for the TAC 2008 summarization task
contains 48 inputs. Each input consists of two sets
of 10 documents each, called docsets A and B.
Both A and B are on the same general topic but
B contains documents published later than those
in A. In addition, the user?s information need as-
sociated with each input is given by a query state-
ment consisting of a title and narrative. An exam-
ple query statement is shown below.
Title: Airbus A380
Narrative: Describe developments in the pro-
duction and launch of the Airbus A380.
A system must produce two summaries: (1) a
query-focused summary of docset A, (2) a compi-
lation of updates from docset B, assuming that the
user has read all the documents in A. The max-
imum length for both types of summaries is 100
words.
There were 57 participating systems in TAC
2008. We use the summaries and evaluations of
these systems for the experiments reported in the
paper.
3.3 Evaluation metrics
Both manual and automatic evaluations were con-
ducted at NIST to assess the quality of summaries
2http://www.nist.gov/tac
307
manual score R-1 recall R-2 recall
Query Focused summaries
pyramid score 0.859 0.905
responsiveness 0.806 0.873
Update summaries
pyramid score 0.912 0.941
responsiveness 0.865 0.884
Table 1: Spearman correlation between manual
scores and ROUGE-1 and ROUGE-2 recall. All
correlations are highly significant with p-value <
0.00001.
produced by the systems.
Pyramid evaluation: The pyramid evaluation
method (Nenkova and Passonneau, 2004) has been
developed for reliable and diagnostic assessment
of content selection quality in summarization and
has been used in several large scale evaluations
(Nenkova et al, 2007). It uses multiple human
models from which annotators identify seman-
tically defined Summary Content Units (SCU).
Each SCU is assigned a weight equal to the
number of human model summaries that express
that SCU. An ideal maximally informative sum-
mary would express a subset of the most highly
weighted SCUs, with multiple maximally infor-
mative summaries being possible. The pyramid
score for a system summary is equal to the ratio
between the sum of weights of SCUs expressed
in a summary (again identified manually) and the
sum of weights of an ideal summary with the same
number of SCUs.
Four human summaries provided by NIST for
each input and task were used for the pyramid
evaluation at TAC.
Responsiveness evaluation: Responsiveness of a
summary is a measure of overall quality combin-
ing both content selection and linguistic quality:
summaries must present useful content in a struc-
tured fashion in order to better satisfy the user?s
need. Assessors directly assigned scores on a
scale of 1 (poor summary) to 5 (very good sum-
mary) to each summary. These assessments are
done without reference to any model summaries.
The (Spearman) correlation between the pyramid
and responsiveness metrics is high but not perfect:
0.88 and 0.92 respectively for query focused and
update summarization.
ROUGE evaluation: NIST also evaluated the
summaries automatically using ROUGE (Lin,
2004; Lin and Hovy, 2003). Comparison between
a summary and the set of four model summaries
is computed using unigram (R1) and bigram over-
laps (R2)3. The correlations between ROUGE and
manual evaluations is shown in Table 1 and varies
between 0.80 and 0.94.
Linguistic quality evaluation: Assessors scored
summaries on a scale from 1 (very poor) to 5 (very
good) for five factors of linguistic quality: gram-
maticality, non-redundancy, referential clarity, fo-
cus, structure and coherence.
We do not make use of any of the linguistic
quality evaluations. Our work focuses on fully au-
tomatic evaluation of content selection, so man-
ual pyramid and responsiveness scores are used
for comparison with our automatic method. The
pyramid metric measures content selection exclu-
sively, while responsiveness incorporates at least
some aspects of linguistic quality.
4 Features for content evaluation
We describe three classes of features to compare
input and summary content: distributional simi-
larity, summary likelihood and use of topic signa-
tures. Both input and summary words were stop-
word filtered and stemmed before computing the
features.
4.1 Distributional Similarity
Measures of similarity between two probability
distributions are a natural choice for the task at
hand. One would expect good summaries to be
characterized by low divergence between proba-
bility distributions of words in the input and sum-
mary, and by high similarity with the input.
We experimented with three common measures:
KL and Jensen Shannon divergence and cosine
similarity. These three metrics have already been
applied for summary evaluation, albeit in differ-
ent contexts. In Lin et al (2006), KL and JS di-
vergences between human and machine summary
distributions were used to evaluate content selec-
tion. The study found that JS divergence always
outperformed KL divergence. Moreover, the per-
formance of JS divergence was better than stan-
dard ROUGE scores for multi-document summa-
rization when multiple human models were used
for the comparison.
The use of cosine similarity in Donaway et
al. (2000) is more directly related to our work.
They show that the difference between evaluations
3The scores were computed after stemming but stop
words were retained in the summaries.
308
based on two different human models is about the
same as the difference between system ranking
based on one model summary and the ranking pro-
duced using input-summary similarity. Inputs and
summaries were compared using only one metric:
cosine similarity.
Kullback Leibler (KL) divergence: The KL di-
vergence between two probability distributions P
and Q is given by
D(P ||Q) =
?
w
p
P
(w) log
2
p
P
(w)
p
Q
(w)
(1)
It is defined as the average number of bits wasted
by coding samples belonging to P using another
distribution Q, an approximate of P . In our case,
the two distributions are those for words in the
input and summary respectively. Since KL di-
vergence is not symmetric, both input-summary
and summary-input divergences are used as fea-
tures. In addition, the divergence is undefined
when p
P
(w) > 0 but p
Q
(w) = 0. We perform
simple smoothing to overcome the problem.
p(w) =
C + ?
N + ? ? B
(2)
Here C is the count of word w and N is the
number of tokens; B = 1.5|V |, where V is the
input vocabulary and ? was set to a small value
of 0.0005 to avoid shifting too much probability
mass to unseen events.
Jensen Shannon (JS) divergence: The JS diver-
gence incorporates the idea that the distance be-
tween two distributions cannot be very different
from the average of distances from their mean dis-
tribution. It is formally defined as
J(P ||Q) =
1
2
[D(P ||A) + D(Q||A)], (3)
where A = P+Q
2
is the mean distribution of P
and Q. In contrast to KL divergence, the JS dis-
tance is symmetric and always defined. We use
both smoothed and unsmoothed versions of the di-
vergence as features.
Similarity between input and summary: The
third metric is cosine overlap between the tf ? idf
vector representations (with max-tf normalization)
of input and summary contents.
cos? =
v
inp
.v
summ
||v
inp
||||v
summ
||
(4)
We compute two variants:
1. Vectors contain all words from input and
summary
2. Vectors contain only topic signatures from
the input and all words of the summary
Topic signatures are words highly descriptive of
the input, as determined by the application of log-
likelihood test (Lin and Hovy, 2000). Using only
topic signatures from the input to represent text is
expected to be more accurate because the reduced
vector has fewer dimensions compared with using
all the words from the input.
4.2 Summary likelihood
The likelihood of a word appearing in the sum-
mary is approximated as being equal to its proba-
bility in the input. We compute both a summary?s
unigram probability as well as its probability un-
der a multinomial model.
Unigram summary probability:
(p
inp
w
1
)
n
1
(p
inp
w
2
)
n
2
...(p
inp
w
r
)
n
r (5)
where p
inp
w
i
is the probability in the input of
word w
i
, n
i
is the number of times w
i
appears
in the summary, and w
1
...w
r
are all words in the
summary vocabulary.
Multinomial summary probability:
N !
n
1
!n
2
!...n
r
!
(p
inp
w
1
)
n
1
(p
inp
w
2
)
n
2
...(p
inp
w
r
)
n
r (6)
where N = n
1
+ n
2
+ ...+ n
r
is the total number
of words in the summary.
4.3 Use of topic words in the summary
Summarization systems that directly optimize for
more topic signatures during content selection
have fared very well in evaluations (Conroy et al,
2006). Hence the number of topic signatures from
the input present in a summary might be a good
indicator of summary content quality. We experi-
ment with two features that quantify the presence
of topic signatures in a summary:
1. Fraction of the summary composed of input?s
topic signatures.
2. Percentage of topic signatures from the input
that also appear in the summary.
While both features will obtain higher values
for summaries containing many topic words, the
first is guided simply by the presence of any topic
word while the second measures the diversity of
topic words used in the summary.
309
4.4 Feature combination using linear
regression
We also evaluated the performance of a linear re-
gression metric combining all of the above fea-
tures. The value of the regression-based score for
each summary was obtained using a leave-one-
out approach. For a particular input and system-
summary combination, the training set consisted
only of examples which included neither the same
input nor the same system. Hence during training,
no examples of either the test input or system were
seen.
5 Correlations with manual evaluations
In this section, we report the correlations between
system ranking using our automatic features and
the manual evaluations. We studied the predictive
power of features in two scenarios.
MACRO LEVEL; PER SYSTEM: The values of fea-
tures were computed for each summary submitted
for evaluation. For each system, the feature values
were averaged across all inputs. All participating
systems were ranked based on the average value.
Similarly, the average manual score, pyramid or
responsiveness, was also computed for each sys-
tem. The correlations between the two rankings
are shown in Tables 2 and 4.
MICRO LEVEL; PER INPUT: The systems were
ranked for each input separately, and correlations
between the summary rankings for each input
were computed (Table 3).
The two levels of analysis address different
questions: Can we automatically identify sys-
tem performance across all test inputs (macro
level) and can we identify which summaries for a
given input were good and which were bad (mi-
cro level)? For the first task, the answer is a defi-
nite ?yes? while for the second task the results are
mixed.
In addition, we compare our results to model-
based evaluations using ROUGE and analyze the
effects of stemming the input and summary vo-
cabularies. In order to allow for in-depth discus-
sion, we will analyze our findings only for query
focused summaries. Similar results were obtained
for the evaluation of update summaries and are de-
scribed in Section 7.
5.1 Performance at macro level
Table 2 shows the Spearman correlation between
manual and automatic scores averaged across the
Features pyramid respons.
JS div -0.880 -0.736
JS div smoothed -0.874 -0.737
% of input topic words 0.795 0.627
KL div summ-inp -0.763 -0.694
cosine overlap 0.712 0.647
% of summ = topic wd 0.712 0.602
topic overlap 0.699 0.629
KL div inp-summ -0.688 -0.585
mult. summary prob. 0.222 0.235
unigram summary prob. -0.188 -0.101
regression 0.867 0.705
ROUGE-1 recall 0.859 0.806
ROUGE-2 recall 0.905 0.873
Table 2: Spearman correlation on macro level for
the query focused task. All results are highly sig-
nificant with p-values < 0.000001 except unigram
and multinomial summary probability, which are
not significant even at the 0.05 level.
48 inputs. We find that both distributional simi-
larity and the topic signature features produce sys-
tem rankings very similar to those produced by hu-
mans. Summary probabilities, on the other hand,
turn out to be unpredictive of content selection
performance. The linear regression combination
of features obtains high correlations with manual
scores but does not lead to better results than the
single best feature: JS divergence.
JS divergence outperforms other features in-
cluding the regression metric and obtains the best
correlations with both types of manual scores, 0.88
with pyramid score and 0.74 with responsiveness.
The regression metric performs comparably with
correlations of 0.86 and 0.70. The correlations ob-
tained by both JS divergence and the regression
metric with pyramid evaluations are in fact better
than that obtained by ROUGE-1 recall (0.85).
The best topic signature based feature?
percentage of input?s topic signatures that are
present in the summary?ranks next only to JS di-
vergence and regression. The correlation between
this feature and pyramid and responsiveness eval-
uations is 0.79 and 0.62 respectively. The propor-
tion of summary content composed of topic words
performs worse as an evaluation metric with cor-
relations 0.71 and 0.60. This result indicates that
summaries that cover more topics from the input
are judged to have better content than those in
which fewer topics are mentioned.
Cosine overlaps and KL divergences obtain
good correlations but still lower than JS diver-
gence or percentage of input topic words. Further,
rankings based on unigram and multinomial sum-
310
mary probabilities do not correlate significantly
with manual scores.
5.2 Performance on micro level
On a per input basis, the proposed metrics are not
that effective in distinguishing which summaries
have better content. The minimum and maximum
correlations with manual evaluations across the 48
inputs are given in Table 3. The number and per-
centage of inputs for which correlations were sig-
nificant are also reported.
Now, JS divergence obtains significant correla-
tions with pyramid scores for 73% of the inputs
but for particular inputs, the correlation can be
as low as 0.27. The results are worse for other
features and for comparison with responsiveness
scores.
At the micro level, combining features with re-
gression gives the best result overall, in contrast to
the findings for the macro level setting. This re-
sult has implications for system development; no
single feature can reliably predict good content for
a particular input. Even a regression combination
of all features is a significant predictor of content
selection quality in only 77% of the cases.
We should note however, that our features are
based only on the distribution of terms in the in-
put and therefore less likely to inform good con-
tent for all input types. For example, a set of
documents each describing different opinion on a
given issue will likely have less repetition on both
lexical and content unit level. The predictiveness
of features like ours will be limited for such in-
puts4. However, model summaries written for the
specific input would give better indication of what
information in the input was important and inter-
esting. This indeed is the case as we shall see in
Section 6.
Overall, the micro level results suggest that the
fully automatic measures we examined will not be
useful for providing information about summary
quality for an individual input. For averages over
many test sets, the fully automatic evaluations give
more reliable and useful results, highly correlated
with rankings produced by manual evaluations.
4In fact, it would be surprising to find an automatically
computable feature or feature combination which would be
able to consistently predict good content for all individual in-
puts. If such features existed, an ideal summarization system
would already exist.
5.3 Effects of stemming
The analysis presented so far is on features com-
puted after stemming the input and summary
words. We also computed the values of the same
features without stemming and found that diver-
gence metrics benefit greatly when stemming is
done. The biggest improvements in correlations
are for JS and KL divergences with respect to re-
sponsiveness. For JS divergence, the correlation
increases from 0.57 to 0.73 and for KL divergence
(summary-input), from 0.52 to 0.69.
Before stemming, the topic signature and bag
of words overlap features are the best predictors
of responsiveness (correlations are 0.63 and 0.64
respectively) but do not change much after stem-
ming (topic overlap?0.62, bag of words?0.64).
Divergences emerge as better metrics only after
stemming.
Stemming also proves beneficial for the likeli-
hood features. Before stemming, their correlations
are directed in the wrong direction, but they im-
prove after stemming to being either positive or
closer to zero. However, even after stemming,
summary probabilities are not good predictors of
content quality.
5.4 Difference in correlations: pyramid and
responsiveness scores
Overall, we find that correlations with pyramid
scores are higher than correlations with respon-
siveness. Clearly our features are designed to
compare input-summary content only. Since re-
sponsiveness judgements were based on both con-
tent and linguistic quality of summaries, it is not
surprising that these rankings are harder to repli-
cate using our content based features. Neverthe-
less, responsiveness scores are dominated by con-
tent quality and the correlation between respon-
siveness and JS divergence is high, 0.73.
Clearly, metrics of linguistic quality should be
integrated with content evaluations to allow for
better predictions of responsiveness. To date, few
attempts have been made to automatically eval-
uate linguistic quality in summarization. Lapata
and Barzilay (2005) proposed a method for co-
herence evaluation which holds promise but has
not been validated so far on large datasets such
as those used in TAC and DUC. In a simpler ap-
proach, Conroy and Dang (2008) use higher order
ROUGE scores to approximate both content and
linguistic quality.
311
pyramid responsiveness
features max min no. significant (%) max min no. significant (%)
JS div -0.714 -0.271 35 (72.9) -0.654 -0.262 35 (72.9)
JS div smoothed -0.712 -0.269 35 (72.9) -0.649 -0.279 33 (68.8)
KL div summ-inp -0.736 -0.276 35 (72.9) -0.628 -0.261 35 (72.9)
% of input topic words 0.701 0.286 31 (64.6) 0.693 0.279 29 (60.4)
cosine overlap 0.622 0.276 31 (64.6) 0.618 0.265 28 (58.3)
KL div inp-summ -0.628 -0.262 28 (58.3) -0.577 -0.267 22 (45.8)
topic overlap 0.597 0.265 30 (62.5) 0.689 0.277 26 (54.2)
% summary = topic wd 0.607 0.269 23 (47.9) 0.534 0.272 23 (47.9)
mult. summary prob. 0.434 0.268 8 (16.7) 0.459 0.272 10 (20.8)
unigram summary prob. 0.292 0.261 2 ( 4.2) 0.466 0.287 2 (4.2)
regression 0.736 0.281 37 (77.1) 0.642 0.262 32 (66.7)
ROUGE-1 recall 0.833 0.264 47 (97.9) 0.754 0.266 46 (95.8)
ROUGE-2 recall 0.875 0.316 48 (100) 0.742 0.299 44 (91.7)
Table 3: Spearman correlations at micro level (query focused task). Only the minimum, maximum
values of the significant correlations are reported together with the number and percentage of significant
correlations.
update input only avg. update & background
features pyramid respons. pyramid respons.
JS div -0.827 -0.764 -0.716 -0.669
JS div smoothed -0.825 -0.764 -0.713 -0.670
% of input topic words 0.770 0.709 0.677 0.616
KL div summ-inp -0.749 -0.709 -0.651 -0.624
KL div inp-summ -0.741 -0.717 -0.644 -0.638
cosine overlap 0.727 0.691 0.649 0.631
% of summary = topic wd 0.721 0.707 0.647 0.636
topic overlap 0.707 0.674 0.645 0.619
mult. summmary prob. 0.284 0.355 0.152 0.224
unigram summary prob. -0.093 0.038 -0.151 -0.053
regression 0.789 0.605 0.699 0.522
ROUGE-1 recall 0.912 0.865 . .
ROUGE-2 recall 0.941 0.884 . .
regression combining features comparing with background and update inputs (without averaging)
correlations = 0.8058 with pyramid, 0.6729 with responsiveness
Table 4: Spearman correlations at macro level for update summarization. Results are reported separately
for features comparing update summaries with the update input only or with both update and background
inputs and averaging the two.
6 Comparison with ROUGE
For manual pyramid scores, the best correlation,
0.88, we observed in our experiments was with
JS divergence. This result is unexpectedly high
for a fully automatic evaluation metric. Note that
the best correlation between pyramid scores and
ROUGE (for R2) is 0.90, practically identical with
JS divergence. For ROUGE-1, the correlation is
0.85.
In the case of manual responsiveness, which
combines aspects of linguistic quality along with
content selection evaluation, the correlation with
JS divergence is 0.73. For ROUGE, it is 0.80
for R1 and 0.87 for R2. Using higher order n-
grams is obviously beneficial as observed from the
differences between unigram and bigram ROUGE
scores. So a natural extension of our features
would be to use distance between bigram distri-
butions. At the same time, for responsiveness,
ROUGE-1 outperforms all the fully automatic fea-
tures. This is evidence that the model summaries
provide information that is unlikely to ever be ap-
proximated by information from the input alone,
regardless of feature sophistication.
At the micro level, ROUGE does clearly better
than all the automatic measures. The results are
shown in the last two rows of Table 3. ROUGE-1
recall obtains significant correlations for over 95%
of inputs for responsiveness and 98% of inputs for
pyramid evaluation compared to 73% (JS diver-
gence) and 77% (regression). Undoubtedly, at the
input level, comparison with model summaries is
substantially more informative.
When reference summaries are available,
ROUGE provides scores that agree best with hu-
man judgements. However, when model sum-
312
maries are not available, our features can provide
reliable estimates of system quality when averaged
over a set of test inputs. For predictions at the level
of individual inputs, our fully automatic features
are less useful.
7 Update Summarization
In Table 4, we report the performance of our fea-
tures for system evaluation on the update task. The
column, ?update input only? summarizes the cor-
relations obtained by features comparing the sum-
maries with only the update inputs (set B). We
also compared the summaries individually to the
update and background (set A) inputs. The two
sets of features were then combined by a) averag-
ing (?avg. update and background?) and b) linear
regression (last line of Table 4).
As in the case of query focused summarization,
JS divergence and percentage of input topic sig-
natures in summary are the best features for the
update task as well. The overall best feature is
JS divergence between the update input and the
summaries?correlations of 0.82 and 0.76 with
pyramid and responsiveness.
Interestingly, the features combining both up-
date and background inputs do not lead to better
correlations than those obtained using the update
input only. The best performance from combined
features is given by the linear regression metric.
Although the correlation of this regression feature
with pyramid scores (0.80) is comparable to JS di-
vergence with update inputs, its correlation with
responsiveness (0.67) is clearly lower. These re-
sults show that the term distributions in the update
input are sufficiently good predictors of content
for update summaries. The role of the background
input appears to be negligable.
8 Discussion
We have presented a successful framework for
model-free evaluations of content which uses the
input as reference. The power of model-free eval-
uations generalizes across at least two summariza-
tion tasks: query focused and update summariza-
tion.
We have analyzed a variety of features for input-
summary comparison and demonstrated that the
strength of different features varies considerably.
Similar term distributions in the input and the sum-
mary and diverse use of topic signatures in the
summary are highly indicative of good content.
We also find that preprocessing like stemming im-
proves the performance of KL and JS divergence
features.
Very good results were obtained from a corre-
lation analysis with human judgements, showing
that input can indeed substitute for model sum-
maries and manual efforts in summary evaluation.
The best correlations were obtained by a single
feature, JS divergence (0.88 with pyramid scores
and 0.73 with responsiveness at system level).
Our best features can therefore be used to eval-
uate the content selection performance of systems
in a new domain where model summaries are un-
available. However, like all other content evalua-
tion metrics, our features must be accompanied by
judgements of linguistic quality to obtain whole-
some indicators of summary quality and system
performance. Evidence for this need is provided
by the lower correlations with responsiveness than
the content-only pyramid evaluations.
The results of our analysis zero in on JS diver-
gence and topic signature as desirable objectives to
optimize during content selection. On the macro
level, they are powerful predictors of content qual-
ity. These findings again emphasize the need for
always including linguistic quality as a component
of evaluation.
Observations from our input-based evaluation
also have important implications for the design of
novel summarization tasks. We find that high cor-
relations with manual evaluations are obtained by
comparing query-focused summaries with the en-
tire input and making no use of the query at all.
Similarly in the update summarization task, the
best predictions of content for update summaries
were obtained using only the update input. The
uncertain role of background inputs and queries
expose possible problems with the task designs.
Under such conditions, it is not clear if query-
focused content selection or ability to compile up-
dates are appropriately captured by any evaluation.
References
J. Conroy and H. Dang. 2008. Mind the gap: Dangers
of divorcing evaluations of summary content from
linguistic quality. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 145?152.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL, short paper.
313
R. Donaway, K. Drummey, and L. Mather. 2000. A
comparison of rankings produced by summarization
evaluation measures. In NAACL-ANLP Workshop
on Automatic Summarization.
H. Jing, R. Barzilay, K. Mckeown, and M. Elhadad.
1998. Summarization evaluation methods: Experi-
ments and analysis. In In AAAI Symposium on Intel-
ligent Summarization, pages 60?68.
M. Lapata and R. Barzilay. 2005. Automatic evalua-
tion of text coherence: Models and representations.
In IJCAI?05.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of the 18th conference on Computational
linguistics, pages 495?501.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurance statistics. In
Proceedings of HLT-NAACL 2003.
C. Lin, G. Cao, J. Gao, and J. Nie. 2006. An
information-theoretic approach to automatic evalu-
ation of summaries. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 463?470.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In ACL Text Summarization
Workshop.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivas-
siloglou, B. Schiffman, and S. Teufel. 2001.
Columbia multi-document summarization: Ap-
proach and evaluation. In DUC?01.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarization: The pyramid
method. In HLT/NAACL.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The pyramid method: Incorporating human con-
tent selection variation in summarization evaluation.
ACM Trans. Speech Lang. Process., 4(2):4.
P. Over, H. Dang, and D. Harman. 2007. Duc in con-
text. Inf. Process. Manage., 43(6):1506?1520.
D. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer,
H. Qi, A. C?elebi, D. Liu, and E. Drabek. 2003.
Evaluation challenges in large-scale multi-document
summarization: the mead project. In Proceedings of
ACL 2003, Sapporo, Japan.
G. J. Rath, A. Resnick, and R. Savage. 1961. The
formation of abstracts by the selection of sentences:
Part 1: sentence selection by man and machines.
American Documentation, 2(12):139?208.
I. Soboroff and D. Harman. 2005. Novelty detec-
tion: the trec experience. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 105?112.
314
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 541?548,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Performance Confidence Estimation for Automatic Summarization
Annie Louis
University of Pennsylvania
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
We address the task of automatically pre-
dicting if summarization system perfor-
mance will be good or bad based on fea-
tures derived directly from either single- or
multi-document inputs. Our labelled cor-
pus for the task is composed of data from
large scale evaluations completed over the
span of several years. The variation of data
between years allows for a comprehensive
analysis of the robustness of features, but
poses a challenge for building a combined
corpus which can be used for training and
testing. Still, we find that the problem can
be mitigated by appropriately normalizing
for differences within each year. We ex-
amine different formulations of the classi-
fication task which considerably influence
performance. The best results are 84%
prediction accuracy for single- and 74%
for multi-document summarization.
1 Introduction
The input to a summarization system significantly
affects the quality of the summary that can be pro-
duced for it, by either a person or an automatic
method. Some inputs are difficult and summaries
produced by any approach will tend to be poor,
while other inputs are easy and systems will ex-
hibit good performance. User satisfaction with the
summaries can be improved, for example by auto-
matically flagging summaries for which a system
expects to perform poorly. In such cases the user
can ignore the summary and avoid the frustration
of reading poor quality text.
(Brandow et al, 1995) describes an intelligent
summarizer system that could identify documents
which would be difficult to summarize based on
structural properties. Documents containing ques-
tion/answer sessions, speeches, tables and embed-
ded lists were identified based on patterns and
these features were used to determine whether an
acceptable summary can be produced. If not, the
inputs were flagged as unsuitable for automatic
summarization. In our work, we provide deeper
insight into how other characteristics of the text
itself and properties of document clusters can be
used to identify difficult inputs.
The task of predicting the confidence in system
performance for a given input is in fact relevant not
only for summarization, but in general for all ap-
plications aimed at facilitating information access.
In question answering for example, a system may
be configured not to answer questions for which
the confidence of producing a correct answer is
low, and in this way increase the overall accuracy
of the system whenever it does produce an answer
(Brill et al, 2002; Dredze and Czuba, 2007).
Similarly in machine translation, some sen-
tences might contain difficult to translate phrases,
that is, portions of the input are likely to lead
to garbled output if automatic translation is at-
tempted. Automatically identifying such phrases
has the potential of improving MT as shown by
an oracle study (Mohit and Hwa, 2007). More re-
cent work (Birch et al, 2008) has shown that prop-
erties of reordering, source and target language
complexity and relatedness can be used to pre-
dict translation quality. In information retrieval,
the problem of predicting system performance has
generated considerable interest and has led to no-
tably good results (Cronen-Townsend et al, 2002;
Yom-Tov et al, 2005; Carmel et al, 2006).
541
2 Task definition
In summarization, researchers have recognized
that some inputs might be more successfully han-
dled by a particular subsystem (McKeown et al,
2001), but little work has been done to qualify the
general characteristics of inputs that lead to subop-
timal performance of systems. Only recently the
issue has drawn attention: (Nenkova and Louis,
2008) present an initial analysis of the factors that
influence system performance in content selection.
This study was based on results from the Doc-
ument Understanding Conference (DUC) evalua-
tions (Over et al, 2007) of multi-document sum-
marization of news. They showed that input, sys-
tem identity and length of the target summary were
all significant factors affecting summary quality.
Longer summaries were consistently better than
shorter ones for the same input, so improvements
can be easy in applications where varying target
size is possible. Indeed, varying summary size is
desirable in many situations (Kaisser et al, 2008).
The most predictive factor of summary quality
was input identity, prompting a closer investiga-
tion of input properties that are indicative of dete-
rioration in performance. For example, summaries
of articles describing different opinions about an
issue or of articles describing multiple distinct
events of the same type were of overall poor qual-
ity, while summaries of more focused inputs, deal-
ing with descriptions of a single event, subject or
person (biographical), were on average better.
A number of features were defined, capturing
aspects of how focused on a single topic a given
input is. Analysis of the predictive power of the
features was done using only one year of DUC
evaluations. Data from later evaluations was used
to train and test a logistic regression classifier for
prediction of expected system performance. The
task could be performed with accuracy of 61.45%,
significantly above chance levels.
The results also indicated that special care needs
to be taken when pooling data from different eval-
uations into a single dataset. Feature selection per-
formed on data from one year was not useful for
prediction on data from other years, and actually
led to worse performance than using all features.
Moreover, directly indicating which evaluation the
data came from was the most predictive feature
when testing on data from more than one year.
In the work described here, we show how the
approach for predicting performance confidence
can be improved considerably by paying special
attention to the way data from different years is
combined, as well as by adopting alternative task
formulations (pairwise comparisons of inputs in-
stead of binary class prediction), and utilizing
more representative examples for good and bad
performance. We also extend the analysis to sin-
gle document summarization, for which predict-
ing system performance turns out to be much more
accurate than for multi-document summarization.
We address three key questions.
What features are predictive of performance on
a given input? In Section 4, we discuss four
classes of features capturing properties of the in-
put, related to input size, information-theoretic
properties of the distribution of words in the input,
presence of descriptive (topic) words and similar-
ity between the documents in multi-document in-
puts. Rather than using a single year of evaluations
for the analysis, we report correlation with ex-
pected system performance for all years and tasks,
showing that in fact the power of these features
varies considerably across years (Section 5).
How to combine data from different years? The
available data spans several years of summariza-
tion evaluations. Between years, systems change,
as well as number of systems and average input
difficulty. All of these changes impact system per-
formance and make data from different years dif-
ficult to analyze when taken together. Still, one
would want to combine all of the available eval-
uations in order to have more data for developing
machine learning models. In Section 6 we demon-
strate that this indeed can be achieved, by normal-
izing within each year by the highest observed per-
formance and only then combining the data.
How to define input difficulty? There are several
possible definitions of ?input difficulty? or ?good
performance?. All the data can be split in two
binary classes of ?good? and ?bad? performance
respectively, or only representative examples in
which there is a clear difference in performance
can be used. In Section 7 we show that these alter-
natives can dramatically influence prediction ac-
curacy: using representative examples improves
accuracy by more than 10%. Formulating the task
as ranking of two inputs, predicting which one is
more difficult, also turns out to be helpful, offering
more data even within the same year of evaluation.
542
3 Data
We use the data from single- and multi-document
evaluations performed as part of the Document
Understanding Conferences (Over et al, 2007)
from 2001 to 2004.1 Generic multi-document
summarization was evaluated in all of these years,
single document summaries were evaluated only
in 2001 and 2002. We use the 100-word sum-
maries from both tasks.
In the years 2002-2004, systems were eval-
uated respectively on 59, 37 and 100 (50
for generic summarization and 50 biographical)
multi-document inputs. There were 149 inputs for
single document summarization in 2001 and 283
inputs in 2002. Combining the datasets from the
different years yields a collection of 432 observa-
tions for single-document summarization, and 196
for multi-document summarization.
Input difficulty, or equivalently expected con-
fidence of system performance, was defined em-
pirically, based on actual content selection evalua-
tions of system summaries. More specifically, ex-
pected performance for each input was defined as
the average coverage score of all participating sys-
tems evaluated on that input. In this way, the per-
formance confidence is not specific to any given
system, but instead reflects what can be expected
from automatic summarizers in general.
The coverage score was manually computed by
NIST evaluators. It measures content selection by
estimating overlap between a human model and a
system summary. The scale for the coverage score
was different in 2001 compared to other years: 0
to 4 scale, switching to a 0 to 1 scale later.
4 Features
For our experiments we use the features proposed,
motivated and described in detail by (Nenkova and
Louis, 2008). Four broad classes of easily com-
putable features were used to capture aspects of
the input predictive of system performance.
Input size-related Number of sentences in the
input, number of tokens, vocabulary size, percent-
age of words used only once, type-token ratio.
Information-theoretic measures Entropy of
the input word distribution and KL divergence be-
tween the input and a large document collection.
1Evaluations from later years did not include generic sum-
marization, but introduced new tasks such as topic-focused
and update summarization.
Log-likelihood ratio for words in the input
Number of topic signature words (Lin and Hovy,
2000; Conroy et al, 2006) and percentage of sig-
nature words in the vocabulary.
Document similarity in the input set These
features apply to multi-document summarization
only. Pairwise similarity of documents within an
input were computed using tf.idf weighted vector
representations of the documents, either using all
words or using only topic signature words. In both
settings, minimum, maximum and average cosine
similarity was computed, resulting in six similar-
ity features.
Multi-document summaries from DUC 2001
were used for feature selection. The 29 sets for
that year were divided according to the average
coverage score of the evaluated systems. Sets with
coverage below the average were deemed to be the
ones that will elicit poor performance and the rest
were considered examples of sets for which sys-
tems perform well. T-tests were used to select fea-
tures that were significantly different between the
two classes. Six features were selected: vocabu-
lary size, entropy, KL divergence, percentage of
topic signatures in the vocabulary, and average co-
sine and topic signature similarity.
5 Correlations with performance
The Pearson correlations between features of the
input and average system performance for each
year is shown in Tables 1 and 2 for multi- and
single-document summarization respectively. The
last two columns show correlations for the com-
bined data from different evaluation years. For
the last column in both tables, the scores in each
year were first normalized by the highest score that
year. Features that were significantly correlated
with expected performance at confidence level of
0.95 are marked with (*). Overall, better perfor-
mance is associated with smaller inputs, lower en-
tropy, higher KL divergence and more signature
terms, as well as with higher document similarity
for multi-document summarization.
Several important observations can be made
from the correlation numbers in the two tables.
Cross-year variation There is a large variation in
the strength of correlation between performance
and various features. For example, KL diver-
gence is significantly correlated with performance
for most years, with correlation of 0.4618 for the
generic summaries in 2004, but the correlation was
543
features 2001 2002 2003 2004G 2004B All(UN) All(N)
tokens -0.2813 -0.2235 -0.3834* -0.4286* -0.1596 -0.2415* -0.2610*
sentences -0.2511 -0.1906 -0.3474* -0.4197* -0.1489 -0.2311* -0.2753*
vocabulary -0.3611* -0.3026* -0.3257* -0.4286* -0.2239 -0.2568* -0.3171*
per-once -0.0026 -0.0375 0.1925 0.2687 0.2081 0.2175* 0.1813*
type/token -0.0276 -0.0160 0.1324 0.0389 -0.1537 -0.0327 -0.0993
entropy -0.4256* -0.2936* -0.1865 -0.3776* -0.1954 -0.2283* -0.2761*
KL divergence 0.3663* 0.1809 0.3220* 0.4618* 0.2359 0.2296* 0.2879*
avg cosine 0.2244 0.2351 0.1409 0.1635 0.2602 0.1894* 0.2483*
min cosine 0.0308 0.2085 -0.5330* -0.1766 0.1839 -0.0337 -0.0494
max cosine 0.1337 0.0305 0.2499 0.1044 -0.0882 0.0918 0.1982*
num sign -0.1880 -0.0773 -0.1799 -0.0149 0.1412 -0.0248 0.0084
% sign. terms 0.3277 0.1645 0.1429 0.3174* 0.3071* 0.1952* 0.2609*
avg topic 0.2860 0.3678* 0.0826 0.0321 0.1215 0.1745* 0.2021*
min topic 0.0414 0.0673 -0.0167 -0.0025 -0.0405 -0.0177 -0.0469
max topic 0.2416 0.0489 0.1815 0.0134 0.0965 0.1252 0.2082*
Table 1: Correlations between input features and average system performance for multi-document inputs
of DUC 2001-2003, 2004G (generic task), 2004B (biographical task), All data (2002-2004) - UNnor-
malized and Normalized coverage scores. P-values smaller than 0.05 are marked by *.
not significant (0.1809) for 2002 data. Similarly,
the average similarity of topic signature vectors is
significant in 2002, but has correlations close to
zero in the following two years. This shows that
no feature exhibits robust predictive power, espe-
cially when there are relatively few datapoints. In
light of this finding, developing additional features
and combining data to obtain a larger collection of
samples are important for future progress.
Normalization Because of the variation from year
to year, normalizing performance scores is benefi-
cial and leads to higher correlation for almost all
features. On average, correlations increase by 0.05
for all features. Two of the features, maximum co-
sine similarity and max topic word similarity, be-
come significant only in the normalized data. As
we will see in the next section, prediction accu-
racy is also considerably improved when scores
are normalized before pooling the data from dif-
ferent years together.
Single- vs. multi-document task The correla-
tions between performance and input features are
higher in single-document summarization than in
multi-document. For example, in the normalized
data KL divergence has correlation of 0.28 for
multi-document summarization but 0.40 for sin-
gle document. The number of signature terms
is highly correlated with performance in single-
document summarization (-0.25) but there is prac-
tically no correlation for multi-document sum-
maries. Consequently, we can expect that the
performance prediction will be more accurate for
single-document summarization.
features 2001 2002 All(N)
tokens -0.3784* -0.2434* -0.3819*
sentences -0.3999* -0.2262* -0.3705*
vocabulary -0.4410* -0.2706* -0.4196*
per-once -0.0718 0.0087 0.0496
type/token 0.1006 0.0952 0.1785
entropy -0.5326* -0.2329* -0.3789*
KL divergence 0.5332* 0.2676* 0.4035*
num sign -0.2212* -0.1127 -0.2519*
% sign 0.3278* 0.1573* 0.2042*
Table 2: Correlations between input features and
average system performance for single doc. inputs
of DUC?01, ?02, All (?01+?02) N-normalized. P-
values smaller than 0.05 are marked by *.
6 Classification experiments
In this section we explore how the alternative task
formulations influence success of predicting sys-
tem performance. Obviously, the two classes of
interest for the prediction will be ?good perfor-
mance? and ?poor performance?. But separat-
ing the real valued coverage scores for inputs into
these two classes can be done in different ways.
All the data can be used and the definition of
?good? or ?bad? can be determined in relation to
the average performance on all inputs. Or only the
best and worst sets can be used as representative
examples. We explore the consequences of adopt-
ing either of these options.
For the first set of experiments, we divide all
inputs based on the mean value of the average sys-
tem scores as in (Nenkova and Louis, 2008). All
multi-document results reported in this paper are
based on the use of the six significant features dis-
cussed in Section 4. DUC 2002, 2003 and 2004
data was used for 10-fold cross validation. We ex-
544
perimented with three classifiers available in R?
logistic regression (LogR), decision tree (DTree)
and support vector machines (SVM). SVM and
decision tree classifiers are libraries under CRAN
packages e1071 and rpart.2 Since our develop-
ment set was very small (only 29 inputs), we did
not perform any parameter tuning.
There is nearly equal number of inputs on either
side of the average system performance and the
random baseline performance in this case would
give 50% accuracy.
6.1 Multi-document task
The classification accuracy for the multi-
document inputs is reported in Table 3. The
partitioning into classes was done based on
the average performance (87 easy sets and 109
difficult sets).
As expected, normalization considerably im-
proves results. The absolute largest improvement
of 10% is for the logistic regression classifier. For
this classifier, prediction accuracy for the non-
normalized data is 54% while for the normalized
data, it is 64%. Logistic regression gives the best
overall classification accuracy on the normalized
data compared to SVM classifier that does best on
the unnormalized data (56% accuracy). Normal-
ization also improves precision and recall for the
SVM and logistic regression classifiers.
The differences in accuracies obtained by the
classifiers is also noticable and we discuss these
further in Section 7.
6.2 Single document task
We now turn to the task of predicting summa-
rization performance for single document inputs.
As we saw in section 5, the features are stronger
predictors for summarization performance in the
single-document task. In addition, there is more
data from evaluations of single document summa-
rizers. Stronger features and more training data
can both help achieve higher prediction accura-
cies. In this section, we separate out the two fac-
tors and demonstrate that indeed the features are
much more predictive for single document sum-
marization than for multidocument.
In order to understand the effect of having more
training data, we did not divide the single doc-
ument inputs into a separate development set to
use for feature selection. Instead, all the features
2http://cran.r-project.org/web/packages/
classifier accuracy P R F
DTree 66.744 66.846 67.382 67.113
LogR 67.907 67.089 69.806 68.421
SVM 69.069 66.277 80.317 72.625
Table 4: Single document input classification Pre-
cision (P), Recall (R),and F score (F) for difficult
inputs on DUC?01 and ?02 (total 432 examples)
divided into 2 classes based on the average cover-
age score (217 difficult and 215 easy inputs).
discussed in Section 4 except the six cosine and
topic signature similarity measures are used. The
coverage score ranges in DUC 2001 and 2002 are
different. They are normalized by the maximum
score within the year, then combined and parti-
tioned in two classes with respect to the average
coverage score. In this way, the 432 observations
are split into almost equal halves, 215 good perfor-
mance examples and 217 bad performance. Table
4 shows the accuracy, precision and recall of the
classifiers on single-document inputs.
From the results in Table 4 it is evident that
all three classifiers achieve accuracies higher than
those for multi-document summarization. The im-
provement is largest for decision tree classifica-
tion, nearly 15%. The SVM classifier has the high-
est accuracy for single document summarization
inputs, (69%), which is 7% absolute improvement
over the performance of the SVM classifier for
the multi-document task. The smallest improve-
ment of 4% is for the logistic regression classi-
fier which is the one with highest accuracy for the
multi-document task
Improved accuracy could be attributed to the
fact that almost double the amount of data is avail-
able for the single-document summarization ex-
periments. To test if this was the main reason for
improvement, we repeated the single-document
experiments using a random sample of 196 inputs,
the same amount of data as for the multi-document
case. Even with reduced data, single-document
inputs are more easily classifiable as difficult or
easy compared to multi-document, as shown in Ta-
bles 3 and 5. The SVM classifier is still the best
for single-document summarization and its accu-
racy is the same with reduced data as with all
data. With less data, the performance of the lo-
gistic regression and decision tree classifiers de-
grades more and is closer to the numbers for multi-
document inputs.
545
Classifier N/UN Acc Pdiff Rdiff Peasy Reasy Fdiff Feasy
DTree UN 51.579 56.580 56.999 46.790 45.591 55.383 44.199N 52.105 56.474 57.786 46.909 45.440 55.709 44.298
LogR UN 54.211 56.877 71.273 50.135 34.074 62.145 39.159N 63.684 63.974 79.536 63.714 45.980 69.815 51.652
SVM UN 55.789 57.416 73.943 50.206 32.753 63.784 38.407N 62.632 61.905 81.714 61.286 38.829 69.873 47.063
Table 3: Multi-document input classification results on UNnormalized and Normalized data from DUC
2002 to 2004. Both Normalized and UNormalized data contain 109 difficult and 87 easy inputs. Since
the split is not balanced, the accuracy of classification as well as the Precision (P), Recall (R) and F score
(F) are reported for both classes of easy and diff(icult) inputs.
classifier accuracy P R F
DTree 53.684 54.613 53.662 51.661
LogR 61.579 63.335 60.400 60.155
SVM 69.474 66.339 85.835 73.551
Table 5: Single-document-input classification Pre-
cision (P), Recall (R), and F score (F) for difficult
inputs on a random sample of 196 observations (99
difficult/97 easy) from DUC?01 and ?02.
7 Learning with representative examples
In the experiments in the previous section, we used
the average coverage score to split inputs into two
classes of expected performance. Poor perfor-
mance was assigned to the inputs for which the
average system coverage score was lower than the
average for all inputs. Good performance was as-
signed to those with higher than average cover-
age score. The best results for this formulation
of the prediction task is 64% accuracy for multi-
document classification (logistic regression classi-
fier; 196 datapoints) and 69% for single-document
(SVM classifier; 432 and 196 datapoints).
However, inputs with coverage scores close to
the average may not be representative of either
class. Moreover, inputs for which performance
was very similar would end up in different classes.
We can refine the dataset by using only those ob-
servations that are highly representative of the cat-
egory they belong to, removing inputs for which
system performance was close to the average. It
is desirable to be able to classify mediocre inputs
as a separate category. Further studies are neces-
sary to come up with better categorization of in-
puts rather than two strict classes of difficult and
easy. For now, we examine the strength of our fea-
tures in distinguishing the extreme types by train-
ing and testing only on inputs that are representa-
tive of these classes.
We test this hypothesis by starting with 196
multi-document inputs and performing the 10-fold
cross validation using only 80%, 60% and 50%
of the data, incrementally throwing away obser-
vations around the mean. For example, the 80%
model was learnt on 156 observations, taking the
extreme 78 observations on each side into the dif-
ficult and easy categories. For the single document
case, we performed the same tests starting with
a random sample of 196 observations as 100%
data.3 All classifiers were trained and tested on
the same division of folds during cross validation
and compared using a paired t-test to determine
the significance of differences if any. Results are
shown in Table 6. In parentheses after the accu-
racy of a given classifier, we indicate the classifiers
that are significantly better than it.
Classifiers trained and tested using only repre-
sentative examples perform more reliably. The
SVM classifier is the best one for the single-
document setting and in most cases significantly
outperforms logistic regression and decision tree
classifiers on accuracy and recall. In the multi-
document setting, SVM provides better overall re-
call than logistic regression. However, with re-
spect to accuracy, SVM and logistic regression
classifiers are indistinguishable. The decision tree
classifier performs worse.
For multi-document classification, the F score
drops initially when data is reduced to only 80%.
But when using only half of the data, accuracy
of prediction reaches 74%, amounting to 10% ab-
solute improvement compared to the scenario in
which all available data is used. In the single-
document case, accuracy for the SVM classifier
increases consistently, reaching accuracy of 84%.
8 Pairwise ranking approach
The task we addressed in previous sections was to
classify inputs into ones for which we expect good
3We use the same amount of data as is available for multi-
document so that the results can be directly comparable.
546
Single document classification Multi-document classification
Data CL Acc P R F Acc P R F
100%
DTree 53.684 (S) 54.613 53.662 (S) 51.661 52.105 (S,L) 56.474 57.786 (S,L) 55.709
LogR 61.579 (S) 63.335 60.400 (S) 60.155 63.684 63.974 79.536 69.815
SVM 69.474 66.339 85.835 73.551 62.632 61.905 81.714 69.873
80%
DTree 62.000 (S) 62.917 (S) 67.089 (S) 62.969 53.333 57.517 55.004 (S) 51.817
LogR 68.000 68.829 69.324 (S) 67.686 58.667 60.401 59.298 (S) 57.988
SVM 71.333 70.009 86.551 75.577 62.000 61.492 71.075 63.905
60%
DTree 68.182 (S) 72.750 60.607 (S) 64.025 57.273 (S) 63.000 58.262 (S) 54.882
LogR 70.909 73.381 69.250 69.861 67.273 68.357 70.167 65.973
SVM 76.364 73.365 82.857 76.959 66.364 68.619 75.738 67.726
50%
DTree 70.000 (S) 69.238 67.905 (S) 66.299 65.000 60.381 (L) 70.809 64.479
LogR 76.000 (S) 76.083 72.500 (S) 72.919 74.000 72.905 70.381 (S) 70.965
SVM 84.000 83.476 89.000 84.379 72.000 67.667 79.143 71.963
Table 6: Performance of multiple classifiers on extreme observations from single and multi-document
data (100% data = 196 data points in both cases divided into 2 classes on the basis of average coverge
score). Reported precision (P), recall (R) and F score (F) are for difficult inputs. Experiments on ex-
tremes use equal number of examples from each class - baseline performance is 50%. Systems whose
performance is significantly better than the specified numbers are shown in brackets (S-SVM, D-Decision
Tree, L-Logistic Regression).
performance and ones for which poor system per-
formance is expected. In this section, we evaluate
a different approach to input difficulty classifica-
tion. Given a pair of inputs, can we identify the
one on which systems will perform better? This
ranking task is easier than requiring a strict deci-
sion on whether performance will be good or not.
Ranking approaches are widely used in text
planning and sentence ordering (Walker et al,
2001; Karamanis, 2003) to select the text with best
structure among a set of possible candidates. Un-
der the summarization framework, (Barzilay and
Lapata, 2008) ranked different summaries for the
same input according to their coherence. Simi-
larly, ranking alternative document clusters on the
same topic to choose the best input will prove an
added advantage to summarizer systems. When
summarization is used as part of an information
access interface, the clustering of related docu-
ments that form the input to a system is done
automatically. Currently, the clustering of docu-
ments is completely independent of the need for
subsequent summarization of the resulting clus-
ters. Techniques for predicting summarizer per-
formance can be used to inform clustering so that
the clusters most suitable for summarization can
be chosen. Also, when sample inputs for which
summaries were deemed to be good are available,
these can be used as a standard with which new
inputs can be compared.
For the pairwise comparison task, the features
are the difference in feature values between the
two inputs A and B that form a pair. The dif-
ference in average system scores of inputs A and
B in the pair is used to determine the input for
which performance was better. Every pair could
give two training examples, one positive and one
negative depending on the direction in which the
differences are computed. We choose one exam-
ple from every pair, maintaining an equal number
of positive and negative instances.
The idea of using representative examples can
be applied for the pairwise formulation of the task
as well?the larger the difference in system perfor-
mance is, the better example the pair represents.
Very small score differences are not as indicative
of performance on one input being better than the
other. Hence the experiments were duplicated on
80%, 60% and 40% of the data where the retained
examples were the ones with biggest difference
between the system performance on the two sets
(as indicated by the average coverage score). The
range of score differences in each year are indi-
cated in the Table 7.
All scores are normalized by the maximum
score within the year. Therefore the smallest and
largest possible differences are 0 and 1 respec-
tively. The entries corresponding to the years
2002, 2003 and 2004 show the SVM classification
results when inputs were paired only with those
within the same year. Next inputs of all years were
paired with no restrictions. We report the classifi-
cation accuracies on a random sample of these ex-
amples equal in size to the number of datapoints
in the 2004 examples.
Using only representative examples leads to
547
Amt Data Min score diff Points Acc.
All
2002 0.00028 1710 65.79
2003 0.00037 666 73.94
2004 0.00023 4948 70.71
2002-2004 0.00005 4948 68.85
80%
2002 0.05037 1368 68.39
2003 0.08771 532 78.87
2004 0.05226 3958 73.36
2002-2004 0.02376 3958 70.68
60%
2002 0.10518 1026 73.04
2003 0.17431 400 82.50
2004 0.11244 2968 77.41
2002-2004 0.04844 2968 71.39
40%
2002 0.16662 684 76.03
2003 0.27083 266 87.31
2004 0.18258 1980 79.34
2002-2004 0.07489 1980 74.95
Maximum score difference 2002 (0.8768), 2003 (0.8969),
2004 (0.8482), 2002-2004 (0.8768)
Table 7: Accuracy of SVM classification of mul-
tidocument input pairs. When inputs are paired
irrespective of year (2002-2004), datapoints equal
in number to that in 2004 were chosen at random.
consistently better results than using all the data.
The best classification accuracy is 76%, 87% and
79% for comparisons within the same year and
74% for comparisons across years. It is important
to observe that when inputs are compared with-
out any regard to the year, the classifier perfor-
mance is worse than when both inputs in the pair
are taken from the same evaluation year, present-
ing additional evidence of the cross-year variation
discussed in Section 5. A possible explanation
is that system improvements in later years might
cause better scores to be obtained on inputs which
were difficult previously.
9 Conclusions
We presented a study of predicting expected sum-
marization performance on a given input. We
demonstrated that prediction of summarization
system performance can be done with high ac-
curacy. Normalization and use of representative
examples of difficult and easy inputs both prove
beneficial for the task. We also find that per-
formance predictions for single-document sum-
marization can be done more accurately than for
multi-document summarization. The best classi-
fier for single-document classification are SVMs,
and the best for multi-document?logistic regres-
sion and SVM. We also record good prediction
performance on pairwise comparisons which can
prove useful in a variety of situations.
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. CL, 34(1):1?34.
A. Birch, M. Osborne, and P. Koehn. 2008. Predicting
success in machine translation. In Proceedings of
EMNLP, pages 745?754.
R. Brandow, K. Mitze, and L. F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Inf. Process. Manage., 31(5):675?685.
E. Brill, S. Dumais, and M. Banko. 2002. An analysis
of the askmsr question-answering system. In Pro-
ceedings of EMNLP.
D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.
2006. What makes a query difficult? In Proceed-
ings of SIGIR, pages 390?397.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL.
S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2002.
Predicting query performance. In Proceedings of SI-
GIR, pages 299?306.
M. Dredze and K. Czuba. 2007. Learning to admit
you?re wrong: Statistical tools for evaluating web
qa. In NIPS Workshop on Machine Learning for Web
Search.
M. Kaisser, M. A. Hearst, and J. B. Lowe. 2008. Im-
proving search results quality by customizing sum-
mary lengths. In Proceedings of ACL: HLT, pages
701?709.
N. Karamanis. 2003. Entity Coherence for Descriptive
Text Structuring. Ph.D. thesis, University of Edin-
burgh.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495?501.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivas-
siloglou, B. Schiffman, and S. Teufel. 2001.
Columbia multi-document summarization: Ap-
proach and evaluation. In Proceedings of DUC.
B. Mohit and R. Hwa. 2007. Localization of difficult-
to-translate phrases. In Proceedings of ACL Work-
shop on Statistical Machine Translations.
A. Nenkova and A. Louis. 2008. Can you summa-
rize this? identifying correlates of input difficulty
for multi-document summarization. In Proceedings
of ACL: HLT, pages 825?833.
P. Over, H. Dang, and D. Harman. 2007. Duc in con-
text. Inf. Process. Manage., 43(6):1506?1520.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proceedings of
NAACL, pages 1?8.
E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.
2005. Learning to estimate query difficulty: includ-
ing applications to missing content detection and
distributed information retrieval. In Proceedings of
SIGIR, pages 512?519.
548
Proceedings of ACL-08: HLT, pages 825?833,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Can you summarize this? Identifying correlates of input difficulty for
generic multi-document summarization
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Abstract
Different summarization requirements could
make the writing of a good summary more dif-
ficult, or easier. Summary length and the char-
acteristics of the input are such constraints in-
fluencing the quality of a potential summary.
In this paper we report the results of a quanti-
tative analysis on data from large-scale evalu-
ations of multi-document summarization, em-
pirically confirming this hypothesis. We fur-
ther show that features measuring the cohe-
siveness of the input are highly correlated with
eventual summary quality and that it is possi-
ble to use these as features to predict the diffi-
culty of new, unseen, summarization inputs.
1 Introduction
In certain situations even the best automatic sum-
marizers or professional writers can find it hard to
write a good summary of a set of articles. If there
is no clear topic shared across the input articles, or
if they follow the development of the same event in
time for a longer period, it could become difficult
to decide what information is most representative
and should be conveyed in a summary. Similarly,
length requirements could pre-determine summary
quality?a short outline of a story might be confus-
ing and unclear but a page long discussion might
give an excellent overview of the same issue.
Even systems that perform well on average pro-
duce summaries of poor quality for some inputs. For
this reason, understanding what aspects of the in-
put make it difficult for summarization becomes an
interesting and important issue that has not been ad-
dressed in the summarization community untill now.
In information retrieval, for example, the variable
system performance has been recognized as a re-
search challenge and numerous studies on identify-
ing query difficulty have been carried out (most re-
cently (Cronen-Townsend et al, 2002; Yom-Tov et
al., 2005; Carmel et al, 2006)).
In this paper we present results supporting the hy-
potheses that input topicality cohesiveness and sum-
mary length are among the factors that determine
summary quality regardless of the choice of summa-
rization strategy (Section 2). The data used for the
analyses comes from the annual Document Under-
standing Conference (DUC) in which various sum-
marization approaches are evaluated on common
data, with new test sets provided each year.
In later sections we define a suite of features cap-
turing aspects of the topicality cohesiveness of the
input (Section 3) and relate these to system perfor-
mance, identifying reliable correlates of input diffi-
culty (Section 4). Finally, in Section 5, we demon-
strate that the features can be used to build a clas-
sifier predicting summarization input difficulty with
accuracy considerably above chance level.
2 Preliminary analysis and distinctions:
DUC 2001
Generic multi-document summarization was fea-
tured as a task at the Document Understanding Con-
ference (DUC) in four years, 2001 through 2004.
In our study we use the DUC 2001 multi-document
task submissions as development data for in-depth
analysis and feature selection. There were 29 in-
put sets and 12 automatic summarizers participating
in the evaluation that year. Summaries of different
825
lengths were produced by each system: 50, 100, 200
and 400 words. Each summary was manually eval-
uated to determine the extent to which its content
overlaped with that of a human model, giving a cov-
erage score. The content comparison was performed
on a subsentence level and was based on elementary
discourse units in the model summary.1
The coverage scores are taken as an indicator of
difficultly of the input: systems achieve low cover-
age for difficult sets and higher coverage for easy
sets. Since we are interested in identifying charac-
teristics of generally difficult inputs rather than in
discovering what types of inputs might be difficult
for one given system, we use the average system
score per set as indicator of general difficulty.
2.1 Analysis of variance
Before attempting to derive characteristics of inputs
difficult for summarization, we first confirm that in-
deed expected performance is influenced by the in-
put itself. We performed analysis of variance for
DUC 2001 data, with automatic system coverage
score as the dependent variable, to gain some insight
into the factors related to summarization difficulty.
The results of the ANOVA with input set, summa-
rizer identity and summary length as factors, as well
as the interaction between these, are shown in Ta-
ble 1.
As expected, summarizer identity is a significant
factor: some summarization strategies/systems are
more effective than others and produce summaries
with higher coverage score. More interestingly, the
input set and summary length factors are also highly
significant and explain more of the variability in
coverage scores than summarizer identity does, as
indicated by the larger values of the F statistic.
Length The average automatic summarizer cov-
erage scores increase steadily as length requirements
are relaxed, going up from 0.50 for 50-word sum-
maries to 0.76 for 400-word summaries as shown in
Table 2 (second row). The general trend we observe
is that on average systems are better at producing
summaries when more space is available. The dif-
1The routinely used tool for automatic evaluation ROUGE
was adopted exactly because it was demonstrated it is highly
correlated with the manual DUC coverage scores (Lin and
Hovy, 2003a; Lin, 2004).
Type 50 100 200 400
Human 1.00 1.17 1.38 1.29
Automatic 0.50 0.55 0.70 0.76
Baseline 0.41 0.46 0.52 0.57
Table 2: Average human, system and baseline coverage
scores for different summary lengths of N words. N =
50, 100, 200, and 400.
ferences are statistically significant2 only between
50-word and 200- and 400-word summaries and be-
tween 100-word and 400-word summaries. The fact
that summary quality improves with increasing sum-
mary length has been observed in prior studies as
well (Radev and Tam, 2003; Lin and Hovy, 2003b;
Kolluru and Gotoh, 2005) but generally little atten-
tion has been paid to this fact in system development
and no specific user studies are available to show
what summary length might be most suitable for
specific applications. In later editions of the DUC
conference, only summaries of 100 words were pro-
duced, focusing development efforts on one of the
more demanding length restrictions. The interaction
between summary length and summarizer is small
but significant (Table 1), with certain summariza-
tion strategies more successful at particular sum-
mary lengths than at others.
Improved performance as measured by increase
in coverage scores is observed for human summa-
rizers as well (shown in the first row of Table 2).
Even the baseline systems (first n words of the most
recent article in the input or first sentences from
different input articles) show improvement when
longer summaries are allowed (performance shown
in the third row of the table). It is important to
notice that the difference between automatic sys-
tem and baseline performance increases as the sum-
mary length increases?the difference between sys-
tems and baselines coverage scores is around 0.1
for the shorter 50- and 100-word summaries but 0.2
for the longer summaries. This fact has favorable
implications for practical system developments be-
cause it indicates that in applications where some-
what longer summaries are appropriate, automati-
cally produced summaries will be much more infor-
mative than a baseline summary.
2One-sided t-test, 95% level of significance.
826
Factor DF Sum of squares Expected mean squares F stat Pr(> F )
input 28 150.702 5.382 59.4227 0
summarizer 11 34.316 3.120 34.4429 0
length 3 16.082 5.361 59.1852 0
input:summarizer 306 65.492 0.214 2.3630 0
input:length 84 36.276 0.432 4.7680 0
summarizer:length 33 6.810 0.206 2.2784 0
Table 1: Analysis of variance for coverage scores of automatic systems with input, summarizer, and length as factors.
Input The input set itself is a highly significant
factor that influences the coverage scores that sys-
tems obtain: some inputs are handled by the systems
better than others. Moreover, the input interacts both
with the summarizers and the summary length.
This is an important finding for several reasons.
First, in system evaluations such as DUC the inputs
for summarization are manually selected by anno-
tators. There is no specific attempt to ensure that
the inputs across different years have on average the
same difficulty. Simply assuming this to be the case
could be misleading: it is possible in a given year to
have ?easier? input test set compared to a previous
year. Then system performance across years can-
not be meaningfully compared, and higher system
scores would not be indicative of system improve-
ment between the evaluations.
Second, in summarization applications there is
some control over the input for summarization. For
example, related documents that need to summa-
rized could be split into smaller subsets that are more
amenable to summarization or routed to an appropri-
ate summarization system than can handle this kind
of input using a different strategy, as done for in-
stance in (McKeown et al, 2002).
Because of these important implications we inves-
tigate input characteristics and define various fea-
tures distinguishing easy inputs from difficult ones.
2.2 Difficulty for people and machines
Before proceeding to the analysis of input difficulty
in multi-document summarization, it is worth men-
tioning that our study is primarily motivated by sys-
tem development needs and consequently the focus
is on finding out what inputs are easy or difficult
for automatic systems. Different factors might make
summarization difficult for people. In order to see to
what extent the notion of summarization input dif-
summary length correlation
50 0.50
100 0.57*
200 0.77**
400 0.70**
Table 3: Pearson correlation between average human and
system coverage scores on the DUC 2001 dataset. Sig-
nificance levels: *p < 0.05 and **p < 0.00001.
ficulty is shared between machines and people, we
computed the correlation between the average sys-
tem and average human coverage score at a given
summary length for all DUC 2001 test sets (shown
in Table 3). The correlation is highest for 200-word
summaries, 0.77, which is also highly significant.
For shorter summaries the correlation between hu-
man and system performance is not significant.
In the remaining part of the paper we deal ex-
clusively with difficulty as defined by system per-
formance, which differs from difficulty for people
summarizing the same material as evidenced by the
correlations in Table 3. We do not attempt to draw
conclusions about any cognitively relevant factors
involved in summarizing.
2.3 Type of summary and difficulty
In DUC 2001, annotators prepared test sets from five
possible predefined input categories:3 .
Single event (3 sets) Documents describing a single
event over a timeline (e.g. The Exxon Valdez
oil spill).
3Participants in the evaluation were aware of the different
categories of input and indeed some groups developed systems
that handled different types of input employing different strate-
gies (McKeown et al, 2001). In later years, the idea of multi-
strategy summarization has been further explored by (Lacatusu
et al, 2006)
827
Subject (6 sets) Documents discussing a single
topic (e.g. Mad cow disease)
Biographical (2 sets) All documents in the input
provide information about the same person
(e.g. Elizabeth Taylor)
Multiple distinct events (12 sets) The documents
discuss different events of the same type (e.g.
different occasions of police misconduct).
Opinion (6 sets) Each document describes a differ-
ent perspective to a common topic (e.g. views
of the senate, congress, public, lawyers etc on
the decision by the senate to count illegal aliens
in the 1990 census).
Figure 1 shows the average system coverage score
for the different input types. The more topically co-
hesive input types such as biographical, single event
and subject, which are more focused on a single en-
tity or news item and narrower in scope, are eas-
ier for systems. The average system coverage score
for them is higher than for the non-cohesive sets
such as multiple distinct events and opinion sets, re-
gardless of summary length. The difference is even
more apparently clear when the scores are plotted af-
ter grouping input types into cohesive (biographical,
single event and subject) and non-cohesive (multi-
ple events and opinion). Such grouping also gives
the necessary power to perform statistical test for
significance, confirming the difference in coverage
scores for the two groups. This is not surprising: a
summary of documents describing multiple distinct
events of the same type is likely to require higher
degree of generalization and abstraction. Summa-
rizing opinions would in addition be highly subjec-
tive. A summary of a cohesive set meanwhile would
contain facts directly from the input and it would be
easier to determine which information is important.
The example human summaries for set D32 (single
event) and set D19 (opinions) shown below give an
idea of the potential difficulties automatic summa-
rizers have to deal with. set D32 On 24 March 1989,
the oil tanker Exxon Valdez ran aground on a reef near
Valdez, Alaska, spilling 8.4 million gallons of crude oil
into Prince William Sound. In two days, the oil spread
over 100 miles with a heavy toll on wildlife. Cleanup
proceeded at a slow pace, and a plan for cleaning 364
miles of Alaskan coastline was released. In June, the
tanker was refloated. By early 1990, only 5 to 9 percent of
spilled oil was recovered. A federal jury indicted Exxon
on five criminal charges and the Valdez skipper was guilty
of negligent discharge of oil.
set D19 Congress is debating whether or not to count ille-
gal aliens in the 1990 census. Congressional House seats
are apportioned to the states and huge sums of federal
money are allocated based on census population. Cali-
fornia, with an estimated half of all illegal aliens, will be
greatly affected. Those arguing for inclusion say that the
Constitution does not mention ?citizens?, but rather, in-
structs that House apportionment be based on the ?whole
number of persons? residing in the various states. Those
opposed say that the framers were unaware of this issue.
?Illegal aliens? did not exist in the U.S. until restrictive
immigration laws were passed in 1875.
The manual set-type labels give an intuitive idea
of what factors might be at play but it is desirable to
devise more specific measures to predict difficulty.
Do such measures exist? Is there a way to automati-
cally distinguish cohesive (easy) from non-cohesive
(difficult) sets? In the next section we define a num-
ber of features that aim to capture the cohesiveness
of an input set and show that some of them are in-
deed significantly related to set difficulty.
3 Features
We implemented 14 features for our analysis of in-
put set difficulty. The working hypothesis is that co-
hesive sets with clear topics are easier to summarize
and the features we define are designed to capture
aspects of input cohesiveness.
Number of sentences in the input, calculated
over all articles in the input set. Shorter inputs
should be easier as there will be less information loss
between the summary and the original material.
Vocabulary size of the input set, equal to the
number of unique words in the input. Smaller vo-
cabularies would be characteristic of easier sets.
Percentage of words used only once in the input.
The rationale behind this feature is that cohesive in-
put sets contain news articles dealing with a clearly
defined topic, so words will be reused across docu-
ments. Sets that cover disparate events and opinions
are likely to contain more words that appear in the
input only once.
Type-token ratio is a measure of the lexical vari-
ation in an input set and is equal to the input vo-
cabulary size divided by the number of words in the
828
Figure 1: Average system coverage scores for summaries in a category
input. A high type-token ratio indicates there is little
(lexical) repetition in the input, a possible side-effect
of non-cohesiveness.
Entropy of the input set. Let X be a discrete ran-
dom variable taking values from the finite set V =
{w1, ..., wn} where V is the vocabulary of the in-
put set and wi are the words that appear in the input.
The probability distribution p(w) = Pr(X = w)
can be easily calculated using frequency counts from
the input. The entropy of the input set is equal to the
entropy of X:
H(X) = ?
i=n
?
i=1
p(wi) log2 p(wi) (1)
Average, minimum and maximum cosine over-
lap between the news articles in the input. Repeti-
tion in the input is often exploited as an indicator of
importance by different summarization approaches
(Luhn, 1958; Barzilay et al, 1999; Radev et al,
2004; Nenkova et al, 2006). The more similar the
different documents in the input are to each other,
the more likely there is repetition across documents
at various granularities.
Cosine similarity between the document vector
representations is probably the easiest and most
commonly used among the various similarity mea-
sures. We use tf*idf weights in the vector represen-
tations, with term frequency (tf) normalized by the
total number of words in the document in order to re-
move bias resulting from high frequencies by virtue
of higher document length alone.
The cosine similarity between two (document
representation) vectors v1 and v2 is given by cos? =
v1.v2
||v1||||v2|| . A value of 0 indicates that the vectors are
orthogonal and dissimilar, a value of 1 indicates per-
fectly similar documents in terms of the words con-
tained in them.
To compute the cosine overlap features, we find
the pairwise cosine similarity between each two
documents in an input set and compute their aver-
age. The minimum and maximum overlap features
are also computed as an indication of the overlap
bounds. We expect cohesive inputs to be composed
of similar documents, hence the cosine overlaps in
these sets of documents must be higher than those in
non-cohesive inputs.
KL divergence Another measure of relatedness
of the documents comprising an input set is the dif-
ference in word distributions in the input compared
to the word distribution in a large collection of di-
verse texts. If the input is found to be largely dif-
ferent from a generic collection, it is plausible to as-
sume that the input is not a random collection of ar-
ticles but rather is defined by a clear topic discussed
within and across the articles. It is reasonable to ex-
pect that the higher the divergence is, the easier it is
to define what is important in the article and hence
the easier it is to produce a good summary.
For computing the distribution of words in a gen-
eral background corpus, we used all the inputs sets
from DUC years 2001 to 2006. The divergence mea-
sure we used is the Kullback Leibler divergence, or
829
relative entropy, between the input (I) and collection
language models. Let pinp(w) be the probability of
the word w in the input and pcoll(w) be the proba-
bility of the word occurring in the large background
collection. Then the relative entropy between the in-
put and the collection is given by
KL divergence =
?
w?I
pinp(w) log2
pinp(w)
pcoll(w)
(2)
Low KL divergence from a random background
collection may be characteristic of highly non-
cohesive inputs consisting of unrelated documents.
Number of topic signature terms for the input
set. The idea of topic signature terms was intro-
duced by Lin and Hovy (Lin and Hovy, 2000) in the
context of single document summarization, and was
later used in several multi-document summarization
systems (Conroy et al, 2006; Lacatusu et al, 2004;
Gupta et al, 2007).
Lin and Hovy?s idea was to automatically iden-
tify words that are descriptive for a cluster of docu-
ments on the same topic, such as the input to a multi-
document summarizer. We will call this cluster T .
Since the goal is to find descriptive terms for the
cluster, a comparison collection of documents not
on the topic is also necessary (we will call this back-
ground collection NT ).
Given T and NT , the likelihood ratio statistic
(Dunning, 1994) is used to identify the topic signa-
ture terms. The probabilistic model of the data al-
lows for statistical inference in order to decide which
terms t are associated with T more strongly than
with NT than one would expect by chance.
More specifically, there are two possibilities for
the distribution of a term t: either it is very indicative
of the topic of cluster T , and appears more often in
T than in documents from NT , or the term t is not
topical and appears with equal frequency across both
T and NT . These two alternatives can be formally
written as the following hypotheses:
H1: P (t|T ) = P (t|NT ) = p (t is not a descrip-
tive term for the input)
H2: P (t|T ) = p1 and P (t|NT ) = p2 and p1 >
p2 (t is a descriptive term)
In order to compute the likelihood of each hypoth-
esis given the collection of the background docu-
ments and the topic cluster, we view them as a se-
quence of words wi: w1w2 . . . wN . The occurrence
of a given word t, wi = t, can thus be viewed a
Bernoulli trial with probability p of success, with
success occurring when wi = t and failure other-
wise.
The probability of observing the term t appearing
k times in N trials is given by the binomial distribu-
tion
b(k,N, p) =
(
N
k
)
pk(1 ? p)N?k (3)
We can now compute
? = Likelihood of the data given H1
Likelihood of the data given H2 (4)
which is equal to
? = b(ct,N, p)b(cT ,NT , p1) ? b(cNT ,NNT , p2)
(5)
The maximum likelihood estimates for the proba-
bilities can be computed directly. p = ctN , where ct is
equal to the number of times term t appeared in the
entire corpus T+NT, and N is the number of words
in the entire corpus. Similarly, p1 = cTNT , where cT
is the number of times term t occurred in T and NT
is the number of all words in T . p2 = cNTNNT , where
cNT is the number of times term t occurred in NT
and NNT is the total number of words in NT.
?2log? has a well-know distribution: ?2. Bigger
values of ?2log? indicate that the likelihood of the
data under H2 is higher, and the ?2 distribution can
be used to determine when it is significantly higher
(?2log? exceeding 10 gives a significance level of
0.001 and is the cut-off we used).
For terms for which the computed ?2log? is
higher than 10, we can infer that they occur more
often with the topic T than in a general corpus NT ,
and we can dub them ?topic signature terms?.
Percentage of signature terms in vocabulary
The number of signature terms gives the total count
of topic signatures over all the documents in the in-
put. However, the number of documents in an input
set and the size of the individual documents across
different sets are not the same. It is therefore possi-
ble that the mere count feature is biased to the length
830
and number of documents in the input set. To ac-
count for this, we add the percentage of topic words
in the vocabulary as a feature.
Average, minimum and maximum topic sig-
nature overlap between the documents in the in-
put. Cosine similarity measures the overlap between
two documents based on all the words appearing in
them. A more refined document representation can
be defined by assuming the document vectors con-
tain only the topic signature words rather than all
words. A high overlap of topic words across two
documents is indicative of shared topicality. The
average, minimum and maximum pairwise cosine
overlap between the tf*idf weighted topic signature
vectors of the two documents are used as features
for predicting input cohesiveness. If the overlap is
large, then the topic is similar across the two docu-
ments and hence their combination will yield a co-
hesive input.
4 Feature selection
Table 4 shows the results from a one-sided t-test
comparing the values of the various features for
the easy and difficult input set classes. The com-
parisons are for summary length of 100 words be-
cause in later years only such summaries were evalu-
ated. The binary easy/difficult classes were assigned
based on the average system coverage score for the
given set, with half of the sets assigned to each class.
In addition to the t-tests we also calculated Pear-
son?s correlation (shown in Table 5) between the fea-
tures and the average system coverage score for each
set. In the correlation analysis the input sets are not
classified into easy or difficult but rather the real val-
ued coverage scores are used directly. Overall, the
features that were identified by the t-test as most de-
scriptive of the differences between easy and diffi-
cult inputs were also the ones with higher correla-
tions with real-valued coverage scores.
Our expectations in defining the features are con-
firmed by the correlation results. For example, sys-
tems have low coverage scores for sets with high-
entropy vocabularies as indicated by the negative
and high by absolute value correlation (-0.4256).
Sets with high entropy are those in which there is
little repetition within and across different articles,
and for which it is subsequently difficult to deter-
feature t-stat p-value
KL divergence* -2.4725 0.01
% of sig. terms in vocab* -2.0956 0.02
average cosine overlap* -2.1227 0.02
vocabulary size* 1.9378 0.03
set entropy* 2.0288 0.03
average sig. term overlap* -1.8803 0.04
max cosine overlap -1.6968 0.05
max topic signature overlap -1.6380 0.06
number of sentences 1.4780 0.08
min topic signature overlap -0.9540 0.17
number of signature terms 0.8057 0.21
min cosine overlap -0.2654 0.39
% of words used only once 0.2497 0.40
type-token ratio 0.2343 0.41
?Significant at a 95% confidence level(p < 0.05)
Table 4: Comparison of non-cohesive (average system
coverage score < median average system score) vs cohe-
sive sets for summary length of 100 words
mine what is the most important content. On the
other hand, sets characterized by bigger KL diver-
gence are easier?there the distribution of words is
skewed compared to a general collection of articles,
with important topic words occurring more often.
Easy to summarize sets are characterized by low
entropy, small vocabulary, high average cosine and
average topic signature overlaps, high KL diver-
gence and a high percentage of the vocabulary con-
sists of topic signature terms.
5 Classification results
We used the 192 sets from multi-document summa-
rization DUC evaluations in 2002 (55 generic sets),
2003 (30 generic summary sets and 7 viewpoint sets)
and 2004 (50 generic and 50 biography sets) to train
and test a logistic regression classifier. The sets from
all years were pooled together and evenly divided
into easy and difficult inputs based on the average
system coverage score for each set.
Table 6 shows the results from 10-fold cross val-
idation. SIG is a classifier based on the six features
identified as significant in distinguishing easy from
difficult inputs based on a t-test comparison (Ta-
ble 4). SIG+yt has two additional features: the year
and the type of summarization input (generic, view-
point and biographical). ALL is a classifier based on
all 14 features defined in the previous section, and
831
feature correlation
set entropy -0.4256
KL divergence 0.3663
vocabulary size -0.3610
% of sig. terms in vocab 0.3277
average sig. term overlap 0.2860
number of sentences -0.2511
max topic signature overlap 0.2416
average cosine overlap 0.2244
number of signature terms -0.1880
max cosine overlap 0.1337
min topic signature overlap 0.0401
min cosine overlap 0.0308
type-token ratio -0.0276
% of words used only once -0.0025
Table 5: Correlation between coverage score and feature
values for the 29 DUC?01 100-word summaries.
features accuracy P R F
SIG 56.25% 0.553 0.600 0.576
SIG+yt 69.27% 0.696 0.674 0.684
ALL 61.45% 0.615 0.589 0.600
ALL+yt 65.10% 0.643 0.663 0.653
Table 6: Logistic regression classification results (accu-
racy, precision, recall and f-measure) for balanced data of
100-word summaries from DUC?02 through DUC?04.
ALL+yt also includes the year and task features.
Classification accuracy is considerably higher
than the 50% random baseline. Using all features
yields better accuracy (61%) than using solely the
6 significant features (accuracy of 56%). In both
cases, adding the year and task leads to extra 3%
net improvement. The best overall results are for
the SIG+yt classifier with net improvement over the
baseline equal to 20%. At the same time, it should
be taken into consideration that the amount of train-
ing data for our experiments is small: a total of 192
sets. Despite this, the measures of input cohesive-
ness capture enough information to result in a clas-
sifier with above-baseline performance.
6 Conclusions
We have addressed the question of what makes the
writing of a summary for a multi-document input
difficult. Summary length is a significant factor,
with all summarizers (people, machines and base-
lines) performing better at longer summary lengths.
An exploratory analysis of DUC 2001 indicated that
systems produce better summaries for cohesive in-
puts dealing with a clear topic (single event, subject
and biographical sets) while non-cohesive sets about
multiple events and opposing opinions are consis-
tently of lower quality. We defined a number of fea-
tures aimed at capturing input cohesiveness, ranging
from simple features such as input length and size
to more sophisticated measures such as input set en-
tropy, KL divergence from a background corpus and
topic signature terms based on log-likelihood ratio.
Generally, easy to summarize sets are character-
ized by low entropy, small vocabulary, high average
cosine and average topic signature overlaps, high
KL divergence and a high percentage of the vocab-
ulary consists of topic signature terms. Experiments
with a logistic regression classifier based on the fea-
tures further confirms that input cohesiveness is pre-
dictive of the difficulty it will pose to automatic sum-
marizers.
Several important notes can be made. First, it is
important to develop strategies that can better handle
non-cohesive inputs, reducing fluctuations in sys-
tem performance. Most current systems are devel-
oped with the expectation they can handle any input
but this is evidently not the case and more attention
should be paid to the issue. Second, the interpre-
tations of year to year evaluations can be affected.
As demonstrated, the properties of the input have a
considerable influence on summarization quality. If
special care is not taken to ensure that the difficulty
of inputs in different evaluations is kept more or less
the same, results from the evaluations are not com-
parable and we cannot make general claims about
progress and system improvements between evalua-
tions. Finally, the presented results are clearly just a
beginning in understanding of summarization diffi-
culty. A more complete characterization of summa-
rization input will be necessary in the future.
References
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan
832
Pelleg. 2006. What makes a query difficult? In SI-
GIR ?06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 390?397.
John Conroy, Judith Schlesinger, and Dianne O?Leary.
2006. Topic-focused multi-document summarization
using an approximate oracle score. In Proceedings of
ACL, companion volume.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2002. Predicting query performance. In Proceedings
of the 25th Annual International ACM SIGIR confer-
ence on Research and Development in Information Re-
trieval (SIGIR 2002), pages 299?306.
Ted Dunning. 1994. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007.
Measuring importance and query relevance in topic-
focused multi-document summarization. In ACL?07,
companion volume.
BalaKrishna Kolluru and Yoshihiko Gotoh. 2005. On
the subjectivity of human authored short summaries.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
Finley Lacatusu, Andrew Hickl, Sanda Harabagiu, and
Luke Nezda. 2004. Lite gistexter at duc2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence (DUC?04).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley,
B. Rink, P. Wang, and L. Taylor. 2006. Lcc?s gistexter
at duc 2006: Multi-strategy multi-document summa-
rization. In DUC?06.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of the 18th conference on Computa-
tional linguistics, pages 495?501.
Chin-Yew Lin and Eduard Hovy. 2003a. Automatic eval-
uation of summaries using n-gram co-occurance statis-
tics. In Proceedings of HLT-NAACL 2003.
Chin-Yew Lin and Eduard Hovy. 2003b. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73?80.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In ACL Text Summarization
Workshop.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Development,
2(2):159?165.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivassiloglou,
B. Schiffman, and S. Teufel. 2001. Columbia multi-
document summarization: Approach and evaluation.
In DUC?01.
Kathleen McKeown, Regina Barzilay, David Evans,
Vasleios Hatzivassiloglou, Judith Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia?s newsblaster. In Pro-
ceedings of the 2nd Human Language Technologies
Conference HLT-02.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that influ-
ence summarization. In Proceedings of SIGIR.
Dragomir Radev and Daniel Tam. 2003. Single-
document and multi-document summary evaluation
via relative utility. In Poster session, International
Conference on Information and Knowledge Manage-
ment (CIKM?03).
Dragomir Radev, Hongyan Jing, Malgorzata Sty, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40:919?938.
Elad Yom-Tov, Shai Fine, David Carmel, and Adam Dar-
low. 2005. Learning to estimate query difficulty: in-
cluding applications to missing content detection and
distributed information retrieval. In SIGIR ?05: Pro-
ceedings of the 28th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 512?519.
833
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683?691,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic sense prediction for implicit discourse relations in text
Emily Pitler, Annie Louis, Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,lannie,nenkova@seas.upenn.edu
Abstract
We present a series of experiments on au-
tomatically identifying the sense of im-
plicit discourse relations, i.e. relations
that are not marked with a discourse con-
nective such as ?but? or ?because?. We
work with a corpus of implicit relations
present in newspaper text and report re-
sults on a test set that is representative
of the naturally occurring distribution of
senses. We use several linguistically in-
formed features, including polarity tags,
Levin verb classes, length of verb phrases,
modality, context, and lexical features. In
addition, we revisit past approaches using
lexical pairs from unannotated text as fea-
tures, explain some of their shortcomings
and propose modifications. Our best com-
bination of features outperforms the base-
line from data intensive approaches by 4%
for comparison and 16% for contingency.
1 Introduction
Implicit discourse relations abound in text and
readers easily recover the sense of such relations
during semantic interpretation. But automatic
sense prediction for implicit relations is an out-
standing challenge in discourse processing.
Discourse relations, such as causal and contrast
relations, are often marked by explicit discourse
connectives (also called cue words) such as ?be-
cause? or ?but?. It is not uncommon, though, for a
discourse relation to hold between two text spans
without an explicit discourse connective, as the ex-
ample below demonstrates:
(1) The 101-year-old magazine has never had to woo ad-
vertisers with quite so much fervor before.
[because] It largely rested on its hard-to-fault demo-
graphics.
In this paper we address the problem of au-
tomatic sense prediction for discourse relations
in newspaper text. For our experiments, we use
the Penn Discourse Treebank, the largest exist-
ing corpus of discourse annotations for both im-
plicit and explicit relations. Our work is also
informed by the long tradition of data intensive
methods that rely on huge amounts of unanno-
tated text rather than on manually tagged corpora
(Marcu and Echihabi, 2001; Blair-Goldensohn et
al., 2007).
In our analysis, we focus only on implicit dis-
course relations and clearly separate these from
explicits. Explicit relations are easy to iden-
tify. The most general senses (comparison, con-
tingency, temporal and expansion) can be disam-
biguated in explicit relations with 93% accuracy
based solely on the discourse connective used to
signal the relation (Pitler et al, 2008). So report-
ing results on explicit and implicit relations sepa-
rately will allow for clearer tracking of progress.
In this paper we investigate the effectiveness of
various features designed to capture lexical and
semantic regularities for identifying the sense of
implicit relations. Given two text spans, previous
work has used the cross-product of the words in
the spans as features. We examine the most infor-
mative word pair features and find that they are not
the semantically-related pairs that researchers had
hoped. We then introduce several other methods
capturing the semantics of the spans (polarity fea-
tures, semantic classes, tense, etc.) and evaluate
their effectiveness. This is the first study which
reports results on classifying naturally occurring
implicit relations in text and uses the natural dis-
tribution of the various senses.
2 Related Work
Experiments on implicit and explicit relations
Previous work has dealt with the prediction of dis-
course relation sense, but often for explicits and at
the sentence level.
Soricut and Marcu (2003) address the task of
683
parsing discourse structures within the same sen-
tence. They use the RST corpus (Carlson et al,
2001), which contains 385 Wall Street Journal ar-
ticles annotated following the Rhetorical Structure
Theory (Mann and Thompson, 1988). Many of
the useful features, syntax in particular, exploit
the fact that both arguments of the connective are
found in the same sentence. Such features would
not be applicable to the analysis of implicit rela-
tions that occur intersententially.
Wellner et al (2006) used the GraphBank (Wolf
and Gibson, 2005), which contains 105 Associated
Press and 30 Wall Street Journal articles annotated
with discourse relations. They achieve 81% accu-
racy in sense disambiguation on this corpus. How-
ever, GraphBank annotations do not differentiate
between implicits and explicits, so it is difficult to
verify success for implicit relations.
Experiments on artificial implicits Marcu and
Echihabi (2001) proposed a method for cheap ac-
quisition of training data for discourse relation
sense prediction. Their idea is to use unambiguous
patterns such as [Arg1, but Arg2.] to create syn-
thetic examples of implicit relations. They delete
the connective and use [Arg1, Arg2] as an example
of an implicit relation.
The approach is tested using binary classifica-
tion between relations on balanced data, a setting
very different from that of any realistic applica-
tion. For example, a question-answering appli-
cation that needs to identify causal relations (i.e.
as in Girju (2003)), must not only differentiate
causal relations from comparison relations, but
also from expansions, temporal relations, and pos-
sibly no relation at all. In addition, using equal
numbers of examples of each type can be mislead-
ing because the distribution of relations is known
to be skewed, with expansions occurring most fre-
quently. Causal and comparison relations, which
are most useful for applications, are less frequent.
Because of this, the recall of the classification
should be the primary metric of success, while
the Marcu and Echihabi (2001) experiments report
only accuracy.
Later work (Blair-Goldensohn et al, 2007;
Sporleder and Lascarides, 2008) has discovered
that the models learned do not perform as well on
implicit relations as one might expect from the test
accuracies on synthetic data.
3 Penn Discourse Treebank
For our experiments, we use the Penn Discourse
Treebank (PDTB; Prasad et al, 2008), the largest
available annotated corpora of discourse relations.
The PDTB contains discourse annotations over the
same 2,312 Wall Street Journal (WSJ) articles as
the Penn Treebank.
For each explicit discourse connective (such as
?but? or ?so?), annotators identified the two text
spans between which the relation holds and the
sense of the relation.
The PDTB also provides information about lo-
cal implicit relations. For each pair of adjacent
sentences within the same paragraph, annotators
selected the explicit discourse connective which
best expressed the relation between the sentences
and then assigned a sense to the relation. In Exam-
ple (1) above, the annotators identified ?because?
as the most appropriate connective between the
sentences, and then labeled the implicit discourse
relation Contingency.
In the PDTB, explicit and implicit relations are
clearly distinguished, allowing us to concentrate
solely on the implicit relations.
As mentioned above, each implicit and explicit
relation is annotated with a sense. The senses
are arranged in a hierarchy, allowing for annota-
tions as specific as Contingency.Cause.reason. In
our experiments, we use only the top level of the
sense annotations: Comparison, Contingency, Ex-
pansion, and Temporal. Using just these four rela-
tions allows us to be theory-neutral; while differ-
ent frameworks (Hobbs, 1979; McKeown, 1985;
Mann and Thompson, 1988; Knott and Sanders,
1998; Asher and Lascarides, 2003) include differ-
ent relations of varying specificities, all of them
include these four core relations, sometimes under
different names.
Each relation in the PDTB takes two arguments.
Example (1) can be seen as the predicate Con-
tingency which takes the two sentences as argu-
ments. For implicits, the span in the first sentence
is called Arg1 and the span in the following sen-
tence is called Arg2.
4 Word pair features in prior work
Cross product of words Discourse connectives
are the most reliable predictors of the semantic
sense of the relation (Marcu, 2000; Pitler et al,
2008). However, in the absence of explicit mark-
ers, the most easily accessible features are the
684
words in the two text spans of the relation. In-
tuitively, one would expect that there is some rela-
tionship that holds between the words in the two
arguments. Consider for example the following
sentences:
The recent explosion of country funds mirrors the ?closed-
end fund mania? of the 1920s, Mr. Foot says, when narrowly
focused funds grew wildly popular. They fell into oblivion
after the 1929 crash.
The words ?popular? and ?oblivion? are almost
antonyms, and one might hypothesize that their
occurrence in the two text spans is what triggers
the contrast relation between the sentences. Sim-
ilarly, a pair of words such as (rain, rot) might be
indicative of a causal relation. If this hypothesis is
correct, pairs of words (w1, w2) such that w1 ap-
pears in the first sentence and w2 appears in the
second sentence would be good features for iden-
tifying contrast relations.
Indeed, word pairs form the basic feature
of most previous work on classifying implicit
relations (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007; Sporleder and Las-
carides, 2008) or the simpler task of predicting
which connective should be used to express a rela-
tion (Lapata and Lascarides, 2004).
Semantic relations vs. function word pairs If
the hypothesis for word pair triggers of discourse
relations were true, the analysis of unambiguous
relations can be used to discover pairs of words
with causal or contrastive relations holding be-
tween them. Yet, feature analysis has not been per-
formed in prior studies to establish or refute this
possibility.
At the same time, feature selection is always
necessary for word pairs, which are numerous and
lead to data sparsity problems. Here, we present a
meta analysis of the feature selection work in three
prior studies.
One approach for reducing the number of fea-
tures follows the hypothesis of semantic rela-
tions between words. Marcu and Echihabi (2001)
considered only nouns, verbs and and other cue
phrases in word pairs. They found that even
with millions of training examples, prediction re-
sults using all words were superior to those based
on only pairs of non-function words. However,
since the learning curve is steeper when function
words were removed, they hypothesize that using
only non-function words will outperform using all
words once enough training data is available.
In a similar vein, Lapata and Lascarides (2004)
used pairings of only verbs, nouns and adjectives
for predicting which temporal connective is most
suitable to express the relation between two given
text spans. Verb pairs turned out to be one of the
best features, but no useful information was ob-
tained using nouns and adjectives.
Blair-Goldensohn et al (2007) proposed sev-
eral refinements of the word pair model. They
show that (i) stemming, (ii) using a small fixed
vocabulary size consisting of only the most fre-
quent stems (which would tend to be dominated
by function words) and (iii) a cutoff on the mini-
mum frequency of a feature, all result in improved
performance. They also report that filtering stop-
words has a negative impact on the results.
Given these findings, we expect that pairs of
function words are informative features helpful in
predicting discourse relation sense. In our work
that we describe next, we use feature selection to
investigate the word pairs in detail.
5 Analysis of word pair features
For the analysis of word pair features, we use
a large collection of automatically extracted ex-
plicit examples from the experiments in Blair-
Goldensohn et al (2007). The data, from now on
referred to as TextRels, has explicit contrast and
causal relations which were extracted from the En-
glish Gigaword Corpus (Graff, 2003) which con-
tains over four million newswire articles.
The explicit cue phrase is removed from each
example and the spans are treated as belonging to
an implicit relation. Besides cause and contrast,
the TextRels data include a no-relation category
which consists of sentences from the same text that
are separated by at least three other sentences.
To identify features useful for classifying com-
parison vs other relations, we chose a random sam-
ple of 5000 examples for Contrast and 5000 Other
relations (2500 each of Cause and No-relation).
For the complete set of 10,000 examples, word
pair features were computed. After removing
word pairs that appear less than 5 times, the re-
maining features were ranked by information gain
using the MALLET toolkit1.
Table 1 lists the word pairs with highest infor-
mation gain for the Contrast vs. Other and Cause
vs. Other classification tasks. All contain very fre-
quent stop words, and interestingly for the Con-
1mallet.cs.umass.edu
685
trast vs. Other task, most of the word pairs contain
discourse connectives.
This is certainly unexpected, given that word
pairs were formed by deleting the discourse con-
nectives from the sentences expressing Contrast.
Word pairs containing ?but? as one of their ele-
ments in fact signal the presence of a relation that
is not Contrast.
Consider the example shown below:
The government says it has reached most isolated townships
by now, but because roads are blocked, getting anything but
basic food supplies to people remains difficult.
Following Marcu and Echihabi (2001), the pair
[The government says it has reached most isolated
townships by now, but] and [roads are blocked,
getting anything but basic food supplies to peo-
ple remains difficult.] is created as an example of
the Cause relation. Because of examples like this,
?but-but? is a very useful word pair feature indi-
cating Cause, as the but would have been removed
for the artifical Contrast examples. In fact, the top
17 features for classifying Contrast versus Other
all contain the word ?but?, and are indications that
the relation is Other.
These findings indicate an unexpected anoma-
lous effect in the use of synthetic data. Since re-
lations are created by removing connectives, if an
unambiguous connective remains, its presence is a
reliable indicator that the example should be clas-
sified as Other. Such features might work well and
lead to high accuracy results in identifying syn-
thetic implicit relations, but are unlikely to be use-
ful in a realistic setting of actual implicits.
Comparison vs. Other Contingency vs. Other
the-but s-but the-in the-and in-the the-of
of-but for-but but-but said-said to-of the-a
in-but was-but it-but a-and a-the of-the
to-but that-but the-it* to-and to-to the-in
and-but but-the to-it* and-and the-the in-in
a-but he-but said-in to-the of-and a-of
said-but they-but of-in in-and in-of s-and
Table 1: Word pairs with highest information gain.
Also note that the only two features predic-
tive of the comparison class (indicated by * in
Table 1): the-it and to-it, contain only func-
tion words rather than semantically related non-
function words. This ranking explains the obser-
vations reported in Blair-Goldensohn et al (2007)
where removing stopwords degraded classifier
performance and why using only nouns, verbs or
adjectives (Marcu and Echihabi, 2001; Lapata and
Lascarides, 2004) is not the best option2.
6 Features for sense prediction of
implicit discourse relations
The contrast between the ?popular?/?oblivion? ex-
ample we started with above can be analyzed in
terms of lexical relations (near antonyms), but also
could be explained by different polarities of the
two words: ?popular? is generally a positive word,
while ?oblivion? has negative connotations.
While we agree that the actual words in the ar-
guments are quite useful, we also define several
higher-level features corresponding to various se-
mantic properties of the words. The words in the
two text spans of a relation are taken from the
gold-standard annotations in the PDTB.
Polarity Tags: We define features that represent
the sentiment of the words in the two spans. Each
word?s polarity was assigned according to its en-
try in the Multi-perspective Question Answering
Opinion Corpus (Wilson et al, 2005). In this re-
source, each sentiment word is annotated as posi-
tive, negative, both, or neutral. We use the number
of negated and non-negated positive, negative, and
neutral sentiment words in the two text spans as
features. If a writer refers to something as ?nice?
in Arg1, that counts towards the positive sentiment
count (Arg1Positive); ?not nice? would count to-
wards Arg1NegatePositive. A sentiment word is
negated if a word with a General Inquirer (Stone
et al, 1966) Negate tag precedes it. We also have
features for the cross products of these polarities
between Arg1 and Arg2.
We expected that these features could help
Comparison examples especially. Consider the
following example:
Executives at Time Inc. Magazine Co., a subsidiary of
Time Warner, have said the joint venture with Mr. Lang
wasn?t a good one. The venture, formed in 1986, was sup-
posed to be Time?s low-cost, safe entry into women?s maga-
zines.
The word good is annotated with positive po-
larity, however it is negated. Safe is tagged as
having positive polarity, so this opposition could
indicate the Comparison relation between the two
sentences.
Inquirer Tags: To get at the meanings of the
spans, we look up what semantic categories each
2In addition, an informal inspection of 100 word pairs
with high information gain for Contrast vs. Other (the longest
word pairs were chosen, as those are more likely to be content
words) found only six semantically opposed pairs.
686
word falls into according to the General Inquirer
lexicon (Stone et al, 1966). The General In-
quirer has classes for positive and negative polar-
ity, as well as more fine-grained categories such as
words related to virtue or vice. The Inquirer even
contains a category called ?Comp? that includes
words that tend to indicate Comparison, such as
?optimal?, ?other?, ?supreme?, or ?ultimate?.
Several of the categories are complementary:
Understatement versus Overstatement, Rise ver-
sus Fall, or Pleasure versus Pain. Pairs where one
argument contains words that indicate Rise and the
other argument indicates Fall might be good evi-
dence for a Comparison relation.
The benefit of using these tags instead of just
the word pairs is that we see more observations for
each semantic class than for any particular word,
reducing the data sparsity problem. For example,
the pair rose:fell often indicates a Comparison re-
lation when speaking about stocks. However, oc-
casionally authors refer to stock prices as ?jump-
ing? rather than ?rising?. Since both jump and rise
are members of the Rise class, new jump examples
can be classified using past rise examples.
Development testing showed that including fea-
tures for all words? tags was not useful, so we in-
clude the Inquirer tags of only the verbs in the two
arguments and their cross-product. Just as for the
polarity features, we include features for both each
tag and its negation.
Money/Percent/Num: If two adjacent sen-
tences both contain numbers, dollar amounts, or
percentages, it is likely that a comparison rela-
tion might hold between the sentences. We in-
cluded a feature for the count of numbers, percent-
ages, and dollar amounts in Arg1 and Arg2. We
also included the number of times each combina-
tion of number/percent/dollar occurs in Arg1 and
Arg2. For example, if Arg1 mentions a percent-
age and Arg2 has two dollar amounts, the feature
Arg1Percent-Arg2Money would have a count of 2.
This feature is probably genre-dependent. Num-
bers and percentages often appear in financial texts
but would be less frequent in other genres.
WSJ-LM: This feature represents the extent to
which the words in the text spans are typical of
each relation. For each sense, we created uni-
gram and bigram language models over the im-
plicit examples in the training set. We compute
each example?s probability according to each of
these language models. The features are the ranks
of the spans? likelihoods according to the vari-
ous language models. For example, if of the un-
igram models, the most likely relation to generate
this example was Contingency, then the example
would include the feature ContingencyUnigram1.
If the third most likely relation according to the
bigram models was Expansion, then it would in-
clude the feature ExpansionBigram3.
Expl-LM: This feature ranks the text spans ac-
cording to language models derived from the ex-
plicit examples in the TextRels corpus. However,
the corpus contains only Cause, Contrast and No-
relation, hence we expect the WSJ language mod-
els to be more helpful.
Verbs: These features include the number of
pairs of verbs in Arg1 and Arg2 from the same
verb class. Two verbs are from the same verb class
if each of their highest Levin verb class (Levin,
1993) levels (in the LCS Database (Dorr, 2001))
are the same. The intuition behind this feature is
that the more related the verbs, the more likely the
relation is an Expansion.
The verb features also include the average
length of verb phrases in each argument, as well
as the cross product of this feature for the two ar-
guments. We hypothesized that verb chunks that
contain more words, such as ?They [are allowed to
proceed]? often contain rationales afterwards (sig-
nifying Contingency relations), while short verb
phrases like ?They proceed? might occur more of-
ten in Expansion or Temporal relations.
Our final verb features were the part of speech
tags (gold-standard from the Penn Treebank) of
the main verb. One would expect that Expansion
would link sentences with the same tense, whereas
Contingency and Temporal relations would con-
tain verbs with different tenses.
First-Last, First3: The first and last words of
a relation?s arguments have been found to be par-
ticularly useful for predicting its sense (Wellner et
al., 2006). Wellner et al (2006) suggest that these
words are such predictive features because they
are often explicit discourse connectives. In our
experiments on implicits, the first and last words
are not connectives. However, some implicits have
been found to be related by connective-like ex-
pressions which often appear in the beginning of
the second argument. In the PDTB, these are an-
notated as alternatively lexicalized relations (Al-
tLexes). To capture such effects, we included the
first and last words of Arg1 as features, the first
687
and last words of Arg2, the pair of the first words
of Arg1 and Arg2, and the pair of the last words.
We also add two additional features which indicate
the first three words of each argument.
Modality: Modal words, such as ?can?,
?should?, and ?may?, are often used to express
conditional statements (i.e. ?If I were a wealthy
man, I wouldn?t have to work hard.?) thus signal-
ing a Contingency relation. We include a feature
for the presence or absence of modals in Arg1 and
Arg2, features for specific modal words, and their
cross-products.
Context: Some implicit relations appear imme-
diately before or immediately after certain explicit
relations far more often than one would expect due
to chance (Pitler et al, 2008). We define a feature
indicating if the immediately preceding (or follow-
ing) relation was an explicit. If it was, we include
the connective trigger of the relation and its sense
as features. We use oracle annotations of the con-
nective sense, however, most of the connectives
are unambiguous.
One might expect a different distribution of re-
lation types in the beginning versus further in the
middle of a paragraph. We capture paragraph-
position information using a feature which indi-
cates if Arg1 begins a paragraph.
Word pairs Four variants of word pair mod-
els were used in our experiments. All the models
were eventually tested on implicit examples from
the PDTB, but the training set-up was varied.
Wordpairs-TextRels In this setting, we trained
a model on word pairs derived from unannotated
text (TextRels corpus).
Wordpairs-PDTBImpl Word pairs for training
were formed from the cross product of words in
the textual spans (Arg1 x Arg2) of the PDTB im-
plicit relations.
Wordpairs-selected Here, only word pairs from
Wordpairs-PDTBImpl with non-zero information
gain on the TextRels corpus were retained.
Wordpairs-PDTBExpl In this case, the model
was formed by using the word pairs from the ex-
plicit relations in the sections of the PDTB used
for training.
7 Classification Results
For all experiments, we used sections 2-20 of the
PDTB for training and sections 21-22 for testing.
Sections 0-1 were used as a development set for
feature design.
We ran four binary classification tasks to iden-
tify each of the main relations from the rest. As
each of the relations besides Expansion are infre-
quent, we train using equal numbers of positive
and negative examples of the target relation. The
negative examples were chosen at random. We
used all of sections 21 and 22 for testing, so the
test set is representative of the natural distribution.
The training sets contained: Comparison (1927
positive, 1927 negative), Contingency (3500
each), Expansion3 (6356 each), and Temporal
(730 each).
The test set contained: 151 examples of Com-
parison, 291 examples of Contingency, 986 exam-
ples of Expansion, 82 examples of Temporal, and
13 examples of No-relation.
We used Naive Bayes, Maximum Entropy
(MaxEnt), and AdaBoost (Freund and Schapire,
1996) classifiers implemented in MALLET.
7.1 Non-Wordpair Features
The performance using only our semantically in-
formed features is shown in Table 7. Only the
Naive Bayes classification results are given, as
space is limited and MaxEnt and AdaBoost gave
slightly lower accuracies overall.
The table lists the f-score for each of the target
relations, with overall accuracy shown in brack-
ets. Given that the experiments are run on natural
distribution of the data, which are skewed towards
Expansion relations, the f-score is the more impor-
tant measure to track.
Our random baseline is the f-score one would
achieve by randomly assigning classes in propor-
tion to its true distribution in the test set. The best
results for all four tasks are considerably higher
than random prediction, but still low overall. Our
features provide 6% to 18% absolute improve-
ments in f-score over the baseline for each of the
four tasks. The largest gain was in the Contin-
gency versus Other prediction task. The least im-
provement was for distinguishing Expansion ver-
sus Other. However, since Expansion forms the
largest class of relations, its f-score is still the
highest overall. We discuss the results per relation
class next.
Comparison We expected that polarity features
would be especially helpful for identifying Com-
3The PDTB also contains annotations of entity relations,
which most frameworks consider a subset of Expansion.
Thus, we include relations annotated as EntRel as positive
examples of Expansion.
688
Features Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Four-way
Money/Percent/Num 19.04 (43.60) 18.78 (56.27) 22.01 (41.37) 10.40 (23.05) (63.38)
Polarity Tags 16.63 (55.22) 19.82 (76.63) 71.29 (59.23) 11.12 (18.12) (65.19)
WSJ-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Expl-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Verbs 18.55 (26.19) 36.59 (62.44) 59.36 (52.53) 12.61 (41.63) (65.33)
First-Last, First3 21.01 (52.59) 36.75 (59.09) 63.22 (56.99) 15.93 (61.20) (65.40)
Inquirer tags 17.37 (43.8) 15.76 (77.54) 70.21 (58.04) 11.56 (37.69) (62.21)
Modality 17.70 (17.6) 21.83 (76.95) 15.38 (37.89) 11.17 (27.91) (65.33)
Context 19.32 (56.66) 29.55 (67.42) 67.77 (57.85) 12.34 (55.22) (64.01)
Random 9.91 19.11 64.74 5.38
Table 2: f-score (accuracy) using different features; Naive Bayes.
parison relations. Surprisingly, polarity was actu-
ally one of the worst classes of features for Com-
parison, achieving an f-score of 16.33 (in contrast
to using the first, last and first three words of the
sentences as features, which leads to an f-score of
21.01). We examined the prevalence of positive-
negative or negative-positive polarity pairs in our
training set. 30% of the Comparison examples
contain one of these opposite polarity pairs, while
31% of the Other examples contain an opposite
polarity pair. To our knowledge, this is the first
study to examine the prevalence of polarity words
in the arguments of discourse relations in their
natural distributions. Contrary to popular belief,
Comparisons do not tend to have more opposite
polarity pairs.
The two most useful classes of features for rec-
ognizing Comparison relations were the first, last
and first three words in the sentence and the con-
text features that indicate the presence of a para-
graph boundary or of an explicit relation just be-
fore or just after the location of the hypothesized
implicit relation (19.32 f-score).
Contingency The two best features for the Con-
tingency vs. Other distinction were verb informa-
tion (36.59 f-score) and first, last and first three
words in the sentence (36.75 f-score). Context
again was one of the features that led to improve-
ment. This makes sense, as Pitler et al (2008)
found that implicit contingencies are often found
immediately following explicit comparisons.
We were surprised that the polarity features
were helpful for Contingency but not Comparison.
Again we looked at the prevalence of opposite po-
larity pairs. While for Comparison versus Other
there was not a significant difference, for Contin-
gency there are quite a few more opposite polarity
pairs (52%) than for not Contingency (41%).
The language model features were completely
useless for distinguishing contingencies from
other relations.
Expansion As Expansion is the majority class
in the natural distribution, recall is less of a prob-
lem than precision. The features that help achieve
the best f-score are all features that were found to
be useful in identifying other relations.
Polarity tags, Inquirer tags and context were
the best features for identifying expansions with
f-scores around 70%.
Temporal Implicit temporal relations are rela-
tively rare, making up only about 5% of our test
set. Most temporal relations are explicitly marked
with a connective like ?when? or ?after?.
Yet again, the first and last words of the sen-
tence turned out to be useful indicators for tem-
poral relations (15.93 f-score). The importance of
the first and last words for this distinction is clear.
It derives from the fact that temporal implicits of-
ten contain words like ?yesterday? or ?Monday? at
the end of the sentence. Context is the next most
helpful feature for temporal relations.
7.2 Which word pairs help?
For Comparison and Contingency, we analyze the
behavior of word pair features under several differ-
ent settings. Specifically we want to address two
important related questions raised in recent work
by others: (i) is unannotated data from explicits
useful for training models that disambiguate im-
plicit discourse relations and (ii) are explicit and
implicit relations intrinsically different from each
other.
Wordpairs-TextRels is the worst approach. The
best use of word pair features is Wordpairs-
selected. This model gives 4% better absolute f-
score for Comparison and 14% for Contingency
over Wordpairs-TextRels. In this setting the Tex-
tRels data was used to choose the word pair fea-
tures, but the probabilities for each feature were
estimated using the training portion of the PDTB
689
Comp. vs. Other
Wordpairs-TextRels 17.13 (46.62)
Wordpairs-PDTBExpl 19.39 (51.41)
Wordpairs-PDTBImpl 20.96 (42.55)
First-last, first3 (best-non-wp) 21.01 (52.59)
Best-non-wp + Wordpairs-selected 21.88 (56.40)
Wordpairs-selected 21.96 (56.59)
Cont. vs. Other
Wordpairs-TextRels 31.10 (41.83)
Wordpairs-PDTBExpl 37.77 (56.73)
Wordpairs-PDTBImpl 43.79 (61.92)
Polarity, verbs, first-last, first3,
modality, context (best-non-wp)
42.14 (66.64)
Wordpairs-selected 45.60 (67.10)
Best-non-wp + Wordpairs-selected 47.13 (67.30)
Expn. vs. Other
Best-non-wp + wordpairs 62.39 (59.55)
Wordpairs-PDTBImpl 63.84 (60.28)
Polarity, inquirer tags, context (best-
non-wp)
76.42 (63.62)
Temp. vs. Other
First-last, first3 (best-non-wp) 15.93 (61.20)
Wordpairs-PDTBImpl 16.21 (61.98)
Best-non-wp + Wordpairs-PDTBImpl 16.76 (63.49)
Table 3: f-score (accuracy) of various feature sets;
Naive Bayes.
implicit examples.
We also confirm that even within the PDTB,
information from annotated explicit relations
(Wordpairs-PDTBExpl) is not as helpful as
information from annotated implicit relations
(Wordpairs-PDTBImpl). The absolute difference
in f-score between the two models is close to 2%
for Comparison, and 6% for Contingency.
7.3 Best results
Adding other features to word pairs leads to im-
proved performance for Contingency, Expansion
and Temporal relations, but not for Comparison.
For contingency detection, the best combina-
tion of our features included polarity, verb in-
formation, first and last words, modality, context
with Wordpairs-selected. This combination led
to a definite improvement, reaching an f-score of
47.13 (16% absolute improvement in f-score over
Wordpairs-TextRels).
For detecting expansions, the best combination
of our features (polarity+Inquirer tags+context)
outperformed Wordpairs-PDTBImpl by a wide
margin, close to 13% absolute improvement (f-
scores of 76.42 and 63.84 respectively).
7.4 Sequence Model of Discourse Relations
Our results from the previous section show that
classification of implicits benefits from informa-
tion about nearby relations, and so we expected
improvements using a sequence model, rather than
classifying each relation independently.
We trained a CRF classifier (Lafferty et al,
2001) over the sequence of implicit examples from
all documents in sections 02 to 20. The test set
is the same as used for the 2-way classifiers. We
compare against a 6-way4 Naive Bayes classifier.
Only word pairs were used as features for both.
Overall 6-way prediction accuracy is 43.27% for
the Naive Bayes model and 44.58% for the CRF
model.
8 Conclusion
We have presented the first study that predicts im-
plicit discourse relations in a realistic setting (dis-
tinguishing a relation of interest from all others,
where the relations occur in their natural distri-
butions). Also unlike prior work, we separate the
task from the easier task of explicit discourse pre-
diction. Our experiments demonstrate that fea-
tures developed to capture word polarity, verb
classes and orientation, as well as some lexical
features are strong indicators of the type of dis-
course relation.
We analyze word pair features used in prior
work that were intended to capture such semantic
oppositions. We show that the features in fact do
not capture semantic relation but rather give infor-
mation about function word co-occurrences. How-
ever, they are still a useful source of information
for discourse relation prediction. The most bene-
ficial application of such features is when they are
selected from a large unannotated corpus of ex-
plicit relations, but then trained on manually an-
notated implicit relations.
Context, in terms of paragraph boundaries and
nearby explicit relations, also proves to be useful
for the prediction of implicit discourse relations.
It is helpful when added as a feature in a standard,
instance-by-instance learning model. A sequence
model also leads to over 1% absolute improvement
for the task.
9 Acknowledgments
This work was partially supported by NSF grants
IIS-0803159, IIS-0705671 and IGERT 0504487.
We would like to thank Sasha Blair-Goldensohn
for providing us with the TextRels data and for
the insightful discussion in the early stages of our
work.
4the four main relations, EntRel, NoRel
690
References
N. Asher and A. Lascarides. 2003. Logics of conver-
sation. Cambridge University Press.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and Refining Rhetorical-
Semantic Relation Models. In Proceedings of
NAACL HLT, pages 428?435.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of the Second SIGdial Workshop on Discourse and
Dialogue, pages 1?10.
B.J. Dorr. 2001. LCS Verb Database. Technical Re-
port Online Software Database, University of Mary-
land, College Park, MD.
Y. Freund and R.E. Schapire. 1996. Experiments with
a New Boosting Algorithm. In Machine Learning:
Proceedings of the Thirteenth International Confer-
ence, pages 148?156.
R. Girju. 2003. Automatic detection of causal relations
for Question Answering. In Proceedings of the ACL
2003 workshop on Multilingual summarization and
question answering-Volume 12, pages 76?83.
D. Graff. 2003. English gigaword corpus. Corpus
number LDC2003T05, Linguistic Data Consortium,
Philadelphia.
J. Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
A. Knott and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135?175.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Interna-
tional Conference on Machine Learning 2001, pages
282?289.
M. Lapata and A. Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT-
NAACL 2004: Main Proceedings.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. Chicago, IL.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu and A. Echihabi. 2001. An unsupervised
approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 368?375.
D. Marcu. 2000. The Theory and Practice of Dis-
course and Summarization. The MIT Press.
K. McKeown. 1985. Text Generation: Using Dis-
course strategies and Focus Constraints to Gener-
ate Natural Language Text. Cambridge University
Press, Cambridge, England.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING08), short paper.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347?354.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249?288.
691
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1157?1168, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A coherence model based on syntactic patterns
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Abstract
We introduce a model of coherence which
captures the intentional discourse structure in
text. Our work is based on the hypothesis that
syntax provides a proxy for the communica-
tive goal of a sentence and therefore the se-
quence of sentences in a coherent discourse
should exhibit detectable structural patterns.
Results show that our method has high dis-
criminating power for separating out coherent
and incoherent news articles reaching accura-
cies of up to 90%. We also show that our syn-
tactic patterns are correlated with manual an-
notations of intentional structure for academic
conference articles and can successfully pre-
dict the coherence of abstract, introduction
and related work sections of these articles.
1 Introduction
Recent studies have introduced successful automatic
methods to predict the structure and coherence of
texts. They include entity approaches for local co-
herence which track the repetition and syntactic re-
alization of entities in adjacent sentences (Barzilay
and Lapata, 2008; Elsner and Charniak, 2008) and
content approaches for global coherence which view
texts as a sequence of topics, each characterized by a
particular distribution of lexical items (Barzilay and
Lee, 2004; Fung and Ngai, 2006). Other work has
shown that co-occurrence of words (Lapata, 2003;
Soricut and Marcu, 2006) and discourse relations
(Pitler and Nenkova, 2008; Lin et al 2011) also pre-
dict coherence.
Early theories (Grosz and Sidner, 1986) posited
that there are three factors which collectively con-
tribute to coherence: intentional structure (purpose
of discourse), attentional structure (what items are
discussed) and the organization of discourse seg-
ments. The highly successful entity approaches cap-
ture attentional structure and content approaches are
related to topic segments but intentional structure
has largely been neglected. Every discourse has a
purpose: explaining a concept, narrating an event,
critiquing an idea and so on. As a result each sen-
tence in the article has a communicative goal and the
sequence of goals helps the author achieve the dis-
course purpose. In this work, we introduce a model
to capture coherence from the intentional structure
dimension. Our key proposal is that syntactic pat-
terns are a useful proxy for intentional structure.
This idea is motivated from the fact that cer-
tain sentence types such as questions and definitions
have distinguishable and unique syntactic structure.
For example, consider the opening sentences of two
descriptive articles1 shown in Table 1. Sentences
(1a) and (2a) are typical instances of definition sen-
tences. Definitions are written with the concept to
be defined expressed as a noun phrase followed by
a copular verb (is/are). The predicate contains two
parts: the first is a noun phrase reporting the concept
as part of a larger class (eg. an aqueduct is a water
supply), the second component is a relative clause
listing unique properties of the concept. These are
examples of syntactic patterns related to the com-
municative goals of individual sentences. Similarly,
sentences (1b) and (2b) which provide further de-
tails about the concept also have some distinguish-
1Wikipedia articles on ?Aqueduct? and ?Cytokine Recep-
tors?
1157
1a) An aqueduct is a water supply or navigable channel
constructed to convey water.
b) In modern engineering, the term is used for any system
of pipes, canals, tunnels, and other structures used for
this purpose.
2a) Cytokine receptors are receptors that binds cytokines.
b) In recent years, the cytokine receptors have come to
demand more attention because their deficiency has now been
directly linked to certain debilitating immunodeficiency states.
Table 1: The first two sentences of two descriptive arti-
cles
ing syntactic features such as the presence of a top-
icalized phrase providing the focus of the sentence.
The two sets of sentences have similar sequence of
communicative goals and so we can expect the syn-
tax of adjacent sentences to also be related.
We aim to characterize this relationship on a
broad scale using a coherence model based entirely
on syntax. The model relies on two assumptions
which summarize our intuitions about syntax and in-
tentional structure:
1. Sentences with similar syntax are likely to have
the same communicative goal.
2. Regularities in intentional structure will be
manifested in syntactic regularities between ad-
jacent sentences.
There is also evidence from recent work that sup-
ports these assumptions. Cheung and Penn (2010)
find that a better syntactic parse of a sentence can be
derived when the syntax of adjacent sentences is also
taken into account. Lin et al(2009) report that the
syntactic productions in adjacent sentences are pow-
erful features for predicting which discourse relation
(cause, contrast, etc.) holds between them. Cocco et
al. (2011) show that significant associations exist be-
tween certain part of speech tags and sentence types
such as explanation, dialog and argumentation.
In our model, syntax is represented either as parse
tree productions or a sequence of phrasal nodes aug-
mented with part of speech tags. Our best perform-
ing method uses a Hidden Markov Model to learn
the patterns in these syntactic items. Sections 3 and
5 discuss the representations and their specific im-
plementations and relative advantages. Results show
that syntax models can distinguish coherent and in-
coherent news articles from two domains with 75-
90% accuracies over a 50% baseline. In addition,
the syntax coherence scores turn out complementary
to scores given by lexical and entity models.
We also study our models? predictions on aca-
demic articles, a genre where intentional structure
is widely studied. Sections in these articles have
well-defined purposes and we find recurring sen-
tence types such as motivation, citations, descrip-
tion, and speculations. There is a large body of work
(Swales, 1990; Teufel et al 1999; Liakata et al
2010) concerned with defining and annotating these
sentence types (called zones) in conference articles.
In Section 6, we describe how indeed some patterns
captured by the syntax-based models are correlated
with zone categories that were proposed in prior lit-
erature. We also present results on coherence pre-
diction: our model can distinguish the introduction
section of conference papers from its perturbed ver-
sions with over 70% accuracy. Further, our model
is able to identify conference from workshop papers
with good accuracies, given that we can expect these
articles to vary in purpose.
2 Evidence for syntactic coherence
We first present a pilot study that confirms that ad-
jacent sentences in discourse exhibit stable patterns
of syntactic co-occurrence. This study validates our
second assumption relating the syntax of adjacent
sentences. Later in Section 6, we examine syntac-
tic patterns in individual sentences (assumption 1)
using a corpus of academic articles where sentences
were manually annotated with communicative goals.
Prior work has reported that certain grammatical
productions are repeated in adjacent sentences more
often than would be expected by chance (Reitter et
al., 2006; Cheung and Penn, 2010). We analyze all
co-occurrence patterns rather than just repetitions.
We use the gold standard parse trees from the
Penn Treebank (Marcus et al 1994). Our unit of
analysis is a pair of adjacent sentences (S1, S2) and
we choose to use Section 0 of the corpus which has
99 documents and 1727 sentence pairs. We enumer-
ate all productions that appear in the syntactic parse
of any sentence and exclude those that appear less
than 25 times, resulting in a list of 197 unique pro-
ductions. Then all ordered pairs2 (p1, p2) of pro-
ductions are formed. For each pair, we compute
2(p1, p2) and (p2, p1) are considered as different pairs.
1158
p1, p2 Sentence 1 Sentence 2
NP? NP NP-ADV The two concerns said they entered into a definitive Also on the takeover front, Jaguar?s ADRs rose
QP? CD CD merger agreement under which Ratners will begin a tender 1/4 to 13 7/8 on turnover of [4.4 million]QP.
offer for all of Weisfield?s common shares for [$57.50 each]NP.
VP? VB VP ?The refund pool may not [be held hostage through another? [Commonwealth Edison]NP-SBJ said it is already
NP-SBJ? NNP NNP round of appeals]VP,? Judge Curry said. appealing the underlying commission order and
is considering appealing Judge Curry?s order.
NP-LOC? NNP ?It has to be considered as an additional risk for the investor,? [?Cray Computer will be a concept?
S-TPC-1? NP-SBJ VP said Gary P. Smaby of Smaby Group Inc., [Minneapolis]NP-LOC. ?stock,?]S-TPC-1 he said.
Table 2: Example sentences for preferred production sequences. The span of the LHS of the corresponding production
is indicated by [] braces.
the following: c(p1p2) = number of sentence pairs
where p1 ? S1 and p2 ? S2; c(p1?p2) = num-
ber of pairs where p1 ? S1 and p2 6? S2; c(?p1p2)
and c(?p1?p2) are computed similarly. Then we
perform a chi-square test to understand if the ob-
served count c(p1p2) is significantly (95% confi-
dence level) greater or lesser than the expected value
if occurrences of p1 and p2 were independent.
Of the 38,809 production pairs, we found that
1,168 pairs occurred in consecutive sentences sig-
nificantly more often than chance and 172 appeared
significantly fewer times than expected. In Table 2
we list, grouped in three simple categories, the 25
pairs of the first kind with most significant p-values.
Some of the preferred pairs are indeed repetitions
as pointed out by prior work. But they form only a
small fraction (5%) of the total preferred production
pairs indicating that there are several other classes
of syntactic regularities beyond priming. Some of
these other sequences can be explained by the fact
that these articles come from the finance domain:
they involve productions containing numbers and
quantities. An example for this type is shown in Ta-
ble 2. Finally, there is also a class that is not repe-
titions or readily observed as domain-specific. The
most frequent one reflects a pattern where the first
sentence introduces a subject and predicate and the
subject in the second sentence is pronominalized.
Examples for two other patterns are given in Table
2. For the sequence (VP ? VB VP | NP-SBJ ? NNP
NNP), a bare verb is present in S1 and is often asso-
ciated with modals. In the corpus, these statements
often present hypothesis or speculation. The follow-
ing sentence S2 has an entity, a person or organiza-
tion, giving an explanation or opinion on the state-
ment. This pattern roughly correponds to a SPECU-
LATE followed by ENDORSE sequence of intentions.
p1 p2 c(p1p2)
? Repetition ?
VP? VBD SBAR VP? VBD SBAR 83
QP? $ CD CD QP? $ CD CD 18
NP? $ CD -NONE- NP? $ CD -NONE- 16
NP? QP -NONE- NP? QP -NONE- 15
NP-ADV? DT NN NP-ADV? DT NN 10
NP? NP NP-ADV NP? NP NP-ADV 7
? Quantities/Amounts ?
NP? QP -NONE- QP? $ CD CD 16
QP? $ CD CD NP? QP -NONE- 15
NP? NP NP-ADV NP? QP -NONE- 11
NP-ADV? DT NN NP? QP -NONE- 11
NP? NP NP-ADV NP-ADV? DT NN 9
NP? $ CD -NONE- NP-ADV? DT NN 8
NP-ADV? DT NN NP? $ CD -NONE- 8
NP-ADV? DT NN NP? NP NP-ADV 8
NP? NP NP-ADV QP? CD CD 6
? Other ?
S? NP-SBJ VP NP-SBJ? PRP 290
VP? VBD SBAR PP-TMP? IN NP 79
S? NP-SBJ-1 VP VP? VBD SBAR 43
VP? VBD NP VP? VBD VP 31
VP? VB VP NP-SBJ? NNP NNP 27
NP-SBJ-1? NNP NNP VP? VBD NP 13
VP? VBZ NP S? PP-TMP , NP-SBJ VP . 8
NP-SBJ? JJ NNS VP? VBP NP 8
NP-PRD? NP PP NP-PRD? NP SBAR 7
NP-LOC? NNP S-TPC-1? NP-SBJ VP 6
Table 3: Top patterns in productions from WSJ
Similarly, in all the six adjacent sentence pairs from
our corpus containing the items (NP-LOC ? NNP | S-
TPC-1 ? NP-SBJ VP), p1 introduces a location name,
and is often associated with the title of a person or
organization. The next sentence has a quote from
that person, where the quotation forms the topical-
ized clause in p2. Here the intentional structure is
INTRODUCE X / STATEMENT BY X.
In the remainder of the paper we formalize our
representation of syntax and the derived model of
coherence and test its efficacy in three domains.
3 Coherence models using syntax
We first describe the two representations of sentence
structure we adopted for our analysis.3 Next, we
3Our representations are similar to features used for rerank-
ing in parsing. Our first representation corresponds to ?rules?
features (Charniak and Johnson, 2005; Collins and Koo, 2005),
and our second representation is related to ?spines? (Carreras et
al., 2008) and edge annotation(Huang, 2008).
1159
present two coherence models: a local model which
captures the co-occurrence of structural features in
adjacent sentences and a global one which learns
from clusters of sentences with similar syntax.
3.1 Representing syntax
Our models rely exclusively on syntactic cues. We
derive representations from constituent parses of the
sentences, and terminals (words) are removed from
the parse tree before any processing is done. The
leaf nodes in our parse trees are part of speech tags.
Productions: In this representation we view each
sentence as the set of grammatical productions, LHS
? RHS, which appear in the parse of the sen-
tence. As we already pointed out, the right-hand side
(RHS) contains only non-terminal nodes. This rep-
resentation is straightforward, however, some pro-
ductions can be rather specific with long right hand
sides. Another apparent drawback of this represen-
tation is that it contains sequence information only
about nodes that belong to the same constituent.
d-sequence: In this representation we aim to pre-
serve more sequence information about adjacent
constituents in the sentence. The simplest approach
would be to represent the sentence as the sequence
of part of speech (POS) tags but then we lose all
the abstraction provided by higher level nodes in
tree. Instead, we introduce a more general represen-
tation, d-sequence where the level of abstraction can
be controlled using a parameter d. The parse tree is
truncated to depth at most d, and the leaves of the
resulting tree listed left to right form the d-sequence
representation. For example, in Figure 1, the line
depicts the cutoff at depth 2.
Next the representation is further augmented; all
phrasal nodes in the d-sequence are annotated (con-
catenated) with the left-most leaf that they domi-
nate in the full non-lexicalized parse tree. This is
shown as suffixes on the S, NP and VP nodes in
the figure. Such annotation conveys richer informa-
tion about the structure of the subtree below nodes
in the d-sequence. For example, ?the chairs?, ?his
chairs?, ?comfortable chairs? will be represented as
NPDT, NPPRP$ and NPJJ. In the resulting representa-
tions, sentences are viewed as sequences of syntactic
words (w1,w2...,wk), k ? p, where p is the length of
the full POS sequence and each wi is either POS tag
or a phrasal node+POS tag combination.
Figure 1: Example for d-sequence representation
In our example, at depth-2, the quotation sentence
gets the representation (w1=? , w2=SDT , w3=, , w4=? ,
w5=NPNNP , w6=VPVBD , w7=.) where the actual quote
is omitted. Sentences that contain attributions are
likely to appear more similar to each other when
compared using this representation in contrast to
representations derived from word or POS sequence.
The depth-3 sequence is also indicated in the figure.
The main verb of a sentence is central to its struc-
ture, so the parameter d is always set to be greater
than that of the main verb and is tuned to optimize
performance for coherence prediction.
3.2 Implementing the model
We adapt two models of coherence to operate over
the two syntactic representations.
3.2.1 Local co-occurrence model
This model is a direct extension from our pilot
study. It allows us to test the assumption that coher-
ent discourse is characterized by syntactic regulari-
ties in adjacent sentences. We estimate the proba-
bilities of pairs of syntactic items from adjacent sen-
tences in the training data and use these probabilities
to compute the coherence of new texts.
The coherence of a text T containing n sentences
(S1...Sn) is computed as:
P (T ) =
n?
i=2
|Si|?
j=1
1
|Si?1|
|Si?1|?
k=1
p(Sji |S
k
i?1)
where Syx indicates the yth item of Sx. Items
are either productions or syntactic word unigrams
depending on the representation. The conditional
probabilities are computed with smoothing:
1160
Cluster a Cluster b
ADJP ? JJ PP | VP ? VBZ ADJP VP ? VB VP | VP ? MD VP
[1] This method VP-[is ADJP-[capable of sequence-specific [1] Our results for the difference in reactivity VP-[can
detection of DNA with high accuracy]-ADJP]-VP . VP-[be linked to experimental observations]-VP]-VP .
[2] The same VP-[is ADJP-[true for synthetic polyamines [2] These phenomena taken together VP-[can VP-[be considered
such as polyallylamine]-ADJP]-VP . as the signature of the gelation process]-VP]-VP .
Table 4: Example syntactic similarity clusters. The top two descriptive productions for each cluster are also listed.
p(wj |wi) =
c(wi, wj) + ?C
c(wi) + ?C ? |V |
wherewi andwj are syntactic items and c(wi, wj) is
the number of sentences that contain the item wi im-
mediately followed by a sentence that contains wj .
|V | is the vocabulary size for syntactic items.
3.2.2 Global structure
Now we turn to a global coherence approach
that implements the assumption that sentences with
similar syntax have the same communicative goal
as well as captures the patterns in communicative
goals in the discourse. This approach uses a Hid-
den Markov Model (HMM) which has been a popu-
lar implementation for modeling coherence (Barzi-
lay and Lee, 2004; Fung and Ngai, 2006; Elsner
et al 2007). The hidden states in our model de-
pict communicative goals by encoding a probability
distribution over syntactic items. This distribution
gives higher weight to syntactic items that are more
likely for that communicative goal. Transitions be-
tween states record the common patterns in inten-
tional structure for the domain.
In this syntax-HMM, states hk are created by
clustering the sentences from the documents in the
training set by syntactic similarity. For the pro-
ductions representation of syntax, the features for
clustering are the number of times a given produc-
tion appeared in the parse of the sentence. For the
d-sequence approach, the features are n-grams of
size one to four of syntactic words from the se-
quence. Clustering was done by optimizing for av-
erage cosine similarity and was implemented using
the CLUTO toolkit (Zhao et al 2005). C clusters
are formed and taken as the states of the model. Ta-
ble 4 shows sentences from two clusters formed on
the abstracts of journal articles using the productions
representation. One of them, cluster (a), appears
to capture descriptive sentences and cluster (b) in-
volves mostly speculation type sentences.
The emission probabilities for each state are mod-
eled as a (syntactic) language model derived from
the sentences in it. For productions representa-
tion, this is the unigram distribution of produc-
tions from the sentences in hk. For d-sequences,
the distribution is computed for bigrams of syntac-
tic words. These language models use Lidstone
smoothing with constant ?E . The probability for a
sentence Sl to be generated from state hk, pE(Sl|hk)
is computed using these syntactic language models.
The transition probability pM from a state hi to
state hj is computed as:
pM (hj |hi) =
d(hi, hj) + ?M
d(hi) + ?M ? C
where d(hi) is the number of documents whose sen-
tences appear in hi and d(hi, hj) is the number of
documents which have a sentence in hi which is im-
mediately followed by a sentence in hj . In addi-
tion to the C states, we add one initial hS and one
final hF state to capture document beginning and
end. Transitions from hS to any state hk records
how likely it is for hk to be the starting state for doc-
uments of that domain. ?M is a smoothing constant.
The likelihood of a text with n sentences is given
by P (T ) =
?
h1...hn
?n
t=1 pM (ht|ht?1)pE(St|ht).
All model parameters?the number of clusters
C, smoothing constants ?C , ?E , ?M and d for
d-sequences?are tuned to optimize how well the
model can distinguish coherent from incoherent ar-
ticles. We describe these settings in Section 5.1.
4 Content and entity grid models
We compare the syntax model with content model
and entity grid methods. These approaches are the
most popular ones from prior work and also allow
us to test the complementary nature of syntax with
1161
lexical statistics and entity structure. This section
explains how we implemented these approaches.
Content models introduced by Barzilay and Lee
(2004) and Fung and Ngai (2006) use lexically
driven HMMs to capture coherence. The hidden
states represent the topics of the domain and en-
code a probability distribution over words. Transi-
tions between states record the probable succession
of topics. We built a content model using our HMM
implementation. Clusters are created using word bi-
gram features after replacing numbers and proper
names with tags NUM and PROP. The emissions are
given by a bigram language model on words from
the clustered sentences. Barzilay and Lee (2004)
also employ an iterative clustering procedure before
finalizing the states of the HMM but our method
only uses one-step clustering. Despite the differ-
ence, the content model accuracies for our imple-
mentation are quite close to that from the original.
For the entity grid model, we follow the gen-
erative approach proposed by Lapata and Barzilay
(2005). A text is converted into a matrix, where rows
correspond to sentences, in the order in which they
appear in the article. Columns are created one for
each entity appearing in the text. Each cell (i,j) is
filled with the grammatical role ri,j of the entity j
in sentence i. We computed the entity grids using
the Brown Coherence Toolkit4. The probability of
the text (T ) is defined using the likely sequence of
grammatical role transitions.
P (T ) =
m?
j=1
n?
i=1
p(ri,j |ri?1,j ...ri?h,j)
for m entities and n sentences. Parameter h controls
the history size for transitions and is tuned during
development. When h = 1, for example, only the
grammatical role for the entity in the previous sen-
tence is considered and earlier roles are ignored.
5 Evaluating syntactic coherence
We follow the common approach from prior work
and use pairs of articles, where one has the original
document order and the other is a random permuta-
tion of the sentences from the same document. Since
the original article is always more coherent than a
random permutation, a model can be evaluated using
4http://www.cs.brown.edu/~melsner/manual.html
the accuracy with which it can identify the original
article in the pair, i.e. it assigns higher probability
to the original article. This setting is not ideal but
has become the de facto standard for evaluation of
coherence models (Barzilay and Lee, 2004; Elsner
et al 2007; Barzilay and Lapata, 2008; Karamanis
et al 2009; Lin et al 2011; Elsner and Charniak,
2011). It is however based on a reasonable assump-
tion as recent work (Lin et al 2011) shows that peo-
ple identify the original article as more coherent than
its permutations with over 90% accuracy and asses-
sors also have high agreement. Later, we present
an experiment distinguishing conference from work-
shop articles as a more realistic evaluation.
We use two corpora that are widely employed for
coherence prediction (Barzilay and Lee, 2004; El-
sner et al 2007; Barzilay and Lapata, 2008; Lin et
al., 2011). One contains reports on airplane acci-
dents from the National Transportation Safety Board
and the other has reports about earthquakes from the
Associated Press. These articles are about 10 sen-
tences long. These corpora were chosen since within
each dataset, the articles have the same intentional
structure. Further, these corpora are also standard
ones used in prior work on lexical, entity and dis-
course relation based coherence models. Later in
Section 6, we show that the models perform well on
the academic genre and longer articles too.
For each of the two corpora, we have 100 arti-
cles for training and 100 (accidents) and 99 (earth-
quakes) for testing. A maximum of 20 random per-
mutations were generated for each test article to cre-
ate the pairwise data (total of 1986 test pairs for the
accident corpus and 1956 for earthquakes).5 The
baseline accuracy for random prediction is 50%.
The articles were parsed using the Stanford parser
(Klein and Manning, 2003).
5.1 Accuracy of the syntax model
For each model, the relevant parameters were tuned
using 10-fold cross validation on the training data.
In each fold, 90 documents were used for training
and evaluation was done on permutations from the
remaining articles. After tuning, the final model was
trained on all 100 articles in the training set.
5We downloaded the permutations from http://people.
csail.mit.edu/regina/coherence/CLsubmission/
1162
Table 5 shows the results on the test set. The
best number of clusters and depth for d-sequences
are also indicated. Overall, the syntax models work
quite well, with accuracies at least 15% or more ab-
solute improvement over the baseline.
In the local co-occurrence approach, both pro-
ductions and d-sequences provide 72% accuracy for
the accidents corpus. For the earthquake corpus,
the accuracies are lower and the d-sequence method
works better. The best depth setting for d-sequence
is rather small: depth of main verb (MVP) + 2 (or 1),
and indicates that a fairly abstract level of nodes is
preferred for the patterns. For comparison, we also
provide results using just the POS tags in the model
and this is worse than the d-sequence approach.
The global HMM model is better than the local
model for each representation type giving 2 to 38%
better accuracies. Here we see a different trend for
the d-sequence representation, with better results for
greater depths. At such depths (8 and 9) below the
main verb, the nodes are mostly POS tags.
Overall both productions and d-sequence work
competitively and give the best accuracies when im-
plemented with the global approach.
5.2 Comparison with other approaches
For our implementations of the content and entity
grid models, the best accuracies are 71% on the ac-
cidents corpus and 85% on the earthquakes one, sim-
ilar to the syntactic models.
Ideally, we would like to combine models but we
do not have separate training data. So we perform
the following classification experiment which com-
bines the predictions made by different models on
the test set. Each test pair (article and permutation)
forms one example and is given a class value of 0 or
1 depending on whether the first article in the pair
is the original one or the second one. The example
is represented as an n-dimensional vector, where n
is the number of models we wish to combine. For
instance, to combine content models and entity grid,
two features are created: one of these records the dif-
ference in log probabilities for the two articles from
the content model, the other feature indicates the dif-
ference in probabilities from the entity grid.
A logistic regression classifier is trained to pre-
dict the class using these features. The test pairs are
created such that an equal number of examples have
Model Accidents Earthquake
Parameter Acc Parameter Acc
A. Local co-occurrence
Prodns 72.8 55.0
d-seq dep. MVP+2 71.8 dep. MVP+1 65.1
POS 61.3 42.6
B. HMM-syntax
Prodns clus. 37 74.6 clus. 5 93.8
d-seq dep. MVP+8 82.2 dep. MVP+9 86.5
clus. 8 clus. 45
C. Other approaches
Egrid history 1 67.6 history 1 82.2
Content clus. 48 71.4 clus. 23 84.5
Table 5: Accuracies on accident and earthquake corpora
Model Accid. Earthq.
Content + Egrid 76.8 90.7
Content + HMM-prodn 74.2 95.3
Content + HMM-d-seq 82.1 90.3
Egrid + HMM-prodn 79.6 93.9
Egrid + HMM-d-seq 84.2 91.1
Egrid + Content + HMM-prodn 79.5 95.0
Egrid + Content + HMM-d-seq 84.1 92.3
Egrid + Content + HMM-prodn 83.6 95.7
+ HMM-d-seq
Table 6: Accuracies for combined approaches
class 0 and 1, so the baseline accuracy is 50%. We
run this experiment using 10-fold cross validation on
the test set after first obtaining the log probabilities
from individual models. In each fold, the training is
done using the pairs from 90 articles and tested on
permutations from the remaining 10 articles. These
accuracies are reported in Table 6. When the accu-
racy of a combination is better than that using any of
its smaller subsets, the value is bolded.
We find that syntax supplements both content and
entity grid methods. While on the airplane corpus
syntax only combines well with the entity grid, on
the earthquake corpus, both entity and content ap-
proaches give better accuracies when combined with
syntax. However, adding all three approaches does
not outperform combinations of any two of them.
This result can be due to the simple approach that
we tested for combination. In prior work, content
and entity grid methods have been combined gen-
eratively (Elsner et al 2007) and using discrimina-
tive training with different objectives (Soricut and
1163
Marcu, 2006). Such approaches might bring out
the complementary strengths of the different aspects
better and we leave such analysis for future work.
6 Predictions on academic articles
The distinctive intentional structure of academic ar-
ticles has motivated several proposals to define and
annotate the communicative purpose (argumentative
zone) of each sentence (Swales, 1990; Teufel et al
1999; Liakata et al 2010). Supervised classifiers
were also built to identify these zones (Teufel and
Moens, 2000; Guo et al 2011). So we expect that
these articles form a good testbed for our models. In
the remainder of the paper, we examine how unsu-
pervised patterns discovered by our approach relate
to zones and how well our models predict coherence
for articles from this genre.
We employ two corpora of scientific articles.
ART Corpus: contains a set of 225 Chemistry jour-
nal articles that were manually annotated for inten-
tional structure (Liakata and Soldatova, 2008). Each
sentence was assigned one of 11 zone labels: Result,
Conclusion, Objective, Method, Goal, Background,
Observation, Experiment, Motivation, Model, Hy-
pothesis. For our study, we use the annotation of
the introduction and the abstract sections. We divide
the data into training, development and test sets. For
abstracts, we have 75, 50 and 100 for these sets re-
spectively. For introductions, this split is 75, 31, 82.6
ACL Anthology Network (AAN) Corpus: Radev
et al(2009) provides the full text of publications
from ACL venues. These articles do not have any
zone annotations. The AAN corpus is produced
from OCR analysis and no section marking is avail-
able. To recreate these, we use the Parscit tagger7
(Councill et al 2008). We use articles from years
1999 to 2011. For training, we randomly choose 70
articles from ACL and NAACL main conferences.
Similarly, we obtain a development corpus of 36
ACL-NAACL articles. We create two test sets: one
has 500 ACL-NAACL conference articles and an-
other has 500 articles from ACL-sponsored work-
shops. We only choose articles in which all three
sections?abstract, introduction and related work?
6Some articles did not have labelled ?introduction? sections
resulting in fewer examples for this setup.
7http://aye.comp.nus.edu.sg/parsCit/
could be successfully identified using Parscit.8
This data was sentence-segmented using MxTer-
minator (Reynar and Ratnaparkhi, 1997) and parsed
with the Stanford Parser (Klein and Manning, 2003).
For each corpus and each section, we train all our
syntactic models: the two local coherence models
using the production and d-sequence representations
and the HMM models with the two representations.
These models are tuned on the respective develop-
ment data, on the task of differentiating the original
from a permuted section. For this purpose, we cre-
ated a maximum of 30 permutations per article.
6.1 Comparison with ART Corpus zones
We perform this analysis using the ART corpus. The
zone annotations present in this corpus allow us to
directly test our first assumption in this work, that
sentences with similar syntax have the same com-
municative goal.
For this analysis, we use the the HMM-prod
model for abstracts and the HMM-d-seq model for
introductions. These models were chosen because
they gave the best performance on the ART corpus
development sets.9 We examine the clusters cre-
ated by these models on the training data and check
whether there are clusters which strongly involve
sentences from some particular annotated zone.
For each possible pair of cluster and zone (Ci,
Zj), we compute c(Ci, Zj): the number of sentences
in Ci that are annotated as zone Zj . Then we use a
chi-square test to identify pairs for which c(Ci, Zj)
is significantly greater than expected (there is a ?pos-
itive? association between Ci and Zj) and pairs
where c(Ci, Zj) is significantly less than chance (Ci
is not associated with Zj). A 95% confidence level
was used to determine significance.
The HMM-prod model for abstracts has 9 clusters
(named Clus0 to 8) and the HMM-d-seq model for
introductions has 6 clusters (Clus0 to 5). The pair-
ings of these clusters with zones which turned out to
be significant are reported in Table 7. We also re-
port for each positively associated cluster-zone pair,
the following numbers: matches c(Ci, Zj), preci-
sion c(Ci, Zj)/|Ci| and recall c(Ci, Zj)/|Zj |.
8We also exclude introduction and related work sections
longer than 50 sentences and those shorter than 4 sentences
since they often have inaccurate section boundaries.
9Their test accuracies are reported in the next section.
1164
Abstracts (HMM-prod 9 clusters)
Positive associations matches prec. recall
Clus5 - Model 7 17.1 43.8
Clus7 - Objective 27 27.6 32.9
Clus7 - Goal 16 16.3 55.2
Clus0 - Conclusion 15 50.0 12.1
Clus6 - Conclusion 27 51.9 21.8
Not associated: Clus7 - Conclusion,
Clus8 - Conclusion
Introductions (HMM-d-seq 6 clusters)
Positive associations matches prec. recall
Clus2-Background 161 64.9 14.2
Clus3-Objective 37 7.9 38.5
Clus4-Goal 29 9.8 32.6
Clus4-Hypothesis 12 4.1 52.2
Clus5-Motivation 61 12.9 37.4
Not associated: Clus1 - Motivation, Clus2 - Goal,
Clus4 - Background, Clus 5 - Model
Table 7: Cluster-Zone mappings on the ART Corpus
The presence of significant associations validate
our intuitions that syntax provides clues about com-
municative goals. Some clusters overwhelmingly
contain the same zone, indicated by high precision,
for example 64% of sentences in Clus2 from intro-
duction sections are background sentences. Other
clusters have high recall of a zone, 55% of all goal
sentences from the abstracts training data is captured
by Clus7. It is particularly interesting to see that
Clus7 of abstracts captures both objective and goal
zone sentences and for introductions, Clus4 is a mix
of hypothesis and goal sentences which intuitively
are closely related categories.
6.2 Original versus permuted sections
We also explore the accuracy of the syntax models
for predicting coherence of articles from the test set
of ART corpus and the 500 test articles from ACL-
NAACL conferences. We use the same experimen-
tal setup as before and create pairs of original and
permuted versions of the test articles. We created a
maximum of 20 permutations for each article. The
baseline accuracy is 50% as before.
For the ART corpus, we also built an oracle model
of annotated zones. We train a first order Markov
Chain to record the sequence of zones in the training
articles. For testing, we assume that the oracle zone
is provided for each sentence and use the model to
predict the likelihood of the zone sequence. Results
from this model represent an upper bound because
an accurate hypothesis of the communicative goal is
available for each sentence.
The accuracies are presented in Table 8. Overall,
the HMM-d-seq model provides the best accuracies.
The highest results are obtained for ACL introduc-
tion sections (74%). These results are lower than
that obtained on the earthquake/accident corpus but
the task here is much harder: the articles are longer
and the ACL corpus also has OCR errors which af-
fect sentence segmentation and parsing accuracies.
When the oracle zones are known, the accuracies are
much higher on the ART corpus indicating that the
intentional structure of academic articles is very pre-
dictive of their coherence.
6.3 Conference versus workshop papers
Finally, we test whether the syntax-based model can
distinguish the structure of conference from work-
shop articles. Conferences publish more complete
and tested work and workshops often present pre-
liminary studies. Workshops are also venues to dis-
cuss a focused and specialized topic. So the way
information is conveyed in the abstracts and intro-
ductions would vary in these articles.
We perform this analysis on the ACL corpus and
no permutations are used, only the original text of
the 500 articles each in the conference and work-
shop test sets. While permutation examples provide
cheap training/test data, they have a few unrealistic
properties. For example, both original and permuted
articles have the same length. Further some permu-
tations could result in an outstandingly incoherent
sample which is easily distinguished from the origi-
nal articles. So we use the conference versus work-
shop task as another evaluation of our model.
We designed a classification experiment for this
task which combines features from the different syn-
tax models that were trained on the ACL conference
training set. We include four features indicating the
perplexity of an article under each model (Local-
prod, Local-d-seq, HMM-prod, HMM-d-seq). We
use perplexity rather than probability because the
length of the articles vary widely in contrast to the
previous permutation-based tests, where both per-
mutation and original article have the same length.
We compute perplexity as P (T )?1/n, where n is
the number of words in the article. We also obtain
the most likely state sequence for the article under
1165
Data Section Test pairs Local-prod Local-d-seq HMM-prod HMM-d-seq Oracle zones
ART Corpus
Abstract 1633 57.0 52.9 64.1 55.0 80.8
Intro 1640 44.5 54.6 58.1 64.6 94.0
ACL Conference
Abstract 8815 44.0 47.2 58.2 63.7
Intro 9966 54.5 53.0 64.4 74.0
Rel. wk. 10,000 54.6 54.4 57.3 67.3
Table 8: Accuracy in differentiating permutation from original sections on ACL and ART test sets.
HMM-prod and HMM-d-seq models using Viterbi
decoding. Then the proportion of sentences from
each state of the two models are added as features.
We also add some fine-grained features from the
local model. We represent sentences in the train-
ing set as either productions or d-sequence items and
compute pairs of associated items (xi, xj) from ad-
jacent sentences using the same chi-square test as
in our pilot study. The most significant (lowest p-
values) 30 pairs (each for production and d-seq) are
taken as features.10 For a test article, we compute
features that represent how often each pair is present
in the article such that xi is in Sm and xj is in Sm+1.
We perform this experiment for each section and
there are about 90 to 140 features for the different
sections. We cast the problem as a binary classifi-
cation task: conference articles belong to one class
and workshop to the other. Each class has 500 ar-
ticles and so the baseline random accuracy is 50%.
We perform 10-fold cross validation using logistic
regression. Our results were 59.3% accuracy for dis-
tinguishing abstracts of conference verus workshop,
50.3% for introductions and 55.4% for related work.
For abstracts and related work, these accuracies are
significantly better than baseline (95% confidence
level from a two-sided paired t-test comparing the
accuracies from the 10 folds). It is possible that in-
troductions in either case, talk in general about the
field and importance of the problem addressed and
hence have similar structure.
Our accuracies are not as high as on permutations
examples because the task is clearly harder. It may
also be the case that the prediction is more difficult
for certain papers than for others. So we also ana-
lyze our results by the confidence provided by the
classifier for the predicted class. We consider only
the examples predicted above a certain confidence
level and compute the accuracy on these predictions.
10A cutoff is applied such that the pair was seen at least 25
times in the training data.
Conf. Abstract Intro Rel wk
>= 0.5 59.3 (100.0) 50.3 (100.0) 55.4 (100.0)
>= 0.6 63.8 (67.2) 50.8 (71.1) 58.6 (75.9)
>= 0.7 67.2 (32.0) 54.4 (38.6) 63.3 (52.8)
>= 0.8 74.0 (10.0) 51.6 (22.0) 63.0 (25.7)
>= 0.9 91.7 (2.0) 30.6 (5.0) 68.1 (7.2)
Table 9: Accuracy (% examples) above each confidence
level for the conference versus workshop task.
These results are shown in Table 9. The proportion
of examples under each setting is also indicated.
When only examples above 0.6 confidence are ex-
amined, the classifier has a higher accuracy of 63.8%
for abstracts and covers close to 70% of the exam-
ples. Similarly, when a cutoff of 0.7 is applied to the
confidence for predicting related work sections, we
achieve 63.3% accuracy for 53% of examples. So
we can consider that 30 to 47% of the examples in
the two sections respectively are harder to tell apart.
Interestingly however even high confidence predic-
tions on introductions remain incorrect.
These results show that our model can success-
fully distinguish the structure of articles beyond just
clearly incoherent permutation examples.
7 Conclusion
Our work is the first to develop an unsupervised
model for intentional structure and to show that
it has good accuracy for coherence prediction and
also complements entity and lexical structure of dis-
course. This result raises interesting questions about
how patterns captured by these different coherence
metrics vary and how they can be combined usefully
for predicting coherence. We plan to explore these
ideas in future work. We also want to analyze genre
differences to understand if the strength of these co-
herence dimensions varies with genre.
Acknowledgements
This work is partially supported by a Google re-
search grant and NSF CAREER 0953445 award.
1166
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT, pages 113?120.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL, pages 9?16.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180.
Jackie C.K. Cheung and Gerald Penn. 2010. Utilizing
extra-sentential context for parsing. In Proceedings of
EMNLP, pages 23?33.
Christelle Cocco, Raphae?l Pittier, Franc?ois Bavaud, and
Aris Xanthos. 2011. Segmentation and clustering of
textual sequences: a typological approach. In Pro-
ceedings of RANLP, pages 427?433.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25?70.
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In Proceedings of LREC, pages 661?667.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
HLT, Short Papers, pages 41?44.
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of ACL-HLT, pages 125?129.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL-HLT, pages
436?443.
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilingual
multidocument summarization. ACM Transactions on
Speech and Language Processing, 3(2):1?16.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 3(12):175?204.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of
EMNLP, pages 273?283.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-HLT, pages 586?594, June.
Nikiforos Karamanis, Chris Mellish, Massimo Poesio,
and Jon Oberlander. 2009. Evaluating centering for
information ordering using corpora. Computational
Linguistics, 35(1):29?46.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of IJCAI.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL, pages 545?552.
Maria Liakata and Larisa Soldatova. 2008. Guidelines
for the annotation of general scientific concepts. JISC
Project Report.
Maria Liakata, Simone Teufel, Advaith Siddharthan, and
Colin Batchelor. 2010. Corpora for the conceptualisa-
tion and zoning of scientific papers. In Proceedings of
LREC.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of EMNLP,
pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse
relations. In Proceedings of ACL-HLT, pages 997?
1006.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of EMNLP, pages 186?195.
Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-
son, and Pradeep Muthukrishnan. 2009. A Bibliomet-
ric and Network Analysis of the field of Computational
Linguistics. Journal of the American Society for Infor-
mation Science and Technology.
David Reitter, Johanna D. Moore, and Frank Keller.
2006. Priming of Syntactic Rules in Task-Oriented
Dialogue and Spontaneous Conversation. In Proceed-
ings of the 28th Annual Conference of the Cognitive
Science Society, pages 685?690.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the fifth conference on
Applied natural language processing, pages 16?19.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of COLING-ACL, pages 803?810.
1167
John Swales. 1990. Genre analysis: English in academic
and research settings, volume 11. Cambridge Univer-
sity Press.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: determining intellectual attribution
in scientific text. In Proceedings of EMNLP, pages 9?
17.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of EACL,
pages 110?117.
Ying Zhao, George Karypis, and Usama Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discov-
ery, 10:141?168.
1168
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155?163,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Structured and Unstructured Cache Models for SMT Domain Adaptation
Annie Louis
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
alouis@inf.ed.ac.uk
Bonnie Webber
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
bonnie@inf.ed.ac.uk
Abstract
We present a French to English transla-
tion system for Wikipedia biography ar-
ticles. We use training data from out-
of-domain corpora and adapt the system
for biographies. We propose two forms
of domain adaptation. The first biases
the system towards words likely in biogra-
phies and encourages repetition of words
across the document. Since biographies in
Wikipedia follow a regular structure, our
second model exploits this structure as a
sequence of topic segments, where each
segment discusses a narrower subtopic of
the biography domain. In this structured
model, the system is encouraged to use
words likely in the current segment?s topic
rather than in biographies as a whole.
We implement both systems using cache-
based translation techniques. We show
that a system trained on Europarl and news
can be adapted for biographies with 0.5
BLEU score improvement using our mod-
els. Further the structure-aware model out-
performs the system which treats the entire
document as a single segment.
1 Introduction
This paper explores domain adaptation of statisti-
cal machine translation (SMT) systems to contexts
where the target documents have predictable reg-
ularity in topic and document structure. Regular-
ities can take the form of high rates of word rep-
etition across documents, similarities in sentence
syntax, similar subtopics and discourse organiza-
tion. Domain adaptation for such documents can
exploit these similarities. In this paper we focus
on topic (lexical) regularities in a domain. We
present a system that translates Wikipedia biogra-
phies from French to English by adapting a system
trained on Europarl and news commentaries. This
task is interesting for the following two reasons.
Many techniques for SMT domain adaption
have focused on rather diverse domains such as us-
ing systems trained on Europarl or news to trans-
late medical articles (Tiedemann, 2010a), blogs
(Su et al., 2012) and transcribed lectures (Federico
et al., 2012). The main challenge for such systems
is translating out-of-vocabulary words (Carpuat et
al., 2012). In contrast, words in biographies are
closer to a training corpus of news commentaries
and parlimentary proceedings and allow us to ex-
amine how well domain adaptation techniques can
disambiguate lexical choices. Such an analysis is
harder to do on very divergent domains.
In addition, biographies have a fairly regu-
lar discourse structure: a central entity (person
who is the topic of the biography), recurring
subtopics such as ?childhood?, ?schooling?, ?ca-
reer? and ?later life?, and a likely chronological
order to these topics. These regularities become
more predictable in documents from sources such
as Wikipedia. This setting allows us to explore the
utility of models which make translation decisions
depending on the discourse structure. Translation
methods for structured documents have only re-
cently been explored in Foster et al. (2010). How-
ever, their system was developed for parlimentary
proceedings and translations were adapted using
separate language models based upon the identity
of the speaker, text type (questions, debate, etc.)
and the year when the proceedings took place.
Biographies constitute a more realistic discourse
context to develop structured models.
This paper introduces a new corpus consisting
of paired French-English translations of biography
articles from Wikipedia.
1
We translate this cor-
pus by developing cache-based domain adaptation
methods, a technique recently proposed by Tiede-
1
Corpus available at http://homepages.inf.ed.
ac.uk/alouis/wikiBio.html.
155
mann (2010a). In such methods, cache(s) can be
filled with relevant items for translation and trans-
lation hypotheses that match a greater number of
cache items are scored higher. These cache scores
are used as additional features during decoding.
We use two types of cache?one which encour-
ages the use of words more indicative of the biog-
raphy domain and another which encourages word
repetition in the same document.
We also show how cache models allow
for straightforward implementation of structured
translation by refreshing the cache in response to
topic segment boundaries. We fill caches with
words relevant to the topic of the current segment
which is being translated. The cache contents are
obtained from an unsupervised topic model which
induces clusters of words that are likely to ap-
pear in the same topic segment. Evaluation re-
sults show that cache-based models give upto 0.5
BLEU score improvements over an out-of-domain
system. In addition, models that take topical struc-
ture into account score 0.3 BLEU points higher
than those which ignore discourse structure.
2 Related work
The study that is closest to our work is that of
Tiedemann (2010a), which proposed cache mod-
els to adapt a Europarl-trained system to medical
documents. The system used caching in two ways:
a cache-based language model (stores target lan-
guage words from translations of preceding sen-
tences in the same document) and a cache-based
translation model (stores phrase pairs from pre-
ceding sentence translations). These caches en-
couraged the system to imitate the ?consistency?
aspect of domain-specific texts i.e., the property
that words or phrases are likely to be repeated in a
domain and within the same document.
Cache models developed in later work, Tiede-
mann (2010b) and Gong et al. (2011), were ap-
plied for translating in-domain documents. Gong
et al. (2011) introduced additional caches to store
(i) words and phrase pairs from training docu-
ments most similar to a current source article,
and (ii) words from topical clusters created on the
training set. However, a central issue in these sys-
tems is that caches become noisy over time, since
they ignore topic shifts in the documents. This pa-
per presents cache models which not only take ad-
vantage of likely words in the domain and consis-
tency, but which also adapt to topic shifts.
A different line of work very relevant to our
study is the creation of topic-specific translations
by either inferring a topic for the source document
as a whole, or at the other extreme, finer topics for
individual sentences (Su et al., 2012; Eidelman et
al., 2012). Neither of these granularities seem in-
tuitive in natural discourse. In this work, we pro-
pose that tailoring translations to topics associated
with discourse segments in the article is likely to
be beneficial for two reasons: a) subtopics of such
granularity can be assumed with reasonable con-
fidence to re-occur in documents from the same
domain and b) we can hypothesize that a domain
will have a small number of segment-level topics.
3 System adaptation for biographies
We introduce two types of translation systems
adapted for biographies:
General domain models (domain-) that use in-
formation about biographies but treat the docu-
ment as a whole.
Structured models (struct-) that are sensitive to
topic segment boundaries and the specific topic of
the segment currently being translated.
We implement both models using caches. Since
we do not have parallel corpora for the biography
domain, our caches contain items in the target lan-
guage only. We use two types of caches:
Topic cache stores target language words (uni-
grams) likely in a particular topic. Each unigram
has an associated score.
Consistency cache favours repetition of words in
the sentences from the same document. It stores
target language words (unigrams) from the 1-best
translations of previous sentences in the same doc-
ument. Each word is associated with an age value
and a score. Age indicates when a word entered
the cache and introduces a ?decay effect?. Words
used in immediately previous sentences have a
low age value while higher age values indicate
words from sentences much prior in the document.
Scores are inversely proportional to age.
Both the types of caches are present in both
the general domain and structured models, but the
cache words and scores are computed differently.
3.1 A general domain model
This system seeks to bias translations towards
words which occur often in biography articles.
The topic cache is filled with word unigrams
that are more likely to occur in biographies com-
156
pared to general news documents. We compare
the words from 1,475 English Wikipedia biogra-
phies articles to those in a large collection (64,875
articles) of New York Times (NYT) news articles
(taken from the NYT Annotated Corpus (Sand-
haus, 2008)). We use a log-likelihood ratio test
(Lin and Hovy, 2000) to identify words which oc-
cur with significantly higher probability in biogra-
phies compared to NYT. We collect only words
indicated with 0.0001 significance by the test to
be more likely in biographies. We rank this set of
18,597 words in decreasing order of frequency in
the biography article set and assign to each word
a score equal to 1/rank of the word. These words
with their associated scores form the contents of
the topic cache. In the general domain model,
these same words are assumed to be useful for the
full document and so the cache contents remain
constant during translation of the full document.
The consistency cache stores words from the
translations of preceding sentences of the same
document. After each sentence is translated, we
collect the words from the 1-best translation and
filter out punctuation marks and out of vocabu-
lary words. The remaining words are assigned an
age of 1. Words already present in the cache have
their age incremented by one. The new words with
age 1 are added to the cache
2
and the scores for
all cache words are recomputed as e
1/age
. The
age therefore gets incremented as each sentence?s
words are inserted into the cache creating a decay.
The cache is cleared at the end of each document.
During decoding, a candidate phrase is split into
unigrams and checked against each cache. Scores
for matching unigrams are summed up to obtain a
score for the phrase. Separate scores are computed
for matches with the topic and consistency caches.
3.2 A structured model
Here we consider topic and consistency at a nar-
rower level?within topic segments of the article.
The topic cache is filled with words likely in
individual topic segments of an article. To do this,
we need to identify the topic of smaller segments
of the article and also store a set of most probable
words for each topic. The topics should also have
bilingual mappings which will allow us to infer for
every French document segment, words that are
likely in such a segment in the English language.
We designed and implemented an unsupervised
2
If the word already exists in the cache, it is first removed.
topic model based on Latent Dirichlet Allocation
(LDA) (Blei et al., 2003) to induce such word clus-
ters. In a first step, we induce subtopics from
monolingual articles in English and French sep-
arately. The topics are subsequently aligned be-
tween the languages as explained below.
In the first step, we learn a topic model which
incorporates two main ideas a) adds sensitivity
to topic boundaries by assigning a single topic
per topic segment b) allows for additional flex-
ibility by not only drawing the words of a seg-
ment from the segment-level topic, but also al-
lows some words to be either specific to the doc-
ument (such as named entities) or stop words. To
address idea b), we have a ?switching variable?
to switch between document-specific word, stop-
word or domain-words.
The generative story to create a monolingual
dataset of biographies is as follows:
? Draw a distribution ? for the proportion of the
three word types in the full corpus (domain
subtopic, document-specific, stopwords) ?
Dirichlet(?)
? For each domain subtopic ?
l
, 1 ? l ? T ,
draw a distribution over word vocabulary ?
Dirichlet(?)
? Draw a distribution ? over word vocabulary
for stopwords ? Dirichlet()
? For each document D
i
:
? Draw a distribution pi
i
over vocab-
ulary for document-specific words ?
Dirichlet(?)
? Draw a distribution ?
i
giving the mix-
ture of domain subtopics for this docu-
ment ? Dirichlet(?)
? For each topic segment M
ij
in D
i
:
? Draw a domain subtopic z
ij
?
Multinomial(?
i
)
? For each word w
ijk
in segment M
ij
:
? Draw a word type s
ijk
?
Multinomial(?)
? Depending on the chosen switch
value s
ijk
, draw the word from
the subtopic of the segment ?
z
ij
or document-specific vocabulary
pi
i
, or stopwords ?
We use the section markings in the Wikipedia
articles as topic segment boundaries while learn-
ing the model. We use symmetric Dirichlet priors
157
for the vocabulary distributions associated with
domain subtopics, document-specific words and
stopwords. The concentration parameters are set
to 0.001 to encourage sparsity. The distribution
?
i
for per-document subtopics is also drawn from
a symmetric Dirichlet distribution with concentra-
tion parameter 0.01. We use asymmetric Dirich-
let priors for ? set to (5, 3, 2) for (domain topic,
document-specific, stopwords). The hyperparam-
eter values were minimally tuned so that the differ-
ent vocabulary distributions behaved as intended.
We perform inference using collapsed Gibbs
sampling where we integrate out many multinomi-
als. The sampler chooses a topic z
ij
for every seg-
ment and then samples a word type s
ijk
for each
word in the segment. We initialize these variables
randomly and the assignment after 1000 Gibbs it-
erations are taken as the final ones. We create
these models separately for English and French,
in each case obtaining T domain subtopics.
The second step creates an alignment between
the source and target topics using a bilingual dic-
tionary
3
. For each French topic, we find the top
matching English topic by scoring the number
of dictionary matches. It is unlikely for every
French topic to have a closely corresponding En-
glish topic. Based on observations about the qual-
ity of topic alignment, we select the top 60% (out
of T ) pairs of French-English aligned topics only.
Note that our method uses two steps to learn
bilingual topics in contrast to some multilingual
topic models which learn aligned topics directly
from parallel or comparable corpora (Zhao and
Xing, 2006; Boyd-Graber and Blei, 2009; Jagar-
lamudi and Daum?e III, 2010). These methods in-
duce topic-specific translations of words. Rather
we choose a less restrictive pairing of word clus-
ters by topic since (i) we have monolingual bi-
ographies in the two languages which could be
quite heterogenous in the types of personalities
discussed, (ii) we seek to identify words likely in a
topic segment for example ?career-related? words
rather than specific translations for source words.
During translation, for each topic segment in the
source document, we infer the French topic most
likely to have produced the segment and find the
corresponding English-side topic. The most prob-
able words for that English topic are then loaded
into the topic cache. The score for a word is its
probability in that topic. When a topic segment
3
A filtered set of 13,400 entries from www.dict.cc
boundary is reached, the topic cache is cleared and
the topic words for the new segment are filled.
The consistency cache?s contents are computed
similarly to the general domain case. However, the
cache gets cleared at segment boundaries.
4 Training and test data
We distinguish two resources for data. The out-
of-domain system is trained using the WMT?12
datasets comprising Europarl and news commen-
tary texts. It has 2,144,820 parallel French-
English sentence pairs. The language model is
trained using the English side of the training cor-
pus. The tuning set has 2,489 sentence pairs.
Our test set is a corpus of French to En-
glish translations of biographies compiled from
Wikipedia. To create the biography corpus, we
collect articles which are marked with a ?Trans-
lation template? in Wikipedia metadata. These
markings indicate a page which is translated from
a corresponding page in a different language and
also contains a link to the source article. (Note
that these article pairs are not those written on
the same topic separately in the two languages.)
We collect pairs of French-English pages with this
template and filter those which do not belong to
the Biography topic (using Wikipedia metadata).
Note, however, that these article pairs are not
very close translations. During translation an edi-
tor may omit or add information and also reorga-
nize parts of the article. So we filter out the paired
documents which differ significantly in length. We
use LFAligner
4
to create sentence alignments for
the remaining document pairs. We constrain the
alignments to be within documents but since sec-
tion headings were not maintained in translations,
we did not further constrain alignments within sec-
tions. We manually corrected the resulting align-
ments and keep only documents which have good
alignments and have manually marked topic seg-
ments (Wikipedia section headings). Unaligned
sentences were filtered out. Table 1 shows a sum-
mary of this data and the split for tuning and test.
The articles are 12 to 87 sentences long and con-
tain 5 topic segments on average.
We also collect a larger set of monolingual
French and English Wikipedia biographies to cre-
ate the domain subtopics. We select only articles
that have at least 10 segments (sections) to ensure
4
http://sourceforge.net/projects/
aligner/
158
Tuning Test
No. of article pairs 15 30
Total sentences pairs 430 1008
Min. article size (in sentences) 13 12
Max. article size (in sentences) 59 85
Average no. of segments per article 4.7 5.3
Table 1: Summary of Wikipedia biographies data
that they are comprehensive ones. This collection
contains 1000 French and 1000 English articles.
5 Experimental settings
We use the Moses phrase-based translation system
(Koehn et al., 2007) to implement our models.
5.1 Out-of-domain model
This baseline model is trained on the WMT 2012
training sets described in the previous section and
uses the six standard features from Koehn et al.
(2003). We build a 5-gram language model us-
ing SRILM. The features were tuned using MERT
(Och, 2003) on the WMT 2012 tuning sets. This
system does not use any data about biographies.
5.2 Biography-adapted models
First we perform experiments using the manually
marked sections in Wikipedia as topic segments.
We also report results with automatic segmenta-
tion in Section 7.
The domain and structured models have two ex-
tra features ?topic cache? and ?consistency cache?.
For the structured model, topic segment bound-
aries and inferred topic is passed as XML markup
on the source documents. For the consistency
cache, we use a wrapper which passes the 1-
best translation (also using XML markup) of the
preceding sentence and updates the cache before
translating every next sentence.
We tune the weights for these new cache fea-
tures as follows. The weights for the baseline fea-
tures from the out-of-domain model are kept con-
stant. The weights for the new cache features are
set using a grid search. This tuning uses the bi-
ographies documents listed in Table 1 as tuning
data. We run the decoding using the baseline fea-
ture weights and a weight for a cache feature and
compute the (case-insensitive) BLEU (Papineni et
al., 2002) scores of each tuning document. The
weight for the cache feature which maximizes the
average BLEU value over the tuning documents
is chosen. We have not tuned the features us-
ing MERT in this study since a grid search al-
lowed us to quantify the influence of increasing
Figure 1: Effect of feature weights and number of
topics on accuracy for structured topic cache
weights on the new features directly. Previous
work has noted that MERT fails to find good set-
tings for cache models (Tiedemann, 2010b). In
future work, we will explore how successful op-
timization of baseline and cache feature weights
could be done jointly. We present the findings
from our grid search below.
The struct-topic cache has two parameters, the
number of topics T and the number of most prob-
able words from each topic which get loaded into
the cache. We ran the tuning for T = 25, 50,
100 and 200 topics (note that 60% of the topics
will be kept after bilingual alignment, see Section
3.2). We also varied the number of topic words
chosen?50, 100, 250 and 500.
The performance did not vary with the number
of topic words used and 50 words gave the same
performance as 500 words for topic models with
any number of topics. This interesting result sug-
gests that only the most likely and basic words
from each topic are useful. The top 50 words from
two topics (one capturing early life and the other
an academic career) taken from the 50-topic model
on English biographies are shown in Table 2.
In Figure 1, we show the performance of sys-
tems using different number of topics. In each
case, the same number of topic words (50) was
added to the cache. We find that 50 topics model
performs best confirming our hypothesis that only
a small number of domain subtopics is plausible.
We choose the 50 topic model with top 50 words
for each topic for the structured topic cache.
The best weights and average document level
BLEU scores on the tuning set are given in Table
3. The scores were computed using the mteval-
v13a.pl script in Moses. BLEU scores for the
159
his a s family on life She child St mother
of in married children They death became whom friends attended
and had He that daughter son marriage lived later work
to was born died wife years met couple I age
he her at she father home moved about husband house
of is He received has included National original Academy French
and The by its used works study list book College
his work Award Medal award His Institute life contributed Year
in he are awarded also title Arts Royal edition awards
s University Prize Society A honorary Library include Sciences recognition
Table 2: Top 50 words from 2 topics of the T = 50 topic model
Cache type weight BLEU-doc
Domain-topic 0.075 19.79
Domain-consistency 0.05 19.70
Domain-topic + consis. 0.05, 0.05 19.80
Struct-topic (50 topics) 1.75 19.94
Struct-consistency 0.125 19.70
Struct-topic + consis. 0.4, 0.1 19.84
Domain-consis. + struct-topic 0.1, 0.25 19.86
Out-of-domain 19.33
Table 3: Best weights for cache features and
BLEU scores (averaged for tuning documents).
out-of-domain model are shown on the last line.
Note that these scores are overall on a lower scale
for a French-English system due to out-of-domain
differences and because the reference translations
from Wikipedia are not very close ones.
These numbers show that cache models have the
potential to provide better translations compared
to an out-of-domain baseline. The structured topic
model system is the best system outperforming the
out-of-domain system and also the domain-topic
system. Hence, treating documents as composed
of topical segments is a useful setting for auto-
matic translation.
The domain and structured versions of the con-
sistency cache however, show no difference. This
result could arise due to the decay factor incor-
porated in the consistency cache. Higher scores
are given to words from immediately previous
sentences compared to those far off. This decay
implicitly gives lower scores to words from ear-
lier topic segments than those from recent ones.
Explicitly refreshing the cache in the structured
model does not give additional benefits.
When consistency and topic caches are used to-
gether in both general domain and structured set-
tings, the combination is not better than individual
caches. We also tried a setting where the consis-
tency cache is document-range and the topic cache
works at segment level (domain-consis. + struct-
topic). This combination also does not outperform
using the structured topic cache alone.
Model BLEU-doc BLEU-sent
Domain-topic 17.63 17.61
Domain-consistency 17.70 17.75
Domain-topic + consis. 17.63 17.63
Struct-topic (50 topics) 17.76 17.84
Struct-consistency 17.33 17.34
Struct-topic + consis. 17.47 17.51
Struct-topic + dom-consis. 17.29 17.25
Out-of-domain 17.37 17.43
Table 4: BLEU scores on the test set. ?doc? in-
dicates BLEU scores averaged over documents,
?sent? indicates sentence-level BLEU
6 Results on the test corpus
The best weights chosen on the tuning corpus are
used to decode the biographies test corpus (sum-
marized in Table 1). Table 4 reports the av-
erage BLEU of documents as well as sentence
level BLEU scores of the corpus. We used the
paired bootstrap resampling method (Koehn 2004)
to compute significance.
The struct-topic model gives the highest im-
provement of 0.4 sentence level BLEU over the
out-of-domain model. Struct-topic is also 0.23
BLEU points better compared to the domain-
topic model confirming the usefulness of model-
ing structure regularities. These improvements at
significant at 95% confidence level.
The second best model is the domain-
consistency model (significantly better than out-
of-domain model at 90% confidence level). But
the performance of this cache decreases in the
structured setting. Moreover, combinations of
caches fail to improve over individual caches.
One hypothesis for this result is that biogra-
phy subtopic words which give good performance
in the topic cache differ from the words which
provide benefits in the consistency cache. For
example, words related to named entities and
other document-specific content words could be
ones that are more consistent within the docu-
ment. Then clearing the consistency cache at topic
boundaries would remove such words from the
160
cache leading to low performance of the ?struc-
tured? version. In our current model, we do not
distinguish between words making up the consis-
tency cache. In future, we plan to experiment
with consistency caches of different ranges and
which hold different types of words. This ap-
proach would require identifying named entities
and parts of speech on the automatic translations
of previous sentences, which is likely to be error-
prone and so require methods for associating a
confidence measure with the cache words.
7 Understanding factors that influence
structured cache models
The documents in our test corpus have varying
lengths, number of segments and segment sizes.
This section explores the behavior of structured
models on these different document types. For
this analysis, we compare the BLEU scores from
the domain and the structured versions of the two
caches. We do not consider the out-of-domain sys-
tem here since we are interested in quantifying
gains from using document structure.
For each document in our test corpus, we com-
pute (i) the difference between the BLEU scores
of struct-topic and domain-topic systems (BLEU-
gain-topic), and (ii) the difference in BLEU
scores between the struct-consistency and domain-
consistency systems (BLEU-gain-consis). Table 5
reports the average BLEU gains binned by a) the
document length (in sentences) b) number of topic
segments in the document and c) the average size
of topic segments in a document (in sentences).
The numbers clearly indicate that performance
is not uniform across different types of docu-
ments. The struct-topic cache performs much bet-
ter on longer documents of over 30 sentences giv-
ing 0.3 to 0.4 BLEU points increase compared to
the general domain model. On the other hand, the
performance worsens when the structured cache
is applied on documents with less than 20 sen-
tences. Similarly, the struct-topic cache is benefi-
cial for documents where the average segment size
is larger than 5 sentences and when the number of
topic segments is around 5 to 7.
The struct-consistency cache generally per-
forms worse than the unstructured version and
there does not appear to be a niche set according
to any of the properties?document length, num-
ber of segments and segment size.
Given these findings, it is possible that the
struct-topic cache can benefit by modifying the
(a) Average BLEU gains and document length
doc. length no. docs gain-topic gain-consis
12 to 19 7 -0.41 -0.20
20 to 29 10 0.17 -0.63
30 to 49 8 0.44 -0.16
50 to 85 5 0.34 -0.45
(b) Average BLEU gains and no. of topic segments
no. segments no. docs gain-topic gain-consis
3 to 4 9 -0.09 -0.21
5 13 0.24 -0.37
6 to 7 5 0.34 -0.74
9 3 -0.03 -0.26
(c) Average BLEU gains and topic segment size
avg. segment size no. docs gain-topic gain-consis
< 5 10 -0.23 -0.41
5 to 10 18 0.33 -0.37
11 to 17 2 0.39 -0.24
Table 5: Average BLEU score gains from a struc-
tured cache (compared to domain caches) split by
different properties of documents in the test set
document structure to match that handled better
by the structured model. We test this hypothe-
sis by segmenting all test documents with an ideal
segment size. The model seems to perform better
when each segment has around 5 to 10 sentences
(longer segments are also preferred but we have
few very long documents in our corpus), so we
try to re-segment the articles to contain approxi-
mately 7 sentences in each segment. We use an
automatic topic segmentation method (Eisenstein
and Barzilay, 2008) to segment the source arti-
cles in our test corpus. For each article we request
(document length)/7 segments to be created.
5
We then run the structured topic and consis-
tency models on the automatically segmented cor-
pus using the same feature weights as before. The
results are shown in Table 6.
Model BLEU (doc) BLEU (sent)
Struct-topic 17.94 17.94
Struct-consistency 17.51 17.46
Table 6: Translation performance on automati-
cally segmented test corpus
The struct-topic cache now reaches our best re-
sult of 0.5 BLEU improvement over the out-of-
domain model and 0.3 improvement over the un-
structured domain model. The consistency cache
is also slightly better using the automatic segmen-
tation than the manual sections. Choosing the
right granularity appears to be important for struc-
tured caches and coarse section headers may not
be ideal. This result also shows automatic segmen-
5
Note that we only specify the number of segments, but
the system could create long or short segments.
161
of (42) he (36) his (36) the (22) to (11) in (9) was (7) one (6) a (3) at (3)
head (3) that (3) construction (3) empire office french bases reconstruction only such
all ban marseille main charged have well researchers openness retreat
an two mechanical events army iron class surrender order thirty
and black objectives factory disciple largest close budget part time
as who ceremony figure majority level even sentence project trained
on seat diplomatic wheat working winner life archaeological 9 during
Table 7: Impact words computed on the test corpus. The number of times each word was found in the
impact list is indicated within parentheses. Words listed without parentheses appeared once in the list.
(1) (S) Pendant la Premi`ere Guerre mondiale, mobilis?e dans les troupes de marine, il combat dans les Balkans et les
Dardanelles.
(R) During the First World War, conscripted into the navy, he fought in the Balkans and the Dardanelles.
(B) During World War I, mobilized in troops navy, it fight in the Balkans and Dardanelles.
(C) During World War I, mobilized troops in the navy, he fight in the Balkans and the Dardanelles.
(2) (S)
`
A l??age de 15 ans, elle a ?et?e choisie par la troupe d?op?era de l?arm?ee chinoise pour ?etre form?ee au chant.
(R) At the age of 15, she was selected by the Chinese Armys Operatic troupe to be trained as a singer.
(B) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be formed the call.
(C) In the age of 15 years, she was chosen by the pool of opera of the Chinese military to be trained to call.
(3) (S) La figure de la Corriveau n?a cess?e, depuis, d?inspirer romans, chansons et pi`eces de th?e?atre et d?alimenter les
controverses.
(R) The figure of Corriveau still inspires novels, songs and plays and is the subject of argument.
(B) The perceived the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
(C) The figure of the Corriveau has stopped, since, inspire novels, songs and parts of theater and fuel controversies.
Table 8: Three examples of impact words in test translations. Abbreviations: S - source sentence, R -
reference translation, B - baseline translation, C - structured topic cache translation
tation can be successfully used in these models.
8 Changes made by the cache models
Here we examine the kinds of changes made by
the cache models which have lead to the im-
proved BLEU scores. We focus on the the topic
cache since its changes are straightforward to
compute compared to consistency. We analyze
the struct-topic cache translations on automati-
cally segmented documents as that provided the
best performance overall.
To do this analysis, we define the notion of an
impact word. An impact word is one which satis-
fies three conditions: (i) the word is not present in
the out-of-domain translation of a sentence, (ii) it
is present in the translation produced by the topic
cache model (iii) the word matches the reference
translation for the sentence.
These impact words provide a simple (albeit ap-
proximate) way to analyze useful changes made
by the topic cache over the out-of-domain system.
On the test corpus (30 documents), 231 impact
word tokens were found and they come from 70
unique word types. So topic cache model signif-
icantly affects translation decisions and over 200
useful word changes were made in the 30 doc-
uments. The impact word types and counts are
shown in Table 7. Several of these changes relate
to function words and pronouns. For example, the
pronoun ?he? and the past tense verb ?was? were
correctly introduced in several sentences such as
Example (1) in Table 8. A content word change is
indicated in examples (2) and (3). These changes
appear to be appropriate for biographies.
9 Conclusions
We have introduced a new corpus of biography
translations which we propose as suitable for ex-
amining discourse-motivated SMT methods. We
showed that cache-based techniques which also
take the topic organization into account, make
more appropriate lexical choices for the domain.
In future work, we plan to explore how other do-
main similarities such as sentence syntax and en-
tity reference, for example biographies have a cen-
tral entity (person), can be used to improve transla-
tion performance. We also plan to take advantage
of recent methods to do document level decoding
(Hardmeier et al., 2012).
Acknowledgements
The first author was supported by a Newton Inter-
national Fellowship (NF120479) from the Royal
Society and The British Academy. We also thank
the NLP group at Edinburgh for their comments
and feedback on this work.
162
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In Proceed-
ings of UAI, pages 75?82.
Marine Carpuat, Hal Daum?e III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Ale?s Tamchyna, Katharine Henry, and
Rachel Rudinger. 2012. Domain adaptation in ma-
chine translation: Final report. In 2012 Johns Hop-
kins Summer Workshop Final Report.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL, pages
115?119.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
Marcello Federico, Mauro Cettolo, Luisa Bentivogli,
Michael Paul, and Sebastian Stueker. 2012.
Overview of the IWSLT 2012 evaluation campaign.
Proceedings of IWSLT.
George Foster, Pierre Isabelle, and Roland Kuhn.
2010. Translating structured documents. In Pro-
ceedings of AMTA.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of EMNLP, pages
909?919.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the EMNLP-CoNLL, pages 1179?1190.
Jagadeesh Jagarlamudi and Hal Daum?e III. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. In Advances in Information Retrieval,
Lecture Notes in Computer Science, pages 444?456.
Springer Berlin Heidelberg.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-HLT, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL meeting on Interac-
tive Poster and Demonstration Sessions, pages 177?
180.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proceedings of COLING, pages 495?501.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. Corpus number LDC2008T19, Lin-
guistic Data Consortium, Philadelphia.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of ACL, pages 459?468.
J?org Tiedemann. 2010a. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing.
J?org Tiedemann. 2010b. To cache or not to cache?:
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 189?194.
Bing Zhao and Eric P. Xing. 2006. Bitam: bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING-ACL, pages 969?976.
163
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 636?644,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Verbose, Laconic or Just Right: A Simple Computational Model of
Content Appropriateness under Length Constraints
Annie Louis
?
School of Infomatics
University of Edinburgh
Edinburgh EH8 9AB
alouis@inf.ed.ac.uk
Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19103
nenkova@seas.upenn.edu
Abstract
Length constraints impose implicit re-
quirements on the type of content that
can be included in a text. Here we pro-
pose the first model to computationally as-
sess if a text deviates from these require-
ments. Specifically, our model predicts
the appropriate length for texts based on
content types present in a snippet of con-
stant length. We consider a range of fea-
tures to approximate content type, includ-
ing syntactic phrasing, constituent com-
pression probability, presence of named
entities, sentence specificity and inter-
sentence continuity. Weights for these fea-
tures are learned using a corpus of sum-
maries written by experts and on high
quality journalistic writing. During test
time, the difference between actual and
predicted length allows us to quantify text
verbosity. We use data from manual eval-
uation of summarization systems to as-
sess the verbosity scores produced by our
model. We show that the automatic ver-
bosity scores are significantly negatively
correlated with manual content quality
scores given to the summaries.
1 Introduction
In dialog, the appropriate length of a speaker turn
and the amount of detail in it are hugely influ-
enced by the pragmatic context. For example what
constitutes an appropriate answer to the question
?How was your vacation?? would be very different
when the question is asked as two acquaintances
pass each other in the corridor or right after two
friends have ordered dinner at a restaurant. Simi-
larly in writing, content is tailored to explicitly de-
fined or implicitly inferred constraints on the ap-
?
Work done while at University of Pennsylvania.
50 word summary:
The De Beers cartel has kept the diamond market stable
by matching supply to demand. African nations have
recently demanded better terms from the cartel. After
the Soviet breakup, De Beers contracted for diamonds
with the Yukutian Republic. The US remains the largest
diamond market, followed by Japan.
100 word summary:
The De Beers cartel, controlled by the Oppenheimer
family controls 80% of the uncut diamond market
through its Central Selling Organization. The cartel
has kept the diamond market stable by maintaining a
buffer pool of diamonds for matching supply to demand.
De Beers opened a new mine in 1992 and extended the
life of two others through underground mining.
Innovations have included automated processing and
bussing workers in daily from their homes. African
nations have recently demanded better terms. After
the Soviet breakup, De Beers contracted for diamonds
with the Yukutian Republic. The US remains the largest
diamond market, followed by Japan.
Table 1: 50 and 100 word summaries written by
the same person for the same set of documents
propriate length of text. Many academics have ex-
perienced the frustration of needing to adjust their
writing when they need to write a short abstract of
two hundred words or an answer to reviewer in no
more than five hundred words.
For a specific application-related example con-
sider the texts in Table 1. These are summaries of
a set of news articles discussing the De Beers di-
amond cartel, written by the same person.
1
The
first text is written with the instruction to produce
a summary of about 50 words while the latter is
in response to a request for a 100 word summary.
Obviously the longer summary contains more de-
tails. It doesn?t however simply extend the shorter
summary with more sentences; additional details
1
These summaries come from the Document Understand-
ing Conference dataset (year 2001).
636
are interspersed with the original shorter summary.
The performance of a range of human-machine
applications can be enhanced if they had the abil-
ity to predict the appropriate length of a system
contribution and the type of content appropriate
for that length. Such applications include docu-
ment generation (O?Donnell, 1997), soccer com-
mentator (Chen and Mooney, 2008) and question
answering with different compression rates for dif-
ferent types of questions (Kaisser et al., 2008).
Predicting the type of content appropriate for the
given length alone would be highly desirable, for
example in automatic essay grading, summariza-
tion and even in information retrieval, in which
verbose writing is particularly undesirable. In this
respect, our work supplements recent computa-
tional methods to predict varied aspects of writing
quality, such as popular writing style and phras-
ing in novels (Ganjigunte Ashok et al., 2013), sci-
ence journalism (Louis and Nenkova, 2013), and
social media content (Danescu-Niculescu-Mizil et
al., 2012; Lakkaraju et al., 2013).
Our work is the first to explore text verbosity.
We introduce a simple application-oriented defi-
nition of verbosity and a model to automatically
predict verbosity scores. We start with a brief
overview of our approach in the next section.
2 Text length and content
appropriateness
In this first model of verbosity, we do not carry
out an elaborate annotation experiment to create
labels for verbosity. There are two main reasons
for this choice: a) People find it hard to distinguish
between individual aspects of quality and often
the ratings for different aspects are highly corre-
lated (Conroy and Dang, 2008; Pitler et al., 2010)
b) Moreover, for verbosity in particular, the most
appropriate data for annotation would be concise
and verbose versions of the same text (possibly of
similar lengths). It is more likely that people can
distinguish between verbosity of these controlled
pairs compared to ratings on an individual arti-
cle. Such writing samples are not easily available.
So we have avoided the uncertainties in annotation
in this first work by adopting a simpler approach
based on three key ideas.
(i) We define a concise article of length l as ?an
article that has the appropriate types of content ex-
pected in an article of length l?. Note that length
is not equal to verbosity in our model. Our defi-
nition allows for articles of different lengths to be
considered concise. Verbosity depends on the ap-
propriateness of content for the article length.
(ii) We model this appropriateness of content
for the given length restriction via a set of easily
computable features that serve as proxies for (a)
type of content and level of detail (syntactic fea-
tures and sentence specificity) (b) sentence com-
plexity (simple readability-related features), (c)
secondary details (syntactic structures with high
compression probability) and (d) structure (dis-
course relations and inter-sentence continuity).
(iii) Forgoing any explicit annotation, we sim-
ply train the model on professionally written text
in which we assume content is appropriately tai-
lored to the length requirements. We train a re-
gression model on the well-written texts to predict
the length of an article based on a single snippet of
fixed (short) length from the article. For a new test
article, we can obtain a predicted length from this
model (length supposing the article is written con-
cisely) based on a short snippet. We use the mis-
match between the predicted and actual text length
of the article to determine if it is verbose.
We believe that this definition of verbosity has
natural uses in applications such as summariza-
tion. For example, current systems do not distin-
guish the task of summary creation for different
target lengths. They simply try to maximize esti-
mated sentence importance and to minimize repet-
itive information. They pay no attention to the fact
that the same type of sentences are unlikely to be
an optimal selection for both a 50 word and a 400
word summary.
We now briefly present the formal definition of
the problem of content appropriateness for a spec-
ified text length. Let T = (t
1
, t
2
, ...t
n
) be a collec-
tion of concisely-written texts and let l(t
i
) denote
the length of text t
i
. The learning task is to obtain
a function based on the content type properties of
t
i
which helps to predict l(t
i
). More specifically,
we are given a snippet from t
i
, called s
t
i
, of a con-
stant length k where k is a parameter of our model
and k < min
t
j
l(t
j
). The mapping f is learned
based on the constant length snippet only and the
aim is to predict the original text length.
f(s
t
i
)?
?
l(t
i
)
In our work we choose to work with topical seg-
ments from documents rather than the complete
documents themselves.
637
Once the model is trained, we identify the ver-
bosity for a test article as follows: Let us consider
a new topic segment t
x
during test time. Let the
length of the segment be l. We obtain a snippet
s
t
x
of size k from t
x
. Now assume that our model
predicts f(s
t
x
) =
?
l.
Case 1:
?
l ' l, the content type in t
x
matches
the content types generally present in articles of
length l. We consider such articles as concise.
Case 2:
?
l  l, the type of content included in
t
x
is really suitable for longer and detailed topic
segments. Thus t
x
is likely conveying too much
detail given its length i.e. it is verbose.
Case 3:
?
l  l, the content in t
x
is of the
type that a skillful writer would include in a much
shorter and less detail-oriented text. Thus t
x
is
likely lacking appropriate details (laconic).
We compute the following scores to quantify
verbosity:
Predicted length. is the model prediction
?
l.
Verbosity degree. This score is the difference
between the predicted length and the actual length
of the text,
?
l ? l. Positive values of the score indi-
cate the degree of verbosity, negative values indi-
cate that the text is laconic.
Deviation score. Since both being verbose and
being laconic is potentially problematic for text,
we define a score which does not differentiate the
type of mismatch. This score is given by the abso-
lute magnitude |
?
l ? l|.
The next section describes the features used for
indicating the content type of a snippet. In Section
4, we test the features on a four-way classification
task to predict the length of a human-written sum-
mary based on a snippet of the summary. In Sec-
tion 5, we extend our model to a regression set-
ting by learning feature weights on news articles of
varied lengths from the New York Times (NYT),
which we consider to be a sample in which content
is chosen appropriately for each article length. Fi-
nally in Section 6 we evaluate the model trained
on NYT articles on machine-produced summaries
and confirm that summaries scored with higher
verbosity by our model also receive poor content
quality scores during manual evaluation.
3 Features mapping content type to
appropriate length
We propose a diverse set of 87 features for charac-
terizing content type. These features are computed
over the constant length snippet sampled from an
article. All the syntax based features are com-
puted from the constituency trees produced from
the Stanford Parser (Klein and Manning, 2003).
Length of units (10 features).
This set of features captures basic word and
sentence length, and redundancy properties of the
snippet. It includes number of sentences, average
sentence length in words, average word length in
characters, and type to token ratio. We also in-
clude the counts of noun phrases, verb phrases
and prepositional phrases and the average length
in words of these three phrase types.
Syntactic realization (30 features).
We compute the grammatical productions in a
set of around 47,000 sentences taken from the
AQUAINT corpus (Graff, 2002) We select the
most frequent 15 productions in this set that in-
volve a description of entities, i.e the LHS (left-
hand side) of the production is a noun phrase. The
count of each of these productions is added as a
feature allowing us to track what type of informa-
tion about the entities is conveyed in the snippet.
We also add features for the most frequent 15 pro-
ductions whose LHS is not a noun phrase.
Discourse relations (5 features).
These features are based on the hypothesis that
different discourse relations would vary in their
appropriateness for articles of different lengths.
For example causal information may be included
only in more detailed texts.
We use a tool (Pitler and Nenkova, 2009) to
identify all explicit discourse connectives in our
snippets, along with the general semantic class
of the connective (temporal, comparison, contin-
gency and expansion). We use the number of dis-
course connectives of each of the four types as fea-
tures, as well as the total number of connectives.
Continuity (6 features).
These features capture the degree to which ad-
jacent sentences in the snippet are related and con-
tinue the topic. The amount of continuity for
subtopics is likely to vary for long and short texts.
We add the number of pronouns and determin-
ers as two features. Another feature is the average
word overlap value between adjacent sentences.
For computing the overlap measure, we represent
every sentence as a vector where each dimension
represents a word. The number of times the word
appears in the sentence is the value for that di-
mension. Cosine similarity is computed between
638
the vectors of adjacent sentences and the average
value of the similarity across all pairs of adjacent
sentences is the feature value.
We also run the Stanford Coreference tool
(Raghunathan et al., 2010) to identity pronoun and
entity coreference links within the snippet. The
number of total coreference links, and the number
of intra- and inter-sentence links are added as three
separate features.
Amount of detail (7 features).
To indicate descriptive words, we compute the
number of adjectives and adverbs (two features).
We also include the total number of named enti-
ties (NEs), average length of NEs in words and
the number of sentences that do not have any NEs.
The named entities were identified using the Stan-
ford NER recognition tool (Finkel et al., 2005).
We also use the predictions of a classifier
trained to identify general versus specific sen-
tences. We use a data set of general and spe-
cific sentences and features described in Louis and
Nenkova (2011) to implement a sentence speci-
ficity model. The classifier produces a binary pre-
diction and also a graded score for specificity. We
add two features?the percentage of specific sen-
tences and the average specificity score of words.
Compression likelihood (29 features).
These features use an external source of infor-
mation about content importance. Specifically, we
use data commonly employed to develop statisti-
cal models for sentence compression (Knight and
Marcu, 2002; McDonald, 2006; Galley and McK-
eown, 2007). It consists of pairs of sentences
in an original text and a professional summary
of that text. In every pair, one of the sentences
(source) appeared in the original text and the other
is a shorter version with the superfluous details
deleted. Both sentences were produced by people.
We use the dataset created by Galley and McKe-
own (2007). The sentences are taken from the Ziff
Davis Corpus which contains articles about tech-
nology products. This data also contains align-
ment between the constituency parse nodes of the
source and summary sentence pair. Through the
alignment it is possible to track nodes that where
preserved during compression.
On this data, we identify for every production
in the source sentence whether it undergoes dele-
tion in the compressed sentence. A production
(LHS ? RHS) is said to undergo deletion when
either the LHS node or any of the nodes in the
RHS do not appear in the compressed sentence.
Only productions which involve non-terminals in
the RHS are used for this analysis as lexical items
could be rather corpus-specific. The proportion
of times a production undergoes deletion is called
the deletion probability. We also incorporate fre-
quency of the production with the deletion proba-
bility to obtain a representative set of 25 produc-
tions which are frequently deleted and also occur
commonly. This deletion score is computed as:
deletion probability * log(frequency of production
in source sentences)
Parentheticals appear in the list as would be
expected and also productions involving con-
junctions, prepositional phrases and subordinate
clauses. We expect that such productions will in-
dicate the presence of details that are only appro-
priate for longer texts.
To compute the compression-related features
for a snippet, we first obtain the set of all pro-
ductions in the sentences from the snippet. We
add features that indicate the number of times each
of the top 25 ?most deleted? productions was used
in the snippet. We also use the sum, average and
product of deletion probabilities for set of snippet
productions as features. The product feature gives
the likelihood of the text being deleted. We also
add the perplexity value based on this likelihood,
P
?1/n
where P is the likelihood and n is the num-
ber of productions from the snippet for which we
have deletion information in our data.
2
For training a model, we need texts which we
can assume are written in a concise manner. We
use two sources of data?summaries written by
people and high quality news articles.
4 A classification model on expert
summaries
Here we use a collection of news summaries writ-
ten by expert analysts for four different lengths
and build a classification model to predict given
a snippet what is the length of the summary from
which the snippet was taken. This task only differ-
entiates four lengths but is a useful first approach
for testing our assumptions and features.
4.1 Data
We use human written summaries from the Doc-
ument Understanding Conference (DUC
3
) evalua-
2
Some productions may not have appeared in the Ziff
Davis Corpus.
3
http://duc.nist.gov
639
tion workshops conducted in 2001 and 2002. An
input given for summarization contains 10 to 15
documents on a topic. The person had to create
50, 100, 200 and 400 word summaries for each of
the inputs. These summary writers are retired in-
formation analysts and we can assume that their
summaries are of high quality and concise nature.
Further, the four different length summaries for an
input are produced by the same person.
4
There-
fore differences in length are not confounded by
differences in writing style of different people.
The 2001 dataset has 90 summaries for each of
the four lengths. In 2002, there are 116 summaries
for each length. All of the summaries are abstracts,
i.e. people wrote the summary in their own words,
with the exception of one set. In 2002, abstracts
were only created for 50, 100 and 200 lengths.
However, extracts created by people are available
for 400 words. In extracts, the summary writer
is only allowed to choose complete sentences (no
edits can be done), however, the sentences can be
ordered in the summary and people tend to create
coherent extractive summaries as well. Since it is
desirable to have data for another length, we also
include the 400-word extracts from the 2002 data.
4.2 Snippet selection
We choose 50 words as the snippet length for
our experiment since the length of the shortest
summaries is 50. We experiment with multiple
ways to select a snippet: the first 50 words of the
summary (START), the last 50 words (END) and
50 words starting at a randomly chosen sentence
(RANDOM). However, we do not truncate any sen-
tence in the middle to meet the constraint for 50
words. We allow a leeway of 20 words so that
snippets can range from 30 to 70 words. When a
snippet could not be created within this word limit
(eg. the summary has one sentence which is longer
than 70 words), we ignore the example.
4.3 Classification results
The task is to predict the length of the summary
from which the fixed length snippet was taken, i.e.
4-way classification?50, 100, 200 or a 400 word
summary. We trained an SVM classifier with a ra-
dial basis kernel on the 2001 data. The regulariza-
tion and kernel parameters were tuned using 10-
fold cross validation on the training set. The accu-
racies of classification on the 2002 data are shown
4
Different inputs however may be summarized by differ-
ent assessors.
snippet position accuracy
START 38.4
RANDOM 34.4
END 39.3
Table 2: Length prediction results on DUC sum-
maries
in Table 2. Since there are four equal classes, the
random baseline performance is 25%.
The START and END position snippets gave the
best accuracies, 38% and 39% which are 13-14%
absolute improvement above the baseline. At the
same time, there is much scope for improvement.
The confusion matrices showed that 50 and 400
word lengths, the extreme ones in this dataset,
were the easiest to predict. Most of the confusions
occur with the 100 and 200 word summaries.
The overall accuracy is slightly better when
snippets from the END of the summary are cho-
sen compared to those from the START. However,
with START snippets, better prediction of different
length summaries was obtained, whereas the ac-
curacy in the END case comes mainly from correct
prediction of 50 and 400 word summaries. So we
use the START selection for further experiments.
5 A regression approach based on New
York Times editorials
We next build a model where we predict a wider
range of lengths compared to just the four classes
we had before. Here our training set comprises
news articles from the New York Times (NYT)
based on the assumption that edited news from a
good source would be of high quality overall.
5.1 Data
We obtain the text of the articles from the NYT
Annotated Corpus (Sandhaus, 2008). We choose
the articles from the opinion section of the news-
paper since they are likely to have good topic con-
tinuity and related content compared to general
news which often contain lists of facts. We fur-
ther use only the editorial articles to ensure that
the articles are of high quality.
We collect 10,724 opinion articles from years
2000 to 2007 of the NYT. We divide each article
into topic segments using the unsupervised topic
segmentation method developed by Eisenstein and
Barzilay (2008). We use the following heuristic to
decide on the number of topic segments for each
article. If the article has fewer than 50 sentences,
we create segments such that the expected length
640
of a segment is 10 sentences, i.e, we assign the
number of segments as number of sentences di-
vided by 10. When the article is longer, we create
5 segments. This step gives us 18,167 topic seg-
ments, ranging in length from 14 to 773 words.
We use a stratified sampling method to select
training and test examples. Starting from 90 words
and upto a maximum length of 500 words, we di-
vide the range into bins in increments of 30 words.
From each bin we select 100 texts for training and
around 35 for testing. There are 2,100 topic seg-
ments in the training set and 681 for testing.
5.2 Training approach
We use 100 word snippets for our experiments.
We learn a linear regression model on the train-
ing data using lm function in R (R Development
Core Team, 2011). The features which turned out
significant in the model are shown in Table 3. The
significance value shown is associated with a t-test
to determine if the feature can be ignored from the
model. We report the coefficients for the signifi-
cant features under column ?Beta?. The R-squared
value of the model is 0.219.
Many of the most significant features are related
to entities. Longer texts are associated with larger
number of noun phrases but they tend not to be
proper names. Average word and sentence length
also increase with article length, at the same time,
longer articles have shorter verb phrases. Specific
sentences and determiners are also positively re-
lated to article length. At the discourse level, com-
parison relations increase with length.
5.3 Accuracy of predictions
On the test data, the lengths predicted by the
model have a Pearson correlation of 0.44 with the
true length of the topic segment. The correlation is
highly significant (p-value < 2.2e-16). The Spear-
man correlation value is 0.43 and the Kendall Tau
is 0.29, both also highly significant. These results
show that our model can distinguish content types
for a range of article lengths.
6 Text quality assessment for automatic
summaries
In the models above, we learned weights which re-
late the features to the length of concisely written
human summaries and NYT articles. Now we use
the model to compute verbosity scores and assess
Feature Beta p-value
Positive coefficients
total noun phrases 6.052e+00 ***
avg. word length 3.201e+01 ***
avg. sent. length 3.430e+00 **
avg. NP length 6.557e+00 *
no. of adverbs 4.244e+00 **
% specific sentences 4.773e+01 **
comparison relations 9.296e+00 .
determiners 2.955e+00 *
NP? NP PP 4.305e+00 *
NP? NP NP 1.174e+01 *
PP? IN S 7.268e+00 .
WHNP?WDT 1.196e+01 **
Negative coefficients
NP? NNP -8.630e+00 ***
no. of sentences -2.498e+01 **
no. of relations -1.128e+01 **
avg. VP length -2.982e+00 **
type token ratio -1.784e+02 *
NP? NP , SBAR -1.567e+01 *
NP? NP , NP -9.582e+00 *
NP? DT NN -3.423e+00 .
VP? VBD -1.189e+01 .
S? S : S . -1.951e+01 .
ADVP? RB -4.198e+00 .
Table 3: Significant regression coefficients in the
length prediction model on NYT editorials. ?***?
indicates p-value < 0.001, ?**? is p-value < 0.01,
?*? is < 0.05 and ?.? is < 0.1
how well they correlate with text quality scores as-
signed by people.
We perform this evaluation for the system sum-
maries produced during the 2006 DUC evalua-
tion workshop. There are 22 automatic systems in
that evaluation.
5
Each system produced 250 word
summaries for each of 20 multidocument inputs.
Each summary was evaluated by DUC assessors
for multiple dimensions of quality. We examine
how the verbosity predictions from our model are
related to these summary scores. In this experi-
ment, we use automatic summaries only.
6.1 Gold-standard summary scores
Two kinds of manual scores?content and linguis-
tic quality?are available for each summary from
the DUC dataset. One type of content score,
the ?pyramid score? (Nenkova et al., 2007) com-
putes the overlap of semantic units of the system
summary with that present in human-written sum-
maries for the same input. For the other content
score, called ?content responsiveness?, assessors
directly provide a rating to summaries on a scale
from 1 (very poor) to 5 (very good) without using
any reference human summaries.
5
We use only the set of systems for which pyramid scores
are also available.
641
Verbosity scores Corr. with actual length
predicted length -0.01
verbosity degree -0.29
deviation score -0.27
Table 4: Relationship between verbosity scores
and summary length
Linguistic quality is evaluated separately from
content for different aspects. Manually assigned
scores are available for non-redundancy (absence
of repetitive information), focus (well-established
topic), and coherence (good flow from sentence to
sentence). For each aspect, the summary is rated
on a scale from 1 (very poor) to 5 (very good).
This dataset is less ideal for our task in some
ways as system summaries often lack coherent
arrangement of sentences. Some of our fea-
tures which rely on coreference and adjacent sen-
tence overlaps when computed on these sum-
maries could be misleading. However, this data
contains large scale quality ratings for different
quality aspects which allow us to examine our ver-
bosity predictions across multiple dimensions.
6.2 Verbosity scores and summary quality
We choose the first 100 words of each summary
as the snippet. No topic segmentation was done
on the summary data. We use the NYT regres-
sion model to predict the expected lengths of these
summaries and compute its verbosity and devia-
tion scores as defined in Section 2.
We also compute two other measures for com-
parison.
Actual length. To understand how the ver-
bosity scores are related to the length of the sum-
mary, we also keep track of the actual number of
words present in the summary.
Redundancy score: We also add a simple score
to our analysis to indicate redundancy between ad-
jacent sentences in the summary. It is simple mea-
sure of verbosity since repetitive information leads
to lower informativeness overall. The score is the
cosine similarity based sentence overlap measure
described in Section 3.
For each of the 22 automatic systems, the scores
of its 20 summaries (one for each input) are av-
eraged. (We ignore empty summaries and those
which are much smaller than the 100 word snip-
pet that we require). We find the average val-
ues for both our verbosity based scores above
and the gold-standard scores (pyramid, content re-
sponsiveness, focus, non-redundancy and coher-
Content quality
scores Pyramid Resp.
actual length 0.64* 0.43*
predicted length -0.29 -0.11
verbosity degree -0.47* -0.23
deviation score -0.44* -0.29
redundancy score -0.01 -0.06
Linguistic quality
scores Non-red Focus Coher.
actual length -0.32 -0.25 -0.32
predicted length 0.48* 0.39
.
0.38
.
verbosity degree 0.55* 0.44* 0.46*
deviation score 0.53* 0.40
.
0.42
.
redundancy score 0.06 0.32 0.23
Table 5: Pearson correlations between verbosity
scores and gold standard summary quality scores
ence). We also compute the average value of the
summary lengths for each system.
First we examine the relationship between ver-
bosity scores and the actual summary lengths. The
Pearson correlations between the three verbosity
measures and true length of the summaries are re-
ported in Table 4. The verbosity scores are not sig-
nificantly related to summary length. They seem
to have an inverse relationship but the correlations
are not significant even at 90% confidence level.
This result supports our hypothesis that verbosity
scores based on expected length are different from
the actual summary length.
Next Table 5 presents the Pearson correlations
of the verbosity measures with gold standard sum-
mary quality scores. Since the number of points
(systems) is only 22, we indicate whether the cor-
relations are significant at two levels, 0.05 (marked
by a ?*? superscript) and 0.1 (a ?.? superscript).
The first line of the table indicates that longer
summaries are associated with higher content
scores both according to pyramid and content re-
sponsiveness evaluations. This result also supports
our hypothesis that length alone does not indicate
verbosity. Longer summaries on average have bet-
ter content quality. The length is not significantly
related to linguistic quality scores but there is a
negative relationship in general.
On the other hand, all the three verbosity scores
have a negative correlation with content scores.
The verbosity degree score is the strongest in-
dicator of summary quality with -0.47 (signifi-
cant) correlation with pyramid score. At the same
time however, verbosity is preferred for linguis-
tic quality. This effect could arise due to the fact
these summaries are bags of unordered sentences.
Therefore verbose style could be perceived as hav-
642
System 23?s summary: Actual length = 253 words, Predicted length = 343 words, Verbosity degree = 90
A senior Scotland Yard police officer apologized to the parents of a black teenager slain five years ago in a race killing
that has become the focus of debate over relations between police and ethnic minorities. Black teenager Stephen Lawrence
was stabbed to death at a bus-stop in Eltham, south London by five white youngsters six years ago. The parents of the
murdered black teenager Stephen Lawrence began legal action against the men suspected of his killing. Two suspects in the
Stephen Lawrence murder case and one other man were arrested on suspicion of theft by Kent Police. The five men
suspected of killing Stephen Lawrence were thumped and pelted with bottles by an enraged crowd Tuesday after a day of
evasive and implausible evidence that made a mockery of their appearance before the public inquiry. The dawn raids came
as police questioned three men in connection with the country?s most notorious racist crime: the unsolved 1993 murder of
black teenager Stephen Lawrence. A public inquiry after the Lawrence case found London police institutionally racist,
prompting a government pledge to take a more active role in combating racial intolerance. The report, commissioned after
police botched the investigation into the 1993 racially motivated murder of a black teenager, Stephen Lawrence has put
pressure on Sir Paul Condon, the Metropolitan Police chief, to resign. British authorities and police have learned from
the 1993 murder of black teen-ager Stephen Lawrence by a gang of white youths and the failure of the police to
System 18?s summary: Actual length = 244 words, Predicted length = 597 words, Verbosity degree = 353
The government, which has received praise from backers of the Lawrence family for its pursuit of the case, came in for
criticism on Monday for actions it took this weekend to prevent publication of a leaked version of the report, which is
due to be made public on Wednesday. Sir William Macpherson, a retired High Court justice who was the author of the
report and chairman of the eight-month government inquiry, defined institutional racism as ?the collective failure of an
organization to provide an appropriate professional service to people because of their color, culture or ethnic origin?
reflected, he said, in ?processes, attitudes and behavior which amounts to discrimination through unwitting prejudice
ignorance, thoughtlessness and racist stereotyping.? Richard Norton-Taylor, whose play about Lawrence?s killing, ?The
Color of Justice,? has been playing to rave reviews in London, said that the attention paid to the Lawrence case and
others was a sign that British attitudes toward the overarching authority of the police and other institutions were
finally being called into question. She said British authorities and police have learned from the 1993 murder of black
teenager Stephen Lawrence by a gang of white youths and the failure of the police to investigate his death adequately
A senior Scotland Yard police officer Wednesday apologized to the parents of a black teenager slain five years ago in a
race killing that has become the focus of debate over relations between police and ethnic minorities.
Table 6: Summaries produced by two systems for input D0624 (DUC 2006) shown with the verbosity
scores from our model
ing greater coherence compared to short and suc-
cinct sentences which are jumbled such that it is
hard to decipher the full story.
The simple redundancy score (last row of the
table) does not have any significant relationship
to quality scores. One reason could be that most
summarization systems make an effort to reduce
redundant information (Carbonell and Goldstein,
1998) and therefore a simple measure of word
overlap is not helpful for distinguishing quality.
As examples of the predictions from our model,
Table 6 shows two summaries produced for the
same input by two different systems. They both
have almost the same actual length but the first re-
ceived a prediction close to its actual length while
the other is predicted with a much higher verbosity
degree score. Intuitively, the second example is
more verbose compared to the first one. According
to the manual evaluations as well, the first sum-
mary receives a higher score of 0.4062 (pyramid)
compared to 0.2969 for the second summary.
7 Conclusions
There are several ways in which our approach can
be improved. In this first work, we have avoided
the complexities of manual annotation. In fu-
ture, we will explore the feasibility of human an-
notations of verbosity on a suitable corpus, such
as news articles on the same topic from different
sources. In addition, our current approach only
considers a snippet of the text or topic segment
during prediction but ignores the writing in the re-
maining text. In future work, we plan to use a slid-
ing window to obtain and aggregate length predic-
tions while considering the full text.
Acknowledgements
This work was partially supported by a NSF CA-
REER 0953445 award. We also thank the anony-
mous reviewers for their comments.
643
References
J. Carbonell and J. Goldstein. 1998. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SIGIR,
pages 335?336.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: a test of grounded language acquisition.
In Proceedings of ICML, pages 128?135.
J. M. Conroy and H. T. Dang. 2008. Mind the gap:
Dangers of divorcing evaluations of summary con-
tent from linguistic quality. In Proceedings of COL-
ING, pages 145?152.
C. Danescu-Niculescu-Mizil, J. Cheng, J. Kleinberg,
and L. Lee. 2012. You had me at hello: How phras-
ing affects memorability. In Proceedings of ACL,
pages 892?901.
J. Eisenstein and R. Barzilay. 2008. Bayesian un-
supervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL, pages 363?370.
M. Galley and K. McKeown. 2007. Lexicalized
markov grammars for sentence compression. In
Proceedings of HLT-NAACL.
V. Ganjigunte Ashok, S. Feng, and Y. Choi. 2013. Suc-
cess with style: Using writing style to predict the
success of novels. In Proceedings of EMNLP, pages
1753?1764.
D. Graff. 2002. The AQUAINT Corpus of English
News Text. Corpus number LDC2002T31, Linguis-
tic Data Consortium, Philadelphia.
M. Kaisser, M. A. Hearst, and J. B. Lowe. 2008. Im-
proving search results quality by customizing sum-
mary lengths. In Proceedings of ACL-HLT, pages
701?709.
D. Klein and C.D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, pages 423?
430.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1).
H. Lakkaraju, J. J. McAuley, and J. Leskovec. 2013.
What?s in a name? understanding the interplay be-
tween titles, content, and communities in social me-
dia. In ICWSM.
A. Louis and A. Nenkova. 2011. Automatic identifica-
tion of general and specific sentences by leveraging
discourse annotations. In Proceedings of IJCNLP,
pages 605?613.
A. Louis and A. Nenkova. 2013. What makes writing
great? first experiments on article quality prediction
in the science journalism domain. TACL, 1:341?
352.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The pyramid method: Incorporating human con-
tent selection variation in summarization evaluation.
ACM Trans. Speech Lang. Process., 4(2):4.
M. O?Donnell. 1997. Variable-length on-line docu-
ment generation. In Proceedings of the 6th Euro-
pean Workshop on Natural Language Generation.
E. Pitler and A. Nenkova. 2009. Using syntax to dis-
ambiguate explicit discourse connectives in text. In
Proceedings of ACL-IJCNLP, pages 13?16.
E. Pitler, A. Louis, and A. Nenkova. 2010. Automatic
evaluation of linguistic quality in multi-document
summarization. In Proceedings of ACL.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Founda-
tion for Statistical Computing.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010. A
multi-pass sieve for coreference resolution. In Pro-
ceedings of EMNLP, pages 492?501.
E. Sandhaus. 2008. The New York Times Annotated
Corpus. Corpus number LDC2008T19, Linguistic
Data Consortium, Philadelphia.
644
Automatically Assessing Machine Summary
Content Without a Gold Standard
Annie Louis?
University of Pennsylvania
Ani Nenkova??
University of Pennsylvania
The most widely adopted approaches for evaluation of summary content follow some protocol
for comparing a summary with gold-standard human summaries, which are traditionally called
model summaries. This evaluation paradigm falls short when human summaries are not available
and becomes less accurate when only a single model is available. We propose three novel
evaluation techniques. Two of them are model-free and do not rely on a gold standard for the
assessment. The third technique improves standard automatic evaluations by expanding the set
of available model summaries with chosen system summaries.
We show that quantifying the similarity between the source text and its summary with
appropriately chosen measures produces summary scores which replicate human assessments
accurately. We also explore ways of increasing evaluation quality when only one human model
summary is available as a gold standard. We introduce pseudomodels, which are system sum-
maries deemed to contain good content according to automatic evaluation. Combining the
pseudomodels with the single human model to form the gold-standard leads to higher correlations
with human judgments compared to using only the one available model. Finally, we explore
the feasibility of another measure?similarity between a system summary and the pool of all
other system summaries for the same input. This method of comparison with the consensus of
systems produces impressively accurate rankings of system summaries, achieving correlation
with human rankings above 0.9.
1. Introduction
In this work, we present evaluation metrics for summary content which make use of
little or no human involvement. Evaluation methods such as manual pyramid scores
(Nenkova, Passonneau, and McKeown 2007) and automatic ROUGE scores (Lin and
Hovy 2003) rely on multiple human summaries as a gold standard (model) against
which they compare a summary to assess how informative the candidate summary
is. It is desirable that evaluation of similar quality be done quickly and cheaply
? E-mail: lannie@seas.upenn.edu.
?? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut St.,
Philadelphia, PA 19104. E-mail: nenkova@seas.upenn.edu.
Submission received: 18 June 2011; revised submission received: 23 March 2012; accepted for publication:
18 April 2012.
doi:10.1162/COLI a 00123
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 2
on non-standard test sets that have few or no human summaries, or on large test
sets for which creating human model summaries is infeasible. In our work, we aim
to identify indicators of summary content quality that do not make use of human
summaries but can replicate scores based on comparison with a gold standard very
accurately.
Such indicators would need to be easily computable from existing resources and
to provide rankings of systems that agree with rankings obtained through human
judgments. There have been some early proposals for alternative methods. Donaway,
Drummey, and Mather (2000) propose that a comparison of the source text with a
summary can tell us how good the summary is. A summary that has higher similarity
with the source text can be considered better than one with lower similarity. Radev and
Tam (2003) perform a large scale evaluation with thousands of test documents. Their
work is set up in a search engine scenario. They first rank the test documents using the
search engine. Then they perform the same experiment now substituting the summaries
from one system in place of the original documents. The systemwhose summaries have
the most similar ranking as that generated for the full documents is considered the
best system because not much information loss is introduced by the summarization
process.
But these methods did not gain much popularity and their performance was never
compared to human evaluations. Part of the reason is that only in the last decade
have several large data sets with system summaries and their ratings from human
judges become available for performing such studies. Our work is the first to provide
a comprehensive report of the strengths of such approaches and we show that human
ratings can be reproduced by these fully automatic metrics with high accuracy. Our
results are based on data for multi-document news summarization.
The key insights of our approach can be summarized as follows:
Input?summary similarity:Good summaries are representative of the input and so one
would expect that the more similar a summary is to the input, the better its content.
Identifying a suitable input?summary similarity metric will provide a means for fully
automatic evaluation of summaries. We present a quantitative analysis of this hypothe-
sis and show that input?summary similarity is highly predictive of scores assigned by
humans for the summaries. The choice of an appropriate metric to measure similarity is
critical, however, and we show that information-theoretic measures turn out to be the
most powerful for this task (Section 4).
Addition of pseudomodels: Having a larger number of model summaries has been
shown to give more stable evaluation results, but for some data sets only a single model
summary is available. We test the utility of pseudomodels, which are system summaries
that are chosen to be added to the human summary pool and that are used as additional
models. We find that augmenting the gold standard with pseudomodels helps obtain
better correlations with human judgments than if a single model is used (Section 5).
System summaries as models: Most current summarization systems perform content
selection reasonably well. We examine an approach to evaluation that exploits system
output and considers all system summaries for a given input as a gold standard (Sec-
tion 6). We find that similarity between a summary and such a gold standard constitutes
a powerful automatic evaluation measure. The correlation between this measure and
human evaluations is over 0.9.
We analyze a number of similarity metrics to identify the ones that perform best
for automatic evaluation. The tool we developed, SIMetrix (Summary Input similarity
268
Louis and Nenkova Automatic Content Evaluation
Metrics), is freely available.1 We test these resource-poor approaches to predict sum-
mary content scores assigned by human assessors. We evaluate the results on data from
the Text Analysis Conferences.2
We find that our automatic methods to estimate summary quality are highly predic-
tive of human judgments. Our best result is 0.93 correlation with human rankings using
no model summaries and this is on par with automatic evaluation methods that do use
human summaries. Our study provides some direction towards alternative methods
of evaluation on non-standard test sets. The goal of our methods is to aid system
development and tuning on new, especially large, data sets using little resources. Our
metrics complement but are not intended to replace existing manual and automatic
approaches to evaluation wherein the latter?s strength and reliability are important
for high confidence evaluations. Some of our findings are also relevant for system
development as we identify desirable properties of automatic summaries that can be
computed from the input (see Section 4). Our results are also strongly suggestive that
system combination has the potential for improving current summarization systems
(Section 6).
We start out with an outline of existing evaluation methods and the potential
shortcomings of these approaches which we wish to address.
2. Current Content Evaluation Methods
Summary quality is defined by two key aspects?content and linguistic quality. A good
summary should contain the most important content in the input and also structure
the content and present it as well-written text. Several methods have been proposed for
evaluating system-produced summaries; some only assess content, others only linguis-
tic quality, and some combine assessment of both. Some of these approaches are manual
and others can be performed automatically.
In our work, we consider the problem of automatic evaluation of content quality.
To establish the context for our work, we provide an overview of current content
evaluation methods used at the annual evaluations run by NIST.
The Text Analysis Conference (TAC, previously called the Document Understand-
ing Conference [DUC]3) conducts large scale evaluation of automatic systems on dif-
ferent summarization tasks. These conferences have been held every year since 2001
and the test sets and evaluation methods adopted by TAC/DUC have become the
standard for reporting results in publications. TAC has employed a range of manual
and automatic metrics over the years.
Manual evaluations of the systems are performed at NIST by trained assessors.
The assessors score the summaries either
a) by comparing with a gold-standard summary written by humans, or
b) by providing a direct rating on a scale (1 to 5 or 1 to 10).
The human summaries against which other summaries are compared are inter-
changeably called models, gold standards, and references. Within TAC, they are typ-
ically calledmodels.
1 SIMetrix can be downloaded at http://www.seas.upenn.edu/?lannie/IEval2.html.
2 http://www.nist.gov/tac/.
3 http://duc.nist.gov/.
269
Computational Linguistics Volume 39, Number 2
2.1 Content Coverage Scores
The methods relying on a gold standard have evolved over the years. In the first
years of DUC, a single model summary was used. System summaries were evaluated
by manually assessing how much of the model?s content is expressed in the system
summary. Each clause in the model represents one unit for the evaluation. For each of
these clauses, assessors specify the extent to which its content is expressed in a given
system summary. The average degree to which the model summary?s clauses overlap
with the system summary?s content is called coverage. These coverage scores were
taken as indicators of content quality for the system summaries.
Different people include very different content in their summaries, however, and
so the coverage scores can vary depending on which model is used (Rath, Resnick, and
Savage 1961). This problem of bias in evaluation was later addressed by the pyramid
technique, which combines information from multiple model summaries to compose
the reference for evaluation. Since 2005, the pyramid evaluation method has become
standard.
2.2 Pyramid Evaluation
The pyramid evaluation method (Nenkova and Passonneau 2004) has been developed
for reliable and diagnostic assessment of content selection quality in summarization and
has been used in several large scale evaluations (Nenkova, Passonneau, and McKeown
2007). It uses multiple human models from which annotators identify semantically
defined Summary Content Units (SCUs). Each SCU is assigned a weight equal to
the number of human model summaries that express that SCU. An ideal maximally
informative summary would express a subset of the most highly weighted SCUs, with
multiple maximally informative summaries being possible. The pyramid score for a
system summary S is equal to the following ratio:
py(S) =
sum of weights of SCUs expressed in S
sum of weights of an ideal summary with the same number of SCUs as S
(1)
In this way, a more reliable score for a summary is obtained usingmultiple reference
summaries. Four human summaries are normally used for pyramid evaluation at TAC.
2.3 Responsiveness Evaluation
Responsiveness of a summary is a measure of overall quality combining both content
selection and linguistic quality. It measures to what extent summaries convey appropri-
ate content in a structured fashion. Responsiveness is assessed by direct ratings given
by the judges. For example, a scale of 1 (poor summary) to 5 (very good summary) is
used and these assessments are done without reference to any model summaries.
Pyramid and responsiveness are the standardly used manual approaches for
content evaluation. They produce rather similar rankings of systems at TAC. The
(Spearman) correlation between the two for ranking systems that participated in
the TAC 2009 conference is 0.85 (p-value 6.8e-16, 53 systems). The responsiveness
measure involves some aspects of linguistic quality whereas the pyramid metric was
designed for content only. Such high correlation indicates that the content factor has
270
Louis and Nenkova Automatic Content Evaluation
substantial influence on the responsiveness judgments, however. The high correlation
also indicates that two types of human judgments made on very different basis?
gold-standard summaries and direct judgments?can agree and provide fairly similar
rankings of summaries.
2.4 ROUGE
Manual evaluation methods require significant human effort. Moreover, the pyramid
evaluation involves detailed annotation for identifying SCUs in human and system
summaries and requires training of assessors to perform the evaluation. Outside of
TAC, therefore, system developments and results are regularly reported using ROUGE,
a suite of automatic evaluation metrics (Lin and Hovy 2003; Lin 2004b).
ROUGE automates the comparison betweenmodel and system summaries based on
n-gram overlaps. These overlap scores have been shown to correlate well with human
assessment (Lin 2004b) and so ROUGE removes the need for manual judgments in this
part of the evaluation.
ROUGE scores are computed typically using unigram (R1) or bigram (R2) overlaps.
In TAC, four human summaries are used as models and their contents are combined
for computing the overlap scores. For fixed length summaries, the recall from the
comparison is used as the quality metric. Other metrics such as longest subsequence
match are also available. Another ROUGE variant is RSU4, which computes the overlap
in terms of skip bigrams, where two unigrams with a gap of up to four intervening
words are considered as bigrams. This latter metric provides some additional flexibility
compared to the stricter R2 scores.
The correlations between ROUGE and manual evaluations for systems in TAC
2009 are shown in Table 1 and vary between 0.76 and 0.94 for the different variants.4
Here, and in all subsequent experiments, Spearman correlations are computed using
the R toolkit (R Development Core Team 2011). In this implementation, significance
values for the correlations are produced using the AS 89 algorithm (Best and Roberts
1975).
These correlations are highly significant and show that ROUGE is a high perfor-
mance automatic evaluation metric.
We can consider the ROUGE results as the upper bound of performance for the
model-free evaluations that we propose because ROUGE involves direct comparison
with the gold-standard summaries. Our metrics are designed to be used when model
summaries are not available.
2.5 Automatic Evaluation Without Gold-Standard Summaries
All of thesemethods require significant human involvement. In evaluations where gold-
standard summaries are needed, assessors first read the input documents (10 or more
per input) and write a summary. Then manual comparison of system and gold standard
is done, which takes additional time. Gillick and Liu (2010) hypothesize that at least
17.5 hours are needed to evaluate two systems under this set up on a standard test
set. Moreover, multiple gold-standard summaries are needed for the same input, so
different assessors have to read and create summaries. The more reliable evaluation
4 The scores were computed after stemming but stop words were retained in the summaries.
271
Computational Linguistics Volume 39, Number 2
Table 1
Spearman correlation between manual scores and ROUGE metrics on TAC 2009 data
(53 systems). All correlations are highly significant with p-value < 10?10.
ROUGE variant Pyramid Responsiveness
ROUGE-1 0.88 0.76
ROUGE-2 0.94 0.82
ROUGE-SU4 0.92 0.79
methods such as pyramid involve even more annotations at the clause level. Although
responsiveness does not require gold-standard summaries, in a system development
setting, responsiveness judgments are resource-intensive. It requires judges to directly
assign scores to summaries, so humans are in the loop each time the evaluation needs
to be done, making it rather costly. For ROUGE, however, once the human summaries
are created, the scores can be computed automatically for repeated system development
runs. This benefit has made ROUGE immensely popular. But the initial investment of
time for gold-standard creation is still necessary.
Another important point is that for TAC, the gold standards are created by trained
assessors at NIST. Non-expert evaluation options such asMechanical Turk have recently
been explored by Gillick and Liu (2010). They provided annotators with gold-standard
references and system summaries and asked them to score the system summaries on a
scale from 1 to 10 with respect to how well they convey the same information as the
models. They analyzed how these scores are related to responsiveness judgments given
by the expert TAC assessors. The study assessed only eight automatic systems from
TAC 2009 and the correlation between the ratings from experts and Mechanical Turk
annotations was 0.62 (Spearman). The analysis concludes that evaluations produced in
this way tend to be noisy.
One reason was that non-expert annotators were quite influenced by the readability
of the summaries. For example, they tended to assign high scores to the baseline
summary that picks the lead paragraph. The baseline summary, however, is ranked
by expert annotators as low in responsiveness compared to other systems? summaries.
Further, the non-expert evaluation led to few significant differences in the system rank-
ings (score of system A is significantly greater/lesser than that of B) compared with the
TAC evaluations of the same systems.
Another problemwith non-expert evaluation is the quality of themodel summaries.
Evaluations based on model summaries assume that the gold standards are of high
quality. Through the years at TAC, considerable effort has been invested to ensure that
the evaluation scores do not vary depending on the particular gold standard. In the
early years of TAC only one gold-standard summary was used. During this time, papers
reported ANOVA tests examining the factors that most influenced summary scores
from the evaluations and found that the identity of the judge turned out to be the most
significant factor (McKeown et al 2001; Harman andOver 2004). But it is desirable that a
model summary or a human judgment be representative of important content in general
and does not depict the individual biases of the person who created the summary or
made the judgment. So the evaluationmethodologywas refined to remove the influence
of the assessor identity on the evaluation. The pyramid evaluation was also developed
with this goal of smoothing out the variation between judges. Gillick and Liu (2010)
point out that Mechanical Turk evaluations have this undesirable outcome: The identity
272
Louis and Nenkova Automatic Content Evaluation
of the judges turns out to be the most significant factor influencing summary scores.
Gillick and Liu do not elicit model summaries, only direct judgments on quality. We
suspect that the task would only be harder if model summaries were to be created by
non-experts.
The problem that has been little addressed by any of these discussed metrics is eval-
uation when there are no gold-standard summaries available. Systems are developed
by fine-tuning on the TAC data sets, but in non-TAC data sets in novel or very large
domains model summaries may not be available. Even though ROUGE provides good
performance in automatic evaluation, it is not usable under these conditions. Further,
pyramid and ROUGE use multiple gold-standard summaries for evaluation (ROUGE
correlates with human judgments better when computed using multiple models; we
discuss this aspect further in Section 5) so even a single gold-standard summary may
not be sufficient for reliable evaluation.
In our work, we propose fully automatic methods for content evaluation which
can be used in the absence of human summaries. We also explore methods to further
improve the evaluation performance when only one model summary is available.
3. Data and Evaluation Plan
In this section, we describe the data we use throughout our article. We carry out
our analysis on the test sets and system scores from TAC 2009. TAC 2009 is also the
year when NIST introduced a special track called AESOP (Automatically Evaluating
Summaries of Peers). The goal of AESOP is to identify automatic metrics that correlate
well with human judgments of summary quality.
We use the data from the TAC 2009 query focused-summarization task.5 Each input
consists of ten news documents. In addition, the user?s information needs associated
with each input is given by a query statement consisting of a title and narrative. An
example query statement is shown here:
Title: Airbus A380
Narrative: Describe developments in the production and launch of the Airbus A380.
A system must produce a summary that addresses the information required by the
query. The maximum length for summaries is 100 words.
The test set contains 44 inputs, and 53 automatic systems (including baselines)
participated that year. These systems were manually evaluated for content using both
pyramid and responsiveness methods. In TAC 2009, two oracle systems were intro-
duced during evaluation whose outputs are in fact summaries created by people. We
ignore these two systems and use only the automatic participant submissions and the
automatic baseline systems.
As a development set, we use the inputs, summaries, and evaluations from the
previous year, TAC 2008. There were 48 inputs in the query-focused task in 2008 and
58 automatic systems participated.
TAC 2009 also involved an update summarization task and we obtained similar
results on the summaries from this task. In this article, for clarity we only present results
5 http://www.nist.gov/tac/2009/Summarization/update.summ.09.guidelines.html.
273
Computational Linguistics Volume 39, Number 2
on evaluating the query-focused summaries, but the update task results are described
in detail in Louis and Nenkova (2008, 2009a, 2009c).
3.1 Evaluating Automatic Metrics
For each of our proposed metrics, we need to assess their performance in replicating
manually produced rankings given by the pyramid and responsiveness evaluations.
We use two measures to compare these human scores for a system with the automatic
scores from one of our metrics:
a) SPEARMAN CORRELATION: Reporting correlations with human evaluation metrics
is the norm for validating automatic metrics. We report Spearman correlation, which
compares the rankings of systems produced by the two methods instead of the actual
scores assigned to systems.
b) PAIRWISE ACCURACY: To complement correlation results with numbers that have
easier intuitive interpretation, we also report the pairwise accuracy of our metrics in
predicting the human scores. For every pair of systems (A, B), we examine whether
their pairwise ranking (eitherA > B,A < B, orA = B) according to the automatic metric
agrees with the ranking of the same pair according to human evaluation. If it does, the
pair is concordant with human judgments. The pairwise accuracy is the percentage of
concordant pairs out of the total system pairs. This accuracy measure is more inter-
pretable than correlations in terms of the errors made by a metric. A metric with 90%
accuracy incorrectly flips 10% of the pairs, on average, in a ranking it produces. This
measure is inspired by the Kendall tau coefficient.
We test themetrics for success in replicating human scores overall across the full test
set as well as identifying good and bad summaries for individual inputs. We therefore
report the correlation and accuracy of our metrics at the following two levels.
a) SYSTEM LEVEL (MACRO): The average score for a system is computed over the entire
set of test inputs using both manual and our automatic methods. The correlations
between ranks assigned to systems by these average scores will be indicative of the
strength of our features to predict overall system rankings on the test set. Similarly, the
pairwise accuracies are computed using the average scores for the systems in the pair.
b) INPUT LEVEL (MICRO): For each individual input, we compare the rankings for the
system summaries using manual and automatic evaluations. Here the correlation or
accuracy is computed for each input. For correlations, we report the percentage of inputs
for which significant correlations (p-value < 0.05) were obtained. For accuracy, the
systems are paired within each input. Then these pairs for all the inputs are put together
and the fraction of concordant pairs is computed. Micro-level analysis highlights the
ability of an evaluation metric to identify good and poor quality system summaries
produced for a specific input and this task is bound to be harder than system level
predictions. For example, even with wrong prediction of rankings on a few inputs, the
average scores (macro-level) for a system might not be affected.
In the following sections, we describe three experiments in which we analyze the
possibility of performing automatic evaluation involving only minimal or no human
judgments: Using input?summary similarity (Section 4), using system summaries as
pseudomodels alongside gold-standard summaries created by people (Section 5), and
using the collection of system summaries as a gold standard (Section 6). All the auto-
matic systems, including baselines, were evaluated.
274
Louis and Nenkova Automatic Content Evaluation
4. Input?Summary Similarity: Evaluation Using Only the Source Text
Here we present and evaluate a suite of metrics which do not require gold-standard
human summaries for evaluation. The underlying intuition is that good summaries will
tend to be similar to the input in terms of content. Accordingly, we use the similarity of
the distribution of terms in the input and summaries as a measure of summary content.
Although the motivation for this metric is highly intuitive, it is not clear how simi-
larity should be defined for this particular problem. Here we provide a comprehensive
study of input?summary similarity metrics and show that some of these measures
can indeed be very accurate predictors of summary quality even while using no gold-
standard human summaries at all.
Prior to our work, the proposal for using the input for evaluation has been brought
up in a few studies. These studies did not involve a direct evaluation of the capacity
of input?summary similarity to replicate human ratings, however, and they did not
compare similarity metrics for the task. Because large scale manual evaluation results
are available now, our work is the first to evaluate this possibility in a direct manner
and involving study of correlations with different types of human evaluations. In the
following section we detail some of the prior studies on input?summary similarity for
summary evaluation.
4.1 Related Work
One of the motivations for using the input text rather than gold-standard summaries
comes from the need to perform large scale evaluations with test sets comprised of
thousands of inputs. Creating human summaries for all of themwould be an impossible
task indeed.
In Radev and Tam (2003), therefore, a large scale fully automatic evaluation of eight
summarization systems on 18,000 documents was performed without any human effort
by using the idea of input?summary similarity. A search engine was used to rank docu-
ments according to their relevance to a given query. The summaries for each document
were also ranked for relevance with respect to the same query. For good summarization
systems, the relevance ranking of summaries is expected to be similar to that of the
full documents. Based on this intuition, the correlation between relevance rankings
of summaries and original documents was used to compare the different systems. A
system whose summaries obtained highly similar rankings to the original documents
can be considered better than a system whose rankings have little agreement.
Another situation where input?summary similarity was hypothesized as a possible
evaluation was in work concerned with reducing human bias in evaluation. Because
humans vary considerably in the content they include for the same input (Rath,
Resnick, and Savage 1961; van Halteren and Teufel 2003), rankings of systems are
rather different depending on the identity of the model summary used (also noted
by McKeown et al [2001] and Jing et al [1998]). Donaway, Drummey, and Mather
(2000) therefore suggested that there are considerable benefits to be had in adopting a
method of evaluation that does not require human gold standards but instead directly
compares the original document and its summary. In their experiments, Donaway,
Drummey, and Mather demonstrated that the correlations between manual evaluation
using a gold-standard summary and
a) manual evaluation using a different gold-standard summary
275
Computational Linguistics Volume 39, Number 2
b) automatic evaluation by directly comparing input and summary6
are the same. Their conclusion was that such automatic methods should be seriously
considered as an alternative to evaluation protocols built around the need to compare
with a gold standard.
These studies, however, do not directly assess the performance of input?summary
similarity for ranking systems. In Louis and Nenkova (2009a), we provided the first
study of several metrics for measuring similarity for this task and presented correla-
tions of these metrics with human produced rankings of systems. We have released a
tool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similarity
metrics that we explored.7
4.2 Metrics for Computing Similarity
In this section, we describe a suite of similarity metrics for comparing the input and
summary content. We use cosine similarity, which is standard for many applications.
The other metrics fall under three main classes: distribution similarity, summary likeli-
hood, and use of topic signature words. The distribution similarity metrics compare the
distribution of words in the input with those in the summary. The summary likelihood
metrics are based on a generative model of word probabilities in the input and use
the model to compute the likelihood of the summary. Topic signature metrics focus on a
small set of descriptive and topical words from the input and compare them to summary
content rather than using the full vocabulary of the input.
Both input and summary words were stopword-filtered and stemmed before com-
puting the features.
4.2.1 Distribution Similarity. Measures of similarity between two probability distribu-
tions are a natural choice for our task. One would expect good summaries to be charac-
terized by low divergence between probability distributions of words in the input and
summary, and by high similarity with the input.
We experimented with three common measures: Kullback Leibler (KL) divergence,
Jensen Shannon (JS) divergence, and cosine similarity.
These three metrics have already been applied for summary evaluation, albeit in
a different context. In their study of model-based evaluation, Lin et al (2006) used KL
and JS divergences to measure the similarity between human and machine summaries.
They found that JS divergence always outperformed KL divergence. Moreover, the per-
formance of JS divergence was better than standard ROUGE scores for multi-document
summarization when multiple human models were used for the comparison.
The use of input?summary similarity in Donaway, Drummey, and Mather (2000),
which we described in the previous section, is more directly related to our work. But
here, inputs and summaries were compared using only one metric: cosine similarity.
Kullback Leibler (KL) divergence: The KL divergence between two probability distri-
butions P and Q is given by
D(P||Q) =
?
w
pP(w) log2
pP(w)
pQ(w)
(2)
6 They used cosine similarity to perform the input?summary comparison.
7 http://www.seas.upenn.edu/?lannie/IEval2.html.
276
Louis and Nenkova Automatic Content Evaluation
It is defined as the average number of bits wasted by coding samples belonging to P
using another distribution Q, an approximate of P. In our case, the two distributions
of word probabilities are estimated from the input and summary, respectively. Because
KL divergence is not symmetric, both input?summary and summary?input divergences
are introduced as metrics. In addition, the divergence is undefined when pP(w) > 0 but
pQ(w) = 0. We perform simple smoothing to overcome the problem.
p(w) =
C+ ?
N + ? ? B (3)
Here C is the count of word w and N is the number of tokens; B = 1.5|V|, where V
is the input vocabulary and ? was set to a small value of 0.0005 to avoid shifting too
much probability mass to unseen events.
Jensen Shannon (JS) divergence: The JS divergence incorporates the idea that the dis-
tance between two distributions cannot be very different from the average of distances
from their mean distribution. It is formally defined as
J(P||Q) = 1
2
[D(P||A)+D(Q||A)], (4)
where A = P+ Q2 is the mean distribution of P and Q. In contrast to KL divergence,
the JS distance is symmetric and always defined. We compute both smoothed and
unsmoothed versions of the divergence as summary scores.
Vector space similarity: The third metric is cosine overlap between the tf ? idf vector
representations of input and summary contents.
cos? =
vinp.vsumm
||vinp|| ||vsumm||
(5)
We compute two variants:
1. Vectors contain all words from input and summary.
2. Vectors contain only topic signature words from the input and all words of
the summary.
Topic signatures are words highly descriptive of the input, as determined by the
application of the log-likelihood test (Lin and Hovy 2000). Using only topic signatures
from the input to represent text is expected to be more accurate because the reduced
vector has fewer dimensions compared with using all the words from the input.
4.2.2 Summary Likelihood. For this approach, we view summaries as being generated
according to word distributions in the input. Then the probability of a word in the input
would be indicative of how likely it is to be emitted into a summary. Under this gen-
erative model, the likelihood of a summary?s content can be computed using different
methods and we expect the likelihood to be higher for better quality summaries.
We compute both a summary?s unigram probability as well as its probability under
a multinomial model.
277
Computational Linguistics Volume 39, Number 2
Unigram summary probability:
(pinpw1)
n1 (pinpw2)
n2 ...(pinpwr)
nr (6)
where pinpwi is the probability in the input of word wi, ni is the number of times wi
appears in the summary, and w1. . .wr are all words in the summary vocabulary.
Multinomial summary probability:
N!
n1!n2! . . . nr!
(pinpw1)
n1 (pinpw2)
n2 . . . (pinpwr)
nr (7)
where N = n1 + n2 + . . .+ nr is the total number of words in the summary.
4.2.3 Use of Topic Words in the Summary. Summarization systems that directly optimize
the number of topic signature words during content selection have fared very well
in evaluations (Conroy, Schlesinger, and O?Leary 2006). Hence the number of topic
signatures from the input present in a summary might be a good indicator of summary
content quality. In contrast to the previous methods, by limiting to topic words, we use
only a representative subset of the input?s words for comparing with summary content.
We experiment with two features that quantify the presence of topic signatures in a
summary:
1. The fraction of the summary composed of input?s topic signatures.
2. The percentage of topic signatures from the input that also appear in the
summary.
Although both features will obtain higher values for summaries containing many
topic words, the first is guided simply by the presence of any topic word and the second
measures the diversity of topic words used in the summary.
4.2.4 Feature Combination Using Linear Regression. We also evaluated the performance
of a linear regression metric combining all of these features. During development, the
value of the regression-based score for each summary was obtained using a leave-one-
out approach. For a particular input and system-summary combination, the training set
consisted only of examples which included neither the same input nor the same system.
Hence during training, no examples of either the test input or system were seen.
4.3 Results
We first present an analysis of all the similarity metrics on our development data,
TAC?08. In the next section, we analyze the performance of our two best features on
the TAC?09 data set.
4.3.1 Feature Analysis: Which Similarity Metric is Best?. Table 2 shows the macro-level
Spearman correlations between manual and automatic scores averaged across the
48 inputs in TAC?08.
Overall, we find that both distribution similarity and topic signature features pro-
duce system rankings very similar to those produced by humans. Summary likelihood,
on the other hand, turns out to not be predictive of content selection performance. The
278
Louis and Nenkova Automatic Content Evaluation
Table 2
Spearman correlation on the macro level for TAC?08 data (58 systems). All results are highly
significant with p-values < 0.000001 except unigram and multinomial summary probability,
which are not significant even at the 0.05 level.
Features Pyramid Responsiveness
JS div ?0.880 ?0.736
JS div smoothed ?0.874 ?0.737
% of input topic words 0.795 0.627
KL div summary?input ?0.763 ?0.694
cosine overlap, all words 0.712 0.647
% of summary = topic words 0.712 0.602
cosine overlap, topic words 0.699 0.629
KL div input?summary ?0.688 ?0.585
multinomial summary probability 0.222 0.235
unigram summary probability ?0.188 ?0.101
regression 0.867 0.705
ROUGE-1 recall 0.859 0.806
ROUGE-2 recall 0.905 0.873
linear regression combination of features obtains high correlations with manual scores
but does not lead to better results than the single best feature: JS divergence.
JS divergence obtains the best correlations with both types of manual scores?
0.88 with pyramid score and 0.74 with responsiveness. The regression metric performs
comparably, with correlations of 0.86 and 0.70. The correlations obtained by both JS
divergence and the regression metric with pyramid evaluations are in fact better than
that obtained by ROUGE-1 recall (0.85).
The best topic signature-based feature?the percentage of input?s topic signatures
that are present in the summary?ranks next only to JS divergence and regression. The
correlations between this feature and pyramid and responsiveness evaluations are 0.79
and 0.62, respectively. The proportion of summary content composed of topic words
performs worse as an evaluation metric with correlations 0.71 and 0.60. This result
indicates that summaries that cover more topics from the input are judged to have better
content than those in which fewer topics are mentioned.
Cosine overlaps and KL divergences obtain good correlations but still lower than
JS divergence and the percentage of input topic words. Further, rankings based on
unigram and multinomial summary likelihood do not correlate significantly with
manual scores.
On a per input basis, the proposed metrics are not that effective in distinguishing
which summaries have good and poor content. The minimum and maximum correla-
tions with manual evaluations across the 48 inputs are given in Table 3. The number
and percentage of inputs for which correlations were significant are also reported.
JS divergence obtains significant correlations with pyramid scores for 73%. The best
correlation was 0.71 on a particular input and the worst performance was 0.27 correla-
tion for another input. The results are worse for other features and for comparison with
responsiveness scores.
At the micro level, combining features with regression gives the best result overall,
in contrast to the findings for the macro-level setting. This result has implications for
system development; no single feature can reliably predict good content for a partic-
ular input. Even a regression combination of all features is a significant predictor of
279
Computational Linguistics Volume 39, Number 2
Table 3
Spearman correlations at micro level for TAC?08 data (58 systems). Only the minimum and
maximum values of the significant correlations are reported, together with the number and
percentage of inputs that obtained significant correlation.
Pyramid Responsiveness
number number
Features max min significant (%) max min significant (%)
JS div ?0.714 ?0.271 35 (72.9) ?0.654 ?0.262 35 (72.9)
JS div smoothed ?0.712 ?0.269 35 (72.9) ?0.649 ?0.279 33 (68.8)
KL div summary-input ?0.736 ?0.276 35 (72.9) ?0.628 ?0.261 35 (72.9)
% of input topic words 0.701 0.286 31 (64.6) 0.693 0.279 29 (60.4)
cosine overlap - all words 0.622 0.276 31 (64.6) 0.618 0.265 28 (58.3)
KL div input-summary ?0.628 ?0.262 28 (58.3) ?0.577 ?0.267 22 (45.8)
cosine overlap - topic words 0.597 0.265 30 (62.5) 0.689 0.277 26 (54.2)
% summary = topic words 0.607 0.269 23 (47.9) 0.534 0.272 23 (47.9)
multinomial summary prob. 0.434 0.268 8 (16.7) 0.459 0.272 10 (20.8)
unigram summary prob. 0.292 0.261 2 (4.2) 0.466 0.287 2 (4.2)
regression 0.736 0.281 37 (77.1) 0.642 0.262 32 (66.7)
ROUGE-1 recall 0.833 0.264 47 (97.9) 0.754 0.266 46 (95.8)
ROUGE-2 recall 0.875 0.316 48 (100) 0.742 0.299 44 (91.7)
content selection quality in only 77% of the cases. For example, a set of documents,
each describing a different opinion on an issue, is likely to have less repetition on
both the lexical and content unit levels. Because the input?summary similarity metrics
rely on the word distribution of the input for clues about important content, their
predictiveness will be limited for such inputs.8 Follow-up work to our first results on
fully automatic evaluation by Saggion et al (2010) has assessed the usefulness of the JS
divergence measure for evaluating summaries from other tasks and for languages other
than English. Whereas JS divergence was significantly predictive of summary quality
for other languages as well, it did not work well for tasks where opinion and biograph-
ical type inputs were summarized. We provide further analysis and some examples in
Section 7.
Overall, the micro level results suggest that the fully automatic measures we ex-
amined will not be useful for providing information about summary quality for an
individual input. For averages over many test sets, the fully automatic evaluations
give more reliable results, and are highly correlated with rankings produced by manual
evaluations. On the other hand, model summaries written for the specific input would
give a better indication of what information in the input was important and interesting.
This is indeed the case as we shall see from the ROUGE scores in the next section.
4.3.2 Comparison with ROUGE. The aim of our study is to assess metrics for evaluation
in the absence of human gold standards, scenarios where ROUGE cannot be used.
We do not intend to directly compare the performance of ROUGE with our metrics,
8 In fact, it would be surprising to find an automatically computable feature or feature combination which
would be able to consistently predict good content for all individual inputs. If such features existed,
an ideal summarization system would already exist.
280
Louis and Nenkova Automatic Content Evaluation
therefore. We discuss the correlations obtained by ROUGE in the following, however, to
provide an idea of the reliability of our metrics compared with evaluation quality that
is provided by ROUGE and multiple human summaries.
At the macro level, the correlation between ROUGE-1 and pyramid scores is 0.85
(Table 2). For ROUGE-2 the correlation with pyramid scores is 0.90, practically identical
with JS divergence.
Because the performance of these two measures seem close, we further analyzed
their errors. The focus of this analysis is to understand if JS divergence and ROUGE-2
are making errors in ordering the same systems or whether their errors are different.
This result would also help us to understand if ROUGE and JS divergence have com-
plementary strengths that can be combined. For this, we considered pairs of systems
and computed the better system in each pair according to the pyramid scores. Then,
for ROUGE-2 and JS divergence, we recorded how often they provided the correct
judgment for the pairs as indicated by the pyramid evaluation. There were 1,653 pairs
of systems at the macro level and the results are in Table 4.
This table shows that a large majority (80%) of the same pairs are correctly predicted
by both ROUGE and JS divergence. Another 6% of the pairs are such that both metrics
do not provide the correct judgment. Therefore, ROUGE and JS divergence appear to
agree on a large majority of the system pairs. There is a small percentage (14%) that is
correctly predicted by only one of the metrics. The chances of combining ROUGE and
JS divergence to get a better metric appears small, therefore. To test this hypothesis, we
trained a simple linear regression model combining JS divergence and ROUGE-2 scores
as predictors for the pyramid scores and tested the predictions of this model on data
from TAC 2009. The combination did not give improved correlations compared with
using ROUGE-2 alone.
In the case of manual responsiveness, which combines aspects of linguistic quality
along with content selection evaluation, the correlation with JS divergence is 0.73.
For ROUGE, it is 0.80 for R1 and 0.87 for R2. Here, ROUGE-1 outperforms all the
fully automatic evaluations. This is evidence that the human gold-standard summaries
provide information that is unlikely to ever be approximated by information from the
input alone, regardless of feature sophistication.
At the micro level, ROUGE clearly does better than all the fully automatic measures
for replicating both pyramid and responsiveness scores. The results are shown in the
last two rows of Table 3. ROUGE-1 recall obtains significant correlations for over 95%
of inputs for responsiveness and 98% of inputs for pyramid evaluation compared to
73% (JS divergence) and 77% (regression). Undoubtedly, at the input level, comparison
with model summaries is substantially more informative.
When gold-standard summaries are not available, however, our features can pro-
vide reliable estimates of system quality when averaged over a set of test inputs.
Table 4
Overlap between ROUGE-2 and JS divergence predictions for the best system in a pair
(TAC 2008, 1,653 pairs). The gold-standard judgment for a better system is computed
using the pyramid scores.
JSD correct JSD incorrect
ROUGE-2 correct 1,319 (79.8%) 133 (8.1%)
ROUGE-2 incorrect 96 (5.8%) 105 (6.3%)
281
Computational Linguistics Volume 39, Number 2
Table 5
Input?summary similarity evaluation: Results on TAC?09 (53 systems).
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Metric py resp py resp py resp py resp
JS div 0.74 0.70 84.1 75.0 78.0 75.7 65.1 50.1
Regr 0.77 0.67 81.8 65.9 80.1 74.8 64.7 49.4
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
4.3.3 Results on TAC?09 Data. To evaluate our metrics for fully automatic evaluation, we
make use of the TAC?09 data. The regression metric was trained on all of the 2008 data
with pyramid scores as the target. Table 5 shows the results on the TAC?09 data. We
also report the correlations obtained by ROUGE-SU4 because it was the official baseline
measure adopted at TAC?09 for comparison of automatic evaluation metrics.
The correlations are lower than on our development set. The highest correlation at
macro level is 0.77 (regression) in contrast to 0.88 (JS divergence) and 0.86 (regression)
obtained on the TAC?08. The regression metric turns out better than JS divergence on
the TAC?09 data for predicting pyramid scores. JS divergence continues to be the best
metric on the basis of correlations with responsiveness, however.
In terms of the pairwise scores, the automatic metrics have 80% accuracy in pre-
dicting the pyramid scores at the system level, about 8% lower than that obtained by
ROUGE. For responsiveness, the best accuracy is obtained by regression (75%). This
result shows that the ranking according to responsiveness is likely to have a large
number of flips. ROUGE is 5 percentage points better than regression for predicting
responsiveness but this value is still low compared to accuracies in replicating the
pyramid scores.
The pairwise accuracy at the micro level is 65% for the automatic metrics and here
the gap between ROUGE and our metrics is 5 percentage points but it is a significant
percentage as the total pairs at micro level are about 60,000 (all pairings of 53 systems
in 44 inputs).
Overall, the performance of the fully automatic evaluation is still high for use
during system development. A further advantage is that these metrics are consistently
predictive across two years as shown by these results. In Section 7, we analyze some
reasons for the difference in performance in the two years. In terms of best metrics, both
JS divergence and regression turn out to be useful with little difference in performance
between them.
5. Pseudomodels: Use of System Summaries in Addition to Human Summaries
Methods such as pyramid use multiple human summaries to avoid bias in evaluation
when using a single gold standard. ROUGE metrics are also currently used with mul-
tiple models, when available. But often, even if gold-standard summaries are available
on non-standard test sets, they are few in number. Data sets with one gold-standard
summary (such as abstracts of scientific papers and editor-produced summaries of news
articles) are common. The question now is whether we can provide the same quality
282
Louis and Nenkova Automatic Content Evaluation
evaluation using a single gold-standard summary as compared to using several gold
standards.
To tackle this problem, we propose the use of pseudomodel system summaries.
Our approach is as follows: We first predict the scores of systems on the basis of the few
available models. The top ranking systems from this evaluation are then considered
as ?pseudo-models;? their summaries are added to the gold-standard set alng with
the existing human models. The final evaluation scores are produced by comparison
with this expanded model set?original model summaries plus the pseudomodels. Our
hypothesis is that the scores produced after the addition of pseudomodels would be
more reliable and correlate better with human scores compared with evaluation using
a single model summary.
Before we describe our method, we provide a glimpse of the variation in evaluation
quality depending on the number of models used. Previous studies have shown that
at the system level, system rankings even with a single model will be stable when
computed over a large enough number of test inputs. Harman and Over (2004) show
that the relative ranks of systems computed using one model do not change when
computed using another model when the number of inputs is large. Again under the
same conditions of having a large number of inputs, Lin (2004a) and Owkzarzak and
Dang (2009) show that ROUGE correlations with human scores are stable when using
few human models. In machine translation evaluation, similar results are noted by
Zhang and Vogel (2010), who found that the lack of additional reference translations
can be handled by evaluating the systems on more test examples.
Multiple models are particularly important for evaluation at the level of individual
inputs, however. Table 6 shows the difference in correlations and pairwise accuracy of
ROUGE with human scores when one and four model summaries are used. We picked
the first model in alphabetical order of their names for the computation of correlation
between metrics and a single model.
At the system level, the correlations from both set-ups are similar. But at the micro
level, there is considerable difference in performance. Using all four models, significant
correlations with pyramid scores are obtained for 95% of the inputs. The evaluations
that rely on a single model produce significant correlations for only 84% of the inputs,
however. For responsiveness scores, which are model-independent, we see that the
micro-level evaluations have a smaller increase as more models are added (79% to 81%).
Again in terms of pairwise accuracy, the accuracy in predicting micro-level pyramid
scores improves by 4% when additional models are used and the improvement is 3%
for predicting responsiveness scores. Given this difference in performance when one
and many models are used, we investigate how to improve evaluation when only one
model is available.
Table 6
ROUGE evaluation with different number of models: macro level (Spearman correlations), micro
level (percentage of inputs with significant correlations on TAC?09 data). No. of systems = 53.
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Task py resp py resp py resp py resp
RSU4 - 1 model 0.92 0.80 84.1 79.5 88.3 80.3 66.1 50.7
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
283
Computational Linguistics Volume 39, Number 2
We explore the possibility of augmenting the model set with good system sum-
maries. These system summaries or ?pseudomodels? are chosen to be the ones which
receive high scores based on the one available model summary. We expect that the
benefit of pseudomodels will be noticeable in micro-level correlations with pyramid
scores. At the macro level, even with multiple human models there is no improvement
in correlations compared with a single model, and the addition of less-ideal system
summaries is not likely to be better than adding human summaries.
5.1 Related Work
The idea of using system output for evaluationwas introduced in the context of machine
translation by Albrecht andHwa (2007, 2008). In their method, Albrecht andHwa (2007)
designate some systems to act as pseudoreferences. Then, every candidate translation
to be evaluated is compared to the translations produced by the pseudoreferences using
a variety of similarity metrics. Each similarity value is then used as a feature and
trained to predict the human assigned score for that candidate translation. They show
that the scores produced by their regression metric using only system-based references
correlates with human judgments to the same extent as scores produced using multiple
human reference translations. Also, when the regression method was used with human
references and some pseudoreferences put together, the correlations obtained by the
final metric was better than using the human references alone.
In Albrecht and Hwa (2007), pseudoreferences of different quality?best, moderate
and worst?are chosen using the gold-standard judgments and evaluated for use as
pseudoreferences. They found that having the best systems as pseudoreferences worked
best, although even adding the worst system as pseudoreference gave reasonable per-
formance as their regression approach is trained to predict quality by comparison to the
standard of the reference. In their work, however, pseudoreferences of different quality
are chosen in an oracle manner (using the human-assigned scores). This setting is not
practical because it depends on the actual system scores. In later work, Albrecht and
Hwa (2008) use off-the-self machine translation systems as pseudoreferences and show
that they can contribute to good results. This later work is a more realistic set-up and
here regression is important because we have no guarantees as to the quality of the
off-the-shelf systems on the test data.
A similar idea of augmenting machine output to human gold standard was ex-
plored in Madnani et al (2007) in the context of machine translation (MT). For tuning
MT systems, often multiple reference translations are required. Madnani et al aug-
mented reference translations of a sentence with automatically generated paraphrases
of the reference. They found in the experiments that such augmentation helped in
MT tuning?the number of reference translations needed could be cut in half and
compensated with automatic paraphrases.
5.2 Choice of Pseudoreference Systems
For this evaluation, the choice of the pseudoreference system is an important step. In
this section, we detail some development experiments that we performed to understand
how to best choose such pseudoreferences for the summary evaluation task.
We examined a similar regression approach as followed by Albrecht and Hwa
(2007). We chose systems of different quality (best, mediocre, worst) based on the
284
Louis and Nenkova Automatic Content Evaluation
oracle human-assigned scores. The remaining systems were taken as the evaluation
set. For each summary in the evaluation data, we computed features to indicate their
similarity with the summaries of the chosen pseudoreference systems. Our similarity
features were the recall scores from ROUGE overlaps. We computed one feature each
for unigram, bigram, trigram, and four-gram ROUGE scores. Each of these four features
is computed for each pseudoreference summary.
The scores are used in a linear regression model to predict the summary score.
We used a cross-validation approach where the summaries from one of the evaluation
systems were used as the test set and the summaries from the remaining systems
are used for training the regression model. Then the average predicted score of each
system in the evaluation data was computed and compared with their average scores
as assigned during manual evaluations.
The experiment was performed using data from four years of DUC conferences,
2001 to 2004. The manual scores in these earlier DUC years were the content coverage
scores (described in Section 2.1), which use a single model summary for comparison.
Table 7 shows the Spearman correlations between the scores from evaluations only
against the pseudoreferences and those from the manual evaluation with the single
model. The different settings for choice of pseudoreference systems are also indicated.
These results showed that using the best systems as pseudoreferences provided the
best performance across different years. When only worst or only mediocre systems
were used, the performance was much worse for predicting system scores. Even when
the best systems were augmented with the worst systems as pseudoreferences, the
evaluation quality decreased compared with using the best systems only.
Whereas Albrecht and Hwa (2007, 2008) obtained a slight improvement by also
using a worst quality pseudoreference in the mix, for summarization it is better to have
only the best systems. One reason for this difference could be that for summary eval-
uation, examples of worse summaries are not very informative. Two good summaries
may have considerable variation in the content. When a summary is similar to a best
system, therefore, we can say that the candidate summary is also of good quality. On
the other hand, when a candidate summary is similar to a worst system summary, it
may either be a worse summary or it may be a good summary with different content
than the best system?s summary. Indeed, when ROUGE was first introduced, it was
heavily emphasized that it is a recall measure and that precision-oriented measures
do worse. Hence the weights learned for the similarity with the worst system may
not be very informative. In summarization, the space of both good summaries and
worse summaries for the same input is large. Having more examples of good sum-
maries appears to benefit evaluation more compared with having samples of worst
quality.
Table 7
Spearman correlations between pseudoreference-based regression scores and manual content
scores. The first column lists the type of pseudoreference chosen.
Pseudoreference 2001 2002 2003 task 2 2004 task 2 2004 task 5
2 best systems 0.58 0.77 0.51 0.93* 0.83*
2 worst systems 0.45 ?0.94* ?0.09 0.13 ?0.23
2 mediocre systems 0.72 0.08 0.53 0.64 0.18
2 best, 2 worst systems 0.38 0.20 0.25 0.92 0.73
*The correlation was significant with p-value < 0.05.
285
Computational Linguistics Volume 39, Number 2
Because the best systems turned out to have the maximum potential for acting
as pseudoreferences, we wanted a way to identify some best systems without having
to rely on the oracle scores, as before. This idea is feasible for our set-up. In our
evaluation, we aimed to augment an existing model, so we used the available model
to automatically obtain an idea of some of the good systems from the pool. Then we
chose some of these top systems as pseudoreferences and combined them with the one
available model to form the reference set for final evaluation. Because the reference set
has mostly best summaries, we did not use a regression approach based on similarity to
the different references. Rather, we considered all of them as models and computed
a single ROUGE score comparing a system summary with the pool of model plus
pseudomodel summaries.
5.3 Experimental Set-up
We now detail our experiments on the TAC 2009 data.
TAC provides four model summaries for each input. We assume that only one is
available and choose a model for each input: the first in alphabetical order of identifier
names. Based on this model, we compute the RSU4 scores for all systems. We use two
methods to choose the pseudomodel systems.
In the first approach, we rank all the systems based on their average scores over the
entire test set. The summaries of the top three overall best systems (global selection) are
added to the set of models for all inputs. Alternatively, we also investigate a different
selection method. For each input, the top scoring three summaries are added as models
for that input (local selection). In both cases RSU4 was used to identify the best systems
according to the single available gold standard.
The final rankings for all systems are produced using the RSU4 comparison based
on the expanded set of models (1 human model + 3 pseudomodel summaries). We
implemented a jackknifing procedure so that the systems selected to be pseudomodels
(and therefore reference systems) could also be compared to other systems. For each
input, one of the reference systems (pseudomodels or human model) was removed at a
time from the set of models and added to the set of systems. The scores for the systems
were then computed by comparison with the three remaining models. The final score
for a system summary (not a pseudomodel) is the mean value of the scores with the
four different sets of reference summaries created by the jackknifing procedure. For
pseudomodel systems, a single score value will be obtained per input resulting from
the comparison with the other three models.
5.4 Results
The system and input level performance before and after the addition of pseudomodels
is shown in Table 8. The performance using four human models is shown in the last line
for comparison.
At the macro level, the pseudomodel summaries provide little improvements. Only
for the global model is there an increase in correlation, from 0.80 to 0.82.
As expected, however, for the micro level, pseudomodels prove beneficial. Both
global and local selection methods improve the number of inputs that receive signif-
icant micro-level correlations with pyramid scores. The improvement is close to 10%
compared with using only one model summary. Also note that, after the addition of
pseudomodels, the percentage of significant correlations is 93%, which is only 2% less
compared with the results using four human models (95%).
286
Louis and Nenkova Automatic Content Evaluation
Table 8
Performance before and after the addition of pseudomodel summaries: TAC?09 data
(53 systems).
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Evaluation type py resp py resp py resp py resp
RSU4 - 1 model 0.92 0.80 84.1 79.5 88.3 80.3 66.1 50.7
Global 0.91 0.82 93.2 79.5 88.6 83.5 66.8 51.3
Local 0.92 0.79 93.2 75.0 89.6 80.8 67.4 51.3
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
For responsiveness scores that are model-independent, however, little improve-
ments are seen at both macro and micro levels. The pairwise accuracy at micro level
for responsiveness is 1% better after the addition of the pseudomodels.
Comparing the two methods for selecting the best system that can serve as a
pseudomodel, the global selection of the system that performed best over the entire
available data set appears to be more desirable. It improves the correlations with pyra-
mid scores while keeping the same correlations with responsiveness as with one model.
Local selection provides the same performance as global selection for pyramid scores,
although it decreases the micro-level evaluation quality for responsiveness.
6. Consensus-Based: Evaluation Using Only Collection of System Summaries
From our experiments with pseudomodels, we see that the addition of system sum-
maries to availablemodels proved beneficial and improved themicro-level performance
of ROUGE. One question that arises is whether the collection of system summaries
together will be useful for evaluation without any human models at all. Again, this idea
is related to model-free evaluation. When several systems are available, we investigate
if their collective knowledge can help assess summary quality.
Systems use varied methods to select content, and agreement among systems could
be indicative of important information. This intuition is similar to that behind the man-
ual pyramid method: Facts mentioned only in one human summary are less important
compared to content that is mentioned in multiple human models. For the experiments
reported in this section, we rely entirely on the combined knowledge from system
summaries as a gold standard.
6.1 Related Work
The closest work to this idea of combining system output can be found in the area of
information retrieval (IR). Soboroff, Nicholas, and Cahan (2001) proposed a method
for evaluating IR systems without relevance judgments. In addition to requiring less
human input, the need for automatic evaluation in IR is also motivated by the fact that
for systems such as those on the Internet, the documents keep changing and so it is
difficult to collect relevance judgments that are stable and meaningful for a long time.
287
Computational Linguistics Volume 39, Number 2
Soboroff, Nicholas, and Cahan (2001) combine the top n results from all the systems
and then sample a certain number of documents from this pool. Those documents
selected by many systems are more likely to be in the chosen sample and assumed
to be most relevant. The systems are then evaluated by considering this chosen set of
documents as the gold-standard relevant set.
In our work, we do not attempt to pick out common content explicitly from the
summary pool. If we were to follow the same approach as IR, we would be sampling
sentences from the summary pool. But in multi-document summarization, sentences
from different documents could contain similar content and we do not want to sample
one sentence and use it in the gold standard because then systems would be penalized
for choosing other similar sentences. In our work, therefore, we break down the sen-
tences and represent the content as a probability distribution over words. A summary
is evaluated by comparing its word distribution to that of the pool. We expect that the
distribution would implicitly capture the common content.
6.2 Evaluation Set-up
For each input, we collect all the summaries produced by automatic systems and
calculate the probabilities of words in the combined set. In this way, we obtain a global
probability distribution of words selected in system summaries. In this distribution,
the content selected by multiple systems will be more prominent, representing the
more important information. The word probabilities from each individual summary
are then calculated and compared to the overall distribution using JS divergence. If we
assume that system summaries are collectively indicative of important content, then
good summaries will tend to have properties that are similar to this global distribution,
resulting in low divergence values. We compute the correlations of these divergence
values with human-assigned summary scores and Table 9 shows the results from this
evaluation.
6.3 Results
The correlations are on par with those based on multiple human gold standards. At
both macro and micro levels, the correlations and pairwise accuracy are similar to those
obtained by ROUGE comparison with four human models. The macro-level correlation
is 0.93 with pyramid scores, which is very high for a metric that uses no human input
at all. Further, the micro-level correlations are also significant for 90% of the inputs. In
our pseudomodel experiments, the gains after the addition of system summaries were
Table 9
Performance of consensus evaluation approach on TAC?09 data (53 systems). For input level
(micro), the percentage of inputs with significant correlations is reported.
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Evaluation type py resp py resp py resp py resp
SysSumm 0.93 0.81 90.9 86.4 88.8 80.7 65.2 52.7
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
288
Louis and Nenkova Automatic Content Evaluation
modest (only at micro level). Here we see that a large collection of system summaries
by themselves have the information required for evaluation.
From this experiment, we find that consensus among system summaries is indica-
tive of important content. This result suggests that by combining the content selected
by multiple systems, one might be able to build a summary that is better than each
of them individually. In fact, this idea of system consensus has been utilized in the
development of MT systems for quite some time. One approach in MT is rescoring the
n-best list from an individual system?s decoder, and picking the (consensus) translation
that is close on average to all translations. Such rescoring is implemented using a
minimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al 2008). The other
approach is system combinationwhere the output frommultiple systems is combined to
produce a new translation. Several techniques includingminimumBayes risk have been
applied to perform system combination in machine translation. Shared tasks on system
combination have also been organized in recent years to encourage the development
of such methods (Callison-Burch et al 2010, 2011). Such strategies could be a useful
direction to explore for summarization as well.
7. Discussion
In this article, we have discussed metrics for summary evaluation when human sum-
maries are not present. Our results have shown that these metrics in fact correlate highly
with human judgments. But we also need to understand how robust these metrics are
and be aware of their limitations. In this section, therefore, we provide a brief discussion
of the use of these metrics in different settings.
7.1 Including Input?Summary Similarity or Consensus-Based Measures in a
Summarization System
Firstly, because input?summary similarity features are computed using the input, they
can be useful features to incorporate in a summarization system. The combination
of systems to perform evaluation also provides a way to build a better system. The
concern would be how the usefulness of these metrics will change if systems were
also optimizing for them. To optimize a metric such as JS divergence exactly would be
difficult because the JS divergence score cannot be factored or divided among individual
sentences, a necessary condition if the problem should be solved using an Integer Linear
Program as in McDonald (2007) and Gillick and Favre (2009). Therefore only greedy
methods are possible. In fact, KL divergence was greedily optimized in Haghighi and
Vanderwende (2009) to obtain a high performance summarizer. Gaming the evaluation
should carry little concern, however, as thesemetrics are proposedwith a view to tuning
systems.
The metrics we presented are developed for evaluation in a new setting where
model summaries are not available and to aid system development and tuning. Further,
notice from the micro-level evaluation that a single metric such as JS divergence does
not predict content selection performance well for all inputs. System developers should
therefore involve other specialized features as well. Regression of similarity metrics is a
better predictor at the micro level but optimizing that would involve computation of all
metrics. Another point to note here is that these similarity measures and the consensus
pool are only indicative of summary content quality. Other key aspects of summary
quality, however, involve sentence ordering, proper generation of referring expressions
289
Computational Linguistics Volume 39, Number 2
and grammatical sentences, and maintaining non-redundancy. Systems should there-
fore be optimizing for a wide variety of factors and thus input?summary similarity and
consensus evaluation can be used in the final output to measure the content quality of
the summary. Any content evaluation should obviously be accompanied by linguistic
quality evaluation in contrast to the current trend to only report content scores.
The high performance of the JS divergence metric also has another implication
for system development. On average, the JS divergence measure is highly predictive
of summary quality. It indicates that for a large number of inputs in the TAC data
sets, good content can be predicted with high accuracy just based on the input?s term
distributions. Such inputs should therefore be easy to summarize for systems. Although
discourse-based and other semantic approaches to summarization have been proposed,
most of the systems in TAC rely on surface features such as word distributions. In
this situation, we may not be focusing on robust systems that can handle a variety of
inputs. In the early years of DUC, the test set comprised a variety of inputs such as
biographies, collections of multiple events, opinions, and descriptions of single events.
Later years switched to more single-event-type test sets. The results from our analysis
point out that current inputs might be too simple for systems and that the range of
inputs in the TAC conference should be expanded to include some input types where
more sophisticated methods become necessary. Perhaps the input?summary similarity
metrics will be helpful in picking out those inputs that need deeper analysis. In the
following section, we provide some further analysis into the cases where the input?
summary similarity turns out less predictive.
7.2 Input?Summary Similarity and Dependence on Input Characteristics
JS divergence is useful for the average rating of systems on the test set and, in our
case, we have 44 examples over which the scores are averaged. At the micro level,
certain inputs received poor evaluations from JS divergence. Here we provide some
insights into the types of inputs where JS divergence worked and the cases which
proved difficult.
Table 10 shows the titles of articles in input D0913, the input that received the best
evaluation from JSD (correlation of 0.86). These articles were all published on the same
day and deal with the same event, a Supreme Court hearing of a case. This input can
be said to be highly cohesive and to be discussing the same topic. For such inputs,
the term distribution in the input would reflect content importance since some words
have higher probability than others because they are discussed repeatedly in the input
documents. Such a term distribution when compared with summaries will give good
evaluation performance. We can also see that the human summaries for this input (also
shown in Table 10) seem to report the common issues observed in the input. In this
case, therefore, input?summary similarity scores can predict the pyramid scores that
were assigned based on the model summaries. We also show in the table the summary
that is chosen to be best according to JS divergence and the summary that had the
worst score. We find that the best summary indeed conveys some of the main issues
also reported in the human summaries. On the other hand, the low-scoring summary
presents a story line about one of the lawyers involved in the case, which is a peripheral
topic described in only one of the input documents. In fact, the summary scored as
worst by JS divergence has a pyramid score of 0, whereas the chosen best summary has
a pyramid score of 0.39.
On the other hand, summaries for input D0940 obtained only 0.3 correlation us-
ing JSD evaluation. Both ROUGE and consensus evaluation (SysSumm) methods can
290
Louis and Nenkova Automatic Content Evaluation
Table 10
Titles of articles and two human summaries for input D0913-A. The summaries chosen as best
and worst according to JS divergence are also listed.
Articles in input D0913-A
Publication date Title
Mar 02 US Supreme Court examines issue of displaying Ten Commandments
Mar 02 US Supreme Court examines Ten Commandments displays
Mar 02 Supreme Court wrestles with Ten Commandments issue
Mar 02 Justices examine 10 Commandments case
Mar 02 High Course argues in 10 Commandments case
Mar 02 Supreme Course wrestles with Ten Commandments
Mar 02 High Court argues Ten Commandments cases
Mar 02 Texas seeks to keep ten commandments display on capitol grounds
Mar 02 An unlikely journey up the legal ladder
Model summary H
The Supreme Court heard arguments in two cases on March 2nd about the conditions under which
the government could display the Ten Commandments and whether such displays violated the
First Amendment.
The Texas case concerns a 40-year-old granite monument of the Ten Commandments, one of 17 monuments
on the grounds of the state capital.
The Kansas case concerns framed copies of the Ten Commandments which hang with non-religious
documents in two county courthouses.
The displays have been modified twice in response to lower court rulings.
Justice O?Connor is expected to cast the swing votes.
Model summary C
On March 2, 2005, the Supreme Court heard two cases concerning the display of the Ten Commandments
on government property.
In Texas, a homeless former lawyer challenged the constitutionality of an inscribed monument displayed
on state capitol grounds.
In Kentucky, the ACLU claimed copies displayed in two courthouses, modified twice before in response
to court rulings, still violated the First Amendment because the original purpose was religious and the
modifications were a sham.
Supporters argued that the Ten Commandments are a recognized symbol of law, with both secular and
religious functions.
The justices will issue an opinion by late June.
Lowest JSD summary: System 26
The Supreme Court Wednesday wrestled with whether the Constitution allows displays of the
Ten Commandments on government property.
Lower courts have issued conflicting rulings in dozens of cases involving Ten Commandments displays
in recent years.
The justices also heard a challenge to Ten Commandments exhibits in Kentucky courthouses.
Abbott, making his first argument before the high court, contended that the Ten Commandments
monument, while containing a sacred religious text.
Abbott, 47, a former state Supreme Court justice.
It is hardly little ? a constitutional challenge to displaying the Ten Commandments on the grounds of the
Texas Capitol, and so on.
Highest JSD summary: System 39
He said his happiest moment was not when he heard the Supreme Court was taking his case but when
his teenage daughter read the story and tracked him down by e-mail, breaking a long estrangement.
If religious, it was unacceptable, Van Orden said.
He ate on food stamps at the upscale Central Market, pitched his tent nightly and took it down each
morning in a wooded location he did not specify, traveled on a free bus pass granted for a veteran?s
disability and read newspapers and magazines free at newsstands.
evaluate the same summaries, however, with correlation of 0.84 (ROUGE) and 0.74
(SysSumm). The titles of the articles in that input and in the human summaries are
provided in Table 11. This input?s topic is the opening of Disneyland in Hong Kong
but its articles cover varied aspects around the topic such as ticket sales, environmental
291
Computational Linguistics Volume 39, Number 2
Table 11
Titles of articles and two human summaries for input D0940-A. The summaries chosen as best
and worst by JS divergence are also listed.
Articles in input D0940-A
Publication date Title
June 22 Opening of HK Disneyland to be divided into 3 phases
June 26 Disney officials consulted feng shui experts for Hong Kong Disneyland
July 01 Hong Kong Disneyland starts on-line tickets selling
July 01 Disneyland rehearsal days to start in August
July 04 HK Disneyland ticket sale proceeds well
Sept 04 High hopes for Hong Kong Disneyland?s economic impact, but critics say Disney
magic overrated
Sept 08 Hong Kong Park: Classic Disney with an Asian accent
Sept 08 Hong Kong Disneyland won?t cut its maximum capacity despite overcrowding fears
Sept 08 All tickets for opening day of HK Disneyland sold out
Sept 10 Shark fins, stray dogs and smog - Hong Kong Disneyland has had a bumpy ride
Model summary B
Hong Kong Disneyland (HKD) was scheduled to open in three phases in the summer of 2005.
Early to mid-August transportation would be available to some areas of the park.
August-Sept 11 all public services would become available.
Sept 12 the grand opening of the park and hotels would take place.
On Aug 16 HKD will begin its rehearsals to which special guests will be invited.
In September, HKD announced it would not cut its daily capacity of 30,000 visitors.
On Sept 9 it was revealed that all tickets for the grand opening had been sold.
Model summary H
Hong Kong Disneyland, a joint venture between Disney and the Hong Kong government, was scheduled
to open on 12 September.
Rehearsal days were staged for a month before opening, giving ?cast members? a chance to practice their
performances.
Disneyland refused to reduce its daily maximum capacity of 30,000 despite complaints from early visitors
about large crowds and long lines.
All 16,000 opening day tickets were sold out.
The park is vintage Disney, with aspects of local culture including feng shui, Asian foods, and signs
in Chinese.
Protests forced them to remove shark fin soup from their menus.
Lowest JSD summary: System 54
But critics say Hong Kong Disneyland is overrated.
The opening of Hong Kong Disneyland is expected to turn on a new page of Hong Kong tourism, with
focus on family tourists, she said.
Hong Kong Disneyland will stage its rehearsal from Aug. 16 to the theme park?s opening on Sept. 12,
the park said in a press release on Friday.
Many in Hong Kong are ready to give Mickey Mouse a big hug for bringing Disneyland to them.
Hong Kong Disneyland Hotel starts at 1,600 (euro 170) a night and Disney?s Hollywood Hotel?s cheapest
room costs 1,000 (euro 106).
Highest JSD summary: System 1
Many in Hong Kong are ready to give Mickey Mouse a big hug for bringing Disneyland to them.
But not dog-lovers, shark-defenders and fireworks foes.
The opposition may seem odd, in a Chinese city where fireworks are a fixture, shark fin soup is hugely
popular, and stray dogs are summarily dealt with as health hazards.
But eight years after the British colony was returned to China, the capitalist city is much freer than the
Communist mainland, and advocacy groups are vocal.
concerns, and use of feng shui. The human summaries for this input focus on different
aspects. The term distributions in such an input by themselves do not provide an
indication of what was important in contrast to the more cohesive input we discussed
previously. The semantics of the content should be better understood to be able to
predict the content that humans would choose in their summaries. Subsequently, we
can also observe that the summary that is top ranked by JS divergence does not have
292
Louis and Nenkova Automatic Content Evaluation
much of the same information as the model summaries and talks about yet another
set of aspects such as hotel rates and family tourism. The worst summary presents a
different set of facts. The pyramid scores for both these summaries are low (0.13 for
the best JS summary and 0.0 for the worst JS summary). Input?summary similarity is
therefore less helpful here and the information provided by model summaries would
be the best gold standard.
Saggion et al (2010) report that trends can be observed in the JSD metric perfor-
mance although it does not provide good evaluations for opinion and biographical type
inputs. Automatic evaluations in different genres therefore have different requirements
and exploring these is an avenue for future work. Input?summary similarity based only
on word distribution works well for evaluating summaries of cohesive-type inputs.
We can also envision a situation where we will be able to predict whether the JS
divergence evaluation will be accurate or not on a particular test set. In prior work in
Nenkova and Louis (2008) and Louis and Nenkova (2009b), we have explored proper-
ties of summarization inputs and provided a characterization of inputs into cohesive
and less cohesive based on automatic features. The less cohesive inputs were found to
be the ones where automatic systems in general performed poorly. In that work, we
proposed features to predict if an input is cohesive or not. We now apply these features
to the TAC?09 data with the intention of automatically identifying inputs suitable for
JS divergence evaluation (the cohesive ones). The features were trained on data from
previous years of TAC evaluations. Among the top ten inputs for which JS divergence
gave the best correlations, six of them were predicted as cohesive, and, similarly for
the bottom ten, six inputs were predicted as ?not cohesive.? This result provides more
validation of the relationship between input and evaluation quality but the automatic
prediction of evaluation quality does not appear to be very accurate based on our
current features. We plan to explore this direction further in future work.
7.3 Requirements for Consensus-Based Evaluation
In a similar vein, one would like to understand the performance guarantees from
the consensus-based evaluation method (SysSumm). Here, the metric depends on the
availability of a number of diverse system summaries. In the TAC workshops, over
50 systems compete and thus we have a large pool of system summaries with which
to compute consensus. For other data sets, when we have to evaluate a few different
systems, it is unclear if the same performance can be obtained. To understand the
dependence on the number of systems, we study how well the consensus evaluation
method works when a small set of standard summarization methods is taken as the
available system pool. We expected that when the standard algorithms are chosen to
be diverse, their strengths can be combined usefully in a similar manner as the TAC
systems.
We choose a set of nine different summarization approaches. They are briefly de-
scribed here.
Baseline: One of the commonly used baseline approaches for multi-document sum-
marization. The first sentence from each document in the input is first included in the
summary. After including the first sentence from each document, the second sentence
is included and so on up to the length limit.
Mead: Radev et al (2004a, 2004b) rank sentences using a combination of three aspects
(sentence length, position in the article, and a centroid score which indicates how central
the content of the sentence is) computed by comparison with all other sentences.
293
Computational Linguistics Volume 39, Number 2
Average probability: This is a competitive summarizer (Nenkova, Vanderwende, and
McKeown 2006) using only the frequency of words as the indicator of content impor-
tance. We implement this method by first computing the unigram probability of all
content words in the documents of the input combined together. Then we score each
sentence by the average value of the probability for the content words in that sentence.
Topic word: This is a strong, yet simple, method for generic summarization (i.e., the
set of documents given as input must be summarized to reflect the sources as best as
possible; in contrast, TAC 2009 tasks can be considered as focused summarizationwhere
either a query is provided or an update is required). This method first computes a set of
topic words from the input using a loglikelihood ratio. The sentences are ranked using
the score introduced by Conroy, Schlesinger, and O?Leary (2006): the ratio of the number
of unique topic words in the sentence to the unique content words in the sentence.
Graph centrality: This approach (Erkan and Radev 2004; Mihalcea and Tarau 2005)
performs selection over a graph representation of the input sentences. Each sentence
is represented in vector space using unigram word counts. Two sentences are linked
when their vectors have a cosine similarity of at least 0.1. When this graph is converted
into aMarkov chain, we can compute the stationary distribution of the transition matrix
defined by the graph?s edges. This stationary distribution gives the probability of visit-
ing each node during repeated random walks through the graph. The high probability
nodes are the ones that are most visited and these correspond to central sentences for
summarization. The probability from the stationary distribution is the ranking score for
this method.
Latent Semantic Analysis (LSA): The LSA technique (Deerwester et al 1990) is based
on the idea of dimensionality reduction. For summarization, first, the input is repre-
sented as a matrix indexed by words (rows) and sentences (columns) and each cell indi-
cates the count of the word in that sentence. This matrix is converted by singular value
decomposition and dimensionality reduction to obtain a matrix of sentences versus
concepts where concepts implicitly capture sets of co-occurring terms. The number of
concepts is much smaller than the size of the vocabulary. The process also produces the
singular values that indicate the importance of concepts. Sentences are selected for each
concept in order of concept importance up to the summary length limit. We obtained
summaries using the approach detailed in Gong and Liu (2001).
Greedy-KL: This method selects sentences by minimizing the KL divergence of the
summary?s word distribution to that of the input. The idea is similar to findings
from our input?summary similarity evaluations. Because the selection of sentences that
minimize divergence can only be done by examining all combinations of sentences,
Haghighi and Vanderwende (2009) introduce a greedy approach that, at each step, adds
the sentence si to the existing summary E such that the combination E ? si has the lowest
KL among all options for si.
CLASSY 04: This system (Conroy and O?Leary 2001) combines the occurrence of topic
words and position of the sentence to predict the score for a sentence. In addition, it
employs a hidden Markov model?based approach, so that the probability of a sentence
being a summary sentence is dependent on the importance of its adjacent sentences in
the document. This system was introduced in the DUC evaluations in 2004 (Conroy
et al 2004). We obtained these summaries (and the subsequent CLASSY 11) from the
authors.
294
Louis and Nenkova Automatic Content Evaluation
CLASSY 11: This is a query-focused summarization system used by Conroy et al (2011)
in TAC 2011. It uses features related to topic words and other important keywords
identified using a graph-based approach. Rather than greedy selection of top sentences,
CLASSY 11 solves an approximate knapsack problem to obtain a more globally optimal
summary. Further, the scoring in this method uses bigrams as the basic unit/keyword in
contrast to the other methods we have described previously that assume that a sentence
is composed of a bag of unigrams.
We generated 100 word summaries from each described system. Except for the
CLASSY system, which performsmore sophisticated redundancy removal, for the other
methods we used the greedy Maximum Marginal Relevance technique (Carbonell and
Goldstein 1998) for reducing redundancy. After the sentence rankings were obtained,
we added each sentence in order if it was not highly similar (a threshold value on
cosine overlap is specified to indicate high similarity) to any of the already added
sentences.
Each of the original TAC systems summaries were evaluated as follows. We added
the candidate summary to the pool of summaries from these other standard methods.
Then we computed the JS divergence between the candidate summary and the com-
bined pool to obtain the score for the candidate. The procedure is the same as the one
we followed in Section 6 except that here we assumed that for each TAC system, we
only had these standard systems as peers rather than the full set of all TAC systems.
The results from this evaluation are shown in Table 12 as SysSumm-std9. The previous
evaluation results, using all TAC systems as consensus, is reproduced in the table as
SysSumm-full.
We found that even with these few systems, the consensus evaluation is rather
strong and produces correlations of 0.91 with pyramid and 0.77 with responsiveness
scores. These results provide additional support for the argument that high quality
evaluation is feasible even with standard systems as peers and that a small set of such
systems appears to be sufficient for forming the consensus.
Because the CLASSY systems are currently some of the top performing systems
at TAC, we also evaluated how useful the consensus is if the CLASSY summaries
are left out. So we evaluated the TAC systems using only the seven other standard
systems (i.e., all except CLASSY04 and CLASSY11) as the peers and the results from
this evaluation are reported in Table 12 as SysSumm-std7. We find that the correlations
Table 12
Performance of consensus evaluation approach on TAC?09 data (53 systems). For input level
(micro), the percentage of inputs with significant correlations is reported. The results using
TAC?09 systems as pseudomodels are indicated as SysSumm-full and those with off-the-shelf
systems as SysSumm-std.
Correlations Pairwise accuracy
Macro level Micro level Macro level Micro level
Evaluation type py resp py resp py resp py resp
SysSumm - full 0.93 0.81 90.9 86.4 88.8 80.7 65.2 52.7
SysSumm - std9 0.91 0.77 86.3 75.0 87.4 78.7 66.4 50.9
SysSumm - std7 0.91 0.78 84.0 75.0 87.7 78.8 65.9 50.6
RSU4 - 4 models 0.92 0.79 95.4 81.8 88.4 80.0 70.5 53.0
295
Computational Linguistics Volume 39, Number 2
remain the same even when the strongest systems are removed. The usefulness of the
consensus therefore is not heavily dependent on the presence of best-quality systems in
the pool.
7.4 Cross-Year Variation in Metric Performance
The performance of a metric should also be discussed under the effect of different test
data sets. In our feature analysis for input?summary similarity, we found that JS diver-
gence produces very high correlations on the TAC 2008 data, about 0.88 with pyramid
scores. The performance on the TAC 2009 data, however, although still high (0.74), is
lower than the previous year?s data. Such a difference could be attributed to different
factors. One possible factor is the cohesiveness of the input, as we have discussed
earlier. In the TAC evaluations, there is no control over the input types, so inputs from
different years may not have the same characteristics. It could be, therefore, that TAC
2009 had more inputs that were less cohesive as our second example above compared
to inputs that have more homogenity in the topic discussed. A further evidence for this
hypothesis can be seen from the cross-year performance of different metrics presented
in Table 13.
Here, improved correlations from 2008 to 2009 are bolded and those that decreased
are italicized. The correlations with pyramid scores increased for SysSumm and ROUGE
evaluations but dropped for JS divergence. This trend indicates that the model-based
evaluations (SysSumm has characteristics similar to model-based evaluation) have
more strength on the 2009 data. For inputs where model information cannot be obtained
from the general term distribution in the inputs (as could have been the case in 2009),
therefore, input?summary similarity that is model-free obtains worse performance. This
model dependence can also explain why ROUGE and SysSummhave lower correlations
with responsiveness in 2009 despite being able to predict pyramid scores better. Because
ROUGE scores are computed based on these specific models, its correlations with the
model-free responsiveness judgments drops in 2009.
8. Conclusion and Future Work
We have presented successful metrics for summary evaluation that require very little
or no human input. We have explored two scenarios: fewer model summaries and no
model summaries at all. For both these cases, our newly proposed evaluation metrics
have provided good performance.
We analyzed two methods for evaluation in the absence of gold-standard
summaries. One was based on input?summary similarity. We examined different
Table 13
Cross-year system level correlations for different metrics. The ROUGE-SU4 scores use all four
human summaries for reference. Improved correlations in 2009 are bolded and decreases in
correlations are italicized.
JSD SysSumm ROUGE-SU4
year py resp py resp py resp
2008 0.89 0.74 0.85 0.82 0.88 0.83
2009 0.74 0.70 0.93 0.81 0.92 0.79
296
Louis and Nenkova Automatic Content Evaluation
possibilities for measuring similarity and quantified their accuracy in predicting
human-assigned scores. Our results showed that the strength of features varies con-
siderably. The best metric is JS divergence, which compares the distribution of terms
in the input and summary. Combination of JS divergence with other metrics such as
cosine similarity and topic word features also gave high correlations with human scores,
around 0.77.
Another method we have introduced is the addition of pseudomodel system sum-
maries to themodel set when the number of models is low. Our aim here was to improve
themicro-level evaluations and our results show that improvements along this linewere
provided by the pseudomodels.
We also proposed a model-free metric that measures the similarity of a system sum-
mary with the collection of all system summaries for that input. This method actually
provided even better performance (0.93 correlations with pyramid scores), which is
competitive with ROUGE scores computed using four human models.
Furthermore, our evaluations provide consistent performance. In Louis and
Nenkova (2009c), we report the correlations for adjacent years showing that our metrics
produce reliable performance for two consecutive years of TAC evaluation and for two
tasks, query and update summarization.
These evaluation methods highlight considerations that have received little atten-
tion so far and give indications of how to perform evaluations on non-standard test
sets with little human input. The situation of having only one model summary is not
uncommon and so are test sets where there are no model summaries at all. Here, one
could use our proposed approaches in system development and then at a later stage
use manual evaluations on a small test set to confirm the results. Further, our metrics
also provide valuable insights for system development. From our results, it is evident
that optimizing for input?summary similarity using an information-theoretic measure
such as JS divergence and optimizing for topic signatures are indeed good approaches
for building a generic summarization system. In addition, the results from consensus
evaluation show that combining summaries from different systems has the potential
of creating a system better than the pool. Currently, more than 50 systems compete in
the TAC summarization tasks and we want to explore system combination techniques
over their summaries in future work.
We also plan to focus on the incorporation of both content and linguistic quality for
evaluation. As we already saw, the correlation between system rankings based on pyra-
mid and responsiveness scores is only 0.85. Furthermore, the correlations of ROUGE as
well as our metrics are lower with responsiveness compared with the pyramid. Content
scores should therefore always be used together with assessments of linguistic quality,
and combining both scores would be necessary for obtaining better correlations with
responsiveness.
Acknowledgments
We would like to thank John Conroy
for providing us with the summaries
from the CLASSY system and Xi Lin for
the implementation of the LSA-based
summarizer. We would also like to
thank the reviewers for their comments;
in particular, the expanded evaluation
of the consenus-based metric was added
based on their feedback. The work
has been partly supported by an NSF
CAREER award (09-53445).
References
Albrecht, Joshua and Rebecca Hwa.
2007. Regression for sentence-level MT
evaluation with pseudo references.
In Proceedings of ACL, pages 296?303,
Prague.
297
Computational Linguistics Volume 39, Number 2
Albrecht, Joshua and Rebecca Hwa.
2008. The role of pseudo references
in MT evaluation. In Proceedings of the
Third Workshop on Statistical Machine
Translation, ACL, pages 187?190,
Columbus, OH.
Best, D. J. and D. E. Roberts. 1975. Algorithm
as 89: The upper tail probabilities of
Spearman?s rho. Journal of the Royal
Statistical Society. Series C (Applied
Statistics), 24(3):377?379.
Callison-Burch, Chris, Philipp Koehn,
Christof Monz, Kay Peterson, Mark
Przybocki, and Omar Zaidan. 2010.
Findings of the 2010 joint workshop on
statistical machine translation and metrics
for machine translation. In Proceedings
of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR,
pages 17?53, Uppsala.
Callison-Burch, Chris, Philipp Koehn,
Christof Monz, and Omar Zaidan.
2011. Findings of the 2011 workshop
on statistical machine translation.
In Proceedings of the Sixth Workshop
on Statistical Machine Translation,
pages 22?64, Edinburgh.
Carbonell, Jaime and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Proceedings of
SIGIR, pages 335?336, Melbourne.
Conroy, John M., Jade Goldstein, Judith D.
Schlesinger, and Dianne P. O?Leary. 2004.
Left-brain/right-brain multi-document
summarization. In Proceedings of the
4th Document Understanding Conference
(DUC?04), Boston, MA. Available at:
http://duc.nist.gov/pubs/2004papers/
ida.conroy.ps.
Conroy, John M. and Dianne P. O?Leary. 2001.
Text summarization via hidden Markov
models. In Proceedings of SIGIR,
pages 406?407, New Orleans, LA.
Conroy, John M., Judith D. Schlesinger,
Jeff Kubina, Peter A. Rankel, and Dianne P.
O?Leary. 2011. Classy 2011 at TAC:
Guided and multi-lingual summaries and
evaluation metrics. In Proceedings of TAC,
Gaithersburg, MD. Available at:
http://www.nist.gov/tac/publications/
2011/participant.papers/CLASSY.
proceedings.pdf.
Conroy, John M., Judith D. Schlesinger,
and Dianne P. O?Leary. 2006. Topic-focused
multi-document summarization using an
approximate oracle score. In Proceedings
of the COLING-ACL, pages 152?159,
Sydney.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
Americal Society for Information Science,
41(6):391?407.
Donaway, Robert L., Kevin W. Drummey,
and Laura A. Mather. 2000. A comparison
of rankings produced by summarization
evaluation measures. In Proceedings of the
NAACL-ANLP Workshop on Automatic
Summarization, pages 69?78, Seattle, WA.
Erkan, Gu?nes? and Dragomir R. Radev. 2004.
Lexpagerank: Prestige in multi-document
text summarization. In Proceedings of
EMNLP, pages 365?371, Barcelona.
Gillick, Dan and Benoit Favre. 2009. A
scalable global model for summarization.
In Proceedings of the Workshop on Integer
Linear Programming for Natural Language
Processing, pages 10?18, Boulder, CO.
Gillick, Dan and Yang Liu. 2010. Non-expert
evaluation of summarization systems is
risky. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and
Language Data with Amazon?s Mechanical
Turk, pages 148?151, Los Angeles, CA.
Gong, Yihong and Xin Liu. 2001. Generic
text summarization using relevance
measure and latent semantic analysis.
In Proceedings of SIGIR, pages 19?25,
New Orleans, LA.
Haghighi, Aria and Lucy Vanderwende.
2009. Exploring content models for
multi-document summarization.
In Proceedings of HLT-NAACL,
pages 362?370, Boulder, CO.
Harman, Donna and Paul Over. 2004.
The effects of human variation in
DUC summarization evaluation.
In Proceedings of the ACL-04 Workshop:
Text Summarization Branches Out,
pages 10?17, Barcelona.
Jing, Hongyan, Regina Barzilay, Kathleen
Mckeown, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In AAAI
Symposium on Intelligent Summarization,
pages 60?68, Palo Alto, CA.
Kumar, Shankar and William Byrne.
2004. Minimum Bayes-risk decoding
for statistical machine translation.
In Proceedings of HLT-NAACL,
pages 169?176, Boston, MA.
Lin, Chin-Yew. 2004a. Looking for a few
good metrics: Automatic summarization
evaluation-how many samples are
enough. In Proceedings of the NTCIR
Workshop, volume 4, pages 1?10, Tokyo.
298
Louis and Nenkova Automatic Content Evaluation
Lin, Chin-Yew. 2004b. ROUGE: A package
for automatic evaluation of summaries. In
Proceedings of the ACL Text Summarization
Workshop, pages 74?81, Barcelona.
Lin, Chin-Yew, Guihong Cao, Jianfeng
Gao, and Jian-Yun Nie. 2006. An
information-theoretic approach to
automatic evaluation of summaries.
In Proceedings of HLT-NAACL,
pages 463?470, New York, NY.
Lin, Chin-Yew and Eduard Hovy. 2000. The
automated acquisition of topic signatures
for text summarization. In Proceedings of
COLING, pages 495?501, Saarbru?cken.
Lin, Chin-Yew and Eduard Hovy. 2003.
Automatic evaluation of summaries
using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 71?78,
Edmonton.
Louis, Annie and Ani Nenkova. 2008.
Automatic summary evaluation without
human models. In Proceedings of TAC,
Gaithersburg, MD. Available at:
http://www.nist.gov/tac/publications/
2008/additional.papers/Penn.
proceedings.pdf.
Louis, Annie and Ani Nenkova. 2009a.
Automatically evaluating content selection
in summarization without human models.
In Proceedings of EMNLP, pages 306?314,
Singapore.
Louis, Annie and Ani Nenkova. 2009b.
Performance confidence estimation for
automatic summarization. In Proceedings
of EACL, pages 541?548, Athens.
Louis, Annie and Ani Nenkova. 2009c.
Predicting summary quality using
limited human input. In Proceedings
of TAC, Gaithersburg, MD. Available at:
http://www.nist.gov/tac/publications/
2009/participant.papers/UPenn.
proceedings.pdf.
Madnani, Nitin, Necip Fazil Ayan,
Philip Resnik, and Bonnie J. Dorr. 2007.
Using paraphrases for parameter
tuning in statistical machine translation.
In Proceedings of the Second Workshop on
Statistical Machine Translation,
pages 120?127, Prague.
McDonald, Ryan. 2007. A study of global
inference algorithms in multi-document
summarization. In Proceedings of ECIR,
pages 557?564, Rome.
McKeown, Kathy, Regina Barzilay,
David Evans, Vasileios Hatzivassiloglou,
Barry Schiffman, and Simone Teufel.
2001. Columbia multi-document
summarization: Approach and evaluation.
In Proceedings of DUC, New Orleans, LA.
Available at: http://www-nlpir.nist.
gov/projects/duc/pubs/2001papers/
columbia redo.pdf.
Mihalcea, Rada and Paul Tarau. 2005.
Multi-document summarization with
iterative graph-based algorithms.
In Proceedings of the First International
Conference on Intelligent Analysis Methods
and Tools (IA 2005), McLean, VA.
Nenkova, Ani and Annie Louis. 2008. Can
you summarize this? Identifying correlates
of input difficulty for multi-document
summarization. In Proceedings of ACL-HLT,
pages 825?833, Columbus, OH.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method. In
Proceedings of HLT-NAACL, pages 145?152,
Boston, MA.
Nenkova, Ani, Rebecca Passonneau, and
Kathleen McKeown. 2007. The pyramid
method: Incorporating human content
selection variation in summarization
evaluation. ACM Transactions on Speech
and Language Processing, 4(2):4.
Nenkova, Ani, Lucy Vanderwende, and
Kathleen McKeown. 2006. A compositional
context sensitive multi-document
summarizer: Exploring the factors that
influence summarization. In Proceedings
of SIGIR, pages 573?580, Seattle, WA.
Owkzarzak, Karolina and Hoa Trang Dang.
2009. Evaluation of automatic summaries:
Metrics under varying data conditions.
In Proceedings of the Workshop on Language
Generation and Summarisation,
pages 23?30, Singapore.
R Development Core Team. 2011.
R: A Language and Environment for
Statistical Computing. R Foundation
for Statistical Computing, Vienna.
Radev, Dragomir, Timothy Allison, Sasha
Blair-Goldensohn, John Blitzer, Arda
C?elebi, Stanko Dimitrov, Elliott Drabek,
Ali Hakim, Wai Lam, Danyu Liu, Jahna
Otterbacher, Hong Qi, Horacio Saggion,
Simone Teufel, Michael Topper,
AdamWinkel, and Zhu Zhang. 2004a.
MEAD?A platform for multidocument
multilingual text summarization.
In Proceedings of LREC 2004, pages 1?4,
Lisbon.
Radev, Dragomir, Hongyan Jing,
Malgorzata Sty, and Daniel Tam. 2004b.
Centroid-based summarization of
multiple documents. Information
Processing and Management, 40:919?938.
Radev, Dragomir and Daniel Tam. 2003.
Single-document and multi-document
299
Computational Linguistics Volume 39, Number 2
summary evaluation via relative utility.
In Proceedings of CIKM, pages 508?511,
New Orleans, LA.
Rath, G. J., A. Resnick, and R. Savage.
1961. The formation of abstracts by
the selection of sentences: Part 1:
Sentence selection by man and
machines. American Documentation,
2(12):139?208.
Saggion, Horacio, Juan-Manuel
Torres Moreno, Iria da Cunha,
Eric SanJuan, and Patricia Velazquez-
Morales. 2010. Multilingual
summarization evaluation without
human models. In Proceedings of
COLING, pages 1,059?1,067, Beijing.
Soboroff, Ian, Charles Nicholas, and
Patrick Cahan. 2001. Ranking retrieval
systems without relevance judgments.
In Proceedings of SIGIR, pages 66?73,
New Orleans, LA.
Tromble, Roy W., Shankar Kumar,
Franz Och, and Wolfgang Macherey.
2008. Lattice minimum Bayes-risk
decoding for statistical machine
translation. In Proceedings of EMNLP,
pages 620?629, Honolulu, HI.
van Halteren, Hans and Simone Teufel.
2003. Examining the consensus
between human summaries: Initial
experiments with factoid analysis.
In Proceedings of the HLT-NAACL DUC
on Text Summarization Workshop,
pages 57?64, Edmonton.
Zhang, Ying and Stephan Vogel. 2010.
Significance tests of automatic machine
translation evaluation metrics.Machine
Translation, 24(1):51?65.
300
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 313?316,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Creating Local Coherence: An Empirical Assessment
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Abstract
Two of the mechanisms for creating natural
transitions between adjacent sentences in a
text, resulting in local coherence, involve dis-
course relations and switches of focus of at-
tention between discourse entities. These two
aspects of local coherence have been tradi-
tionally discussed and studied separately. But
some empirical studies have given strong evi-
dence for the necessity of understanding how
the two types of coherence-creating devices
interact. Here we present a joint corpus study
of discourse relations and entity coherence ex-
hibited in news texts from the Wall Street Jour-
nal and test several hypotheses expressed in
earlier work about their interaction.
1 Introduction
Coherent discourse is characterized by local prop-
erties that are crucial for comprehension. In fact, a
long line of linguistics and computational linguistics
tradition has proposed that several levels of struc-
ture contribute to the creation of coherent discourse.
Among these, the attentional structure (Grosz et
al., 1995) and the relational structure (Mann and
Thompson, 1988) of text, are the most widely dis-
cussed in the literature.
Centering theory (Grosz et al, 1995) is the dom-
inant approach for describing and analyzing atten-
tional structure. It assumes that readers of the text
focus (center) their attention on a small number of
salient discourse entities at a time and that there are
preferred patterns for switching attention between
entities mentioned in adjacent sentences. Relational
This work was partially supported by NSF Grant IIS -07-
05671.
structure theories, on the other hand, describe how
certain discourse (also called rhetorical or coher-
ence) relations such as CONTRAST and CAUSE are
inferred by the reader between adjacent units of text.
The existence of richly annotated corpora and the
development of automatic applications based on the
theories have allowed empirical assessments of the
validity and generality of these theories individually.
Such work has also provided increasingly strong
evidence that attentional and relational structures
need to be taken into account simultaneously. The
motivation behind such proposals have been the em-
pirical findings that ?weak? discourse relations such
as ELABORATION are the most common type of re-
lations, and that a large percentage of adjacent sen-
tences in fact do not have any entities in common.
In particular, a corpus based evaluation of Center-
ing theory found that only 42% of pairs of adjacent
sentences have at least one entity in common and
hypothesized that discourse relations are responsible
for creating local coherence in the remaining cases
(Poesio et al, 2004). At the same time, the work
of Knott et al (2001) has discussed several theo-
retical complications arising from the existence of
the very common and semantically weak ELABO-
RATION relation. These researchers propose that re-
placing ELABORATION by an account of entity co-
herence such as Centering will be most beneficial.
But work in information ordering (Karamanis, 2007)
has not been able to confirm such claims that better
results can be obtained by combining entity coher-
ence with discourse relations.
Till recently, the absence of large corpora anno-
tated for both discourse relations and coreference in-
formation has prohibited a detailed joint analysis of
attentional and relational structure. We combine two
313
recently released corpora: discourse relations from
the Penn Discourse Treebank and coreference an-
notations from the OntoNotes corpus, to assess the
prevalence and interplay between factors that create
local coherence in newspaper text.
Specifically, in our study we examine how three
hypotheses formulated in prior work play out in the
Wall Street Journal texts in our corpus:
Hypothesis 1 Adjacent sentences that do not share
entities are related by non-elaboration discourse re-
lations [Poesio et al (2004) Sec. 5.2.2 Pg. 354].
Hypothesis 2 Adjacent sentences joined by non-
elaboration discourse relation have lower entity co-
herence: such pairs are less likely to mention the
same entities [Knott et al (2001) Sec. 7 Pg. 10].
Hypothesis 3 Almost all pairs of sentences in a co-
herent text either share entities or participate in non-
elaboration discourse relation (Knott et al, 2001;
Poesio et al, 2004).
None of these hypotheses are validated. We find
that only 38.65% of pairs that do not share enti-
ties participate in ?core? relations such as tempo-
ral, contingency or comparison; the rate of coref-
erence in these ?core? relations is similar to that in
weaker elaboration relations; about 30% of all sen-
tence pairs neither share entities nor participate in a
?core? discourse relation.
2 Data
In order to jointly analyze both discourse relations
and noun phrase coreference between adjacent sen-
tences, we combine annotations from two corpora,
OntoNotes and the Penn Discourse Treebank. The
two individual corpora are larger, but a smaller sub-
set of 590 Wall Street Journal articles appear in both.
All our analysis is for adjacent sentences within
paragraphs in this subset of texts.
Penn Discourse Treebank The Penn Discourse
Treebank (PDTB) (Prasad et al, 2008) is the largest
available corpus of annotations for discourse rela-
tions, covering one million words of the Wall Street
Journal (WSJ). In the PDTB, two kinds of discourse
relations are annotated. In explicit relations, a dis-
course connective such as ?because?, ?but? or ?so?
is present, as in the example below.
[Ex. 1] There is an incredible pressure on school systems
and teachers to raise test scores. So efforts to beat the tests are
also on the rise.
On the other hand, relations can also exist without
an explicit signal. In Ex. 2, it is clear that the second
sentence is the result of the event mentioned in the
first.
[Ex. 2] In July, the Environmental Protection Agency im-
posed a gradual ban on virtually all uses of asbestos. By 1997,
almost all remaining uses of cancer-causing asbestos will be
outlawed.
Such relations are called implicit relations. In the
PDTB, they are annotated between all adjacent sen-
tences within a paragraph which do not already par-
ticipate in an explicit discourse relation.
For both implicit and explicit relations, the se-
mantic sense of the discourse relation is assigned
from a hierarchy of senses. There are four classes
of discourse relations at the topmost general level.
The second level senses are shown within paran-
theses: Comparison (Concession, Contrast, Pragmatic
Concession/Contrast), Contingency (Cause, Condition,
Pragmatic Cause/Condition), Temporal (Asynchronous,
Synchronous) and Expansion (Alternative, Conjunction,
Exception, Instantiation, List, Restatement).
Some of the adjacent sentences in the texts, how-
ever, were found not to have a discourse relation be-
tween the events or propositions mentioned in them.
Instead, they were related because both sentences
mention the same entity, directly or indirectly, and
the second sentence provides some further descrip-
tion of that entity. An Entity Relation (EntRel) was
annotated for such sentence pairs as below.
[Ex. 3] Pierre Vinken, 61 years old, will join the board as
a nonexecutive director Nov. 29. Mr. Vinken is chairman of
Elsevier N.V., the Dutch publishing group.
OntoNotes For coreference information, we use
the WSJ portion of the OntoNotes corpus version 2.9
(Hovy et al, 2006) which contains 590 documents.
For these documents, we also have the PDTB anno-
tations available. In OntoNotes, all noun phrases?
pronouns, names and nominals are marked for coref-
erence without any limitation to specific semantic
categories.
3 Corpus study findings
For ease of presentation in the following analy-
sis, we will call the Expansion and Entity relations
?weak? and Temporal, Contingency and Compari-
314
0 to 100 0.41 500 to 1000 0.50
100 to 200 0.37 above 1000 0.51
200 to 500 0.48
Table 1: Proportion of sentence pairs that don?t share any
entities for different document lengths (in words)
Type Relation No shared entities
Core
Temporal 122 (2.98)
Contingency 752 (18.40)
Comparison 706 (17.27)
Weak Expansion 1870 (45.74)EntRel 638 (15.61)
Table 2: Distribution of the 4088 non-entity sharing sen-
tence pairs. The proportions are shown in brackets.
son relations ?core?, following the intuition that the
semantics of the latter class is much more clearly
defined. Implicit and explicit relations were not dis-
tinguished in the analysis.1
Hypothesis 1 The first hypothesis is that adja-
cent sentences that do not share entities participate
in core relations and so remain locally coherent.
Pairs of adjacent sentences that do not share any
entities are common. In prior work, Poesio et al
(2004) found that 58% of adjacent sentence pairs in
their corpus of museum object descriptions did not
have overlapping mentions of any entity. The distri-
bution in the WSJ texts is similar, as seen in Table 1.
Depending on the length of the article, 37% to 51%
of sentence pairs do not have any entity in common.2
Table 2 shows the distribution of discourse rela-
tions for the 4088 sentence pairs in the corpus that
do not share any entities. Contrary to expectation,
the majority of such pairs, 61%, are related by a
weak discourse relation. Especially unexpected is
the high percentage of entity relations (EntRel) that
don?t have actual entity overlap:
[Ex. 4] All four demonstrators were arrested. The law,
which Bush allowed to take effect without his signature, went
into force Friday.
[Ex. 5] Authorities in Hawaii said the wreckage of a missing
commuter plane with 20 people aboard was spotted in a remote
valley on the island of Molokai. There wasn?t any evidence of
survivors.
1For brevity we present combined results for both implicit
and explicit relations. However, most of our conclusions remain
the same when the two types are distinguished.
2There are around 100 documents in each length range.
Type Relation Total Share entities
Core
Temporal 365 243 (66.57)
Contingency 1570 818 (52.10)
Comparison 1477 771 (52.20)
Weak Expansion 3569 1699 (47.60)EntRel 1424 786 (55.20)
Table 3: Rate of entity sharing
Share entities No sharing
core relations 1832 (21.80) 1580 (18.80)
weak relations 2485 (29.56) 2508 (29.84)
Table 4: Total number (proportion) of sentence pairs in
the corpus in the given categories
Hypothesis 2 The second hypothesis states that
adjacent sentences joined by a core discourse rela-
tion are less likely to mention the same entities in
comparison to weak relations. But as Table 3 re-
veals, this is generally not the case.
Adjacent sentences in Temporal relation are very
likely to share entities?almost 70% of them do.
Over half of all Contingency and Comparison rela-
tions also share entities. But, the rates of sharing in
Comparison and Contingency relations are signifi-
cantly lower compared to Entity relations (under a
two-sided binomial test). Ex. 6 shows a Contin-
gency relation without entity sharing.
[Ex. 6] Without machines, good farms can?t get bigger. So
the potato crop, once 47 million tons, is down to 35.
However, adjacent sentences with Expansion rela-
tion turn out least likely to share entities. The entity
sharing rates of all the other relations were found to
be significantly higher than Expansion.
Hypothesis 3 Finally, we test the hypothesis that
the majority of adjacent sentences exhibit coherence
because they either share entities or form the argu-
ments of a core discourse relation.
This hypothesis is not supported in the WSJ data
(see Table 4): 30% of all sentence pairs are in a weak
discourse relation?Expansion or EntRel?and do
not have any entities in common. In a sense, nei-
ther of the theories of entity or relational coherence
can explain what mechanisms create the local coher-
ence for these pairs. In order to glean some insights
of how coherence is created there, we examine the
behavior of different types of Elaboration relations
(Table 5). There is a wide variation between the dif-
315
Alternative 67 (0.63) Instantiation 490 (0.34)
Restatement 960 (0.56) List 165 (0.28)
Conjunction 1021 (0.48) EntRel 1424 (0.55)
Table 5: Total number of different Expansion relations
and their coreference probability (in brackets)
ferent types of Expansions.
When the function of an expansion sentence is to
provide an alternative explanation or restate an ut-
terance, the probability of entity overlap is very high
and patterns similarily with entity relations (around
60%). Below is an example restatement sentence.
[Ex. 7] {Researchers in Belgium}r said {they}r have de-
veloped a genetic engineering technique for creating hybrid
plants for a number of crops, such as cotton, soybeans and
rice. {The scientists at Plant Genetic Systems N.V.}r isolated
a gene that could lead to a generation of plants possessing a
high-production trait.
However, the two classes?Instantiation and List
largely appear with very little entity overlap, 37%
and 29% respectively. An Instantiation relation is
used to provide an example and hence has little over-
lap with the previous sentence (Ex. 8 and 9).
[Ex. 8] There may be a few cases where the law breaking is
well pinpointed and so completely non-invasive of the rights of
others that it is difficult to criticize it. The case of Rosa Parks,
the black woman who refused to sit at the back of the bus, comes
to mind as an illustration.
[Ex. 9] The economy is showing signs of weakness, partic-
ularly among manufacturers. Exports, which played a key role
in fueling growth over the last two years, seem to have stalled.
List relations, on the other hand, connect sen-
tences where each of them elaborates on a common
proposition mentioned earlier in the discourse. Here
is an example five sentence paragraph with list rela-
tions but no entity repetition at all.
[Ex. 10] Designs call for a L-shaped structure with
a playground in the center. [Implicit List] There
will be recreation and movie rooms. [Implicit List]
Teens will be able to listen to music with head-
sets. [Implicit List] Study halls, complete with ref-
erence materials will be available. [Explicit List]
And there will be a nurse?s station and rooms for
children to meet with the social workers.
In Ex. 10, as well as some others, a broad no-
tion of entity coherence?bridging anaphora can be
applied. Poesio et al (2004) also note this fact, but
also say that such instances are very difficult to an-
notate reliably. Our work is based on coreference
annotations which can be marked with considerably
high inter-annotator agreement.
4 Conclusions
The recent release of corpora annotated for corefer-
ence and discourse relations for the same texts have
made possible to empirically assess claims about the
interaction between two types of local coherence:
relational and entity. We find that about half of the
pairs of adjacent sentences do not share any entities
at all, and about 60% are related by weak discourse
relations. We test the hypothesis from prior work
that these two types of coherence are complemen-
tary to each other and taken together explain most
local coherence. We find that the two types of co-
herence mechanisms are neither mutually exclusive
nor do they explain all the data. Future work in dis-
course analysis will need to develop better under-
standing of how the two types of coherence interact.
References
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of dis-
course. Computational Linguistics, 21(2):203?226.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In Proceedings of NAACL-HLT, pages 57?60.
N. Karamanis. 2007. Supplementing entity coherence
with local rhetorical relations for information order-
ing. Journal of Logic, Language and Information,
16(4):445?464.
A. Knott, J. Oberlander, M. O?Donnell, and C. Mellish.
2001. Beyond elaboration: the interaction of rela-
tions and focus in coherent text. In Text Representa-
tion: Linguistic and Psycholinguistic Aspects, chapter
7, pages 181?196.
W.C. Mann and S.A. Thompson. 1988. Rhetorical struc-
ture theory: Towards a functional theory of text orga-
nization. Text, 8.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. Hitze-
man. 2004. Centering: A parametric theory and its
instantiations. Computational Linguistics, pages 309?
363.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The penn discourse
treebank 2.0. In Proceedings of LREC?08.
316
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 54?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Automatic Metrics for Genre-specific Text Quality
Annie Louis
University of Pennsylvania
Philadelphia, PA 19103, USA
lannie@seas.upenn.edu
Abstract
To date, researchers have proposed different
ways to compute the readability and coher-
ence of a text using a variety of lexical, syn-
tax, entity and discourse properties. But these
metrics have not been defined with special rel-
evance to any particular genre but rather pro-
posed as general indicators of writing qual-
ity. In this thesis, we propose and evalu-
ate novel text quality metrics that utilize the
unique properties of different genres. We fo-
cus on three genres: academic publications,
news articles about science, and machine gen-
erated text, in particular the output from auto-
matic text summarization systems.
1 Introduction
Automatic methods to measure the writing quality of
a text can be quite useful for several applications, for
example search and recommendation systems, and
writing support and grading tools. There are two
main categories of prior work on this topic. The first
is studies on ?readability? which have proposed met-
rics to select texts appropriate (easy to read) for an
audience of given age and education level (Flesch,
1948; Collins-Thompson and Callan, 2004). These
metrics typically classify texts as suitable for adult
or child, or into a more fine-grained set of 12 ed-
ucational grade levels. The second line of work
are recent computational metrics to predict coher-
ence. These methods identify regularities in words
(Barzilay and Lee, 2004), entity coreference (Barzi-
lay and Lapata, 2008) and discourse relations (Pitler
and Nenkova, 2008) from a large collection of ar-
ticles and use these patterns to predict the coher-
ence. They assume a particular competency level
(adult educated readers) and also fix the text (typi-
cally news articles, which are appropriate for adult
readers). By removing the focus on age/education
level, these methods compute textual differences be-
tween good and poorly written texts as perceived by
a single audience level.
In my thesis, I propose a new definition ? text
quality: the overall well-written characteristic of an
article. It differs from prior work in three respects:
1. We consider a single fixed audience level and
the texts that audience is typically exposed to.
For example, a college educated reader of a
newspaper might find some articles better writ-
ten than others, even though he understands and
can read nearly all of them with ease.
2. It is a holistic property of texts. At a mini-
mum, at least four factors influence quality: the
content/topic that is discussed, sentence level-
grammaticality, discourse coherence and writ-
ing style. Here writing style refers to extra
properties introduced into the text by the au-
thor but do not necessarily interfere with co-
herence if not provided. For example, the use
of metaphors, examples and humour can have
connections with quality. Previous work on co-
herence metrics do not consider these aspects.
3. Such a property would also have genre-specific
dimensions: an academic article should above
all be clear and a thriller-story should be fast-
paced and interesting. Further even if the same
54
quality aspect is relevant for multiple genres, it
has higher weight in one versus another. Prior
readability and coherence studies were not pro-
posed with relvance to any particular genre.
These aspects make the investigation of text qual-
ity linguistically interesting because by definition
the focus is on a wide range of properties of the text
itself rather than appropriateness for a reader.
In this thesis, we propose computable measures
to capture genre-specific text quality. Our hypoth-
esis is that writing quality is a combination some
generic aspects that matter for most texts, such as
grammatical sentences, and other unique ones which
have high impact in a particular genre.
Specifically, we consider three genres which
have high relevance for writing quality research?
academic writing, science journalism and output of
automatic summarization systems.
Both academic writing and science news articles
describe science, but their audience is quite differ-
ent. Academic writing aims to clearly explain the
details of the research to other experts, while sci-
ence news conveys interesting research findings to
lay readers. This fact creates distinctive content and
writing style in the two genres. There is also a huge
opportunity in these genres for developing applica-
tions involving text quality, for example, authoring
tools for academic writing and information retrival
and recommendation for news articles. We also in-
clude a third genre?automatically generated sum-
maries. Here, when systems produce multi-sentence
text, they must ensure that the text is readable and
coherent. Automatic evaluation of content and lin-
guistic quality is therefore necessary for system de-
velopment in this genre.
2 Thesis Summary and Contributions
For this thesis, we only consider the discourse and
style components of text quality, aspects that have
received less focus in prior work. Sentence-level
problems have been widely explored and recently,
even specifically for academic writing (Dale and
Kilgarriff, 2010). We also do not consider content
in our work, for example, academic writing quality
also depends on the ideas and arguments presented
but these aspects are outside the scope of this thesis.
As defined previously, we focus on a fixed audience
level. We assume a reader at the top level of the
competency spectrum: an adult educated reader for
science news and automatic summaries, and for aca-
demic articles, an expert on the topic. This definition
has minimal focus on reader abilities and allows us
to analyze textual differences exclusively.
The specific contributions of this thesis are:
1. Defining text quality in terms of linguistic as-
pects rather than readability: Our work is the first
to propose a quality definition where well-written
nature is the central focus and including genre-
dependent aspects and writing style.
2. Investigating genre-specific metrics: This study
is also the first to design and evaluate genre-specific
features for text quality prediction. For each genre:
academic writing, science journalism and automatic
summaries, we develop metrics unique to the genre
and evaluate their ability to predict text quality both
individually and in combination with generic fea-
tures put forth in prior work.
3. Proposing new discourse-level features: In
prior work, there are discourse-based features based
on coreference, discourse relations and word co-
occurrence between adjacent sentences. We intro-
duce new features which capture aspects such as
organization of communicative goals and general-
specific nature of sentences.
Specifically, we introduce the following metrics:
a) Patterns in communicative goals (Section 5):
Every text has a purpose and the author uses a
sequence of communicative goals realized as sen-
tences to convey that purpose. We introduce a met-
ric that predicts coherence based on the size and se-
quence of communicative goals for a genre. This as-
pect is most relevant for research writing: academic
and science journalism because there is a clear goal
and well-defined purpose for these articles.
b) General-specific nature of sentences (Section
6): Some sentences in a text convey only general
content, others provide details and a well-written
text would have a certain balance between the two.
Particularly, while creating summaries, there is a
length contraint, so it cannot include all specific con-
tent but some information must be made more gen-
eral. We introduce a method to predict the specificity
for a sentence and examine how specificity and se-
quence of general-specific sentences is related to the
55
quality of automatic summaries.
c) Information cohesiveness (Section 7): This
idea is also proposed for automatic summaries, they
must have a focus and present a small set of ideas
with easy to understand links between them. We
show that cohesiveness properties (computed auto-
matically) of the source text to be summarized can
be linked to the expected content quality of sum-
maries that can be generated for that text. This work
will be extended to analyze the relationship of cohe-
siveness with ratings of focus for the summaries.
d) Aspects of style (Section 8): Here we inves-
tigate metrics beyond coherence and related to ex-
tra features included in the article. We consider the
genre of science journalism and investigate whether
surprise-invoking sentence construction, visual de-
scriptions and emotional content of the articles are
also correlated with perceived quality.
We will evaluate our approaches in two ways:
1. We investigate the extent to which genre-
specific metrics are indicative of text quality and
whether they complement generic features.
2. We also examine how unique these metrics are
for a given genre, for example: are surprising arti-
cles always considered well-written even if they are
not science-news? For this analysis, we will con-
sider a set of randomly selected news texts (no genre
division) with text quality ratings. On this set, we
will test the performance of generic and each set
of genre-specific metrics. We expect that on this
data, the generic features would be best with little
improvement from the genre-specific metrics.
So far, we have designed some of the metrics that
we described above and have found them to be pre-
dictive of writing quality. We will carry out exten-
sive evaluation of these measures in future work.
3 Related work
Early readability metrics used sentence length, num-
ber of syllables in words and number of ?easy?
words to distinguish texts from different grade lev-
els (Flesch, 1948; Gunning, 1952; Dale and Chall,
1948). Other measures are based on word familiarity
(Collins-Thompson and Callan, 2004; Si and Callan,
2001), difficulty of concepts (Zhao and Kan, 2010)
and features of sentence syntax (Schwarm and Os-
tendorf, 2005). There are also readability studies for
audience distinctions other than grade levels. Feng
et al (2009) consider adult readers with intellectual
disability and therefore introduce features such as
the number of entities a person should keep in work-
ing memory for that text and how far entity links
stretch. Heilman et al (2007) show that grammati-
cal features make a bigger impact while predicting
readability for second language learners in contrast
to native speakers.
Newer coherence measures do not focus on reader
abilities. They are typically run on news articles
and assume an adult audience. They show that
word co-occurrence (Soricut and Marcu, 2006), sub-
topic structure (Barzilay and Lee, 2004), discourse
relations (Pitler and Nenkova, 2008; Lin et al,
2011) and coreference patterns (Barzilay and Lap-
ata, 2008) learn from large corpora can be used to
predict coherence.
But prior metrics are not proposed as unique to
any genre. Some metrics using word patterns (Si and
Callan, 2001; Barzilay and Lee, 2004) are domain-
dependent in that they require documents from the
target domain for training. But they can be trained
for any domain in this manner.
However recent work show that genre-specific
indicators could be quite useful for applications.
McIntyre and Lapata (2009) automatically generate
short children?s stories using patterns of event and
entity co-occurrences. They find that people judge
their stories as better when the text is optimized not
only for coherence and but also its interesting nature.
They use a supervised approach to predict the inter-
est value for a story during the generation process.
Burstein et al (2010) find that for predicting the co-
herence of student essays, better accuracies can be
obtained by augmenting generic coherence metrics
with features related to student writing such as word
variety and spelling errors.
In my own work on automatic evaluation of sum-
maries (Pitler et al, 2010), I have observed the im-
pact of genre. We consider a corpus of summaries
written by people and those produced by automatic
systems. Psycholinguistic metrics previously pro-
posed for analyzing coherence of human texts work
successfully on human summaries but are less ac-
curate for system summaries. Similarly, metrics
which predict the fluency of machine translations
accurately, work barely above baseline for judging
the grammaticality of sentences from human sum-
56
maries. But they give high accuracies on machine
summary sentences. So for machine and human gen-
erated text, clearly different features matter.
4 Corpora for text quality
For the automatic summarization genre, several
years of evaluation workshops organized by NIST1
have created large-scale datasets of automatic sum-
maries rated manually by people for content and lin-
guistic quality. We utilize this data for our experi-
ments but such corpora do not exist for other genres.
For academic writing, we plan to use a collection
of biology journal articles marked with the impact
factor of the journal. The intuition is that the pop-
ular journals are more competitive and so the writ-
ing is on average better than less impactful venues.
It is however not a direct measure of text quality.
For some of our experiments done so far, we have
taken an approach that is common with prior studies
on coherence (Barzilay and Lee, 2004; Barzilay and
Lapata, 2008; Lin et al, 2011). We take an original
article and create a random permutation of its sen-
tences, the latter we consider as an incoherent article
and the original version as coherent.
For science news, we expect that Amazon Me-
chanical Turk will be a suitable platform for obtain-
ing ratings of popular and interesting articles from
the target audience. We also plan to use proxies such
as lists of most emailed/viewed articles from news
websites. Here the negative examples would be
other articles published during the same day/period
but not appearing in the popular article list.
5 Patterns in communicative goals
Consider the related work section of a conference
paper. One might suppose that a good structure for
this section would contain a description of an at-
tribute of the current work, followed by previous
work on the topic and then reporting how the current
work is different and addresses shortcomings if any
of prior work. In fact, this intuition of seeing texts
as a sequence of semantic zones is well-understood
for the academic writing genre. Prior research has
identified that a small set of argumentative zones ex-
ist in academic articles such as motivation, results,
prior work, speculations and descriptions. They also
1http://www.nist.gov/tac/
found that sentences could be manually annotated
into zones with high agreement and automatically
predicting the zone for a sentence can also be done
with high accuracy (Teufel and Moens, 2000; Li-
akata et al, 2010). We hypothesize that these zones
would also have a certain distribution and sequence
in well-written articles versus others and propose a
metric based on this aspect for the academic writing
and science journalism genres.
Rather than using a predefined set of communica-
tive goals, we develop an unsupervised technique
to identify analogs to semantic zones and use the
patterns in zones to predict coherence (Louis and
Nenkova, 2012a). Our key idea is that the syntax
of a sentence can be a useful proxy for its commu-
nicative goal. For example, questions and definition
sentences have unique syntax. We extend this idea
to a large scale analysis. Our model represents a sen-
tence either using productions from its constituency
parse tree or as a sequence of phrasal nodes. Then
we employ two methods that learn patterns in these
representations from a collection of articles. The
first local method detects patterns in the syntax of
adjacent sentences. The second approach is global,
where sentences are first grouped into clusters based
on syntactic similarity and a Hidden Markov Model
is used to record patterns. Each hidden state is as-
sumed to generate the syntax of sentences from a
particular zone.
We have evaluated our method on conference
publications from the ACL anthology. Our results
indicate that we can distinguish an original introduc-
tion, abstract or related work section from a corre-
sponding perturbed version (where the sentences are
randomly permuted and is therefore incoherent text)
with accuracies of 64 to 74% over a 50% baseline.
6 General-specific nature of sentences
In any article, some sentences convey the topic at
a high level with other sentences providing details
such as justification and examples. The idea is par-
ticularly relevant for summaries. Since summaries
are much shorter than their source documents, they
cannot include all the details from the source. Some
details have to be omitted and others made more
general. So we explore the preferred degree of
general-specific content and its relationship to text
quality for summaries.
57
We developed a classifier to distinguish between
general and specific sentences from news articles
(Louis and Nenkova, 2011a; Louis and Nenkova,
2012b). The classifier uses features such as the word
specificity, presence of named entities, word polar-
ity, counts of different phrase types, sentence length,
likelihood under language models and the identities
of the words themselves. For example, sentences
with named entities tended to be specific whereas
sentences with shorter verb phrases and more polar-
ity words were general. This classifier was trained
on sentences multiply annotated by people as gen-
eral or specific and produces an accuracy of about
79%. Further the classifier confidence was found to
be indicative of the annotator agreement on the sen-
tences; when there was high agreement that a sen-
tence was either general or specific, the classifier
also made a very confident prediction for the correct
class. So our system also provides a graded score
for specificity rather than binary predictions.
Using the classifier we analyzed a large corpus of
news summaries created by people and by automatic
systems (Louis and Nenkova, 2011b). We found
that summaries written by people have more general
content than automatic summaries. Similarly, when
people were asked to rate automatic summaries for
content quality, they gave higher scores to general
summaries than specific. On the linguistic quality
side an opposite trend was found. Summaries that
were more specific had higher scores. Our examina-
tions revealed that general sentences, since they are
topic oriented and high level, need to be followed
by proper substantiation and details. But automatic
systems are rather poor at achieving such ordering.
So even though more general content is preferred in
summaries, proper ordering of general-specific sen-
tences is needed to create the right effect.
7 Information cohesiveness
If an article has too many ideas it would be difficult
to read. Also if the ideas were not closely related
in the article that would create additional difficulty.
This aspect is important for machine generated text:
an automatic summary should focus on a few main
aspects rather than present a bag of many unrelated
facts. In fact, in large scale evaluation workshops,
automatic summaries are also manually graded for a
?focus? aspect. For this purpose, we want to identify
metrics which can indicate cohesiveness and focus
of an article. In our studies so far, we have have
developed cohesiveness metrics for clusters of arti-
cles (Nenkova and Louis, 2008; Louis and Nenkova,
2009). In future work, we will explore how these
metrics work for individual articles.
Information quality also arises in the context of
source documents given for automatic summariza-
tion. Particularly for systems which summarize on-
line news, the input is created by clustering together
news on the same topic from different sources. For
example, a cluster may be created for the Japanese
earthquake and aftermath. When the period covered
is too large or when the documents discuss many
different opinions and ideas it becomes hard for a
system to point out the most relevant facts. So one
proxy for cohesiveness of the input cluster is the av-
erage quality of a number of automatic summaries
produced for it by different methods. If most of
these methods fail to produce a good summary, then
that input can be deemed as difficult and incohesive.
We used a large collection of inputs, their au-
tomatic summaries and summary scores from the
DUC workshops. We computed the average content
quality score given by people to each summary and
computed the average performance on summaries
created for the same input. This value represents the
expected system performance for that input and we
develop features to predict the same. We simplify
the task as binary prediction, average system perfor-
mance above mean value ? low difficulty, and high
difficulty otherwise.
One indicative feature was the entropy of the dis-
tribution of words in the input. When the entropy
was low, the difficulty was less since there are few
main ideas to summarize. Another useful feature
was the divergence computed between the word dis-
tribution in an input and that of a random collection
of documents not on any topic. If the input distri-
bution was closer to random documents it indicates
the lack of a coherent topic for the source cluster
and such inputs were under the hard category. We
envision that similar features might help to predict
judgements of focus for automatic summaries.
8 Current and future work
For future work, we want to focus on metrics related
to style of writing. We will do this analysis for sci-
58
ence news articles since journalists employ creative
ways to convey technical research content to non-
experts readers. For example, authors use analogies
and visual language and incorporate a story line. We
also noticed that some of the most emailed articles
are entertaining and even contain humor. Two exam-
ple snippets from such articles are provided below to
demonstrate some of our intuitions about text quality
in this genre. Our aim is to obtain lexical and syn-
tactic correlates that capture some of these unique
factors for this domain.
[1]... caused by defects in the cilia?solitary slivers
that poke out of almost every cell in the body. They are
not the wisps that wave Rockette-like in our airways.
[2] News flash: we?re boring. New research that makes
creative use of sensitive location-tracking data from cell-
phones in Europe suggests that most people can be found
in one of just a few locations at any time.
Future work will also include extensive evaluation
of our proposed models.
References
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1?34.
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models, with applications to generation
and summarization. In Proceedings of NAACL-HLT,
pages 113?120.
J. Burstein, J. Tetreault, and S. Andreyev. 2010. Using
entity-based features to model coherence in student es-
says. In Proceedings of HLT-NAACL, pages 681?684.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT-NAACL, pages 193?200.
E. Dale and J. S. Chall. 1948. A formula for predicting
readability. Edu. Research Bulletin, 27(1):11?28.
R. Dale and A. Kilgarriff. 2010. Helping our own:
text massaging for computational linguistics as a new
shared task. In Proceedings of INLG, pages 263?267.
L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cog-
nitively motivated features for readability assessment.
In Proceedings of EACL, pages 229?237.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32:221 ? 233.
R. Gunning. 1952. The technique of clear writing.
McGraw-Hill; Fouth Printing edition.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammati-
cal features to improve readability measures for first
and second language texts. In Proceedings of HLT-
NAACL, pages 460?467.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of LREC.
Z. Lin, H. Ng, and M. Kan. 2011. Automatically evalu-
ating text coherence using discourse relations. In Pro-
ceedings of ACL-HLT, pages 997?1006.
A. Louis and A. Nenkova. 2009. Performance con-
fidence estimation for automatic summarization. In
Proceedings of EACL, pages 541?548.
A. Louis and A. Nenkova. 2011a. Automatic identi-
fication of general and specific sentences by leverag-
ing discourse annotations. In Proceedings of IJCNLP,
pages 605?613.
A. Louis and A. Nenkova. 2011b. Text specificity and
impact on quality of news summaries. In Proceedings
of the Workshop on Monolingual Text-To-Text Genera-
tion, pages 34?42.
A. Louis and A. Nenkova. 2012a. A coherence model
based on syntactic patterns. Technical Report, Univer-
sity of Pennsylvania.
A. Louis and A. Nenkova. 2012b. A corpus of general
and specific sentences from news. In Proceedings of
LREC.
N. McIntyre and M. Lapata. 2009. Learning to tell tales:
A data-driven approach to story generation. In Pro-
ceedings of ACL-IJCNLP, pages 217?225.
A. Nenkova and A. Louis. 2008. Can you summa-
rize this? identifying correlates of input difficulty for
multi-document summarization. In Proceedings of
ACL-HLT, pages 825?833.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of EMNLP, pages 186?195.
E. Pitler, A. Louis, and A. Nenkova. 2010. Automatic
evaluation of linguistic quality in multi-document
summarization. In Proceedings of ACL.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of ACL, pages 523?
530.
L. Si and J. Callan. 2001. A statistical model for scien-
tific readability. In Proceedings of CIKM, pages 574?
576.
R. Soricut and D. Marcu. 2006. Discourse generation us-
ing utility-trained coherence models. In Proceedings
of COLING-ACL, pages 803?810.
S. Teufel and M. Moens. 2000. What?s yours and what?s
mine: determining intellectual attribution in scientific
text. In Proceedings of EMNLP, pages 9?17.
J. Zhao and M. Kan. 2010. Domain-specific iterative
readability computation. In Proceedings of JDCL,
pages 205?214.
59
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 544?554,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Linguistic Quality in Multi-Document
Summarization
Emily Pitler, Annie Louis, Ani Nenkova
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
epitler,lannie,nenkova@seas.upenn.edu
Abstract
To date, few attempts have been made
to develop and validate methods for au-
tomatic evaluation of linguistic quality in
text summarization. We present the first
systematic assessment of several diverse
classes of metrics designed to capture var-
ious aspects of well-written text. We train
and test linguistic quality models on con-
secutive years of NIST evaluation data in
order to show the generality of results. For
grammaticality, the best results come from
a set of syntactic features. Focus, coher-
ence and referential clarity are best evalu-
ated by a class of features measuring local
coherence on the basis of cosine similarity
between sentences, coreference informa-
tion, and summarization specific features.
Our best results are 90% accuracy for pair-
wise comparisons of competing systems
over a test set of several inputs and 70%
for ranking summaries of a specific input.
1 Introduction
Efforts for the development of automatic text sum-
marizers have focused almost exclusively on im-
proving content selection capabilities of systems,
ignoring the linguistic quality of the system out-
put. Part of the reason for this imbalance is the
existence of ROUGE (Lin and Hovy, 2003; Lin,
2004), the system for automatic evaluation of con-
tent selection, which allows for frequent evalua-
tion during system development and for report-
ing results of experiments performed outside of
the annual NIST-led evaluations, the Document
Understanding Conference (DUC)1 and the Text
Analysis Conference (TAC)2. Few metrics, how-
ever, have been proposed for evaluating linguistic
1http://duc.nist.gov/
2http://www.nist.gov/tac/
quality and none have been validated on data from
NIST evaluations.
In their pioneering work on automatic evalua-
tion of summary coherence, Lapata and Barzilay
(2005) provide a correlation analysis between hu-
man coherence assessments and (1) semantic re-
latedness between adjacent sentences and (2) mea-
sures that characterize how mentions of the same
entity in different syntactic positions are spread
across adjacent sentences. Several of their models
exhibit a statistically significant agreement with
human ratings and complement each other, yield-
ing an even higher correlation when combined.
Lapata and Barzilay (2005) and Barzilay and
Lapata (2008) both show the effectiveness of
entity-based coherence in evaluating summaries.
However, fewer than five automatic summarizers
were used in these studies. Further, both sets
of experiments perform evaluations of mixed sets
of human-produced and machine-produced sum-
maries, so the results may be influenced by the
ease of discriminating between a human and ma-
chine written summary. Therefore, we believe it is
an open question how well these features predict
the quality of automatically generated summaries.
In this work, we focus on linguistic quality eval-
uation for automatic systems only. We analyze
how well different types of features can rank good
and poor machine-produced summaries. Good
performance on this task is the most desired prop-
erty of evaluation metrics during system develop-
ment. We begin in Section 2 by reviewing the
various aspects of linguistic quality that are rel-
evant for machine-produced summaries and cur-
rently used in manual evaluations. In Section 3,
we introduce and motivate diverse classes of fea-
tures to capture vocabulary, sentence fluency, and
local coherence properties of summaries. We eval-
uate the predictive power of these linguistic qual-
ity metrics by training and testing models on con-
secutive years of NIST evaluations (data described
544
in Section 4). We test the performance of differ-
ent sets of features separately and in combination
with each other (Section 5). Results are presented
in Section 6, showing the robustness of each class
and their abilities to reproduce human rankings of
systems and summaries with high accuracy.
2 Aspects of linguistic quality
We focus on the five aspects of linguistic qual-
ity that were used to evaluate summaries in DUC:
grammaticality, non-redundancy, referential clar-
ity, focus, and structure/coherence.3 For each of
the questions, all summaries were manually rated
on a scale from 1 to 5, in which 5 is the best.
The exact definitions that were provided to the
human assessors are reproduced below.
Grammaticality: The summary should have no datelines,
system-internal formatting, capitalization errors or obviously
ungrammatical sentences (e.g., fragments, missing compo-
nents) that make the text difficult to read.
Non-redundancy: There should be no unnecessary repeti-
tion in the summary. Unnecessary repetition might take the
form of whole sentences that are repeated, or repeated facts,
or the repeated use of a noun or noun phrase (e.g., ?Bill Clin-
ton?) when a pronoun (?he?) would suffice.
Referential clarity: It should be easy to identify who or what
the pronouns and noun phrases in the summary are referring
to. If a person or other entity is mentioned, it should be clear
what their role in the story is. So, a reference would be un-
clear if an entity is referenced but its identity or relation to
the story remains unclear.
Focus: The summary should have a focus; sentences should
only contain information that is related to the rest of the sum-
mary.
Structure and Coherence: The summary should be well-
structured and well-organized. The summary should not just
be a heap of related information, but should build from sen-
tence to sentence to a coherent body of information about a
topic.
These five questions get at different aspects of
what makes a well-written text. We therefore pre-
dict each aspect of linguistic quality separately.
3 Indicators of linguistic quality
Multiple factors influence the linguistic quality of
text in general, including: word choice, the ref-
erence form of entities, and local coherence. We
extract features which serve as proxies for each of
the factors mentioned above (Sections 3.1 to 3.5).
In addition, we investigate some models of gram-
maticality (Chae and Nenkova, 2009) and coher-
ence (Graesser et al, 2004; Soricut and Marcu,
2006; Barzilay and Lapata, 2008) from prior work
(Sections 3.6 to 3.9).
3http://www-nlpir.nist.gov/projects/
duc/duc2006/quality-questions.txt
All of the features we investigate can be com-
puted automatically directly from text, but some
require considerable linguistic processing. Several
of our features require a syntactic parse. To extract
these, all summaries were parsed by the Stanford
parser (Klein and Manning, 2003).
3.1 Word choice: language models
Psycholinguistic studies have shown that people
read frequent words and phrases more quickly
(Haberlandt and Graesser, 1985; Just and Carpen-
ter, 1987), so the words that appear in a text might
influence people?s perception of its quality. Lan-
guage models (LM) are a way of computing how
familiar a text is to readers using the distribution
of words from a large background corpus. Bigram
and trigram LMs additionally capture grammati-
cality of sentences using properties of local tran-
sitions between words. For this reason, LMs are
widely used in applications such as generation and
machine translation to guide the production of sen-
tences. Judging from the effectiveness of LMs in
these applications, we expect that they will pro-
vide a strong baseline for the evaluation of at least
some of the linguistic quality aspects.
We built unigram, bigram, and trigram lan-
guage models with Good-Turing smoothing over
the New York Times (NYT) section of the English
Gigaword corpus (over 900 million words). We
used the SRI Language Modeling Toolkit (Stol-
cke, 2002) for this purpose. For each of the three
ngram language models, we include the min, max,
and average log probability of the sentences con-
tained in a summary, as well as the overall log
probability of the entire summary.
3.2 Reference form: Named entities
This set of features examines whether named enti-
ties have informative descriptions in the summary.
We focus on named entities because they appear
often in summaries of news documents and are of-
ten not known to the reader beforehand. In addi-
tion, first mentions of entities in text introduce the
entity into the discourse and so must be informa-
tive and properly descriptive (Prince, 1981; Frau-
rud, 1990; Elsner and Charniak, 2008).
We run the Stanford Named Entity Recognizer
(Finkel et al, 2005) and record the number of
PERSONs, ORGANIZATIONs, and LOCATIONs.
First mentions to people Feature exploration on
our development set found that under-specified
545
references to people are much more disruptive
to a summary than short references to organiza-
tions or locations. In fact, prior work in Nenkova
and McKeown (2003) found that summaries that
have been rewritten so that first mentions of peo-
ple are informative descriptions and subsequent
mentions are replaced with more concise reference
forms are overwhelmingly preferred to summaries
whose entity references have not been rewritten.
In this class, we include features that reflect
the modification properties of noun phrases (NPs)
in the summary that are first mentions to people.
Noun phrases can include pre-modifiers, apposi-
tives, prepositional phrases, etc. Rather than pre-
specifying all the different ways a person expres-
sion can be modified, we hoped to discover the
best patterns automatically, by including features
for the average number of each Part of Speech
(POS) tag occurring before, each syntactic phrase
occurring before4, each POS tag occurring after,
and each syntactic phrase occurring after the head
of the first mention NP for a PERSON. To measure
if the lack of pre or post modification is particu-
larly detrimental, we also include the proportion
of PERSON first mention NPs with no words be-
fore and with no words after the head of the NP.
Summarization specific Most summarization
systems today are extractive and create summaries
using complete sentences from the source docu-
ments. A subsequent mention of an entity in a
source document which is extracted to be the first
mention of the entity in the summary is proba-
bly not informative enough. For each type of
named entity (PERSON, ORGANIZATION, LO-
CATION), we separately record the number of in-
stances which appear as first mentions in the sum-
mary but correspond to non-first mentions in the
source documents.
3.3 Reference form: NP syntax
Some summaries might not include people and
other named entities at all. To measure how en-
tities are referred to more generally, we include
features about the overall syntactic patterns found
in NPs: the average number of each POS tag and
each syntactic phrase occurring inside NPs.
4We define a linear order based on a preorder traversal of
the tree, so syntactic phrases which dominate the head are
considered occurring before the head.
3.4 Local coherence: Cohesive devices
In coherent text, constituent clauses and sentences
are related and depend on each other for their in-
terpretation. Referring expressions such as pro-
nouns link the current utterance to those where the
entities were previously mentioned. In addition,
discourse connectives such as ?but? or ?because?
relate propositions or events expressed by differ-
ent clauses or sentences. Both these categories
are known cohesive or linking devices in human-
produced text (Halliday and Hasan, 1976). The
mere presence of such items in a text would be in-
dicative of better structure and coherence.
We compute a number of shallow features that
provide a cheap way of capturing the above intu-
itions: the number of demonstratives, pronouns,
and definite descriptions as well as the number of
sentence-initial discourse connectives.
3.5 Local coherence: Continuity
This class of linguistic quality indicators is a com-
bination of factors related to coreference, adjacent
sentence similarity, and summary-specific context
of surface cohesive devices.
Summarization specific Extractive multi-
document summaries often lack appropriate
antecedents for pronouns and proper context for
the use of discourse connectives.
In fact, early work in summarization (Paice,
1980; Paice, 1990) has pointed out that the pres-
ence of cohesive devices described in the previous
section might in fact be the source of problems.
A manual analysis of automatic summaries (Ot-
terbacher et al, 2002) also revealed that anaphoric
references that cannot be resolved and unclear dis-
course relations constitute more than 30% of all
revisions required to manually rewrite summaries
into a more coherent form.
To identify these potential problems, we adapt
the features for surface cohesive devices to indi-
cate whether referring expressions and discourse
connectives appear in the summary with the same
context as in the input documents.
For each of the cohesive devices discussed in
Section 3.4?demonstratives, pronouns, definite
descriptions, and sentence-initial discourse con-
nectives?we compare the previous sentence in
the summary with the previous sentence in the in-
put article. Two features are computed for each
type of cohesive device: (1) number of times the
preceding sentence in the summary is the same
546
as the preceding sentence in the input and (2) the
number of times the preceding sentence in sum-
mary is different from that in the input. Since
the previous sentence in the input text often con-
tains the antecedent of pronouns in the current
sentence, if the previous sentence from the input
is also included in the summary, the pronoun is
highly likely to have a proper antecedent.
We also compute the proportion of adjacent sen-
tences in the summary that were extracted from the
same input document.
Coreference Steinberger et al (2007) compare the
coreference chains in input documents and in sum-
maries in order to locate potential problems. We
instead define a set of more general features re-
lated to coreference that are not specific to sum-
marization and are applicable for any text. Our
features check the existence of proper antecedents
for pronouns in the summary without reference to
the text of the input documents.
We use the publicly available pronoun reso-
lution system described in Charniak and Elsner
(2009) to mark possible antecedents for pronouns
in the summary. We then compute as features the
number of times an antecedent for a pronoun was
found in the previous sentence, in the same sen-
tence, or neither. In addition, we modified the pro-
noun resolution system to also output the probabil-
ity of the most likely antecedent and include the
average antecedent probability for the pronouns
in the text. Automatic coreference systems are
trained on human-produced texts and we expect
their accuracies to drop when applied to automat-
ically generated summaries. However, the predic-
tions and confidence scores still reflect whether
or not possible antecedents exist in previous sen-
tences that match in gender/number, and so may
still be useful for coherence evaluation.
Cosine similarity We use cosine similarity to
compute the overlap of words in adjacent sen-
tences si and si+1 as a measure of continuity.
cos? =
vsi .vsi+1
||vsi ||||vsi+1 ||
(1)
The dimensions of the two vectors (vsi and
vsi+1) are the total number of word types from
both sentences si and si+1. Stop words were re-
tained. The value of each dimension for a sentence
is the number of tokens of that word type in that
sentence. We compute the min, max, and average
value of cosine similarity over the entire summary.
While some repetition is beneficial for cohe-
sion, too much repetition leads to redundancy in
the summary. Cosine similarity is thus indicative
of both continuity and redundancy.
3.6 Sentence fluency: Chae and Nenkova
(2009)
We test the usefulness of a suite of 38 shallow
syntactic features studied by Chae and Nenkova
(2009). These features are weakly but signif-
icantly correlated with the fluency of machine
translated sentences. These include sentence
length, number of fragments, average lengths of
the different types of syntactic phrases, total length
of modifiers in noun phrases, and various other
syntactic features. We expect that these structural
features will be better at detecting ungrammatical
sentences than the local language model features.
Since all of these features are calculated over in-
dividual sentences, we use the average value over
all the sentences in a summary in our experiments.
3.7 Coh-Metrix: Graesser et al (2004)
The Coh-Metrix tool5 provides an implementation
of 54 features known in the psycholinguistic lit-
erature to correlate with the coherence of human-
written texts (Graesser et al, 2004). These include
commonly used readability metrics based on sen-
tence length and number of syllables in constituent
words. Other measures implemented in the sys-
tem are surface text properties known to contribute
to text processing difficulty. Also included are
measures of cohesion between adjacent sentences
such as similarity under a latent semantic analysis
(LSA) model (Deerwester et al, 1990), stem and
content word overlap, syntactic similarity between
adjacent sentences, and use of discourse connec-
tives. Coh-Metrix has been designed with the
goal of capturing properties of coherent text and
has been used for grade level assessment, predict-
ing student essay grades, and various other tasks.
Given the heterogeneity of features in this class,
we expect that they will provide reasonable accu-
racies for all the linguistic quality measures. In
particular, the overlap features might serve as a
measure of redundancy and local coherence.
5http://cohmetrix.memphis.edu/
547
3.8 Word coherence: Soricut and Marcu
(2006)
Word co-occurrence patterns across adjacent sen-
tences provide a way of measuring local coherence
that is not linguistically informed but which can
be easily computed using large amounts of unan-
notated text (Lapata, 2003; Soricut and Marcu,
2006). Word coherence can be considered as the
analog of language models at the inter-sentence
level. Specifically, we used the two features in-
troduced by Soricut and Marcu (2006).
Soricut and Marcu (2006) make an analogy to
machine translation: two words are likely to be
translations of each other if they often appear in
parallel sentences; in texts, two words are likely to
signal local coherence if they often appear in ad-
jacent sentences. The two features we computed
are forward likelihood, the likelihood of observ-
ing the words in sentence si conditioned on si?1,
and backward likelihood, the likelihood of observ-
ing the words in sentence si conditioned on sen-
tence si+1. ?Parallel texts? of 5 million adjacent
sentences were extracted from the NYT section of
GigaWord. We used the GIZA++6 implementa-
tion of IBM Model 1 to align the words in adjacent
sentences and obtain all relevant probabilities.
3.9 Entity coherence: Barzilay and Lapata
(2008)
Linguistic theories, and Centering theory (Grosz
et al, 1995) in particular, have hypothesized that
the properties of the transition of attention from
entities in one sentence to those in the next, play a
major role in the determination of local coherence.
Barzilay and Lapata (2008), inspired by Center-
ing, proposed a method to compute the local co-
herence of texts on the basis of the sequences of
entity mentions appearing in them.
In their Entity Grid model, a text is represented
by a matrix with rows corresponding to each sen-
tence in a text, and columns to each entity men-
tioned anywhere in the text. The value of a cell
in the grid is the entity?s grammatical role in that
sentence (Subject, Object, Neither, or Absent). An
entity transition is a particular entity?s role in two
adjacent sentences. The actual entity coherence
features are the fraction of each type of these tran-
sitions in the entire entity grid for the text. One
would expect that coherent texts would contain
a certain distribution of entity transitions which
6http://www.fjoch.com/GIZA++.html
would differ from those in incoherent sequences.
We use the Brown Coherence Toolkit7 (Elsner
et al, 2007) to construct the grids. The tool does
not perform full coreference resolution. Instead,
noun phrases are considered to refer to the same
entity if their heads are identical.
Entity coherence features are the only ones that
have been previously applied with success for pre-
dicting summary coherence. They can therefore
be considered to be the state-of-the-art approach
for automatic evaluation of linguistic quality.
4 Summarization data
For our experiments, we use data from the
multi-document summarization tasks of the Doc-
ument Understanding Conference (DUC) work-
shops (Over et al, 2007).
Our training and development data comes from
DUC 2006 and our test data from DUC 2007.
These were the most recent years in which the
summaries were evaluated according to specific
linguistic quality questions. Each input consists
of a set of 25 related documents on a topic and the
target length of summaries is 250 words.
In DUC 2006, there were 50 inputs to be sum-
marized and 35 summarization systems which par-
ticipated in the evaluation. This included 34 au-
tomatic systems submitted by participants, and a
baseline system that simply extracted the lead-
ing sentences from the most recent article. In
DUC 2007, there were 45 inputs and 32 different
summarization systems. Apart from the leading
sentences baseline, a high performance automatic
summarizer from a previous year was also used
as a baseline. All these automatic systems are in-
cluded in our evaluation experiments.
4.1 System performance on linguistic quality
Each summary was evaluated according to the
five linguistic quality questions introduced in Sec-
tion 2: grammaticality, non-redundancy, referen-
tial clarity, focus, and structure. For each of these
questions, all summaries were manually rated on a
scale from 1 to 5, in which 5 is the best.
The distributions of system scores in the 2006
data are shown in Figure 1. Systems are currently
the worst at structure, middling at referential clar-
ity, and relatively better at grammaticality, focus,
7http://www.cs.brown.edu/
?
melsner/
manual.html
548
Figure 1: Distribution of system scores on the five
linguistic quality questions
Gram Non-redun Ref Focus Struct
Content .02 -.40 * .29 .28 .09
Gram .38 * .25 .24 .54 *
Non-redun -.07 -.09 .27
Ref .89 * .76 *
Focus .80 *
Table 1: Spearman correlations between the man-
ual ratings for systems averaged over the 50 inputs
in 2006; * p < .05
and non-redundancy. Structure is the aspect of lin-
guistic quality where there is the most room for
improvement. The only system with an average
structure score above 3.5 in DUC 2006 was the
leading sentences baseline system.
As can be expected, people are unlikely to be
able to focus on a single aspect of linguistic quality
exclusively while ignoring the rest. Some of the
linguistic quality ratings are significantly corre-
lated with each other, particularly referential clar-
ity, focus, and structure (Table 1).
More importantly, the systems that produce
summaries with good content8 are not necessar-
ily the systems producing the most readable sum-
maries. Notice from the first row of Table 1 that
none of the system rankings based on these mea-
sures of linguistic quality are significantly posi-
tively correlated with system rankings of content.
The development of automatic linguistic quality
measurements will allow researchers to optimize
both content and linguistic quality.
8as measured by summary responsiveness ratings on a 1
to 5 scale, without regard to linguistic quality
5 Experimental setup
We use the summaries from DUC 2006 for train-
ing and feature development and DUC 2007
served as the test set. Validating the results on con-
secutive years of evaluation is important, as results
that hold for the data in one year might not carry
over to the next, as happened for example in Con-
roy and Dang (2008)?s work.
Following Barzilay and Lapata (2008), we re-
port summary ranking accuracy as the fraction of
correct pairwise rankings in the test set.
We use a Ranking SVM (SV M light (Joachims,
2002)) to score summaries using our features. The
Ranking SVM seeks to minimize the number of
discordant pairs (pairs in which the gold stan-
dard has x1 ranked strictly higher than x2, but the
learner ranks x2 strictly higher than x1). The out-
put of the ranker is always a real valued score, so a
global rank order is always obtained. The default
regularization parameter was used.
5.1 Combining predictions
To combine information from the different feature
classes, we train a meta ranker using the predic-
tions from each class as features.
First, we use a leave-one out (jackknife) pro-
cedure to get the predictions of our features for
the entire 2006 data set. To predict rankings of
systems on one input, we train all the individual
rankers, one for each of the classes of features in-
troduced above, on data from the remaining in-
puts. We then apply these rankers to the sum-
maries produced for the held-out input. By repeat-
ing this process for each input in turn, we obtain
the predicted scores for each summary.
Once this is done, we use these predicted scores
as features for the meta ranker, which is trained on
all 2006 data. To test on a new summary pair in
2007, we first apply each individual ranker to get
its predictions, and then apply the meta ranker.
In either case (meta ranker or individual feature
class), all training is performed on 2006 data, and
all testing is done on 2007 data which guarantees
the results generalize well at least from one year
of evaluation to the next.
5.2 Evaluation of rankings
We examine the predictive power of our features
for each of the five linguistic quality questions in
two settings. In system-level evaluation, we would
like to rank all participating systems according to
549
their performance on the entire test set. In input-
level evaluation, we would like to rank all sum-
maries produced for a single given input.
For input-level evaluation, the pairs are formed
from summaries of the same input. Pairs in which
the gold standard ratings are tied are not included.
After removing the ties, the test set consists of 13K
to 16K pairs for each linguistic quality question.
Note that there were 45 inputs and 32 automatic
systems in DUC 2007. So, there are a total of
45?
(32
2
)
= 22, 320 possible summary pairs.
For system-level evaluation, we treat the real-
valued output of the SVM ranker for each sum-
mary as the linguistic quality score. The 45 indi-
vidual scores for summaries produced by a given
system are averaged to obtain an overall score for
the system. The gold-standard system-level qual-
ity rating is equal to the average human ratings for
the system?s summaries over the 45 inputs. At the
system level, there are about 500 non-tied pairs in
the test set for each question.
For both evaluation settings, a random baseline
which ranked the summaries in a random order
would have an expected pairwise accuracy of 50%.
6 Results and discussion
6.1 System-level evaluation
System-level accuracies for each class of features
are shown in Table 2. All classes of features per-
form well, with at least a 20% absolute increase
in accuracy over the random baseline (50% ac-
curacy). For each of the linguistic quality ques-
tions, the corresponding best class of features
gives prediction accuracies around 90%. In other
words, if these features were used to fully auto-
matically compare systems that participated in the
2007 DUC evaluation, only one out of ten com-
parisons would have been incorrect. These results
set a high standard for future work on automatic
system-level evaluation of linguistic quality.
The state-of-the-art entity coherence features
perform well but are not the best for any of the five
aspects of linguistic quality. As expected, sentence
fluency is the best feature class for grammatical-
ity. For all four other questions, the best feature
set is Continuity, which is a combination of sum-
marization specific features, coreference features
and cosine similarity of adjacent sentences. Conti-
nuity features outperform entity coherence by 3 to
4% absolute difference on referential quality, fo-
cus, and coherence. Accuracies from the language
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 87.6 83.0 91.2 85.2 86.3
Named ent. 78.5 83.6 82.1 74.0 69.6
NP syntax 85.0 83.8 87.0 76.6 79.2
Coh. devices 82.1 79.5 82.7 82.3 83.7
Continuity 88.8 88.5 92.9 89.2 91.4
Sent. fluency 91.7 78.9 87.6 82.3 84.9
Coh-Metrix 87.2 86.0 88.6 83.9 86.3
Word coh. 81.7 76.0 87.8 81.7 79.0
Entity coh. 90.2 88.1 89.6 85.0 87.1
Meta ranker 92.9 87.9 91.9 87.8 90.0
Table 2: System-level prediction accuracies (%)
model features are within 1% of entity coherence
for these three aspects of summary quality.
Coh-Metrix, which has been proposed as a com-
prehensive characterization of text, does not per-
form as well as the language model and the en-
tity coherence classes, which contain considerably
fewer features related to only one aspect of text.
The classes of features specific to named enti-
ties and noun phrase syntax are the weakest pre-
dictors. It is apparent from the results that conti-
nuity, entity coherence, sentence fluency and lan-
guage models are the most powerful classes of fea-
tures that should be used in automation of evalu-
ation and against which novel predictors of text
quality should be compared.
Combining all feature classes with the meta
ranker only yields higher results for grammatical-
ity. For the other aspects of linguistic quality, it is
better to use Continuity by itself to rank systems.
One certainly unexpected result is that features
designed to capture one aspect of well-written text
turn out to perform well for other questions as
well. For instance, entity coherence and continuity
features predict grammaticality with very high ac-
curacy of around 90%, and are surpassed only by
the sentence fluency features. These findings war-
rant further investigation because we would not
expect characteristics of local transitions indica-
tive of text structure to have anything to do with
sentence grammaticality or fluency. The results
are probably due to the significant correlation be-
tween structure and grammaticality (Table 1).
6.2 Input-level evaluation
The results of the input-level ranking experiments
are shown in Table 3. Understandably, input-
level prediction is more difficult and the results are
lower compared to the system-level predictions:
even with wrong predictions for some of the sum-
maries by two systems, the overall judgment that
550
one system is better than the other over the entire
test set can still be accurate.
While for system-level predictions the meta
ranker was only useful for grammaticality, at the
input level it outperforms every individual feature
class for each of the five questions, obtaining ac-
curacies around 70%.
These input-level accuracies compare favorably
with automatic evaluation metrics for other nat-
ural language processing tasks. For example, at
the 2008 ACL Workshop on Statistical Machine
Translation, all fifteen automatic evaluation met-
rics, including variants of BLEU scores, achieved
between 42% and 56% pairwise accuracy with hu-
man judgments at the sentence level (Callison-
Burch et al, 2008).
As in system-level prediction, for referential
clarity, focus, and structure, the best feature class
is Continuity. Sentence fluency again is the best
class for identifying grammaticality.
Coh-Metrix features are now best for determin-
ing redundancy. Both Coh-Metrix and Continuity
(the top two features for redundancy) include over-
lap measures between adjacent sentences, which
serve as a good proxy for redundancy.
Surprisingly, the relative performance of the
feature classes at input level is not the same as
for system-level prediction. For example, the lan-
guage model features, which are the second best
class for the system-level, do not fare as well at
the input-level. Word co-occurrence which ob-
tained good accuracies at the system level is the
least useful class at the input level with accuracies
just above chance in all cases.
6.3 Components of continuity
The class of features capturing sentence-to-
sentence continuity in the summary (Section 3.5)
are the most effective for predicting referential
clarity, focus, and structure at the input level.
We now investigate to what extent each of its
components?summary-specific features, corefer-
ence, and cosine similarity between adjacent
sentences?contribute to performance.
Results obtained after excluding each of the
components of continuity is shown in Table 4;
each line in the table represents Continuity mi-
nus a feature subclass. Removing cosine over-
lap causes the largest drop in prediction accuracy,
with results about 10% lower than those for the
complete Continuity class. Summary specific fea-
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 66.3 57.6 62.2 60.5 62.5
Named ent. 52.9 54.4 60.0 54.1 52.5
NP Syntax 59.0 50.8 59.1 54.5 55.1
Coh. devices 56.8 54.4 55.2 52.7 53.6
Continuity 61.7 62.5 69.7 65.4 70.4
Sent. fluency 69.4 52.5 64.4 61.9 62.6
Coh-Metrix 65.5 67.6 67.9 63.0 62.4
Word coh. 54.7 55.5 53.3 53.2 53.7
Entity coh. 61.3 62.0 64.3 64.2 63.6
Meta ranker 71.0 68.6 73.1 67.4 70.7
Table 3: Input-level prediction accuracies (%)
tures, which compare the context of a sentence
in the summary with the context in the original
document where it appeared, also contribute sub-
stantially to the success of the Continuity class in
predicting structure and referential clarity. Accu-
racies drop by about 7% when these features are
excluded. However, the coreference features do
not seem to contribute much towards predicting
summary linguistic quality. The accuracies of the
Continuity class are not affected at all when these
coreference features are not included.
6.4 Impact of summarization methods
In this paper, we have discussed an analysis of the
outputs of current research systems. Almost all
of these systems still use extractive methods. The
summarization specific continuity features reward
systems that include the necessary preceding con-
text from the original document. These features
have high prediction accuracies (Section 6.3) of
linguistic quality, however note that the support-
ing context could often contain less important con-
tent. Therefore, there is a tension between strate-
gies for optimizing linguistic quality and for op-
timizing content, which warrants the development
of abstractive methods.
As the field moves towards more abstractive
summaries, we expect to see differences in both
a) summary linguistic quality and b) the features
predictive of linguistic aspects.
As discussed in Section 4.1, systems are cur-
rently worst at structure/coherence. However,
grammaticality will become more of an issue as
systems use sentence compression (Knight and
Marcu, 2002), reference rewriting (Nenkova and
McKeown, 2003), and other techniques to produce
their own sentences.
The number of discourse connectives is cur-
rently significantly negatively correlated with
structure/coherence (Spearman correlation of r =
551
Ref. Focus Struct.
Continuity 69.7 65.4 70.4
- Sum-specific 63.9 64.2 63.5
- Coref 70.1 65.2 70.6
- Cosine 60.2 56.6 60.7
Table 4: Ablation within the Continuity class;
pairwise accuracy for input-level predictions (%)
-.06, p = .008 on DUC 2006 system summaries).
This can be explained by the fact that they of-
ten lack proper context in an extractive summary.
However, an abstractive system could plan a dis-
course structure and insert appropriate connectives
(Saggion, 2009). In this case, we would expect the
presence of discourse connectives to be a mark of
a well-written summary.
6.5 Results on human-written abstracts
Since abstractive summaries would have markedly
different properties from extracts, it would be in-
teresting to know how well these sets of features
would work for predicting the quality of machine-
produced abstracts. However, since current sys-
tems are extractive, such a data set is not available.
Therefore we experiment on human-written ab-
stracts to get an estimate of the expected per-
formance of our features on abstractive system
summaries. In both DUC 2006 and DUC 2007,
ten NIST assessors wrote summaries for the var-
ious inputs. There are four human-written sum-
maries for each input and these summaries were
judged on the same five linguistic quality aspects
as the machine-written summaries. We train on the
human-written summaries from DUC 2006 and
test on the human-written summaries from DUC
2007, using the same set-up as in Section 5.
These results are shown in Table 5. We only re-
port results on the input level, as we are interested
in distinguishing between the quality of the sum-
maries, not the NIST assessors? writing skills.
Except for grammaticality, the prediction accu-
racies of the best feature classes for human ab-
stracts are better than those at input level for ma-
chine extracts. This result is promising, as it shows
that similar features for evaluating linguistic qual-
ity will be valid for abstractive summaries as well.
Note however that the relative performance of
the feature sets changes between the machine and
human results. While for the machines Continu-
ity feature class is the best predictor of referential
clarity, focus, and structure (Table 3), for humans,
language models and sentence fluency are best for
Feature set Gram. Redun. Ref. Focus Struct.
Lang. models 52.1 60.8 76.5 71.9 78.4
Named ent. 62.5 66.7 47.1 43.9 59.1
NP Syntax 64.6 49.0 43.1 49.1 58.0
Coh. devices 54.2 68.6 66.7 49.1 64.8
Continuity 54.2 49.0 62.7 61.4 71.6
Sent. fluency 54.2 64.7 80.4 71.9 72.7
Coh-Metrix 54.2 52.9 68.6 56.1 69.3
Word coh. 62.5 58.8 62.7 70.2 60.2
Entity coh. 45.8 49.0 54.9 52.6 56.8
Meta ranker 62.5 56.9 80.4 50.9 67.0
Table 5: Input-level prediction accuracies for
human-written summaries (%)
these three aspects of linguistic quality. A possi-
ble explanation for this difference could be that in
system-produced extracts, incoherent organization
influences human perception of linguistic quality
to a great extent and so local coherence features
turned out very predictive. But in human sum-
maries, sentences are clearly well-organized and
here, continuity features appear less useful. Sen-
tence level fluency seems to be more predictive of
the linguistic quality of these summaries.
7 Conclusion
We have presented an analysis of a wide variety
of features for the linguistic quality of summaries.
Continuity between adjacent sentences was con-
sistently indicative of the quality of machine gen-
erated summaries. Sentence fluency was useful for
identifying grammaticality. Language model and
entity coherence features also performed well and
should be considered in future endeavors for auto-
matic linguistic quality evaluation.
The high prediction accuracies for input-level
evaluation and the even higher accuracies for
system-level evaluation confirm that questions re-
garding the linguistic quality of summaries can be
answered reasonably using existing computational
techniques. Automatic evaluation will make test-
ing easier during system development and enable
reporting results obtained outside of the cycles of
NIST evaluation.
Acknowledgments
This material is based upon work supported under
a National Science Foundation Graduate Research
Fellowship and NSF CAREER award 0953445.
We would like to thank Bonnie Webber for pro-
ductive discussions.
552
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2008. Further meta-evaluation of ma-
chine translation. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 70?
106.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies
of machine translation and human-written text. In
Proceedings of EACL, pages 139?147.
E. Charniak and M. Elsner. 2009. EM works for pro-
noun anaphora resolution. In Proceedings of EACL,
pages 148?156.
J.M. Conroy and H.T. Dang. 2008. Mind the gap: dan-
gers of divorcing evaluations of summary content
from linguistic quality. In Proceedings of COLING,
pages 145?152.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41:391?407.
M. Elsner and E. Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of
ACL/HLT: Short Papers, pages 41?44.
M. Elsner, J. Austerweil, and E. Charniak. 2007. A
unified local and global model for discourse coher-
ence. In Proceedings of NAACL/HLT.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL, pages 363?370.
K. Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Se-
mantics, 7(4):395.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods
Instruments and Computers, 36(2):193?202.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of
discourse. Computational Linguistics, 21(2):203?
226.
K.F. Haberlandt and A.C. Graesser. 1985. Component
processes in text comprehension and some of their
interactions. Journal of Experimental Psychology:
General, 114(3):357?374.
M.A.K. Halliday and R. Hasan. 1976. Cohesion in
English. Longman Group Ltd, London, U.K.
T. Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
M.A. Just and P.A. Carpenter. 1987. The psychology
of reading and language comprehension. Allyn and
Bacon Boston, MA.
D. Klein and C.D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, pages 423?
430.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
M. Lapata and R. Barzilay. 2005. Automatic evalua-
tion of text coherence: Models and representations.
In International Joint Conference On Artificial In-
telligence, volume 19, page 1085.
M. Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings
of ACL, pages 545?552.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of NAACL/HLT, page 78.
C.Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out (WAS
2004), pages 25?26.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. In Proceedings of
HLT/NAACL 2003 (short paper).
J. Otterbacher, D. Radev, and A. Luo. 2002. Revi-
sions that improve cohesion in multi-document sum-
maries: a preliminary study. In Proceedings of the
Workshop on Automatic Summarization, ACL.
P. Over, H. Dang, and D. Harman. 2007. Duc
in context. Information Processing Management,
43(6):1506?1520.
C.D. Paice. 1980. The automatic generation of litera-
ture abstracts: an approach based on the identifica-
tion of self-indicating phrases. In Proceedings of the
3rd annual ACM conference on Research and devel-
opment in information retrieval, pages 172?191.
C.D. Paice. 1990. Constructing literature abstracts by
computer: Techniques and prospects. Information
Processing Management, 26(1):171?186.
E.F. Prince. 1981. Toward a taxonomy of given-new
information. Radical pragmatics, 223:255.
H. Saggion. 2009. A Classification Algorithm for Pre-
dicting the Structure of Summaries. Proceedings
of the 2009 Workshop on Language Generation and
Summarisation, page 31.
553
R. Soricut and D. Marcu. 2006. Discourse generation
using utility-trained coherence models. In Proceed-
ings of ACL.
J. Steinberger, M. Poesio, M.A. Kabadjov, and K. Jeek.
2007. Two uses of anaphora resolution in sum-
marization. Information Processing Management,
43(6):1663?1680.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
554
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 333?338,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Bayesian Method to Incorporate Background Knowledge during
Automatic Text Summarization
Annie Louis
ILCC, School of Informatics,
University of Edinburgh,
Edinburgh EH8 9AB, UK
alouis@inf.ed.ac.uk
Abstract
In order to summarize a document, it is
often useful to have a background set
of documents from the domain to serve
as a reference for determining new and
important information in the input doc-
ument. We present a model based on
Bayesian surprise which provides an in-
tuitive way to identify surprising informa-
tion from a summarization input with re-
spect to a background corpus. Specifically,
the method quantifies the degree to which
pieces of information in the input change
one?s beliefs? about the world represented
in the background. We develop sys-
tems for generic and update summariza-
tion based on this idea. Our method pro-
vides competitive content selection perfor-
mance with particular advantages in the
update task where systems are given a
small and topical background corpus.
1 Introduction
Important facts in a new text are those which devi-
ate from previous knowledge on the topic. When
people create summaries, they use their knowl-
edge about the world to decide what content in an
input document is informative to include in a sum-
mary. Understandably in automatic summariza-
tion as well, it is useful to keep a background set
of documents to represent general facts and their
frequency in the domain.
For example, in the simplest setting of multi-
document summarization of news, systems are
asked to summarize an input set of topically-
related news documents to reflect its central con-
tent. In this GENERIC task, some of the best re-
ported results were obtained by a system (Conroy
et al, 2006) which computed importance scores
for words in the input by examining if the word
occurs with significantly higher probability in the
input compared to a large background collection
of news articles. Other specialized summarization
tasks explicitly require the use of background in-
formation. In the UPDATE summarization task, a
system is given two sets of news documents on the
same topic; the second contains articles published
later in time. The system should summarize the
important updates from the second set assuming a
user has already read the first set of articles.
In this work, we present a Bayesian model for
assessing the novelty of a sentence taken from a
summarization input with respect to a background
corpus of documents.
Our model is based on the idea of Bayesian Sur-
prise (Itti and Baldi, 2006). For illustration, as-
sume that a user?s background knowledge com-
prises of multiple hypotheses about the current
state of the world and a probability distribution
over these hypotheses indicates his degree of be-
lief in each hypothesis. For example, one hypoth-
esis may be that the political situation in Ukraine
is peaceful, another where it is not. Apriori as-
sume the user favors the hypothesis about a peace-
ful Ukraine, i.e. the hypothesis has higher prob-
ability in the prior distribution. Given new data,
the evidence can be incorporated using Bayes Rule
to compute the posterior distribution over the hy-
potheses. For example, upon viewing news reports
about riots in the country, a user would update his
beliefs and the posterior distribution of the user?s
knowledge would have a higher probability for a
riotous Ukraine. Bayesian surprise is the differ-
ence between the prior and posterior distributions
over the hypotheses which quantifies the extent to
which the new data (the news report) has changed
a user?s prior beliefs about the world.
In this work, we exemplify how Bayesian sur-
prise can be used to do content selection for text
summarization. Here a user?s prior knowledge
is approximated by a background corpus and we
333
show how to identify sentences from the input
set which are most surprising with respect to this
background. We use the method to do two types
of summarization tasks: a) GENERIC news sum-
marization which uses a large random collection
of news articles as the background, and b) UP-
DATE summarization where the background is a
smaller but specific set of news documents on
the same topic as the input set. We find that
our method performs competitively with a previ-
ous log-likelihood ratio approach which identifies
words with significantly higher probability in the
input compared to the background. The Bayesian
approach is more advantageous in the update task,
where the background corpus is smaller in size.
2 Related work
Computing new information is useful in many ap-
plications. The TREC novelty tasks (Allan et al,
2003; Soboroff and Harman, 2005; Schiffman,
2005) tested the ability of systems to find novel
information in an IR setting. Systems were given
a list of documents ranked according to relevance
to a query. The goal is to find sentences in each
document which are relevant to the query, and at
the same time is new information given the content
of documents higher in the relevance list.
For update summarization of news, methods
range from textual entailment techniques (Ben-
tivogli et al, 2010) to find facts in the input which
are not entailed by the background, to Bayesian
topic models (Delort and Alfonseca, 2012) which
aim to learn and use topics discussed only in back-
ground, those only in the update input and those
that overlap across the two sets.
Even for generic summarization, some of the
best results were obtained by Conroy et al (2006)
by using a large random corpus of news articles
as the background while summarizing a new arti-
cle, an idea first proposed by Lin and Hovy (2000).
Central to this approach is the use of a likelihood
ratio test to compute topic words, words that have
significantly higher probability in the input com-
pared to the background corpus, and are hence
descriptive of the input?s topic. In this work,
we compare our system to topic word based ones
since the latter is also a general method to find sur-
prising new words in a set of input documents but
is not a bayesian approach. We briefly explain the
topic words based approach below.
Computing topic words: Let us call the input
set I and the background B. The log-likelihood
ratio test compares two hypotheses:
H
1
: A word t is not a topic word and occurs
with equal probability in I and B, i.e. p(t|I) =
p(t|B) = p
H
2
: t is a topic word, hence p(t|I) = p
1
and
p(t|B) = p
2
and p
1
> p
2
A set of documents D containing N tokens is
viewed as a sequence of words w
1
w
2
...w
N
. The
word in each position i is assumed to be generated
by a Bernoulli trial which succeeds when the gen-
erated word w
i
= t and fails when w
i
is not t.
Suppose that the probability of success is p. Then
the probability of a word t appearing k times in a
dataset of N tokens is the binomial probability:
b(k,N, p) =
(
N
k
)
p
k
(1? p)
N?k
(1)
The likelihood ratio compares the likelihood of
the data D = {B, I} under the two hypotheses.
? =
P (D|H
1
)
P (D|H
2
)
=
b(c
t
, N, p)
b(c
I
, N
I
, p
1
) b(c
B
, N
B
, p
2
)
(2)
p, p
1
and p
2
are estimated by maximum likeli-
hood. p = c
t
/N where c
t
is the number of times
word t appears in the total set of tokens compris-
ing {B, I}. p
1
= c
I
t
/N
I
and p
2
= c
B
t
/N
B
are the
probabilities of t estimated only from the input and
only from the background respectively.
A convenient aspect of this approach is that
?2 log ? is asymptotically ?
2
distributed. So for a
resulting?2 log ? value, we can use the ?
2
table to
find the significance level with which the null hy-
pothesis H
1
can be rejected. For example, a value
of 10 corresponds to a significance level of 0.001
and is standardly used as the cutoff. Words with
?2 log ? > 10 are considered topic words. Con-
roy et al (2006)?s system gives a weight of 1 to the
topic words and scores sentences using the number
of topic words normalized by sentence length.
3 Bayesian Surprise
First we present the formal definition of Bayesian
surprise given by Itti and Baldi (2006) without ref-
erence to the summarization task.
Let H be the space of all hypotheses represent-
ing the background knowledge of a user. The user
has a probability P (H) associated with each hy-
pothesis H ? H. Let D be a new observation. The
posterior probability of a single hypothesis H can
be computed as:
P (H|D) =
P (D|H)P (H)
P (D)
(3)
334
The surprise S(D,H) created by D on hypoth-
esis space H is defined as the difference between
the prior and posterior distributions over the hy-
potheses, and is computed using KL divergence.
S(D,H) = KL(P (H|D), P (H))) (4)
=
?
H
P (H|D) log
P (H|D)
P (H)
(5)
Note that since KL-divergence is not symmet-
ric, we could also compute KL(P (H), P (H|D))
as the surprise value. In some cases, surprise can
be computed analytically, in particular when the
prior distribution is conjugate to the form of the
hypothesis, and so the posterior has the same func-
tional form as the prior. (See Baldi and Itti (2010)
for the surprise computation for different families
of probability distributions).
4 Summarization with Bayesian Surprise
We consider the hypothesis space H as the set of
all the hypotheses encoding background knowl-
edge. A single hypothesis about the background
takes the form of a multinomial distribution over
word unigrams. For example, one multinomial
may have higher word probabilities for ?Ukraine?
and ?peaceful? and another multinomial has higher
probabilities for ?Ukraine? and ?riots?. P (H) gives
a prior probability to each hypothesis based on
the information in the background corpus. In our
case, P (H) is a Dirichlet distribution, the conju-
gate prior for multinomials. Suppose that the vo-
cabulary size of the background corpus is V and
we label the word types as (w
1
, w
2
, ... w
V
). Then,
P (H) = Dir(?
1
, ?
2
, ...?
V
) (6)
where ?
1:V
are the concentration parameters of
the Dirichlet distribution (and will be set using the
background corpus as explained in Section 4.2).
Now consider a new observation I (a text, sen-
tence, or paragraph from the summarization input)
and the word counts in I given by (c
1
, c
2
, ..., c
V
).
Then the posterior over H is the dirichlet:
P (H|I) = Dir(?
1
+ c
1
, ?
2
+ c
2
, ..., ?
V
+ c
V
)
(7)
The surprise due to observing I , S(I,H) is the
KL divergence between the two dirichlet distribu-
tions. (Details about computing KL divergence
between two dirichlet distributions can be found
in Penny (2001) and Baldi and Itti (2010)).
Below we propose a general algorithm for sum-
marization using surprise computation. Then we
define the prior distribution P (H) for each of our
two tasks, GENERIC and UPDATE summarization.
4.1 Extractive summarization algorithm
We first compute a surprise value for each word
type in the summarization input. Word scores are
aggregated to obtain a score for each sentence.
Step 1: Word score. Suppose that word type
w
i
appears c
i
times in the summarization input
I . We obtain the posterior distribution after see-
ing all instances of this word (w
i
) as P (H|w
i
) =
Dir(?
1
, ?
2
, ...?
i
+ c
i
, ...?
V
). The score for w
i
is
the surprise computed as KL divergence between
P (H|w
i
) and the prior P (H) (eqn. 6).
Step 2: Sentence score. The composition
functions to obtain sentence scores from word
scores can impact content selection performance
(Nenkova et al, 2006). We experiment with sum
and average value of the word scores.
1
Step 3: Sentence selection. The goal is to se-
lect a subset of sentences with high surprise val-
ues. We follow a greedy approach to optimize the
summary surprise by choosing the most surprising
sentence, the next most surprising and so on. At
the same time, we aim to avoid redundancy, i.e.
selecting sentences with similar content. After a
sentence is selected for the summary, the surprise
for words from this sentence are set to zero. We re-
compute the surprise for the remaining sentences
using step 2 and the selection process continues
until the summary length limit is reached.
The key differences between our Bayesian ap-
proach and a method such as topic words are: (i)
The Bayesian approach keeps multiple hypothe-
ses about the background rather than a single one.
Surprise is computed based on the changes in
probabilities of all of these hypotheses upon see-
ing the summarization input. (ii) The computation
of topic words is local, it assumes a binomial dis-
tribution and the occurrence of a word is indepen-
dent of others. In contrast, word surprise although
computed for each word type separately, quantifies
the surprise when incorporating the new counts of
this word into the background multinomials.
4.2 Input and background
Here we describe the input sets and background
corpus used for the two summarization tasks and
1
An alternative algorithm could directly compute the sur-
prise of a sentence by incorporating the words from the sen-
tence into the posterior. However, we found this specific
method to not work well probably because the few and un-
repeated content words from a sentence did not change the
posterior much. In future, we plan to use latent topic models
to assign a topic to a sentence so that the counts of all the
sentence?s words can be aggregated into one dimension.
335
define the prior distribution for each. We use data
from the DUC
2
and TAC
3
summarization evalua-
tion workshops conducted by NIST.
Generic summarization. We use multidocument
inputs from DUC 2004. There were 50 inputs,
each contains around 10 documents on a common
topic. Each input is also provided with 4 manually
written summaries created by NIST assessors. We
use these manual summaries for evaluation.
The background corpus is a collection of 5000
randomly selected articles from the English Giga-
word corpus. We use a list of 571 stop words from
the SMART IR system (Buckley, 1985) and the re-
maining content word vocabulary has 59,497 word
types. The count of each word in the background
is calculated and used as the ? parameters of the
prior Dirichlet distribution P (H) (eqn. 6).
Update summarization. This task uses data from
TAC 2009. An input has two sets of documents, A
and B, each containing 10 documents. Both A and
B are on same topic but documents in B were pub-
lished at a later time than A (background). There
were 44 inputs and 4 manual update summaries
are provided for each.
The prior parameters are the counts of words
in A for that input (using the same stoplist). The
vocabulary of these A sets is smaller, ranging from
400 to 3000 words for the different inputs.
In practice for both tasks, a new summarization
input can have words unseen in the background.
So new words in an input are added to the back-
ground corpus with a count of 1 and the counts of
existing words in the background are incremented
by 1 before computing the prior parameters. The
summary length limit is 100 words in both tasks.
5 Systems for comparison
We compare against three types of systems, (i)
those which similarly to surprise, use a back-
ground corpus to identify important sentences, (ii)
a system that uses information from the input set
only and no background, and (iii) systems that
combine scores from the input and background.
KL
back
: represents a simple baseline for sur-
prise computation from a background corpus. A
single unigram probability distribution B is cre-
ated from the background using maximum like-
lihood. The summary is created by greedily
adding sentences which maximize KL divergence
2
http://www-nlpir.nist.gov/projects/
duc/index.html
3
http://www.nist.gov/tac/
between B and the current summary. Suppose
the set of sentences currently chosen in the sum-
mary is S. The next step chooses the sentence
s
l
= argmax
s
i
KL({S ? s
i
}||B) .
TS
sum
, TS
avg
: use topic words computed as de-
scribed in Section 2 and utilizing the same back-
ground corpus for the generic and update tasks
as the surprise-based methods. For the generic
task, we use a critical value of 10 (0.001 signif-
icance level) for the ?
2
distribution during topic
word computation. In the update task however, the
background corpus A is smaller and for most in-
puts, no words exceeded this cutoff. We lower the
significance level to the generally accepted value
of 0.05 and take words scoring above this as topic
words. The number of topic words is still small
(ranging from 1 to 30) for different inputs.
The TS
sum
system selects sentences with greater
counts of topic words and TS
avg
computes the
number of topic words normalized by sentence
length. A greedy selection procedure is used. To
reduce redundancy, once a sentence is added, the
topic words contained in it are removed from the
topic word list before the next sentence selection.
KL
inp
: represents the system that does not use
background information. Rather the method cre-
ates a summary by optimizing for high similarity
of the summary with the input word distribution.
Suppose the input unigram distribution is I and
the current summary is S, the method chooses the
sentence s
l
= argmin
s
i
KL({S ? s
i
}||I) at each
iteration. Since {S ? s
i
} is used to compute diver-
gence, redundancy is implicitly controlled in this
approach. Such a KL objective was used in com-
petitive systems in the past (Daum?e III and Marcu,
2006; Haghighi and Vanderwende, 2009).
Input + background: These systems com-
bine (i) a score based on the background (KL
back
,
TS or SR) with (ii) the score based on the input
only (KL
inp
). For example, to combine TS
sum
and
KL
inp
: for each sentence, we compute its scores
based on the two methods. Then we normalize the
two sets of scores for candidate sentences using z-
scores and compute the best sentence as argmax
s
i
(TS
sum
(s
i
) - KL
inp
(s
i
)). Redundancy control is
done similarly to the TS only systems.
6 Content selection results
For evaluation, we compare each summary to the
four manual summaries using ROUGE (Lin and
Hovy, 2003; Lin, 2004). All summaries were trun-
cated to 100 words, stemming was performed and
336
ROUGE-1 ROUGE-2
KL
back
0.2276 (TS, SR) 0.0250 (TS, SR)
TS
sum
0.3078 0.0616
TS
avg
0.2841 (TS
sum
, SR
sum
) 0.0493 (TS
sum
)
SR
sum
0.3120 0.0580
SR
avg
0.3003 0.0549
KL
inp
0.3075 (KL
inp
+TS
avg
) 0.0684
KL
inp
+TS
sum
0.3250 0.0725
KL
inp
+TS
avg
0.3410 0.0795
KL
inp
+SR
sum
0.3187 (KL
inp
+TS
avg
) 0.0660 (KL
inp
+TS
avg
)
KL
inp
+SR
avg
0.3220 (KL
inp
+TS
avg
) 0.0696
Table 1: Evaluation results for generic summaries.
Systems in parentheses are significantly better.
stop words were not removed, as is standard in
TAC evaluations. We report the ROUGE-1 and
ROUGE-2 recall scores (average over the inputs)
for each system. We use the Wilcoxon signed-rank
test to check for significant differences in mean
scores. Table 1 shows the scores for generic sum-
maries and 2 for the update task. For each system,
the peer systems with significantly better scores
(p-value < 0.05) are indicated within parentheses.
We refer to the surprise-based summaries as
SR
sum
and SR
avg
depending on the type of com-
position function (Section 4.1).
First, consider GENERIC summarization and the
systems which use the background corpus only
(those above the horizontal line). The KL
back
baseline performs significantly worse than topic
words and surprise summaries. Numerically,
SR
sum
has the highest ROUGE-1 score and TS
sum
tops according to ROUGE-2. As per the Wilcoxon
test, TS
sum
, SR
sum
and SR
avg
scores are statisti-
cally indistinguishable at 95% confidence level.
Systems below the horizontal line in Table 1
use an objective which combines both similarity
with the input and difference from the background.
The first line here shows that a system optimiz-
ing only for input similarity, KL
inp
, by itself has
higher scores (though not significant) than those
using background information only. This result is
not surprising for generic summarization where all
the topical content is present in the input and the
background is a non-focused random collection.
At the same time, adding either TS or SR scores
to KL
inp
almost always leads to better results with
KL
inp
+ TS
avg
giving the best score.
In UPDATE summarization, the surprise-based
methods have an advantage over the topic word
ones. SR
avg
is significantly better than TS
avg
for both ROUGE-1 and ROUGE-2 scores and
better than TS
sum
according to ROUGE-1. In
fact, the surprise methods have numerically higher
ROUGE-1 ROUGE-2
KL
back
0.2246 (TS, SR) 0.0213 (TS, SR)
TS
sum
0.3037 (SR
avg
) 0.0563
TS
avg
0.2909 (SR
sum
, SR
avg
) 0.0477 (SR
sum
, SR
avg
)
SR
sum
0.3201 0.0640
SR
avg
0.3226 0.0639
KL
inp
0.3098 (KL
inp
+SR
avg
) 0.0710
KL
inp
+TS
sum
0.3010 (KL
inp
+SR
sum, avg
) 0.0635
KL
inp
+TS
avg
0.3021 (KL
inp
+SR
sum, avg
) 0.0543 (KL
inp
,
KL
inp
+SR
sum, avg
)
KL
inp
+SR
sum
0.3292 0.0721
KL
inp
+SR
avg
0.3379 0.0767
Table 2: Evaluation results for update summaries.
Systems in parentheses are significantly better.
ROUGE-1 scores compared to input similarity
(KL
inp
) in contrast to generic summarization.
When combined with KL
inp
, the surprise meth-
ods provide improved results, significantly better
in terms of ROUGE-1 scores. The TS methods do
not lead to any improvement, and KL
inp
+ TS
avg
is significantly worse than KL
inp
only. The limi-
tation of the TS approach arises from the paucity
of topic words that exceed the significance cutoff
applied on the log-likelihood ratio. But Bayesian
surprise is robust on the small background corpus
and does not need any tuning for cutoff values de-
pending on the size of the background set.
Note that these models do not perform on par
with summarization systems that use multiple in-
dicators of content importance, involve supervised
training and which perform sentence compression.
Rather our goal in this work is to demonstrate a
simple and intuitive unsupervised model.
7 Conclusion
We have introduced a Bayesian summarization
method that strongly aligns with intuitions about
how people use existing knowledge to identify im-
portant events or content in new observations.
Our method is especially valuable when a sys-
tem must utilize a small background corpus.
While the update task datasets we have used were
carefully selected and grouped by NIST assesors
into initial and background sets, for systems on
the web, there is little control over the number of
background documents on a particular topic. A
system should be able to use smaller amounts of
background information and as new data arrives,
be able to incorporate the evidence. Our Bayesian
approach is a natural fit in such a setting.
Acknowledgements
The author was supported by a Newton Interna-
tional Fellowship (NF120479) from the Royal So-
ciety and the British Academy.
337
References
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and
novelty detection at the sentence level. In Proceed-
ings of SIGIR, pages 314?321.
P. Baldi and L. Itti. 2010. Of bits and wows: a
bayesian theory of surprise with applications to at-
tention. Neural Networks, 23(5):649?666.
L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo.
2010. The sixth PASCAL recognizing textual en-
tailment challenge. Proceedings of TAC.
C. Buckley. 1985. Implementation of the SMART in-
formation retrieval system. Technical report, Cor-
nell University.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
COLING-ACL, pages 152?159.
H. Daum?e III and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL,
pages 305?312.
J. Delort and E. Alfonseca. 2012. DualSum: A topic-
model based approach for update summarization. In
Proceedings of EACL, pages 214?223.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of NAACL-HLT, pages 362?370.
L. Itti and P. F. Baldi. 2006. Bayesian surprise attracts
human attention. In Proceedings of NIPS, pages
547?554.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495?501.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, pages 1085?1090.
C. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proceedings of Text Sum-
marization Branches Out Workshop, ACL, pages 74?
81.
A. Nenkova, L. Vanderwende, and K. McKeown.
2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In Proceedings of SIGIR,
pages 573?580.
W. D Penny. 2001. Kullback-liebler divergences
of normal, gamma, dirichlet and wishart densities.
Wellcome Department of Cognitive Neurology.
B. Schiffman. 2005. Learning to Identify New Infor-
mation. Ph.D. thesis, Columbia University.
I. Soboroff and D. Harman. 2005. Novelty detection:
the trec experience. In Proceedings of HLT-EMNLP,
pages 105?112.
338
What Makes Writing Great? First Experiments on Article Quality
Prediction in the Science Journalism Domain
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104
nenkova@seas.upenn.edu
Abstract
Great writing is rare and highly admired.
Readers seek out articles that are beautifully
written, informative and entertaining. Yet
information-access technologies lack capabil-
ities for predicting article quality at this level.
In this paper we present first experiments on
article quality prediction in the science jour-
nalism domain. We introduce a corpus of
great pieces of science journalism, along with
typical articles from the genre. We imple-
ment features to capture aspects of great writ-
ing, including surprising, visual and emotional
content, as well as general features related to
discourse organization and sentence structure.
We show that the distinction between great
and typical articles can be detected fairly ac-
curately, and that the entire spectrum of our
features contribute to the distinction.
1 Introduction
Measures of article quality would be hugely bene-
ficial for information retrieval and recommendation
systems. In this paper, we describe a dataset of New
York Times science journalism articles which we
have categorized for quality differences and present
a system that can automatically make the distinction.
Science journalism conveys complex scientific
ideas, entertaining and educating at the same time.
Consider the following opening of a 2005 article by
David Quammen from Harper?s magazine:
One morning early last winter a small item appeared in
my local newspaper announcing the birth of an extraordi-
nary animal. A team of researchers at Texas A&M Uni-
versity had succeeded in cloning a whitetail deer. Never
done before. The fawn, known as Dewey, was developing
normally and seemed to be healthy. He had no mother,
just a surrogate who had carried his fetus to term. He
had no father, just a ?donor? of all his chromosomes. He
was the genetic duplicate of a certain trophy buck out
of south Texas whose skin cells had been cultured in a
laboratory. One of those cells furnished a nucleus that,
transplanted and rejiggered, became the DNA core of an
egg cell, which became an embryo, which in time be-
came Dewey. So he was wildlife, in a sense, and in an-
other sense elaborately synthetic. This is the sort of news,
quirky but epochal, that can cause a person with a mouth-
ful of toast to pause and marvel. What a dumb idea, I
marveled.
The writing is clear and well-organized but the
text also contains creative use of language and a
clever story-like explanation of the scientific con-
tribution. Such properties make science journalism
an attractive genre for studying writing quality. Sci-
ence journalism is also a highly relevant domain for
information retrieval in the context of educational
as well as entertaining applications. Article quality
measures can hugely benefit such systems.
Prior work indicates that three aspects of article
quality can be successfully predicted: a) whether
a text meets the acceptable standards for spelling
(Brill and Moore, 2000), grammar (Tetreault and
Chodorow, 2008; Rozovskaya and Roth, 2010) and
discourse organization (Barzilay et al, 2002; Lap-
ata, 2003); b) has a topic that is interesting to a par-
ticular user. For example, content-based recommen-
dation systems standardly represent user interest us-
ing frequent words from articles in a user?s history
and retrieve other articles on the same topics (Paz-
341
Transactions of the Association for Computational Linguistics, 1 (2013) 341?352. Action Editor: Mirella Lapata.
Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
zani et al, 1996; Mooney and Roy, 2000); and c) is
easy to read for a target readership. Shorter words
(Flesch, 1948), less complex syntax (Schwarm and
Ostendorf, 2005) and high cohesion between sen-
tences (Graesser et al, 2004) typically indicate eas-
ier and more ?readable? articles.
Less understood is the question of what makes an
article interesting and beautifully written. An early
and influential work on readability (Flesch, 1948)
also computed an interest measure with the hypoth-
esis that interesting articles would be easier to read.
More recently, McIntyre and Lapata (2009) found
that people?s ratings of interest for fairy tales can be
successfully predicted using token-level scores re-
lated to syntactic items and categories from a psy-
cholinguistic database. But large scale studies of in-
terest measures for adult educated readers have not
been carried out.
Further, there have been little attempts to measure
article quality in a genre-specific setting. But it is
reasonable to expect that properties related to the
unique aspects of a genre should contribute to the
prediction of quality in the same way that domain-
specific spelling and grammar correction (Cucerzan
and Brill, 2004; Bao et al, 2011; Dale and Kilgar-
riff, 2010) techniques have been successful.
Here we address the above two issues by develop-
ing measures related to interesting and well-written
nature specifically for science journalism. Central
to our work is a corpus of science news articles with
two categories: written by popular journalists and
typical articles in science columns (Section 2). We
introduce a set of genre-specific features related to
beautiful writing, visual nature and affective content
(Section 3) and show that they have high predictive
accuracies, 20% above the baseline, for distinguish-
ing our quality categories (Section 4). Our final sys-
tem combines the measures for interest and genre-
specific features with those proposed for identifying
readable, well-written and topically interesting arti-
cles, giving an accuracy of 84% (Section 5).
2 Article quality corpus
Our corpus1 contains chosen articles from the larger
New York Times (NYT) corpus (Sandhaus, 2008),
the latter containing a wealth of metadata about each
1Available from http://www.cis.upenn.edu/
?nlp/corpora/scinewscorpus.html
article including author information and manually
assigned topic tags.
2.1 General corpus
The articles in the VERY GOOD category include all
contributions to the NYT by authors whose writing
appeared in ?The Best American Science Writing?
anthology published annually since 1999. Articles
from the science columns of leading newspapers are
nominated and expert journalists choose a set they
consider exceptional to appear in these anthologies.
There are 63 NYT articles in the anthology (between
years 1999 and 2007) that are also part of the digital
NYT corpus; these articles form the seed set of the
VERY GOOD category.
We further include in the VERY GOOD category
all other science articles contributed to NYT by the
authors of the seed examples. Science articles by
other authors not in our seed set form the TYPICAL
category. We perform this expansion by first creat-
ing a relevant set of science articles. There is no
single meta-data tag in the NYT which refers to all
the science articles. So we use the topic tags from
the seed articles as an initial set of research tags.
We then compute the minimal set of research tags
that cover all best articles. We greedily add tags
into the minimal set, at each iteration choosing the
tag that is present in the majority of articles that re-
main uncovered. This minimal set contains 14 tags
such as ?Medicine and Health?, ?Space?, ?Research?,
?Physics? and ?Evolution?.
We collect articles from the NYT which have at
least one of the minimal set tags. However, even
a cursory mention of a research topic results in a
research-related tag being assigned to the article. So
we also use a dictionary of research-related terms
to determine whether the article passes a minimum
threshold for research content. We created this dic-
tionary manually and it contains the following words
and their morphological variants (total 63 items).
We used our intuition about a few categories of re-
search words to create this list. The category is
shown in capital letters below.
PEOPLE: researcher, scientist, physicist, biologist, economist,
anthropologist, environmentalist, linguist, professor, dr, student
PROCESS: discover, found, experiment, work, finding, study,
question, project, discuss
TOPIC: biology, physics, chemistry, anthropology, primatology
342
PUBLICATIONS: report, published, journal, paper, author, issue
OTHER: human, science, research, knowledge, university, lab-
oratory, lab
ENDINGS: -ology -gist, -list, -mist, -uist, -phy
The items in the ENDINGS category are used
to match word suffixes. An article is considered
science-related if at least 10 of its tokens match the
dictionary and in addition, at least 5 unique words
from the dictionary are matched. Since the time span
of the best articles is 1999 to 2007, we limit our col-
lection to this timespan. In addition, we only con-
sider articles that are at least 500 words long. This
filtered set of 23,709 articles form the relevant set of
science journalism.
The 63 seed samples of great writing were con-
tributed by about 40 authors. Some authors have
multiple articles selected for the best writing book
series, supporting the idea that these authors produce
high quality pieces that can be considered distinct
from typical articles. Separating the articles from
these authors gives us 3,467 extra samples of VERY
GOOD writing. In total, the VERY GOOD set has
3,530 articles. The remaining articles from the rele-
vant set, 20,242, written by about 3000 other authors
form the TYPICAL category.
2.2 Topic-paired corpus
The general corpus of science writing introduced so
far contains articles on diverse topics including bi-
ology, astronomy, religion and sports. The VERY
GOOD and TYPICAL categories created above al-
low us to study writing quality without regard to
topic. However a typical information retrieval sce-
nario would involve comparison between articles of
the same topic, i.e. relevant to the same query. To
investigate how quality differentiation can be done
within topics, we created another corpus where we
paired articles of VERY GOOD and TYPICAL quality.
For each article in the VERY GOOD category, we
compute similarity with all articles in the TYPICAL
set. This similarity is computed by comparing the
topic words (computed using a loglikelihood ratio
test (Lin and Hovy, 2000)) of the two articles. We
retain the most similar 10 TYPICAL articles for each
VERY GOOD article. We enumerate all pairs of VERY
GOOD with matched up TYPICAL ARTICLES (10 in
number) giving a total of 35,300 pairs.
There are two distinguishing aspects of our cor-
pus. First, the average quality of articles is high.
They are unlikely to have spelling, grammar and ba-
sic organization problems allowing us to investigate
article quality rather than the detection of errors.
Second, our corpus contains more realistic samples
of quality differences for IR or article recommen-
dation compared to prior work, where system pro-
duced texts and permuted versions of an original ar-
ticle were used as proxies for lower quality text.
2.3 Tasks
We perform two types of classification tasks. We
divide our corpus into development and test sets for
these tasks in the following way.
Any topic: Here the goal is to separate out VERY
GOOD versus TYPICAL articles without regard to
topic. The setting roughly corresponds to picking
out an interesting article from an archive or a day?s
newspaper. The test set contains 3,430 VERY GOOD
articles and we randomly sample 3,430 articles from
the TYPICAL category to comprise the negative set.
Same topic: Here we use the topic-paired VERY
GOOD and TYPICAL articles. The goal is to predict
which article in the pair is the VERY GOOD one. This
task is closer to a information retrieval setting, where
articles similar in topic (retrieved for a user query)
need to be distinguished for quality. For test set, we
selected 34,300 pairs.
Development data: We randomly selected 100
VERY GOOD articles and their paired (10 each)
TYPICAL articles from the topic-normalized corpus.
Overall, these constitute 1,000 pairs which we use
for developing the same-topic classifier. From these
selected pairs we take the 100 VERY GOOD articles
and sample 100 unique articles from the TYPICAL
articles making up the pairs. These 200 articles are
used to tune the any-topic classifier.
3 Facets of science writing
In this section, we discuss six prominent facets of
science writing which we hypothesized will have
an impact on text quality. These are the presence
of passages of highly visual nature, people-oriented
content, use of beautiful language, sub-genres, sen-
timent or affect, and the depth of research descrip-
tion. Several other properties of science writing
could also be relevant to quality such as the use of
343
humor, metaphor, suspense and clarity of explana-
tions and we plan to explore these in future work.
We describe how we computed features related to
each property and tested how these features are dis-
tributed in the VERY GOOD and TYPICAL categories.
To do this analysis, we randomly sampled 1,000 ar-
ticles from each of the two categories as representa-
tive examples. We compute the value of each feature
on these articles and use a two-sided t-test to check
if the mean value of the feature is higher in one class
of articles. A p-value less than 0.05 is taken to in-
dicate significantly different trend for the feature in
the VERY GOOD versus TYPICAL articles.
Note that our feature computation step is not
tuned for the quality prediction task in any way.
Rather we aim to represent each facet as accurately
as possible. Ideally we would require manual anno-
tations for each facet (visual, sentiment nature etc.)
to achieve this goal. At this time, we simply check
some chosen features? values on a random collection
of snippets from our corpus and check if they behave
as intended without resorting to these annotations.
3.1 Visual nature of articles
Some texts create an image in the reader?s mind. For
example, the snippet below has a high visual effect.
When the sea lions approached close, seemingly as curious
about us as we were about them, their big brown eyes were
encircled by light fur that looked like makeup. One sea lion
played with a conch shell as if it were a ball.
Such vivid descriptions can engage and entertain
a reader. Kosslyn (1980) found that people spon-
taneously form images of concrete words that they
hear and use them to answer questions or perform
other tasks. Books written for student science jour-
nalists (Blum et al, 2006; Stocking, 2010) also em-
phasize the importance of visual descriptions.
We measure the visual nature of a text by count-
ing the number of visual words. Currently, the only
resource of imagery ratings for words is the MRC
psycholinguistic database (Wilson, 1988). It con-
tains a list of 3,394 words rated for their ability to
invoke an image, so the list contains both words that
are highly visual along with words that are not visual
at all. With a cutoff value we adopted, of 4.5 for the
Gilhooly-Logie and 350 for the Bristol Norms2 we
2The visual words resource in MRC contains two lists?
obtain 1,966 visual words. So the coverage of that
lexicon is likely to be low for our corpus.
We collect a larger set of visual words from a cor-
pus of tagged images from the ESP game (von Ahn
and Dabbish, 2004). The corpus contains 83,904
total images and 27,466 unique tags. The average
number of tags per picture is 14.5. The tags were
collected in a game setting where two users individ-
ually saw the same image and had to guess words
related to it. The players increased their scores when
the word guessed by one player matched that of the
other. Due to the simple annotation method, there
is considerable noise and non-visual words assigned
as tags. So we performed filtering to find high pre-
cision image words and also group them into topics.
We use Latent Dirichlet Allocation (Blei et al,
2003) to cluster image tags into topics. We treat each
picture as a document and the tags assigned to the
picture are the document?s contents. We use sym-
metric priors set to 0.01 for both topic mixture and
word distribution within each topic. We filter out the
30 most common words in the corpus, words that ap-
pear in less than four pictures and images with fewer
than five tags. The remaining words are clustered
into 100 topics with the Stanford Topic Modeling
Toolbox3 (Ramage et al, 2009). We did not tune the
number of topics and choose the value of 100 based
on the intuition that the number of visual topics is
likely to be small.
To select clean visual clusters, we make the as-
sumption that visual words are likely to be clustered
with other visual terms. Topics that are not visual
are discarded altogether. We use the manual an-
notations available with the MRC database to de-
termine which clusters are visual. For each of the
100 topics from the topic model, we examine the
top 200 words with highest probability in that topic.
We compute the precision of each topic as the pro-
portion of these 200 words that match the MRC list
of visual words (1,966 words using the cutoff men-
tioned above). Only those topics which had a pre-
cision of at least 25% were retained, resulting in 68
visual topics. Some example topics, with manually
created headings, include:
landscape. grass, mountain, green, hill, blue, field,
brown, sand, desert, dirt, landscape, sky
Gilhooly-Logie and Bristol Norms.
3http://nlp.stanford.edu/software/tmt/tmt-0.4/
344
jewellery. silver, white, diamond, gold, necklace,
chain, ring, jewel, wedding, diamonds, jewelry
shapes. round, ball, circles, logo, dots, square, dot,
sphere, glass, hole, oval, circle
Combining these 68 topics, there are 5,347 unique
visual words because topics can overlap in the list of
most probable words. 2,832 words from this set are
not present in the MRC database. Some examples
of new words in our list are ?daffodil?, ?sailor?, ?hel-
met?, ?postcard?, ?sticker?, ?carousel?, ?kayak?, and
?camouflage?. For later experiments we consider the
5,347 words as the visual word set and also keep
the information about the top 200 words in the 68
selected topics. We compute two classes of features
one based on all visual words and the other on visual
topics. We consider only the adjectives, adverbs,
verbs and common nouns in an article as candidate
words for computing visual quality.
Overall visual use: We compute the propor-
tion of candidate words that match the visual
word list as the TOTAL VISUAL feature. We also
compute the proportions based only on the first
200 words of the article (BEG VISUAL), the last
200 words (END VISUAL) and the middle region
(MID VISUAL) as features. We also divide the arti-
cle into five equally sized bins of words where each
bin captures consecutive words in the article. Within
each bin we compute the proportion of visual words.
We treat these values as a probability distribution
and compute its entropy (ENTROPY VISUAL). We
expected these position features to indicate how the
placement of visual words is related to quality.
Topic-based features: We also compute what pro-
portion of the words we identify as visual matches
the list under each topic. The maximum proportion
from a single topic (MAX TOPIC VISUAL) is a fea-
ture. We also compute a greedy cover set of top-
ics for the visual words in the article. The topic
that matches the most visual words is added first,
and the next topic is selected based on the remain-
ing unmatched words. The number of topics needed
to cover 50% of the article?s visual words is the
TOPIC COVER VISUAL feature. These features cap-
ture the mix of visual words from different topics.
Disregarding topic information, we also compute a
feature NUM PICTURES which is the number of im-
ages in the corpus where 40% of the image?s tags are
matched in the article.
We found 8 features to vary significantly be-
tween the two types of articles. The fea-
tures with significantly higher values in VERY
GOOD articles are: BEG VISUAL, END VISUAL,
MAX TOPIC VISUAL. The features with signifi-
cantly higher values in the TYPICAL articles are:
TOTAL VISUAL, MID VISUAL, ENTROPY VISUAL,
TOPIC COVER VISUAL, NUM PICTURES.
It appears that the simple expectation that VERY
GOOD articles contain more visual words overall
does not hold true here. However the great writ-
ing samples have a higher degree of visual content
in the beginning and ends of articles. Good articles
also have lower entropy for the distribution of vi-
sual words indicating that they appear in localized
positions in contrast to being distributed throughout.
The topic-based features further indicate that for the
VERY GOOD articles, the visual words come from
only a few topics (compared to TYPICAL articles)
and so may evoke a coherent image or scene.
3.2 The use of people in the story
We hypothesized that articles containing research
findings that directly affect people in some way, and
therefore involve explicit references to people in the
story, will make a bigger impact on the reader. For
example, the most frequent topic among our VERY
GOOD samples is ?medicine and health? where ar-
ticles are often written from the view of a patient,
doctor or scientist. An example is below.
Dr. Remington was born in Reedville, Va., in 1922, to Maud
and P. Sheldon Remington, a school headmaster. Charles spent
his boyhood chasing butterflies alongside his father, also a col-
lector. During his graduate studies at Harvard, he founded the
Lepidopterists? Society with an equally butterfly-smitten under-
graduate, Harry Clench.
We approximate this facet by computing the num-
ber of explicit references to people, relying on three
sources of information about animacy of words. The
first is named entity (NE) tags (PERSON, ORGANI-
ZATION and LOCATION) returned by the Stanford
NE recognition tool (Finkel et al, 2005). We also
created a list of personal pronouns such as ?he?, ?my-
self? etc. which standardly indicate animate entities
(animate pronouns).
Our third resource contains the number of times
different noun phrases (NP) were followed by each
of the relative pronouns ?who?, ?where? and ?which?.
345
These counts for 664,673 noun phrases were col-
lected by Ji and Lin (2009) from the Google Ngram
Corpus (Lin et al, 2010). We use a simple heuris-
tic to obtain a list of animate (google animate) and
inanimate nouns (google inanimate) from this list.
The head of each NP is taken as a candidate noun.
If the noun does not occur with ?who? in any of the
noun phrases where it is the head, then it is inani-
mate. In contrast, if it appears only with ?who? in
all noun phrases, it is animate. Otherwise, for each
NP where the noun is a head, we check whether the
count of times the noun phrase appeared with ?who?
is greater than each of the occurrences of ?which?,
?where? and ?when? (taken individually) with that
noun phrase. If the condition holds for at least one
noun phrase, the noun is marked as animate.
When computing the features for an article, we
consider all nouns and pronouns as candidate words.
If the word is a pronoun and appears in our list of an-
imate pronouns, it is assigned an ?animate? label and
?inanimate? otherwise. If the word is a proper noun
and tagged with the PERSON NE tag, we mark it
as ?animate? and if it is a ORGANIZATION or LO-
CATION tag, the word is ?inanimate?. For common
nouns, we check it if appears in the google animate
and inanimate lists. Any match is labelled accord-
ingly as ?animate? and ?inanimate?. Note that this
procedure may leave some nouns without any labels.
Our features are counts of animate tokens
(ANIM), inanimate tokens (INAMIN) and both these
counts normalized by total words in the article
(ANIM PROP, INANIM PROP). Three of these fea-
tures had significantly higher mean values in the
TYPICAL category of articles: ANIM, ANIM PROP,
INANIM PROP. We found upon observation that sev-
eral articles that talk about government policies in-
volve a lot of references to people but are often in the
TYPICAL category. These findings suggest that the
?human? dimension might need to be computed not
only based on simple counts of references to people
but also involve finer distinctions between them.
3.3 Beautiful language
Beautiful phrasing and word choice can entertain
the reader and leave a positive impression. Multi-
ple studies in the education genre (Diederich, 1974;
Spandel, 2004) note that when teachers and expert
adult readers graded student writing, word choice
and phrasing always turn out as a significant factors
influencing the raters? scores.
We implement a method for detecting creative
language based on a simple idea that creative words
and phrases are sometimes those that are used in un-
usual contexts and combinations or those that sound
unusual. We compute measures of unusual language
both at the level of individual words and for the com-
bination of words in a syntactic relation.
Word level measures: Unusual words in an ar-
ticle are likely to be those with low frequencies
in a background corpus. We use the full set of
articles (not only science) from year 1996 in the
NYT corpus as a background (these do not over-
lap with our corpus for article quality). We also ex-
plore patterns of letters and phoneme sequences with
the idea that unusual combination of characters and
phonemes could create interesting words. We used
the CMU pronunciation dictionary (Weide, 1998) to
get the phoneme information for words and built a 4-
gram model of phonemes on the background corpus.
Laplace smoothing is used to compute probabilities
from the model. However, the CMU dictionary does
not contain phoneme information for several words
in our corpus. So we also compute an approximate
model using the letters in the words and obtain an-
other 4-gram model.4 Only words that are longer
than 4 characters are used in both models and we fil-
ter out proper names, named entities and numbers.
During development, we analyzed the articles
from an entire year of NYT, 1997, with the three
models to identify unusual words. Below is the list
of words with lowest frequency and those with high-
est perplexity under the phoneme and letter models.
Low frequency. undersheriff, woggle, ahmok,
hofman, volga, oceanaut, trachoma, baneful, truffler,
acrimal, corvair, entomopter
High perplexity-phoneme model. showroom, yahoo,
dossier, powwow, plowshare, oomph, chihuahua, iono-
sphere, boudoir, superb, zaire, oeuvre
High perplexity-letter model. kudzu, muumuu, qi-
pao, yugoslav, kohlrabi, iraqi, yaqui, yakuza, jujitsu, oeu-
vre, yaohan, kaffiyeh
For computing the features, we consider only
nouns, verbs, adjectives and adverbs. We also
require that the words are at least 5 letters long
4We found that higher order n-grams provided better pre-
dictions of unusual nature during development.
346
and do not contain a hyphen5. Three types of
scores are computed. FREQ NYT is the aver-
age of word frequencies computed from the back-
ground corpus. The second set of features are
based on the phoneme model. We compute the
average perplexity of words under the model,
AVR PHONEME PERP ALL. In addition, we also or-
der the words in an article based on decreasing per-
plexity values and the average perplexity of the top
10, 20 and 30 words in this list are added as fea-
tures (AVR PHONEME PERP 10, 20, 30). We ob-
tain similar features from the letter n-gram model
(AVR CHAR PERP ALL, AVR CHAR PERP 10, 20,
30). In phoneme features, we ignore words that do
not have an entry in the CMU dictionary.
Word pair measures: Next we attempt to detect un-
usual combinations of words. We do this calculation
only for certain types of syntactic relations?a) nouns
and their adjective modifiers, b) verbs with adverb
modifiers, c) adjacent nouns in a noun phrase and
d) verb and subject pairs. Counts for co-occurrence
again come from NYT 1996 articles. The syntactic
relations are obtained using the constituency and de-
pendency parses from the Stanford parser (Klein and
Manning, 2003; De Marneffe et al, 2006). To avoid
the influence of proper names and named entities,
we replace them with tags (NNP for proper names
and PERSON, ORG, LOC for named entities).
We treat the words for which the dependency
holds as a (auxiliary word, main word) pair. For
adjective-noun and adverb-verb pairs, the auxiliary
is the adjective or adverb; for noun-noun pairs, it is
the first noun; and for verb-subject pairs, the auxil-
iary is the subject. Our idea is to compute usualness
scores based on frequency with which a particular
pair of words appears in the background.
Specifically, we compute the conditional proba-
bility of the auxiliary word given the main word
as the score for likelihood of observing the pair.
We consider the main word as related to the article
topic, so we use the conditional probability of auxil-
iary given main word and not the other way around.
However, the conditional probability has no infor-
mation about the frequency of the auxiliary word. So
we apply ideas from interpolation smoothing (Chen
5We noticed that in this genre several new words are created
using hyphen to concatenate common words.
ADJ-NOUN ADV-VERB
hypoactive NNP suburbs said
plasticky woman integral was
psychogenic problems collective do
yoplait television physiologically do
subminimal level amuck run
ehatchery investment illegitimately put
NOUN-NOUN SUBJ-VERB
specification today blog said
auditory system briefer said
pal programs hr said
steganography programs knucklehead said
wastewater system lymphedema have
autism conference permissions have
Table 1: Unusual word-pairs from different categories
and Goodman, 1996) and compute the conditional
probability as a interpolated quantity together with
the unigram probability of the auxiliary word.
p?(aux|main) = ??p(aux|main)+(1??)?p(aux)
The unigram and conditional probabilities are
also smoothed using Laplace method. We train the
lambda values to optimize data likelihood using the
Baum Welch algorithm and use the pairs from NYT
1997 year articles as a development set. The lambda
values across all types of pairs tended to be lower
than 0.5 giving higher weight to the unigram proba-
bility of the auxiliary word.
Based on our observations on the development
set, we picked a cutoff of 0.0001 on the proba-
bility (0.001 for adverb-verb pairs) and consider
phrases with probability below this value as un-
usual. For each test article, we compute the num-
ber of unusual phrases (total for all categories)
as a feature (SURP) and also this value normal-
ized by total number of word tokens in the article
(SURP WD) and normalized by number of phrases
(SURP PH). We also compute features for indi-
vidual pair types and in each case, the number of
unusual phrases is normalized by the total words
in the article (SURP ADJ NOUN, SURP ADV VERB,
SURP NOUN NOUN, SURP SUBJ VERB).
A list of the top unusual words under the different
pair types are shown in Table 1. These were com-
puted on pairs from a random set of articles from our
corpus. Several of the top pairs involve hyphenated
words which are unusual by themselves, so we only
show in the table the top words without hyphens.
347
Most of these features are significantly
different between the two classes. Those
with higher values in the VERY GOOD
set include: AVR PHONEME PERP ALL,
AVR CHAR PERP (ALL, 10), SURP, SURP PH,
SURP WD, SURP ADJ NOUN, SURP NOUN NOUN,
SURP SUBJ VERB. The FREQ NYT feature has
higher value in the TYPICAL class.
All these trends indicate that unusual phrases are
associated with the VERY GOOD category of articles.
3.4 Sub-genres
There are several sub-genres within science writing
(Stocking, 2010): short descriptions of discoveries,
longer explanatory articles, narratives, stories about
scientists, reports on meetings, review articles and
blog posts. Naturally, some of these sub-genres will
be more appealing to readers. To investigate this
aspect, we compute scores for some sub-genres of
interest?narrative, attribution and interview.
Narrative texts typically have characters and
events (Nakhimovsky, 1988), so we look for entities
and past tense in the articles. We count the number
of sentences where the first verb in surface order is
in the past tense. Then among these sentences, we
pick those which have either a personal pronoun or a
proper noun before the target verb (again in surface
order). The proportion of such sentences in the text
is taken as the NARRATIVE score.
We also developed a measure to identify the de-
gree to which the article?s content is attributed to ex-
ternal sources as opposed to the author?s own state-
ments. Attribution to other sources is frequent in
the news domain since many comments and opin-
ions are not the views of the journalist (Semetko and
Valkenburg, 2000). For science news, attribution be-
comes more important since the research findings
were obtained by scientists and reported in a second-
hand manner by the journalists. The ATTRIB score
is the proportion of sentences in the article that have
a quote symbol, or the words ?said? and ?says?.
We also compute a score to indicate if the article
is the account of an interview. There are easy clues
in NYT for this genre with paragraphs in the inter-
view portion of the article beginning with either ?Q.?
(question) or ?A.? (answer). We count the total num-
ber of ?Q.? and ?A.? prefixes combined and divide
the value by the total number of sentences (INTER-
VIEW). When either the number of ?Q.? tags is zero
or ?A.? tags is zero, the score is set to zero.
All three scores are significantly higher for the
TYPICAL class.
3.5 Affective content
Some articles, for example those detailing research
on health, crime, ethics, can provoke emotional re-
actions in readers as shown in the snippet below.
Medicine is a constant trade-off, a struggle to cure the dis-
ease without killing the patient first. Chemotherapy, for exam-
ple, involves purposely poisoning someone ? but with the ex-
pectation that the short-term injury will be outweighed by the
eventual benefits.
We compute affect-related features using three
lexicons. The MPQA (Wilson et al, 2005) and Gen-
eral Inquirer (Stone et al, 1966) give lists of positive
and negative sentiment words. The third resource
is emotion-related words from FrameNet (Baker et
al., 1998). The sizes of these lexicon are 8,221,
5,395, and 653 words respectively. We compute
the counts of positive, negative, polar, and emotion
words, each normalized by the total number of con-
tent words in the article (POS PROP, NEG PROP, PO-
LAR PROP, EMOT PROP). We also include the pro-
portion of emotion and polar words taken together
(POLAR EMOT PROP) and the ratio between count
of positive and negative words (POS BY NEG).
The features with higher values in the VERY
GOOD class are NEG PROP, POLAR PROP,
EMOT POLAR PROP. In TYPICAL articles,
POS BY NEG, EMOT PROP have higher values.
VERY GOOD articles have more sentiment words,
mostly skewed towards negative sentiment.
3.6 Amount of research content
For a lay audience, a science writer presents only the
most relevant findings and methods from a research
study and interleaves research information with de-
tails about the relevance of the finding, people in-
volved in the research and general information about
the topic. As a result, the degree of explicit research
descriptions in the articles varies considerably.
To test how this aspect is associated with qual-
ity, we count references to research methods and re-
searchers in the article. We use the research dictio-
nary that we introduced in Section 2 as the source
of research-related words. We count the total num-
348
ber of words in the article that match the dictionary
(RES TOTAL) and also the number of unique match-
ing words (RES UNIQ). We also normalize these
counts by the total words in the article and create
features RES TOTAL PROP and RES UNIQ PROP.
All four features have significantly higher values
in the VERY GOOD articles which indicate that great
articles are also associated with a great amount of
direct research content and explanations.
4 Classification accuracy
We trained classifiers using all the above features for
for the two settings??any-topic? and ?same-topic? in-
troduced in Section 2.3. The baseline random accu-
racy in both cases is 50%. We use a SVM classi-
fier with a radial basis kernel (R Development Core
Team, 2011) and parameters were tuned using cross
validation on the development data.
The best parameters were then used to classify the
test set in a 10 fold cross-validation setting. We di-
vide the test set into 10 parts, train on 9 parts and
test on the held-out data. The average accuracies in
the 10 experiments are 75.3% accuracy for the ?any-
topic? setup, and 68% accuracy for the topic-paired
?same-topic? setup. These accuracies are consider-
able improvements over the baseline.
The ?same-topic? data contains article pairs with
varying similarity. So we investigate the relationship
between topic similarity and accuracy of prediction
more closely for this setting. We divide the article
pairs into bins based on the similarity value. We
compute the 10-fold cross validation predictions us-
ing the different feature classes above and collect the
predicted values across all the folds. Then we com-
pute accuracy of examples within each bin. These
results are plotted in Figure 1. int-science refers to
the full set of features and the results from the six
feature classes are also indicated.
As the similarity increases, the prediction task be-
comes harder. The combination of all features gives
66% accuracy for pairs above 0.4 similarity and 74%
when the similarity is less than 0.15. Most individ-
ual feature classes also show a similar trend. This
result is understandable because articles on simi-
lar topics could exhibit similar properties. For ex-
ample, two articles about ?controversies surround-
ing vaccination? are likely to have similar levels of
people-oriented nature or written in a narrative style
Figure 1: Accuracy on pairs with different similarity
in the same way as two space-related articles are
both likely to contain high visual content. There are
however two exceptions?affect and research. For
these features, the accuracies improve with higher
similarity; affect features give 51% for pairs with
similarity 0.1 and 62% for pairs above 0.4 similar-
ity, accuracy of research features goes from 52% to
57% for the same similarity values. This finding il-
lustrates that even articles on very similar topics can
be written differently, with the articles by the excel-
lent authors associated with greater degree of senti-
ment, and deeper study of the research problem.
5 Combining aspects of article quality
We now compare and combine the genre-specific
interest-science features (41 total) with those dis-
cussed in work on readability, well-written nature,
interest and topic classification.
Readability (16 features): We test the full set of
readability features studied in Pitler and Nenkova
(2008), involving token-type ratio, word and sen-
tence length, language model features, cohesion
scores and syntactic estimates of complexity.
Well-written nature (23 features): For well-
written nature, we use two classes of features, both
related to discourse. One is the probabilities of dif-
ferent types of entity transitions from the Entity Grid
model (Barzilay and Lapata, 2008) which we com-
pute with the Brown Coherence Toolkit (Elsner et
al., 2007). The other class of features are those de-
fined in Pitler and Nenkova (2008) for likelihoods
and counts of explicit discourse relations. We iden-
tified the relations for texts in our corpus using the
349
AddDiscourse tool (Pitler and Nenkova, 2009).
Interesting fiction (22 features): are those intro-
duced by McIntyre and Lapata (2009) for predicting
interest ratings on fairy tales. They include counts of
syntactic items and relations, and token categories
from the MRC psycholinguistic database. We nor-
malize each feature by the total words in the article.
Content: features are based on the words present
in the articles. Word features are standard in
content-based recommendation systems (Pazzani et
al., 1996; Mooney and Roy, 2000) where they are
used to pick out articles similar to those which a user
has already read. In our work the features are the
most frequent n words in our corpus after removing
the 50 most frequent ones. The word?s count in the
article is the feature value. Note that word features
indicate topic as well as other content in the article
such as sentiment and research. A random sample of
the word features for n = 1000 is shown below and
reflects this aspect. ?matter, series, wear, nation, ac-
count, surgery, high, receive, remember, support, worry,
enough, office, prevent, biggest, customer?.
Table 2 compares the accuracies of SVM classi-
fiers trained on features from different classes and
their combinations.6 The readability, well-written
nature and interesting fiction classes provide good
accuracies 60% and above. The genre-specific
interesting-science features are individually much
stronger than these classes. Different writing as-
pects (without content) are clearly complementary
and when combined give 76% to 79% accuracy for
the ?any-topic? task and 74% for the topic pairs task.
The simple bag of words features work remark-
ably well giving 80% accuracy in both settings. As
mentioned before these word features are a mix of
topic indicators as well as other content of the ar-
ticles, ie., they also implicitly indicate animacy, re-
search or sentiment. But the high accuracy of word
features above all the writing categories indicates
that topic plays an important role in article quality.
However, despite the high accuracy, word features
are not easily interpretable in different classes re-
lated to writing as we have done with other writing
features. Further, the total set of writing features is
6For classifiers involving content features, we did not tune
the SVM parameters because of the small size of development
data compared to number of features. Default SVM settings
were used instead.
Feature set Any Topic Same
Interesting-science 75.3 68.0
Readable 65.5 63.0
Well-written 59.1 59.9
Interesting-fiction 67.9 62.8
Readable + well-writ 64.7 64.3
Readable + well-writ + Int-fict 71.0 70.3
Readable + well-writ + Int-sci 79.5 73.2
All writing aspects 76.7 74.7
Content (500 words) 81.7 79.4
Content (1000 words) 81.2 82.1
Combination: Writing (all) + Content (1000w)
In feature vector 82.6* 84.0*
Sum of confidence scores 81.6 84.9
Oracle 87.6 93.8
Table 2: Accuracy of different article quality aspects
only 102 in contrast to 1000 word features. In our
interest-science feature set, we aimed to highlight
how well some of the aspects considered important
to good science writing can predict quality ratings.
We also combined writing and word features to
mix topic with writing related predictors. We do the
combination in three ways a) word and writing fea-
tures are included together in the feature vector; b)
two separate classifiers are trained (one using word
features and the other using writing ones) and the
sum of confidence measures is used to decide on the
final class; c) an oracle method: two classifiers are
trained just as in option (b) but when they disagree
on the class, we pick the correct label. The oracle
method gives a simple upper bound on the accuracy
obtainable by combination. These values are 87%
for ?any-topic? and a higher 93.8% for ?same-topic?.
The automatic methods, both feature vector combi-
nation and classifier combination also give better ac-
curacies than only the word or writing features. The
accuracies for the folds from 10 fold cross valida-
tion in the feature vector combination method were
also found to be significantly higher than those from
word features only (using a paired Wilcoxon signed-
rank test). Therefore both topic and writing features
are clearly useful for identifying great articles.
6 Conclusion
Our work is a step towards measuring overall arti-
cle quality by showing the complementary benefits
of general and domain-specific writing measures as
well as indicators of article topic. In future we plan
to focus on development of more features as well as
better methods for combining different measures.
350
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL, pages 86?90.
Z. Bao, B. Kimelfeld, and Y. Li. 2011. A graph ap-
proach to spelling correction in domain-centric search.
In Proceedings of ACL-HLT, pages 905?914.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1?34.
R. Barzilay, N. Elhadad, and K. McKeown. 2002.
Inferring strategies for sentence ordering in multi-
document summar ization. Journal of Artificial Intel-
ligence Research, 17:35?55.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learning
research, 3:993?1022.
D. Blum, M. Knudson, and R. M. Henig, editors. 2006.
A field guide for science writers: the official guide of
the national association of science writers. Oxford
University Press, New York.
E. Brill and R.C. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of ACL, pages 286?293.
S. F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proceedings of ACL, pages 310?318.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of EMNLP, pages 293?
300.
R. Dale and A. Kilgarriff. 2010. Helping our own:
text massaging for computational linguistics as a new
shared task. In Proceedings of INLG, pages 263?267.
M. C. De Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC, vol-
ume 6, pages 449?454.
P. Diederich. 1974. Measuring Growth in English. Na-
tional Council of Teachers of English.
M. Elsner, J. Austerweil, and E. Charniak. 2007. A uni-
fied local and global model for discourse coherence.
In Proceedings of NAACL-HLT, pages 436?443.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
ACL, pages 363?370.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32:221 ? 233.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods In-
struments and Computers, 36(2):193?202.
H. Ji and D. Lin. 2009. Gender and animacy knowledge
discovery from web-scale n-grams for unsupervised
person name detection. In Proceedings of PACLIC.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL, pages 423?430.
S.M. Kosslyn. 1980. Image and mind. Harvard Univer-
sity Press.
M. Lapata. 2003. Probabilistic text structuring: Experi-
ments with sentence ordering. In Proceedings of ACL,
pages 545?552.
C. Lin and E. Hovy. 2000. The automated acquisition of
topic signatures for text summarization. In Proceed-
ings of COLING, pages 495?501.
D. Lin, K. W. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2010. New tools for web-
scale n-grams. In Proceedings of LREC.
N. McIntyre and M. Lapata. 2009. Learning to tell tales:
A data-driven approach to story generation. In Pro-
ceedings of ACL-IJCNLP, pages 217?225.
R. J. Mooney and L. Roy. 2000. Content-based book
recommending using learning for text categorization.
In Proceedings of the fifth ACM conference on Digital
libraries, pages 195?204.
A. Nakhimovsky. 1988. Aspect, aspectual class, and the
temporal structure of narrative. Computational Lin-
guistics, 14(2):29?43, June.
M. Pazzani, J. Muramatsu, and D. Billsus. 1996. Syskill
& Webert: Identifying interesting web sites. In Pro-
ceedings of AAAI, pages 54?61.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of EMNLP, pages 186?195.
E. Pitler and A. Nenkova. 2009. Using syntax to dis-
ambiguate explicit discourse connectives in text. In
Proceedings of ACL-IJCNLP, pages 13?16.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009. Labeled lda: A supervised topic model for
credit attribution in multi-labeled corpora. In Proceed-
ings of EMNLP, pages 248?256.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of EMNLP, pages 961?970.
E. Sandhaus. 2008. The new york times annotated cor-
pus. Corpus number LDC2008T19, Linguistic Data
Consortium, Philadelphia.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of ACL, pages 523?
530.
351
H.A. Semetko and P.M. Valkenburg. 2000. Framing eu-
ropean politics: A content analysis of press and televi-
sion news. Journal of communication, 50(2):93?109.
V. Spandel. 2004. Creating Writers Through 6-Trait
Writing: Assessment and Instruction. Allyn and Ba-
con, Inc.
S. H. Stocking. 2010. The New York Times Reader: Sci-
ence and Technology. CQ Press, Washington DC.
P. J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press.
J. R. Tetreault and M. Chodorow. 2008. The ups and
downs of preposition error detection in esl writing. In
Proceedings of COLING, pages 865?872.
L. von Ahn and L. Dabbish. 2004. Labeling images with
a computer game. In Proceedings of CHI, pages 319?
326.
R. L. Weide. 1998. The cmu pronunciation dictio-
nary, release 0.6. http://www.speech.cs.cmu.edu/cgi-
bin/cmudict.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of HLT-EMNLP, pages 347?354.
M. Wilson. 1988. MRC psycholinguistic database:
Machine-usable dictionary, version 2.00. Behavior
Research Methods, 20(1):6?10.
352
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 92?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Off-topic essay detection using short prompt texts
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Derrick Higgins
Educational Testing Service
Princeton, NJ 08541, USA
dhiggins@ets.org
Abstract
Our work addresses the problem of predict-
ing whether an essay is off-topic to a given
prompt or question without any previously-
seen essays as training data. Prior work has
used similarity between essay vocabulary and
prompt words to estimate the degree of on-
topic content. In our corpus of opinion es-
says, prompts are very short, and using sim-
ilarity with such prompts to detect off-topic
essays yields error rates of about 10%. We
propose two methods to enable better compar-
ison of prompt and essay text. We automat-
ically expand short prompts before compari-
son, with words likely to appear in an essay
to that prompt. We also apply spelling correc-
tion to the essay texts. Both methods reduce
the error rates during off-topic essay detection
and turn out to be complementary, leading to
even better performance when used in unison.
1 Introduction
It is important to limit the opportunity to sub-
mit uncooperative responses to educational software
(Baker et al, 2009). We address the task of detect-
ing essays that are irrelevant to a given prompt (es-
say question) when training data is not available and
the prompt text is very short.
When example essays for a prompt are available,
they can be used to learn word patterns to distin-
guish on-topic from off-topic essays. Alternatively,
prior work (Higgins et al, 2006) has motivated us-
ing similarity between essay and prompt vocabular-
ies to detect off-topic essays. In Section 2, we exam-
ine the performance of prompt-essay comparison for
four different essay types. We show that in the case
of prompts with 9 or 13 content words on average,
the error rates are higher compared to those with 60
or more content words. In addition, more errors are
observed when the method is used on essays written
by English language learners compared to more ad-
vanced test takers. An example short prompt from
our opinion essays? corpus is shown below. Test-
takers provided arguments for/or against the opinion
expressed by the prompt.
[1] ?In the past, people were more friendly than
they are today.?
To address this problem, we propose two en-
hancements. We use unsupervised methods to ex-
pand the prompt text with words likely to appear
in essays to that prompt. Our approach is based
on the intuition that regularities exist in the words
which appear in essays, beyond the prevalence of
actual prompt words. In a similar vein, misspellings
in the essays, particulary of the prompt words, are
also problematic for prompt-based methods. There-
fore we apply spelling correction to the essay text
before comparison. Our results show that both meth-
ods lower the error rates. The relative performance
of the two methods varies depending on the essay
type; however, their combination gives the overall
best results regardless of essay type.
2 Effect of prompt and essay properties
In this section, we analyze the off-topic essay pre-
diction accuracies resulting from direct comparison
of original prompt and essay texts. We use four dif-
ferent corpora of essays collected and scored during
high stakes tests with an English writing component.
They differ in task type and average prompt length,
as well as the skill level expected from the test taker.
92
In one of the tasks, the test taker reads a passage
and listens to a lecture and then writes a summary
of the main points. For such essays, the prompt
text (reading passage plus lecture transcript) avail-
able for comparison is quite long (about 276 con-
tent words). In the other 3 tasks, the test taker has
to provide an argument for or against some opin-
ion expressed in the prompt. One of these has long
prompts (60 content words). The other two involve
only single sentence prompts as in example [1] and
have 13 and 9 content words on average. Two of
these tasks focused on English language learners and
the other two involved advanced users (applicants to
graduate study programs in the U.S.). See Table 1
for a summary of the essay types.1
2.1 Data
For each of the task types described above, our cor-
pus contains essays written to 10 different prompts.
We used essays to 3 prompts as development data.
To build an evaluation test set, we randomly sam-
pled 350 essays for each of the 7 remaining prompts
to use as positive examples. It is difficult to as-
semble a sufficient number of naturally-occurring
off-topic essays for testing. However, an essay on-
topic to a particular prompt can be considered as
pseudo off-topic to a different prompt. Hence, to
complement the positive examples for each prompt,
an equal number of negative examples were chosen
at random from essays to the remaining 6 prompts.
2.2 Experimental setup
We use the approach for off-topic essay detection
suggested in prior work by Higgins et al (2006).
The method uses cosine overlap between tf*idf vec-
tors of prompt and essay content words to measure
the similarity between a prompt-essay pair.
sim(prompt, essay) = vessay .vprompt?vessay??vprompt?
(1)
An essay is compared with the target prompt
(prompt with which topicality must be checked) to-
gether with a set of reference prompts, different from
the target. The reference prompts are also chosen
to be different from the actual prompts of the neg-
ative examples in our dataset. If the target prompt
1Essay sources: Type 1-TOEFL integrated writing task,
Type 4-TOEFL independent writing task, Types 2 & 3-
argument and issue tasks in Analytical Writing section of GRE
Type Skill Prompt len. Avg FP Avg FN
1 Learners 276 0.73 11.79
2 Advanced 60 0.20 6.20
3 Advanced 13 2.94 8.90
4 Learners 9 9.73 11.07
Table 1: Effect of essay types: average prompt length,
false positive and false negative rates
is ranked as most similar2 in the list of compared
prompts, the essay is classified as on-topic. 9 refer-
ence prompts were used in our experiments.
We compute two error rates.
FALSE POSITIVE - percentage of on-topic essays in-
correctly flagged as off-topic.
FALSE NEGATIVE - percentage of off-topic essays
which the system failed to flag.
In this task, it is of utmost importance to maintain
very low false positive rates, as incorrect labeling of
an on-topic essay as off-topic is undesirable.
2.3 Observations
In Table 1, we report the average false positive and
false negative rates for the 7 prompts in the test set
for each essay type. For long prompts, both Types 1
and 2, the false positive rates are very low. The clas-
sification of Type 2 essays which were also written
by advanced test takers is the most accurate.
However, for essays with shorter prompts (Types
3 and 4), the false positive rates are higher. In fact,
in the case of Type 4 essays written by English lan-
guage learners, the false positive rates are as high as
10%. Therefore we focus on improving the results
in these two cases which involve short prompts.
Both prompt length and the English proficiency
of the test taker seem to influence the prediction ac-
curacies for off-topic essay detection. In our work,
we address these two challenges by: a) automatic
expansion of short prompts (Section 3) and b) cor-
rection of spelling errors in essay texts (Section 4).
3 Prompt expansion
We designed four automatic methods to add relevant
words to the prompt text.
2Less strict cutoffs may be used, for example, on-topic if
target prompt is within rank 3 or 5, etc. However even a cutoff
of 2 incorrectly classifies 25% of off-topic essays as on-topic.
93
3.1 Unsupervised methods
Inflected forms: Given a prompt word, ?friendly?,
its morphological variants??friend?, ?friendlier?,
?friendliness??are also likely to be used in essays
to that prompt. Inflected forms are the simplest and
most restrictive class in our set of expansions. They
were obtained by a rule-based approach (Leacock
and Chodorow, 2003) which adds/modifies prefixes
and suffixes of words to obtain inflected forms.
These rules were adapted from WordNet rules de-
signed to get the base forms of inflected words.
Synonyms: Words with the same meaning as
prompt words might also be mentioned over the
course of an essay. For example, ?favorable?
and ?well-disposed? are synonyms for the word
?friendly? and likely to be good expansions. We
used an in-house tool to obtain synonyms from
WordNet for each of the prompt words. The lookup
involves a word sense disambiguation step to choose
the most relevant sense for polysemous words. All
the synonyms for the chosen sense of the prompt
word are added as expansions.
Distributionally similar words: We also consider
as expansions words that appear in similar contexts
as the prompt words. For example, ?cordial?, ?po-
lite?, ?cheerful?, ?hostile?, ?calm?, ?lively? and
?affable? often appear in the same contexts as the
word ?friendly?. Such related words form part of
a concept like ?behavioral characteristics of people?
and are likely to appear in a discussion of any one
aspect. These expansions could comprise antonyms
and other related words too. This idea of word simi-
larity was implemented in work by Lin (1998). Sim-
ilarity between two words is estimated by examin-
ing the degree of overlap of their contexts in a large
corpus. We access Lin?s similarity estimates using
a tool from Leacock and Chodorow (2003) that re-
turns words with similarity values above a cutoff.
Word association norms: Word associations have
been of great interest in psycholinguistic research.
Participants are given a target word and asked to
mention words that readily come to mind. The most
frequent among these are recorded as free associa-
tions for that target. They form another interesting
category of expansions for our purpose because they
are known to be frequently recalled by human sub-
jects for a particular stimulus word. We added the
associations for prompt words from a collection of
5000 target words with their associations produced
by about 6000 participants (Nelson et al, 1998).
Sample associations for the word ?friendly? include
?smile?, ?amiable?, ?greet? and ?mean?.
3.2 Weighting of prompt words and expansions
After expansion, the prompt lengths vary between
87 (word associations) and 229 (distributionally
similar words) content words, considerably higher
than the original average length of 9 and 13 content
words. We use a simple weighting scheme3 to mit-
igate the influence of noisy expansions. We assign
a weight of 20 to original prompt words and 1 to all
the expansions. While computing similarity, we use
these weight values as the assumed frequency of the
word in the prompt. In this case, the term frequency
of original words is set as 20 and all expansion terms
are considered to appear once in the new prompt.
4 Spelling correction of essay text
Essays written by learners of a language are prone to
spelling errors. When such errors occur in the use of
the prompt words, prompt-based techniques will fail
to identify the essay as on-topic even if it actually is.
The usefulness of expansion could also be limited
if there are several spelling errors in the essay text.
Hence we explored the correction of spelling errors
in the essay before off-topic detection.
We use a tool from Leacock and Chodorow
(2003) to perform directed spelling correction, ie.,
focusing on correcting the spellings of words most
likely to match a given target list. We use the prompt
words as the targets. We also explore the simultane-
ous use of spelling correction and expansion. We
first obtain expansion words from one of our unsu-
pervised methods. We then use these along with
the prompt words for spelling correction followed
by matching of the expanded prompt and essay text.
5 Results and discussion
We used our proposed methods on the two essay col-
lections with very short prompts, Type 3 written by
3Without any weighting there was an increase in error rates
during development tests. We also experimented with a graph-
based approach to term weighting which gave similar results.
94
advanced test takers and Type 4 written by learn-
ers of English. Table 2 compares the suggested en-
hancements with the previously proposed method by
Higgins et al (2006). As discussed in Section 2.3,
using only the original prompt words, error rates are
around 10% for both essay types. For advanced test
takers, the false positive rates are lower, around 3%.
Usefulness of expanded prompts All the expansion
methods lower the false positive error rates on es-
says written by learners with almost no increase in
the rate of false negatives. On average, the false
positive errors are reduced by about 3%. Inflected
forms constitute the best individual expansion cat-
egory. The overall best performance on this type
of essays is obtained by combining inflected forms
with word associations.
In contrast, for essays written by advanced test
takers, inflected forms is the worst expansion cate-
gory. Here word associations give the best results
reducing both false positive and false negative er-
rors; the reduction in false positives is almost 50%.
These results suggest that advanced users of English
use more diverse vocabulary in their essays which
are best matched by word associations.
Effect of spelling correction For essays written by
learners, spell-correcting the essay text before com-
parison (Spell) leads to huge reductions in error
rates. Using only the original prompt, the false pos-
itive rate is 4% lower with spelling correction than
without. Note that this result is even better than the
best expansion technique?inflected forms. However,
for essays written by advanced users, spelling cor-
rection does not provide any benefits. This result
is expected since these test-takers are less likely to
produce many spelling errors.
Combination of methods The benefits of the two
methods appear to be population dependent. For
learners of English, a spelling correction module
is necessary while for advanced users, the benefits
are minimal. On the other hand, prompt expansion
works extremely well for essays written by advanced
users. The expansions are also useful for essays
written by learners but the benefits are lower com-
pared to spelling correction. However, for both es-
say types, the combination of spelling correction and
best prompt expansion method (Spell + best expn.)
is better compared to either of them individually.
Learners Advanced
Method FP FN FP FN
Prompt only 9.73 11.07 2.94 9.06
Synonyms 7.03 12.01 1.39 9.76
Dist. 6.45 11.77 1.63 8.98
WAN 6.33 11.97 1.59 8.74
Infl. forms 6.25 11.65 2.53 9.06
Infl. forms + WAN 6.04 11.48 - -
Spell 5.43 12.71 2.53 9.27
Spell + best expn. 4.66 11.97 1.47 9.02
Table 2: Average error rates after prompt expansion and
spelling correction
Therefore the best policy would be to use both en-
hancements together for prompt-based methods.
6 Conclusion
We have described methods for improving the accu-
racy of off-topic essay detection for short prompts.
We showed that it is possible to predict words that
are likely to be used in an essay based on words that
appear in its prompt. By adding such words to the
prompt automatically, we built a better representa-
tion of prompt content to compare with the essay
text. The best combination included inflected forms
and word associations, reducing the false positives
by almost 4%. We also showed that spelling correc-
tion is a very useful preprocessing step before off-
topic essay detection.
References
R.S.J.d. Baker, A.M.J.B. de Carvalho, J. Raspat,
V. Aleven, A.T. Corbett, and K.R. Koedinger. 2009.
Educational software features that encourage and dis-
courage ?gaming the system?. In Proceedings of the
International Conference on Artificial Intelligence in
Education.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12(2):145?159.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):389?405.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In COLING-ACL, pages 768?774.
D. L Nelson, C. L. McEvoy, and T. A. Schreiber.
1998. The University of South Florida word
association, rhyme, and word fragment norms,
http://www.usf.edu/FreeAssociation/.
95
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?62,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Using entity features to classify implicit discourse relations
Annie Louis, Aravind Joshi, Rashmi Prasad, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,rjprasad,nenkova}@seas.upenn.edu
Abstract
We report results on predicting the sense
of implicit discourse relations between ad-
jacent sentences in text. Our investigation
concentrates on the association between
discourse relations and properties of the
referring expressions that appear in the re-
lated sentences. The properties of inter-
est include coreference information, gram-
matical role, information status and syn-
tactic form of referring expressions. Pre-
dicting the sense of implicit discourse re-
lations based on these features is consid-
erably better than a random baseline and
several of the most discriminative features
conform with linguistic intuitions. How-
ever, these features do not perform as well
as lexical features traditionally used for
sense prediction.
1 Introduction
Coherent text is described in terms of discourse re-
lations such as ?cause? and ?contrast? between its
constituent clauses. It is also characterized by en-
tity coherence, where the connectedness of the text
is created by virtue of the mentioned entities and
the properties of referring expressions. We aim to
investigate the association between discourse rela-
tions and the way in which references to entities
are realized. In our work, we employ features re-
lated to entity realization to automatically identify
discourse relations in text.
We focus on implicit relations that hold be-
tween adjacent sentences in the absence of dis-
course connectives such as ?because? or ?but?.
Previous studies on this task have zeroed in on
lexical indicators of relation sense: dependencies
between words (Marcu and Echihabi, 2001; Blair-
Goldensohn et al, 2007) and the semantic orien-
tation of words (Pitler et al, 2009), or on general
syntactic regularities (Lin et al, 2009).
The role of entities has also been hypothesized
as important for this task and entity-related fea-
tures have been used alongside others (Corston-
Oliver, 1998; Sporleder and Lascarides, 2008).
Corpus studies and reading time experiments per-
formed by Wolf and Gibson (2006) have in fact
demonstrated that the type of discourse relation
linking two clauses influences the resolution of
pronouns in them. However, the predictive power
of entity-related features has not been studied in-
dependently of other factors. Further motivation
for studying this type of features comes from new
corpus evidence (Prasad et al, 2008), that about a
quarter of all adjacent sentences are linked purely
by entity coherence, solely because they talk about
the same entity. Entity-related features would be
expected to better separate out such relations.
We present the first comprehensive study of the
connection between entity features and discourse
relations. We show that there are notable differ-
ences in properties of referring expressions across
the different relations. Sense prediction can be
done with results better than random baseline us-
ing only entity realization information. Their per-
formance, however, is lower than a knowledge-
poor approach using only the words in the sen-
tences as features. The addition of entity features
to these basic word features is also not beneficial.
2 Data
We use 590 Wall Street Journal (WSJ) articles
with overlapping annotations for discourse, coref-
erence and syntax from three corpora.
The Penn Discourse Treebank (PDTB) (Prasad
et al, 2008) is the largest available resource of
discourse relation annotations. In the PDTB, im-
plicit relations are annotated between adjacent
sentences in the same paragraph. They are as-
signed senses from a hierarchy containing four top
level categories?Comparison, Contingency, Tem-
poral and Expansion.
59
An example ?Contingency? relation is shown
below. Here, the second sentence provides the
cause for the belief expressed in the first.
Ex 1. These rate indications aren?t directly comparable.
Lending practices vary widely by location.
Adjacent sentences can also become related
solely by talking about a common entity without
any of the above discourse relation links between
their propositions. Such pairs are annotated as En-
tity Relations (EntRels) in the PDTB, for example:
Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S
sales to remain steady at about 1,200 cars in 1990. The luxury
auto maker last year sold 1,214 cars in the U.S.
We use the coreference annotations from the
Ontonotes corpus (version 2.9) (Hovy et al, 2006)
to compute our gold-standard entity features. The
WSJ portion of this corpus contains 590 articles.
Here, nominalizations and temporal expressions
are also annotated for coreference but we use the
links between noun phrases only. We expect these
features computed on the gold-standard annota-
tions to represent an upper bound on the perfor-
mance of entity features.
Finally, the Penn Treebank corpus (Marcus et
al., 1994) is used to obtain gold-standard parse and
grammatical role information.
Only adjacent sentences within the same para-
graph are used in our experiments.
3 Entity-related features
We associate each referring expression in a sen-
tence with a set of attributes as described below.
In Section 3.2, we detail how we combine these
attributes to compute features for a sentence pair.
3.1 Referring expression attributes
Grammatical role. In exploratory analysis of
Comparison relations, we often observed parallel
syntactic realizations for entities in the subject po-
sition of the two sentences:
Ex 3. {Longer maturities}E1 are thought to indicate de-
clining interest rates. {Shorter maturities}E2 are considered
a sign of rising rates because portfolio managers can capture
higher rates sooner.
So, for each noun phrase, we record whether
it is the subject of a main clause (msubj), subject
of other clauses in the sentence (esubj) or a noun
phrase not in subject position (other).
Given vs. New. When an entity is first intro-
duced in the text, it is considered a new entity.
Subsequent mentions of the same entity are given
(Prince, 1992). New-given distinction could help
to identify some of the Expansion and Entity re-
lations. When a sentence elaborates on another, it
might contain a greater number of new entities.
We use the Ontonotes coreference annotations
to mark the information status for entities. For
an entity, if an antecedent is found in the previ-
ous sentences, it is marked as given, otherwise it
is a new entity.
Syntactic realization. In Entity relations, the sec-
ond sentence provides more information about a
specific entity in the first and a definite description
for this second mention seems likely. Also, given
the importance of named entities in news, entities
with proper names might be the ones frequently
described using Entity relations.
We use the part of speech (POS) tag associated
with the head of the noun phrase to assign one of
the following categories: pronoun, nominal, name
or expletive. When the head does not belong to
the above classes, we simply record its POS tag.
We also mark whether the noun phrase is a definite
description using the presence of the article ?the?.
Modification. We expected modification proper-
ties to be most useful for predicting Comparison
relations. Also, named or new entities in Entity
relations are very likely to have post modification.
We record whether there are premodifiers or
postmodifiers in a given referring expression. In
the absence of pre- and postmodifiers, we indicate
bare head realization.
Topicalization. Preposed prepositional or ad-
verbial phrases before the subject of a sentence
indicate the topic under which the sentence is
framed. We observed that this property is frequent
in Comparison and Temporal relations. An exam-
ple Comparison is shown below.
Ex 4. {Under British rules}T1, Blue Arrow was able to
write off at once $1.15 billion in goodwill arising from the
purchase. {As a US-based company}T2, Blue Arrow would
have to amortize the good will over as many as 40 years, cre-
ating a continuing drag on reported earnings.
When the left sibling of a referring expression is
a topicalized phrase, we mark the topic attribute.
Number. Using the POS tag of the head word, we
note whether the entity is singular or plural.
3.2 Features for classification
Next, for each sentence pair, we associate two sets
of features using the attributes described above.
60
Let S1 and S2 denote the two adjacent sentences
in a relation, where S1 occurs first in the text.
Sentence level. These features characterize S1
and S2 individually. For each sentence, we add a
feature for each of the attributes described above.
The value of the feature is the number of times that
attribute is observed in the sentence; i.e., the fea-
ture S1given would have a value of 3 if there are 3
given entities in the first sentence.
Sentence pair. These features capture the interac-
tions between the entities present in S1 and S2.
Firstly, for each pair of entities (a, b), such that
a appears in S1 and b appears in S2, we assign
one of the following classes: (i) SAME: a and b
are coreferent, (ii) RELATED: their head words are
identical, (iii) DIFFERENT: neither coreferent nor
related. The RELATED category was introduced to
capture the parallelism often present in Compari-
son relations. Even though the entities themselves
are not coreferent, they share the same head word
(i.e. longer maturities and shorter maturities).
For features, we use the combination of the
class ((i), (ii) or (iii)) with the cross product of
the attributes for a and b. For example if a has
attributes {msubj, noun, ...} and b has attributes
{esubj, defdesc, ...} and a and b are corefer-
ent, we would increment the count for features?
{sameS1msubjS2esubj, sameS1msubjS2defdesc,
sameS1nounS2esubj, sameS1nounS2defdesc ...}.
Our total set of features observed for instances
in the training data is about 2000.
We experimented with two variants of fea-
tures: one using coreference annotations from
the Ontonotes corpus (gold-standard) and an-
other based on approximate coreference informa-
tion where entities with identical head words are
marked as coreferent.
4 Experimental setup
We define five classification tasks which disam-
biguate if a specific PDTB relation holds between
adjacent sentences. In each task, we classify the
relation of interest (positive) versus a category
with a naturally occurring distribution of all of the
other relations (negative).
Sentence pairs from sections 0 to 22 of WSJ are
used as training data and we test on sections 23
and 24. Given the skewed distribution of positive
and negative examples for each task, we randomly
downsample the negative instances in the training
set to be equal to the positive examples. The sizes
of training sets for the tasks are
Expansion vs other (4716)
Contingency vs other (2466)
Comparison vs other (1138)
Temporal vs other (474)
EntRel vs other (2378)
Half of these examples are positive and the
other negative in each case.
The test set contains 1002 sentence pairs:
Comp. (133), Cont. (230), Temp. (34), Expn.
(369), EntRel (229), NoRel1 (7). We do not down-
sample our test set. Instead, we evaluate our pre-
dictions on the natural distribution present in the
data to get a realistic estimate of performance.
We train a linear SVM classifier (LIBLIN-
EAR2) for each task.3 The optimum regulariza-
tion parameter was chosen using cross validation
on the training data.
5 Results
5.1 Feature analysis
We ranked the features (based on gold-standard
coreference information) in the training sets by
their information gain. We then checked which
attributes are common among the top five features
for different classification tasks.
As we had expected, the topicalization attribute
and RELATED entities frequently appear among
the top features for Comparison.
Features with the name attribute were highly
predictive of Entity relations as hypothesized.
However, while we had expected Entity relations
to have a high rate of coreference, we found coref-
erent mentions to be very indicative of Temporal
relations: all the top features involve the SAME at-
tribute. A post-analysis showed that close to 70%
of Temporal relations involve coreferent entities
compared to around 50% for the other classes.
The number of pronouns in the second sentence
was most characteristic of the Contingency rela-
tion. In the training set for Contingency task,
about 45% of sentences pairs belonging to Contin-
gency relation have a pronoun in the second sen-
tence. This is considerably larger than 32%, which
is the percentage of sentence pairs in the negative
examples with a pronoun in second sentence.
1PDTB relation for sentence pair when both entity and
discourse relations are absent, very rare about 1% of our data.
2http://www.csie.ntu.edu.tw/?cjlin/liblinear/
3SVMs with linear kernel gave the best performance. We
also experimented with SVMs with radial basis kernel, Naive
Bayes and MaxEnt classifiers.
61
5.2 Performance on sense prediction
The classification results (fscores) are shown in
Table 1. The random baseline (Base.) represents
the results if we predicted positive and negative re-
lations according to their proportion in the test set.
Entity features based on both gold-standard
(EntGS) and approximate coreference (EntApp)
outperform the random baseline for all the tasks.
The drop in performance without gold-standard
coreference information is strongly noticable only
for Expansion relations.
The best improvement from the baseline is seen
for predicting Contingency and Entity relations,
with around 15% absolute improvement in fscore
with both EntGS and EntApp features. The im-
provements for Comparisons and Expansions are
around 11% in the approximate case. Temporal
relations benefit least from these features. These
relations are rare, comprising 3% of the test set
and harder to isolate from other relations. Overall,
our results indicate that discourse relations and en-
tity realization have a strong association.
5.3 Comparison with lexical features
In the context of using entity features for sense
prediction, one would also like to test how these
linguistically rich features compare with simpler
knowledge-lean approaches used in prior work.
Specifically, we compare with word pairs, a
simple yet powerful set of features introduced by
Marcu and Echihabi (2001). These features are the
cross product of words in the first sentence with
those in the second.
We trained classifiers on the word pairs from the
sentences in the PDTB training sets. In Table 1,
we report the performance of word pairs (WP) as
well as their combination with gold-standard en-
tity features (WP+EntGS). Word pairs turn out as
stronger predictors for all discourse relations com-
pared to our entity features (except for Expansion
prediction with EntGS features). Further, no ben-
efits over word pair results are obtained by com-
bining entity realization information.
6 Conclusion
In this work, we used a task-based approach to
show that the two components of coherence?
discourse relations and entities?are related and
interact with each other. Coreference, givenness,
syntactic form and grammatical role of entities can
predict the implicit discourse relation between ad-
Task Base. EntGS EntApp WP WP+EntGS
Comp vs Oth. 13.27 24.18 24.14 27.30 26.19
Cont vs Oth. 22.95 37.57 38.16 38.17 38.99
Temp vs Oth. 3.39 7.58 5.61 11.09 10.04
Expn vs Oth. 36.82 52.42 47.82 48.54 49.06
Ent vs Oth. 22.85 38.03 36.73 38.48 38.14
Table 1: Fscore results
jacent sentences with results better than random
baseline. However, with respect to developing au-
tomatic discourse parsers, these entity features are
less likely to be useful. They do not outperform
or complement simpler lexical features. It would
be interesting to explore whether other aspects of
entity reference might be useful for this task, such
as bridging anaphora. But currently, annotations
and tools for these phenomena are not available.
References
S. Blair-Goldensohn, K. McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic re-
lation models. In HLT-NAACL.
S.H. Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In NAACL-HLT.
Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In EMNLP.
D. Marcu and A. Echihabi. 2001. An unsupervised ap-
proach to recognizing discourse relations. In ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In ACL-IJCNLP.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.
E. Prince. 1992. The zpg letter: subject, definiteness,
and information status. In Discourse description:
diverse analyses of a fund raising text, pages 295?
325. John Benjamins.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
F. Wolf and E. Gibson. 2006. Coherence in natural
language: data structures and applications. MIT
Press.
62
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147?156,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Discourse indicators for content selection in summarization
Annie Louis, Aravind Joshi, Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104, USA
{lannie,joshi,nenkova}@seas.upenn.edu
Abstract
We present analyses aimed at eliciting
which specific aspects of discourse pro-
vide the strongest indication for text im-
portance. In the context of content selec-
tion for single document summarization of
news, we examine the benefits of both the
graph structure of text provided by dis-
course relations and the semantic sense
of these relations. We find that structure
information is the most robust indicator
of importance. Semantic sense only pro-
vides constraints on content selection but
is not indicative of important content by it-
self. However, sense features complement
structure information and lead to improved
performance. Further, both types of dis-
course information prove complementary
to non-discourse features. While our re-
sults establish the usefulness of discourse
features, we also find that lexical overlap
provides a simple and cheap alternative
to discourse for computing text structure
with comparable performance for the task
of content selection.
1 Introduction
Discourse relations such as cause, contrast or
elaboration are considered critical for text inter-
pretation, as they signal in what way parts of a text
relate to each other to form a coherent whole. For
this reason, the discourse structure of a text can be
seen as an intermediate representation, over which
an automatic summarizer can perform computa-
tions in order to identify important spans of text
to include in a summary (Ono et al, 1994; Marcu,
1998; Wolf and Gibson, 2004). In our work, we
study the content selection performance of differ-
ent types of discourse-based features.
Discourse relations interconnect units of a text
and discourse formalisms have proposed different
resulting structures for the full text, i.e. tree (Mann
and Thompson, 1988) and graph (Wolf and Gib-
son, 2005). This structure is one source of in-
formation from discourse which can be used to
compute the importance of text units. The seman-
tics of the discourse relations between sentences
could be another indicator of content importance.
For example, text units connected by ?cause? and
?contrast? relationships might be more important
content for summaries compared to those convey-
ing ?elaboration?. While previous work have fo-
cused on developing content selection methods
based upon individual frameworks (Marcu, 1998;
Wolf and Gibson, 2004; Uzda et al, 2008), little is
known about which aspects of discourse are actu-
ally correlated with content selection power.
In our work, we separate out structural and se-
mantic features and examine their usefulness. We
also investigate whether simpler intermediate rep-
resentations can be used in lieu of discourse. More
parsimonious, easy to compute representations of
text have been proposed for summarization. For
example, a text can be reduced to a set of highly
descriptive topical words, the presence of which
is used to signal importance for content selection
(Lin and Hovy, 2002; Conroy et al, 2006). Sim-
ilarly, a graph representation of the text can be
computed, in which vertices represent sentences,
and the nodes are connected when the sentences
are similar in terms of word overlap; properties of
the graph would then determine the importance of
the nodes (Erkan and Radev, 2004; Mihalcea and
Tarau, 2005) and guide content selection.
We compare the utility of discourse features for
single-document text summarization from three
frameworks: Rhetorical Structure Theory (Mann
and Thompson, 1988), Graph Bank (Wolf and
Gibson, 2005), and Penn Discourse Treebank
(PDTB) (Prasad et al, 2008). We present a de-
tailed analysis of the predictive power of different
types of discourse features for content selection
147
and compare discourse-based selection to simpler
non-discourse methods.
2 Data
We use a collection of Wall Street Journal (WSJ)
articles manually annotated for discourse infor-
mation according to three discourse frameworks.
The Rhetorical Structure Theory (RST) and Graph
Bank (GB) corpora are relatively small compared
to the Penn Discourse Treebank (PDTB) annota-
tions that cover the 1 million word WSJ part of the
Penn Treebank corpus (Marcus et al, 1994). Our
evaluation requires gold standard summaries writ-
ten by humans, so we perform our experiments on
a subset of the overlapping documents for which
we also have human summaries available.
2.1 RST corpus
RST (Mann and Thompson, 1988) proposes that
coherent text can be represented as a tree formed
by the combination of text units via discourse re-
lations. The RST corpus developed by Carlson et
al. (2001) contains discourse tree annotations for
385 WSJ articles from the Penn Treebank corpus.
The smallest annotation units in the RST corpus
are sub-sentential clauses, also called elementary
discourse units (EDUs). Adjacent EDUs combine
through rhetorical relations into larger spans such
as sentences. The larger units recursively partici-
pate in relations with others, yielding one hierar-
chical tree structure covering the entire text.
The discourse units participating in a RST re-
lation are assigned either nucleus or satellite sta-
tus; a nucleus is considered to be more central,
or important, in the text than a satellite. Rela-
tions composed of one nucleus and one satellite
are called mononuclear relations. On the other
hand, in multinuclear relations, two or more text
units participate, and all are considered equally
important. The RST corpus is annotated with 53
mononuclear and 25 multinuclear relations. Rela-
tions that convey similar meaning are grouped, re-
sulting in 16 classes of relations: Cause, Comparison,
Condition, Contrast, Attribution, Background, Elaboration,
Enablement, Evaluation, Explanation, Joint, Manner-Means,
Topic-Comment, Summary, Temporal and Topic-Change.
2.2 Graph Bank corpus
Sometimes, texts cannot be described in a tree
structure as hypothesized by the RST. For exam-
ple, crossing dependencies and nodes with multi-
ple parents appear frequently in texts and do not
allow a tree structure to be built (Lee et al, 2008).
To address this problem, general graph representa-
tion was proposed by Wolf and Gibson (2005) as
a more realistic model of discourse structure.
Graph annotations of discourse are available for
135 documents (105 from AP Newswire and 30
from the WSJ) as part of the Graph Bank cor-
pus (Wolf and Gibson, 2005). Clauses are the ba-
sic discourse segments in this annotation. These
units are represented as the nodes in a graph, and
are linked with one another through 11 differ-
ent rhetorical relations: Cause-effect, Condition, Vio-
lated expectation, Elaboration, Example, Generalization, At-
tribution, Temporal sequence, Similarity, Contrast and Same.
The edge between two nodes representing a rela-
tion is directed in the case of asymmetric relations
such as Cause and Condition and undirected for
symmetric relations like Similarity and Contrast.
2.3 Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) is theory-neutral and does not make
any assumptions about the form of the overall dis-
course structure of text. Instead, this approach fo-
cuses on local and lexically-triggered discourse re-
lations. Annotators identify explicit signals such
as discourse connectives: ?but?, ?because?, ?while?
and mark the text spans which they relate. The
relations between these spans are called explicit
relations. In addition, adjacent sentences in a dis-
course are also semantically related even in the ab-
sence of explicit markers. In the PDTB, these are
called implicit relations and are annotated between
adjacent sentences in the same paragraph.
For both implicit and explicit relations, senses
are assigned from a hierarchy containing four
top-level categories: Comparison (contrast, prag-
matic contrast, concession, pragmatic concession), Contin-
gency (cause, pragmatic cause, condition, pragmatic con-
dition) , Expansion (conjunction, instantiation, restate-
ment, alternative, exception, list) and Temporal (asyn-
chronous, synchronous). The top level senses are di-
vided into types and subtypes that represent more
fine grained senses?the second level senses are
listed in parentheses above.
PDTB also provides annotations for the text
spans of the two arguments (referred to Arg1 and
Arg2) involved in a relation. In explicit relations,
the argument syntactically bound to the discourse
connective is called Arg2. The other argument is
148
referred to as Arg1. For implicit relations, the ar-
gument occurring first in the text is named Arg1,
the one appearing later is called Arg2.
2.4 Human summaries
Human summaries are available for some of the
WSJ articles. These summaries are extractive: hu-
man judges identified and extracted important text
units from the source articles and used them as
such to compose the summary.
The RST corpus contains summaries for 150
documents. Two annotators selected the most im-
portant EDUs from these documents and created
summaries that contain about square root of the
number of EDUs in the source document. For
convenience, we adopt sentences as the common
unit for comparison across all frameworks. So,
we mapped the summary EDUs to the sentences
which contain them. Two variable length sum-
maries for each document were obtained in this
way. In some documents, it was not possible to
align EDUs automatically with gold standard sen-
tence boundaries given by the Penn Treebank and
these were not used in our work. We perform
our experiments on the remaining 124 document-
summary pairs. These documents consisted of
4,765 sentences in total, of which 1,152 were la-
beled as important sentences because they con-
tained EDUs selected by at least one annotator.
The Graph Bank corpus also contains human
summaries. However, only 15 are for documents
for which RST and PDTB annotations are also
available. These summaries were created by fif-
teen human annotators who ranked the sentences
in each document on a scale from 1 (low impor-
tance) to 7 (very important for a summary). For
each document, we ordered the sentences accord-
ing to the average rank from the annotators, and
created a summary of 100 words using the top
ranked sentences. The number of summary (im-
portant) sentences is 67, out of a total of 308 sen-
tences from the 15 documents.
3 Features for content selection
In this section, we describe two sets of discourse
features?structural and semantic. The structure
features are derived from RST trees and do not
involve specific relations. Rather they compute
the importance of a segment as a function of its
position in the global structure of the entire text.
On the other hand, semantic features indicate the
sense of a relation between two sentences and
do not involve structure information. We com-
pute these from the PDTB annotations. To un-
derstand the benefits of discourse information, we
also study the performance of some non-discourse
features standardly used in summarization.
3.1 Structural features: RST-based
Prior work in text summarization has developed
content selection methods using properties of the
RST tree: the nucleus-satellite distinction, notions
of salience and the level of an EDU in the tree.
In early work, Ono et al (1994) suggested
a penalty score for every EDU based on their
nucleus-satellite status. Since satellites of rela-
tions are considered less important than the corre-
sponding nuclei, spans that appear as satellites can
be assigned a lower score than the nucleus spans.
This intuition is implemented by Ono et al (1994)
as a penalty value for each EDU, defined as the
number of satellite nodes found on the path from
the root of the tree to that EDU. Figure 1 shows
the RST tree (Carlson et al, 2002) for the follow-
ing sentence which contains four EDUs.
1. [Mr. Watkins said] 2. [volume on Interprovincial?s sys-
tem is down about 2% since January] 3. [and is expected to
fall further,] 4. [making expansion unnecessary until perhaps
the mid-1990s.]
The spans of individual EDUs are represented
at the leaves of the tree. At the root of the tree, the
span covers the entire text. The path from EDU 1
to the root contains one satellite node. It is there-
fore assigned a penalty of 1. Paths to the root from
all other EDUs involve only nucleus nodes and
subsequently these EDUs do not incur any penalty.
Figure 1: RST tree for the example sentence in
Section 3.1.
Marcu (1998) proposed another method to uti-
lize the nucleus-satellite distinction, rewarding nu-
cleus status instead of penalizing satellite. He put
forward the idea of a promotion set, consisting of
149
salient/important units of a text span. The nu-
cleus is the more salient unit in the full span of
a mononuclear relation. In a multinuclear relation,
all the nuclei are salient units of the larger span.
For example, in Figure 1, EDUs 2 and 3 partici-
pate in a multinuclear (List) relation. As a result,
both EDUs 2 and 3 appear in the promotion set of
their combined span. The salient units (promotion
set) of each text span are shown above the horizon-
tal line which represents the span. At the leaves,
salient units are the EDUs themselves.
For the purpose of identifying important con-
tent, units in the promotion sets of nodes close to
the root are hypothesized to be more important
than those at lower levels. The highest promo-
tion of an EDU occurs at the node closest to the
root which contains that EDU in its promotion set.
The depth of the tree from the highest promotion
is assigned as the score for that EDU. Hence, the
closer to the root an EDU is promoted, the better
its score. Since EDUs 2, 3 and 4 are promoted all
the way up to the root of the tree, the score as-
signed to them is equal to 4, the total depth of the
tree. EDU 1 receives a depth score of 3.
However, notice that EDUs 2 and 3 are pro-
moted to the root from a greater depth than EDU
4 but all three receive the same depth score. But
an EDU promoted successively over multiple lev-
els should be more important than one which is
promoted fewer times. In order to make this dis-
tinction, a promotion score was also introduced by
Marcu (1998) which is a measure of the number
of levels over which an EDU is promoted. Now,
EDUs 2 and 3 receive a promotion score of three
while the score of EDU 4 is only two.
For our experiments, we use the nucleus-
satellite penalty, depth and promotion based scores
as features. Because all these scores depend on the
length of the document, another set of the same
features normalized by number of words in the
document are also included. The penalty/score for
a sentence is computed as the maximum of the
penalties/scores of its constituent EDUs.
3.2 Semantic features: PDTB-based
These features represent sentences purely in terms
of the relations which they participate in. For each
sentence, we use the PDTB annotations to encode
the sense of the relation expressed by the sentence
and the type of realization (explicit or implicit).
For example, the sentence below expresses a
Contingency relation.
In addition, its machines are easier to operate, so cus-
tomers require less assistance from software.
For such sentences that contain both the argu-
ments of a relation ie., expresses the relation by
itself, we set the feature ?expresses relation?. For
the above sentence, the binary feature ?expresses
Contingency relation? would be true.
Alternatively, sentences participating in multi-
sentential relations will have one of the following
features on: ?contains Arg1 of relation? or ?con-
tains Arg2 of relation?. Therefore, for the follow-
ing sentences in an Expansion relation, we record
the feature ?contains Arg1 of Expansion relation?
for sentence (1) and for sentence (2), ?contains
Arg2 of Expansion relation?.
(1) Wednesday?s dominant issue was Yasuda &Marine In-
surance, which continued to surge on rumors of speculative
buying. (2) It ended the day up 80 yen to 1880 yen.
We combine the implicit/explicit type distinc-
tion of the relations with the other features de-
scribed so far, doubling the number of features.
We also added features that use the second level
sense of a relation. So, the relevant features for
sentence (1) above would be ?contains Arg1 of
Implicit Expansion relation? as well as ?contains
Arg1 of Implicit Restatement relation? (Restate-
ment is a type of Expansion relation (Section 2.3)).
In addition, we include features measuring the
number of relations shared by a sentence (implicit,
explicit and total) and the distance between argu-
ments of explicit relations (the distance of Arg1
when the sentence contains Arg2).
3.3 Non-discourse features
We use standard non-discourse features used in
summarization: length of the sentence, whether
the sentence is paragraph initial or the first sen-
tence of a document, and its offsets from docu-
ment beginning as well as paragraph beginning
and end (Edmundson, 1969). We also include the
average, sum and product probabilities of the con-
tent words appearing in sentences (Nenkova et al,
2006) and the number of topic signature words in
the sentence (Lin and Hovy, 2000).
4 Predictive power of features
We used the human summaries from the RST cor-
pus to study which features strongly correlate with
the important sentences selected by humans. For
binary features such as ?does the sentence con-
150
tain a Contingency relation?, a chi-square test was
computed to measure the association between a
feature and sentence class (in summary or not in
summary). For real-valued features, comparison
between important and unimportant/non-summary
sentences was done using a two-sided t-test. The
significant features from our different classes are
reported in the Appendix?Tables 5, 6 and 7. A
brief summary of the results is provided below.
Significant features that have higher values for
sentences selected in a summary are:
Structural: depth score and promotion score?both normal-
ized and unnormalized.
Semantic-PDTB-level11: contains Arg1 of Explicit Expan-
sion, contains Arg1 of Implicit Contingency, contains Arg1
of Implicit Expansion, distance of other argument
Non-discourse: length, is the first sentence in the article, is
the first sentence in the paragraph, offset from paragraph end,
number of topic signature terms present, average probability
of content words, sum of probabilities of content words
Significant features that have higher values for
sentences not selected in a summary are:
Structural: Ono penalty?normalized and unnormalized.
Semantic-PDTB-level1: expresses Explicit Expansion, ex-
presses Explicit Contingency, contains Arg2 of Implicit Tem-
poral relation, contains Arg2 of Implicit Contingency, con-
tains Arg2 of Implicit Expansion, contains Arg2 of Implicit
Comparison, number of shared implicit relations, total shared
relations
Non-discourse: offset from paragraph beginning, offset
from article beginning, sentence probability based on content
words.
All the structural features prove to be strong in-
dicators for content selection. RST depth and pro-
motion scores are higher for important sentences.
Unimportant sentences have high penalties.
On the other hand, note that most of the sig-
nificant sense features are descriptive of the ma-
jority class of sentences?those not important or
not selected to appear in the summary (refer Ta-
ble 7). For example, the second arguments of
all the first level implicit PDTB relations are not
preferred in human summaries. Most of the sec-
ond level sense features also serve as indicators
for what content should not be included in a sum-
mary. Such features can be used to derive con-
straints on what content is not important, but there
are only few indicators associated with important
sentences. Overall, out of the 25 first and second
1Features based on the PDTB level 1 senses. The signif-
icant features based on the level 2 senses are reported in the
appendix.
level sense features which turned out to be signifi-
cantly related to a sentence class, only 8 are those
indicative of important content.
Another compelling observation is that highly
cognitively salient discourse relations such as
Contrast and Cause are not indicative of important
sentences. Of the features that indicate the occur-
rence of a particular relation in a sentence, only
two are significant, but they are predictive of non-
important sentences. These are ?expresses Ex-
plicit Expansion? (also subtypes Conjunction and
List) and ?expresses Explicit Contingency?.
An additional noteworthy fact is the differences
between implicit and explicit relations that hold
across sentences. For implicit relations, the tests
show a strong indication that the second arguments
of Implicit Contingency or Expansion would not
be included in a summary, their first arguments
however are often important and likely to appear
in a summary. At the same time, for explicit rela-
tions, there is no regularity for any of the relations
of which of the two arguments is more important.
All the non-discourse features turned out highly
significant (Table 6). Longer sentences, those in
the beginning of an article or its paragraphs and
sentences containing frequent content words are
preferred in human summaries.
5 Classification performance
We now test the strengths and complementary be-
havior of these features in a classification task to
predict important sentences from input texts.
5.1 Comparison of feature classes
Table 1 gives the overall accuracy, as well as pre-
cision and recall for the important/summary sen-
tences. Features classes were combined using lo-
gistic regression. The reported results are from 10-
fold cross-validation runs on sentences from the
124 WSJ articles for which human summaries are
available in the RST corpus. For the classifier us-
ing sense information from the PDTB, all the fea-
tures described in Section 3.2 were used.
The best class of features turn out to be the
structure-based ones. They outperform both non-
discourse (ND) and sense features by a large mar-
gin. F-measure for the RST-based classifier is
33.50%. The semantic type of relations, on the
other hand, gives no indication of content impor-
tance obtaining an F-score of only 9%. Non-
discourse features provide an F-score of 19%,
151
which is much better than the semantic class but
still less than structural discourse features.
The structure and semantic features are com-
plementary to each other. The performance of
the classifier is substantially improved when both
types of features are used (line 6 in Table 1). The
F-score for the combined classifier is 40%, which
amounts to 7% absolute improvement over the
structure-only classifier.
Discourse information is also complementary
to non-discourse. Adding discourse structure
or sense features to non-discourse (ND) features
leads to better classification decisions (lines 4, 5
in Table 1). Particularly notable is the improve-
ment when sense and non-discourse features are
combined?over 10% better F-score than the classi-
fier using only non-discourse features. The overall
best classifier is the combination of discourse?
structure as well as sense?and non-discourse fea-
tures. Here, recall for important sentences is 34%
and the precision of predictions is 62%.
We also evaluated the features using ROUGE
(Lin and Hovy, 2003; Lin, 2004). ROUGE com-
putes ngram overlaps between human reference
summaries and a given system summary. This
measure allows us to compare the human sum-
maries and classifier predictions at word level
rather than using full sentence matches.
To perform ROUGE evaluation, summaries for
our different classes of features were obtained as
follows. Important sentences for each document
were predicted using a logistic regression classi-
fier trained on all other documents. When the
number of sentences predicted to be important
was not sufficient to meet the required summary
length, sentences predicted with lowest confidence
to be non-important were selected. All summaries
were truncated to 100 words. Stemming was used,
and stop words were excluded from the calcula-
tion. Both human extracts were used as references.
The results from this evaluation are shown in
Table 2. They closely mirror the results obtained
using precision and recall. The sense features per-
form worse than the structural and non-discourse
features. The best set of features is the one com-
bining structure, sense and non-discourse features,
with ROUGE-1 score (unigram overlap) of 0.479.
Overall, combining types of features considerably
improves results in all cases. However, unlike
in the precision and recall evaluation, structural
and non-discourse features perform very similarly.
Features used Acc P R F
structural 78.11 63.38 22.77 33.50
semantic 75.53 44.31 5.04 9.05
non-discourse (ND) 77.25 67.48 11.02 18.95
ND + semantic 77.38 59.38 20.62 30.61
ND + structural 78.51 63.49 26.05 36.94
semantic + structural 77.94 58.39 30.47 40.04
structural + semantic + ND 78.93 61.85 34.42 44.23
Table 1: Accuracy (Acc) and Precision (P), Recall
(R) and F-score (F) of important sentences.
Features ROUGE Features ROUGE
structural + semantic + ND 0.479 ND 0.432
structural + ND 0.468 LEAD 0.411
structural + semantic 0.453 semantic 0.369
semantic + ND 0.444 TS 0.338
structural 0.433
Table 2: ROUGE-1 recall scores
Their ROUGE-1 recall scores are 0.433 and 0.432
respectively. The top ranked sentences by both
sets of features appear to contain similar content.
We also evaluated sentences chosen by two
baseline summarizers. The first, LEAD, includes
sentences from the beginning of the article up to
the word limit. This simple method is a very com-
petitive baseline for single document summariza-
tion. The second baseline ranks sentences based
on the proportion of topic signature (TS) words
contained in the sentences (Conroy et al, 2006).
This approach leads to very good results in identi-
fying important content for multi-document sum-
maries where there is more redundancy, but it is
the worst when measured by ROUGE-1 on this
single document task. Structure and non-discourse
features outperform both these baselines.
5.2 Tree vs. graph discourse structure
Wolf and Gibson (2004) showed that the Graph
Bank annotations of texts can be used for sum-
marization with results superior to that based on
RST trees. In order to derive the importance of
sentences from the graph representation, they use
the PageRank algorithm (Page et al, 1998). These
scores, similar to RST features, are based only on
the link structure; the semantic type of the relation
linking the sentences is not used. In Table 3, we
report the performance of structural features from
RST and Graph Bank on the 15 documents with
overlapping annotations from the two frameworks.
As discussed by Wolf and Gibson (2004), we
find that the Graph Bank discourse representation
(GB) leads to better sentence choices than using
RST trees. The F-score is 48% for the GB clas-
152
Features Acc P R F ROUGE
RST-struct. 81.61 63.00 31.56 42.05 0.569
GB-struct. 82.58 62.50 39.16 48.15 0.508
Table 3: Tree vs graph-based discourse features
sifier and 42% for the RST classifier. The better
performance of GB method comes from higher re-
call scores compared to RST. Their precision val-
ues are comparable. But, in terms of ngram-based
ROUGE scores, the results from RST (0.569)
turn out slightly better than GB (0.508). Over-
all, discourse features based on structure turn out
as strong indicators of sentence importance and
we find both tree and graph representations to be
equally useful for this purpose.
6 Lexical approximation to discourse
structure
In prior work on summarization, graph models of
text have been proposed that do not rely on dis-
course. Rather, lexical similarity between sen-
tences is used to induce graph structure (Erkan
and Radev, 2004; Mihalcea and Tarau, 2005).
PageRank-based computation of sentence impor-
tance have been used on these models with good
results. Now, we would like to see if the discourse
graphs from the Graph Bank (GB) corpus would
be more helpful for determining content impor-
tance than the general text graph based on lexi-
cal similarity (LEX). We perform this comparison
on the 15 documents that we used in the previous
section for evaluating tree versus graph structures.
We used cosine similarity to link sentences in the
lexical graph. Links with similarity less than 0.1
were removed to filter out weak relationships.
The classification results are shown in Table 4.
The similarity graph representation is even more
helpful than RST or GB: the F-score is 53% com-
pared to 42% for RST and 48% for GB. The most
significant improvement from the lexical graph is
in terms of precision 75% which is more than 10%
higher compared to RST and GB features. Using
ROUGE as the evaluation metric, the lexical sim-
ilarity graph, LEX (0.557), gives comparable per-
formance with both GB (0.508) and RST (0.569)
representations (refer Table 3). Therefore, for use
in content selection, lexical overlap information
appears to be a good proxy for building text struc-
ture in place of discourse relations.
Features Acc P R F ROUGE
LEX-struct. 83.23 75.17 41.14 53.18 0.557
Table 4: Performance of lexrank summarizer
7 Discussion
We have analyzed the contribution of different
types of discourse features?structural and seman-
tic. Our results provide strong evidence that dis-
course structure is the most useful aspect. Both
tree and graph representations of discourse can be
used to compute the importance of text units with
very good results. On the other hand, sense in-
formation from discourse does not provide strong
indicators of good content but some constraints
as to which content should not be included in
a summary. These sense features complement
structure information leading to improved perfor-
mance. Further, both these types of discourse fea-
tures are complementary to standardly used non-
discourse features for content selection.
However, building automatic parsers for dis-
course information has proven to be a hard task
overall (Marcu, 2000; Soricut and Marcu, 2003;
Wellner et al, 2006; Sporleder and Lascarides,
2008; Pitler et al, 2009) and the state of cur-
rent parsers might limit the benefits obtainable
from discourse. Moreover, discourse-based struc-
ture is only as useful for content selection as sim-
pler text structure built using lexical similarity.
Even with gold standard annotations, the perfor-
mance of structural features based on the RST
and Graph Bank representations is not better than
that obtained from automatically computed lexical
graphs. So, even if robust discourse parsers exist
to use these features on other test sets, it is not
likely that discourse features would provide better
performance than lexical similarity. Therefore, for
content selection in summarization, current sys-
tems can make use of simple lexical structures to
obtain similar performance as discourse features.
But it should be remembered that summary
quality does not depend on content selection per-
formance alone. Systems should also produce lin-
guistically well formed summaries and currently
systems perform poorly on this aspect. To address
this problem, discourse information is vital. The
most comprehensive study of text quality of au-
tomatically produced summaries was performed
by Otterbacher et al (2002). A collection of 15
automatically produced summaries was manually
edited in order to correct any problems. The study
153
found that discourse and temporal ordering prob-
lems account for 34% and 22% respectively of all
the required revisions. Therefore, we suspect that
for building summarization systems, most benefits
from discourse can be obtained with regard to text
quality compared to the task of content selection.
We plan to focus on this aspect of discourse use
for our future work.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of SIGdial, pages 1?10.
L. Carlson, D. Marcu, andM. E. Okurowski. 2002. Rst
discourse treebank. Corpus number LDC 2002T07,
Linguistic Data Consortium, Philadelphia.
J. Conroy, J. Schlesinger, and D. O?Leary. 2006.
Topic-focused multi-document summarization using
an approximate oracle score. In Proceedings of
ACL.
H.P. Edmundson. 1969. New methods in automatic
extracting. Journal of the ACM, 16(2):264?285.
G. Erkan and D. Radev. 2004. Lexrank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
A. Lee, R. Prasad, A. Joshi, and B. Webber. 2008. De-
partures from Tree Structures in Discourse: Shared
Arguments in the Penn Discourse Treebank. In Pro-
ceedings of the Constraints in Discourse Workshop.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING, pages 495?501.
C. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In Proceedings of the ACL
Workshop on Automatic Summarization.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In Proceedings of ACL Text
Summarization Workshop.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu. 1998. To build text summaries of high qual-
ity, nuclearity is not sufficient. In Working Notes
of the the AAAI-98 Spring Symposium on Intelligent
Text Summarization, pages 1?8.
D. Marcu. 2000. The rhetorical parsing of unrestricted
texts: A surface-based approach. Computational
Linguistics, 26(3):395?448.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
R. Mihalcea and P. Tarau. 2005. An algorithm for
language independent single and multiple document
summarization. In Proceedings of IJCNLP.
A. Nenkova, L. Vanderwende, and K. McKeown.
2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In Proceedings of SIGIR.
K. Ono, K. Sumita, and S. Miike. 1994. Abstract gen-
eration based on rhetorical structure extraction. In
Proceedings of COLING, pages 344?348.
J.C. Otterbacher, D.R. Radev, and A. Luo. 2002. Revi-
sions that improve cohesion in multi-document sum-
maries: a preliminary study. In Proceedings of ACL
Text Summarization Workshop, pages 27?36.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proceedings of ACL-IJCNLP, pages 683?
691.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In Proceedings of
LREC.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369?416.
V.R. Uzda, T.A.S. Pardo, and M.G. Nunes. 2008.
Evaluation of automatic text summarization meth-
ods based on rhetorical structure theory. Intelligent
Systems Design and Applications, 2:389?394.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Saur??. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of SIGdial,
pages 117?125.
F. Wolf and E. Gibson. 2004. Paragraph-, word-, and
coherence-based approaches to sentence ranking: A
comparison of algorithm and human performance.
In Proceedings of ACL, pages 383?390.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249?288.
154
Appendix: Feature analysis
This appendix provides the results from statistical tests for identifying predictive features from the dif-
ferent classes (RST-based structural features?Table 5, Non-discourse features?Table 6 and PDTB-based
sense features?Table 7).
For real-valued features, we performed a two sided t-test between the corresponding feature values
for important versus non-important sentences. For features which turned out significant in each set, the
value of the test statistic and significance levels are reported in the tables.
For binary features, we report results from a chi-square test to measure how indicative a feature is
for the class of important or non-important sentences. For results from the chi-square test, a (+/-) sign
is enclosed within parentheses for each significant feature to indicate whether the observed number of
times the feature was true in important sentences is greater (+) than the expected value (indication that
this feature is frequently associated with important sentences). When the observed frequency is less than
the expected value, a (-) sign is appended.
RST Features t-stat p-value
Ono penalty -21.31 2.2e-16
Depth score 16.75 2.2e-16
Promotion score 16.00 2.2e-16
Normalized penalty -11.24 2.2e-16
Normalized depth score 17.24 2.2e-16
Normalized promotion score 14.36 2.2e-16
Table 5: Significant RST-based features
Non-discourse features t-stat p-value
Sentence length 3.14 0.0017
Average probability of content words 9.32 2.2e-16
Sum probability of content words 11.83 2.2e-16
Product probability of content words -5.09 3.8e-07
Number of topic signature terms 9.47 2.2e-16
Offset from article beginning -12.54 2.2e-16
Offset from paragraph beginning -28.81 2.2e-16
Offset from paragraph end 7.26 5.8e-13
?
2 p-value
First sentence? 224.63 (+) 2.2e-16
Paragraph initial? 655.82 (+) 2.2e-16
Table 6: Significant non-discourse features
155
PDTB features t-stat p-value
No. of implicit relations involved -9.13 2.2e-16
Total relations involved -6.95 4.9e-12
Distance of Arg1 3.99 6.6e-05
Based on level 1 senses
?
2 p-value
Expresses explicit Expansion 12.96 (-) 0.0003
Expresses explicit Contingency 7.35 (-) 0.0067
Arg1 explicit Expansion 12.87 (+) 0.0003
Arg1 implicit Contingency 13.84 (+) 0.0002
Arg1 implicit Expansion 29.10 (+) 6.8e-08
Arg2 implicit Temporal 4.58 (-) 0.0323
Arg2 implicit Contingency 60.28 (-) 8.2e-15
Arg2 implicit Expansion 134.60 (-) 2.2e-16
Arg2 implicit Comparison 27.59 (-) 1.5e-07
Based on level 2 senses
?
2 p-value
Expresses explicit Conjunction 8.60 (-) 0.0034
Expresses explicit List 4.41 (-) 0.0358
Arg1 explicit Conjunction 10.35 (+) 0.0013
Arg1 implicit Conjunction 5.26 (+) 0.0218
Arg1 implicit Instantiation 18.94 (+) 1.4e-05
Arg1 implicit Restatement 15.35 (+) 8.9-05
Arg1 implicit Cause 12.78 (+) 0.0004
Arg1 implicit List 5.89 (-) 0.0153
Arg2 explicit Asynchronous 4.23 (-) 0.0398
Arg2 explicit Instantiation 10.92 (-) 0.0009
Arg2 implicit Conjunction 51.57 (-) 6.9e-13
Arg2 implicit Instantiation 12.08 (-) 0.0005
Arg2 implicit Restatement 28.24 (-) 1.1e-07
Arg2 implicit Cause 58.62 (-) 1.9e-14
Arg2 implicit Contrast 30.08 (-) 4.2e-08
Arg2 implicit List 12.31 (-) 1.9e-14
Table 7: Significant PDTB-based features
156
Workshop on Monolingual Text-To-Text Generation, pages 34?42,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34?42,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Text specificity and impact on quality of news summaries
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104
lannie@seas.upenn.edu
Ani Nenkova
University of Pennsylvania
Philadelphia, PA 19104
nenkova@seas.upenn.edu
Abstract
In our work we use an existing classifier to
quantify and analyze the level of specific and
general content in news documents and their
human and automatic summaries. We dis-
cover that while human abstracts contain a
more balanced mix of general and specific
content, automatic summaries are overwhelm-
ingly specific. We also provide an analysis of
summary specificity and the summary qual-
ity scores assigned by people. We find that
too much specificity could adversely affect the
quality of content in the summary. Our find-
ings give strong evidence for the need for a
new task in abstractive summarization: identi-
fication and generation of general sentences.
1 Introduction
Traditional summarization systems are primarily
concerned with the identification of important and
unimportant content in the text to be summarized.
Placing the focus on this distinction naturally leads
the summarizers to completely avoid the task of text-
to-text generation and instead just select sentences
for inclusion in the summary. In this work, we argue
that the general and specific nature of the content is
also taken into account by human summarizers; we
show that this distinction is directly related to the
quality of the summary and it also calls for the use
and refinement of text-to-text generation techniques.
General sentences are overview statements. Spe-
cific sentences supply details. An example general
and specific sentence from different parts of a news
article are shown in Table 1.
[1] The first shock let up as the eye of the storm moved
across the city.
[2] The National Hurricane Center in Miami reported its
position at 2 a.m. Sunday at latitude 16.1 north, longitude
67.5 west, about 140 miles south of Ponce, Puerto Rico,
and 200 miles southeast of Santo Domingo.
Table 1: General (in italics) and specific sentences
Prior studies have advocated that the distinction
between general and specific content is relevant for
text summarization. Jing and McKeown (2000)
studied what edits people use to create summaries
from sentences in the source text. Two of the op-
erations they identify are generalization and specifi-
cation where the source content gets changed in the
summary with respect to specificity. In more recent
work, Haghighi and Vanderwende (2009) built a
summarization system based on topic models, where
both topics at general document level as well as
those at specific subtopic levels were learnt. The
underlying idea here is that summaries are gener-
ated by a combination of content from both these
levels. But since the preference for these two types
of content is not known, Haghighi and Vanderwende
(2009) use some heuristic proportions.
Many systems that deal with sentence compres-
sion (Knight and Marcu, 2002; McDonald, 2006;
Galley and McKeown, 2007; Clarke and Lapata,
2008) and fusion (Barzilay and McKeown, 2005;
Filippova and Strube, 2008), do not take into ac-
count the specificity of the original or desired sen-
tence. However, Wan et al (2008) introduce a gen-
eration task where a summary sentence is created
by combining content from a key (general) sentence
and its supporting sentences in the source. More
34
recently, Marsi et al (2010) manually annotated
the transformations between source and compressed
phrases and observe that generalization is a frequent
transformation.
But it is not known what distribution of general
and specific content is natural for summaries. In ad-
dition, an analysis of whether this aspect is related
to quality of the summary has also not been done so
far. We address this issue in our work, making use
of an accurate classifier to identify general and spe-
cific sentences that we have developed (Louis and
Nenkova, 2011).
We present the first quantitative analysis of gen-
eral and specific content in a large corpus of news
documents and human and automatic summaries
produced for them. Our findings reveal that human-
written abstracts have much more general content
compared to human and system produced extractive
summaries. We also provide an analysis of how this
difference in specificity is related to aspects of sum-
mary quality. We show that too much specificity
could adversely affect the quality of summary con-
tent. So we propose the task of creating general
sentences for use in summaries. As a starting point
in this direction, we discuss some insights into the
identification and generation of general sentences.
2 Data
We obtained news documents and their sum-
maries from the Document Understanding Confer-
ence (DUC) evaluations. We use the data from
2002 because they contain the three different types
of summaries we wish to analyze?abstracts and
extracts produced by people, and automatic sum-
maries. For extracts, the person could only select
complete sentences, without any modification, from
the input articles. When writing abstracts people
were free to write the summary in their own words.
We use data from the generic multi-document
summarization task. There were 59 input sets, each
containing 5 to 15 news documents on a topic. The
task is to provide a 200 word summary. Two human-
written abstracts and two extracts were produced for
each input by trained assessors at NIST. Nine au-
tomatic systems participated in the conference that
year and we have 524 automatic summaries overall.
3 General and specific sentences in news
Before we present our analysis of general and spe-
cific content in news summaries, we provide a brief
description of our classifier and some example pre-
dictions. Our classifier is designed to predict for a
given sentence, its class as general or specific.
As in our example in Table 1, a general sentence
hints at a topic the writer wishes to convey but does
not provide details. So a reader expects to see more
explanation and specific sentences satisfy this role.
We observed that certain properties are prominent
in general sentences. They either express a strong
sentiment, are vague or contain surprising content.
Accordingly our features were based on word speci-
ficity, language models, length of syntactic phrases
and the presence of polarity words. Just the words in
the sentences were also a strong indicator of general
or specific nature. But we found the combination of
all non-lexical features to provide the best accuracy
and is the setup we use in this work.
We trained our classifier on general and specific
sentences from news texts. Initially, we utilized ex-
isting annotations of discourse relations as training
data. This choice was based on our hypotheses that
discourse relations such as exemplification relate a
general with a specific sentence. Later, we verified
the performance of the classifier on human anno-
tated general and specific sentences, also from two
genre of news articles, and obtained similar and ac-
curate predictions. Detailed description of the fea-
tures and training data can be found in Louis and
Nenkova (2011).
Our classifier uses logistic regression and so apart
from hard prediction into general/specific classes,
we can also obtain a confidence (probability) mea-
sure for membership in a particular class. In our
tests, we found that for sentences where there is high
annotator agreement for placing in a particular class,
the classifier also produces a high confidence predic-
tion on the correct class. When the agreement was
not high, the classifier confidence was lower. In this
way, the confidence score indicates the level of gen-
eral or specific content. So for our experiments in
this paper, we choose to use the confidence score for
a sentence belonging to a class rather than the clas-
sification decision.
The overall accuracy of the classifier in binary
35
[G1] ?The crisis is not over?.
[G2] No casualties have been reported, but experts are concerned that a major eruption could occur soon.
[G3] Seismologists said the volcano had plenty of built-up magma and even more severe eruptions could come later.
[G4] Their predictions might be a false alarm ? the volcano may have done its worst already.
[S1] (These volcanoes ? including Mount Lassen in Shasta County, and Mount Rainier and Mount St. Helens in Washington, all
in the Cascade Range ? arise where one of the earth?s immense crust plates is slowly diving beneath another.); Pinatubo?s
last eruption, 600 years ago, is thought to have yielded at least as much molten rock ? half a cubic kilometer ? as Mount
St. Helens did when it erupted in 1980.
[S2] The initial explosions on Mount Pinatubo at 8:51 a.m. Wednesday sent a 10-mile-high mushroom cloud of swirling ash and
rock fragments into the skies over Clark Air Base, forcing the Air Force to evacuate hundreds of American volunteers who had
stayed behind to guard it and to tend sensitive communications equipment.
[S3] Raymundo Punongbayan, director of the Philippine Institute of Vulcanology and Seismology, said Friday?s blasts were part
of a single eruption, the largest since Mount Pinatubo awoke Sunday from its 600-year slumber.
Table 2: General (G) and specific (S) sentences from input d073b
classification is 75%. More accurate predictions are
made on the examples with high annotator agree-
ment reaching over 90% accuracy on sentences
where there was complete agreement between five
annotators. So we expect the predictions from the
classifier to be reliable for analysis in a task setting.
In Table 2, we show the top general and specific
sentences (ranked by the classifier confidence) for
one of the inputs, d073b, from DUC 2002. This in-
put contains articles about the volcanic eruption at
Mount Pinatubo. Here, the specific sentences pro-
vide a lot of details such as the time and impact of
the eruption, information about previous volcanoes
and about the people and organizations involved.
In the next section, we analyze the actual distri-
bution of specific and general content in articles and
their summaries for the entire DUC 2002 dataset.
4 Specificity analysis
For each text?input, human abstract, human extract
and automatic summary?we compute a measure of
specificity as follows. We use the classifier to mark
for each sentence the confidence for belonging to the
specific class. Each token in the text is assigned the
confidence level of the sentence it belongs to. The
average specificity of words is computed as the mean
value of the confidence score over all the tokens.
The histogram of this measure for each type of
text is shown in Figure 1.
For inputs, the average specificity of words ranges
between 50 to 80% with a mean value of 65%. So,
news articles tend to have more specific content than
generic but the distribution is not highly skewed to-
wards either of the extreme ends.
The remaining three graphs in Figure 1 represent
the amount of specific content in summaries for the
same inputs. Human abstracts, in contrast to the in-
puts, are spread over a wider range of specificity lev-
els. Some abstracts have as low as 40% specificity
and a few actually score over 80%. However, the
sharper contrast with inputs comes from the large
number of abstracts that have 40 to 60% specificity.
This trend indicates that abstracts contain more gen-
eral content compared to inputs. An unpaired two-
sided t-test between the specificity values of inputs
and abstracts confirmed that abstracts have signif-
icantly lower specificity. The mean value for ab-
stracts is 62% while for inputs it is 65%.
The results of the analysis are opposite for hu-
man extracts and system summaries. The mean
specificity value for human extracts is 72%, 10%
higher compared to abstractive summaries for the
same inputs. This difference is also statistically sig-
nificant. System-produced summaries also show a
similar trend as extracts but are even more heavily
biased towards specific content. There are even ex-
amples of automatic summaries where the average
specificity level reaches 100%. The mean specificity
value is 74% which turned out significantly higher
than all other types of texts, inputs and both types of
human summaries. So system summaries appear to
be overwhelmingly specific.
The first surprising result is the opposite charac-
teristics of human abstracts and extracts. While ab-
stracts tend to be more general compared to the in-
put texts, extracts are more specific. Even though
36
Figure 1: Specific content in inputs and summaries
both types of summaries were produced by people,
we see that the summarization method deeply influ-
ences the nature of the summary content. The task of
creating extractive summaries biases towards more
specific content. So it is obvious that systems which
mainly use extractive techniques would also create
very specific summaries. Further, since high speci-
ficity arises as a result of the limitations associated
with extractive techniques, perhaps, overly specific
content would be detrimental to summary quality.
We investigate this aspect in the next section.
5 Specificity and summary quality
In this section, we examine if the difference in speci-
ficity that we have observed is related to the per-
ceived quality of the summary. Haghighi and Van-
derwende (2009) report that their topic model based
system was designed to use both a general content
distribution and distributions of content for specific
subtopics. However, using the general distribution
yielded summaries with better content than using the
specific topics. Here we directly study the relation-
ship between specificity of system summaries and
their content and linguistic quality scores. We also
examine how the specificity measure is related to
the quality of specialized summaries where people
were explicitly told to include only general content
or only specific details in their summaries. For this
analysis, we focus on system produced summaries.
5.1 Content quality
At DUC, each summary is evaluated by human
judges for content and linguistic quality. The qual-
ity of content was assessed in 2002 by means of a
coverage score. The coverage score reflects the sim-
ilarity between content chosen in a system summary
and that which is present in a human-written sum-
mary for the same input. A human abstract is cho-
sen as the reference. It is divided into clauses and
for each of these clauses, judges decide how well it
is expressed by the system produced summary (as a
percentage value). The average extent to which the
system summary expresses the clauses of the human
summary is considered as the coverage score. So
these scores range between 0 and 1.
We computed the Pearson correlation between the
specificity of a summary and its coverage score, and
obtained a value of -0.16. The correlation is not very
high but it is significant (pvalue 0.0006). So speci-
ficity does impact content quality and more specific
content indicates decreased quality.
We have seen from our analysis in the previous
section that when people produce abstracts, they
keep a mix of general and specific content but the
abstracts are neither too general nor too specific. So
it is not surprising that the correlation value is not
very high. Further, it should be remembered that the
notion of general and specific is more or less inde-
pendent of the importance of the content itself. Two
summaries can have the same level of generality but
vary greatly in terms of the importance of the con-
tent present. So we performed an analysis to check
the contribution of generality to the content scores
in addition to the importance factor.
We combine a measure of content importance
37
Predictor Mean ? Stdev. ? t value p-value
(Intercept) 0.212 0.03 6.87 2.3e-11 *
rouge2 1.299 0.11 11.74 < 2e-16 *
avgspec -0.166 0.04 -4.21 3.1e-05 *
Table 3: Results from regression test
from the ROUGE automatic evaluation (Lin and
Hovy, 2003; Lin, 2004) with generality to predict
the coverage scores. We use the same reference as
used for the official coverage score evaluation and
compute ROUGE-2 which is the recall of bigrams of
the human summary by the system summary. Next
we train a regression model on our data using the
ROUGE-2 score and specificity as predictors of the
coverage score. We then inspected the weights learnt
in the regression model to identify the influence of
the predictors. Table 3 shows the mean values and
standard deviation of the beta coefficients. We also
report the results from a test to determine if the beta
coefficient for a particular predictor could be set to
zero. The p-value for rejection of this hypothesis
is shown in the last column and the test statistic is
shown as the ?t value?. We used the lm function in
the R toolkit1 to perform the regression.
From the table, we see that both ROUGE-2 and
average specificity of words (avgspec) turn out as
significant predictors of summary quality. Relevant
content is highly important as shown by the positive
beta coefficient for ROUGE-2. At the same time, it
is preferable to maintain low specificity, a negative
value is assigned to the coefficient for this predictor.
So too much specificity should be avoided by sys-
tems and we must find ways to increase the general-
ity of summaries. We discuss this aspect in Sections
6 and 7.
5.2 Linguistic quality
We have seen from the above results that maintain-
ing a good level of generality improves content qual-
ity. A related question is the influence of specificity
on the linguistic quality of a summary. Does the
amount of general and specific content have any re-
lationship with how clear a summary is to read? We
briefly examine this aspect here.
In DUC 2002 linguistic quality scores were only
mentioned as the number of errors in a summary,
not a holistic score. Moreover, it was specified as
1http://www.r-project.org/
ling score sums. avg specificity
1, 2 202 0.71
5 400 0.72
9, 10 79 0.77
Table 4: Number of summaries at extreme levels of lin-
guistic quality scores and their average specificity values
a range?errors between 1 and 5 receive the same
score. So we use another dataset for this analy-
sis only. We use the system summaries and their
linguistic quality scores from the TAC ?09 query
focused summarization task2. Each summary was
manually judged by NIST assessors and assigned a
score between 1 to 10 to reflect how clear it is to
read. The score combines multiple aspects of lin-
guistic quality such as clarity of references, amount
of redundancy, grammaticality and coherence.
Since these scores are on an integer scale, we do
not compute correlations. Rather we study the speci-
ficity, computed in the same manner as described
previously, of summaries at different score levels.
Here there were 44 inputs and 55 systems. In Table
4, we show the number of summaries and their av-
erage specificity for 3 representative score levels?
best quality (9 or 10), worst (1 or 2) and mediocre
(5). We only used summaries with more than 2 sen-
tences as it may not be reasonable to compare the
linguistic quality of summaries of very short lengths.
From this table, we see that the summaries with
greater score have a higher level of specificity. The
specificity of the best summaries (9, 10) are signifi-
cantly higher than that with medium and low scores
(two-sided t-test). This result is opposite to our find-
ing with content quality and calls attention to an im-
portant point. General sentences cannot stand alone
and need adequate support and details. But cur-
rently, very few systems even make an attempt to
organize their summaries. So overly general con-
tent and general content without proper context can
be detrimental to the linguistic quality. Such sum-
maries can appear uncontentful and difficult to read
as the example in Table 5 demonstrates. This sum-
mary has an average specificity of 0.45 and its lin-
guistic quality score is 1.
So we see an effect of specificity on both content
2http://www.nist.gov/tac/2009/
Summarization/update.summ.09.guidelines.
html
38
?We are quite a ways from that, actually.?
As ice and snow at the poles melt, the loss of their reflective surfaces leads to exposed land and water absorbing more heat.
It is in the middle of an area whose population?and electricity demands?are growing.
It was from that municipal utility framework, city and school officials say, that the dormitory project took root.
?We could offer such a plan in Houston next year if we find customer demand, but we have n?t gone to the expense of marketing the plan.?
?We get no answers.?
Table 5: Example general summary with poor linguistic quality
and linguistic quality though in opposite directions.
5.3 Quality of general and specific summaries
So far, we examined the effect of specificity on the
quality of generic summaries. Now, we examine
whether this aspect is related to the quality of sum-
maries when they are optimized to be either gen-
eral or specific content. We perform this analysis
on DUC 20053 data where the task was to create a
general summary for certain inputs. For others, a
specific summary giving details should be produced.
The definitions of a general and specific summary
are given in the task guidelines.4
We tested whether the degree of specificity is re-
lated to the content scores5 of system summaries of
these two types?general and specific. The Pearson
correlation values are shown in Table 6. Here we
find that for specific summaries, the level of speci-
ficity is significantly positively correlated with con-
tent scores. For the general summaries there is no
relationship between specificity and content quality.
These results show that specificity scores are
not consistently predictive of distinctions within the
same class of summaries. Within general sum-
maries, the level of generality does not influence the
scores obtained by them. This finding again high-
lights the disparity between content relevance and
specific nature. When all summaries are specific or
general, their levels of specificity are no longer in-
dicative of quality. We also computed the regres-
sion models for these two sets of summaries with
ROUGE scores and specificity, and specificity level
was not a significant predictor of content scores.
Our findings in this section confirm that general
sentences are useful content for summaries. So we
3http://duc.nist.gov/duc2005/
4http://duc.nist.gov/duc2005/assessor.
summarization.instructions.pdf
5We use the official scores computed using the Pyramid
evaluation method (Nenkova et al, 2007)
Summaries correlation p-value
DUC 2005 general -0.03 0.53
DUC 2005 specific 0.18* 0.004
Table 6: Correlations between content scores and speci-
ficity for general and specific summaries in DUC 2005
face the issue of creating general sentences which
are summary-worthy. We concentrate on this aspect
for the rest of this paper. In Section 6, we pro-
vide an analysis of the types of general sentences
extracted from the source text and used in human
extracts. We move from this limited view and exam-
ine in Section 7, the possibility of generating general
sentences from specific sentences in the source text.
Our analysis is preliminary but we hope that it will
initiate this new task of using general sentences for
summary creation.
6 Extraction of general sentences
We examine general sentences that were chosen in
human extracts to understand what properties sys-
tems could use to identify such sentences from the
source text. We show in Table 7, the ten extract sen-
tences that were predicted to be general with highest
confidence. The first sentence has a 0.96 confidence
level, the last sentence has 0.81.
These statements definitely create expectation and
need further details to be included. Taken out of con-
text, these sentences do not appear very contentful.
However despite the length restriction while creat-
ing summaries, humans tend to include these gen-
eral sentences. Table 8 shows the full extract which
contains one of the general sentences ([9] ?Instead it
sank like the Bismarck.?).
When considered in the context of the extract, we
see clearly the role of this general sentence. It intro-
duces the topic of opposition to Bush?s nomination
for a defense secretary. Moreover, it provides a com-
parison between the ease with which such a propo-
sition could have been accepted and the strikingly
39
opposite situation that arose?the overwhelming re-
jection of the candidate by the senate. So sentence
[9] plays the role of a topic sentence. It conveys the
main point the author wishes to make in the sum-
mary and further details follow this sentence.
But given current content selection methods, such
sentences would rank very low for inclusion into
summaries. So the prediction of general sentences
could prove a valuable task enabling systems to se-
lect good topic sentences for their summaries. How-
ever, proper ordering of sentences will be necessary
to convey the right impact but this approach could
be a first step towards creating summaries that have
an overall theme rather than just the selection of sen-
tences with important content.
We also noticed some other patterns in the general
sentences chosen for extracts. A crude categoriza-
tion was performed on the 75 sentences predicted
with confidence above 0.65 and are shown below:
first sentence : 6 (0.08)
last sentence : 13 (0.17)
comparisons : 4 (0.05)
attributions : 14 (0.18)
A significant fraction of these general sentences
(25%) were used in the extracts to start and end
the summary, likely positions for topic sentences.
Some of these (5%) involve comparisons. We de-
tected these sentences by looking for the presence
of connectives such as ?but?, ?however? and ?al-
though?. The most overwhelming pattern is pres-
ence of quotations, covering 18% of the sentences
we examined. These quotations were identified us-
ing the words ?say?, ?says?, ?said? and the presence
of quotes. We can also see that three of the top 10
general sentences in Table 7 are quotes.
So far we have analyzed sentences chosen by
summary authors directly from the input articles.
In the next section, we analyze the edit operations
made by people while creating abstractive sum-
maries. Our focus is on the generalization operation
where specific sentences are made general. Such
a transformation would be the generation-based ap-
proach to obtain general sentences.
7 Generation of general sentences
We perform our analysis on data created for sen-
tence compression. In this line of work (Knight and
[1] Folksy was an understatement.
[2] ?Long live democracy?!
[3] The dogs are frequent winners in best of breed and
best of show categories.
[4] Go to court.
[5] Tajikistan was hit most hard.
[6] Some critics have said the 16-inch guns are outmoded
and dangerous.
[7] Details of Maxwell?s death are sketchy.
[8] ?Several thousands of people who were in the shelters
and the tens of thousands of people who evacuated inland
were potential victims of injury and death?.
[9] Instead it sank like the Bismarck.
[10] ?The buildings that collapsed did so because of a
combination of two things: very poor soil and very poor
structural design,? said Peter I. Yanev, chairman of EQE
Inc., a structural engineering firm in San Francisco.
Table 7: Example general sentences in humans extracts
Marcu, 2002; McDonald, 2006; Galley and McKe-
own, 2007), compressions are learnt by analyzing
pairs of sentences, one from the source text, the
other from human-written abstracts such that they
both have the same content. We use the sentence
pairs available in the Ziff-Davis Tree Alignment
corpus (Galley and McKeown, 2007). These sen-
tences come from the Ziff-Davis Corpus (Harman
and Liberman, 1993) which contains articles about
technology products. Each article is also associated
with an abstract. The alignment pairs are produced
by allowing a limited number of edit operations to
match a source sentence to one in the abstract. In
this corpus, alignments are kept between pairs that
have any number of deletions and upto 7 substitu-
tions. There are 15964 such pairs in this data. It is
worth noting that these limited alignments only map
25% of the abstract sentences, so they do not cover
all the cases. Still, an analysis on this data could be
beneficial to observe the trends.
We ran the classifier individually on each source
sentence and abstract sentence in this corpus. Then
we counted the number of pairs which undergo each
transformation such as general-general, general-
specific from the source to an abstract sentence.
These results are reported in Table 9. The table also
provides the average number of deletion and substi-
tution operations associated with sentence pairs in
that category as well as the length of the uncom-
pressed sentence and the compression rate. Com-
pression rate is defined as the ratio between the
40
Summary d118i-f:
- President-elect Bush designated Tower as his defense secretary on Dec. 16. [Specific]
- Tower?s qualifications for the job ?intelligence, patriotism and past chairmanship of the Armed Services Committee ?the nomination
should have sailed through with flying colors. [Specific]
- Instead it sank like the Bismarck. [General]
- In written testimony to the Senate panel on Jan. 26, Tower said he could ?recall no actions in connection with any defense activities?
in connection with his work for the U.S. subsidiary. [Specific]
- Tower has acknowledged that he drank excessively in the 1970s, but says he has reduced his intake to wine with dinner. [General]
- The Democratic-controlled Senate today rejected the nomination of former Texas Sen. John Tower as defense secretary, delivering
a major rebuke to President Bush just 49 days into his term.[Specific]
- The Senate?s 53-47 vote came after a bitter and divisive debate focused on Tower?s drinking habits, behavior toward women and his
business dealings with defense contractors. [General]
Table 8: Example extract with classifier predictions and a general sentence from Table 7
Type Total % total Avg deletions Avg subs. Orig length Compr. rate
SS 6371 39.9 16.3 3.9 33.4 56.6
SG 5679 35.6 21.4 3.7 33.5 40.8
GG 3562 22.3 9.3 3.3 21.5 60.8
GS 352 2.2 8.4 4.0 22.7 66.0
Table 9: Types of transformation of source into abstract sentences
length in words of the compressed sentence and the
length of the uncompressed sentence. So lower com-
pression rates indicate greater compression.
We find that the most frequent transformations are
specific-specific (SS) and specific-general (SG). To-
gether they constitute 75% of all transformations.
But for our analysis, the SG transformation is most
interesting. One third of the sentences in this data
are converted from originally specific content to be-
ing general in the abstracts. So abstracts do tend to
involve a lot of generalization.
Studying the SG transition in more detail, we can
see that the original sentences are much longer com-
pared to other transitions. This situation arises from
the fact that specific sentences in this corpus are
longer. In terms of the number of deletions, we see
that both SS and SG involve more than 15 deletions,
much higher than that performed on the general sen-
tences. However, we do not know if these operations
are proportional to the original length of the sen-
tences. But looking at the compression rates, we get
a clearer picture, the SG sentences after compres-
sion are only 40% their original length, the maxi-
mum compression seen for the transformation types.
For GG and GS, about 60% of the original sentence
words are kept. For the SG transition, long sentences
are chosen and are compressed aggressively. In Ta-
ble 10, we show some example sentence pairs un-
dergoing the SG transition.
Currently, compression systems do not achieve
the level of compression in human abstracts. Sen-
tences that humans create are shorter than what sys-
tems produce. Our results predict that these could be
the cases where specific sentences get converted into
general. One reason why systems do not attain this
compression level could be because they only con-
sider a limited set of factors while compressing, such
as importance and grammaticality. We believe that
generality can be an additional objective which can
be used to produce even shorter sentences which we
have seen in our work, will also lead to summaries
with better content.
8 Conclusion
In this work, we have provided the first quantitative
analysis of general and specific content as relevant
to the task of automatic summarization. We find
that general content is useful for summaries how-
ever, current content selection methods appear to not
include much general content. So we have proposed
the task of identifying general content which could
be used in summaries. There are two ways of achiev-
ing this?by identifying relevant general sentences
from the input and by conversion from specific to
41
[1] American Mitac offers free technical support for one year at a toll-free number from 7:30 to 5:30 P.S.T.
American Mitac offers toll-free technical support for one year.
[2] In addition to Yurman, several other government officials have served on the steering committee that formed the group.
Several government officials also served on the steering committee.
[3] All version of the new tape drives, which, according to Goldbach, offer the lowest cost per megabyte for HSC-based 8mm tape
storage, are available within 30 days of order.
The products are available within 30 days of order.
[4] In a different vein is Edward Tufte ?s ?The Visual Display of Quantitative Information? (Graphics Press, 1983), a book covering
the theory and practice of designing statistical charts, maps, tables and graphics.
Tufte ?s book covers the theory and practice of designing statistical charts, maps, tables and graphics.
[5] In addition, Anderson said two Ada 9X competitive procurements?a mapping and revision contract and an implementation and
demonstration contract?will be awarded in fiscal 1990.
Two competitive procurements will be awarded in fiscal 1989.
Table 10: Example specific to general (in italics) compressions
general content. We have provided a brief overview
of these two approaches.
Our work underscores the importance of com-
pression and other post-processing approaches over
extractive summaries. Otherwise system content
could contain too much extraneous details which
take up space where other useful content could have
been discussed.
Our study also highlights a semantic view of sum-
mary creation. Summaries are not just a bag of im-
portant sentences as viewed by most methods today.
Rather a text should have a balance between sen-
tences which introduce a topic and those which dis-
cuss them in detail. So another approach to content
selection could be the joint selection of a general
sentence with its substantiation. In future work, it
would be interesting to observe if such summaries
are judged more responsive and of better linguistic
quality than summaries which do not have such a
structure.
References
R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3).
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
31(1):399?429.
K. Filippova and M. Strube. 2008. Sentence fusion
via dependency graph compression. In Proceedings
of EMNLP, pages 177?185.
M. Galley and K. McKeown. 2007. Lexicalized markov
grammars for sentence compression. In Proceedings
NAACL-HLT.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of NAACL-HLT, pages 362?370.
D. Harman and M. Liberman. 1993. Tipster complete.
Corpus number LDC93T3A, Linguistic Data Consor-
tium, Philadelphia.
H. Jing and K. McKeown. 2000. Cut and paste based
text summarization. In Proceedings of NAACL.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sen-
tence compression. Artificial Intelligence, 139(1).
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of HLT-NAACL.
C. Lin. 2004. ROUGE: a package for automatic evalua-
tion of summaries. In ACL Text Summarization Work-
shop.
A. Louis and A. Nenkova. 2011. General versus spe-
cific sentences: automatic identification and applica-
tion to analysis of news summaries. Technical Re-
port No. MS-CIS-11-07, University of Pennsylvania
Department of Computer and Information Science.
E. Marsi, E. Krahmer, I. Hendrickx, and W. Daelemans.
2010. On the limits of sentence compression by dele-
tion. In E. Krahmer and M. Theune, editors, Empirical
methods in natural language generation, pages 45?66.
Springer-Verlag, Berlin, Heidelberg.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic evidence. In EACL?06.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The pyramid method: Incorporating human content se-
lection variation in summarization evaluation. ACM
Trans. Speech Lang. Process., 4(2):4.
S. Wan, R. Dale, M. Dras, and C. Paris. 2008. Seed
and grow: augmenting statistically generated sum-
mary sentences using schematic word patterns. In Pro-
ceedings of EMNLP, pages 543?552.
42
