Chinese Word Segmentation in FTRD Beijing 
Heng LI 
France Telecom R&D Bei-
jing 
heng.li@francetelec
om.com
Yuan DONG 
France Telecom R&D Beijing
yuan.dong@francetele
com.com
Xinnian MAO 
France Telecom R&D Bei-
jing
xin-
nian.mao@francetelec
om.com
Haila WANG 
France Telecom R&D Bei-
jing  
haila.wang@francete
lecom.com
Wu LIU 
Beijing University of Posts 
and Telecommunications  
wu.liu@francetelecom
.com.cn
Abstract
This paper presents a word segmenta-
tion system in France Telecom R&D 
Beijing, which uses a unified approach 
to word breaking and OOV identifica-
tion. The output can be customized to 
meet different segmentation standards 
through the application of an ordered 
list of transformation. The system par-
ticipated in all the tracks of the seg-
mentation bakeoff -- PK-open, PK-
closed, AS-open, AS-closed, HK-open, 
HK-closed, MSR-open and MSR-
closed -- and achieved the state-of-the-
art performance in MSR-open, MSR-
close and PK-open tracks. Analysis of 
the results shows that each component 
of the system contributed to the scores. 
1 Introduction
The development of the Chinese word segmen-
tation system presented in this bakeoff began in 
Feb. this year, and will last for one year with the 
support of the ILAB Beijing initial project 
within France Telecom R&D.  
Although the project last only half year by 
now, the main components of the system has 
been implemented, including code identification 
and conversion, basic segmentation, factoid de-
tection, morphological analysis, name entity 
identification, segmentation standards adaptor, 
except the components of code identification 
and conversion and segmentation standards 
adaptors, other components are integrated in a 
statistical framework of n-gram language model. 
2 System Description 
2.1 Code identification and conversion 
For processing both Simplified and Traditional 
Chinese text from a variety of locales, including 
Mainland China, Hong Kong and Taiwan, we 
choose UTF-8 as internal character representa-
tion within the system. The ability to transpar-
ently handle Chinese text from any Chinese 
locale greatly simplifies the logic of the segmen-
tation system. 
2.2 N-gram language model  
In our system, Chinese words can be categorized 
into one of the following types: lexicon words, 
morphological words, factoids, name entities. 
These types of words are processed in different 
ways in our system, and are incorporated into a 
unified statistical framework of the trigram lan-
guage model. 
2.2.1 Basic segmentation 
Each input sentence is first segmented into indi-
vidual characters. These characters and the char-
acter strings are then looked up in a lexicon. For 
the efficient search, the lexicon is represented by 
a TRIE compressed in a double-array data struc-
150
ture. Given a character string, all its prefix 
strings that form lexicon words can be retrieved 
efficiently by browsing the TRIE whose root 
represents its first character.  
2.2.2 Factoid detection 
There are twenty four kinds of factoid words, 
such as time, date, money, etc. All the factoid 
words are represented as regular expressions, 
and compiled into a compressed DFA with the 
row-index algorithm. 
2.2.3 Morphological analysis 
As (Wu 2003) discussed in the paper, it is those 
morphologically derived words (MDWs hereaf-
ter) that are most controversial and most likely 
to be treated differently in different standards 
and different systems. In our system, there are 
six main categories of morphological processes, 
affixation, directional verb, resultative verb, 
splitting verb, reduplication and merging, and 
we employ a chart parsing algorithm augmented 
with word lattices structure which incorporates 
the morphological rules especially designed for 
Chinese languages with restrictive CFG.  
2.2.4 Name entity identification  
Our NE identification concentrates on three 
types of NEs, namely, personal names (PERs), 
location names (LOCs) and organization names 
(ORGs). For Chinese person names, we only 
consider PN candidates that begin with a family 
name stored in the family name list and follow a 
given name which is of one or two characters 
long. For transliterations of foreign person 
names, a PN candidate would be generated if it 
contains only characters stored in a transliterated 
character list. For location names and organiza-
tions names, we only use the LN list and ON list 
to generate the candidates. 
2.3 Segmentation standards adaptor 
In this bakeoff, there are four segmentation 
standards and slightly different from ours. Stan-
dard adaptation is conducted with the applica-
tion of an ordered list of transformations on the 
output of our segmentation system. The method 
we use is Transformation-Based Learning, and 
the transformation templates are lexicalized 
templates. In our system, we designed 14 lexi-
calized templates. 
2.4 Speed 
As we optimized our lexicon and decoding 
process, the speed of segmentation is very fast. 
On a single 2.80 GHz, 1G bytes memory, Xeon 
machine, the system is able to process about 
0.73 Mega bytes per second. 
The speed may vary according to the sen-
tence lengths: given texts of the same size, those 
containing longer sentences will take more time. 
The number reported here is an average of the 
time taken to process the test sets of the eight 
tracks we participated in. 
3 Evaluation 
3.1 Open tracks 
In the open tracks, we used four lexicons of 
210,319 entries, 165,103 entries, 174,268 entries, 
165,655 entries respectively on AS-open, HK-
open, MSR-open, PK-open tracks, which in-
clude the entries of 2,430 MDWs, 12,487 PNs, 
22,907 LNs and 29,032 ONs, 10,414 four-
character idioms, plus the word lists generated 
from the training data provided by the bakeoff. 
We use the training data provided by the bakeoff 
for training our trigram word-based language 
model. We also used a family name list (which 
contains 399 entries in our system), and a 1,021-
entry transliterated name character list. 
3.2 Closed tracks 
In the close tracks, the lexicon we use could 
only be generated from the training data pro-
vided by the bakeoff. We could only use the 
training data provided by the bakeoff for train-
ing our word-based language model. Also, since 
the training data we used is only from the bake-
off, there does not exist any different standards, 
standards adaptor component is not necessarily 
needed.  
3.3 Result analysis 
Our system is designed so that components such 
as the factoid detection and NE identification 
can be switched on or off, so that we can inves-
tigate the relative contribution of each compo-
nent to the overall word segmentation 
performance. The results are summarized in the 
table 1. For comparison, we also include in the 
table (Row 1) the results of using FMM. Row 2 
shows the baseline results of our system, where 
only the lexicon is used. Each cell in the table 
has six fields. From the top, there are respec-
tively Precision, Recall, F-measure, OOV Recall, 
IV Recall and Speed (Mega bytes/second). We 
don't list the speed in Row 6 since it decreases a 
factor of 10 to 60 because of application of 
thousands of TBL rules.  
151
PKo PKc MSRo MSRc ASo ASc HKo HKc
1. FMM 
0.857 
0.925 
0.891 
0.143 
0.947 
2.435 
0.841 
0.906 
0.872 
0.069 
0.957 
2.570 
0.921 
0.968 
0.945 
0.107 
0.971 
2.951 
0.917 
0.957 
0.936 
0.025 
0.982 
3.090 
0.871
0.925
0.898
0.097
0.947
2.813
0.864 
0.911 
0.887 
0.014 
0.952 
2.937 
0.842 
0.928 
0.885 
0.175 
0.961 
2.748 
0.838 
0.908 
0.872 
0.162 
0.968 
2.850 
2. Baseline 
0.869 
0.941 
0.905 
0.235 
0.960 
0.967 
0.855 
0.928 
0.890 
0.069 
0.987 
1.017 
0.931 
0.973 
0.952 
0.275 
0.987 
0.879 
0.926 
0.969 
0.947 
0.025 
0.995 
0.923 
0.891
0.943
0.917
0.132
0.982
0.703
0.877 
0.942 
0.908 
0.014 
0.984 
0.728 
0.863 
0.930 
0.897 
0.194 
0.985 
0.921 
0.851 
0.929 
0.888 
0.162 
0.990 
0.956 
3. 2+FT 
0.946 
0.951 
0.948 
0.748 
0.963 
0.819 
0.919 
0.950 
0.934 
0.448 
0.980 
0.879 
0.950 
0.973 
0.961 
0.396 
0.990 
0.779 
0.940 
0.973 
0.956 
0.205 
0.944 
0.787 
0.903
0.945
0.924
0.180
0.979
0.631
0.900 
0.947 
0.923 
0.156 
0.983 
0.635 
0.873 
0.932 
0.902 
0.292 
0.983 
0.821 
0.862 
0.932 
0.895 
0.215 
0.989 
0.830 
4. 3+MA 
0.946 
0.951 
0.948 
0.748 
0.963 
0.807 
0.919 
0.950 
0.934 
0.448 
0.980 
0.879 
0.950 
0.973 
0.961 
0.371 
0.989 
0.753 
0.940 
0.973 
0.956 
0.205 
0.944 
0.787 
0.903
0.945
0.924
0.181
0.979
0.626
0.900 
0.947 
0.923 
0.156 
0.983 
0.635 
0.873 
0.932 
0.902 
0.295 
0.983 
0.815 
0.862 
0.932 
0.895 
0.215 
0.989 
0.830 
5. 4+NE 
0.951 
0.957 
0.954 
0.788 
0.967 
0.679 
0.919 
0.950 
0.934 
0.448 
0.980 
0.879 
0.956 
0.973 
0.965 
0.454 
0.956 
0.716 
0.940 
0.973 
0.956 
0.205 
0.944 
0.787 
0.920
0.949
0.934
0.330
0.977
0.604
0.900 
0.947 
0.923 
0.156 
0.983 
0.635 
0.900 
0.938 
0.918 
0.411 
0.980 
0.748 
0.862 
0.932 
0.895 
0.215 
0.989 
0.830 
6. 5+adaptation 
0.960 
0.964 
0.962 
0.788 
0.974 
0.919 
0.950 
0.934 
0.449 
0.980 
0.957 
0.975 
0.966 
0.453 
0.989 
0.940 
0.974 
0.957 
0.210 
0.995 
0.919
0.952
0.935
0.311
0.981
0.900 
0.948 
0.923 
0.158 
0.983 
0.901 
0.940 
0.920 
0.410 
0.982 
0.862 
0.932 
0.895 
0.215 
0.989 
Table 1. Our system results on all the tracks. 
From Table 1 we can find that, in rows 1 and 2, 
the dictionary-based methods already achieve 
quite good recall, but the precisions are not very 
good because they cannot correctly identify un-
known words that are not in the lexicon such as 
factoids and name entities. We also find that 
even using the same lexicon, our approach that 
is based on the N-gram language models outper-
forms the greedy approach because the use of 
context model resolves more ambiguities in 
segmentation. As shown in Rows 3 to 5, when 
components are switched on in turn, the overall 
word segmentation performance increases con-
sistently. The morphological analysis has no 
contribution to the overall performance in Row 
4. The main reason is that the number of MDWs 
used in our system is very small (only 2,430) 
and there may exist very small MDWs in the test 
sets. The similar cases occur on NE identifica-
tion in the close tracks in Row 5 since we would 
not do NE identification at all in the close tracks. 
We also notice that the contribution of NE iden-
tification is very little in the open tracks, which 
shows that the performance of NE identification 
is not very good in our system, and explains 
why our OOV recall is not very high compared 
152
with other participants in the bakeoff. This is 
one area of our future work to improve. The re-
sults of standards adaptation on four bakeoff test 
sets are shown in Row 6. It turns out that per-
formance except IV recall improves slightly 
across the board in all four test sets. The main 
reason is that the training data and lexicon we 
used are mainly from the four providers in the 
bakeoff, there does not exist any different seg-
mentation standards.  
4 Conclusions 
The evaluation results show that the closed tests 
is not very good compared with other partici-
pants, the one main reason is that the word-
based language model we used is not competi-
tive compared with other algorithms in the 
closed tracks. One area of our future work is to 
apply other machine learning algorithm, like 
Maximum Entropy (ME), Support Vector Ma-
chine (SVM), Conditional Random Field (CRF), 
etc. 
Acknowledgements 
The work reported here was a team effort. We 
thank Wu Liu, Haitao Zeng, Nan He for their 
help in the experimentation and evaluation of 
the system. 
References 
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in Chinese. Interna-
tional Journal of Computational Linguistics and 
Chinese Language Processing, 8(1): 1-27. 
Aoe, J. 1989. An Efficient Digital Search Algorithm 
by Using a Double-Array Structure. IEEE Trans-
actions on Software Engineering, Vol. 15, 9: 
1066-1077. 
George Anton Kiraz. 1999. Compressed Storage of 
Sparse Finite-State Transducers. 4th International 
Workshop on Automata Implementation, Pages: 
109-121. 
Jian Sun, Ming Zhou and Jianfeng Gao. 2003. Chi-
nese named entity identification using class-based 
language model. International Journal of Compu-
tational Linguistics and Chinese Language Proc-
essing, 8(1). 
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang. 2004a. Chinese word segmentation: a 
pragmatic approach. Microsoft Research Techni-
cal Report, MSR-TR-2004-123. 
Julia Hockenmaier, Chris Brew. 1998. Error driven 
segmentation of Chinese. Communications of 
COLIPS, 8(1): 69-84. 
Xinnian Mao, Heng Li, Yuan Dong, Haila Wang. 
2005. Chinese Morphological Analyzer. IEEE
NLP-KE 2005, submitted. 
153
Chinese Word Segmentation and Named Entity Recognition Based on 
Conditional Random Fields 
Xinnian Mao 
France Telecom R&D Center (Beijing), Bei-
jing, 100080, P.R.China 
xinnian.mao@orange-
ftgroup.com 
Saike He 
University of Posts and Telecommunications, 
Beijing, 100876, P.R.China 
  
Sencheng Bao 
University of Posts and Telecommunications, 
Beijing, 100876, P.R.China  
Yuan Dong1,2 
1France Telecom R&D Center (Beijing), 
Beijing, 100080, P.R.China 
2University of Posts and Telecommunica-
tions, Beijing, 100876, P.R.China 
yuan.dong@orange-
ftgroup.com 
Haila Wang 
France Telecom R&D Center (Beijing), Bei-
jing, 100080, P.R.China 
haila.wang@orange-
ftgroup.com  
 
Abstract 
Chinese word segmentation (CWS), named 
entity recognition (NER) and part-of-
speech tagging is the lexical processing in 
Chinese language. This paper describes the 
work on these tasks done by France Tele-
com Team (Beijing) at the fourth Interna-
tional Chinese Language Processing Bake-
off. In particular, we employ Conditional 
Random Fields with different features for 
these tasks. In order to improve NER rela-
tively low recall; we exploit non-local fea-
tures and alleviate class imbalanced distri-
bution on NER dataset to enhance the re-
call and keep its relatively high precision. 
Some other post-processing measures such 
as consistency checking and transforma-
tion-based error-driven learning are used to 
improve word segmentation performance. 
Our systems participated in most CWS and 
POS tagging evaluations and all the NER 
tracks. As a result, our NER system 
achieves the first ranks on MSRA open 
track and MSRA/CityU closed track. Our 
CWS system achieves the first rank on 
CityU open track, which means that our 
systems achieve state-of-the-art perform-
ance on Chinese lexical processing. 
1 Introduction 
Different from most European languages, there is 
no space to mark word boundary between Chinese 
characters, so Chinese word segmentation (CWS) 
is the first step for Chinese language processing. 
From another point that there is no capitalization 
information to indicate entity boundary, which 
makes Chinese named entity recognition (NER) 
more difficult than European languages. And part-
of-speech tagging (POS tagging) provides valuable 
information for deep language processing such as 
parsing, semantic role labeling and etc. This paper 
presents recent research progress on CWS, NER 
and POS tagging done by France Telecom Team 
(Beijing). Recently, Conditional Random Fields1 
(CRFs) (Lafferty et al, 2001) have been success-
fully employed in various natural language proc-
essing tasks and achieve the state-of-the-art per-
formance, in our system, we use it as the basic 
framework and incorporate some other post-
processing measures for CWS, NER and POS tag-
ging tasks.  
2 Chinese Named Entity Recognition 
NER is always limited by its lower recall due to 
the imbalanced distribution where the NONE class 
dominates the entity classes. Classifiers built on 
such dataset typically have a higher precision and a 
lower recall and tend to overproduce the NONE 
                                                 
1 We use the CRF++ V4.5 software from 
http://chasen.org/~taku/software/CRF++/ 
90
Sixth SIGHAN Workshop on Chinese Language Processing
class (Kambhatla, 2006). Taking SIGHAN Bakeoff 
2006 (Levow, 2006) as an example, the recall is 
lower about 5% than the precision for each submit-
ted system on MSRA and CityU closed track. If we 
could improve NER recall but keep its relatively 
high precision, the overall F-measure will be im-
proved as a result. We design two kinds of effec-
tive features: 0/1 features and non-local features to 
achieve this objective. Our final systems utilize 
these features together with the local features to 
perform NER task. 
2.1 Local Features 
The local features are character-based and are in-
stantiated from the following temples: 
Unigram: Cn (n=-2,-1, 0, 1, 2). 
Bigram: CnCn+1 (n=-2,-1, 0, 1) and C-1C1. 
Where C0 is the current character, C1 the next 
character, C2 the second character after C0, C-1 the 
character preceding C0, and C-2 the second charac-
ter before C0.  
2.2 0/1 Features 
In order to alleviate the imbalanced class distribu-
tion, we assign 1 to all the characters which are 
labeled as entity and 0 to all the characters which 
are labeled as NONE in training data. In such way, 
the class distribution can be alleviated greatly, tak-
ing Bakeoff 2006 MSRA NER training data for 
example, if we label the corpus with 10 classes, the 
class distribution is 0.81(B-PER):1.70(B-LOC):0.95(B-
ORG):0.81(I-PER):0.88(I-LOC):2.87(I-ORG):0.76(E-
PER):1.42(E-LOC):0.94(E-ORG):88.86(NONE), if we 
change the label scheme to 2 labels (0/1), the class 
distribution is 11.14 (entity):88.86(NONE). We 
train the 0/1 CRFs tagger using the local features 
alone. For the 0/1 features, during the training 
stage, they are assigned with 2-fold cross valida-
tion, and during the testing stage, they are assigned 
with the 0/1 tagger.  
2.3 Non-local Features 
Most empirical approaches including CRFs cur-
rently employed in NER task make decision only 
on local context for extract inference, which is 
based on the data independent assumption. But 
often this assumption does not hold because non-
local dependencies are prevalent in natural lan-
guage (including the NER task). How to utilize the 
non-local dependencies is a key issue in NER task. 
Up to now, few researches have been devoted to 
this issue; existing works mainly focus on using 
the non-local information for improving NER label 
consistency (Krishnan and Manning, 2006). There 
are two methods to use non-local information. One 
is to add additional edges to graphical model struc-
ture to represent the distant dependencies and the 
other is to encode the non-locality with non-local 
features. In the first approach, heuristic rules are 
used to find the dependencies (Bunescu and 
Mooney, 2004) or penalties for label inconsistency 
are required to handset ad-hoc (Finkel et al, 2005). 
Furthermore, high computational cost is spent for 
approximate inference. In order to establish the 
long dependencies easily and overcome the disad-
vantage of the approximate inference, Krishnan 
and Manning (2006) propose a two-stage approach 
using CRFs framework with extract inference. 
They represent the non-locality with non-local fea-
tures, and extract them from the output of the first 
stage CRF with local context alone; then they in-
corporate the non-local features into the second 
CRF. But the features in this approach are only 
used to improve label consistency in European 
languages. Similar with their work encoding the 
non-local information with non-local feature, and 
we also exploit the non-local features under two-
stage architecture. Different from their features are 
activated on the recognized entities coming from 
the first CRF, the non-local features we design are 
used to recall more missed entities which are seen 
in the training data or unseen entities but some of 
their occurrences being recognized correctly in the 
first stage, so our non-local features are activated 
on the raw character sequence.  
Different NER in European languages, where 
entity semantic classification is more difficult 
compared with boundary detection, in Chinese, the 
situation is opposite.  So we encode different use-
ful information for Chinese NER two subtasks: 
entity boundary detection and entity semantic clas-
sification. Three kinds of non-local features are 
designed; they are fired on the token sequences if 
they are matched with certain entity in the entity 
list in forward maximum matching (FMM) way. 
Token-position features (NF1): These refer to 
the position information (start, middle and last) 
assigned to the token sequence which is matched 
with the entity list exactly. These features enable 
us to capture the dependencies between the identi-
cal candidate entities and their boundaries. 
91
Sixth SIGHAN Workshop on Chinese Language Processing
Entity-majority features (NF2): These refer to 
the majority label assigned to the token sequence 
which is matched with the entity list exactly. These 
features enable us to capture the dependencies be-
tween the identical entities and their classes, so 
that the same candidate entities of different occur-
rences can be recalled favorably, and their label 
consistencies can be considered too. 
Token-position & entity-majority features 
(NF3): These features capture non-local informa-
tion from NF1 and NF2 simultaneously. They take 
into account the entity boundary and semantic 
class information at the same time. 
Figure 1 shows the flow of using non-local fea-
tures under CRFs framework in two-stage architec-
ture. The first CRF is trained with local features 
alone, and then we test the testing data with the 
first CRF and get the entities plus their type from 
the output. The second CRF utilizes the 0/1 fea-
tures and the non-local features derived from the 
entity list which is merged by the output of the first 
CRF from the testing data and the entities extracted 
directly from the training data. We compare the 
three kinds of non-local features on MSRA and 
CityU closed track in SIGHAN 2006 and we find 
that the NF3 is the best (Mao etc, 2007). So we 
only incorporate the NF3 into our final NER sys-
tem.  
 
Figure 1. The flow using non-local features 
in two-stage architecture 
2.4 Results 
We employ BIOE1 label scheme for the NER task 
because we found it performs better than IOB2 on 
Bakeoff 2006 (Levow, 2006) NER MSRA and 
CityU corpora. Table 1 presents the official results 
on the MSRA and CityU corpus. The F-measure 
on MSRA open track is so high just because the 
testing data in Bakeoff 2007 is part of its Bakeoff 
2006 training dataset and we utilize this corpus for 
training the final CRFs classifier. The F-measure 
on CityU open track is not much superior to its 
closed track because we only use its Bakeoff 2006 
corpus to train the 0/1 CRFs, but not use the Bake-
off 2006 corpus to train final classifier. 
 
Run ID  F-Score  Run ID  F-Score 
cityu_c  84.99 cityu_o  87.92 
msra_c  92.81 msra_o 99.88 
 Table 1: The official results on NER  
closed(c) tracks and open(o) tracks 
3 Chinese Word Segmentation 
Type Feature 
Unigram Cn (n=-2,-1, 0, 1, 2). 
Bigram CnCn+1 (n=-2,-1,0, 1) 
Jump C-1C1 
Punc Pu (C0) 
Date, Digit, Letter T-1T0T1 
Table 2: The features used in our CWS systems 
 
Table 2 lists the features we used in our CWS sys-
tems. After the raw corpus is processed by CRFs, 
two other post-processing measures are performed. 
We utilize transformation-based error-driven learn-
ing (TBL)2 to further improve CWS and perform 
consistency checking among different occurrences 
of a particular character sequence. For TBL, we 
use the template defined in (He et al). Our CWS 
system participate almost all the tracks and table 3 
lists the official results.  
 
Run ID F-Score Run ID F-Score 
cityu_c_a 94.43 cityu_o_a 96.97 
cityu_c_b error (94.31) cityu_o_b 96.86 
ckip_c_a 93.17 ckip_o_a 93.25 
ckip_c_b 93.06 ckip_o_b 93.64 
ctb_c_a 94.86 ctb_o_a 97.93 
ctb_c_b 94.74 ctb_o_b 97.28 
ncc_c_a 92.99 sxu_c_a 95.46 
ncc_c_b 92.89 sxu_c_b 95.17 
Table 3: The official results on CWS closed(c) 
tracks and open(o) tracks 
 
In the table 3, run (a) means that we only per-
form consistency checking; run (b) means that 
                                                 
2 We use the TBL software from 
http://nlp.cs.jhu.edu/~rflorian/fntbl/index.html 
92
Sixth SIGHAN Workshop on Chinese Language Processing
TBL is performed after consistency checking is 
done. We make a mistake on cityu_c_b because we 
rename cityu_c_a as cityu_c_b, so the two results 
are the same, after we correct the mistake and 
score again; we achieve an F-measure of 94.31%. 
In the closed tracks, we first train initial CRFs 
with 3-fold cross-validation; then we test the train-
ing data (three parts) with the three trained CRFs, 
we train the TBL learner on the training data com-
pared it with the testing result from the initial 
CRFs. The consistency checking is inspired by (Ng 
and Low, 2004). Table 4 lists the corpus used to 
train the CRFs and TBL learner in the open tracks. 
 
 CRFs  TBL  
CityU 2005,2006,2007 2003 
CKIP 2007 2006 
CTB 2006,2007 2007 
Table 4. Corpora used to train the CRFs classi-
fier and the TBL learner 
 
In the open track, we collect the consistency list 
from all its correspondent Bakeoff corpora, the 
gazetteer extract from People Daily 2000 and idi-
oms, slang from GKB. From the table 3 in the 
closed test, we can confirm that TBL may not im-
prove CWS performance, while in most cases, per-
formance will surely draw back. The reason lies in 
the fact that the learning capability of CRFs is su-
perior to that of TBL, if they are trained with the 
same corpus, TBL may modify some correctly tags 
by CRFs. This can be seen from Table 3 that re-
sults without TBL (in run (a)) are almost superior 
to that with TBL (in run (b)).  
4 Part-of-speech Tagging 
For POS tagging task, apart from the local features 
same as used in NER, two other features are de-
signed to improve the performance. 
? Ambiguous part-of-speech: this feature is 
true when the word has more than 2 kinds 
of part-of-speech. 
? Major part-of-speech: The feature is as-
signed as the major part-of-speech for any 
word. We do not assign the value to the 
new words. 
Table 5 shows the performance in the closed 
tracks. Because we only used the simple features 
and do not process the unknown word specially, 
our performance is not satisfactory. 
 
Run ID F-Score Run ID F-Score 
cityu_c 87.93 ctb_c 92.03 
ckip_c 87.93 ncc_c 91.72 
ctb_c 92.03   
Table 5: The official results on POS tagging in 
closed tracks 
References 
R. Bunescu and R. J. Mooney. 2004. Collective Infor-
mation Extraction with Relational Markov Networks. 
In Proceedings of the 42nd ACL, 439?446. 
J. Finkel, T. Grenager, and C. D. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings 
of the 42nd ACL, 363?370. 
Nan He, Xinnian Mao, Yuan Dong, Haila Wang, 2007. 
Transformation-based Error-driven Learning as Post-
processing for Chinese Word Segmentation, In Pro-
ceedings of the 7th International Conference on Chi-
nese Computing, 46-51, Wuhan, China.   
N. Kambhatla. 2006. Minority Vote: At-Least-N Voting 
Improves Recall for Extracting Relations. In Pro-
ceeding of the 44th ACL, 460?466. 
V. Krishnan and C. D Manning. 2006. An Effective 
Two-Stage Model for Exploiting Non-Local Depend-
encies in Named Entity Recognition. In Proceedings 
of the 44th ACL, 1121?1128. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the 18th ICML, 282?289, San Francisco, CA. 
G. Levow. 2006. The Third International Chinese Lan-
guage Processing Bakeoff: Word Segmentation and 
Named Entity Recognition. In Proceedings of 
SIGHAN-2006, 108-117. Sydney, Australia.  
Xinnian Mao, Xu Wei, Yuan Dong, Saike He and Haila 
Wang, 2007. Using Non-local Features to Improve 
Named Entity Recognition Recall, In Proceedings of 
the 21th Pacific Asia Conference on Language, In-
formation and Computation, 303-310, Seoul, Korea. 
Hwee Tou Ng, Jin Kiat Low, 2004. Chinese Part-of-
Speech Tagging: One-at-a-Time or All at Once? 
Word-based or Character based? In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, Spain. 
93
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 122?125,
Sydney, July 2006. c?2006 Association for Computational Linguistics
France Telecom R&D Beijing Word Segmenter  
for Sighan Bakeoff 2006 
 
Wu Liu 
France Telecom R&D  
Beijing 
wu.liu@franceteleco
m.com 
Heng Li 
France Telecom R&D Bei-
jing 
heng.li@francetele
com.com 
Yuan Dong 
Beijing University of  
Posts and Telecommunications 
yuandong@bupt.edu.cn 
Nan He 
Beijing University of Posts 
and Telecommunications 
hn.ft.pris@gmail.co
m 
Haitao Luo 
Northeastern University of 
China 
luoht@ics.neu.edu.
cn 
Haila Wang 
France Telecom R&D Beijing 
haila.wang@francetele
com.com 
 
 
Abstract 
This paper presents two word segmenta-
tion (WS) systems and a named entity 
recognition (NER) system in France 
Telecom R&D Beijing. The one system 
of WS is for open tracks based on n-
gram language model and another one is 
for closed tracks based on maximum en-
tropy approach. The NER system uses a 
hybrid algorithm based on Class-based 
language model and rule-based knowl-
edge. These systems are all augmented 
with a set of post-processors.  
1 Introduction 
The FTRD team participated in MSRA Open, 
MSRA Closed and CityU Closed tracks of the 
WS bakeoff and MSRA Open track of the NER 
bakeoff, and achieved the state-of-the-art per-
formance in these tracks. Analysis of the results 
shows that each component of these systems 
contributed to the scores. 
2 System Description 
2.1 MSRA Open track of WS 
The system used in open track of WS is based on 
the system (Li 2005) participated in the second 
international WS bakeoff. We mainly modify the 
factoid detection rules and add the GKB (The 
Grammatical Knowledge-base of Contemporary 
Chinese) dictionary. The system also has a few 
postprocessors. The main postprocessors include 
named entity recognizers and TBL (Transforma-
tion-Based Learning) component. 
2.1.1 Basic system  
In our basic system, Chinese words can be cate-
gorized into one of the following types: lexicon 
words, morphological words, factoids, name en-
tities. These types of words were processed in 
different ways in our system, and were incorpo-
rated into a unified statistical framework of the 
trigram language model. The details about the 
basic system are reported in (Li 2005). 
2.1.2 Factoid detection 
The factoid rules used in the basic system were 
summarized according to the MSRA training 
data. The Tokenization Guidelines of Chinese 
Text (V5.0) was provided by MSRA in this 
bakeoff. We used the Guidelines to rewrite the 
factoid rules, and the performance had the dis-
tinct improvement.  
122
2.1.3 Named entity identification  
The named entity recognizer is the one partici-
pated in the NER bakeoff, as shown in figure 1. 
In the section 2.3, we will describe in detail. 
2.2 System Used in Close tracks 
The system used in closed tracks of WS is based 
on maximum entropy approach. The system also 
has a few postprocessors. The main postproces-
sors include combining the separated words and 
TBL component. 
2.2.1 Basic system 
The basic system is similar to (Ng and Low, 
2004). We used the Tsujii laboratory maximum 
entropy package v2.0 (http://www-tsujii.is.s.u-
tokyo.ac.jp/~tsuruoka/maxent/) to train our mod-
els. For CityU closed track, the basic features are 
the same as (Ng and Low, 2004). For MSRA 
closed track, we used two sets of basic features. 
The one is similar to (Ng and Low, 2004) and we 
change the window size of another one from 2 to 
3, so we trained two models for MSRA closed 
track and submitted two results.  
2.2.2 Post processing 
Firstly, we extracted one lexicon from each train-
ing data. For MSRA closed track, the postpro-
cessor only combined the words which appeared 
in the lexicon but were separated in the test result. 
For CityU closed track, we firstly used the fac-
toid tool provided by the open system of WS to 
combine the separated factoid words, and then 
we used the lexicon to combine the separated 
words, at last the TBL was applied to the test 
result. 
2.3 MSRA Open track of NER 
The system used a hybrid algorithm which can 
combine a class-based statistical model (Gao 
2004) with various types of rule-based knowl-
edge very well. All the words were categorized 
into three types: Lexicon words (LWs), Factoid 
words (FTs), Named Entity (NEs). Accordingly, 
three main components were included to identify 
each kind of named entities: basic word candi-
dates, NE combination and Viterbi search, as 
shown in Figure 1.  
  
Figure 1 FTRD NE Recognizer 
The recognizer was applied to open track of WS 
and we used it to participate in the MSRA open 
track of NER. The system also had a TBL post-
processor. 
2.4 TBL 
In our system, the open source toolkit fnTBL 
(http://nlp.cs.jhu.edu/~rflorian/fntbl/index.html) 
is chosen. Coping with word segmentation task, 
we utilized a method called ?LMR? tagging 
which was the same as (Nianwen Xue and Libin 
Shen 2003). Two rule template sets were used in 
our system. The complicated one had 40 tem-
plates, which covered various kinds of words 
position and tag position occurrence, i.e., consid-
ering contextual information of words and tags. 
For example, rule ?pos_0 word_0 word_1 
word_2 => pos? could generate rules containing 
information about current word, current word?s 
tag, the next word and the word after next. The 
other rule template neglected tag information, it 
took only contextual word information into ac-
count. For an instance, ?word_0 word_1 word_2 
=> pos?. The task of WS applied the two rule 
template sets, and the task of NER only applied 
the complicated one. In the Section 3, we will 
compare the two rule template sets. 
3 Evaluation 
3.1 Open tracks 
3.1.1 MSRA Open track of WS 
In this open track, we used one lexicon of 
294,382 entries, which included the entries of 
42,430 MDWs (Morphological Derived Words) 
generated from the GKB dictionary, 12,487 PNs, 
22,907 LNs and 29,032 ONs, 10,414 four-
character idioms, plus the word lists generated 
from the training data provided by the second 
international Chinese Word Segmentation bake-
off and 80114 GKB words. We also used the 
training data provided by the last bakeoff for 
training our trigram word-based language model.  
123
    Table 1 presents the results of this track. For 
comparison, we also include in the table (Row 1) 
the results of basic system. From Row 2 to Row 
11, it shows the relative contribution of each 
component and resource to the overall word 
segmentation performance. The second column 
shows the recall, the third column the precision, 
and the fourth column F-score. The last two col-
umns present the recall of the OOV words and 
the recall of IV words, respectively. 
(%)     R      P     F  Roov    Riv 
1.basic 
system 0.971 0.958 0.964 0.590 0.984 
2.1+new 
factoid 0.966 0.958 0.962 0.642 0.978 
3.1+GK
B lexicon 0.975 0.966 0.971 0.716 0.984 
4.3+new 
factoid 0.971 0.967 0.969 0.768 0.978 
5. 
4+NE  0.971 0.973 0.972 0.838 0.975 
6. 
5+TBL 0.977 0.976 0.977 0.840 0.982 
7.5+new 
TBL 0.980 0.978 0.979 0.839 0.985 
8. 
4+TBL 0.977 0.970 0.974 0.769 0.984 
9.4+new 
TBL 0.980 0.971 0.975 0.769 0.987 
10. 
8+NE  0.977 0.976 0.977 0.840 0.982 
11. 
9+NE 0.979 0.978 0.979 0.841 0.984 
Table1: Our system results on Open tracks 
From Table 1 we can find that, in Row 1, the 
basic system participated in the last bakeoff al-
ready achieves quite good recall, but the recall of 
OOV is not very good because it cannot correctly 
identify unknown words that are not in the lexi-
con such as factoids and name entities (espe-
cially the nested named entity) and new words 
(except factoids, named entities and words ab-
stracted from training data). In Row 2, we only 
rewrite the factoid rules according to the MSRA 
Guidelines, and the recall of OOV improves sig-
nificantly while the recall of IV falls slightly. It 
shows that the factoid detection affects the recall 
of IV. As shown in Table 1, the GKB lexicon has 
made significant and persistent progress in all 
performance because the GKB lexicon is refined 
and the words are conformed to the MSRA stan-
dard. We also find that the NE postprocessor can 
improve the recall of OOV but affects slightly 
the recall of IV in all experiments. It shows that 
our named entity recognition has make im-
provement compared with that of last year. As 
shown in Table 1, TBL has made slightly but 
persistent progress in all steps it applies to. After 
TBL adaptation OOV recall stays almost un-
changed, for the rules are derived from training 
corpus, and no OOV words would meet the con-
dition of applying them in theory, but IV recall 
improves, which compensates the loss of IV re-
call caused by NE post-process and the factoid 
detection. It is interesting comparing the per-
formance of two TBL template sets, the first 
template set is simple and the threshold for gen-
erating rules is 3 by default (called TBL in Table 
1), and the second is more complicated with a 
"0" threshold (called New TBL in Table 1). The 
number of rules generated is 1061 and 12135 
respectively. Our experiments demonstrate that 
more precise rule template set with low threshold 
always leads to better performance, for they 
could cover more situations, although a simple 
rule template set with high threshold does better 
in OOV word recognition. 
3.1.2 MSRA Open track of NER 
In the track, we used People's Daily 2000 corpus 
(Yu, 2003) for building our lexicon and training 
our model.  
    Considering that organization names are ir-
regular in their forms compared with person 
names and location names, and there are many 
abbreviations and anaphora, TBL adaptation may 
degrade the performance of organization,   we 
submitted two results, as shown in Table 2. 
1+TBL1 means that TBL only adapt person and 
location results of basic system, the organization 
performance of basic system and 1+TBL1 would 
be identical. 1+TBL2 means TBL adapt all three 
types of NE.  For comparison, we list (Column 2) 
the results of basic system. The Row 2 to Row 
13 shows the recall, the precision, and the F-
score of PN, LN, ON and total.  
 (%) 1.basic 1+TBL1 1+TBL2 
R 87.28 91.43 91.74 
P 90.63 92.56 92.77 
 
  PN 
F 88.92 91.99 92.25 
R 80.18 87.39 89.74 
P 81.68 87.51 89.77 
 
  LN 
F 80.92 87.45 89.76 
R 65.59 65.59 76.48 
P 73.80 73.80 75.44 
 
 ON 
F 69.45 69.45 76.11 
R 79.31 83.99 87.53 
P 82.98 86.45 87.67 
 
Total 
F 81.10 85.20 87.60 
124
Table 2: MSRA Open track of NER 
To our surprise, performance listed in Table 2 
demonstrates that applying TBL causes a dra-
matic improvement in all three types of NE, es-
pecially organization performance. The great 
similarity between training corpus and test cor-
pus of MSRA may explain this. For the inconsis-
tency of standard between MSRA and PKU, the 
recall, especially of the ONs, is not very good. 
We did some effort in the standard adaptation, 
such as constraint the length and type of candi-
date words in combining the named entities, but 
the result is not very good. 
3.2 Closed tracks 
The Table 3 and Table 4 present the results of 
MSRA and CityU closed tracks respectively.    
(%)     R      P     F  Roov    Riv 
1.basic 
system(2) 0.924 0.877 0.900 0.575 0.936 
2.1+traini
ng lexicon 0.955 0.953 0.954 0.575 0.969 
3.2+TBL 0.960 0.955 0.958 0.575 0.973 
4.basic 
system(3) 0.919 0.880 0.899 0.602 0.930 
5.4+traini
ng lexicon  0.950 0.954 0.952 0.602 0.962 
6.5+TBL 0.954 0.955 0.955 0.603 0.966 
Table 3: Our system results on MSRA Closed 
(%)     R      P     F  Roov    Riv 
1.basic 
system 0.947 0.916 0.931 0.716 0.957 
2.1+traini
ng lexicon 0.959 0.960 0.959 0.716 0.969 
3.2+TBL 0.969 0.964 0.967 0.716 0.980 
4.1+factoi
d tool 0.946 0.915 0.931 0.713 0.956 
5.4+traini
ng lexicon 0.958 0.959 0.959 0.713 0.968 
6.5+TBL 0.969 0.964 0.966 0.712 0.980 
6' 0.962 0.962 0.962 0.722 0.972 
Table 4: Our system results on CityU Closed 
In Table 3, the basic system (2) shows the win-
dow size of the template is 2 and the basic sys-
tem (3) is 3. As is shown in the table, except the 
precision and the recall of OOV, the performance 
of window size with 2 outperforms that of win-
dow size with 3. 
    In Table 4, the system 6' is the one we submit-
ted in this closed CityU track, but the system 6 is 
better than the system 6'. In TBL training, we 
made a mistake that the training data weren't 
processed by factoid tool and lexicon combining. 
We also can find that the factoid tool doesn't im-
prove the performance. The system 6 isn't the 
best one (system 3).  
    Combining the separated words according to 
training lexicon improved the performance of 
both MSRA and CITYU closed track. In the 
meantime, TBL worked considerably well in all 
closed tracks. 
4 Conclusions 
The evaluation results show that the performance 
of NER need be improved in abbreviations rec-
ognition and anaphora resolution.  
Acknowledgements 
The work reported here was a team effort. We 
thank Yonggang Xue, Duo Ji, Haitao Luo, Nan 
He and Xinnian Mao for their help in the ex-
perimentation and evaluation of the system. We 
also thank Prof. Shiwen Yu for the People's 
Daily 2000 corpus (Yu 2003) and GKB (Yu 
2002) lexicon.  
References 
Heng Li, etc. 2005. Chinese Word Segmentation in 
FTRD Beijing. Proceedings of the Fourth SIGHAN 
workshop on Chinese Language Processing. 
Pages:150-154 
Hwee Tou Ng, Jin Kiat Low. 2004. Chiense part-of-
speech tagging: One-at-a-time or all-at-once? 
Word-based or character-based?. Proceedings of 
the 2004 conference on Empirical Methods in 
Natural Language Processing. Pages:277-284 
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning 
Huang. 2004a. Chinese word segmentation: a 
pragmatic approach. Microsoft Research Technical 
Report, MSR-TR-2004-123. 
Nianwen Xue, Libin Shen. July 2003. Chinese word 
segmentation as LMR tagging. Proceedings of the 
Second SIGHAN workshop on Chinese Language 
Processing. Pages:176-179. 
Shiwen Yu, etc. 2003. Specification for Corpus Proc-
essing at Peking University:Word Segmentation, 
POS Tagging and Phonetic Notation. Journal of 
Chinese Language and Computing, 13(2) 121-158. 
Shiwen Yu, etc. 2002. The Grammatical Knowledge-
base of Contemporary Chinese --- A Complete 
Specification. Tsinghua University Press. 
 
 
 
125
