Coling 2010: Poster Volume, pages 1292?1300,
Beijing, August 2010
Phrase Structure Parsing with Dependency Structure 
Zhiguo Wang and Chengqing Zong
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{zgwang, cqzong}@nlpr.ia.ac.cn 
Abstract
In this paper we present a novel phrase 
structure parsing approach with the help 
of dependency structure. Different with 
existing phrase parsers, in our approach 
the inference procedure is guided by 
dependency structure, which makes the 
parsing procedure flexibly.  The 
experimental results show our approach is 
much more accurate. With the help of 
golden dependency trees, F1 score of our 
parser achieves 96.08% on Penn English 
Treebank and 90.61% on Penn Chinese 
Treebank. With the help of N-best 
dependency trees generated by modified 
MSTParser, F1 score achieves 90.54% 
for English and 83.93% for Chinese. 
1 Introduction 
Over the past few years, several high-precision 
phrase parsers have been presented, and most of 
them are employing probabilistic context-free 
grammar (PCFG). As we all know, the basic 
PCFG has the problems that the independence 
assumption is too strong and lacks of lexical 
conditioning (Jurafsky and Martin, 2007). 
Although researchers have proposed various 
models and inference algorithms aiming to solve 
these problems, the performance of existing 
phrase parsers is still remained to further 
improve. Most of the existing approaches can be 
classified into two categories: unlexicalized 
PCFG based (Johnson, 1998; Klein and 
Manning, 2003; Levy and Manning, 2003; 
Matsuzaki et al, 2005; Petrov et al, 2006) and 
lexicalized PCFG based (Collins, 1999a; 
Charniak, 1997; Bikel, 2004; Charniak and 
Johnson, 2005). 
Unlexicalized PCFG based approach attempts 
to weaken the independence assumption by 
annotating non-terminal symbols with labels of 
ancestor, siblings and even the latent annotations 
encoded by local information. In lexicalized 
PCFG based approach, researchers believe that 
the forms of a constituent and its sub-
constituents are determined more by the 
constituent?s head than any other of its lexical 
items (Charniak, 1997), so they annotate non-
terminal symbols with the head words 
information. 
Both of the two PCFG based approaches have 
improved the basic PCFG based parsers 
significantly. However, neither of them has been 
guided by enough linguistic priori knowledge. 
Their parsing procedures are too mechanical. 
Because of this, the efficiency is always worse, 
and much more artificial ambiguities, which are 
different from linguistic ambiguities (Krotov et 
al., 1998; Johnson, 1998), are generated. We 
believe parsing procedure guided by more 
linguistic priori knowledge will help to 
overcome the drawbacks in some extent. From 
our intuition, dependency structure, another type 
of syntactic structure with much linguistic 
knowledge, will be a good candidate to guide 
phrase parsing procedure.  
In this paper we present a novel approach to 
using dependency structure to guide phrase 
parsing. This novel approach has its virtues from 
multiple angles. First, dependency structure 
offers a good compromise between the 
conflicting demands of analysis depth, which 
makes it much easier to get through hand 
annotating than phrase structure (Nivre, 2004). 
So, when we want to build a phrase structure 
corpus, we can build a dependency structure 
corpus first, and get the corresponding phrase 
structure automatically with the help of 
dependency structure. Second, many parsing 
algorithms with linear-time complexity used in 
dependency parsers can still achieve the state-
of-the-art results (Johansson, 2007), but almost 
all phrase parsers with high-precision have no 
efficient algorithms superior to cubic-time 
complexity. So, in order to get an efficient 
1292
parser, we can first get a dependency structure 
through linear-time algorithm, and then obtain 
the phrase structure with the help of dependency 
structure more efficiently. Third, the lexicalized 
PCFG based parsers which just bring the head 
words into account have got a highly improved 
performance. It gives us reasons to believe 
dependency structure which takes the 
relationship of all the words will bring phrase 
parser a great help. 
Remainder of this paper is organized as 
follows: Section 2 introduces the related work. 
Section 3 gives a consistency between 
dependency structure and phrase structure, and 
presents an approach to parsing phrase structure 
with dependency structure. In Section 4, we 
discuss the experiments and analysis. Finally, 
we conclude this paper and point out some 
future work in Section 5. 
2 Related work 
Unlexicalized PCFG based parsers (Johnson, 
1998; Klein and Manning, 2003; Levy and 
Manning, 2003; Matsuzaki et al, 2005; Petrov 
et al, 2006) are the most successful parsing 
tools. They regard parsing as a pure machine 
learning question. However, they haven?t taken 
any extra linguistic priori knowledge directly 
into account. Lexicalized PCFG based parsers 
(Collins, 1999a; Charniak, 1997; Bikel, 2004; 
Charniak and Johnson, 2005) just bring a little 
linguistic priori knowledge (head word 
information) into learning phase. In inference 
phase, both of the unlexicalized PCFG based 
approach and lexicalized PCFG based approach 
are using the pure searching algorithms, which 
try to parse a sentence monotonously, either 
from left to right or from right to left. From 
these states, we can find that manners of current 
parsers are too mechanical. Because of this, the 
efficiency of phrase parsers is always worse, and 
much more artificial ambiguities are generated. 
There have been some work (Collins et al, 
1999b; Xia and Palmer, 2001) about converting 
dependency structures to phrase structures. 
Collins et al (1999b) proposed an algorithm to 
convert the Czech dependency Treebank into a 
phrase structure Treebank and do dependency 
parsing through Collins (1999a)?s model. 
Results showed the accuracy of dependency 
parsing for Czech was improved largely. Xia 
and Palmer (2001) proposed a more generalized 
algorithm according to X-bar theory and Collins 
et al (1999b), and they did some experiments on 
Penn Treebank. The results showed their 
algorithm produced phrase structures that were 
very close to the ones in Penn Treebank. 
However, we have to point out that they only 
computed the unlabeled performance but lost all 
the exact syntactic symbols. Different from tree-
transformed PCFG based approach and 
lexicalized PCFG based approach, both of 
Collins et al (1999b) and Xia and Palmer (2001) 
attempted to build some heuristic rules through 
linguistic theory, but didn?t try to learn anything 
from Treebank. 
Li and Zong (2005) presented a hierarchical 
parsing algorithm for long complex Chinese 
sentences with the help of punctuations. They 
first divided a long sentence into short ones 
according to punctuation marks, then parsed the 
short ones into sub-trees individually, and at last 
combined all the sub-trees into a whole tree. 
Experimental results showed the parsing time 
was reduced largely, and performance was 
improved too. Although the procedure of their 
parser is more close to human beings? manner, it 
appears a little shallow just using the 
punctuation marks. 
In this paper our motivations are to bring 
more linguistic priori knowledge into phrase 
parsing procedure with the help of dependency 
structure, and make the parsing procedure 
flexibly. 
Matsuzaki et al (2005) defined a generative 
model called PCFG with latent annotations 
(PCFG-LA). Using EM-algorithm each non-
terminal symbols was annotated with a latent 
variable, and a fine-grained model can be got.  
In order to get a more compact PCFG-LA model, 
Petrov et al (2006) presented a split-and-merge 
method which can get PCFG-LA model 
hierarchically, and their final result 
outperformed state-of-the-art phrase parsers. To 
make the parsing process of hierarchical PCFG-
LA model more efficient, Petrov and Klein 
(2007) presented a coarse-to-fine inference 
algorithm. In Section 4 of this paper, we try to 
combine the hierarchical PCFG-LA model in 
learning phase and coarse-to-fine method in 
inference phase into our parser in order to get an 
accurate and efficient parser. 
1293
3 Our framework 
In this section, we first compare phrase structure 
with dependency structure of the same sentence, 
and get a consistent relationship among them. 
Then, based on this relationship, we present an 
inference framework to make the parsing 
procedure flexible and more efficient.  
3.1 Analysis on consistency between phrase 
structure and dependency structure 
Phrase structure and dependency structure are 
two different ways to represent syntactic 
structures of sentences. Phrase structure 
represents sentences by nesting of multi-word 
constituents, while dependency structure 
represents sentences as trees, whose nodes are 
words and edges represent the relations among 
words.
As we know, there are two kinds of 
dependency structures, projective structure and 
non-projective structure. For free-word order 
languages, non-projectivity is a common 
phenomenon, e.g. Czech. For languages like 
English and Chinese, the dependency structures 
are often projective trees. In this paper, we only 
consider English parsing based on Penn 
Treebank (PTB) and Chinese parsing based on 
Penn Chinese Treebank (PCTB), so we just 
research the consistency between phrase 
structure and projective dependency structure 
through PTB/PCTB.  
Information carried by the two structures isn?t 
equal. Phrase structure is more flexible, carries 
more information, and even contains all the 
information of dependency structure. So the task 
to convert a phrase structure to dependency 
structure is more straight, e.g. Nivre and Scholz 
(2004), Johansson and Nugues (2007). However, 
the reverse procedure is much more difficult, 
because dependency structure lacks the syntactic 
symbols, which are indispensable in phrase 
structure.
join
Vinken will board as 29
the director Nov
a nonexecutive
(a) Dependency structure
(1)
(2)
(3)
S
NP VP
NNP
Vinken
MD VP
will VB NP PP NP
join
DT NN
the board
IN
as
NP NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
(b) Phrase structure
(1)
(2)
(3)
Figure 1. The consistency between phrase 
structure and dependency structure 
Although the two structures are completely 
different, they have consistency in some deep 
level. In this paper we analyze the consistency 
from a practical perspective in order to do 
phrase parsing with the help of dependency 
structure. Having investigated the two kinds of 
trees with dependency structure and phrase 
structure, we find a consistency1 that each sub-
tree in dependency structure must correspond to 
a sub-tree in phrase structure who dominates all 
the words appearing in dependency sub-tree. 
Figure 1 shows this relationship more intuitively. 
The dependency sub-tree surrounded by circle 
(1) in Figure 1(a) is a one-layer sub-tree, and has 
a corresponding phrase sub-tree surrounded by 
circle (1) in Figure 1(b). Both of the two sub-
trees dominate the same word ?Vinken?. This 
consistency is also satisfied in other cases, e.g. 
two-layer sub-tree surrounded by circle (3) and 
three-layer sub-tree surrounded by circle (2) in 
Figure 1(a). These dependency sub-trees 
respectively have their corresponding phrase 
sub-trees dominating the same words in Figure 
1(b).
This consistency brings us inspiration to make 
use of dependency structure for phrase parsing. 
In other words, in our method when a phrase 
sub-tree is generated from a dependency sub-
tree, it must dominate all the same words with 
ones in the corresponding dependency sub-tree. 
3.2 Inference framework 
1 Be aware that the consistency is irreversible and not every 
phrase sub-tree has its corresponding dependency sub-tree. 
1294
As we mentioned in Section 2, most of current 
inference algorithms are monotonous, which 
generate much more artificial ambiguities. For 
example, in Figure 1, if a sub-tree only 
dominating ?board? and ?as? is built (actually it 
is not occurred in golden tree) an artificial 
ambiguity is generated, and it thus will further 
bring a worse effect to other parts. The final 
precision will certainly descend. However, if we 
are given a corresponding dependency structure, 
those errors will be avoided. The consistency 
analyzed above tells us that there isn?t a sub-tree 
dominating only ?board? and ?as? in 
dependency tree, so the two words can?t build a 
sub-tree independently in phrase parsing. 
According to this strategy, we design an 
inference framework for phrase parsing. 
NP
NNP
Vinken
MD
will
VB
join
DT NN
the board
IN
as
NNP CD
Nov 29
DT JJ NN
a nonexecutive director
NP
NNP
Vinken
MD
will
VB
NP
NP
join
DT NN
the board
IN
as
NP
NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
join
Vinken will board as 29
the director Nov
a nonexecutive
NP
NNP
Vinken
MD
will
VB
NP
PP
NP
join
DT NN
the board
IN
as
NP
NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
join
Vinken will board as 29
the director Nov
a nonexecutive
S
NP VP
NNP
Vinken
MD VP
will VB NP PP NP
join
DT NN
the board
IN
as
NP NNP CD
Nov 29
DT JJ NN
a
nonexecutive
director
join
Vinken will board as 29
the director Nov
a nonexecutive
(a) fill cell[i,i] for each word
(b) fill spans guided by two-layer dependency sub-trees
(0) (1) (2) (3) (4) (5) (6) (7) (8) (9) (10)
d[3,4]
d[6,8]
d[9,10]
d[5,8]
d[0,10]
cell[3,4]
cell[6,8]
cell[9,10]
cell[5,8]
cell[0,10]
(c) fill spans guided by three-layer dependency sub-trees
(d) fill spans guided by four-layer dependency sub-trees
Figure 2. Parsing procedure of our 
inference framework guided by 
dependency structure
Our inference framework parses a sentence 
flexibly with a traditional inference algorithm. 
The following terms will help to explain our 
work. A key data structure is cell[i,j], which is 
used to store phrase sub-trees spanning words 
from positions i to j of the input sentence. d[i,j]
is a dependency sub-tree spanning words from 
positions i to j. cells[i,j] is an array to store all 
the cells which can be combined to build 
cell[i,j]. The pseudo-code of our inference 
framework is shown in Algorithm1. The line 
indicated by (1) and (2) gives us freedom to 
select any kinds of inference algorithms and 
matching parsing models. 
Algorithm 1
InferenceFramework(sentence S, dependency tree D)
 initialize a List for the input sentence 
for each word wi in S do
fill cell[i, i] and add it to a list L
 parse the cells[] hierarchically 
for each d[s, t] of D in topological order do
fill cells[s, t] with spans in L
fill cell[s, t] with cells[s, t] through  
traditional inference algorithm   (1) 
add cell[s, t] to L
 extract the best tree 
estimate all trees in cell[0, n]
through parsing model             (2) 
return the best phrase tree
Now, let?s illustrate the flexible parsing 
procedure step by step through an example. 
Please see Figure 2.  For simplicity, we just 
draw sub-trees of the final best tree, and ignore 
all the others. Figure 2(a) shows the procedure 
of filling cell[i,i] for each word. In Figure 2(b), 
there are three two-layer dependency sub-trees 
d[3,4], d[6,8] and d[9,10]. So we try to generate 
phrase sub-trees for cell[3,4], cell[6,8] and 
cell[9,10], which have been annotated with bold 
edges. For example, we use sub-trees contained 
in cell[6,6], cell[7,7] and cell[8,8] to 
1295
build new sub-trees for cell[6,8]. Figure 2(c) 
and Figure 2(d) show the same procedure for 
parsing with the help of three-layer dependency 
sub-trees and four-layer dependency sub-trees 
individually. The generated phrase sub-trees are 
all annotated with bold edges in the figure. 
Obviously, the biggest dependency sub-tree is 
the whole dependency tree of sentence. In this 
example, when the four-layer dependency sub-
tree is processed, the whole phrase trees are built. 
Usually, more than one phrase trees with the 
similar skeletons are generated. So we use a 
model to evaluate candidate results, and get out 
the one with the highest score as the final result.
Benefiting from the dependency structure, we 
can parse a sentence flexibly. Comparing with 
previous work on converting dependency 
structure to phrase structure (Collins et al, 
1999b; Xia and Palmer, 2001), we make use of 
Treebank knowledge more sufficient with the 
help of traditional parsing technology. The 
search space has been pruned tremendously. As 
we know, the traditional parsing approach often 
tries to search all the n*(n+1)/2  cells for input 
sentence which has n words, but our parsing 
framework search cells intelligently with the 
help of corresponding dependency structure. 
Let?s get a view of this through the sentence 
shown in Figure 2. From the whole parsing 
procedure shown in Figure 2, our framework 
just tries to fill 16 cells, which are cell[i,i] for 
each word, cell[3,4], cell[6,8], cell[9,10],
cell[5,8] and cell[0,10] hierarchically, but 
traditional parsing approach would try to fill all 
66 cells. So 75.76% searching space has been 
pruned.
4 Experiments and results 
In order to evaluate the effectiveness of our 
approach, we have done some experiments both 
for English parsing and Chinese parsing. 
4.1 Preparation
To make comparison with previous work fairly, 
our experiments are based on general Treebank 
according to standard settings. We choose Penn 
English Treebank for English parsing 
experiments and Penn Chinese Treebank for 
Chinese. Table 1 shows the standard settings we 
take.
? ???? ????? ???? ? ??????? ?
PU NP NP NP PU
VP PUVP
IP
?????????????? ? ??????? ?
PUIPPU PUIP
IP
(b) Golden phrase tree
(c) Parsing result of PCFG model and CYK algorithm
? ???? ????? ???? ? ????? ?
PU NP NP NP PU
VP PUVP
IP
(d) Parsing result after pruning strategy added
??
NP
? ???? ????? ???? ? ??????? ?
PU NP NP NP PU
VP PUVP
IP
(e) Parsing result of PCFG-LA model
(a) Golden dependency tree
??/hold
??/ceremony ??/today ?/in ?
??/Shanghai??/collaborate ??/project ??/signing? ?
?/America
?/China ?/high
??/technology
Figure 4. An example showing 
experimental results
English Chinese
Train Set Sections 2-21 Art. 1-270, 400-1151 
Dev Set Section 22 Articles 301-325 
Test Set Section 23 Articles 271-300 
Table 1. Experimental settings 
Because the two Treebanks are in type of 
phrase structure, we should get dependency 
structures corresponding with them. There are 
two ways to accomplish this work. First, use 
converting tools to get dependency trees directly 
through converting the original Treebanks, and 
the generated trees are always considered as 
golden trees during dependency parsing. Second, 
use a dependency parser with state-of-the-art 
1296
performance to parse all the sentences 
automatically. In this paper, we design two 
groups of experiments, as following: 
(1) Phrase parsing with the help of golden 
dependency trees. 
(2) Phrase parsing with the help of N-best 
dependency trees generated automatically. 
4.2 Phrase parsing with golden dependency 
trees
In order to verify how much dependency 
structure can help phrase parsing and get an 
upper bound of our approach, we do phrase 
parsing with the help of golden dependency 
trees in this subsection. 
Based on the parsing framework shown in 
Figure 3, we only use the basic PCFG in 
learning phase and our inference framework 
with basic CYK algorithm in inference phase. 
The parsing results are shown with the mark (1) 
in Table 2 for English and Table 3 for Chinese 
respectively. 
Having investigated the generated trees with 
golden trees, we find the consistency of 
dependency structure and phrase structure is 
broken by some trees. Let?s get a view of this 
through an example from Penn Chinese 
Treebank. In Figure 4(a), the dependency sub-
tree surrounded by circle tells us that there must 
be a phrase sub-tree which dominate the word 
sequence of ??????????
? ?  ? ?  ? (the signing ceremony of 
collaborating in high technology between 
America and China), and the golden phrase tree 
shown in Figure 4(b) has a corresponding sub-
tree surrounded by circle indeed. However, the 
parsing tree generated by our approach shown in 
Figure 4(c) doesn?t conform. There are three 
sub-trees dominating the word sequence 
mutually, but they don?t construct a whole one. 
In our opinion, the contradiction derived from 
binarizing process of CYK 2 . The binary trees 
generated by our algorithm have consisted with 
the consistency originally, but after debinarizing 
process the consistency is broken. 
Trying to check our opinion, we add a 
pruning strategy to the original inference 
2 The premise of using CYK is that all the rules must have 
CNF form. So we usually bring some medial nodes to 
binarize rules gathered from Treebank. 
algorithm to prune all the medial nodes which 
may break the consistency during parsing 
procedure. With the help of pruning strategy, the 
performances of English and Chinese are all 
improved further. Corresponding figures are 
shown in Table 2 and Table 3 with the mark (2). 
The parsing result of above example is shown in 
Figure 4(d) and the error appearing in Figure 4(c) 
is corrected naturally after the pruning strategy 
added.
Comparing with previous work which have 
done much work in learning phase, our 
algorithm achieves such amazing results only 
using basic PCFG model. From this aspect, our 
inference framework is much more effective. 
However, there are still some errors our 
approach can?t deal with. For example, in Figure 
4(d) the sub-tree rooted at NP and dominating 
word sequence of  ????????? (hold 
in Shanghai today) is separated by two sub-trees. 
The reason is that the model (basic PCFG) we 
use in learning phase is too coarse to 
disambiguate sufficiently. So we don?t pin all 
hopes in inference phase. We also modify the 
model in learning phase. PCFG-LA is one of the 
most successful models in phrase parsing, so we 
choose PCFG-LA as the model in learning phase. 
After this modification, performance of our 
approach has been improved delightedly. F1 
score is 96.08% for English and 90.06% for 
Chinese. The line marked with (3) in Table 2 
and Table 3 shows more details. 
4.3 Phrase parsing with N-best dependency 
trees generated automatically 
The experimental results shown in subsection 
4.2 bring us confidence that do phrase parsing 
with the help of dependency structure is a highly 
effective approach. However, we don?t usually 
have golden dependency structures, and a more 
acceptable way is using a dependency parser to 
generate dependency trees automatically. In this 
subsection we explore feasibility and 
effectiveness of phrase parsing with the help of 
dependency trees generated automatically. As 
we all know, even state-of-the-art dependency 
parser cannot generate totally correct result. So in 
order to make our system more robust we use N-
best dependency structures to guide phrase 
parsing procedure. 
1297
length<=40 all sentences ?
Precision Recall F1 Precision Recall F1 
(1) Using PCFG and CYK 90.28 88.41 89.34 90.11 88.32 89.21 
(2) Using pruning strategy 90.69 89.53 90.11 90.51 89.45 89.97 
(3) Using PCFG-LA 96.28 95.97 96.13 96.25 95.91 96.08
Table 2. Parsing performance (%) for English with the help of golden dependency tree.  
length<=40 all sentences ?
Precision Recall F1 Precision Recall F1 
(1) Using PCFG and CYK 86.89 78.25 82.34 85.56 77.43 81.29 
(2) Using pruning strategy 87.65 82.33 84.91 86.39 81.45 83.85 
(3) Using PCFG-LA 91.51 91.26 91.38 90.43 90.79 90.61
Table 3. Parsing performance (%) for Chinese with the help of golden dependency tree.  
We choose MSTParser 3  which is the most 
famous dependency parser and modify it to 
generate N-best dependency trees. The oracle 
unlabeled accuracy of N-best dependency trees 
generated from 1-order model is shown in 
Table 4. To show the effectiveness of our 
approach, we choose Berkeleyparser 4  as the 
baseline parser, take the same configuration and 
combine it into our general parsing framework 
shown in Figure 3. 
The experiment of parsing with golden 
dependency structure gets an amazing 
performance. It brings us a new way to build 
PTB/PCTB style phrase structure corpus. 
Because dependency structure is much easier to 
get through hand annotating than phrase 
structure, we can build a dependency structure 
corpus first, and then get phrase structure 
corpus through our approach guided by the 
dependency structure corpus. 
The experiment of parsing with N-best 
dependency structures generated automatically 
uplifts the parsing performance to a new height. 
It brings us a more applied parsing tool for 
other NLP applications. 
Considering the number of dependency 
structures (N-best) will affect the final result, 
we make use of the development set shown in 
Table1 to turning parameters. We parse the 
development set many times with different 
number of dependency structures. The F1 
scores are shown in Figure 5 for English and 
Figure 6 5  for Chinese. From Figure 5 and 
Figure 6, we can find when we use 10-best 
dependency structures the performance is better. 
So we choose 10-best dependency trees for the 
test set. 
From the experiments in Section 4.2, we can 
find that even using the golden dependency 
structure we can?t get totally correct phrase 
structure. The reason is that although every 
dependency sub-tree has its corresponding 
phrase sub-tree, not every phrase sub-tree has 
its corresponding dependency sub-tree. So the 
remainder errors can?t be solved only by 
dependency structure and a better way is to 
modify the parsing model. 
The final performances of test set comparing 
with previous work are shown in Table 5 and 
Table 6. We can easily find that our approach 
has outperformed all the parsers which aren?t 
improved through reranking stage or semi-
supervised approach. Although there is still a 
margin between our parser and reranked parser 
or semi-supervised parser, we believe that the 
parsing performance can be improved further if 
we bring the reranking or semi-supervised 
approaches into our parsing framework.
5 Conclusion and Future work  
In this paper, we present a novel phrase parsing 
approach with the help of dependency structure. 
Based on the consistency between phrase 
structure and dependency structure, we propose 
a novel inference framework. Guided by the 
inference framework, inference algorithms 
parse sentences hierarchically with the help of 
dependency structures. Experimental results 
show that our approach can efficiently get 
better performance with both golden 
dependency structure and N-best dependency 
4.4 Discussion 
3 http://www.ryanmcd.com/MSTParser/MSTParser.html 
4 http://code.google.com/p/berkeleyparser/ 
5 F1 score at n=0 is the result of Berkeley parser running 
on my machine 
1298







          
1EHVW
)
OHQJWK  DOOVHQWHQFHV
Figure 6. F1 scores (%) of Dev Set for Chinese 
with the help of N-best dependency trees 

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 623?627,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lattice-based Framework for Joint Chinese Word Segmentation, 
POS Tagging and Parsing 
Zhiguo Wang1, Chengqing Zong1 and Nianwen Xue2 
1National Laboratory of Pattern Recognition, 
Institute of Automation, Chinese Academy of Sciences, Beijing, China, 100190 
2Computer Science Department, Brandeis University, Waltham, MA 02452 
{zgwang, cqzong}@nlpr.ia.ac.cn   xuen@brandeis.edu 
 
Abstract 
For the cascaded task of Chinese word seg-
mentation, POS tagging and parsing, the pipe-
line approach suffers from error propagation 
while the joint learning approach suffers from 
inefficient decoding due to the large combined 
search space. In this paper, we present a novel 
lattice-based framework in which a Chinese 
sentence is first segmented into a word lattice, 
and then a lattice-based POS tagger and a lat-
tice-based parser are used to process the lattice 
from two different viewpoints: sequential POS 
tagging and hierarchical tree building. A strat-
egy is designed to exploit the complementary 
strengths of the tagger and parser, and encour-
age them to predict agreed structures. Experi-
mental results on Chinese Treebank show that 
our lattice-based framework significantly im-
proves the accuracy of the three sub-tasks. 
1 Introduction 
Previous work on syntactic parsing generally 
assumes a processing pipeline where an input 
sentence is first tokenized, POS-tagged and then 
parsed (Collins, 1999; Charniak, 2000; Petrov 
and Klein, 2007). This approach works well for 
languages like English where automatic tokeni-
zation and POS tagging can be performed with 
high accuracy without the guidance of the high-
level syntactic structure. Such an approach, how-
ever, is not optimal for languages like Chinese 
where there are no natural delimiters for word 
boundaries, and word segmentation (or tokeniza-
tion) is a non-trivial research problem by itself. 
Errors in word segmentation would propagate to 
later processing stages such as POS tagging and 
syntactic parsing. More importantly, Chinese is a 
language that lacks the morphological clues that 
help determine the POS tag of a word. For ex-
ample, ??  (?investigate/investigation?) can 
either be a verb (?investigate?) or a noun (?inves-
tigation?), and there is no morphological varia-
tion between its verbal form and nominal form. 
This contributes to the relatively low accuracy 
(95% or below) in Chinese POS tagging when 
evaluated as a stand-alone task (Sun and Uszko-
reit, 2012), and the noun/verb ambiguity is a ma-
jor source of error.  
More recently, joint inference approaches 
have been proposed to address the shortcomings 
of the pipeline approach. Qian and Liu (2012) 
proposed a joint inference approach where syn-
tactic parsing can provide feedback to word 
segmentation and POS tagging and showed that 
the joint inference approach leads to improve-
ments in all three sub-tasks. However, a major 
challenge for joint inference approach is that the 
large combined search space makes efficient de-
coding and parameter estimation very hard.  
In this paper, we present a novel lattice-based 
framework for Chinese. An input Chinese sen-
tence is first segmented into a word lattice, 
which is a compact representation of a small set 
of high-quality word segmentations. Then, a lat-
tice-based POS tagger and a lattice-based parser 
are used to process the word lattice from two 
different viewpoints. We next employ the dual 
decomposition method to exploit the comple-
mentary strengths of the tagger and parser, and 
encourage them to predict agreed structures. Ex-
perimental results show that our lattice-based 
framework significantly improves the accuracies 
of the three sub-tasks  
2 The Lattice-based Framework 
Figure 1 gives the organization of the framework. 
There are four types of linguistic structures: a 
Chinese sentence, the word lattice, tagged word 
sequence and parse tree of the Chinese sentence. 
An example for each structure is provided in 
Figure 2. We can see that the terminals and pre-
terminals of a parse tree constitute a tagged word 
sequence. Therefore, we define a comparator 
between a tagged word sequence and a parse tree: 
if they contain the same word sequence and POS 
tags, they are equal, otherwise unequal. 
623
Figure 1 also shows the workflow of the 
framework. First, the Chinese sentence is seg-
mented into a word lattice using the word seg-
mentation system. Then the word lattice is fed 
into the lattice-based POS tagger to produce a 
tagged word sequence   and into the lattice-
based parser to separately produce a parse tree  . 
We then compare   with   to see whether they 
are equal. If they are equal, we output   as the 
final result. Otherwise, the guidance generator 
generates some guidance orders based on the 
difference between   and  , and guides the tag-
ger and the parser to process the lattice again. 
This procedure may iterate many times until the 
tagger and parser predict equal structures. 
 
 
The motivation to design such a framework is 
as follows. First, state-of-the-art word segmenta-
tion systems can now perform with high accura-
cy. We can easily get an F1 score greater than 
96%, and an oracle (upper bound) F1 score 
greater than 99%  for the word lattice (Jiang et 
al., 2008). Therefore, a word lattice provides us a 
good enough search space to allow sufficient 
interaction among word segmentation, POS tag-
ging and parsing systems. Second, both the lat-
tice-based POS tagger and the lattice-based pars-
er can select word segmentation from the word 
lattice and predict POS tags, but they do so from 
two different perspectives. The lattice-based POS 
tagger looks at a path in a word lattice as a se-
quence and performs sequence labeling based on 
linear local context, while the lattice-based pars-
er builds the parse trees in a hierarchical manner. 
They have different strengths with regard to 
word segmentation and POS tagging. We hypo-
thesize that exploring the complementary 
strengths of the tagger and parser would improve 
each of the sub-tasks. 
We build a character-based model (Xue, 2003) 
for the word segmentation system, and treat 
segmentation as a sequence labeling task, where 
each Chinese character is labeled with a tag. We 
use the tag set provided in Wang et al (2011) 
and use the same feature templates. We use the 
Maximum Entropy (ME) model to estimate the 
feature weights. To get a word lattice, we first 
generate N-best word segmentation results, and 
then compact the N-best lists into a word lattice 
by collapsing all the identical words into one 
edge. We also assign a probability to each edge, 
which is calculated by multiplying the tagging 
probabilities of each character in the word. 
    The goal of the lattice-based POS tagger is to 
predict a tagged word sequence   for an input 
word lattice  :   = argmax ?    ( ) ?  ( ) 
where     ( ) represents the set of all possible 
tagged word sequences derived from the word 
lattice  .  ( ) is used to map   onto a global fea-
ture vector, and   is the corresponding weight 
vector. We use the same non-local feature tem-
plates used in Jiang et al (2008) and a similar 
decoding algorithm. We use the perceptron algo-
rithm (Collins, 2002) for parameter estimation. 
Goldberg and Elhadad (2011) proposed a lat-
tice-based parser for Heberw based on the 
PCFG-LA model (Matsuzaki et al, 2005). We 
adopted their approach, but found the un-
weighted word lattice their parser takes as input 
to be ineffective for our Chinese experiments. 
Instead, we use a weighted lattice as input and 
weigh each edge in the lattice with the word 
probability. In our model, each syntactic catego-
ry   is split into multiple subcategories  [ ] by 
labeling a latent annotation  . Then, a parse tree 
????????????? 
Brown?s group will leave Shanghai to Guangzhou tonight. 
(a) Chinese Sentence 
 
 (b) Word Lattice 
?? ?????????
NR NRVVNRPNTP PU
??
NN
Brown .GuangzhougoShanghaileavetonightingroup  
(c) Tagged Word Sequence 
Brown
.
Guangzhou
go
Shanghai
leavetonight
ingroup
?? ?
?? ?
??
?
NR P
NT
NP
PP
VV
NR
NP
VP
PUNP
IP
VP
??
NN
NP NP
?
?
VV
NR
NP
VP
VP
 
(d) Parse Tree 
Figure 2: Linguistic structure examples. 
Chinese Sentence
Word Segmentation
Word Lattice
Lattice-based Parser Lattice-based POS Tagger
Guidance Generator
Parse Tree Tagged Word 
Sequence
The Final Parse Tree
No
Yes
Equal?
 
Figure 1: The lattice-based framework. 
624
  is refined into  [ ], where X is the latent an-
notation vector for all non-terminals in  . The 
probability of  [ ] is calculated as:  ( [ ]) =   ( [ ] ?  [ ] [ ]) ?  ( [ ] ?  )?  ( ) 
where the three terms are products of all syntac-
tic rule probabilities, lexical rule probabilities 
and word probabilities in  [ ] respectively. 
3 Combined Optimization Between The 
Lattice-based POS Tagger and The 
Lattice-based Parser  
We first define some variables to make it easier 
to compare a tagged word sequence   with a 
parse tree  . We define   as the set of all POS 
tags. For  , we define  ( ,  , )=1 if   contains a 
POS tag  ?   spanning from the i-th character 
to the j-th character, otherwise  ( ,  , ) = 0. We 
also define  ( ,  , #) = 1 if   contains the word 
spanning from the i-th character to the j-th cha-
racter, otherwise  ( ,  , #) = 0. Similarly, for  , 
we define  ( ,  , )=1 if   contains a POS tag  ?   spanning from the i-th character to the j-th 
character, otherwise  ( ,  ,  ) = 0. We also define  ( ,  , #)  = 1 if   contains the word spanning 
from the i-th character to the j-th character, oth-
erwise  ( ,  , #) = 0. Therefore,   and   are equal, 
only if  ( ,  ,  ) =  ( ,  ,  )  for all  ? [0,  ] ,  ? [ + 1,  ] and  ?  ? #, otherwise unequal. 
Our framework expects the tagger and the 
parser to predict equal structures and we formu-
late it as a constraint optimization problem:    ,   = argmax ,    ( ) +   ( ) 
Such that for all  ? [0, ] ,  ? [ + 1, ]  and  ?  ? #:  ( ,  ,  ) =  ( ,  , ) 
 
where   ( ) =  ?  ( )  is a scoring function 
from the viewpoint of the lattice-based POS tag-
ger, and   ( ) = log  ( ) is a scoring function 
from the viewpoint of the lattice-based parser.  
The dual decomposition (a special case of La-
grangian relaxation) method introduced in Ko-
modakis et al (2007) is suitable for this problem. 
Using this method, we solve the primal con-
straint optimization problem by optimizing the 
dual problem. First, we introduce a vector of La-
grange multipliers  ( ,  ,  )  for each equality 
constraint. Then, the Lagrangian is formulated as:  ( ,  ,  ) =   ( ) +   ( ) +   ( ,  , )( ( ,  ,  )?  ( ,  , )) , ,  
By grouping the terms that depend on   and  , 
we rewrite the Lagrangian as  ( , , ) =    ( ) +   ( ,  , ) ( ,  , ) , ,   +   ( )?  ( ,  , ) ( ,  , ) , ,   
Then, the dual objective is  ( ) = max ,  ( , , ) = max    ( ) +   ( ,  , ) ( ,  , ) , ,  + max    ( )?  ( ,  , ) ( ,  , ) , ,   
The dual problem is to find min  ( ). 
    We use the subgradient method (Boyd et al, 
2003) to minimize the dual. Following Rush et al 
(2010), we define the subgradient of   ( ) as:  ( ,  , ) =  ( ,  , )?  ( ,  ,  )  for all ( ,  , ) 
Then, adjust  ( ,  ,  ) as follows:   ( ,  , ) =  ( ,  , )?  ( ( ,  ,  )?  ( ,  , )) 
where  >0 is a step size. 
 
Algorithm 1 presents the subgradient method 
to solve the dual problem. The algorithm initia-
lizes the Lagrange multiplier values with 0 (line 
1) and then iterates many times. In each iteration, 
the algorithm finds the best   ( )  and   ( )  by 
running the lattice-based POS tagger (line 3) and 
the lattice-based parser (line 4). If   ( ) and    ( ) 
share the same tagged word sequence (line 5), 
then the algorithm returns the solution (line 6). 
Otherwise, the algorithm adjusts the Lagrange 
multiplier values based on the differences be-
tween    ( ) and   ( ) (line 8). A crucial point is 
that the argmax problems in line 3 and line 4 can 
be solved efficiently using the original decoding 
algorithms, because the Lagrange multiplier can 
be regarded as adjustments for lexical rule prob-
abilities and word probabilities.  
4 Experiments 
We conduct experiments on the Chinese Tree-
bank Version 5.0 and use the standard data split 
Algorithm 1: Combined Optimization 
1: Set  ( )( ,  , )=0, for all  ( ,  , ) 
2: For k=1 to K 
3:     ( ) ? argmax    ( ) + ?   (   )( ,  , ) ( ,  , )  , ,    
4:     ( ) ? argmax    ( )? ?   (   )( ,  ,  ) ( ,  ,  )  , ,   
5:   If  ( )( ,  ,  ) =  ( )( ,  ,  ) for all ( ,  ,  )  
6:      Return (  ( ),   ( )) 
7:   Else  
8:       ( )( ,  ,  ) =  (   )( ,  ,  ) ?  ( ( )( ,  ,  )?  ( )( ,  , ))  
 
625
(Petrov and Klein, 2007). The traditional evalua-
tion metrics for POS tagging and parsing are not 
suitable for the joint task. Following with Qian 
and Liu (2012), we redefine precision and recall 
by computing the span of a constituent based on 
character offsets rather than word offsets.  
4.1 Performance of the Basic Sub-systems 
We train the word segmentation system with 100 
iterations of the Maximum Entropy model using 
the OpenNLP toolkit. Table 1 shows the perfor-
mance. It shows that our word segmentation sys-
tem is comparable with the state-of-the-art sys-
tems and the upper bound F1 score of the word 
lattice exceeds 99.6%. This indicates that our 
word segmentation system can provide a good 
search space for the lattice-based POS tagger and 
the lattice-based parser. 
 
To train the lattice-based POS tagger, we gen-
erate the word lattice for each sentence in the 
training set using cross validation approach. We 
divide the entire training set into 18 folds on av-
erage (each fold contains 1,000 sentences). For 
each fold, we segment each sentence in the fold 
into a word lattice by compacting 20-best seg-
mentation list produced with a model trained on 
the other 17 folds. Then, we train the lattice-
based POS tagger with 20 iterations of the aver-
age perceptron algorithm. Table 2 presents the 
joint word segmentation and POS tagging per-
formance and shows that our lattice-based POS 
tagger obtains results that are comparable with 
state-of-the-art systems. 
 
We implement the lattice-based parser by 
modifying the Berkeley Parser, and train it with 
5 iterations of the split-merge-smooth strategy 
(Petrov et al, 2006). Table 3 shows the perfor-
mance, where the ?Pipeline Parser? represents 
the system taking one-best segmentation result 
from our word segmentation system as input and 
?Lattice-based Parser? represents the system tak-
ing the compacted word lattice as input. We find 
the lattice-based parser gets better performance 
than the pipeline system among all three sub-
tasks. 
 
4.2 Performance of the Framework 
For the lattice-based framework, we set the max-
imum iteration in Algorithm 1 as K = 20. The 
step size   is tuned on the development set and 
empirically set to be 0.8. Table 4 shows the pars-
ing performance on the test set. It shows that the 
lattice-based framework achieves improvement 
over the lattice-based parser alone among all 
three sub-tasks: 0.16 points for word segmenta-
tion, 1.19 points for POS tagging and 1.65 points 
for parsing. It also outperforms the lattice-based 
POS tagger by 0.65 points on POS tagging accu-
racy. Our lattice-based framework also improves 
over the best joint inference parsing system 
(Qian and Liu, 2012) by 0.57 points. 
 
5 Conclusion  
In this paper, we present a novel lattice-based 
framework for the cascaded task of Chinese 
word segmentation, POS tagging and parsing. 
We first segment a Chinese sentence into a word 
lattice, then process the lattice using a lattice-
based POS tagger and a lattice-based parser. We 
also design a strategy to exploit the complemen-
tary strengths of the tagger and the parser and 
encourage them to predict agreed structures. Ex-
perimental results show that the lattice-based 
framework significantly improves the accuracies 
of the three tasks. The parsing accuracy of the 
framework also outperforms the best joint pars-
ing system reported in the literature. 
  P R F 
(Qian and Liu, 
2012) 
 
Seg. 97.56 98.36 97.96 
POS 93.43 94.2 93.81 
Parse 83.03 82.66 82.85 
Lattice-based  
Framework 
Seg. 97.82 97.9 97.86 
POS 94.36 94.44 94.40 
Parse 83.34 83.5 83.42 
 Table 4: Lattice-based framework evaluation. 
  P R F 
Pipeline Parser 
 
Seg. 96.97 98.06 97.52 
POS 92.01 93.04 92.52 
Parse 80.86 81.47 81.17 
 
Lattice-based 
 Parser 
Seg. 97.73 97.66 97.70 
POS 93.24 93.18 93.21 
Parse 81.83 81.71 81.77 
 Table 3: Parsing evaluation. 
 P R F (Kruengkrai et al, 2009) 93.28 94.07 93.67 
(Zhang and Clark, 2010) - - 93.67 
(Qian and Liu, 2012) 93.1 93.96 93.53 
(Sun, 2011) - - 94.02 
Lattice-based POS tagger 93.64 93.87 93.75 
Table 2: POS tagging evaluation. 
  P R F 
(Kruengkrai et al, 2009) 97.46 98.29 97.87 
(Zhang and Clark, 2010) - - 97.78 
(Qian and Liu, 2012) 97.45 98.24 97.85 
(Sun, 2011) - - 98.17 
Our Word Seg. System 96.97 98.06 97.52 
Word Lattice Upper Bound 99.55 99.75 99.65 
Table 1: Word segmentation evaluation. 
626
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program ("863" 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. This work is also supported 
in part by the DAPRA via contract HR0011-11-
C-0145 entitled "Linguistic Resources for Multi-
lingual Processing". 
References  
S. Boyd, L. Xiao and A. Mutapcic. 2003. Subgradient 
methods. Lecture notes of EE392o, Stanford Uni-
versity. 
E. Charniak. 2000. A maximum?entropy?inspired 
parser. In NAACL ?00, page 132?139. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. In Proc. of 
EMNLP2002, pages 1-8. 
Yoav Goldberg and Michael Elhadad. 2011. Joint 
Hebrew segmentation and parsing using a PCFG-
LA lattice parser. In Proc. of ACL2011. 
Wenbin Jiang, Haitao Mi and Qun Liu. 2008. Word 
lattice reranking for Chinese word segmentation 
and part-of-speech tagging. In Proc. of Coling 2008, 
pages 385-392. 
Komodakis, N., Paragios, N., and Tziritas, G. 2007. 
MRF optimization via dual decomposition: Mes-
sage-passing revisited. In ICCV 2007. 
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. 
Torisawa and H. Isahara. 2009. An error-driven 
word-character hybrid model for joint Chinese 
word segmentation and POS tagging. In Proc. of 
ACL2009, pages 513-521. 
Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Probabilistic CFG with latent annotations. In 
Proc. of ACL2005, pages 75-82. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of ACL2006, 
pages 433-440. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proc. of NAACL2007, 
pages 404-411. 
Xian Qian and Yang Liu. 2012. Joint Chinese Word 
segmentation, POS Tagging Parsing. In Proc. of 
EMNLP 2012, pages 501-511. 
Alexander M. Rush, David Sontag, Michael Collins 
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural 
language processing. In Proc. of EMNLP2010, 
pages 1-11. 
Weiwei Sun. 2011. A stacked sub-word model for 
joint Chinese word segmentation and part-of-
speech tagging. In Proc. of ACL2011, pages 1385-
1394. 
Weiwei Sun and Hans Uszkoreit. Capturing paradig-
matic and syntagmatic lexical relations: Towards 
accurate Chinese part-of-speech tagging. In Proc. 
of ACL2012. 
Yiou Wang, Jun'ichi Kazama, Yoshimasa Tsuruoka, 
Wenliang Chen, Yujie Zhang and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation 
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proc. of 
IJCNLP2011, pages 309-317. 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. Computational Linguistics and 
Chinese Language Processing, 8 (1). pages 29-48. 
Yue Zhang and Stephen Clark. 2010. A fast decoder 
for joint word segmentation and POS-tagging using 
a single discriminative model. In Proc. of 
EMNLP2010, pages 843-852. 
627
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733?742,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Joint POS Tagging and Transition-based Constituent Parsing in Chinese
with Non-local Features
Zhiguo Wang
Brandeis University
Waltham, MA, USA
zgwang@brandeis.edu
Nianwen Xue
Brandeis University
Waltham, MA, USA
xuen@brandeis.edu
Abstract
We propose three improvements to ad-
dress the drawbacks of state-of-the-art
transition-based constituent parsers. First,
to resolve the error propagation problem
of the traditional pipeline approach, we
incorporate POS tagging into the syntac-
tic parsing process. Second, to allevi-
ate the negative influence of size differ-
ences among competing action sequences,
we align parser states during beam-search
decoding. Third, to enhance the pow-
er of parsing models, we enlarge the fea-
ture set with non-local features and semi-
supervised word cluster features. Exper-
imental results show that these modifica-
tions improve parsing performance signif-
icantly. Evaluated on the Chinese Tree-
Bank (CTB), our final performance reach-
es 86.3% (F1) when trained on CTB 5.1,
and 87.1% when trained on CTB 6.0, and
these results outperform all state-of-the-art
parsers.
1 Introduction
Constituent parsing is one of the most fundamen-
tal tasks in Natural Language Processing (NLP). It
seeks to uncover the underlying recursive phrase
structure of sentences. Most of the state-of-the-
art parsers are based on the PCFG paradigm and
chart-based decoding algorithms (Collins, 1999;
Charniak, 2000; Petrov et al, 2006). Chart-based
parsers perform exhaustive search with dynam-
ic programming, which contributes to their high
accuracy, but they also suffer from higher run-
time complexity and can only exploit simple local
structural information.
Transition-based constituent parsing (Sagae and
Lavie, 2005; Wang et al, 2006; Zhang and Clark,
2009) is an attractive alternative. It utilizes a se-
ries of deterministic shift-reduce decisions to con-
struct syntactic trees. Therefore, it runs in linear
time and can take advantage of arbitrarily complex
structural features from already constructed sub-
trees. The downside is that they only search a tiny
fraction of the whole space and are therefore com-
monly considered to be less accurate than chart-
based parsers. Recent studies (Zhu et al, 2013;
Zhang et al, 2013) show, however, that this ap-
proach can also achieve the state-of-the-art perfor-
mance with improved training procedures and the
use of additional source of information as features.
However, there is still room for improvemen-
t for these state-of-the-art transition-based con-
stituent parsers. First, POS tagging is typically
performed separately as a preliminary step, and
POS tagging errors will propagate to the parsing
process. This problem is especially severe for lan-
guages where the POS tagging accuracy is rela-
tively low, and this is the case for Chinese where
there are fewer contextual clues that can be used
to inform the tagging process and some of the
tagging decisions are actually influenced by the
syntactic structure of the sentence. This creates
a chicken and egg problem that needs to be ad-
dressed when designing a parsing model. Second,
due to the existence of unary rules in constituen-
t trees, competing candidate parses often have d-
ifferent number of actions, and this increases the
disambiguation difficulty for the parsing model.
Third, transition-based parsers have the freedom
to define arbitrarily complex structural features,
but this freedom has not fully been taken advan-
tage of and most of the present approaches only
use simple structural features.
In this paper, we address these drawbacks to
improve the transition-based constituent parsing
for Chinese. First, we integrate POS tagging in-
to the parsing process and jointly optimize these
two processes simultaneously. Because non-local
syntactic information is now available to POS tag
733
determination, the accuracy of POS tagging im-
proves, and this will in turn improve parsing ac-
curacy. Second, we propose a novel state align-
ment strategy to align candidate parses with dif-
ferent action sizes during beam-search decoding.
With this strategy, parser states and their unary
extensions are put into the same beam, therefore
the parsing model could decide whether or not
to use unary actions within local decision beam-
s. Third, we take into account two groups of
complex structural features that have not been
previously used in transition-based parsing: non-
local features (Charniak and Johnson, 2005) and
semi-supervised word cluster features (Koo et al,
2008). With the help of the non-local features,
our transition-based parsing system outperform-
s all previous single systems in Chinese. After
integrating semi-supervised word cluster features,
the parsing accuracy is further improved to 86.3%
when trained on CTB 5.1 and 87.1% when trained
on CTB 6.0, and this is the best reported perfor-
mance for Chinese.
The remainder of this paper is organized as fol-
lows: Section 2 introduces the standard transition-
based constituent parsing approach. Section 3
describes our three improvements to standard
transition-based constituent parsing. We discuss
and analyze the experimental results in Section 4.
Section 5 discusses related work. Finally, we con-
clude this paper in Section 6.
2 Transition-based Constituent Parsing
This section describes the transition-based con-
stituent parsing model, which is the basis of Sec-
tion 3 and the baseline model in Section 4.
2.1 Transition-based Constituent Parsing
Model
A transition-based constituent parsing model is a
quadruple C = (S, T, s
0
, S
t
), where S is a set of
parser states (sometimes called configurations), T
is a finite set of actions, s
0
is an initialization func-
tion to map each input sentence into a unique ini-
tial state, and S
t
? S is a set of terminal states.
Each action t ? T is a transition function to tran-
sit a state into a new state. A parser state s ? S is
defined as a tuple s = (?, ?), where ? is a stack
which is maintained to hold partial subtrees that
are already constructed, and ? is a queue which is
used for storing word-POS pairs that remain un-
processed. In particular, the initial state has an
B
0,3
c
2,3
w
2
A
0,2
b
1,2
w
1
a
0,1
w
0
sh,sh,rr-A,sh,rl-B
(a)
B
0,3
F
2,3
c
2,3
w
2
E
0,2
A
0,2
D
1,2
b
1,2
w
1
C
0,1
a
0,1
w
0
sh,ru-C,sh,ru-D,rr-A,
ru-E,sh,ru-F,rl-B
(b)
Figure 1: Two constituent trees for an example
sentence w
0
w
1
w
2
with POS tags abc. The cor-
responding action sequences are given below, the
spans of each nodes are annotated and the head n-
odes are written with Bold font type.
empty stack ? and a queue ? containing the entire
input sentence (word-POS pairs), and the terminal
states have an empty queue ? and a stack ? con-
taining only one complete parse tree. The task of
transition-based constituent parsing is to scan the
input POS-tagged sentence from left to right and
perform a sequence of actions to transform the ini-
tial state into a terminal state.
In order to construct lexicalized constituen-
t parse trees, we define the following actions for
the action set T according to (Sagae and Lavie,
2005; Wang et al, 2006; Zhang and Clark, 2009):
? SHIFT (sh): remove the first word-POS pair
from ?, and push it onto the top of ?;
? REDUCE-UNARY-X (ru-x): pop the top
subtree from ?, construct a new unary node
labeled with X for the subtree, then push the
new subtree back onto ?. The head of the
new subtree is inherited from its child;
? REDUCE-BINARY-{L/R}-X (rl/rr-x): pop
the top two subtrees from ?, combine them
into a new tree with a node labeled with X,
then push the new subtree back onto ?. The
left (L) and right (R) versions of the action
indicate whether the head of the new subtree
is inherited from its left or right child.
With these actions, our parser can process trees
with unary and binary branches easily. For exam-
ple, in Figure 1, for the input sentence w
0
w
1
w
2
and its POS tags abc, our parser can construct t-
wo parse trees using action sequences given below
these trees. However, parse trees in Treebanks of-
ten contain an arbitrary number of branches. To
734
Type Feature Templates
unigrams
p
0
tc, p
0
wc, p
1
tc, p
1
wc, p
2
tc
p
2
wc, p
3
tc, p
3
wc, q
0
wt, q
1
wt
q
2
wt, q
3
wt, p
0l
wc, p
0r
wc
p
0u
wc, p
1l
wc, p
1r
wc, p
1u
wc
bigrams
p
0
wp
1
w, p
0
wp
1
c, p
0
cp
1
w, p
0
cp
1
c
p
0
wq
0
w, p
0
wq
0
t, p
0
cq
0
w, p
0
cq
0
t
q
0
wq
1
w, q
0
wq
1
t, q
0
tq
1
w, q
0
tq
1
t
p
1
wq
0
w, p
1
wq
0
t, p
1
cq
0
w, p
1
cq
0
t
trigrams
p
0
cp
1
cp
2
c, p
0
wp
1
cp
2
c, p
0
cp
1
wq
0
t
p
0
cp
1
cp
2
w, p
0
cp
1
cq
0
t, p
0
wp
1
cq
0
t
p
0
cp
1
wq
0
t, p
0
cp
1
cq
0
w
Table 1: Baseline features, where p
i
represents the
i
th
subtree in the stack ? and q
i
denotes the i
th
item in the queue ?. w refers to the head lexicon,
t refers to the head POS, and c refers to the con-
stituent label. p
il
and p
ir
refer to the left and right
child for a binary subtree p
i
, and p
iu
refers to the
child of a unary subtree p
i
.
process such trees, we employ binarization and
debinarization processes described in Zhang and
Clark (2009) to transform multi-branch trees into
binary-branch trees and restore the generated bi-
nary trees back to their original forms.
2.2 Modeling, Training and Decoding
To determine which action t ? T should the parser
perform at a state s ? S, we use a linear model to
score each possible ?s, t? combination:
score(s, t) = ~w ? ?(s, t) =
?
i
w
i
f
i
(s, t) (1)
where ?(s, t) is the feature function used for map-
ping a state-action pair into a feature vector, and
~w is the weight vector. The score of a parser state
s is the sum of the scores for all state-action pairs
in the transition path from the initial state to the
current state. Table 1 lists the feature templates
used in our baseline parser, which is adopted from
Zhang and Clark (2009). To train the weight vec-
tor ~w, we employ the averaged perceptron algo-
rithm with early update (Collins and Roark, 2004).
We employ the beam search decoding algorith-
m (Zhang and Clark, 2009) to balance the trade-
off between accuracy and efficiency. Algorithm
1 gives details of the process. In the algorithm,
we maintain a beam (sometimes called agenda)
to keep k best states at each step. The first beam
0
Algorithm 1 Beam-search Constituent Parsing
Input: A POS-tagged sentence, beam size k.
Output: A constituent parse tree.
1: beam
0
? {s
0
} . initialization
2: i? 0 . step index
3: loop
4: P ? {} . a priority queue
5: while beam
i
is not empty do
6: s? POP(beam
i
)
7: for all possible t ? T do
8: s
new
? apply t to s
9: score s
new
with E.q (1)
10: insert s
new
into P
11: beam
i+1
? k best states of P
12: s
best
? best state in beam
i+1
13: if s
best
? S
t
then
14: return s
best
15: i? i+ 1
is initialized with the initial state s
0
(line 1). At
step i, each of the k states in beam
i
is extended
by applying all possible actions (line 5-10). For
all newly generated states, only the k best states
are preserved for beam
i+1
(line 11). The decod-
ing process repeats until the highest scored state in
beam
i+1
reaches a terminal state (line 12-14).
3 Joint POS Tagging and Parsing with
Non-local Features
To address the drawbacks of the standard
transition-based constituent parsing model (de-
scribed in Section 1), we propose a model to joint-
ly solve POS tagging and constituent parsing with
non-local features.
3.1 Joint POS Tagging and Parsing
POS tagging is often taken as a preliminary step
for transition-based constituent parsing, therefore
the accuracy of POS tagging would greatly affec-
t parsing performance. In our experiment (de-
scribed in Section 4.2), parsing accuracy would
decrease by 8.5% in F
1
in Chinese parsing when
using automatically generated POS tags instead of
gold-standard ones. To tackle this issue, we inte-
grate POS tagging into the transition-based con-
stituent parsing process and jointly optimize these
two processes simultaneously. Inspired from Ha-
tori et al (2011), we modify the sh action by as-
signing a POS tag for the word when it is shifted:
? SHIFT-X (sh-x): remove the first word from
735
?, assign POS tag X to the word and push it
onto the top of ?.
With such an action, POS tagging becomes a nat-
ural part of transition-based parsing. However,
some feature templates in Table 1 become unavail-
able, because POS tags for the look-ahead words
are not specified yet under the joint framework.
For example, for the template q
0
wt , the POS tag
of the first word q
0
in the queue ? is required, but
it is not specified yet at the present state.
To overcome the lack of look-ahead POS tags,
we borrow the concept of delayed features origi-
nally developed for dependency parsing (Hatori et
al., 2011). Features that require look-ahead POS
tags are defined as delayed features. In these fea-
tures, look-ahead POS tags are taken as variables.
During parsing, delayed features are extracted and
passed from one state to the next state. When a
sh-x action is performed, the look-ahead POS
tag of some delayed features is specified, there-
fore these delayed features can be transformed in-
to normal features (by replacing variable with the
newly specified POS tag). The remaining delayed
features will be transformed similarly when their
look-ahead POS tags are specified during the fol-
lowing parsing steps.
3.2 State Alignment
Assuming an input sentence contains n words, in
order to reach a terminal state, the initial state re-
quires n sh-x actions to consume all words in ?,
and n ? 1 rl/rr-x actions to construct a com-
plete parse tree by consuming all the subtrees in
?. However, ru-x is a very special action. It on-
ly constructs a new unary node for the subtree on
top of ?, but does not consume any items in ? or
?. As a result, the number of ru-x actions varies
among terminal states for the same sentence. For
example, the parse tree in Figure 1a contains no
ru-x action, while the parse tree for the same in-
put sentence in Figure 1b contains four ru-x ac-
tions. This makes the lengths of complete action
sequences very different, and the parsing model
has to disambiguate among terminal states with
varying action sizes. Zhu et al (2013) proposed a
padding method to align terminal states containing
different number of actions. The idea is to append
some IDLE actions to terminal states with shorter
action sequence, and make sure all terminal states
contain the same number of actions (including I-
DLE actions).
Algorithm 2 Beam-search with State Alignment
Input: A word-segmented sentence, beam size k.
Output: A constituent parse tree.
1: beam
0
? {s
0
} . initialization
2: for i? 0 to 2n? 1 do . n is sentence length
3: P
0
? {}, P
1
? {} . two priority queues
4: while beam
i
is not empty do
5: s? POP(beam
i
)
6: for t ? {sh-x,rl-x,rr-x} do
7: s
new
? apply t to s
8: score s
new
with E.q (1)
9: insert s
new
into P
0
10: for all state s in P
0
do
11: for all possible t ? {ru-x} do
12: s
new
? apply t to s
13: score s
new
with E.q (1)
14: insert s
new
into P
1
15: insert all states of P
1
into P
0
16: beam
i+1
? k best states of P
0
17: return the best state in beam
2n?1
We propose a novel method to align states dur-
ing the parsing process instead of just aligning ter-
minal states like Zhu et al (2013). We classify all
the actions into two groups according to whether
they consume items in ? or ?. sh-x, rl-x, and
rr-x belong to consuming actions, and ru-x be-
longs to non-consuming action. Algorithm 2 gives
the details of our method. It is based on the beam
search decoding algorithm described in Algorith-
m 1. Different from Algorithm 1, Algorithm 2 is
guaranteed to perform 2n? 1 parsing steps for an
input sentence containing n words (line 2), and
divides each parsing step into two parsing phas-
es. In the first phase (line 4-9), each of the k s-
tates in beam
i
is extended by consuming action-
s. In the second phase (line 10-14), each of the
newly generated states is further extended by non-
consuming actions. Then, all these states extend-
ed by both consuming and non-consuming action-
s are considered together (line 15), and only the
k highest-scored states are preserved for beam
i+1
(line 16). After these 2n ? 1 parsing steps, the
highest scored state in beam
2n?1
is returned as
the final result (line 17). Figure 2 shows the states
aligning process for the two trees in Figure 1. We
find that our new method aligns states with their
ru-x extensions in the same beam, therefore the
parsing model could make decisions on whether
using ru-x actions or not within local decision
736
s0
a
0,1
b
1,2
A
0,2
c
2,3
B
0,3
T
0
C
0,1
b
1,2
D
1,2
A
0,2
E
0,2
c
2,3
F
2,3
B
0,3
T
1
beam
0
beam
1
beam
2
beam
3
beam
4
beam
5
Figure 2: State alignment for the two trees in Fig-
ure 1, where s
0
is the initial state, T
0
and T
1
are
terminal states corresponding to the two trees in
Figure 1. For clarity, we represent each state as a
rectangle with the label of top subtree in the stack
?. We also denote sh-x with?, ru-x with ? or
?, rl-x with?, and rr-x with?.
beams.
3.3 Feature Extension
One advantage of transition-based constituen-
t parsing is that it is capable of incorporating ar-
bitrarily complex structural features from the al-
ready constructed subtrees in ? and unprocessed
words in ?. However, all the feature templates
given in Table 1 are just some simple structural
features. To further improve the performance of
our transition-based constituent parser, we con-
sider two group of complex structural features:
non-local features (Charniak and Johnson, 2005;
Collins and Koo, 2005) and semi-supervised word
cluster features (Koo et al, 2008).
Table 2 lists all the non-local features we want
to use. These features have been proved very help-
ful for constituent parsing (Charniak and Johnson,
2005; Collins and Koo, 2005). But almost all pre-
vious work considered non-local features only in
parse reranking frameworks. Instead, we attempt
to extract non-local features from newly construct-
ed subtrees during the decoding process as they
become incrementally available and score newly
generated parser states with them. One difficul-
ty is that the subtrees built by our baseline pars-
er are binary trees (only the complete parse tree
is debinarized into its original multi-branch form),
but most of the non-local features need to be ex-
tracted from their original multi-branch forms. To
resolve this conflict, we integrate the debinariza-
tion process into the parsing process, i.e., when a
(Collins and Koo, 2005) (Charniak and Johnson, 2005)
Rules CoPar HeadTree
Bigrams CoLenPar
Grandparent Rules RightBranch
Grandparent Bigrams Heavy
Lexical Bigrams Neighbours
Two-level Rules NGramTree
Two-level Bigrams Heads
Trigrams Wproj
Head-Modifiers Word
Table 2: Non-local features for constituent pars-
ing.
new subtree is constructed during parsing, we de-
binarize it immediately if it is not rooted with an
intermediate node
1
. The other subtrees for sub-
sequent parsing steps will be built based on these
debinarized subtrees. After the modification, our
parser can extract non-local features incrementally
during the parsing process.
Semi-supervised word cluster features have
been successfully applied to many NLP tasks
(Miller et al, 2004; Koo et al, 2008; Zhu et
al., 2013). Here, we adopt such features for our
transition-based constituent parser. Given a large-
scale unlabeled corpus (word segmentation should
be performed), we employ the Brown cluster al-
gorithm (Liang, 2005) to cluster all words into a
binary tree. Within this binary tree, words ap-
pear as leaves, left branches are labeled with 0 and
right branches are labeled with 1. Each word can
be uniquely identified by its path from the root,
and represented as a bit-string. By using various
length of prefixes of the bit-string, we can produce
word clusters of different granularities (Miller et
al., 2004). Inspired from Koo et al (2008), we
employ two types of word clusters: (1) taking 4
bit-string prefixes of word clusters as replacements
of POS tags, and (2) taking 8 bit-string prefixes as
replacements of words. Using these two types of
clusters, we construct semi-supervised word clus-
ter features by mimicking the template structure of
the original baseline features in Table 1.
4 Experiment
4.1 Experimental Setting
We conducted experiments on the Penn Chinese
Treebank (CTB) version 5.1 (Xue et al, 2005):
Articles 001-270 and 400-1151 were used as the
training set, Articles 301-325 were used as the
development set, and Articles 271-300 were used
1
Intermediate nodes are produced by binarization process.
737
as the test set. Standard corpus preparation step-
s were performed before our experiments: emp-
ty nodes and functional tags were removed, and
the unary chains were collapsed to single unary
rules as Harper and Huang (2011). To build word
clusters, we used the unlabeled Chinese Gigaword
(LDC2003T09) and conducted Chinese word seg-
mentation using a CRF-based segmenter.
We used EVALB
2
tool to evaluate parsing per-
formance. The metrics include labeled precision
(LP ), labeled recall (LR), bracketing F
1
and POS
tagging accuracy. We set the beam size k to 16,
which brings a good balance between efficiency
and accuracy. We tuned the optimal number of
iterations of perceptron training algorithm on the
development set.
4.2 Pipeline Approach vs Joint POS Tagging
and Parsing
In this subsection, we conducted some experi-
ments to illustrate the drawbacks of the pipeline
approach and the advantages of our joint approach.
We built three parsing systems: Pipeline-Gold
system is our baseline parser (described in Sec-
tion 2) taking gold-standard POS tags as input;
Pipeline system is our baseline parser taking as
input POS tags automatically assigned by Stan-
ford POS Tagger
3
; and JointParsing system is
our joint POS tagging and transition-based pars-
ing system described in subsection 3.1. We trained
these three systems on the training set and evalu-
ated them on the development set. The second,
third and forth rows in Table 3 show the parsing
performances. We can see that the parsing F
1
de-
creased by about 8.5 percentage points in F
1
score
when using automatically assigned POS tags in-
stead of gold-standard ones, and this shows that
the pipeline approach is greatly affected by the
quality of its preliminary POS tagging step. Af-
ter integrating the POS tagging step into the pars-
ing process, our JointParsing system improved the
POS tagging accuracy to 94.8% and parsing F
1
to 85.8%, which are significantly better than the
Pipeline system. Therefore, the joint parsing ap-
proach is much more effective for transition-based
constituent parsing.
4.3 State Alignment Evaluation
We built two new systems to verify the effective-
ness of our state alignment strategy proposed in
2
http://nlp.cs.nyu.edu/evalb/
3
http://nlp.stanford.edu/downloads/tagger.shtml
System LP LR F
1
POS
Pipeline-Gold 92.2 92.5 92.4 100
Pipeline 83.9 83.8 83.8 93.0
JointParsing 85.1 86.6 85.8 94.8
Padding 85.4 86.4 85.9 94.8
StateAlign 86.9 85.9 86.4 95.2
Nonlocal 88.0 86.5 87.2 95.3
Cluster 89.0 88.3 88.7 96.3
Nonlocal&Cluster 89.4 88.7 89.1 96.2
Table 3: Parsing performance on Chinese devel-
opment set.
Subsection 3.2. The first system Padding extend-
s our JointParsing system by aligning terminal s-
tates with the padding strategy proposed in Zhu et
al. (2013), and the second system StateAlign ex-
tends the JointParsing system with our state align-
ment strategy. The fifth and sixth rows of Table 3
give the performances of these two systems. Com-
pared with the JointParsing system which does not
employ any alignment strategy, the Padding sys-
tem only achieved a slight improvement on pars-
ing F
1
score, but no improvement on POS tag-
ging accuracy. In contrast, our StateAlign system
achieved an improvement of 0.6% on parsing F
1
s-
core and 0.4% on POS tagging accuracy. All these
results show us that our state alignment strategy is
more helpful for beam-search decoding.
4.4 Feature Extension Evaluation
In this subsection, we examined the usefulness
of the new non-local features and the semi-
supervised word cluster features described in Sub-
section 3.3. We built three new parsing system-
s based on the StateAlign system: Nonlocal sys-
tem extends the feature set of StateAlign system
with non-local features, Cluster system extends
the feature set with semi-supervised word cluster
features, and Nonlocal&Cluster system extend the
feature set with both groups of features. Parsing
performances of the three systems are shown in
the last three rows of Table 3. Compared with the
StateAlign system which takes only the baseline
features, the non-local features improved parsing
F
1
by 0.8%, while the semi-supervised word clus-
ter features result in an improvement of 2.3% in
parsing F
1
and an 1.1% improvement on POS tag-
ging accuracy. When integrating both groups of
features, the final parsing F
1
reaches 89.1%. Al-
738
Type System LP LR F
1
POS
Our Systems
Pipeline 80.0 80.3 80.1 94.0
JointParsing 82.4 83.0 82.7 95.1
Padding 82.7 83.6 83.2 95.1
StateAlign 84.2 82.9 83.6 95.5
Nonlocal 85.6 84.2 84.9 95.9
Cluster 85.2 84.5 84.9 95.8
Nonlocal&Cluster 86.6 85.9 86.3 96.0
Single Systems
Petrov and Klein (2007) 81.9 84.8 83.3 -
Zhu et al (2013) 82.1 84.3 83.2 -
Reranking Systems
Charniak and Johnson (2005)
?
80.8 83.8 82.3 -
Wang and Zong (2011) - - 85.7 -
Semi-supervised Systems Zhu et al (2013) 84.4 86.8 85.6 -
Table 4: Parsing performance on Chinese test set.
?
Huang (2009) adapted the parse reranker to CTB5.
l these results show that both the non-local fea-
tures and the semi-supervised features are helpful
for our transition-based constituent parser.
4.5 Final Results on Test Set
In this subsection, we present the performances of
our systems on the CTB test set. The correspond-
ing results are listed in the top rows of Table 4.
We can see that all these systems maintain a simi-
lar relative relationship as they do on the develop-
ment set, which shows the stability of our systems.
To further illustrate the effectiveness of our
systems, we compare them with some state-of-
the-art systems. We group parsing systems into
three categories: single systems, reranking sys-
tems and semi-supervised systems. Our Pipeline,
JointParsing, Padding, StateAlign and Nonlocal
systems belong to the category of single system-
s, because they don?t utilize any extra process-
ing steps or resources. Our Cluster and Nonlo-
cal&Cluster systems belong to semi-supervised
systems, because both of them have employed
semi-supervised word cluster features. The pars-
ing performances of state-of-the-art systems are
shown in the bottom rows of Table 4. We can see
that the final F
1
of our Nonlocal system reached
84.9%, and it outperforms state-of-the-art single
systems by more than 1.6%. As far as we know,
this is the best result on the CTB test set acquired
by single systems. Our Nonlocal&Cluster sys-
tem further improved the parsing F
1
to 86.3%,
and it outperforms all reranking systems and semi-
supervised systems. To our knowledge, this is the
System F
1
Huang and Harper (2009) 85.2
Nonlocal&Cluster 87.1
Table 5: Parsing performance based on CTB 6.
best reported performance in Chinese parsing.
All previous experiments were conducted on
CTB 5. To check whether more labeled data can
further improve our parsing system, we evaluat-
ed our Nonlocal&Cluster system on the Chinese
TreeBank version 6.0 (CTB6), which is a super
set of CTB5 and contains more annotated data.
We used the same development set and test set
as CTB5, and took all the remaining data as the
new training set. Table 5 shows the parsing per-
formances on CTB6. Our Nonlocal&Cluster sys-
tem improved the final F
1
to 87.1%, which is 1.9%
better than the state-of-the-art performance on CT-
B6 (Huang and Harper, 2009). Compared with it-
s performance on CTB5 (in Table 4), our Nonlo-
cal&Cluster system also got 0.8% improvemen-
t. All these results show that our approach can
become more powerful when given more labeled
training data.
4.6 Error Analysis
To better understand the linguistic behavior of
our systems, we employed the berkeley-parser-
analyser tool
4
(Kummerfeld et al, 2013) to cat-
egorize the errors. Table 6 presents the average
4
http://code.google.com/p/berkeley-parser-analyser/
739
System
NP
Int.
Unary
1-Word
Span
Coord
Mod.
Attach
Verb
Args
Diff
Label
Clause
Attach
Noun
Edge
Worst 1.75 0.74 0.44 0.49 0.39 0.37 0.29 0.15 0.14
Pipeline
JointParsing
Padding
StateAlign
Nonlocal
Cluster
Nonlocal&Cluster
Best 1.33 0.42 0.28 0.29 0.19 0.21 0.17 0.07 0.09
Table 6: Parse errors on Chinese test set. The shaded area of each bar indicates average number of that
error type per sentence, and the completely full bar indicates the number in the Worst row.
System VV?NN NN?VV DEC?DEG JJ?NN NR?NN DEG?DEC NN?NR NN?JJ
Worst 0.26 0.18 0.15 0.09 0.08 0.07 0.06 0.05
Pipeline
JointParsing
Padding
StateAlign
Nonlocal
Cluster
Nonlocal&Cluster
Best 0.14 0.10 0.03 0.07 0.05 0.03 0.03 0.02
Table 7: POS tagging error patterns on Chinese test set. For each error pattern, the left hand side tag is
the gold-standard tag, and the right hand side is the wrongly assigned tag.
number of errors for each error type by our pars-
ing systems. We can see that almost all the Worst
numbers are produced by the Pipeline system. The
JointParsing system reduced errors of all types
produced by the Pipeline system except for the
coordination error type (Coord). The StateAlign
system corrected a lot of the NP-internal errors
(NP Int.). The Nonlocal system and the Cluster
system produced similar numbers of errors for al-
l error types. The Nonlocal&Cluster system pro-
duced the Best numbers for all the error types. NP-
internal errors are still the most frequent error type
in our parsing systems.
Table 7 presents the statistics of frequent POS
tagging error patterns. We can see that JointPars-
ing system disambiguates {VV, NN} and {DEC,
DEG} better than Pipeline system, but cannot deal
with the NN?JJ pattern very well. StateAlign
system got better results in most of the patterns,
but cannot disambiguate {NR, NN} well. Non-
local&Cluster system got the best results in dis-
ambiguating the most ambiguous POS tag pairs of
{VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, N-
R}.
5 Related Work
Joint POS tagging with parsing is not a new idea.
In PCFG-based parsing (Collins, 1999; Charniak,
2000; Petrov et al, 2006), POS tagging is consid-
ered as a natural step of parsing by employing lex-
ical rules. For transition-based parsing, Hatori et
al. (2011) proposed to integrate POS tagging with
dependency parsing. Our joint approach can be
seen as an adaption of Hatori et al (2011)?s ap-
proach for constituent parsing. Zhang et al (2013)
proposed a transition-based constituent parser to
process an input sentence from the character level.
However, manual annotation of the word-internal
structures need to be added to the original Tree-
bank in order to train such a parser.
Non-local features have been successfully used
for constituent parsing (Charniak and Johnson,
2005; Collins and Koo, 2005; Huang, 2008).
However, almost all of the previous work use non-
local features at the parse reranking stage. The
reason is that the single-stage chart-based parser
cannot use non-local structural features. In con-
trast, the transition-based parser can use arbitrari-
ly complex structural features. Therefore, we can
concisely utilize non-local features in a single-
740
stage parsing system.
6 Conclusion
In this paper, we proposed three improvements to
transition-based constituent parsing for Chinese.
First, we incorporated POS tagging into transition-
based constituent parsing to resolve the error prop-
agation problem of the pipeline approach. Second,
we proposed a state alignment strategy to align
competing decision sequences that have different
number of actions. Finally, we enhanced our pars-
ing model by enlarging the feature set with non-
local features and semi-supervised word cluster
features. Experimental results show that all these
methods improved the parsing performance sub-
stantially, and the final performance of our parsing
system outperformed all state-of-the-art systems.
Acknowledgments
We thank three anonymous reviewers for their
cogent comments. This work is funded by the
DAPRA via contract HR0011-11-C-0145 entitled
/Linguistic Resources for Multilingual Process-
ing0. All opinions expressed here are those of the
authors and do not necessarily reflect the views of
DARPA.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139. Asso-
ciation for Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?70.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 1999. HEAD-DRIVEN STATISTI-
CAL MODELS FOR NATURAL LANGUAGE PARS-
ING. Ph.D. thesis, University of Pennsylvania.
Mary Harper and Zhongqiang Huang. 2011. Chinese
statistical parsing. Handbook of Natural Language
Processing and Machine Translation.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 1216?1224, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations
across languages. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 832?841.
Association for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Ling-Ya Huang. 2009. Improve chinese parsing with
max-ent reranking parser. Master Project Report,
Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-
ran, and Dan Klein. 2013. An empirical examina-
tion of challenges in chinese parsing. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 98?103, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342. Citeseer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computation-
al Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125?132. Association for Com-
putational Linguistics.
741
Zhiguo Wang and Chengqing Zong. 2011. Parse r-
eranking based on higher-order lexical dependen-
cies. In IJCNLP, pages 1251?1259.
Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.
2006. A fast, accurate deterministic parser for chi-
nese. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computation-
al Linguistics, pages 425?432. Association for Com-
putational Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207?238.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162?171. Association for Computational Linguistic-
s.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 125?134, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434?443, Sofia, Bulgaria, August. Association for
Computational Linguistics.
742
Large-scale Word Alignment Using Soft Dependency Cohesion 
Constraints 
Zhiguo Wang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{zgwang, cqzong}@nlpr.ia.ac.cn 
  
 
 
Abstract 
Dependency cohesion refers to the 
observation that phrases dominated by 
disjoint dependency subtrees in the source 
language generally do not overlap in the 
target language. It has been verified to be a 
useful constraint for word alignment. 
However, previous work either treats this 
as a hard constraint or uses it as a feature in 
discriminative models, which is ineffective 
for large-scale tasks. In this paper, we take 
dependency cohesion as a soft constraint, 
and integrate it into a generative model for 
large-scale word alignment experiments. 
We also propose an approximate EM 
algorithm and a Gibbs sampling algorithm 
to estimate model parameters in an 
unsupervised manner. Experiments on 
large-scale Chinese-English translation 
tasks demonstrate that our model achieves 
improvements in both alignment quality 
and translation quality. 
1 Introduction 
Word alignment is the task of identifying word 
correspondences between parallel sentence pairs. 
Word alignment has become a vital component of 
statistical machine translation (SMT) systems, 
since it is required by almost all state-of-the-art 
SMT systems for the purpose of extracting phrase 
tables or even syntactic transformation rules 
(Koehn et al, 2007; Galley et al, 2004). 
During the past two decades, generative word 
alignment models such as the IBM Models (Brown 
et al, 1993) and the HMM model (Vogel et al, 
1996) have been widely used, primarily because 
they are trained on bilingual sentences in an 
unsupervised manner and the implementation is 
freely available in the GIZA++ toolkit (Och and 
Ney, 2003). However, the word alignment quality 
of generative models is still far from satisfactory 
for SMT systems. In recent years, discriminative 
alignment models incorporating linguistically 
motivated features have become increasingly 
popular (Moore, 2005; Taskar et al, 2005; Riesa 
and Marcu, 2010; Saers et al, 2010; Riesa et al, 
2011). These models are usually trained with 
manually annotated parallel data. However, when 
moving to a new language pair, large amount of 
hand-aligned data are usually unavailable and 
expensive to create.  
A more practical way to improve large-scale 
word alignment quality is to introduce syntactic 
knowledge into a generative model and train the 
model in an unsupervised manner (Wu, 1997; 
Yamada and Knight, 2001; Lopez and Resnik, 
2005; DeNero and Klein, 2007; Pauls et al, 2010). 
In this paper, we take dependency cohesion (Fox, 
2002) into account, which assumes phrases 
dominated by disjoint dependency subtrees tend 
not to overlap after translation. Instead of treating 
dependency cohesion as a hard constraint (Lin and 
Cherry, 2003) or using it as a feature in 
discriminative models (Cherry and Lin, 2006b), we 
treat dependency cohesion as a distortion 
constraint, and integrate it into a modified HMM 
word alignment model to softly influence the 
probabilities of alignment candidates.  We also 
propose an approximate EM algorithm and an 
explicit Gibbs sampling algorithm to train the 
model in an unsupervised manner. Experiments on 
a large-scale Chinese-English translation task 
demonstrate that our model achieves 
improvements in both word alignment quality and 
machine translation quality. 
The remainder of this paper is organized as 
follows: Section 2 introduces dependency cohesion 
291
Transactions of the Association for Computational Linguistics, 1 (2013) 291?300. Action Editor: Chris Callison-Burch.
Submitted 5/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
constraint for word alignment. Section 3 presents 
our generative model for word alignment using 
dependency cohesion constraint. Section 4 
describes algorithms for parameter estimation. We 
discuss and analyze the experiments in Section 5. 
Section 6 gives the related work. Finally, we 
conclude this paper and mention future work in 
Section 7. 
2 Dependency Cohesion Constraint for 
Word Alignment 
Given a source (foreign) sentence ?1
? = ?1, ?2, ? , ?? 
and a target (English) sentence ?1
? = ?1, ?2, ? , ?? , 
the alignment ? between ?1
?
and ?1
?  is defined as a 
subset of the Cartesian product of word positions: 
? ? {(?, ?): ? = 1,? , ?; ? = 1,? , ?} 
When given the source side dependency tree ?, we 
can project dependency subtrees in ?  onto the 
target sentence through the alignment ? . 
Dependency cohesion assumes projection spans of 
disjoint subtrees tend not to overlap. Let ?(??) be 
the subtree of ? rooted at ??, we define two kinds 
of projection span for the node ??: subtree span and 
head span. The subtree span is the projection span 
of the total subtree ?(??), while the head span is 
the projection span of the node ?? itself. Following 
Fox (2002) and Lin and Cherry (2003), we 
consider two types of dependency cohesion: head-
modifier cohesion and modifier-modifier cohesion. 
Head-modifier cohesion is defined as the subtree 
span of a node does not overlap with the head span 
of its head (parent) node, while modifier-modifier 
cohesion is defined as subtree spans of two nodes 
under the same head node do not overlap each 
other. We call a situation where cohesion is not 
maintained crossing. 
Using the dependency tree in Figure 1 as an 
example, given the correct alignment ?R?, the 
subtree span of ??/have? is [8, 14] , and the head 
span of its head node ???/one of? is [3, 4]. They 
do not overlap each other, so the head-modifier 
cohesion is maintained. Similarly, the subtree span 
of ???/few? is [6, 6], and it does not overlap the 
subtree span of  ??/have?, so a modifier-modifier 
cohesion is maintained. However, when ?R? is 
replaced with the incorrect alignment ?W?, the 
subtree span of ??/have? becomes [3, 14], and it 
overlaps the head span of its head ???/one of?, 
so a head-modifier crossing occurs. Meanwhile, 
the subtree spans of the two nodes ??/have? and 
??? /few? overlap each other, so a modifier-
modifier crossing occurs.  
 
Fox (2002) showed that dependency cohesion is 
generally maintained between English and French. 
To test how well this assumption holds between 
Chinese and English, we measure the dependency 
cohesion between the two languages with a 
manually annotated bilingual Chinese-English data 
set of 502 sentence pairs 1 . We use the head-
modifier cohesion percentage (HCP) and the 
modifier-modifier cohesion percentage (MCP) to 
measure the degree of cohesion in the corpus. HCP 
(or MCP) is used for measuring how many head-
modifier (or modifier-modifier) pairs are actually 
cohesive. Table 1 lists the relative percentages in 
both Chinese-to-English (ch-en, using Chinese side 
dependency trees) and English-to-Chinese (en-ch, 
using English side dependency trees) directions. 
As we see from Table 1, dependency cohesion is 
                                                          
1 The data set is the development set used in Section 5. 
?
?
? ? ?
?
? ??
?
?
?
?
?
??
?
Australia
is
one
of
the
few
countries
that
have
diplomatic
relations
with
North
Korea
.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
 
Figure 1: A Chinese-English sentence pair 
including the word alignments and the Chinese 
side dependency tree. The Chinese and English 
words are listed horizontally and vertically, 
respectively. The black grids are gold-standard 
alignments. For the Chinese word ??/have?, 
we give two alignment positions, where ?R? is 
the correct alignment and ?W? is the incorrect 
alignment. 
292
generally maintained between Chinese and English. 
So dependency cohesion would be helpful for 
word alignment between Chinese and English. 
However, there are still a number of crossings. If 
we restrict alignment space with a hard cohesion 
constraint, the correct alignments that result in 
crossings will be ruled out directly. In the next 
section, we describe an approach to integrating 
dependency cohesion constraint into a generative 
model to softly influence the probabilities of 
alignment candidates. We show that our new 
approach addresses the shortcomings of using 
dependency cohesion as a hard constraint. 
 
3 A Generative Word Alignment Model 
with Dependency Cohesion Constraint  
The most influential generative word alignment 
models are the IBM Models 1-5 and the HMM 
model (Brown et al, 1993; Vogel et al, 1996; Och 
and Ney, 2003). These models can be classified 
into sequence-based models (IBM Models 1, 2 and 
HMM) and fertility-based models (IBM Models 3, 
4 and 5). The sequence-based model is easier to 
implement, and recent experiments have shown 
that appropriately modified sequence-based model 
can produce comparable performance with 
fertility-based models (Lopez and Resnik, 2005; 
Liang et al, 2006; DeNero and Klein, 2007; Zhao 
and Gildea, 2010; Bansal et al, 2011). So we built 
a generative word alignment model with 
dependency cohesion constraint based on the 
sequence-based model. 
3.1 The Sequence-based Alignment Model 
According to Brown et al (1993) and Och and Ney 
(2003), the sequence-based model is built as a 
noisy channel model, where the source sentence ?1
? 
and the alignment ?1
? are generated conditioning on 
the target sentence ?1
? . The model assumes each 
source word is assigned to exactly one target word, 
and defines an asymmetric alignment for the 
sentence pair as ?1
? = ?1, ?2, ? , ?? , ? , ??, where each 
?? ? [0, ?] is an alignment from the source position j 
to the target position ?? , ?? = 0  means ??  is not 
aligned with any target words. The sequence-based 
model divides alignment procedure into two stages 
(distortion and translation) and factors as: 
?(?1
?, ?1
?|?1
? ) = ? ??(??|???1, ?)??(??|???)
?
?=1       (1) 
where ??  is the distortion model and ??  is the 
translation model. IBM Models 1, 2 and the HMM 
model all assume the same translation model 
 ??(??|???) . However, they use three different 
distortion models. IBM Model 1 assumes a 
uniform distortion probability 1/(I+1), IBM Model 
2 assumes ??(??|?) that depends on word position j 
and HMM model assumes ??(??|???1, ?)  that 
depends on the previous alignment ???1. Recently, 
tree distance models (Lopez and Resnik, 2005; 
DeNero and Klein, 2007) formulate the distortion 
model as ??(??|???1, ?) , where the distance 
between ??  and ???1  are calculated by walking 
through the phrase (or dependency) tree T. 
3.2 Proposed Model 
To integrate dependency cohesion constraint into a 
generative model, we refine the sequence-based 
model in two ways with the help of the source side 
dependency tree ??.  
First, we design a new word alignment order. In 
the sequence-based model, source words are 
aligned from left to right by taking source sentence 
as a linear sequence. However, to apply 
dependency cohesion constraint, the subtree span 
of a head node is computed based on the 
alignments of its children, so children must be 
aligned before the head node. Riesa and Marcu 
(2010) propose a hierarchical search procedure to 
traverse all nodes in a phrase structure tree. 
Similarly, we define a bottom-up topological order 
(BUT-order) to traverse all words in the source 
side dependency tree ?? . In the BUT-order, tree 
nodes are aligned bottom-up with ?? as a backbone. 
For all children under the same head node, left 
children are aligned from right to left, and then 
right children are aligned from left to right. For 
example, the BUT-order for the following 
dependency tree is  ?C B E F D A H G?.  
 
A HGFEDCB
 
ch-en en-ch 
HCP MCP HCP MCP 
88.43 95.82 81.53 91.62 
Table 1: Cohesion percentages (%) of a manually 
annotated data set between Chinese and English. 
293
For the sake of clarity, we define a function to 
map all nodes in ??  into their BUT-order, and 
notate it as BUT(??) = ?1, ?2, ? , ?? , ? , ?? , where ?? 
means the j-th node in BUT-order is the ??-th word 
in the original source sentence. We arrange 
alignment sequence ?1
?  according the BUT-order 
and notate it as ?[1,?] = ??1 , ? , ??? , ? , ??? , where 
??? is the aligned position for a node ???. We also 
notate the sub-sequence ??? , ? , ???as ?[?,?]. 
Second, we keep the same translation model as 
the sequence-based model and integrate the 
dependency cohesion constraints into the distortion 
model. The main idea is to influence the distortion 
procedure with the dependency cohesion 
constraints. Assume node ??  and node ??  are a 
head-modifier pair in ??, where ?? is the head and 
??  is the modifier. The head-modifier cohesion 
relationship between them is notated as ??,? ?
{????????, ????????} . When the head-modifier 
cohesion is maintained ??,? = ????????, otherwise 
??,? = ???????? . We represent the set of head-
modifier cohesion relationships for all the head-
modifier pairs in ?? as: 
      ? = {??,? | ? ? [1, ?], ? ? [1, ?], ? ? ?,  
    ?? and ?? are a head-modifier pair in ??} 
The set of head-modifier cohesion relationships for 
all the head-modifier pairs taking ??  as the head 
node can be represented as: 
      ?? = {??,? | ? ? [1, ?], ? ? ?,   
               ?? and ?? are a head-modifier pair in ??} 
Obviously, ? = ? ??
?
?=0 . 
Similarly, we assume node ?? and node ?? are a 
modifier-modifier pair in ?? . To avoid repetition, 
we assume ??  is the node sitting at the position 
after  ??  in BUT-order and call ??  as the higher-
order node of the pair. The modifier-modifier 
cohesion relationship between them is notated as 
??,? ? {????????, ????????} . When the modifier-
modifier cohesion is maintained ??,? = ???????? , 
otherwise ??,? = ????????. We represent the set of 
modifier-modifier cohesion relationships for all the 
modifier-modifier pairs in ?? as: 
      ? = {??,? | ? ? [1, ?], ? ? [1, ?], ? ? ?,  
         ?? and ?? are a modifier-modifier pair in ??} 
The set of modifier-modifier cohesion 
relationships for all the modifier-modifier pairs 
taking ??  as the higher-order node can be 
represented as: 
      ?? = {??,? | ? ? [1, ?], ? ? ?,   
                ?? and ?? are a modifier-modifier pair in ??} 
Obviously, ? = ? ??
?
?=0 . 
With the above notations, we formulate the 
distortion probability for a node ???  as 
?? (??? , ??? ,???|?[1,??1]). 
 
According to Eq. (1) and the two improvements, 
we formulated our model as: 
?(?1
?, ?[1,?]|?1
? , ??) = ?(?[1,?], ?,?, ?1
? , |?1
? , ??) 
? ? ?? (??? , ??? ,???|?[1,??1]) ?? (???|????
)??????(??)   
   (2) 
Here, we use the approximation symbol, 
because the right hand side is not guaranteed to 
be normalized. In practice, we only compute 
ratios of these terms, so it is not actually a 
problem. Such model is called deficient (Brown 
et al, 1993), and many successful unsupervised 
models are deficient, e.g., IBM model 3 and 
IBM model 4.  
3.3 Dependency Cohesive Distortion Model 
We assume the distortion procedure is influenced 
by three factors: words distance, head-modifier 
cohesion and modifier-modifier cohesion. 
Therefore, we further decompose the distortion 
model ?? into three terms as follows: 
?? (??? , ??? ,???|?[1,??1]) 
= ? (???|?[1,??1]) ? (???|?[1,?]) ? (???|?[1,?], ???) 
? ??? (???|????1 , ?) ??? (???|?[1,?]) ??? (???|?[1,?]) 
(3) 
where ??? is the words distance term, ??? is  the 
head-modifier cohesion term and ???  is the 
modifier-modifier cohesion term. 
The word distance term ??? has been verified to 
be very useful in the HMM alignment model. 
However, in our model, the word distance is 
calculated based on the previous node in BUT-
order rather than the previous word in the original 
sentence. We follow the HMM word alignment 
model (Vogel et al, 1996) and parameterize ??? in 
terms of the jump width: 
???(?|?
?, ?) =
?(????)
? ?(??????)???
       (4) 
where ?(?) is the count of jump width. 
294
The head-modifier cohesion term ??? is used to 
penalize the distortion probability according to 
relationships between the head node and its 
children (modifiers). Therefore, we define ???  as 
the product of probabilities for all head-modifier 
pairs taking ??? as head node: 
??? (???|?[1,?]) = ? ?? (???,?|??, ????
, ???)???,?????
 (5) 
where ???,? ? {????????, ????????}  is the head-
modifier cohesion relationship between ???  and 
one of its child ?? ,  ??  is the corresponding 
probability, ????
and ???  are the aligned words for 
??? and ??.  
Similarly, the modifier-modifier cohesion term 
???  is used to penalize the distortion probability 
according to relationships between ???  and its 
siblings. Therefore, we define  ??? as the product 
of probabilities for all the modifier-modifier pairs 
taking ??? as the higher-order node: 
??? (???|?[1,?]) = ? ?? (???,?|??, ????
, ???)???,?????
  
 (6) 
where ???,? ? {????????, ????????} is the modifier-
modifier cohesion relationship between  ???  and 
one of its sibling ?? , ??  is the corresponding 
probability, ????
 and ??? are the aligned words for 
??? and ??.  
Both  ?? and ??  in Eq. (5) and Eq. (6) are 
conditioned on three words, which would make 
them very sparse. To cope with this problem, we 
use the word clustering toolkit, mkcls (Och et al, 
1999), to cluster all words into 50 classes, and 
replace the three words with their classes. 
4 Parameter Estimation 
To align sentence pairs with the model in Eq. (2), 
we have to estimate some parameters: ??, ???, ?? 
and ?? . The traditional approach for sequence-
based models uses Expectation Maximization (EM) 
algorithm to estimate parameters. However, in our 
model, it is hard to find an efficient way to sum 
over all the possible alignments, which is required 
in the E-step of EM algorithm. Therefore, we 
propose an approximate EM algorithm and a Gibbs 
sampling algorithm for parameter estimation. 
4.1 Approximate EM Algorithm 
The approximate EM algorithm is similar to the 
training algorithm for fertility-based alignment 
models (Och and Ney, 2003). The main idea is to 
enumerate only a small subset of good alignments 
in the E-step, then collect expectation counts and 
estimate parameters among the small subset in M-
step. Following with Och and Ney (2003), we 
employ neighbor alignments of the Viterbi 
alignment as the small subset. Neighbor 
alignments are obtained by performing one swap 
or move operation over the Viterbi alignment.  
Obtaining the Viterbi alignment itself is not so 
easy for our model. Therefore, we take the Viterbi 
alignment of the sequence-based model (HMM 
model) as the starting point, and iterate the hill-
climbing algorithm (Brown et al, 1993) many 
times to get the best alignment greedily. In each 
iteration, we find the best alignment with Eq. (2) 
among neighbor alignments of the initial point, and 
then make the best alignment as the initial point for 
the next iteration. The algorithm iterates until no 
update could be made. 
4.2 Gibbs Sampling Algorithm 
Gibbs sampling is another effective algorithm for 
unsupervised learning problems. As is described in 
the literatures (Johnson et al, 2007; Gao and 
Johnson, 2008), there are two types of Gibbs 
samplers: explicit and collapsed. An explicit 
sampler represents and samples the model 
parameters in addition to the word alignments, 
while in a collapsed sampler the parameters are 
integrated out and only alignments are sampled. 
Mermer and Sara?lar (2011) proposed a collapsed 
sampler for IBM Model 1. However, their sampler 
updates parameters constantly and thus cannot run 
efficiently on large-scale tasks. Instead, we take 
advantage of explicit Gibbs sampling to make a 
highly parallelizable sampler. Our Gibbs sampler 
is similar to the MCMC algorithm in Zhao and 
Gildea (2010), but we assume Dirichlet priors 
when sampling model parameters and take a 
different sampling approach based on the source 
side dependency tree. 
Our sampler performs a sequence of consecutive 
iterations. Each iteration consists of two sampling 
steps. The first step samples the aligned position 
for each dependency node according to the BUT-
order.  Concretely, when sampling the aligned 
295
position ???
(?+1)
 for node ???  on iteration ?+1,  the 
aligned positions for ?[1,??1] are fixed on the new 
sampling results ?[1,??1]
(?+1)
on iteration ?+1, and the 
aligned positions for ?[?+1,?]  are fixed on the old 
sampling results ?[?+1,?]
(?)
 on iteration ? . Therefore, 
we sample the aligned position ???
(?+1)
 as follows: 
???
(?+1)
  ~   ? (???|?[1,??1]
(?+1)
, ?[?+1,?]
(?)
, ?1
?, ?1
?)
=
? (?1
?, ??????
|?1
? )
? ? (?1
? , ??????
|?1
? )????{0,1,?,?}
 
(7) 
where ??????
= ?[1,??1]
(?+1)
? ??? ? ?[?+1,?]
(?)
, the numerator 
is the probability of aligning ???  with ????
 (the 
alignments for other nodes are fixed at ?[1,??1]
(?+1)
and 
?[?+1,?]
(?)
) calculated with Eq. (2), and the 
denominator is the summation of the probabilities 
of aligning ??? with each target word. The second 
step of our sampler calculates parameters ??, ???, 
??  and ??  using their counts, where all these 
counts can be easily collected during the first 
sampling step. Because all these parameters follow 
multinomial distributions, we consider Dirichlet 
priors for them, which would greatly simplify the 
inference procedure. 
In the first sampling step, all the sentence pairs 
are processed independently. So we can make this 
step parallel and process all the sentence pairs 
efficiently with multi-threads. When using the 
Gibbs sampler for decoding, we just ignore the 
second sampling step and iterate the first sampling 
step many times.  
5 Experiments 
We performed a series of experiments to evaluate 
our model. All the experiments are conducted on 
the Chinese-English language pair. We employ 
two training sets: FBIS and LARGE. The size and 
source corpus of these training sets are listed in 
Table 2. We will use the smaller training set FBIS 
to evaluate the characters of our model and use the 
LARGE training set to evaluate whether our model 
is adaptable for large-scale task. For word 
alignment quality evaluation, we take the hand-
aligned data sets from SSMT20072, which contains 
                                                          
2 http://nlp.ict.ac.cn/guidelines/guidelines-2007-
SSMT(English).doc 
505 sentence pairs in the testing set and 502 
sentence pairs in the development set. Following 
Och and Ney (2003), we evaluate word alignment 
quality with the alignment error rate (AER), where 
lower AER is better. 
Because our model takes dependency trees as 
input, we parse both sides of the two training sets, 
the development set and the testing set with 
Berkeley parser (Petrov et al, 2006), and then 
convert the generated phrase trees into dependency 
trees according to Wang and Zong (2010; 2011). 
Our model is an asymmetric model, so we perform 
word alignment in both forward (Chinese?English) 
and reverse (English?Chinese) directions. 
 
5.1 Effectiveness of Cohesion Constraints 
In Eq. (3), the distortion probability ??  is 
decomposed into three terms: ??? , ???  and ??? . 
To study whether cohesion constraints are effective 
for word alignment, we construct four sub-models 
as follows:  
(1) wd: ?? = ???;  
(2) wd-hc: ?? = ??? ? ???;  
(3) wd-mc: ?? = ??? ? ???; 
(4) wd-hc-mc: ?? = ??? ? ??? ? ???. 
We train these four models with the approximate 
EM and the Gibbs sampling algorithms on the 
FBIS training set. For approximate EM algorithm, 
we first train a HMM model (with 5 iterations of 
IBM model 1 and 5 iterations of HMM model), 
then train these four sub-models with 10 iterations 
of the approximate EM algorithm. For Gibbs 
sampling, we choose symmetric Dirichlet priors 
identically with all hyper-parameters equals 0.0001 
to obtain a sparse Dirichlet prior. Then, we make 
the alignments produced by the HMM model as the 
initial points, and train these sub-models with 20 
iterations of the Gibbs sampling.  
AERs on the development set are listed in Table 
3. We can easily find: 1) when employing the 
head-modifier cohesion constraint, the wd-hc 
model yields better AERs than the wd model; 2) 
Train Set Source Corpus # Words 
FBIS FBIS newswire data Ch: 7.1M 
En: 9.1M 
 
LARGE 
LDC2000T50, LDC2003E14, 
LDC2003E07, LDC2004T07, 
LDC2005T06, LDC2002L27, 
LDC2005T10, LDC2005T34 
 
Ch: 27.6M 
En: 31.8M 
Table 2: The size and the source corpus of the two 
training sets. 
296
when employing the modifier-modifier cohesion 
constraint, the wd-mc model also yields better 
AERs than the wd model; and 3) when employing 
both head-modifier cohesion constraint and 
modifier-modifier cohesion constraint together, the 
wd-hc-mc model yields the best AERs among the 
four sub-models. So both head-modifier cohesion 
constraint and modifier-modifier cohesion 
constraint are helpful for word alignment. Table 3 
also shows that the approximate EM algorithm 
yields better AERs in the forward direction than 
reverse direction, while the Gibbs sampling 
algorithm yields close AERs in both directions. 
 
5.2 Comparison with State-of-the-Art Models 
To show the effectiveness of our model, we 
compare our model with some of the state-of-the-
art models. All the systems are listed as follows: 
1) IBM4: The fertility-based model (IBM model 4) 
which is implemented in GIZA++ toolkit. The 
training scheme is 5 iterations of IBM model 1, 
5 iterations of the HMM model and 10 
iterations of IBM model 4. 
2) IBM4-L0: A modification to the GIZA++ 
toolkit which extends IBM models with ?0 -
norm (Vaswani et al, 2012). The training 
scheme is the same as IBM4. 
3) IBM4-Prior: A modification to the GIZA++ 
toolkit which extends the translation model of 
IBM models with Dirichlet priors (Riley and 
Gildea, 2012). The training scheme is the same 
as IBM4. 
4) Agree-HMM: The HMM alignment model by 
jointly training the forward and reverse models 
(Liang et al, 2006), which is implemented in 
the BerkeleyAligner. The training scheme is 5 
iterations of jointly training IBM model 1 and 5 
iterations of jointly training HMM model. 
5) Tree-Distance: The tree distance alignment 
model proposed in DeNero and Klein (2007), 
which is implemented in the BerkeleyAligner. 
The training scheme is 5 iterations of jointly 
training IBM model 1 and 5 iterations of jointly 
training the tree distance model. 
6) Hard-Cohesion: The implemented ?Cohesion 
Checking Algorithm? (Lin and Cherry, 2003) 
which takes dependency cohesion as a hard 
constraint during beam search word alignment 
decoding. We use the model trained by the 
Agree-HMM system to estimate alignment 
candidates.  
We also build two systems for our soft 
dependency cohesion model: 
7) Soft-Cohesion-EM: the wd-hc-mc sub-model 
trained with the approximate EM algorithm as 
described in sub-section 5.1. 
8) Soft-Cohesion-Gibbs: the wd-hc-mc sub-model 
trained with the Gibbs sampling algorithm as 
described in sub-section 5.1. 
We train all these systems on the FBIS training 
set, and test them on the testing set. We also 
combine the forward and reverse alignments with 
the grow-diag-final-and (GDFA) heuristic (Koehn 
et al, 2007). All AERs are listed in Table 4. We 
find our soft cohesion systems produce better 
AERs than the Hard-Cohesion system as well as 
the other systems. Table 5 gives the head-modifier 
cohesion percentage (HCP) and the modifier-
modifier cohesion percentage (MCP) of each 
system. We find HCPs and MCPs of our soft 
cohesion systems are much closer to the gold-
standard alignments.  
 
To evaluate whether our model is adaptable for 
large-scale task, we retrained these systems using 
the LARGE training set. AERs on the testing set 
are listed in Table3 6. Compared with Table 4, we 
                                                          
3 Tree-Distance system requires too much memory to run on 
our server when using the LARGE data set, so we can?t get the 
result. 
  forward reverse GDFA 
IBM4 42.90 42.81 44.32 
IBM4-L0 42.59 41.04 43.19 
IBM4-Prior 41.94 40.46 42.44 
Agree-HMM 38.03 37.91 41.01 
Tree-Distance 34.21 37.22 38.42 
Hard-Cohesion 37.32 38.92 38.92 
Soft-Cohesion-EM 33.65 34.74 35.85 
Soft-Cohesion-Gibbs 34.45 33.72 34.46 
Table 4: AERs on the testing set (trained on the 
FBIS data set). 
  
EM Gibbs 
forward reverse forward reverse 
wd 26.12  28.66  27.09  26.40  
wd-hc 24.67  25.86  26.24  24.39  
wd-mc 24.49  26.53  25.51  25.40  
wd-hc-mc 23.63  25.17  24.65  24.33  
Table 3: AERs on the development set (trained 
on the FBIS data set). 
297
find all the systems yield better performance when 
using more training data. Our soft cohesion 
systems still produce better AERs than other 
systems, suggesting that our soft cohesion model is 
very effective for large-scale word alignment tasks. 
 
 
5.3 Machine Translation Quality Comparison 
We then evaluate the effect of word alignment on 
machine translation quality using the phrase-based 
translation system Moses (Koehn et al, 2007). We 
take NIST MT03 test data as the development set, 
NIST MT05 test data as the testing set. We train a 
5-gram language model with the Xinhua portion of 
English Gigaword corpus and the English side of 
the training set using the SRILM Toolkit (Stolcke, 
2002).  
We train machine translation models using 
GDFA alignments of each system. BLEU scores 
on NIST MT05 are listed in Table 7, where BLEU 
scores are calculated using lowercased and 
tokenized data (Papineni et al, 2002). Although 
the IBM4-L0, Agree-HMM, Tree-Distance and 
Hard-Cohesion systems improve word alignment 
than IBM4, they fail to outperform the IBM4 
system on machine translation. The BLEU score of 
our Soft-Cohesion-EM system is better than the 
IBM4 system when using the FBIS training set, but 
worse when using the LARGE training set. Our 
Soft-Cohesion-Gibbs system produces the best 
BLEU score when using both training sets. We 
also performed a statistical significance test using 
bootstrap resampling with 1000 samples (Koehn, 
2004; Zhang et al, 2004). Experimental results 
show the Soft-Cohesion-Gibbs system is 
significantly better (p<0.05) than the IBM4 system. 
The IBM4-Prior system slightly outperforms IBM4, 
but it?s not significant. 
 
6 Related Work 
There have been many proposals of integrating 
syntactic knowledge into generative alignment 
models. Wu (1997) proposed the inversion 
transduction grammar (ITG) to model word 
alignment as synchronous parsing for a sentence 
pair. Yamada and Knight (2001) represented 
translation as a sequence of re-ordering operations 
over child nodes of a syntactic tree. Gildea (2003) 
introduced a ?loosely? tree-based alignment 
technique, which allows alignments to violate 
syntactic constraints by incurring a cost in 
probability. Pauls et al (2010) gave a new instance 
of the ITG formalism, in which one side of the 
synchronous derivation is constrained by the 
syntactic tree. 
Fox (2002) measured syntactic cohesion in gold 
standard alignments and showed syntactic 
cohesion is generally maintained between English 
and French. She also compared three variant 
syntactic representations (phrase tree, verb phrase 
flattening tree and dependency tree), and found the 
dependency tree produced the highest degree of 
cohesion. So Cherry and Lin (2003; 2006a) used 
dependency cohesion as a hard constraint to 
restrict the alignment space, where all potential 
alignments violating cohesion constraint are ruled 
  
forward reverse 
HCP MCP HCP MCP 
IBM4 60.53 63.94 56.15 64.80 
IBM4-L0 60.57 62.53 66.49 65.68 
IBM4-Prior 66.48 74.65 67.19 72.32 
Agree-HMM 75.52 66.61 73.88 66.07 
Tree-Distance 81.37 74.69 78.00 71.73 
Hard-Cohesion 98.70 97.43 98.25 97.84 
Soft-Cohesion-EM 85.21 81.96 82.96 81.36 
Soft-Cohesion-Gibbs 88.74 85.55 87.81 84.83 
gold-standard 88.43 95.82 81.53 91.62 
Table 5: HCPs and MCPs on the development 
set. 
  FBIS LARGE 
IBM4 30.7 33.1 
IBM4-L0 30.4 32.3 
IBM4-Prior 30.9 33.2 
Agree-HMM 27.2 30.1 
Tree-Distance 28.2 N/A 
Hard-Cohesion 30.4 32.2 
Soft-Cohesion-EM 30.9 33.1 
Soft-Cohesion-Gibbs   31.6*   33.9* 
Table 7: BLEU scores, where * indicates 
significantly better than IBM4 (p<0.05). 
  forward reverse GDFA 
IBM4 37.45 39.18 40.52 
IBM4-L0 38.17 38.88 39.82 
IBM4-Prior 35.86 36.71 37.08 
Agree-HMM 35.58 35.73 39.10 
Hard-Cohesion 35.04 37.59 37.63 
Soft-Cohesion-EM 30.93 32.67 33.65 
Soft-Cohesion-Gibbs 32.07 32.68 32.28 
Table 6: AERs on the testing set (trained on the 
LARGE data set). 
298
out directly. Although the alignment quality is 
improved, they ignored situations where a small set 
of correct alignments can violate cohesion. To 
address this limitation, Cherry and Lin (2006b) 
proposed a soft constraint approach, which took 
dependency cohesion as a feature of a 
discriminative model, and verified that the soft 
constraint works better than the hard constraint. 
However, the training procedure is very time-
consuming, and they trained the model with only 
100 hand-annotated sentence pairs. Therefore, their 
method is not suitable for large-scale tasks. In this 
paper, we also use dependency cohesion as a soft 
constraint. But, unlike Cherry and Lin (2006b), we 
integrate the soft dependency cohesion constraint 
into a generative model that is more suitable for 
large-scale word alignment tasks. 
7 Conclusion and Future Work  
We described a generative model for word 
alignment that uses dependency cohesion as a soft 
constraint. We proposed an approximate EM 
algorithm and an explicit Gibbs sampling 
algorithm for parameter estimation in an 
unsupervised manner. Experimental results 
performed on a large-scale data set show that our 
model improves word alignment quality as well as 
machine translation quality. Our experimental 
results also indicate that the soft constraint 
approach is much better than the hard constraint 
approach.  
It is possible that our word alignment model can 
be improved further. First, we generated word 
alignments in both forward and reverse directions 
separately, but it might be helpful to use 
dependency trees of the two sides simultaneously. 
Second, we only used the one-best automatically 
generated dependency trees in the model. However, 
errors are inevitable in those trees, so we will 
investigate how to use N-best dependency trees or 
dependency forests (Hayashi et al, 2011) to see if 
they can improve our model. 
Acknowledgments 
We would like to thank Nianwen Xue for 
insightful discussions on writing this article. We 
are grateful to anonymous reviewers for many 
helpful suggestions that helped improve the final 
version of this article. The research work has been 
funded by the Hi-Tech Research and Development 
Program ("863" Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. This work is also supported in 
part by the DAPRA via contract HR0011-11-C-
0145 entitled "Linguistic Resources for 
Multilingual Processing".  
 
References  
Mohit Bansal, Chris Quirk, and Robert Moore, 2011. 
Gappy Phrasal Alignment By Agreement. In Proc. of 
ACL 2011. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer, 1993. The 
mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 19 
(2). pages 263-311. 
C. Cherry and D. Lin, 2003. A probability model to 
improve word alignment. In Proc. of ACL '03, pages 
88-95. 
C. Cherry and D. Lin, 2006a. A comparison of 
syntactically motivated word alignment spaces. In 
Proc. of EACL '06, pages 145-152. 
C. Cherry and D. Lin, 2006b. Soft syntactic constraints 
for word alignment through discriminative training. 
In Proc. of COLING/ACL '06, pages 105-112. 
John DeNero and Dan Klein, 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL '07, pages 17. 
C. Dyer, J. Clark, A. Lavie and N.A. Smith, 2011. 
Unsupervised word alignment with arbitrary features. 
In Proc. of ACL '11, pages 409-419. 
Heidi J. Fox, 2002. Phrasal cohesion and statistical 
machine translation. In Proc. of EMNLP '02, pages 
304-3111. 
Michel Galley, Mark Hopkins, Kevin Knight, Daniel 
Marcu, 2004. What's in a translation rule? In Proc. of 
NAACL '04, pages 344-352. 
J. Gao and M. Johnson, 2008. A comparison of 
Bayesian estimators for unsupervised Hidden 
Markov Model POS taggers. In Proc. of EMNLP '08, 
pages 344-352. 
Daniel Gildea, 2003. Loosely Tree-Based Alignment for 
Machine Translation. In Proc. of ACL'03, pages 80-
87. 
299
K. Hayashi, T. Watanabe, M. Asahara and Y. 
Matsumoto, 2011. Third-order Variational Reranking 
on Packed-Shared Dependency Forests. In Proc. of 
EMNLP '11. 
M. Johnson, T. Griffiths and S. Goldwater, 2007. 
Bayesian inference for PCFGs via Markov chain 
Monte Carlo. In Proc. of NAACL '07, pages 139-146. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of 
EMNLP'04. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran 
and R. Zens, 2007. Moses: Open source toolkit for 
statistical machine translation. In Proc. of ACL '07, 
Demonstration Session, pages 177-180. 
Percy Liang, Ben Taskar and Dan Klein, 2006. 
Alignment by agreement. In Proc. of HLT-NAACL 
06, pages 104-111. 
D. Lin and C. Cherry, 2003. Word alignment with 
cohesion constraint. In Proc. of NAACL '03, pages 
49-51. 
Adam Lopez and Philip Resnik, 2005. Improved HMM 
alignment models for languages with scarce 
resources. In ACL Workshop on Building and Using 
Parallel Texts '05, pages 83-86. 
Cos k?un Mermer and Murat Sara?lar, 2011. Bayesian 
word alignment for statistical machine translation. In 
Proc. of ACL '11, pages 182-187. 
R.C. Moore, 2005. A discriminative framework for 
bilingual word alignment. In Proc. of EMNLP '05, 
pages 81-88. 
F.J. Och, C. Tillmann and H. Ney, 1999. Improved 
alignment models for statistical machine translation. 
In Proc. of EMNLP/WVLC '99, pages 20-28. 
Franz Josef Och and Hermann Ney, 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29 (1). pages 19-51. 
K. Papineni, S. Roukos, T. Ward and W.J. Zhu, 2002. 
BLEU: a method for automatic evaluation of 
machine translation. In Proc. of ACL '02, pages 311-
318. 
Adam Pauls, Dan Klein, David Chiang and Kevin 
Knight, 2010. Unsupervised Syntactic Alignment 
with Inversion Transduction Grammars. In Proc. of 
NAACL '10. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of ACL 2006. 
Jason Riesa and Daniel Marcu, 2010. Hierarchical 
search for word alignment. In Proc. of ACL '10, 
pages 157-166. 
Jason Riesa, Ann Irvine and Daniel Marcu, 2011. 
Feature-Rich Language-Independent Syntax-Based 
Alignment for Statistical Machine Translation. In 
Proc. of EMNLP '11. 
Darcey Riley and Daniel Gildea, 2012. Improving the 
IBM Alignment Models Using Variational Bayes. In 
Proc. of ACL '12. 
M. Saers, J. Nivre and D. Wu, 2010. Word alignment 
with stochastic bracketing linear inversion 
transduction grammar. In Proc. of NAACL '10, pages 
341-344. 
A. Stolcke, 2002. SRILM-an extensible language 
modeling toolkit. In ICSLP '02. 
B. Taskar, S. Lacoste-Julien and D. Klein, 2005. A 
discriminative matching approach to word alignment. 
In Proc. of EMNLP '05, pages 73-80. 
Ashish Vaswani, Liang Huang, and David Chiang, 2012. 
Smaller alignment models for better translations: 
unsupervised word alignment with the l0 norm. In 
Proc. ACL'12, pages 311?319. 
Stephan Vogel, Hermann Ney and Christoph Tillmann, 
1996. HMM-based word alignment in statistical 
translation. In Proc. of COLING-96, pages 836-841. 
D. Wu, 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23 (3). pages 377-403. 
Zhiguo Wang, Chengqing Zong, 2010. Phrase Structure 
Parsing with Dependency Structure, In Proc. of 
COLING 2010, pages 1292-1300. 
Zhiguo Wang, Chengqing Zong, 2011. Parse Reranking 
Based on Higher-Order Lexical Dependencies, In 
Proc. Of IJCNLP 2011, pages 1251-1259. 
Kenji Yamada and Kevin Knight, 2001. A syntax-based 
statistical translation model. In Proc. of ACL '01, 
pages 523-530. 
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. 
Interpreting BLEU/NIST scores: How much 
improvement do we need to have a better system? In 
Proc. of LREC. 
Shaojun Zhao and Daniel Gildea, 2010. A fast fertility 
hidden Markov model for word alignment using 
MCMC. In Proc. of EMNLP '10, pages 596-605. 
 
300
Treebank Conversion based Self-training Strategy for Parsing 
Zhiguo Wang and Chengqing Zong
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{zgwang, cqzong}@nlpr.ia.ac.cn 
Abstract
In this paper, we propose a novel self-
training strategy for parsing which is 
based on Treebank conversion (SSPTC). 
In SSPTC, we make full use of the 
strong points of Treebank conversion 
and self-training, and offset their 
weaknesses with each other. To provide 
good parse selection strategies which are 
needed in self-training, we score the 
automatically generated parse trees with 
parse trees in source Treebank as a 
reference. To maintain the constituency 
between source Treebank and conversion 
Treebank which is needed in Treebank 
conversion, we get the conversion trees 
with the help of self-training. In our 
experiments, SSPTC strategy is utilized 
to parse Tsinghua Chinese Treebank 
with the help of Penn Chinese Treebank. 
The results significantly outperform the 
baseline parser. 
1 Introduction 
Syntax parsing is one of the most fundamental 
tasks in natural language processing (NLP) and 
has attracted extensive attention during the past 
few decades. In statistical area, according to the 
type of data used in training stage, the parsing 
approaches can be classified into three 
categories: supervised, semi-supervised and 
unsupervised. In supervised parsing approach, a 
high-performance parser can be built when given 
sufficient labeled data (Charniak, 2000; Collins, 
2003; Henderson, 2004). The semi-supervised 
approach utilizes some labeled data to annotate 
unlabeled data, then uses the annotated data to 
improve original model, e.g., self-training 
(McClosky et al, 2006) and co-training (Hwa et 
al., 2003). In unsupervised parsing, the labeled 
data was not employed and all annotations and 
grammars are discovered automatically from 
unlabeled data. 
State-of-the-art supervised parsers (Charniak, 
2000; Collins, 2003; Henderson, 2004) require 
large amounts of manually annotated training 
data, such as the Penn Treebank (Marcus et al, 
1993), to achieve high performance. However, it 
is quite costly and time-consuming to create 
high quality labeled data. So it becomes a key 
bottleneck for supervised approach to acquire 
sufficient labeled training data. Self-training is 
an effective strategy to overcome this shortage. 
It tries to enlarge the training set with 
automatically annotated unlabeled data and 
trains a parser with the enlarged training set.  
During the last few decades, many Treebanks 
annotated with different grammar formalisms 
are released (Zhou, 2004; Xue et al, 2005). 
Although they are annotated with different 
schemes, they have some linguistic consistency 
in some extent. Intuitively, we can convert 
Treebank annotated with one grammar 
formalisms into another Treebank annotated 
with grammar formalism that we are interested 
in. For simplicity, we call the first source 
Treebank, and the second target Treebank. And 
we call this strategy as Treebank conversion. 
Although both self-training and Treebank 
conversion can overcome the limitation of 
labeled data shortage for supervised parsing in 
some extent, they all have drawbacks. For self-
training, the quality of automatically annotated 
unlabeled data will affect the performance of 
semi-supervised parsers highly. For example, 
McClosky et al (2006) shows that when the 
parser-best list is used for self-training, the 
parsing performance isn?t improved, but after 
using reranker-best list, the retrained parser 
achieves an absolute 1.1% improvement. For 
Treebank conversion, different types among 
Treebanks make the converting procedure very 
complicated, and it is very hard to get a 
conversion Treebank constituent with target 
Treebank.
To overcome the limitations mentioned above, 
we propose a Treebank conversion based self-
training strategy for parsing, which tries to 
combine self-training and Treebank conversion 
together.
Remainder of this paper is organized as 
follows. In Section 2, we introduce some related 
work. Section 3 describes details of our SSPTC 
strategy. In Section 4, we propose a head finding 
method for Task21 in CLP2010. The 
experiments and analysis is given in Section 5. 
The last section draws conclusions and describes 
the future work. 
2 Related Work 
With the development of statistical parsing 
approaches, large scale corpus has become an 
indispensable resource. Because of the limited 
amount of existing labeled training data and the 
hardness of constructing corpus, many strategies 
have been proposed and experimented to 
overcome the contradiction. 
Self-training is one of the most successful 
strategies. McClosky et al (2006) shows that 
self-training effectively improves the accuracy 
of English parsing. First, they trained a two-
stage reranking parser(Charniak and Johnson, 
2005) using Penn Treebank (PTB)(Marcus et al, 
1993) and parsed 1,750k unlabeled sentences 
from North American News Text corpus 
(NANC). Then they combined the labeled 
NANC sentences with PTB together as training 
set and retrained the first stage of the parser. The 
final result got a 1.1% improvement over the 
previous best parser for section 23 of the Penn 
Treebank. Huang and Harper (2009) combined 
self-training into a PCFG-LA based parser both 
for English and Chinese. Experimental result 
showed that self-training contributed 0.83% 
absolute improvement using only 210k 
unlabeled sentences with a single generative 
parser. For the Chinese parsing, self-training 
contributed 1.03% absolute improvement. 
Treebank Conversion is another potential 
strategy to reuse existing source Treebanks for 
the study of target grammar parsing. Wang et al 
(1994) proposed a Treebank conversion 
algorithm for corpus sharing. They employed a 
parser with target grammar formalism to get N-
best parse list for each sentence in source 
Treebank, selected the best conversion tree from 
the list using their algorithm, then inserted the 
conversion trees into training set, and finally 
retrained the parser with the enlarged training set. 
Experimental result shows their algorithm is 
effective. Collins et al (1999) performed 
statistical constituency parsing of Czech on a 
Treebank that was converted from the Prague 
Dependency Treebank under the guidance of 
conversion rules and heuristic rules, and the final 
performance was also improved. Xia and Palmer 
(2001) proposed three methods to convert 
dependency trees into phrase structure trees with 
some hand-written heuristic rules. For 
acquisition of better conversion rules, Xia et al 
(2008) proposed a method to automatically 
extract conversion rules from a target Treebank. 
Niu et al (2009) tried to exploit heterogeneous 
Treebanks for parsing. They proposed a 
grammar formalism conversion algorithm to 
convert dependency formalism Treebank into 
phrase structure formalism, and did phrase 
structure parsing with the conversion trees. Their 
experiments are done in Chinese parsing, and the 
final performance is improved indeed. 
In summary, from the existing work we are 
confident that the strategies of self-training and 
Treebank conversion are effective to improve 
the performance of parser. 
3 Our Strategy 
3.1 Parsing Algorithm 
Although self-training and Treebank Conversion 
are effective for training set enlarging, they all 
have drawbacks. Self-training needs some parse 
selection strategies to select higher quality 
parsers. Treebank Conversion needs us to 
maintain the consistency between conversed 
Treebank and target Treebank. On the other 
hand, self-training strategy provides us a good 
idea to get annotated trees consistent with target 
grammar formalism, and the parse trees in 
source side provide a reference for higher 
quality parsers selecting. So we can combine 
self-training and Treebank Conversion together, 
use self-training strategy to get converted 
candidates for sentences in source Treebank, and 
select higher quality parses according to trees in 
source Treebank. We call this strategy Treebank 
Conversion based Self-training, and show more 
details in Algorithm 1. 
In Algorithm 1, target Treebank tT  and source 
Treebank sT  are input first (line 1). Then tT  is 
split into two parts: training set trainT  and 
development set devT  (line 3). And we train an 
initial parser with trainT and devT  in line 4. From 
line 6 to line 12, we train parsers with SSPTC 
strategy Iter times iteratively. Let is tT o be the 
automatically converted Treebank from source 
Treebank to target Treebank grammar formalism 
during the i-th iteration. From line 8 to line 11, 
we try to get a conversion tree with target 
grammar for each of the N sentences in source 
Treebank. We get N-best parse list kParseList  for 
sentence ks with 1iParser   (line 9), select the 
parse ? kp  with the highest score from kParseList
(line 10), and insert it into is tT o  (line 11). This 
procedure runs iteratively until all the trees in 
source Treebank have been converted, finally, 
we train a new parser iParser  with trainT , devT  and 
i
s tT o (line 12). 
3.2 Parse selection 
In line 10 of Algorithm 1, we select the highest 
quality parse ? kp  from kParseList according to 
function ( , )s s tScore p p o , where sp denotes a tree 
in source Treebank and s tp o denotes a 
conversion tree with target Treebank grammar 
formalism for sp . ( , )s s tScore p p o  compares 
s tp o  with sp  and computes a score for s tp o
taken sp  as a reference. According to the idea 
proposed in Wang et al (1994), we use the 
number of aligned constituents in the source and 
target trees to construct ( , )s s tScore p p o . We 
propose two types of ( , )s s tScore p p o as follows. 
(1) Unlabeled aligned constituents F1 score 
(UAF)
First, we define a constituent as tag[i,j], which 
represents a non-terminal node labeled with tag
and spanning words from positions i to j of the 
input sentence. A non-terminal node in s tp o
aligns with a non-terminal node in sp  when they 
span the same words. If two nodes are aligned, 
we call them an aligned constituent and denote 
the aligned relationship as [ , ] [ , ]s ttag i j tag i j? .
For example in Figure 1, there are three aligned 
constituents between the source Treebank tree 
and the conversion tree, and we can denote them 
as [0, 7] [0, 7]s tIP dj? , [0, 2] [0, 2]s tNR sp? and
[2, 6] [2, 6]s tNR np? , respectively. 
When given sp and s tp o , we can easily 
collect all the aligned constituents. So we define 
Unlabeled aligned constituents Precision (UAP) 
and Unlabeled aligned constituents Recall (UAR) 
as follows. 
,
,
( [ , ] [ , ])
( [ , ])
s t
i j
t
i j
Count tag i j tag i j
UAP
Count tag i j
?
 
?
?
,
,
( [ , ] [ , ])
( [ , ])
s t
i j
s
i j
Count tag i j tag i j
UAR
Count tag i j
?
 
?
?
Algorithm 1 
1: Input: tT and sT
2:  initialize 
3: { , ( )}train dev tT T Split Tm
4: 0 ( , )train devParser Train T Tm
5:   Iter iterations 
6: for im 1? Iter do
7:    is tT Io m
8:    for k m 1? N do
9:        1( , )k i kParseList Nbest Parser sm
10:       ,? arg max ( , )j kk s k jp ParseListp Score p p? 
11:       ?is t kT po m
12:   ( , , )ii train dev s tParser Train T T T om
13: return IterParser
Then Unlabeled aligned constituents F1 score 
(UAF) is defined as: 
,
,
2
( , )
2 ( [ , ] [ , ])
( ( [ , ]) ( [ , ]))
s s t
s t
i j
s t
i j
UAP UAR
Score p p
UAP UAR
Count tag i j tag i j
Count tag i j Count tag i j
o
u u
 

u ?
 

?
?
    (1) 
(2) Labeled aligned constituents F1 score 
(LAF)
In the last subsection, we define ( , )s s tScore p p o
according to UAF. In fact, the tags of 
constituents bring us much information to score 
conversion trees. So we 
define ( , )s s tScore p p o with Labeled aligned 
constituents F1 score (LAF) in this subsection. 
Because the annotation schemes are different, 
constituent tags in source Treebank may be 
much more different from target Treebank. The 
number of such tags may be drastically different 
and the mapping may not be one-to-one. To 
eliminate the contradiction, we assume that each 
tag in source Treebank can be converted into 
every tag in target Treebank with various 
probabilities. So there is a converting matrix 
representing the converting probabilities, and we 
can calculate the converting matrix through 
source Treebank and N-best conversion trees. 
Given the source Treebank and N-best 
conversion trees, first we align all the 
constituents, then collect all the aligned tags and 
compute the converting probability as the 
following equation.  
( )
( )
( )
s t
s t
s
Count tag tag
p tag tag
Count tag
?
o  
          (2) 
Finally, we modify UAF computed by 
equation (1) into LAF as below. 
,
,
( )
2 (1 ( )) ( [ , ] [ , ])
( ( [ , ]) ( [ , ]))
,
s t s t
i j
s t
i j
s s tScore
p tag tag Count tag i j tag i j
Count tag i j Count tag i j
p p
J
o  
u  u o u ?

?
?
                                                                           (3) 
In equation (3), J  is a tunable variable, which 
is used to weight the converting probability. 
Especially, LAF will be transferred into UAF 
when J =0.
3.3 Corpus weighting technique 
In line 12 of Algorithm 1, we train a new parser 
with target Treebank and conversion trees. 
However, the errors in automatically conversion 
trees are unavoidable and they would limit the 
accuracy of the self-trained model. So we have 
to take some measures to weight the gold target 
Treebank and the automatically conversion trees. 
McClosky et al (2006) and Niu et al (2009) 
take the strategy that duplicates the gold 
Treebank data many times. However, this 
strategy isn?t suitable for PCFG-LA 
parser 1 (Matsuzaki et al, 2005; Petrov et al, 
2006), because PCFG-LA employs an EM 
algorithm in training stage, so duplicating gold 
Treebank would increase the training time 
tremendously. Instead, according to Huang and 
Harper (2009), we weight the posterior 
probabilities computed for the gold and 
automatically converted trees to balance their 
importance. 
Let ( | )count A tEo be the count of rule 
A Eo  in a parse tree t . tT  and s tT o  are the sets 
of target Treebank and automatically converted 
trees from source Treebank respectively. The 
posterior probability of rule A Eo  (with 
weighting parameterD ) can be expressed as: 
1 We will use BerkeleyParser as our baseline parser, 
which is a PCFG-LA based parser. 
?? ?? ?? ? ?? ?? ??
NR NR NN CC NN NN VV
?? ?? ?? ? ?? ?? ??
nS nS vN cC n vN v
NR [0,2] NR [2,6] VP [6,7]
VP [0,6]
IP [0,7]
sp [0,2] np [4,6]
np [2,6]
vp [2,7]
dj [0,7]
(a) parse tree in source Treebank
(b) conversion tree with target Treebank grammar
Figure 1: source tree and its conversion 
tree with target grammar formalism 
( )
( | ) ( | )
( ( | ) ( | ))
t s t
t s t
t T t T
t T t T
p A
Count A t Count A t
Count A t Count A t
E
E
E D E
E D E
o
o
? ?
? ?
o  
o  o
o  o
? ?
? ? ?
(4)
4 Head Finding 
In Task21 of CLP2010, we are required to find 
heads for each constituent. Our method is to 
make head finding as a post procedure after 
parsing.
We treat head finding problem as a 
classification problem, which is to classify each 
context-free production into categories labelled 
with their heads. For example, there are three 
types of heads: -0, -0-2 and -2 for 
vp vp wP vpo , so we try to classify this 
production into categories labelled with -0, -0-2 
and -2. First, we scan the train set and collect all 
the heads for each context-free production. Then 
we train a Maxent classifier to classify each 
context-free production into categories. We take 
the same feature templates for the classification 
as Chen et al (2009) did, which is described in 
Table 1. 
The head finding procedure proceeds in a 
bottom-up fashion, so that we can make use of 
heads of productions in lower layers as features 
for classification of the higher layers. 
To evaluate the accuracy of our head finding 
method, we randomly select a development set, 
remove all head information and use our Maxent 
classifier to retrieve the heads. Experimental 
results show the accuracy has reached 98.28%. 
However, the final performance would drop 
much when the parse trees are generated 
automatically. Because the automatically 
generated parse trees would bring many errors, 
and the post procedure of head finding can?t 
correct the errors. 
5 Experiments and Analysis 
5.1 Data Preparation 
In order to evaluate the effectiveness of our 
approach, we do experiments for Chinese 
parsing using Tsinghua Chinese Treebank 
(TCTB) on target side and Penn Chinese 
Treebank (PCTB) on source side. We divide the 
training portion of the Tsinghua Chinese 
Treebank provided by CLP2010 into three parts 
as follows: 500 trees are randomly extracted as 
development set, another 500 as validating set 
and the rest trees are taken as training set. For 
trees in PCTB, all the empty-node and function 
tag information are removed. All the ParseVal 
measures reported in this paper are evaluated by 
the EVALB tool2.
5.2 Experiments 
In order to get a good final accuracy, we choose 
BerkeleyParser 3 , which is a state-of-the-art 
unlexicalized parser, and train a model with the 
training set as our baseline. The F1 score of 
validating set parsed by baseline parser is 
85.72%. In the following of this subsection, we 
try to combine our strategies into the baseline 
parser and evaluate the effectiveness. Because 
mult-time iterations can?t improve parsing 
performance tremendously but cost much time 
during our experiments, we take Iter=1 here. 
(1) Corpus weighting experiment 
To evaluate the corpus weighting strategy, we 
take sentences (ignore the tree structure) in 
PCTB as unlabeled data, and train a parser with 
self-training strategy. F1 scores of validating set 
varying with D in equation (4) are shown in 
Figure 2. From Figure 2, we find that the F1 
score varies with D , and reaches 86.46% 
2 http://nlp.cs.nyu.edu/evalb/ 
3 http://code.google.com/p/berkeleyparser/ 
Feature templates 
The label of the current constituent; 
The label of the left most child, the middle child and the right most child; 
The head word of the left most child, the middle child and the right most child; 
The POS tag of the head word of the left most child, the middle child and the right most child; 
Bigram of label, head word and POS tag of head word of the children: L/M, M/R; 
Trigram of label, head word and POS tag of head word of the children: L/M/R; 
The number of children; 
Table 1: Feature Templates for Head Finding 
when D =1. The 0.74 absolute improvement 
comparing with the baseline certifies the 
effectiveness of our corpus weighting strategy. 
(2) Parse selection experiments 
In this subsection we evaluate our parse 
selection strategies with the help of PCTB. 
According to Algorithm 1, we train an initial 
parser with training set and development set. 
Then we generate 50-best parses list with the 
initial parser for each sentence in PCTB, and 
select a higher-score parse for each sentence 
through our parse selection strategies to build a 
conversion Treebank. Finally, we retrain a parser 
with training set and the conversion Treebank 
with the help of corpus weighting strategy. 
Figure 3 shows F1 scores of validating set 
using UAF to select higher quality parses. 
When D =0.3, F1 score reaches 86.92%. The 
improvement over baseline is 1.2 percentage 
points. Comparing with the highest F1 score of 
self-training, we got 0.46 more improvement. So 
our parse selection strategy with UAF is 
effective.
Because the highest F1 score is at the point 
D =0.3 in Figure 3, we choose D =0.3 to 
evaluating LAF strategy. Figure 4 shows F1 
scores on validating set using LAF. The highest 
F1 score is 87.44% at the pointJ =6, and it gets 
1.72 percentage points improvement over 
baseline. Comparing with UAF, LAF gets 0.52 
more improvement. So we can conclude that the 
parse selection strategy with LAF is much more 
effective.
5.3 Discussion 
Table 2 reports the highest performances of 
various strategies. From the table we can easily 
find that all strategies outperform the baseline 
parser. Corpus weighting experiment tells us that 
balancing the importance of gold target 
Treebank and conversion trees is helpful for the 
final performance. Using UAF to select 
conversion trees can get more improvement than 
self-training which just selects the best-first trees. 
This fact proves that our SSPTC strategy is 
reasonable and effective. Making use of LAF, 
we get more improvement than UAF. It tells us 
that exploiting source Treebank deeply can bring 
us more useful knowledge which is helpful to 
develop high-performance parser. 
6 Experiments for Task 2 of CLP2010 
Task 2 of CLP2010 includes two sub-tasks: sub-
sentence parsing and complete sentence parsing. 
For each sub-task, there are two tracks: closed 
track and open track. To accomplish tasks in 
closed track, we make use of our baseline parser 
shown in section 5 and train it with different 
parameters and data set. For open track, we 
make use of our SSPTC strategy and train it with 
different parameters and data set. We tuned the 
parameters on the development set and selected 
Strategy F1 score 
Baseline 85.72% 
Corpus weighting 86.46% 
UAF 86.92% 
LAF 87.44% 
Table 2: F1 scores of various strategies 
Figure 4: F1 score of LAF strategy 
Figure 2: F1 score of self-training 
Figure 3: F1 score of UAF strategy 
some configurations which achieve higher 
performance on the development set(more 
details have been shown in section 5). The final 
parameters and training data of our systems are 
shown in Table 34. We also make use of the 
approach explained in section 4 for the head 
finding procedure. 
The parsing results of our systems on the test 
set can be found on the official ranking report. 
Our systems training with SSPTC strategy bring 
us an amazing performance which outperforms 
other systems in both the two sub-tasks. 
7 Conclusion and Future work 
In this paper, we propose a novel self-training 
strategy for parsing which is based on Treebank 
conversion. Benefiting from SSPTC strategy, we 
have gotten higher quality parse trees with the 
help of source Treebank, and gotten conversion 
Treebank with target Treebank grammar 
formalism simply and consistently. The parsing 
results on validating set show SSPTC is 
effective. We apply SSPTC to the test set of 
Task 2 in CLP2010, and get 1.275 percentage 
points improvement over baseline parser using 
the parameters tuned on validating set.  
4 The parsing result for system b in open track of sub-
task1 has been submitted mistakenly, so the figures of 
this system on the official ranking report have no 
reference value. 
5 The F1 score of baseline parser is 75.24%, and it 
reaches 76.51% using TCBS strategy. 
All the delightful results tell us that SSPTC is 
a promoting strategy for parsing. However, there 
is much knowledge in source Treebank remained 
to further exploit, e.g. the POS tags in source 
Treebank is a good resource to improve the POS 
tagging accuracy of target Treebank. So, in the 
next step we will exploit source Treebank deeply 
and try to get more knowledge from it for 
parsing.
Acknowledgement 
The research work has been partially funded by 
the Natural Science Foundation of China under 
Grant No. 6097 5053, 90820303 and 60736014, 
the National Key Technology R&D Program 
under Grant No. 2006BAH03B02, the Hi-Tech 
Research and Development Program (?863? 
Program) of China under Grant No. 
2006AA010108-4, and also supported by the 
China-Singapore Institute of Digital Media 
(CSIDM) project under grant No. CSIDM-
200804. 
References 
Eugene Charniak, 2000. A maximum-entropy-
inspired parser. In NAACL-2000  
Eugene Charniak and Mark Johnson, 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative 
reranking. In ACL-05. 
Xiao Chen, Changning Huang, Mu Li and Chunyu 
Kit, 2009. Better Parser Combination. In CIPS.  
 Sub-task Track  ID Parser Parameters Train data
 a  Berkeley -- TS
  Closed 
 b  Berkeley -- TS && VS
a SSPTC  0.3  5D J  TS && PTCB
  Sub-task 1
Open
b SSPTC  0.3  5D J   TS && VS && PTCB 
a   Berkeley -- TS
  Closed
b   Berkeley -- TS && VS
a SSPTC  0.3  6D J  TS && PTCB
b SSPTC  0.3  5D J   TS && VS && PTCB
c SSPTC  0.3  5D J  TS && PTCB
 Sub-task 2
Open
d SSPTC  0.3  3D J  TS && PTCB
Table 3: The configurations of our systems. The abbreviations in the last column mean 
training set(TS) and validating set(VS) explaining in section 5.1. 
Michael Collins, 2003. Head-driven statistical models 
for natural language parsing. Computational 
Linguistics, 29 (4). pages 589-637.  
M Collins, J Hajic, L Ramshaw and C Tillman, 1999. 
A statistical parser for Czech. In ACL-99. J 
Henderson, 2004. Discriminative training of a 
neural network statistical parser. 
Zhongqiang Huang and Mary Harper, 2009. Self-
Training PCFG grammars with latent annotations 
across languages. ACL-09. 
R Hwa, M Osborne, A Sarkar and M Steedman, 2003. 
Corrected co-training for statistical parsers. 
Citeseer.
MP Marcus, B Santorini and MA Marcinkiewicz, 
1993. Building a large annotated corpus of English: 
The Penn Treebank. Computational Linguistics, 19 
(2). pages 313-330. 
Takuya Matsuzaki, Yusuke Miyao and Jun'ichi Tsujii, 
2005. Probabilistic CFG with latent annotations. In 
ACL-05.  
David McClosky, Eugene Charniak and Mark 
Johnson, 2006. Effective self-training for parsing. 
In ACL-06.  
Zheng-Yu Niu, Haifeng Wang and Hua Wu, 2009. 
Exploiting heterogeneous treebanks for parsing. In 
ACL-09, pages 46-54. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In ACL-06.  
Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su, 
1994. An automatic treebank conversion algorithm 
for corpus sharing. In ACL-94. 
Fei Xia and Martha Palmer, 2001. Converting 
dependency structures to phrase structures. In The 
1st Human Language Technology Conference 
(HLT-2001). 
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha 
Palmer and Dipti Misra Sharma, 2008. Towards a 
multi-representational treebank. Proc. of the 7th 
Int'lWorkshop on Treebanks and Linguistic 
Theories (TLT-7). pages 207-238. 
Qiang Zhou, 2004. Annotation Scheme for Chinese 
Treebank. Journal of Chinese Information 
Processing, 18 (004). 
