Analysis and Detection of Reading Miscues for Interactive Literacy Tutors 
Katherine Lee, Andreas Hagen, Nicholas Romanyshyn, Sean Martin, Bryan Pellom 
Center for Spoken Language Research  
University of Colorado at Boulder 
pellom@cslr.colorado.edu 
 
Abstract 
The Colorado Literacy Tutor (CLT) is a 
technology-based literacy program, designed 
on the basis of cognitive theory and 
scientifically motivated reading research, 
which aims to improve literacy and student 
achievement in public schools. One of the 
critical components of the CLT is a speech 
recognition system which is used to track the 
child?s progress during oral reading and to 
provide sufficient information to detect 
reading miscues.  In this paper, we extend on 
prior work by examining a novel labeling of 
children?s oral reading audio data in order to 
better understand the factors that contribute 
most significantly to speech recognition 
errors.  While these events make up nearly 8% 
of the data, they are shown to account for 
approximately 30% of the word errors in a 
state-of-the-art speech recognizer.  Next, we 
consider the problem of detecting miscues 
during oral reading. Using features derived 
from the speech recognizer, we demonstrate 
that 67% of reading miscues can be detected at 
a false alarm rate of 3%. 
1 Introduction 
  Pioneering research by MIT and CMU as well as 
more recent work by the IBM Watch-me-Read 
project has demonstrated human language 
technologies can play an effective role in systems 
designed to improve children?s reading abilities 
(McCandless, 1992; Mostow et al, 1994; Zue et 
al., 1996). In CMU?s Project LISTEN, for example, 
the tutor operates by prompting children to read 
individual sentences out loud.  The tutor listens to 
the child using speech recognition and extracts 
features that can be used to detect oral reading 
miscues (Mostow et al, 2002; Tam et al 2003).  
The most common miscues that children make 
while reading out loud are word substitutions, 
repetitions, and self-corrections with word 
omissions and insertions being less frequent 
(Fogarty et al 2001). 
  Upon detecting such reading errors, the tutor must 
provide appropriate feedback to the child.  While 
the type of feedback and level of feedback is the 
current subject of much debate within the research 
community, recent results have shown that 
automated reading tutors can improve student 
achievement (Mostow et al, 2003).  In fact, 
providing real time feedback by simply 
highlighting words as they are read out loud is the 
basis of at least one commercial product today1.  
Cole et al (2003) and Wise et al (in press) 
describe a new scientifically-based literacy 
program, Foundations to Fluency, in which a 
virtual tutor?a lifelike 3D computer model?
interacts with children in multimodal learning tasks 
to teach them to read. A key component of this 
program is the Interactive Book, which combines 
real-time multilingual speech recognition, facial 
animation, and natural language understanding 
capabilities to teach children to read and 
comprehend text.  Within the context of this 
reading program, Hagen et al (2003) demonstrated 
an initial speech recognition system that provides 
real-time reading tracking for children.  This work 
was later extended by Hagen et al (2004) to 
incorporate improved acoustic and language 
modeling strategies.  When tested on 106 children 
(ages 9-11) who were asked to read one of a 
number of short age-appropriate stories, a final 
system word error rate of 8.0% was demonstrated.   
While reporting raw word error rate is useful for 
comparison purposes to prior research, we point 
out that it does not provide any diagnostic 
information which can be used to understand 
factors that contribute to speech recognition error 
within such children?s literacy tutor programs.  
Therefore, this paper extends our earlier work in 
two important ways.  First, in order to understand 
where future improvements can be obtained, we 
provide a novel ?event? labeling of our children?s 
speech corpus and examine the performance of the 
current speech recognition system under each 
labeled event condition.  Second, we describe the 
construction of an automated classifier which can 
detect reading miscues in children?s speech.     
This paper is organized as follows.  First, 
Section 2 provides an introduction and overview of 
the Colorado Literacy Tutor project.   Section 3 
describes the audio corpus used in the experiments 
                                                     
1 http://www.soliloquy.com 
provided in this paper and Section 4 describes our 
baseline speech recognition system.  Next, Section 
5 describes the event labeling methodology and 
word error analysis under each labeled event 
condition.  Finally Section 6 describes our initial 
work towards developing a system to detect 
reading miscues based on the output of our 
baseline speech recognition system.  Conclusions 
and future work are outlined in Section 7. 
2 The Colorado Literacy Tutor 
The Colorado Literacy Tutor (CLT)2 is a 
technology-based literacy program, designed on 
the basis of cognitive theory and scientifically 
motivated reading research, which aims to improve 
literacy and student achievement in public schools. 
The goal of the Colorado Literacy Tutor is to 
provide computer-based learning tools that will 
improve student achievement in any subject area 
by helping students learn to read fluently, to 
acquire new knowledge through deep 
understanding of what they read, to make 
connections to other knowledge and experiences, 
and to express their ideas concisely and creatively 
through writing. A second goal is to scale up the 
program to both state and national levels in the 
U.S. by providing accessible, inexpensive and 
effective computer-based learning tools.    
   The CLT project consists of four tightly 
integrated components: Managed Learning 
Environment, Foundational Reading Skills Tutors, 
Interactive Books, and Latent-Semantic Analysis 
(LSA)-based comprehension training (Steinhart 
2001; Deerwester et al, 1990; Landauer and 
Dumais, 1997).   A key feature of the project is the 
use of leading edge human communication 
technologies in learning tasks. The project has 
become a test bed for research and development of 
perceptive animated agents that integrate auditory 
and visual behaviors during face-to-face 
conversational interaction with human learners. 
The project enables us to evaluate component 
technologies with real users?students in 
classrooms?and to evaluate how the technology 
integration affects learning using standardized 
assessment tools.  
Within the CLT, Interactive Books are the main 
platform for research and development of natural 
language technologies and perceptive animated 
agents. Figure 1 shows a page of an Interactive 
Book. Interactive Books incorporate speech 
recognition, spoken dialogue, natural language 
processing, and computer animation technologies 
to enable natural face-to-face conversational 
                                                     
2 http://www.colit.org 
interaction with users. The integration of these 
technologies is performed using a client-server 
architecture that provides a platform-independent 
user interface for Web-based delivery of 
multimedia learning tools.  Interactive Book 
authoring tools are designed for easy use by project 
staff, teachers and students to enable authors to 
design and format books by combining text, 
images, videos and animated characters. Once text 
and illustrations have been imported or input into 
the authoring environment, authors can orchestrate 
interactions between users, animated characters 
and media objects. Developers can populate 
illustrations (digital images) with animated 
characters, and cause them to converse with each 
other, with the user, or speak their parts in the 
stories using naturally recorded or synthetic 
speech. A mark up language enables authors to 
control characters? facial expressions and gestures 
while speaking. The authoring tools also enable 
authors to pre-record sentences and/or individual 
words in the text as well as utterances to be 
produced by animated characters during 
conversations.    
 
 
Figure 1: An example interactive book 
 
Interactive Books enable a wide range of user 
and system behaviors. These include having the 
story narrated by animated characters, having 
conversations with animated characters in 
structured or mixed-initiative dialogues, having the 
student read out loud while words are highlighted, 
enabling the student to click on words to have 
them spoken by the agent or to have the agent 
interact with the student to sound out the word, 
having the student respond to questions posed by 
the agent either by clicking on objects in images or 
saying or typing responses, and having the student 
produce typed or spoken story summaries which 
can be analyzed for content using natural language 
processing techniques.  
3 CU Children?s Read Story Corpus 
Within the context of the CLT project, we have 
collected a corpus of audio data consisting of read 
stories spoken by children. Known as the CU 
Children?s Read Story Corpus3, the data currently 
contains speech and associated word-level 
transcriptions from 106 children who were asked 
to read a short age-appropriate story and to provide 
a spontaneous spoken summary of the material.  In 
addition, each child was prompted to read 25 
phonetically balanced sentences for future use in 
exploring strategies for speaker adaptation.   
The data were collected from native English 
speaking children in the Boulder Valley School 
District (Boulder, Colorado, USA).  We have 
initially collected and transcribed stories from 
children in grades 3, 4, and 5 (grade 3: 17 
speakers, grade 4: 28 speakers, grade 5: 61 
speakers).  The data were originally collected in a 
quiet room using a commonly available head-
mounted microphone.  The current 16 kHz 
sampled corpus consists of 10 different stories.  
Each story contains an average of 1054 words (min 
532 words / max 1926 words) with an average of 
413 unique words per story.    Note that while each 
story is accompanied by a spontaneous summary 
produced by the child, we do not consider those 
data for this paper. 
4 Baseline Speech Recognition System 
The CLT uses the SONIC speech recognition 
system as a basis for providing real-time 
recognition of children?s speech (Pellom, 2001; 
Pellom and Hacioglu, 2003; Hagen et al 2004)4.  
The recognizer implements an efficient time-
synchronous, beam-pruned Viterbi token-passing 
search through a static re-entrant lexical prefix tree 
while utilizing continuous density mixture 
Gaussian Hidden Markov Models (HMMs).  The 
recognizer uses PMVDR cepstral coefficients 
(Yapanel and Hansen, 2003) as its feature 
representation. Children?s acoustic models were 
estimated from 46 hours of audio from the CU 
Read and Prompted Children?s Speech Corpus 
(Hagen et al, 2003)5 and the OGI Kids? speech 
corpus (Shobaki et al, 2000). 
During oral reading, the speech recognizer 
models the story text using statistical n-gram 
language models.  This approach gives the 
recognizer flexibility to insert/delete/substitute 
                                                     
3 The CU Children?s Read Story Corpus is made 
available for research purposes (http://cslr.colorado.edu) 
4 SONIC is freely downloadable for research use 
from (http://cslr.colorado.edu) 
5 This corpus differs from the test corpus in Section 3. 
words based on acoustics and to provide accurate 
confidence information from the word-lattice.  The 
recognizer receives packets of audio and 
automatically detects voice activity.  When the 
child speaks, the partial hypotheses are sent to a 
reading tracking module.  The reading tracking 
module determines the current reading location by 
aligning each partial hypothesis with the story text 
using a Dynamic Programming search.  In order to 
allow for skipping of words or even skipping to a 
different place within the text, the search finds 
words that when strung together minimize a 
weighted cost function of adjacent word-proximity 
and distance from the reader's last active reading 
location. The Dynamic Programming search 
additionally incorporates constraints to account for 
boundary effects at the ends of each partial phrase. 
Hagen et al (2004) describes more recent 
advances made to both acoustic and language 
modeling for oral-reading recognition of children?s 
speech.  Specifically, that work describes the use 
of cross-utterance word history modeling, position-
sensitive dynamic n-gram language modeling, as 
well as vocal tract length normalization, speaker-
adaptive training, and iterative unsupervised 
speaker adaptation for improved recognition.  The 
final system was shown to have an overall word 
error rate of 8.0% on the speech corpus described 
in Section 3.  This system serves as the baseline for 
our experiments in the remainder of the paper. 
5 Event-based Word Error Analysis 
While our earlier work in Hagen et al (2003) 
and Hagen et al (2004) described consistent 
improvements in speech recognition accuracy on 
children?s speech, the use of raw word error rate 
does not reveal much information in terms of 
where future improvements in system performance 
may be obtained.  Because of this, we annotated 
the CU Children?s Read Story Corpus in terms of a 
set of event labels which we feel might have most 
relation to speech recognition error rate.  Next, in 
Section 5.1, we describe the event labeling 
methodology and then provide a detailed error 
analysis of our baseline system in Section 5.2. 
5.1 Event Labeling Methodology 
The event labels for this project were chosen 
based on the most common types of errors children 
make when reading aloud. Also included in the 
labels are other acoustic events that occur 
frequently, such as breaths and pauses, which may 
contribute to an error made by the speech 
recognizer.  The event labels for this study are 
summarized in Table 1. Common errors as stated 
before are word repetitions, omissions, 
substitutions, insertions, and self-corrections.  
Although pauses (PS) are natural in speech, too 
many can disrupt the fluency of the read story. If a 
pause is extended, the recognizer may potentially 
insert a word (during the silence region). Similarly, 
we marked breath placements (BR) if they were 
audible.  We hypothesize that words may be 
inserted during periods of breath if not properly 
accounted for by the speech recognizer. 
Mispronunciations (MP) tend to occur when a 
child is faced with word he/she is not familiar with 
and makes an attempt at either sounding it out (or 
speak fluently with an inappropriate phonetic 
realization). The use of wrong words (WW) is 
commonly a result of fast reading. The child may 
only read the first part of the word and guess on 
the rest replacing the word with one that is 
phonologically similar. An interjection (IJ) is any 
word inserted into a sentence that is not in the 
original text (e.g., ?um? or ?ah?).  Repetitions 
(REP) occur when the child realizes he/she has 
made a mistake and self corrects him/herself 
usually by repeating the misread word or by 
beginning the sentence over again. In some cases 
the child catches his/her error before finishing the 
word and thus creating a partial word, however, 
since it is a conscious act by the child the word is 
marked as a repetition assuming he/she did repeat 
it to self correct.  
Other important factors to be tracked by the 
recognizer are over-articulations (OA), hesitations 
(HS), non-speech segments (NS) and background 
noises (BN). An over-articulation is considered to 
be a deliberate sounding out of the word where 
each sound may be heard separately. A child may 
additionally hesitate on a word while looking 
ahead at the next word causing parts of the word to 
be elongated (e.g., stretched vowels). The non-
speech sound and background noise labels are 
meant to indicate any noise outside of the child?s 
reading such as a cough or a door closing.   We 
also considered including a label for head-colds 
(HC), but later removed this label due to 
inconsistencies and subjective assessments needed. 
These labels were applied to 106 read stories 
from the audio corpus described in Section 3.  
Each file was analyzed by one of three listeners 
and marked using these labels. Reliability between 
the listeners was checked by overlapping the files 
analyzed and comparing mark ups.   The event 
labeling and word-level transcription of the audio 
corpus were conducted using the freely available 
Transcriber software6. 
 
                                                     
6 http://www.etca.fr/CTA/gip/Projets/Transcriber/ 
Event Label  
& Event Description 
Total 
Words 
(%) 
Word 
Error 
(%) 
None No Labeled Event 92.26 5.7
REP Word Repetition 2.46 22.4
BR Breath 1.44 26.1
PW Partial Word 0.70 49.6
PS Pause 0.70 40.5
HS Hesitation/Elongation 0.67 13.8
WW Wrong Word 0.60 48.1
MP Mispronunciation 0.36 36.2
BN Background Noise 0.30 15.5
IJ Interjection / Insertion 0.28 61.3
NS Non-Speech Sound 0.27 58.8
OA Over-articulation 0.10 38.3
Table 1: Event labels used in speech recognition 
error analysis on the CU Children?s Read Story 
Corpus.  Total words aligned to each condition are 
shown (in %) along with the average word error 
rate of the baseline system under each condition.  
The baseline system has a word error rate of 8.0%. 
5.2 Speech Recognition Error Analysis 
Using the NIST Speech Recognition Scoring 
Toolkit (SCTK)7 we obtained the alignments of the 
reference word-level transcription with the 
hypothesized string from our baseline speech 
recognition system. By using the associated timing 
information, each word was then marked as 
belonging to one of the event classes shown in 
Table 1 (or possibly no class marking).  Each word 
was further marked as correctly or incorrectly 
recognized by the speech recognizer using the 
scoring software.  Based upon this analysis we are 
able to deduce the percentage of words that are 
output from the speech recognizer and associated 
with each event condition (column 2 of Table 1).  
We also can determine the average word-error rate 
for each labeled event type (column 3 of Table 1). 
What is most striking from Table 1 is that the 
average system word error rate during non-event 
labeled conditions is 5.7% while the average word 
error rate for words associated with the labeled 
event conditions is 31.5%.  While the speech 
recognizer output during the labeled events is 
small (approximately 7.7% of the words), the 
events contribute to nearly 30% of the word error 
rate of the system.  Most troubling are instances of 
repeated words and breaths made by the child 
during read-aloud.  We suggest that future progress 
can be made by focusing on (1) flexible n-gram 
language modeling which may take into account 
the problem of word-repetition, and (2) more 
accurate acoustic modeling and rejection of breath 
events during oral reading. 
                                                     
7 http://www.nist.gov/speech/tools/ 
6 Automatic Detection of Reading Miscues 
An important aspect in an automated reading 
tutor is the capability of detecting reading miscues 
and utilizing this knowledge to provide appropriate 
feedback. The level of detail present in the 
feedback strongly depends on the event detection 
accuracy, which is investigated in this paper.  We 
leave the problem of determining what feedback to 
provide as an area of future work.  First, we define 
our miscue detection problem and then provide a 
description of the features and classifier utilized.  
Finally, we evaluate our miscue detection system 
using the baseline speech recognition system 
described in Section 4.  
6.1 Problem Formulation 
Our main criterion for detecting events in our 
system is based on word alignments which 
compare the reference transcription of the child?s 
speech to the reference story text.  Similarly to 
Tam et al (2003), in order to detect reading 
miscues the speech recognizer's hypothesized 
output is aligned against the target story text using 
the Viterbi algorithm (i.e., hypothesis-target 
alignment). Furthermore the alignment of the 
human-based transcription against the story text is 
needed in the classification / evaluation process to 
determine where reading miscues actually occur 
(i.e., transcription-target algnment).  
We define a reading miscue event as any 
instance in which the child inserts, deletes or 
substitutes a word during oral reading.  Therefore 
each word spoken by the child is associated with 
an event label (insertion, deletion or substitution) 
or non-event (i.e., correct word). 
Given this word-level miscue labeling of the data 
we can propose a detection problem.  Here, each 
recognized word is submitted to a classifier.  This 
classifier labels each output word as correct or 
incorrect (i.e., a miscue event).  By thresholding 
the classifier output we can determine a detection 
rate for a given false alarm rate and therefore 
describe a Receiver Operating Characteristic 
(ROC).  The detection rate is defined as the 
number of times the hypothesis-target and 
transcription-target algnments show miscues at the 
same position divided by the number of 
transcription-target miscues. The false alarm rate is 
defined as the number of times the hypothesis-
target algnment shows a miscue at a position 
where the transcription-target algnment does not.  
We stress that we are not interested in the exact 
reading miscue (wrong word, correct word but 
pronounced incorrectly, partial word, etc.) that 
occurred, which would request too specific 
information for a current state of the art system to 
give reliable feedback. Rather, we wish to design 
an indicator that can accurately report the detection 
of a miscue event whenever the text was not read 
correctly.  
In order to be able to map one alignment to the 
other, the two alignments need to be synchronized. 
Our approach synchronizes the two alignments 
over the target words in the actual story text. 
Therefore each target word represents a unique 
position within both alignments. If one or more 
insertions occurred before a certain word in the 
target sentence this event is noted in a data 
structure attached to the specific target word 
stating the number of inserted words before the 
actual spoken word. If the word was replaced with 
another word in the hypothesis or transcription, the 
wrong word will be aligned with the actual target 
word, if a word is left out, no word from the 
hypothesis or transcription will be aligned with the 
specific deleted target word. Therefore the number 
of tokens with additional information about 
substitutions, deletions and insertions in the 
hypothesis-target algnment and transcription will 
be the same for both alignments and therefore 
word-based synchronization is ensured. To 
illustrate the process a short example is given. The 
target sentence,  
it was the first day of summer vacation  
might be spoken by the child (and transcribed) as,  
it was  it was the third day of summer vacation  
and the recognition hypothesis might state, 
it it was the first day vacation 
Therefore this transcription would have two 
insertions and one substitution events. The 
hypothesis would have one insertion and two 
deletions (?of summer?). The alignments along 
with the attached information are shown in Table 
2. The miscue columns indicate an event occurring 
at a specific position in the target text or right 
before it in the case of one or more insertions 
before a certain word.  
Story 
(Target)
Trans. 
(Ref.) 
Actual 
Miscue 
Recognizer
(Hyp.) 
Hyp. 
Miscue
it it  0-0-0 it  0-0-0 
was was 0-0-0 was 0-0-1 
the the 0-0-2 the 0-0-0 
first third 1-0-0 first 0-0-0 
day day 0-0-0 day  0-0-0 
of of 0-0-0 <no_word> 0-1-0 
summer summer 0-0-0 <no_word> 0-1-0 
vacation vacation 0-0-0 vacation 0-0-0 
Table 2: Transcription-target algnment and 
hypothesis-target algnment with substitution-
deletion-insertion (s-d-i) miscue annotation. 
This setup enables us to compute the detection 
and false alarm rates based on the synchronized 
alignments. Within the Viterbi-alignment process a 
soft decision is made whether to classify a word as 
a substitution or not. If the phonemes of the 
hypothesized word match the phonemes of the 
target word by at least 75% (determined by 
phoneme alignment) the word is accepted as 
correct. This softer decision overcomes less 
important events like misses of an ?s/z? sound at 
the end of a word (e.g., ?piano? vs. ?pianos?). 
6.2 Features for Miscue Detection 
The alignment based miscue detection is only 
capable of providing a single operating point 
(detection rate / false alarm rate). We next 
introduce additional features which allow us to 
threshold the classifier output and allow the system 
to operate at any point along the ROC curve. 
In order to be able to operate the detector at 
different levels of sensitivity additional features to 
the alignment used in a classifier are a useful 
extension. The features we chose are, 
 
? the word alignment  
(either 1 if the hypothesized word aligns to the 
target story word or 0 otherwise) 
? the speech recognizer language model score  
(computed per word) 
? the speech recognizer acoustic score  
(per word, normalized by frame count) 
? the length of the pause in seconds before the 
current word (0 if no pause exists) 
? the number of phonemes in the current word 
 
The alignment is obtained as discussed in 
Section 6.1. The language model score and the 
normalized acoustic score are indicators for the 
quality of the match between the hypothesized 
word?s model and the observed features. The 
length of the pause before a word indicates a 
hesitation that might be a hint for a reading 
irregularity. The number of phonemes should 
reflect the assumption that longer words are 
generally harder to read, especially for bad readers.  
6.3 Classifier Formulation 
We trained a linear classifier based on the 
features discussed above. The use of a linear 
classifier was motivated by earlier work of Hazen 
et al (2001) which demonstrated that such a 
classifier can generate acceptable performance for 
speech recognizer confidence estimation given that 
the decision surface is relatively simple. The 
classifier can be expressed as, 
 
 
 
fpr T
vv=  
 
where pv  is the trained classification vector and 
f
v
is the feature vector described above. The final 
classification is based on a threshold value. If r is 
greater than the threshold value, the instance under 
investigation is classified as a miscue, otherwise as 
a non-event. By varying r over a certain range the 
receiver operating characteristic (ROC) curve can 
be obtained. 
6.4 Evaluation 
The data set used to train the classifier consists 
of 50% of the CU Children?s Read Story Corpus 
randomly chosen such that age and grade levels are 
distributed similarly to the entire corpus. The 
training examples represent both miscue and non-
miscue events. The miscues are those examples 
that represent substitutions, deletions, or insertions 
within the transcription-target algnment. The 
negative examples are chosen from the non-miscue 
examples. There are 4,875 miscue and 8,715 non-
miscue examples used to train the classifier. 
We tested the classifier on the remaining 50% of 
the corpus. There are approximately 5,000 miscues 
in the test set. The ROC curve resulting from the 
classification system applied to the test set is 
shown in Figure 2.  It can be seen that the overall 
performance has a relatively high detection rate of 
67% with a false alarm rate of less than 3.0%. With 
the detection rate adjusted to 70% and higher the 
false alarm rate increases rapidly. 
 
 
 
 
DT (%) 55.0 60.0 65.0 70.0 75.0 80.0 
FA (%) 2.6 2.7 2.9 5.1 19.8 36.4 
Figure 2: Detection rate vs. false alarm rate ROC 
for the CU Children?s Read Story corpus.  
Example operating points are shown below. 
7 Conclusions 
In this paper we have described the Colorado 
Literacy Tutor (CLT) which aims to improve 
literacy and student achievement in public schools. 
We extended on our previous work in several 
novel aspects.  First, we have collected and 
annotated a children?s speech corpus in terms of a 
set of labeled event conditions which we believe 
strongly correlate to speech recognition error.  In 
fact while these events make up nearly 8% of the 
data, they were shown to account for 
approximately 30% of the word errors in a state-of-
the-art speech recognition system.  To our 
knowledge, previous work has not considered such 
a detailed word error analysis on a children?s 
speech corpus.  We then provided our initial 
framework for detecting oral reading miscues.  
Using a simple linear classifier and using features 
derived from a speech recognizer, we 
demonstrated that 67% of reading miscues can be 
detected at a false alarm rate of 3%.  While this 
system appears to outperform the previous results 
presented in Tam et al (2003), we point out that 
there is currently no standardized test set available 
to directly compare those systems.  Therefore, the 
audio corpus and event labeling presented in this 
paper will be made available to researchers to 
promote community-wide benchmarking.  In the 
future we plan to correlate the miscue detection 
performance with the event labeling strategy 
outlined in Section 5 of the paper.  We expect that 
such an error analysis will continue to provide 
insight to areas for system development. 
8 Acknowledgements 
   This work was supported by grants from the 
National Science Foundation's ITR and IERI 
Programs under grants NSF/ITR: REC-0115419, 
NSF/IERI: EIA-0121201, NSF/ITR: IIS-0086107, 
NSF/IERI: 1R01HD-44276.01; and the Coleman 
Institute for Cognitive Disabilities. The views 
expressed in this paper do not necessarily represent 
the views of the NSF. 
References  
R. Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma, 
J. Movellan, S. Schwartz, D. Wade-Stein, W. Ward, J. 
Yan. 2003. Perceptive Animated Interfaces: First 
Steps Toward a New Paradigm for Human Computer 
Interaction. Proceedings of the IEEE, Vol. 91, No. 9, 
pp. 1391-1405. 
S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and 
R. Harshman. 1990. Indexing by Latent Semantic 
Analysis. Journal of the Society for Information 
Science, vol. 41, no. 6, pp. 391-407. 
J. Fogarty, L. Dabbish, D. Steck, and J. Mostow.  2001. 
Mining a Database of Reading Mistakes: For What 
should an Automated Reading Tutor Listen? In J. D. 
Moore, C. L. Redfield, and W. L. Johnson (Eds.), 
Artificial Intelligence in Education:  AI-ED in the 
Wired and Wireless Future, pp. 422-433. 
A. Hagen, B. Pellom, and R. Cole. 2003. Children?s 
Speech Recognition with Application to Interactive 
Books and Tutors. ASRU-2003, St. Thomas, USA. 
A. Hagen, B. Pellom, S. Van Vuuren, R. Cole. 2004.   
Advances in Children?s Speech Recognition within an 
Interactive Literacy Tutor. HLT-NAACL, Boston 
Massachusetts, USA. 
T. Hazen, S. Seneff, and J. Polifroni. 2002. Recognition 
Confidence Scoring and its Use in Speech 
Understanding Systems. Computer Speech and 
Language, Vol. 16,No. 1, pp. 49-67. 
E. Kintsch, D. Steinhart, G. Stahl, C. Matthews, R. 
Lamb, and LRG. 2000. Developing Summarization 
Skills through the Use of LSA-based Feedback. 
Interactive Learning Environments, Vol. 8, pp. 87-
109. 
T. Landauer and S. Dumais. 1997. A Solution to Plato's 
Problem: The Latent Semantic Analysis Theory of 
Acquisition, Induction and Representation of 
Knowledge. Psych. Review, Vol. 104, pp. 211-240. 
M. McCandless. 1992. Word Rejection for a Literacy 
Tutor. Bachelor of Science Thesis, MIT. 
J. Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo, 
S. Eitelman, C. Huang, B. Junker, M. B. Sklar, and B. 
Tobin. 2003. Evaluation of an Automated Reading 
Tutor that Listens:  Comparison to Human Tutoring 
and Classroom Instruction. Journal of Educational 
Computing Research, 29(1), 61-117. 
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin. 
2002. Predicting Oral Reading Miscues. ICSLP-02, 
Denver, Colorado. 
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane. 
1994. A Prototype Reading Coach that Listens. 
AAAI-94, Seattle, WA, pp. 785-792.  
B. Pellom. 2001. SONIC: The University of Colorado 
Continuous Speech Recognizer. Technical Report TR-
CSLR-2001-01, University of Colorado. 
B. Pellom, K. Hacioglu. 2003. Recent Improvements in 
the CU SONIC ASR System for Noisy Speech: The 
SPINE Task. Proc. ICASSP, Hong Kong. 
K. Shobaki, J.-P. Hosom, and R. Cole. 2000. The OGI 
Kids' Speech Corpus and Recognizers. Proc. ICSLP-
2000, Beijing, China. 
D. Steinhart. 2001. Summary Street: An Intelligent 
Tutoring System for Improving Student Writing 
through the Use of Latent Semantic Analysis. Ph.D. 
Dissertation, Dept. Psychology, Univ. of Colorado, 
Boulder, CO. 
Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee. 2003. 
Training a Confidence Measure for a Reading Tutor 
that Listens. Proc. Eurospeech, Geneva, Switzerland, 
3161-3164. 
U. Yapanel, J. H.L. Hansen. 2003. A New Perspective 
on Feature Extraction for Robust In-vehicle Speech 
Recognition. Proc. Eurospeech, Geneva, Switzerland. 
V. Zue, S. Seneff, J. Polifroni, H. Meng, J. Glass. 1996. 
Multilingual Human-Computer Interactions: From 
Information Access to Language Learning. ICSLP-96, 
Philadelphia, PA. 
University of Colorado Dialog Systems for 
Travel and Navigation 
B. Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. Pradhan 
Center for Spoken Language Research, University of Colorado 
Boulder, Colorado 80303, USA 
{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.edu 
 
ABSTRACT 
This paper presents recent improvements in the development of 
the University of Colorado ?CU Communicator? and ?CU-
Move? spoken dialog systems. First, we describe the CU 
Communicator system that integrates speech recognition, 
synthesis and natural language understanding technologies using 
the DARPA Hub Architecture. Users are able to converse with an 
automated travel agent over the phone to retrieve up-to-date 
travel information such as flight schedules, pricing, along with 
hotel and rental car availability.  The CU Communicator has 
been under development since April of 1999 and represents our 
test-bed system for developing robust human-computer 
interactions where reusability and dialogue system portability 
serve as two main goals of our work.  Next, we describe our more 
recent work on the CU Move dialog system for in-vehicle route 
planning and guidance.  This work is in joint collaboration with 
HRL and is sponsored as part of the DARPA Communicator 
program.  Specifically, we will provide an overview of the task, 
describe the data collection environment for in-vehicle systems 
development, and describe our initial dialog system constructed 
for route planning. 
1. CU COMMUNICATOR 
1.1 Overview  
The Travel Planning Task 
The CU Communicator system [1,2] is a Hub compliant 
implementation of the DARPA Communicator task [3].  The 
system combines continuous speech recognition, natural 
language understanding and flexible dialogue control to enable 
natural conversational interaction by telephone callers to access 
information from the Internet pertaining to airline flights, hotels 
and rental cars.  Specifically, users can describe a desired airline 
flight itinerary to the Communicator and use natural dialog to 
negotiate a flight plan.  Users can also inquire about hotel 
availability and pricing as well as obtain rental car reservation 
information.   
System Overview 
The dialog system is composed of a Hub and several servers as 
shown in Fig. 1.  The Hub is used as a centralized message router 
through which servers can communicate with one another [4].  
Frames containing keys and values are emitted by each server, 
routed by the hub, and received by a secondary server based on 
rules defined in a ?Hub script?.   
 
 
 
 
 
 
 
 
 
Figure 1.  Block diagram of the functional components that 
comprise the CU Communicator system1. 
1.2 Audio Server 
The audio server is responsible for answering the incoming call, 
playing prompts and recording user input.  Currently, our system 
uses the MIT/MITRE audio server that was provided to DARPA 
Communicator program participants.  The telephony hardware 
consists of an external serial modem device that connects to the 
microphone input and speaker output terminals on the host 
computer.  The record process is pipelined to the speech 
recognition server and the play process is pipelined the text-to-
speech server.  This audio server does not support barge-in. 
Recently we have developed a new audio server that supports 
barge-in using the Dialogic hardware platform.  The new audio 
server implements a Fast Normalized Least-Mean-Square (LMS) 
algorithm for software-based echo cancellation.  During 
operation, the echo from the system speech is actively cancelled 
from the recorded audio to allow the user to cut through while 
                                                          
1
 This work was supported by DARPA through SPAWAR under 
Grant No. N66001-002-8906.  The ?CU Move? system is 
supported in part through a joint collaboration with HRL 
Laboratories. 
 
 
 
Language
Generator
Language
enerator
Hub
Speech 
Recognizer
Speech 
Recognizer
Speech 
Synthesizer
Speech 
Synthesizer
Semantic
Parser
Se antic
Parser
Dialogue
Manager
Dialogue
anager
Data Base / 
Backend
Data Base / 
Backend
Confidence
Server
Confidence
Server
Audio ServerAudio Server
www
the system is speaking.  The new audio server operates in the 
Linux environment and is currently being field-tested at CSLR.  
Because the server implements software-based echo cancellation, 
it can work on virtually any low-cost Dialogic hardware 
platform.  This server will be made available to the research 
community as a resource in the near future. 
1.3 Speech Recognizer 
We are currently using the Carnegie Mellon University Sphinx-II 
system [5] in our speech recognition server. This is a semi-
continuous Hidden Markov Model recognizer with a class 
trigram language model. The recognition server receives the 
input vectors from the audio server. The recognition server 
produces a word lattice from which a single best hypothesis is 
picked and sent to the hub for processing by the dialog manager. 
Acoustic Modeling 
During dialog interaction with the user, the audio server sends 
the acoustic samples to three Sphinx-II speech recognizers.  
While the language model is the same for each decoder, the 
acoustic models consist of (i) speaker independent analog 
telephone, (ii) female adapted analog telephone, and (iii) cellular 
telephone adapted acoustic model sets.   Each decoder outputs a 
word string hypothesis along with a word-sequence probability 
for the best path.  An intermediate server is used to examine each 
hypothesis and pass the most likely word string onto the natural 
language understanding module.   
Language Modeling 
The Communicator system is designed for end users to get up-to-
date worldwide air travel, hotel and rental car information via the 
telephone. In the task there are word lists for countries, cities, 
states, airlines, etc.  To train a robust language model, names are 
clustered into different classes. An utterance with class tagging is 
shown in Fig.2.  In this example, city, hour_number, and am_pm 
are class names. 
Figure 2.  Examples of class-based and grammar-based 
language modeling  
Each commonly used word takes one class. The probability of 
word Wi given class Ci is estimated from training corpora. After 
the corpora are correctly tagged, a back-off class-based trigram 
language model can be computed from the tagged corpora.  We 
use the CMU-Cambridge Statistical Language Modeling Toolkit 
to compute our language models. 
More recently, we have developed a dialog context dependent 
language model (LM) combining stochastic context free 
grammars (SCFGs) and n-grams [6,7].  Based on a spoken 
language production model in which a user picks a set of 
concepts with respective values and constructs word sequences 
using phrase generators associated with each concept in 
accordance with the dialog context, this LM computes the 
probability of a word, P(W), as 
 
         P(W) = P(W/C) P(C/S)          (1) 
 
where W is the sequence of words, C is the sequence of concepts 
and S is the dialog context. Here, the assumptions are (i) S is 
given, (ii) W is independent of S but C, and (iii) W and C 
associations are unambiguous. This formulation can be 
considered as a general extension of the standard class word 
based statistical language model as seen in Fig. 2. 
 
The first term in (1) is modeled by SCFGs, one for each concept. 
The concepts are classes of phrases with the same meaning. Each 
SCFG is compiled into a stochastic recursive transition network 
(STRN). Our grammar is a semantic grammar since the 
nonterminals correspond to semantic concepts instead of 
syntactic constituents. The set of task specific concepts is 
augmented with a single word, multiple word and a small number 
of broad but unambigious part of speech (POS) classes to 
account for the phrases that are not covered by the grammar. 
These classes are considered as "filler" concepts within a unified 
framework. The second term in (1) is modeled as a pool of 
concept n-gram LMs. That is, we have a separate LM for each 
dialog context. At the moment, the dialog context is selected as 
the last question prompted by the system, as it is very simple and 
yet strongly predictive and constraining. SCFG and n-gram 
probabilities are learned by simple counting and smoothing. Our 
semantic grammars have a low degree of ambiguity and therefore 
do not require computationally intensive stochastic training and 
parsing techniques. 
 
Experimental results with N-best list rescoring were found 
promising (5-6% relative improvement in WER).  In addition, we 
have shown that a dynamic combining of our new LM and the 
standard class word n-gram (the LM currently in use in our 
system) should result in further improvements. At the present, we 
are interfacing the grammar LM to the speech recognizer using a 
word graph. 
1.4 Confidence Server 
Our prior work on confidence assessment has considered 
detection and rejection of word-level speech recognition errors 
and out-of-domain phrases using language model features [8].  
More recently [9], we have considered detection and rejection of 
misrecognized units at the concept level.  Because concepts are 
used to update the state of the dialog system, we believe that 
concept level confidence is vitally important to ensuring a 
graceful human-computer interaction.  Our current work on 
concept error detection has considered language model features 
(e.g., LM back-off behavior, language model score) as well as 
acoustic features from the speech recognizer (e.g., normalized 
acoustic score, lattice density, phone perplexity).  Confidence 
Original Utterance 
I want to go from Boston to Portland around nine a_m 
Class-Tagged Utterance 
I want to go from [city:Boston] to [city:Portland] 
around [hour_number:nine] [am_pm:a_m] 
Concept-Tagged Utterance 
[I_want: I want to go] [depart_loc: from Boston] 
[arrive_loc: to Portland] [time:around nine a_m] 
features are combined to compute word-level, concept-level, and 
utterance-level confidence scores.  
1.5 Language Understanding 
We use a modified version of the Phoenix [10] parser to map the 
speech recognizer output onto a sequence of semantic frames. A 
Phoenix frame is a named set of slots, where the slots represent 
related pieces of information. Each slot has an associated 
context-free semantic grammar that specifies word string patterns 
that can fill the slot. The grammars are compiled into Recursive 
Transition Networks, which are matched against the recognizer 
output to fill slots. Each filled slot contains a semantic parse tree 
with the slot name as root.  
Phoenix has been modified to also produce an extracted 
representation of the parse that maps directly onto the task 
concept structures. For example, the utterance  
?I want to go from Boston to Denver Tuesday morning?  
would produce the extracted parse: 
Flight_Constraint: Depart_Location.City.Boston 
Flight_Constraint: Arrive_Location.City.Denver 
Flight Constraints:[Date_Time].[Date].[Day_Name].tuesday 
                             [Time_Range].[Period_Of_Day].morning 
1.6 Dialog Management 
The Dialogue Manager controls the system?s interaction with the 
user and the application server. It is responsible for deciding 
what action the system will take at each step in the interaction. 
The Dialogue Manager has several functions. It resolves 
ambiguities in the current interpretation; Estimates confidence in 
the extracted information; Clarifies the interpretation with the 
user if required; Integrates new input with the dialogue context; 
Builds database queries (SQL); Sends information to NL 
generation for presentation to user; and prompts the user for 
missing information. 
We have developed a flexible, event driven dialogue manager in 
which the current context of the system is used to decide what to 
do next. The system does not use a dialogue network or a 
dialogue script, rather a general engine operates on the semantic 
representations and the current context to control the interaction 
flow.  The Dialogue Manager receives the extracted parse. It then 
integrates the parse into the current context. Context consists of a 
set of frames and a set of global variables. As new extracted 
information arrives, it is put into the context frames and 
sometimes used to set global variables. The system provides a 
general-purpose library of routines for manipulating frames. 
This ?event driven? architecture functions similar to a production 
system. An incoming parse causes a set of actions to fire which 
modify the current context. After the parse has been integrated 
into the current context, the DM examines the context to decide 
what action to take next. The DM attempts the following actions 
in the order listed: 
? Clarify if necessary  
? Sign off if all done  
? Retrieve data and present to user  
? Prompt user for required information  
The rules for deciding what to prompt for next are very 
straightforward. The frame in focus is set to be the frame 
produced in response to the user, or to the last system prompt.  
? If there are unfilled required slots in the focus frame, then 
prompt for the highest priority unfilled slot in the frame. 
? If there are no unfilled required slots in the focus frame, 
then prompt for the highest priority missing piece of 
information in the context.  
Our mechanism does not have separate ?user initiative? and 
?system initiative? modes. If the system has enough information 
to act on, then it does it. If it needs information, then it asks for 
it. The system does not require that the user respond to the 
prompt. The user can respond with anything and the system will 
parse the utterance and set the focus to the resulting frame. This 
allows the user to drive the dialog, but doesn?t require it. The 
system prompts are organized locally, at the frame level. The 
dialog manager or user puts a frame in focus, and the system tries 
to fill it. This representation is easy to author, there is no separate 
dialog control specification required. It is also robust in that it 
has a simple control that has no state to lose track of. 
An additional benefit of Dialog Manager mechanism is that it is 
very largely declarative. Most of the work done by a developer 
will be the creation of frames, forms and grammars. The system 
developer creates a task file that specifies the system ontology 
and templates for communicating about nodes in the hierarchy. 
The templates are filled in from the values in the frames to 
generate output in the desired language. This is the way we 
currently generate SQL queries and user prompts. An example 
task frame specification is: 
Frame:Air 
 [Depart_Loc]+ 
    Prompt: "where are you departing from" 
    [City_Name]* 
 Confirm: "You are departing from $([City_Name]).  
    Is that correct?" 
 Sql: "dep_$[leg_num] in (select airport_code from 
 airport_codes where city like '!%' $(and state_province 
like '[Depart_Loc].[State]' ) )" 
    [Airport_Code]* 
 
This example defines a frame with name Air and slot 
[Depart_Loc]. The child nodes of Depart_Loc are are 
[City_Name] and [Airport_Code]. The ?+? after [Depart_Loc] 
indicates that it is a mandatory field. The Prompt string is the 
template for prompting for the node information. The ?*? after 
[City_Name] and [Airport_Code] indicate that if either of them is 
filled, the parent node [Depart_Loc] is filled. The Confirm string 
is a template to prompt the user to confirm the values. The SQL 
string is the template to use the value in an SQL query to the 
database. 
The system will prompt for all mandatory nodes that have 
prompts. Users may specify information in any order, but the 
system will prompt for whatever information is missing until the 
frame is complete.   
1.7 Database & Internet Interface  
The back-end interface consists of an SQL database and domain-
specific Perl scripts for accessing information from the Internet.  
During operation, database requests are transmitted by the Dialog 
Manager to the database server via a formatted frame. 
The back-end consists of a static and dynamic information 
component.  Static tables contain data such as conversions 
between 3-letter airport codes and the city, state, and country of 
the airport (e.g., BOS for Boston Massachusetts).  There are over 
8000 airports in our database, 200 hotel chains, and 50 car rental 
companies.  The dynamic information content consists of 
database tables for car, hotel, and airline flights.   
When a database request is received, the Dialog Manager?s SQL 
command is used to select records in local memory.  If no 
records are found to match, the back-end can submit an HTTP-
based request for the information via the Internet.  Records 
returned from the Internet are then inserted as rows into the local 
SQL database and the SQL statement is once again applied.   
1.8 Language Generation 
The language generation module uses templates to generate text 
based on dialog speech acts.  Example dialog acts include 
?prompt? for prompting the user for needed information, 
?summarize? for summarization of flights, hotels, and rental cars, 
and ?clarify? for clarifying information such as departure and 
arrival cities that share the same name. 
1.9 Text-to-Speech Synthesis 
For audio output, we have developed a domain-dependent 
concatenative speech synthesizer.  Our concatenative synthesizer 
can adjoin units ranging from phonemes, to words, to phrases 
and sentences.   For domain modeling, we use a voice talent to 
record entire task-dependent utterances  (e.g., ?What are your 
travel plans??) as well as short phrases with carefully determined 
break points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departs 
Anchorage at?).    Each utterance is orthographically transcribed 
and phonetically aligned using a HMM-based recognizer.   Our 
research efforts for data collection are currently focused on 
methods for reducing the audible distortion at segment 
boundaries, optimization schemes for prompt generation, as well 
as tools for rapidly correcting boundary misalignments.  In 
general, we find that some degree of hand-correction is always 
required in order to reduce distortions at concatenation points. 
During synthesis, the text is automatically divided into individual 
sentences that are then synthesized and pipelined to the audio 
server.  A text-to-phoneme conversion is applied using a 
phonetic dictionary.  Words that do not appear in the phonetic 
dictionary are automatically pronounced using a multi-layer 
perceptron based pronunciation module.  Here, a 5-letter context 
is extracted from the word to be pronounced.  The letter input is 
fed through the MLP and a phonetic symbol (or possibly epsilon) 
is output by the network.  By sliding the context window, we can 
extract the phonetic pronunciation of the word.   The MLP is 
trained using letter-context and symbol output pairs from a large 
phonetic dictionary. 
The selection of units to concatenate is determined using a hybrid 
search algorithm that operates at the word or phoneme level.  
During synthesis, sections of word-level text that have been 
recorded are automatically concatenated.  Unrecorded words or 
word sequences are synthesized using a Viterbi beam search 
across all available phonetic units.  The cost function includes 
information regarding phonetic context, pitch, duration, and 
signal amplitude.  Audio segments making up the best-path are 
then concatenated to generate the final sentence waveform.   
2. DATA COLLECTION & EVALUATION 
2.1 Data Collection Efforts 
Local Collection Effort 
The Center for Spoken Language Research maintains a dialup 
Communicator system for data collection1. Users wishing to use 
the dialogue system can register at our web site [1] and receive a 
PIN code and system telephone number. To date, our system has 
fielded over 1750 calls totaling over 25,000 utterances from 
nearly 400 registered users.  
NIST Multi-Site Data Collection 
During the months of June and July of 2000, The National 
Institute of Standards (NIST) conducted a multi-site data 
collection effort for the nine DARPA Communicator 
participants.  Participating sites included: AT&T, IBM, BBN, 
SRI, CMU, Colorado, MIT, Lucent, and MITRE.  In this data 
collection, a pool of potential users was selected from various 
parts of the United States by a market research firm.  The 
selected subjects were native speakers of American English who 
were possible frequent travelers.  Users were asked to perform 
nine tasks.  The first seven tasks consisted of fixed scenarios for 
one-way and round-trip flights both within and outside of the 
United States. The final two tasks consisted of users making 
open-ended business or vacation.   
2.2 System Evaluation 
Task Completion 
A total of 72 calls from NIST participants were received by the 
CU Communicator system.  Of these, 44 callers were female and 
28 were male.  Each scenario was inspected by hand and 
compared against the scenario provided by NIST to the subject. 
For the two open-ended tasks, judgment was made based on what 
the user asked for with that of the data provided to the user. In 
total, 53/72 (73.6%) of the tasks were completed successfully.   
A detailed error analysis can be found in [11]. 
Word Error Rate Analysis 
A total of 1327 utterances were recorded from the 72 NIST calls.  
Of these, 1264 contained user speech.  At the time of the June 
2000 NIST evaluation, the CU Communicator system did not 
implement voice-based barge-in.  We noticed that one source of 
error was due to users who spoke before the recording process 
was started.  Even though a tone was presented to the user to 
signify the time to speak, 6.9% of the utterances contained 
instances in which the user spoke before the tone.  Since all users 
were exposed to several other Communicator systems that 
                                                          
2
 The system can be accessed toll-free at 1-866-735-5189 
employed voice barge-in, there may be some effect from 
exposure to those systems. Table 3 summarizes the word error 
rates for the system utilizing the June 2000 NIST data as the test 
set.  Overall, the system had a word error rate (WER) of 26.0% 
when parallel gender-dependent decoders were utilized. Since 
June of 2000, we have collected an additional 15,000 task-
dependent utterances.  With the extra data, we were able to 
remove our dependence on the CMU Communicator training 
data [12].  When the language model was reestimated and 
language model weights reoptimized using only CU 
Communicator data, the WER dropped from 26.0% to 22.5%.  
This amounts to a 13.5% relative reduction in WER. 
Table 1: CU Communicator Word Error Rates for (A) 
Speaker Independent acoustic models and June 2000 
language model, (B) Gender-dependent parallel recognizers 
with June 2000 Language Model, and (C) Language Model 
retrained in December 2000. 
June 2000 NIST Evaluation Data, 1264 
utterances, 72 speakers 
Word Error 
Rate 
(A) Speaker Indep. HMMs (LM#1) 29.8% 
(B) Gender Dependent HMMs (LM#1) 26.0% 
(C) Gender Dependent HMMs (LM#2)  22.5% 
 
Core Metrics 
Sites in the DARPA Communicator program agreed to log a 
common set of metrics for their systems. The proposed set of 
metrics was: Task Completion, Time to Completion, Turns to 
Completion, User Words/Turn, System Words/Turn, User 
Concepts/Turn, Concept Efficiency, State of Itinerary, Error 
Messages, Help Messages, Response Latency, User Words to 
Completion, System Words to Completion, User Repeats, System 
Repeats/Reprompts, Word Error, Mean Length of System 
Utterance, and Mean Length of System Turn. 
Table 2: Dialogue system evaluation metrics 
Item Min Mean Max 
Time to Completion (secs) 120.9 260.3 537.2 
Total Turns to Completion 23 37.6 61 
Response Latency (secs) 1.5 1.9 2.4 
User Words to Task End 19 39.4 105 
System Words to End 173 331.9 914 
Number of Reprompts 0 2.4 15 
 
Table 2 summarizes results obtained from metrics derived 
automatically from the logged timing markers for the calls in 
which the user completed the task assigned to them.  The average 
time to task completion is 260.  During this period there are an 
average of 19 user turns and 19 computer turns (37.6 average 
total turns).  The average response latency was 1.86 seconds.  
The response latency also includes the time required to access the 
data live from the Internet travel information provider. 
3. CU MOVE 
3.1 Task Overview 
The ?CU Move? system represents our work towards achieving 
graceful human-computer interaction in automobile 
environments.  Initially, we have considered the task of vehicle 
route planning and navigation.  As our work progresses, we will 
expand our dialog system to new tasks such as information 
retrieval and summarization and multimedia access. 
The problem of voice dialog within vehicle environments offers 
some important speech research challenges. Speech recognition 
in car environments is in general fragile, with word-error-rates 
(WER) ranging from 30-65% depending on driving conditions. 
These changing environmental conditions include speaker 
changes (task stress, emotion, Lombard effect, etc.) as well as the 
acoustic environment (road/wind noise from windows, air 
conditioning, engine noise, exterior traffic, etc.).   
In developing the CU-Move system [13,14], there are a number 
of research challenges that must be overcome to achieve reliable 
and natural voice interaction within the car environment. Since 
the speaker is performing a task (driving the vehicle), the driver 
will experience a measured level of user task stress and therefore 
this should be included in the speaker-modeling phase. Previous 
studies have clearly shown that the effects of speaker stress and 
Lombard effect can cause speech recognition systems to fail 
rapidly. In addition, microphone type and placement for in-
vehicle speech collection can impact the level of acoustic 
background noise and speech recognition performance.    
3.2 Signal Processing  
Our research for robust recognition in automobile environments 
is concentrated on development of an intelligent microphone 
array.  Here, we employ a Gaussian Mixture Model (GMM) 
based environmental classification scheme to characterize the 
noise conditions in the automobile.  By integrating an 
environmental classification system into the microphone array 
design, decisions can be made as to how best to utilize a noise-
adaptive frequency-partitioned iterative enhancement algorithm 
[15,16] or model-based adaptation algorithms [17,18] during 
decoding to optimize speech recognition accuracy on the beam-
formed signal. 
3.3 Data Collection 
A five-channel microphone array was constructed using Knowles 
microphones and a multi-channel data recorder housing built 
(Fostex) for in-vehicle data collection. An additional reference 
microphone is situated behind the driver?s seat.  Fig. 3 shows the 
constructed microphone array and data recorder housing. 
      
Figure 3: Microphone array and reference microphone (left), 
Fostex multi-channel data recorder (right). 
As part of the CU-Move system formulation, a two phase data 
collection plan has been initiated. Phase I focuses on collecting 
acoustic noise and probe speech from a variety of cars and 
driving conditions. Phase II focuses on a extensive speaker 
collection across multiple U.S. sites. A total of eight vehicles 
have been selected for acoustic noise analysis. These include the 
following: a compact car, minivan, cargo van, sport utility 
vehicle (SUV), compact and full size trucks, sports car, full size 
luxury car.  A fixed 10 mile route through Boulder, CO was used 
for Phase I data collection. The route consisted of city (25 & 
45mph) and highway driving (45 & 65mph). The route included 
stop-and-go traffic, and prescribed locations where 
driver/passenger windows, turn signals, wiper blades, air 
conditioning were operated. Each data collection run per car 
lasted approximately 35-45 minutes.  A detailed acoustic analysis 
of Phase I data can be found in [13]. Our plan is to begin Phase 
II speech/dialogue data collection during spring 2001, which will 
include (i) phonetically balanced utterances, (ii) task-specific 
vocabularies, (iii) natural extemporaneous speech, and (iv) 
human-to-human and Wizard-of-Oz (WOZ) interaction with CU-
Communicator and CU-Move dialog systems. 
3.4 Prototype Dialog System 
Finally, we have developed a prototype dialog system for data 
collection in the car environment.  The dialog system is based on 
the MIT Galaxy-II Hub architecture with base system 
components derived from the CU Communicator system [1].  
Users interacting with the dialog system can enter their origin 
and destination address by voice. Currently, 1107 street names 
for Boulder, CO area are modeled.  The system can resolve street 
addresses by business name via interaction with an Internet 
telephone book.  This allows users to ask more natural route 
queries (e.g., ?I need an auto repair shop?, or ?I need to get to the 
Boulder Marriott?).  The dialog system automatically retrieves 
the driving instructions from the Internet using an online WWW 
route direction provider.  Once downloaded, the driving 
directions are queried locally from an SQL database.  During 
interaction, users mark their location on the route by providing 
spoken odometer readings.  Odometer readings are needed since 
GPS information has not yet been integrated into the prototype 
dialog system. Given the odometer reading of the vehicle as an 
estimate of position, route information such as turn descriptions, 
distances, and summaries can be queried during travel (e.g., 
"What's my next turn", "How far is it", etc.).  
The prototype system uses the CMU Sphinx-II speech recognizer 
with cellular telephone acoustic models along with the Phoenix 
Parser [10] for semantic parsing.  The dialog manager is mixed-
initiative and event driven.  For route guidance, the natural 
language generator formats the driving instructions before 
presentation to the user by the text-to-speech server.   For 
example, the direction,  "Park Ave W. becomes 22nd St." is 
reformatted to, "Park Avenue West becomes Twenty Second  
Street".  Here, knowledge of the task-domain can be used to 
significantly improve the quality of the output text.   For speech 
synthesis, we have developed a Hub-compliant server that 
interfaces to the AT&T NextGen speech synthesizer.   
3.5 Future Work 
We have developed a Hub compliant server that interfaces a 
Garmin GPS-III global positioning device to a mobile computer 
via a serial port link.  The GPS server reports vehicle velocity in 
the X,Y,Z directions as well as real-time updates of  vehicle 
position in latitude and longitude.  HRL Laboratories has 
developed a route server that interfaces to a major navigation 
content provider.  The HRL route server can take GPS 
coordinates as inputs and can describe route maneuvers in terms 
of GPS coordinates.  In the near-term, we will interface our GPS 
server to the HRL route server in order to provide real-time 
updating of vehicle position.  This will eliminate the need for 
periodic location update by the user and also will allow for more 
interesting dialogs to be established (e.g., the computer might 
proactively tell the user about upcoming points of interest, etc.). 
 
4. REFERENCES 
[1] http://communicator.colorado.edu 
[2] W. Ward, B. Pellom, "The CU Communicator System," IEEE 
Workshop on Automatic Speech Recognition and Understanding, 
Keystone Colorado, December, 1999. 
[3] http://fofoca.mitre.org 
[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V., 
?Galaxy-II: A Reference Architecture for Conversational System 
Development,? Proc. ICSLP, Sydney Australia, Vol. 3, pp. 931-
934, 1998. 
[5] Ravishankar, M.K., ?Efficient Algorithms for Speech 
Recognition?. Unpublished Dissertation CMU-CS-96-138, 
Carnegie Mellon University, 1996 
[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent Language 
Modeling Using N-Grams and Stochastic Context-Free Grammars", 
Proc. IEEE ICASSP, Salt Lake City, May 2001. 
[7] K. Hacioglu, W. Ward, "Combining Language Models : Oracle 
Approach", Proc. Human Language Technology Conference, San 
Diego, March 2001. 
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "Confidence 
Measures for Dialogue Management in the CU Communicator 
System," Proc. IEEE ICASSP, Istanbul Turkey, June 2000. 
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M. Pardo, 
"Confidence Measures for Dialogue Systems," Proc. IEEE ICASSP, 
Salt Lake City, May 2001.  
[10] Ward, W., ?Extracting Information From Spontaneous Speech?, 
Proc. ICSLP, September 1994. 
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: An 
Architecture for Dialogue Systems", Proc. ICSLP, Beijing China, 
November 2000. 
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,  
Brennan, R., Bennett, K., Allen, J., ?Data Collection and 
Processing in the Carnegie Mellon Communicator,?   Proc. 
Eurospeech-99, Budapest, Hungary. 
[13] J.H.L. Hansen, J. Plucienkowski, S. Gallant, B.L. Pellom, W. Ward, 
"CU-Move: Robust Speech Processing for In-Vehicle Speech 
Systems," Proc. ICSLP, vol. 1, pp. 524-527, Beijing, China, Oct. 
2000. 
[14] http://cumove.colorado.edu/ 
[15] J.H.L. Hansen, M.A. Clements, ?Constrained Iterative Speech 
Enhancement with Application to Speech Recognition,? IEEE 
Trans. Signal Proc., 39(4):795-805, 1991. 
[16] B. Pellom, J.H.L. Hansen, ?An Improved Constrained Iterative 
Speech Enhancement Algorithm for Colored Noise Environments," 
IEEE Trans. Speech & Audio Proc., 6(6):573-79, 1998. 
[17] R. Sarikaya, J.H.L. Hansen, "Improved Jacobian Adaptation for 
Fast Acoustic Model Adaptation in Noisy Speech Recognition," 
Proc. ICSLP, vol. 3, pp. 702-705, Beijing, China, Oct. 2000. 
[18] R. Sarikaya, J.H.L. Hansen, "PCA-PMC: A novel use of a priori 
knowledge for fast model combination," Proc. ICASSP, vol. II, pp. 
1113-1116, Istanbul, Turkey, June 2000. 
Advances in Children?s Speech Recognition  
within an Interactive Literacy Tutor 
 
Andreas Hagen, Bryan Pellom, Sarel Van Vuuren, and Ronald Cole 
Center for Spoken Language Research 
University of Colorado at Boulder 
http://cslr.colorado.edu 
 
 
 
 
Abstract1 
In this paper we present recent advances in 
acoustic and language modeling that improve 
recognition performance when children read 
out loud within digital books. First we extend 
previous work by incorporating cross-
utterance word history information and dy-
namic n-gram language modeling. By addi-
tionally incorporating Vocal Tract Length 
Normalization (VTLN), Speaker-Adaptive 
Training (SAT) and iterative unsupervised 
structural maximum a posteriori linear regres-
sion (SMAPLR) adaptation we demonstrate a 
54% reduction in word error rate.  Next, we 
show how data from children?s read-aloud 
sessions can be utilized to improve accuracy 
in a spontaneous story summarization task.  
An error reduction of 15% over previous pub-
lished results is shown.  Finally we describe a 
novel real-time implementation of our re-
search system that incorporates time-adaptive 
acoustic and language modeling. 
1 Introduction 
Pioneering research by MIT and CMU as well as more 
recent work by the IBM Watch-me-Read Project have 
demonstrated that speech recognition can play an effec-
tive role in systems designed to improve children?s 
reading abilities (Mostow et al, 1994; Zue et al, 1996). 
In CMU?s Project LISTEN, for example, the tutor oper-
ates by prompting children to read individual sentences 
out loud.  The tutor listens to the child using speech 
recognition and extracts features that can be used to 
detect oral reading miscues (Mostow et al, 2002; Tam 
et al 2003).   Upon detecting reading miscues, the tutor 
provides appropriate feedback to the child.  Recent re-
                                                        
1
 This work was supported in part by grants from the National Science 
Foundation's Information Technology Research (ITR) Program and 
the Interagency Educational Research Initiative (IERI) under grants 
NSF/ITR: REC-0115419, NSF/IERI: EIA-0121201, NSF/ITR: IIS-
0086107, NSF/IERI: 1R01HD-44276.01, NSF: INT-0206207; and the 
Coleman Institute for Cognitive Disabilities. The views expressed in 
this paper do not necessarily represent the views of the NSF. 
sults show that such automated reading tutors can im-
prove student achievement (Mostow et al 2003). Pro-
viding real time feedback by highlighting words as the 
are read out loud is the basis of at least one commercial 
product today (http://www.soliloquy.com).  
Cole et al (2003) and Wise et al (in press) describe 
a new scientifically-based literacy program, Founda-
tions to Fluency, in which a virtual tutor?a lifelike 3D 
computer model?interacts with children in multimodal 
learning tasks to teach them to read. A key component 
of this program is the Interactive Book, which combines 
real-time speech recognition, facial animation, and natu-
ral language understanding capabilities to teach children 
to read and comprehend text.  Interactive Books are 
designed to improve student achievement by helping 
students to learn to read fluently, to acquire new knowl-
edge through deep understanding of what they read, to 
make connections to other knowledge, and to express 
their ideas concisely through spoken or written summa-
ries. Transcribed spoken summaries can be graded 
automatically to provide feedback to the student about 
their comprehension.  
During reading out loud activities in Interactive 
Books, the goal is to design a computer interface and 
speech recognizer that combine to teach the student to 
read fluently and naturally.  Here, speech recognition is 
used to track a child?s position within the text during 
read-aloud sessions in addition to providing timing and 
confidence information which can be used for reading 
assessment. The speech recognizer must follow the stu-
dents verbal behaviors accurately and quickly, so the 
cursor (or highlighted word) appears at the right place 
and right time when the student is reading fluently, and 
pauses when the student hesitates to sound out a word. 
The recognizer must also score mispronounced words 
accurately so that the student can revisit these words 
and receive feedback about their pronunciation after 
completing a paragraph or page (since highlighting hy-
pothesized mispronounced words when reading out loud 
may disrupt fluent reading behavior).   
In this paper we focus on the problem of speech rec-
ognition to track and provide feedback during reading 
out loud and to transcribe spoken summaries of text. 
Specifically, we describe several new methods for in-
corporating language modeling knowledge into the read 
aloud task.  In addition, through use of speaker adapta-
tion, we also demonstrate the potential for significant 
gains in recognition accuracy.  Finally, we leverage 
improvements in speech recognition for read aloud 
tracking to improve performance for spoken story sum-
marization.  Work reported here extends previous work 
in several important ways: by integrating the research 
advances into a real time system, and by including time-
adaptive language modeling and time-adaptive acoustic 
modeling of the child?s voice into the system. 
The paper is organized as follows. Sect. 2 describes 
our baseline speech recognition system and reading 
tracking method. Sect. 3 presents our rationale for using 
word-error-rate as a measure of performance.  Sect. 4 
describes the read aloud and story summarization cor-
pora used in this work. Sect. 5 describes and evaluates 
proposed improvements in a read aloud speech recogni-
tion task. Sect. 6 describes how these improvements 
translate to improved recognition of story summaries 
produced by a child. Sect. 7 details our real-time system 
implementation. 
2 Baseline System 
For this work we use the SONIC speech recognition 
system (Pellom, 2001; Pellom and Hacioglu, 2003).  
The recognizer implements an efficient time-
synchronous, beam-pruned Viterbi token-passing search 
through a static re-entrant lexical prefix tree while 
utilizing continuous density mixture Gaussian HMMs.  
For children?s speech, the recognizer has been trained 
on 46 hours of data from children in grades K through 9 
extracted from the CU Read and Prompted speech 
corpus (Hagen et al, 2003) and the OGI Kids? speech 
corpus (Shobaki et al, 2000).  Further, the baseline 
system utilizes PMVDR cepstral coefficients (Yapanel 
and Hansen, 2003) for improved noise robustness. 
During read-aloud operation, the speech recognizer 
models the story text using statistical n-gram language 
models.  This approach gives the recognizer flexibility 
to insert/delete/substitute words based on acoustics and 
to provide accurate confidence information from the 
word-lattice.  The recognizer receives packets of audio 
and automatically detects voice activity.  When the 
child speaks, the partial hypotheses are sent to a reading 
tracking module.  The reading tracking module deter-
mines the current reading location by aligning each par-
tial hypothesis with the book text using a Dynamic 
Programming search.  In order to allow for skipping of 
words or even skipping to a different place within the 
text, the search finds words that when strung together 
minimize a weighted cost function of adjacent word-
proximity and distance from the reader's last active 
reading location. The Dynamic Programming search 
additionally incorporates constraints to account for 
boundary effects at the ends of each partial phrase. 
3 Evaluation Methodology 
There are many different ways in which speech recogni-
tion can be used to serve children. In computer-based 
literacy tutors, speech recognition can be used to meas-
ure children's ability to read fluently and pronounce 
words while reading out loud, to engage in spoken dia-
logues with an animated agent to assess and train com-
prehension, or to transcribe spoken summaries of stories 
that can be graded automatically.  Because of the variety 
of ways of using speech recognition systems, it is criti-
cally important to establish common metrics that are 
used by the research community so that progress can be 
measured both within and across systems. 
For this reason, we argue that word error rate calcu-
lations using the widely accepted NIST scoring software 
provides the most widely accepted, easy to use and 
highly valid metric.  In this scoring procedure, word 
error rate is computed strictly by comparing the speech 
recognizer output against a known human transcription 
(or the text in a book).  Of course, authors are free to 
define and report other measures, such as detection/false 
alarm curves for useful events such as reading miscues.  
However, such analyses should always supplement re-
ports of word error rates using a single standardized 
measure. Adopting this strategy enables fair and bal-
anced comparisons within and across systems for any 
speech data given a known word-level transcription. 
4 Experimental Data 
For all experiments in this paper we use speech data and 
associated transcriptions from 106 children (grade 3: 17 
speakers, grade 4: 28 speakers, and grade 5: 61 speak-
ers) who were asked to read one of ten stories and to 
provide a spoken story summary.  The 16 kHz audio 
data contains an average of 1054 words (min 532 
words; max 1926 words) with an average of 413 unique 
words per story.  The resulting summaries spoken by 
children contain an average of 168 words. 
5 Improved Read-Aloud Recognition 
Baseline: Our baseline read-aloud system utilizes a 
trigram language model constructed from a normalized 
version of the story text. Text normalization consists 
primarily of punctuation removal and determination of 
sentence-like units.  For example,  
 
It was the first day of summer vacation.  Sue and Billy were 
eating breakfast.  ?What can we do today?? Billy asked. 
 
is normalized as: 
 
<s> IT WAS THE FIRST DAY OF SUMMERVACATION</s> 
<s> SUE AND BILLY WERE EATING BREAKFAST</s> 
<s> WHAT CAN WE DO TODAY </s> 
<s> BILLY ASKED </s> 
 
The resulting text is used to estimate a back-off trigram 
language model. We stress that only the story text is 
used to construct the language model. Details on the 
story texts are provided in Hagen et al (2003). Note that 
the sentence markers (<s> and </s>) are used to repre-
sent positions of expected speaker pause.  This baseline 
system is shown in Table 1(A) to produce a 17.4% word 
error rate. 
Improved Sentence Context Modeling: It is impor-
tant in the context of this research to note that children 
do not pause between each estimated sentence bound-
ary.  Instead, many children read fluently across phrases 
and sentences, where more experienced readers would 
pause. For this reason, we improved upon our baseline 
system by estimating language model parameters using 
a combined text material that is generated both with and 
without the contextual sentence markers (<s> and </s>).  
Results of this modification are shown in Table 1(B) 
and show a reduction in error from 17.4% to 13.5%. 
Improved Word History Modeling:  Most speech 
recognition systems operate on the utterance as a pri-
mary unit of recognition.  Word history information 
typically is not maintained across segmented utterances.  
However, in our text example, the words ?do today? 
should provide useful information to the recognizer that 
?Billy asked? may follow.  We therefore modify the 
recognizer to incorporate knowledge of previous utter-
ance word history. During token-passing search, the 
initial word-history tokens are modified to account for 
the fact that the incoming sentence may be either the 
beginning of a new sentence or a direct extension of the 
previous utterance?s word-end history.  Incorporating 
this constraint lowers the word error rate from 13.5% to 
12.7% as shown in Table 1(C). 
Dynamic n-gram Language Modeling:  During story 
reading we can anticipate words that are likely to be 
spoken next based upon the words in the text that are 
currently being read aloud.  To account for this knowl-
edge, we estimate a series of position-sensitive n-gram 
language models by partitioning the story into overlap-
ping regions containing at most 150 words (i.e., each 
region is centered on 50 words of text with 50 words 
before and 50 words after).  For each partition, we con-
struct an n-gram language model by using the entire 
normalized story text in addition to a 10x weighting of 
text within the partition.  Each position-sensitive lan-
guage model therefore contains the entire story vocabu-
lary.  We also compute a general language model 
estimated solely from the entire story text (similar to 
Table 1(C)).   At run-time, the recognizer implements a 
word-history buffer containing the most recent 15 rec-
ognized words.  After decoding each utterance, the 
probability of the text within the word history buffer is 
computed using each of the position-sensitive language 
models.  The language model with the highest probabil-
ity is selected for the first-pass decoding of the subse-
quent utterance.  This modification decreases the word 
error rate from 12.7% to 10.7% (Table 1(D)). 
Vocal Tract Normalization and Acoustic Adaptation:  
We further extend on our baseline system by incorporat-
ing the Vocal Tract Length Normalization (VTLN) 
method described in Welling et al (1999).  Based on 
results shown in Table 1(E), we see that VTLN provides 
only a marginal gain (0.1% absolute).  Our final set of 
acoustic models for the read aloud task are both VTLN 
normalized and estimated using Speaker Adaptive 
Training (SAT).  The SAT models are determined by 
estimating a single linear feature space transform for 
each training speaker (Gales, 1997).  The means and 
variances of the VTLN/SAT models are then iteratively 
adapted using the SMAPLR algorithm (Siohan, 2002) to 
yield a final recognition error rate of 8.0% absolute (Ta-
ble 1(G)).  By combining all of these techniques, we 
achieved a 54% reduction in word error rate relative to 
the baseline system.    
 
Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline: single n-gram 
language model 17.7% 17.4% 
(B) (A) + Begin/End Sentence 
Context Modeling 14.0% 13.5% 
(C) (B) + between utterance 
word history modeling 13.0% 12.7% 
(D) (C) + dynamic  
n-gram language model 11.0% 10.7% 
(E) (D) + VTLN 10.9% 10.6% 
(F) (E) + VTLN/SAT + 
SMAPLR (iteration 1) 8.2% 8.2% 
(G) (E) + VTLN/SAT + 
SMAPLR (iteration 2) 8.0% 8.0% 
Table 1: Recognition of children?s read out-loud data. 
6 Improved Story Summary Recognition 
One of the unique and powerful features of our interac-
tive books is the notion of assessing and training com-
prehension by providing feedback to the student about a 
typed summary of text that the student has just read 
(Cole et al, 2003). Verbal input is especially important 
for younger children who often can not type well. Util-
izing summaries from the children?s speech corpus, 
Hagen et al (2003) showed that an error rate of 42.6% 
could be achieved.  The previous work, however, did 
not consider utilizing the read story material to provide 
improved initial acoustic models for the summarization 
task.  In Table 2 we demonstrate several findings using 
a language model trained on story text and example 
summaries produced by children (leaving out data from 
the child under test).  Without any adaptation the error 
rate is 47.1%.  However, utilizing adapted models from 
the read stories (see Table 1(G)) provides an initial per-
formance gain of nearly 10% absolute.  Further use 
SMAPLR adaptation reduces the error rate to 36.1%. 
 Word Error Rate (%) Experimental Configuration MFCC PMVDR 
(A) Baseline / no adaptation 47.0% 47.1% 
(B) Read-aloud adapted models 
(VTLN/SAT) 37.2% 38.0% 
(C) (B) + SMAPLR  
adaptation iteration #1 36.0% 36.6% 
(D) (C) + SMAPLR 
adaptation iteration #2 35.1% 36.1% 
Table 2:  Recognition of spontaneous story summaries 
7 Practical Real-Time Implementation 
The research systems described in Sect. 5 and 6 do not 
operate in real-time since multiple adaptation passes 
over the data are required.  To address this issue, we 
have implemented a real-time system that operates on 
small pipelined audio segments (250ms on average).  
When evaluated on the read-aloud task (Sect. 5), the 
initial baseline system achieves an error rate of 19.5%.  
This system has a real-time factor of 0.56 on a 2.4 GHz 
Intel Pentium 4 PC with 512MB of RAM. When inte-
grated, the proposed methods show the error rate can be 
reduced from 19.5% to 12.7% (compare with 10.7% 
error research system in Table 1(D)).  The revised sys-
tem which incorporates dynamic language modeling 
operates 35% faster than the single language model 
method while also reducing the variance in real-time 
factor for each processed chunk of audio.  Further gains 
are possible by incorporating adaptation in an incre-
mental manner.  For example, in Table 3(C) a real-time 
system that incorporates incremental unsupervised 
maximum likelihood linear regression (MLLR) adapta-
tion of the Gaussian means is shown.  This final real-
time system simultaneously adapts both language and 
acoustic model parameters during system use. The sys-
tem is now being refined for deployment in classrooms 
within the CLT project. We were able to further im-
prove the system after the submission deadline. The 
current WER on the story read aloud task improved to 
7.6%; while a WER of 32.2% was achieved on the 
summary recognition task. The improvements are due to 
the inclusion of a breath model and the additional use of 
audio data from 103 second graders for more accurate 
acoustic modeling. 
 
PMVDR Front-End System Description WER (%) RTF 
(A) Baseline: single LM 19.5% 0.56  (  2=0.11) 
(B) Proposed System 12.7% 0.36 (  2=0.06) 
(C) (B) + Incremental 
MLLR adaptation 11.5% 
0.80 
(  2=0.33) 
Table 3:  Evaluation of real-time read out-loud system. 
References 
 
V. Zue, S. Seneff, J. Polifroni, H. Meng, J. Glass 
(1996). ?Multilingual Human-Computer Interactions: 
From Information Acess to Language Learning,? 
ICSLP-96, Philadelphia, PA 
J. Mostow, S. Roth, A. G. Hauptmann, and M. Kane 
(1994). "A Prototype Reading Coach that Listens", 
AAAI-94, Seattle, WA, pp. 785-792.  
Y-C. Tam, J. Mostow, J. Beck, and S. Banerjee (2003). 
?Training a Confidence Measure for a Reading Tutor 
that Listens?. Eurospeech, Geneva, Switzerland, 
3161-3164. 
J. Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin 
(2002). ?Predicting oral reading miscues? ICSLP-02, 
Denver, Colorado. 
J. Mostow, G. Aist, P. Burkhead, A. Corbett, A. Cuneo, 
S. Eitelman, C. Huang, B. Junker, M. B. Sklar, and 
B. Tobin (2003). ?Evaluation of an automated Read-
ing Tutor that listens:  Comparison to human tutoring 
and classroom instruction?. Journal of Educational 
Computing Research, 29(1), 61-117 
R. Cole, S. van Vuuren, B. Pellom, K. Hacioglu, J. Ma, 
J. Movellan, S. Schwartz, D. Wade-Stein, W. Ward, 
J. Yan (2003). ?Perceptive Animated Interfaces: First 
Steps Toward a New Paradigm for Human Computer 
Interaction,? Proceedings of the IEEE, Vol. 91, No. 
9, pp. 1391-1405 
A. Hagen, B. Pellom, and R. Cole (2003). "Children?s 
Speech Recognition with Application to Interactive 
Books and Tutors", ASRU-2003, St. Thomas, USA 
B. Pellom (2001). "SONIC: The University of Colorado 
Continuous Speech Recognizer", Technical Report 
TR-CSLR-2001-01, University of Colorado. 
B. Pellom and K. Hacioglu (2003). "Recent Improve-
ments in the CU Sonic ASR System for Noisy 
Speech: The SPINE Task", ICASSP-2003, Hong 
Kong, China. 
U. Yapanel, J. H.L. Hansen (2003). "A New Perspective    
on Feature Extraction for Robust In-vehicle Speech  
Recognition" Eurospeech, Geneva, Switzerland. 
K. Shobaki, J.-P. Hosom, and R. Cole (2000). "The OGI 
Kids' Speech Corpus and Recognizers", Proc. 
ICSLP-2000, Beijing, China. 
L. Welling, S. Kanthak, and H. Ney. (1999) "Improved 
Methods for Vocal Tract Length Normalization", 
ICASSP, Phoenix, Arizona. 
M. Gales (1997). Maximum Likelihood Linear Trans-
formations for HMM-Based Speech Recognition", 
Tech. Report, CUED/F-INFENG/TR291, Cambridge 
University. 
O. Siohan, T. Myrvoll, and C.-H. Lee (2002) "Structural 
Maximum a Posteriori Linear Regression for Fast 
HMM Adaptation", Computer, Speech and Lan-
guage, 16, pp. 5-24.  
Proceedings of NAACL HLT 2009: Short Papers, pages 77?80,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Generating Synthetic Children's Acoustic Models from Adult Models 
 
Andreas Hagen, Bryan Pellom, and Kadri Hacioglu 
Rosetta Stone Labs  
{ahagen, bpellom, khacioglu}@rosettastone.com 
 
 
Abstract 
This work focuses on generating children?s 
HMM-based acoustic models for speech rec-
ognition from adult acoustic models. Collect-
ing children?s speech data is more costly 
compared to adult?s speech. The patent-
pending method developed in this work re-
quires only adult data to estimate synthetic 
children?s acoustic models in any language 
and works as follows: For a new language 
where only adult data is available, an adult 
male and an adult female model is trained. A 
linear transformation from each male HMM 
mean vector to its closest female mean vector 
is estimated. This transform is then scaled to a 
certain power and applied to the female model 
to obtain a synthetic children?s model. In a 
pronunciation verification task the method 
yields 19% and 3.7% relative improvement on 
native English and Spanish children?s data, re-
spectively, compared to the best adult model. 
For Spanish data, the new model outperforms 
the available real children?s data based model 
by 13% relative. 
1 Introduction 
Language learning is becoming more and more 
important in the age of globalization. Depending 
on their work or cultural situation some people are 
confronted with various different languages on a 
daily basis. While it is very desirable to learn lan-
guages at any age, language learning, among other 
learning experiences, is comparably simpler for 
children than for adults and should therefore be 
encouraged at early ages. 
Even though the children?s language learning mar-
ket is highly important, comprising effective 
speech recognition tools for pronunciation assess-
ment is relatively hard due to the special characte-
ristics of children?s speech and the limited 
availability of children?s speech data in many lan-
guages in the speech research community. Adult 
speech data is usually easier to obtain. By under-
standing the characteristics of children?s speech the 
unconditional need for children?s speech data can 
be lessened by altering adult acoustic models such 
that they are suitable for children?s speech. 
Children?s speech has higher pitch and formants 
than female speech. Further, female speech has 
higher pitch and formants than male speech. Child-
ren?s speech is more variable than female speech, 
and, as research has shown, female speech is more 
variable than male speech (Lee et al, 1999). Given 
this transitive chain of argumentation, the trans-
formation from a male to a female acoustic model 
can be estimated for a language and applied (at a 
certain adjustable degree) to the female model. 
This process results in a synthetic children?s 
speech model designed on the basis of the female 
model. Therefore, for a new language an effective 
synthetic children?s acoustic model can be derived 
without the need of children?s data (Hagen et al, 
2008). 
2 Related Work  
Extensive research has been done in the field of 
children?s speech analysis and recognition in the 
past few years. A detailed overview of children?s 
speech characteristics can be found in (Lee et al, 
1999). The paper presents research results showing 
the higher variability in speech characteristics 
among children compared to adult speech. The 
properties of children?s speech that were re-
searched were duration of vowels and sentences, 
pitch, and formant locations. 
When designing acoustic models specially suited 
for children, properties as the formant locations 
and higher variability of children?s speech need to 
be accounted for. The best solution for building 
children?s speech models is to collect children?s 
speech data and to train models from scratch (Ha-
77
gen et al, 2003, Cosi et al 2005). Researchers 
have also tried to apply adult acoustic models us-
ing speaker normalization techniques to recognize 
children?s speech (Elenius et al, 2005, Potamianos 
et al 1997). Adult acoustic models were adapted 
towards children?s speech. A limited amount of 
children?s speech data was available for adapta-
tion. In (Gustafson et al, 2002) children?s voices 
were transformed before being sent to the recog-
nizer using adult acoustic models. In (Claes et al, 
1997) children?s acoustic models were built based 
on a VTL adaptation of cepstral parameters based 
on the third formant frequency. The method 
showed to be effective for building children?s 
speech models. 
3 Building Synthetic Children?s Models 
from Adult Models 
As mentioned in Section 1, research has shown 
that pitch and formants of children?s speech are 
higher than for female speech. Female speech has 
higher pitch and formants than male speech. In 
order to exploit these research results a transforma-
tion from a male acoustic model to a female acous-
tic model can be derived. This transformation will 
map a male model as close as possible to a female 
model. The transformation can be adjusted and 
applied to the female model. The resulting synthet-
ic model can be tested on children?s data. 
Parameters that are subject to transformation in 
this process are the mean vectors of the HMM 
states. The transformation can be represented as a 
square matrix in the dimension of the mean vec-
tors. The transformation chosen in this approach is 
therefore linear and is for example capable of 
representing a vocal tract length adaptation as it 
was shown in (Pitz et al, 2005). Linear transfor-
mations (i.e. matrices) are also chosen in adapta-
tion approaches as MAPLR and MLLR, whose 
benefit has been shown to be additive to the benefit 
of VTLN in speaker adaptation applications. A 
linear transform in the form of a matrix is therefore 
well suited due to its expressive power as well as 
its mathematical manageability. 
3.1 Transformation Matrix 
The transformation matrix used in this approach is 
estimated by mapping the male to the female 
acoustic model, such that each HMM state mean 
vector in the male model is assigned a correspond-
ing mean vector in the female model. Information 
used in the mapping process is the basic phoneme 
and context. The resulting mean vector pairs are 
used as source and target features in the training 
process of the transformation matrix. During train-
ing the matrix is initialized as the identity matrix 
and the estimate of the mapping is refined by gra-
dient descent. In a typical acoustic model there are 
several hundred, sometimes thousands, of these 
mean vector pairs to train the transformation ma-
trix. The expression that needs to be minimized is: 
2
),(
)(minarg yAxT
pairsyxA
?= ?   
where T is the error-minimizing transformation 
matrix; x is a male model?s source vector and y it 
corresponding female model?s target vector.  
In this optimization process the Matrix A is initia-
lized as the identity matrix. Each matrix entry ija is 
updated (to the new value 'ija ) in the following way 
by gradient descent: 
( ) jiiijij xyxAkaa ?+='  
where iA  is the i-th line of matrix A and k deter-
mines the descent step size (k<0 and incorporates 
the factor of 2 resulting from the differentiation). 
The gradient descent needs to be run multiple 
times over all vector pairs (x,y) for the matrix to 
converge to an acceptable approximation which is 
called the transformation matrix T. 
3.2 Synthetic Children?s Model Creation 
The transformation matrix can be applied to the 
female model in order to create a new synthetic 
acoustic model which should suit children?s speech 
better than adult acoustic models. It is unlikely that 
the transformation applied ?as is? will result in the 
best model possible, therefore the transformation 
can be altered (amplified or weakened) in order to 
yield the best results. An intuitive way to alter the 
impact of the transformation is taking the matrix T 
to a certain power p. Synthetic models can be 
created by applying pT  to the female model1, for 
various values p. If children?s data is available for 
evaluation purposes, the best value of p can be de-
termined. The power p is claimed to be language 
independent. It might vary in nuances, but experi-
                                                          
1
 Taking a matrix to the power of p is meant in the sense 
TT
pp
=
/1
, IdentityT =0 , TT =1  
78
ments have shown that a value around 0.25 is a 
reasonable choice. 
3.3 Transformation Algorithm 
The previous section presented the theoretical 
means necessary for the synthetic children?s model 
creation process. The precise, patent-pending algo-
rithm to create a synthetic children?s model in a 
new language is as follows (Hagen et al, 2008): 
 
1. Train a male and a female acoustic model 
2. Estimate the transform T from the male 
to the female model 
3. Determine the power p by which the 
transform T should be adjusted 
4. Apply pT  to the female acoustic model 
to create the synthetic children?s model 
 
Step 3, the determination of the power p, can be 
done in two different ways. If children?s test data 
in the relevant language is available, various mod-
els based on different p-values can be evaluated 
and the best one chosen. If there is no children?s 
data available in a new language, p can be esti-
mated by evaluations in a language where there is 
enough male, female, and children?s speech data 
available. The claim here is that the power p is rel-
atively language independent and estimating p in a 
different language is superior to a simple guess. 
4 Experiments 
The algorithm was tested on two languages: US 
English and Spanish. For both languages sufficient 
male, female, and children?s speech data was 
available (more than 20 hours) in order to train 
valid acoustic models and to have reference child-
ren?s acoustic models available. For English test 
data we used a corpus of 22 native speakers in the 
age range of 5 to 14. The number of utterances is 
2,182. For Spanish test data the corpus is com-
prised of 19 speakers in the age range of 8 to 13 
years. The number of utterances is 2,598. 
The transform from the male to the female model 
was estimated in English. The power of p was 
gradually increased and the transformation matrix 
was adjusted. With this adjusted matrix pT  a syn-
thetic children?s model was built. This synthetic 
children?s model was evaluated on children?s test 
data and the results were compared to the reference 
children?s model?s and the female model?s perfor-
mance. 
When speech is evaluated in a language learning 
system, the first step is utterance verification, 
meaning the task of evaluating if the user actually 
tried to produce the desired utterance. The Equal 
Error Rate (EER) on the utterance level is a means 
of evaluating this performance. For each utterance 
an in- and out-of-grammar likelihood score is de-
termined. The EER operating points, determined 
by the cutting point of the two distributions (in-
grammar and out-of-grammar), are reported as an 
error metric. Figure 1 shows the EER values of the 
synthetic model applied to children?s data. 
 
 
 
Figure 1: Synthetic model?s EER performance de-
pending on the power p used for model creation. 
 
It can be seen that the best performance is reached 
at about p=0.25. The overview of the results is 
given in Table 1. 
 
 Equal Error Rate  
Real Children?s Model 1.90% 
Male Model 4.07% 
Female Model 2.92% 
Synthetic Model 2.36% 
 
Table 1: EER numbers when using a real children?s 
model compared to a male, female, and synthetic 
model for children?s data evaluation. 
 
The results show that the synthetic children?s mod-
el yields good classification results when applied 
to children?s data. The gold standard, the real 
children?s model application, results in the best 
EER performance. 
If the same evaluation scenario is applied to Span-
ish, a very similar picture evolves. Figure 2 shows 
the EER results versus transformation power p for 
Spanish children?s data. 
 
79
  
Figure 2: Spanish synthetic model?s EER perfor-
mance depending on the power p used for model 
creation. 
 
In Figure 2 it can be seen that the optimal setting 
for p is about 0.27. This value is very similar to the 
one found for US English, which supports, but cer-
tainly does not prove, the language independence 
claim. Results for Spanish are given in Table 2. 
 
 Equal Error Rate 
Real Children?s model 2.40% 
Male model 5.62% 
Female model 2.17% 
Synthetic model 2.09% 
 
Table 2: EER numbers for Spanish when using a 
real children?s model compared to a male, female, 
and synthetic model for Spanish children?s data 
evaluation. 
 
Similar to English, the Spanish synthetic model 
performs better than the female model on child-
ren?s speech. Interestingly, the acoustic model 
purely trained on children?s data performs worse 
than the female and the synthetic model. It is not 
clear why the children?s model does not outper-
form the female and the synthetic model; an expla-
nation could be diverse and variable training data 
that hurts classification performance. 
It can be seen that for US English and Spanish the 
power p used to adjust the transformation is about 
0.25. Therefore, for a new language where only 
adult data is available, the transformation from the 
male to the female model can be estimated and 
applied to the female model (after being adjusted 
by p=0.25). The resulting synthetic model will 
work reasonably well and could be refined as soon 
as children?s data becomes available. 
 
5 Conclusion 
This work presented a new technique to create 
children?s acoustic models from adult acoustic 
models without the need for children?s training 
data when applied to a new language. While it can 
be assumed that the availability of children?s data 
would improve the resulting acoustic models, the 
approach is effective if children?s data is not avail-
able. It will be interesting to see how performance 
of this technique compares to adapting adult mod-
els by adaptation techniques, i.e. MLLR, when li-
mited amounts of children?s data are available. 
Two scenarios are possible: With increasing 
amount of children?s data speaker adaptation will 
draw even and/or be superior. The other possibility 
is that the presented technique yields better results 
regardless how much real children?s data is availa-
ble, due to the higher variability and noise-
pollution of children?s data. 
References  
Claes, T., Dologlou, I, ten Bosch, L., Van Compernolle, 
D. 1997. New Transformations of Cepstral Parame-
ters for Automatic Vocal Tract Length Normalization 
in Speech Recognition, 5th Europ. Conf. on Speech 
Comm. and Technology, Vol. 3: 1363-1366. 
Cosi, P., Pellom, B. 2005. Italian children's speech rec-
ognition for advanced interactive literacy tutors. 
Proceedings Interspeech, Lisbon, Portugal. 
Elenius, D. and Blomberg, M. 2005. Adaptation and 
Normalization Experiments in Speech Recognition 
for 4 to 8 Year old Children. Proceedings Inters-
peech, Lisbon, Portugal. 
Gustafson, J., Sj?lander, K. 2002. Voice transformations 
for improving children?s speech recognition in a pub-
licly available dialogue system. ICSLP, Denver. 
Hagen, A., Pellom, B., and Cole, R. 2003. Children's 
Speech Recognition with Application to Interactive 
Books and Tutors. Proceedings ASRU, USA. 
Lee, S., Potamianos, A., and Narayanan, S. 1999. 
Acoustics of children's speech: Developmental 
changes of temporal and spectral parameter. J. 
Acoust. Soc. Am., Vol. 105(3):1455-1468. 
Pitz, M., Ney, H. 2005. Vocal Tract Normalization 
Equals Linear Transformation in Cepstral Space. 
IEEE Trans. Speech & Audio Proc., 13(5): 930-944. 
Potamianos, A., Narayanan, S., and Lee, S. 1997. Auto-
matic Speech Recognition for Children. Proceedings 
Eurospeech, Rhodes, Greece. 
Hagen, A., Pellom, B., and Hacioglu, K. 2008. Method 
for Creating a Speech Model. US Patent Pending.  
 
80
