The order of prenominal adjectives
in natural language generation
Robert Malouf
Alfa Informatica
Rijksuniversiteit Groningen
Postbus 716
9700 AS Groningen
The Netherlands
malouf@let.rug.nl
Abstract
The order of prenominal adjectival
modifiers in English is governed by
complex and difficult to describe con-
straints which straddle the boundary
between competence and performance.
This paper describes and compares
a number of statistical and machine
learning techniques for ordering se-
quences of adjectives in the context of
a natural language generation system.
1 The problem
The question of robustness is a perennial prob-
lem for parsing systems. In order to be useful,
a parser must be able to accept a wide range of
input types, and must be able to gracefully deal
with dysfluencies, false starts, and other ungram-
matical input. In natural language generation, on
the other hand, robustness is not an issue in the
same way. While a tactical generator must be able
to deal with a wide range of semantic inputs, it
only needs to produce grammatical strings, and
the grammar writer can select in advance which
construction types will be considered grammati-
cal. However, it is important that a generator not
produce strings which are strictly speaking gram-
matical but for some reason unusual. This is a
particular problem for dialog systems which use
the same grammar for both parsing and genera-
tion. The looseness required for robust parsing
is in direct opposition to the tightness needed for
high quality generation.
One area where this tension shows itself clearly
is in the order of prenominal modifiers in English.
In principle, prenominal adjectives can, depend-
ing on context, occur in almost any order:
the large red American car
??the American red large car
*car American red the large
Some orders are more marked than others, but
none are strictly speaking ungrammatical. So, the
grammar should not put any strong constraints on
adjective order. For a generation system, how-
ever, it is important that sequences of adjectives
be produced in the ?correct? order. Any other or-
der will at best sound odd and at worst convey an
unintended meaning.
Unfortunately, while there are rules of thumb
for ordering adjectives, none lend themselves to a
computational implementation. For example, ad-
jectives denoting size do tend to precede adjec-
tives denoting color. However, these rules under-
specify the relative order for many pairs of adjec-
tives and are often difficult to apply in practice.
In this paper, we will discuss a number of statisti-
cal and machine learning approaches to automati-
cally extracting from large corpora the constraints
on the order of prenominal adjectives in English.
2 Word bigram model
The problem of generating ordered sequences of
adjectives is an instance of the more general prob-
lem of selecting among a number of possible
outputs from a natural language generation sys-
tem. One approach to this more general problem,
taken by the ?Nitrogen? generator (Langkilde and
Knight, 1998a; Langkilde and Knight, 1998b),
takes advantage of standard statistical techniques
by generating a lattice of all possible strings given
a semantic representation as input and selecting
the most likely output using a bigram language
model.
Langkilde and Knight report that this strategy
yields good results for problems like generating
verb/object collocations and for selecting the cor-
rect morphological form of a word. It also should
be straightforwardly applicable to the more spe-
cific problem we are addressing here. To deter-
mine the correct order for a sequence of prenom-
inal adjectives, we can simply generate all possi-
ble orderings and choose the one with the high-
est probability. This has the advantage of reduc-
ing the problem of adjective ordering to the prob-
lem of estimating n-gram probabilities, some-
thing which is relatively well understood.
To test the effectiveness of this strategy, we
took as a dataset the first one million sentences
of the written portion of the British National Cor-
pus (Burnard, 1995).1 We held out a randomly se-
lected 10% of this dataset and constructed a back-
off bigram model from the remaining 90% using
the CMU-Cambridge statistical language model-
ing toolkit (Clarkson and Rosenfeld, 1997). We
then evaluated the model by extracting all se-
quences of two or more adjectives followed by
a noun from the held-out test data and counted
the number of such sequences for which the most
likely order was the actually observed order. Note
that while the model was constructed using the
entire training set, it was evaluated based on only
sequences of adjectives.
The results of this experiment were some-
what disappointing. Of 5,113 adjective sequences
found in the test data, the order was correctly pre-
dicted for only 3,864 for an overall prediction ac-
curacy of 75.57%. The apparent reason that this
method performs as poorly as it does for this par-
ticular problem is that sequences of adjectives are
relatively rare in written English. This is evi-
denced by the fact that in the test data only one se-
quence of adjectives was found for every twenty
sentences. With adjective sequences so rare, the
chances of finding information about any particu-
lar sequence of adjectives is extremely small. The
data is simply too sparse for this to be a reliable
method.
1The relevant files were identified by the absence of the
<settDesc> (spoken text ?setting description?) SGML tag
in the file header. Thanks to John Carroll for help in prepar-
ing the corpus.
3 The experiments
Since Langkilde and Knight?s general approach
does not seem to be very effective in this particu-
lar case, we instead chose to pursue more focused
solutions to the problem of generating correctly
ordered sequences of prenominal adjectives. In
addition, at least one generation algorithm (Car-
roll et al, 1999) inserts adjectival modifiers in a
post-processing step. This makes it easy to in-
tegrate a distinct adjective-ordering module with
the rest of the generation system.
3.1 The data
To evaluate various methods for ordering
prenominal adjectives, we first constructed a
dataset by taking all sequences of two or more
adjectives followed by a common noun in the 100
million tokens of written English in the British
National Corpus. From 247,032 sequences, we
produced 262,838 individual pairs of adjectives.
Among these pairs, there were 127,016 different
pair types, and 23,941 different adjective types.
For test purposes, we then randomly held out
10% of the pairs, and used the remaining 90% as
the training sample.
Before we look at the different methods for
predicting the order of adjective pairs, there are
two properties of this dataset which bear noting.
First, it is quite sparse. More than 76% of the
adjective pair types occur only once, and 49%
of the adjective types only occur once. Second,
we get no useful information about the syntag-
matic context in which a pair appears. The left-
hand context is almost always a determiner, and
including information about the modified head
noun would only make the data even sparser. This
lack of context makes this problem different from
other problems, such as part-of-speech tagging
and grapheme-to-phoneme conversion, for which
statistical and machine learning solutions have
been proposed.
3.2 Direct evidence
The simplest strategy for ordering adjectives is
what Shaw and Hatzivassiloglou (1999) call the
direct evidence method. To order the pair {a,b},
count how many times the ordered sequences
?a,b? and ?b,a? appear in the training data and
output the pair in the order which occurred more
often.
This method has the advantage of being con-
ceptually very simple, easy to implement, and
highly accurate for pairs of adjectives which ac-
tually appear in the training data. Applying this
method to the adjectives sequences taken from
the BNC yields better than 98% accuracy for
pairs that occurred in the training data. However,
since as we have seen, the majority of pairs occur
only once, the overall accuracy of this method is
59.72%, only slightly better than random guess-
ing. Fortunately, another strength of this method
is that it is easy to identify those pairs for which
it is likely to give the right result. This means
that one can fall back on another less accurate but
more general method for pairs which did not oc-
cur in the training data. In particular, if we ran-
domly assign an order to unseen pairs, we can cut
the error rate in half and raise the overall accuracy
to 78.28%.
It should be noted that the direct evidence
method as employed here is slightly different
from Shaw and Hatzivassiloglou?s: we simply
compare raw token counts and take the larger
value, while they applied a significance test to es-
timate the probability that a difference between
counts arose strictly by chance. Like one finds in
a trade-off between precision and recall, the use
of a significance test slightly improved the accu-
racy of the method for those pairs which it had
an opinion about, but also increased the number
of pairs which had to be randomly assigned an
order. As a result, the net impact of using a sig-
nificance test for the BNC data was a very slight
decrease in the overall prediction accuracy.
The direct evidence method is straightforward
to implement and gives impressive results for ap-
plications that involve a small number of frequent
adjectives which occur in all relevant combina-
tions in the training data. However, as a general
approach to ordering adjectives, it leaves quite
a bit to be desired. In order to overcome the
sparseness inherent to this kind of data, we need
a method which can generalize from the pairs
which occur in the training data to unseen pairs.
3.3 Transitivity
One way to think of the direct evidence method is
to see that it defines a relation ? on the set of En-
glish adjectives. Given two adjectives, if the or-
dered pair ?a,b? appears in the training data more
often then the pair ?b,a?, then a ? b. If the re-
verse is true, and ?b,a? is found more often than
?a,b?, then b ? a. If neither order appears in the
training data, then neither a? b nor b? a and an
order must be randomly assigned.
Shaw and Hatzivassiloglou (1999) propose to
generalize the direct evidence method so that it
can apply to unseen pairs of adjectives by com-
puting the transitive closure of the ordering re-
lation ?. That is, if a ? c and c ? b, we can
conclude that a ? b. To take an example from
the BNC, the adjectives large and green never oc-
cur together in the training data, and so would
be assigned a random order by the direct evi-
dence method. However, the pairs ?large,new?
and ?new,green? occur fairly frequently. There-
fore, in the face of this evidence we can assign
this pair the order ?large,green?, which not coin-
cidently is the correct English word order.
The difficulty with applying the transitive clo-
sure method to any large dataset is that there of-
ten will be evidence for both orders of any given
pair. For instance, alongside the evidence sup-
porting the order ?large,green?, we also find the
pairs ?green,byzantine?, ?byzantine,decorative?,
and ?decorative,new?, which suggest the order
?green, large?.
Intuitively, the evidence for the first order is
quite a bit stronger than the evidence for the sec-
ond. The first ordered pairs are more frequent, as
are the individual adjectives involved. To quan-
tify the relative strengths of these transitive in-
ferences, Shaw and Hatzivassiloglou (1999) pro-
pose to assign a weight to each link. Say the order
?a,b? occurs m times and the pair {a,b} occurs n
times in total. Then the weight of the pair a? b
is:
? log
(
1?
n
?
k=m
(
n
k
)
?
1
2
n
)
This weight decreases as the probability that the
observed order did not occur strictly by chance
increases. This way, the problem of finding the
order best supported by the evidence can be stated
as a general shortest path problem: to find the pre-
ferred order for {a,b}, find the sum of the weights
of the pairs in the lowest-weighted path from a to
b and from b to a and choose whichever is lower.
Using this method, Shaw and Hatzivassiloglou
report predictions ranging from 81% to 95% ac-
curacy on small, domain specific samples. How-
ever, they note that the results are very domain-
specific. Applying a graph trained on one domain
to a text from another another generally gives
very poor results, ranging from 54% to 58% accu-
racy. Applying this method to the BNC data gives
83.91% accuracy, in line with Shaw and Hatzivas-
siloglou?s results and considerably better than the
direct evidence method. However, applying the
method is computationally a bit expensive. Like
the direct evidence method, it requires storing ev-
ery pair of adjectives found in the training data
along with its frequency. In addition, it also re-
quires solving the all-pairs shortest path problem,
for which common algorithms run in O(n3) time.
3.4 Adjective bigrams
Another way to look at the direct evidence
method is as a comparison between two proba-
bilities. Given an adjective pair {a,b}, we com-
pare the number of times we observed the order
?a,b? to the number of times we observed the or-
der ?b,a?. Dividing each of these counts by the
total number of times {a,b} occurred gives us the
maximum likelihood estimate of the probabilities
P(?a,b?|{a,b}) and P(?b,a?|{a,b}).
Looking at it this way, it should be clear why
the direct evidence method does not work well, as
maximum likelihood estimation of bigram proba-
bilities is well known to fail in the face of sparse
data. It should also be clear how we might im-
prove the direct evidence method. Using the same
strategy as described in section 2, we constructed
a back-off bigram model of adjective pairs, again
using the CMU-Cambridge toolkit. Since this
model was constructed using only data specifi-
cally about adjective sequences, the relative in-
frequency of such sequences does not degrade its
performance. Therefore, while the word bigram
model gave an accuracy of only 75.57%, the ad-
jective bigram model yields an overall prediction
accuracy of 88.02% for the BNC data.
3.5 Memory-based learning
An important property of the direct evidence
method for ordering adjectives is that it requires
storing all of the adjective pairs observed in the
training data. In this respect, the direct evidence
method can be thought of as a kind of memory-
based learning.
Memory-based (also known as lazy, near-
est neighbor, instance-based, or case-based) ap-
proaches to classification work by storing all of
the instances in the training data, along with their
classes. To classify a new instance, the store of
previously seen instances is searched to find those
instances which most resemble the new instance
with respect to some similarity metric. The new
instance is then assigned a class based on the ma-
jority class of its nearest neighbors in the space of
previously seen instances.
To make the comparison between the direct
evidence method and memory-based learning
clearer, we can frame the problem of adjective or-
dering as a classification problem. Given an un-
ordered pair {a,b}, we can assign it some canon-
ical order to get an instance ab. Then, if a pre-
cedes b more often than b precedes a in the train-
ing data, we assign the instance ab to the class
a? b. Otherwise, we assign it to the class b? a.
Seen as a solution to a classification problem,
the direct evidence method then is an application
of memory-based learning where the chosen sim-
ilarity metric is strict identity. As with the inter-
pretation of the direct evidence method explored
in the previous section, this view both reveals a
reason why the method is not very effective and
also indicates a direction which can be taken to
improve it. By requiring the new instance to be
identical to a previously seen instance in order to
classify it, the direct evidence method is unable to
generalize from seen pairs to unseen pairs. There-
fore, to improve the method, we need a more ap-
propriate similarity metric that allows the classi-
fier to get information from previously seen pairs
which are relevant to but not identical to new un-
seen pairs.
Following the conventional linguistic wisdom
(Quirk et al, 1985, e.g.), this similarity metric
should pick out adjectives which belong to the
same semantic class. Unfortunately, for many
adjectives this information is difficult or impos-
sible to come by. Machine readable dictionar-
ies and lexical databases such as WordNet (Fell-
baum, 1998) do provide some information about
semantic classes. However, the semantic classifi-
cation in a lexical database may not make exactly
the distinctions required for predicting adjective
order. More seriously, available lexical databases
are by necessity limited to a relatively small num-
ber of words, of which a relatively small fraction
are adjectives. In practice, the available sources
of semantic information only provide semantic
classifications for fairly common adjectives, and
these are precisely the adjectives which are found
frequently in the training data and so for which
semantic information is least necessary.
While we do not reliably have access to the
meaning of an adjective, we do always have ac-
cess to its form. And, fortunately, for many of
the cases in which the direct evidence method
fails, finding a previously seen pair of adjec-
tives with a similar form has the effect of find-
ing a pair with a similar meaning. For ex-
ample, suppose we want to order the adjective
pair {21-year-old,Armenian}. If this pair ap-
pears in the training data, then the previous oc-
currences of this pair will be used to predict
the order and the method reduces to direct ev-
idence. If, on the other hand, that particu-
lar pair did not appear in the training data, we
can base the classification on previously seen
pairs with a similar form. In this way, we
may find pairs like {73-year-old,Colombian} and
{44-year-old,Norwegian}, which have more or
less the same distribution as the target pair.
To test the effectiveness of a form-based sim-
ilarity metric, we encoded each adjective pair ab
as a vector of 16 features (the last 8 characters
of a and the last 8 characters of b) and a class
a ? b or b ? a. Constructing the instance base
and testing the classification was performed using
the TiMBL 3.0 (Daelemans et al, 2000) memory-
based learning system. Instances to be classified
were compared to previously seen instances by
counting the number of feature values that the two
instances had in common.
In computing the similarity score, features
were weighted by their information gain, an in-
formation theoretic measure of the relevance of a
feature for determining the correct classification
(Quinlan, 1986; Daelemans and van den Bosch,
1992). This weighting reduces the sensitivity of
memory based learning to the presence of irrele-
vant features.
Given the probability pi of finding each class
i in the instance base D, we can compute the en-
tropy H(D), a measure of the amount of uncer-
tainty in D:
H(D) =??
pi
pi log2 pi
In the case of the adjective ordering data, there
are two classes a ? b and b ? a, each of which
occurs with a probability of roughly 0.5, so the
entropy of the instance base is close to 1 bit. We
can also compute the entropy of a feature f which
takes values V as the weighted sum of the entropy
of each of the values V :
H(D f ) = ?
vi?V
H(D f =vi)
|D f =vi |
|D|
Here H(D f =vi) is the entropy of subset of the in-
stance base which has value vi for feature f . The
information gain of a feature then is simply the
difference between the total entropy of the in-
stance base and the entropy of a single feature:
G(D, f ) = H(D)?H(D f )
The information gain G(D, f ) is the reduction in
uncertainty in D we expect to achieve by learning
the value of the feature f . In other words, know-
ing the value of a feature with a higher G gets us
closer on average to knowing the class of an in-
stance than knowing the value of a feature with a
lower G does.
The similarity ? between two instances then is
the number of feature values they have in com-
mon, weighted by the information gain:
?(X ,Y ) =
n
?
i=1
G(D, i)?(xi,yi)
where:
?(xi,yi) =
{
1 if xi = yi
0 otherwise
Classification was based on the five training in-
stances most similar to the instance to be classi-
fied, and produced an overall prediction accuracy
of 89.34% for the BNC data.
3.6 Positional probabilities
One difficulty faced by each of the methods de-
scribed so far is that they all to one degree or an-
other depend on finding particular pairs of adjec-
tives. For example, in order for the direct evi-
dence method to assign an order to a pair of ad-
jectives like {blue, large}, this specific pair must
have appeared in the training data. If not, an or-
der will have to be assigned randomly, even if
the individual adjectives blue and large appear
quite frequently in combination with a wide vari-
ety of other adjectives. Both the adjective bigram
method and the memory-based learning method
reduce this dependency on pairs to a certain ex-
tent, but these methods still suffer from the fact
that even for common adjectives one is much less
likely to find a specific pair in the training data
than to find some pair of which a specific adjec-
tive is a member.
Recall that the adjective bigram method
depended on estimating the probabilities
P(?a,b?|{a,b}) and P(?b,a?|{a,b}). Suppose we
now assume that the probability of a particular
adjective appearing first in a sequence depends
only on that adjective, and not the the other ad-
jectives in the sequence. We can easily estimate
the probability that if an adjective pair includes
some given adjective a, then that adjective occurs
first (let us call that P(?a,x?|{a,x})) by looking
at each pair in the training data that includes
that adjective a. Then, given the assumption of
independence, the probability P(?a,b?|{a,b})
is simply the product of P(?a,x?|{a,x}) and
P(?x,b?|{b,x}). Taking the most likely order
for a pair of adjectives using this alternative
method for estimating P(?a,b?|{a,b}) and
P(?a,b?|{a,b}) gives quite good results: a
prediction accuracy of 89.73% for the BNC data.
At first glance, the effectiveness of this method
may be surprising since it is based on an indepen-
dence assumption which common sense indicates
must not be true. However, to order a pair of ad-
jectives, this method brings to bear information
from all the previously seen pairs which include
either of adjectives in the pair in question. Since
it makes much more effective use of the train-
ing data, it can nevertheless achieve high accu-
racy. This method also has the advantage of be-
ing computationally quite simple. Applying this
method requires only one easy-to-calculate value
be stored for each possible adjective. Compared
to the other methods, which require at a mini-
mum that all of the training data be available dur-
ing classification, this represents a considerable
resource savings.
3.7 Combined method
The two highest scoring methods, using memory-
based learning and positional probability, perform
similarly, and from the point of view of accuracy
there is little to recommend one method over the
other. However, it is interesting to note that the er-
rors made by the two methods do not completely
overlap: while either of the methods gives the
right answer for about 89% of the test data, one
of the two is right 95.00% of the time. This in-
dicates that a method which combined the infor-
mation used by the memory-based learning and
positional probability methods ought to be able
to perform better than either one individually.
To test this possibility, we added two new fea-
tures to the representation described in section
3.5. Besides information about the morphological
form of the adjectives in the pair, we also included
the positional probabilities P(?a,x?|{a,x}) and
P(?b,x?|{b,x}) as real-valued features. For nu-
meric features, the similarity metric ? is com-
puted using the scaled difference between the val-
ues:
?(xi,yi) =
xi? yi
maxi?mini
Repeating the MBL experiment with these two
additional features yields 91.85% accuracy for
the BNC data, a 24% reduction in error rate over
purely morphological MBL with only a modest
increase in resource requirements.
4 Future directions
To get an idea of what the upper bound on ac-
curacy is for this task, we tried applying the di-
rect evidence method trained on both the train-
ing data and the held-out test data. This gave
an accuracy of approximately 99%, which means
that 1% of the pairs in the corpus are in the
?wrong? order. For an even larger percentage of
pairs either order is acceptable, so an evaluation
procedure which assumes that the observed or-
der is the only correct order will underestimate
the classification accuracy. Native speaker intu-
itions about infrequently-occurring adjectives are
not very strong, so it is difficult to estimate what
fraction of adjective pairs in the corpus are ac-
tually unordered. However, it should be clear
that even a perfect method for ordering adjectives
would score well below 100% given the experi-
mental set-up described here.
While the combined MBL method achieves
reasonably good results even given the limitations
of the evaluation method, there is still clearly
room for improvement. Future work will pur-
sue at least two directions for improving the re-
sults. First, while semantic information is not
available for all adjectives, it is clearly available
for some. Furthermore, any realistic dialog sys-
tem would make use of some limited vocabulary
Direct evidence 78.28%
Adjective bigrams 88.02%
MBL (morphological) 89.34% (*)
Positional probabilities 89.73% (*)
MBL (combined) 91.85%
Table 1: Summary of results. With the exception
of the starred values, all differences are statisti-
cally significant (p< 0.005)
for which semantic information would be avail-
able. More generally, distributional clustering
techniques (Schu?tze, 1992; Pereira et al, 1993)
could be applied to extract semantic classes from
the corpus itself. Since the constraints on adjec-
tive ordering in English depend largely on seman-
tic classes, the addition of semantic information
to the model ought to improve the results.
The second area where the methods described
here could be improved is in the way that multi-
ple information sources are integrated. The tech-
nique method described in section 3.7 is a fairly
crude method for combining frequency informa-
tion with symbolic data. It would be worthwhile
to investigate applying some of the more sophis-
ticated ensemble learning techniques which have
been proposed in the literature (Dietterich, 1997).
In particular, boosting (Schapire, 1999; Abney et
al., 1999) offers the possibility of achieving high
accuracy from a collection of classifiers which in-
dividually perform quite poorly.
5 Conclusion
In this paper, we have presented the results of ap-
plying a number of statistical and machine learn-
ing techniques to the problem of predicting the
order of prenominal adjectives in English. The
scores for each of the methods are summarized in
table 1. The best methods yield around 90% ac-
curacy, better than the best previously published
methods when applied to the broad domain data
of the British National Corpus. Note that Mc-
Nemar?s test (Dietterich, 1998) confirms the sig-
nificance of all of the differences reflected here
(with p< 0.005) with the exception of the differ-
ence between purely morphological MBL and the
method based on positional probabilities.
From this investigation, we can draw some ad-
ditional conclusions. First, a solution specific
to adjective ordering works better than a gen-
eral probabilistic filter. Second, machine learn-
ing techniques can be applied to a different kind
of linguistic problem with some success, even in
the absence of syntagmatic context, and can be
used to augment a hand-built competence gram-
mar. Third, in some cases statistical and memory
based learning techniques can be combined in a
way that performs better than either individually.
6 Acknowledgments
I am indebted to Carol Bleyle, John Carroll, Ann
Copestake, Guido Minnen, Miles Osborne, au-
diences at the University of Groningen and the
University of Sussex, and three anonymous re-
viewers for their comments and suggestions. The
work described here was supported by the School
of Behavioral and Cognitive Neurosciences at the
University of Groningen.
References
Steven Abney, Robert E. Schapire, and Yoram Singer.
1999. Boosting applied to tagging and PP attach-
ment. In Proceedings of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Lou Burnard. 1995. Users reference guide for the
British National Corpus, version 1.0. Technical re-
port, Oxford University Computing Services.
John Carroll, Ann Copestake, Dan Flickinger, and
Victor Poznanski. 1999. An efficient chart gen-
erator for (semi-)lexicalist grammars. In Proceed-
ings of the 7th European Workshop on Natural
Language Generation (EWNLG?99), pages 86?95,
Toulouse.
Philip R. Clarkson and Ronald Rosenfeld. 1997.
Statistical language modeling using the CMU-
Cambridge Toolkit. In G. Kokkinakis, N. Fako-
takis, and E. Dermatas, editors, Eurospeech ?97
Proceedings, pages 2707?2710.
Walter Daelemans and Antal van den Bosch. 1992.
Generalization performance of backpropagation
learning on a syllabification task. In M.F.J.
Drossaers and A. Nijholt, editors, Proceedings of
TWLT3: Connectionism and Natural Language
Processing, Enschede. University of Twente.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2000. TiMBL:
Tilburg memory based learner, version 3.0, refer-
ence guide. ILK Technical Report 00-01, Tilburg
University. Available from http://ilk.kub.nl/
~ilk/papers/ilk0001.ps.gz.
Thomas G. Dietterich. 1997. Machine learning
research: four current directions. AI Magazine,
18:97?136.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learn-
ing algorithms. Neural Computation, 10(7):1895?
1924.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Irene Langkilde and Kevin Knight. 1998a. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proceedings of 36th Annual Meeting of
the Association for Computational Linguistics and
17th International Conference on Computational
Linguistics, pages 704?710, Montreal.
Irene Langkilde and Kevin Knight. 1998b. The practi-
cal value of n-grams in generation. In Proceedings
of the International Natural Language Generation
Workshop, Niagara-on-the-Lake, Ontario.
Fernando Pereira, Naftali Tishby, and Lilian Lee.
1993. Distributional clustering of English words.
In Proceedings of the 30th annual meeting of the
Association for Computational Linguistics, pages
183?190.
J. Ross Quinlan. 1986. Induction of decision trees.
Machine Learning, 1:81?106.
Randolf Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, London.
Robert E. Schapire. 1999. A brief introduction to
boosting. In Proceedings of the Sixteenth Interna-
tional Joint Conference on Artificial Intelligence.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In Proceedings of Supercomputing, pages 787?796,
Minneapolis.
James Shaw and Vasileios Hatzivassiloglou. 1999.
Ordering among premodifiers. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 135?143, Col-
lege Park, Maryland.
Using an open-source unification-based system for CL/NLP teaching
Ann Copestake
Computer Laboratory
University of Cambridge
Cambridge, UK
aac@cl.cam.ac.uk
John Carroll
Cognitive and Computing Sciences
University of Sussex
Falmer, Brighton, UK
johnca@cogs.susx.ac.uk
Dan Flickinger
CSLI, Stanford University and
YY Software
Ventura Hall,
Stanford, USA
danf@csli.stanford.edu
Robert Malouf
Alfa Informatica,
University of Groningen,
Postbus 716, 9700 AS Groningen,
The Netherlands
malouf@let.rug.nl
Stephan Oepen
YY Software and
CSLI, Stanford University
110 Pioneer Way
Mountain View, USA
oe@yy.com
Abstract
We demonstrate the open-source LKB
system which has been used to teach the
fundamentals of constraint-based gram-
mar development to several groups of
students.
1 Overview of the LKB system
The LKB system is a grammar development
environment that is distributed as part of the
open source LinGO tools (http://www-
csli.stanford.edu/?aac/lkb.html
and http://lingo.stanford.edu, see
also Copestake and Flickinger, 2000). It is an
open-source grammar development environment
implemented in Common Lisp, distributed not
only as source but also as a standalone application
that can be run on Linux, Solaris and Windows
(see the website for specific requirements). It
will also run under Macintosh Common Lisp,
but for this a license is required. The LKB in-
cludes a parser, generator, support for large-scale
inheritance hierarchies (including the use of
defaults), various tools for manipulating semantic
representations, a rich set of graphical tools
for analyzing and debugging grammars, and
extensive on-line documentation. Grammars of
all sizes have been written using the LKB, for
several languages, mostly within the linguistic
frameworks of Categorial Grammar and Head-
Driven Phrase Structure Grammar. The LKB
system was initially developed in 1991, but has
gone through multiple versions since then. It
is in active use by a considerable number of
researchers worldwide. An introductory book
on implementing grammars in typed feature
structure formalisms using the LKB is near
completion (Copestake, in preparation).
2 Demo outline
Although the LKB has been successfully used for
large-scale grammar development, this demon-
stration will concentrate on its use with relatively
small scale teaching grammars, of a type which
can be developed by students in practical exer-
cises. We will show an English grammar frag-
ment which is linked to a textbook on formal syn-
tax (Sag and Wasow, 1999) to illustrate how the
system may be used in conjunction with more tra-
ditional materials in a relatively linguistically ori-
ented course. We will demonstrate the tools for
analyzing parses and for debugging and also dis-
cuss the way that parse selection mechanisms can
be incorporated in the system. If time permits, we
will show how semantic analyses produced with a
somewhat more complex grammar can be linked
up to a theorem prover and also exploited in se-
mantic transfer for Machine Translation. Exer-
cises where the grammar is part of a larger system
are generally appropriate for advanced courses or
for NLP application courses.
The screen dump in the figure is from a session
working with a grammar fragment for Esperanto.
This shares its basic types and rules with the
English textbook grammar fragment mentioned
above. The windows shown are:
1. The LKB Top interaction window: main
  
 
 
 
Figure 1: Screen dump of the LKB system
menus plus feedback and error messages
2. Type hierarchy window (fragment): the
more general types are on the left. Nodes in
the hierarchy have menus that provide more
information about the types, such as their as-
sociated constraints.
3. Type constraint for the type intrans-verb:
again nodes are clickable for further infor-
mation.
4. Parse tree for La knabo dormas (the boy
sleeps): a larger display for parse trees is
also available, but this scale is useful for
summary information. Menus associated
with trees allow for display of associated se-
mantic information if any is included in the
grammar and for generation. Here the dis-
play shows inflectional rules as well as nor-
mal syntactic rules: hence the VP node un-
der dormas, which corresponds to the stem.
5. In the middle is an emacs window displaying
the source file for the lexicon associated with
this grammar.1 It shows the entry for the lex-
1(We generally use emacs as an editor when teaching,
eme dorm, which, like most lexical entries in
this grammar, just specifies a spelling and a
type (here intrans-verb).
6. Part of the parse chart corresponding to the
tree is shown in the bottom window: nodes
which have knabo as a descendant are high-
lighted. Again, these nodes are active: one
very useful facility associated with them is a
unification checker which allows the gram-
mar writer to establish why a rule did not
apply to a phrase or phrases.
3 Use of the LKB in teaching
Teaching uses of the LKB have included under-
graduate and graduate courses on formal syntax
and on computational linguistics at several sites,
grammar engineering courses at two ESSLLI
summer schools, and numerous student projects
at undergraduate, masters and doctoral levels. An
advantage of the LKB is that students learn to use
a system which is sufficiently heavy duty for more
advanced work, up to the scale at least of research
although this causes some overhead, especially for students
who are only used to word processing programs.
prototypes. This provides them with a good plat-
form on which to build for further research. Feed-
back from the courses we have taught has mostly
been very positive, but we have found a ratio of
six students to one instructor (or teaching assis-
tant) to be the maximum that is workable. One
major reason is that debugging students? gram-
mars and teaching debugging techniques is time-
consuming.
When teaching an introductory course with the
LKB, we start the students off with a very sim-
ple grammar, which they are asked to expand
in specific ways. We introduce various addi-
tional techniques and formal devices (such as in-
flectional and lexical rules, defaults, difference
lists and gaps) gradually during a course. Mate-
rial from our ESSLLI courses, including starting
grammars, exercises and solutions is distributed
via the website. Several other small grammars
developed by students are also distributed as part
of the LKB system and we would welcome fur-
ther contributions. We are hoping to facilitate this
by making it easier for people outside the LinGO
group to add and modify grammars.
Several graduate students have used versions
of the LKB system as part of their thesis work,
for diverse projects including machine transla-
tion and grammar learning. It has been used
in the development of several large grammars,
especially the LinGO English Resource Gram-
mar (ERG), which is itself open-source. Re-
search applications for the ERG include spoken
language machine translation in Verbmobil, gen-
eration for a speech prosthesis, and automated
email response, under development for commer-
cial use. The LKB/ERG combination can be used
by researchers who require a grammar which pro-
vides a detailed semantic analysis and reason-
ably broad coverage, for instance for experiments
on dialogue. The LKB has also been used as
a grammar preprocessor to facilitate experiments
on efficiency using the ERG with other systems
(Flickinger et al 2000).
4 Comparison with other work
There is a long history of the use of fea-
ture structure based systems in teaching, dat-
ing back at least to PATR (Shieber, 1986:
see http://www.ling.gu.se/?li/). The
Alvey Natural Language Tools (Briscoe et al
1987) have been used for teaching at several uni-
versities: Briscoe and Grover developed an ex-
tensive set of teaching examples and exercises,
which is however unpublished. Versions of the
SRI Core Language Engine (Alshawi, 1992) and
of the XTAG grammar (XTAG group, 1995) and
parser have also been used for teaching. Besides
the LKB, typed feature structure environments
have been used at many universities, though un-
like the systems cited above, most have only been
used with small grammars and may not scale
up. Hands on courses using various systems have
been run at many recent summer schools includ-
ing ESSLLI 99 (using the Xerox XLE, see Butt
et al 1999) and ESSLLI 97 and the 1999 LSA
summer school (both using ConTroll, see Hin-
richs and Meurers, 1999). Very little seems to
have been formally published describing expe-
riences in teaching with grammar development
environments, though Bouma (1999) describes
material for teaching a computational linguistics
course that includes exercises using the Hdrug
unification-based enviroment to extend a gram-
mar.
Despite this rich variety of tools, we believe
that the LKB system has a combination of fea-
tures which make it distinctive and give it a useful
niche in teaching. The most important points are
that its availability as open source, combined with
scale and efficiency, allow advanced projects to be
supported as well as introductory courses. As far
as we are aware, it is the only system freely avail-
able with a broad coverage grammar that sup-
ports semantic interpretation and generation. Es-
pecially for more linguistically oriented courses,
the link to the Sag and Wasow textbook is also
important. Similar grammars could be developed
for other systems, but would be less directly com-
parable to the textbook since this assumes a de-
fault formalism which so far is only implemented
in the LKB.
On the other hand, the LKB is not a suitable ba-
sis for a course that involves the students learning
to implement a unifier, parser and so on. The sys-
tem is quite complex (about 120 files and 40,000
lines of Lisp code) and though the vast majority
of this is concerned with non-core functionality,
such as the graphical interfaces, it is still some-
what daunting. This seems an inevitable trade-
off of having a system powerful enough for real
applications (see Bouma (1999) for related dis-
cussion). It is questionable whether the LKB is
entirely satisfactory as a student?s first computa-
tional grammar system, although we have used it
with students who have no prior experience of this
sort: ideally we would suggest starting off with
brief exercises with a pure context-free grammar
to explain the concepts of well-formedness, re-
cursion and so on. We also wouldn?t necessar-
ily advocate using the LKB as a core component
of a first course on formal syntax for linguistic
students, since the specifics of dealing with an
implementation may interfere with understanding
of basic concepts, though it is suitable as a sup-
plement to an initial course or as the basis for a
slightly more advanced course.
We think there is considerable potential for
building materials for courses that allow students
to work with realistic but transparent applications
using the LKB and a large grammar as a compo-
nent. Developing such materials is clearly nec-
essary in order to give students useful practical
experience. It is however very time-consuming,
and most probably will have to be undertaken as
part of a cooperative, open-source development
involving people from several different institu-
tions.
Acknowledgements
This research was partially supported by the Na-
tional Science Foundation, grant number IRI-
9612682. The current versions of the English
grammars associated with the Sag and Wasow
textbook were largely developed by Christopher
Callison-Burch while he was an undergraduate at
Stanford.
References
Alshawi, Hiyan (ed). [1992] The Core Language
Engine, MIT Press, Cambridge, MA.
Bouma, Gosse. [1999] ?A modern computa-
tional linguistics course using Dutch.? In Frank
van Eynde and Ineke Schuurman, editors, CLIN
1998, Papers from the Ninth CLIN Meeting, Am-
sterdam. Rodopi Press.
Briscoe, Ted, Claire Grover, Bran Boguraev
and John Carroll. [1987] ?A formalism and en-
vironment for the development of a large gram-
mar of English?, Proceedings of the 10th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-87), Milan, Italy, 703?708.
Butt, Miriam, Anette Frank and Jonas Kuhn.
[1999] ?Development of large scale LFG gram-
mars ? Linguistics, Engineering and Resources?,
http://www.xrce.xerox.com/people/
frank/esslli99-hp/index.html
Copestake, Ann. [in preparation] Implementing
typed feature structure grammars, CSLI Publica-
tions, Stanford.
Copestake, Ann and Dan Flickinger. [2000]
?An open-source grammar development environ-
ment and broad-coverage English grammar us-
ing HPSG?, Second conference on Language Re-
sources and Evaluation (LREC-2000), Athens,
Greece.
Flickinger, Daniel, Stephan Oepen, Hans
Uszkoreit and Jun?ichi Tsujii. [2000] Journal of
Natural Language Engineering. Special Issue on
Efficient Processing with HPSG: Methods, Sys-
tems, Evaluation, 6(1).
Hinrichs, Erhard and Detmar Meurers [1999]
?Grammar Development in Constraint-Based
Formalisms?,
http://www.ling.ohio-state.edu/
?dm/lehre/lsa99/material.html,
see also http://www.sfs.nphil.uni-
tuebingen.de/controll/
Sag, Ivan, and Tom Wasow [1999] Syntactic
Theory: An Introduction, CSLI Publications.
Shieber, Stuart [1986] An Introduction to
Unification-based Approaches to Grammar,
CSLI Publications.
The XTAG Research Group [1995]. ?A Lex-
icalized Tree Adjoining Grammar for English?
IRCS Report 95-03, University of Pennsylvania?
A comparison of algorithms for maximum entropy parameter estimation
Robert Malouf
Alfa-Informatica
Rijksuniversiteit Groningen
Postbus 716
9700AS Groningen
The Netherlands
malouf@let.rug.nl
Abstract
Conditional maximum entropy (ME) models pro-
vide a general purpose machine learning technique
which has been successfully applied to fields as
diverse as computer vision and econometrics, and
which is used for a wide variety of classification
problems in natural language processing. However,
the flexibility of ME models is not without cost.
While parameter estimation for ME models is con-
ceptually straightforward, in practice ME models
for typical natural language tasks are very large, and
may well contain many thousands of free parame-
ters. In this paper, we consider a number of algo-
rithms for estimating the parameters of ME mod-
els, including iterative scaling, gradient ascent, con-
jugate gradient, and variable metric methods. Sur-
prisingly, the standardly used iterative scaling algo-
rithms perform quite poorly in comparison to the
others, and for all of the test problems, a limited-
memory variable metric algorithm outperformed the
other choices.
1 Introduction
Maximum entropy (ME) models, variously known
as log-linear, Gibbs, exponential, and multinomial
logit models, provide a general purpose machine
learning technique for classification and prediction
which has been successfully applied to fields as di-
verse as computer vision and econometrics. In natu-
ral language processing, recent years have seen ME
techniques used for sentence boundary detection,
part of speech tagging, parse selection and ambigu-
ity resolution, and stochastic attribute-value gram-
mars, to name just a few applications (Abney, 1997;
Berger et al, 1996; Ratnaparkhi, 1998; Johnson et
al., 1999).
A leading advantage of ME models is their flex-
ibility: they allow stochastic rule systems to be
augmented with additional syntactic, semantic, and
pragmatic features. However, the richness of the
representations is not without cost. Even mod-
est ME models can require considerable computa-
tional resources and very large quantities of anno-
tated training data in order to accurately estimate
the model?s parameters. While parameter estima-
tion for ME models is conceptually straightforward,
in practice ME models for typical natural language
tasks are usually quite large, and frequently contain
hundreds of thousands of free parameters. Estima-
tion of such large models is not only expensive, but
also, due to sparsely distributed features, sensitive
to round-off errors. Thus, highly efficient, accurate,
scalable methods are required for estimating the pa-
rameters of practical models.
In this paper, we consider a number of algorithms
for estimating the parameters of ME models, in-
cluding Generalized Iterative Scaling and Improved
Iterative Scaling, as well as general purpose opti-
mization techniques such as gradient ascent, conju-
gate gradient, and variable metric methods. Sur-
prisingly, the widely used iterative scaling algo-
rithms perform quite poorly, and for all of the test
problems, a limited memory variable metric algo-
rithm outperformed the other choices.
2 Maximum likelihood estimation
Suppose we are given a probability distribution p
over a set of events X which are characterized by a
d dimensional feature vector function f : X ? Rd .
In addition, we have also a set of contexts W and a
function Y which partitions the members of X . In
the case of a stochastic context-free grammar, for
example, X might be the set of possible trees, the
feature vectors might represent the number of times
each rule applied in the derivation of each tree, W
might be the set of possible strings of words, and
Y (w) the set of trees whose yield is w ?W . A con-
ditional maximum entropy model q?(x|w) for p has
the parametric form (Berger et al, 1996; Chi, 1998;
Johnson et al, 1999):
q?(x|w) =
exp
(
?T f (x))
?y?Y (w) exp(?T f (y))
(1)
where ? is a d-dimensional parameter vector and
?T f (x) is the inner product of the parameter vector
and a feature vector.
Given the parametric form of an ME model in
(1), fitting an ME model to a collection of training
data entails finding values for the parameter vector
? which minimize the Kullback-Leibler divergence
between the model q? and the empirical distribu-
tion p:
D(p||q?) = ?
w,x
p(x,w) log p(x|w)
q?(x|w)
or, equivalently, which maximize the log likelihood:
L(?) = ?
w,x
p(w,x) logq?(x|w) (2)
The gradient of the log likelihood function, or the
vector of its first derivatives with respect to the pa-
rameter ? is:
G(?) = Ep[ f ]?Eq? [ f ] (3)
Since the likelihood function (2) is concave over
the parameter space, it has a global maximum where
the gradient is zero. Unfortunately, simply setting
G(?) = 0 and solving for ? does not yield a closed
form solution, so we proceed iteratively. At each
step, we adjust an estimate of the parameters ?(k)
to a new estimate ?(k+1) based on the divergence
between the estimated probability distribution q(k)
and the empirical distribution p. We continue until
successive improvements fail to yield a sufficiently
large decrease in the divergence.
While all parameter estimation algorithms we
will consider take the same general form, the
method for computing the updates ?(k) at each
search step differs substantially. As we shall see,
this difference can have a dramatic impact on the
number of updates required to reach convergence.
2.1 Iterative Scaling
One popular method for iteratively refining the
model parameters is Generalized Iterative Scaling
(GIS), due to Darroch and Ratcliff (1972). An
extension of Iterative Proportional Fitting (Dem-
ing and Stephan, 1940), GIS scales the probabil-
ity distribution q(k) by a factor proportional to the
ratio of Ep[ f ] to Eq(k) [ f ], with the restriction that
? j f j(x) = C for each event x in the training data
(a condition which can be easily satisfied by the ad-
dition of a correction feature). We can adapt GIS
to estimate the model parameters ? rather than the
model probabilities q, yielding the update rule:
?(k) = log
(
Ep[ f ]
Eq(k) [ f ]
) 1
C
The step size, and thus the rate of convergence,
depends on the constant C: the larger the value of
C, the smaller the step size. In case not all rows of
the training data sum to a constant, the addition of a
correction feature effectively slows convergence to
match the most difficult case. To avoid this slowed
convergence and the need for a correction feature,
Della Pietra et al (1997) propose an Improved Iter-
ative Scaling (IIS) algorithm, whose update rule is
the solution to the equation:
Ep[ f ] = ?
w,x
p(w)q(k)(x|w) f (x)exp(M(x)?(k))
where M(x) is the sum of the feature values for an
event x in the training data. This is a polynomial in
exp
(
?(k)
)
, and the solution can be found straight-
forwardly using, for example, the Newton-Raphson
method.
2.2 First order methods
Iterative scaling algorithms have a long tradition in
statistics and are still widely used for analysis of
contingency tables. Their primary strength is that
on each iteration they only require computation of
the expected values Eq(k) . They do not depend on
evaluation of the gradient of the log-likelihood func-
tion, which, depending on the distribution, could be
prohibitively expensive. In the case of ME models,
however, the vector of expected values required by
iterative scaling essentially is the gradient G. Thus,
it makes sense to consider methods which use the
gradient directly.
The most obvious way of making explicit use of
the gradient is by Cauchy?s method, or the method
of steepest ascent. The gradient of a function is a
vector which points in the direction in which the
function?s value increases most rapidly. Since our
goal is to maximize the log-likelihood function, a
natural strategy is to shift our current estimate of
the parameters in the direction of the gradient via
the update rule:
?(k) = ?(k)G(?(k))
where the step size ?(k) is chosen to maximize
L(?(k) + ?(k)). Finding the optimal step size is itself
an optimization problem, though only in one dimen-
sion and, in practice, only an approximate solution
is required to guarantee global convergence.
Since the log-likelihood function is concave, the
method of steepest ascent is guaranteed to find the
global maximum. However, while the steps taken
on each iteration are in a very narrow sense locally
optimal, the global convergence rate of steepest as-
cent is very poor. Each new search direction is or-
thogonal (or, if an approximate line search is used,
nearly so) to the previous direction. This leads to
a characteristic ?zig-zag? ascent, with convergence
slowing as the maximum is approached.
One way of looking at the problem with steep-
est ascent is that it considers the same search di-
rections many times. We would prefer an algo-
rithm which considered each possible search direc-
tion only once, in each iteration taking a step of ex-
actly the right length in a direction orthogonal to all
previous search directions. This intuition underlies
conjugate gradient methods, which choose a search
direction which is a linear combination of the steep-
est ascent direction and the previous search direc-
tion. The step size is selected by an approximate
line search, as in the steepest ascent method. Sev-
eral non-linear conjugate gradient methods, such as
the Fletcher-Reeves (cg-fr) and the Polak-Ribie`re-
Positive (cf-prp) algorithms, have been proposed.
While theoretically equivalent, they use slighly dif-
ferent update rules and thus show different numeric
properties.
2.3 Second order methods
Another way of looking at the problem with steep-
est ascent is that while it takes into account the gra-
dient of the log-likelihood function, it fails to take
into account its curvature, or the gradient of the gra-
dient. The usefulness of the curvature is made clear
if we consider a second-order Taylor series approx-
imation of L(? + ?):
L(? + ?)? L(?)+ ?T G(?)+ 1
2
?T H(?)? (4)
where H is Hessian matrix of the log-likelihood
function, the d ? d matrix of its second partial
derivatives with respect to ?. If we set the deriva-
tive of (4) to zero and solve for ?, we get the update
rule for Newton?s method:
?(k) = H?1(?(k))G(?(k)) (5)
Newton?s method converges very quickly (for
quadratic objective functions, in one step), but it re-
quires the computation of the inverse of the Hessian
matrix on each iteration.
While the log-likelihood function for ME models
in (2) is twice differentiable, for large scale prob-
lems the evaluation of the Hessian matrix is com-
putationally impractical, and Newton?s method is
not competitive with iterative scaling or first order
methods. Variable metric or quasi-Newton methods
avoid explicit evaluation of the Hessian by building
up an approximation of it using successive evalua-
tions of the gradient. That is, we replace H?1(?(k))
in (5) with a local approximation of the inverse Hes-
sian B(k):
?(k) = B(k)G(?(k))
with B(k) a symmatric, positive definite matrix
which satisfies the equation:
B(k)y(k) = ?(k?1)
where y(k) = G(?(k))?G(?(k?1)).
Variable metric methods also show excellent con-
vergence properties and can be much more efficient
than using true Newton updates, but for large scale
problems with hundreds of thousands of parame-
ters, even storing the approximate Hessian is pro-
hibitively expensive. For such cases, we can apply
limited memory variable metric methods, which im-
plicitly approximate the Hessian matrix in the vicin-
ity of the current estimate of ?(k) using the previous
m values of y(k) and ?(k). Since in practical applica-
tions values of m between 3 and 10 suffice, this can
offer a substantial savings in storage requirements
over variable metric methods, while still giving fa-
vorable convergence properties.1
3 Comparing estimation techniques
The performance of optimization algorithms is
highly dependent on the specific properties of the
problem to be solved. Worst-case analysis typically
1Space constraints preclude a more detailed discussion of
these methods here. For algorithmic details and theoretical
analysis of first and second order methods, see, e.g., Nocedal
(1997) or Nocedal and Wright (1999).
does not reflect the actual behavior on actual prob-
lems. Therefore, in order to evaluate the perfor-
mance of the optimization techniques sketched in
previous section when applied to the problem of pa-
rameter estimation, we need to compare the perfor-
mance of actual implementations on realistic data
sets (Dolan and More?, 2002).
Minka (2001) offers a comparison of iterative
scaling with other algorithms for parameter esti-
mation in logistic regression, a problem similar to
the one considered here, but it is difficult to trans-
fer Minka?s results to ME models. For one, he
evaluates the algorithms with randomly generated
training data. However, the performance and accu-
racy of optimization algorithms can be sensitive to
the specific numerical properties of the function be-
ing optimized; results based on random data may
or may not carry over to more realistic problems.
And, the test problems Minka considers are rela-
tively small (100?500 dimensions). As we have
seen, though, algorithms which perform well for
small and medium scale problems may not always
be applicable to problems with many thousands of
dimensions.
3.1 Implementation
As a basis for the implementation, we have used
PETSc (the ?Portable, Extensible Toolkit for Sci-
entific Computation?), a software library designed
to ease development of programs which solve large
systems of partial differential equations (Balay et
al., 2001; Balay et al, 1997; Balay et al, 2002).
PETSc offers data structures and routines for paral-
lel and sequential storage, manipulation, and visu-
alization of very large sparse matrices.
For any of the estimation techniques, the most ex-
pensive operation is computing the probability dis-
tribution q and the expectations Eq[ f ] for each it-
eration. In order to make use of the facilities pro-
vided by PETSc, we can store the training data as
a (sparse) matrix F , with rows corresponding to
events and columns to features. Then given a pa-
rameter vector ?, the unnormalized probabilities q??
are the matrix-vector product:
q?? = expF?
and the feature expectations are the transposed
matrix-vector product:
Eq? [ f ] = FT q?
By expressing these computations as matrix-vector
operations, we can take advantage of the high per-
formance sparse matrix primitives of PETSc.
For the comparison, we implemented both Gener-
alized and Improved Iterative Scaling in C++ using
the primitives provided by PETSc. For the other op-
timization techniques, we used TAO (the ?Toolkit
for Advanced Optimization?), a library layered on
top of the foundation of PETSc for solving non-
linear optimization problems (Benson et al, 2002).
TAO offers the building blocks for writing optimiza-
tion programs (such as line searches and conver-
gence tests) as well as high-quality implementations
of standard optimization algorithms (including con-
jugate gradient and variable metric methods).
Before turning to the results of the comparison,
two additional points need to be made. First, in
order to assure a consistent comparison, we need
to use the same stopping rule for each algorithm.
For these experiments, we judged that convergence
was reached when the relative change in the log-
likelihood between iterations fell below a predeter-
mined threshold. That is, each run was stopped
when:
|L(?(k))?L(?(k?1))|
L(?(k))
< ? (6)
where the relative tolerance ? = 10?7. For any par-
ticular application, this may or may not be an appro-
priate stopping rule, but is only used here for pur-
poses of comparison.
Finally, it should be noted that in the current im-
plementation, we have not applied any of the possi-
ble optimizations that appear in the literature (Laf-
ferty and Suhm, 1996; Wu and Khudanpur, 2000;
Lafferty et al, 2001) to speed up normalization of
the probability distribution q. These improvements
take advantage of a model?s structure to simplify the
evaluation of the denominator in (1). The particular
data sets examined here are unstructured, and such
optimizations are unlikely to give any improvement.
However, when these optimizations are appropriate,
they will give a proportional speed-up to all of the
algorithms. Thus, the use of such optimizations is
independent of the choice of parameter estimation
method.
3.2 Experiments
To compare the algorithms described in ?2, we ap-
plied the implementation outlined in the previous
section to four training data sets (described in Table
1) drawn from the domain of natural language pro-
cessing. The ?rules? and ?lex? datasets are examples
dataset classes contexts features non-zeros
rules 29,602 2,525 246 732,384
lex 42,509 2,547 135,182 3,930,406
summary 24,044 12,022 198,467 396,626
shallow 8,625,782 375,034 264,142 55,192,723
Table 1: Datasets used in experiments
of stochastic attribute value grammars, one with a
small set of SCFG-like features, and with a very
large set of fine-grained lexical features (Bouma
et al, 2001). The ?summary? dataset is part of a
sentence extraction task (Osborne, to appear), and
the ?shallow? dataset is drawn from a text chunking
application (Osborne, 2002). These datasets vary
widely in their size and composition, and are repre-
sentative of the kinds of datasets typically encoun-
tered in applying ME models to NLP classification
tasks.
The results of applying each of the parameter es-
timation algorithms to each of the datasets is sum-
marized in Table 2. For each run, we report the KL
divergence between the fitted model and the train-
ing data at convergence, the prediction accuracy of
fitted model on a held-out test set (the fraction of
contexts for which the event with the highest prob-
ability under the model also had the highest proba-
bility under the reference distribution), the number
of iterations required, the number of log-likelihood
and gradient evaluations required (algorithms which
use a line search may require several function eval-
uations per iteration), and the total elapsed time (in
seconds).2
There are a few things to observe about these
results. First, while IIS converges in fewer steps
the GIS, it takes substantially more time. At least
for this implementation, the additional bookkeeping
overhead required by IIS more than cancels any im-
provements in speed offered by accelerated conver-
gence. This may be a misleading conclusion, how-
ever, since a more finely tuned implementation of
IIS may well take much less time per iteration than
the one used for these experiments. However, even
if each iteration of IIS could be made as fast as an
2The reported time does not include the time required to in-
put the training data, which is difficult to reproduce and which
is the same for all the algorithms being tested. All tests were
run using one CPU of a dual processor 1700MHz Pentium 4
with 2 gigabytes of main memory at the Center for High Per-
formance Computing and Visualisation, University of Gronin-
gen.
iteration of GIS (which seems unlikely), the bene-
fits of IIS over GIS would in these cases be quite
modest.
Second, note that for three of the four datasets,
the KL divergence at convergence is roughly the
same for all of the algorithms. For the ?summary?
dataset, however, they differ by up to two orders of
magnitude. This is an indication that the conver-
gence test in (6) is sensitive to the rate of conver-
gence and thus to the choice of algorithm. Any de-
gree of precision desired could be reached by any
of the algorithms, with the appropriate value of ?.
However, GIS, say, would require many more itera-
tions than reported in Table 2 to reach the precision
achieved by the limited memory variable metric al-
gorithm.
Third, the prediction accuracy is, in most cases,
more or less the same for all of the algorithms.
Some variability is to be expected?all of the data
sets being considered here are badly ill-conditioned,
and many different models will yield the same like-
lihood. In a few cases, however, the prediction
accuracy differs more substantially. For the two
SAVG data sets (?rules? and ?lex?), GIS has a small
advantage over the other methods. More dramati-
cally, both iterative scaling methods perform very
poorly on the ?shallow? dataset. In this case, the
training data is very sparse. Many features are
nearly ?pseudo-minimal? in the sense of Johnson et
al. (1999), and so receive weights approaching ??.
Smoothing the reference probabilities would likely
improve the results for all of the methods and re-
duce the observed differences. However, this does
suggest that gradient-based methods are robust to
certain problems with the training data.
Finally, the most significant lesson to be drawn
from these results is that, with the exception of
steepest ascent, gradient-based methods outperform
iterative scaling by a wide margin for almost all the
datasets, as measured by both number of function
evaluations and by the total elapsed time. And, in
each case, the limited memory variable metric algo-
Dataset Method KL Div. Acc Iters Evals Time
rules gis 5.124?10?2 47.00 1186 1187 16.68
iis 5.079?10?2 43.82 917 918 31.36
steepest ascent 5.065?10?2 44.88 224 350 4.80
conjugate gradient (fr) 5.007?10?2 44.17 66 181 2.57
conjugate gradient (prp) 5.013?10?2 46.29 59 142 1.93
limited memory variable metric 5.007?10?2 44.52 72 81 1.13
lex gis 1.573?10?3 46.74 363 364 31.69
iis 1.487?10?3 42.15 235 236 95.09
steepest ascent 3.341?10?3 42.92 980 1545 114.21
conjugate gradient (fr) 1.377?10?3 43.30 148 408 30.36
conjugate gradient (prp) 1.893?10?3 44.06 114 281 21.72
limited memory variable metric 1.366?10?3 43.30 168 176 20.02
summary gis 1.857?10?3 96.10 1424 1425 107.05
iis 1.081?10?3 96.10 593 594 188.54
steepest ascent 2.489?10?3 96.33 1094 3321 190.22
conjugate gradient (fr) 9.053?10?5 95.87 157 849 49.48
conjugate gradient (prp) 3.297?10?4 96.10 112 537 31.66
limited memory variable metric 5.598?10?5 95.54 63 69 8.52
shallow gis 3.314?10?2 14.19 3494 3495 21223.86
iis 3.238?10?2 5.42 3264 3265 66855.92
steepest ascent 7.303?10?2 26.74 3677 14527 85062.53
conjugate gradient (fr) 2.585?10?2 24.72 1157 6823 39038.31
conjugate gradient (prp) 3.534?10?2 24.72 536 2813 16251.12
limited memory variable metric 3.024?10?2 23.82 403 421 2420.30
Table 2: Results of comparison.
rithm performs substantially better than any of the
competing methods.
4 Conclusions
In this paper, we have described experiments com-
paring the performance of a number of different al-
gorithms for estimating the parameters of a con-
ditional ME model. The results show that vari-
ants of iterative scaling, the algorithms which are
most widely used in the literature, perform quite
poorly when compared to general function opti-
mization algorithms such as conjugate gradient and
variable metric methods. And, more specifically,
for the NLP classification tasks considered, the lim-
ited memory variable metric algorithm of Benson
and More? (2001) outperforms the other choices by
a substantial margin.
This conclusion has obvious consequences for the
field. ME modeling is a commonly used machine
learning technique, and the application of improved
parameter estimation algorithms will it practical to
construct larger, more complex models. And, since
the parameters of individual models can be esti-
mated quite quickly, this will further open up the
possibility for more sophisticated model and feature
selection techniques which compare large numbers
of alternative model specifications. This suggests
that more comprehensive experiments to compare
the convergence rate and accuracy of various algo-
rithms on a wider range of problems is called for.
In addition, there is a larger lesson to be drawn
from these results. We typically think of computa-
tional linguistics as being primarily a symbolic dis-
cipline. However, statistical natural language pro-
cessing involves non-trivial numeric computations.
As these results show, natural language processing
can take great advantage of the algorithms and soft-
ware libraries developed by and for more quantita-
tively oriented engineering and computational sci-
ences.
Acknowledgements
The research of Dr. Malouf has been made possible by
a fellowship of the Royal Netherlands Academy of Arts
and Sciences and by the NWO PIONIER project Algo-
rithms for Linguistic Processing. Thanks also to Stephen
Clark, Andreas Eisele, Detlef Prescher, Miles Osborne,
and Gertjan van Noord for helpful comments and test
data.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23:597?
618.
Satish Balay, William D. Gropp, Lois Curfman
McInnes, and Barry F. Smith. 1997. Efficienct
management of parallelism in object oriented nu-
merical software libraries. In E. Arge, A. M. Bru-
aset, and H. P. Langtangen, editors, Modern Soft-
ware Tools in Scientific Computing, pages 163?
202. Birkhauser Press.
Satish Balay, Kris Buschelman, William D. Gropp,
Dinesh Kaushik, Lois Curfman McInnes, and
Barry F. Smith. 2001. PETSc home page.
http://www.mcs.anl.gov/petsc.
Satish Balay, William D. Gropp, Lois Curfman
McInnes, and Barry F. Smith. 2002. PETSc users
manual. Technical Report ANL-95/11?Revision
2.1.2, Argonne National Laboratory.
Steven J. Benson and Jorge J. More?. 2001. A lim-
ited memory variable metric method for bound
constrained minimization. Preprint ANL/ACS-
P909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J.
More?, and Jason Sarich. 2002. TAO users
manual. Technical Report ANL/MCS-TM-242?
Revision 1.4, Argonne National Laboratory.
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Compu-
tational Linguistics, 22.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Alpino: wide coverage computational
analysis of Dutch. In W. Daelemans, K. Sima?an,
J. Veenstra, and J. Zavrel, editors, Computational
Linguistics in the Netherlands 2000, pages 45?
59. Rodolpi, Amsterdam.
Zhiyi Chi. 1998. Probability models for complex
systems. Ph.D. thesis, Brown University.
J. Darroch and D. Ratcliff. 1972. Generalized it-
erative scaling for log-linear models. Ann. Math.
Statistics, 43:1470?1480.
Stephen Della Pietra, Vincent Della Pietra, and
John Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 19:380?393.
W.E. Deming and F.F. Stephan. 1940. On a least
squares adjustment of a sampled frequency table
when the expected marginals are known. Annals
of Mathematical Statistics, 11:427?444.
Elizabeth D. Dolan and Jorge J. More?. 2002.
Benchmarking optimization software with per-
formance profiles. Mathematical Programming,
91:201?213.
Mark Johnson, Stuart Geman, Stephen Canon,
Zhiyi Chi, and Stefan Riezler. 1999. Estimators
for stochastic ?unification-based? grammars. In
Proceedings of the 37th Annual Meeting of the
ACL, pages 535?541, College Park, Maryland.
John Lafferty and Bernhard Suhm. 1996. Cluster
expansions and iterative scaling for maximum en-
tropy language models. In K. Hanson and R. Sil-
ver, editors, Maximum Entropy and Bayesian
Methods. Kluwer.
John Lafferty, Fernando Pereira, and Andrew Mc-
Callum. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML).
Thomas P. Minka. 2001. Algorithms for
maximum-likelihood logistic regression. Statis-
tics Tech Report 758, CMU.
Jorge Nocedal and Stephen J. Wright. 1999. Nu-
merical Optimization. Springer, New York.
Jorge Nocedal. 1997. Large scale unconstrained
optimization. In A. Watson and I. Duff, editors,
The State of the Art in Numerical Analysis, pages
311?338. Oxford University Press.
Miles Osborne. 2002. Shallow parsing using noisy
and non-stationary training material. Journal of
Machine Learning Research, 2:695?719.
Miles Osborne. to appear. Using maximum entropy
for sentence extraction. In Proceedings of the
ACL 2002 Workshop on Automatic Summariza-
tion, Philadelphia.
Adwait Ratnaparkhi. 1998. Maximum entropy
models for natural language ambiguity resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient
training methods for maximum entropy language
modelling. In Proceedings of ICSLP2000, vol-
ume 3, pages 114?117, Beijing.
Markov models for language-independent named entity recognition
Robert Malouf
Alfa-Informatica
Rijksuniversiteit Groningen
Postbus 716
9700AS Groningen
The Netherlands
 	

1 Introduction
This report describes the application of Markov
models to the problem of language-independent
named entity recognition for the CoNLL-2002
shared task (Tjong Kim Sang, 2002).
We approach the problem of identifying named
entities as a kind of probabilistic tagging: given a
sequence of words w1  wn, we want to find the
corresponding sequence of tags t1  tn, drawn from
a vocabulary of possible tags T , which satisfies:
S Maximal Consistent Subsets
Robert Malouf?
San Diego State University
Default unification operations combine strict information with information from one or more
defeasible feature structures. Many such operations require finding the maximal subsets of a set
of atomic constraints that are consistent with each other and with the strict feature structure,
where a subset is maximally consistent with respect to the subsumption ordering if no constraint
can be added to it without creating an inconsistency. Although this problem is NP-complete,
there are a number of heuristic optimizations that can be used to substantially reduce the size of
the search space. In this article, we propose a novel optimization, leaf pruning, which in some
cases yields an improvement in running time of several orders of magnitude over previously
described algorithms. This makes default unification efficient enough to be practical for a wide
range of problems and applications.
1. Introduction
In unification-based grammatical frameworks, it often desirable to combine information
from possibly inconsistent sources. Over the years, a number of default unification
operations have been proposed1, which combine a strict feature structure with one or
more defeasible feature structures. These operations preserve all information in the
strict feature structure, while bringing in as much information as possible from the
defeasible structures. Default unification has been used to address a wide variety of
linguistic knowledge representation problems, including lexical inheritance hierarchies
(Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake
1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora
resolution (Grover et al 1994; Pru?st, Scha, and van den Berg 1994), and discourse
processing (Gurevych et al 2003; Alexandersson, Becker, and Pfleger 2004), among
many others.
Although the various default unification operators differ in their particulars, most
involve something like Carpenter (1992)?s credulous default unification as one step.
The result of credulously adding the default information in G to the strict information
in F is:
cred-unify(F, G) ? {unify(F, G?), where G? subsumes G and
G? is maximal such that unify(F, G?) is defined}
? Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500
Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu.
1 For example, Bouma (1992), Carpenter (1992), Pru?st (1992), Calder (1993), Lascarides and Copestake
(1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002). See Bouma (2006) for a
recent overview.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 2
In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent
subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent
with respect to the subsumption ordering if no constraint can be added to it without cre-
ating an inconsistency. In general, there may be more than one subset of the constraints
in G that is consistent with F and maximal, so the result of credulous default unification
is a set of feature structures.
One example of the use of credulous default unification for the resolution of dis-
course anaphora comes from Grover et al (1994). Consider the mini-discourse: Jessy likes
her brother. So does Hannah. To resolve the anaphoric predicate in the second sentence, we
can set up meaning representations for the source and the target:
Source:
?
?
?
?
REL like
AGENT 1 jessy
PATIENT
[
REL brother
THEME 1
]
?
?
?
?
Target:
[
AGENT hannah
]
and credulously unify the source with the strict information in the target.
To proceed, we first decompose the source feature structure into the following five
atomic ground constraints:
{[
REL like
]
,
[
AGENT jessy
]
,
[
PATIENT|REL brother
]
,
[
PATIENT|THEME jessy
]
,
[
AGENT 1
PATIENT|THEME 1
]}
Then, we find the maximal subsets of the remaining constraints which are mutually
consistent with the target. This yields two solutions:
?
?
?
?
REL like
AGENT 1 hannah
PATIENT
[
REL brother
THEME 1
]
?
?
?
?
?
?
?
?
REL like
AGENT hannah
PATIENT
[
REL brother
THEME jessy
]
?
?
?
?
corresponding to the sloppy identity and the strict identity readings of the anaphoric
expression.
2. Algorithm
A key step for applying most default unification operators is finding the maximal con-
sistent subsets of a set of constraints C. Unfortunately, finding these maximal consistent
subsets is an expensive operation, and, in fact, is NP-complete. Let T = {T1, . . . , Tm} be
the set of conflicts in C, where a conflict is a minimal set of constraints that are mutually
inconsistent with the target. For this example, T consists of the two conflicts:
{{
[
PATIENT|THEME jessy
]
,
[
AGENT 1
PATIENT|THEME 1
]}
,
{[
AGENT jessy
]}
}
Removing any one of the constraints from a conflict Ti would break that conflict, so if we
could remove from C at least one member of each Ti, the remaining constraints would be
154
Malouf Maximal Consistent Subsets
mutually consistent with the target information. Finding the maximal consistent subsets
of C then is equivalent to finding the minimal subset C? ? C such that each Ti contains
at least one member of C?. This is the hitting subset problem, a classic NP-complete
problem (Karp 1972).
An algorithm to find the maximal consistent subsets of C must check each subset of
C for consistency. One way to proceed is to construct a spanning tree of the boolean
lattice of subsets of C. This takes the form of a binomial tree, as in Figure 1 (Bird
and Hinze 2003). At each node, we keep track of k, the index of the constraint that
was removed from the parent set to create that subset. For example, subset {c1, c3}
was formed by removing c2 from {c1, c2, c3}, so k = 2. The descendants of a node are
constructed by successively dropping each of the constraints ci where k < i ? |C|. This
ensures that we will visit every subset of C exactly once.
The algorithm described by Grover et al (1994) performs a breadth-first search
of the subset lattice, with one important optimization. Because the cardinality of the
subsets at each level is one greater than those on the level below it, a breadth-first
search of this tree will consider all larger sets before considering any smaller ones.
Furthermore, because each subset is produced by removing constraints from its parent
set, every node in a subtree is a subset of its root. This means that once a consistent set
is found, no descendants of that set can be maximal, and that subtree can be pruned
from the search space. However, consistent subsets that are maximal in their branch of
the tree may turn out not to be globally maximal. For example, in Figure 1, if {c1, c2}
is consistent and {c1, c3} is not, a breadth-first search would identify both {c1, c2} and
{c1} as consistent and (locally) maximal. A final post-check for set inclusion can remove
pseudo-maximal results like {c1}.
In addition to pruning branches rooted by a consistent subset (call this root prun-
ing), the organization of the search space into a binomial tree allows another valuable
optimization. The deepest leaf node in any subtree is the set formed from the root by
removing all constraints ck<i?|C|, and every set in the subset is a superset of that deepest
leaf. Because no superset of an inconsistent set of constraints can be consistent, if the
foot of a subtree is inconsistent then clearly no node in the tree can be consistent, and
the entire tree can be skipped (call this leaf pruning). Taking both root pruning and
leaf pruning together, the only subtrees that need to be explored are those whose root is
Figure 1
Boolean lattice and binomial spanning tree for |C| = 3.
155
Computational Linguistics Volume 33, Number 2
inconsistent but whose deepest leaf is consistent. No other subtrees can possibly contain
a solution.
Figure 2 gives a breadth-first search algorithm that implements these optimiza-
tions. Like Grover et al (1994), this algorithm requires a post-check to remove pseudo-
maximal subsets from results. A queue is used to keep track of subsets S that are yet to be
checked for consistency, along with the index k of the constraint that was last dropped,
and a flag leftmost that indicates whether that subset is the leftmost child. Because the
deepest leaf node of the leftmost child is the same as the deepest leaf node of the parent,
we are guaranteed that the deepest leaf of a leftmost child is consistent. Keeping track
of leftmost children allows us to avoid a substantial number of redundant consistency
checks.
3. Evaluation
The graphs in Figure 3 and Figure 4 show an empirical comparison between a breadth-
first search with root pruning (BFS-R) and a breadth-first search with root and leaf
pruning (BFS-RL) on randomly generated problems. The graphs show the number of
subsets that were checked for consistency, as it relates to |C|, the number of constraints,
and p, the probability that two members of C are consistent. Larger values for p generally
lead to fewer but larger maximal consistent subsets. All counts are averaged across 100
randomly generated sets of ground constraints. In generating random problems, we
make the simplifying assumptions that all constraints are consistent with any inde-
feasible information, and that a subset of constraints that are pairwise consistent is a
consistent subset.
The first thing to note in these graphs is that root pruning by itself provides very
little benefit. For most values of p, the number of subsets checked by BFS-R is very
close to the worst case maximum 2|C|. A possible reason for this is that root pruning
Figure 2
Find the maximal consistent subsets of C. Performs a breadth-first search of the subset tree, with
root and leaf pruning.
156
Malouf Maximal Consistent Subsets
Figure 3
Comparison of breadth-first search using root pruning alone (BFS-R) and in combination with
leaf pruning (BFS-RL). |C| is the number of ground constraints, p is the fraction of the ground
constraints which are pairwise consistent, and ?Subsets visited? is the number of subsets of C
which were checked for consistency (on a logarithmic scale). All counts are based on the average
of 100 randomly generated problems.
will have the greatest effect when consistent subsets are found in the interior nodes of
the binomial search tree. However, the configuration of the search space is such that
most nodes are either leaves or very close to leaves, and only a few nodes have a large
number of descendants. Therefore, root pruning mostly removes very small subtrees
and has only a small effect on the overall cost of the algorithm.
The next observation to make is that for small values of |C| (in these experiments,
less than 7), BFS-RL is very slightly more expensive than BFS-R. In these cases, the
advantages of leaf pruning do not outweigh the cost of the extra consistency checks
required to implement it. As |C| increases, though, leaf pruning can offer substantial
improvements. For |C| = 19 and p = 0.1, leaf pruning eliminates more than 99.5% of
the search space, leading to a 185-fold improvement in running time. As p increases,
the benefits of leaf pruning do become more modest. Larger values of p mean fewer
inconsistent leaf nodes, so fewer subtrees are able to be eliminated. Even so, the savings
157
Computational Linguistics Volume 33, Number 2
Figure 4
Comparison of subsets visited by BFS-R and BFS-RL as a function of p for |C| = 15. All counts
are based on the average of 100 randomly generated problems.
from leaf pruning can still be dramatic: at |C| = 19 and p = 0.9, leaf pruning yields a
nearly five-fold improvement in speed.
Values of |C| and p that can be realistically expected will vary widely from ap-
plication to application. An anonymous reviewer reports that in one application, the
resolution of non-monotonic lexical inheritance for constraint-based grammars, p is
generally greater than 0.7. This may be due in part to the fact that most constraint-
based grammar development platforms do not support defaults (Copestake?s [2002]
LKB is a notable exception), and so grammar engineers tend to avoid the use of default
overriding. Ginzburg and Sag (2001) propose a more comprehensive use of defaults, and
grammars written following these principles would likely have a much lower value of
p. To my knowledge, however, these ideas have not yet made their way into any large-
scale grammar implementations.
Ninomiya, Miyao, and Tsujii (2002) describe experiments using default unification
for robust parsing and automatic grammar augmentation via a kind of explanation-
based learning. For this application, all features of a rule in the base XHPSG grammar
(Tateisi et al 1998) are considered defaults that can be overridden if necessary to get a
successful parse of a sentence. In this case, |C| is likely very large and grows quickly
with the length of the sentences being parsed. The value of p will depend on the
coverage of the base grammar, but can be expected to be fairly close to 1.0 for most
domains. In situations such as this, where p is expected to fall close to the worst case
for leaf pruning, one could consider inverting the search direction of the algorithm in
Figure 2. Rather than beginning with C and removing constraints until a consistent
subset is found, we could instead begin with the empty subset and add constraints
until an inconsistency is found. In either case, the frontier in the search space between
consistent and inconsistent subsets is where maximally consistent subsets will be found,
158
Malouf Maximal Consistent Subsets
and leaf pruning can be used to eliminate regions of the search space that contain only
consistent or inconsistent subsets.
4. Conclusions
Finding the maximal consistent subsets of a set of ground constraints is an impor-
tant sub-problem for many natural language processing and knowledge representa-
tion tasks. Unfortunately, the problem is NP-complete, and in the worst case requires
checking all 2|C| subsets of C for consistency. Previously proposed algorithms have
produced approximate solutions (Boppana and Halldo?rsson 1992), or have weakened
the requirements to make finding a solution easier (Ninomiya, Miyao, and Tsujii 2002).
By using deepest leaf pruning, the algorithm described in the previous sections
improves on the method of Grover et al (1994) and is able to achieve substantial gains
over the worst case running time for a large class of problems. An efficient method for
finding maximal consistent subsets can make default unification practical for problems
such as large-scale lexical representation, on-line discourse processing, or ontology
construction.
References
Alexandersson, Jan and Tilman Becker. 2001.
Overlay as the basic operation for
discourse processing in a multimodal
dialogue system. In Proceedings of the IJCAI
Workshop ?Knowledge and Reasoning in
Practical Dialogue Systems,? pages 8?14,
Seattle, WA.
Alexandersson, Jan, Tilman Becker, and
Norbert Pfleger. 2004. Scoring for overlay
based on informational distance. In
Proceedings of KONVENS 2004, pages 1?4,
Vienna, Austria.
Bird, Richard and Ralf Hinze. 2003.
Functional pearl: Trouble shared is trouble
halved. In Proceedings of the 2003 ACM
SIGPLAN Workshop on Haskell, pages 1?6,
Uppsala, Sweden.
Boppana, Ravi and Magnu?s M. Halldo?rsson.
1992. Approximating maximum
independent sets by excluding subgraphs.
BIT Numerical Mathematics, 32(2):180?196.
Bouma, Gosse. 1992. Feature structures and
nonmonotonicity. Computational
Linguistics, 18(2):183?204.
Bouma, Gosse. 2006. Unification: Classical
and default. In Keith Brown, editor,
Encyclopedia of Language and Linguistics.
Elsevier, New York.
Briscoe, E. J. 1999. The acquisition of
grammar in an evolving population of
language agents. Electronic Transactions in
Artificial Intelligence. Special Issue: Machine
Intelligence, 3(035):47?77.
Calder, Jonathan. 1993. Feature-value logics:
Some limits on the role of defaults. In C. J.
Rupp, Mike Rosner, and Rod Johnson,
editors, Constraints, Language and
Computation. Academic Press, London,
pages 20?32.
Carpenter, Bob. 1992. Skeptical and
creduluous default unification with
applications to templates and inheritance.
In Ted Briscoe, Anne Copestake, and
Valerie de Paiva, editors, Default
Inheritance within Unification-Based
Approaches to the Lexicon. Cambridge
University Press, Cambridge, UK,
pages 13?37.
Copestake, Ann. 1993. Defaults in lexical
representation. In E. J. Briscoe,
A. Copestake, and V. de Paiva, editors,
Inheritance, Defaults and the Lexicon.
Cambridge University Press, Cambridge,
UK, pages 223?245.
Copestake, Ann. 2002. Implementing Typed
Feature Structure Grammars. CSLI
Publications, Stanford, CA.
Ginzburg, Jonathan and Ivan A. Sag. 2001.
Interrogative Investigations. CSLI
Publications, Stanford, CA.
Grover, Claire, Chris Brew, Marc Moens,
and Suresh Manandhar. 1994. Priority
union and generalisation in discourse
grammar. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics, pages 17?24, Las Cruces,
New Mexico.
Gurevych, Iryna, Robert Porzel, Elena Slinko,
Norbert Pfleger, Jan Alexandersson,
and Stefan Merten. 2003. Less is more:
Using a single knowledge representation
159
Computational Linguistics Volume 33, Number 2
in dialog systems. In Proceedings of the
HLT-NAACL 2003 Workshop on Text
Meaning, pages 14?21, Edmonton, Alberta.
Karp, Richard. 1972. Reducibility among
combinatorial problems. In R. E. Miller
and J. W. Thatcher, editors, Complexity of
Computer Computations. Plenum Press,
New York, pages 85?103.
Lascarides, Alex and Ann Copestake. 1998.
Pragmatics and word meaning. Journal of
Linguistics, 34:387?414.
Lascarides, Alex and Ann Copestake. 1999.
Default representation in constraint-based
frameworks. Computational Linguistics,
25:55?105.
Ninomiya, Takashi, Yusuke Miyao, and
Jun?ichi Tsujii. 2002. Lenient default
unification for robust processing within
unification based grammar formalisms.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING), pages 1?7, Taipei, Taiwan.
Petersen, Wiebke. 2004. A set-theoretic
approach for the induction of inheritance
hierarchies. Electronic Notes in Theoretical
Computer Science, 53:296?308.
Pru?st, Hub. 1992. On Discourse Structuring,
VP Anaphora and Gapping. Ph.D. thesis,
University of Amsterdam.
Pru?st, Hub, Remko Scha, and Martin
van den Berg. 1994. Discourse grammar
and verb phrase anaphora. Linguistics and
Philosophy, 17(3):261?327.
Tateisi, Yuka, Kentaro Torisawa, Yusuke
Miyao, and Jun?ichi Tsujii. 1998.
Translating the XTAG English grammar
to HPSG. In Proceedings of the Fourth
International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+4),
pages 172?175, Philadelphia, PA.
Villavicencio, Aline. 2002. The Acquisition
of a Unification-Based Generalised Categorial
Grammar. Ph.D. thesis, Cambridge
University.
160
