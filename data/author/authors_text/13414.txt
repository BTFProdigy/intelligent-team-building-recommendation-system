Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 244?254,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Interactive Machine Translation using Hierarchical Translation Models
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, Jose?-Miguel Bened??, Francisco Casacuberta
D. de Sistemas Informa?ticos y Computacio?n
Universitat Polite`cnica de Vale`ncia
Camino de Vera s/n, 46021 Valencia (Spain)
{jegonzalez,dortiz,jbenedi,fcn}@dsic.upv.es
Abstract
Current automatic machine translation sys-
tems are not able to generate error-free trans-
lations and human intervention is often re-
quired to correct their output. Alternatively,
an interactive framework that integrates the
human knowledge into the translation pro-
cess has been presented in previous works.
Here, we describe a new interactive ma-
chine translation approach that is able to work
with phrase-based and hierarchical translation
models, and integrates error-correction all in
a unified statistical framework. In our experi-
ments, our approach outperforms previous in-
teractive translation systems, and achieves es-
timated effort reductions of as much as 48%
relative over a traditional post-edition system.
1 Introduction
Research in the field of machine translation (MT)
aims to develop computer systems which are able
to translate between languages automatically, with-
out human intervention. However, the quality of
the translations produced by any automatic MT sys-
tem still remain below than that of human transla-
tion. Typical solutions to reach human-level quality
require a subsequent manual post-editing process.
Such decoupled post-edition solution is rather inef-
ficient and tedious for the human translator. More-
over, it prevents the MT system from taking advan-
tage of the knowledge of the human translator and,
reciprocal, the human translator cannot take advan-
tage of the adapting ability of MT technology.
An alternative way to take advantage of the exist-
ing MT technology is to use them in collaboration
with human translators within a computer-assisted
translation (CAT) or interactive framework (Isabelle
and Church, 1998). The TransType and TransType2
projects (Foster et al, 1998; Langlais and Lapalme,
2002; Barrachina et al, 2009) entailed an interesting
focus shift in CAT technology by aiming interaction
directly at the production of the target text. These
research projects proposed to embed an MT system
within an interactive translation environment. This
way, the human translator can ensure a high-quality
output while the MT system ensures a significant
gain of productivity. Particularly interesting is the
interactive machine translation (IMT) approach pro-
posed in (Barrachina et al, 2009). In this scenario,
a statistical MT (SMT) system uses the source sen-
tence and a previously validated part (prefix1) of its
translation to propose a suitable continuation. Then
the user finds and corrects the next system error,
thereby providing a longer prefix which the system
uses to suggests a new, hopefully better continua-
tion. The reported results showed that IMT can save
a significant amount of human effort.
Barrachina et al. (2009) provide a thorough de-
scription of the IMT approach and describe algo-
rithms for its practical implementation. Neverthe-
less, we identify two basic problems for which we
think there is room for improvement. The first prob-
lem arises when the system cannot generate the pre-
fix validated by the user. To solve this problem,
the authors simply provide an ad-hoc heuristic error-
correction technique. The second problem is how
the system deals with word reordering. Particularly,
the models used by the system were either mono-
1We use the terms prefix and suffix to denote any sub-string
at the beginning and end respectively of a string of characters
(including spaces and punctuation). These terms do not imply
any morphological significance as they usually do in linguistics.
244
tonic by nature or non-monotonic but heuristically
defined (not estimated from training data).
We work on the foundations of Barrachina et
al., (2009) and provide formal solutions to these
two challenges. On the one hand, we adopt the
statistical formalization of the IMT framework de-
scribed in (Ortiz-Mart??nez, 2011), which includes
a stochastic error-correction model in its formaliza-
tion to address prefix coverage problems. Moreover,
we refine this formalization proposing an alternative
error-correction formalization for the IMT frame-
work (Section 2). Additionally, we also propose a
specific error-correction model based on a statisti-
cal interpretation of the Levenshtein distance (Lev-
enshtein, 1966). These formalizations provide a
unified statistical framework for the IMT model in
comparison to the ad-hoc heuristic error-correction
methods previously used.
In order to address the problem of properly deal
with reordering in IMT, we introduce the use of hi-
erarchical MT models (Chiang, 2005; Zollmann and
Venugopal, 2006). These methods provide a natural
approach to handle long range dependencies and al-
low the incorporation of reordering information into
a consistent statistical framework. Here, we also de-
scribe how state-of-the-art hierarchical MT models
can be extended to handle IMT (Sections 3 and 4).
We evaluate the proposed IMT approach on two
different translation task. The comparative results
against the IMT approach described by Barrachina
et al, (2009) and a conventional post-edition ap-
proach show that our IMT formalization for hier-
archical SMT models indeed outperform other ap-
proaches (Sections 5 and 6). Moreover, it leads to
large reductions in the human effort required to gen-
erate error-free translations.
2 Statistical Framework
2.1 Statistical Machine Translation
Assuming that we are given a sentence s in a source
language, the translation problem can be stated as
finding its translation t in a target language of max-
imum probability (Brown et al, 1993):
t? = argmax
t
Pr(t | s) (1)
= argmax
t
Pr(t) ? Pr(s | t) (2)
source (s): Para ver la lista de recursos
desired translation (t?): To view a listing of resources
IT-0
p
ts To view the resources list
IT-1
p To view
k a
ts list of resources
IT-2
p To view a list
k list i
ts list i ng resources
IT-3
p To view a listing
k o
ts o f resources
END p To view a listing of resources
Figure 1: IMT session to translate a Spanish sentence into
English. The desired translation is the translation the hu-
man user wants to obtain. At IT-0, the system suggests
a translation (ts). At IT-1, the user moves the mouse to
accept the first eight characters ?To view ? and presses the
a key (k), then the system suggests completing the sen-
tence with ?list of resources? (a new ts). Iterations 2 and
3 are similar. In the final iteration, the user accepts the
current translation.
The terms in the latter equation are the lan-
guage model probability Pr(t) that represents the
well-formedness of t (n-gram models are usu-
ally adopted), and the (inverted) translation model
Pr(s | t) that represents the relationship between the
source sentence and its translation.
In practice all of these models (and possibly oth-
ers) are often combined into a log-linear model for
Pr(t | s) (Och and Ney, 2002):
t? = argmax
t
{
N?
n=1
?n ? log(fn(t, s))
}
(3)
where fn(t, s) can be any model that represents an
important feature for the translation, N is the num-
ber of models (or features), and ?n are the weights
of the log-linear combination.
2.2 Statistical Interactive Machine Translation
Unfortunately, current MT technology is still far
from perfect. This implies that, in order to achieve
good translations, manual post-editing is needed.
An alternative to this decoupled approach (first
MT, then manual correction) is given by the IMT
245
paradigm (Barrachina et al, 2009). Under this
paradigm, translation is considered as an iterative
left-to-right process where the human and the com-
puter collaborate to generate the final translation.
Figure 1 shows an example of the IMT approach.
There, a source Spanish sentence s =?Para ver la
lista de recursos? is to be translated into a target En-
glish sentence t?. Initially, with no user feedback, the
system suggests a complete translation ts =?To view
the resources list?. From this translation, the user
marks a prefix p =?To view? as correct and begins
to type the rest of the target sentence. Depending on
the system or the user?s preferences, the user might
type the full next word, or only some letters of it (in
our example, the user types the single next charac-
ter ?a?). Then, the MT system suggests a new suffix
ts =?list of resources? that completes the validated
prefix and the input the user has just typed (p =?To
view a?). The interaction continues with a new pre-
fix validation followed, if necessary, by new input
from the user, and so on, until the user considers the
translation to be complete and satisfactory.
The crucial step of the process is the production
of the suffix. Again decision theory tells us to max-
imize the probability of the suffix given the avail-
able information. Formally, the best suffix of a given
length will be:
t?s = argmax
ts
Pr(ts | s,p) (4)
which can be straightforwardly rewritten as:
t?s = argmax
ts
Pr(p, ts | s) (5)
= argmax
ts
Pr(p, ts) ? Pr(s | p, ts) (6)
Note that, since p ts = t, this equation is very
similar to Equation (2). The main difference is that
now the search process is restricted to those target
sentences t that contains p as prefix. This implies
that we can use the same MT models (including
the log-linear approach) if the search procedures are
adequately modified (Och et al, 2003). Finally, it
should be noted that the statistical models are usu-
ally defined at word level, while the IMT process
described in this section works at character level. To
deal with this problem, during the search process it
is necessary to verify the compatibility between t
and p at character level.
2.3 IMT with Stochastic Error-Correction
A common problem in IMT arises when the user sets
a prefix which cannot be explained by the statistical
models. To solve this problem, IMT systems typi-
cally include ad-hoc error-correction techniques to
guarantee that the suffixes can be generated (Bar-
rachina et al, 2009). As an alternative to this heuris-
tic approach, Ortiz-Mart??nez (2011) proposed a for-
malization of the IMT framework that does include
stochastic error-correction models in its statistical
formalization. The starting point of this alternative
IMT formalization accounts for the problem of find-
ing the translation t that, at the same time, better
explains the source sentence s and the prefix given
by the user p:
t? = argmax
t
Pr(t | s,p) (7)
= argmax
t
Pr(t) ? Pr(s,p | t) (8)
The following na??ve Bayes? assumption is now
made: the source sentence s and the user prefix p are
statistically independent variables given the transla-
tion t, obtaining:
t? = argmax
t
Pr(t) ? Pr(s | t) ? Pr(p | t) (9)
where Pr(t) can be approximated with a language
model, Pr(s | t) can be approximated with a trans-
lation model, and Pr(p | t) can be approximated
by an error correction model that measures the com-
patibility between the user-defined prefix p and the
hypothesized translation t.
Note that the translation result, t?, given by Equa-
tion (9) may not contain p as prefix because every
translation is compatible with p with a certain prob-
ability. Thus, despite being close, Equation (9) is not
equivalent to the IMT formalization in Equation (6).
To solve this problem, we define an alignment,
a, between the user-defined prefix p and the hy-
pothesized translation t, so that the unaligned words
of t, in an appropriate order, constitute the suffix
searched in IMT. This allows us to rewrite the error
correction probability as follows:
Pr(p | t) =
?
a
Pr(p,a | t) (10)
To simplify things, we assume that p is mono-
tonically aligned to t, leaving the potential word-
reordering to the language and translation models.
246
Under this assumption, a determines an alignment
for t, such that t = tpts, where tp is fully-aligned to
p and ts remains unaligned. Taking all these things
into consideration, and following a maximum ap-
proximation, we finally arrive to the expression:
(t?, a?) = argmax
t,a
Pr(t)?Pr(s | t)?Pr(p,a | t) (11)
where the suffix required in IMT is obtained as the
portion of t? that is not aligned with the user prefix.
In practice, we combine the models in Equa-
tion (11) in a log-linear fashion as it is typically done
in SMT (see Equation (3)).
2.4 Alternative Formalization for IMT with
Stochastic Error-Correction
Alternatively to Equation (11), we can operate from
Equation (9) and reach a different formalization for
IMT with error-correction. We can re-write the first
and last terms of Equation (9) as:
Pr(t) ? Pr(p | t) = Pr(p) ? Pr(t | p) (12)
As in the previous section, we introduce an align-
ment variable, a, between t and p, giving:
Pr(t | p) =
?
a
Pr(t,a | p) (13)
=
?
a
Pr(a | p) ? Pr(t | p,a) (14)
If we consider monotonic alignments, a defines
again an alignment between a prefix of the system
translation (tp) and the user prefix, producing the
suffix required in IMT (ts) as the unaligned part.
Thus, we can re-write Pr(t | p,a) as:
Pr(t | p,a) = Pr(tp, ts | p,a) (15)
? Pr(tp | p,a) ? Pr(ts | p,a) (16)
where Equation (16) has been obtained following a
na??ve Bayes? decomposition.
Combining equations (12), (14), and (16) into
Equation (9), and following a maximum approxima-
tion for the summation of the alignment variable a,
we arrive to the following expression:
(t?, a?) = argmax
t,a
Pr(s |t)?Pr(tp |p,a)?Pr(ts |p,a) (17)
where Pr(p) and Pr(a|p) have been dropped down
because the former does not participate in the maxi-
mization and the latter is assumed uniform.
The terms in this equation can be interpreted sim-
ilarly as those in Equation (9): Pr(s | t) is the trans-
lation model, Pr(tp | p,a) is the error-correction
probability that measures the compatibility between
the prefix tp of the hypothesized translation and the
user-defined prefix p, and Pr(ts | p,a) is the lan-
guage model for the corresponding suffix ts condi-
tioned by the user-defined prefix. Again, in the ex-
periments we combine the different models in a log-
linear fashion.
The main difference between the two alternative
IMT formalization (Equations (11) and (17)) is that
in the latter the suffix to be returned is conditioned
by the user-validated prefix p. Thus, in the fol-
lowing we will refer to Equation (11) as indepen-
dent suffix formalization while we will denote Equa-
tion (17) by conditioned suffix formalization.
3 Statistical Models
We now present the statistical models used to esti-
mate the probability distributions described in the
previous section. Section 3.1 describes the error-
correction model, while Section 3.2 describes the
models for the conditional translation probability.
3.1 Statistical Error-Correction Model
Following the vast majority of IMT systems de-
scribed in the literature, we implement an error-
correction model based on the concept of edit dis-
tance (Levenshtein, 1966). Typically, IMT systems
use non-probabilistic error correction models. The
first stochastic error correction model for IMT was
proposed in (Ortiz-Mart??nez, 2011) and it is based
on probabilistic finite state machines. Here, we pro-
pose a simpler approach which can be seen as a
particular case of the previous one. Specifically,
the proposed approach models the edit distance as a
Bernoulli process where each character of the candi-
date string has a probability pe of being erroneous.
Under this interpretation, the number of characters
that need to be edited E in a sentence of length n
is a random variable that follows a binomial distri-
bution, E ? B(n, pe), with parameters n and pe.
The probability of performing exactly k edits in a
247
sentence of n characters is given by the following
probability mass function:
f(k;n, pe) =
n!
k!(n? k)!
pke(1? pe)
n?k (18)
Note that this error-correction model penalizes
equally all edit operations. Alternatively, we can
model the distance with a multinomial distribution
and assign different probabilities to different types
of edit operations. Nevertheless, we adhere to the
binomial approximation due to its simplicity.
Finally, we compute the error-correction proba-
bility between two strings from the total number of
edits required to transform the candidate translation
into the reference translation. Specifically, we define
the error-correction distribution in Equation (11) as:
Pr(p,a | t) ?
|p|!
k!(|p| ? k)!
pke(1? pe)
|p|?k (19)
where k = Lev(p, ta) is the character-level Lev-
enshtein distance between the user defined prefix p
and the prefix ta of the hypothesized translation t
defined by alignment a. The error-correction prob-
ability Pr(tp | p,a) in Equation (17) is computed
analogously.
The probability of edition pe is the single free pa-
rameter of this formulation. We will use a separate
development corpus to find an adequate value for it.
3.2 Statistical Machine Translation Models
Next sections briefly describe the statistical transla-
tion models used to estimate the conditional proba-
bility distribution Pr(s | t). A detailed description
of each model can be found in the provided citations.
3.2.1 Phrase-Based Translation Models
Phrase-based translation models (Koehn et al,
2003) are an instance of the noisy-channel approach
in Equation (2). The translation of a source sentence
s is obtained through a generative process composed
of three steps: first, the s is divided into K segments
(phrases), next, each source phrase, s?, is translated
into a target phrase t?, and finally the target phrases
are reordered to compose the final translation.
The usual phrase-based implementation of the
translation probability takes a log-linear form:
Pr(s | t) ? ?1 ? |t|+ ?2 ?K+
K?
k=1
[
?3 ? log(P (s?k | t?k)) + ?4 ? d(j)
]
(20)
where P (s? | t?) is the translation probability between
source phrase s? and target phrase t?, and d(j) is a
function (distortion model) that returns the score of
translating the k-th source phrase given that it is sep-
arated j words from the (k?1)-th phrase. Weights ?1
and ?2 play a special role since they are used to con-
trol the number of words and the number of phrases
of the target sentence to be generated, respectively.
3.2.2 Hierarchical Translation Models
Phrase-based models have shown a very strong
performance when translating between languages
that have similar word orders. However, they are not
able to adequately capture the complex relationships
that exist between the word orders of languages of
different families such as English and Chinese. Hi-
erarchical translation models provide a solution to
this challenge by allowing gaps in the phrases (Chi-
ang, 2005):
yu X1 you X2? have X2 with X1
where subscripts denote placeholders for sub-
phrases. Since these rules generalize over possi-
ble phrases, they act as discontinuous phrase pairs
and may also act as phrase-reordering rules. Hence,
they are not only considerably more powerful than
conventional phrase pairs, but they also integrate re-
ordering information into a consistent framework.
These hierarchical phrase pairs are formalized as
rewrite rules of a synchronous context-free grammar
(CFG) (Aho and Ullman, 1969):
X ?< ?,?,?> (21)
where X is a non-terminal, ? and ? are both strings
of terminals (words) and non-terminals , and ? is
a one-to-one correspondence between non-terminal
occurrences in ? and ?. Given the example above,
? ??yu X1 you X2?, ? ??have X2 with X1?, and?
is indicated by the subscript numbers.
Additionally, two glue rules are also defined:
S ?<S1X2 , S1X2> S ?<X1 , X1>
248
These give the model the option to build only par-
tial translations using hierarchical phrases, and then
combine them serially as in a phrase-based model.
The typical implementation of the hierarchical
translation model also takes the form of a log-linear
model. Let s? and t? be the source and target strings
generated by a derivation ? of the grammar. Then,
the conditional translation probability is given by:
Pr(s? | t?) ? ?1 ? |t?|+ ?2 ? |?|+ ?3 ?#g(?)+
?
r??
[?4 ? w(r)] (22)
where |?| denotes the total number of rules used
in ?, #g(?) returns the number of applications of
the glue rules, r ? ? are the rules in ?, and w(r)
is the weight of rule r. Weights ?1 and ?2 have
a similar interpretation as for phrase-based models,
they respectively give some control over the total
number of words and rules that conform the trans-
lation. Additionally, ?3 controls the model?s prefer-
ence for hierarchical phrases over serial combination
of phrases. Note that no distortion model is included
in the previous equation. Here, reordering is defined
at rule level by the one-to-one non-terminal corre-
spondence. In other words, reordering is a property
inherent to each rule and it is the individual score of
each rule what defines, at each step of the derivation,
the importance of reordering.
It should be noted that the IMT formalizations
presented in Section 2 can be applied to other hier-
archical or syntax-based SMT models such as those
described in (Zollmann and Venugopal, 2006; Shen
et al, 2010).
4 Search
In offline MT, the generation of the best trans-
lation for a given source sentence is carried out
by incrementally generating the target sentence2.
This process fits nicely into a dynamic program-
ming (DP) (Bellman, 1957) framework, as hypothe-
ses which are indistinguishable by the models can
be recombined. Since the DP search space grows
exponentially with the size of the input, standard DP
search is prohibitive, and search algorithms usually
resort to a beam-search heuristic (Jelinek, 1997).
2Phrase-based systems follow a left-to-right generation or-
der while hierarchical systems rely on a CYK-like order.
6
1
5
I saw a man with a telescope
2 3
4
I saw a man with a telescope
I saw with a telescope a man
Figure 2: Example of a hypergraph encoding two differ-
ent translations (one solid and one dotted) for the Spanish
sentence ?Vi a un hombre con un telescopio?.
Due to the demanding temporal constraints inher-
ent to any interactive environment, performing a full
search each time the user validates a new prefix is
unfeasible. The usual approach is to rely on a certain
representation of the search space that includes the
most probable translations of the source sentence.
The computational cost of this approach is much
lower, as the whole search for the translation must
be carried out only once, and the generated represen-
tation can be reused for further completion requests.
Next, we introduce hypergraphs, the formalism
chosen to represent the search space of both phrase-
based and hierarchical systems (Section 4.1). Then,
we describe the algorithms implemented to search
for suffix completions in them (Section 4.2).
4.1 Hypergraphs
A hypergraph is a generalization of the concept
of graph where the edges (now called hyperedges)
may connect several nodes (hypernodes) at the same
time. Formally, a hypergraph is a weighted acyclic
graph represented by a pair < V, E >, where V is a
set of hypernodes and E is a set of hyperedges. Each
hyperedge e ? E connects a head hypernode and a
set of tail hypernodes. The number of tail nodes is
called the arity of the hyperedge and the arity of a
hypergraph is the maximum arity of its hyperedges.
We can use hypergraphs to represent the deriva-
tions for a given CFG. Each hypernode represents
a partial translation generated during the decoding
process. Each ingoing hyperedge represents the rule
with which the corresponding non-terminal was sub-
stituted. Moreover, hypergraphs can represent a
whole set of possible translations. An example is
249
shown in Figure 2. Two alternative translations are
constructed from the leave nodes (1, 2 and 3) up to
the root node (6) of the hypergraph. Additionally,
hypernodes and hyperedges may be shared among
different derivations if they represent the same in-
formation. Thus, we can achieve a compact repre-
sentation of the translation space that allows us to
derive efficient search algorithms.
Note that word-graphs (Ueffing et al, 2002),
which are used to represent the search space for
phrase-based models, are a special case of hyper-
graphs in which the maximum arity is one. Thus,
hypergraphs allow us to represent both phrase-based
and hierarchical systems in a unified framework.
4.2 Suffix Search on Hypergraphs
Now, we describe a unified search process to obtain
the suffix ts that completes a prefix p given by the
user according to the two IMT formulations (Equa-
tion (11) and Equation (17)) described in Section 2.
Given an hypergraph, certain hypernodes define a
possible solution to the maximization defined in the
two IMT formulations. Specifically, only those hy-
pernodes that generate a prefix of a potential trans-
lation are to be taken into account3. The prob-
ability of the solution defined by each hypernode
has two components, namely the probability of the
SMT model (given by the language and translation
models) and the probability of the error-correction
model. On the one hand, the SMT model probabil-
ity is given by the translation of maximum probabil-
ity through the hypernode. On the other hand, the
error-correction probability is computed between p
and the partial translation of maximum probability
actually covered by the hypernode. Among all the
solutions defined by the hypernodes, we finally se-
lect that of maximum probability.
Once the best-scoring hypernode is identified, the
rest of the translation not covered by it is returned as
the suffix completion required in IMT.
5 Experimental Framework
The models and search procedure introduced in the
previous sections were assessed through a series of
3For example, in Figure 2 the hypernodes that generate pre-
fixes are those labeled with numbers 1 (?I saw?), 4 (?I saw with
a telescope) and 6 (?I saw a man with a telescope? and ?I saw
with a telescope a man?).
EU (Es/En)
Train Development Test
Sentences 214K 400 800
Token 5.9M / 5.2M 12K / 10K 23K / 20K
Vocabulary 97K / 84K 3K / 3K 5K / 4K
TED (Zh/En)
Train Development Test
Sentences 107K 934 1664
Token 2M / 2M 22K / 20K 33K / 32K
Vocabulary 42K / 52K 4K / 3K 4K / 4K
Table 1: Main figures of the processed EU and TED cor-
pora. K and M stand for thousands and millions of ele-
ments respectively.
IMT experiments with different corpora. These cor-
pora, the experimental methodology, and the evalu-
ation measures are presented in this section.
5.1 EU and TED corpora
We tested the proposed methods in two different
translation tasks each one involving a different lan-
guage pair: Spanish-to-English (Es?En) for the EU
(Bulletin of the European Union) task, and Chinese-
to-English (Zh?En) for the TED (TED4 talks) task.
The EU corpora were extracted from the Bul-
letin of the European Union, which exists in all of-
ficial languages of the European Union and is pub-
licly available on the Internet. Particularly, the cho-
sen Es?En corpus was part of the evaluation of the
TransType2 project (Barrachina et al, 2009). The
TED talks is a collection of recordings of public
speeches covering a variety of topics, and for which
high quality transcriptions and translations into sev-
eral languages are available. The Zh?En corpus
used in the experiments was part of the MT track
in the 2011 evaluation campaign of the workshop on
spoken language translation (Federico et al, 2011).
Specifically, we used the dev2010 partition for de-
velopment and the test2010 partition for test.
We process the Spanish and English parts of the
EU corpus to separate words and punctuation marks
keeping sentences truecase. Regarding the TED cor-
pus, we tokenized and lowercased the English part
(Chinese has no case information), and split Chi-
nese sentences into words with the Stanford word
4www.ted.com
250
segmenter (Tseng et al, 2005). Table 1 shows the
main figures of the processed EU and TED corpora.
5.2 Model Estimation and User Simulation
We used the standard configuration of the Moses
toolkit (Koehn et al, 2007) to estimate one phrase-
based and one hierarchical model for each cor-
pus; log-linear weights were optimized by minimum
error-rate training (Och, 2003) with the development
partitions. Then, the optimized models were used to
generate the word-graphs and hypergraphs with the
translations of the development and test partitions.
A direct evaluation of the proposed IMT proce-
dures involving human users would have been slow
and expensive. Thus, following previous works in
the literature (Barrachina et al, 2009; Gonza?lez-
Rubio et al, 2010), we used the references in the
corpora to simulate the translations that a human
user would want to obtain. Each time the system
suggested a new translation, it was compared to
the reference and the longest common prefix (LCP)
was obtained. Then, the first non-matching charac-
ter was replaced by the corresponding character in
the reference and a new system suggestion was pro-
duced. This process is iterated until a full match with
the reference was obtained.
Finally, we used this user simulation to optimize
the value for the probability of edition pe in the
error-correction model (Section 3.1), and for the log-
linear weights in the proposed IMT formulations. In
this case, these values were chosen so that they min-
imize the estimated user effort required to interac-
tively translate the development partitions.
5.3 Evaluation Measures
Different measures have been adopted to evaluate
the proposed IMT approach. On the one hand, dif-
ferent IMT systems can be compared according to
the effort needed by a human user to generate the de-
sired translations. This effort is usually estimated as
the number of actions performed by the user while
interacting with the system. In the user simulation
described above these actions are: looking for the
next error and moving the mouse pointer to that po-
sition (LCP computation), and correcting errors with
some key strokes. Hence, we implement the follow-
ing IMT effort measure (Barrachina et al, 2009):
Key-stroke and mouse-action ratio (KSMR):
number of key strokes plus mouse movements per-
formed by the user, divided by the total number of
characters in the reference.
On the other hand, we also want to compare the
proposed IMT approach against a conventional CAT
approach without interactivity, such as a decoupled
post-edition system. For such systems, character-
level user effort is usually measured by the Charac-
ter Error Rate (CER). However, it is clear that CER
is at a disadvantage due to the auto-completion ap-
proach of IMT. To perform a fairer comparison be-
tween post-edition and IMT, we implement a post-
editing system with autocompletion. Here, when
the user enters a character to correct some incor-
rect word, the system automatically completes the
word with the most probable word in the task vo-
cabulary. To evaluate the effort of a user using such
a system, we implement the following measure pro-
posed in (Romero et al, 2010):
Post-editing key stroke ratio (PKSR): using
a post-edition system with word-autocompleting,
number of user key strokes divided by the total num-
ber of reference characters.
The counterpart of PKSR in an IMT scenario
is (Barrachina et al, 2009):
Key-stroke ratio (KSR): number of key strokes,
divided by the number of reference characters.
PKSR and KSR are fairly comparable and the rel-
ative difference between them gives us a good es-
timate of the reduction in human effort that can be
achieved by using IMT instead of a conventional
post-edition system.
We also evaluate the quality of the automatic
translations generated by the MT models with the
widespread BLEU score (Papineni et al, 2002).
Finally, we provide both confidence intervals for
the results and statistical significance of the ob-
served differences in performance. Confidence in-
tervals were computed by pair-wise re-sampling as
in (Zhang and Vogel, 2004) while statistical signifi-
cance was computed using the Tukey?s HSD (honest
significance difference) test (Hsu, 1996).
251
EU TED
WG HG WG HG
1-best BLEU [%] 45.0 45.1 11.0 11.2
1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0
Table 2: BLEU score of the word-graphs (WG) and hy-
pergraphs (HG) used to implement the IMT procedures.
IMT EU TED
Setup PB HT PB HT
ISF 27.4?.5? 26.5?.5? 53.0?.4? 52.3?.4?
CSF 26.6?.5? 25.1?.5F 52.2?.4? 50.8?.4F
Table 3: IMT results (KSMR [%]) for the EU and
TED tasks using the independent suffix formalization
(ISF) and the conditioned suffix formalization (CSF). PB
stands for phrase-based model and HT stands for hierar-
chical translation model. For each task, the best result
is displayed boldface, an asterisk ? denotes a statistically
significant better result (99% confidence) with respect to
ISF with PB, and a star F denotes a statistically signifi-
cant difference with respect to all the other systems.
6 Results
We start by reporting conventional MT quality re-
sults to test if the generated word-graphs and hyper-
graphs encode translations of similar quality. Ta-
ble 2 displays the quality (BLEU (Papineni et al,
2002)) of the automatic translations generated for
the test partitions. The lower 1-best BLEU results
obtained for TED show that this is a much more dif-
ficult task than EU. Additionally, the similar aver-
age BLEU results obtained for the 1000-best transla-
tions indicate that word-graphs and hypergraphs en-
code translations of similar quality. Thus, the IMT
systems that use these word-graphs and hypergraphs
can be compared in a fair way.
Then, we evaluated different setups of the pro-
posed IMT approach. Table 3 displays the IMT re-
sults obtained for the EU and TED tasks. We report
KSMR (as a percentage) for the independent suffix
formalization (ISF) and the conditioned suffix for-
malization (CSF) using both phase-based (PB) and
hierarchical (HT) translation models. The KSMR
result of ISF using a phrase-based model can be con-
sidered our baseline since this setup is comparable
to that used in (Barrachina et al, 2009). Results for
HT consistently outperformed the corresponding re-
sults for PB. Similarly, results for CSF were con-
EU TED
PE IMT PE IMT
PKSR [%] KSR [%] PKSR [%] KSR [%]
27.1 14.1 (48%) 40.8 29.7 (27.2%)
Table 4: Estimation of the effort required to translate
the test partition of the EU and TED tasks using post-
editing with word-completion (PE) and IMT under the
independent suffix formalization (IMT). We used hierar-
chical MT in both approaches. In parenthesis we display
the estimated effort reduction of IMT with respect to PE.
sistently better than those for ISF. More specifically,
no statistically significant difference were found be-
tween ISF with HT and CSF with PB but both sta-
tistically outperformed the baseline (ISF with PB).
Finally, CSF with HT statistically outperformed the
other three configurations reducing KSMR by ?2.2
points with respect to the baseline. We hypothe-
size that the better results of HT can be explained
by its more efficient representation of word reorder-
ing. Regarding the CSF, its better results are due to
the better suffixes that can be obtained by taking into
account the actual prefix validated by the user.
Finally, we compared the estimated human effort
required to translate the test partitions of the EU and
TED corpora with the best IMT configuration (inde-
pendent suffix formalization with hierarchical trans-
lation model) and a conventional post-editing (PE)
CAT system with word-completion. That is, when
the user corrects a character, the PE system auto-
matically proposes a different word that begins with
the given word prefix but, obviously, the rest of the
sentence is not changed. According to the results,
the estimated human effort to generate the error-free
translations was significantly reduced with respect
to using the conventional PE approach. IMT can
save about 48% of the overall eastimated effort for
the EU task and about 27% for the TED task.
7 Summary and Future Work
We have proposed a new IMT approach that uses hi-
erarchical SMT models as its underlying translation
technology. This approach is based on a statistical
formalization previously described in the literature
that includes stochastic error correction. Addition-
ally, we have proposed a refined formalization that
improves the quality of the IMT suffixes by taking
252
into account the prefix validated by the user. More-
over, since word-graphs constitute a particular case
of hypergraphs, we are able to manage both phrase-
based and hierarchical translation models in a uni-
fied IMT framework.
Simulated results on two different translation
tasks showed that hierarchical translation models
outperform phrase-based models in our IMT frame-
work. Additionally, the proposed alternative IMT
formalization also allows to improve the results of
the IMT formalization previously described in the
literature. Finally, the proposed IMT system with
hierarchical SMT models largely reduces the esti-
mated user effort required to generate correct trans-
lations in comparison to that of a conventional post-
edition system. We look forward to corroborating
these result in test with human translators.
There are many ways to build on the work de-
scribed here. In the near future, we plan to explore
the following research directions:
? Alternative IMT scenarios where the user is not
bounded to correct translation errors in a left-
to-right fashion. In such scenarios, the user will
be allowed to correct errors at any position in
the translation while the IMT system will be
required to derive translations compatible with
these isolated corrections.
? Adaptive translation engines that take advan-
tage of the user?s corrections to improve its sta-
tistical models. As the translator works and
corrects the proposed translations, the transla-
tion engine will be able to make better predic-
tions. One of the first works on this topic was
proposed in (Nepveu et al, 2004). More re-
cently, Ortiz-Mart??nez et al (2010) described a
set of techniques to obtain an incrementally up-
dateable IMT system, solving technical prob-
lems encountered in previous works.
? More sophisticated measures to estimate the
human effort. Specifically, measures that esti-
mate the cognitive load involve in reading, un-
derstanding and detecting an error in a trans-
lation (Foster et al, 2002), in contrast KSMR
simply considers a constant cost. This will lead
to a more accurate estimation of the improve-
ments that may be expected by a human user.
Acknowledgments
Work supported by the European Union 7th Frame-
work Program (FP7/2007-2013) under the Cas-
MaCat project (grans agreement no 287576), by
Spanish MICINN under grant TIN2012-31723, and
by the Generalitat Valenciana under grant ALMPR
(Prometeo/2009/014).
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syn-
tax directed translations and the pushdown assembler.
Journal of Computer and Systems Science, 3(1):37?
56, February.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi, An-
tonio Lagarda, Hermann Ney, Jesu?s Toma?s, Enrique
Vidal, and Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Computa-
tional Linguistics, 35:3?28, March.
Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ, USA, 1 edi-
tion.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
M. Federico, L. Bentivogli, M. Paul, and S. Stu?ker. 2011.
Overview of the iwslt 2011 evaluation campaign. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 11?20.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1998. Target-text mediated interactive machine trans-
lation. Machine Translation, 12(1/2):175?194, Jan-
uary.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proceedings of the 2002 conference on Empirical
methods in natural language processing - Volume 10,
pages 148?155.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2010. Balancing user effort and
translation error in interactive machine translation via
confidence measures. In Proceedings of the ACL 2010
Conference Short Papers, pages 173?177.
Jason Hsu. 1996. Multiple Comparisons: Theory and
Methods. Chapman and Hall/CRC.
253
Pierre Isabelle and Ken Church. 1998. Special issue on:
New tools for human translators, volume 12. Kluwer
Academic Publishers, January.
Frederick Jelinek. 1997. Statistical methods for speech
recognition. MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, demonstration session, June.
Philippe Langlais and Guy Lapalme. 2002. TransType:
development-evaluation cycles to boost translator?s
productivity. Machine Translation, 17(2):77?98,
September.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710, February.
Laurent Nepveu, Guy Lapalme, Philippe Langlais, and
George Foster. 2004. Adaptive language and trans-
lation models for interactive machine translation. In
Proceedings of the conference on Empirical Methods
on Natural Language Processing, pages 190?197.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295?302.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In Proceedings of the European
chapter of the Association for Computational Linguis-
tics, pages 387?393.
Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 160?167. Association for
Computational Linguistics.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In Proceedings
of the 2010 Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 546?554.
Daniel Ortiz-Mart??nez. 2011. Advances in Fully-
Automatic and Interactive Phrase-Based Statistical
Machine Translation. Ph.D. thesis, Universitat
Polite`cnica de Vale`ncia. Advisors: Ismael Garc??a
Varea and Francisco Casacuberta.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318. Associa-
tion for Computational Linguistics.
Veronica Romero, Alejandro H. Toselli, and Enrique Vi-
dal. 2010. Character-level interaction in computer-
assisted transcription of text images. In Proceedings
of the 12th International Conference on Frontiers in
Handwriting Recognition, pages 539?544.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649?671, Decem-
ber.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing.
Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine trans-
lation. In Proceedings of the conference on Empirical
Methods in Natural Language Processing, pages 156?
163.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of The international Confer-
ence on Theoretical and Methodological Issues in Ma-
chine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138?141.
254
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 245?254,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Active learning for interactive machine translation
Jesu?s Gonza?lez-Rubio and Daniel Ortiz-Mart??nez and Francisco Casacuberta
D. de Sistemas Informa?ticos y Computacio?n
U. Polite`cnica de Vale`ncia
C. de Vera s/n, 46022 Valencia, Spain
{jegonzalez,dortiz,fcn}@dsic.upv.es
Abstract
Translation needs have greatly increased
during the last years. In many situa-
tions, text to be translated constitutes an
unbounded stream of data that grows con-
tinually with time. An effective approach
to translate text documents is to follow
an interactive-predictive paradigm in which
both the system is guided by the user
and the user is assisted by the system to
generate error-free translations. Unfortu-
nately, when processing such unbounded
data streams even this approach requires an
overwhelming amount of manpower. Is in
this scenario where the use of active learn-
ing techniques is compelling. In this work,
we propose different active learning tech-
niques for interactive machine translation.
Results show that for a given translation
quality the use of active learning allows us
to greatly reduce the human effort required
to translate the sentences in the stream.
1 Introduction
Translation needs have greatly increased during
the last years due to phenomena such as global-
ization and technologic development. For exam-
ple, the European Parliament1 translates its pro-
ceedings to 22 languages in a regular basis or
Project Syndicate2 that translates editorials into
different languages. In these and many other ex-
amples, data can be viewed as an incoming un-
bounded stream since it grows continually with
time (Levenberg et al 2010). Manual translation
of such streams of data is extremely expensive
given the huge volume of translation required,
1http://www.europarl.europa.eu
2http://project-syndicate.org
therefore various automatic machine translation
methods have been proposed.
However, automatic statistical machine trans-
lation (SMT) systems are far from generating
error-free translations and their outputs usually
require human post-editing in order to achieve
high-quality translations. One way of taking ad-
vantage of SMT systems is to combine them
with the knowledge of a human translator in the
interactive-predictive machine translation (IMT)
framework (Foster et al 1998; Langlais and La-
palme, 2002; Barrachina et al 2009), which is
a particular case of the computer-assisted trans-
lation paradigm (Isabelle and Church, 1997). In
the IMT framework, a state-of-the-art SMT model
and a human translator collaborate to obtain high-
quality translations while minimizing required
human effort.
Unfortunately, the application of either post-
editing or IMT to data streams with massive data
volumes is still too expensive, simply because
manual supervision of all instances requires huge
amounts of manpower. For such massive data
streams the need of employing active learning
(AL) is compelling. AL techniques for IMT se-
lectively ask an oracle (e.g. a human transla-
tor) to supervise a small portion of the incoming
sentences. Sentences are selected so that SMT
models estimated from them translate new sen-
tences as accurately as possible. There are three
challenges when applying AL to unbounded data
streams (Zhu et al 2010). These challenges can
be instantiated to IMT as follows:
1. The pool of candidate sentences is dynam-
ically changing, whereas existing AL algo-
rithms are dealing with static datasets only.
245
2. Concepts such as optimum translation and
translation probability distribution are con-
tinually evolving whereas existing AL algo-
rithms only deal with constant concepts.
3. Data volume is unbounded which makes
impractical to batch-learn one single sys-
tem from all previously translated sentences.
Therefore, model training must be done in an
incremental fashion.
In this work, we present a proposal of AL for
IMT specifically designed to work with stream
data. In short, our proposal divides the data
stream into blocks where AL techniques for static
datasets are applied. Additionally, we implement
an incremental learning technique to efficiently
train the base SMT models as new data is avail-
able.
2 Related work
A body of work has recently been proposed to ap-
ply AL techniques to SMT (Haffari et al 2009;
Ambati et al 2010; Bloodgood and Callison-
Burch, 2010). The aim of these works is to
build one single optimal SMT model from manu-
ally translated data extracted from static datasets.
None of them fit in the setting of data streams.
Some of the above described challenges of AL
from unbounded streams have been previously ad-
dressed in the MT literature. In order to deal with
the evolutionary nature of the problem, Nepveu et
al. (2004) propose an IMT system with dynamic
adaptation via cache-based model extensions for
language and translation models. Pursuing the
same goal for SMT, Levenberg et al (2010)
study how to bound the space when processing
(potentially) unbounded streams of parallel data
and propose a method to incrementally retrain
SMT models. Another method to efficiently re-
train a SMT model with new data was presented
in (Ortiz-Mart??nez et al 2010). In this work,
the authors describe an application of the online
learning paradigm to the IMT framework.
To the best of our knowledge, the only previ-
ous work on AL for IMT is (Gonza?lez-Rubio et
al., 2011). There, the authors present a na??ve ap-
plication of the AL paradigm for IMT that do not
take into account the dynamic change in proba-
bility distribution of the stream. Nevertheless, re-
sults show that even that simple AL framework
halves the required human effort to obtain a cer-
tain translation quality.
In this work, the AL framework presented
in (Gonza?lez-Rubio et al 2011) is extended in
an effort to address all the above described chal-
lenges. In short, we propose an AL framework for
IMT that splits the data stream into blocks. This
approach allows us to have more context to model
the changing probability distribution of the stream
(challenge 2) and results in a more accurate sam-
pling of the changing pool of sentences (chal-
lenge 1). In contrast to the proposal described
in (Gonza?lez-Rubio et al 2011), we define sen-
tence sampling strategies whose underlying mod-
els can be updated with the newly available data.
This way, the sentences to be supervised by the
user are chosen taking into account previously su-
pervised sentences. To efficiently retrain the un-
derlying SMT models of the IMT system (chal-
lenge 3), we follow the online learning technique
described in (Ortiz-Mart??nez et al 2010). Finally,
we integrate all these elements to define an AL
framework for IMT with an objective of obtaining
an optimum balance between translation quality
and human user effort.
3 Interactive machine translation
IMT can be seen as an evolution of the SMT
framework. Given a sentence f from a source
language to be translated into a sentence e of
a target language, the fundamental equation of
SMT (Brown et al 1993) is defined as follows:
e? = argmax
e
Pr(e | f) (1)
where Pr(e | f) is usually approximated by a log
linear translation model (Koehn et al 2003). In
this case, the decision rule is given by the expres-
sion:
e? = argmax
e
{
M?
m=1
?mhm(e, f)
}
(2)
where each hm(e, f) is a feature function repre-
senting a statistical model and ?m its weight.
In the IMT framework, a human translator is in-
troduced in the translation process to collaborate
with an SMT model. For a given source sentence,
the SMT model fully automatically generates an
initial translation. The human user checks this
translation, from left to right, correcting the first
246
source (f ): Para ver la lista de recursos
desired translation (e?): To view a listing of resources
inter.-0
ep
es To view the resources list
inter.-1
ep To view
k a
es list of resources
inter.-2
ep To view a list
k list i
es list i ng resources
inter.-3
ep To view a listing
k o
es o f resources
accept ep To view a listing of resources
Figure 1: IMT session to translate a Spanish sentence
into English. The desired translation is the translation
the human user have in mind. At interaction-0, the sys-
tem suggests a translation (es). At interaction-1, the
user moves the mouse to accept the first eight charac-
ters ?To view ? and presses the a key (k), then the
system suggests completing the sentence with ?list of
resources? (a new es). Interactions 2 and 3 are simi-
lar. In the final interaction, the user accepts the current
translation.
error. Then, the SMT model proposes a new ex-
tension taking the correct prefix, ep, into account.
These steps are repeated until the user accepts the
translation. Figure 1 illustrates a typical IMT ses-
sion. In the resulting decision rule, we have to
find an extension es for a given prefix ep. To do
this we reformulate equation (1) as follows, where
the term Pr(ep | f) has been dropped since it does
not depend on es:
e?s = argmax
es
Pr(ep, es | f) (3)
? argmax
es
p(es | f , ep) (4)
The search is restricted to those sentences e
which contain ep as prefix. Since e ? ep es, we
can use the same log-linear SMT model, equa-
tion (2), whenever the search procedures are ad-
equately modified (Barrachina et al 2009).
4 Active learning for IMT
The aim of the IMT framework is to obtain high-
quality translations while minimizing the required
human effort. Despite the fact that IMT may
reduce the required effort with respect to post-
editing, it still requires the user to supervise all
the translations. To address this problem, we pro-
pose to use AL techniques to select only a small
number of sentences whose translations are worth
to be supervised by the human expert.
This approach implies a modification of the
user-machine interaction protocol. For a given
source sentence, the SMT model generates an ini-
tial translation. Then, if this initial translation is
classified as incorrect or ?worth of supervision?,
we perform a conventional IMT procedure as in
Figure 1. If not, we directly return the initial au-
tomatic translation and no effort is required from
the user. At the end of the process, we use the new
sentence pair (f , e) available to refine the SMT
models used by the IMT system.
In this scenario, the user only checks a small
number of sentences, thus, final translations are
not error-free as in conventional IMT. However,
results in previous works (Gonza?lez-Rubio et al
2011) show that this approach yields important
reduction in human effort. Moreover, depending
on the definition of the sampling strategy, we can
modify the ratio of sentences that are interactively
translated to adapt our system to the requirements
of a specific translation task. For example, if the
main priority is to minimize human effort, our
system can be configured to translate all the sen-
tences without user intervention.
Algorithm 1 describes the basic algorithm to
implement AL for IMT. The algorithm receives as
input an initial SMT model, M , a sampling strat-
egy, S, a stream of source sentences, F, and the
block size, B. First, a block of B sentences, X ,
is extracted from the data stream (line 3). From
this block, we sample those sentences, Y , that
are worth to be supervised by the human expert
(line 4). For each of the sentences in X , the cur-
rent SMT model generates an initial translation,
e?, (line 6). If the sentence has been sampled as
worthy of supervision, f ? Y , the user is required
to interactively translate it (lines 8?13) as exem-
plified in Figure 1. The source sentence f and its
human-supervised translation, e, are then used to
retrain the SMT model (line 14). Otherwise, we
directly output the automatic translation e? as our
final translation (line 17).
Most of the functions in the algorithm denote
different steps in the interaction between the hu-
man user and the machine:
? translate(M, f): returns the most proba-
ble automatic translation of f given by M .
? validPrefix(e): returns the prefix of e
247
input : M (initial SMT model)
S (sampling strategy)
F (stream of source sentences)
B (block size)
auxiliar : X (block of sentences)
Y (sentences worth of supervision)
begin1
repeat2
X = getSentsFromStream (B,F);3
Y = S(X,M);4
foreach f ? X do5
e? = translate(M, f);6
if f ? Y then7
e = e?;8
repeat9
ep = validPrefix(e);10
e?s = genSuffix(M, f , ep);11
e = ep e?s;12
until validTranslation(e) ;13
M = retrain(M, (f , e));14
output(e);15
else16
output(e?);17
until True ;18
end19
Algorithm 1: Pseudo-code of the proposed
algorithm to implement AL for IMT from
unbounded data streams.
validated by the user as correct. This prefix
includes the correction k.
? genSuffix(M, f , ep): returns the suffix of
maximum probability that extends prefix ep.
? validTranslation(e): returns True if
the user considers the current translation to
be correct and False otherwise.
Apart from these, the two elements that define
the performance of our algorithm are the sampling
strategy S(X,M) and the retrain(M, (f , e))
function. On the one hand, the sampling strat-
egy decides which sentences should be supervised
by the user, which defines the human effort re-
quired by the algorithm. Section 5 describes our
implementation of the sentence sampling to deal
with the dynamic nature of data streams. On the
other hand, the retrain(?) function incremen-
tally trains the SMT model with each new training
pair (f , e). Section 6 describes the implementa-
tion of this function.
5 Sentence sampling strategies
A good sentence sampling strategy must be able
to select those sentences that along with their cor-
rect translations improve most the performance of
the SMT model. To do that, the sampling strat-
egy have to correctly discriminate ?informative?
sentences from those that are not. We can make
different approximations to measure the informa-
tiveness of a given sentence. In the following
sections, we describe the three different sampling
strategies tested in our experimentation.
5.1 Random sampling
Arguably, the simplest sampling approach is ran-
dom sampling, where the sentences are randomly
selected to be interactively translated. Although
simple, it turns out that random sampling per-
form surprisingly well in practice. The success
of random sampling stem from the fact that in
data stream environments the translation proba-
bility distributions may vary significantly through
time. While general AL algorithms ask the user to
translate informative sentences, they may signifi-
cantly change probability distributions by favor-
ing certain translations, consequently, the previ-
ously human-translated sentences may no longer
reveal the genuine translation distribution in the
current point of the data stream (Zhu et al 2007).
This problem is less severe for static data where
the candidate pool is fixed and AL algorithms are
able to survey all instances. Random sampling
avoids this problem by randomly selecting sen-
tences for human supervision. As a result, it al-
ways selects those sentences with the most similar
distribution to the current sentence distribution in
the data stream.
5.2 n-gram coverage sampling
One technique to measure the informativeness
of a sentence is to directly measure the amount
of new information that it will add to the SMT
model. This sampling strategy considers that
sentences with rare n-grams are more informa-
tive. The intuition for this approach is that rare
n-grams need to be seen several times in order to
accurately estimate their probability.
To do that, we store the counts for each n-gram
present in the sentences used to train the SMT
model. We assume that an n-gram is accurately
represented when it appears A or more times in
248
the training samples. Therefore, the score for a
given sentence f is computed as:
C(f) =
?N
n=1 |N
<A
n (f)|
?N
n=1 |Nn(f)|
(5)
where Nn(f) is the set of n-grams of size n
in f , N<An (f) is the set of n-grams of size n in
f that are inaccurately represented in the training
data and N is the maximum n-gram order. In
the experimentation, we assume N = 4 as the
maximum n-gram order and a value of 10 for the
threshold A. This sampling strategy works by se-
lecting a given percentage of the highest scoring
sentences.
We update the counts of the n-grams seen by
the SMT model with each new sentence pair.
Hence, the sampling strategy is always up-to-date
with the last training data.
5.3 Dynamic confidence sampling
Another technique is to consider that the most in-
formative sentence is the one the current SMT
model translates worst. The intuition behind this
approach is that an SMT model can not generate
good translations unless it has enough informa-
tion to translate the sentence.
The usual approach to compute the quality of a
translation hypothesis is to compare it to a refer-
ence translation, but, in this case, it is not a valid
option since reference translations are not avail-
able. Hence, we use confidence estimation (Gan-
drabur and Foster, 2003; Blatz et al 2004; Ueff-
ing and Ney, 2007) to estimate the probability of
correctness of the translations. Specifically, we
estimate the quality of a translation from the con-
fidence scores of their individual words.
The confidence score of a word ei of the trans-
lation e = e1 . . . ei . . . eI generated from the
source sentence f = f1 . . . fj . . . fJ is computed
as described in (Ueffing and Ney, 2005):
Cw(ei, f) = max
0?j?| f |
p(ei|fj) (6)
where p(ei|fj) is an IBM model 1 (Brown et al
1993) bilingual lexicon probability and f0 is the
empty source word. The confidence score for the
full translation e is computed as the ratio of its
words classified as correct by the word confidence
measure. Therefore, we define the confidence-
based informativeness score as:
C(e, f) = 1?
|{ei | Cw(ei, f) > ?w}|
| e |
(7)
Finally, this sampling strategy works by select-
ing a given percentage of the highest scoring sen-
tences.
We dynamically update the confidence sampler
each time a new sentence pair is added to the SMT
model. The incremental version of the EM algo-
rithm (Neal and Hinton, 1999) is used to incre-
mentally train the IBM model 1.
6 Retraining of the SMT model
To retrain the SMT model, we implement the
online learning techniques proposed in (Ortiz-
Mart??nez et al 2010). In that work, a state-
of-the-art log-linear model (Och and Ney, 2002)
and a set of techniques to incrementally train this
model were defined. The log-linear model is com-
posed of a set of feature functions governing dif-
ferent aspects of the translation process, includ-
ing a language model, a source sentence?length
model, inverse and direct translation models, a
target phrase?length model, a source phrase?
length model and a distortion model.
The incremental learning algorithm allows us
to process each new training sample in constant
time (i.e. the computational complexity of train-
ing a new sample does not depend on the num-
ber of previously seen training samples). To do
that, a set of sufficient statistics is maintained for
each feature function. If the estimation of the
feature function does not require the use of the
well-known expectation?maximization (EM) al-
gorithm (Dempster et al 1977) (e.g. n-gram lan-
guage models), then it is generally easy to incre-
mentally extend the model given a new training
sample. By contrast, if the EM algorithm is re-
quired (e.g. word alignment models), the estima-
tion procedure has to be modified, since the con-
ventional EM algorithm is designed for its use in
batch learning scenarios. For such models, the in-
cremental version of the EM algorithm (Neal and
Hinton, 1999) is applied. A detailed description
of the update algorithm for each of the models in
the log-linear combination is presented in (Ortiz-
Mart??nez et al 2010).
7 Experiments
We carried out experiments to assess the perfor-
mance of the proposed AL implementation for
IMT. In each experiments, we started with an
initial SMT model that is incrementally updated
249
corpus use sentences
words
(Spa/Eng)
Europarl
train 731K 15M/15M
devel. 2K 60K/58K
News
test 51K 1.5M/1.2M
Commentary
Table 1: Size of the Spanish?English corpora used in
the experiments. K and M stand for thousands and
millions of elements respectively.
with the sentences selected by the current sam-
pling strategy. Due to the unavailability of public
benchmark data streams, we selected a relatively
large corpus and treated it as a data stream for AL.
To simulate the interaction with the user, we used
the reference translations in the data stream cor-
pus as the translation the human user would like
to obtain. Since each experiment is carried out
under the same conditions, if one sampling strat-
egy outperforms its peers, then we can safely con-
clude that this is because the sentences selected to
be translated are more informative.
7.1 Training corpus and data stream
The training data comes from the Europarl corpus
as distributed for the shared task in the NAACL
2006 workshop on statistical machine transla-
tion (Koehn and Monz, 2006). We used this data
to estimate the initial log-linear model used by our
IMT system (see Section 6). The weights of the
different feature functions were tuned by means
of minimum error?rate training (Och, 2003) exe-
cuted on the Europarl development corpus. Once
the SMT model was trained, we use the News
Commentary corpus (Callison-Burch et al 2007)
to simulate the data stream. The size of these cor-
pora is shown in Table 1. The reasons to choose
the News Commentary corpus to carry out our
experiments are threefold: first, its size is large
enough to simulate a data stream and test our
AL techniques in the long term; second, it is
out-of-domain data which allows us to simulate
a real-world situation that may occur in a trans-
lation company, and, finally, it consists in edito-
rials from eclectic domain: general politics, eco-
nomics and science, which effectively represents
the variations in the sentence distributions of the
simulated data stream.
7.2 Assessment criteria
We want to measure both the quality of the gener-
ated translations and the human effort required to
obtain them.
We measure translation quality with the well-
known BLEU (Papineni et al 2002) score.
To estimate human user effort, we simulate the
actions taken by a human user in its interaction
with the IMT system. The first translation hypoth-
esis for each given source sentence is compared
with a single reference translation and the longest
common character prefix (LCP) is obtained. The
first non-matching character is replaced by the
corresponding reference character and then a new
translation hypothesis is produced (see Figure 1).
This process is iterated until a full match with the
reference is obtained. Each computation of the
LCP would correspond to the user looking for the
next error and moving the pointer to the corre-
sponding position of the translation hypothesis.
Each character replacement, on the other hand,
would correspond to a keystroke of the user.
Bearing this in mind, we measure the user ef-
fort by means of the keystroke and mouse-action
ratio (KSMR) (Barrachina et al 2009). This mea-
sure has been extensively used to report results in
the IMT literature. KSMR is calculated as the
number of keystrokes plus the number of mouse
movements divided by the total number of refer-
ence characters. From a user point of view the
two types of actions are different and require dif-
ferent types of effort (Macklovitch, 2006). In any
case, as an approximation, KSMR assumes that
both actions require a similar effort.
7.3 Experimental results
In this section, we report results for three different
experiments. First, we studied the performance
of the sampling strategies when dealing with the
sampling bias problem. In the second experiment,
we carried out a typical AL experiment measur-
ing the performance of the sampling strategies as
a function of the percentage of the corpus used
to retrain the SMT model. Finally, we tested our
AL implementation for IMT in order to study the
tradeoff between required human effort and final
translation quality.
7.3.1 Dealing with the sampling bias
In this experiment, we want to study the perfor-
mance of the different sampling strategies when
250
 16
 17
 18
 19
 20
 21
 22
 0  10  20  30  40  50
BL
EU
Block number
DCS NS RS
Figure 2: Performance of the AL methods across dif-
ferent data blocks. Block size 500. Human supervision
10% of the corpus.
dealing with the sampling bias problem. Fig-
ure 2 shows the evolution of the translation qual-
ity, in terms of BLEU, across different data blocks
for the three sampling strategies described in sec-
tion 5, namely, dynamic confidence sampling
(DCS), n-gram coverage sampling (NS) and ran-
dom sampling (RS). On the one hand, the x-axis
represents the data blocks number in their tempo-
ral order. On the other hand, the y-axis represents
the BLEU score when automatically translating a
block. Such translation is obtained by the SMT
model trained with translations supervised by the
user up to that point of the data stream. To fairly
compare the different methods, we fixed the per-
centage of words supervised by the human user
(10%). In addition to this, we used a block size of
500 sentences. Similar results were obtained for
other block sizes.
Results in Figure 2 indicate that the perfor-
mances for the data blocks fluctuate and fluctu-
ations are quite significant. This phenomenon is
due to the eclectic domain of the sentences in the
data stream. Additionally, the steady increase in
performance is caused by the increasing amount
of data used to retrain the SMT model.
Regarding the results for the different sam-
pling strategies, DCS consistently outperformed
RS and NS. This observation asserts that for con-
cept drifting data streams with constant changing
translation distributions, DCS can adaptively ask
the user to translate sentences to build a superior
SMT model. On the other hand, NS obtains worse
results that RS. This result can be explained by the
 15
 16
 17
 18
 19
 20
 21
 22
 23
 0  5  10  15  20
BL
EU
Percentage (%) of the corpus in words
DCS NS SCS RS
 17
 18
 19
 20
 2  4  6  8
Figure 3: BLEU of the initial automatic translations
as a function of the percentage of the corpus used to
retrain the model.
fact that NS is independent of the target language
and just looks into the source language, while
DCS takes into account both the source sentence
and its automatic translation. Similar phenomena
has been reported in a previous work on AL for
SMT (Haffari et al 2009).
7.3.2 AL performance
We carried out experiments to study the perfor-
mance of the different sampling strategies. To this
end, we compare the quality of the initial auto-
matic translations generated in our AL implemen-
tation for IMT (line 6 in Algorithm 1). Figure 3
shows the BLEU score of these initial translations
represented as a function of the percentage of the
corpus used to retrain the SMT model. The per-
centage of the corpus is measured in number of
running words.
In Figure 3, we present results for the three
sampling strategies described in section 5. Ad-
ditionally, we also compare our techniques with
the AL technique for IMT proposed in (Gonza?lez-
Rubio et al 2011). Such technique is similar to
DCS but it does not update the IBM model 1 used
by the confidence sampler with the newly avail-
able human-translated sentences. This technique
is referred to as static confidence sampler (SCS).
Results in Figure 3 indicate that the perfor-
mance of the retrained SMT models increased as
more data was incorporated. Regarding the sam-
pling strategies, DCS improved the results ob-
tained by the other sampling strategies. NS ob-
tained by far the worst results, which confirms the
results shown in the previous experiment. Finally,
251
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70
BL
EU
KSMR
DCSNS SCSRS w/o AL
 50 55
 60 65
 70 75
 16  18  20  22  24
Figure 4: Quality of the data stream translation
(BLEU) as a function of the required human effort
(KSMR). w/o AL denotes a system with no retraining.
as it can be seen, SCS obtained slightly worst re-
sults than DCS showing the importance of dy-
namically adapting the underlying model used by
the sampling strategy.
7.3.3 Balancing human effort and
translation quality
Finally, we studied the balance between re-
quired human effort and final translation error.
This can be useful in a real-world scenario where
a translation company is hired to translate a
stream of sentences. Under these circumstances,
it would be important to be able to predict the ef-
fort required from the human translators to obtain
a certain translation quality.
The experiment simulate this situation using
our proposed IMT system with AL to translate
the stream of sentences. To have a broad view
of the behavior of our system, we repeated this
translation process multiple times requiring an in-
creasing human effort each time. Experiments
range from a fully-automatic translation system
with no need of human intervention to a system
where the human is required to supervise all the
sentences. Figure 4 presents results for SCS (see
section 7.3.2) and the sentence selection strate-
gies presented in section 5. In addition, we also
present results for a static system without AL (w/o
AL). This system is equal to SCS but it do not per-
form any SMT retraining.
Results in Figure 4 show a consistent reduction
in required user effort when using AL. For a given
human effort the use of AL methods allowed to
obtain twice the translation quality. Regarding the
different AL sampling strategies, DCS obtains the
better results but differences with other methods
are slight.
Varying the sentence classifier, we can achieve
a balance between final translation quality and re-
quired human effort. This feature allows us to
adapt the system to suit the requirements of the
particular translation task or to the available eco-
nomic or human resources. For example, if a
translation quality of 60 BLEU points is satisfac-
tory, then the human translators would need to
modify only a 20% of the characters of the au-
tomatically generated translations.
Finally, it should be noted that our IMT sys-
tems with AL are able to generate new suffixes
and retrain with new sentence pairs in tenths of a
second. Thus, it can be applied in real time sce-
narios.
8 Conclusions and future work
In this work, we have presented an AL frame-
work for IMT specially designed to process data
streams with massive volumes of data. Our pro-
posal splits the data stream in blocks of sentences
of a certain size and applies AL techniques indi-
vidually for each block. For this purpose, we im-
plemented different sampling strategies that mea-
sure the informativeness of a sentence according
to different criteria.
To evaluate the performance of our proposed
sampling strategies, we carried out experiments
comparing them with random sampling and the
only previously proposed AL technique for IMT
described in (Gonza?lez-Rubio et al 2011). Ac-
cording to the results, one of the proposed sam-
pling strategies, specifically the dynamic con-
fidence sampling strategy, consistently outper-
formed all the other strategies.
The results in the experimentation show that the
use of AL techniques allows us to make a tradeoff
between required human effort and final transla-
tion quality. In other words, we can adapt our sys-
tem to meet the translation quality requirements
of the translation task or the available human re-
sources.
As future work, we plan to investigate on
more sophisticated sampling strategies such as
those based in information density or query-by-
committee. Additionally, we will conduct exper-
iments with real users to confirm the results ob-
tained by our user simulation.
252
Acknowledgements
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no 287576. Work also supported
by the EC (FEDER/FSE) and the Spanish MEC
under the MIPRCV Consolider Ingenio 2010 pro-
gram (CSD2007-00018) and iTrans2 (TIN2009-
14511) project and by the Generalitat Valenciana
under grant ALMPR (Prometeo/2009/01).
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proc. of the conference on
International Language Resources and Evaluation,
pages 2169?2174.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jesu?s Toma?s, En-
rique Vidal, and Juan-Miguel Vilar. 2009. Sta-
tistical approaches to computer-assisted translation.
Computational Linguistics, 35:3?28.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proc. of the in-
ternational conference on Computational Linguis-
tics, pages 315?321.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: large-scale cost-focused active
learning for statistical machine translation. In Proc.
of the Association for Computational Linguistics,
pages 854?864.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 136?158.
Arthur Dempster, Nan Laird, and Donald Rubin.
1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statis-
tical Society., 39(1):1?38.
George Foster, Pierre Isabelle, and Pierre Plamon-
don. 1998. Target-text mediated interactive ma-
chine translation. Machine Translation, 12:175?
194.
Simona Gandrabur and George Foster. 2003. Confi-
dence estimation for text prediction. In Proc. of the
Conference on Computational Natural Language
Learning, pages 315?321.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco casacuberta. 2011. An active learn-
ing scenario for interactive machine translation. In
Proc. of the 13thInternational Conference on Mul-
timodal Interaction. ACM.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proc. of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 415?423.
Pierre Isabelle and Kenneth Ward Church. 1997. Spe-
cial issue on new tools for human translators. Ma-
chine Translation, 12(1-2):1?2.
Philipp Koehn and Christof Monz. 2006. Man-
ual and automatic evaluation of machine transla-
tion between european languages. In Proc. of the
Workshop on Statistical Machine Translation, pages
102?121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54.
Philippe Langlais and Guy Lapalme. 2002. Trans
Type: development-evaluation cycles to boost trans-
lator?s productivity. Machine Translation, 17:77?
98.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Proc. of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 394?402, Los Angeles,
California, June.
Elliott Macklovitch. 2006. TransType2: the last word.
In Proc. of the conference on International Lan-
guage Resources and Evaluation, pages 167?17.
Radford Neal and Geoffrey Hinton. 1999. A view of
the EM algorithm that justifies incremental, sparse,
and other variants. Learning in graphical models,
pages 355?368.
Laurent Nepveu, Guy Lapalme, Philippe Langlais, and
George Foster. 2004. Adaptive language and trans-
lation models for interactive machine translation. In
Proc, of EMNLP, pages 190?197, Barcelona, Spain,
July.
Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Association
for Computational Linguistics, pages 295?302.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the Associa-
tion for Computational Linguistics, pages 160?167.
253
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and
Francisco Casacuberta. 2010. Online learning for
interactive statistical machine translation. In Proc.
of the North American Chapter of the Association
for Computational Linguistics, pages 546?554.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of the Association for Computational Linguistics,
pages 311?318.
Nicola Ueffing and Hermann Ney. 2005. Applica-
tion of word-level confidence measures in interac-
tive statistical machine translation. In Proc. of the
European Association for Machine Translation con-
ference, pages 262?270.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33:9?40.
Xingquan Zhu, Peng Zhang, Xiaodong Lin, and Yong
Shi. 2007. Active learning from data streams. In
Proc. of the 7th IEEE International Conference on
Data Mining, pages 757?762. IEEE Computer So-
ciety.
Xingquan Zhu, Peng Zhang, Xiaodong Lin, and Yong
Shi. 2010. Active learning from stream data using
optimal weight classifier ensemble. Transactions
on Systems, Man and Cybernetics Part B, 40:1607?
1621, December.
254
Proceedings of the ACL 2010 Conference Short Papers, pages 173?177,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Balancing User Effort and Translation Error in Interactive Machine
Translation Via Confidence Measures
Jesu?s Gonza?lez-Rubio
Inst. Tec. de Informa?tica
Univ. Polite?c. de Valencia
46021 Valencia, Spain
jegonzalez@iti.upv.es
Daniel Ortiz-Mart??nez
Dpto. de Sist Inf. y Comp.
Univ. Polite?c. de Valencia
46021 Valencia, Spain
dortiz@dsic.upv.es
Francisco Casacuberta
Dpto. de Sist Inf. y Comp.
Univ. Polite?c. de Valencia
46021 Valencia, Spain
fcn@dsic.upv.es
Abstract
This work deals with the application of
confidence measures within an interactive-
predictive machine translation system in
order to reduce human effort. If a small
loss in translation quality can be tolerated
for the sake of efficiency, user effort can
be saved by interactively translating only
those initial translations which the confi-
dence measure classifies as incorrect. We
apply confidence estimation as a way to
achieve a balance between user effort sav-
ings and final translation error. Empiri-
cal results show that our proposal allows
to obtain almost perfect translations while
significantly reducing user effort.
1 Introduction
In Statistical Machine Translation (SMT), the
translation is modelled as a decission process. For
a given source string fJ1 = f1 . . . fj . . . fJ , we
seek for the target string eI1 = e1 . . . ei . . . eI
which maximises posterior probability:
e?I?1 = argmax
I,eI1
Pr(eI1|fJ1 ) . (1)
Within the Interactive-predictive Machine
Translation (IMT) framework, a state-of-the-art
SMT system is employed in the following way:
For a given source sentence, the SMT system
fully automatically generates an initial translation.
A human translator checks this translation from
left to right, correcting the first error. The SMT
system then proposes a new extension, taking the
correct prefix ei1 = e1 . . . ei into account. These
steps are repeated until the whole input sentence
has been correctly translated. In the resulting
decision rule, we maximise over all possible
extensions eIi+1 of ei1:
e?I?i+1 = argmax
I,eIi+1
Pr(eIi+1|ei1, fJ1 ) . (2)
An implementation of the IMT famework was
performed in the TransType project (Foster et al,
1997; Langlais et al, 2002) and further improved
within the TransType2 project (Esteban et al,
2004; Barrachina et al, 2009).
IMT aims at reducing the effort and increas-
ing the productivity of translators, while preserv-
ing high-quality translation. In this work, we inte-
grate Confidence Measures (CMs) within the IMT
framework to further reduce the user effort. As
will be shown, our proposal allows to balance the
ratio between user effort and final translation error.
1.1 Confidence Measures
Confidence estimation have been extensively stud-
ied for speech recognition. Only recently have re-
searchers started to investigate CMs for MT (Gan-
drabur and Foster, 2003; Blatz et al, 2004; Ueffing
and Ney, 2007).
Different TransType-style MT systems use con-
fidence information to improve translation predic-
tion accuracy (Gandrabur and Foster, 2003; Ueff-
ing and Ney, 2005). In this work, we propose a fo-
cus shift in which CMs are used to modify the in-
teraction between the user and the system instead
of modify the IMT translation predictions.
To compute CMs we have to select suitable con-
fidence features and define a binary classifier. Typ-
ically, the classification is carried out depending
on whether the confidence value exceeds a given
threshold or not.
2 IMT with Sentence CMs
In the conventional IMT scenario a human trans-
lator and a SMT system collaborate in order to
obtain the translation the user has in mind. Once
the user has interactively translated the source sen-
tences, the output translations are error-free. We
propose an alternative scenario where not all the
source sentences are interactively translated by the
user. Specifically, only those source sentences
173
whose initial fully automatic translation are incor-
rect, according to some quality criterion, are in-
teractively translated. We propose to use CMs as
the quality criterion to classify those initial trans-
lations.
Our approach implies a modification of the
user-machine interaction protocol. For a given
source sentence, the SMT system generates an ini-
tial translation. Then, if the CM classifies this
translation as correct, we output it as our final
translation. On the contrary, if the initial trans-
lation is classified as incorrect, we perform a con-
ventional IMT procedure, validating correct pre-
fixes and generating new suffixes, until the sen-
tence that the user has in mind is reached.
In our scenario, we allow the final translations
to be different from the ones the user has in mind.
This implies that the output may contain errors.
If a small loss in translation can be tolerated for
the sake of efficiency, user effort can be saved by
interactively translating only those sentences that
the CMs classify as incorrect.
It is worth of notice that our proposal can be
seen as a generalisation of the conventional IMT
approach. Varying the value of the CM classifi-
cation threshold, we can range from a fully auto-
matic SMT system where all sentences are clas-
sified as correct to a conventional IMT system
where all sentences are classified as incorrect.
2.1 Selecting a CM for IMT
We compute sentence CMs by combining the
scores given by a word CM based on the IBM
model 1 (Brown et al, 1993), similar to the one
described in (Blatz et al, 2004). We modified this
word CM by replacing the average by the max-
imal lexicon probability, because the average is
dominated by this maximum (Ueffing and Ney,
2005). We choose this word CM because it can be
calculated very fast during search, which is cru-
cial given the time constraints of the IMT sys-
tems. Moreover, its performance is similar to that
of other word CMs as results presented in (Blatz
et al, 2003; Blatz et al, 2004) show. The word
confidence value of word ei, cw(ei), is given by
cw(ei) = max
0?j?J
p(ei|fj) , (3)
where p(ei|fj) is the IBM model 1 lexicon proba-
bility, and f0 is the empty source word.
From this word CM, we compute two sentence
CMs which differ in the way the word confidence
Spanish English
Tr
a
in Sentences 214.5K
Running words 5.8M 5.2M
Vocabulary 97.4K 83.7K
D
ev
. Sentences 400
Running words 11.5K 10.1K
Perplexity (trigrams) 46.1 59.4
Te
st
Sentences 800
Running words 22.6K 19.9K
Perplexity (trigrams) 45.2 60.8
Table 1: Statistics of the Spanish?English EU cor-
pora. K and M denote thousands and millions of
elements respectively.
scores cw(ei) are combined:
MEAN CM (cM (eI1)) is computed as the geo-
metric mean of the confidence scores of the
words in the sentence:
cM (eI1) =
I
?
?
?
?
I
?
i=1
cw(ei) . (4)
RATIO CM (cR(eI1)) is computed as the percent-
age of words classified as correct in the sen-
tence. A word is classified as correct if
its confidence exceeds a word classification
threshold ?w.
cR(eI1) =
|{ei / cw(ei) > ?w}|
I
(5)
After computing the confidence value, each sen-
tence is classified as either correct or incorrect, de-
pending on whether its confidence value exceeds
or not a sentence clasiffication threshold ?s. If
?s = 0.0 then all the sentences will be classified
as correct whereas if ?s = 1.0 all the sentences
will be classified as incorrect.
3 Experimentation
The aim of the experimentation was to study the
possibly trade-off between saved user effort and
translation error obtained when using sentence
CMs within the IMT framework.
3.1 System evaluation
In this paper, we report our results as measured
by Word Stroke Ratio (WSR) (Barrachina et al,
2009). WSR is used in the context of IMT to mea-
sure the effort required by the user to generate her
174
 0
 20
 40
 60
 80
 100
 0  0.2  0.4  0.6  0.8  1
 0
 20
 40
 60
 80
 100
W
SR
BL
EU
Threshold (?s)
WSR IMT-CM
BLEU IMT-CM
WSR IMT
BLEU SMT
Figure 1: BLEU translation scores versus WSR
for different values of the sentence classification
threshold using the MEAN CM.
translations. WSR is computed as the ratio be-
tween the number of word-strokes a user would
need to achieve the translation she has in mind and
the total number of words in the sentence. In this
context, a word-stroke is interpreted as a single ac-
tion, in which the user types a complete word, and
is assumed to have constant cost.
Additionally, and because our proposal allows
differences between its output and the reference
translation, we will also present translation qual-
ity results in terms of BiLingual Evaluation Un-
derstudy (BLEU) (Papineni et al, 2002). BLEU
computes a geometric mean of the precision of n-
grams multiplied by a factor to penalise short sen-
tences.
3.2 Experimental Setup
Our experiments were carried out on the EU cor-
pora (Barrachina et al, 2009). The EU corpora
were extracted from the Bulletin of the European
Union. The EU corpora is composed of sentences
given in three different language pairs. Here, we
will focus on the Spanish?English part of the EU
corpora. The corpus is divided into training, de-
velopment and test sets. The main figures of the
corpus can be seen in Table 1.
As a first step, be built a SMT system to trans-
late from Spanish into English. This was done
by means of the Thot toolkit (Ortiz et al, 2005),
which is a complete system for building phrase-
based SMT models. This toolkit involves the esti-
mation, from the training set, of different statisti-
cal models, which are in turn combined in a log-
linear fashion by adjusting a weight for each of
them by means of the MERT (Och, 2003) proce-
 0
 20
 40
 60
 80
 100
 0  0.2  0.4  0.6  0.8  1
 0
 20
 40
 60
 80
 100
W
SR
BL
EU
Threshold (?s)
WSR IMT-CM (?w=0.4)
BLEU IMT-CM (?w=0.4)
WSR IMT
BLEU SMT
Figure 2: BLEU translation scores versus WSR
for different values of the sentence classification
threshold using the RATIO CM with ?w = 0.4.
dure, optimising the BLEU score on the develop-
ment set.
The IMT system which we have implemented
relies on the use of word graphs (Ueffing et al,
2002) to efficiently compute the suffix for a given
prefix. A word graph has to be generated for each
sentence to be interactively translated. For this
purpose, we used a multi-stack phrase-based de-
coder which will be distributed in the near future
together with the Thot toolkit. We discarded to
use the state-of-the-art Moses toolkit (Koehn et
al., 2007) because preliminary experiments per-
formed with it revealed that the decoder by Ortiz-
Mart??nez et al (2005) performs better in terms of
WSR when used to generate word graphs for their
use in IMT (Sanchis-Trilles et al, 2008). More-
over, the performance difference in regular SMT is
negligible. The decoder was set to only consider
monotonic translation, since in real IMT scenar-
ios considering non-monotonic translation leads to
excessive response time for the user.
Finally, the obtained word graphs were used
within the IMT procedure to produce the refer-
ence translations in the test set, measuring WSR
and BLEU.
3.3 Results
We carried out a series of experiments ranging the
value of the sentence classification threshold ?s,
between 0.0 (equivalent to a fully automatic SMT
system) and 1.0 (equivalent to a conventional IMT
system), for both the MEAN and RATIO CMs.
For each threshold value, we calculated the effort
of the user in terms of WSR, and the translation
quality of the final output as measured by BLEU.
175
src-1 DECLARACI ?ON (No 17) relativa al derecho de acceso a la informacio?n
ref-1 DECLARATION (No 17) on the right of access to information
tra-1 DECLARATION (No 17) on the right of access to information
src-2 Conclusiones del Consejo sobre el comercio electro?nico y los impuestos indirectos.
ref-2 Council conclusions on electronic commerce and indirect taxation.
tra-2 Council conclusions on e-commerce and indirect taxation.
src-3 participacio?n de los pa??ses candidatos en los programas comunitarios.
ref-3 participation of the applicant countries in Community programmes.
tra-3 countries? involvement in Community programmes.
Example 1: Examples of initial fully automatically generated sentences classified as correct by the CMs.
Figure 1 shows WSR (WSR IMT-CM) and
BLEU (BLEU IMT-CM) scores obtained varying
?s for the MEAN CM. Additionally, we also show
the BLEU score (BLEU SMT) obtained by a fully
automatic SMT system as translation quality base-
line, and the WSR score (WSR IMT) obtained by
a conventional IMT system as user effort baseline.
This figure shows a continuous transition between
the fully automatic SMT system and the conven-
tional IMT system. This transition occurs when
ranging ?s between 0.0 and 0.6. This is an unde-
sired effect, since for almost a half of the possible
values for ?s there is no change in the behaviour
of our proposed IMT system.
The RATIO CM confidence values depend on
a word classification threshold ?w. We have car-
ried out experimentation ranging ?w between 0.0
and 1.0 and found that this value can be used to
solve the above mentioned undesired effect for
the MEAN CM. Specifically, varying the value of
?w we can stretch the interval in which the tran-
sition between the fully automatic SMT system
and the conventional IMT system is produced, al-
lowing us to obtain smother transitions. Figure 2
shows WSR and BLEU scores for different val-
ues of the sentence classification threshold ?s us-
ing ?w = 0.4. We show results only for this value
of ?w due to paper space limitations and because
?w = 0.4 produced the smoothest transition. Ac-
cording to Figure 2, using a sentence classification
threshold value of 0.6 we obtain a WSR reduction
of 20% relative and an almost perfect translation
quality of 87 BLEU points.
It is worth of notice that the final translations
are compared with only one reference, therefore,
the reported translation quality scores are clearly
pessimistic. Better results are expected using a
multi-reference corpus. Example 1 shows the
source sentence (src), the reference translation
(ref) and the final translation (tra) for three of the
initial fully automatically generated translations
that were classified as correct by our CMs, and
thus, were not interactively translated by the user.
The first translation (tra-1) is identical to the corre-
sponding reference translation (ref-1). The second
translation (tra-2) corresponds to a correct trans-
lation of the source sentence (src-2) that is differ-
ent from the corresponding reference (ref-2). Fi-
nally, the third translation (tra-3) is an example of
a slightly incorrect translation.
4 Concluding Remarks
In this paper, we have presented a novel proposal
that introduces sentence CMs into an IMT system
to reduce user effort. Our proposal entails a mod-
ification of the user-machine interaction protocol
that allows to achieve a balance between the user
effort and the final translation error.
We have carried out experimentation using two
different sentence CMs. Varying the value of
the sentence classification threshold, we can range
from a fully automatic SMT system to a conven-
tional IMT system. Empirical results show that
our proposal allows to obtain almost perfect trans-
lations while significantly reducing user effort.
Future research aims at the investigation of im-
proved CMs to be integrated in our IMT system.
Acknowledgments
Work supported by the EC (FEDER/FSE) and
the Spanish MEC/MICINN under the MIPRCV
?Consolider Ingenio 2010? program (CSD2007-
00018), the iTransDoc (TIN2006-15694-CO2-01)
and iTrans2 (TIN2009-14511) projects and the
FPU scholarship AP2006-00691. Also supported
by the Spanish MITyC under the erudito.com
(TSI-020110-2009-439) project and by the Gener-
alitat Valenciana under grant Prometeo/2009/014.
176
References
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,
and E. Vidal. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kuesza, A. Sanchis, and N. Ueffing.
2004. Confidence estimation for machine transla-
tion. In Proc. COLING, page 315.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?311.
J. Esteban, J. Lorenzo, A. Valderra?banos, and G. La-
palme. 2004. Transtype2: an innovative computer-
assisted translation system. In Proc. ACL, page 1.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Target-
text mediated interactive machine translation. Ma-
chine Translation, 12:12?175.
S. Gandrabur and G. Foster. 2003. Confidence esti-
mation for text prediction. In Proc. CoNLL, pages
315?321.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. ACL,
pages 177?180.
P. Langlais, G. Lapalme, and M. Loranger. 2002.
Transtype: Development-evaluation cycles to boost
translator?s productivity. Machine Translation,
15(4):77?98.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. ACL, pages 160?
167.
D. Ortiz, I. Garc??a-Varea, and F. Casacuberta. 2005.
Thot: a toolkit to train phrase-based statistical trans-
lation models. In Proc. MT Summit, pages 141?148.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of MT.
In Proc. ACL, pages 311?318.
G. Sanchis-Trilles, D. Ortiz-Mart??nez, J. Civera,
F. Casacuberta, E. Vidal, and H. Hoang. 2008. Im-
proving interactive machine translation via mouse
actions. In Proc. EMNLP, pages 25?27.
N. Ueffing and H. Ney. 2005. Application of word-
level confidence measures in interactive statistical
machine translation. In Proc. EAMT, pages 262?
270.
N. Ueffing and H. Ney. 2007. Word-level confidence
estimation for machine translation. Comput. Lin-
guist., 33(1):9?40.
N. Ueffing, F.J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proc. EMNLP, pages 156?163.
177
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1268?1277,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Minimum Bayes-risk System Combination
Jesu?s Gonza?lez-Rubio
Instituto Tecnolo?gico de Informa?tica
U. Polite`cnica de Vale`ncia
46022 Valencia, Spain
jegonzalez@iti.upv.es
Alfons Juan Francisco Casacuberta
D. de Sistemas Informa?ticos y Computacio?n
U. Polite`cnica de Vale`ncia
46022 Valencia, Spain
{ajuan,fcn}@dsic.upv.es
Abstract
We present minimum Bayes-risk system com-
bination, a method that integrates consen-
sus decoding and system combination into
a unified multi-system minimum Bayes-risk
(MBR) technique. Unlike other MBR meth-
ods that re-rank translations of a single SMT
system, MBR system combination uses the
MBR decision rule and a linear combina-
tion of the component systems? probability
distributions to search for the minimum risk
translation among all the finite-length strings
over the output vocabulary. We introduce ex-
pected BLEU, an approximation to the BLEU
score that allows to efficiently apply MBR in
these conditions. MBR system combination is
a general method that is independent of spe-
cific SMT models, enabling us to combine
systems with heterogeneous structure. Exper-
iments show that our approach bring sig-
nificant improvements to single-system-based
MBR decoding and achieves comparable re-
sults to different state-of-the-art system com-
bination methods.
1 Introduction
Once statistical models are trained, a decoding ap-
proach determines what translations are finally se-
lected. Two parallel lines of research have shown
consistent improvements over the max?derivation
decoding objective, which selects the highest prob-
ability derivation. Consensus decoding procedures
select translations for a single system with a mini-
mum Bayes risk (MBR) (Kumar and Byrne, 2004).
System combination procedures, on the other hand,
generate translations from the output of multiple
component systems by combining the best frag-
ments of these outputs (Frederking and Nirenburg,
1994). In this paper, we present minimum Bayes
risk system combination, a technique that unifies
these two approaches by learning a consensus trans-
lation over multiple underlying component systems.
MBR system combination operates directly on the
outputs of the component models. We perform an
MBR decoding using a linear combination of the
component models? probability distributions. In-
stead of re-ranking the translations provided by the
component systems, we search for the hypothesis
with the minimum expected translation error among
all the possible finite-length strings in the target lan-
guage. By using a loss function based on BLEU (Pa-
pineni et al, 2002), we avoid the hypothesis align-
ment problem that is central to standard system com-
bination approaches (Rosti et al, 2007). MBR sys-
tem combination assumes only that each translation
model can produce expectations of n-gram counts;
the latent derivation structures of the component sys-
tems can differ arbitrary. This flexibility allows us to
combine a great variety of SMT systems.
The key contributions of this paper are three: the
usage of a linear combination of distributions within
the MBR decoding, which allows multiple SMT
models to be involved in, and makes the computa-
tion of n-grams statistics to be more accurate; the
decoding in an extended search space, which allows
to find better hypotheses than the evidences pro-
vided by the component models; and the use of an
expected BLEU score instead of the sentence-wise
BLEU, which allows to efficiently apply MBR de-
coding in the huge search space under consideration.
We evaluate in a multi-source translation task ob-
taining improvements of up to +2.0 BLEU abs. over
the best single system max-derivation, and state-of-
the-art performance in the system combination task
of the ACL 2010 workshop on SMT.
1268
2 Related Work
MBR system combination is a multi-system gener-
alization of MBR decoding where the space of hy-
potheses is not constrained to the space of evidences.
We expand the space of hypotheses following some
underlying ideas of system combination techniques.
2.1 Minimum Bayes risk
In SMT, MBR decoding allows to minimize the
loss of the output for a single translation system.
MBR is generally implemented by re-ranking an N -
best list of translations produced by a first pass de-
coder (Kumar and Byrne, 2004). Different tech-
niques to widen the search space have been de-
scribed (Tromble et al, 2008; DeNero et al, 2009;
Kumar et al, 2009; Li et al, 2009). These works
extend the traditional MBR algorithms based on N -
best lists to work with lattices.
The use of MBR to combine the outputs of vari-
ous MT systems has also been explored previously.
Duan et al (2010) present an MBR decoding that
makes use of a mixture of different SMT systems to
improve translation accuracy. Our technique differs
in that we use a linear combination instead of a mix-
ture, which avoids the problem of component sys-
tems not sharing the same search space; perform the
decoding in a search space larger than the outputs
of the component models; and optimize an expected
BLEU score instead of the linear approximation to
it described in (Tromble et al, 2008).
DeNero et al (2010) present model combination,
a multi-system lattice MBR decoding on the con-
joined evidences spaces of the component systems.
Our technique differs in that we perform the search
in an extended search space not restricted to the pro-
vided evidences, have fewer parameters to learn, and
optimizes an expected BLEU score instead of the
linear BLEU approximation.
Another MBR-related technique to combine the
outputs of various MT systems was presented by
Gonza?lez-Rubio and Casacuberta (2010). They use
different median string (Fu, 1982) algorithms to
combine various machine translation systems. Our
approach differs in that we take into account the pos-
terior distribution over translations instead of con-
sidering each translation equally likely, optimize the
expected BLEU score instead of a sentence-wise
measure such as the edit distance or the sentence-
level BLEU, and take into account the quality dif-
ferences by associating a tunable scaling factor to
each system.
2.2 System Combination
System combination techniques in MT take as in-
put the outputs {e1, ? ? ? , eN} of N translation sys-
tems, where en is a structured translation object
(or N -best lists thereof), typically viewed as a se-
quence of words. The dominant approach in the
field chooses a primary translation ep as a backbone,
then finds an alignment an to the backbone for each
en. A new search space is constructed from these
backbone-aligned outputs and then a voting proce-
dure of feature-based model predicts a final consen-
sus translation (Rosti et al, 2007). MBR system
combination entirely avoids this alignment prob-
lem by considering hypotheses as n-gram occur-
rence vectors rather than word sequences. MBR sys-
tem combination performs the decoding in a larger
search space and includes statistics from the compo-
nents? posteriors, whereas system combination tech-
niques typically do not.
Despite these advantages, system combination
may be more appropriate in some settings. In par-
ticular, MBR system combination is designed pri-
marily for statistical systems that generate N -best
or lattice outputs. MBR system combination can in-
tegrate non-statistical systems that generate either a
single or an unweighted output. However, we would
not expect the same strong performance from MBR
system combination in these constrained settings.
3 Minimum Bayes risk Decoding
MBR decoding aims to find the candidate hypothesis
that has the least expected loss under a probability
model (Bickel and Doksum, 1977). We begin with a
review of MBR for SMT.
SMT can be described as a mapping of a word se-
quence f in a source language to a word sequence
e in a target language; this mapping is produced by
the MT decoder D(f). If the reference translation
e is known, the decoder performance can be mea-
sured by the loss function L(e,D(f)). Given such a
loss function L(e, e?) between an automatic transla-
tion e? and a reference e, and an underlying proba-
1269
bility model P (e|f), MBR decoding has the follow-
ing form (Goel and Byrne, 2000; Kumar and Byrne,
2004):
e? = arg min
e??E
R(e?) (1)
= arg min
e??E
?
e?E
P (e|f) ? L(e, e?) , (2)
where R(e?) denotes the Bayes risk of candidate
translation e? under loss function L, and E repre-
sents the space of translations.
If the loss function between any two hypotheses
can be bounded: L(e, e?) ? Lmax, the MBR de-
coder can be rewritten in term of a similarity func-
tion S(e, e?) = Lmax ? L(e, e?). In this case, in-
stead of minimizing the Bayes risk, we maximize
the Bayes gain G(e?):
e? = arg max
e??E
G(e?) (3)
= arg max
e??E
?
e?E
P (e|f) ? S(e, e?) . (4)
MBR decoding can use different spaces for hy-
pothesis selection and gain computation (arg max
and summatory in Eq. (4)). Therefore, the MBR de-
coder can be more generally written as follows:
e? = arg max
e??Eh
?
e?Ee
P (e|f) ? S(e, e?) , (5)
where Eh refers to the hypotheses space form where
the translations are chosen and Ee refers to the evi-
dences space that is used to compute the Bayes gain.
We will investigate the expansion of the hypotheses
space while keeping the evidences space as provided
by the decoder.
4 MBR System Combination
MBR system combination is a multi-system gener-
alization of MBR decoding. It uses the MBR de-
cision rule on a linear combination of the probabil-
ity distributions of the component systems. Unlike
existing MBR decoding methods that re-rank trans-
lation outputs, MBR system combination search for
the minimum risk hypotheses on the complete set of
finite-length hypotheses over the output vocabulary.
We assume the component systems to be statistically
independent and define the Bayes gain as a linear
combination of the Bayes gains of the components.
Each system provides its own space of evidences
Dn(f) and its posterior distribution over translations
Pn(e|f). Given a sentence f in the source language,
MBR system combination is written as follows:
e? = arg max
e??Eh
G(e?) (6)
? arg max
e??Eh
N?
n=1
?n ? Gn(e?) (7)
= arg max
e??Eh
N?
n=1
?n ?
?
e?Dn(f)
Pn(e|f) ? S(e, e?) , (8)
where N is the total number of component systems,
Eh represents the hypotheses space where the search
is performed, Gn(e?) is the Bayes gain of hypothe-
sis e? given by the nth component system and ?n is
a scaling factor introduced to take into account the
differences in quality of the component models. It is
worth mentioning that by using a linear combination
instead of a mixture model, we avoid the problem
of component systems not sharing the same search
space (Duan et al, 2010).
MBR system combination parameters training
and decoding in the extended hypotheses space are
described below.
4.1 Model Training
We learn the scaling factors in Eq. (8) using min-
imum error rate training (MERT) (Och, 2003).
MERT maximizes the translation quality of e? on a
held-out set, according to an evaluation metric that
compares to a reference set. We used BLEU, choos-
ing the scaling factors to maximize BLEU score
of the set of translations predicted by MBR sys-
tem combination. We perform the maximization by
means of the down-hill simplex algorithm (Nelder
and Mead, 1965).
4.2 Model Decoding
In most MBR algorithms, the hypotheses space is
equal to the evidences space. Following the underly-
ing idea of system combination, we are interested in
extend the hypotheses space by including new sen-
tences created using fragments of the hypotheses in
the evidences spaces of the component models. We
perform the search (argmax operation in Eq. (8))
1270
Algorithm 1 MBR system combination decoding.
Require: Initial hypothesis e
Require: Vocabulary the evidences ?
1: e?? e
2: repeat
3: ecur ? e?
4: for j = 1 to |ecur| do
5: e?s ? ecur
6: for a ? ? do
7: e?s ? Substitute(ecur, a, j)
8: if G(e?s) > G(e?s) then
9: e?s ? e?s
10: e?d ? Delete(ecur, j)
11: e?i ? ecur
12: for a ? ? do
13: e?i ? Insert(ecur, a, j)
14: if G(e?i) > G(e?i) then
15: e?i ? e?i
16: e?? arg maxe??{ecur,e?s,e?d,e?i} G(e
?)
17: until G(e?) 6> G(ecur)
18: return ecur
Ensure: G(ecur) ? G(e)
using the approximate median string (AMS) algo-
rithm (Mart??nez et al, 2000). AMS algorithm per-
form a search on a hypotheses space equal to the
free monoid ?? of the vocabulary of the evidences
? = V oc(Ee).
The AMS algorithm is shown in Algorithm 1.
AMS starts with an initial hypothesis e that is mod-
ified using edit operations until there is no improve-
ment in the Bayes gain (Lines 3?16). On each posi-
tion j of the current solution ecur, we apply all the
possible single edit operations: substitution of the
jth word of ecur by each word a in the vocabulary
(Lines 5?9), deletion of the jth word of ecur (Line
10) and insertion of each word a in the vocabulary in
the jth position of ecur (Lines 11?15). If the Bayes
gain of any of the new edited hypotheses is higher
than the Bayes gain of the current hypothesis (Line
17), we repeat the loop with this new hypotheses e?,
in other case, we return the current hypothesis.
AMS algorithm takes as input an initial hypothe-
sis e and the combined vocabulary of the evidences
spaces ?. Its output is a possibly new hypothesis
whose Bayes gain is assured to be higher or equal
than the Bayes gain of the initial hypothesis.
The complexity of the main loop (lines 2-17) is
O(|ecur| ? |?| ? CG), where CG is the cost of com-
puting the gain of a hypothesis, and usually only a
moderate number of iterations (< 10) is needed to
converge (Mart??nez et al, 2000).
5 Computing BLEU-based Gain
We are interested in performing MBR system com-
bination under BLEU. BLEU behaves as a score
function: its value ranges between 0 and 1 and a
larger value reflects a higher similarity. Therefore,
we rewrite the gain function G(?) using single evi-
dence (or reference) BLEU (Papineni et al, 2002)
as the similarity function:
Gn(e?) =
?
e?Dn(f)
Pn(e|f) ? BLEU(e, e?) (9)
BLEU =
4?
k=1
(
mk
ck
) 1
4
?min
(
e1?
r
c , 1.0
)
, (10)
where r is the length of the evidence, c the length of
the hypothesis, mk the number of n-gram matches
of size k, and ck the count of n-grams of size k in
the hypothesis.
The evidences space Dn(f) may contain a huge
number of hypotheses1 which often make impracti-
cal to compute Eq. (9) directly. To avoid this prob-
lem, Tromble et al (2008) propose linear BLEU, an
approximation to the BLEU score to efficiently per-
form MBR decoding when the search space is repre-
sented with lattices. However, our hypotheses space
is the full set of finite-length strings in the target vo-
cabulary and can not be represented in a lattice.
In Eq. (9), we have one hypothesis e? that is to be
compared to a set of evidences e ? Dn(f) which
follow a probability distribution Pn(e|f). Instead
of computing the expected BLEU score by calcu-
lating the BLEU score with respect to each of the
evidences, our approach will be to use the expected
n-gram counts and sentence length of the evidences
to compute a single-reference BLEU score. We re-
place the reference statistics (r and mn in Eq. (10))
by the expected statistics (r? and m?n) given the pos-
1For example, in a lattice the number of hypotheses may be
exponential in the size of its state set.
1271
terior distribution Pn(e|f) over the evidences:
Gn(e?) =
4?
k=1
(
m?k
ck
) 1
4
?min
(
e1?
r?
c , 1.0
)
(11)
r? =
?
e?Dn(f)
|e| ? Pn(e|f) (12)
m?k =
?
ng?Nk(e?)
min(Ce?(ng), C
?(ng)) (13)
C ?(ng) =
?
e?Dn(f)
Ce(ng) ? Pn(e|f) , (14)
where Nk(e?) is the set of n-grams of size k in the
hypothesis, Ce?(ng) is the count of the n-gram ng in
the hypothesis and C ?(ng) is the expected count of
ng in the evidences. To compute the n-gram match-
ings m?k, the count of each n-gram is truncated, if
necessary, to not exceed the expected count for that
n-gram in the evidences.
We have replaced a summation over a possibly ex-
ponential number of items (e? ? Dn(f) in Eq. (9))
with a summation over a polynomial number of n-
grams that occur in the evidences2. Both, the ex-
pected length of the evidences r? and their expected
n-gram counts m?k can be pre-computed efficiently
from N -best lists and translation lattices (Kumar et
al., 2009; DeNero et al, 2010).
6 Experiments
We report results on a multi-source translation
task. From the Europarl corpus released for the
ACL 2006 workshop on MT (WMT2006), we se-
lect those sentence pairs from the German?English
(de?en), Spanish?English (es?en) and French?
English (fr?en) sub-corpora that share the same En-
glish translation. We obtain a multi-source corpus
with German, Spanish and French as source lan-
guages and English as target language. All the ex-
periments were carried out with the lowercased and
tokenized version of this corpus.
We report results using BLEU (Papineni et al,
2002) and translation edit rate (Snover et al, 2006)
(TER). We measure statistical significance using
2If Dn(f) is represented by a lattice, the number of n-grams is
polynomial in the number of edges in the lattice.
System
dev test
BLEU TER BLEU TER
de?en
MAX 25.3 60.5 25.6? 60.3
MBR 25.1 60.7 25.4? 60.5
es?en
MAX 30.9? 53.3? 30.4? 53.9?
MBR 31.0? 53.4? 30.4? 54.0?
fr?en
MAX 30.7? 53.9? 30.8? 53.4?
MBR 30.7? 53.8? 30.9? 53.4?
Table 1: Performance of base systems.
Approach
dev test
BLEU TER BLEU TER
Best MAX 30.9? 53.3? 30.8? 53.4?
Best MBR 31.0? 53.4? 30.9? 53.4?
MBR-SC 32.3 52.5 32.8 52.3
Table 2: Performance from best single system max-
derivation decoding (Best MAX), the best single system
minimum Bayes risk decoding (Best MBR) and mini-
mum Bayes risk system combination (MBR-SC) combin-
ing three systems.
95% confidence intervals computed using paired
bootstrap re-sampling (Zhang and Vogel, 2004). In
all table cells (except for Table 3) systems without
statistically significant differences are marked with
the same superscript.
6.1 Base Systems
We combine outputs from three systems, each one
translating from one source language (German,
Spanish or French) into English. Each individual
system is a phrase-based system trained using the
Moses toolkit (Koehn et al, 2007). The parame-
ters of the systems were tuned using MERT (Och,
2003) to optimize BLEU on the development set.
Each base system yields state-of-the-art perfor-
mance, summarized in Table 1. For each system,
we report the performance of max-derivation decod-
ing (MAX) and 1000-best3 MBR decoding (Kumar
and Byrne, 2004).
6.2 Experimental Results
Table 2 compares MBR system combination (MBR-
SC) to the best MAX and MBR systems. Both Best
3Ehling et al (2007) studied up to 10000-best and show that the
use of 1000-best candidates is sufficient for MBR decoding.
1272
Setup BLEU TER
Best MBR 30.9 53.4
MBR-SC Expected 30.9 53.5
MBR-SC E/Conjoin 32.4 52.1
MBR-SC E/C/evidences-best 30.9 53.5
MBR-SC E/C/hypotheses-best 31.8 52.5
MBR-SC E/C/Extended 32.7 52.3
MBR-SC E/C/Ex/MERT 32.8 52.3
Table 3: Results on the test set for different setups of
minimum Bayes risk system combination.
MBR and MBR-SC were computed on 1000-best
lists. MBR-SC uses expected BLEU as gain func-
tion using the conjoined evidences spaces of the
three systems to compute expected BLEU statistics.
It performs the search in the free monoid of the out-
put vocabulary, and its model parameters were tuned
using MERT on the development set. This is the
standard setup for MBR system combination, and
we refer to it as MBR-SC-E/C/Ex/MERT in Table 3.
MBR system combination improves single Best
MAX system by +2.0 BLEU points in test, and al-
ways improves over MBR. This improvement could
arise due to multiple reasons: the expected BLEU
gain, the larger evidences space, the extended hy-
potheses space, or the MERT tuned scaling factor
values. Table 3 teases apart these contributions.
We first apply MBR-SC to the best system (MBR-
SC-Expected). Best MBR and MBR-SC-Expected
differ only in the gain function: MBR uses sentence
level BLEU while MBR-SC-Expected uses the ex-
pected BLEU gain described in Section 5. MBR-
SC-Expected performance is comparable to MBR
decoding on the 1000-best list from the single best
system. The expected BLEU approximation per-
forms as well as sentence-level BLEU and addition-
ally requires less total computation.
We now extend the evidences space to the con-
joined 1000-best lists (MBR-SC-E/Conjoin). MBR-
SC-E/Conjoin is much better than the best MBR on
a single system. This implies that either the ex-
pected BLEU statistics computed in the conjoined
evidences space are stronger or the larger conjoined
evidences spaces introduce better hypotheses.
When we restrict the BLEU statistics to be com-
puted from only the best system?s evidences space
(MBR-SC-E/C/evidences-best), BLEU scores dra-
matically decrease relative to MBR-SC-E/Conjoin.
This implies that the expected BLEU statistics com-
puted over the conjoined 1000-best lists are stronger
than the corresponding statistics from the single best
system. On the other hand, if we restrict the search
space to only the 1000-best list of the best sys-
tem (MBR-SC-E/C/hypotheses-best), BLEU scores
also decrease relative to MBR-SC-E/Conjoin. This
implies that the conjoined search space also con-
tains better hypotheses than the single best system?s
search space.
These results validate our approach. The linear
combination of the probability distributions in the
conjoined evidences spaces allows to compute much
stronger statistics for the expected BLEU gain and
also contains some better hypotheses than the single
best system?s search space does.
We next expand the conjoined evidences spaces
using the decoding algorithm described in Sec-
tion 4.2 (MBR-SC-E/C/Extended). In this case, the
expected BLEU statistics are computed from the
conjoined 1000-best lists of the three systems, but
the hypotheses space where we perform the decod-
ing is expanded to the set of all possible finite-
length hypotheses over the vocabulary of the evi-
dences. We take the output of MBR-SC-E/Conjoin
as the initial hypotheses of the decoding (see Algo-
rithm 1). MBR-SC-E/C/Extended improves BLEU
score of MBR-SC-E/Conjoin but obtains a slightly
worse TER score. Since these two systems are iden-
tical in their expected BLEU statistics, the improve-
ments in BLEU imply that the extended search space
has introduced better hypotheses. The degradation
in TER performance can be explained by the use of a
BLEU-based gain function in the decoding process.
We finally compute the optimum values for
the scaling factors of the different system us-
ing MERT (MBR-SC-E/C/Ex/MERT). MBR-SC-
E/C/Ex/MERT slightly improves BLEU score of
MBR-SC-E/C/Extended. This implies that the op-
timal values of the scaling factors do not deviate
much from 1.0; a similar result was reported in (Och
and Ney, 2001). We hypothesize that this is because
the three component systems share the same SMT
model, pre-process and decoding. We expect to ob-
tain larger improvements when combining systems
implementing different MT paradigms.
1273
 30.5
 31
 31.5
 32
 32.5
 33
100 101 102 103
BL
EU
Number of hypotheses in the N-best lists
Best MAX
MBR-SCMBR-SC C/ExtendedMBR-SC Conjoin
Figure 1: Performance of minimum Bayes risk system
combination (MBR-SC) for different sizes of the evi-
dences space in comparison to other MBR-SC setups.
MBR-SC-E/C/Ex/MERT is the standard setup for
MBR system combination and, from now, on we will
refer to it as MBR-SC.
We next evaluate performance of MBR system
combination on N -best lists of increasing sizes, and
compare it to MBR-SC-E/C/Extended and MBR-
SC-E/Conjoin in the same N -best lists. We list the
results of the Best MAX system for comparison.
Results in Figure 1 confirm the conclusions ex-
tracted from results displayed in Table 3. MBR-SC-
Conjoin is consistently better than the Best MAX
system, and differences in BLEU increase with
the size of the evidences space. This implies that
the linear combination of posterior probabilities al-
low to compute stronger statistics for the expected
BLEU gain, and, in addition, the larger the evi-
dences space is, the stronger the computed statistics
are. MBR-SC-C/Extended is also consistently better
than MBR-SC-Conjoin with an almost constant im-
provement of +0.4 BLEU points. This result show
that the extended search space always contains bet-
ter hypotheses than the conjoined evidences spaces;
also confirms the soundness of Algorithm 1 that al-
lows to reach them. Finally, MBR-SC also slightly
improves MBR-SC-C/Extended. The optimization
of the scaling factors allows only small improve-
ments in BLEU.
Figure 2 display the MBR system combination
translation and compare it to the max-derivation
translations of the three component systems. Refer-
ence translation is also listed for comparison. MBR-
MAX de?en i will return later .
MAX es?en i shall come back to that later .
MAX fr?en i will return to this later .
MBR-SC i will return to this point later .
Reference i will return to this point later .
Figure 2: MBR system combination example.
SC adds word ?point? to create a new translation
equal to the reference. MBR-SC is able to detect
that this is valuable word even though it does not
appear in the max-derivation hypotheses.
6.3 Comparison to System Combination
Figure 3 compares MBR system combination
(MBR-SC) with state-of-the-art system combination
techniques presented to the system combination task
of the ACL 2010 workshop on MT (WMT2010).
All system combination techniques build a ?word
sausage? from the outputs of the different compo-
nent systems and choose a path trough the sausage
with the highest score under different models. A de-
scription of these systems can be found in (Callison-
Burch et al, 2010).
In this task, the output of the component systems
are single hypotheses or unweighted lists thereof.
Therefore, we lack of the statistics of the com-
ponents? posteriors which is one of the main ad-
vantages of MBR system combination over sys-
tem combination techniques. However, we find that,
even in these constrained setting, MBR system com-
bination performance is similar to the best sys-
tem combination techniques for all translation di-
rections. These experiments validate our approach.
MBR system combination yields state-of-the-art
performance while avoiding the challenge of align-
ing translation hypotheses.
7 Conclusion
MBR system combination integrates consensus de-
coding and system combination into a unified multi-
system MBR technique. MBR system combination
uses the MBR decision rule on a linear combina-
tion of the component systems? probability distri-
butions to search for the sentence with the mini-
mum Bayes risk on the complete set of finite-length
1274
 16
 18
 20
 22
 24
 26
 28
 30
 32
cz-en en-cz de-en en-de es-en en-es fr-en en-fr
B
LE
U
MBR-SC
BBN
CMU
DCU
JHU
KOC
LIUM
RWTH
Figure 3: Performance of minimum Bayes risk system combination (MBR-SC) for different language directions in
comparison to the rest of system combination techniques presented in the WMT2010 system combination task.
strings in the output vocabulary. Component sys-
tems can have varied decoding strategies; we only
require that each system produce an N -best list (or
a lattice) of translations. This flexibility allows the
technique to be applied quite broadly. For instance,
Leusch et al (2010) generate intermediate transla-
tions in several pivot languages, translate them sep-
arately into the target language, and generate a con-
sensus translation out of these using a system combi-
nation technique. Likewise, these pivot translations
could be combined via MBR system combination.
MBR system combination has two significant ad-
vantages over current approaches to system combi-
nation. First, it does not rely on hypothesis align-
ment between outputs of individual systems. Align-
ing translation hypotheses can be challenging and
has a substantial effect on combination perfor-
mance (He et al, 2008). Instead of aligning the sen-
tences, we view the sentences as vectors of n-gram
counts and compute the expected statistics of the
BLEU score to compute the Bayes gain. Second, we
do not need to pick a backbone system for combina-
tion. Choosing a backbone system can also be chal-
lenging and also affects system combination per-
formance (He and Toutanova, 2009). MBR system
combination sidesteps this issue by working directly
on the conjoined evidences space produced by the
outputs of the component systems, and allows the
consensus model to express system preferences via
scaling factors.
Despite its simplicity, MBR system combination
provides strong performance by leveraging different
consensus, decoding and training techniques. It out-
performs best MAX or MBR derivation on each of
the component systems. In addition, it obtains state-
of-the-art performance in a constrained setting better
suited for dominant system combination techniques.
Acknowledgements
Work supported by the EC (FEDER/FSE) and the
Spanish MEC/MICINN under the MIPRCV ?Con-
solider Ingenio 2010? program (CSD2007-00018),
the iTrans2 (TIN2009-14511) project, the UPV
1275
under grant 20091027 and the FPU scholarship
AP2006-00691. Also supported by the Spanish
MITyC under the erudito.com (TSI-020110-2009-
439) project and by the Generalitat Valenciana under
grant Prometeo/2009/014.
References
Peter J. Bickel and Kjell A Doksum. 1977. Mathe-
matical statistics : basic ideas and selected topics.
Holden-Day, San Francisco.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Morristown, NJ, USA. Association for
Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, pages 567?575,
Morristown, NJ, USA. Association for Computational
Linguistics.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine trans-
lation. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
975?983, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.
2010. Mixture model-based minimum bayes risk de-
coding using multiple machine translation systems. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 313?
321, Beijing, China, August. Coling 2010 Organizing
Committee.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum bayes risk decoding for bleu. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 101?
104, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the fourth
conference on Applied natural language processing,
pages 95?100, Morristown, NJ, USA. Association for
Computational Linguistics.
K.S. Fu. 1982. Syntactic Pattern Recognition and Appli-
cations. Prentice Hall.
Vaibhava Goel and William J. Byrne. 2000. Minimum
bayes-risk automatic speech recognition. Computer
Speech & Language, 14(2):115?135.
Jesu?s Gonza?lez-Rubio and Francisco Casacuberta. 2010.
On the use of median string for multi-source transla-
tion. In In Proceedings of the International Confer-
ence on Pattern Recognition (ICPR2010), pages 4328?
4331.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3
- Volume 3, pages 1202?1211, Morristown, NJ, USA.
Association for Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 98?107, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Shankar Kumar and William J. Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In HLT-NAACL, pages 169?176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1 - Volume 1,
pages 163?171, Morristown, NJ, USA. Association for
Computational Linguistics.
Gregor Leusch, Aure?lien Max, Josep Maria Crego, and
Hermann Ney. 2010. Multi-pivot translation by sys-
tem combination. In International Workshop on Spo-
ken Language Translation, Paris, France, December.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
1276
ing of the AFNLP: Volume 2 - Volume 2, pages 593?
601, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
C. D. Mart??nez, A. Juan, and F. Casacuberta. 2000. Use
of Median String for Classification. In Proceedings of
the 15th International Conference on Pattern Recog-
nition, volume 2, pages 907?910, Barcelona (Spain),
September.
John A. Nelder and Roger Mead. 1965. A Simplex
Method for Function Minimization. The Computer
Journal, 7(4):308?313, January.
Franz Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In In Machine Translation
Summit, pages 253?258.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
228?235, Rochester, New York, April. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and Ralph Weischedel. 2006. A study
of translation error rate with targeted human annota-
tion. In In Proceedings of the Association for Machine
Transaltion in the Americas.
Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 620?629, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In In Proceedings of the 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI-2004, pages 4?6.
1277
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172?176,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UPV-PRHLT English?Spanish system for WMT10
Germa?n Sanchis-Trilles and Jesu?s Andre?s-Ferrer and Guillem Gasco?
Jesu?s Gonza?lez-Rubio and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{gsanchis|jandres|fcn}@dsic.upv.es
{ggasco|jegonzalez|pmartinez}@dsic.upv.es
{mrocha|jandreu}@dsic.upv.es
Abstract
In this paper, the system submitted by
the PRHLT group for the Fifth Work-
shop on Statistical Machine Translation of
ACL2010 is presented. On this evalua-
tion campaign, we have worked on the
English?Spanish language pair, putting
special emphasis on two problems derived
from the large amount of data available.
The first one, how to optimize the use of
the monolingual data within the language
model, and the second one, how to make
good use of all the bilingual data provided
without making use of unnecessary com-
putational resources.
1 Introduction
For this year?s translation shared task, the Pat-
tern Recognition and Human Language Technolo-
gies (PRHLT) research group of the Universidad
Polite?cnica de Valencia submitted runs for the
English?Spanish translation task. In this paper, we
report the configuration of such a system, together
with preliminary experiments performed to estab-
lish the final setup.
As in 2009, the central focus of the Shared Task
is on Domain Adaptation, where a system typi-
cally trained using out-of-domain data is adjusted
to translate news commentaries.
For the preliminary experiments, we used only a
small amount of the largest available bilingual cor-
pus, i.e. the United Nations corpus, by including
into our system only those sentences which were
considered similar.
Language model interpolation using a develop-
ment set was explored in this work, together with
a technique to cope with the problem of ?out of
vocabulary words?.
Finally, a reordering constraint using walls and
zones was used in order to improve the perfor-
mance of the submitted system.
In the final evaluation, our system was ranked
fifth, considering only primary runs.
2 Language Model interpolation
Nowadays, it is quite common to have very large
amounts of monolingual data available from sev-
eral different domains. Despite of this fact, in
most of the cases we are only interested in trans-
lating from one specific domain, as is the case in
this year?s shared task, where the provided mono-
lingual training data belonged to European parlia-
mentary proceedings, news related domains, and
the United Nations corpus, which consists of data
crawled from the web.
Although the most obvious thing to do is to con-
catenate all the data available and train a single
language model on the whole data, we also inves-
tigated a ?smarter? use of such data, by training
one language model for each of the available cor-
pora.
3 Similar sentences selection
Currently, it is common to of huge bilingual cor-
pora for SMT. For some common language pairs,
corpora of millions of parallel sentences are avail-
able. In some of the cases big corpora are used
as out-of-domain corpora. For example, in the
case of the shared task, we try to translate a news
text using a small in-domain bilingual news corpus
(News Commentary) and two big out-of-domain
corpora: Europarl and United Nations.
Europarl is a medium size corpus and can be
completely incorporated to the training set. How-
ever, the use of the UN corpus requires a big com-
putational effort. In order to alleviate this prob-
lem, we have chosen only those bilingual sen-
tences from the United Nations that are similar to
the in-domain corpus sentences. As a similarity
measure, we have chosen the alignment score.
Alignment scores have already been used as a
172
filter for noisy corpora (Khadivi and Ney, 2005).
We trained an IBM model 4 using GIZA++ (Och
and Ney, 2003) with the in-domain corpus and
computed the alignment scores over the United
Nations sentences. We assume that the alignment
score is a good measure of similarity.
An important factor in the alignment score is
the length of the sentences, so we clustered the
bilingual sentences in groups with the same sum of
source and target language sentence sizes. In each
of the groups, the higher the alignment score is,
the more similar the sentence is to the in-domain
corpus sentences. Hence, we computed the aver-
age alignment score for each one of the clusters
obtained for the corpus considered in-domain (i.e.
the News-Commentary corpus). This being done,
we assessed the similarity of a given sentence by
computing the probability of such sentence with
respect to the alignment model of the in-domain
corpus, and established the following similarity
levels:
? Level 1: Sentences with an alignment score
equal or higher than the in-domain average.
? Level 2: Sentences with an alignment score
equal or higher than the in-domain average,
minus one standard deviation.
? Level 3: Sentences with an alignment score
equal or higher than the in-domain average,
minus two standard deviations.
Naturally, such similarity levels establish parti-
tions of the out-of-domain corpus. Then, such par-
titions were included into the training set used for
building the SMT system, and re-built the com-
plete system from scratch.
4 Out of Vocabulary Recovery
As stated in the previous section, in order to avoid
a big computational effort, we do not use the
whole United Nations corpus to train the trans-
lation system. Out of vocabulary words are a
common problem for machine translation systems.
When translating the test set, there are test words
that are not in the reduced training set (out of vo-
cabulary words). Some of those out of vocabulary
words are present in the sentences discarded from
the United Nations Corpus. Thus, recovering the
discarded sentences with out of vocabulary words
is needed.
The out of vocabulary words recovery method
is simple: the out of vocabulary words from the
test, when taking into account the reduced training
set, are obtained and then discarded sentences that
contain at least one of them are retrieved. Then,
those sentences are added to the reduced training
set.
Finally, alignments with the resulting training
set were computed and the usual training proce-
dure for phrase-based systems was performed.
5 Walls and zones
In translation, as in other linguistics areas, punc-
tuation marks are essential as they help to un-
derstand the intention of a message and organise
the ideas to avoid ambiguity. They also indicate
pauses, hierarchies and emphasis.
In our system, punctuation marks have been
taken into account during decoding. Traditionally,
in SMT punctuation marks are treated as words
and this has undesirable effects (Koehn and Had-
dow, 2009). For example, commas have a high
probability of occurrence and many possible trans-
lations are generated. Most of them are not consis-
tent across languages. This introduces too much
noise to the phrase tables.
(Koehn and Haddow, 2009) established a
framework to specify reordering constraints with
walls and zones, where commas and end
of sentence are not mixed with various clauses.
Gains between 0.1 and 0.2 of BLEU are reported.
Specifying zones and walls with XML tags
in input sentences allows us to identify structured
fragments that the Moses decoder uses with the
following restrictions:
1. If a <zone> tag is detected, then a block
is identified and must be translated until a
</zone> tag is found. The text between tags
<zone> and </zone> is identified and trans-
lated as a block.
2. If the decoder detects a <wall/> tag, the text
is divided into a prefix and suffix and Moses
must translate all the words of the prefix be-
fore the suffix.
3. If both zones and walls are specified,
then local walls are considered where
the constraint 2 applies only to the area es-
tablished by zones.
173
corpus Language |S| |W | |V |
Europarl v5
Spanish
1272K
28M 154K
English 27M 106K
NC
Spanish
81K
1.8M 54K
English 1.6M 39K
Table 1: Main figures of the Europarl v5 and
News-Commentary (NC) corpora. K/M stands
for thousands/millions. |S| is the number of sen-
tences, |W | the number of running words, and |V |
the vocabulary size. Statistics are reported on the
tokenised and lowercased corpora.
We used quotation marks, parentheses, brackets
and dashes as zone delimiters. Quotation marks
(when appearing once in the sentence), com-
mas, colons, semicolons, exclamation and ques-
tion marks and periods are used as wall delimiters.
The use of zone delimiters do not alter the per-
formance. When using walls, a gain of 0.1
BLEU is obtained in our best model.
6 Experiments
6.1 Experimental setup
For building our SMT systems, the open-source
SMT toolkit Moses (Koehn et al, 2007) was used
in its standard setup. The decoder includes a log-
linear model comprising a phrase-based transla-
tion model, a language model, a lexicalised dis-
tortion model and word and phrase penalties. The
weights of the log-linear interpolation were opti-
mised by means of MERT (Och, 2003). In addi-
tion, a 5-gram LM with Kneser-Ney (Kneser and
Ney, 1995) smoothing and interpolation was built
by means of the SRILM (Stolcke, 2002) toolkit.
For building our baseline system, the News-
Commentary and Europarl v5 (Koehn, 2005) data
were employed, with maximum sentence length
set to 40 in the case of the data used to build the
translation models, and without restriction in the
case of the LM. Statistics of the bilingual data can
be seen in Table 1.
In all the experiments reported, MERT was run
on the 2008 test set, whereas the test set 2009 was
considered as test set as such. In addition, all the
experiments described below were performed in
lowercase and tokenised conditions. For the fi-
nal run, the detokenisation and recasing was per-
formed according to the technique described in the
Workshop baseline description.
corpus |S| |W | |V |
Europarl 1822K 51M 172K
NC 108K 3M 68K
UN 6.2M 214M 411K
News 3.9M 107M 512K
Table 2: Main figures of the Spanish resources
provided: Europarl v5, News-Commentary (NC),
United Nations (UN) and News-shuffled (News).
6.2 Language Model interpolation
The final system submitted to the shared task
included a linear interpolation of four language
models, one for each of the monolingual resources
available for Spanish (see Table 2). The results
can be seen in Table 3. As a first experiment, only
the in-domain corpus, i.e. the News-Commentary
data (NC data) was used for building the LM.
Then, all the available monolingual Spanish data
was included into a single LM, by concatenat-
ing all the data together (pooled). Next, in
interpolated, one LM for each one of the
provided monolingual resources was trained, and
then they were linearly interpolated so as to min-
imise the perplexity of the 2008 test set, and fed
such interpolation to the SMT system. We found
out that weights were distributed quite unevenly,
since the News-shuffled LM received a weight of
0.67, whereas the other three corpora received a
weight of 0.11 each. It must be noted that even
the in-domain LM received a weight of 0.11 (less
than the News-shuffled LM). The reason for this
might be that, although the in-domain LM should
be more appropriate and should receive a higher
weight, the News-shuffled corpus is also news re-
lated (hence not really out-of-domain), but much
larger. For this reason, the result of using only
such LM (News) was also analysed. As expected,
the translation quality dropped slightly. Never-
theless, since the differences are not statistically
significant, we used the News-shuffled LM for in-
ternal development purposes, and the interpolated
LM only whenever an improvement prooved to be
useful.
6.3 Including UN data
We analysed the impact of the selection technique
detailed in Section 3. In this case, the LM used
was the interpolated LM described in the previous
section. The result can be seen in Table 4. As
it can be seen, translation quality as measured by
174
Table 3: Effect of considering different LMs
LM used BLEU
NC data 21.86
pooled 23.53
interpolated 24.97
news 24.79
BLEU improves constantly as the number of sen-
tences selected increases. However, further sen-
tences were not included for computational rea-
sons.
In the same table, we also report the effect of
adding the UN sentences selected by our out-of-
vocabulary technique described in Section 4. In
this context, it should be noted that MERT was
not rerun once such sentences had been selected,
since such sentences are related with the test set,
and not with the development set on which MERT
is run.
Table 4: Effect of including selected sentences
system BLEU
baseline 24.97
+ oovs 25.08
+ Level 1 24.98
+ Level 2 25.07
+ Level 3 25.13
6.4 Final system
Since the News-shuffled, UN and Europarl cor-
pora are large corpora, a new LM interpolation
was estimated by using a 6-gram LM on each one
of these corpora, obtaining a gain of 0.17 BLEU
points by doing so. Further increments in the n-
gram order did not show further improvements.
In addition, preliminary experimentation re-
vealed that the use of walls, as described in
Section 5, also provided slight improvements, al-
though using zones or combining both did not
prove to improve further. Hence, only walls
were included into the final system.
Lastly, the final system submitted to the Work-
shop was the result of combining all the techniques
described above. Such combination yielded a fi-
nal BLEU score of 25.31 on the 2009 test set, and
28.76 BLEU score on the 2010 test set, both in
tokenised and lowercased conditions.
7 Conclusions and future work
In this paper, the SMT system presented by the
UPV-PRHLT team for WMT 2010 has been de-
scribed. Specifically, preliminary results about
how to make use of larger data collections for
translating more focused test sets have been pre-
sented.
In this context, there are still some things which
need a deeper investigation, since the results pre-
sented here give only a small insight about the po-
tential of the similar sentence selection technique
described.
However, a deeper analysis is needed in order
to assess the potential of such technique and other
strategies should be implemented to explore new
kids of reordering constraints.
Acknowledgments
This paper is based upon work supported by
the EC (FEDER/FSE) and the Spanish MICINN
under the MIPRCV ?Consolider Ingenio 2010?
program (CSD2007-00018),iTrans2 (TIN2009-
14511) project, and the FPU scholarship AP2006-
00691. This work was also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and by the Generalitat Valen-
ciana under grant Prometeo/2009/014 and schol-
arships BFPI/2007/117 and ACIF/2010/226 and
by the Mexican government under the PROMEP-
DGEST program.
References
Shahram Khadivi and Hermann Ney. 2005. Automatic
filtering of bilingual corpora for statistical machine
translation. In Natural Language Processing and In-
formation Systems, 10th Int. Conf. on Applications
of Natural Language to Information Systems, vol-
ume 3513 of Lecture Notes in Computer Science,
pages 263?274, Alicante, Spain, June. Springer.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, II:181?184, May.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In The 4th EACL Workshop on Statistical
Machine Translation, ACL, pages 160?164, Athens,
Greece, March. Springer.
P. Koehn et al 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of
175
the ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. of ICSLP?02, pages 901?
904, September.
176
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 296?300,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The UPV-PRHLT Combination System for WMT 2010
Jesu?s Gonza?lez-Rubio and Jesu?s Andre?s-Ferrer and Germa?n Sanchis-Trilles
Guillem Gasco? and Pascual Mart??nez-Go?mez and Martha-Alicia Rocha
Joan-Andreu Sa?nchez and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{jegonzalez|jandres|gsanchis}@dsic.upv.es
{ggasco|pmartinez|mrocha}@dsic.upv.es
{jandreu|fcn}@dsic.upv.es
Abstract
UPV-PRHLT participated in the System
Combination task of the Fifth Workshop
on Statistical Machine Translation (WMT
2010). On each translation direction, all
the submitted systems were combined into
a consensus translation. These consen-
sus translations always improve transla-
tion quality of the best individual system.
1 Introduction
The UPV-PRHLT approach to MT system combi-
nation is based on a refined version of the algo-
rithm described in (Gonza?lez-Rubio and Casacu-
berta, 2010), with additional information to cope
with hypotheses of different quality.
In contrast to most of the previous approaches
to combine the outputs of multiple MT sys-
tems (Bangalore et al, 2001; Jayaraman and
Lavie, 2005; Matusov et al, 2006; Schroeder et
al., 2009), which are variations over the ROVER
voting scheme (Fiscus, 1997), we consider the
problem of computing a consensus translation as
the problem of modelling a set of string patterns
with an adequate prototype. Under this frame-
work, the translation hypotheses of each of the
MT systems are considered as individual patterns
in a set of string patterns. The (generalised) me-
dian string, which is the optimal prototype of a set
of strings (Fu, 1982), is the chosen prototype to
model the set of strings.
2 System Combination Algorithm
The median string of a set is defined as the string
that minimises the sum of distances to the strings
in the set. Therefore, defining a distance between
strings is the primary problem to deal with.
The most common definition of distance be-
tween two strings is the Levenshtein distance,
also known as edit distance (ED). This metric
computes the optimal sequence of edit operations
(insertions, deletions and substitutions of words)
needed to transform one string into the other. The
main problem with the ED is its dependence on the
length of the compared strings. This fact led to the
definition of a new distance whose value is inde-
pendent from the length of the strings compared.
This normalised edit distance (NED) (Vidal et al,
1995) is computed by averaging the number of edit
operations by the length of the edit path. The ex-
perimentation in this work was carried out using
the NED.
2.1 Median String
Given a set E = e1, . . . , en, . . . , eN of translation
hypotheses from N MT systems, let ? be the vo-
cabulary in the target language and ?? be the free
monoid over that vocabulary (E ? ??). The me-
dian string of the set E (noted as M(E)) can be
formally defined as:
M(E) = argmin
e????
N
?
n=1
[
wn ? D(e?, en)
]
, (1)
where D is the distance used to compare two
strings and the value wn, 1 ? n ? N weights
the contribution of the hypothesis n to the sum of
distances, and therefore, it denotes the significance
of hypothesis n in the computation of the median
string. The value wn can be seen as a measure of
the ?quality? of hypothesis n.
Computing the median string is a NP-Hard
problem (de la Higuera and Casacuberta, 2000),
therefore we can only build approximations to the
median string by using several heuristics. In this
work, we follow two different approximations: the
set median string (Fu, 1982) and the approximate
median string (Mart??nez et al, 2000).
296
2.2 Set Median String
The most straightforward approximation to the
median string corresponds to the search of a set
median string. Under this approximation, the
search is constrained to the strings in the given in-
put set. The set median string can be informally
defined as the most ?centred? string in the set. The
set median string of the set E (noted as Ms(E))
is given by:
Ms(E) = argmin
e??E
N
?
n=1
[
wn ? D(e?, en)
]
. (2)
The set median string can be computed in poly-
nomial time (Fu, 1982; Juan and Vidal, 1998).
Unfortunately, in some cases, the set median may
not be a good approximation to the median string.
For example, in the extreme case of a set of two
strings, either achieves the minimum accumulated
distance to the set. However, the set median string
is a useful initialisation in the computation of the
approximate median string.
2.3 Approximate Median String
A good approximation to efficiently compute the
median string is proposed in (Mart??nez et al,
2000). To compute the approximate median string
of the set E, the algorithm starts with an initial
string e which is improved by successive refine-
ments in an iterative process. This iterative pro-
cess is based on the application of different edit
operations over each position of the string e look-
ing for a reduction of the accumulated distance to
the strings in the set. Algorithm 1 describes this
iterative process.
The initial string can be a random string or
a string computed from the set E. Martinez et
al. (2000) proposed two kinds of initial strings: the
set median string of E and a string computed by a
greedy algorithm, both of them obtained similar
results. In this work, we start with the set median
string in the initialisation of the computation of the
approximate median string of the set E. Over this
initial string we apply the iterative procedure de-
scribed in Algorithm 1 until there is no improve-
ment. The final median string may be different
from the original hypotheses.
The computational time cost of Algorithm 1 is
linear with the number of hypotheses in the com-
bination, and usually only a moderate number of
iterations is needed to converge.
For each position i in the string e:
1. Build alternatives:
Substitution: Make x = e. For each word a ? ?:
? Make x? the result string of substituting the ith
word of x by a.
? If the accumulated distance of x? to E is lower
than the accumulated distance from x to E, then
make x = x?.
Deletion: Make y the result string of deleting the ith
word of e.
Insertion: Make z = e. For each word a ? ?:
? Make z? the result of inserting a at position i of
e.
? If the accumulated distance from z? to E is lower
than the accumulated distance from z to E, then
make z = z?.
2. Choose an alternative:
? From the set {e,x,y, z} take the string e? with
less accumulated distance to E. Make e = e?.
Algorithm 1: Iterative process to refine a string
e in order to reduce its accumulated distance to a
given set E.
3 Experiments
Experiments were conducted on all the 8 transla-
tion directions cz?en, en?cz, de?en, en?de,
es?en, en?es, fr?en and en?fr. Some of the
entrants to the shared translation task submit lists
of n-best translations, but, in our experience, if a
large number of systems is available, using n-best
translations does not allow to obtain better consen-
sus translations than using single best translations,
but raises computation time significantly. Conse-
quently, we compute consensus translations only
using the single best translation of each individ-
ual MT system. Table 1 shows the number of sys-
tems submitted and gives an overview of the test
corpus on each translation direction. The number
of running words is the average number of run-
ning words in the test corpora, from where the
consensus translations were computed; the vocab-
ulary is the merged vocabulary of these test cor-
pora. All the experiments were carried out with
the true-cased, detokenised version of the tuning
and test corpora, following the WMT 2010 sub-
mission guidelines.
3.1 Evaluation Criteria
We will present translation quality results in terms
of translation edit rate (TER) (Snover et al, 2006)
and bilingual evaluation understudy (BLEU) (Pa-
297
cz?en en?cz de?en en?de es?en en?es fr?en en?fr
Submitted systems 6 11 16 12 8 10 14 13
Avg. Running words 45K 37K 47K 41K 47K 47K 47K 49K
Distinct words 24K 51K 38K 40K 23K 30K 27K 37K
Table 1: Number of systems submitted and main figures of test corpora on each translation direction. K
stands for thousands of elements.
pineni et al, 2002). TER is computed as the num-
ber of edit operations (insertions, deletions and
substitutions of single words and shifts of word se-
quences) to convert the system hypothesis into the
reference translation. BLEU computes a geomet-
ric mean of the precision of n-grams multiplied by
a factor to penalise short sentences.
3.2 Weighted Sum of Distances
In section 2, we define the median string of a set
as the string which minimises a weighted sum of
distances to the strings in the set (Eq. (1)). The
weights wn in the sum can be tuned. We compute
a weight value for each MT system as a whole, i.e.
all the hypotheses of a given MT system share the
same weight value. We study the performance of
different sets of weight looking for improvements
in the quality of the consensus translations. These
weight values are derived from different automatic
MT evaluation measures:
? BLEU score of each system.
? 1.0 minus TER score of each system.
? Number of times the hypothesis of each sys-
tem is the best TER-scoring translation.
We estimate these scores on the tuning corpora.
A normalisation is performed to transform these
scores into the range [0.0, 1.0]. After the normal-
isation, a weight value of 0.0 is assigned to the
lowest-scoring hypothesis, i.e. the lowest-scoring
hypothesis is not taking into account in the com-
putation of the median string.
3.3 System Combination Results
Our framework to compute consensus translations
allows multiple combinations varying the median
string algorithm or the set of weight values used
in the weighted sum of distances. To assure the
soundness of our submission to the WMT 2010
system combination task, the experiments on the
tuning corpora were carried out in a leaving-one-
out fashion dividing the tuning data into 5 parts
and averaging translation results over these 5 par-
titions. On each of the experiments, 4 of the par-
titions are devoted to obtain the weight values for
the weighted sum of distances while BLEU and
TER scores are calculated on the consensus trans-
lations of the remaining partition.
Table 2 shows, on each translation direction,
the performance of the consensus translations on
the tuning corpora. The consensus translations
were computed with the set median string and the
approximated median string using different sets
of weight values: Uniform, all weights are set
to 1.0, BLEU-based weights, TER-based weights
and oracle-based weights. In addition, we display
the performance of the best of the individual MT
systems for comparison purposes. The number of
MT systems combined for each translation direc-
tion is displayed between parentheses.
On all the translation directions under study, the
consensus translations improved the results of the
best individual systems. E.g. TER improved from
66.0 to 63.3 when translating from German into
English. On average, the set median strings per-
formed better than the best individual system, but
its results were always below the performance of
the approximate median string. The use of weight
values computed from MT quality measures al-
lows to improve the quality of the consensus trans-
lation computed. Specially, oracle-based weight
values that, except for the cz?en task, always per-
form equal or better than the other sets of weight
values. We have observed that no improvements
can be achieved with uniform weight values; it is
necessary to penalise low quality hypotheses.
To compute our primary submission to the
WMT 2010 system combination task we choose
the configurations that obtain consensus transla-
tions with highest BLEU score on the tuning cor-
pora. The approximate median string using oracle-
based scores is the chosen configuration for all
translation directions, except on the cz?en trans-
lation direction for which TER-based weights per-
formed better. As our secondary submission we
298
Single Set median Approximated median
best Uniform Bleu Ter Oracle Uniform Bleu Ter Oracle
cz?en (6) BLEU 17.6 16.5 17.8 18.2 17.6 17.1 18.5 18.5 18.0TER 64.5 68.7 67.6 65.2 64.5 67.0 65.9 65.4 64.4
en?cz (11) BLEU 11.4 10.1 10.9 10.7 11.0 10.1 10.7 10.7 11.0TER 75.3 75.1 74.3 74.2 74.2 73.9 73.4 73.3 73.0
de?en (16) BLEU 19.0 19.0 19.1 19.3 19.7 19.3 19.8 19.9 20.1TER 66.0 65.4 65.2 65.0 64.6 64.4 63.4 63.4 63.3
en?de (12) BLEU 11.9 11.6 11.7 11.7 12.0 11.6 11.8 11.8 12.0TER 74.3 74.1 74.1 74.0 73.7 72.7 72.9 72.7 72.6
es?en (8) BLEU 23.2 23.0 23.3 23.2 23.6 23.1 23.9 23.8 24.2TER 60.2 60.6 59.8 59.8 59.5 60.0 59.2 59.4 59.1
en?es (10) BLEU 23.3 23.0 23.3 23.4 24.0 23.6 23.8 23.8 24.2TER 60.1 60.1 59.9 59.7 59.5 59.0 59.1 58.9 58.6
fr?en (14) BLEU 23.3 22.9 23.2 23.2 23.4 23.4 23.8 23.8 23.9TER 61.1 61.2 60.9 60.9 60.7 60.6 60.0 60.1 59.9
en?fr (13) BLEU 22.7 23.4 23.5 23.6 23.8 23.3 23.6 23.7 23.8TER 62.3 61.0 61.0 60.9 60.6 60.2 60.1 60.0 60.0
Table 2: Consensus translation results (case-sensitive) on the tuning corpora with the set median string
and the approximate median string using different sets of weights: Uniform, BLEU-based, TER-based
and oracle-based. The number of systems being combined for each translation direction is in parentheses.
Best consensus translation scores are in bold.
Best Secondary Primary
BLEU TER BLEU TER BLEU TER
cz?en 18.2 63.9 18.3 66.7 19.0 65.1
en?cz 10.8 75.2 11.3 73.6 11.6 71.9
de?en 18.3 66.6 19.1 65.4 19.6 63.9
en?de 11.6 73.4 11.7 72.9 11.9 71.7
es?en 24.7 59.0 24.9 58.9 25.0 58.2
en?es 24.3 58.4 24.9 57.3 25.3 56.3
fr?en 23.7 59.7 23.6 59.8 23.9 59.4
en?fr 23.3 61.3 23.6 59.9 24.1 58.9
Table 3: Translation scores (case-sensitive) on the
test corpora of our primary and secondary submis-
sions to the WMT 2010 system combination task.
chose the set median string using the same set of
weight values chosen for the primary submission.
We compute MT quality scores on the WMT
2010 test corpora to verify the results on the tuning
data. Table 3 displays, on each translation direc-
tion, the results on the test corpora of our primary
and secondary submissions and of the best indi-
vidual system. These results confirm the results
on the tuning data. On all translation directions,
our submissions perform better than the best indi-
vidual systems as measured by BLEU and TER.
4 Summary
We have studied the performance of two consen-
sus translation algorithms that based in the compu-
tation of two different approximations to the me-
dian string. Our algorithms use a weighted sum of
distances whose weight values can be tuned. We
show that using weight values derived from auto-
matic MT quality measures computed on the tun-
ing corpora allow to improve the performance of
the best individual system on all the translation di-
rections under study.
Acknowledgements
This paper is based upon work supported
by the EC (FEDER/FSE) and the Spanish
MICINN under the MIPRCV ?Consolider In-
genio 2010? program (CSD2007-00018), the
iTransDoc (TIN2006-15694-CO2-01) and iTrans2
(TIN2009-14511) projects and the FPU scholar-
ship AP2006-00691. This work was also sup-
ported by the Spanish MITyC under the eru-
dito.com (TSI-020110-2009-439) project and by
the Generalitat Valenciana under grant Prom-
eteo/2009/014 and scholarships BFPI/2007/117
and ACIF/2010/226 and by the Mexican govern-
ment under the PROMEP-DGEST program.
299
References
S. Bangalore, G. Bodel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In IEEE Workshop on ASRU,
pages 351?354.
C. de la Higuera and F. Casacuberta. 2000. Topology
of strings: Median string is np-complete. Theoreti-
cal Computer Science, 230:39?48.
J. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recogniser output voting
error reduction (rover).
K.S. Fu. 1982. Syntactic Pattern Recognition and Ap-
plications. Prentice Hall.
J. Gonza?lez-Rubio and F. Casacuberta. 2010. On the
use of median string for multi-source translation.
In Proceedings of 20th International Conference on
Pattern Recognition, Istambul, Turkey, May 27-28.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of EAMT, pages 143?152.
A. Juan and E. Vidal. 1998. Fast Median Search in
Metric Spaces. In Proc. of SPR, volume 1451 of
Lecture Notes in Computer Science, pages 905?912.
C. D. Mart??nez, A. Juan, and F. Casacuberta. 2000.
Use of Median String for Classification. In Proc. of
ICPR, volume 2, pages 907?910.
E. Matusov, N. Ueffing, and H-Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Proc. of EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
J. Schroeder, T. Cohn, and P. Koehn. 2009. Word lat-
tices for multi-source translation. In Proc. of EACL,
pages 719?727.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of TER with targeted
human annotation. In Proc. of AMTA, pages 223?
231.
E. Vidal, A. Marzal, and P. Aibar. 1995. Fast compu-
tation of normalized edit distances. IEEE Transac-
tions on PAMI, 17(9):899?902.
300
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 140?144,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The UPV-PRHLT combination system for WMT 2011
Jesu?s Gonza?lez-Rubio and Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Departamento de Sistemas Informa?ticos y Computacio?n
Universitat Polite`cnica de Vale`ncia
{jegonzalez|fcn}@dsic.upv.es
Abstract
This paper presents the submissions of the pat-
tern recognition and human language technol-
ogy (PRHLT) group to the system combina-
tion task of the sixth workshop on statistical
machine translation (WMT 2011). Each sub-
missions is generated by a multi-system mini-
mum Bayes risk (MBR) technique. Our tech-
nique uses the MBR decision rule and a linear
combination of the component systems? prob-
ability distributions to search for the minimum
risk translation among all the sentences in the
target language.
1 Introduction
The UPV-PHRLT approach to machine translation
(MT) system combination is based on the mini-
mum Bayes risk system combination (MBRSC) al-
gorithm (Gonzlez-Rubio et al, 2011). A multi-
system MBR technique that computes consensus
translations over multiple component systems.
MBRSC operates directly on the outputs of the
component models. We perform an MBR decod-
ing using a linear combination of the component
models? probability distributions. Instead of re-
ranking the translations provided by the component
systems, we search for the hypothesis with the min-
imum expected translation error among all the pos-
sible finite-length strings in the target language. By
using a loss function based on BLEU (Papineni et
al., 2002), we avoid the hypothesis alignment prob-
lem that is central to standard system combination
approaches (Rosti et al, 2007). MBRSC assumes
only that each translation model can produce expec-
tations of n-gram counts; the latent derivation struc-
tures of the component systems can differ arbitrary.
This flexibility allows us to combine a great variety
of MT systems.
2 Minimum Bayes risk Decoding
SMT can be described as a mapping of a word se-
quence f in a source language to a word sequence
e in a target language; this mapping is produced by
the MT decoder D(f). If the reference translation
e is known, the decoder performance can be mea-
sured by the loss function L(e,D(f)). Given such a
loss function L(e, e?) between an automatic transla-
tion e? and a reference e, and an underlying proba-
bility model P (e|f), MBR decoding has the follow-
ing form (Goel and Byrne, 2000; Kumar and Byrne,
2004):
e? = arg min
e??E
R(e?) (1)
= arg min
e??E
?
e?E
P (e|f) ? L(e, e?) , (2)
where R(e?) denotes the Bayes risk of candidate
translation e? under loss function L, and E repre-
sents the space of translations.
If the loss function between any two hypotheses
can be bounded: L(e, e?) ? Lmax, the MBR de-
coder can be rewritten in term of a similarity func-
tion S(e, e?) = Lmax ? L(e, e?). In this case, in-
stead of minimizing the Bayes risk, we maximize
the Bayes gain G(e?):
e? = arg max
e??E
G(e?) (3)
= arg max
e??E
?
e?E
P (e|f) ? S(e, e?) . (4)
MBR decoding can use different spaces for hy-
pothesis selection and gain computation (arg max
and sum in Eq. (4)). Therefore, the MBR decoder
can be more generally written as follows:
e? = arg max
e??Eh
?
e?Ee
P (e|f) ? S(e, e?) , (5)
140
where Eh refers to the hypotheses space form where
the translations are chosen and Ee refers to the evi-
dences space that is used to compute the Bayes gain.
We will investigate the expansion of the hypotheses
space while keeping the evidences space as provided
by the decoder.
3 MBR System Combination
MBRSC is a multi-system generalization of MBR
decoding. It uses the MBR decision rule on a linear
combination of the probability distributions of the
component systems. Unlike existing MBR decoding
methods that re-rank translation outputs, MBRSC
search for the minimum risk hypotheses on the com-
plete set of finite-length hypotheses over the out-
put vocabulary. We assume the component systems
to be statistically independent and define the Bayes
gain as a linear combination of the Bayes gains
of the components. Each system provides its own
space of evidences Dn(f) and its posterior distribu-
tion over translations Pn(e|f). Given a sentence f in
the source language, MBRSC is written as follows:
e? = arg max
e??Eh
G(e?) (6)
? arg max
e??Eh
N?
n=1
?n ? Gn(e?) (7)
= arg max
e??Eh
N?
n=1
?n ?
?
e?Dn(f)
Pn(e|f) ? S(e, e?) , (8)
where N is the total number of component systems,
Eh represents the hypotheses space where the search
is performed, Gn(e?) is the Bayes gain of hypothe-
sis e? given by the nth component system and ?n is
a scaling factor introduced to take into account the
differences in quality of the component models. It is
worth mentioning that by using a linear combination
instead of a mixture model, we avoid the problem
of component systems not sharing the same search
space (Duan et al, 2010).
3.1 Computing BLEU-based Gain
We are interested in performing MBRSC under
BLEU. Therefore, we rewrite the gain function G(?)
using single evidence (or reference) BLEU (Pap-
ineni et al, 2002) as the similarity function:
Gn(e?) =
?
e?Dn(f)
Pn(e|f) ? BLEU(e, e?) (9)
BLEU =
4?
k=1
(
mk
ck
) 1
4
?min
(
e1?
r
c , 1.0
)
, (10)
where r is the length of the evidence, c the length of
the hypothesis, mk the number of n-gram matches
of size k, and ck the count of n-grams of size k in
the hypothesis.
The evidences space Dn(f) may contain a huge
number of hypotheses1 which often make impracti-
cal to compute Eq. (9) directly. To avoid this prob-
lem, Tromble et al (2008) propose linear BLEU, an
approximation to the BLEU score to efficiently per-
form MBR decoding on the lattices provided by the
component systems. However, we want to explore a
hypotheses space not restricted to the evidences pro-
vided by the systems.
In Eq. (9), we have one hypothesis e? that is to be
compared to a set of evidences e ? Dn(f) which
follow a probability distribution Pn(e|f). Instead
of computing the expected BLEU score by calcu-
lating the BLEU score with respect to each of the
evidences, our approach will be to use the expected
n-gram counts and sentence length of the evidences
to compute a single-reference BLEU score. We re-
place the reference statistics (r and mn in Eq. (10))
by the expected statistics (r? and m?n) given the pos-
terior distribution Pn(e|f) over the evidences:
Gn(e?) =
4?
k=1
(
m?k
ck
) 1
4
?min
(
e1?
r?
c , 1.0
)
(11)
r? =
?
e?Dn(f)
|e| ? Pn(e|f) (12)
m?k =
?
ng?Nk(e?)
min(Ce?(ng), C
?(ng)) (13)
C ?(ng) =
?
e?Dn(f)
Ce(ng) ? Pn(e|f) , (14)
where Nk(e?) is the set of n-grams of size k in the
hypothesis, Ce?(ng) is the count of the n-gram ng in
1For example, in a lattice the number of hypotheses may be
exponential in the size of its state set.
141
the hypothesis and C ?(ng) is the expected count of
ng in the evidences. To compute the n-gram match-
ings m?k, the count of each n-gram is truncated, if
necessary, to not exceed the expected count for that
n-gram in the evidences.
We have replaced a summation over a possibly ex-
ponential number of items (e? ? Dn(f) in Eq. (9))
with a summation over a polynomial number of n-
grams that occur in the evidences2. Both, the ex-
pected length of the evidences r? and their expected
n-gram counts m?k can be pre-computed efficiently
from N -best lists and translation lattices (Kumar et
al., 2009; DeNero et al, 2010).
3.2 Model Training
The scaling factors in Eq. (8) denote the ?quality? of
each system with respect to the rest of them, i.e. the
relative importance of each system in the Bayes gain
computation. This scaling factors must be carefully
tuned to obtain good translations.
We compute the scaling factor of each system as
the number of times the hypothesis of the system is
the best TER-scoring translation in the tuning cor-
pora. Previous works show that this measure ob-
tains the best translation results among other heuris-
tic measures (Gonza?lez-Rubio et al, 2010) and even
as good results as more complex methods such as
MERT (Och, 2003). A normalization is performed
to transform these counts into the range [0.0, 1.0].
After the normalization, a weight value of 0.0 is as-
signed to the lowest-scoring system, i.e. the lowest-
scoring system is discarded and not taken into ac-
count in the computation of the Bayes gain.
3.3 Model Decoding
In most MBR algorithms, the hypotheses space is
equal to the evidences space. However, we are inter-
ested in extend the hypotheses space by including
new sentences created using fragments of the hy-
potheses in the evidences spaces of the component
models. We perform the search (argmax opera-
tion in Eq. (8)) using the approximate median string
(AMS) algorithm (Mart??nez et al, 2000). AMS
algorithm perform a hill-climbing search on a hy-
potheses space equal to the free monoid ?? of the
vocabulary of the evidences ? = V oc(Ee).
2If Dn(f) is represented by a lattice, the number of n-grams
Algorithm 1 MBRSC decoding algorithm.
Require: Initial hypothesis e
Require: Vocabulary the evidences ?
1: e?? e
2: repeat
3: ecur ? e?
4: for j = 1 to |ecur| do
5: e?s ? ecur
6: for a ? ? do
7: e?s ? Substitute(ecur, a, j)
8: if G(e?s) > G(e?s) then
9: e?s ? e?s
10: e?d ? Delete(ecur, j)
11: e?i ? ecur
12: for a ? ? do
13: e?i ? Insert(ecur, a, j)
14: if G(e?i) > G(e?i) then
15: e?i ? e?i
16: e?? arg maxe??{ecur,e?s,e?d,e?i} G(e
?)
17: until G(e?) 6> G(ecur)
18: return ecur
Ensure: G(ecur) ? G(e)
The AMS algorithm is shown in Algorithm 1.
AMS starts with an initial hypothesis e3 that is mod-
ified using edit operations until there is no improve-
ment in the Bayes gain (Lines 3?16). On each posi-
tion j of the current solution ecur, we apply all the
possible single edit operations: substitution of the
jth word of ecur by each word a in the vocabulary
(Lines 5?9), deletion of the jth word of ecur (Line
10) and insertion of each word a in the vocabulary in
the jth position of ecur (Lines 11?15). If the Bayes
gain of any of the new edited hypotheses is higher
than the Bayes gain of the current hypothesis (Line
17), we repeat the loop with this new hypotheses e?,
in other case, we return the current hypothesis.
AMS algorithm takes as input an initial hypothe-
sis e and the combined vocabulary of the evidences
spaces ?. Its output is a possibly new hypothesis
whose Bayes gain is assured to be higher or equal
than the Bayes gain of the initial hypothesis.
The complexity of the main loop (lines 2-17) is
O(|ecur| ? |?| ? CG), where CG is the cost of com-
is polynomial in the number of edges in the lattice.
3In the experimentation we use the evidence with minimum
Bayes? risk as the initial hypothesis of the algorithm.
142
cz?en en?cz de?en en?de es?en en?es fr?en en?fr
#systems 12 14 25 34 15 22 23 21
de
v
Worst 15.6 8.8 12.8 4.5 15.1 20.3 15.8 13.9
Best 25.9 16.9 22.2 16.3 27.8 32.7 28.6 35.5
MBRSC 26.7 15.9 22.2 17.1 30.5 33.3 30.2 34.7
te
st
Worst 13.3 9.1 12.9 5.1 14.7 20.7 16.1 13.0
Best 27.2 18.6 21.9 16.7 27.4 32.5 28.1 33.5
MBRSC 27.9 17.7 22.1 16.5 30.4 32.9 29.6 32.7
Table 1: BLEU scores (case-sensitive) on the shared translation task development and test corpora of the best and
worst single systems and MBRSC. For each translation direction, we show the number of systems being combined.
Best translation results are in bold.
puting the gain of a hypothesis, and usually only a
moderate number of iterations (< 10) is needed to
converge (Mart??nez et al, 2000).
4 Results
Experiments were conducted on all the 8 translation
directions of the shared translation task Czech?
English (cz?en), German?English (de?en),
Spanish?English (es?en) and French?English
(fr?en) and also on the raw and clean versions
of the Haitian creole?English featured translation
task (ht?en). All the experiments were carried
out with the true-cased, detokenized version of the
tuning and test corpora, following the WMT 2011
submission guidelines.
4.1 Shared translation task
Table 1 shows the BLEU scores of MBRSC on the
development and test corpora in comparison with
the score of the best and worst individual systems.
In most of the translation directions, MBRSC im-
proved the results of the best individual system,
e.g. +2.7/+3.0 BLEU point in es?en. However,
in en?cz and en?fr, MBRSC performs worse than
the best individual system. One thing we noticed is
that for these translation directions, the translations
from one provided single system (online-B) were
much better in terms of BLEU than those of all other
systems (in the former case by more than 14% rel-
ative in development). In our experience, MBRSC
requires ?comparably good? systems to be able to
achieve significant improvements (particularly if us-
ing heuristic scaling factors). On the other hand, we
would have achieved improvements over all remain-
ing systems leaving out online-B.
4.2 Featured translation task
Regarding the ht?en featured translation task,
MBRSC is not able to improve the results of the
best individual system in any case. As in the en?cz
and en?fr translation directions, one of the systems
(bm-i2r) perform much much better than all other
systems. We can notice the surprisingly low score
of one of the systems (umd-hu) in the clean task.
The translations of this system are all equal (?N /
A?) so we suppose that some error occurred during
the translation or submission processes.
ht?en
raw clean
#systems 8 16
worst 15.4 2.9
best 29.6 33.1
MBRSC 28.6 32.2
Table 2: BLEU scores (case-sensitive) on the featured
translation task development corpora of the best and
worst single systems and MBRSC. Best translation re-
sults are in bold.
5 summary
The UPV-PRHLT submissions for WMT 2011 sys-
tem combination task were described in this paper.
The combination was based on a multi-system MBR
technique that uses the MBR decision rule and a lin-
ear combination of the component systems? proba-
bility distributions to search for the minimum risk
translation among all the finite-length strings in the
output vocabulary. We introduced expected BLEU,
143
an approximation to the BLEU score that allows to
efficiently apply MBR in these conditions. In most
of the translation directions we were able to obtain
BLEU gains over the best individual systems.
Acknowledgements
This paper is based upon work supported by the EC
(FEDER/FSE) and the Spanish MEC/MICINN un-
der the MIPRCV ?Consolider Ingenio 2010? pro-
gram (CSD2007-00018), the iTrans2 (TIN2009-
14511) project and the UPV under grant 20091027.
Also supported by the Spanish MITyC under the
erudito.com (TSI-020110-2009-439) project and by
the Generalitat Valenciana under grant Prome-
teo/2009/014.
References
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine trans-
lation. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
975?983, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.
2010. Mixture model-based minimum bayes risk de-
coding using multiple machine translation systems. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 313?
321, Beijing, China, August. Coling 2010 Organizing
Committee.
Vaibhava Goel and William J. Byrne. 2000. Minimum
bayes-risk automatic speech recognition. Computer
Speech & Language, 14(2):115?135.
Jesu?s Gonza?lez-Rubio, Germa?n Sanchis-Trilles, Joan-
Andreu Sa?nchez, Jesu?s Andre?s-Ferrer, Guillem Gasco?,
Pascual Mart??nez-Go?mez, Martha-Alicia Rocha, and
Francisco Casacuberta. 2010. The upv-prhlt combi-
nation system for wmt 2010. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 296?300, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Jess Gonzlez-Rubio, Alfons Juan, and Francisco Casacu-
berta. 2011. Minimum bayes-risk system combina-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1268?1277, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Shankar Kumar and William J. Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In HLT-NAACL, pages 169?176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1 - Volume 1,
pages 163?171, Morristown, NJ, USA. Association for
Computational Linguistics.
C. D. Mart??nez, A. Juan, and F. Casacuberta. 2000. Use
of Median String for Classification. In Proceedings of
the 15th International Conference on Pattern Recog-
nition, volume 2, pages 907?910, Barcelona (Spain),
September.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
228?235, Rochester, New York, April. Association for
Computational Linguistics.
Roy W. Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 620?629, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
144
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 104?108,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
PRHLT Submission to the WMT12 Quality Estimation Task
Jesu?s Gonza?lez Rubio and Alberto Sanchis and Francisco Casacuberta
D. Sistemas Informa?ticos y Computacio?n
Universitat Polite`cnica de Vale`ncia
Camino de vera s/n, 46022, Valencia, Spain
{jegonzalez,josanna,fcn}@dsic.upv.es
Abstract
This is a description of the submissions made
by the pattern recognition and human lan-
guage technology group (PRHLT) of the Uni-
versitat Polite`cnica de Vale`ncia to the qual-
ity estimation task of the seventh workshop
on statistical machine translation (WMT12).
We focus on two different issues: how to ef-
fectively combine subsequence-level features
into sentence-level features, and how to select
the most adequate subset of features. Results
showed that an adequate selection of a subset
of highly discriminative features can improve
efficiency and performance of the quality esti-
mation system.
1 Introduction
Quality estimation (QE) (Ueffing et al, 2003; Blatz
et al, 2004; Sanchis et al, 2007; Specia and Farzin-
dar, 2010) is a topic of increasing interest in machine
translation (MT). It aims at providing a quality indi-
cator for unseen translations at various granularity
levels. Different from MT evaluation, QE do not
rely on reference translations and is generally ad-
dressed using machine learning techniques to pre-
dict quality scores.
Our main focus in this article is in the combi-
nation of subsequence features into sentence fea-
tures, and in the selection of a subset of relevant fea-
tures to improve performance and efficiency. Sec-
tion 2 describes the features and the learning algo-
rithm used in the experiments. Section 3 describe
two different approaches implemented to select the
best-performing subset of features. Section 4 dis-
plays the results of the experimentation intended to
determine the optimal setup to train our final sub-
mission. Finally, section 5 summarizes the submis-
sion and discusses the results.
2 Features and Learning Algorithm
2.1 Available Sources of Information
The WMT12 QE task is carried out on English?
Spanish news texts produced by a phrase-based MT
system. As training data we are given 1832 trans-
lations manually annotated for quality in terms of
post-editing effort (scores in the range [1, 5]), to-
gether with their source sentences, decoding in-
formation, reference translations, and post-edited
translations. Additional training data can be used,
as deemed appropriate. Any of these information
sources can be used to extract the features, however,
test data consists only on source sentence, transla-
tion, and search information. Thus, features were
extracted from the sources of information available
in test data only. Additionally, we compute some
extra features from the WMT12 translation task
(WMT12TT) training data.
2.2 Features
We extracted a total of 475 features classified into
sentence-level and subsequence-level features. We
considered subsequences of sizes one to four.
Sentence-level features
? Source and target sentence lengths, and ratio.
? Proportion of dead nodes in the search graph.
? Number of source phrases.
? Number and average size of the translation op-
tions under consideration during search.
104
? Source and target sentence probability and per-
plexities computed by language models of or-
der one to five.
? Target sentence probability, probability divided
by sentence length, and perplexities computed
by language models of order one to five. Lan-
guage models were trained on the 1000-best
translations.
? 1000-best average sentence length, 1000-best
vocabulary divided by average length, and
1000-best vocabulary divided by source sen-
tence length.
? Percentage of subsequences (sizes one to four)
previously unseen in the source training data.
Subsequence-level features
? Frequency of source subsequences in the
WMT12TT data.
? IBM Model-1 confidence score for each word
in the translation (Ueffing et al, 2003).
? Subsequence confidence scores computed on
1000-best translations as described in (Ueffing
et al, 2003; Sanchis et al, 2007). We use
four subsequence correctness criteria (Levens-
thein position, target position, average position,
and any position) and three weighting schemes
(translation probability, translation rank, and
relative frequencies).
? Subsequence confidence scores computed by a
smoothed na??ve bayes classifier (Sanchis et al,
2007). We computed a confidence score for
each correctness criteria (Levensthein, target,
average and any). The smoothed classifier was
tuned to improve classification error rate on a
separate development set (union of news-test
sets for years 2008 to 2011).
2.3 Combination of Subsequence-level
Features
Since WMT12 focuses on sentence-level QE,
subsequence-level features must be combined to ob-
tain sentence-level indicators. We used two different
methods to combine subsequence features:
? Average value of subsequence-level scores, as
done in (Blatz et al, 2004).
? Percentage of subsequence scores belonging to
each frequency quartile1, as done in (Specia
and Farzindar, 2010).
Thus, each subsequence-level feature was repre-
sented as five sentence-level features: one average
score plus four quartile percentages.
Both methods aim at summarizing the scores of
the subsequences in a translations. The average is
a rough indicator that measures the ?middle? value
of the scores while the percentages of subsequences
belonging to each quartile are more fine-grained in-
dicators that try to capture how spread out the sub-
sequence scores are.
2.4 Learning Algorithm
We trained our quality estimation model using an
implementation of support vector machines (Vap-
nik, 1995) for regression. Specifically, we used
SVMlight (Joachims, 2002) for regression with a ra-
dial basis function kernel with the parameters C, w
and ? optimized. The optimization was performed
by cross-validation using ten random subsamples of
the training set (1648 samples for training and 184
samples for validation).
3 Feature Selection
One of the principal challenges that we had to con-
front is the small size of the training data (only
1832 samples) in comparison with the large number
of features, 475. This inadequate amount of train-
ing data did not allow for an acceptable training of
the regression model which yielded instable systems
with poor performance. We also verified that many
features were highly correlated and were even re-
dundant sometimes. Since the amount of training
data is fixed, we tried to improve the robustness of
our regression systems by selecting a subset of rele-
vant features.
We implemented two different feature selection
techniques: one based on partial component anal-
ysis (PCA), and a greedy selection according to the
individual performance of each feature.
3.1 PCA Selection (PS)
Principal component analysis (Pearson, 1901)
(PCA) is a mathematical procedure that uses an or-
1Quartile values were computed on the WMT12TT data.
105
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 50  100  150  200  250  300  350  400
De
lta 
Av
era
ge
Number of features
GS PS Baseline
(a) Delta average score
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.63
 50  100  150  200  250  300  350  400
MA
E
Number of features
GS PS Baseline
(b) Mean Average Error
Figure 1: Delta average score (a) (higher is better) and mean average error (b) (lower is better) as a function of the
number of features. Cross-validation results for PCA selection (PS), and greedy selection (GS) methods.
thogonal transformation to convert a set of observa-
tions of possibly correlated variables into a set of
values of linearly uncorrelated variables called prin-
cipal components. This transformation is defined in
such a way that the first principal component has
the largest possible variance (that is, accounts for as
much of the variability in the data as possible), and
each succeeding component in turn has the highest
variance possible under the constraint that it be un-
correlated with the preceding components. Strictly
speaking, PCA does not perform a feature selection
because the principal components are linear combi-
nations of the individual features.
PCA generates sets of features (the principal com-
ponents) with almost no correlation. However, it ig-
nores the quality scores to be predicted. Since we
want to obtain the best-performing subset of fea-
tures, there is a mismatch between the selection cri-
terion of PCA and the criterion we are interested in.
In other words, although the features generated by
PCA contain almost no redundancy, they do not nec-
essarily have to constitute the best-performing sub-
set of features.
3.2 Greedy Performance-driven Selection (GS)
We also implemented a greedy feature selection
method which iteratively creates subsets of increas-
ing size with the best-scoring individual features.
The score of each feature is given by the perfor-
mance of a system trained solely on that feature. At
a given iteration, we select the K best scoring fea-
tures and train a regression system with them.
Since we select the features incrementally accord-
ing to their individual performance, we expect to ob-
tain the subset of features that yield the best perfor-
mance. However, we do not take into account the
correlations that may exist between the different fea-
tures, thus, the final subset is almost sure to contain
a large number of redundant features.
4 Experiments
4.1 Assessment Measures
The organizers propose two variations of the task
that will be evaluated separately:
Ranking: Participants are required to submit a
ranking of translations. This ranking will used
to split the data into n quantiles. The evalua-
tion will be performed in terms of delta average
score, the average difference over n between
the scores of the top quantiles and the overall
score of the corpus. The Spearman correlation
will be used as tie-breaking metric.
Scoring: Participants are required to assign a score
in the range [1, 5] for each translation. The
evaluation will be performed in terms of mean
average error (MAE). Root mean squared error
(RMSE) will be used as tie-breaking metric.
4.2 Pre-Submission Results
We now describe a number of experiments whose
goal is to determine the optimal training setup.
106
Specifically, we wanted to determine which selec-
tion method to use (PCA or greedy) and which fea-
tures yield a better system. As a preliminary step,
we extracted all the features described in section 2.
The complete training data consisted on 1832 sam-
ples each one with 475 features.
We trained systems using feature sets of increas-
ing size as given by PCA selection (PS) or greedy
selection (GS). The parameters of each system were
tuned to optimize each of the evaluation measures
under consideration. Performance was measured as
the average of a ten-fold cross-validation experiment
on the training data.
Figure 1 shows the results obtained for the ex-
periments that optimized delta average, and MAE
(result optimizing Spearman and RMSE were quite
similar). We also display the performance of a sys-
tem trained on the baseline features. We observed
that both selection methods yielded a better perfor-
mance than the baseline system. PS allowed for a
quick improvement in performance as more features
are selected, reaching its best results when select-
ing approximately 80 features. After that, perfor-
mance rapidly deteriorate. Regarding GS, its im-
provements in performance were slower in com-
parison with PS. However, GS finally reached the
best scores of the experimentation when selecting
? 225 features. Specifically, the best performance
was reached using the top 222 features for delta av-
erage, and using the top 254 features for MAE.
According to these results, our submissions were
trained on the best subsets of features as given by
the GS method. 222 features were selected accord-
ing to their delta average score for the ranking task
variation, and 254 according to their MAE value for
the scoring task variation. Final submissions were
trained on the complete training set.
Most of the selected features are sentence-level
features calculated from subsequence-based scores.
For instance, among the 222 features of the rank-
ing variation of the task, 174 were computed from
subsequence scores. Among these 174 features,
129 were calculated from confidence scores com-
puted on 1000-best translations, 29 from confidence
scores computed by a smoothed na??ve bayes classi-
fier, 11 from the frequencies of the subsequences in
the WMT12TT data, and 5 from IBM Model-1 word
confidence scores.
Participant ID Delta average? MAE?
SDL Language Weaver 0.63 0.61
Uppsala U. 0.58 0.64
LORIA Institute ? 0.68
Trinity College Dublin 0.56 0.68
Baseline 0.55 0.69
PRHLT 0.55 0.70
U. Edinburgh 0.54 0.68
Shanghai Jiao Tong U. 0.53 0.69
U. Wolverhampton/Sheffield 0.51 0.69
DFKI 0.46 0.82
Dublin City U. 0.44 0.75
U. Polite`cnica Catalunya 0.22 0.84
Table 1: Best official evaluation results on each task of
the different participating teams. Results for our submis-
sions are displayed in bold. Baseline results in italics.
-10-5
 0 5
 10 15
 20 25
 30
 1  2  3  4  5  6  7  8  9  10 11 12 13 14 15
M
e
a
n
 
v
a
l
u
e
Feature number
Train data Test data
Figure 2: Average value (? std. deviation) of the first
15 features used in our final submissions. Feature values
follow a similar distribution in the training and test data.
4.3 Official Evaluation Results
After establishing the optimal training setup, we
now show the official evaluation results for our sub-
missions. Table 1 shows the performance of the var-
ious participants in the ranking (delta average) and
scoring (MAE) tasks. Surprisingly our submissions
yielded a slightly worse result than the baseline fea-
tures. However, given the large improvements over
the baseline system obtained in the pre-submission
experiments, we expected to obtain similar improve-
ments over Baseline in test.
We considered two possible explanations for this
counterintuitive result. First, a possibly divergence
between the underlying distributions of the training
and test data. To investigate this possibility, we stud-
107
ied the distributions of feature values in the training
and test data. Figure 2 displays mean?std. deviation
for the first 15 features used in our final submissions
(similar results are obtained for all the 222 features).
We can observe that feature values in training and
test data follow a similar distribution, although test
values tend to be slightly lower than training values.
A second plausible explanation is the small
amount of training data (only 1832 samples). Lim-
ited data favors simpler systems that can train its few
free parameters more accurately. This is the case of
the Baseline system that was trained using only 11
features, in comparison with the 222 features used
in our submissions. Since the training and test data
seem to have been generated following the same un-
derlying distribution, we hypothesize that the lim-
ited training data is the main explanation for the poor
test performance of our submissions.
5 Summary and Discussion
We have presented the submissions of the PRHLT
group to the WMT12 QE task. The estimation sys-
tems were based on support vector machines for re-
gression. Several features were used to train the
systems in order to predict human-annotated post-
editing effort scores. Our main focus in this article
have been the combination of subsequence features
into sentence features, and the selection of a subset
of relevant features to improve the submitted sys-
tems performance.
Results of the experiments showed that PCA
selection was able to obtain better performance
when selecting a small number of features while
GS yielded the best-performing systems but us-
ing much more features. Among the selected fea-
tures, the larger percentage of them were calculated
from subsequence features. These facts indicate
that the combination of subsequence features yields
sentence-level features with a strong individual per-
formance. However, the high number of features se-
lected by GS indicate that these top-scoring features
are highly correlated.
Official evaluation results differ from what we
expected; baseline system performs better than
our submissions while pre-submission experiments
yielded just opposite results. After discarding a pos-
sibly discrepancy between training and test data dis-
tributions, and given that smaller models such as the
baseline system can be trained more accurately with
limited data, we concluded that the limited training
data is the main explanation for the disparity be-
tween our training and test results.
A future line of research could be the study of
methods that allow to select sets of uncorrelated fea-
tures, that unlike PCA, also take into account the in-
dividual performance of each feature. Specifically,
we plan to study a features selection technique based
on partial least squares regression.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement no 287576. Work also supported by
the EC (FEDER/FSE) and the Spanish MEC under
the MIPRCV ?Consolider Ingenio 2010? program
(CSD2007-00018) and iTrans2 (TIN2009-14511)
project and by the Generalitat Valenciana under
grant ALMPR (Prometeo/2009/01).
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In M. Rollins, editor, Mental Im-
agery. Yale University Press.
Thorsten Joachims. 2002. SVM light.
Karl Pearson. 1901. On lines and planes of closest fit to
systems of points in space. Philosophical Magazine,
2(11):559?572.
Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2007.
Estimation of confidence measures for machine trans-
lation. In In Procedings of the MT Summit XI.
Springer-Verlag.
Lucia Specia and Atefeh Farzindar. 2010. Estimat-
ing machine translation post-editing effort with hter.
In AMTA 2010- workshop, Bringing MT to the User:
MT Research and the Translation Industry. The Ninth
Conference of the Association for Machine Transla-
tion in the Americas, nov.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence measures for statistical machine
translation. In In Proceedings of the MT Summit IX,
pages 394?401. Springer-Verlag.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
108
