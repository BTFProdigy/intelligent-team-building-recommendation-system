Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 145?148,
Dublin, Ireland, August 23-24, 2014.
BUAP: Evaluating Compositional Distributional Semantic Models on Full
Sentences through Semantic Relatedness and Textual Entailment
Sau?l Leo?n, Darnes Vilarin?o, David Pinto, Mireya Tovar, Beatriz Beltra?n
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{saul.leon,darnes,dpinto,mtovar,bbeltran}@cs.buap.mx
Abstract
The results obtained by the BUAP team at
Task 1 of SemEval 2014 are presented in this
paper. The run submitted is a supervised ver-
sion based on two classification models: 1)
We used logistic regression for determining
the semantic relatedness between a pair of
sentences, and 2) We employed support vec-
tor machines for identifying textual entailment
degree between the two sentences. The be-
haviour for the second subtask (textual entail-
ment) obtained much better performance than
the one evaluated at the first subtask (related-
ness), ranking our approach in the 7th position
of 18 teams that participated at the competi-
tion.
1 Introduction
The Compositional Distributional Semantic Models
(CDSM) applied to sentences aim to approximate
the meaning of those sentences with vectors summa-
rizing their patterns of co-occurrence in corpora. In
the Task 1 of SemEval 2014, the organizers aimed
to evaluate the performance of this kind of models
through the following two tasks: semantic related-
ness and textual entailment. Semantic relatedness
captures the degree of semantic similarity, in this
case, between a pair of sentences, whereas textual
entailment allows to determine the entailment rela-
tion holding between two sentences.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
This document is a description paper, therefore,
we focus the rest of it on the features and models we
used for carrying out the experiments. A complete
description of the task and the dataset used are given
in Marelli et al. (2014a) and in Marelli et al. (2014b),
respectively.
The remaining of this paper is structured as fol-
lows. In Section 2 we describe the general model
we used for comparing two sentences and the set of
the features used for constructing the vectorial rep-
resentation for each sentence. Section 3 shows how
we integrate the features calculated in a single vector
which fed a supervised classifier aiming to construct
a classication model that solves the two aforemen-
tioned problems: semantic relatedness and textual
entailment. In the same section we show the ob-
tained results. Finally, in Section 4 we present our
findings.
2 Description of the Distributional
Semantic Model Used
Given a sentence S = w
1
w
2
? ? ?w
|S|
, with w
i
a sen-
tence word, we have calculated different correlated
terms (t
i,j
) or a numeric vector (V
i
) for each word
w
i
as follows:
1. {t
i,j
|relation(t
i,j
, w
i
)} such as ?relation? is
one the following dependency relations: ?ob-
ject?, ?subject? or ?property?.
2. {t
i,j
|t
i,j
= c
k
? ? ? c
k+n
} with n = 2, ? ? ? , 5, and
c
k
? w
i
; these tokens are also known as n-
grams of length n.
3. {t
i,j
|t
i,j
= c
k
? ? ? c
k+((n?1)?r)
} with n =
145
2, ? ? ? , 5, r = 2, ? ? ? , 5, and c
k
? w
i
; these to-
kens are also known as skip-grams of length
n.
4. V
i
is obtained by applying the Latent Semantic
Analysis (LSA) algorithm implemented in the
R software environment for statistical comput-
ing and graphics. V
i
is basically a vector of val-
ues that represent relation of the word w
i
with
it context, calculated by using a corpus con-
structed by us, by integrating information from
Europarl, Project-Gutenberg and Open Office
Thesaurus.
3 A Classification Model for Semantic
Relatedness and Textual Entailment
based on DSM
Once each sentence has been represented by means
of a vectorial representation of patterns, we con-
structed a single vector, ??u , for each pair of sen-
tences with the aim of capturing the semantic relat-
edness on the basis of a training corpus.
The entries of this representation vector are calcu-
lated by obtaining the semantic similarity between
each pair of sentences, using each of the DSM
shown in the previous section. In order to calcu-
late each entry, we have found the maximum similar-
ity between each word of the first sentence with re-
spect to the second sentence and, thereafter, we have
added all these values, thus, ??u = {f
1
, ? ? ? , f
9
}.
Given a pair of sentences S
1
=
w
1,1
w
2,1
? ? ?w
|S
1
|,1
and S
2
= w
1,2
w
2,2
? ? ?w
|S
2
|,2
,
such as each w
i,k
is represented according to the
correlated terms or numeric vectors established
at Section 2, the entry f
i
of ??u is calculated
as: f
l
=
?
|S
1
|
i=1
max{sim(w
i,1
, w
j,2
)}, with
j = 1, ? ? ? , |S
2
|.
The specific similarity measure (sim()) and the
correlated term or numeric vector used for each f
l
is
described as follows:
1. f
1
: w
i,k
is the ?object? of w
i
(as defined
in 2), and sim() is the maximum similar-
ity obtained by using the following six Word-
Net similarity metrics offered by NLTK: Lea-
cock & Chodorow (Leacock and Chodorow,
1998), Lesk (Lesk, 1986), Wu & Palmer (Wu
and Palmer, 1994), Resnik (Resnik, 1995), Lin
(Lin, 1998), and Jiang & Conrath1 (Jiang and
Conrath, 1997).
2. f
2
: w
i,k
is the ?subject? of w
i
, and sim() is
the maximum similarity obtained by using the
same six WordNet similarity metrics.
3. f
3
: w
i,k
is the ?property? of w
i
, and sim() is
the maximum similarity obtained by using the
same six WordNet similarity metrics.
4. f
4
: w
i,k
is an n-gram containing w
i
, and sim()
is the cosine similarity measure.
5. f
5
: w
i,k
is an skip-gram containing w
i
, and
sim() is the cosine similarity measure.
6. f
6
: w
i,k
is numeric vector obtained with LSA,
and sim() is the Rada Mihalcea semantic sim-
ilarity measure (Mihalcea et al., 2006).
7. f
7
: w
i,k
is numeric vector obtained with LSA,
and sim() is the cosine similarity measure.
8. f
8
: w
i,k
is numeric vector obtained with LSA,
and sim() is the euclidean distance.
9. f
9
: w
i,k
is numeric vector obtained with LSA,
and sim() is the Chebyshev distance.
All these 9 features were introduced to a logistic
regression classifier in order to obtain a classifica-
tion model which allows us to determine the value of
relatedness between a new pair of sentences2. Here,
we use as supervised class, the value of relatedness
given to each pair of sentences on the training cor-
pus.
The obtained results for the relatedness subtask
are given in Table 1. In columns 2, 3 and 5, a large
value signals a more efficient system, but a large
MSE (column 4) means a less efficient system. As
can be seen, our run obtained the rank 12 of 17, with
values slightly below the overall average.
3.1 Textual Entailment
In order to calculate the textual entailment judgment,
we have enriched the vectorial representation previ-
ously mentioned with synonyms, antonyms and cue-
1Natural Language Toolkit of Python; http://www.nltk.org/
2We have employed the Weka tool with the default settings
for this purpose
146
Table 1: Results obtained at the substask ?Relatedness? of the Semeval 2014 Task 1
TEAM ID PEARSON SPEARMAN MSE Rank
ECNU run1 0.82795 0.76892 0.32504 1
StanfordNLP run5 0.82723 0.75594 0.32300 2
The Meaning Factory run1 0.82680 0.77219 0.32237 3
UNAL-NLP run1 0.80432 0.74582 0.35933 4
Illinois-LH run1 0.79925 0.75378 0.36915 5
CECL ALL run1 0.78044 0.73166 0.39819 6
SemantiKLUE run1 0.78019 0.73598 0.40347 7
CNGL run1 0.76391 0.68769 0.42906 8
UTexas run1 0.71455 0.67444 0.49900 9
UoW run1 0.71116 0.67870 0.51137 10
FBK-TR run3 0.70892 0.64430 0.59135 11
BUAP run1 0.69698 0.64524 0.52774 12
UANLPCourse run2 0.69327 0.60269 0.54225 13
UQeResearch run1 0.64185 0.62565 0.82252 14
ASAP run1 0.62780 0.59709 0.66208 15
Yamraj run1 0.53471 0.53561 2.66520 16
asjai run5 0.47952 0.46128 1.10372 17
overall average 0.71876 0.67159 0.63852 8-9
Our difference against the overall average -2% -3% 11% -
words (?no?, ?not?, ?nobody? and ?none?) for de-
tecting negation at the sentences3 . Thus, if some of
these new features exist on the training pair of sen-
tences, we add a boolean value of 1, otherwise we
set the feature to zero.
This new set of vectors is introduced to a support
vector machine classifier4, using as class the textual
entailment judgment given on the training corpus.
The obtained results for the textual entailment
subtask are given in Table 2. Our run obtained the
rank 7 of 18, with values above the overall average.
We consider that this improvement over the related-
ness task was a result of using other features that
are quite important for semantic relatedness, such
as lexical relations (synonyms and antonyms), and
the consideration of the negation phenomenon in the
sentences.
4 Conclusions
This paper describes the use of compositional distri-
butional semantic models for solving the problems
3Synonyms were extracted from WordNet, whereas the
antonyms were collected from Wikipedia.
4Again, we have employed the weka tool with the default
settings for this purpose.
of semantic relatedness and textual entailment. We
proposed different features and measures for that
purpose. The obtained results show a competitive
approach that may be further improved by consider-
ing more lexical relations or other type of semantic
similarity measures.
In general, we obtained the 7th place in the official
ranking list from a total of 18 teams that participated
at the textual entailment subtask. The result at the
semantic relatedness subtask could be improved if
we were considered to add the new features taken
into consideration at the textual entailment subtask,
an idea that we will implement in the future.
References
Jay J. Jiang and David W. Conrath. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proc of 10th International Conference
on Research in Computational Linguistics, RO-
CLING?97, pages 19?33, 1997.
Claudia Leacock and Martin Chodorow. Combin-
ing local context and wordnet similarity for word
sense identification. In Christiane Fellfaum, edi-
tor, MIT Press, pages 265?283, 1998.
147
Table 2: Results obtained at the substask ?Textual Entailment? of the Semeval 2014 Task 1
TEAM ID ACCURACY Rank
Illinois-LH run1 84.575 1
ECNU run1 83.641 2
UNAL-NLP run1 83.053 3
SemantiKLUE run1 82.322 4
The Meaning Factory run1 81.591 5
CECL ALL run1 79.988 6
BUAP run1 79.663 7
UoW run1 78.526 8
CDT run1 77.106 9
UIO-Lien run1 77.004 10
FBK-TR run3 75.401 11
StanfordNLP run5 74.488 12
UTexas run1 73.229 13
Yamraj run1 70.753 14
asjai run5 69.758 15
haLF run2 69.413 16
CNGL run1 67.201 17
UANLPCourse run2 48.731 18
Overall average 75.358 11-12
Our difference against the overall average 4.31% -
Michael Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference
on Systems Documentation, pages 24?26. ACM,
1986.
Dekang Lin. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA,
1998. Morgan Kaufmann Publishers Inc.
Marco Marelli, Luisa Bentivogli, Marco Baroni,
Raffaella Bernardi, Stefano Menini, and Roberto
Zamparelli. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on
full sentences through semantic relatedness and
textual entailment. In Proceedings of the 8th
International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, 2014a.
Marco Marelli, Stefano Menini, Marco Baroni,
Luisa Bentivogli, Raffaella Bernardi, and Roberto
Zamparelli. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014, Reykjavik, Iceland,
2014b.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial In-
telligence, pages 775?780, 2006.
Philip Resnik. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, IJCAI?95, pages 448?453,
San Francisco, CA, USA, 1995.
Zhibiao Wu and Martha Stone Palmer. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, 1994.
148
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 149?153,
Dublin, Ireland, August 23-24, 2014.
BUAP: Evaluating Features for Multilingual and Cross-Level Semantic
Textual Similarity
Darnes Vilarin?o, David Pinto, Sau?l Leo?n, Mireya Tovar,Beatriz Beltra?n
Beneme?rita Universidad Auto?noma de Puebla
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes,dpinto,saul.leon,mtovar,bbeltran}@cs.buap.mx
Abstract
In this paper we present the evaluation of
different features for multiligual and cross-
level semantic textual similarity. Three dif-
ferent types of features were used: lexical,
knowledge-based and corpus-based. The re-
sults obtained at the Semeval competition rank
our approaches above the average of the rest
of the teams highlighting the usefulness of the
features presented in this paper.
1 Introduction
Semantic textual similarity aims to capture whether
the meaning of two texts are similar. This concept
is somehow different from the textual similarity def-
inition itself, because in the latter we are only in-
terested in measuring the number of lexical com-
ponents that the two texts share. Therefore, tex-
tual similarity can range from exact semantic equiv-
alence to a complete unrelatedness pair of texts.
Finding the semantic similarity between a pair
of texts has become a big challenge for specialists
in Natural Language Processing (NLP), because it
has applications in some NLP task such as machine
translation, automatic construction of summaries,
authorship attribution, machine reading comprehen-
sion, information retrieval, among others, which
usually need a manner to calculate degrees of simi-
larity between two given texts.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Semantic textual similarity can be calculated us-
ing texts of different sizes, for example between, a
paragraph and a sentence, or a sentence and a phrase,
or a phrase and a word, or even a word and a sense.
When we consider this difference, we say the task is
called ?Cross-Level Semantic Similarity?, but when
this distinction is not considered, then we call the
task just as ?Semantic Textual Similarity?.
In this paper, we evaluate different features for de-
termining those that obtain the best performances for
calculating both, cross-level semantic similarity and
multilingual semantic textual similarity.
The remaining of this paper is structured as fol-
lows. Section 2 presents the features used in both
experiments. Section 3 shows the manner we used
the features for determining the degree of seman-
tic textual similarity. Section 4, on the other hand,
shows the experiments we have carried out for de-
termining cross-level semantic similarity. Finally, in
Section 5 the conclusions and findings are given.
2 Description of Features
In this section we describe the different features used
for evaluation semantic textual similarity. Basically,
we have used three different types of features: lex-
ical, knowledge-based and corpus-based. The first
one, counts the frequency of ocurrence of lexical
features which include n-grams of characters, skip-
grams1, words and some lexical relationships such
as synonymy or hypernymy. Additionally, we have
used two other features: the Jaccard coefficient be-
tween the two text, expanding each term with a set of
1They are also known as disperse n-grams because they con-
sider to ?skip? a certain number of characters.
149
synonyms taken from WordReference Carrillo et al.
(2012), and the cosine between the two texts repre-
sented each by a bag of character n-grams and skip-
grams. In this case, we did not applied any word
sense disambiguation system before expanding with
synonyms, a procedure that may be performed in a
further work.
The second set of features considers the following
six word similarity metrics offered by NLTK: Lea-
cock & Chodorow (Leacock and Chodorow, 1998),
Lesk (Lesk, 1986), Wu & Palmer (Wu and Palmer,
1994), Resnik (Resnik, 1995), Lin (Lin, 1998), and
Jiang & Conrath2 (Jiang and Conrath, 1997). In
this case, we determine the similarity between two
texts as the maximum possible pair of words similar-
ity. The third set of features considers two corpus-
based measures, both based on Rada Mihalcea?s tex-
tual semantic similarity (Mihalcea et al., 2006). The
first one uses Pointwise Mutual Information (PMI)
(Turney, 2001) for calculating the similarity between
pairs of words, whereas the second one uses Latent
Semantic Analysis (LSA) (Landauer et al., 1998)
(implemented in the R software environment for sta-
tistical computing) for that purpose. In particular,
the PMI and LSA values were obtained using a cor-
pus built on the basis of Europarl, Project-Gutenberg
and Open Office Thesaurus. A summary of these
features can be seen in Table 1.
3 Multilingual Semantic Textual Similarity
This task aims to find the semantic textual similar-
ity between two texts written in the same language.
Two different languages were considered: English
and Spanish. The degree of semantic similarity
ranges from 0 to 5; the bigger this value, the best se-
mantic match between the two texts. For the experi-
ments we have used the training datasets provided at
2012, 2013 and 2014 Semeval competitions. These
datasets are completely described at the task descrip-
tion papers of these Semeval editions Agirre et al.
(2013, 2014).
In order to calculate the semantic textual simi-
larity for the English language, we have used all
the features mentioned at Section 2. We have con-
structed a single vector for each pair of texts of the
training corpus, thus resulting 6,627 vectors in total.
2Natural Language Toolkit of Python; http://www.nltk.org/
The resulting set of vectors fed a supervised classi-
fier, in particular, a logistic regression model3. This
approach has been named as BUAP-EN-run1. The
most representative results obtained at the competi-
tion for the English language can be seen in Table 2.
As can be seen, we outperformed the average result
in all the cases, except on the case that the OnWN
corpus was used.
In order to calculate the semantic textual similar-
ity for the Spanish language, we have submitted two
runs, the first one is a supervised approach which
constructs a regression model, similar that the one
constructed for the English language, but consider-
ing only the following features: character n-grams,
character skip-grams, and the cosine similarity of
bag of character n-grams and skip-grams. This ap-
proach was named BUAP-run1. Given that the num-
ber of Spanish samples was so small, we decided
to investigate the behaviour of training with English
and testing with Spanish language. It is quite inter-
esting that this approach obtained a relevant ranking
(17 from 22 runs), even if the type of features used
were na??ve.
The second approach submitted for determining
the semantic textual similarity for the Spanish lan-
guage is an unsupervised one. It uses the same fea-
tures of the supervised approach for Spanish, but
these features were used to create a representation
vector for each text (independently), so that we may
be able to calculate the similarity by means of the
cosine measure between the two vectors. The ap-
proach was named BUAP-run2.
The most representative results obtained at the
competition for the Spanish language can be seen
in Table 3. There we can see that our unsupervised
approach slightly outperformed the overall average,
but the supervised approach was below the overall
average, a fact that is expected since we have trained
using the English corpus and testing with the Span-
ish language. Despite this, it is quite interesting that
the result obtained with this supervised approach is
not so bad.
Due to space constraints, we did not reported the
complete set of results of the competition, however,
these results can be seen at the task 10 description
3We used the version of the logistic classifier implemented
in the the Weka toolkit
150
Table 1: Features used for calculating semantic textual similarity
Feature Type
n-grams of characters (n = 2, ? ? ? , 5) Lexical
skip-grams of characters (skip = 2, ? ? ? , 5) Lexical
Number of words shared Lexical
Number of synonyms shared Lexical
Number of hypernyms shared Lexical
Jaccard coefficient with synonyms expansion Lexical
Cosine of bag of character n-grams and skip-grams Lexical
Leacock & Chodorow?s word similarity Knowledge-based
Lesk?s word similarity Knowledge-based
Wu & Palmer?s word similarity Knowledge-based
Resnik?s word similarity Knowledge-based
Lin?s word similarity Knowledge-based
Jiang & Conrath?s word similarity Knowledge-based
Rada Mihalcea?s metric using PMI Corpus-based
Rada Mihalcea?s metric using LSA Corpus-based
Table 2: Results obtained at the Task 10 of the Semeval competition for the English language
Team Name deft-forum deft-news headlines images OnWN tweet-news Weighted mean Rank
DLS@CU-run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639 0.7610 1
Meerkat Mafia-pairingWords 0.4711 0.7628 0.7597 0.8013 0.8745 0.7793 0.7605 2
NTNU-run3 0.5305 0.7813 0.7837 0.8343 0.8502 0.6755 0.7549 3
BUAP-EN-run1 0.4557 0.6855 0.6888 0.6966 0.6539 0.7706 0.6715 19
Overall average 0.3607 0.6198 0.5885 0.6760 0.6786 0.6001 0.6015 27-28
Bielefeld SC-run2 0.2108 0.4307 0.3112 0.3558 0.3607 0.4087 0.3470 36
UNED-run22 p np 0.1043 0.3148 0.0374 0.3243 0.5086 0.4898 0.3097 37
LIPN-run2 0.0843 - - - - - 0.0101 38
Our difference against the average 9% 7% 10% 2% -2% 17% 7% -
Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol
denotes a system that used Wikipedia to build its model for the Wikipedia test dataset)
Team Name System type Wikipedia News Weighted correlation Rank
UMCC DLSI-run2 supervised 0.7802 0.8254 0.8072 1
Meerkat Mafia-run2 unsupervised 0.7431 0.8454 0.8042 2
UNAL-NLP-run1 weakly supervised 0.7804 0.8154 0.8013 3
BUAP-run2 unsupervised 0.6396 0.7637 0.7137 14
Overall average - 0.6193 0.7504 0.6976 14-15
BUAP-run1 supervised 0.5504 0.6785 0.6269 17
RTM-DCU-run2 supervised 0.3689 0.6253 0.5219 20
Bielefeld SC-run2 unsupervised* 0.2646 0.5546 0.4377 21
Bielefeld SC-run1 unsupervised* 0.2632 0.5545 0.4371 22
Difference between our run1 and the overall average - -7% -7% -7% -
Difference between our run2 and the overall average - 2% 1% 2% -
paper (Agirre et al., 2014) of Semeval 2014.
4 Cross-Level Semantic Similarity
This task aims to find semantic similarity between
a pair of texts of different length written in En-
glish language, actually each text belong to a dif-
ferent level of representation of language (para-
graph, sentence, phrase, word, and sense). Thus,
the pair of levels that were required to be compared
in order to determine their semantic similarity were:
paragraph-to-sentence, sentence-to-phrase, phrase-
to-word, and word-to-sense.
The task cross level similarity judgments are
based on five rating levels which goes from 0 to
151
4. The first (0) implies that the two items do not
mean the same thing and are not on the same topic,
whereas the last one (4) implies that the two items
have very similar meanings and the most important
ideas, concepts, or actions in the larger text are rep-
resented in the smaller text. The remaining rating
levels imply something in the middle.
For word-to-sense comparison, a sense is paired
with a word and the perceived meaning of the word
is modulated by virtue of the comparison with the
paired sense?s definition. For the experiments pre-
sented at the competition, a corpus of 2,000 pairs
of texts were provided for training and other 2,000
pairs for testing. This dataset considered 500 pairs
for each type of level of semantic similarity. The
complete description of this task together with the
dataset employed is given in the task description pa-
per Jurgens et al. (2014).
We submitted two supervised approaches, to this
task employing all the features presented at Section
2. The first approach simply constructs a single vec-
tor for each pair of training texts using the afore-
mentioned features. These vectors are introduced in
Weka for constructing a classification model based
on logistic regression. This approach was named
BUAP-run1.
We have observed that when comparing texts of
different length, there may be a high discrepancy
between those texts because a very small length in
the texts may difficult the process of determining the
semantic similarity. Therefore, we have proposed
to expand small text with the aim of having more
term useful in the process of calculating the degree
of semantic similarity. In particular, we have ex-
panded words for the phrase-to-word and word-to-
sense cases. The expansion has been done as fol-
lows. When we calculated the similarity between
phrases and words, we expanded the word compo-
nent with those related terms obtained by means of
the Related-Tags Service of Flickr. When we cal-
culated the semantic similarity between words and
senses, we expanded the word component with their
WordNet Synsets (none word sense disambiguation
method was employed). This second approach was
named BUAP-run2.
The most representative results for the cross-level
semantic similarity task (which include our results)
are shown in Table 4. There we can see that the fea-
tures obtained a good performance when we com-
puted the semantic similarity between paragraphs
and sentences, and when we calculated the simili-
raty between sentences to phrases. Actually, both
runs obtained exactly the same result, because the
main difference between these two runs is that the
second one expands the word/sense using the Re-
lated Tags of Flickr. However, the set of expansion
words did not work properly, in particular when cal-
culating the semantic similarity between phrases and
words. We consider that this behaviour is due to
the domain of the expansion set do not match with
the domain of the dataset to be evaluated. In the
case of expanding words for calculating the similar-
ity between words and senses, we obtained a slightly
better performance, but again, this values are not
sufficient to highly outperform the overall average.
As future work we consider to implement a self-
expansion technique for obtaining a set of related
terms by means of the same training corpus. This
technique has proved to be useful when the expan-
sion process is needed in restricted domains Pinto
et al. (2011).
5 Conclusions
This paper presents the results obtained by the
BUAP team at the Task 3 and 10 of SemEval 2014.
In both task we have used a set of similar features,
due to the aim of these two task are quite similar:
determining semantic similarity. Some special mod-
ifications has been done according to each task in
order to tackle some issues like the language or the
text length.
In general, the features evaluated performed well
over the two approaches, however, some issues arise
that let us know that we need to tune the approaches
presented here. For example, a better expansion set
is required in the case of the Task 3, and a great num-
ber of samples for the spanish samples of Task 10
will be required.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor
Gonzalez-Agirre, and Weiwei Guo. *sem 2013
shared task: Semantic textual similarity. In 2nd
Joint Conference on Lexical and Computational
152
Table 4: Results obtained at Task 3 of Semeval 2014
Team System Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word Word-to-Sense Rank
SimCompass run1 0.811 0.742 0.415 0.356 1
ECNU run1 0.834 0.771 0.315 0.269 2
UNAL-NLP run2 0.837 0.738 0.274 0.256 3
BUAP run1 0.805 0.714 0.162 0.201 9
BUAP run2 0.805 0.714 0.142 0.194 10
Overall average - 0.728 0.651 0.198 0.192 11-12
Our run1 - Overall average 8% 6% -4% 1% -
Our run2 - Overall average 8% 6% -6% 0% -
Semantics (*SEM), pages 32?43, Atlanta, Geor-
gia, USA, 2013.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. Semeval-2014 task 10: Multilingual se-
mantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evalua-
tion (SemEval-2014), Dublin, Ireland, 2014.
Maya Carrillo, Darnes Vilarin?o, David Pinto,
Mireya Tovar, Saul Leo?n, and Esteban Castillo.
Fcc: Three approaches for semantic textual sim-
ilarity. In Proceedings of the 1st Joint Con-
ference on Lexical and Computational Seman-
tics (SemEval 2012), pages 631?634, Montre?al,
Canada, 2012.
Jay J. Jiang and David W. Conrath. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proc of 10th International Conference
on Research in Computational Linguistics, RO-
CLING?97, pages 19?33, 1997.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. Semeval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evalua-
tion (SemEval-2014), Dublin, Ireland, 2014.
Thomas K. Landauer, Peter W. Foltz, and Darrell
Laham. An Introduction to Latent Semantic Anal-
ysis. Discourse Processes, (25):259?284, 1998.
Claudia Leacock and Martin Chodorow. Combin-
ing local context and wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, MIT Press, pages 265?283, 1998.
Michael Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceed-
ings of the 5th Annual International Conference
on Systems Documentation, pages 24?26. ACM,
1986.
Dekang Lin. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Inter-
national Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA,
1998. Morgan Kaufmann Publishers Inc.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial In-
telligence, pages 775?780, 2006.
David Pinto, Paolo Rosso, and He?ctor Jime?nez-
Salazar. A self-enriching methodology for clus-
tering narrow domain short texts. Computer Jour-
nal, 54(7):1148?1165, 2011.
Philip Resnik. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, IJCAI?95, pages 448?453,
San Francisco, CA, USA, 1995.
Peter D. Turney. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the
12th European Conference on Machine Learning,
pages 491?502. Springer-Verlag, 2001.
Zhibiao Wu and Martha Stone Palmer. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, 1994.
153
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 154?159,
Dublin, Ireland, August 23-24, 2014.
BUAP: Polarity Classification of Short Texts
David Pinto1, Darnes Vilarin?o1, Saul Leo?n1, Miguel Jasso1,2, Cupertino Lucero2
1 Beneme?rita Universidad Auto?noma de Puebla
14 Sur y Av. San Claudio, CU, 72570
Puebla, Puebla, Me?xico
{dpinto,darnes,saul.leon}@cs.buap.mx
2 Universidad Tecnolo?gica de Izu?car de Matamoros
Prolongacio?n Reforma 168, Santiago Mihuacan, 74420
Izu?car de Matamoros, Puebla, Me?xico
migueljhdz18@yahoo.com.mx, cuper lucero@hotmail.com
Abstract
We report the results we obtained at the sub-
task B (Message Polarity Classification) of Se-
mEval 2014 Task 9. The features used for
representing the messages were basically tri-
grams of characters, trigrams of PoS and a
number of words selected by means of a graph
mining tool. Our approach performed slightly
below the overall average, except when a cor-
pus of tweets with sarcasm was evaluated,
in which we performed quite well obtaining
around 6% above the overall average.
1 Introduction
Analyzing polarity in texts is an important task that
may have various applications in real life. There ex-
ist plenty of tasks that may be benefited of computa-
tional procedures that automatically allow to detect
if the author intention has been to express himself as
a positive, negative, neutral or objective manner. Let
us consider, for instance, when a public figure (such
as a politician, celebrity, or business leader) would
like to investigate its reputation in public media. An-
other example would be to calculate the reputation
of a public or private institution. In any case, the
construction of methods for determining the polar-
ity of messages at Internet would help to investigate
their reputation.
In this paper, we present the results we obtained
when we carried out experiments for the subtask B
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
of Semeval 2014 Task 9, which was named ?Mes-
sage Polarity Classification?, and was defined as fol-
low: ?Given a message, decide whether the mes-
sage is of positive, negative, or neutral sentiment.
For messages conveying both a positive and nega-
tive sentiment, whichever is the stronger sentiment
should be chosen?.
The remaining of this paper is structured as fol-
lows. In Section 2 we present some related work
found at the literature with respect to the identifica-
tion of emotions in short texts such as twitter. Sec-
tion 3 presents the description of the features and
classification model used in our experiments. The
results obtained together with a discussion of these
results are given in Section 4. Finally, the conclu-
sions are given in Section 5.
2 Related Work
There exist a number of works in literature associ-
ated to the automatic identification of emotions in
Twitter, mainly due to the massification of this so-
cial network around the world and the easy manner
we can access to the Tweets from API?s provided by
Twitter itself. Some of these works have focused on
the contribution of some particular features, such as
Part of Speech (PoS) tags, emoticons, etc. on the
aforementioned task. In Agarwal et al. (2011), for
example, the a priori likelihood of each PoS is cal-
culated. They use up to 100 additional features that
include emoticons and a dictionary of positive and
negative words. They have reported a 60% of ac-
curacy in the task. On the other hand, in Mukher-
jee and Bhattacharyya (2012), a strategy based on
discursive relations, such as conectiveness and con-
154
ditionals, with low number of lexical resources is
proposed. These relations are integrated in classi-
cal models of representation like bag of words with
the aim of improving the accuracy values obtained
in the process of classification. The influence of se-
mantic operators such as modals and negations are
analyzed, in particular, the degree in which they af-
fect the emotion present in a given paragraph or sen-
tence.
One of the major advances obtained in the task
of sentiment analysis has been done in the frame-
work of the SemEval competition. In 2013, several
teams have participated with different approaches
Becker et al. (2013); Han et al. (2013); Chawla et al.
(2013); Balahur and Turchi (2013); Balage Filho
and Pardo (2013); Moreira et al. (2013); Reckman
et al. (2013); Tiantian et al. (2013); Marchand et al.
(2013); Clark and Wicentwoski (2013); Hamdan
et al. (2013); Mart??nez-Ca?mara et al. (2013); Lev-
allois (2013). Most of these works have contributed
in the mentioned task by proposing methods, tech-
niques for representing and classifying documents
towards the automatic classification of sentiment in
Tweets.
3 Description of the Presented Approach
We have employed a supervised approach based on
machine learning in which we construct a classifica-
tion model using the following general features ob-
tained from the training corpus.
1. Character trigrams
2. PoS tags trigrams
3. Significant Tweet words obtained by using a
graph mining tool known as SubDue
The description of how we calculated each feature
in order to construct a representation vector for each
message is given as follows.
The probability of each character trigram given
the polarity class, P (trigram|class), was cal-
culated in the training corpus. Thereafter, we
assigned a normalized probability to each sen-
tence polarity by combining the probability of
each character trigram of the sentence, i.e.,
?
|message|
i=1
log [P (trigram
i
|class)]. Since we
have four classes (?positive?,?negative?,?neutral?
and ?objective?), we have obtained four features for
the final vectorial representation of the message.
We then calculated other four features by per-
forming a similar calculation than the previous one,
but in this case, using the PoS tags of the message.
For this purpose, we used the Twitter NLP and Part-
of-Speech Tagging tool provided by the Carnegie
Mellon University (Owoputi et al., 2013). Since the
PoS tag given by this tool is basically a character,
then the same procedure can be applied.
We performed preliminary experiments by using
these eight features on a trial corpus, and we ob-
served that the results may be improved by select-
ing significant words that may not be discovered
by the statistical techniques used until now. So,
we decided to make use of techniques based on
graph mining for attempting to find those signifi-
cant words. In order to find them, we constructed a
graph representation for each message class (?pos-
itive?,?negative?,?neutral? and ?objective?), using
the training corpus. The manner we constructed
those graphs is shown as follows.
Formally, given a graph G = (V,E,L, f) with V
being the non-empty set of vertices, E ? V ?V the
edges, L the tag set, and f : E ? L, a function
that assigns a tag to a pair of associated vertices.
This graph-based representation attempt to capture
the sequence among the sentence words, so as the
sequence among their PoS tags with the aim of feed-
ing a graph mining tool which may extract relevant
features that may be further used for representing the
texts. Thus, the set V is constructed from the differ-
ent words and PoS of the target document.
In order to demonstrate the way we construct the
graph for each short text, consider the following
message: ?ooh i love you for posting this :-)?. The
associated graph representation to this message is
shown in Figure 1.
Once each paragraph is represented by means of
a graph, we apply a data mining algorithm in or-
der to find subgraphs from which we will be able
to find the significant words which will be, in our
case, basically, the nodes of these subgraphs. Sub-
due is a data mining tool widely used in structured
domains. This tool has been used for discovering
structured patterns in texts represented by means of
graphs Olmos et al. (2005). Subdue uses an eval-
uation model named ?Minimum encoding?, a tech-
155
Figure 1: Graph based message representation with words and their corresponding PoS tags
nique derived from the minimum description length
principle Rissanen (1989), in which t he best graph
sub-structures are chosen. The best subgraphs are
those that minimize the number of bits that repre-
sent the graph. In this case, the number of bits is
calculated consi dering the size of the graph adjan-
cency matrix. Thus, the best substructure is the one
that minimizes I(S) + I(G|S), where I(S) is the
number of bits required to describe the sub structure
S, and I(G|S) is the number of bits required to de-
scribe graph G after it has been compacted by the
substructure S.
By applying this procedure we obtained 597 sig-
nicant negative words, 445 positive words, 616 ob-
jective words and 925 positive words. For the final
representation vector we compiled the union of these
words, obtaining 1915 significant words. Therefore,
the total number of features for each message was
1,923.
We have used the training corpus provided at the
competition (Rosenthal et al., 2014), however, we
removed all those messsages tagged as the class
?objective-OR-neutral?, because all these messages
introduced noise to the classification process. In to-
tal, we constructed 5,217 vectors of message repre-
sentation which fed a support vector machine classi-
fier. We have used the SVM implementation of the
WEKA tool with default parameters for our exper-
iments (Hall et al., 2009). The obtained results are
shown in the next section.
4 Experimental Results
The test corpus was made up short texts (mes-
sages) categorized as: ?LiveJournal2014?,
?SMS2013?, ?Twitter2013?, ?Twitter2014? and
?Twitter2014Sarcasm?. A complete description of
the training and test datasets can be found at the
task description paper (Rosenthal et al., 2014).
In Table 1 we can see the results obtained at the
competition. Our approach performed in almost all
the cases slightly below to the overall average, ex-
cept when we processed the corpus of Twitter with
Sarcasm characteristics. We consider that two main
problems were the cause of this result: 1) The corpus
was very unbalanced and our approaches for allevi-
ating this problem were not sufficient, and 2) From
our particular point of view, there were a high differ-
ence between the vocabulary of the training and the
test corpus, thus, leading the classification model to
fail.
156
Table 1: Results obtained at the substask B of the Semeval 2014 Task 9
System LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm Average
NRC-Canada-B 74.84 70.28 70.75 69.85 58.16 68.78
CISUC KIS-B-late 74.46 65.90 67.56 67.95 55.49 66.27
coooolll-B 72.90 67.68 70.40 70.14 46.66 65.56
TeamX-B 69.44 57.36 72.12 70.96 56.50 65.28
RTRGO-B 72.20 67.51 69.10 69.95 47.09 65.17
AUEB-B 70.75 64.32 63.92 66.38 56.16 64.31
SWISS-CHOCOLATE-B 73.25 66.43 64.81 67.54 49.46 64.30
SentiKLUE-B 73.99 67.40 69.06 67.02 43.36 64.17
TUGAS-B 69.79 62.77 65.64 69.00 52.87 64.01
SAIL-B 69.34 56.98 66.80 67.77 57.26 63.63
senti.ue-B 71.39 59.34 67.34 63.81 55.31 63.44
Synalp-Empathic-B 71.75 62.54 63.65 67.43 51.06 63.29
Lt 3-B 68.56 64.78 65.56 65.47 47.76 62.43
UKPDIPF-B 71.92 60.56 60.65 63.77 54.59 62.30
AMI ERIC-B 65.32 60.29 70.09 66.55 48.19 62.09
ECNU-B 69.44 59.75 62.31 63.17 51.43 61.22
LyS-B 69.79 60.45 66.92 64.92 42.40 60.90
SU-FMI-B-late 68.24 61.67 60.96 63.62 48.34 60.57
NILC USP-B-twitter 69.02 61.35 65.39 63.94 42.06 60.35
CMU-Qatar-B-late 65.63 62.95 65.11 65.53 40.52 59.95
columbia nlp-B 68.79 59.84 64.60 65.42 40.02 59.73
CMUQ-Hybrid-B-late 65.14 61.75 63.22 62.71 40.95 58.75
Citius-B 62.40 57.69 62.53 61.92 41.00 57.11
KUNLPLab-B 63.77 55.89 58.12 61.72 44.60 56.82
USP Biocom-B 67.80 53.57 58.05 59.21 43.56 56.44
UPV-ELiRF-B 64.11 55.36 63.97 59.33 37.46 56.05
Rapanakis-B 59.71 54.02 58.52 63.01 44.69 55.99
DejaVu-B 64.69 55.57 57.43 57.02 42.46 55.43
GPLSI-B 57.32 46.63 57.49 56.06 53.90 54.28
Indian Inst of Tech-Patna-B 60.39 51.96 52.58 57.25 41.33 52.70
BUAP-B 53.94 44.27 56.85 55.76 51.52 52.47
SAP-RI-B 57.86 49.00 50.18 55.47 48.64 52.23
UMCC DLSI Sem 53.12 50.01 51.96 55.40 42.76 50.65
Alberta-B 52.38 49.05 53.85 52.06 40.40 49.55
SINAI-B 58.33 57.34 50.59 49.50 31.15 49.38
IBM EG-B 59.24 46.62 54.51 52.26 34.14 49.35
SU-sentilab-B-tweet 55.11 49.60 50.17 49.52 31.49 47.18
lsis lif-B 61.09 38.56 46.38 52.02 34.64 46.54
IITPatna-B 54.68 40.56 50.32 48.22 36.73 46.10
UMCC DLSI Graph-B 47.81 36.66 43.24 45.49 53.15 45.27
University-of-Warwick-B 39.60 29.50 39.17 45.56 39.77 38.72
DAEDALUS-B 40.83 40.86 36.57 33.03 28.96 36.05
Overall average 63.81 55.82 59.72 60.30 45.43 57.02
5 Conclusions
We have presented an approach for detecting mes-
sage polarity using basically three kind of features:
character trigrams, PoS tags trigrams and significant
words obtained by means of a graph mining tool.
The obtained results show that these features were
not sufficient for detecting the correct polarity of a
given message with high precision. We consider that
the unbalanced characteristic and the fact the vocab-
ulary changed significantly from the training to the
test corpus influenced the results we obtained at the
competition. However, a deep analysis we plan to
do to the datasets evaluated will allow us in the fu-
ture to find more accurate features for the message
polarity detection task.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen
Rambow, and Rebecca Passonneau. Sentiment
analysis of twitter data. In Proceedings of the
Workshop on Language in Social Media (LSM
2011), pages 30?38, Portland, Oregon, June 2011.
Pedro Balage Filho and Thiago Pardo. Nilc usp:
A hybrid system for sentiment analysis in twitter
messages. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
157
Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013),
pages 568?572, Atlanta, Georgia, USA, June
2013.
Alexandra Balahur and Marco Turchi. Improving
sentiment analysis in twitter using multilingual
machine translated data. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing RANLP 2013, pages 49?
55, Hissar, Bulgaria, September 2013. INCOMA
Ltd. Shoumen, BULGARIA.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. Avaya: Sentiment analysis on twitter
with self-training and polarity lexicon expansion.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages
333?340, Atlanta, Georgia, USA, June 2013.
Karan Chawla, Ankit Ramteke, and Pushpak Bhat-
tacharyya. Iitb-sentiment-analysts: Participation
in sentiment analysis in twitter semeval 2013 task.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages
495?500, Atlanta, Georgia, USA, June 2013.
Sam Clark and Rich Wicentwoski. Swatcs: Combin-
ing simple classifiers with estimated accuracy. In
Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 425?
429, Atlanta, Georgia, USA, June 2013.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Wit-
ten. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November
2009. ISSN 1931-0145.
Hussam Hamdan, Frederic Be?chet, and Patrice Bel-
lot. Experiments with dbpedia, wordnet and sen-
tiwordnet as resources for sentiment analysis in
micro-blogging. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), pages 455?459, Atlanta, Georgia,
USA, June 2013.
Qi Han, Junfei Guo, and Hinrich Schuetze. Codex:
Combining an svm classifier and character n-
gram language models for sentiment analysis on
twitter text. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), pages 520?524, Atlanta, Georgia,
USA, June 2013.
Clement Levallois. Umigon: sentiment analysis for
tweets based on terms lists and heuristics. In Sec-
ond Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 414?417,
Atlanta, Georgia, USA, June 2013.
Morgane Marchand, Alexandru Ginsca, Romaric
Besanc?on, and Olivier Mesnard. [lvic-limsi]: Us-
ing syntactic features and multi-polarity words for
sentiment analysis in twitter. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation
(SemEval 2013), pages 418?424, Atlanta, Geor-
gia, USA, June 2013.
Eugenio Mart??nez-Ca?mara, Arturo Montejo-Ra?ez,
M. Teresa Mart??n-Valdivia, and L. Alfonso Uren?a
Lo?pez. Sinai: Machine learning and emotion of
the crowd for sentiment analysis in microblogs. In
Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 402?
407, Atlanta, Georgia, USA, June 2013.
Silvio Moreira, Joa?o Filgueiras, Bruno Martins,
Francisco Couto, and Ma?rio J. Silva. Reac-
tion: A naive machine learning approach for sen-
timent classification. In Second Joint Confer-
ence on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation
(SemEval 2013), pages 490?494, Atlanta, Geor-
gia, USA, June 2013.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
158
Sentiment analysis in Twitter with lightweight
discourse analysis. In Proceedings of COLING
2012, pages 1847?1864, Mumbai, India, Decem-
ber 2012. The COLING 2012 Organizing Com-
mittee.
Ivan Olmos, Jesus A. Gonzalez, and Mauricio Os-
orio. Subgraph isomorphism detection using a
code based representation. In FLAIRS Confer-
ence, pages 474?479, 2005.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. Improved part-of-speech tagging for
online conversational text with word clusters.
In Proceedings of NAACL-HLT, pages 380?390,
2013.
Hilke Reckman, Cheyanne Baird, Jean Crawford,
Richard Crowell, Linnea Micciulla, Saratendu
Sethi, and Fruzsina Veress. teragram: Rule-based
detection of sentiment phrases using sas senti-
ment analysis. In Second Joint Conference on
Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (Se-
mEval 2013), pages 513?519, Atlanta, Georgia,
USA, June 2013.
Jorma Rissanen. Stochastic Complexity in Statis-
tical Inquiry Theory. World Scientific Publish-
ing Co., Inc., River Edge, NJ, USA, 1989. ISBN
981020311X.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. Semeval-2014 task 9: Senti-
ment analysis in twitter. In Proceedings of the 8th
International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, 2014.
Zhu Tiantian, Zhang Fangxi, and Man Lan. Ec-
nucs: A surface information based system de-
scription of sentiment analysis in twitter in the
semeval-2013 (task 2). In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation
(SemEval 2013), pages 408?413, Atlanta, Geor-
gia, USA, June 2013.
159
