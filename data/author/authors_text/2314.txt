Maytag: A multi-staged approach to identifying
complex events in textual data
Conrad Chang, Lisa Ferro, John Gibson, Janet Hitzeman, Suzi Lubar, Justin Palmer,
Sean Munson, Marc Vilain, and Benjamin Wellner
The MITRE Corporation
202 Burlington Rd.
Bedford, MA 01730 USA
contact: mbv@mitre.org (Vilain)
Abstract
We present a novel application of NLP
and text mining to the analysis of finan-
cial documents. In particular, we de-
scribe an implemented prototype, May-
tag, which combines information extrac-
tion and subject classification tools in an
interactive exploratory framework. We
present experimental results on their per-
formance, as tailored to the financial do-
main, and some forward-looking exten-
sions to the approach that enables users
to specify classifications on the fly.
1 Introduction
Our goal is to support the discovery of complex
events in text. By complex events, we mean
events that might be structured out of multiple
occurrences of other events, or that might occur
over a span of time. In financial analysis, the
domain that concerns us here, an example of
what we mean is the problem of understanding
corporate acquisition practices. To gauge a
company?s modus operandi in acquiring other
companies, it isn?t enough to know just that an
acquisition occurred, but it may also be impor-
tant to understand the degree to which it was
debt-leveraged, or whether it was performed
through reciprocal stock exchanges.
In other words, complex events are often
composed of multiple facets beyond the basic
event itself. One of our concerns is therefore to
enable end users to access complex events
through a combination of their possible facets.
Another key characteristic of rich domains
like financial analysis, is that facts and events are
subject to interpretation in context. To a finan-
cial analyst, it makes a difference whether a
multi-million-dollar loss occurs in the context of
recurring operations (a potentially chronic prob-
lem), or in the context of a one-time event, such
as a merger or layoff. A second concern is thus
to enable end users to interpret facts and events
through automated context assessment.
The route we have taken towards this end is to
model the domain of corporate finance through
an interactive suite of language processing tools.
Maytag, our prototype, makes the following
novel contribution. Rather than trying to model
complex events monolithically, we provide a
range of multi-purpose information extraction
and text classification methods, and allow the
end user to combine these interactively. Think
of it as Boolean queries where the query terms
are not keywords but extracted facts, events, en-
tities, and contextual text classifications.
2 The Maytag prototype
Figure 1, below, shows the Maytag prototype
in action. In this instance, the user is browsing a
particular document in the collection, the 2003
securities filings for 3M Corporation. The user
has imposed a context of interpretation by select-
ing the ?Legal matters? subject code, which
causes the browser to only retrieve those portions
of the document that were statistically identified
as pertaining to law suits. The user has also se-
lected retrieval based on extracted facts, in this
case monetary expenses greater than $10 million.
This in turn causes the browser to further restrict
retrieval to those portions of the document that
contain the appropriate linguistic expressions,
e.g., ?$73 million pre-tax charge.?
As the figure shows, the granularity of these
operations in our browser is that of the para-
graph, which strikes a reasonable compromise
between providing enough context to interpret
retrieval results, but not too much. It is also ef-
131
fective at enabling combination of query terms.
Whereas the original document contains 5161
paragraphs, the number of these that were tagged
with the ?Legal matters? code is 27, or .5 percent
of the overall document. Likewise, the query for
expenses greater than $10 million restricts the
return set to 26 paragraphs (.5 percent). The
conjunction of both queries yields a common
intersection of only 4 paragraphs, thus precisely
targeting .07 percent of the overall document.
Under the hood, Maytag consists of both an
on-line component and an off-line one. The on-
line part is a web-based GUI that is connected to
a relational database via CGI scripts (html,
JavaScript, and Python). The off-line part of the
system hosts the bulk of the linguistic and statis-
tical processing that creates document meta-data:
name tagging, relationship extraction, subject
identification, and the like. These processes are
applied to documents entering the text collection,
and the results are stored as meta-data tables.
The tables link the results of the off-line process-
ing to the paragraphs in which they were found,
thereby supporting the kind of extraction- and
classification-based retrieval shown in Figure 1.
3 Extraction in Maytag
As is common practice, Maytag approaches
extraction in stages. We begin with atomic
named entities, and then detect structured
entities, relationships, and events. To do so, we
rely on both rule-based and statistical means.
3.1 Named entities
In Maytag, we currently extract named entities
with a tried-but-true rule-based tagger based on
the legacy Alembic system (Vilain, 1999). Al-
though we?ve also developed more modern sta-
tistical methods (Burger et al 1999, Wellner &
Vilain, 2006), we do not currently have adequate
amounts of hand-marked financial data to train
these systems. We therefore found it more con-
venient to adapt the Alembic name tagger by
manual hill climbing. Because this tagger was
originally designed for a similar newswire task,
we were able to make the port using relatively
small amounts of training data. We relied on two
100+ page-long Securities filings (singly anno-
tated), one for training, and the other for test, on
which we achieve an accuracy of F=94.
We found several characteristics of our finan-
cial data to be especially challenging. The first is
the widespread presence of company name look-
alikes, by which we mean phrases like ?Health
Care Markets? or ?Business Services? that may
look like company names, but in fact denote
business segments or the like. To circumvent
this, we had to explicitly model non-names, in
effect creating a business segment tagger that
captures company name look-alikes and prevents
them from being tagged as companies.
Another challenging characteristic of these fi-
nancial reports is their length, commonly reach-
ing hundreds of pages. This poses a quandary
Figure 1: The Maytag interface
132
for the way we handle discourse effects. As with
most name taggers, we keep a ?found names? list
to compensate for the fact that a name may not
be clearly identified throughout the entire span of
the input text. This list allows the tagger to
propagate a name from clear identifying contexts
to non-identified occurrences elsewhere in the
discourse. In newswire, this strategy boosts re-
call at very little cost to precision, but the sheer
length of financial reports creates a dispropor-
tionate opportunity for found name lists to intro-
duce precision errors, and then propagate them.
3.2 Structured entities, relations, and events
Another way in which financial writing differs
from general news stories is the prevalence of
what we?ve called structured entities, i.e., name-
like entities that have key structural attributes.
The most common of these relate to money. In
financial writing, one doesn?t simply talk of
money: one talks of a loss, gain or expense, of
the business purpose associated therewith, and of
the time period in which it is incurred. Consider:
Worldwide expenses for environmental
compliance [were] $163 million in 2003.
To capture such cases as this, we?ve defined a
repertoire of structured entities. Fine-grained
distinctions about money are encoded as color of
money entities, with such attributes as their color
(in this case, an operating expense), time stamp,
and so forth. We also have structured entities for
expressions of stock shares, assets, and debt.
Finally, we?ve included a number of constructs
that are more properly understood as relations
(job title) or events (acquisitions).
3.3 Statistical training
Because we had no existing methods to address
financial events or relations, we took this oppor-
tunity to develop a trainable approach. Recent
work has begun to address relation and event
extraction through trainable means, chiefly SVM
classification (Zelenko et al 2003, Zhou et al
2005). The approach we?ve used here is classi-
fier-based as well, but relies on maximum en-
tropy modeling instead.
Most trainable approaches to event extraction
are entity-anchored: given a pair of relevant enti-
ties (e.g., a pair of companies), the object of the
endeavor is to identify the relation that holds be-
tween them (e.g., acquisition or subsidiary). We
turn this around: starting with the head of the
relation, we try to find the entities that fill its
constituent roles. This is, unavoidably, a
strongly lexicalized approach. To detect an
event such as a merger or acquisition, we start
from indicative head words, e.g., ?acquire,?
?purchases,? ?acquisition,? and the like.
The process proceeds in two stages. Once
we?ve scanned a text to find instances of our in-
dicator heads, we classify the heads to determine
whether their embedding sentence represents a
valid instance of the target concept. In the case
of acquisitions, this filtering stage eliminates
such non-acquisitions as the use of the word
?purchases? in ?the company purchases raw ma-
terials.? If a head passes this filter, we find the
fillers of its constituent roles through a second
classification stage
The role stage uses a shallow parser to chunk
the sentence, and considers the nominal chunks
and named entities as candidate role fillers. For
acquisition events, for example, these roles in-
clude the object of the acquisition, the buying
agent, the bought assets, the date of acquisition,
and so forth (a total of six roles). E.g.
In the fourth quarter of 2000 (WHEN), 3M
[AGENT] also acquired the multi-layer inte-
grated circuit packaging line [ASSETS] of
W.L. Gore and Associates [OBJECT].
The maximum entropy role classifier relies on
a range of feature types: the semantic type of the
phrase (for named entities), the phrase vocabu-
lary, the distance to the target head, and local
context (words and phrases).
Our initial evaluation of this approach has
given us encouraging first results. Based on a
hand-annotated corpus of acquisition events,
we?ve measured filtering performance at F=79,
and role assignment at F=84 for the critical case
of the object role. A more recent round of ex-
periments has produced considerably higher per-
formance, which we will report on later this year.
4 Subject Classification
Financial events with similar descriptions can
mean different things depending on where these
events appear in a document or in what context
they appear. We attempt to extract this important
contextual information using text classification
methods. We also use text classification methods
to help users to more quickly focus on an area
where interesting transactions exist in an interac-
tive environment. Specifically, we classify each
paragraph in our document collection into one of
several interested financial areas. Examples in-
clude: Accounting Rule Change, Acquisitions
and Mergers, Debt, Derivatives, Legal, etc.
133
4.1 Experiments
In our experiments, we picked 3 corporate an-
nual reports as the training and test document set.
Paragraphs from these 3 documents, which are
from 50 to 150 pages long, were annotated with
the types of financial transactions they are most
related to. Paragraphs that did not fall into a
category of interest were classified as ?other?.
The annotated paragraphs were divided into ran-
dom 4x4 test/training splits for this test. The
?other? category, due to its size, was sub-
sampled to the size of the next-largest category.
As in the work of Nigam et al(2002) or Lodhi
et al(2002), we performed a series of experi-
ments using maximum entropy and support vec-
tor machines. Besides including the words that
appeared in the paragraphs as features, we also
experimented with adding named entity expres-
sions (money, date, location, and organization),
removal of stop words, and stemming. In gen-
eral, each of these variations resulted in little dif-
ference compared with the baseline features con-
sisting of only the words in the paragraphs.
Overall results ranged from F-measures of 70-75
for more frequent categories down to above 30-
40 for categories appearing less frequently.
4.2 Online Learning
We have embedded our text classification
method into an online learning framework that
allows users to select text segments, specify
categories for those segments and subsequently
receive automatically classified paragraphs simi-
lar to those already identified. The highest con-
fidence paragraphs, as determined by the classi-
fier, are presented to the user for verification and
possible re-classification.
Figure 1, at the start of this paper, shows the
way this is implemented in the Maytag interface.
Checkboxes labeled pos and neg are provided
next to each displayed paragraph: by selecting
one or the other of these checkboxes, users indi-
cate whether the paragraph is to be treated as a
positive or a negative example of the category
they are elaborating. In our preliminary studies,
we were able to achieve the peak performance
(the highest F1 score) within the first 20 training
examples using 4 different categories.
5 Discussion and future work
The ability to combine a range of analytic
processing tools, and the ability to explore their
results interactively are the backbone of our ap-
proach. In this paper, we?ve covered the frame-
work of our Maytag prototype, and have looked
under its hood at our extraction and classification
methods, especially as they apply to financial
texts. Much new work is in the offing.
Many experiments are in progress now to as-
sess performance on other text types (financial
news), and to pin down performance on a wider
range of events, relations, and structured entities.
Another question we would like to address is
how best to manage the interaction between clas-
sification and extraction: a mutual feedback
process may well exist here.
We are also concerned with supporting finan-
cial analysis across multiple documents. This
has implications in the area of cross-document
coreference, and is also leading us to investigate
visual ways to define queries that go beyond the
paragraph and span many texts over many years.
Finally, we are hoping to conduct user studies
to validate our fundamental assumption. Indeed,
this work presupposes that interactive application
of multi-purpose classification and extraction
techniques can model complex events as well as
monolithic extraction tools ? laMUC.
Acknowledgements
This research was performed under a MITRE
Corporation sponsored research project.
References
Zhou, G., Su J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction.
Proc. of the 43rd ACL Conf, Ann Arbor, MI.
Nigam, K., Lafferty, J., and McCallum, A. 1999. Us-
ing maximum entropy for text classification. Proc.
of IJCAI ?99 Workshop on Information Filtering.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini,
and N., Watkins, C. 2002. Text classification using
string kernels. Journal of Machine Learning Re-
search, Vol. 2, pp. 419-444.
Vilain, M. and Day, D. 1996. Finite-state Phrase Pars-
ing by Rule Sequences, Proc. of COLING-96.
Vilain, M. 1999. Inferential information extraction.
In Pazienza, M.T. & Basili, R., Information Ex-
traction. Springer Verlag.
Wellner, B., and Vilain, M. (2006) Leveraging ma-
chine readable dictionaries in discriminative se-
quence models. Proc. of LREC 2006 (to appear).
Zelenko D., Aone C. and Richardella. 2003. Kernel
methods for relation extraction. Journal of Ma-
chine Learning Research. pp1083-1106.
134
Guidelines for Annotating Temporal Information
Inderjeet Mani, George Wilson
The MITRE Corporation, W640
11493 Sunset Hills Road
Reston, Virginia 20190-5214, USA
+1-703-883-6149
imani@mitre.org
Lisa Ferro
The MITRE Corporation, K329
202 Burlington Road, Rte. 62
Bedford, MA 01730-1420, USA
+1-781-271-5875
lferro@mitre.org
Beth Sundheim
SPAWAR Systems Center, D44208
53140 Gatchell Road, Room 424B
Sand Diego, CA 92152-7420, USA
+1-619-553-4195
sundheim@spawar.navy.mil
ABSTRACT
This paper introduces a set of guidelines for annotating time
expressions with a canonicalized representation of the times they
refer to. Applications that can benefit from such an annotated
corpus include information extraction (e.g., normalizing temporal
references for database entry), question answering (answering
?when? questions), summarization (temporally ordering
information), machine translation (translating and normalizing
temporal references), and information visualization (viewing
event chronologies).
Keywords
Annotation, temporal information, semantics, ISO-8601.
1.
 
INTRODUCTION
The processing of temporal information poses numerous
challenges for NLP. Progress on these challenges may be
accelerated through the use of corpus-based methods. This paper
introduces a set of guidelines for annotating time expressions with
a canonicalized representation of the times they refer to.
Applications that can benefit from such an annotated corpus
include information extraction (e.g., normalizing temporal
references for database entry), question answering (answering
?when? questions), summarization (temporally ordering
information), machine translation (translating and normalizing
temporal references), and information visualization (viewing
event chronologies).
Our annotation scheme, described in detail in [Ferro et al 2000],
has several novel features:
?
 
It goes well beyond the one used in the Message
Understanding Conference [MUC7 1998], not only in terms
of the range of expressions that are flagged, but, also, more
importantly, in terms of representing and normalizing the
time values that are communicated by the expressions.
?
 
In addition to handling fully-specified time expressions [e.g.,
September 3rd, 1997), it also handles context-dependent
expressions. This is significant because of the ubiquity of
context-dependent time expressions; a recent corpus study
[Mani and Wilson 2000] revealed that more than two-thirds
of time expressions in print and broadcast news were
context-dependent ones. The context can be local (within the
same sentence), e.g., In 1995, the months of June and July
were devilishly hot, or global (outside the sentence), e.g., The
hostages were beheaded that afternoon. A subclass of these
context-dependent expressions are ?indexical? expressions,
which require knowing when the speaker is speaking to
determine the intended time value, e.g., now, today,
yesterday, tomorrow, next Tuesday, two weeks ago, etc.
Our scheme differs from the recent scheme of [Setzer and
Gaizauskas 2000] in terms of our in-depth focus on
representations for the values of specific classes of time
expressions, and in the application of our scheme to a variety of
different genres, including print news, broadcast news, and
meeting scheduling dialogs.
The annotation scheme has been designed to meet the
following criteria:
Simplicity with precision: We have tried to keep the scheme
simple enough to be executed confidently by humans, and yet
precise enough for use in various natural language processing
tasks.
Naturalness: We assume that the annotation scheme should reflect
those distinctions that a human could be expected to reliably
annotate, rather than reflecting an artificially-defined smaller set
of distinctions that automated systems might be expected to make.
This means that some aspects of the annotation will be well
beyond the reach of current systems.
Expressiveness:  The guidelines require that one specify time
values as fully as possible, within the bounds of what can be
confidently inferred by annotators. The use of ?parameters? and
the representation of ?granularity? (described below) are tools to
help ensure this.
Reproducibility: In addition to leveraging the [ISO-8601 1997]
format for representing time values, we have tried to ensure
consistency among annotators by providing an example-based
approach, with each guideline closely tied to specific examples.
While the representation accommodates both points and intervals,
the guidelines are aimed at using the point representation to the
extent possible, further helping enforce consistency.
The annotation process is decomposed into two steps: flagging a
temporal expression in a document, and identifying the time value
that the expression designates, or that the speaker intends for it to
designate. The flagging of temporal expressions is restricted to
those temporal expressions which contain a reserved time word
used in a temporal sense, called a ?lexical trigger?, which include
words like day, week, weekend, now, Monday, current, future, etc.
2. SEMANTIC DISTINCTIONS
Three different kinds of time values are represented: points in
time (answering the question ?when??), durations (answering
?how long??), and frequencies (answering ?how often??).
Points in time are calendar dates and times-of-day, or a
combination of both, e.g., Monday 3 pm, Monday next week, a
Friday, early Tuesday morning, the weekend. These are all
represented with values (the tag attribute VAL) in the ISO format,
which allows for representation of date of the month, month of the
year, day of the week, week of the year, and time of day, e.g.,
<TIMEX2 VAL=?2000-11-29-T16:30?>4:30 p.m. yesterday
afternoon</TIMEX2>.
Durations also use the ISO format to represent a period of time.
When only the period of time is known, the value is represented
as a duration, e.g.,
<TIMEX2 VAL=?P3D?>a three-day</TIMEX2> visit.
Frequencies reference sets of time points rather than particular
points.   SET and GRANULARITY attributes are used for such
expressions, with the PERIODICITY attribute being used for
regularly recurring times, e.g., <TIMEX2 VAL=?XXXX-WXX-2?
SET=?YES? PERIODICITY=?F1W?
GRANULARITY=?G1D?>every Tuesday</TIMEX2>. Here
?F1W? means frequency of once a week, and the granularity
?G1D? means the set members are counted in day-sized units.
The annotation scheme also addresses several semantic problems
characteristic of temporal expressions:
Fuzzy boundaries. Expressions like Saturday morning and Fall
are fuzzy in their intended value with respect to when the time
period starts and ends; the early 60?s is fuzzy as to which part of
the 1960?s is included. Our format for representing time values
includes parameters such as FA (for Fall), EARLY (for early,
etc.), PRESENT_REF (for today, current, etc.), among others.
For example, we have <TIMEX2 VAL=?1990-SU?>Summer of
1990</TIMEX2>. Fuzziness in modifiers is also represented, e.g.,
<TIMEX2 VAL=?1990? MOD=?BEFORE?>more than a
decade ago</TIMEX2>. The intent here is that a given
application may choose to assign specific values to these
parameters if desired; the guidelines themselves don?t dictate the
specific values.
Non-Specificity. Our scheme directs the annotator to represent the
values, where possible, of temporal expressions that do not
indicate a specific time.  These non-specific expressions include
generics, which state a generalization or regularity of some kind,
e.g., <TIMEX2 VAL=?XXXX-04?
NON_SPECIFIC=?YES?>April</TIMEX2> is usually wet, and
non-specific indefinites, like <TIMEX2 VAL="1999-06-XX"
NON_SPECIFIC="YES? GRANULARITY="G1D">a sunny day
in <TIMEX2 VAL="1999-06">June</TIMEX2></TIMEX2>.
3. USEFULNESS
Based on the guidelines, we have annotated a small reference
corpus, consisting of 35,000 words of newspaper text and 78,000
words of broadcast news [TDT2 1999]. Portions of this corpus
were used to train and evaluate a time tagger with a reported F-
measure of .83 [Mani and Wilson 2000]; the corpus has also been
used to order events for summarization.
Others have used temporal annotation schemes for the much more
constrained domain of meeting scheduling, e.g., [Wiebe et al
1998], [Alexandersson et al 1997], [Busemann et al 1997]; our
scheme has been applied to such domains as well. In particular,
we have begun annotation of the ?Enthusiast? corpus of meeting
scheduling dialogs used at CMU and by [Wiebe et al 1998]. Only
minor revisions to the guidelines? rules for tag extent have so far
been required for these dialogs.
This annotation scheme is also being leveraged in the Automatic
Content Extraction (ACE) program of the U.S. Department of
Defense, whose focus is on extraction of time-dependent relations
between pairs of ?entities? (persons, organizations, etc.).
Finally, initial feedback from Machine Translation system
grammar writers [Levin, personal communication] indicates that
the guidelines were found to be useful in extending an existing
interlingua for machine translation.
4. CONCLUSION
The annotation scheme we have developed appears applicable to a
wide variety of different genres of text. The semantic
representation used is also highly language-independent. In
Spring 2001, we will be embarking on a large-scale annotation
effort using a merged corpus consisting of Enthusiast data as well
as additional TDT2 data (inter-annotator agreement will also be
measured then). An initial annotation exercise carried out on a
sample of this merged corpus by 20 linguistics students using our
guidelines has been encouraging, with 12 of the students
following the guidelines in a satisfactory manner. In the future, we
expect to extend this scheme to multilingual corpora.
5. ACKNOWLEDGMENTS
Our thanks to Lynn Carlson (Department of Defense), Lori Levin
(Carnegie Mellon University), and Janyce Wiebe (University of
Pittsburgh) for providing the Enthusiast corpus to us.
6. REFERENCES
[1] Alexandersson, J., Riethinger, N. and Maier, E.
Insights into the Dialogue Processing of VERBMOBIL.
Proceedings of the Fifth Conference on Applied Natural
Language Processing, 1997, 33-40.
[2] Busemann, S., Decleck, T., Diagne, A. K., Dini,
L., Klein, J. and Schmeier, S. Natural Language Dialogue
Service for Appointment Scheduling Agents. Proceedings of
the Fifth Conference on Applied Natural Language
Processing, 1997, 25-32.
[3] Ferro, L., Mani, I., Sundheim, B., and Wilson, G.
TIDES Temporal Annotation Guidelines. Draft Version
1.0. MITRE Technical Report MTR 00W0000094, October
2000.
[4] ISO-8601 ftp://ftp.qsl.net/pub/g1smd/8601v03.pdf
1997.
 [5] Mani, I. and Wilson, G. Robust Temporal
Processing of News, Proceedings of the ACL'2000
Conference, 3-6 October 2000, Hong Kong.
[6] MUC-7. Proceedings of the Seventh Message
Understanding Conference, DARPA. 1998.
[7] Setzer, A. and Gaizauskas, R. Annotating Events
and Temporal Information in Newswire Texts. Proceedings
of the Second International Conference On Language
Resources And Evaluation (LREC-2000), Athens, Greece,
31 May- 2 June 2000.
[8] TDT2
http://morph.ldc.upenn.edu/Catalog/LDC99T37.html 1999
[9] Wiebe,  J. M., O?Hara, T. P., Ohrstrom-Sandgren,
T. and McKeever, K. J. An Empirical Approach to
Temporal Reference Resolution. Journal of Artificial
Intelligence Research, 9, 1998, pp. 247-293.
Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
A Probabilistic Rasch Analysis of Question Answering Evaluations 
 
Rense Lange 
Integrated Knowledge Systems  
rlange@iknowsys.org  
Warren R. Greiff 
The MITRE Corporation 
greiff@mitre.org  
Juan Moran 
University of Illinois, Urbana-Champaign 
jmoran@ncsa.uiuc.edu 
Lisa Ferro 
The MITRE Corporation 
lferro@mitre.org 
 
 
Abstract 
The field of Psychometrics routinely grapples 
with the question of what it means to measure 
the inherent ability of an organism to perform 
a given task, and for the last forty years, the 
field has increasingly relied on probabilistic 
methods such as the Rasch model for test con-
struction and the analysis of test results.  Be-
cause the underlying issues of measuring 
ability apply to human language technologies 
as well, such probabilistic methods can be ad-
vantageously applied to the evaluation of 
those technologies. To test this claim, Rasch 
measurement was applied to the results of 67 
systems participating in the Question Answer-
ing track of the 2002 Text REtrieval Confer-
ence (TREC) competition. Satisfactory model 
fit was obtained, and the paper illustrates the 
theoretical and practical strengths of Rasch 
scaling for evaluating systems as well as ques-
tions. Most important, simulations indicate 
that a test invariant metric can be defined by 
carrying forward 20 to 50 equating questions, 
thus placing the yearly results on a common 
scale. 
1 Introduction 
For a number of years, objective evaluation of state-of-
the-art computational systems on realistic language 
processing tasks has been a driving force in the advance 
of Human Language Technology (HLT). Often, such 
evaluations are based on the use of simple sum-scores 
(i.e., the number of correct answers) and derivatives 
thereof (e.g., percentages), or on ad-hoc ways to rank or 
order system responses according to their correctness. 
Unfortunately, research in other areas indicates that 
such approaches rarely yield a cumulative body of 
knowledge, thereby complicating theory formation and 
practical decision making alike. In fact, although it is 
often taken for granted that sums or percentages ade-
quately reflect systems? performance, this assumption 
does not agree with many models currently used in edu-
cational testing (cf., Hambleton and Swaminathan, 
1985; Stout, 2002). To address this situation, we present 
the use of Rasch (1960/1980) measurement to the HLT 
research community, in general, and to the Question 
Answering (QA) research community, in particular.  
Rasch measurement has evolved over the last forty 
years to rigorously quantify performance aspects in such 
diverse areas as educational testing, cognitive develop-
ment, moral judgment, eating disorders (see e.g., Bond 
and Fox, 2001), as well as olfactory screening for Alz-
heimer?s disease (Lange et al, 2002) and model glider 
competitions (Lange, 2003). In each case, the major 
contribution of Rasch measurement is to decompose 
performance into two additive sources: the difficulty of 
the task and the ability of the person or system perform-
ing this task. While Rasch measurement is new to the 
evaluation of the performance of HLT systems, we in-
tend to demonstrate that this approach applies here as 
well, and that it potentially provides significant advan-
tages over traditional evaluation approaches.  
Our principal theoretical argument in favor of 
Rasch modeling is that the decomposition of perform-
ance into task difficulty and system ability creates the 
potential for formulating detailed and testable hypothe-
ses in other areas of language technology.  For QA, the 
existence of a well-defined, precise, mathematical for-
mulation of question difficulty and system ability can 
provide the basis for the study of the dimensions inher-
ent in the answering task, the formal characterization of 
questions, and the methodical analysis of the strengths 
and weaknesses of competing algorithmic approaches. 
As Bond and Fox (2001, p. 3) explain: ?The goal is to 
create abstractions that transcend the raw data, just as in 
the physical sciences, so that inferences can be made 
about constructs rather than mere descriptions about raw 
data.? Researchers are then in a position to formulate 
 initial theories, validate the consequences of theories on 
real data, refine theories in light of empirical data, and 
follow up with revised experimentation in a dialectic 
process that forms the essence of scientific discovery. 
Rasch modeling offers a number of direct practical 
advantages as well.  Among these are: 
? Quantification of question difficulty and system 
ability on a single scale with a common metric. 
? Support for the creation of tailor-made questions 
and the compilation of questions that suit well-
defined evaluation objectives. 
? Equating (calibration) of distinct question corpora 
so that systems participating in distinct evaluation 
cycles can be directly compared. 
? Assessment of the degree to which independent 
evaluations assess the same system abilities. 
? Availability of rigorous statistical techniques for 
the following: 
- analysis of fit of the data produced from systems? 
performance to the Rasch modeling assumptions; 
- identification of individual systems whose per-
formance behavior does not conform to the per-
formance patterns of the population as a whole; 
- identification of individual test questions that ap-
pear to be testing facets distinct from those evalu-
ated by the test as a whole; 
- assessment of the reliability of the test ? that is, 
the degree to which we can expect estimates of 
systems? abilities to be replicated if these systems 
are given another test of equivalent questions; 
- identification of unmodeled sources of variation 
in the data through a variety of methods, includ-
ing bias tests and analysis of residual terms. 
The remainder of the paper is organized as follows.  
First, we present in section 2 the basic concepts of 
Rasch modeling.  We continue in section 3 with an ap-
plication of Rasch modeling to the data resulting from 
the QA track of the 2002 Text REtrieval Conference 
(TREC) competition.  We fit the model to the data, ana-
lyze the resulting fit, and demonstrate some of the bene-
fits that can be derived from this approach. In section 4 
we present simulation results on test equating. Finally, 
we conclude with a summary of our findings and pre-
sent ideas for continuing research into the application of 
Rasch models to technology development and scientific 
theory formation in the various fields of human lan-
guage processing. 
2 The Rasch Model for Binary Data 
For binary results, Rasch?s (1960/1980) measurement 
requires that the outcome of an encounter between com-
puter systems (1, ?, s, ?, ns) and questions (1, ?, q, ?, 
nq) should depend solely on the differences between 
these systems? abilities (Ss) and the questions? difficul-
ties (Qq). Together with mild and standard scaling as-
sumptions, the preceding implies that:  
11 ??+= )e(P sq SQsq              (1) 
In a QA context, Psq is the probability that a system with 
the ability Ss will answer a question with difficulty Qq 
correctly. For a rigorous derivation of Equation 1 and an 
overview of the assumptions involved, we refer the 
reader to work by Fisher (1995). Fisher also proves that 
sum-scores (and hence percentages of correct answers) 
are sufficient performance statistics if and only if the 
assumptions of the Rasch model are satisfied. 
10 8 6 4 2 0 2 4 6 8
0
0.25
0.5
0.75
1
Latent Dimension (Logits)
P
(c
or
re
ct
)
0.5
5? 2
 
Figure 1.  Three Sample Rasch Response 
Curves 
The binary Rasch model has several interesting 
properties. First, as is illustrated by the three solid lines 
in Figure 1, Equation 1 defines a set of non-intersecting 
logistic response-curves such that Psq = 0.5 whenever Ss 
= Qq. In the following, such points are also referred to 
as question?s locations. For instance, the locations of the 
three questions depicted in Figure 1 are -5, 0, and 2. 
Second, for each pair of systems i and j with Si > Sj, for 
any question q, system i has a better chance of respond-
ing correctly than system j, i.e., Piq > Pjq. Thus, the 
questions that are answered correctly by less capable 
systems always form a probabilistic subset of those an-
swered correctly by more capable systems. Third, restat-
ing Equation 1 as is shown below highlights that the 
question and system parameters are additive and ex-
pressed in a common metric: 
qsP
P
QS)ln(
sq
sq ?=?1  (2) 
Given the left-hand side of Equation 2, this metric?s 
units are called Logits. Note that this equation further 
implies that Ss and Qq are determined up to an additive 
constant only (i.e., their common origin is arbitrary).  
Finally, efficient maximum-likelihood procedures 
exist to estimate Ss and Qq independently, together with 
their respective standard errors SEs and SEq (see e.g., 
Wright and Stone, 1979). These procedures do not re-
quire any assumptions about the magnitudes or the dis-
tribution of the Ss in order to estimate the Qq, and vice-
 versa.  Accordingly, systems? abilities can be deter-
mined in a ?question free? fashion, as different sets of 
questions from the same pool will yield statistically 
equivalent Ss estimates. Analogously, questions? loca-
tions can be estimated in a ?system free? fashion, as 
similarly spaced Qq estimates should be obtained across 
different samples of systems. In this paper, the model 
parameters, together with their errors of estimate, will 
be computed via the Winsteps Rasch scaling software 
(Linacre, 2003).  
 
0.0 0.5 1.0 1.5 2.0
Question Outfit (Mean-Square)
-4
-2
0
2
Q
ue
st
io
n 
lo
ca
tio
ns
 Q
q 
(L
og
its
) -
 Q
ue
st
io
n 
D
iff
ic
ul
ty
2. Questions w ith Outfit > 2.0 are show n as X
1. The size of the dots is proportional to SEq
Note:
3. Symbols have been slightly jittered along the
E
as
y
H
ar
d
X-axis to reveal overlapping points
 
Figure 2.  Questions by Qq, Outfitq, and SEq
3 Analysis of TREC Evaluation Data 
We used the results from the Question Answering track 
of the 2002 TREC competition to test the feasibility of 
applying Rasch modeling to QA evaluation. Sixty-seven 
systems participated, and answered 500 questions by 
returning a single precise response extracted from a 3-
gigabyte corpus of texts. While the NIST judges as-
sessed each answer as correct, incorrect, non-exact, or 
unsupported, we created binary responses by treating 
each of these last three assessments as incorrect. Ten 
questions were excluded from all analyses, as these 
were not answered correctly by any system.1 The final 
                                                          
1 When all respondents answer some question q correctly (or 
data set thus consisted of 67 systems? responses to 490 
questions.  
 
0.0 0.5 1.0 1.5 2.0
System Outfit (Mean-Square)
-4
-2
0
2
S
ys
te
m
 lo
ca
tio
ns
 S
s 
(L
og
its
) -
 S
ys
te
m
 C
ap
ab
ili
ty Outfit = 2.68
Lo
w
 A
bi
lit
y
H
ig
h 
A
bi
lit
y
Note:
1. The size of the dots is proportional to SEs
2. The circled dot is displaced to fit the graph
 
Figure 3.  Systems by Ss, Outfits, and SEs
3.1 Question Difficulty and System Ability 
Maximum-likelihood estimates of the questions? diffi-
culty and the systems? abilities were computed via Win-
steps. Figure 2 displays the results associated with the 
questions, whereas Figure 3 addresses the systems. Each 
dot in these displays corresponds to one question or one 
system.  For questions, the Y-value gives the estimate of 
the questions? difficulty (i.e., Qq); for systems, the Y-
value reflects the estimate of systems? ability (Ss).  For 
questions, lower values correspond to easier questions, 
while higher values to difficult questions.  For systems, 
higher values correspond to greater ability. As is cus-
tomary, the mean difficulty of the questions is set at 
zero, thereby implicitly fixing the origin of the esti-
mated system abilities at -1.98. As was noted earlier, a 
                                                                                           
incorrectly), the parameter Qq cannot be estimated. Note that 
raw-score approaches implicitly ignore such questions as well 
since including them does not affect the order of systems? 
?number right.? Of course, by changing the denominator, 
percentages of right or wrong questions are affected.  
 constant value can be added to each Qq and Ss without 
thereby affecting the validity of the results.  The X-axes 
of Figures 2 and 3 refer to the quality of fit, as described 
in section 3.3 below. 
As an example, consider a question with difficulty 
level -2. This means that a system whose ability level is 
-2 has a probability of .5 (odds=1) of getting this ques-
tion correct. The odds of a system with ability of -1 get-
ting this same question correct would increase by a 
factor2 of 2.72, thus increasing the probability of a cor-
rect answer to Psq = 2.72/(1+2.72) = .73. For a system 
at ability level 0, the odds increase by another factor of 
2.72 to 7.39, giving a probability of .88. On the other 
hand, a system with an ability of -3, would have the 
even odds decrease by a factor of 2.72 to .369, yielding 
Psq = .27. In other words, increasing (decreasing) ques-
tions? difficulties or decreasing (increasing) systems? 
abilities by the same amounts affects the log-odds in the 
same fashion. The preceding thus illustrates that ques-
tion difficulty and system ability have additive proper-
ties on the log-odds, or, Logit, scale.3   
The smoothed densities in Figure 4 summarize the 
locations of the 490 questions (dotted distribution) and 
the 67 systems (solid).  It can be seen that question dif-
ficulties range approximately from -3 to +3, and that 
most questions fall in a region about -1.  Systems? abili-
ties mostly cover a lower range such that the questions? 
locations (MeanQ = 0 Logits) far exceed those of the 
systems (MeanS = -1.98 Logits). In other words, most 
questions are very hard for these systems. The vast ma-
jority of systems (those located near -1 or below) have 
only a small chance (below 15%) of answering a sig-
nificant portion of the questions (those located above 1), 
and an even smaller chance (below 5%) on a non-
negligible number of questions (those above 2). Of 
those systems, a large portion (those at -2 or below) will 
have even less of a chance on these questions.   
-5 -3 -1 1 3 5
Rasch Dimension (Logits)
0.0
0.1
0.2
0.3
0.4
0.5
D
en
si
ty
 (S
 a
nd
 Q
)
0.0
0.2
0.4
0.6
0.8
1.0
S
E
Q
 a
nd
 S
E
S
SES
SEQ
Question Density
System Density
 
Figure 4. Smoothed System and Question Location 
Densities 
                                                          
                                                          
2 The value of e, since we are working with natural logarithms.   
3 Note that a number of measures used in the physical sciences 
likewise achieve additivity by adopting a log scale. 
3.2 Standard Error of Estimate 
The two U-shaped curves in Figure 4 reflect the esti-
mates of error, SEq for questions (dotted curve) and SEs 
for systems (solid curve), as a function of their esti-
mated locations (X-axis). As is also reflected by the size 
of the dots in Figure 3, it can be seen that SEs is smaller 
for intermediate and high performing systems (i.e., Ss 
between -3 and 1 Logits) than for low performing sys-
tems (Ss < -3 Logits). This pattern suits the ?horse-race? 
nature of the TREC evaluation well since the top per-
forming systems are assessed with nearly optimal preci-
sion. While the most capable system shows somewhat 
greater SEs, calculation shows its performance is still 
significantly higher than that of the runner-up (z = 
10.64, p < .001).  
Figure 4 further shows that SEq increases dramati-
cally beyond -1 Logits (this is also reflected in the size 
of the dots in Figure 2). For instance, the standard error 
of estimate SEq exceeds 0.5 Logits for questions with Qq 
> 1 Logit. Thus, the locations of the hardest questions 
are known with very poor precision only. 
3.3 Question and System Fit 
According to the Rasch model, a system, A, with mid-
dling performance is expected to perform well on the 
easier questions and poorly on the harder questions.  
However, it is possible that some system, B, achieved 
the same score on the test by doing poorly on the easy 
questions and well on the harder questions. While the 
behavior of system A agrees with the model, system B 
does not. Accordingly, the fit of system B is said to be 
poor as this system contradicts the knowledge embodied 
in the data as a whole. Analogous comments can be 
made with respect to questions. Rasch modeling formal-
izes the preceding account in a statistical fashion. In 
particular, for each response to Question q by System s, 
Equation 1 allows the computation of a standardized 
residual zsq, which is the difference between an observed 
datum (i.e., 0 or 1) and the probability estimate Psq after 
division by its standard deviation. Since the zsq follow 
an approximately normal distribution, unexpected re-
sults (e.g., |zsq|>3) are easily identified. The overall fit 
for systems (across questions) and for questions (across 
systems) is quantified by their Outfit.4 For instance, for 
System s:  
)n/(zOutfit q
q
sqs 1
2 ??=  (3) 
Since the summed z2sq in Equation 3 define a ?2 statistic 
with expected value nq ? 1, the Outfit statistic ranges 
4 Additionally, systems? (or questions?) ?Infit? statistic is de-
fined by weighting the z2sq contributions inversely to their 
distance to the contributing questions (or systems). As such, 
Infit statistics are less sensitive to outlying observations. Since 
this paper focuses on overall model fit, Infit statistics are not 
reported.  
 from 0 to ?, with an expected value of 1. As a rule of 
thumb, for rather small samples such as the present, 
Outfit values in the range 0.6 to 1.6 are considered as 
being within the expected range of variation.  
Figure 2 shows 46 questions whose Outfit exceeds 
1.6 (those to the right of the rightmost dashed vertical 
line) and the Outfit values of 24 of these exceed 2.0 
(shown in the graph by Xs, plotted at the right with hori-
zontal jitter). These are questions on which low per-
forming systems performed surprisingly well, and/or 
high performing systems performed unexpectedly 
poorly. Thus, there is a clear indication that these ques-
tions have characteristics that differentiate them from 
typical questions. Such questions are worthy of individ-
ual attention by the system developers.  
Questions and systems with uncharacteristically 
small Outfit values are of interest as well. For instance, 
in the present context it seems plausible that some ques-
tions simply cannot be answered by systems lacking 
certain capabilities (e.g., pronominal anaphora resolu-
tion, acronym expansion, temporal phrase recognition), 
while such questions are easily answered by systems 
that possess such capabilities. We might find that these 
questions would be answered by the vast majority, if not 
all, of the high performing systems and very few if any 
of the low ability systems. The estimated fit would be 
much better (small Outfit) than expected by chance. 
Again, the identification and analysis of such overfitting 
questions and, similiarly, underfitting systems may 
greatly enhance our understanding of both. 
3.4 Example of System with large Outfit 
Note that Figure 3 above shows that the best performing 
system also exhibits the largest Outfit (2.68), and we 
investigated this system?s residuals in detail. Table 1 
indicates that this system failed (Datum = 0) on eight 
questions (q) where its modeled probability of success 
was very high (Psq > 0.98). Thus, the misfit results from 
this system?s failure to answer correctly questions that 
proved quite easy for most other systems. 
q Qq Datum Psq Residual z 
1411 -1.51 0 0.98 -0.98 -7.39 
1418 -1.96 0 0.99 -0.99 -9.26 
1465 -1.74 0 0.99 -0.99 -8.28 
1672 -1.59 0 0.98 -0.98 -7.67 
1671 -1.51 0 0.98 -0.98 -7.39 
1686 -2.11 0 0.99 -0.99 -9.97 
1697 -1.89 0 0.99 -0.99 -8.92 
1841 -1.66 0 0.98 -0.98 -7.97 
 
Table 1. Misfit Diagnosis of Best Performing Sys-
tem 
These are the eight questions listed in Table 1: 
1411 What Spanish explorer discovered the Mississippi 
River? 
1418 When was the Rosenberg trial? 
1465 What company makes Bentley cars? 
1642 What do you call a baby sloth? 
1671 Where is Big Ben? 
1686 Who defeated the Spanish armada? 
1697 Where is the Statue of Liberty? 
1841 What?s the final line in the Edgar Allen Poe poem 
?The Raven?? 
This situation should be highly relevant to the sys-
tem?s developers. Informally speaking, the best system 
studied here ?should have gotten these questions right,? 
and it might thus prove fruitful to determine exactly 
why the system failed. Even if no obvious mistakes can 
be identified, doing so could reveal strategies for system 
improvement by focusing on seemingly ?easy? issues 
first. Alternatively, it might turn out that precisely those 
aspects of the system that enable it do so well overall 
also cause it to falter on the easier questions.  Ascertain-
ing this might or might not be of help to the system?s 
designers, but it would certainly foster the development 
of a scientific theory of automatic question answering. 
3.5 Impact of Misfit 
The existence of misfitting entities raises the possibility 
that the estimated Rasch system abilities are distorted by 
the question and system misfit. We therefore recom-
puted systems? locations by iteratively removing the 
worst fitting questions until 372 questions with Outfitq< 
1.6 remained. In support of the robustness of the Rasch 
model, Figure 5 shows that the correlation between the 
two sets of estimates is nearly perfect (r = 0.99), indi-
cating that the original and the ?purified? questions pro-
duce essentially equivalent system evaluations. Thus, 
the observed misfit had negligible effect on the system 
parameter estimates. 
-4 -2 0 2
Using best 372 fitting items only (Logits)
-4
-2
0
2
U
si
ng
 a
ll 
ite
m
s 
(L
og
its
)
r = 0.99
 
Figure 5.  Effect of Removing Misfitting Questions 
on the Estimated System Abilities Ss
 4 Test Equating Simulation 
A major motivation for introducing Rasch models in 
educational assessment was that this allows for the cali-
bration, or equating, of different tests based on a limited 
set of common (i.e., repeated) questions. The purpose of 
equating is to achieve equivalent test scores across dif-
ferent test sets. Thus, equating opens the door to cali-
brating the difficulty of questions and the performance 
of systems across the test sets used in different years. 
Since appropriate data from different years are 
lacking, a simulation study was performed based on 
different subsets of the 490 available questions. We 
show how system abilities can be expressed in the same 
metric, even though systems are evaluated with a com-
pletely different set of questions. To rule out the possi-
bility that such a correspondence might come about by 
chance (e.g., equally difficult sets of questions might 
accidentally be produced), a worst-case scenario is used. 
The simulation also provides a powerful means to dem-
onstrate the superiority of the Rasch Logit metric com-
pared to raw scores as indices of system performance.  
To this end, we divide the available questions from 
TREC 2002 into two sets of equal size. The Easy set 
contains the easiest questions (lowest Qq) as identified 
in earlier sections.  For the simulation, this will be the 
test set for one year?s evaluation.  A second, Hard set, 
serves as the test set for a subsequent evaluation, and it 
contains the remaining 50% of the questions (highest 
Qq).  By design, the difference in questions? difficulties 
is far more extreme than is likely to be encountered in 
practice. The Rasch model is then fitted to the responses 
to the Easy set of questions. Next, based on questions? 
difficulties and their fit to the Rasch model, a subset of 
the Easy questions is selected for inclusion in the sec-
ond test in conjunction with the Hard question set. 
These questions are said to comprise the Equating set, 
as they serve to fix the overall locations of the questions 
in the Hard set.  
Normally, this second test would be administered to 
a new set of systems (some completely new, others im-
proved versions of systems evaluated previously). How-
ever, for the purposes of this simulation, we 
?administer? the second test to the same systems.  The 
Rasch model is then applied to the Hard and Equating 
questions combined, while fixing the locations of the 
Equating questions to those derived while scaling the 
Easy set. The Winsteps software achieves this by shift-
ing the locations in the Hard set to be consistent with 
the Equating set ? but without adjusting the spacing of 
the questions in the Hard or Easy sets. If the assump-
tions of the Rasch model hold, then the Easy and Hard 
question sets will now behave as if their levels had been 
estimated simultaneously. Since the same systems are 
used in the simulation, and the questions have been 
calibrated to be on the same scale, the estimated system 
abilities Ss as derived from the Easy and Hard question 
sets should be statistically identical. That is, these two 
estimates should show a high linear correlation and they 
should have very similar means and standard deviations 
(see e.g., Wright and Stone, 1979, p. 83-126).  
Common wisdom in the Rasch scaling community 
holds that relatively few questions are needed to achieve 
satisfactory equating results. For this reason, the simula-
tion study was performed three times, using Equating 
sets with 20, 30, and 50 questions, respectively (i.e., 
about 4, 6, and 10% of the total number of questions in 
the present study). 
4.1 Findings 
The simulation results are summarized in Table 2, 
whose rows reflect the sizes of the respective Equating 
sets (i.e., 20, 30, and 50). Each first sub-row reports the 
Rasch scaling results, while the second sub-row reports 
the raw-score (i.e., number correct) findings. The col-
umns report a number of basic statistics, including the 
mean (M) and standard deviations (SD) of the Logit and 
raw-score values in the Easy and Hard sets, and the 
correlation (rlinear) between systems? estimated abilities 
based on the Easy and Hard sets. 
 
Size of 
Equat-
ing Set Index Measy SDeasy Mhard SDhard rlinear
20 Rasch  -0.66 1.10 -0.65 1.23 0.90 
 # Correct 92.40 47.53 27.39 30.70 0.77 
       
30 Rasch  -0.68 1.10 -0.66 1.21 0.92 
 # Correct 92.88 47.92 29.82 31.80 0.80 
       
50 Rasch  -0.78 1.11 -0.77 1.18 0.94 
  # Correct 94.76 49.62 31.01 32.29 0.82 
 
Table 2.  Results of the Simulation Study 
The major findings are as follows. First, inspection 
of the rlinear columns indicates that Rasch equating con-
sistently produced higher correlations between systems? 
estimated abilities as estimated via the Easy and Hard 
question sets than did the raw scores for each set. Sec-
ond, for obvious reasons the raw-score estimates based 
on the Easy sets are considerably higher than those 
based on the Hard sets. However, Table 2 also shows 
that the standard deviations of the number correct esti-
mates obtained for the Easy sets exceed those of the 
Hard sets as well (sometimes by over 100%). In other 
words, when raw scores (or percentages) are used, the 
?spacing? of the systems is affected by the difficulty of 
the questions.  
 Third, the Rasch approach by contrast produces 
very similar means and standard deviations for the ca-
pability estimates based on the Easy and Hard question 
sets. This holds regardless of the size of the Equating 
sets. For instance, when 50 equating questions are used, 
the estimated abilities based on the Easy and Hard sets 
have nearly identical SD (i.e., 1.11 and 1.18 Logits, re-
spectively). The corresponding averages for this case 
are -0.78 and -0.77 Logits, i.e., a standardized difference 
(effect size) of less than 0.01 SD. Similarly small effects 
sizes are obtained for the other cases. Further, given the 
superior values of rlinear, it thus appears that Rasch 
equating provides an acceptable equating mechanism 
even when maximally different question sets are used. 
In fact, already for Equating sets of size 20 a correlation 
of  0.90 is produced. 
Fourth, as a check on the results, scatter plots of the 
various cases summarized in Table 2 were produced. 
The left panel of Figure 6 shows the Rasch capability 
estimates obtained for the Hard question set plotted 
against those for the Easy set, and it can be seen that 
these estimates are highly correlated (rlinear = 0.94). The 
corresponding raw scores are plotted in the right panel 
of Figure 6. In addition to showing a lower correlation 
(rlinear = 0.82), raw scores also clearly posses a non-
linear component, and in fact the quadratic trend is 
highly significant (tquad = 13.10, p < .001). Thus, in 
addition to being question-dependent, raw score and 
percentage based comparisons suffer from pronounced 
non-linearity. 
Despite the favorable results, we remind the reader 
that the above simulations represented a worse-case 
scenario. Indeed, more realistic simulations not reported 
here indicate that Rasch equating can further be im-
proved by omitting misfitting questions and by using 
less extreme question sets. 
5 Conclusions 
In this paper we have described the Rasch model for 
binary data and applied it to the 2002 TREC QA results. 
We addressed the estimation of question difficulty and 
system ability, the estimation of standard errors for 
these parameters, and how to assess the fit of individual 
questions and systems. Finally, we presented a simula-
tion which demonstrated the advantage of using Rasch 
modeling for calibration of question sets. 
Based on our findings, we recommend that test 
equating be introduced in formal evaluations of HLT. In 
particular, for the QA track of the TREC competition, 
we propose that NIST include a set of questions to be 
reused in the following year for calibration purposes.  
For instance, after evaluating the systems? performance 
in the 2004 competition, NIST would select a set of 
questions consistent with the criteria outlined above. 
Using twenty to fifty questions from a set of 500 will 
probably be sufficient, especially when misfitting ques-
tions are eliminated. When the results are released to the 
participants, they would be asked not to look at these 
equating questions, and not to use them to train their 
systems in the future. These equating questions would 
then be included in the 2005 question set so as to place 
the 2004 and 2005 results on the same Logit scale. The 
process would continue in each consecutive year.  
The approach outlined above serves several pur-
poses. For instance, the availability of equated tests 
would increase the confidence that the testing indeed 
measures progress, and not simply the unavoidable 
-4 -2 0 2 4
Estimated capablity based on "Easy" questions (Logits)
E
st
im
at
ed
 c
ap
ab
ili
ty
 b
as
ed
 o
n 
"H
ar
d"
 q
ue
st
io
ns
 (
Lo
gi
ts
)
rlinear = 0.94
Y 
= X
-4
-2
0
2
4
0 50 100 150
Raw  score on "Easy" questions
0
50
100
150
R
aw
 s
co
re
 o
n 
"H
ar
d"
 q
ue
st
io
ns
rlinear = 0.82
Y 
= X
Y = 24.32 - 0.55X + 0.0051 X2Y = 0.016 + 0.998 X
 
 
Figure 6. Systems? Performance on Easy vs. Hard Questions Based on Rasch Scaling (left) and Raw Scores (right)
 
 variations in difficulty across each year?s question set. 
Additionally, it would support the goal of making each 
competition increasingly more challenging by correctly 
identifying easy and  difficult questions. Further, cali-
brated questions could be combined into increasingly 
large corpora, and these corpora could then be used to 
provide researchers with immediate performance feed-
back in the same metric as the NIST evaluation scale. 
The availability of large corpora of equated questions 
might also provide the basis for the development of 
methods to predict question difficulty, thus stimulating 
important theoretical research in QA. 
The work presented here only begins to scratch the 
surface of adopting a probabilistic approach such as the 
Rasch model for the evaluation of human language 
technologies.  First, as was discussed above, questions 
displaying unexpectedly large or small Outfit values can 
be identified for further study. The questions themselves 
can be analyzed in terms of both content and linguistic 
expression. With the objective of beginning to form a 
theory of question difficulty, questions can be analyzed 
in concert with the occurrence of correct answers in the 
document corpus and the incorrect answers returned by 
systems.  Also, experimentation with more complex 
scaling models could be conducted to uncover informa-
tion other than questions? difficulty levels. For example, 
so-called 2-parameter IRT models (see e.g., Hambleton 
and Swaminathan, 1985) would allow for the estimation 
of a discrimination parameter together with the diffi-
culty parameter for each question. More direct informa-
tion concerning the diagnosis of systems? skill defects 
are described in Stout (2002).  
It is also possible to incorporate into the model 
other factors and variables affecting a system?s per-
formance. Rasch modeling can be extended to many 
other HLT evaluation contexts since Rasch measure-
ment procedures exist to deal with multi-level re-
sponses, counts, proportions, and rater effects. Of 
particular interest is application to technology areas that 
use metrics other than percent of items processed cor-
rectly. Measures such as average precision, R-precision 
and precision at fixed document cutoff, which are used 
in Information Retrieval (Voorhees and Harman, 1999), 
metrics such as BiLingual Evaluation Understudy 
(BLUE) (Papineni et al, 2002) used in Machine Trans-
lation, and F-measure (Van Rijsbergen, 1979) com-
monly used for evaluation of a variety of NLP tasks are 
just a few of the variety of metrics used for evaluation 
of language technologies that can benefit from Rasch 
scaling and related techniques. 
References 
Bond, T.G. and Fox, C.M. (2001). Applying the Rasch 
Model: Fundamental Measurement in the Human 
Sciences. New Jersey: Lawrence Erlbaum Associates. 
Fischer, G.H. (1995). Derivations of the Rasch model. 
In G.H. Fischer & I.W. Molenaar (Eds.), Rasch mod-
els: Foundations, recent developments, and applica-
tions. (pp. 15-38) New York: Springer. 
Hambleton, R.K. and Swaminathan, H. (1985). Item 
response theory: Principles and applications. Boston: 
Kluwer ? Nijhoff. 
Lange, R. (2003). Model Sailplane Competition: From 
Awarding Points to Measuring Performance Skills. 
RC Soaring Digest, August Issue. (This paper is also 
available as: http://www.iknowsys.org/Download/ -
Model_Sailplanes.pdf). 
Lange, R., Donathan, C.L., and Hughes, L.F. (2002). 
Assessing olfactory abilities with the University of 
Pensylvania smell identification test: A Rasch scaling 
approach. Journal of Alzheimer?s Disease, 4, 77-91. 
Linacre, J. M. (2003). WINSTEPS Rasch measurement 
computer program. Chicago, IL: Winsteps.com. 
Papineni, K., Roukos, S., Ward, T, Henderson, J. and  
Reeder F. (2002). Corpus-based Comprehensive and 
Diagnostic MT Evaluation: Initial Arabic, Chinese, 
French, and Spanish Results. Proceedings of the 2002 
Conference on Human Language Technology (pp. 
124-127). San Diego, CA, 2002. 
Stout W.F. (2002). Psychometrics from practice to the-
ory and back. Psychometrika, 67, 485-518. 
http://www.psychometricsociety.org/journal/online/A
RTICLEstout2002.pdf
Rasch, G. (1960/1980). Probabilistic models for some 
intelligence and attainment tests. Chicago, IL: MESA 
Press. 
Van Rijsbergen, C. J.  (1979).  Information Retrieval. 
Dept. of Computer Science, University of Glasgow. 
Voorhees, E. M. and Harman, D. K. (eds.). 1999. Pro-
ceedings of the Eighth Text REtrieval Conference 
(TREC-8), NIST Special Publication 500-246. 
Wright, B.D. and Stone, M.H. (1979). Best test design. 
Chicago, IL: MESA Press. 
A Multilingual Approach to Annotating
and Extracting Temporal Information1
George Wilson
Inderjeet Mani2
The MITRE Corporation,
W640
11493 Sunset Hills Road
Reston, VA 20190-5214
USA
gwilson@mitre.org
imani@mitre.org
Beth Sundheim
SPAWAR Systems Center,
D44208, 53140 Gatchell Rd.
San Diego, CA 92152-7420
USA
sundheim@spawar.navy.mil
Lisa Ferro
The MITRE Corporation,
K329, 202 Burlington Road
Bedford, MA 01730-1420
USA
lferro@mitre.org
                                                          
1 This work has been funded by DARPA?s Translingual Information Detection, Extraction, and Summarization (TIDES)
research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049.
2 Also at the Department of Linguistics, Georgetown University, Washington, DC 20037.
Abstract
This paper introduces a set of
guidelines for annotating time
expressions with a canonicalized
representation of the times they refer
to, and describes methods for
extracting such time expressions from
multiple languages.
1 Introduction
The processing of temporal information poses
numerous challenges for NLP. Progress on these
challenges may be accelerated through the use
of corpus-based methods. This paper introduces
a set of guidelines for annotating time
expressions with a canonicalized representation
of the times they refer to, and describes methods
for extracting such time expressions from
multiple languages. Applications that can
benefit include information extraction (e.g.,
normalizing temporal references for database
entry), question answering (answering ?when?
questions), summarization (temporally ordering
information), machine translation (translating
and normalizing temporal references), and
information visualization (viewing event
chronologies).
Our annotation scheme, described in
detail in (Ferro et al 2000), has several novel
features, including the following:
It goes well beyond the one used in the Message
Understanding Conference (MUC7 1998), not
only in terms of the range of expressions that are
flagged, but, also, more importantly, in terms of
representing and normalizing the time values
that are communicated by the expressions.
In addition to handling fully-specified time
expressions (e.g., September 3rd, 1997), it also
handles context-dependent expressions. This is
significant because of the ubiquity of context-
dependent time expressions; a recent corpus
study (Mani and Wilson 2000) revealed that
more than two-thirds of time expressions in print
and broadcast news were context-dependent
ones. The context can be local (within the same
sentence), e.g., In 1995, the months of June and
July were devilishly hot, or global (outside the
sentence), e.g., The hostages were beheaded that
afternoon. A subclass of these context-
dependent expressions are ?indexical?
expressions, which require knowing when the
speaker is speaking to determine the intended
time value, e.g., now, today, yesterday,
tomorrow, next Tuesday, two weeks ago, etc.
The annotation scheme has been
designed to meet the following criteria:
? Simplicity with precision: We have tried to
keep the scheme simple enough to be
executed confidently by humans, and yet
precise enough for use in various natural
language processing tasks.
? Naturalness: We assume that the annotation
scheme should reflect those distinctions that
a human could be expected to reliably
annotate, rather than reflecting an
artificially-defined smaller set of
distinctions that automated systems might
be expected to make. This means that some
aspects of the annotation will be well
beyond the reach of current systems.
? Expressiveness:  The guidelines require that
one specify time values as fully as possible,
within the bounds of what can be
confidently inferred by annotators. The use
of ?parameters? and the representation of
?granularity? (described below) are tools to
help ensure this.
? Reproducibility: In addition to leveraging
the (ISO-8601 1997) format for representing
time values, we have tried to ensure
consistency among annotators by providing
an example-based approach, with each
guideline closely tied to specific examples.
While the representation accommodates
both points and intervals, the guidelines are
aimed at using the point representation to
the extent possible, further helping enforce
consistency.
The annotation process is decomposed into two
steps: flagging a temporal expression in a
document (based on the presence of specific
lexical trigger words), and identifying the time
value that the expression designates, or that the
speaker intends for it to designate. The flagging
of temporal expressions is restricted to those
temporal expressions which contain a reserved
time word used in a temporal sense, called a
?lexical trigger?, which include words like day,
week, weekend, now, Monday, current, future,
etc.
2 Interlingual Representation
2.1 Introduction
Although the guidelines were developed with
detailed examples drawn from English (along
with English-specific tokenization rules and
guidelines for determining tag extent), the
semantic representation we use is intended for
use across languages. This will permit the
development of temporal taggers for different
languages trained using a common annotation
scheme.
It will also allow for new methods for
evaluating machine translation of temporal
expressions at the level of interpretation as well
as at the surface level. As discussed in
(Hirschman et al 2000), time expressions
generally fall into the class of so-called named
entities, which includes proper names and
various kinds of numerical expressions.  The
translation of named entities is less variable
stylistically than the translation of general text,
and once predictable variations due to
differences in transliteration, etc. are accounted
for, the alignment of the machine-translated
expressions with a reference translation
produced by a human can readily be
accomplished. A variant of the word-error
metric used to evaluate the output of automatic
speech transcription can then be applied to
produce an accuracy score. In the case of our
current work on temporal expressions, it will
also be possible to use the normalized time
values to participate in the alignment and
scoring.
2.2 Semantic Distinctions
Three different kinds of time values are
represented: points in time (answering the
question ?when??), durations (answering ?how
long??), and frequencies (answering ?how
often??).
? Points in time are calendar dates and times-
of-day, or a combination of both, e.g.,
Monday 3 pm, Monday next week, a Friday,
early Tuesday morning, the weekend. These
are all represented with values (the tag
attribute VAL) in the ISO format, which
allows for representation of date of the
month, month of the year, day of the week,
week of the year, and time of day, e.g.,
<TIMEX2 VAL=?2000-11-
29T16:30?>4:30 p.m. yesterday afternoon
</TIMEX2>.
? Durations also use the ISO format to
represent a period of time. When only the
period of time is known, the value is
represented as a duration, e.g., <TIMEX2
VAL=?P3D?>a three-day </TIMEX2>
visit.
? Frequencies reference sets of time points
rather than particular points.  SET and
GRANULARITY attributes are used for
such expressions, with the PERIODICITY
attribute being used for regularly recurring
times, e.g., <TIMEX2 VAL=?XXXX-WXX-
2? SET=?YES? PERIODICITY=?F1W?
GRANULARITY=?G1D?>every
Tuesday</TIMEX2>.
Here ?F1W? means frequency of once a week,
and the granularity ?G1D? means the set
members are counted in day-sized units.
The annotation scheme also addresses several
semantic problems characteristic of temporal
expressions:
? Fuzzy boundaries. Expressions like
Saturday morning and Fall are fuzzy in their
intended value with respect to when the time
period starts and ends; the early 60?s is
fuzzy as to which part of the 1960?s is
included. Our format for representing time
values includes parameters such as FA (for
Fall), EARLY (for early, etc.),
PRESENT_REF (for today, current, etc.),
among others. For example, we have
<TIMEX2 VAL=?1990-SU?>Summer of
1990</TIMEX2>. Fuzziness in modifiers is
also represented, e.g., <TIMEX2
VAL=?1990? MOD=?BEFORE?>more
than a decade ago</TIMEX2>. The intent
here is that a given application may choose
to assign specific values to these parameters
if desired; the guidelines themselves don?t
dictate the specific values.
? Non-Specificity. Our scheme directs the
annotator to represent the values, where
possible, of temporal expressions that do not
indicate a specific time.  These non-specific
expressions include generics, which state a
generalization or regularity of some kind,
e.g., <TIMEX2 VAL=?XXXX-04?
NON_SPECIFIC=?YES?>April</TIMEX>
is usually wet, and non-specific indefinites,
like <TIMEX2 VAL="1999-06-XX"
NON_SPECIFIC="YES? GRANULARITY=
"G1D">a sunny day in <TIMEX2
VAL="199906">June</TIMEX2>
</TIMEX2>.
3 Reference Corpus
Based on the guidelines, we have arranged for 6
subjects to annotate an English reference corpus,
consisting of 32,000 words of a telephone dialog
corpus ? English translations of the ?Enthusiast?
corpus of Spanish meeting scheduling dialogs
used at CMU and by (Wiebe et al 1998), 35,000
words of New York Times newspaper text and
120,000 words of broadcast news (TDT2 1999).
This corpus will soon be made available to the
research community.
4 Time Tagger System
4.1 Architecture
The tagging program takes in a document which
has been tokenized into words and sentences and
tagged for part-of-speech. The program passes
each sentence first to a module that flags time
expressions, and then to another module (SC)
that resolves self-contained (i.e., ?absolute?)
time expressions. Absolute expressions are
typically processed through a lookup table that
translates them into a point or period that can be
described by the ISO standard.
The program then takes the entire
document and passes it to a discourse processing
module (DP) which resolves context-dependent
(i.e., ?relative?) time expressions (indexicals as
well as other expressions). The DP module
tracks transitions in temporal focus, using
syntactic clues and various other knowledge
sources.
The module uses a notion of Reference
Time to help resolve context-dependent
expressions. Here, the Reference Time is the
time a context-dependent expression is relative
to. The reference time (italicized here) must
either be described (as in ?a week from
Wednesday?) or implied (as in ?three days ago
[from today]?). In our work, the reference time
is assigned the value of either the Temporal
Focus or the document (creation) date. The
Temporal Focus is the time currently being
talked about in the narrative. The initial
reference time is the document date.
4.2 Assignment of Time Values
We now discuss the assigning of values to
identified time expressions. Times which are
fully specified are tagged with their value, e.g,
?June 1999? as 1999-06 by the SC module. The
DP module uses an ordered sequence of rules to
handle the context-dependent expressions. These
cover the following cases:
? Explicit offsets from reference time:
indexicals like ?yesterday?, ?today?,
?tomorrow?, ?this afternoon?, etc., are
ambiguous between a specific and a non-
specific reading. The specific use
(distinguished from the generic one by
machine learned rules discussed in (Mani
and Wilson 2000)) gets assigned a value
based on an offset from the reference time,
but the generic use does not. For example, if
?fall? is immediately preceded by ?last? or
?next?, then ?fall? is seasonal  (97.3%
accurate rule). If ?fall? is followed 2 words
after by a year expression, then ?fall? is
seasonal (86.3% accurate).
? Positional offsets from reference time:
Expressions like ?next month?, ?last year?
and ?this coming Thursday? use lexical
markers (underlined) to describe the
direction and magnitude of the offset from
the reference time.
? Implicit offsets based on verb tense:
Expressions like ?Thursday? in ?the action
taken Thursday?, or bare month names like
?February? are passed to rules that try to
determine the direction of the offset from
the reference time, and the magnitude of the
offset. The tense of a neighboring verb is
used to decide what direction to look to
resolve the expression.
? Further use of lexical markers:  Other
expressions lacking a value are examined
for the nearby presence of a few additional
markers, such as ?since? and ?until?, that
suggest the direction of the offset.
? Nearby Dates:  If a direction from the
reference time has not been determined,
some dates, like ?Feb. 14?, and other
expressions that indicate a particular date,
like ?Valentine?s Day?, may still be
untagged because the year has not been
determined.  If the year can be chosen in a
way that makes the date in question less
than a month from the reference date, that
year is chosen. Dates more than a month
away are not assigned values by this rule.
4.3 Time Tagging Performance
The system performance on a test set of 221
articles from the print and broadcast news
section of the reference corpus (the test set had a
total of 78,171 words) is shown in Table 13.
Note that if the human said the tag had no value,
and the system decided it had a value,  this is
treated as  an error. A baseline of just tagging
values of absolute, fully specified expressions
(e.g., ?January 31st, 1999?) is shown for
comparison in parentheses.
Type Human
Found
Correct
System
Found
System
Correct
F-
measure
TIMEX2 728 719 696 96.2
VAL 728 719 602
(234)
83.2
(32.3)
Table 1: Performance of Time Tagger
(English)
5 Multilingual Tagging
The development of a tagging program for other
languages closely parallels the process for
English and reuses some of the code. Each
language has its own set of lexical trigger words
that signal a temporal expression. Many of
these, e.g. day, week, etc., are simply
translations of English words.
Often, there will be some additional
triggers with no corresponding word in English.
For example, some languages contain a single
lexical item that would translate in English as
?the day after tomorrow?. For each language,
the triggers and lexical markers must be
identified.
As in the case of English, the SC
module for a new language handles the case of
absolute expressions, with the DP module
                                                          
3 The evaluated version of the system does not adjust the
Reference Time for subsequent sentences.
handling the relative ones. It appears that in
most languages, in the absence of other context,
relative expressions with an implied reference
time are relative to the present. Thus, tools built
for one language that compute offsets from a
base reference time will carry over to other
languages.
As an example, we will briefly describe
the changes that were needed to develop a
Spanish module, given our English one. Most of
the work involved pairing the Spanish surface
forms with the already existing computations,
e.g. we already computed ?yesterday? as
meaning ?one day back from the reference
point?. This had to be attached to the new
surface form ?ayer?. Because not all computers
generate the required character encodings, we
allowed expressions both with and without
diacritical marks, e.g., ma?ana and manana.
Besides the surface forms, there are a
few differences in conventions that had to be
accounted for. Times are mostly stated using a
24-hour clock. Dates are usually written in the
European form day/month/year rather than the
US-English convention of month/day/year.
A difficulty arises because of the use of
multiple calendric systems. While the Gregorian
calendar is widely used for business across the
world, holidays and other social events are often
represented in terms of other calendars. For
example, the month of Ramadan is a regularly
recurring event in the Islamic calendar, but
shifts around in the Gregorian4.
Here are some examples of tagging of
parallel text from Spanish and English with a
common representation.
<TIMEX2 VAL="2001-04-
01">hoy</TIMEX2>
<TIMEX2 VAL="2001-04-
01">today</TIMEX2>
<TIMEX2 VAL="1999-03-13">el trece de
marzo de 1999</TIMEX2>
<TIMEX2 VAL="1999-03-13">the thirteenth of
March, 1999</TIMEX2>
                                                          
4 Our annotation guidelines state that a holiday name is
markable but should receive a value only when that value
can be inferred from the context of the text, rather than
from cultural and world knowledge.
<TIMEX2 VAL="2001-W12">la semana
pasada</TIMEX2>
<TIMEX2 VAL="2001-W12">last
week</TIMEX2>
6 Related Work
Our scheme differs from the recent scheme of
(Setzer and Gaizauskas 2000) in terms of our in-
depth focus on representations for the values of
specific classes of time expressions, and in the
application of our scheme to a variety of
different genres, including print news, broadcast
news, and meeting scheduling dialogs. Others
have used temporal annotation schemes for the
much more constrained domain of meeting
scheduling, e.g., (Wiebe et al 1998),
(Alexandersson et al 1997), (Busemann et al
1997).  Our scheme has been applied to such
domains as well, our annotation of the
Enthusiast corpus being an example.
7 Conclusion
In the future, we hope to extend our English
annotation guidelines into a set of multilingual
annotation guidelines, which would include
language-specific supplements specifying
examples, tokenization rules, and rules for
determining tag extents. To support
development of such guidelines, we expect to
develop large keyword-in-context concordances,
and would like to use the time-tagger system as
a tool in that effort.  Our approach would be (1)
to run the tagger over the desired text corpora;
(2) to run the concordance creation utility over
the annotated version of the same corpora, using
not only TIMEX2 tags but also lexical trigger
words as input criteria; and (3) to partition the
output of the creation utility into entries that are
tagged as temporal expressions and entries that
are not so tagged.  We can then review the
untagged entries to discover classes of cases that
are not yet covered by the tagger (and hence,
possibly not yet covered by the guidelines), and
we can review the tagged entries to discover any
spuriously tagged cases that may correspond to
guidelines that need to be tightened up.
We also expect to create and distribute
multilingual corpora annotated according to
these guidelines. Initial feedback from machine
translation system grammar writers (Levin
2000) indicates that the guidelines were found to
be useful in extending an existing interlingua for
machine translation. For the existing English
annotations, we are currently carrying out inter-
annotator agreement studies of the work of the 6
annotators.
References
J. Alexandersson, N. Reithinger, and E. Maier.
Insights into the Dialogue Processing of
VERBMOBIL. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, 1997, 33-40.
S. Busemann, T. Declerck, A. K. Diagne, L. Dini, J.
Klein, and S. Schmeier. Natural Language
Dialogue Service for Appointment Scheduling
Agents. Proceedings of the Fifth Conference on
Applied Natural Language Processing, 1997, 25-
32.
L. Ferro, I. Mani, B. Sundheim, and G. Wilson.
TIDES Temporal Annotation Guidelines. Draft
Version 1.0. MITRE Technical Report MTR
00W0000094, October 2000.
L. Hirschman, F. Reeder, J. Burger, and K. Miller,
Name Translation as a Machine Translation
Evaluation Task. Proceedings of LREC?2000.
ISO-8601 ftp://ftp.qsl.net/pub/g1smd/8601v03.pdf
1997.
L. Levin. Personal Communication.
I. Mani and G. Wilson. Robust Temporal Processing
of News, Proceedings of the ACL'2000
Conference, 3-6 October 2000, Hong Kong.
MUC-7. Proceedings of the Seventh Message
Understanding Conference, DARPA. 1998.
http://www.itl.nist.gov/iad/894.02/related_projects/muc/
A. Setzer and R. Gaizauskas. Annotating Events and
Temporal Information in Newswire Texts.
Proceedings of the Second International
Conference On Language Resources And
Evaluation (LREC-2000), Athens, Greece, 31
May- 2 June 2000.
TDT2
http://morph.ldc.upenn.edu/Catalog/LDC99T37.ht
ml 1999
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren,
and K. J. McKeever. An Empirical Approach to
Temporal Reference Resolution. Journal of
Artificial Intelligence Research, 9, 1998, pp. 247-
293.
Appendix 1: Annotated
Corpus: Enthusiast Dialog
Example (one utterance)
Transcript of Spanish source:
EL LUNES DIECISIETE IMAGINO QUE
QUIERE DECIR EL DIECISIETE TENGO UN
SEMINARIO DESDE LAS DIEZ HASTA LAS
CINCO
Annotated English translation:
<TIMEX2 VAL=?2000-05-17?>MONDAY
THE SEVENTEENTH</TIMEX2> I IMAGINE
YOU MEAN <TIMEX2 VAL=?2000-05-
17?>THE SEVENTEENTH</TIMEX2> I
HAVE A SEMINAR FROM <TIMEX2
VAL=?2000-05-17T10?>TEN </TIMEX2>
UNTIL <TIMEX2 VAL=?2000-05-
17T17?>FIVE
</TIMEX2>
Note:  Elements of range expressions are tagged
separately.  The VAL includes date as well as
time because of the larger context.  The
annotator has confidently inferred that the
seminar is during the daytime, and has coded the
time portion of the VAL accordingly.
Appendix 2: Annotated
Corpus: New York Times
Article (excerpt)
   Dominique Strauss-Kahn, France's finance
minister, said: "<TIMEX2 VAL=?1999-01-01?>
Today</TIMEX2> is clearly <TIMEX2
NON_SPECIFIC=?YES?>a historic day for the
European enterprise</TIMEX2>. Europe will be
strong, stronger than in <TIMEX2
VAL=?PAST_REF?>the past</TIMEX2>,
because it will speak with a single monetary
voice."
   But even on <TIMEX2 VAL=?1998-12-31?>
Thursday </TIMEX2>, there were signs of
potential battles ahead.
   One hint came from Duisenberg, a former
Dutch central banker who was named president
of the European Central Bank only after a bitter
political fight <TIMEX2 VAL=?1998-05?>last
May</TIMEX2> between France and Germany.
Duisenberg, a conservative on monetary policy,
was favored by Helmut Kohl, who was
<TIMEX2 VAL=?1998-05?>then</TIMEX2>
chancellor of Germany. But President Jacques
Chirac of France insisted on the head of the
Bank of France, Jean-Claude Trichet.
   Germany and France eventually cut a deal
under which Duisenberg would become
president of the new European bank, but
"voluntarily" agree to step down well ahead of
<TIMEX2 VAL=?P8Y? MOD=?END?>the end
of his eight-year term</TIMEX2>.
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 49?54,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Generating an Entailment Corpus from News Headlines   John Burger, Lisa Ferro The MITRE Corporation 202 Burlington Rd. Bedford, MA 01730, USA {john,lferro} @ mitre.org     Abstract We describe our efforts to generate a large (100,000 instance) corpus of textual entailment pairs from the lead paragraph and headline of news articles.  We manu-ally inspected a small set of news stories in order to locate the most productive source of entailments, then built an anno-tation interface for rapid manual evalua-tion of further exemplars. With this training data we built an SVM-based document classifier, which we used for corpus refinement purposes?we believe that roughly three-quarters of the resulting corpus are genuine entailment pairs.  We also discuss the difficulties inherent in manual entailment judgment, and suggest ways to ameliorate some of these. 1 Introduction MITRE has a long-standing interest in robust text understanding, and, like many, we believe that adequate progress in such an endeavor requires a well-designed evaluation methodology.  We have explored in great depth the use of human reading comprehension exams for this purpose (Hirschman  et al, 1999, Wellner et al, 2005) as well as TREC-style question answering (Burger, 2004). In this context, the recent Pascal RTE evaluation (Recognizing Textual Entailment, Dagan et al, 2005) captured our interest.  The goal of RTE is to assess systems? abilities at judging semantic en-tailment with respect to a pair of sentences, e.g.: 
? Fred spilled wine on the carpet. ? The rug was wet. In RTE parlance, the antecedent sentence is known as the text, while the consequent sentence is known as the hypothesis.  Simply put, the chal-lenge for an RTE system is to judge whether the text entails the hypothesis.  Judgments are Boo-lean, and the primary evaluation metric is simple accuracy, although there were other, secondary metrics used in the evaluation. The RTE organizers provided 567 exemplar sen-tence pairs.  This is adequate for system develop-ment, but not for the application of large-scale statistical models.  In particular, we wished to cast the problem as one of statistical alignment as used in machine translation.  MT systems typically use millions of sentence pairs, and so we decided to find or generate a much larger corpus.  This paper describes our efforts along these lines, as well as some observations about the problems of annotat-ing entailment data.  In Section 2 we describe our initial search for an entailment corpus.  Section 3 briefly describes an annotation interface we de-vised, as well as our efforts to refine our corpus.  Section 4 explains many of the issues and prob-lems inherent in manual annotation of entailment data. 2 Finding Entailment Data In our study of the Pascal RTE development cor-pus, we found that a considerable majority of the TRUE pairs exhibit a stronger relationship than entailment; namely, the hypothesis is a paraphrase of a subset of the text.  For instance, given the text 
49
John murdered Bill yesterday, the hypothesis Bill is dead is an entailment, while the hypothesis Bill was killed by John exhibits the stronger partial paraphrase relationship to the text.  We found that 94% (131/140) of the TRUE pairs in the Pascal RTE dev2 corpus were these sorts of paraphrases. In our search for an entailment corpus, we ob-served that the headline of a news article is often a partial paraphrase of the lead paragraph, much like the RTE data, or is sometimes a genuine entail-ment.  We thus deduced that headlines and their corresponding lead paragraphs might provide a readily available source of training data.  As an initial test of this hypothesis, we manually in-spected over 200 news stories from 11 different sources.  We found a great deal of variety in head-line formats, and ultimately found the Xinhua News Agency English Service articles from the Gigaword corpus (Graff, 2003) to be the richest source, though somewhat limited in subject do-main.  We describe here our data collection and analysis process. Because our goal was to automatically generate an extremely large corpus of exemplars, we fo-cused on large data sources.  We first examined 111 news stories culled from MiTAP (Damianos et al, 2003), which collects over one million articles per month from approximately 75 different sources.  By first counting the number of articles typically collected for each source, we selected a mixture of sources that each had more than 10,000 articles for our sample period of one and half months.  As discussed further below, part way through our investigation it became clear that we needed to include more native English sources, so the Christian Science Monitor articles were added, 
though they fell below our arbitrary 10K mark. Figure 1 summarizes the MiTAP news sources ex-amined. For each lead paragraph/headline pair, a human rendered a judgment of yes, no, or maybe as to whether the lead paragraph entailed the headline, where maybe meant that the headline was very close to being an entailment or paraphrase.  This is likely equivalent to the notion of ?more or less se-mantically equivalent? used in the Microsoft Re-search Paraphrase Corpus (Dolan et al, 2005).  The purpose of maybe in this case was that we thought that many of the near-miss pairs would make adequate training data for statistical algo-rithms, in spite of being less than perfect. There were many types of news articles in the MiTAP data that did not yield good headline/lead paragraph pairs for our purposes.  Many would be difficult to filter out using automated heuristics.  Two frequent examples of this were opinion-ed itorial pieces and daily Wall Street summaries.  Others would be more amenable to automatic elimination, including obituaries and collections of news snippets like the Washington Post?s ?World in Brief?.  Articles consisting of personal narra-tives never yielded good headlines, but these could easily be eliminated by recognizing first person pronouns in the lead paragraph.  Figure 2 shows the judgments for all the MiTAP articles examined, where the Filtered row excludes these easily elimi-nated article types. As Figure 2 shows, the MiTAP data did not yield a high percentage of good pairs.  In addition, whether due to poor machine translation or English dialectal differences, our evaluator found it diffi-cult to understand some of the text from sources that were not English-primary.  A certain amount of ill-formed text was acceptable, since the Pascal RTE challenge included training and test data drawn from MT scenarios, but we did not wish our data to be too dominated by such sources.  Thus, we selected additional native-English articles to add to our sample set. Despite the overall poor yield from this data, it 
Source No. articles examined No. articles in 1.5 mos. miami-herald (US) 19 94,278 washington-post (US) 18 13,813 cs-monitor (US) 11 7,102 all-africa 18 68,521 dawn (Pakistan) 17 46,839 gulf-daily-news 10 26,837 national-post (Canada) 18 14,124 Figure 1: MiTAP News Sources Examined 
 Yes No Maybe Total All Pairs 54 (49%) 39 (35%) 18 (16%) 111 Filtered 54 (53%) 33 (33%) 14 (14%) 101 Figure 2: MiTAP Corpus Results 
Source Yes No Maybe Total APW 8 (31% ) 12 (46%) 6 (23%) 26 AFE  14 (56%) 4 (16%) 7 (28%) 25 NYT 8 (31%) 17 (65%) 1 (4%) 26 XIE 22 (85%) 4 (15%)  0 (0%) 26 Total 52 (50%) 37 (36%) 14 (14%) 103 Figure 3: Gigaword Corpus Results 
50
was apparent that some news sources tended to be more fruitful than others.  For example, 13 out of 18 of the Washington Post articles yielded good pairs, as opposed to only 1 of the 11 Christian Sci-ence Monitor articles. This generalization was likewise true in the sec-ond corpus we examined, the Gigaword newswire corpus (Graff, 2003).  Gigaword contains over 4 million documents from four news sources:  ? Agence France Press English Service (AFE) ? Associated Press Worldstream English Service (APW) ? The New York Times Newswire Service (NYT) ? The Xinhua News Agency English Service (XIE) For each source, Gigaword articles are classified into several types, including newswire advisories, etc.  We restricted our investigations to actual news stories.  As Figure 3 shows, overall results were much the same as the MiTAP articles, but 85% of the XIE articles yielded adequate pairs. 
Based on these preliminary results we decided to focus further manual investigations on the XIE articles from Gigaword.  We also decided to ex-pend some effort on an annotation tool that would allow us to proceed more quickly than the early annotation experiments described above. 3 Refining the Data MITRE has developed a series of annotation tools for a variety of linguistic phenomena (Day et al 1997; Day et al 2004), but these are primarily de-signed for fine-grained tasks such as named entity and syntactic annotation.  For our headline corpus, we wanted the ability to rapidly annotate at a docu-ment level from a small set of categories.  Further, we wanted the interface to easily support distributed annotation efforts. The resulting annotation interface is shown in Figure 4.  It is web-based, and annotations and other document information are stored in an SQL database.  The document to be evaluated is dis-played in the user?s chosen browser, with the XML 
 Figure 4: Entailment Tagging Interface 
51
document zoning tags visible so that the user can easily identify the headline and lead paragraph. At the top of the document are three buttons from which to select a yes/no/maybe judgment.  The user can also add a comment before moving to the next document. Typically several documents can be judged per minute. The client-server architec-ture supports multiple annotations of the same document by different annotators?accordingly, it has a mode enabling reconciliation of inter-annotator disagreements.  All further annotation efforts discussed below were carried out with this tool. Using the tool, we tagged approximately 900 randomly chosen Gigaword documents, including 520 XIE documents.  From this, we estimate that 70% of the XIE headlines in Gigaword are entailed by the corresponding lead paragraph.  (This is lower than the rough estimate described in Section 2, but that was based on a very small sample.)  We decided to explore ways to refine the data in order to arrive at a smaller, but less noisy subcorpus. We observed that different subgenres within the news-paper corpus evinced the lead-entails-headline quality to different degrees.  For example, articles about sports or entertainment often had whimsical (non-entailed) headlines, while articles about poli-tics or business more frequently had the headline quality we sought. Accordingly, we decided to treat the data re-finement process as a text classification problem, one of finding the mix of genres or topics that would most likely possess the lead-entails-headline quality.  We used SVM-light (Joachims, 2002) as a document classifier, training it on the initial set of annotated articles.  (Note that these text classifica-tion experiments made use of the entire article, not just the lead and headline.)  We experimented with a variety of feature representations and SVM pa-rameters, but found the best performance with a Boolean bag-of-words representation, and a simple linear kernel.  Leave-one-out estimates indicate that SVM-light could identify documents with the requisite entailment quality with 77% accuracy. We performed one round of active learning (Tong & Koller, 2000), in which we used SVM-light to classify a large subset of the unannotated corpus, and then selected a 100-document subset about which the classifier was least certain.  The rationale is that annotating these uncertain docu-ments will be more informative to further learning 
runs than a randomly selected subset.  In the case of large-margin classifiers like SVMs, the natural choice is to select the instances closest to the mar-gin.  These were then annotated, and added back to the training data for the next learning run.  How-ever, leave-one-out estimates indicated that the classifier benefited little from these new instances. As described above, we estimate that the base rate of the headline entailment property in the XIE portion of Gigaword is 70%.  Our hypothesis in training the SVM was that we could identify a smaller but less noisy subset.  In order to evaluate this, we ran the trained SVM on all 679,000 of the unannotated XIE documents, and selected the 100,000 ?best? instances?that is, the documents most likely (according to the SVM) to evince the headline quality.  We selected a random subset of these best documents, and annotated them to evaluate our hypothesis.  74% of these possessed the lead-entails-headline property, a difference of 4% absolute over the XIE base rate.  We used the lead-headline pairs  from this 100,000-best subset to train our MT-alignment-based system for the RTE evaluation (Bayer et al, 2005).  This system was one of the best performers in the evaluation, which we ascribe to our large training corpus Later examination showed that the 4% ?im-provement? in purity is not statistically significant.  We intend to perform further experiments in data refinement, but this may prove unnecessary.  Per-haps the base rate of the entailment phenomenon in the XIE documents is sufficient to train an effec-tive alignment-based entailment system.  In this case, all of the XIE documents could be used, per-haps resulting in a more robust, and even better performing system. 4 Judging Headline Entailments In the process of generating the training data, we doubly-judged an additional 300 XIE documents to measure inter-judge reliability.  As in the pilot phase described above, each pair was labeled as yes, no, or maybe. In addition, the judges were given a comment field to record their reasoning and misgivings.  The judging was performed in two steps, first on a set of 100 documents and then on a set of 200.  One of the judges was already well versed in the RTE task, and had performed the earlier pilot investigations.  Prior to judging the first set, the second judge was given a brief verbal 
52
overview of the task.  After the first 100 docu-ments had been doubly-judged, the more experi-enced judge then reviewed the differences and drafted a set of guidelines.  The guidelines pro-vided a synopsis of the official RTE guidelines, plus a few rules unique to headlines.  For example, one rule specified what to do when partial entail-ment only held if the lead were combined with lo-cation or date information from the dateline. The two evaluators then judged the second set.  The results for both sets are shown in Figure 5. As these results show, the guidelines had only a small effect on the strict measure of agreement. Three problem areas existed: (1) Raw, messy data.  The Gigaword corpus was automatically collected and zoned.  Thus, the head-lines in particular contained a number of irregulari-ties that made it difficult to judge their appropriateness.  Such irregularities included trun-cations, phrases lacking any proposition, pre-pended alerts like URGENT:, and bylines and date lines miszoned into the headline. (2)  Disagreement on what constitutes synon-ymy.   Our judges found they had irreconcilable differences about differences in meaning.  For ex-ample, in the following pair, the judges disagreed about whether safe operation in the lead paragraph meant the same thing as, and thus entailed, oper-ates smoothly in the headline: ? Shanghai's Hongqiao Airport Operates Smoothly ? As of Saturday, Shanghai's Hongqiao Airport has performed safe operation for some 2,600 consecutive days, setting a record in the country. (3) Disagreement on the amount of world knowledge permitted.  Figure 5 shows that if maybe is counted as equivalent to yes, the agree-ment level improves significantly.  This is likely because there were two important aspects of the RTE definition of entailment that were not im-parted to the second judge until the written guide-lines:  that one can assume ?common human understanding of language and some common background knowledge.?  However, our judges did 
not always agree on what counts as ?common,? which accounts for much of the high overlap be-tween yes and maybe.  Nevertheless, our 90% agreement compares favorably to the 83% agree-ment rate reported by Dolan et al (2005) for their judgments on ?more or less semantically equiva-lent? pairs. Our 78% strict agreement compares favorably to the 80% agreement achieved by Da-gan et al (2005), given that our data was messier than the pairs crafted for the RTE challenge. Like Dagan et al (2005), we did not force reso-lution on all disagreements.  Disagreements over synonymy and common knowledge result in irrec-oncilable differences, because it is neither possible nor desirable to use guidelines to force a shared understanding of an utterance.  Thus, for the first set of data 15 (15%) of the pairs were left unrecon-ciled.  In the second set, 42 (21%) were left un-reconciled.  Eleven (6%) of the irreconcilable pairs in the second set were due to confusion stemming from the telegraphic nature of headlines, which led to misunderstandings about how to judge truncated headlines (Chinese President Vows to Open New Chapters With) vs. headlines lacking propositions (subject headings like Mandela?s Speech) vs. well-formed but terse headlines (Crackdown on Auto-Mafia in Bulgaria).    Despite the high number of irreconcilable pairs, one encouraging sign was evident from the com-ment field.  The judges? comments revealed that on pairs where they disagreed on how to label the pair, they often agreed on what the problem was. Our experience in generating a training corpus, particularly the number of irreconcilable cases we encountered, raises an important issue, namely, the feasibility of semantic equivalence tasks.  We sug-gest that the optimum method for empirically modeling semantic equivalence is to capture the variation in human judgments.  Three judges would evaluate each pair, so that there would al-ways be a tie breaker.  After reconciling for dis-agreements arising from human error, each distinct judgment would become part of the data set.  We also recommend that where there is genuine dis-agreement, the questionable portions of each pair be annotated in some way to capture the source of the problem, going one step further than the com-ment field we found beneficial in our annotation interface.  The three judgments would result in a four way classification of pairs: 
Condition Set 1 (100 docs) Set 2 (200 docs) strict match 75.00% 77.50% maybe = yes 79.00% 90.00% maybe = no 84.00% 81.00% maybe = * 88.00% 94.00% Figure 5: Agreement for Two XIE Data Sets 
53
TTT = TRUE TTF = Likely TRUE, but possibly FALSE TFF = Likely FALSE, but possibly TRUE FFF = FALSE System developers could choose to train on all the data, or limit themselves to the TTT/FFF cases.  For evaluation purposes, the systems? results on the TTF/TFF pairs could be evaluated in light of the human variation, providing a more realistic measure of the complexity of the task. 5 Conclusion  Given the number of natural language processing applications that require the ability to recognize semantic equivalence and entailment, there is an obvious need for both robust evaluation method-ologies and adequate development and test data. We?ve described here our work in generating sup-plemental training data for the recent Pascal RTE evaluation, with which we produced a competitive system.  Some news corpora provide a rich source of exemplars, and an automatic document classifier can be used to reduce the noisiness of the data.  There are lingering difficulties in achieving high inter-judge agreement in determining paraphrase and entailment, and we believe the best way to cope with this is to allow the data to reflect the variance that exists in cross-human judgments. Acknowledgments This paper reports on work supported by the MITRE Sponsored Research Program.  We would also like to extend our thanks to Sam Bayer, John Henderson and Alex Yeh for their invaluable sug-gestions and comments.  Our gratitude also goes to Laurie Damianos, who provided us with statistics on MiTAP?s resources and served as one of the evaluators in our inter-judge reliability study.  References  Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and Alexander Yeh, 2005.  MITRE?s submissions to the EU Pascal RTE Challenge. PASCAL Proceedings of the First Challenge Work-shop, Recognizing Textual Entailment, 11?13 April, 2005, Southampton, U.K. John D. Burger, 2004. MITRE?s Qanda at TREC-12. The Twelvth Text REtrieval Conference.  NIST Spe-cial Publication SP 500?255. 
Ido Dagan, Oren Glickman, and Bernado Magnini, 2005.  The PASCAL recognizing textual entailment challenge.  PASCAL Proceedings of the First Chal-lenge Workshop, Recognizing Textual Entailment, 11?13 April, 2005, Southampton, U.K. Laurie Damianos, Steve Wohlever, Robyn Kozierok, and Jay Ponte, 2003. mitap for real users, real data, real problems.  In Proceedings of the Conference on Human Factors of Computing Systems (CHI 2003), Fort Lauderdale, FL April 5?10. David Day, John Aberdeen, Lynette Hirschman, Robyn Kozierok, Patricia Robinson and Marc Vilain, 1997. Mixed initiative development of language processing systems. Proceedings of the Fifth Conference on Ap-plied Natural Language Processing. David Day, Chad McHenry, Robyn Kozierok, Laurel Riek, 2004.  Callisto: A configurable annotation workbench. International Conference on Language Resources and Evaluation. Bill Dolan, Chris Brockett., and Chris Quirk,  2005. Microsoft Research Paraphrase Corpus. http://research.microsoft.com/research/nlp/msr_paraphrase.htm David Graff, 2003.  English Gigaword. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2003T05 Dan Gusfield, 1997. Algorithms on Strings, Trees and Sequences.  Cambridge University Press. Lynette Hirschman, Marc Light, Eric Breck, and John D. Burger, 1999.  Deep Read: A reading comprehen-sion system.  Proceedings of the 37th annual meeting of the Association for Computational Linguistics. Thorsten Joachims, 2002.  Learning to Classify Text Using Support Vector Machines.  Kluwer. Guido Minnen, John Carroll, and Darren Pearce, 2001. Applied morphological processing of English. Natu-ral Language Engineering, 7(3). Franz Josef Och and Hermann Ney, 2003.  A systematic comparison of various statistical alignment models.  Computational Linguistics, 29(1). Simon Tong and Daphne Koller, 2000.  support vector machine active learning with applications to text classification.  Proceedings of ICML-00, 17th Inter-national Conference on Machine Learning. Ben Wellner, Lisa Ferro, Warren Greiff, and Lynette Hirschman, 2005. Reading comprehension tests for computer-based understanding evaluation. Natural Language Engineering (to appear).  
54
