Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Extractive Summarization using Inter- and Intra- Event Relevance 
 
Wenjie Li, Mingli Wu and Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University 
{cswjli,csmlwu,csluqin}@comp
.polyu.edu.hk 
Wei Xu and Chunfa Yuan 
Department of Computer Science and 
Technology, Tsinghua University 
{vivian00,cfyuan}@mail.ts
inghua.edu.cn 
 
 
 
Abstract 
Event-based summarization attempts to 
select and organize the sentences in a 
summary with respect to the events or 
the sub-events that the sentences de-
scribe. Each event has its own internal 
structure, and meanwhile often relates to 
other events semantically, temporally, 
spatially, causally or conditionally. In 
this paper, we define an event as one or 
more event terms along with the named 
entities associated, and present a novel 
approach to derive intra- and inter- event 
relevance using the information of inter-
nal association, semantic relatedness, 
distributional similarity and named en-
tity clustering. We then apply PageRank 
ranking algorithm to estimate the sig-
nificance of an event for inclusion in a 
summary from the event relevance de-
rived. Experiments on the DUC 2001 
test data shows that the relevance of the 
named entities involved in events 
achieves better result when their rele-
vance is derived from the event terms 
they associate. It also reveals that the 
topic-specific relevance from documents 
themselves outperforms the semantic 
relevance from a general purpose 
knowledge base like Word-Net. 
 
 
1. Introduction 
Extractive summarization selects sentences 
which contain the most salient concepts in 
documents. Two important issues with it are 
how the concepts are defined and what criteria 
should be used to judge the salience of the con-
cepts. Existing work has typically been based on 
techniques that extract key textual elements, 
such as keywords (also known as significant 
terms) as weighed by their tf*idf score, or con-
cepts (such as events or entities) with linguistic 
and/or statistical analysis. Then, sentences are 
selected according to either the important textual 
units they contain or certain types of inter-
sentence relations they hold.  
Event-based summarization which has e-
merged recently attempts to select and organize 
sentences in a summary with respect to events or 
sub-events that the sentences describe. With re-
gard to the concept of events, people do not 
have the same definition when introducing it in 
different domains. While traditional linguistics 
work on semantic theory of events and the se-
mantic structures of verbs, studies in 
information retrieval (IR) within topic detection 
and tracking framework look at events as 
narrowly defined topics which can be 
categorized or clustered as a set of related 
documents (TDT). IR events are broader (or to 
say complex) events in the sense that they may 
include happenings and their causes, 
consequences or even more extended effects. In 
the information extraction (IE) community, 
events are defined as the pre-specified and struc-
tured templates that relate an action to its 
participants, times, locations and other entities 
involved (MUC-7). IE defines what people call 
atomic events. Regardless of their distinct perspectives, peo-
ple all agree that events are collections of activi-
ties together with associated entities. To apply 
the concept of events in the context of text sum-
marization, we believe it is more appropriate to 
consider events at the sentence level, rather than 
at the document level. To avoid the complexity 
of deep semantic and syntactic processing, we 
complement the advantages of statistical 
techniques from the IR community and struc-
tured information provided by the IE community. 
369
 We propose to extract semi-structured events 
with shallow natural language processing (NLP) 
techniques and estimate their importance for 
inclusion in a summary with IR techniques. 
Though it is most likely that documents nar-
rate more than one similar or related event, most 
event-based summarization techniques reported 
so far explore the importance of the events inde-
pendently. Motivated by this observation, this 
paper addresses the task of event-relevance 
based summarization and explores what sorts of 
relevance make a contribution. To this end, we 
investigate intra-event relevance, that is action-
entity relevance, and inter-event relevance, that 
is event-event relevance. While intra-event rele-
vance is measured with frequencies of the asso-
ciated events and entities directly, inter-event 
relevance is derived indirectly from a general 
WordNet similarity utility, distributional simi-
larity in the documents to be summarized, 
named entity clustering and so on. Pagerank 
ranking algorithm is then applied to estimate the 
event importance for inclusion in a summary 
using the aforesaid relevance.  
The remainder of this paper is organized as 
follows. Section 2 introduces related work. Sec-
tions 3 introduces our proposed event-based 
summarization approaches which make use of 
intra- and inter- event relevance. Section 4 pre-
sents experiments and evaluates different ap-
proaches. Finally, Section 5 concludes the paper. 
2. Related Work 
Event-based summarization has been investi-
gated in recent research. It was first presented in 
(Daniel, Radev and Allison, 2003), who treated 
a news topic in multi-document summarization 
as a series of sub-events according to human 
understanding of the topic. They determined the 
degree of sentence relevance to each sub-event 
through human judgment and evaluated six ex-
tractive approaches. Their paper concluded that 
recognizing the sub-events that comprise a sin-
gle news event is essential for producing better 
summaries. However, it is difficult to automati-
cally break a news topic into sub-events.  
Later, atomic events were defined as the rela-
tionships between the important named entities 
(Filatova and Hatzivassiloglou, 2004), such as 
participants, locations and times (which are 
called relations) through the verbs or action 
nouns labeling the events themselves (which are 
called connectors). They evaluated sentences 
based on co-occurrence statistics of the named 
entity relations and the event connectors in-
volved. The proposed approach claimed to out-
perform conventional tf*idf approach. Appar-
ently, named entities are key elements in their 
model. However, the constraints defining events 
seemed quite stringent.  
The application of dependency parsing, 
anaphora and co-reference resolution in recog-
nizing events were presented involving NLP and 
IE techniques more or less (Yoshioka and Hara-
guchi, 2004), (Vanderwende, Banko and Mene-
zes, 2004) and (Leskovec, Grobelnik and Fral-
ing, 2004). Rather than pre-specifying events, 
these efforts extracted (verb)-(dependent rela-
tion)-(noun) triples as events and took the triples 
to form a graph merged by relations.  
As a matter of fact, events in documents are 
related in some ways. Judging whether the sen-
tences are salient or not and organizing them in 
a coherent summary can take advantage from 
event relevance. Unfortunately, this was ne-
glected in most previous work. Barzilay and La-
pata (2005) exploited the use of the distribu-
tional and referential information of discourse 
entities to improve summary coherence. While 
they captured text relatedness with entity transi-
tion sequences, i.e. entity-based summarization, 
we are particularly interested in relevance be-
tween events in event-based summarization. 
Extractive summarization requires ranking 
sentences with respect to their importance. 
Successfully used in Web-link analysis and 
more recently in text summarization, Google?s 
PageRank (Brin and Page, 1998) is one of the 
most popular ranking algorithms. It is a kind of 
graph-based ranking algorithm deciding on the 
importance of a node within a graph by taking 
into account the global information recursively 
computed from the entire graph, rather than re-
lying on only the local node-specific infor-
mation. A graph can be constructed by adding a 
node for each sentence, phrase or word. Edges 
between nodes are established using inter-
sentence similarity relations as a function of 
content overlap or grammatically relations be-
tween words or phrases.  
The application of PageRank in sentence ex-
traction was first reported in (Erkan and Radev, 
2004). The similarity between two sentence 
nodes according to their term vectors was used 
to generate links and define link strength. The 
same idea was followed and investigated exten-
370
 sively (Mihalcea, 2005). Yoshioka and Haragu-
chi (2004) went one step further toward event-
based summarization. Two sentences were 
linked if they shared similar events. When tested 
on TSC-3, the approach favoured longer sum-
maries. In contrast, the importance of the verbs 
and nouns constructing events was evaluated 
with PageRank as individual nodes aligned by 
their dependence relations (Vanderwende, 2004; 
Leskovec, 2004).  
Although we agree that the fabric of event 
constitutions constructed by their syntactic rela-
tions can help dig out the important events, we 
have two comments. First, not all verbs denote 
event happenings. Second, semantic similarity 
or relatedness between action words should be 
taken into account. 
3. Event-based Summarization 
3.1. Event Definition and Event Map 
Events can be broadly defined as ?Who did 
What to Whom When and Where?. Both lin-
guistic and empirical studies acknowledge that 
event arguments help characterize the effects of 
a verb?s event structure even though verbs or 
other words denoting event determine the se-
mantics of an event. In this paper, we choose 
verbs (such as ?elect?) and action nouns (such as 
?supervision?) as event terms that can character-
ize or partially characterize actions or incident 
occurrences. They roughly relate to ?did What?. 
One or more associated named entities are con-
sidered as what are denoted by linguists as event 
arguments. Four types of named entities are cur-
rently under the consideration. These are <Per-
son>, <Organization>, <Location> and <Date>. 
They convey the information of ?Who?, 
?Whom?, ?When? and ?Where?. A verb or an 
action noun is deemed as an event term only 
when it presents itself at least once between two 
named entities. 
Events are commonly related with one an-
other semantically, temporally, spatially, caus-
ally or conditionally, especially when the docu-
ments to be summarized are about the same or 
very similar topics. Therefore, all event terms 
and named entities involved can be explicitly 
connected or implicitly related and weave a 
document or a set of documents into an event 
fabric, i.e. an event graphical representation (see 
Figure 1). The nodes in the graph are of two 
types. Event terms (ET) are indicated by rectan-
gles and named entities (NE) are indicated by 
ellipses. They represent concepts rather than 
instances. Words in either their original form or 
morphological variations are represented with a 
single node in the graph regardless of how many 
times they appear in documents. We call this 
representation an event map, from which the 
most important concepts can be pick out in the 
summary. 
 
 
 
Figure 1 Sample sentences and their graphical representation 
 
 
The advantage of representing with separated 
action and entity nodes over simply combining 
them into one event or sentence node is to pro-
vide a convenient way for analyzing the rele-
vance among event terms and named entities 
either by their semantic or distributional similar-
ity. More importantly, this favors extraction of 
concepts and brings the conceptual compression 
available. 
We then integrate the strength of the connec-
tions between nodes into this graphical model in 
terms of the relevance defined from different 
perspectives. The relevance is indicated by 
),( ji nodenoder , where inode  and jnode  repre-
sent two nodes, and are either event terms ( iet ) 
or named entities ( jne ). Then, the significance 
of each node, indicated by )( inodew , is calcu-
<Organization> America Online </Organization> was to buy <Organization> 
Netscape </Organization> and forge a partnership with <Organization> Sun 
</Organization>, benefiting all three and giving technological independence 
from <Organization> Microsoft </Organization>. 
371
 lated with PageRank ranking algorithm. Sec-
tions 3.2 and 3.3 address the issues of deriving 
),( ji nodenoder  according to intra- or/and inter- 
event relevance and calculating )( inodew  in de-
tail. 
3.2 Intra- and Inter- Event Relevance 
We consider both intra-event and inter-event 
relevance for summarization. Intra-event rele-
vance measures how an action itself is associ-
ated with its associated arguments. It is indi-
cated as ),( NEETR  and ),( ETNER  in Table 1 
below. This is a kind of direct relevance as the 
connections between actions and arguments are 
established from the text surface directly. No 
inference or background knowledge is required. 
We consider that when the connection between 
an event term iet  and a named entity jne  is 
symmetry, then TNEETRETNER ),(),( = . Events 
are related as explained in Section 2. By means 
of inter-event relevance, we consider how an 
event term (or a named entity involved in an 
event) associate to another event term (or an-
other named entity involved in the same or dif-
ferent events) syntactically, semantically and 
distributionally. It is indicated by ),( ETETR or 
),( NENER in Table 1 and measures an indirect 
connection which is not explicit in the event 
map needing to be derived from the external 
resource or overall event distribution. 
 Event Term 
(ET) 
Named En-
tity (NE) 
Event Term (ET) ),( ETETR  ),( NEETR  
Named Entity (NE) ),( ETNER  ),( NENER
Table 1 Relevance Matrix 
The complete relevance matrix is: 
??
???
?=
),(),(
),(),(
NENERETNER
NEETRETETR
R  
The intra-event relevance ),( NEETR can be 
simply established by counting how many times 
iet  and jne  are associated, i.e.  
),(),( jijiDocument neetfreqneetr =  (E1) 
One way to measure the term relevance is to 
make use of a general language knowledge base, 
such as WordNet (Fellbaum 1998). Word-
Net::Similarity is a freely available software 
package that makes it possible to measure the 
semantic relatedness between a pair of concepts, 
or in our case event terms, based on WordNet 
(Pedersen, Patwardhan and Michelizzi, 2004). It 
supports three measures. The one we choose is 
the function lesk. 
),(),(),( jijijiWordNet etetlesketetsimilarityetetr ==
      (E2) 
Alternatively, term relevance can be meas-
ured according to their distributions in the speci-
fied documents. We believe that if two events 
are concerned with the same participants, occur 
at same location, or at the same time, these two 
events are interrelated with each other in some 
ways. This observation motivates us to try deriv-
ing event term relevance from the number of 
name entities they share. 
|)()(|),( jijiDocument etNEetNEetetr ?=  (E3) 
Where )( ietNE is the set of named entities iet  
associate. | | indicates the number of the ele-
ments in the set. The relevance of named entities 
can be derived in a similar way. 
|)()(|),( jijiDocument neETneETnener ?=  (E4) 
The relevance derived with (E3) and (E4) are 
indirect relevance. In previous work, a cluster-
ing algorithm, shown in Figure 2, has been pro-
posed (Xu et al 2006) to merge the named en-
tity that refer to the same person (such as 
Ranariddh, Prince Norodom Ranariddh and Presi-
dent Prince Norodom Ranariddh). It is used for 
co-reference resolution and aims at joining the 
same concept into a single node in the event 
map. The experimental result suggests that 
merging named entity improves performance in 
some extend but not evidently. When applying 
the same algorithm for clustering all four types 
of name entities in DUC data, we observe that 
the name entities in the same cluster do not al-
ways refer to the same objects, even when they 
are indeed related in some way. For example, 
?Mississippi? is a state in the southeast United 
States, while ?Mississippi River? is the second-
longest rever in the United States and flows 
through ?Mississippi?. 
Step1: Each name entity is represented by 
ikiii wwwne ...21= , where iw  is the ith 
word in it. The cluster it belongs to, in-
dicated by )( ineC , is initialled by 
ikii www ...21 itself.  
Step2: For each name entity  
           ikiii wwwne ...21=  
For each name entity 
372
 jljjj wwwne ...21= , if )( ineC  is a 
sub-string of )( jneC , then 
)()( ji neCneC = . 
Continue Step 2 until no change occurs. 
Figure 2 The algorithm proposed to merge the 
named entities 
Location Person Date Organization
Mississippi 
 
Professor Sir 
Richard 
Southwood 
first six 
months of 
last year 
Long Beach 
City Council 
Sir Richard 
Southwood 
San Jose City 
Council 
Mississippi 
River 
Richard 
Southwood 
last year 
City Council 
Table 2 Some results of the named entity 
merged 
It therefore provides a second way to measure 
named entity relevance based on the clusters 
found. It is actually a kind of measure of lexical 
similarity. 
??
?=
otherwise      ,0
cluster same in the are ,      ,1
),( jijiCluster
nene
nener
     (E5) 
In addition, the relevance of the named enti-
ties can be sometimes revealed by sentence con-
text. Take the following most frequently used 
sentence patterns as examples: 
 
Figure 3 The example patterns  
Considering that two neighbouring name enti-
ties in a sentence are usually relevant, the fol-
lowing window-based relevance is also experi-
mented with. 
??
?=
otherwise      ,0
size  windowspecified-pre a within are ,      1,
),(
ji
jiPattern
nene
nener
     (E6) 
3.3 Significance of Concepts 
The significance score, i.e. the weight 
)( inodew  of each inode , is then estimated recur-
sively with PageRank ranking algorithm which 
assigns the significance score to each node ac-
cording to the number of nodes connecting to it 
as well as the strength of their connections. The 
equation calculating )( inodew using PageRank 
of a certain inode  is shown as follows. 
)
),(
)(
...
),(
)(
...
),(
)(()1()(
1
1
ti
t
ji
j
i
i
nodenoder
nodew
nodenoder
nodew
nodenoder
nodewddnodew
+++
++?=
 (E7) 
In (E7), jnode ( tj ,...2,1= , ij ? ) are the 
nodes linking to inode . d is the factor used to 
avoid the limitation of loop in the map structure. 
It is set to 0.85 experimentally. The significance 
of each sentence to be included in the summary 
is then obtained from the significance of the 
events it contains. The sentences with higher 
significance are picked up into the summary as 
long as they are not exactly the same sentences. 
We are aware of the important roles of informa-
tion fusion and sentence compression in sum-
mary generation. However, the focus of this pa-
per is to evaluate event-based approaches in ex-
tracting the most important sentences. Concep-
tual extraction based on event relevance is our 
future direction. 
4. Experiments and Discussions 
To evaluate the event based summarization ap-
proaches proposed, we conduct a set of experi-
ments on 30 English document sets provide by 
the DUC 2001 multi-document summarization 
task. The documents are pre-processed with 
GATE to recognize the previously mentioned 
four types of name entities. On average, each set 
contains 10.3 documents, 602 sentences, 216 
event terms and 148.5 name entities. 
To evaluate the quality of the generated 
summaries, we choose an automatic summary 
evaluation metric ROUGE, which has been used 
in DUCs. ROUGE is a recall-based metric for 
fixed length summaries. It bases on N-gram co-
occurrence and compares the system generated 
summaries to human judges (Lin and Hovy, 
2003). For each DUC document set, the system 
creates a summary of 200 word length and pre-
sent three of the ROUGE metrics: ROUGE-1 
(unigram-based), ROUGE-2 (bigram-based), 
and ROUGE-W (based on longest common sub-
sequence weighed by the length) in the follow-
ing experiments and evaluations.  
We first evaluate the summaries generated 
based on ),( NEETR  itself. In the pre-evaluation 
experiments, we have observed that some fre-
<Person>, a-position-name of <Organization>, 
does something. 
<Person> and another <Person> do something. 
373
 quently occurring nouns, such as ?doctors? and 
?hospitals?, by themselves are not marked by 
general NE taggers. But they indicate persons, 
organizations or locations. We compare the 
ROUGE scores of adding frequent nouns or not 
to the set of named entities in Table 3. A noun is 
considered as a frequent noun when its fre-
quency is larger than 10. Roughly 5% improve-
ment is achieved when high frequent nouns are 
taken into the consideration. Hereafter, when we 
mention NE in latter experiments, the high fre-
quent nouns are included. 
),( NEETR  NE Without High 
Frequency Nouns 
NE With High 
Frequency Nouns
ROUGE-1 0.33320 0.34859 
ROUGE-2 0.06260 0.07157 
ROUGE-W 0.12965 0.13471 
Table 3 ROUGE scores using ),( NEETR  itself 
Table 4 below then presents the summariza-
tion results by using ),( ETETR  itself. It com-
pares two relevance derivation approaches, 
WordNetR  and DocumentR . The topic-specific rele-
vance derived from the documents to be summa-
rized outperforms the general purpose Word-Net 
relevance by about 4%. This result is reasonable 
as WordNet may introduce the word relatedness 
which is not necessary in the topic-specific 
documents. When we examine the relevance 
matrix from the event term pairs with the high-
est relevant, we find that the pairs, like ?abort? 
and ?confirm?, ?vote? and confirm?, do reflect 
semantics (antonymous) and associated (causal) 
relations to some degree.  
),( ETETR  Semantic Rele-
vance from 
Word-Net 
Topic-Specific 
Relevance from 
Documents 
ROUGE-1 0.32917 0.34178 
ROUGE-2 0.05737 0.06852 
ROUGE-W 0.11959 0.13262 
Table 4 ROUGE scores using ),( ETETR  itself 
Surprisingly, the best individual result is from 
document distributional similarity DocumentR  
),( NENE  in Table 5. Looking more closely, we 
conclude that compared to event terms, named 
entities are more representative of the docu-
ments in which they are included. In other words, 
event terms are more likely to be distributed 
around all the document sets, whereas named 
entities are more topic-specific and therefore 
cluster in a particular document set more. Ex-
amples of high related named entities in rele-
vance matrix are ?Andrew? and ?Florida?, 
?Louisiana? and ?Florida?. Although their rele-
vance is not as explicit as the same of event 
terms (their relevance is more contextual than 
semantic), we can still deduce that some events 
may happen in both Louisiana and Florida, or 
about Andrew in Florida. In addition, it also 
shows that the relevance we would have ex-
pected to be derived from patterns and clustering 
can also be discovered by ),( NENERDocument . 
The window size is set to 5 experimentally in 
window-based practice.  
),( NENER Relevance 
from 
Documents
Relevance 
from 
Clustering 
Relevance 
from Window-
based Context
ROUGE-1 0.35212 0.33561 0.34466 
ROUGE-2 0.07107 0.07286 0.07508 
ROUGE-W 0.13603 0.13109 0.13523 
Table 5 ROUGE scores using ),( NENER  itself 
Next, we evaluate the integration of 
),( NEETR , ),( ETETR  and ),( NENER . As 
DUC 2001 provides 4 different summary sizes 
for evaluation, it satisfies our desire to test the 
sensibility of the proposed event-based summa-
rization techniques to the length of summaries. 
While the previously presented results are 
evaluated on 200 word summaries, now we 
move to check the results in four different sizes, 
i.e. 50, 100, 200 and 400 words. The experi-
ments results show that the event-based ap-
proaches indeed prefer longer summaries. This 
is coincident with what we have hypothesized. 
For this set of experiments, we choose to inte-
grate the best method from each individual 
evaluation presented previously. It appears that 
using the named entities relevance which is de-
rived from the event terms gives the best 
ROUGE scores in almost all the summery sizes. 
Compared with the results provided in (Filatova 
and Hatzivassiloglou, 2004) whose average 
ROUGE-1 score is below 0.3 on the same data 
set, the significant improvement is revealed. Of 
course, we need to test on more data in the fu-
ture. 
),( NENER 50 100 200 400 
ROUGE-1 0.22383 0.28584 0.35212 0.41612
ROUGE-2 0.03376 0.05489 0.07107 0.10275
ROUGE-W 0.10203 0.11610 0.13603 0.13877
),( NEETR 50 100 200 400 
ROUGE-1 0.22224 0.27947 0.34859 0.41644
ROUGE-2 0.03310 0.05073 0.07157 0.10369
ROUGE-W 0.10229 0.11497 0.13471 0.13850
),( ETETR 50 100 200 400 
374
 ROUGE-1 0.20616 0.26923 0.34178 0.41201
ROUGE-2 0.02347 0.04575 0.06852 0.10263
ROUGE-W 0.09212 0.11081 0.13262 0.13742
),( NEETR + 
),( ETETR + 
),( NENER  
 
50 
 
100 
 
200 
 
400 
ROUGE-1 0.21311 0.27939 0.34630 0.41639
ROUGE-2 0.03068 0.05127 0.07057 0.10579
ROUGE-W 0.09532 0.11371 0.13416 0.13913
Table 6 ROUGE scores using complete R matrix 
and with different summary lengths 
As discussed in Section 3.2, the named enti-
ties in the same cluster may often be relevant but 
not always be co-referred. In the following last 
set of experiments, we evaluate the two ways to 
use the clustering results. One is to consider 
them as related as if they are in the same cluster 
and derive the NE-NE relevance with (E5). The 
other is to merge the entities in one cluster as 
one reprehensive named entity and then use it in 
ET-NE with (E1). The rationality of the former 
approach is validated. 
 Clustering is 
used to derive 
NE-NE 
Clustering is used to 
merge entities and 
then to derive ET-NE
ROUGE-1 0.34072 0.33006 
ROUGE-2 0.06727 0.06154 
ROUGE-W 0.13229 0.12845 
Table 7 ROUGE scores with regard to how to 
use the clustering information 
5. Conclusion 
In this paper, we propose to integrate event-
based approaches to extractive summarization. 
Both inter-event and intra-event relevance are 
investigated and PageRank algorithm is used to 
evaluate the significance of each concept (in-
cluding both event terms and named entities). 
The sentences containing more concepts and 
highest significance scores are chosen in the 
summary as long as they are not the same sen-
tences.  
To derive event relevance, we consider the 
associations at the syntactic, semantic and con-
textual levels. An important finding on the DUC 
2001 data set is that making use of named entity 
relevance derived from the event terms they as-
sociate with achieves the best result. The result 
of 0.35212 significantly outperforms the one 
reported in the closely related work whose aver-
age is below 0.3. We are interested in the issue 
of how to improve an event representation in 
order to build a more powerful event-based 
summarization system. This would be one of our 
future directions. We also want to see how con-
cepts rather than sentences are selected into the 
summary in order to develop a more flexible 
compression technique and to know what char-
acteristics of a document set is appropriate for 
applying event-based summarization techniques.  
 
Acknowledgements 
The work presented in this paper is supported 
partially by Research Grants Council on Hong 
Kong (reference number CERG PolyU5181/03E) 
and partially by National Natural Science Foun-
dation of China (reference number: NSFC 
60573186). 
 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL 2003, pp71-78. 
Christiane Fellbaum. 1998, WordNet: An Electronic 
Lexical Database. MIT Press. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive summarization. In Pro-
ceedings of ACL 2004 Workshop on Summariza-
tion, pp104-111.  
Gunes Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text Sum-
marization. Journal of Artificial Intelligence Re-
search. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of Docu-
ment Semantic Graphs for Document Summariza-
tion. In LinkKDD 2004.  
Lucy Vanderwende, Michele Banko and Arul Mene-
zes. 2004. Event-Centric Summary Generation. In 
Working Notes of DUC 2004. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information. In Working Notes 
of NTCIR-4, Tokyo. 
MUC-7. http://www-nlpir.nist.gov/related_projects/ 
muc/proceeings/ muc_7_toc.html 
Naomi Daniel, Dragomir Radev and Timothy Allison. 
2003. Sub-event based Multi-document Summari-
zation. In Proceedings of the HLT-NAACL 2003 
Workshop on Text Summarization, pp9-16. 
375
 Page Lawrence, Brin Sergey, Motwani Rajeev and 
Winograd Terry. 1998. The PageRank Citation 
Ranking: Bring Order to the Web. Technical Re-
port, Stanford University. 
Rada Mihalcea. 2005. Language Independent Extrac-
tive Summarization. ACL 2005 poster. 
Regina Barzilay and Michael Elhadad. 2005. Model-
ling Local Coherence: An Entity-based Approach. 
In Proceedings of ACL, pp141-148. 
TDT. http://projects.ldc.upenn.edu/TDT. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity ? Measur-
ing the Relatedness of Concepts. In Proceedings of 
AAAI, pp25-29. 
Wei Xu, Wenjie Li, Mingli Wu, Wei Li and Chunfa 
Yuan. 2006. Deriving Event Relevance from the 
Ontology Constructed with Formal Concept 
Analysis, in Proceedings of CiCling?06, pp480-
489. 
 
376
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 37?42,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Investigations on Event-Based Summarization 
 
 
Mingli Wu 
Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
csmlwu@comp.polyu.edu.hk 
 
  
 
Abstract 
We investigate independent and relevant 
event-based extractive mutli-document 
summarization approaches. In this paper, 
events are defined as event terms and as-
sociated event elements. With independ-
ent approach, we identify important con-
tents by frequency of events. With rele-
vant approach, we identify important 
contents by PageRank algorithm on the 
event map constructed from documents. 
Experimental results are encouraging. 
1 Introduction 
With the growing of online information, it is in-
efficient for a computer user to browse a great 
number of individual news documents. Auto-
matic summarization is a powerful way to over-
come such difficulty. However, the research lit-
erature demonstrates that machine summaries 
need to be improved further.  
The previous research on text summarization 
can date back to (Luhn 1958) and (Edmundson 
1969). In the following periods, some researchers 
focus on extraction-based summarization, as it is 
effective and simple. Others try to generate ab-
stractions, but these works are highly domain-
dependent or just preliminary investigations. Re-
cently, query-based summarization has received 
much attention. However, it is highly related to 
information retrieval, another research subject. In 
this paper, we focus on generic summarization. 
News reports are crucial to our daily life. In this 
paper, we focus on effective summarization ap-
proaches for news reports.  
Extractive summarization is widely investi-
gated in the past. It extracts part of document(s) 
based on some weighting scheme, in which dif-
ferent features are exploited, such as position in 
document, term frequency, and key phrases. Re-
cent extraction approaches may also employ ma-
chine learning approaches to decide which sen-
tences or phrases should be extracted. They 
achieve preliminary success in different applica-
tion and wait to be improved further. 
Previous extractive approaches identify the 
important content mainly based on terms. Bag of 
words is not a good representation to specify an 
event. There are multiple possible explanations 
for the same collection of words. A predefined 
template is a better choice to represent the event. 
However it is domain-dependent and need much 
effort to create and fill it. This tension motivates 
us to seek a balance between effective imple-
mentation and deep understanding. 
According to related works (Filatovia and 
Hatzivassiloglou, 2004) (Vanderwende et al, 
2004), we assume that event may be a natural 
unit to convey meanings of documents. In this 
paper, event is defined as the collection of event 
terms and associated event elements in clause 
level. Event terms express the meaning of actions 
themselves, such as ?incorporate?. In addition to 
verbs, action nouns can also express meaning of 
actions and should be regarded as event terms. 
For example, ?incorporation? is action noun. 
Event elements include named entities, such as 
person name, organization name, location, time. 
These named entities are tagged with GATE 
(Cunningham et al, 2002). Based on our event 
definition, independent and relevant event-based 
approaches are investigated in this research. Ex-
periments show that both of them achieve en-
couraging results.  
The related works are discussed in Section 2. 
Independent event-based summarization ap-
proach is described in Section 3. Relevant event-
based summarization approach is described in 
Section 4. Section 5 presents the experiments and 
37
evaluations. Then the strength and limitation of 
our approaches are discussed in Section 6. Fi-
nally, we conclude the work in Section 7. 
2 Related Work 
Term-based extractive summarization can date 
back to (Luhn, 1958) and (Edmundson, 1969). 
This approach is simple but rather applicable. It 
represents the content of documents mainly by 
bag of words. Luhn (1958) establishes a set of 
?significant? words, whose frequency is between 
a higher bound and a lower bound. Edmundson 
(1969) collects common words, cue words, ti-
tle/heading words from documents. Weight 
scores of sentences are computed based on 
type/frequency of terms. Sentences with higher 
scores will be included in summaries. Later re-
searchers adopt tf*idf score to discriminate 
words (Brandow et al, 1995) (Radev et al, 
2004). Other surface features are also exploited 
to extract important sentence, such as position of 
sentence and length of sentence (Teufel and 
Moens, 1999) (Radev et al, 2004). To make the 
extraction model suitable for documents in dif-
ferent domains, recently machine learning ap-
proaches are widely employed (Kupiec et al, 
1995) (Conroy and Schlesinger, 2004).  
To represent deep meaning of documents, 
other researchers have investigated different 
structures. Barzilay and Elhadad (1997) segment 
the original text and construct lexical chains. 
They employ strong chains to represent impor-
tant parts of documents. Marcu (1997) describes 
a rhetorical parsing approach which takes unre-
stricted text as input and derives the rhetorical 
structure tree. They express documents with 
structure trees. Dejong (1978) adopts predefined 
templates to express documents. For each topic, 
the user predefines frames of expected informa-
tion types, together with recognition criteria. 
However, these approaches just achieve moder-
ate results. 
Recently, event receives attention to represent 
documents. Filatovia and Hatzivassiloglou 
(2004) define event as action (verbs/action 
nouns) and named entities. After identifying ac-
tions and event entities, they adopt frequency 
weighting scheme to identify important sentence. 
Vanderwende et al (2004) represent event by 
dependency triples. After analysis of triples they 
connect nodes (words or phrases) by way of se-
mantic relationships. Yoshioka and Haraguchi 
(2004) adopt a similar approach to build a map, 
but they regard sentence as the nodes of the map. 
After construction of a map representation for 
documents, Vanderwende et al (2004), and Yo-
shioka and Haraguchi (2004) all employ PageR-
ank algorithm to select the important sentences. 
Although these approaches employ event repre-
sentation and PageRank algorithm, it should be 
noted that our event representation is different 
with theirs. Our event representation is based on 
named entities and event terms, without help of 
dependency parsing. These previous event-based 
approaches achieved promising results.  
3 Independent Event-based Summari-
zation 
Based on our observation, we assume that events 
in the documents may have different importance. 
Important event terms will be repeated and al-
ways occur with more event elements, because 
reporters hope to state them clearly. At the same 
time, people may omit time or location of an im-
portant event after they describe the event previ-
ously. Therefore in our research, event terms oc-
curs in different circumstances will be assigned 
different weights. Event terms occur between 
two event elements should be more important 
than event terms occurring just beside one event 
elements. Event terms co-occurring with partici-
pants may be more important than event terms 
just beside time or location.  
The approach on independent event-based 
summarization involves following steps.  
1. Given a cluster of documents, analyze 
each sentence one at a time. Ignore sen-
tences that do not contain any event ele-
ment. 
2. Tag the event terms in the sentence, which 
is between two event elements or near an 
event element with the distance limitation. 
For example, [Event Element A, Even 
Term, Event Element B], [Event Term, 
Event Element A], [Event Element A, 
Event Term] 
3. Assign different weights to different event 
terms, according to contexts of event 
terms. Different weight configurations are 
described in Section 5.2. Contexts refer to 
number of event elements beside event 
terms and types of these event elements. 
4. Get the average tf*idf score as the weight 
of every event term or event element. The 
algorithm is similar with Centroid.  
38
5. Sum up the weights of event terms and 
event elements in a sentence. 
6. Select the top sentences with highest 
weights, according to the length of sum-
mary. 
4 Relevant Event-based Summarization  
Independent event-based approaches do not ex-
ploit relevance between events. However, we 
think that it may be useful to identify important 
events. After a document is represented by 
events, relevant events are linked together. We 
made the assumption that important events may 
be mentioned often and events associated to im-
portant events may be important also. PageRank 
is a suitable algorithm to identify the importance 
of events from a map, according to the previous 
assumption. In the following sections, we will 
discuss how to represent documents by events 
and how to identify important event with PageR-
ank algorithm. 
4.1 Document Representation 
We employ an event map to represent content of 
a document cluster, which is about a certain 
topic. In an event map, nodes are event terms or 
event elements, and edges represent association 
or modification between two nodes. Since the 
sentence is a natural unit to express meanings, 
we assume that all event terms in a sentence are 
all relevant and should be linked together. The 
links between every two nodes are undirectional.  
In an ideal case, event elements should be 
linked to the associated event terms. At the same 
time, an event element may modify another ele-
ment. For example, one element is a head noun 
and another one is the modifier. An event term 
(e.g., verb variants) may modify an event ele-
ment or event term of another event. In this case, 
a full parser should be employed to get associa-
tions or modifications between different nodes in 
the map. Because the performance of current 
parsing technology is not perfect, an effective 
approach is to simulate the parse tree to avoid 
introducing errors of a parser. The simplifica-
tions are described as follows. Only event ele-
ments are attached with corresponding event 
terms. An event term will not be attached to an 
event element of another event. Also, an event 
element will not be attached to another event 
element. Heuristics are used to attach event ele-
ments with corresponding event terms. 
Given a sentence ?Andrew had become little 
more than a strong rainstorm early yesterday, 
moving across Mississippi state and heading for 
the north-eastern US?, the event map is shown in 
Fig. 1. After each sentence is represented by a 
map, there will be multiple maps for a cluster of 
documents. If nodes from different maps are 
lexical match, they may denote same thing and 
should be linked. For example, if named entity 
?Andrew? occurred in Sentence A, B and C, then 
the three occurrences OA, OB and OC will be 
linked as OA?OB, OB?OC, OC?OA. By this 
way, maps for sentences can be linked based on 
same concepts. 
 
 
Figure 1. Document representation with event 
map 
4.2 Importance Identification by PageRank 
Given a whole map for a cluster of documents, 
the next step is to identify focus of these docu-
ments. Based on our assumption about important 
content in the previous section, PageRank algo-
rithm (Page et al, 1998) is employed to fulfill 
this task. PageRank assumes that if a node is 
connected with more other nodes, it may be more 
likely to represent a salient concept. The nodes 
relevant to the significant nodes are closer to the 
salient concept than those not. The algorithm 
assigns the significance score to each node ac-
cording to the number of nodes linking to it as 
well as the significance of the nodes. In PageR-
ank algorithm, we use two directional links in-
stead for every unidirectional link in Figure 1. 
 The equation to calculate the importance (in-
dicated by PR) of a certain node A is shown as 
follows: 
)
)(
)(...
)(
)(
)(
)(()1()(
2
2
1
1
t
t
BC
BPR
BC
BPR
BC
BPRddAPR ++++?=
 
Where B1, B2,?, Bt are all nodes which link to 
the node A. C(Bi) is the number of outgoing links 
from the node Bi. The weight score of each node 
can be gotten by this equation recursively. d is 
the factor used to avoid the limitation of loop in 
the map structure. As the literature (Page et al, 
1998) suggested, d is set as 0.85. The signifi-
cance of each sentence to be included in the    
39
summary is then derived from the significance of 
the event terms and event elements it contains. 
5 Evaluation 
5.1 Dataset and Evaluation Metrics 
DUC 2001 dataset is employed to evaluate our 
summarization approaches. It contains 30 clus-
ters and a total of 308 documents. The number of 
documents in each cluster is between 3 and 20. 
These documents are from some English news 
agencies, such as Wall Street Journal. The con-
tents of each cluster are about some specific 
topic, such as the hurricane in Florida. For each 
cluster, there are 3 different model summaries, 
which are provided manually. These model 
summaries are created by NIST assessors for the 
DUC task of generic summarization. Manual 
summaries with 50 words, 100 words, 200 words 
and 400 words are provided. 
Since manual evaluation is time-consuming 
and may be subjective, the typical evaluation 
package, ROUGE (Lin and Hovy, 2003), is em-
ployed to test the quality of summaries. ROUGE 
compares the machine-generated summaries with 
manually provided summaries, based on uni-
gram overlap, bigram overlap, and overlap with 
long distance. It is a recall-based measure and 
requires that the length of the summaries be lim-
ited to allow meaningful comparison. ROUGE is 
not a comprehensive evaluation method and in-
tends to provide a rough description about the 
performance of machine generated summary. 
5.2 Experimental Configuration 
In the following experiments for independent 
event-based summarization, we investigate the 
effectiveness of the approach. In addition, we 
attempt to test the importance of contextual in-
formation in scoring event terms. The number of 
associated event terms and the type of event 
terms are considered to set the weights of event 
terms. The weights parameters in the following 
experiments are chosen according to empirical 
estimations. 
Experiment 1: Weight of any entity is 1. 
Weight of any verb/action noun, which is be-
tween two entities or just beside one entity, is 1. 
Experiment 2: Weight of any entity is 1. 
Weight of any verb/action noun, which is be-
tween two entities, is 3. Weight of any 
verb/action noun, which is just beside one entity, 
is 1. 
Experiment 3: Weight of any entity is 1. 
Weight of any verb/action noun, which is be-
tween two entities and the first entity is person or 
organization, is 5. Weight of any verb/action 
noun, which is between two entities and the first 
entity is not person and not organization, is 3. 
Weight of any verb/action noun, which is just 
after a person or organization, is 2. Weight of 
any verb/action noun, which is just before one 
entity, is 1. Weight of any verb/action noun, 
which is just after one entity and the entity is not 
person and not organization, is 1. 
In the following experiments, we investigate 
the effectiveness of our approaches on under dif-
ferent length limitation of summary. Based on 
the algorithm of experiment 3, we design ex-
periment to generate summaries with length 50 
words, 100 words, 200 words, 400 words. They 
are named Experiment 4, Experiment 5, Ex-
periment 3 and Experiment 6. 
In other experiments for relevant event-based 
summarization, we investigate the function of 
relevance between events. The configurations are 
described as follows. 
Experiment 7: Event terms and event ele-
ments are identified as we discussed in Section 3. 
In this experiment, event elements just include 
named entities. Occurrences of event terms or 
event elements are linked with by exact matches. 
Finally, the PageRank is employed to select im-
portant events and then important sentences. 
Experiment 8: For reference, we select one of 
the four model summaries as the final summary 
for each cluster of documents. ROUGE is em-
ployed to evaluate the performance of these 
manual summaries. 
5.3 Experimental Results 
The experiment results on independent event-
based summarization are shown in table 1. The 
results for relevant event-based summarization 
are shown in table 3. 
 
 Exp. 1 Exp. 2 Exp. 3 
Rouge-1 0.315 0.322 0.323 
Rouge-2 0.049 0.055 0.055 
Rouge-L 0.299 0.305 0.306 
Table 1. Results on independent event-based 
summarization (summary with length of 200 
words) 
 
From table 1, we can see that results of Ex-
periment 2 are better than those of Experiment 1. 
It proves our assumption that importance of 
event terms is different when these event terms 
occur with different number of event elements. 
Results of Experiment 3 are not significant better 
than those of Experiment 2, so it seems that the 
40
assumption that importance of event terms is not 
very different when these event terms occur with 
different types of event elements. Another possi-
ble explanation is that after adjustment of the 
weight for event terms, the difference between 
the results of Experiment 2 and Experiment 3 
may be extended.  
\ 
Table 2. Results on independent event-based 
summarization (summary with different length)  
 
Four experiments of table 2 show that per-
formance of our event based summarization are 
getting better, when the length of summaries is 
expanded. One reason is that event based ap-
proach prefers sentences with more event terms 
and more event elements, so the preferred 
lengths of sentences are longer. While in a short 
summary, people always condense sentences 
from original documents, and use some new 
words to substitute original concepts in docu-
ments. Then the Rouge score, which evaluates 
recall aspect, is not good in our event-based ap-
proach. In contrast, if the summaries are longer, 
people will adopt detail event descriptions in 
original documents, and so our performance is 
improved. 
 
 
 Exp. 7 Exp. 8 
Rouge-1 0.325 0.595 
Rouge-2 0.060 0.394 
Rouge-L 0.305 0.586 
Table 3. Results on relevant event-based 
summarization and a reference experiment 
(summary with length of 200 words) 
 
In table 3, we found the Rouge-score of rele-
vant event-based summarization (Experiment 7) 
is better than independent approach (Experiment 
1). In Experiment 1, we do not discriminate the 
weight of event element and event terms. In Ex-
periment 7, we also did not discriminate the 
weight of event element and event terms. It is 
fair to compare Experiment 7 with Experiment 1 
and it?s unfair to compare Experiment 7 with 
Experiment 3. It looks like the relevance between 
nodes (event terms or event elements) can help to 
improve the performance. However, performance 
of both dependent and independent event-based 
summarization need to be improved further, 
compared with human performance in Experi-
ment 8. 
6 Discussion 
As discussed in Section 2, event-based ap-
proaches are also employed in previous works. 
We evaluate our work in this context. As event-
based approaches in this paper are similar with 
that of Filatovia and Hatzivassiloglou (2004), and 
the evaluation data set is the same one, the re-
sults are compared with theirs.  Exp. 4 Exp. 5 Exp. 3 Exp. 6 
Rouge-1 0.197 0.249 0.323 0.382 
Rouge-2 0.021 0.031 0.055 0.081 
Rouge-L 0.176 0.231 0.306 0.367 
 
Fi t-gure 2. Results reported in (Filatovia and Ha
zivassiloglou 2004) 
 
 
Figure 3. Results of relevant event-based ap-
proach 
 
Filatovia and Hatzivassiloglou (2004) report 
the ROUGE scores according to each cluster of 
DUC 2001 data collection in Figure 2. In this 
figure, the bold line represents their event-based 
approach and the light line refers to tf*idf ap-
proach. It can be seen that the event-based ap-
proach performs better. The evaluation of the 
relevant event-based approach presented this pa-
per is shown in Figure 3. The proposed approach 
achieves significant improvement on most 
document clusters. The reason seems that the 
relevance between events is exploited.  
Centroid is a successful term-based summari-
zation approach. For caparison, we employ 
MEAD (Radev et.al., 2004) to generate Cen-
troid-based summaries. Results show that Cen-
troid is better than our relevant event-based ap-
proach. After comparing the summaries given by 
the two approaches, we found some limitation of 
our approach.   
41
Event-based approach does not work well on 
documents with rare events. We plan to dis-
criminate the type of documents and apply event-
based approach on suitable documents. Our rele-
vant event-based approach is instance-based and 
too sensitive to number of instances of entities. 
Concepts seem better to represent meanings of 
events, as they are really things we care about. In 
the future, the event map will be build based on 
concepts and relationships between them. Exter-
nal knowledge may be exploited to refine this 
concept map. 
7 Conclusion 
In this study, we investigated generic summari-
zation. An event-based scheme was employed to 
represent document and identify important con-
tent. The independent event-based approach 
identified important content according to event 
frequency. We also investigated the different 
importance of event terms in different context. 
Experiment showed that this idea achieved prom-
ising results. Then we explored summarization 
under different length limitation. We found that 
our independent event-based approaches acted 
well with longer summaries. 
In the relevant event-based approach, events 
were linked together by same or similar event 
terms and event elements. Experiments showed 
that the relevance between events can improve 
the performance of summarization. Compared 
with close related work, we achieved encourag-
ing improvement.  
References  
Regina Barzilay, and Michael Elhadad. 1997. Using 
lexical chains for text summarization. In Proceed-
ings of the ACL?97/EACL?97 Workshop on Intel-
ligent Scalable Text Summarization, 10-17. 
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995. 
Automatic condensation of electronic publications 
by sentence selection. Information Processing and 
Management 31(5):675-686. 
John M. Conroy and Judith D. Schlesinger. 2004. 
Left-brain/right-brain multi-document summariza-
tion. Available at http://duc.nist.gov/pubs.html
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva, Valentin Tablan. 2002. GATE: a 
framework and graphical development environ-
ment for robust NLP tools and applications. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for computational Linguistics (ACL?02). 
Gerald Francis DeJong. 1978. Fast skimming of news 
stories: the FRUMP system. Ph.D. thesis, Yale 
University. 
H.P. Edmundson. 1969. New methods in automatic 
extracting. Journal of the Association for comput-
ing machinery, 16(2):264-285. 
Elena Filatova and Vasileios Hatzivassiloglou. Event-
based extractive summarization. 2004. In Proceed-
ings of the ACL-04 Workshop, 104-111. 
Julian Kupiec, Jan Pedersen and Francine Chen. 1995. 
A trainable document summarizer. In Proceedings 
of the 18th ACM-SIGIR conference, 68-73. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL, Edmonton, Canada, May. 
H.P. Luhn. 1958. The automatic creation of literature 
abstracts. IBM Journal of Research and Develop-
ment 2:159-165. 
Daniel Marcu. 1997. The rhetorical parsing of natural 
language texts. In Proceedings of the 35th Annual 
Meeting of the Association for computational Lin-
guistics (ACL?97), 96-103. 
Dragomir R. Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko 
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, 
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio 
Saggion, Simone Teufel, Michael Topper, Adam 
Winkel, Zhu Zhang. 2004. MEAD - a platform for 
multidocument multilingual text summarization. 
LREC 2004. 
Simone Teufel and Marc Moens. 1999. Argumenta-
tive classification of extracted sentences as a first 
step towards flexible abstracting. Advances in 
Automatic Text Summarization, Inderjeet Mani 
and Mark T. Maybury (editors), 137-154. Cam-
bridge, Massachusetts: MIT Press. 
Larry Page, Sergey Brin, et al 1998. The PageRank 
Citation Ranking: Bring Order to the Web. Techni-
cal Report, Stanford University, 1998. 
Lucy Vanderwende, Michele Banko, and Arul 
Menezes. 2004. Event-centric summary generation. 
Available at http://duc.nist.gov/pubs.html
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple news articles summarization based on 
event reference information. In Working Notes of 
the Fourth NTCIR Workshop Meeting, National 
Institute of Informatics, 2004. 
 
42
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 185?188,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extractive Summarization Based on Event Term Clustering 
Maofu Liu1,2, Wenjie Li1, Mingli Wu1 and Qin Lu1 
 
1Department of Computing 
The Hong Kong Polytechnic University 
{csmfliu, cswjli, csmlwu, 
csluqin}@comp.polyu.edu.hk 
2College of Computer Science and Technology 
Wuhan University of Science and Technology 
mfliu_china@hotmail.com 
 
Abstract 
Event-based summarization extracts and 
organizes summary sentences in terms of 
the events that the sentences describe. In 
this work, we focus on semantic relations 
among event terms. By connecting terms 
with relations, we build up event term 
graph, upon which relevant terms are 
grouped into clusters. We assume that each 
cluster represents a topic of documents. 
Then two summarization strategies are 
investigated, i.e. selecting one term as the 
representative of each topic so as to cover 
all the topics, or selecting all terms in one 
most significant topic so as to highlight the 
relevant information related to this topic. 
The selected terms are then responsible to 
pick out the most appropriate sentences 
describing them. The evaluation of 
clustering-based summarization on DUC 
2001 document sets shows encouraging 
improvement over the well-known 
PageRank-based summarization. 
1 Introduction 
Event-based extractive summarization has emerged 
recently (Filatova and Hatzivassiloglou, 2004). It 
extracts and organizes summary sentences in terms 
of the events that sentences describe.  
We follow the common agreement that event 
can be formulated as ?[Who] did [What] to [Whom] 
[When] and [Where]? and ?did [What]? denotes 
the key element of an event, i.e. the action within 
the formulation. We approximately define the 
verbs and action nouns as the event terms which 
can characterize or partially characterize the event 
occurrences. 
Most existing event-based summarization 
approaches rely on the statistical features derived 
from documents and generally associated with 
single events, but they neglect the relations among 
events. However, events are commonly related 
with one another especially when the documents to 
be summarized are about the same or very similar 
topics. Li et al(2006) report that the improved 
performance can be achieved by taking into 
account of event distributional similarities, but it 
does not benefit much from semantic similarities. 
This motivated us to further investigate whether 
event-based summarization can take advantage of 
the semantic relations of event terms, and most 
importantly, how to make use of those relations. 
Our idea is grouping the terms connected by the 
relations into the clusters, which are assumed to 
represent some topics described in documents. 
In the past, various clustering approaches have 
been investigated in document summarization. 
Hatzivassiloglou et al(2001) apply clustering 
method to organize the highly similar paragraphs 
into tight clusters based on primitive or composite 
features. Then one paragraph per cluster is selected 
to form the summary by extraction or by 
reformulation. Zha (2002) uses spectral graph 
clustering algorithm to partition sentences into 
topical groups. Within each cluster, the saliency 
scores of terms and sentences are calculated using 
mutual reinforcement principal, which assigns high 
salience scores to the sentences that contain many 
terms with high salience scores. The sentences and 
key phrases are selected by their saliency scores to 
generate the summary. The similar work based on 
topic or event is also reported in (Guo and Stylios, 
2005).
The granularity of clustering units mentioned 
above is rather coarse, either sentence or paragraph. 
In this paper, we define event term as clustering 
185
unit and implement a clustering algorithm based on 
semantic relations. We extract event terms from 
documents and construct the event term graph by 
linking terms with the relations. We then regard a 
group of closely related terms as a topic and make 
the following two alterative assumptions:  
(1) If we could find the most significant topic as 
the main topic of documents and select all terms in 
it, we could summarize the documents with this 
main topic.  
(2) If we could find all topics and pick out one 
term as the representative of each topic, we could 
obtain the condensed version of topics described in 
the documents.  
Based on these two assumptions, a set of cluster 
ranking, term selection and ranking and sentence 
extraction strategies are developed. The remainder 
of this paper is organized as follows. Section 2 
introduces the proposed extractive summarization 
approach based on event term clustering. Section 3 
presents experiments and evaluations. Finally, 
Section 4 concludes the paper. 
2 Summarization Based on Event Term 
Clustering 
2.1 Event Term Graph 
We introduce VerbOcean (Chklovski and Pantel, 
2004), a broad-coverage repository of semantic 
verb relations, into event-based summarization. 
Different from other thesaurus like WordNet, 
VerbOcean provides five types of semantic verb 
relations at finer level. This just fits in with our 
idea to introduce event term relations into 
summarization. Currently, only the stronger-than 
relation is explored. When two verbs are similar, 
one may denote a more intense, thorough, 
comprehensive or absolute action. In the case of 
change-of-state verbs, one may denote a more 
complete change. This is identified as the stronger-
than relation in (Timothy and Patrick, 2004). In 
this paper, only stronger-than is taken into account 
but we consider extending our future work with 
other applicable relations types.  
The event term graph connected by term 
semantic relations is defined formally as 
, where V is a set of event terms and E 
is a set of relation links connecting the event terms 
in V. The graph is directed if the semantic relation 
has the characteristic of the asymmetric. Otherwise, 
it is undirected. Figure 1 shows a sample of event 
term graph built from one DUC 2001 document set. 
It is a directed graph as the stronger-than relation 
in VerbOcean exhibits the conspicuous asymmetric 
characteristic. For example, ?fight? means to 
attempt to harm by blows or with weapons, while 
?resist? means to keep from giving in. Therefore, a 
directed link from ?fight? to ?resist? is shown in 
the following Figure 1.  
),( EVG =
Relations link terms together and form the event 
term graph. Based upon it, term significance is 
evaluated and in turn sentence is judged whether to 
be extracted in the summary. 
 
Figure 1. Terms connected by semantic relations 
2.2 Event Term Clustering 
Note that in Figure 1, some linked event terms, 
such as ?kill?, ?rob?, ?threaten? and ?infect?, are 
semantically closely related. They may describe 
the same or similar topic somehow. In contrast, 
?toler?, ?resist? and ?fight? are clearly involved in 
another topic; although they are also reachable 
from ?kill?. Based on this observation, a clustering 
algorithm is required to group the similar and 
related event terms into the cluster of the topic.  
In this work, event terms are clustered by the 
DBSCAN, a density-based clustering algorithm 
proposed in (Easter et al 1996). The key idea 
behind it is that for each term of a cluster the 
neighborhood of a given radius has to contain at 
least a minimum number of terms, i.e. the density 
in the neighborhood has to exceed some threshold. 
By using this algorithm, we need to figure out 
appropriate values for two basic parameters, 
namely, Eps (denoting the searching radius from 
each term) and MinPts (denoting the minimum 
number of terms in the neighborhood of the term). 
We assign one semantic relation step to Eps since 
there is no clear distance concept in the event term 
186
graph. The value of Eps is experimentally set in 
our experiments. We also make some modification 
on Easter?s DBSCAN in order to accommodate to 
our task.  
Figure 2 shows the seven term clusters 
generated by the modified DBSCAN clustering 
algorithm from the graph in Figure 1. We represent 
each cluster by the starting event term in bold font.  
fight
resist
consider
expect
announce
offer
list public
accept
honor
publish study
found
place
prepare
toler
pass
fear
threaten
kill
feel suffer
live
survive
undergo
ambush
rob
infect
endure
run
moverush
report
investigate
file
satisfy
please
manage
accept
Figure 2. Term clusters generated from Figure 1 
2.3 Cluster Ranking 
The significance of the cluster is calculated by  
? ??
? ??
=
CC Ct
t
Ct
ti
i ii
ddCsc /)(  
where  is the degree of the term t  in the term 
graph. C  is the set of term clusters obtained by the 
modified DBSCAN clustering algorithm and  is 
the ith one. Obviously, the significance of the 
cluster is calculated from global point of view, i.e. 
the sum of the degree of all terms in the same 
cluster is divided by the total degree of the terms in 
all clusters. 
td
iC
2.4 Term Selection  and Ranking 
Representative terms are selected according to the 
significance of the event terms calculated within 
each cluster (i.e. from local point of view) or in all 
clusters (i.e. from global point of view) by  
LOCAL:  or ?
?
=
ict
tt ddtst /)(
GLOBAL:  ? ?
? ?
=
Cc ct
tt
i i
ddtst /)(
Then two strategies are developed to select the 
representative terms from the clusters.  
(1) One Cluster All Terms (OCAT) selects all 
terms within the first rank cluster. The selected 
terms are then ranked according to their 
significance.  
(2) One Term All Cluster (OTAC) selects one 
most significant term from each cluster. Notice that 
because terms compete with each other within 
clusters, it is not surprising to see )()( 21 tsttst <  
even when , . To 
address this problem, the representative terms are 
ranked according to the significance of the clusters 
they belong to.  
)()( 21 csccsc > ),( 2211 ctct ??
2.5 Sentence Evaluation and Extraction 
A representative event term may associate to more 
than one sentence. We extract only one of them as 
the description of the event. To this end, sentences 
are compared according to the significance of the 
terms in them. MAX compares the maximum 
significance scores, while SUM compares the sum 
of the significance scores. The sentence with either 
higher MAX or SUM wins the competition and is 
picked up as a candidate summary sentence. If the 
sentence in the first place has been selected by 
another term, the one in the second place is chosen. 
The ranks of these candidates are the same as the 
ranks of the terms they are selected for. Finally, 
candidate sentences are selected in the summary 
until the length limitation is reached. 
3 Experiments 
We evaluate the proposed approaches on DUC 
2001 corpus which contains 30 English document 
sets. There are 431 event terms on average in each 
document set. The automatic evaluation tool, 
ROUGE (Lin and Hovy, 2003), is run to evaluate 
the quality of the generated summaries (200 words 
in length). The tool presents three values including 
unigram-based ROUGE-1, bigram-based ROUGE-
2 and ROUGE-W which is based on longest 
common subsequence weighted by the length. 
Google?s PageRank (Page and Brin, 1998) is 
one of the most popular ranking algorithms. It is 
also graph-based and has been successfully applied 
in summarization. Table 1 lists the result of our 
implementation of PageRank based on event terms. 
We then compare it with the results of the event 
term clustering-based approaches illustrated in 
Table 2. 
 PageRank  
ROUGE-1 0.32749 
187
ROUGE-2 0.05670 
ROUGE-W 0.11500 
Table 1. Evaluations of PageRank-based 
Summarization 
LOCAL+OTAC MAX SUM 
ROUGE-1 0.32771 0.33243
ROUGE-2 0.05334 0.05569
ROUGE-W 0.11633 0.11718
GLOBAL+OTAC MAX SUM 
ROUGE-1 0.32549 0.32966
ROUGE-2 0.05254 0.05257
ROUGE-W 0.11670 0.11641
LOCAL+OCAT MAX SUM 
ROUGE-1 0.33519 0.33397
ROUGE-2 0.05662 0.05869
ROUGE-W 0.11917 0.11849
GLOBAL+OCAT MAX SUM 
ROUGE-1 0.33568 0.33872
ROUGE-2 0.05506 0.05933
ROUGE-W 0.11795 0.12011
Table 2. Evaluations of Clustering-based  
Summarization 
The experiments show that both assumptions are 
reasonable. It is encouraging to find that our event 
term clustering-based approaches could outperform 
the PageRank-based approach. The results based 
on the second assumption are even better. This 
suggests indeed there is a main topic in a DUC 
2001 document set. 
4 Conclusion 
In this paper, we put forward to apply clustering 
algorithm on the event term graph connected by 
semantic relations derived from external linguistic 
resource. The experiment results based on our two 
assumptions are encouraging. Event term 
clustering-based approaches perform better than 
PageRank-based approach. Current approaches 
simply utilize the degrees of event terms in the 
graph. In the future, we would like to further 
explore and integrate more information derived 
from documents in order to achieve more 
significant results using the event term clustering-
based approaches. 
Acknowledgments 
The work described in this paper was fully 
supported by a grant from the Research Grants 
Council of the Hong Kong Special Administrative 
Region, China (Project No. PolyU5181/03E). 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram 
Cooccurrence Statistics. In Proceedings of HLT/ 
NAACL 2003, pp71-78. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In 
Proceedings of ACL 2004 Workshop on 
Summarization, pp104-111. 
Hongyuan Zha. 2002. Generic Summarization and 
keyphrase Extraction using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of the 25th annual international ACM SIGIR 
conference on Research and development in 
information retrieval, 2002. pp113-120.   
Lawrence Page and Sergey Brin, Motwani Rajeev 
and Winograd Terry. 1998. The PageRank 
CitationRanking: Bring Order to the Web. 
Technical Report,Stanford University. 
Martin Easter, Hans-Peter Kriegel, J?rg Sander, et al 
1996. A Density-Based Algorithm for Discovering 
Clusters in Large Spatial Databases with Noise. In 
Proceedings of the 2nd International Conference 
on Knowledge Discovery and Data Mining, Menlo 
Park, CA, 1996. 226-231.  
Lawrence Page, Sergey Brin, Rajeev Motwani and 
Terry Winograd. 1998. The PageRank 
CitationRanking: Bring Order to the Web. 
Technical Report,Stanford University. 
Timothy Chklovski and Patrick Pantel. 2004. 
VerbOcean: Mining the Web for Fine-Grained 
Semantic Verb Relations. In Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing, 2004. 
Vasileios Hatzivassiloglou, Judith L. Klavans, 
Melissa L. Holcombe, et al 2001. Simfinder: A 
Flexible Clustering Tool for Summarization. In 
Workshop on Automatic Summarization, NAACL, 
2001. 
Wenjie Li, Wei Xu, Mingli Wu, et al 2006. 
Extractive Summarization using Inter- and Intra- 
Event Relevance. In Proceedings of ACL 2006, 
pp369-376. 
Yi Guo and George Stylios. 2005. An intelligent 
summarization system based on cognitive 
psychology. Journal of Information Sciences, 
Volume 174, Issue 1-2, Jun. 2005, pp1-36. 
188
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985?992
Manchester, August 2008
Extractive Summarization Using Supervised and Semi-supervised 
Learning 
Kam-Fai Wong*, Mingli Wu*?  
*Department of Systems Engineering and 
Engineering Management  
The Chinese University of Hong Kong 
New Territories, Hong Kong 
{kfwong,mlwu}@se.cuhk.edu.hk
Wenjie Li? 
?Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
cswjli@comp.polyu.edu.hk 
 
 
Abstract 
It is difficult to identify sentence impor-
tance from a single point of view. In this 
paper, we propose a learning-based ap-
proach to combine various sentence fea-
tures. They are categorized as surface, 
content, relevance and event features. 
Surface features are related to extrinsic 
aspects of a sentence. Content features 
measure a sentence based on content-
conveying words. Event features repre-
sent sentences by events they contained. 
Relevance features evaluate a sentence 
from its relatedness with other sentences. 
Experiments show that the combined fea-
tures improved summarization perform-
ance significantly. Although the evalua-
tion results are encouraging, supervised 
learning approach requires much labeled 
data. Therefore we investigate co-training 
by combining labeled and unlabeled data. 
Experiments show that this semi-
supervised learning approach achieves 
comparable performance to its supervised 
counterpart and saves about half of the 
labeling time cost. 
1 Introduction 
1 Automatic text summarization involves con-
densing a document or a document set to produce 
a human comprehensible summary. Two kinds of 
summarization approaches were suggested in the 
past, i.e., extractive (Radev et al, 2004; Li et al, 
2006) and abstractive summarization (Dejong, 
1978). The abstractive approaches typically need 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
to ?understand? and then paraphrase the salient 
concepts across documents. Due to the limita-
tions in natural language processing technology, 
abstractive approaches are restricted to specific 
domains. In contrast, extractive approaches 
commonly select sentences that contain the most 
significant concepts in the documents. These ap-
proaches tend to be more practical. 
Recently various effective sentence features 
have been proposed for extractive summarization, 
such as signature word, event and sentence rele-
vance. Although encouraging results have been 
reported, most of these features are investigated 
individually. We argue that it is ineffective to 
identify sentence importance from a single point 
of view. Each sentence feature has its unique 
contribution, and combing them would be advan-
tageous. Therefore we investigate combined sen-
tence features for extractive summarization. To 
determine weights of different features, we em-
ploy a supervised learning framework to identify 
how likely a sentence is important. Some re-
searchers explored learning based summarization, 
but the new emerging features are not concerned, 
such as event features (Li et. al, 2006). 
We investigate the effectiveness of different 
sentence features with supervised learning to de-
cide which sentences are important for summari-
zation. After feature vectors of sentences are ex-
amined, a supervised learning classifier is then 
employed.  Particularly, considering the length of 
final summaries is fixed, candidate sentences are 
re-ranked. Finally, the top sentences are ex-
tracted to compile the final summaries. Experi-
ments show that combined features improve 
summarization performance significantly. 
Our supervised learning approach generates 
promising results based on combined features. 
However, it requires much labeled data. As this 
procedure is time consuming and costly, we in-
vestigate semi-supervised learning to combine 
labeled data and unlabeled data. A semi-
985
supervised learning classifier is used instead of a 
supervised one in our extractive summarization 
framework. Two classifiers are co-trained itera-
tively to exploit unlabeled data. In each iteration 
step, the unlabeled training examples with top 
classifying confidence are included in the labeled 
training set, and the two classifiers are trained on 
the new training data. Experiments show that the 
performance of our semi-supervised learning 
approach is comparable to its supervised learning 
counterpart and it can reduce the labeling time 
cost by 50%. 
The remainder of this paper is organized as 
follows. Section 2 gives related work and Section 
3 describes our learning-based extractive summa-
rization framework. Section 4 outlines the vari-
ous sentence features and Section 5 describes 
supervised/semi-supervised learning approaches. 
Section 6 presents experiments and results. Fi-
nally, Section 7 concludes the paper. 
2 Related Work 
Traditionally, features for summarization were 
studied separately. Radev et al (2004) reported 
that position and length are useful surface fea-
tures. They observed that sentences located at the 
document head most likely contained important 
information. Recently, content features were also 
well studied, including centroid (Radev et al, 
2004), signature terms (Lin and Hovy, 2000) and 
high frequency words (Nenkova e t al., 2006). 
Radev et al (2004) defined centroid words as 
those whose average tf*idf score were higher 
than a threshold. Lin and Hovy (2000) identified 
signature terms that were strongly associated 
with documents based on statistics measures. 
Nenkova et al (2006) later reported that high 
frequency words were crucial in reflecting the 
focus of the document.  
Bag of words is somewhat loose and omits 
structural information. Document structure is 
another possible feature for summarization. Bar-
zilay and Elhadad (1997) constructed lexical 
chains and extracted strong chains in summaries. 
Marcu (1997) parsed documents as rhetorical 
trees and identified important sentences based on 
the trees. However, only moderate results were 
reported. On the other hand, Dejong (1978) rep-
resented documents using predefined templates. 
The procedure to create and fill the templates 
was time consuming and it was hard to adapt the 
method to different domains.  
Recently, semi-structure events (Filatovia and 
Hatzivassiloglou, 2004; Li et al, 2006; Wu, 2006) 
have been investigated by many researchers as 
they balanced document representation with 
words and structures. They defined events as 
verbs (or action nouns) plus the associated 
named entities. For instance, given the sentence 
?Yasser Arafat on Tuesday accused the United 
States of threatening to kill PLO officials?, they 
first identified ?accused?, ?threatening? and 
?kill? as event terms; and ?Yasser Arafat?, 
?United States?, ?PLO? and ?Tuesday? as event 
elements. Encouraging results based on events 
were reported for news stories.  
From another point of view, sentences in a 
document are somehow connected. Sentence 
relevance has been used as an alternative means 
to identify important sentences. Erkan and Radev 
(2004) and Yoshioka (2004) evaluate the rele-
vance (similarity) between any two sentences 
first. Then a web analysis approach, PageRank, 
was used to select important sentences from a 
sentence map built on relevance. Promising re-
sults were reported. However, the combination of 
these features is not well studied. Wu et al (2007) 
conducted preliminary research on this problem, 
but event features were not considered. 
Normally labeling procedure in supervised 
learning is very time consuming. Blum and 
Mitchell (1998) proposed co-training approach to 
exploit labeled and unlabeled data. Promising 
results were reported from their experiments on 
web page classification. A number of successful 
studies emerged thereafter for other natural lan-
guage processing tasks, such as text classification 
(Denis and Gilleron, 2003), noun phrase chunk-
ing (Pierce and Cardie, 2001), parsing (Sarkar, 
2001) and reference or relation resolution (Mul-
ler et al, 2001; Li et al, 2004). To our knowl-
edge, there is little research in the application of 
co-training techniques to extractive summariza-
tion. 
3 The Framework for Extractive Sum-
marization 
Extractive summarization can be regarded as a 
classification problem. Given the features of a 
sentence, a machine-learning based classification 
model will judge how likely the sentence is im-
portant. The classification model can be super-
vised or semi-supervised learning. Supervised 
approaches normally perform better, but require 
more labeled training data. SVMs perform well 
in many classification problems. Thus we em-
ploy it for supervised learning. For semi-
supervised learning, we co-trained a probabilistic 
986
SVM and a Na?ve Bayesian classifier to exploit 
unlabeled data. 
 
Figure 1. Learning-based Extractive Summariza-
tion Framework 
The automatic summarization procedure is 
shown in Figure 1. First, each input sentence is 
examined by going through the pre-specified fea-
ture functions. The classification model will then 
predict the importance of each sentence accord-
ing to its feature values. A re-ranking algorithm 
is then used to revise the order. Finally, the top 
sentences are included in the summaries until the 
length limitation is reached. The re-ranking algo-
rithm is crucial, as more important content are 
expected to be contained in the final summary 
with fixed length. Important sentences above a 
threshold are regarded as candidates. The one 
with less words and located at the beginning part 
of a document is ranked first. The re-ranking al-
gorithm is described as follows. 
Ranki = RankPosi + RankLengthi  
where RankPosi is the rank of sentence i accord-
ing to its position in a document (i.e. the sentence 
no.) and RankLengthi is rank of sentence i ac-
cording to its length. 
4 Sentence Features for Extractive 
Summarization 
This section provides a detailed description on 
the four types of sentence features, i.e., surface, 
content, event and relevance features, which will 
be examined systematically. 
4.1 Surface Features 
Surface features are based on structure of 
documents or sentences, including sentence 
position in the document, the number of words in 
the sentence, and the number of quoted words in 
the sentence (see Table 1).  
 
Name Description 
Position 1/sentence no. 
Doc_First Whether it is the first sentence of a document  
Para_First Whether it is the first sentence of a paragraph 
Length The number of words in a sentence 
Quote The number of quoted words in a sen-tence  
Table 1. Types of surface features 
 
The intuition with respect to the importance of 
a sentence stems from the following observations: 
(1) the first sentence in a document or a para-
graph is important; (2) the sentences in the ear-
lier parts of a document is more important than 
sentences in later parts; (3) a sentence is impor-
tant if the number of words (except stop words) 
in it is within a certain range; (4) a sentence con-
taining too many quoted words is unimportant.  
4.2 Content Features 
We integrate three well-known sentence features 
based on content-bearing words i.e., centroid 
words, signature terms, and high frequency 
words. Both unigram and bigram representations 
have been investigated. Table 2 summarizes the 
six content features we studied.  
 
Name Description 
Centroid_Uni The sum of  the weights of cen-troid uni-gram  
Centroid_Bi The sum of  the weights of cen-troid bi-grams  
SigTerm_Uni The number of signature uni-grams  
SigTerm_Bi The number of signature bi-grams 
FreqWord_Uni The sum of  the weights of fre-quent uni-grams  
FreqWord_Bi The sum of  the weights of fre-quent bi-grams  
Table 2. Types of content features 
4.3 Event Features 
An event is comprised of an event term and asso-
ciated event elements. In this study, we choose 
verbs (such as ?elect and incorporate?) and ac-
tion nouns (such as ?election and incorporation?) 
as event terms that can characterize actions. They 
relate to ?did what?. One or more associated 
named entities are considered as event elements. 
Four types of named entities are currently under 
987
consideration. The GATE system (Cunningham 
et al, 2002) is used to tag named entities, which 
are categorized as <Person>, <Organization>, 
<Location> and <Date>. They convey the infor-
mation about ?who?, ?whom?, ?when? and 
?where?. A verb or an action noun is deemed an 
event term only when it appears at least once 
between two named entities. 
Event summarization approaches based on in-
stances or concepts are investigated. An occur-
rence of an event term (or event element) in a 
document is considered as an instance, while the 
collection of the same event terms (or event ele-
ments) is considered as a concept. Given a 
document set, instances of event terms and event 
elements are identified first. An event map is 
then built based on event instances or concepts 
(Wu , 2006; Li et al, 2006). PageRank algorithm 
is used to assign weight to each node (an instance 
or concept) in the event map. The final weight of 
a sentence is the sum of weights of event in-
stances contained in the sentence. 
4.4 Relevance Features 
Relevance features are incorporated to exploit 
inter-sentence relationships. It is assumed that: (1) 
sentences related to important sentences are im-
portant; (2) sentences related to many other sen-
tences are important. The first sentence in a 
document or a paragraph is important, and other 
sentences in a document are compared with the 
leading ones. Two types of sentence relevance, 
FirstRel_Doc and FirstRel_Para (see Table 3), 
are measured by comparing pairs of sentences 
using word-based cosine similarity. 
Another way to exploit sentence relevance is 
to build a sentence map. Every two sentences are 
regarded relevant if their similarity is above a 
threshold. Every two relevant sentences are con-
nected with a unidirectional link. Based on this 
map, PageRank algorithm is applied to evaluate 
the importance of a sentence. These relevance 
features are shown in Table 3.  
 
Name Description 
FirstRel_Doc Similarity with the first sentence in the document  
FirstRel_Para Similarity with the first sentence in the paragraph  
PageRankRel PageRank value of the sentence based on the sentence map  
 
Table 3. Types of relevance features 
5 Supervised/Semi-supervised Learning 
Approaches  
To incorporate features described in Section 4, 
we investigate supervised and semi-supervised 
learning approaches. Probabilistic Support Vec-
tor Machine (PSVM)  is employed as supervised 
learning (Wu et al, 2004), while the co-training 
of PSVM and Na?ve Bayesian Classifier (NBC) 
is used for semi-supervised learning. The two 
learning-based classification approaches, PSVM 
and NBC, are described in following sections. 
5.1 Probabilistic Support Vector Machine 
(PSVM) 
For a set of training examples ( ix , iy ), 
li ,...,1= , where ix  is an instance and iy  the 
corresponding label, basic SVM requires the so-
lution of the following optimization problem. 
?
=
+
l
i
i
T
bw
Cww
1
,, 2
1  min ??  
subject to    
0
1  ))(( ,
?
??+
i
ii
T
i bxwy
?
??
 
 
Here the SVM classifier is expected to find a 
hyper-plane to separate testing examples as posi-
tive and negative. Wu et al (2004) extend the 
basic SVM to a probabilistic version. Its goal is 
to estimate  
 
kixiyppi ,...1 ),|( === . 
First the pairwise (one-against-one) probabilities 
) ,or  |( xjiyiyprij ==?  is estimated using 
BAfij e
r ++? 1
1
 
where A and B are estimated by minimizing the 
negative log-likelihood function using training 
data and their decision values f. Then ip  is ob-
tained by solving the following optimization 
problem 
? ?
= ?
?
k
i ijj
jijijip
prpr
1 :
2)(
2
1  min  
 
subject to    
0
1  ))(( ,
?
??+
i
ii
T
i bxwy
?
??
 
The problem can be reformulated as  
QPP T
P 2
1  min  
988
where      ??
??
?
??
== ? ?
ji if       
ji if  
Q
2
:
ij
ijji
siiss
rr
r
 
The problem is convex and the optimality condi-
tions a scalar b such that   
??
???
?=??
???
???
???
?
1
z
  
b
P
 
0Te
eQ
 
where e is the vector of all 1s and z is the vector 
of all 0s, and b is the Lagrangian multiplier of the 
equality constraint ?
=
=
k
i
ip
1
1 . 
5.2 Na?ve Bayesian Classier (NBC) 
Na?ve Bayesian Classier assumes features are 
independent. It learns prior probability and con-
ditional probability of each feature, and predicts 
the class label by highest posterior probability. 
Given a feature vector (F1, F2, F3,?, Fn), the 
classifier need to decide the label c: 
 
),...,,|(maxarg 321 n
c
FFFFcPc =  
 
By applying Bayesian rule, we have  
 
),...,,,(
)|,...,,,()(
),...,,,|(
321
321
321
n
n
n FFFFP
cFFFFPcP
FFFFcP =
 
Since the denominator does not depend on c and 
the values of Fi are given, therefore the denomi-
nator is a constant and we are only interested in 
the numerator. As features are assumed inde-
pendent,  
 
?
=
?
=
n
i
i
nn
cFPcP
cFFFFPcPFFFFcP
1
321321
)|()(
)|,...,,()(),...,,|(
 
 
where )|( cFP i is estimated with MLE from 
training data with Laplace Smoothing. 
5.3 Co-Training (COT) 
Supervised learning approaches require much 
labeled data and the labeling procedure is very 
time-consuming. Literature (Blum and Mitchell, 
1998; Collins, 1999) has suggested that unla-
beled data can be exploited together with labeled 
data by co-training two classifiers. (Blum and 
Mitchell, 1998) trained two classifiers of same 
type on different features, and (Li et al, 2004) 
trained two classifiers of different types. In this 
paper, as the number of involved features is not 
too many, we train two different classifiers, 
PSVM and NBC, on the same feature spaces. 
The co-training algorithm is described as follows. 
 
Given: 
L is the set of labeled training examples 
U is the set of unlabeled training examples 
Loop: until the unlabeled data is exhausted 
Train the first classifier C1 (PSVM) on L 
Train the second classifier C2 (NBC) on L 
For each classifier Ci 
Ci labels examples from U 
Ci chooses p positive and n negative ex-
amples E from U. These examples have 
top classifying confidence. 
Ci removes examples E from U 
Ci adds examples E with the correspond-
ing labels to L 
End 
Output: label the test examples by the optimal 
classifier which is evaluated on training data ac-
cording to the classification performance. 
6 Experiments 
DUC 20012 has been used in our experiments. It 
contains 30 clusters of relevant documents and 
308 documents in total. Each cluster deals with a 
specific topic (e.g. a hurricane) and comes with 
model summaries created by NIST assessors. 50, 
100, 200 and 400 word summaries are provided. 
Twenty-five of the thirty document clusters are 
used as training data and the remaining five are 
used as testing. The training/testing configuration 
is same in experiments of supervised learning 
and semi-supervised learning, while the differ-
ence is that some sentences in training data are 
not tagged for semi-supervised learning. 
An automatic evaluation package, i.e., 
ROUGE (Lin and Hovy, 2003) is employed to 
evaluate the summarization performance. It 
compares machine-generated summaries with 
model summaries based on the overlap. Precision 
and recall measures are used to evaluate the clas-
sification performance. For comparison, we 
evaluate our approaches on DUC 2004 data set 
also. It contains 50 clusters of documents. Only 
665-character summaries are given by assessors 
for each cluster. 
6.1 Experiments on Supervised Learning 
Approach 
We use LibSVM3 as our classification model for 
SVM classifiers normally perform better. Types 
of features presented in previous section are 
evaluated individually first. Precision measures 
                                                 
2 http://duc.nist.gov/ 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
989
the percentage of true important sentences 
among all important sentences labeled by the 
classifier. Recall measures the percentage of true 
important sentences labeled by the classifier 
among all true important sentences.  
Table 4 shows the precisions and recalls of 
different feature groups under the PSVM classi-
fier. Table 5 records the ROUGE evaluation re-
sults ? ROUGE-1, ROUGE-2 and ROUGE-L. 
They evaluate the overlap between machine-
generated summaries and model summaries 
based on unigram, bigram and long distance re-
spectively. The summary length is limited to 200 
words here.  
 
Feature Precision Recall 
Sur 0.488 0.146 
Con 0.407 0.167 
Rel 0.488 0.146 
Event 0.344 0.146 
Sur+Con 0.575 0.160 
Sur+Rel 0.488 0.146 
Con+Rel 0.588 0.139 
Sur+Event 0.600 0.125 
Con+Event 0.384 0.194 
Rel+Event 0.543 0.132 
Sur+Con+Event 0.595 0.153 
Sur+Rel+Event 0.553 0.146 
Con+Rel+Event 0.581 0.125 
Sur+Con+Rel 0.595 0.174 
Sur+Con+Rel+Event 0.579 0.153 
Table 4. Classification performance based on 
different feature groups 
 
 
Feature Rouge-1 
Rouge-
2 
Rouge-
L 
Sur 0.373 0.103 0.356 
Con 0.352 0.074 0.334 
Rel 0.373 0.103 0.356 
Event 0.344 0.064 0.325 
Sur+Con 0.380 0.109 0.363 
Sur+Rel 0.373 0.103 0.356 
Con+Rel 0.375 0.103 0.358 
Sur+Event 0.348 0.091 0.332 
Con+Event 0.344 0.071 0.330 
Rel+Event 0.349 0.089 0.356 
Sur+Con+Event 0.379 0.106 0.363 
Sur+Rel+Event 0.371 0.101 0.353 
Sur+Con+Rel 0.396 0.116 0.358 
Sur+Con+Rel+Event 0.375 0.106 0.359 
Table 5. ROUGE evaluation results for differ-
ent feature groups 
From Table 4, we can see the most useful fea-
ture groups are ?surface? and ?relevance?, i.e. 
the external characteristics of a sentence in the 
document and the relationships of a sentence 
with other sentences in a cluster. The evaluation 
scores from surface features and relevance fea-
tures are the same. We found that the reason is 
that the dominating feature in each feature group 
is about whether a sentence is the first sentence 
in a document. The influence of event features is 
not very positive. Based on our analysis the rea-
son is that not all clusters contain enough event 
terms/elements to build a good event map. 
From Table 5, it can be seen that the combina-
tion of multiple features or multiple feature 
groups outperforms individual feature or feature 
groups. When surface, content and relevance fea-
tures are employed, the best performance is 
achieved, i.e., ROUGE-1 and ROUGE-2 score 
are 0.396 and 0.116 respectively. In our prelimi-
nary experiments, we find ROUGE-1 score of a 
model summary is 0.422 (without stemming and 
filtering stop words). Therefore summaries gen-
erated by our supervised learning approach re-
ceived comparable performance with model 
summaries when evaluated by ROUGE. Al-
though ROUGE is not perfect at this time, it is 
automatic and good complement to subjective 
evaluations.  
 We also find that the Rouge scores are similar 
for variations on the feature set. Sentences from 
original documents are selected to build the final 
summaries. Normally, only four to six sentences 
are contained in one 200-word summary in our 
experiments, i.e., few sentences will be kept in a 
summary. As variations of the feature set only 
induce little change of the order of most impor-
tant sentences, the ROUGE scores change little. 
6.2 Experiments on Semi-supervised Learn-
ing Approach 
Supervised learning approaches normally 
achieve good performance but require manually 
labeled data. Recent literature (Blum and 
Mitchell, 1998; Collins, 199) has suggested that 
co-training techniques reduce the amount of la-
beled data. They trained two homogeneous clas-
sifiers based on different feature spaces. How-
ever this method is unsuitable for our application 
as the number of required features in our case is 
not too many. Therefore we develop a co-
training approach to train different classifiers 
based on same feature space. PSVM and NBC 
are applied to the combination of surface, content 
and relevance features. 
The capability of different learning approaches 
to identify important sentences is shown in Fig-
990
ure 2. The ?x? axis shows the number of labeled 
sentences employed. The remained training sen-
tences in DUC 2001 are employed as unlabeled 
training data. The y axis shows f-measures of 
important sentences identified from the test set. 
The size of the training seed set is investigated. 
For each size, three different seed sets which are 
chose randomly are used. The average evaluation 
scores are used as the final performance. This 
procedure avoids the variance of the final evalua-
tion results. The ROUGE evaluation results of 
these supervised learning approaches and semi-
supervised learning approaches are shown in Ta-
ble 6 (2000 labeled sentences). It can be seen that 
the ROUGE performance of co-trained classifiers 
is better than that of individual classifiers. 
0
0.1
0.2
0.3
0.4
50 100 200 500 1000 2000
 Number of Labeled Sentences
F
-M
ea
su
re Cotrain
Bayes
Svm
 
Figure 2. Performance of supervised learning 
and semi-supervised learning approaches 
 
Learning  
Approaches Rouge-1 Rouge-2 Rouge-L
PSVM 0.358 0.082 0.323 
NBC 0.353 0.061 0.317 
COT 0.366 0.090 0.329 
Table 6. ROUGE evaluation results of supervised 
learning and semi-supervised learning 
6.3 Experiments on Summary Length 
In DUC 2001 dataset, 50, 100, 200 and 400-word 
summaries are provided to evaluate summaries 
with different length. Our supervised approach, 
which generates the best performance in previous 
experiments, is employed. The ROUGE scores of 
evaluations on different summary length are 
shown in Table 7. Our summaries consist of ex-
tracted sentences. It can be seen that these sum-
maries achieve lower ROUGE scores when the 
length of summary is reduced. The reason is that 
when people try to write a more concise sum-
mary, condensed contents are included in the 
summaries, which may not use the original con-
tents directly. Therefore the word-overlapping 
test tool in ROUGE generates lower scores.  
We then tested the same classifier and same 
features on DUC 2004. The length of summaries 
is only 665 characters (about 100 words). 
ROUGE-1 and ROUGE-2 are 0.329 and 0.073 
respectively. It confirms that the performance of 
our approach is sensitive to the length of the 
summary.  
Sum_length Rouge-1 Rouge-2 Rouge-L
50 0.241 0.036 0.205 
100 0.309 0.085 0.277 
200 0.396 0.116 0.358 
400 0.423 0.118 0.402 
Table 7. ROUGE evaluation results for differ-
ent summary length 
7 Conclusions and Future Work 
We explore surface, content, event, relevance 
features and their combinations for extractive 
summarization with supervised learning ap-
proach. Experiments show that the combination 
of surface, content and relevance features per-
form best. The highest ROUGE-1, ROUGE-2 
scores are 0.396 and 0.116 respectively. The 
Rouge-1 score of manually generated summaries 
is 0.422. This shows the ROUGE performance of 
our supervised learning approach is comparable 
to that of manually generated summaries. The 
ROUGE-1 scores of extractive summarization 
based on centroid, signature word, high fre-
quency word and event individually are 0.319, 
0.356, 0.371 and 0.374 respectively. It can be 
seen that our summarization approach based on 
combination of features improves the perform-
ance obviously.  
Although the results of supervised learning 
approach are encouraging, it required much la-
beled data. To reduce labeling cost, we apply co-
training to combine labeled and unlabeled data. 
Experiments show that compare with supervised 
learning, semi-supervised learning approach 
saves half of the labeling cost and maintains 
comparable performance (0.366 vs 0.396). We 
also find that our extractive summarization is 
sensitive to length of the summary. When the 
length is extended, the ROUGE scores of same 
summarization method are improved. In the fu-
ture, we plan to investigate sentence compression 
to improve performance of our summarization 
approaches on short summaries. 
Acknowledgement 
The research described in this paper is partially 
supported by Research Grants Council of Hong 
Kong (RGC: PolyU5217/07E), CUHK Strategic 
Grant Scheme (No: 4410001) and Direct Grant 
Scheme (No: 2050417).  
991
References 
Regina Barzilay, and Michael Elhadad. 1997. Using 
lexical chains for text summarization. In Proceed-
ings of the 35th Annual Meeting of the Association 
for Computational Linguistics Workshop on Intel-
ligent Scalable Text Summarization, pages 10-17. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In 
Proceedings of the 11th Annual Conference on 
Computational Learning Theory, pages 92-100. 
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva, Valentin Tablan. 2002. GATE: a 
framework and graphical development environ-
ment for robust NLP tools and applications. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for computational Linguistics. 
Francois Denis and Remi Gilleron. 2003. Text classi-
fication and co-training from positive and unla-
beled examples. In Proceedings of the 20th Inter-
national Conference on Machine Learning Work-
shop: the Continuum from Labeled Data to Unla-
beled Data in Machine Learning and Data Mining. 
Gunes Erkan and Dragomir R. Radev. 2004. LexPag-
eRank: prestige in multi-document text summariza-
tion. In Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 365-371. 
Elena Filatova and Vasileios Hatzivassiloglou. Event-
based extractive summarization. 2004. In Proceed-
ings of the 42nd Annual Meeting of the Association 
for Computational Linguistics Workshop, pages 
104-111. 
Gerald Francis DeJong. 1978. Fast skimming of news 
stories: the FRUMP system. Ph.D. thesis, Yale 
University. 
Wenjie Li, Guihong Cao, Kam-Fai Wong and Chunfa 
Yuan. 2004. Applying machine learning to Chinese 
temporal relation resolution. In Proceedings of the 
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 583-589. 
Wenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu. 
2006. Extractive summarization using inter- and in-
tra- event relevance. In proceedings of Proceedings 
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
369-376. 
Chin-Yew Lin; Eduard Hovy. 2000. The automated 
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495-
501. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, Edmonton, Canada. 
Daniel Marcu. 1997. The rhetorical parsing of natural 
language texts. In Proceedings of the 35th Annual 
Meeting of the Association for computational Lin-
guistics, pages 96-103. 
Christoph Muller, Stefan Rapp and Michael Strube. 
2001. Applying co-training to reference resolution. 
In Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A compositional context sensi-
tive multi-document summarizer: exploring the 
factors that influence summarization. In Proceed-
ings of the 29th Annual International ACM SIGIR 
Conference on Research and Development in In-
formation Retrieval. 
David Pierce and Claire Cardie. 2001. Limitations of 
co-training for natural language learning from large 
datasets. In Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 1-9. 
Dragomir R. Radev, Timothy Allison, et al 2004. 
MEAD - a platform for multidocument multilin-
gual text summarization. In Proceedings of 4th In-
ternational Conference on Language Resources 
and Evaluation. 
Anoop Sarkar. 2001. Applying co-training methods to 
statistical parsing. In Proceedings of 2nd Meeting 
of the North American Chapter of the Association 
for Computational Linguistics on Language Tech-
nologies. 
Mingli Wu. 2006. Investigations on event-based 
summarization. In proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the Association for 
Computational Linguistics Student Research Work-
shop, pages 37-42. 
Mingli Wu, Wenjie Li, Furu Wei, Qin Lu and Kam-
Fai Wong. 2007. Exploiting surface, content and 
relevance features for learning-based extractive 
summarization. In Proceedings of 2007 IEEE In-
ternational Conference on Natural Language 
Processing and Knowledge Engineering.  
Ting-Fan Wu, Chih-Jen Lin and Ruby C. Weng. 2004. 
Probability estimates for multi-class classification 
by pairwise coupling. Journal of Machine Learning 
Research, 5:975-1005.  
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple news articles summarization based on 
event reference information. In Working Notes of 
the Fourth NTCIR Workshop Meeting, National In-
stitute of Informatics. 
992
