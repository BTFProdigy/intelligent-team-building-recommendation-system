Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 455?459, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Experiments with DBpedia, WordNet and SentiWordNet as re-
sources for sentiment analysis in micro-blogging 
 
 
Abstract 
Sentiment Analysis in Twitter has become an 
important task due to the huge user-generated 
content published over such media. Such 
analysis could be useful for many domains 
such as Marketing, Finance, Politics, and So-
cial. We propose to use many features in order 
to improve a trained classifier of Twitter mes-
sages; these features extend the feature vector 
of uni-gram model by the concepts extracted 
from DBpedia, the verb groups and the similar 
adjectives extracted from WordNet, the Senti-
features extracted using SentiWordNet and 
some useful domain specific features. We also 
built a dictionary for emotion icons, abbrevia-
tion and slang words in tweets which is useful 
before extending the tweets with different fea-
tures. Adding these features has improved the 
f-measure accuracy 2% with SVM and 4% 
with NaiveBayes. 
1 Introduction 
In recent years, the explosion of social media has 
changed the relation between the users and the 
web. The world has become closer and more ?real-
time? than ever. People have increasingly been part 
of virtual society where they have created their 
content, shared it, interacted with others in differ-
ent ways and at a very increasingly rate.  Twitter is 
one of the most important social media, with 1 
billion tweets1 posted per week and 637 million 
users2. 
                                                        
1http://blog.kissmetrics.com/twitter-statistics/ 
2http://twopcharts.com/twitter500million.php 
     With the availability of such content, it attracts 
the attention from who want to understand the 
opinion and interestingness of individuals. Thus, it 
would be useful in various domains such as poli-
tics, financing, marketing and social. In this con-
text, the efficacy of sentiment analysis of twitter 
has been demonstrated at improving prediction of 
box-office revenues of movies in advance of their 
release (Asur and Huberman, 2010). Sentiment 
Analysis has been used to study the impact of 13 
twitter accounts of celebrated person on their fol-
lowers (Bae and Lee, 2012) and for forecasting the 
interesting tweets which are more probably to be 
reposted by the followers many times (Naveed, 
Gottron et al, 2011). 
     However, sentiment analysis of microblogs 
faces several challenges, the limited size of posts 
(e.g., maximum 140 characters in Twitter), the 
informal language of such content containing slang 
words and non-standard expressions (e.g. gr8 in-
stead of great, LOL instead of laughing out loud, 
goooood etc.), and the high level of noise in the 
posts due to the absence of correctness verification 
by user or spelling checker tools. 
   Three different approaches can be identified in 
the literature of Sentiment Analysis, the first ap-
proach is the  lexicon based  which uses specific 
types of lexicons to derive the polarity of a text, 
this approach is suffering from the limited size of 
lexicon and requires human expertise to build the 
lexicon (Joshi, Balamurali et al, 2011). The 
second one is machine learning approach which 
uses annotated texts with a given label to learn a 
statistical model and an early work was done on a 
movie review dataset (Pang, Lee et al, 2002). Both 
lexicon and machine learning approaches can be 
Hussam Hamdan*,**,*** Frederic B?chet** Patrice Bellot*,***   
 
hussam.hamdan@lsis-
.org 
frederic.bechet@lif-
.univ-mrs.fr 
patrice.bellot@lsis-
.org  
*LSIS 
Aix-Marseille Universit? CNRS 
Av. Esc. Normandie Niemen,  
13397 Marseille Cedex 20, 
France 
**LIF 
Aix-Marseille Universit? CNRS 
Avenue de Luminy  
13288 Marseille Cedex 9, 
France 
***OpenEdition  
Aix-Marseille Universit? CNRS 
3 pl. V. Hugo, case n?86 
13331 Marseille Cedex 3, 
France 
455
combined to achieve a better performance (Khuc, 
Shivade et al 2012). The third one is social ap-
proach which exploits social network properties 
and data for enhancing the accuracy of the classifi-
cation (Speriosu, Sudan et al, 2011; Tan, Lee et al 
2011; Hu, Tang et al, 2013) (Hu, Tang et al, 
2013) (Tan, Lee et al, 2011). 
    In this paper, we employ machine learning. Each 
text is represented by a vector in which the features 
have to be selected carefully. They can be the 
words of the text, their POS tags (part of speech), 
or any other syntactic or semantic features. 
     We propose to exploit some additional features 
(section 3) for sentiment analysis that extend the 
representation of tweets by:  
? the concepts extracted from DBpedia3,  
? the related adjectives and verb groups ex-
tracted from WordNet4,  
? some ?social? features such as the number 
of happy and bad emotion icons,  
? the number of exclamation and question 
marks,  
? the existence of URL (binary feature),  
? if the tweet is re-tweeted (binary feature),  
? the number of symbols the tweet contains,  
? the number of uppercase words,  
? some other senti-features extracted from 
SentiWordNet5 such as the number of 
positive, negative and neutral words that 
allow estimating a score of the negativity, 
positivity and objectivity of the tweets, 
their polarity and subjectivity.  
     We extended the unigram model with these 
features (section 4.2). We also constructed a dic-
tionary for the abbreviations and the slang words 
used in Twitter in order to overcome the ambiguity 
of the tweets. 
     We tested various combinations (section 4.2) of 
these features, and then we chose the one that gave 
the highest F-measure for negative and positive 
classes (submission for Tweet subtask B of senti-
ment analysis in twitter task of SemEval2013 
(Wilson, Kozareva et al 2013)). We tested differ-
ent machine learning models: Na?ve Bayes, SVM, 
IcsiBoost6 but the submitted runs exploited SVM 
only6. 
                                                        
3
 http://dbpedia.org/About 
4
 http://wordnet.princeton.edu/ 
5
 http://sentiwordnet.isti.cnr.it/ 
6
 http://code.google.com/p/icsiboost/ 
     The rest of this paper is organized as follows. 
Section 2 outlines existing work of sentiment anal-
ysis over Twitter. Section 3 presents the features 
we used for training a classifier. Our experiments 
are described in section 4 and future work is pre-
sented in section 5.  
2 Related Work  
We can identify three main approaches for senti-
ment analysis in Twitter. The lexicon based ap-
proaches which depend on dictionaries of positive 
and negative words and calculate the polarity ac-
cording to the positive and negative words in the 
text. Many dictionaries have been created manual-
ly such as ANEW (Aaffective Norms for English 
Words) or automatically such as SentiWordNet 
(Baccianella, Esuli et al 2010). Four lexicon dic-
tionaries were used to overcome the lack of words 
in each one (Joshi, Balamurali et al 2011; Mukher-
jee, Malu et al 2012). Automatically construction 
of a Twitter lexicon was implemented by Khuc, 
Shivade et al (2012). 
      Machine learning approaches were employed 
from annotated tweets by using Naive Bayes, Max-
imum Entropy MaxEnt and Support Vector Ma-
chines (SVM) (Go, Bhayani et al 2009).  Go et al 
(2009) reported that SVM outperforms other clas-
sifiers. They tried a unigram and a bigram model in 
conjunction with parts-of-speech (POS) features; 
they noted that the unigram model outperforms all 
other models when using SVM and that POS fea-
tures decline the results. N-gram with lexicon fea-
tures and microbloging features were useful but 
POS features were not (Kouloumpis, Wilson et al 
2011). In contrast, Pak & Paroubek (2010) re-
ported that POS and bigrams both help. Barbosa & 
Feng (2010) proposed the use of syntax features of 
tweets like retweet, hashtags, link, punctuation and 
exclamation marks in conjunction with features 
like prior polarity of words and POS of words, 
Agarwal et al (2011) extended their approach by 
using real valued prior polarity and by combining 
prior polarity with POS. They build models for 
classifying tweets into positive, negative and neu-
tral sentiment classes and three models were pro-
posed: a unigram model, a feature based model and 
a tree kernel based model which presented a new 
tree representation for tweets. Both combining 
unigrams with their features and combining the 
features with the tree kernel outperformed the uni-
456
gram baseline. Saif et al (2012) proposed to use 
the semantic features, therefore they extracted the 
hidden concepts in the tweets. They demonstrated 
that incorporating semantic features extracted us-
ing AlchemyAPI7 improves the accuracy of senti-
ment classification through three different tweet 
corpuses. 
     The third main approach takes into account the 
influence of users on their followers and the rela-
tion between the users and the tweets they wrote. 
Using the Twitter follower graph might improve 
the polarity classification. Speriosu, Sudan et al 
(2011) demonstrated that using label propagation 
with Twitter follower graph improves the polarity 
classification. Tan, Lee et al (2011) employed 
social relation for user-level sentiment analysis. 
Hu, Tang et al (2013) proposed a sociological 
approach to handling the noisy and short text 
(SANT) for supervised sentiment classification, 
they reported that social theories such as Sentiment 
Consistency and Emotional Contagion could be 
helpful for sentiment analysis. 
3 Feature Extraction  
We used different types of features in order to 
improve the accuracy of sentiment classification. 
? Bag of words (uni-gram) 
The most commonly used features in text analysis 
are the bag of words which represent a text as un-
ordered set of words. It assumes that words are 
independent from each other and also disregards 
their order of appearance. We used these features 
as a baseline model.  
? Domain specific features 
We extracted some domain specific features of 
tweets which are: presence of an URL or not, the 
tweet was retweeted or not, the number of ?Not?, 
the number of happy emotion icons, the number of 
sad emotion icons, exclamation and question 
marks, the number of words starting by a capital 
letter, the number of @.  
? DBpedia features 
We used the DBpedia Spotlight8 Web service to 
extract the concepts of each tweet. For example, 
                                                        
7
 http://www.alchemyapi.com/ 
8
 http://dbpedia-spotlight.github.io/ 
for the previous tweet, the DBpedia concepts for 
Chapel Hill are (Settlement, PopulatedPlace, 
Place). Therefore, if we suppose that people post 
positively about settlement, it would be more prob-
able to post positively about Chapel Hill. 
? WordNet features 
We used WordNet for extracting the synonyms of 
nouns, verbs and adjectives, the verb groups (the 
hierarchies in which the verb synsets are arranged), 
the similar adjectives (synset) and the concepts of 
nouns which are related by the relation is-a in 
WordNet. 
We chose the first synonym set for each noun, 
adjective and verb, then the concepts of the first 
noun synonym set, the similar adjectives of the 
first adjective synonym set and the verb group of 
the first verb synonym set. We think that those 
features would improve the accuracy because they 
could overcome the ambiguity and the diversity of 
the vocabulary. 
- Senti-features 
We used SentiWordNet for extracting the number 
and the scores of positive, negative and neutral 
words in tweets, the polarity (the number of posi-
tive words divided by the number of negative ones 
incremented by one) and subjectivity (the number 
of positive and negative words divided by the neu-
tral ones incremented by one).  
4 Evaluations 
4.1 Data collection 
We used the data set provided in SemEval 2013 for 
subtask B of sentiment analysis in Twitter (Wilson, 
Kozareva et al 2013). The participants were pro-
vided with training tweets annotated positive, neg-
ative or neutral. We downloaded these tweets using 
the given script. Among 9646 tweets, we could 
only download 8498 of them because of protected 
profiles and deleted tweets. Then, we used the 
development set containing 1654 tweets for eva-
luating our methods. The method which gave the 
highest accuracy for the average of positive and 
negative classes was chosen for the submitted runs. 
Lastly, we combined the development set with 
training set and built a new model which predicted 
the labels of the 3813 tweets in the test set.  
457
4.2 Experiments 
We have done various experiments using the fea-
tures presented in Section 3 with SVM model us-
ing linear kernel and the following parameters: 
weighting value=1, degree=3, cost=1, nu=0.5 and 
seed=1. We firstly constructed feature vector of 
tweet terms which gave 0.52% for f-measure of the 
negative and positive classes. Then, we augmented  
this vector by the similar adjectives of WordNet 
which improves a little the f-measure, particularly  
for the positive class. After that, we added the con-
cepts of DBpedia which also improved the quality 
of the positive class and declined the negative one. 
Finally, we added all the verb groups, senti-
features and domain specific features which im-
proved the f-measure for both negative and posi-
tive classes but particularly for the positive one. 
Table 1 presents the results for each kind of feature 
vector. 
F
eatu
re
 vector
 
 
U
ni
-gram
 
+
adjectives
 
+DBp
edia
 
+
verb
 group
s+
 
 
syntactic
 +
 senti
-
featu
res
 
f
-m
easu
re
 
Positive 0.603 0.619 0.622 0.637 
Negative 0.443 0.436 0.417 0.440 
Neutral 0.683 0.685 0.691 0.689 
Avg neg+pos 0.523 0.527 0.520 0.538 
Table 1. The results of different feature vectors using linear 
SVM model (degree=3, weight=1, nu=0.5)  
 
F
eatu
re
 vector
 
 
U
ni
-gram
 
+
adjectives
 
+DBp
edia
 
+
verb
 group
s+
 
 
syntactic
 +
 senti
-
featu
res
 
f
-m
easu
re
 
Positive 0.514 0.563 0.562 0.540 
Negative 0.397 0.422 0.427 0.424 
Neutral 0.608 0.652 0.648 0.636 
Avg neg+pos 0.456 0.493 0.495 0.482 
Table 2. The results of different feature vectors using a     
NaiveBayes approach. 
 
     We remark that the DBpedia concepts improved 
the accuracy, and just the similar adjectives and 
group verbs of  WordNet improved it, but the other 
synonyms and concepts declined it. The reason 
may be linked to a perturbation added by the syn-
onyms. Moreover, the first synonym set is not ne-
cessary to be the most suitable one. Many domain 
specific and Senti-WordNet features improved the 
accuracy, but others did not, such as the number of 
neutral words, whether the tweet is reposted or not, 
the number of @ and the number of #. So we ex-
cluded the features that declined the accuracy. 
    We have done some experiments using Naive-
Bayes (Table 2). Na?ve Bayes improved the accu-
racy of the negative and positive classes, and the 
highest f-measure was obtained by adding the ad-
jectives and the DBpedia concepts. Using such 
features improved the f-measure for the positive 
and negative classes: about 2% with SVM and 4% 
with NaiveBayes. The improvement given by 
means of the Na?ve Bayes model was more signifi-
cant than the one obtained with SVM and needed 
fewer features, but the higher accuracy was ob-
tained by SVM. 
5 Discussion and Future Work 
In this paper we experimented the value of using 
DBpedia, WordNet and SentiWordNet for the sen-
timent classification of tweets. We extended the 
feature vector of tweets by the concepts of DBpe-
dia, verb groups and similar adjectives from 
WordNet, the senti-features from SentiWordNet 
and other domain specific features. We think that 
using other lexicon dictionaries with SentiWord-
Net is more useful, we did not use POS Tagger for 
detecting the part of speech. We augmented the 
feature vector by all these features. In fact, for 
some tweets this expansion is not the best strategy. 
However, it will be important to find out a way for 
selecting only the features that improve the accura-
cy. 
    We verified that the adjectives are useful fea-
tures and we should now focus on extracting the 
suitable and similar adjectives. For the abbrevia-
tion LOL (loud of laughing), it might be more use-
ful to replace it by funny or by another adjective 
that reflects the sentiment of the writer. However, 
we could enhance our dictionary by these adjec-
tives. We could handle the emotion icons in a simi-
lar way. 
     We also plan to combine the results of different 
classifiers for improving the total accuracy. 
 
458
References  
Agarwal, A., B. Xie, et al (2011). Sentiment analysis of 
Twitter data.Proceedings of the Workshop on Lan-
guages in Social Media. Portland, Oregon, Associa-
tion for Computational Linguistics: 30-38. 
Asur, S. and B. A. Huberman (2010). Predicting the 
Future with Social Media. Proceedings of the 2010 
IEEE/WIC/ACM International Conference on Web 
Intelligence and Intelligent Agent Technology - Vo-
lume 01, IEEE Computer Society: 492-499. 
Baccianella, S., A. Esuli, et al (2010). SentiWordNet 
3.0: An Enhanced Lexical Resource for Sentiment 
Analysis and Opinion Mining. Proceedings of the 
Seventh Conference on International Language Re-
sources and Evaluation (LREC'10), European Lan-
guage Resources Association (ELRA). 
Bae, Y. and H. Lee (2012). "Sentiment analysis of twit-
ter audiences: Measuring the positive or negative in-
fluence of popular twitterers." J. Am. Soc. Inf. Sci. 
Technol.63(12): 2521-2535. 
Barbosa, L. and J. Feng (2010). Robust sentiment detec-
tion on Twitter from biased and noisy da-
ta.Proceedings of the 23rd International Conference 
on Computational Linguistics: Posters. Beijing, Chi-
na, Association for Computational Linguistics: 36-
44. 
Go, A., R. Bhayani, et al (2009). Twitter Sentiment 
Classification using Distant Supervision. 
Hu, X., L. Tang, et al (2013). Exploiting social rela-
tions for sentiment analysis in microblogging. Pro-
ceedings of the sixth ACM international conference 
on Web search and data mining.Rome, Italy, ACM: 
537-546. 
Joshi, A., A. R. Balamurali, et al (2011). C-Feel-It: a 
sentiment analyzer for micro-blogs. Proceedings of 
the 49th Annual Meeting of the Association for 
Computational Linguistics: Human Language Tech-
nologies: Systems Demonstrations. Portland, Oregon, 
Association for Computational Linguistics: 127-132. 
Khuc, V. N., C. Shivade, et al (2012). Towards build-
ing large-scale distributed systems for twitter senti-
ment analysis. Proceedings of the 27th Annual ACM 
Symposium on Applied Computing. Trento, Italy, 
ACM: 459-464. 
Kouloumpis, E., T. Wilson, et al (2011).Twitter Senti-
ment Analysis: The Good the Bad and the OMG! 
Fifth International AAAI Conference on Weblogs 
and Social Media. 
Mukherjee, S., A. Malu, et al (2012).TwiSent: a multis-
tage system for analyzing sentiment in twitter. Pro-
ceedings of the 21st ACM international conference 
on Information and knowledge management.Maui, 
Hawaii, USA, ACM: 2531-2534. 
Naveed, N., T. Gottron, et al (2011). Bad News Travels 
Fast: A Content-based Analysis of Interestingness on 
Twitter. Proc. Web Science Conf. 
Pak, A. and P. Paroubek (2010). Twitter as a corpus for 
sentiment analysis and opinion mining. 
Pang, B., L. Lee, et al (2002). Thumbs up?: sentiment 
classification using machine learning techniques. 
Proceedings of the ACL-02 conference on Empirical 
methods in natural language processing - Volume 10, 
Association for Computational Linguistics: 79-86. 
Saif, H., Y. He, et al (2012).Semantic sentiment analy-
sis of twitter.Proceedings of the 11th international 
conference on The Semantic Web - Volume Part I. 
Boston, MA, Springer-Verlag: 508-524. 
Speriosu, M., N. Sudan, et al (2011). Twitter polarity 
classification with label propagation over lexical 
links and the follower graph. Proceedings of the First 
Workshop on Unsupervised Learning in NLP. Edin-
burgh, Scotland, Association for Computational Lin-
guistics: 53-63. 
Tan, C., L. Lee, et al (2011). User-level sentiment anal-
ysis incorporating social networks. Proceedings of 
the 17th ACM SIGKDD international conference on 
Knowledge discovery and data mining. San Diego, 
California, USA, ACM: 1397-1405. 
Khuc, V. N., C. Shivade, et al (2012). Towards build-
ing large-scale distributed systems for twitter senti-
ment analysis. Proceedings of the 27th Annual ACM 
Symposium on Applied Computing. Trento, Italy, 
ACM: 459-464. 
Wilson, T., Z. Kozareva, et al (2013). "SemEval-2013 
Task 2: Sentiment Analysis in Twitter." Proceedings 
of the 7th International Workshop on Semantic Eval-
uation. Association for Computational Linguistics. 
 
 
459
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 596?600,
Dublin, Ireland, August 23-24, 2014.
Supervised Methods for Aspect-Based Sentiment Analysis 
 
 
Hussam Hamdan*,**,*** 
*LSIS 
Aix-Marseille Universit? CNRS 
Av. Esc. Normandie Niemen,  
13397 Marseille Cedex 20, 
France 
hussam.hamdan@lsis.org 
Patrice Bellot*,** 
**OpenEdition  
Aix-Marseille Universit? CNRS 
3 pl. V. Hugo, case n?86 
13331 Marseille Cedex 3, 
France 
patrice.bellot@lsis.org  
 
Frederic B?chet*** 
***LIF 
Aix-Marseille Universit? CNRS 
Avenue de Luminy  
13288 Marseille Cedex 9, 
France 
frederic.bechet@lif.univ-mrs.fr 
 
  
 
Abstract 
In this paper, we present our contribution in 
SemEval2014 ABSA task, some supervised 
methods for Aspect-Based Sentiment Analysis of 
restaurant and laptop reviews are proposed, im-
plemented and evaluated. We focus on determin-
ing the aspect terms existing in each sentence, 
finding out their polarities, detecting the catego-
ries of the sentence and the polarity of each cate-
gory. The evaluation results of our proposed 
methods exhibit a significant improvement in 
terms of accuracy and f-measure over all four 
subtasks regarding to the baseline proposed by 
SemEval organisers. 
1 Introduction 
The increasing amount of user-generated textual 
data has increased the need of efficient tech-
niques for analysing it. Sentiment Analysis (SA) 
has become more and more interesting since the 
year 2000 (Liu 2012), many techniques in Natu-
ral Language Processing have been used to un-
derstand the expressed sentiment on an entity. 
Many levels of granularity have been also distin-
guished: Document Level SA considers the 
whole document is about an entity and classifies 
whether the expressed sentiment is positive, neg-
ative or neutral; Sentence Level SA determines 
the sentiment of each sentence, some works have 
been done on Clause Level SA but they are still 
not enough; Entity or Aspect-Based SA performs 
finer-grained analysis in which all entities and  
 
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and 
proceedings footer are added by the organisers. Li-
cence details: 
http://creativecommons.org/licenses/by/4.0/ 
 
their aspects should be extracted and the senti-
ment on them should also be determined. 
Aspect-Based SA task consists of several sub-
problems, the document is about many entities 
which could be for example a restaurant, a lap-
top, a printer. Users may refer to an entity by 
different writings but normally there are not a lot 
of variations to indicate the same entity, each 
entity has many aspects which could be its parts 
or attributes, some aspects could be another enti-
ty such as screen of laptop, but most works did 
not take this case into account. Therefore, we 
could define the opinion by the quintuple (Liu 
2012) (ei, aij, sijkl, hk, tl) where ei is the entity i, aij 
are the aspects of the entity i,  sijkl  is the ex-
pressed sentiment on the aspect at the time tl, hk 
the holder which created the document or the 
text. 
This definition does not take into account that the 
entity has aspects that could have also other as-
pects which leads to an aspect hierarchy, in order 
to avoid this information loss, few works have 
handled this issue, they proposed to represent the 
aspect as a tree of aspect terms (Wei and Gulla 
2010; Kim, Zhang et al. 2013).  
Supervised and unsupervised methods have been 
used for handling this task, in this paper, we pro-
pose supervised methods and test them over two 
datasets related to laptop reviews and restaurant 
reviews provided by the ABSA task of 
SemEval2014 (Pontiki, Galanis et al. 2014). We 
tackle four subtasks: 
1. Aspect term extraction: CRF model is 
proposed. 
2. Aspect Term Polarity Detection:  
Multinomial Naive-Bayes classifier with 
some features such as Z-score, POS and 
prior polarity extracted from Subjectivity 
596
Lexicon (Wilson, Wiebe et al. 2005) and 
Bing Liu's Opinion Lexicon1. 
3. Category Detection: 
Z-score model for category detection has 
been used. 
4. Category Polarity Detection: 
The same model proposed for aspect 
term polarity detection has been adopted. 
2 Related works 
Several methods concerning the ABSA have 
been proposed, some of them are supervised, and 
others unsupervised.  The earliest work on aspect 
detection from on-line reviews presented by Hu 
and Liu used association rule mining based on 
Apriori algorithm to extract frequent noun 
phrases as product features, they used two seed 
sets of 30 positive and negative adjectives, then 
WordNet has been used to find and add the seed 
words synonyms. Infrequent aspects had been 
processed by finding the noun related to an opin-
ionated word (Hu and Liu 2004). 
Opinion Digger (Moghaddam and Ester 2010) 
used also Apriori algorithm to extract the fre-
quent aspects then it filters the non-aspects by 
applying a constraint -learned from the training 
data- on the extracted aspects. KNN algorithm is 
applied to estimate the aspect rating scaling from 
1 to 5 stands for (Excellent, Good, Average, 
Poor, Terrible), assuming that the sentiment is 
expressed by the nearest adjectives to the aspect 
term in the sentence segment, WordNet is used 
for finding the synonyms of sentiment word in 
order to use them to estimate the distance be-
tween it and the words of rating scale. 
Some unsupervised methods based on LDA 
(Latent Dirichlet allocation) were proposed. 
Brody and Elhadad used LDA to find the as-
pects, determined the number of topics by apply-
ing a clustering method (Brody and Elhadad 
2010), then they used a similar method proposed 
by Hatzivassiloglou and McKeown 
(Hatzivassiloglou and McKeown 1997) to extract 
the conjunctive adjectives but not the disjunctive 
due to the specificity of the domain, seed sets 
were used and assigned scores, these scores were 
propagated using propagation method through 
the aspect-sentiment graph building from the 
pairs of aspect and related adjectives. 
  Other works make one LDA based model for 
the aspect and sentiment extraction. Lin and He 
                                                 
1
 http://www.cs.uic.edu/~liub/FBS/sentiment-
analysis.html#lexicon 
(Lin and He 2009)proposed Joint model of Sen-
timent and Topic (JST) which extends the state-
of-the-art topic model, Latent Dirichlet Alloca-
tion (LDA) by adding a sentiment layer, this 
model is fully unsupervised and it can detect sen-
timent and topic simultaneously. 
Wei and Gulla (Wei and Gulla 2010) modelled 
the hierarchical relation between product aspects. 
They defined SOT Sentiment Ontology Tree to 
formulate the knowledge of hierarchical relation-
ships among product attributes and tackle the 
problem of sentiment analysis as a hierarchical 
classification problem. Unsupervised hierarchical 
aspect  
Sentiment model (HASM) was proposed by Kim 
et al (Kim, Zhang et al. 2013) to discover a hier-
archical structure of aspect-based sentiments 
from unlabelled online reviews. 
Supervised methods uses normally a CRF or 
HMM models. Jin and Ho (Jin and Ho 2009) 
applied a lexicalized HMM model to extract as-
pects using the words and their part-of-speech 
tags in order to learn a model, then unsupervised 
algorithm for determining the aspect sentiment 
using the nearest opinion word to the aspect and 
taking into account the polarity reversal words 
(such as not). CRF model was used by Jakob and 
Gurevych (Jakob and Gurevych 2010) with these 
features: tokens, POS tags, syntactic dependency 
(if the aspect has a relation with the opinionated 
word), word distance (the distance between the 
word in the closest noun phrase and the opinion-
ated word), and opinion sentences (each token in 
the sentence containing an opinionated expres-
sion is labelled by this feature), the input of this 
method is also the opinionated expressions, they 
use these expressions for predicting the aspect 
sentiment using the dependency parsing for re-
trieving the pair aspect-expression from the train-
ing set. 
Our method for aspect extraction is closed to 
(Jakob and Gurevych 2010), where we used CRF 
model with different features for aspect extrac-
tion, but another method for sentiment detection. 
The second and fourth subtasks are concerning 
the polarity detection, so besides to all previous 
discussed works, we can handle them as sentence 
level SA. We choose to use Multinomial Naive 
Bayes with some features (POS, Z-score, pre-
polarity). The most related work is (Hamdan, 
B?chet et al. 2013) where they used Naive Bays 
with WordNet, DBpedia and SentiWordNet fea-
tures.  
597
3 The System 
Our system is composed of four subtasks: 
3.1 Subtask1: Aspect Terms Extraction 
The objective of this subtask is to extract all 
aspect terms in the review sentence, aspect terms 
could be a word or multiple words. For this pur-
pose we have used CRF (Conditional Random 
Field) which have been used for information ex-
traction. We choose the IOB notation, therefore 
we distinguish the terms at the Beginning, the 
Inside and the Outside of aspect term expression. 
Then, we propose 16 features, for each term we 
extract the following features: 
- Its root (Porter Stemmer); 
- Its POS tag; 
-The stemming roots for all three words before 
and after the term; 
-The POS tags for all three words before and 
after the term; 
- A feature indicates if the word starts with 
capital letter; 
-A feature indicates if the word is capitalised. 
For example, for this review ?But the staff was 
so horrible to us.? Where staff is the aspect term, 
the target of each word will be: 
 But:O the:O staff:B was:O so:O horrible:O to:O 
us:O. 
3.2 Subtask2: Aspect Term Polarity Detection 
This subtask can be seen as sentence level or 
phrase level sentiment Analysis, the first step (1) 
we should detect the context or the words related 
to the aspect term, then to compute its polarity 
according to these words. Dependency parsing 
could be used to determine these words or simple 
distance function. We extract the context of as-
pect term according to the syntax and other as-
pect terms. Therefore, the context is the term it-
self and all the surrounding terms enclosed be-
tween two separators (commas in general), if 
another aspect is also enclosed by these separa-
tors we consider it as a separator instead of the 
comma, and we do not take the terms after it or 
before it (according to its direction to the aspect 
term). If the sentence has only an aspect term the 
separators will be the beginning and the end of 
the sentence. For example, for this review ?It 
took half an hour to get our check, which was 
perfect since we could sit, have drinks and talk!? 
where we have two aspect terms drinks and 
check, the context of check will be ?It took half 
an hour to get our check? and the context of 
drinks will be ?have drinks and talk!?. Another 
example, ?All the money went into the interior 
decoration, none of it went to the chefs.? The 
context for interior decoration will be ?All the 
money went into the interior decoration? and the 
context for chefs will be ?none of it went to the 
chefs?. 
The second step (2) we should determine the 
polarity, which could be positive, negative, neu-
tral or conflict. We propose to use Multinomial 
Naive-Bayes for learning a classifier based on 
different features:  
- The terms in the sentence (term frequency); 
- The POS features (the number of adjectives, 
adverbs, verbs, nouns, connectors) 
- The pre-polarity features (the number of pos-
itive and negative words in the sentence ex-
tracted from Subjectivity lexicon and Bing 
Liu's Opinion Lexicon); 
- Z-score features (the number of words which 
have Z-score more than three in each senti-
ment class),  Z_score  is described in 3.3. 
3.3 Subtask3: Category Detection 
Determining the categories of each sentence 
can be seen as a multi-label classification prob-
lem at sentence level.  
We propose to use Z-score which is capable of 
distinguishing the importance of a term in a cate-
gory. The more the term is important in a catego-
ry the more its Z-score is high in this category 
and low in other categories in which it is not im-
portant. Thus, we compute the Z-score for all 
terms using the annotated data, then for each 
given sentence, the sum of Z-score over each 
category is computed if the Z-score of term in a 
category is less than zero, we ignore it in this 
category because it is not important, the sentence 
will be attributed to the category having the 
highest Z-score, if some categories have the 
same Z-scores the sentence will be attributed to 
the both. The algorithm steps: 
For each tem t in the sentence: 
For each category c: 
If z-score(t,c)>0: 
Z_sc[c]+= z-score(t,c) 
Categories=max(Z_sc) 
 
We assume that the term frequency follows the 
multinomial distribution. Thus, Z_score can be 
seen as a standardization of the term frequency. 
We compute Z score for each term ti in a class Cj 
(tij) by calculating its term relative frequency tfrij 
in a particular class Cj, as well as the mean 
598
(meani) which is the term probability over the 
whole corpus multiplied by nj the number of 
terms in the class Cj, and standard deviation (sdi) 
of term ti according to the underlying corpus (see 
Eq. (1,2)). 
 
Z?????????? =
???????????
??? Eq. (1) 
 
Z?????????? =
??????????(??)
?????(??)?(???(??))Eq. (2) 
 
 Z_score was exploited for SA by (Zubaryeva 
and Savoy 2010), they choose a threshold (Z>2) 
for selecting the number of terms having Z_score 
more than the threshold, then they used a logistic 
regression for combining these scores. We use 
Z_score as added features for multinomial Naive 
Bayes classifier. 
3.4 Subtask4:  Category Polarity Detection 
We have used Multinomial Naive-Bayes as in 
the subtask2 step (2) with the same features, but 
the different that we add also the name of the 
category as a feature. Thus, for each sentence 
having n category we add n examples to the 
training set, the difference between them is the 
feature of the category.  
4 Experiments and Evaluations 
We tested our system using the training and test-
ing data provided by SemEval 2014 ABSA task. 
Two data sets were provided; the first con-
tains3Ksentences of restaurant reviews annotated 
by the aspect terms, their polarities, their catego-
ries, the polarities of each category. The second 
contains of 3K sentences of laptop reviews anno-
tated just by the aspect terms, their polarities. 
The evaluation process was done in two steps. 
First step is concerning the subtasks 1 and 3 
which involves the aspect terms extraction and 
category detection, we were provided with res-
taurant review and laptop review sentences and 
we had to extract the aspect terms for both data 
sets and the categories for the restaurant one. 
Baseline methods were provided; Table1 demon-
strates the results of these subtasks in terms of 
precision P, recall R and f-measure F for our sys-
tem and the baseline2. 
We remark that our system is 24% and 21% 
above the baseline for aspect terms extraction in 
restaurant and laptop reviews respectively, and 
                                                 
2http://alt.qcri.org/semeval2014/task4/data/uploads/ba
selinesystemdescription.pdf 
above 3% for category detection in restaurant 
reviews. 
 
Data subtask  P R F 
Res 1 Baseline 0,52 0,42 0,47 
System 0.81 0.63 0.71 
3 Baseline 0,73 0,59 0,65 
System 0.77 0.60 0.68 
Lap 1 Baseline 0,44 0,29 0,35 
System 0.76 0.45 0.56 
Table 1. Results of subtask 1, 2 for restaurant reviews, sub-
task 1 for laptop reviews 
 
The second step involves the evaluation of 
subtask 2 and 4, we were provided with(1) res-
taurant review sentences annotated by their as-
pect terms, and categories, we had to determine 
the polarity for each aspect term and category; 
(2) laptop review sentences annotated by aspect 
terms and we had to determine the aspect term 
polarity. Table 2 demonstrates the results of our 
system and the baseline (A: accuracy, R: number 
of true retrieved examples, All: number of all 
true examples). 
 
Data subtask  R All A 
Res 2 Baseline 673 1134 0,64 
System 818 1134 0.72 
4 Baseline 673 1025 0,65 
System 739 1025 0.72 
Lap 2 Baseline 336 654 0,51 
System 424 654 0,64 
Table 2. Results of subtask 2, 4 for restaurant reviews, sub-
task 2 for laptop reviews 
 
We remark that our system is 8% and 13% above 
the baseline for aspect terms polarity detection in 
restaurant and laptop reviews respectively, and 
7% above for category polarity detection in res-
taurant reviews. 
5 Conclusion 
We have built a system for Aspect-Based Sen-
timent Analysis; we proposed different super-
vised methods for the four sub-tasks. Our results 
are always above the baseline proposed by the 
organiser of SemEval. We proposed to use CRF 
for aspect term extraction, Z-score model for cat-
egory detection, Multinomial Naive-Bayes with 
some new features for polarity detection. We 
find that the use of Z-score is useful for the cate-
gory and polarity detection, we are going to test 
it in another sentiment analysis tasks of another 
domains. 
 
599
Reference 
Samuel Brody and Noemie Elhadad (2010). An 
unsupervised aspect-sentiment model for 
online reviews. Human Language 
Technologies: The 2010 Annual Conference 
of the North American Chapter of the 
Association for Computational Linguistics. 
Los Angeles, California, Association for 
Computational Linguistics: 804-812. 
Hussam Hamdan,Frederic B?chet and Patrice Bellot 
(2013). Experiments with DBpedia, 
WordNet and SentiWordNet as resources for 
sentiment analysis in micro-blogging. 
Proceedings of the Seventh International 
Workshop on Semantic Evaluation (SemEval 
2013), Atlanta, Georgia, USA. 
Vasileios Hatzivassiloglou and Kathleen R Mckeown 
(1997). Predicting the semantic orientation of 
adjectives. Proceedings of the 35th Annual 
Meeting of the Association for 
Computational Linguistics and Eighth 
Conference of the European Chapter of the 
Association for Computational Linguistics, 
Association for Computational Linguistics. 
Minqing Hu and Bing Liu (2004). Mining and 
summarizing customer reviews. Proceedings 
of the tenth ACM SIGKDD international 
conference on Knowledge discovery and 
data mining. Seattle, WA, USA, ACM: 168-
177. 
Niklas Jakob and Iryna Gurevych (2010). Extracting 
opinion targets in a single- and cross-domain 
setting with conditional random fields. 
Proceedings of the 2010 Conference on 
Empirical Methods in Natural Language 
Processing. Cambridge, Massachusetts, 
Association for Computational Linguistics: 
1035-1045. 
Wei Jin and Hung Hay Ho (2009). A novel 
lexicalized HMM-based learning framework 
for web opinion mining. Proceedings of the 
26th Annual International Conference on 
Machine Learning. Montreal, Quebec, 
Canada, ACM: 465-472. 
Suin Kim,Jianwen Zhang,Zheng Chen,Alice Oh and 
Shixia Liu (2013). A Hierarchical Aspect-
Sentiment Model for Online Reviews. 
Chenghua Lin and Yulan He (2009). Joint 
sentiment/topic model for sentiment analysis. 
Proceedings of the 18th ACM conference on 
Information and knowledge management. 
Hong Kong, China, ACM: 375-384. 
Bing Liu (2012). Sentiment Analysis and Opinion 
Mining, Morgan &amp; Claypool Publishers. 
Samaneh Moghaddam and Martin Ester (2010). 
Opinion digger: an unsupervised opinion 
miner from unstructured product reviews. 
Proceedings of the 19th ACM international 
conference on Information and knowledge 
management. Toronto, ON, Canada, ACM: 
1825-1828. 
Maria Pontiki,Dimitrios Galanis,John 
Pavlopoulos,Harris Papageorgiou,Ion 
Androutsopoulos and Suresh Manandhar. 
(2014). "SemEval-2014 Task 4: Aspect 
Based Sentiment Analysis." Proceedings of 
the International Workshop on Semantic 
Evaluation (SemEval). 
Wei Wei and Jon Atle Gulla (2010). Sentiment 
learning on product reviews via sentiment 
ontology tree. Proceedings of the 48th 
Annual Meeting of the Association for 
Computational Linguistics. Uppsala, 
Sweden, Association for Computational 
Linguistics: 404-413. 
Theresa Wilson,Janyce Wiebe and Paul Hoffmann 
(2005). Recognizing contextual polarity in 
phrase-level sentiment analysis. Proceedings 
of the conference on Human Language 
Technology and Empirical Methods in 
Natural Language Processing. Vancouver, 
British Columbia, Canada, Association for 
Computational Linguistics: 347-354. 
Olena Zubaryeva and Jacques Savoy (2010). "Opinion 
Detection by Combining Machine Learning 
& Linguistic Tools." In Proceedings of the 
8th NTCIR, Workshop Meeting on 
Evaluation of Information Access 
Technologies: InformationRetrieval, 
Question Answering and Cross-Lingual 
Information Access. 
 
 
600
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 636?641,
Dublin, Ireland, August 23-24, 2014.
The Impact of Z_score on Twitter Sentiment Analysis 
 
Hussam Hamdan*,**,*** 
*LSIS 
Aix-Marseille Universit? CNRS 
Av. Esc. Normandie Niemen,  
13397 Marseille Cedex 20, 
France 
hussam.hamdan@lsis.org 
Patrice Bellot*,** 
          **OpenEdition  
Aix-Marseille Universit? CNRS 
3 pl. V. Hugo, case n?86 
13331 Marseille Cedex 3, 
France 
patrice.bellot@lsis.org  
 
Frederic B?chet*** 
***LIF 
Aix-Marseille Universit? CNRS 
Avenue de Luminy  
13288 Marseille Cedex 9, 
France 
frederic.bechet@lif.univ-mrs.fr 
 
  
 
Abstract 
Twitter has become more and more an im-
portant resource of user-generated data. Sen-
timent Analysis in Twitter is interesting for 
many applications and objectives. In this pa-
per, we propose to exploit some features 
which can be useful for this task; the main 
contribution is the use of Z-scores as features 
for sentiment classification in addition to 
pre-polarity and POS tags features. Our ex-
periments have been evaluated using the test 
data provided by SemEval 2013 and 2014. 
The evaluation demonstrates that Z_scores 
features can significantly improve the predic-
tion performance. 
1 Introduction 
The interactive Web has changed the relation 
between the users and the web. Users have be-
come an important source of content. They ex-
press their opinion towards different issues. The-
se opinions are important for others who are in-
terested in understanding users? interests such as 
buyers, sellers and producers. 
 Twitter is one of the most important platforms in 
which the users express their opinions. Many 
works have exploited this media for predicting 
valuable issues depending on Sentiment Analysis 
(SA). The authors in (Asur and Huberman 2010) 
predicted the box-office revenues of movies in 
advance of their releases using the tweets talking 
about them. In (Bae and Lee 2012) Sentiment 
 
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers 
and proceedings footer are added by the organisers. 
Licence details: 
http://creativecommons.org/licenses/by/4.0/ 
 
Analysis has been used to study the impact of 13 
twitter accounts of famous persons on their fol-
lowers and also for forecasting the interesting 
tweets which are more probably to be reposted 
by the followers (Naveed, Gottron et al. 2011). 
Sentiment Analysis can be done in different lev-
els; Document level; Sentence level; Clause level 
or Aspect-Based level. SA in Twitter can be seen 
as a sentence level task, but some limitations 
should be considered in such sentences. The size 
of tweets is limited to 140 characters, informal 
language, emotion icons and non-standard ex-
pressions are commonly used, and many spelling 
errors can be found due to the absence of cor-
rectness verification. 
   Three different approaches can be identified in 
the literature of Sentiment Analysis in Twitter, 
the first approach is lexicon based, using specific 
types of lexicons to derive the polarity of a text, 
this approach suffers from the limited size of lex-
icon and requires human expertise to build man-
ual lexicon (Joshi, Balamurali et al. 2011), in the 
other hand the automatic lexicons are not so effi-
cient. The second one is machine learning ap-
proach which uses annotated texts with a given 
labels to learn a classification model, an early 
work was done on a movie review dataset (Pang, 
Lee et al. 2002). Both lexicon and machine learn-
ing approaches can be combined to achieve a 
better performance (Khuc, Shivade et al. 2012). 
These two approaches are used for SA task but 
the third one is specific for Twitter or social con-
tent, the social approach exploits social network 
properties and data for enhancing the accuracy of 
the classification (Speriosu, Sudan et al. 2011). 
    In this paper, we exploit machine learning al-
gorithm with the aid of some features: 
? The original Terms: the terms represent-
ing the tweet after the tokenization and  
stemming; 
636
? Pre-polarity features: the number of neg-
ative, positive and neutral words extract-
ed from two sentiment lexicons; 
? POS tags: the number of adjectives, con-
nectors, verbs, nouns, adverbs in the 
tweet; 
? Z-score: The numbers of terms having Z-
score value more than three for each 
class positive, negative and neutral. 
   We extended the original terms with these last 
features. We also constructed a dictionary for the 
abbreviations and the slang words used in Twit-
ter in order to overcome the ambiguity of the 
tweets.  We tested the performance of every pos-
sible combination of these features. 
     The rest of this paper is organized as follows. 
Section 2 outlines previous work that focused on 
sentiment analysis in Twitter. Section 3 presents 
the Z_score features and the others which we 
used for training a classifier. Our experiments are 
described in section 4, conclusion and future 
work is presented in section 5. 
2 Related Works 
We can identify three main approaches for sen-
timent analysis in Twitter. The lexicon based 
approaches which depend on sentiment lexicons 
containing positive, negative and neutral words 
or expressions; they calculate the polarity ac-
cording to the number of common opinionated 
words between the lexicons and the text. Many 
dictionaries have been created manually such as 
ANEW (Affective Norms for English Words) or 
automatically such as SentiWordNet 
(Baccianella, Esuli et al. 2010). Four lexicon dic-
tionaries were used to overcome the lack of 
words in each one (Joshi, Balamurali et al. 2011; 
Mukherjee, Malu et al. 2012). Automatically 
construction of a Twitter lexicon was imple-
mented by (Khuc, Shivade et al. 2012). 
      Machine learning approaches were employed 
from annotated tweets by using Naive Bayes, 
Maximum Entropy MaxEnt and Support Vector 
Machines (SVM). The authors (Go, Bhayani et 
al. 2009) reported that SVM outperforms other 
classifiers. They tried a unigram and a bigram 
model in conjunction with parts-of-speech (POS) 
features; they noted that the unigram model out-
performs all other models when using SVM and 
that POS features decrease the quality of results. 
The authors in (Kouloumpis, Wilson et al. 2011) 
found that N-gram with lexicon features and mi-
cro-blogging features are useful but POS features 
are not.  In contrast, in (Pak and Paroubek 2010) 
they reported that POS and bigrams both help. In 
(Barbosa and Feng 2010) the authors proposed 
the use of syntax features of tweets like retweet, 
hashtags, link, punctuation and exclamation 
marks in conjunction with features like prior po-
larity of words and POS tags, in (Agarwal, Xie et 
al. 2011) this approach was extended by using 
real valued prior polarity and by combining prior 
polarity with POS. Authors in (Saif, He et al. 
2012) proposed to use the semantic features, 
therefore they extracted the named entities in the 
tweets. Authors in (Hamdan, B?chet et al. 2013) 
used the concepts extracted from DBpedia and 
the adjectives from WordNet, they reported that 
the DBpedia concepts are useful with Na?ve-
Bayes classifier but less useful with SVM. 
     The third main approach takes into account 
the influence of users on their followers and the 
relation between the users and the tweets they 
wrote. It assumes that using the Twitter follower 
graph might improve the polarity classification. 
In (Speriosu, Sudan et al. 2011) they demonstrat-
ed that using label propagation with Twitter fol-
lower graph improves the polarity classification. 
In  (Tan, Lee et al. 2011) they employed social 
relation for user-level sentiment analysis. In (Hu, 
Tang et al. 2013) a Sociological Approach to 
handling the Noisy and short Text (SANT) for 
supervised sentiment classification is used; they 
reported that social theories such as Sentiment 
Consistency and Emotional Contagion could be 
helpful for sentiment analysis. 
3 Feature Selection 
We used different types of features in order to 
improve the accuracy of sentiment classification. 
- Bag of words (Terms) 
The most commonly used features in text analy-
sis are the bag of words which represent a text as 
unordered set of words or terms. It assumes that 
words are independent from each other and also 
disregards their order of appearance. We 
stemmed the words using Porter Stemmer and 
used them as a baseline features.  
 
- Z_score Features (Z) 
We suggest using a new type of features for Sen-
timent Analysis, Z_score can distinguish the im-
portance of each term in each class. We compute 
the number of terms having Z_score more than 
three for each class over each tweet. We assume 
that the term frequencies follow the multinomial 
distribution. Thus, Z_score can be seen as a 
standardization of the term. We compute the 
637
Z_score for each term ti in a class Cj (tij) by cal-
culating its term relative frequency tfrij in a par-
ticular class Cj, as well as the mean (meani) 
which is the term probability over the whole cor-
pus multiplied by nj the number of terms in the 
class Cj, and standard deviation (sdi) of term ti 
according to the underlying corpus (see Eq. 
(1,2)).  
 
Z?????????? =
???????????
???              Eq. (1) 
 
Z?????????? =
??????????(??)
?????(??)?(???(??))   Eq. (2) 
 
 The term which has salient frequency in a class 
in compassion to others will have a salient 
Z_score. Z_score was exploited for SA by 
(Zubaryeva and Savoy  2010) , they choose a 
threshold (>2) for selecting the number of terms 
having Z_score more than the threshold, then 
they used a logistic regression for combining 
these scores. We use Z_scores as added features 
for classification because the tweet is too short, 
therefore many tweets does not have any words 
with salient Z_score. The three following figures 
1,2,3 show the distribution of Z_score over each 
class, we remark that the majority of terms has 
Z_score between -1.5 and 2.5 in each class and 
the rest are either vey frequent (>2.5) or very rare 
(<-1.5). It should indicate that negative value 
means that the term is not frequent in this class in 
comparison with its frequencies in other classes. 
Table1 demonstrates the first ten terms having 
the highest Z_scores in each class. We have test-
ed to use different values for the threshold, the 
best results was obtained when the threshold is 3. 
 
p
o
sitiv
e
 
Z_
sco
re
 
n
eg
ativ
e
 
Z_
sco
re
 
N
eutral
 
Z_
sco
re
 
Love 
Good 
Happy 
Great 
Excite 
Best 
Thank 
Hope 
Cant 
Wait 
14.31 
14.01 
12.30 
11.10 
10.35 
9.24 
9.21 
8.24 
8.10 
8.05 
Not 
Fuck 
Don?t 
Shit 
Bad 
Hate 
Sad 
Sorry 
Cancel 
stupid 
13.99 
12.97 
10.97 
8.99 
8.40 
8.29 
8.28 
8.11 
7.53 
6.83 
Httpbit 
Httpfb 
Httpbnd 
Intern 
Nov 
Httpdlvr 
Open 
Live 
Cloud 
begin 
6.44 
4.56 
3.78 
3.58 
3.45 
3.40 
3.30 
3.28 
3.28 
3.17 
Table1. The first ten terms having the highest Z_score in 
each class 
 
-  Sentiment Lexicon Features (POL) 
We used two sentiment lexicons, MPQA Subjec-
tivity Lexicon(Wilson, Wiebe et al. 2005) and 
Bing Liu's Opinion Lexicon which is created by 
(Hu and Liu 2004) and augmented in many latter 
works. We extract the number of positive, nega-
tive and neutral words in tweets according to the-
se lexicons. Bing Liu's lexicon only contains 
negative and positive annotation but Subjectivity 
contains negative, positive and neutral. 
 
- Part Of Speech (POS) 
We annotate each word in the tweet by its POS 
tag, and then we compute the number of adjec-
tives, verbs, nouns, adverbs and connectors in 
each tweet. 
4 Evaluation 
4.1 Data collection 
  We used the data set provided in SemEval 2013 
and 2014 for subtask B of sentiment analysis in 
Twitter(Rosenthal, Ritter et al. 2014) (Wilson, 
Kozareva et al. 2013). The participants were 
provided with training tweets annotated as posi-
tive, negative or neutral. We downloaded these 
tweets using a given script. Among 9646 tweets, 
we could only download 8498 of them because 
of protected profiles and deleted tweets. Then, 
we used the development set containing 1654 
tweets for evaluating our methods. We combined 
the development set with training set and built a 
new model which predicted the labels of the test 
set 2013 and 2014.  
 
4.2 Experiments 
 
Official Results 
   The results of our system submitted for 
SemEval evaluation gave 46.38%, 52.02% for 
test set 2013 and 2014 respectively. It should 
mention that these results are not correct because 
of a software bug discovered after the submis-
sion deadline, therefore the correct results is 
demonstrated as non-official results. In fact the 
previous results are the output of our classifier 
which is trained by all the features in section 3, 
but because of index shifting error the test set 
was represented by all the features except the 
terms. 
 
Non-official Results 
  We have done various experiments using the 
features presented in Section 3 with Multinomial 
Na?ve-Bayes model. We firstly constructed fea-
ture vector of tweet terms which gave 49%, 46% 
for test set 2013, 2014 respectively. Then, we 
augmented this original vector by the Z_score 
638
features which improve the performance by 6.5% 
and 10.9%, then by pre-polarity features which 
also improve the f-measure by 4%, 6%, but the 
extending with POS tags decreases the f-
measure. We also test all combinations with the-
se previous features, Table2 demonstrates the 
results of each combination, we remark that POS 
tags are not useful over all the experiments, the 
best result is obtained by combining Z_score and 
pre-polarity features. We find that Z_score fea-
tures improve significantly the f-measure and 
they are better than pre-polarity features.    
 
 
Figure 1 Z_score distribution in positive class 
 
Figure 2 Z_score distribution in neutral class 
 
Figure 3 Z_score distribution in negative class 
 Features F-measure 
2013 2014 
Terms 49.42 46.31 
Terms+Z 55.90 57.28 
Terms+POS 43.45 41.14 
Terms+POL 53.53 52.73 
Terms+Z+POS 52.59 54.43 
Terms+Z+POL 58.34 59.38 
Terms+POS+POL 48.42 50.03 
Terms+Z+POS+POL 55.35 58.58 
Table 2. Average f-measures for positive and negative clas-
ses of SemEval2013 and 2014 test sets. 
We repeated all previous experiments after using 
a twitter dictionary where we extend the tweet by 
the expressions related to each emotion icons or 
abbreviations in tweets. The results in Table3 
demonstrate that using that dictionary improves 
the f-measure over all the experiments, the best 
results obtained also by combining Z_scores and 
pre-polarity features. 
 
Features F-measure 
2013 2014 
Terms 50.15 48.56 
Terms+Z 57.17 58.37 
Terms+POS 44.07 42.64 
Terms+POL 54.72 54.53 
Terms+Z+POS 53.20 56.47 
Terms+Z+POL 59.66 61.07 
Terms+POS+POL 48.97 51.90 
Terms+Z+POS+POL 55.83 60.22 
Table 3. Average f-measures for positive and negative clas-
ses of SemEval2013 and 2014 test sets after using a twitter 
dictionary. 
5 Conclusion 
  In this paper we tested the impact of using 
Twitter Dictionary, Sentiment Lexicons, Z_score 
features and POS tags for the sentiment classifi-
cation of tweets. We extended the feature vector 
of tweets by all these features; we have proposed 
new type of features Z_score and demonstrated 
that they can improve the performance. 
We think that Z_score can be used in different 
ways for improving the Sentiment Analysis, we 
are going to test it in another type of corpus and 
using other methods in order to combine these 
features. 
Reference 
Apoorv Agarwal,Boyi Xie,Ilia Vovsha,Owen 
Rambow and Rebecca Passonneau (2011). 
Sentiment analysis of Twitter data. 
Proceedings of the Workshop on Languages 
639
in Social Media. Portland, Oregon, 
Association for Computational Linguistics: 
30-38. 
Sitaram Asur and Bernardo A. Huberman (2010). 
Predicting the Future with Social Media. 
Proceedings of the 2010 IEEE/WIC/ACM 
International Conference on Web 
Intelligence and Intelligent Agent 
Technology - Volume 01, IEEE Computer 
Society: 492-499. 
Stefano Baccianella,Andrea Esuli and Fabrizio 
Sebastiani (2010). SentiWordNet 3.0: An 
Enhanced Lexical Resource for Sentiment 
Analysis and Opinion Mining. Proceedings 
of the Seventh Conference on International 
Language Resources and Evaluation 
(LREC'10), European Language Resources 
Association (ELRA). 
Younggue Bae and Hongchul Lee (2012). "Sentiment 
analysis of twitter audiences: Measuring the 
positive or negative influence of popular 
twitterers." J. Am. Soc. Inf. Sci. Technol. 
63(12): 2521-2535. 
Luciano Barbosa and Junlan Feng (2010). Robust 
sentiment detection on Twitter from biased 
and noisy data. Proceedings of the 23rd 
International Conference on Computational 
Linguistics: Posters. Beijing, China, 
Association for Computational Linguistics: 
36-44. 
Alec Go,Richa Bhayani and Lei Huang Twitter 
Sentiment Classification using Distant 
Supervision. 
Hussam Hamdan,Frederic B?chet and Patrice Bellot 
(2013). Experiments with DBpedia, 
WordNet and SentiWordNet as resources for 
sentiment analysis in micro-blogging. 
Proceedings of the Seventh International 
Workshop on Semantic Evaluation (SemEval 
2013), Atlanta, Georgia, USA. 
Minqing Hu and Bing Liu (2004). Mining and 
summarizing customer reviews. Proceedings 
of the tenth ACM SIGKDD international 
conference on Knowledge discovery and 
data mining. Seattle, WA, USA, ACM: 168-
177. 
Xia Hu,Lei Tang,Jiliang Tang and Huan Liu (2013). 
Exploiting social relations for sentiment 
analysis in microblogging. Proceedings of 
the sixth ACM international conference on 
Web search and data mining. Rome, Italy, 
ACM: 537-546. 
Aditya Joshi,A. R. Balamurali,Pushpak Bhattacharyya 
and Rajat Mohanty (2011). C-Feel-It: a 
sentiment analyzer for micro-blogs. 
Proceedings of the 49th Annual Meeting of 
the Association for Computational 
Linguistics: Human Language Technologies: 
Systems Demonstrations. Portland, Oregon, 
Association for Computational Linguistics: 
127-132. 
Vinh Ngoc Khuc,Chaitanya Shivade,Rajiv Ramnath 
and Jay Ramanathan (2012). Towards 
building large-scale distributed systems for 
twitter sentiment analysis. Proceedings of the 
27th Annual ACM Symposium on Applied 
Computing. Trento, Italy, ACM: 459-464. 
E. Kouloumpis,T. Wilson and J. Moore (2011). 
Twitter Sentiment Analysis: The Good the 
Bad and the OMG! Fifth International AAAI 
Conference on Weblogs and Social Media. 
Subhabrata Mukherjee,Akshat Malu,Balamurali A.R. 
and Pushpak Bhattacharyya (2012). TwiSent: 
a multistage system for analyzing sentiment 
in twitter. Proceedings of the 21st ACM 
international conference on Information and 
knowledge management. Maui, Hawaii, 
USA, ACM: 2531-2534. 
Nasir Naveed,Thomas Gottron,J\'Er\^Ome Kunegis 
and Arifah Che Alhadi (2011). Bad News 
Travels Fast: A Content-based Analysis of 
Interestingness on Twitter. Proc. Web 
Science Conf. 
Alexander Pak and Patrick Paroubek (2010). Twitter 
as a Corpus for Sentiment Analysis and 
Opinion Mining. Proceedings of the Seventh 
conference on International Language 
Resources and Evaluation (LREC'10), 
Valletta, Malta, European Language 
Resources Association (ELRA). 
Bo Pang,Lillian Lee and Shivakumar Vaithyanathan 
(2002). Thumbs up?: sentiment classification 
using machine learning techniques. 
Proceedings of the ACL-02 conference on 
Empirical methods in natural language 
processing - Volume 10, Association for 
Computational Linguistics: 79-86. 
Sara Rosenthal,Alan Ritter,Veselin Stoyanov and 
Preslav Nakov (2014). "SemEval-2014 Task 
9: Sentiment Analysis in Twitter." In 
Proceedings of the Eighth International 
Workshop on Semantic Evaluation 
(SemEval'14).August 23-24, Dublin, Ireland. 
Hassan Saif,Yulan He and Harith Alani (2012). 
Semantic sentiment analysis of twitter. 
Proceedings of the 11th international 
conference on The Semantic Web - Volume 
Part I. Boston, MA, Springer-Verlag: 508-
524. 
Michael Speriosu,Nikita Sudan,Sid Upadhyay and 
Jason Baldridge (2011). Twitter polarity 
classification with label propagation over 
lexical links and the follower graph. 
Proceedings of the First Workshop on 
Unsupervised Learning in NLP. Edinburgh, 
Scotland, Association for Computational 
Linguistics: 53-63. 
Chenhao Tan,Lillian Lee,Jie Tang,Long Jiang,Ming 
Zhou and Ping Li (2011). User-level 
640
sentiment analysis incorporating social 
networks. Proceedings of the 17th ACM 
SIGKDD international conference on 
Knowledge discovery and data mining. San 
Diego, California, USA, ACM: 1397-1405. 
Theresa Wilson,Zornitsa Kozareva,Preslav 
Nakov,Alan Ritter,Sara Rosenthal and 
Veselin Stoyanov (2013). "SemEval-2013 
Task 2: Sentiment Analysis in Twitter." 
Proceedings of the 7th International 
Workshop on Semantic Evaluation. 
Association for Computational Linguistics. 
Theresa Wilson,Janyce Wiebe and Paul Hoffmann 
(2005). Recognizing contextual polarity in 
phrase-level sentiment analysis. Proceedings 
of the conference on Human Language 
Technology and Empirical Methods in 
Natural Language Processing. Vancouver, 
British Columbia, Canada, Association for 
Computational Linguistics: 347-354. 
Olena Zubaryeva and Jacques Savoy (2010). "Opinion 
Detection by Combining Machine Learning 
& Linguistic Tools." In Proceedings of the 
8th NTCIR, Workshop Meeting on 
Evaluation of Information Access 
Technologies: InformationRetrieval, 
Question Answering and Cross-Lingual 
Information Access. 
 
 
641
