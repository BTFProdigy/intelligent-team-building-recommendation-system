Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 40?46,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Ontology Label Translation
Mihael Arcan and Paul Buitelaar
Unit for Natural Language Processing,
Digital Enterprise Research Institute (DERI)
National University of Ireland Galway (NUIG)
Galway, Ireland
{mihael.arcan , paul.buitelaar}@deri.org
Abstract
Our research investigates the translation of on-
tology labels, which has applications in mul-
tilingual knowledge access. Ontologies are
often defined only in one language, mostly
English. To enable knowledge access across
languages, such monolingual ontologies need
to be translated into other languages. The
primary challenge in ontology label trans-
lation is the lack of context, which makes
this task rather different than document trans-
lation. The core objective therefore, is to
provide statistical machine translation (SMT)
systems with additional context information.
In our approach, we first extend standard SMT
by enhancing a translation model with context
information that keeps track of surrounding
words for each translation. We compute a se-
mantic similarity between the phrase pair con-
text vector from the parallel corpus and a vec-
tor of noun phrases that occur in surrounding
ontology labels. We applied our approach to
the translation of a financial ontology, translat-
ing from English to German, using Europarl as
parallel corpus. This experiment showed that
our approach can provide a slight improve-
ment over standard SMT for this task, with-
out exploiting any additional domain-specific
resources.
1 Introduction
The biggest barrier for EU-wide cross-lingual busi-
ness intelligence is the large number of various lan-
guages used by banks or investment firms for their
financial reports. In contrast to that, most of the
ontologies used for knowledge access are available
in English, e.g. the financial ontology FINREP1
(FINancial REPorting) or COREP2 (COmmon sol-
vency ratio REPorting). To make the targeted trans-
parency of financial information possible, these on-
tologies have to be translated first into another lan-
guage; see also (Declerck et al, 2010). The chal-
lenge here lies in translating domain-specific on-
tology vocabulary, e.g. Equity-equivalent partner
loans, Subordinated capital or Write-downs of long-
term financial assets and securities.
Since domain-specific parallel corpora for SMT
are hardly available, we used a large general parallel
corpus, whereby a translation model built by such
a resource will tend to translate a segment into the
most common word sense. This can be seen for in-
stance when we translate the financial ontology label
Equity-equivalent partner loans from the German
GAAP ontology (cf. Section 3.1). Using a baseline
SMT system we get the translation Gerechtigkeit-
gleichwertige Partner Darlehen. Although this la-
bel provides contextual information, equity is trans-
lated into its general meaning, i.e. Gerechtigkeit in
the meaning of justice, righteousness or fairness, al-
though Eigenkapital would be the preferred transla-
tion in the financial domain.
To achieve accurate disambiguation we developed
a method using context vectors. We extract semantic
information from the ontology, i.e. the vocabulary
and relations between labels and compare it with the
contextual information extracted from a parallel cor-
pus.
The remainder of the paper is organized as fol-
1http://eba.europa.eu/Supervisory-Reporting/FINER.aspx
2http://eba.europa.eu/Supervisory-Reporting/COREP.aspx
40
lows. Section 2 gives an overview of the related
work on including semantic information into SMT.
Section 3 describes the ontology and the parallel
corpus used in our experiment. Then we describe
the approach of enhancing the standard SMT model
with ontological knowledge for improving the trans-
lation of labels in Section 4. In Section 5 the results
of exploiting the ontological knowledge described in
the previous section are illustrated. Finally we con-
clude our findings and give an outlook for further
research.
2 Related Work
Word sense disambiguation (WSD) systems gener-
ally perform on the word level, for an input word
they generate the most probable meaning. On the
other hand, state of the art translation systems op-
erate on sequences of words. This discrepancy be-
tween unigrams versus n-grams was first described
in (Carpuat and Wu, 2005). Likewise, (Apidianaki
et al, 2012) use a WSD classifier to generate a prob-
ability distribution of phrase pairs and to build a lo-
cal language model. They show that the classifier
does not only improve the translation of ambiguous
words, but also the translation of neighbour words.
We investigate this discrepancy as part of our re-
search in enriching the ontology label translation
with ontological information. Similar to their work
we incorporate the idea of enriching the translation
model with neighbour words information, whereby
we extend the window to 5-grams.
(Mauser et al, 2009) generate a lexicon that pre-
dicts the bag of output words from the bag of input
words. In their research, no alignment between input
and output words is used, words are chosen based
on the input context. The word predictions of the in-
put sentences are considered as an additional feature
that is used in the decoding process. This feature de-
fines a new probability score that favours the trans-
lation hypothesis containing words, which were pre-
dicted by the lexicon model. Similarly, (Patry and
Langlais, 2011) train a model by translating a bag-
of-words. In contrast to their work, our approach
uses bag-of-word information to enrich the missing
contextual information that arises from translating
ontology labels in isolation.
(McCrae et al, 2011) exploit in their research
the ontology structure for translation of ontologies
and taxonomies. They compare the structure of
the monolingual ontology to the structure of already
translated multilingual ontologies, where the source
and target labels are used for the disambiguation
process of phrase pairs. We incorporated the idea of
using the ontology structure, but avoided the draw-
back of exploiting existing domain-specific multilin-
gual ontologies.
3 Data sets
For our experiment we used a general parallel cor-
pus to generate the mandatory SMT phrase table
and language model. Further, the corpus was used
to generate feature vectors on the basis of the con-
textual information provided by surrounding words.
Finally we calculate the semantic similarity between
the extracted information from the parallel corpus
and the ontology vocabulary.
3.1 Financial ontology
For our experiment we used the financial ontol-
ogy German GAAP (Generally Accepted Account-
ing Practice),3 which holds 2794 concepts with la-
bels in German and English.
Balance sheet
. . . Total equity and liabilities
Equity
Equity-equivalent partner loans Revenue reserves
Legal reserve
Legal reserve, of which transferred from prior year net retained profits
. . .
. . .
Figure 1: The financial label Equity-equivalent partner
loans and its neighbours in the German GAAP ontology
As seen in Figure 1 the financial labels do not cor-
respond to phrases from a linguistic point of view.
They are used in financial or accounting reports as
unique financial expressions or identifiers to organ-
ise and retrieve the reported information automati-
cally. Therefore it is important to translate these fi-
nancial labels with exact meaning preservation.
3http://www.xbrl.de/
41
3.2 Europarl
As a baseline approach we used the Europarl par-
allel corpus,4 which holds proceedings of the Euro-
pean Parliament in 21 European languages. We used
the English-German parallel corpus with around 1.9
million aligned sentences and 40 million English
and 43 million German tokens (Koehn, 2005).
Although previous research showed that a trans-
lation model built by using a general parallel cor-
pus cannot be used for domain-specific vocabulary
translation (Wu et al, 2008), we decided to train a
baseline translation model on this general corpus to
illustrate any improvement steps gained by enrich-
ing the standard approach with the semantic infor-
mation of the ontology vocabulary and structure.
4 Experiment
Since ontology labels (or label segments) translated
by the Moses toolkit (Section 4.1) do not have much
contextual information, we addressed this lack of
information and generated fromthe Europarl corpus
a new resource with contextual information of sur-
rounding words as feature vectors (Section 4.2). A
similar approach was done with the ontology struc-
ture and vocabulary (Section 4.3).
4.1 Moses toolkit
To translate the English financial labels into Ger-
man, we used the statistical translation toolkit Moses
(Koehn et al, 2007), where the word alignments
were built with the GIZA++ toolkit (Och and Ney,
2003). The SRILM toolkit (Stolcke, 2002) was used
to build the 5-gram language model.
4.2 Building the contextual-semantic resource
from the parallel corpus Europarl
To enhance the baseline approach with additional se-
mantic information, we built a new resource of con-
textual information from Europarl.
From the original phrase table, which was gen-
erated from the Europarl corpus, we used the sub-
phrase table, which was generated to translate the
German GAAP financial ontology in the baseline
approach. Although this sub-phrase table holds only
segments necessary to translate the financial labels,
it still contains 2,394,513 phrase pairs. Due to the
4http://www.statmt.org/europarl/, version 7
scalability issue, we reduced the number of phrase
pairs by filtering the sub-phrase table based on the
following criteria:
a) the direct phrase translation probability ?(e|f)
has to be larger than 0.0001
b) a phrase pair should not start or end with a
functional word, i.e. prepositions, conjunctions,
modal verbs, pronouns
c) a phrase pair should not start with punctuation
After applying these criteria to the sub-phrase ta-
ble, the new filtered phrase table holds 53,283 enti-
ties, where phrase pairs, e.g. tax rate ||| Steuersatz
or tax liabilities ||| Steuerschulden were preserved.
In the next step, the phrase pairs stored in the fil-
tered phrase table were used to find sentences in Eu-
roparl, where these phrase pairs appear. The goal
was to extract the surrounding words as the con-
textual information of these phrase pairs. If a seg-
ment from the filtered phrase table appeared in the
sentence we extracted the lemmatised contextual in-
formation of the phrase pair, whereby we consid-
ered 10 tokens to the left and 10 to the right of
the analysed phrase pair. To address the problem
of different inflected forms (financial asset vs. fi-
nancial assets) of the same lexical entity (financial
asset) we lemmatised the English part of the Eu-
roparl corpus with TreeTagger(Schmid, 1995). Sim-
ilar to the phrase table filtering approach, an n-gram
should not start with a functional word or punctua-
tion. The extracted surrounding words were stored
together with its phrase pairs, i.e. for the phrase
pairs Equity-Gerechtigkeit and Equity-Eigenkapital
different contextual vectors were generated.
Example 1.a) illustrates a sentence, which holds
the source segment Equity from the filtered phrase
table. Example 1.b) represents its translation into
German. This example illustrates the context in
which Equity is translated into the German expres-
sion Gerechtigkeit. The segment Equity is also
present in the second sentence, (example 2.a)), in
contrast to the first one, equity is translated into
Eigenkapital, (2.b)), since the sentence reports fi-
nancial information.
1. a) ... which could guarantee a high standard of ef-
ficiency, safety and equity for employees and
users alike, right away.
42
b) ... , der heute ein hohes Niveau an Leistung,
Qualita?t, Sicherheit und Gerechtigkeit fu?r die
Bediensteten und die Nutzer garantieren ko?nnte.
2. a) ... or organisations from making any finance,
such as loans or equity, available to named
Burmese state-owned enterprises.
b) ... bzw. Organisationen zu verbieten, bir-
manischen staatlichen Unternehmen jegliche Fi-
nanzmittel wie Darlehen oder Eigenkapital zur
Verfu?gung zu stellen.
Applying this methodology on all 1.9 million sen-
tences in Europarl, we generated a resource with
feature vectors for all phrase pairs of the filtered
phrase table. Table 1 illustrates the contextual differ-
ences between the vectors for Equity-Gerechtigkeit
and Equity-Eigenkapital phrase pairs.
4.3 Contextual-semantic resource generation
for the financial ontology German GAAP
To compare the contextual information extracted
from Europarl a similar approach was applied to the
vocabulary in the German GAAP ontology.
First, to avoid unnecessary segments, e.g. provi-
sions for or losses from executory, we parsed the fi-
nancial ontology with the Stanford parser (Klein and
Manning, 2003) and extracted meaningful segments
from the ontology labels. This step was done pri-
marily to avoid comparing all possible n-gram seg-
ments with the filtered segments extracted from the
Europarl corpus (cf. Subsection 4.2). With the syn-
tactical information given by the Stanford parser we
extracted a set of noun segments for the ontology la-
bels, which we defined by the rules shown in Table
2.
# Syntactic Patterns
1 (NN(S) w+)
2 (NP (NN(S) w+)+))
3 (NP (JJ w+)+ (NN(S) w+)+))
4 (NP (NN(S) w+)+ (CC w+) (NN(S) w+)+)
5 (NP (NN(S) w+)+ (PP (IN/.. w+) (NP (NN(S) w+)+))
Table 2: Syntactic patterns for extracting noun segments
from the parsed financial ontology labels
Applying these patterns to the ontology label Pro-
visions for expected losses from executory contracts
extracts the following noun segments: provisions,
losses and contracts (pattern 1), expected losses and
executory contracts (pattern 3), provisions for ex-
pected losses and expected losses from executory
contracts (pattern 5).
In the next step, for all 2794 labels from the finan-
cial ontology, a unique contextual vector was gen-
erated as follows: for the label Equity-equivalent
partner loans (cf. Figure 1), the vector holds the
extracted (lemmatised) noun segments of the direct
parent, Equity, and all its siblings in the ontology,
e.g. Revenue reserves . . . (Table 3).
targeted label: Equity-equivalent partner loans
contextual information: capital (6), reserve (3), loss
(3), balance sheet (2) . . . currency translation (1),
negative consolidation difference (1), profit (1)
Table 3: Contextual information for the financial label
Equity-equivalent partner loans
4.4 Calculating the Semantic Similarity
Using the resources described in the previous sec-
tions in a final step we apply the Cosine, Jaccard and
Dice similarity measures on these feature vectors.
For the first evaluation step we translated all finan-
cial labels with the general translation model. Ta-
ble 4 illustrates the translation of the financial ex-
pression equity as part of the label Equity-equivalent
partner loans.5
With the n-best (n=50) translations for each fi-
nancial label we calculated the semantic similarity
between the contextual information of the phrase
pairs (equity-Eigenkapital) extracted from the par-
allel corpus (cf. Table 1) with the semantic informa-
tion of the financial label Equity extracted from the
ontology (cf. Table 3).
After calculating a semantic similarity, we reorder
the translations based on this additional information,
which can be seen in Table 5.
5ger. Gerechtigkeit-gleichwertige Partner Darlehen
Source label Target label p(e|f)
equity Gerechtigkeit -10.6227
equity Gleichheit -11.5476
equity Eigenkapital -12.7612
equity Gleichbehandlung -13.0936
equity Fairness -13.6301
Table 4: Top five translations and its translation probabil-
ities generated by the Europarl translation model
43
Source label Target label Context (frequency)
equity Gerechtigkeit social (19), efficiency (18), efficiency and equity (14), justice (13), social eq-
uity (11), education (9), principle (8), transparency (7), training (7), great (7)
equity Eigenkapital capital (19), equity capital (15), venture (3), venture capital (3), rule (2), capital
and risk (2), equity capital and risk (2), bank (2), risk (2), debt (1)
Table 1: Contextual information for Equity with its target labels Gerechtigkeit and Eigenkapital extracted from the
Europarl corpus
Source label Target label Jaccard
equity Eigenkapital 0.0780169232
equity Equity 0.0358268041
equity Kapitalbeteiligung 0.0341965597
equity Gleichheit 0.0273327211
equity Gerechtigkeit 0.0266209669
Table 5: Top five re-ranked translations after calculating
the Jaccard similarity
5 Evaluation
Our evaluation was conducted on the translations
generated by the baseline approach, using only Eu-
roparl, and the ontology-enhanced translations of fi-
nancial labels.
We undertook an automatic evaluation using the
BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), TER (Snover et al, 2006), and Me-
teor6 (Denkowski and Lavie, 2011) algorithms.
5.1 Baseline Evaluation of general corpus
At the beginning of our experiment, we trans-
lated the financial labels with the Moses Toolkit,
where the translation model was generated from the
English-German Europarl aligned corpus. The re-
sults are shown in Table 7 as baseline.
5.2 Baseline Evaluation of filtered general
corpus
A second evaluation on translations was done on
a filtered Europarl corpus, depending if a sentence
holds the vocabulary of the ontology to be translated.
We generated five training sets, based on n-grams of
the ontology vocabulary (from unigram to 5-gram)
appearing in the sentence. From the set of aligned
sentences we generated new translation models and
translated again the financial ontology labels with
them. Table 6 illustrates the results of filtering the
6Meteor configuration: -l de, exact, stem, paraphrase
Europarl parallel corpus into smaller (n-gram) train-
ing sets, whereby no training set outperforms signif-
icantly the baseline approach.
model sentences BLEU-4 Meteor OOV
baseline 1920209 4.22 0.1138 37
unigram 1591520 4.25 0.1144 37
bigram 322607 4.22 0.1077 46
3-gram 76775 1.99 0.0932 92
4-gram 4380 2.45 0.0825 296
5-gram 259 0.69 0.0460 743
Table 6: Evaluation results for the filtered Europarl base-
line translation model (OOV - out of vocabulary)
5.3 Evaluation of the knowledge enhanced
general translation model
The final part of our research concentrated on trans-
lations where the general translation model was en-
hanced with ontological knowledge. Table 7 illus-
trates the results using the different similarity mea-
sures, i.e. Dice, Jaccard, Cosine similarity coeffi-
cient.
For the Cosine coefficient we performed two ap-
proaches. For the first step we used only binary val-
ues (bv) from the vector, where in the second ap-
proach we used the frequencies of the contextual in-
formation as real values (rv). The results show that
the Cosine measure using frequencies (rv) performs
best for the METEOR metric. On the other hand the
binary Cosine measure (bv) performs better than the
other metrics in BLEU-2 and NIST metrics.
The Jaccard and Dice similarity coefficient per-
form very similar. They both outperform the general
translation model in BLEU, NIST and TER metrics,
whereby the Jaccard coefficient performs slightly
better than the Dice coefficient. On the other hand
both measures perform worse on the METEOR met-
ric regarding the general model. Overall we observe
that the Jaccard coefficient outperforms the baseline
44
Bleu-2 Bleu-4 NIST Meteor TER
baseline 13.05 4.22 1.789 0.113 1.113
Dice 13.16 4.43 1.800 0.111 1.075
Jaccard 13.17 4.44 1.802 0.111 1.074
Cosine (rv) 12.91 4.20 1.783 0.117 1.108
Cosine (bv) 13.27 4.34 1.825 0.116 1.077
Table 7: Evaluation results for Europarl baseline transla-
tion model and the different similarity measures
approach by 0.22 BLEU points.
5.4 Comparison of translations provided by
the general model and Jaccard similarity
Table 7 illustrates the different approaches that were
performed in our research. As the automatic metrics
give just a slight intuition about the improvements of
the different approaches, we compared the transla-
tions of the general translation model manually with
the translations on which Jaccard similarity coeffi-
cient was performed.
As discussed, Equity can be translated into Ger-
man as Gerechtigkeit when translating it in a gen-
eral domain or into Eigenkapital when translat-
ing it in the financial domain. In the financial
ontology, the segment Equity appears 126 times.
The general translation model translates it wrongly
as Gerechtigkeit, whereby the Jaccard coefficient,
with the help of contextual information, favours
the preferred translation Eigenkapital. Further-
more Equit can be also part of a larger financial
label, e.g. Equity-equivalent partner loans, but
the general translation model still translates it into
Gerechtigkeit. This can be explained by the seg-
mentation during the decoding process, i.e. the SMT
system tokenises this label into separate tokens and
translates each token separately from each other. On
the contrary, the Jaccard similarity coefficient cor-
rects the unigram segment to Eigenkapital.
As part of the label Uncalled unpaid contributions
to subscribed capital (deducted from equity on the
face of the balance sheet), equity is again translated
by the general translation model as Gerechtigkeit. In
this case the Jaccard coefficient cannot correct the
translation, which is caused by the general model
itself, since in all n-best (n=50) translations equity is
translated as Gerechtigkeit. In this case the Jaccard
coefficient reordering does not have any affect.
The manual analysis further showed that the am-
biguous ontology label Securities, e.g. in Write-
downs of long-term financial assets and securities
was also often translated as Sicherheiten7 in the
meaning of certainties or safeties, but was corrected
by the Jaccard coefficient into Wertpapiere, which is
the correct translation in the financial domain.
Finally, the analysis showed that the segment Bal-
ance in Central bank balances was often trans-
lated by the baseline model into Gleichgewichte,8
i.e. Zentralbank Gleichgewichte, whereas the Jac-
card coefficient favoured the preferred translation
Guthaben, i.e. Zentralbank Bankguthaben.
Conclusion and Future Work
Our approach to re-using existing resources showed
slight improvements in the translation quality of the
financial vocabulary. Although the contextual infor-
mation favoured correct translations in the financial
domain, we see a need for more research on the con-
textual information stored in the parallel corpus and
also in the ontology. Also more work has to be done
on analysis of the overlap of the contextual informa-
tion and the ontology vocabulary, e.g. which con-
textual words should have more weight for the simi-
larity measure. Furthermore, dealing with the ontol-
ogy structure, the relations between the labels, i.e.
part-of and parent-child relations, have to be consid-
ered. Once these questions are answered, the next
step will be to compare the classical cosine mea-
sure against more sophisticated similarity measures,
i.e. Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). Instead of measuring simi-
larity between the vectors directly using cosine, we
will investigate the application of ESA to calculate
the similarities between short texts by taking their
linguistic variations into account (Aggarwal et al,
2012).
Acknowledgments
This work is supported in part by the European
Union under Grant No. 248458 for the Monnet
project and Grant No. 296277 for the EuroSenti-
ment project as well as by the Science Foundation
Ireland under Grant No. SFI/08/CE/I1380 (Lion-2).
7ger. Abschreibungen der langfristigen finanziellen
Vermo?genswerte und Sicherheiten
8en. equilibrium, equation, balance
45
References
Aggarwal, N., Asooja, K., and Buitelaar, P. (2012).
DERI&UPM: Pushing corpus based relatedness to
similarity: Shared task system description. In
SemEval-2012.
Apidianaki, M., Wisniewski, G., Sokolov, A., Max, A.,
and Yvon, F. (2012). Wsd for n-best reranking and
local language modeling in smt. In Proceedings of the
Sixth Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 1?9, Jeju, Republic of
Korea. Association for Computational Linguistics.
Carpuat, M. and Wu, D. (2005). Word sense disambigua-
tion vs. statistical machine translation. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ?05, pages 387?394,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Declerck, T., Krieger, H.-U., Thomas, S. M., Buitelaar,
P., O?Riain, S., Wunner, T., Maguet, G., McCrae, J.,
Spohr, D., and Montiel-Ponsoda, E. (2010). Ontology-
based multilingual access to financial reports for shar-
ing business knowledge across europe. In Internal
Financial Control Assessment Applying Multilingual
Ontology Framework.
Denkowski, M. and Lavie, A. (2011). Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalua-
tion of Machine Translation Systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 85?91, Edinburgh, Scotland. Association
for Computational Linguistics.
Doddington, G. (2002). Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145.
Gabrilovich, E. and Markovitch, S. (2007). Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of The Twentieth In-
ternational Joint Conference for Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
Klein, D. and Manning, C. D. (2003). Accurate unlex-
icalized parsing. In Proceeding of the 41st annual
meeting of the association for computational linguis-
tics, pages 423?430.
Koehn, P. (2005). Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79?86. AAMT.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,
C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and
Herbst, E. (2007). Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL, ACL ?07, pages 177?
180.
Mauser, A., Hasan, S., and Ney, H. (2009). Extend-
ing statistical machine translation with discriminative
and trigger-based lexicon models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217, Singapore.
McCrae, J., Espinoza, M., Montiel-Ponsoda, E., Aguado-
de Cea, G., and Cimiano, P. (2011). Combining sta-
tistical and semantic approaches to the translation of
ontologies and taxonomies. In Fifth workshop on Syn-
tax, Structure and Semantics in Statistical Translation
(SSST-5).
Och, F. J. and Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Computa-
tional Linguistics, 29.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: A Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, ACL ?02, pages 311?318, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Patry, A. and Langlais, P. (2011). Going beyond word
cooccurrences in global lexical selection for statisti-
cal machine translation using a multilayer perceptron.
In 5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP?11), pages 658?666, Chi-
ang Mai, Thailand.
Schmid, H. (1995). Improvements in part-of-speech tag-
ging with an application to german. In In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In In Proceedings of
Association for Machine Translation in the Americas,
pages 223?231.
Stolcke, A. (2002). Srilm-an extensible language model-
ing toolkit. In Proceedings International Conference
on Spoken Language Processing, pages 257?286.
Wu, H., Wang, H., and Zong, C. (2008). Domain adap-
tation for statistical machine translation with domain
dictionary and monolingual corpora. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08, pages
993?1000.
46
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86?94,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Domain-specific and Collaborative Resources for Term Translation
Mihael Arcan, Paul Buitelaar
Unit for Natural Language Processing
Digital Enterprise Research Institute
Galway, Ireland
firstname.lastname@deri.org
Christian Federmann
Language Technology Lab
German Research Center for AI
Saarbru?cken, Germany
cfedermann@dfki.de
Abstract
In this article we investigate the translation
of terms from English into German and vice
versa in the isolation of an ontology vocab-
ulary. For this study we built new domain-
specific resources from the translation search
engine Linguee and from the online encyclo-
pedia Wikipedia. We learned that a domain-
specific resource produces better results than
a bigger, but more general one. The first find-
ing of our research is that the vocabulary and
the structure of the parallel corpus are impor-
tant. By integrating the multilingual knowl-
edge base Wikipedia, we further improved the
translation wrt. the domain-specific resources,
whereby some translation evaluation metrics
outperformed the results of Google Translate.
This finding leads us to the conclusion that
a hybrid translation system, a combination of
bilingual terminological resources and statis-
tical machine translation can help to improve
translation of domain-specific terms.
1 Introduction
Our research on translation of ontology vocabularies
is motivated by the challenge of translating domain-
specific terms with restricted or no additional textual
context that in other cases can be used for transla-
tion improvement. For our experiment we started
by translating financial terms with baseline systems
trained on the EuroParl (Koehn, 2005) corpus and
the JRC-Acquis (Steinberger et al, 2006) corpus.
Although both resources contain a large amount of
parallel data, the translations were not satisfying. To
improve the translations of the financial ontology
vocabulary we built a new parallel resource, which
was generated using Linguee1, an online translation
query service. With this data, we could train a small
system, which produced better translations than the
baseline model using only general resources.
Since the manual development of terminological
resources is a time intensive and expensive task, we
used Wikipedia as a background knowledge base
and examined articles, tagged with domain-specific
categories. With this extracted domain-specific data
we built a specialised English-German lexicon to
store translations of domain-specific terms. These
terms were then used in a pre-processing method in
the decoding approach. This approach incorporates
the work by Aggarwal et al (2011), which suggests
a sub-term analysis. We split the financial terms
into n-grams and search for financial sub-terms in
Wikipedia.
The remainder of the paper is organised like this.
In Section 2 we describe related work while in Sec-
tion 3 the ontology data, the training data that we
used in training the language model, and the trans-
lation decoder are discussed. Section 4 presents the
new resources which were used for improving the
term translation. In Section 5 we discuss the results
of exploiting the different resources. We conclude
with a summary and give an outlook on future work
in Section 6.
2 Related Work
Kerremans (2010) presents the issue of terminologi-
cal variation in the context of specialised translation
on a parallel corpus of biodiversity texts. He shows
that a term often cannot be aligned to any term in
1See www.linguee.com
86
the target language. As a result, he proposes that
specialised translation dictionaries should store dif-
ferent translation possibilities or term variants.
Weller et al (2011) describe methods for termi-
nology extraction and bilingual term alignment from
comparable corpora. In their compound translation
task, they are using a dictionary to avoid out-of-
domain translation.
Zesch et al (2008) address issues in accessing
the largest collaborative resources: Wikipedia and
Wiktionary. They describe several modules and
APIs for converting a Wikipedia XML Dump into a
more suitable format. Instead of parsing the large
Wikipedia XML Dump, they suggest to store the
Dump into a database, which significantly increases
the performance in retrieval time of queries.
Wikipedia has not only a dense link structure be-
tween articles, it has also inter-language links be-
tween articles in different languages, which was the
main reason to use this invaluable collaborative re-
source. Erdmann et al (2008) regarded the titles of
Wikipedia articles as terminology. They assumed
that two articles connected by an Interlanguage link
are likely to have the same content and thus an
equivalent title.
Vivaldi and Rodriguez (2010) proposed a method-
ology for term extraction in the biomedical domain
with the help of Wikipedia. As a starting point, they
manually select a set of seed words for a domain,
which is used to find corresponding nodes in this re-
source. For cleaning their collected data, they use
thresholds to avoid storing undesirable categories.
Mu?ller and Gurevych (2008) use Wikipedia and
Wiktionary as knowledge bases to integrate seman-
tic knowledge into Information retrieval. Their
models, text semantic relatedness (for Wikipedia)
and word semantic relatedness (for Wiktionary),
are compared to a statistical model implemented in
Lucene. In their approach to Bilingual Retrieval,
they use the cross-language links in Wikipedia,
which improved the retrieval performance in their
experiment, especially when the machine translation
system generated incorrect translations.
3 Experiments
Our experiment started with an analysis of the terms
in the ontology to be translated, which was stored
in RDF2 data model. These terms were used to
automatically extract any corresponding Wikipedia
Categories, which helped us to define more exactly
the domain(s) of the ontology to be translated. The
collected Categories were further used to build a
domain-specific lexicon to be used for improving
term translation. At the same time a new parallel
corpus was built, which was also generated with the
help of the ontology terms. This new data was then
used to pre-process the input data for the decoder
and to build a specialised training model which
yielded to a translation improvement.
In this section, several types of data will be
presented and furthermore the translation decoder,
which has to access this data to build the training
models. Section 3.1 gives an overview of the data
that was used in translation. In Sections 3.2 and
3.3 we describe the data that is used to train the
translation and language model. We used differ-
ent parallel corpora, JRC-Acquis, EuroParl and a
domain-specific corpus built from Linguee. In Sec-
tion 3.4, we discuss a domain-specific lexicon, ex-
tracted from Wikipedia. In the last Section 3.5 we
describe the phrase-based machine translation de-
coder Moses that we used for our experiments.
3.1 xEBR Dataset
For the translation dataset a financial ontology de-
veloped by the XBRL European Business Registers3
(xEBR) Working Group was used. This financial
ontology is a framework for describing financial ac-
counting and profile information of business entities
across Europe, see also Declerck et al (2010). The
ontology holds 263 concepts and is partially trans-
lated into German, Dutch, Spanish, French and Ital-
ian. The terms in each language are aligned via
the SKOS4 Exact Match mechanism to the xEBR
core taxonomy. In this partially translated taxon-
omy, we identified 63 English financial terms and
their German equivalents, which were used as refer-
ence translations in evaluating the different experi-
ment steps.
The xEBR financial terms are not really terms
from a linguistic point of view, but they are used
in financial or accounting reports as unique finan-
2RDF: Resource Description Framework
3XBRL: eXtensible Business Reporting Language
4SKOS: Simple Knowledge Organization System
87
Length Count Examples
11 1 Taxes Remuneration And Social Security
Payable After More Than One Year
10 2 Amounts Owed To Credit Institutions After
More Than One Year, Variation In Stocks Of
Finished Goods And Work In Progress
. . .
2 57 Net Turnover, Liquid Assets, . . .
1 10 Assets, Capital, Equity, . . .
Table 1: Examples of xEBR terms
cial expressions or tags to organize and retrieve au-
tomatically reported information. Therefore it is im-
portant to translate these financial terms exactly.
Table 1 illustrates the structure of xEBR terms.
It is obvious that they are not comparable to gen-
eral language, but instead are more like headlines in
newspapers, which are often short, very informative
and written in a telegraphic style. xEBR terms are
often only noun phrases without determiners. The
length of the financial terms varies, e.g. the longest
financial term considered for translation has a length
of 11 tokens, while others may consist of 1 or 2.
3.2 General Resources: EuroParl and
JRC-Acquis
As a baseline, the largest available parallel corpora
were used: EuroParl and the JRC-Acquis parallel
corpus. The EuroParl parallel corpus holds the pro-
ceedings of the European Parliament in 11 European
languages. The JRC-Acquis corpus is available in
almost all EU official languages (except Irish) and is
a collection of legislative texts written between 1950
and today.
Although research work proved, that a training
model built by using a general resource cannot be
used to translate domain-specific terms (Wu et al,
2008), we decided to train a baseline model on these
resources to illustrate any improvement steps from a
general resource to specialised domain resources.
3.3 Domain Resource: Linguee
Linguee is a combination of a dictionary and a
search engine, which indexes around 100 Million
bilingual texts on words and expressions. Linguee
search results show example sentences that depict
how the searched expression has been translated in
context.
In contrast to translation engines like Google
Translate and Bing Translator, which give you the
most probable translation of a source text, every en-
try in the Linguee database has been translated by
humans. The bilingual dataset was gathered from
the web, particularly from multilingual websites
of companies, organisations or universities. Other
sources include EU documents and patent specifica-
tions.
The language pairs available for query-
ing are English?German, English?Spanish,
English?French and English?Portuguese.
Since Linguee includes EU documents, they also
use parallel sentences from EuroParl and JRC-
Acquis. We investigated the proportion of sentences
returned by Linguee which are contained in Eu-
roParl or JRC-Acquis. The outcome is that the num-
ber of sentences is very low, where 131 sentences
(0.54%) are gathered from JRC-Acquis corpus and
466 (1.92%) from EuroParl.
3.4 Collaborative Resource: Wikipedia
Wikipedia is a multilingual, freely available ency-
clopedia that was built by a collaborative effort of
voluntary contributors. All combined Wikipedias
hold approximately 20 million articles or more than
8 billion words in more than 280 languages. With
these facts it is the largest collection of freely avail-
able knowledge5.
With the heavily interlinked information base,
Wikipedia forms a rich lexical and semantic re-
source. Besides a large amount of articles, it
also holds a hierarchy of Categories that Wikipedia
Articles are tagged with. It includes knowledge
about named entities, domain-specific terms and
word senses. Furthermore, the redirect system of
Wikipedia articles can be used as a dictionary for
synonyms, spelling variations and abbreviations.
3.5 Translation System: Moses
For generating translations from English into Ger-
man and vice versa, the statistical translation toolkit
Moses (Koehn et al, 2007) was used to build the
training model and for decoding. For this approach,
a phrase-based approach was taken instead of a tree
based model. Further, we aimed at improving the
translations only on the surface level, and therefore
no part-of-speech information was taken into ac-
count. Word and phrase alignments were built with
5
http://en.wikipedia.org/wiki/Wikipedia:Size_comparison
88
the GIZA++ toolkit (Och and Ney, 2003), whereby
the 5-gram language model was built by SRILM
(Stolcke, 2002).
4 Domain-specific Resource Generation
In this section, two different types of data and the
approach of building them will be presented. Sec-
tion 4.1 gives an overview of generating a paral-
lel resource from Linguee, which was used in gen-
erating a new domain-specific training model. In
Section 4.2 a detailed description is given how we
extracted terms from Wikipedia for generating a
domain-specific lexicon.
4.1 Domain-specific parallel corpus generation
To build a new training model that is specialised on
our xEBR ontology, we used the Linguee search en-
gine. This resource can be queried on single words
and on word expressions with or without quotation
marks. We stored the HTML output of the Linguee
queries on our financial terms and parsed these files
to extract plain parallel text. From this, we built a fi-
nancial parallel corpus with 13,289 translation pairs,
including single words, multi-word expressions and
sentences. The English part of the parallel resource
contained 410,649 tokens, the German part 347,246.
4.2 Domain-specific lexicon generation
To improve translation based on the domain-specific
parallel corpus, we built a cross-lingual terminolog-
ical lexicon extracted from Wikipedia. From the
Wikipedia Articles we used different information
units, i.e. the Title of a Wikipedia Article, the Cat-
egory (or Categories) of the Title and the internal
Interwiki
Interlanguage links of the Title. The concept of
Interwiki links can be used to make links to other
Wikipedia Articles in the same language or to an-
other Wikipedia language i.e. Interlanguage links.
In our first approach, we used Wikipedia to de-
termine the domain (or several domains) of the on-
tology. This approach (a) is to understand as the
identification of the domain through the vocabulary
of the ontology. For this approach, the financial
terms, which were extracted from the ontology, were
used to query the Wikipedia knowledge base6. The
6For the Wikipedia Query we used the Wikipedia XML
Collected Wikipedia Categories
Frequency Name
8 Generally Accepted Accounting Principles
4 Debt
4 Accounting terminology
. . .
1 Political science terms
1 Physical punishments
Table 2: Collected Wikipedia Categories based on the ex-
tracted financial terms
Wikipedia Article was considered for further exami-
nation, if its Title is equivalent to our financial terms.
In this first step, 7 terms of our ontology were iden-
tified in the Wikipedia knowledge base. With this
step, we collected the Categories of these Titles,
which was the main goal of this approach. In a sec-
ond round, we split all financial terms into all pos-
sible n-grams and repeated the query again to find
additional Categories based on the split n-grams. Ta-
ble 2 shows the collected Categories of the first ap-
proach and how often they appeared in respect to the
extracted financial terms.
After storing all Categories, only such Categories
were considered, which frequency had a value more
than the calculated arithmetic mean of all frequen-
cies (> 3.15). For the calculation of the arithmetic
mean only Categories were considered, which had
a frequency more than 1, since 2,262 of 3,615 col-
lected Categories (62.6%) had a frequency equals 1.
With this threshold we avoided extraction of a vo-
cabulary that is not related to the ontology. Without
this threshold, out-of-domain Categories would be
stored, which would extend the lexicon with vocab-
ulary that would not benefit the ontology translation,
e.g. Physical punishments, which was access by the
financial term Stocks.
In the next step, we further extended the list of
Categories collected previously by use of full and
split terms. This was done by storing new Categories
based on the Wikipedia Interwiki links of each Arti-
cle which was tagged with a Category from Table 2.
For example, we collected all Categories wherewith
the Article Balance sheet7 is tagged and the Cate-
gories of the 106 Interwiki links of the Article Bal-
ance sheet. The frequencies of these Categories
were summed up for all Interwiki links. Finally a
dump; enwiki-20120104-pages-articles.xml
7Financial statements, Accounting terminology
89
Final Category List
Frequency Name
95 Economics terminology
62 Generally Accepted Accounting Principles
61 Macroeconomics
55 Accounting terminology
47 Finance
44 Economic theories
. . .
Table 3: Most frequent Categories based on the xEBR
terms and their Interwiki links
new Category was added to the final Category list, if
the new Category frequency exceeds the arithmetic
mean threshold (> 18.40).
The final Category list contained 33 financial
Wikipedia Categories (Table 3), which was in the
next step used for financial term extraction.
With the final list of Categories, we started an
investigation of all Wikipedia articles tagged with
these financial Categories. Each Wikipedia Title
was considered as a useful domain-specific term
and was stored in our lexicon if a German title in
the Wikipedia knowledge base also existed. As
an example, we examined the Category Account-
ing terminology and stored the English Wikipedia
Title Balance sheet with the German equivalent
Wikipedia Title Bilanz.
At the end of the lexicon generation we examined
5228 Wikipedia Articles, which were tagged with
one or more financial Categories. From this set of
Articles we were able to generate a terminological
lexicon with 3228 English-German entities.
5 Evaluation
Tables 4 to 5 illustrate the final results for our exper-
iments on translating xEBR ontology terms, using
the NIST (Doddington, 2002), BLEU (Papineni et
al., 2002), and Meteor (Lavie and Agarwal, 2005)
algorithms. To further study any translation im-
provements of our experiment, we also used Google
Translate8 in translating 63 financial xEBR terms
(cf. Section 3.1) from English into German and from
German into English.
5.1 Interpretation of Evaluation Metrics
In our experiments translation models built from
a general resource performed worst. These re-
8Translations were generated on February 2012.
Scoring Metric
Source # correct BLEU NIST Meteor
Google Translate 18 0.264 4.382 0.369
JRC-Acquis 12 0.167 3.598 0.323
EuroParl 4 0.113 2.630 0.326
Linguee 25 0.347 4.567 0.408
Lexical substitution 4 0.006 0.223 0.233
Linguee+Wiki 25 0.324 4.744 0.432
Table 4: Evaluation scores for German term translations
Scoring Metric
Source # correct BLEU NIST Meteor
Google Translate 21 0.452 4.830 0.641
JRC-Acquis 9 0.127 2.458 0.480
EuroParl 5 0.021 1.307 0.412
Linguee 15 0.364 3.938 0.631
Lexical substitution 4 0.006 0.243 0.260
Linguee+Wiki 22 0.348 3.993 0.644
Table 5: Evaluation scores for English term translations
sults show that building resources from general lan-
guage does not improve the translation of terms.
The Linguee financial corpus, which is built from
13,289 sentences and holds 304K English and Ger-
man 250K words, however demonstrates the ben-
efit of domain-specific resources. Its size is less
than two percent of that of the JRC-Acquis cor-
pus (1,131,922 sentences, 21M English words, 19M
German words), but evaluation scores are more than
double than those for JRC-Acquis. This is clear evi-
dence that such a resource benefits the translation of
terms in a specific domain.
The models produced by the Linguee search en-
gine are generating better translations than those
produced by general resources. This approach out-
performs Google Translate translations from Ger-
man into English for all used evaluation metrics.
The table further shows results for our approach
in using extracted Wikipedia terms as an example-
based approach. For this we used the terms extracted
from Wikipedia and exchanged English terms with
German translations and vice versa. The evaluation
metrics are very low in this case; only for Correct
Translation we generate four positive findings.
Finally, the table gives results for our approach
in using a combination of domain-specific paral-
lel financial corpus with the lexicon extracted from
Wikipedia. The domain-specific lexicon contains
3228 English-German translations, which were ex-
tracted from 18 different financial Categories. This
90
combination of highly specialised resources gives
the best results in our experiment. Translating fi-
nancial terms into German, we get more Correct
Translations as well as the Meteor metric shows
better results compared to Google Translate. For
translations into English, all used evaluation metrics
show better results than those of Google Translate.
As a final observation, we learned that translations
made by domain-specific resources are on the same
quality level, either if we translate from English
into German or vice versa. In comparison, we see
that Google Translate has a larger discrepancy when
translating into German or English respectively. Our
research showed that translations from English into
German built by specialised resources were slightly
better, which goes along with Google Translate that
also produces better translations into German.
5.2 Manual Evaluation of Translation Quality
In addition to the automatic evaluation with BLEU,
NIST, and Meteor scores, we have also undertaken
a manual evaluation campaign to assess the transla-
tion quality of the different systems. In this section,
we will a) describe the annotation setup and task
presented to the human annotators, b) report on the
translation quality achieved by the different systems,
and c) present inter-annotator agreement scores that
allow to judge the reliability of the human rankings.
5.2.1 Annotation Setup
In order to manually assess the translation quality
of the different systems under investigation, we de-
signed a simple classification scheme consisting of
three distinct classes:
1. Acceptable (A): terms classified as acceptable
are either fully identical to the reference term
or semantically equivalent;
2. Can easily be fixed (C): terms in this class
require some minor correction (such as fixing
of typos, removal of punctuation, etc.) but are
nearly acceptable. The general semantics of
the reference term are correctly conveyed to
the reader.
3. None of both (N): the translation of the term
does not match the intended semantics or it is
plain wrong. Items in this class are considered
severe errors which cannot easily be fixed and
hence should be avoided wherever possible.
Classes
System A C N
Linguee+Wiki 58% 27% 15%
Google Translate 55% 31% 14%
Linguee 51% 37% 12%
JRC-Acquis 32% 28% 40%
EuroParl 5% 25% 70%
Table 6: Results from the manual evaluation into German
Classes
System A C N
Linguee+Wiki 56% 32% 12%
Linguee 56% 31% 13%
Google Translate 39% 40% 21%
JRC-Acquis 39% 31% 30%
EuroParl 15% 30% 55%
Table 7: Results from the manual evaluation into English
5.2.2 Annotation Data
We setup ten evaluation tasks, five for transla-
tions into English, five for translations into German.
Each of these sets was comprised of 63 term transla-
tions and the corresponding reference. Every set was
given to at least three human annotators who then
classified the observed translation output according
to the classification scheme described above. The
human annotators included both domain experts and
lay users without knowledge of the terms domain.
In total, we collected 2,520 classification items
from six annotators. Tables 6, 7 show the results
from the manual evaluation for term translations into
German and English, respectively. We report the
distribution of classes per evaluation task which are
displayed in best-to-worst order.
In order to better be able to interpret these rank-
ings, we computed the inter-annotator agreement be-
tween human annotators. We report scores gener-
ated with the following agreement metrics:
? S (Bennet et al, 1954);
? pi (averaged across annotators) (Scott, 1955);
? ? (Fleiss and others, 1971);
? ? (Krippendorff, 1980).
Tables 8, 9 present the aforementioned metrics
scores for German and English term translations.
Overall, we achieve an average ? score of 0.463,
which can be interpreted as moderate agreement fol-
lowing (Landis and Koch, 1977). Notably, we also
reach substantial agreement for one of the anno-
tation tasks with a ? score of 0.657. Given the
91
Agreement Metric
System S pi ? ?
Linguee+Wiki 0.599 0.528 0.533 0.530
Google Translate 0.698 0.655 0.657 0.657
Linguee 0.484 0.416 0.437 0.419
JRC-Acquis 0.412 0.406 0.413 0.408
EuroParl 0.515 0.270 0.269 0.273
Table 8: Annotator agreement scores for German
Agreement Metric
System S pi ? ?
Linguee+Wiki 0.532 0.452 0.457 0.454
Linguee 0.599 0.537 0.540 0.539
Google Translate 0.480 0.460 0.465 0.463
JRC-Acquis 0.363 0.359 0.366 0.360
EuroParl 0.552 0.493 0.499 0.495
Table 9: Annotator agreement scores for English
observed inter-annotator agreement, we expect the
reported ranking results to be meaningful. Our
Linguee+Wiki system performs best for both trans-
lation directions while out-of-domain systems such
as JRC-Acquis and EuroParl perform badly.
5.3 Manual error analysis
Table 10 provides a manual analysis of the provided
translations from Google Translate and the com-
bined Linguee and Wikipedia Lexicon approach.
Example Ex. 1 shows the results for [Other intan-
gible] fixed assets. Since both translating systems
translate it the same, namely Vermo?genswerte, they
could be considered as term variants.
A similar example is [Receivables and other] as-
sets in Ex. 4. Google Translate translates the
segment asset into Vermo?gensgegensta?nde, whereby
the domain-specific approach translates it into
Vermo?genswerte. These examples prove the re-
search by Kerremans (2010) that one term does not
necessarily have only one translation on the target
side. As term variants can further be considered
Aufwendungen and Kosten, which were translated
from Costs [of old age pensions] (Ex. 5).
In contrast, the German term in [sonstige be-
triebliche] Aufwendungen (Ex. 8) is according to the
xEBR translated into [Other operating] expenses,
which was translated correctly by both systems.
A deeper terminological analysis has to be done
in the translation of the English term [Cost of] old
age pensions (Ex. 5). In general it can be translated
into Altersversorgung (provided by Google Trans-
late and xEBR) or Altersrente (generated by the
domain-specific model). Doing a compound anal-
ysis, the translation of [Alters]versorgung is supply
or maintenance. On the other side, the translation of
[Alters]rente is pension, which has a stronger con-
nection to the financial term in this domain.
Ex. 6 shows an improvement of domain spe-
cific translation model in comparison to a general
resource. Both general resources translated Securi-
ties as Sicherheiten, which is correct but not in the fi-
nancial domain. The domain-specific trained model
translates the ambiguous term correctly, namely
Wertpapiere. Google Translate generates the same
term as on the source site, Securities. Further, the
term Equity (Ex. 7) is translated by Google Translate
as Gerechtigkeit, the domain-specific model trans-
lates it as Eigenkapital, which is the correct trans-
lation. Finally, Ex. 2 and Ex. 3 open the issue of
accurateness of the references for translation evalu-
ation. The translations of these terms are correct if
we consider the source language. On the other hand,
if we compare them with the proposed references,
they are not the same. In Ex. 2 they are truncated
or extended in Ex. 3, which opens up problems in
translation evaluation.
5.4 Discussion
Our approach shows the differences between im-
proving translations with different resources. It was
shown to be necessary to use additional language
resources, i.e. specialised parallel corpora and if
available, specialised lexica with appropriate trans-
lations. Nevertheless, to move further in this direc-
tion, translation of specific terms, more research is
required in several areas that we identified in our ex-
periment. One is the quality of the translation model.
Because the translation model can only translate
terms that are in the training model, it is necessary
to use a domain-specific resource. Although we got
better results with a smaller resource (if we translate
into English), comparing those results with Google
Translate, we learned that more effort has to be done
in the direction of extending the size and quality of
domain-specific resources.
Apart from that, with the aid of Wikipedia, which
can be easily adapted for other language pairs, we
further improved the translations into English to a
92
Term Translations
# Source Reference Google Domain-specific
1 Other intangible sonstige immaterielle Sonstige immaterielle Sonstige immaterielle
fixed assets Vermo?gensgegensta?nde Vermo?genswerte Vermo?genswerte
2 Long-term Finanzanlagen Langfristige finanzielle Langfristige finanzielle
financial assets Vermo?genswerte Vermo?genswerte
3 Financial result Finanz- und Finanzergebnis Finanzergebnis
Beteiligungsergebnis
4 Receivables and Forderungen und sonstige Forderungen und sonstige Forderungen und sonstige
other assets Vermo?gensgegensta?nde Vermo?gensgegensta?nde Vermo?genswerte
5 Cost of old age Aufwendungen fu?r Aufwendungen fu?r Kosten der Altersrenten
pensions Altersversorgung Altersversorgung
6 Securities Wertpapiere Securities Wertpapiere
7 Equity Eigenkapital Gerechtigkeit Eigenkapital
8 sonstige betriebliche Other operating expenses other operating expenses other operating expenses
Aufwendungen (TC)
Table 10: Translations provided by Google Translate and by the domain-specific resource
point where we outperform translations provided
by Google Translate. Nevertheless, our experiment
showed that the translations into German were bet-
ter in regard of Google translate only for the Meteor
evaluation system, for BLEU and NIST we did not
achieve significant improvements. Also here more
work has to be done in domain adaptation in a more
sophisticated way to avoid building out-of-domain
vocabulary.
6 Conclusion
The approach of building new resources showed a
large impact on the translation quality. Therefore,
generating specialised resources for different do-
mains will be the focus of our future work. On
the one hand, building appropriate training models
is important, but our experiment also highlighted
the importance of additional collaborative resources,
like Wikipedia, Wiktionary, and DBpedia. Besides
extracting Wikipedia Articles with their multilin-
gual equivalents, as shown in Section 4.2, Wikipedia
holds much more information in the articles itself.
Therefore exploiting non-parallel resources, shown
by Fis?er et al (2011), would clearly help the trans-
lation system to improve performance. Future work
needs to better include the redirect system, which
would allow a better understanding of synonymy
and spelling variety of terms.
Focusing on translating ontologies, we will try
to better exploit the structure of the ontology itself.
Therefore, more work has to be done in the combi-
nation of linguistic and semantic information (struc-
ture of an ontology) as demonstrated by Aggarwal et
al. (2011), which showed first experiments in com-
bining semantic, terminological and linguistic infor-
mation. They suggest that a deeper semantic analy-
sis of terms, i.e. understanding the relations between
terms and analysing sub-terms needs to be consid-
ered. Another source of useful information may be
found in using existing translations for improving
the translation of other related terms in the ontology.
Acknowledgments
This work has been funded under the Seventh
Framework Programme for Research and Techno-
logical Development of the European Commission
through the T4ME contract (grant agreement no.:
249119) and in part by the European Union under
Grant No. 248458 for the Monnet project as well as
by the Science Foundation Ireland under Grant No.
SFI/08/CE/I1380 (Lion-2). The authors would like
to thank Susan-Marie Thomas, Tobias Wunner, Ni-
tish Aggarwal and Derek De Brandt for their help
with the manual evaluation. We are grateful to the
anonymous reviewers for their valuable feedback.
References
Nitish Aggarwal, Tobias Wunner, Mihael Arcan, Paul
Buitelaar, and Sea?n O?Riain. 2011. A similarity mea-
sure based on semantic, terminological and linguistic
93
information. In The Sixth International Workshop on
Ontology Matching collocated with the 10th Interna-
tional Semantic Web Conference (ISWC?11).
E. M. Bennet, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited response question-
ing. Public Opinion Quarterly, 18:303?308.
Thierry Declerck, Hans-Ulrich Krieger, Susan M.
Thomas, Paul Buitelaar, Sean O?Riain, Tobias Wun-
ner, Gilles Maguet, John McCrae, Dennis Spohr, and
Elena Montiel-Ponsoda. 2010. Ontology-based mul-
tilingual access to financial reports for sharing busi-
ness knowledge across europe. In Internal Financial
Control Assessment Applying Multilingual Ontology
Framework.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from wikipedia. Lecture Notes in Computer Sci-
ence, (4947):380?392. Springer.
Darja Fis?er, S?pela Vintar, Nikola Ljubes?ic?, and Senja Pol-
lak. 2011. Building and using comparable corpora for
domain-specific bilingual lexicon extraction. In Pro-
ceedings of the 4th Workshop on Building and Using
Comparable Corpora: Comparable Corpora and the
Web, BUCC ?11, pages 19?26.
J.L. Fleiss et al 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Koen Kerremans. 2010. A comparative study of termino-
logical variation in specialised translation. In Recon-
ceptualizing LSP Online proceedings of the XVII Eu-
ropean LSP Symposium 2009, pages 1?14.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL, ACL
?07, pages 177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86. AAMT.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Methodology. Sage Publications, Inc.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
Alon Lavie and Abhaya Agarwal. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
EMNLP 2011 Workshop on Statistical Machine Trans-
lation, pages 65?72.
Christof Mu?ller and Iryna Gurevych. 2008. Using
wikipedia and wiktionary in domain-specific informa-
tion retrieval. In Working Notes for the CLEF 2008
Workshop.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318.
W. A. Scott. 1955. Reliability of Content Analysis: The
Case of Nominal Scale Coding. Public Opinion Quar-
terly, 19:321?325.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel
Varga. 2006. The jrc-acquis: A multilingual aligned
parallel corpus with 20+ languages. In Proceedings
of the 5th International Conference on Language Re-
sources and Evaluation (LREC?2006).
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286.
Jorge Vivaldi and Horacio Rodriguez. 2010. Using
wikipedia for term extraction in the biomedical do-
main: first experiences. Procesamiento del Lenguaje
Natural, 45:251?254.
Marion Weller, Anita Gojun, Ulrich Heid, Be?atrice
Daille, and Rima Harastani. 2011. Simple methods
for dealing with term variation and term alignment.
In Proceedings of the 9th International Conference on
Terminology and Artificial Intelligence, pages 87?93.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
wikipedia and wiktionary. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC?08).
94
Proceedings of the 4th International Workshop on Computational Terminology, pages 22?31,
Dublin, Ireland, August 23 2014.
Identification of Bilingual Terms from Monolingual Documents for
Statistical Machine Translation
Mihael Arcan1 Claudio Giuliano2 Marco Turchi2 Paul Buitelaar1
1 Unit for Natural Language Processing, Insight @ NUI Galway, Ireland
{mihael.arcan , paul.buitelaar}@insight-centre.org
2 FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
{giuliano, turchi}@fbk.eu
Abstract
The automatic translation of domain-specific documents is often a hard task for generic Sta-
tistical Machine Translation (SMT) systems, which are not able to correctly translate the large
number of terms encountered in the text. In this paper, we address the problems of automatic
identification of bilingual terminology using Wikipedia as a lexical resource, and its integration
into an SMT system. The correct translation equivalent of the disambiguated term identified in
the monolingual text is obtained by taking advantage of the multilingual versions of Wikipedia.
This approach is compared to the bilingual terminology provided by the Terminology as a Ser-
vice (TaaS) platform. The small amount of high quality domain-specific terms is passed to the
SMT system using the XML markup and the Fill-Up model methods, which produced a relative
translation improvement up to 13% BLEU score points
1 Introduction
Translation tasks often need to deal with domain-specific terms in technical documents, which require
specific lexical knowledge of the domain. Nowadays, SMT systems are suitable to translate very frequent
expressions but fail in translating domain-specific terms. This mostly depends on a lack of domain-
specific parallel data from which the SMT systems can learn. Translation tools such as Google Translate
or open source phrase-based SMT systems, trained on generic data, are the most common solutions and
they are often used to translate manuals or very specific texts, resulting in unsatisfactory translations.
This problem is particular relevant for professional translators that work with documents coming from
different domains and are supported by generic SMT systems. A valuable solution to help them in han-
dling domain-specific terms is represented by online terminology resources, e.g. IATE - Inter-Active
Terminology for Europe,1 which are continuously updated and can be easily queried. However, the man-
ual use of these services can be very time demanding. For this reason, the identification and embedding
of domain-specific terms in an SMT system is a crucial step towards increasing translator productivity
and translation quality in highly specific domains.
In this paper, we propose an approach to automatically detect monolingual domain-specific terms from
a source language document and identify their equivalents using Wikipedia cross-lingual links. For this
purpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, adding
two more components able to first identify domain-specific terms, and to find their translations in a target
language. The identified bilingual terms are then compared with those obtained by TaaS (Skadins? et al.,
2013). The embedding of the domain-specific terms into an SMT system is performed by use of the
XML markup approach, which uses the terms as preferred translation candidates at run time, and the
Fill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms.
Our results show that the performance of our technique and TaaS are comparable in the identification
of monolingual and bilingual domain-specific terms. From the machine translation point of view, our
experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative
improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1 http://iate.europa.eu/ 2 https://bitbucket.org/fbk/thewikimachine/
Terminology questions in texts authored by patients
Noemie Elhadad
Department of Biomedical Informatics
Columbia University, USA
noemie@dbmi.columbia.edu
This work is c er a Creative Com ons Attribution 4.0 Internatio l License. Page numb rs and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
22
2 Methodology
Given a source document, it is processed by our pipeline that: (i) with the help of The Wiki Machine, it
identifies, disambiguates and links all terms in the document to the Wikipedia pages; (ii) the terms and
their links are used to identify the domain of the document and filter out the terms that are not domain-
specific; (iii) the translation of such terms is obtained following the Wikipedia cross-lingual links; (iv)
the bilingual domain-specific terms are embedded into the SMT system using different strategies. In the
rest of this section, each step is described in detail.
2.1 Bilingual Term Identification
Term Detection and Linking The Wiki Machine is a tool for linking terms in text to Wikipedia pages
and enriching them with information extracted from Wikipedia and Linked Open Data (LOD) resources
such as DBPedia or Freebase. The Wiki Machine has been preferred among other approaches because it
achieves the best performance in term disambiguation and linking (Mendes et al., 2011), and facilitates
the extraction of structured information from Wikipedia.
The annotation process consists of a three-step pipeline based on statistical and machine learning
methods that exclusively uses Wikipedia to train the models. No linguistic processing, such as stemming,
morphology analysis, POS tagging, or parsing, is performed. This choice facilitates the portability of the
system as the only requirement is the existence of a Wikipedia version with a sufficient coverage for the
specific language and domain. The first step identifies and ranks the terms by relevance using a simple
statistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated
and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages.
The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must
be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense,
a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses
an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano
et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means
of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The
third step enriches the linked terms using information extracted from Wikipedia and LOD resources.
The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e.,
orthographical and morphological variants, synonyms, and related terms), images, topic, type, cross
language links, etc. For example, in the text ?click right mouse key to pop up menu and Gnome panel?,
The Wiki Machine identifies the terms mouse, key, pop up menu and Gnome panel. For the ambiguous
term mouse, the linking algorithm returns the Wikipedia page ?Mouse (computing)?, and the other terms
used to link that page in Wikipedia with their frequency, i.e., computer mouse, mice, and Mouse.
In the context of the experiments reported here, we were specifically interested in the identification of
domain-specific bilingual terminology to be embedded into the SMT system. For this reason, we extend
The Wiki Machine adding the functionality of filtering out terms that do not belong to the document
domain, and of automatically retrieving term translations.
Domain Detection To identify specific terms, we assign a domain to each linked term in a text, after
that we obtain the most frequent domain and filter out the terms that are out of scope. In the example
above, the term mouse is accepted because it belongs to the domain computer science, as the majority of
terms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected.
The large number of languages and domains to cover prevents us from using standard text classification
techniques to categorize the document. For this reason, we implemented an approach based on the
mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia
categories are created and assigned by different human editors, and are therefore less rigorous, coherent
and consistent than usual ontologies. In addition, the Wikipedia?s category hierarchy forms a cyclic graph
(Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a
hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural
Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity,
allows us reducing the number of domains to few tens instead of some hundred thousands (800,000
23
categories in the English Wikipedia) and does not require any language-specific training data. Wikipedia
categories that contain more pages (?1,000) have been manually mapped to WordNet domains. The
domain for a term is obtained as follows. First, for each term, we extract its set of categories, C, from
the Wikipedia page linked to it. Second, by means of a recursive procedure, all possible outgoing paths
(usually in a large number) from each category in C are followed in the graph of Wikipedia categories.
When one of the mapped categories to a WordNet domain is found, the approach stops and associates the
relative WordNet domain to the term. In this way, more and more domains are assigned to a single term.
Third, to isolate the most relevant one, these domains are ranked according the number of times they have
been found following all the paths. The most frequent domain is assigned to the terms. Although this
process needs the human intervention for the manual mapping, it is done once and it is less demanding
than annotating large amounts of training documents for text classification, because it does not require
the reading of the document for topic identification.
Bilingual Term Extraction The last phase consists in finding the translation of the domain terminol-
ogy. We exploit the Wikipedia cross-language links, which, however, provide an alignment at page level
not at term level. To deal with this issue we introduced the following procedure. If the term is equal to
the source page title (ignoring case) we return the target page; otherwise, we return the most frequent al-
ternative form of the term in the target language. From the previous example, the system is able to return
the Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse in
computer science. Using this information, the term mouse is paired with its translation into Italian.
2.2 Integration of Bilingual Terms into SMT
A straightforward approach for adding bilingual terms to the SMT system consists of concatenating the
training data and the terms. Although it has been shown to perform better than more complex techniques
(Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications.
In particular, when small amounts of bilingual terms are concatenated with a large training dataset, terms
with ambiguous translations are penalised, because the most frequent and general translations often
receive the highest probability, which drives the SMT system to ignore specific translations.
In this paper, we focus on two techniques that give more priority to specific translations than generic
ones: the Fill-Up model and the XML markup approach. The Fill-Up model has been developed to
address a common scenario where a large generic background model exists, and only a small quantity
of in-domain data can be used to build an in-domain model. Its goal is to leverage the large coverage
of the background model, while preserving the domain-specific knowledge coming from the in-domain
data. Given the generic and the in-domain phrase tables, they are merged. For those phrase pairs that
appear in both tables, only one instance is reported in the Fill-Up model with the largest probabilities
according to the tables. To keep track of a phrase pair?s provenance, a binary feature that penalises if
the phrase pair comes from the background table is added. The same strategy is used for reordering
tables. In our experiments, we use the bilingual terms identified from the source data as in-domain
data. Word alignments are computed on the concatenation of the data. Phrase extraction and scoring
are carried out separately on each corpus. The XML markup approach makes it possible to directly pass
external knowledge to the decoder, specifying translations for particular spans of the source sentence. In
our scenario, the source term is used to identify a span in the source sentence, while the target term is
directly passed to the decoder. With the setting exclusive, the decoder uses only the specified translations
ignoring other possible translations in the translation model.
3 Experimental Setting
In our experiments, we used different English-Italian and Italian-English test sets from two domains: (i)
a small subset of the GNOME project data3 (4,3K tokens) and KDE4 Data4 (9,5K) for the IT domain
and (ii) a subset of the EMEA corpus (11K) for the medical domain.
In order to assess the quality of the monolingual and bilingual terms, we create a terminological gold
standard. Two annotators with a linguistic background and English and Italian proficiency were asked
3 https://l10n.gnome.org/ 4 http://i18n.kde.org/
24
to mark all domain-specific terms in a set of 66 English and Italian documents of the GNOME corpus,
and a set of 100 paragraphs (4,3K tokens) from the KDE4 corpus.5 Domain-specificity was defined as
all (multi-)words that are typically used in the IT domain and that may have different Italian translations
in other domains. The average Cohen?s Kappa of GNOME and KDE anno computed at token level was
0.66 for English and 0.53 for Italian. Following Landis and Koch (1977), this corresponds to a substantial
and moderate agreement between the annotators.
Finally the gold standard dataset was generated by the intersection of the annotations of the two an-
notators. In detail, for the GNOME dataset the annotators marked 93 single-word and 134 multi-word
expressions (MWEs), resulting 227 terms in overall. For the KDE anno dataset, 321 monolingual terms
for the GNOME dataset were annotated, whereby 192 of them were multi-word expressions. This results
in 190 unique bilingual terms for the GNOME corpus and 355 for the KDE anno dataset.
We compare the monolingual and bilingual terms identified by our approach to the terms obtained
by the online service TaaS,6 which is a cloud-based platform for terminology services based on the
state-of-the-art terminology extraction and bilingual terminology alignment methods. TaaS provides
several options in term identification, of which we selected TWSC, Tilde wrapper system for CollTerm,
(Pinnis et al., 2012). TWSC is based on linguistic analysis, i.e. part of speech tagging and morpho-
syntactic patterns, enriched with statistical features. TaaS allows for lookup in several manually and
automatically built monolingual and bilingual terminological resources and for our experiment we use
EuroTermBank (ETB), Taus Data and Web Data. Accessing several resources, TaaS may provide several
translations for a unique source term, but not an indicator of their translation quality. To avoid assigning
the same probability to all the translations of the same source term, we prioritise a translation by the
resource it was provided. In our case, we favour first the translation provided by ETB. If no translation
is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting
the term extraction approach, TaaS requires manual specification of the source and target languages, the
domain, and the source document. Since we focused on the IT and medical domains we set the options
to ?Information and communication technology? and ?Medicine and pharmacy?, respectively.
For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where
the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit
(Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage,
we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl
(Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of
?37M tokens and a development set of ?10K tokens.
In our experiments, an instance of Moses trained on the generic parallel dataset was used in three
different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup
approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the
Fill-Up method as background translation model.
4 Evaluation
In this Section, we report the performance of the different term identification tools and term embedding
methods for the two domains: IT and the medical domain. For evaluating the extracted monolingual
and bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE anno
and GNOME datasets. In addition, we perform a manual inspection of a subset of the bilingual identi-
fied terms. The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality of
the translations. The metric calculates the overlap of n-grams between the SMT system output and a
reference translation, provided by a professional translator.
4.1 Monolingual Term Identification
In Table 1, the column ?Ident.? represents the number of identified terms for each tool, whereby we
observed TaaS always extracts more terms than The Wiki Machine. While extracting Italian terms,
TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower
5 In the rest of the paper, we refer to the annotated part of KDE4 as KDE anno
6 https://demo.taas-project.eu/
25
English Italian
KDE anno Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 431 144 287 0.442 0.594 0.507 518 147 371 0.326 0.511 0.398
The Wiki Machine 327 247 80 0.400 0.406 0.403 207 184 23 0.429 0.268 0.330
GNOME Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 311 119 192 0.260 0.355 0.301 359 110 249 0.272 0.415 0.329
The Wiki Machine 275 199 76 0.303 0.364 0.330 196 167 29 0.331 0.275 0.301
Table 1: Evaluation of monolingual term identification for the KDE anno and GNOME dataset.
amount of Italian pages in Wikipedia compared to the English version. Focusing on the amount of
identified single-word and multi-word expressions, it is interesting to notice that TaaS, independently of
the language, extracts around twice as more MWEs than single words. Differently, The Wiki Machine
identifies mostly single-word terms, whereby they represent around three-fourth of all identified terms
for English and around 12% for Italian.
For the KDE anno dataset, TaaS in most cases (except in precision for the Italian KDE anno dataset)
outperforms The Wiki Machine approach in all metrics. Especially we observed a higher recall produced
by the TaaS approach, which can be deduced from the higher number of extracted MWEs compared to
The Wiki Machine approach. On the English GNOME dataset, The Wiki Machine performs comparable
results to TaaS, with a slightly higher recall and F1. On the Italian side, The Wiki Machine identifies less
MWEs than TaaS, which results in a low recall and F1.
In summary, we observe that TaaS performs best on the KDE anno dataset, whereas The Wiki Machine
and TaaS perform comparable results on the GNOME dataset. Analysing the overall results, we notice
that precision, recall and F1 are generally better in English than in Italian. This is due to the fact that
Italian tends to use more words to express the same concept compared to English.
4.2 Bilingual Term Identification
Table 2 reports the performance of The Wiki Machine and TaaS in the identification of bilingual terms
evaluated against the manually produced list of terms. In both language pairs and datasets, TaaS and The
Wiki Machine mostly identify similar amounts of bilingual terms (column ?Ident.?) and match with the
gold standard (column ?Mat.?). Only for KDE anno, It?En, TaaS identifies almost 50% more bilingual
terms than The Wiki Machine.
It is worth noticing that, although TaaS is accessing high quality manually-produced termbases, e.g.
ETB in our results, there is no evidence that it works significantly better than The Wiki Machine access-
ing Wikipedia. In fact, in terms of F1, The Wiki Machine performs best on the GNOME annotated test
set, while it is outperformed by TaaS on KDE anno. In both cases, differences in performance are mini-
mal. According to the precision measure, The Wiki Machine seems to be able to produce more accurate
bilingual terms.
The automatic evaluation shows difficulties (low F1 scores) for The Wiki Machine and TaaS in iden-
tifying bilingual terms that perfectly match the gold standard. To better understand the quality of term
translations, we asked one of the annotators involved in the creation of the gold standard to perform a
manual evaluation of a subset of fifty bilingual terms randomly selected from each list. We used the
four error categories proposed in (Aker et al., 2013): 1) The terms are exact translations of each other
in the domain; 2) Inclusion: Not an exact translation, but an exact translation of one term is entirely
contained within the term in the other language; 3) Overlap: Not category 1 or 2, but the terms share at
least one translated word; 4) Unrelated: No word in either term is a translation of a word in the other.
The percentages of bilingual terms assigned to each class are shown in Table 3.
In terms of comparison between the two tools, the manual evaluation confirms that there is no evidence
that a tool produces better term translations than the other in all the test sets. In fact, except for KDE anno
En?It where TaaS outperforms The Wiki Machine, the percentage of bilingual terms assigned to class
1 for both the tools is almost similar. In terms of absolute scores, the manual evaluation shows that
the quality of the identified bilingual terms is relatively high (merging the terms assigned to classes 1
26
GNOME En?It Ident. Mat. Precision Recall F1
TaaS 145 20 0.138 0.105 0.119
The Wiki Machine 156 25 0.160 0.130 0.144
GNOME It?En Ident. Mat. Precision Recall F1
TaaS 139 21 0.151 0.110 0.127
The Wiki Machine 140 23 0.164 0.121 0.139
KDE anno En?It Ident. Mat. Precision Recall F1
TaaS 249 65 0.261 0.183 0.215
The Wiki Machine 229 49 0.202 0.138 0.164
KDE anno It?En Ident. Mat. Precision Recall F1
TaaS 228 58 0.254 0.163 0.199
The Wiki Machine 155 48 0.292 0.135 0.185
Table 2: Automatic evaluation of bilingual terms ex-
tracted from GNOME and KDE anno.
GNOME En?It 1 2 3 4
TaaS 0.66 0.08 0.00 0.26
The Wiki Machine 0.70 0.08 0.06 0.16
GNOME It?En 1 2 3 4
TaaS 0.78 0.08 0.02 0.12
The Wiki Machine 0.68 0.12 0.04 0.16
KDE anno En?It 1 2 3 4
TaaS 0.90 0.00 0.06 0.04
The Wiki Machine 0.70 0.10 0.06 0.14
KDE anno It?En 1 2 3 4
TaaS 0.70 0.10 0.10 0.10
The Wiki Machine 0.64 0.22 0.08 0.06
Table 3: Manual evaluation of bilingual terms
based on four error categories (1-4).
and 2, we reach a score, in most of the cases, larger than 80%). This is in contrast with the automatic
evaluation, which reports limited performances (F1 ? 0.2) for both methods. The main reason is that
the automatic evaluation requires a perfect match between the identified and the gold standard bilingual
terms to measure an improvement in F1, while the manual evaluation can reward bilingual terms that do
not perfectly match any gold standard terms but are correct translations of each other. An example is
the multi-word bilingual term ?settings of the network connection? impostazioni della connessione di
rete? that is present in the gold standard as a single multi-word term, while it is identified by The Wiki
Machine as two distinct bilingual terms, i.e. ?network connection? connessione di rete? and ?settings
? impostazioni?. From the translation point of view, both the distinct terms are correct and they are
assigned to class 1 during the manual evaluation, but they are ignored by the automatic evaluation.
The analysis of terms assigned to error class four shows that both methods are affected by similar
problems. The main source of error is the correct detection of the source term domain, which results in
a translated term that does not belong to the correct domain. For instance, in the bilingual term ?stringhe
? shoe and boot laces?, the term ?stringhe? (?strings? in the IT domain) is translated into ?laces?. Simi-
larly, the English term ?launchers? (?lanciatori? in Italian in the IT domain) is translated into ?lanciarazzi
multiplo? (?multiple rocket launchers? in English), which is clearly not an IT term. Furthermore, The
Wiki Machine seems to have more problems in identifying the right morphological variation, e.g. ?in-
dirizzi ip? ip address?, where ?indirizzi? is a plural noun and needs to be translated into ?addresses?.
This is expected because page titles in Wikipedia are not always inflected. An interesting example high-
lighted by the annotator in the TaaS translations is: ?percorso di ricerca? ? ?how do i access refresh
grid texture??, where the Italian term (?search path? in English) is translated with a completely wrong
translation. In the next Section we evaluate whether the automatic identified bilingual terms can improve
the performance of an SMT system and if it is robust to the aforementioned errors.
4.3 Embedding Terminology into SMT
Our further experiments focused on the automatic evaluation of the translation quality of the EMEA,
GNOME and KDE test sets (Table 4). The obtained bilingual terminology from TaaS and The Wiki Ma-
chine was embedded through the Fill-Up and XML markup approaches. The approximate randomization
approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances
are statistically significant with a p-value < 0.05. The parameters of the baseline method and the Fill-Up
models were optimized on the development set.
Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases. XML markup
outperforms the general baseline approach in three (out of eight) datasets, whereby three of them are
statistically significant (GNOME En?It, KDE anno En?It). Embedding the same bilingual terminol-
ogy into the Fill-Up model helped to outperform the baseline approach for all test sets, whereby only the
result for EMEA En?It is not statistically significant.
27
GNOME KDE anno EMEA KDE4
En?It It?En En?It It?En En?It It?En En?It It?En
general baseline 15.39 21.62 15.58 22.64 25.88 25.75 19.22 23.54
XML Mark-up (TaaS) 15.87 22.45* 17.62* 23.88* 25.84 25.74 18.97 24.27*
Fill-Up Model (TaaS) 16.22* 22.73* 17.61* 23.45* 25.95 26.02* 19.69* 24.56*
XML Mark-up (The Wiki Machine) 15.49 20.57 17.19* 23.44* 25.59 24.97 17.74 22.16
Fill-Up Model (The Wiki Machine) 15.82 21.70 16.48* 23.28* 26.35* 26.44* 19.61* 24.14*
Table 4: Automatic BLEU Evaluation on GNOME, KDE and EMEA datasets with different term em-
bedding strategies (bold results = best performance ; * statistically significant compared to baseline).
Finally, we investigate the impact of embedding the identified terms provided by The Wiki Machine.
When we suggest translation candidates with the XML markup, it only slightly outperforms the baseline
approach for GNOME En?It, but statistically significant improves the translations for the KDE anno
test set for both language directions. Similarly to previous observations, the Fill-Up model improves
further the translations, i.e. the translations are statistically significant better than the baseline for both
language pairs of both KDE test sets as well as for EMEA.
To better understand our translation results, we manually inspected the EMEA En?It sentences, which
have the best translation performance. For each of the source sentence and the translation method,
we analyse the translated sentences and the bilingual terms that match at least one word in the source
sentence. Both translation strategies tried to encapsulate the bilingual terms, but there is clear evidence
that the Fill-Up model better embeds the target terms in the context of the translation. For instance in
the following example, the target sentence produced by the XML markup (XML trg) does not contain
the article ?la?, uses a wrong conjunction (?di? instead of ?per?) and wrongly orders the adjective with
the noun (?adulti pazienti? instead of ?pazienti adulti?). All these issues are correctly addressed by the
Fill-Up model (Fill-Up trg) which produces a smoother translation.
source sentence: adult patients receive therapy for tumours
reference sentence: pazienti adulti ricevono la terapia per i tumori
bilingual terms: therapy? terapia, patients? pazienti, adult? adulti
XML trg: adulti pazienti ricevono terapia di tumori
Fill-Up trg: pazienti adulti ricevono la terapia per i tumori
Analysing the number of suggested bilingual terms per sentence, we notice that The Wiki Machine
tends to propose more terms than TaaS (on average, The Wiki Machine 3.1, TaaS 2.5 per sentence).
Of these terms, TaaS provides on average more translations for each unique source term than The Wiki
Machine (on average, TaaS 1.51, The Wiki Machine 1).
In addition to evaluating the performance of TaaS and The Wiki Machine separately, for the EMEA
dataset we concatenate the terminological lists provided by the tools and supply it to the XML markup
and the Fill-Up approach. Embedding the combined terminology with the XML markup produces a
BLEU score of 25.59 for En?It and 24.92 for It?En. This performance is similar to the scores obtained
using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole
terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En?It and 27.02 for It?En,
which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of
the two term identification methods and suggests a novel research direction.
5 Related Work
The main focus of our research is on bilingual term identification and the embedding of this knowledge
into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that
an SMT system built by using a large general resource cannot be used to translate domain-specific terms,
we have to provide the system domain-specific lexical knowledge.
Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term
identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual
dictionary entries from Wikipedia to support the machine translation system. Based on exact string
28
matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual
dictionary. Besides the interwiki link system, Erdmann et al. (2009) enhances their bilingual dictionary
by using redirection page titles and anchor text within Wikipedia. To filter out incorrect term translation
pairs, the authors use the backward link information to prove if a redirect page title or an anchor text
represents a synonymous expression. Niehues and Waibel (2011) analyse different methods to integrate
the extracted Wikipedia titles into their system, whereby they explore methods to disambiguate between
different translations by using the text in the articles. In addition, the authors use morphological forms
of terms to enhance the extracted bilingual dictionary. The results show that the number of out-of-
vocabulary words could be reduced by 50% on computer science lectures, which improved the translation
quality by more than 1 BLEU point. Arcan et al. (2013) restrict term identification to the observed
domain by using the frequency information of Wikipedia categories. Different from these approaches
we focus on domain-specific dictionary generation, ignoring identified terms which do not belong to the
domain to be observed. Furthermore, we take advantage of the Wikipedia category graph representation
and its linking to WordNet domain, which allowed us to identify the domain we were interested in.
Furthermore, research has been done on the integration of domain-specific parallel data into SMT,
either by retraining small domain-specific and large general resources as one concatenated parallel data
(Koehn and Schroeder, 2007), adding new phrase pairs directly into the phrase table (Langlais, 2002;
Ren et al., 2009; Haddow and Koehn, 2012) or assigning adequate weights to the in- and out-of-domain
translation models (Foster and Kuhn (2007); La?ubli et al. (2013)). Bouamor et al. (2012) address the
problem of finding the best approach to integrate new obtained knowledge in an SMT system, and show
that they should be used as additional parallel sentences to train the translation model. In our approach,
we use the XML markup and the Fill-Up approach, which handles the in-domain parallel data equally
to the out-domain data. Furthermore, Okita and Way (2010) investigate the effect of integrating bilin-
gual terminology in the training step of an SMT system, and analyse in particular the performance and
sensitivity of the word aligner. As opposed to their approach, we do not have prior knowledge about the
bilingual terminology, since we extract it from the document to be translated.
6 Conclusion
In this paper we presented an approach to identify bilingual domain-specific terms starting from a mono-
lingual text and to integrate these into an SMT system. With the help of terminological and lexical
resources, we are able to discover a small amount (?200) of high-quality domain-specific terms and
enhanced the performance of an SMT system trained on large amounts (1.8M) of parallel sentences.
Monolingual and bilingual term evaluation showed no evidence that one of the tested tools (The Wiki
Machine or TaaS) produces better terms than the other in all the test sets. Depending on the manual map-
ping between the Wikipedia categories and WordNet domains and the existence of a Wikipedia version,
our approach is language and domain independent, does not need training data and is able to overcome
the sparseness and coherence problems of the Wikipedia categories. Evaluation of the two systems on
different language directions and domains shows significant improvements over the baseline in terms
of two BLEU scores (up to 13%) and confirms the applicability of such techniques in a real scenario.
It is interesting to notice that the Fill-Up technique regularly outperforms the XML markup approach,
taking advantage of all terms and not only the overlapping terms in the text to be translated. Our contri-
bution shows a different context of using Fill-Up and extends the usability of it in terms of embedding
terminological knowledge into SMT. In future work, we plan to focus on exploiting morphological term
variations taking advantage of the alternative terms (i.e., orthographical and morphological variants,
synonyms, and related terms) provided by The Wiki Machine. This will make it possible to increase the
coverage adding new terms and the accuracy of the proposed method for bilingual term identification.
Acknowledgments
This publication has emanated from research supported in part by a research grant from Science Founda-
tion Ireland (SFI) under Grant Number SFI/12/RC/2289 and by the European Union supported projects
EuroSentiment (Grant No. 296277), LIDER (Grant No. 610782) and MateCat (ICT-2011.4.2-287688).
29
References
Ahmet Aker, Monica Paramita, and Robert Gaizauskas. 2013. Extracting bilingual terminologies from comparable
corpora. In Proceedings of ACL, Sofia, Bulgaria.
Mihael Arcan, Susan Marie Thomas, Derek De Brandt, and Paul Buitelaar. 2013. Translating the FINREP taxon-
omy using a domain-specific corpus. In Machine Translation Summit XIV, pages 199?206.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains
hierarchy: semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic
Ressources, pages 101?108. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In Proceedings of IWSLT.
Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2012. Identifying bilingual multi-word expres-
sions for statistical machine translation. In Proceedings of the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instability . In Proceedings of the Association for Computational
Lingustics.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2009. Improving the extraction of bilin-
gual terminology from wikipedia. ACM Trans. Multimedia Comput. Commun. Appl., 5(4):31:1?31:17, Novem-
ber.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages 1618?1621. ISCA.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo Strapparava. 2009. Kernel methods for minimally
supervised wsd. Computational Linguistics, 35(4):513?528.
Barry Haddow and Philipp Koehn. 2012. Analysing the Effect of Out-of-Domain Data on SMT Systems. In
Proceedings of the Seventh Workshop on Statistical Machine Translation, Montre?al, Canada. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual
Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages 79?86. AAMT.
J. Richard Landis and Gary G. Koch. 1977. Measurement of Observer Agreement for Categorical Data. In
Biometrics, volume 33, pages 159?174.
Philippe Langlais. 2002. Improving a general-purpose statistical translation engine by terminological lexicons. In
Proceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ?2002, Taipei,
Taiwan, pages 1?7.
Samuel La?ubli, Mark Fishel, Martin Volk, and Manuela Weibel. 2013. Combining statistical machine translation
and translation memories with domain adaptation. In Stephan Oepen, Kristin Hagen, and Janne Bondi Johan-
nesse, editors, Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013),
May 22?24, 2013, Oslo University, Norway, Linko?ping Electronic Conference Proceedings, pages 331?341,
Oslo, May. Linko?pings universitet Electronic Press.
30
Pablo N Mendes, Max Jakob, Andre?s Garc??a-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light
on the web of documents. In Proceedings of the 7th International Conference on Semantic Systems, pages 1?8.
ACM.
Rada Mihalcea. 2007. Using Wikipedia for Automatic Word Sense Disambiguation. In Proceedings of NAACL-
HLT, pages 196?203.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia to Translate Domain-specific Terms in SMT. In nterna-
tional Workshop on Spoken Language Translation, San Francisco, CA, USA.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29.
Tsuyoshi Okita and Andy Way. 2010. Statistical Machine Translation with Terminology. In Proceedings of the
First Symposium on Patent Information Processing (SPIP), Tokyo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 311?318.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay. 2012.
Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the Terminology
and Knowledge Engineering (TKE2012) Conference.
Zhixiang Ren, Yajuan Lu?, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation
using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions:
Identification, Interpretation, Disambiguation and Applications, MWE ?09, pages 47?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Raivis Skadins?, Marcis Pinnis, Tatiana Gornostay, and Andrejs Vasiljevs. 2013. Application of online terminology
services in statistical machine translation. In Proceedings of the XIV Machine Translation Summit, Nice, France.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel Varga.
2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (LREC?2006).
Jo?rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Francis M. Tyers and Jacques A. Pieanaar. 2008. Extracting bilingual word pairs from wikipedia. In Collabo-
ration: interoperability between people in the creation of language resources for less-resourced languages (A
SALTMIL workshop).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08, pages 993?1000.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In
Proceedings of the TextGraphs-2 Workshop (NAACL-HLT), pages 1?8, Rochester, April. Association for Com-
putational Linguistics.
31
