Proceedings of the Linguistic Annotation Workshop, pages 49?52,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Criteria for the Manual Grouping of Verb Senses  Cecily Jill Duffield, Jena D. Hwang, Susan Windisch Brown,  Dmitriy Dligach, Sarah E.Vieweg, Jenny Davis, Martha Palmer Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA {cecily.duffield, hwangd, susan.brown, dmitry.dligach,  sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu  
Abstract  In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a case-by-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators.  1  Credits  We gratefully acknowledge the support of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022, a subcontract from the BBN-AGILE Team.  2  Introduction  Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation. Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, perhaps more so for distinguishing between multiple senses of verbs than for any other grammatical category. WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various 
lexical items (Felbaum, 1998). When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance. Annotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity. Our goal in this project is to create verb sense distinctions at a middle level of granularity that allow us to capture as much information as possible from a lexical item while still attaining high ITA scores and high system performance in automatic sense disambiguation. We have demonstrated that clear sense distinctions improve annotator productivity and accuracy. System performance typically lags around 10% behind ITA rates. ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance. Training on this new data, Chen et al, (2006) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features. (Also Semeval071) They also report state-of-the-art performance on fine-grained senses, but the results are more than 16% lower. We begin by describing the overall process.  3  The Grouping and Annotation Process  The process for building our database with the appropriate level of verb sense distinctions                                                 1 Task 17,  http://nlp.cs.swarthmore.edu/semeval/.  
49
  
involves two steps: sense grouping and annotation (Figure 1). During our sense grouping process, linguists (henceforth, ?groupers?) cluster fine-grained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings. These rough clusters of WordNet entries are based on speaker intuition. Other resources, including PropBank, VerbNet (based on Levin?s verb classes (Levin, 1993)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et. al., 2005, Kipper et al, 2006). To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency. Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings. We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Google search results. These examples are intended to guide annotators faced with similar challenges in the data to be tagged.  Completed verb sense groupings are sent through sample-annotation and tagged by two annotators. Groupings that receive an ITA score of 90% or above are then used to annotate all instances of that verb in our corpora in actual-annotation. Groupings that receive less than 90% ITA scores are regrouped (Hovy et al, 2006). Revisions are made based on a second grouper?s evaluation of the original grouping, as well as patterns of annotator disagreement. Verb groupings receiving ITA scores of 85% or above are sent through actual-annotation. Verbs scoring below 85% are regrouped by a third grouper, and in some cases, by the entire grouping team. It is sometimes impossible to get ITA scores over 85% for high   
frequency verbs that also have high entropy. These have to be carefully adjudicated to produce a gold standard. Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sample-annotation. Verbs subject to the re-annotation process are tagged by different annotators. Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators. The adjudicated data is then used as the gold standard for automatic annotation. The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al, 2005).  Verbs are selected based on frequency of appearance in the WSJ corpus. As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues. The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency. They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses. Of these, 307 verb groupings have undergone regrouping to some extent. A total of 670 verbs have completed actual-annotation and adjudication. The next 660 verbs have been divided into rough semantic domains based on VerbNet classes, and grouping will proceed according to these semantic domains rather than by verb frequency. As groupers create sense groupings for new verbs, old verb sense groupings in the same semantic domain are consulted. This organization allows for more consistent grouping methodologies, as well as more efficiency in integrating our sense groupings into the Ontology.  
 Figure 1:  The grouping and annotation process.  
50
  
4  Grouping Methodology  Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and collocations, syntactic features and alternations, and semantic features, similarly to Senseval2 (Palmer, et. al. 2006). Because these criteria do not apply uniformly to every verb, groupers take various approaches when creating sense groupings. Groupers recognize that there are many alternate ways to cluster senses at this level of granularity; each grouping represents only one possible clustering as a middle ground between PropBank and WordNet senses for each verb. Our highest priority is to then create clear distinctions among sense groupings that will be easily understood by the annotators and consequently result in high ITA scores. Initial clustering is based on groupers? intuitions of the most salient categories. Many verb groupings, such as that for the verb kill, provide little detailed syntactic or semantic analysis and yet have received high ITA scores. The success of these intuitive sense groupings is not due to lack of polysemy; kill has 15 WordNet senses and 2 multi-word expressions clustered into 9 sense groupings, yet it received 94% ITA in first round sample-annotation.  While annotators have little trouble tagging text with verb senses that fall neatly into intuitive categories, many verbs have fine-grained WordNet senses that fall on a continuum between two distinct lexical usages. In such cases, syntactic and semantic aspects of the verb and its arguments help groupers cluster senses in such a way that annotators can make consistent decisions in tagging the text. 
Syntactic criteria: Annotators have found syntactic frames, such as those defining VerbNet classes, to be useful in understanding boundaries between sense groupings. For example, split was originally grouped with consideration for the units resulting from a splitting event (i.e. whether a whole unit had been split into incomplete portions of the whole, or into smaller, but complete, individual units.)  This grouping proved difficult for annotators to distinguish, with an ITA of 42%. Using the causative/inchoative alternation for verbs in the ?break-45.1? class to regroup resulted in higher consistency among annotators, increasing the ITA score to 95%. Semantic criteria: When senses of a verb have similar syntactic frames, and usages fall along a continuum between these senses, semantic features of the arguments, or less often, of the verb itself, can clarify these senses and help groupers draw clear distinctions between them. Argument features that are considered when creating sense groupings include [+/-attribute], [+/-patient], and [+/-locative]. It is most common for groupers to mark these features on nominal arguments, but a prepositional phrase may also be described in semantic terms. Semantic features of the verb that are considered include aspectual features, as illustrated by the use of [+/-punctual] in sense groupings for make (Figure 2). However, it may be argued that this feature is unnecessary for annotators to be able to distinguish between the sense groupings, as the prepositional phrase in sense 9 is a more salient feature for annotators. Other features of the verb that were used earlier in the project include concrete/abstract, continuative, stative, and others. However, these features proved less useful than those Sense group Description and Commentary WordNet 2.1 senses Examples 8 Attain or reach something desired NP1[+agent] MAKE[+punctual] NP2[desired goal, destination, state] This sense implies the goal has been met. Includes: MAKE IT 
make 13, 22, 38 - He made the basketball team. - We barely made the plane. - I made the opening act in plenty of time. - Can you believe it? We made it!  
9 Move toward or away from a location NP1[+agent] MAKE[-punctual] (pronoun+way) PP/INFP  
make 30, 37 make off 1 make way 1  
- As the enemy approached our town, we made for the hills. - He made his way carefully across the icy parking lot. - They made off with the jewels. Figure 2: Sense groupings 8 and 9 for ?make.? Senses are distinguished in part by aspectual features marked on the verb.  
51
 described above, and annotators not familiar with linguistic theory found them to be confusing. Therefore, they are now rarely used to label sense groupings. Such concepts, when used, are more likely to be described in prose commentary for the sake of the annotators. Certain compositional features of verbs have also proven to be confusing for annotators. In several cases, attempts to distinguish sense groupings based on manner and path have resulted in increased annotator disagreement. In the first attempt at grouping roll, syntactic and semantic information, as well as prose commentary, was presented to help annotators distinguish the manner and path sense groupings. Despite this, the admissibility of certain prepositions in both senses (?The baby rolled over,? vs ?She rolled over to the wall,?) may have blurred the distinction. In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for roll, which were then merged in the final version of the sense groupings.  5  Conclusion  Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al, 2004; Palmer et al, 2007), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al, 2005). This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet (or similar) senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators. Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as possible when grouping verbs in similar semantic domains as defined by VerbNet. By using this approach when creating sense groupings, we are 
able to provide annotators with clear and reliable descriptions of senses, resulting in improved accuracy and performance.  References Chen, J., A. Schein, L. Ungar and M. Palmer. 2006. An Empirical Study of the Behavior of Word Sense Disambiguation. Proceedings of HLT-NAACL 2006. New York, NY. Fellbaum, C. (ed.) 1998. WordNet: An On-line Lexical Database and Some of its Applications. MIT Press, Cambridge, MA. Kipper, K., A. Korhonen, N. Ryant, and M. Palmer. 2006. Extensive Classifications of English Verbs. Proceedings of the 12th EURALEX International Congress. Turin, Italy. Levin, B. 1993. English Verb Classes and Alternations. The University of Chicago Press, Chicago, IL. OntoNotes, 2006. Hovy, E.H., M. Marcus, M. Palmer, S. Pradhan, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. Short paper. Proceedings of HLT-NAACL 2006. New York, NY. Palmer, M., O. Babko-Malaya, and H.T. Dang. 2004. Different Sense Granularities for Different Applications. Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NAACL 2004). Boston, MA.  Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 2007). Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics Journal, 31:1, 2005. Philpot, A., E.H. Hovy, and P. Pantel. 2005. The Omega Ontology. Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP). Jeju Island, Korea.  
52
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 23?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Twitter in Mass Emergency:  What NLP Techniques Can Contribute  William J. Corvey1, Sarah Vieweg2, Travis Rood1 & Martha Palmer1 1Department of Linguistics, 2ATLAS Institute University of Colorado Boulder, CO 80309 William.Corvey, Sarah.Vieweg, Travis.Rood, Martha.Palmer@colorado.edu 
Abstract 
We detail methods for entity span identifica-tion and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motiva-tion, method and preliminary results. 
1 Introduction During times of mass emergency, many turn to Twitter to gather and disperse relevant, timely in-formation (Starbird et al 2010; Vieweg et al 2010). However, the sheer amount of information now communicated via Twitter during these time- and safety-critical situations can make it difficult for individuals to locate personally meaningful and actionable information. In this paper, we discuss natural language processing (NLP) techniques de-signed for Twitter data that will lead to the location and extraction of specific information during times of mass emergency.  2 Twitter Use in Mass Emergency Twitter communications are comprised of 140-character messages called ?tweets.?  During times of mass emergency, Twitter users send detailed information that may help those affected to better make critical decisions.   Our goal is to develop techniques to automatically identify crucial pieces of information in these tweets. This process will lead to the automatic ex-traction of information that helps people under-stand the situation ?on the ground? during mass emergencies. Relevant information would include such things as warnings, road closures, and evacuations among other timely information.  
3 The Annotation Process A foundational level of linguistic annotation for many natural language processing tasks is Named Entity (or nominal entity) tagging (Bikel 1999).  Typical labeled entities that were included in the Automatic Content Extraction (ACE) guidelines (LDC 2004) are: Person, Location, Organization, and Facility, the four maximal entity classes. Our preliminary annotation task consists of identifying the syntactic span and entity class for these four types of entities in a pilot set of Twitter data (200 tweets from a data set generated during the 2009 Oklahoma grassfires). In future annotation, the ontology will be expanded to include event and relation annotations, as well as additional sub-classes of the entities now examined.  Annotations are done using Knowtator (Ogren 2006), a tool built within the Prot?g? framework (http://protege.stanford.edu/).  The ontology devel-opment is data-driven; as such it is likely that cer-tain ACE annotations will never emerge and other annotations (such as disaster-relevant materials) will be necessary additions.   Three annotators undertook pilot annotation as part of the construction of preliminary annotation guidelines; the top pairwise ITA score is reported below. Twitter data makes reference to numerous entity spans that are of specific interest to this an-notation task, such as road intersections and multi-word named entities. The example below, from the pilot annotation set, shows a relatively simple span delineation.  [PERSON Velma area residents]: [PERSON Officials] say to take [FACILTY Old Hwy 7] to [FACILTY Speedy G] to safely evacuate. [LOCATION Stephens Co Fair-grounds] in [LOCATION Duncan] for shel-ter  
23
Because of the varying length of entities, annota-tors cannot be given simple rules for deciding the spans for annotations. This difficulty is reflected in markedly lower rates for span identification inter-annotator agreement (IAA) rates than for simple class assignment.   4 Preliminary Results  IAA calculations were performed using the Know-tator IAA functionality. When annotations are re-quired to be both the same span and class, the pilot annotation yielded an F-score of 56.27 (An addi-tional 4% have exact span matches but different classes). However, when annotations are required to have the same class assignment but only over-lapping spans, this F-score rises to 72.85. While Facility and Location are the most commonly con-fused classes, span-matching remains a difficult issue for all entity classes.  5 Discussion While these ITA rates are significantly lower than published results from previous ACE annotation efforts (LDC 2004), we believe that the crisis communications domain, particularly with regard to Twitter analysis, provides challenges not en-countered in newswire, broadcast transcripts, or newspaper data. First, determining the maximal span of interest for a given class assignment is non-trivial. The constraint of 140 characters neces-sarily results in very limited syntactic and semantic contexts, making spans and entity class assign-ments much harder to determine.   A large source of disagreement was on the treat-ment of coordinated or listed noun phrases. In cer-tain contexts, each entity (cities below) requires its own span (e.g. ?Firestorms in Oklahoma. [Midwest City], [Lake Draper]. Some houses lost?), whereas in other contexts we find multiple entities per span (e.g. ?Midwest City to evacuate between SE 15th and Rena and Anderson and Hiwassee also [Tur-tlewood, Wingsong, and Oakwood additions]?).  Equally, class assignment cannot be a mechanistic process or accomplished by reference to lists, as it is important to distinguish between cases where terms have been elided due to limited space and cases where no elision has taken place. For in-stance, the entity ?Attorney General? (as opposed 
to ?Attorney General?s Office?) might be anno-tated ?Person? or ?Organization? depending on con-text, or simply ambiguous, i.e. lacking sufficient context. It is primarily these unclear cases of class assignment that will require careful discussion in the annotation guidelines and in future mappings to an ontology.  In summary, this pilot study represents a new ap-plication of ACE annotation practices to a uniquely challenging domain. We outline issues that place special demands on annotators and future direc-tions for ongoing research. We are confident that as we refine our guidelines and provide more cues and examples for the annotators that the determina-tion of spans and entity classes will improve. Acknowledgments This work is supported by the US National Science Foundation IIS-0546315 and IIS-0910586 but does not represent the views of the NSF. This work was conducted using the Prot?g? resource, supported by grant LM007885 from the US NLM. References  Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel. 1999. An Algorithm that Learns What?s in a Name. In: the Machine Learning Journal Special Issue on Natural Language Learning. George Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. (2004). The Automatic Content Extraction (ACE) Program ? Tasks, Data, and Evaluation. In: Proceedings of Con-ference on Language Resources and Evaluation (LREC 2004). Kate Starbird, Leysia Palen, Amanda L. Hughes and Sarah Vieweg. 2010. Chatter on The Red: What Hazards Threat Reveals About the Social Life of Mi-croblogged Information. In: Proc. CSCW 2010. ACM Press. LDC, 2004, Automatic Content Extraction [www.ldc.upenn.edu/Projects/ACE/] Philip Ogren. 2006. Knowtator: A Prot?g? plug-in for annotated corpus construction. In : Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology 2006. ACM Press. Sarah Vieweg, Amanda L. Hughes, Kate Starbird and Leysia Palen. 2010. Microblogging During Two Natural Hazards Events: What Twitter May Contrib-ute to Situational Awareness. In: Proc. CHI 2010. ACM Press. 
24
