Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151?161,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Recursive Autoencoders
for Predicting Sentiment Distributions
Richard Socher Jeffrey Pennington? Eric H. Huang Andrew Y. Ng Christopher D. Manning
Computer Science Department, Stanford University, Stanford, CA 94305, USA
?SLAC National Accelerator Laboratory, Stanford University, Stanford, CA 94309, USA
richard@socher.org {jpennin,ehhuang,ang,manning}@stanford.edu ang@cs.stanford.edu
Abstract
We introduce a novel machine learning frame-
work based on recursive autoencoders for
sentence-level prediction of sentiment label
distributions. Our method learns vector space
representations for multi-word phrases. In
sentiment prediction tasks these represen-
tations outperform other state-of-the-art ap-
proaches on commonly used datasets, such as
movie reviews, without using any pre-defined
sentiment lexica or polarity shifting rules. We
also evaluate the model?s ability to predict
sentiment distributions on a new dataset based
on confessions from the experience project.
The dataset consists of personal user stories
annotated with multiple labels which, when
aggregated, form a multinomial distribution
that captures emotional reactions. Our al-
gorithm can more accurately predict distri-
butions over such labels compared to several
competitive baselines.
1 Introduction
The ability to identify sentiments about personal ex-
periences, products, movies etc. is crucial to un-
derstand user generated content in social networks,
blogs or product reviews. Detecting sentiment in
these data is a challenging task which has recently
spawned a lot of interest (Pang and Lee, 2008).
Current baseline methods often use bag-of-words
representations which cannot properly capture more
complex linguistic phenomena in sentiment analy-
sis (Pang et al, 2002). For instance, while the two
phrases ?white blood cells destroying an infection?
and ?an infection destroying white blood cells? have
the same bag-of-words representation, the former is
a positive reaction while the later is very negative.
More advanced methods such as (Nakagawa et al,
IndicesWords
Semantic Representations
Recursive Autoencoder
i         walked      into         a        parked     car
Sorry, Hugs      You Rock       Teehee    I Understand    Wow, Just Wow
Predicted Sentiment Distribution
Figure 1: Illustration of our recursive autoencoder archi-
tecture which learns semantic vector representations of
phrases. Word indices (orange) are first mapped into a
semantic vector space (blue). Then they are recursively
merged by the same autoencoder network into a fixed
length sentence representation. The vectors at each node
are used as features to predict a distribution over senti-
ment labels.
2010) that can capture such phenomena use many
manually constructed resources (sentiment lexica,
parsers, polarity-shifting rules). This limits the ap-
plicability of these methods to a broader range of
tasks and languages. Lastly, almost all previous
work is based on single, positive/negative categories
or scales such as star ratings. Examples are movie
reviews (Pang and Lee, 2005), opinions (Wiebe et
al., 2005), customer reviews (Ding et al, 2008) or
multiple aspects of restaurants (Snyder and Barzilay,
2007). Such a one-dimensional scale does not accu-
rately reflect the complexity of human emotions and
sentiments.
In this work, we seek to address three issues. (i)
Instead of using a bag-of-words representation, our
model exploits hierarchical structure and uses com-
positional semantics to understand sentiment. (ii)
Our system can be trained both on unlabeled do-
main data and on supervised sentiment data and does
not require any language-specific sentiment lexica,
151
parsers, etc. (iii) Rather than limiting sentiment to
a positive/negative scale, we predict a multidimen-
sional distribution over several complex, intercon-
nected sentiments.
We introduce an approach based on semi-
supervised, recursive autoencoders (RAE) which
use as input continuous word vectors. Fig. 1 shows
an illustration of the model which learns vector rep-
resentations of phrases and full sentences as well as
their hierarchical structure from unsupervised text.
We extend our model to also learn a distribution over
sentiment labels at each node of the hierarchy.
We evaluate our approach on several standard
datasets where we achieve state-of-the art perfor-
mance. Furthermore, we show results on the re-
cently introduced experience project (EP) dataset
(Potts, 2010) that captures a broader spectrum of
human sentiments and emotions. The dataset con-
sists of very personal confessions anonymously
made by people on the experience project website
www.experienceproject.com. Confessions are la-
beled with a set of five reactions by other users. Re-
action labels are you rock (expressing approvement),
tehee (amusement), I understand, Sorry, hugs and
Wow, just wow (displaying shock). For evaluation on
this dataset we predict both the label with the most
votes as well as the full distribution over the senti-
ment categories. On both tasks our model outper-
forms competitive baselines. A set of over 31,000
confessions as well as the code of our model are
available at www.socher.org.
After describing the model in detail, we evalu-
ate it qualitatively by analyzing the learned n-gram
vector representations and compare quantitatively
against other methods on standard datasets and the
EP dataset.
2 Semi-Supervised Recursive
Autoencoders
Our model aims to find vector representations for
variable-sized phrases in either unsupervised or
semi-supervised training regimes. These representa-
tions can then be used for subsequent tasks. We first
describe neural word representations and then pro-
ceed to review a related recursive model based on
autoencoders, introduce our recursive autoencoder
(RAE) and describe how it can be modified to jointly
learn phrase representations, phrase structure and
sentiment distributions.
2.1 Neural Word Representations
We represent words as continuous vectors of param-
eters. We explore two settings. In the first setting
we simply initialize each word vector x ? Rn by
sampling it from a zero mean Gaussian distribution:
x ? N (0, ?2). These word vectors are then stacked
into a word embedding matrix L ? Rn?|V |, where
|V | is the size of the vocabulary. This initialization
works well in supervised settings where a network
can subsequently modify these vectors to capture
certain label distributions.
In the second setting, we pre-train the word vec-
tors with an unsupervised neural language model
(Bengio et al, 2003; Collobert and Weston, 2008).
These models jointly learn an embedding of words
into a vector space and use these vectors to predict
how likely a word occurs given its context. After
learning via gradient ascent the word vectors cap-
ture syntactic and semantic information from their
co-occurrence statistics.
In both cases we can use the resulting matrix of
word vectors L for subsequent tasks as follows. As-
sume we are given a sentence as an ordered list of
m words. Each word has an associated vocabulary
index k into the embedding matrix which we use to
retrieve the word?s vector representation. Mathemat-
ically, this look-up operation can be seen as a sim-
ple projection layer where we use a binary vector b
which is zero in all positions except at the kth index,
xi = Lbk ? Rn. (1)
In the remainder of this paper, we represent a sen-
tence (or any n-gram) as an ordered list of these
vectors (x1, . . . , xm). This word representation is
better suited to autoencoders than the binary number
representations used in previous related autoencoder
models such as the recursive autoassociative mem-
ory (RAAM) model (Pollack, 1990; Voegtlin and
Dominey, 2005) or recurrent neural networks (El-
man, 1991) since sigmoid units are inherently con-
tinuous. Pollack circumvented this problem by hav-
ing vocabularies with only a handful of words and
by manually defining a threshold to binarize the re-
sulting vectors.
152
x1 x3 x4x2
y1=f(W(1)[x3;x4] + b)
y2=f(W(1)[x2;y1] + b)
y3=f(W(1)[x1;y2] + b)
Figure 2: Illustration of an application of a recursive au-
toencoder to a binary tree. The nodes which are not filled
are only used to compute reconstruction errors. A stan-
dard autoencoder (in box) is re-used at each node of the
tree.
2.2 Traditional Recursive Autoencoders
The goal of autoencoders is to learn a representation
of their inputs. In this section we describe how to
obtain a reduced dimensional vector representation
for sentences.
In the past autoencoders have only been used in
setting where the tree structure was given a-priori.
We review this setting before continuing with our
model which does not require a given tree structure.
Fig. 2 shows an instance of a recursive autoencoder
(RAE) applied to a given tree. Assume we are given
a list of word vectors x = (x1, . . . , xm) as described
in the previous section as well as a binary tree struc-
ture for this input in the form of branching triplets
of parents with children: (p ? c1c2). Each child
can be either an input word vector xi or a nontermi-
nal node in the tree. For the example in Fig. 2, we
have the following triplets: ((y1 ? x3x4), (y2 ?
x2y1), (y1 ? x1y2)). In order to be able to apply
the same neural network to each pair of children, the
hidden representations yi have to have the same di-
mensionality as the xi?s.
Given this tree structure, we can now compute the
parent representations. The first parent vector y1 is
computed from the children (c1, c2) = (x3, x4):
p = f(W (1)[c1; c2] + b(1)), (2)
where we multiplied a matrix of parameters W (1) ?
Rn?2n by the concatenation of the two children.
After adding a bias term we applied an element-
wise activation function such as tanh to the result-
ing vector. One way of assessing how well this n-
dimensional vector represents its children is to try to
reconstruct the children in a reconstruction layer:
[
c?1; c?2
]
= W (2)p+ b(2). (3)
During training, the goal is to minimize the recon-
struction errors of this input pair. For each pair, we
compute the Euclidean distance between the original
input and its reconstruction:
Erec([c1; c2]) =
1
2
????[c1; c2]?
[
c?1; c?2
]????2 . (4)
This model of a standard autoencoder is boxed in
Fig. 2. Now that we have defined how an autoen-
coder can be used to compute an n-dimensional vec-
tor representation (p) of two n-dimensional children
(c1, c2), we can describe how such a network can be
used for the rest of the tree.
Essentially, the same steps repeat. Now that y1
is given, we can use Eq. 2 to compute y2 by setting
the children to be (c1, c2) = (x2, y1). Again, after
computing the intermediate parent vector y2, we can
assess how well this vector capture the content of
the children by computing the reconstruction error
as in Eq. 4. The process repeat until the full tree
is constructed and we have a reconstruction error at
each nonterminal node. This model is similar to the
RAAM model (Pollack, 1990) which also requires a
fixed tree structure.
2.3 Unsupervised Recursive Autoencoder for
Structure Prediction
Now, assume there is no tree structure given for
the input vectors in x. The goal of our structure-
prediction RAE is to minimize the reconstruction er-
ror of all vector pairs of children in a tree. We de-
fine A(x) as the set of all possible trees that can be
built from an input sentence x. Further, let T (y) be
a function that returns the triplets of a tree indexed
by s of all the non-terminal nodes in a tree. Using
the reconstruction error of Eq. 4, we compute
RAE?(x) = argmin
y?A(x)
?
s?T (y)
Erec([c1; c2]s) (5)
We now describe a greedy approximation that con-
structs such a tree.
153
Greedy Unsupervised RAE. For a sentence with
m words, we apply the autoencoder recursively. It
takes the first pair of neighboring vectors, defines
them as potential children of a phrase (c1; c2) =
(x1;x2), concatenates them and gives them as in-
put to the autoencoder. For each word pair, we save
the potential parent node p and the resulting recon-
struction error.
After computing the score for the first pair, the
network is shifted by one position and takes as input
vectors (c1, c2) = (x2, x3) and again computes a po-
tential parent node and a score. This process repeats
until it hits the last pair of words in the sentence:
(c1, c2) = (xm?1, xm). Next, it selects the pair
which had the lowest reconstruction error (Erec) and
its parent representation p will represent this phrase
and replace both children in the sentence word list.
For instance, consider the sequence (x1, x2, x3, x4)
and assume the lowestErec was obtained by the pair
(x3, x4). After the first pass, the new sequence then
consists of (x1, x2, p(3,4)). The process repeats and
treats the new vector p(3,4) like any other input vec-
tor. For instance, subsequent states could be either:
(x1, p(2,(3,4))) or (p(1,2), p(3,4)). Both states would
then finish with a deterministic choice of collapsing
the remaining two states into one parent to obtain
(p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively. The tree
is then recovered by unfolding the collapsing deci-
sions.
The resulting tree structure captures as much of
the single-word information as possible (in order
to allow reconstructing the word vectors) but does
not necessarily follow standard syntactic constraints.
We also experimented with a method that finds bet-
ter solutions to Eq. 5 based on CKY-like beam
search algorithms (Socher et al, 2010; Socher et al,
2011) but the performance is similar and the greedy
version is much faster.
Weighted Reconstruction. One problem with
simply using the reconstruction error of both chil-
dren equally as describe in Eq. 4 is that each child
could represent a different number of previously
collapsed words and is hence of bigger importance
for the overall meaning reconstruction of the sen-
tence. For instance in the case of (x1, p(2,(3,4)))
one would like to give more importance to recon-
structing p than x1. We capture this desideratum
by adjusting the reconstruction error. Let n1, n2 be
the number of words underneath a current poten-
tial child, we re-define the reconstruction error to be
Erec([c1; c2]; ?) =
n1
n1 + n2
????c1 ? c?1
????2 + n2n1 + n2
????c2 ? c?2
????2 (6)
Length Normalization. One of the goals of
RAEs is to induce semantic vector representations
that allow us to compare n-grams of different
lengths. The RAE tries to lower reconstruction error
of not only the bigrams but also of nodes higher in
the tree. Unfortunately, since the RAE computes the
hidden representations it then tries to reconstruct, it
can just lower reconstruction error by making the
hidden layer very small in magnitude. To prevent
such undesirable behavior, we modify the hidden
layer such that the resulting parent representation al-
ways has length one, after computing p as in Eq. 2,
we simply set: p = p||p|| .
2.4 Semi-Supervised Recursive Autoencoders
So far, the RAE was completely unsupervised and
induced general representations that capture the se-
mantics of multi-word phrases.In this section, we
extend RAEs to a semi-supervised setting in order
to predict a sentence- or phrase-level target distribu-
tion t.1
One of the main advantages of the RAE is that
each node of the tree built by the RAE has associ-
ated with it a distributed vector representation (the
parent vector p) which could also be seen as fea-
tures describing that phrase. We can leverage this
representation by adding on top of each parent node
a simple softmax layer to predict class distributions:
d(p; ?) = softmax(W labelp). (7)
Assuming there are K labels, d ? RK is
a K-dimensional multinomial distribution and?
k=1 dk = 1. Fig. 3 shows such a semi-supervised
RAE unit. Let tk be the kth element of the multino-
mial target label distribution t for one entry. The
softmax layer?s outputs are interpreted as condi-
tional probabilities dk = p(k|[c1; c2]), hence the
cross-entropy error is
EcE(p, t; ?) = ?
K?
k=1
tk log dk(p; ?). (8)
1For the binary label classification case, the distribution is
of the form [1, 0] for class 1 and [0, 1] for class 2.
154
R e c o n s t r u c t i o n  e r r o r            C r o s s - e n t r o p y e r r o r
W(1)
W(2) W(l a be l )
Figure 3: Illustration of an RAE unit at a nonterminal tree
node. Red nodes show the supervised softmax layer for
label distribution prediction.
Using this cross-entropy error for the label and the
reconstruction error from Eq. 6, the final semi-
supervised RAE objective over (sentences,label)
pairs (x, t) in a corpus becomes
J = 1N
?
(x,t)
E(x, t; ?) + ?2 ||?||
2, (9)
where we have an error for each entry in the training
set that is the sum over the error at the nodes of the
tree that is constructed by the greedy RAE:
E(x, t; ?) =
?
s?T (RAE?(x))
E([c1; c2]s, ps, t, ?).
The error at each nonterminal node is the weighted
sum of reconstruction and cross-entropy errors,
E([c1; c2]s, ps, t, ?) =
?Erec([c1; c2]s; ?) + (1? ?)EcE(ps, t; ?).
The hyperparameter ? weighs reconstruction and
cross-entropy error. When minimizing the cross-
entropy error of this softmax layer, the error will
backpropagate and influence both the RAE param-
eters and the word representations. Initially, words
such as good and bad have very similar representa-
tions. This is also the case for Brown clusters and
other methods that use only cooccurrence statistics
in a small window around each word. When learn-
ing with positive/negative sentiment, the word em-
beddings get modified and capture less syntactic and
more sentiment information.
In order to predict the sentiment distribution of a
sentence with this model, we use the learned vector
representation of the top tree node and train a simple
logistic regression classifier.
3 Learning
Let ? = (W (1), b(1),W (2), b(1),W label, L) be the set
of our model parameters, then the gradient becomes:
?J
?? =
1
N
?
(x,t)
?E(x, t; ?)
?? + ??. (10)
To compute this gradient, we first greedily construct
all trees and then derivatives for these trees are com-
puted efficiently via backpropagation through struc-
ture (Goller and Ku?chler, 1996). Because the algo-
rithm is greedy and the derivatives of the supervised
cross-entropy error also modify the matrix W (1),
this objective is not necessarily continuous and a
step in the gradient descent direction may not nec-
essarily decrease the objective. However, we found
that L-BFGS run over the complete training data
(batch mode) to minimize the objective works well
in practice, and that convergence is smooth, with the
algorithm typically finding a good solution quickly.
4 Experiments
We first describe the new experience project (EP)
dataset, results of standard classification tasks on
this dataset and how to predict its sentiment label
distributions. We then show results on other com-
monly used datasets and conclude with an analysis
of the important parameters of the model.
In all experiments involving our model, we repre-
sent words using 100-dimensional word vectors. We
explore the two settings mentioned in Sec. 2.1. We
compare performance on standard datasets when us-
ing randomly initialized word vectors (random word
init.) or word vectors trained by the model of Col-
lobert and Weston (2008) and provided by Turian
et al (2010).2 These vectors were trained on an
unlabeled corpus of the English Wikipedia. Note
that alternatives such as Brown clusters are not suit-
able since they do not capture sentiment information
(good and bad are usually in the same cluster) and
cannot be modified via backpropagation.
2http://metaoptimize.com/projects/
wordreprs/
155
Corpus K Instances Distr.(+/-) Avg|W |
MPQA 2 10,624 0.31/0.69 3
MR 2 10,662 0.5/0.5 22
EP 5 31,675 .2/.2/.1/.4/.1 113
EP? 4 5 6,129 .2/.2/.1/.4/.1 129
Table 1: Statistics on the different datasets. K is the num-
ber of classes. Distr. is the distribution of the different
classes (in the case of 2, the positive/negative classes, for
EP the rounded distribution of total votes in each class).
|W | is the average number of words per instance. We use
EP? 4, a subset of entries with at least 4 votes.
4.1 EP Dataset: The Experience Project
The confessions section of the experience project
website3 lets people anonymously write short per-
sonal stories or ?confessions?. Once a story is on
the site, each user can give a single vote to one of
five label categories (with our interpretation):
1 Sorry, Hugs: User offers condolences to author.
2. You Rock: Indicating approval, congratulations.
3. Teehee: User found the anecdote amusing.
4. I Understand: Show of empathy.
5. Wow, Just Wow: Expression of surprise,shock.
The EP dataset has 31,676 confession entries, a to-
tal number of 74,859 votes for the 5 labels above, the
average number of votes per entry is 2.4 (with a vari-
ance of 33). For the five categories, the numbers of
votes are [14, 816; 13, 325; 10, 073; 30, 844; 5, 801].
Since an entry with less than 4 votes is not very well
identified, we train and test only on entries with at
least 4 total votes. There are 6,129 total such entries.
The distribution over total votes in the 5 classes
is similar: [0.22; 0.2; 0.11; 0.37; 0.1]. The average
length of entries is 129 words. Some entries con-
tain multiple sentences. In these cases, we average
the predicted label distributions from the sentences.
Table 1 shows statistics of this and other commonly
used sentiment datasets (which we compare on in
later experiments). Table 2 shows example entries
as well as gold and predicted label distributions as
described in the next sections.
Compared to other datasets, the EP dataset con-
tains a wider range of human emotions that goes far
beyond positive/negative product or movie reviews.
Each item is labeled with a multinomial distribu-
3http://www.experienceproject.com/
confessions.php
tion over interconnected response categories. This
is in contrast to most other datasets (including multi-
aspect rating) where several distinct aspects are rated
independently but on the same scale. The topics
range from generic happy statements, daily clumsi-
ness reports, love, loneliness, to relationship abuse
and suicidal notes. As is evident from the total num-
ber of label votes, the most common user reaction
is one of empathy and an ability to relate to the au-
thors experience. However, some stories describe
horrible scenarios that are not common and hence
receive more offers of condolence. In the following
sections we show some examples of stories with pre-
dicted and true distributions but refrain from listing
the most horrible experiences.
For all experiments on the EP dataset, we split the
data into train (49%), development (21%) and test
data (30%).
4.2 EP: Predicting the Label with Most Votes
The first task for our evaluation on the EP dataset is
to simply predict the single class that receives the
most votes. In order to compare our novel joint
phrase representation and classifier learning frame-
work to traditional methods, we use the following
baselines:
Random Since there are five classes, this gives 20%
accuracy.
Most Frequent Selecting the class which most fre-
quently has the most votes (the class I under-
stand).
Baseline 1: Binary BoW This baseline uses logis-
tic regression on binary bag-of-word represen-
tations that are 1 if a word is present and 0 oth-
erwise.
Baseline 2: Features This model is similar to tra-
ditional approaches to sentiment classification
in that it uses many hand-engineered resources.
We first used a spell-checker and Wordnet to
map words and their misspellings to synsets to
reduce the total number of words. We then re-
placed sentiment words with a sentiment cat-
egory identifier using the sentiment lexica of
the Harvard Inquirer (Stone, 1966) and LIWC
(Pennebaker et al, 2007). Lastly, we used tf-idf
weighting on the bag-of-word representations
and trained an SVM.156
KL Predicted&Gold V. Entry (Shortened if it ends with ...)
.03
.16 .16 .16 .33 .16
6 I reguarly shoplift. I got caught once and went to jail, but I?ve found that this was not a deterrent. I don?t buy
groceries, I don?t buy school supplies for my kids, I don?t buy gifts for my kids, we don?t pay for movies, and I
dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...
.03
.38 .04 .06 .35 .14
165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1
hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights.
i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict...
.05
.14 .28 .14 .28 .14
7 Hi there, Im a guy that loves a girl, the same old bloody story... I met her a while ago, while studying, she Is so
perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so
did I with her, she has been the first person, male or female that has ever made that bond with me,...
.07
.27 .18 .00 .45 .09
11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i?ve ruined everything. i?ve
piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn?t feel because
i?ve never had you close enough. we?ve never touched, but i still feel as though a part of me is missing. ...
.05 23 Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more
and more frustrated that I have not found you yet. I?m also tired of spending so much heart on an old dream. ...
.05 5 I wish I knew somone to talk to here.
.06 24 I loved her but I screwed it up. Now she?s moved on. I?ll never have her again. I don?t know if I?ll ever stop
thinking about her.
.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do?s not care about how it affects me or my
sisters i want to care but the truthis i dont care if he dies
.13 6 well i think hairy women are attractive
.35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard. I need it. It?ll make my soul
feel a bit better :)
.36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12
years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just
can?t meet men. (before you judge, no Im not terribly picky!) What is wrong with me?
.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy. Then the teacher found out
it was one of us and made us go two days without freetime. It might be a little late now, but sorry guys it was
me haha
.92 4 My paper is due in less than 24 hours and I?m still dancing round my room!
Table 2: Example EP confessions from the test data with KL divergence between our predicted distribution (light blue,
left bar on each of the 5 classes) and ground truth distribution (red bar and numbers underneath), number of votes. The
5 classes are [Sorry, Hugs ;You Rock; Teehee;I Understand;Wow, Just Wow]. Even when the KL divergence is higher,
our model makes reasonable alternative label choices. Some entries are shortened.
Baseline 3: Word Vectors We can ignore the RAE
tree structure and only train softmax layers di-
rectly on the pre-trained words in order to influ-
ence the word vectors. This is followed by an
SVM trained on the average of the word vec-
tors.
We also experimented with latent Dirichlet aloca-
tion (Blei et al, 2003) but performance was very
low.
Table 3 shows the results for predicting the class
with the most votes. Even the approach that is based
on sentiment lexica and other resources is outper-
formed by our model by almost 3%, showing that
for tasks involving complex broad-range human sen-
timent, the often used sentiment lexica lack in cover-
age and traditional bag-of-words representations are
not powerful enough.
4.3 EP: Predicting Sentiment Distributions
We now turn to evaluating our distribution-
prediction approach. In both this and the previous
Method Accuracy
Random 20.0
Most Frequent 38.1
Baseline 1: Binary BoW 46.4
Baseline 2: Features 47.0
Baseline 3: Word Vectors 45.5
RAE (our method) 50.1
Table 3: Accuracy of predicting the class with most votes.
maximum label task, we backprop using the gold
multinomial distribution as a target. Since we max-
imize likelihood and because we want to predict a
distribution that is closest to the distribution of labels
that people would assign to a story, we evaluate us-
ing KL divergence: KL(g||p) = ?i gi log(gi/pi),
where g is the gold distribution and p is the predicted
one. We report the average KL divergence, where a
smaller value indicates better predictive power. To
get an idea of the values of KL divergence, predict-
157
Avg.Distr. BoW Features Word Vec. RAE0.6
0.7
0.8
0.83 0.81 0.72 0.73 0.70
Figure 4: Average KL-divergence between gold and pre-
dicted sentiment distributions (lower is better).
ing random distributions gives a an average of 1.2 in
KL divergence, predicting simply the average distri-
bution in the training data give 0.83. Fig. 4 shows
that our RAE-based model outperforms the other
baselines. Table 2 shows EP example entries with
predicted and gold distributions, as well as numbers
of votes.
4.4 Binary Polarity Classification
In order to compare our approach to other meth-
ods we also show results on commonly used sen-
timent datasets: movie reviews4 (MR) (Pang and
Lee, 2005) and opinions5 (MPQA) (Wiebe et al,
2005).We give statistical information on these and
the EP corpus in Table 1.
We compare to the state-of-the-art system of
(Nakagawa et al, 2010), a dependency tree based
classification method that uses CRFs with hidden
variables. We use the same training and testing regi-
men (10-fold cross validation) as well as their base-
lines: majority phrase voting using sentiment and
reversal lexica; rule-based reversal using a depen-
dency tree; Bag-of-Features and their full Tree-CRF
model. As shown in Table 4, our algorithm outper-
forms their approach on both datasets. For the movie
review (MR) data set, we do not use any hand-
designed lexica. An error analysis on the MPQA
dataset showed several cases of single words which
never occurred in the training set. Correctly classify-
ing these instances can only be the result of having
them in the original sentiment lexicon. Hence, for
the experiment on MPQA we added the same sen-
timent lexicon that (Nakagawa et al, 2010) used in
their system to our training set. This improved ac-
curacy from 86.0 to 86.4.Using the pre-trained word
vectors boosts performance by less than 1% com-
4www.cs.cornell.edu/people/pabo/
movie-review-data/
5www.cs.pitt.edu/mpqa/
Method MR MPQA
Voting with two lexica 63.1 81.7
Rule-based reversal on trees 62.9 82.8
Bag of features with reversal 76.4 84.1
Tree-CRF (Nakagawa et al?10) 77.3 86.1
RAE (random word init.) 76.8 85.7
RAE (our method) 77.7 86.4
Table 4: Accuracy of sentiment classification on movie
review polarity (MR) and the MPQA dataset.
0 0.2 0.4 0.6 0.8 10.83
0.84
0.85
0.86
0.87
Figure 5: Accuracy on the development split of the MR
polarity dataset for different weightings of reconstruction
error and supervised cross-entropy error: err = ?Erec+
(1? ?)EcE .
pared to randomly initialized word vectors (setting:
random word init). This shows that our method can
work well even in settings with little training data.
We visualize the semantic vectors that the recursive
autoencoder learns by listing n-grams that give the
highest probability for each polarity. Table 5 shows
such n-grams for different lengths when the RAE is
trained on the movie review polarity dataset.
On a 4-core machine, training time for the smaller
corpora such as the movie reviews takes around 3
hours and for the larger EP corpus around 12 hours
until convergence. Testing of hundreds of movie re-
views takes only a few seconds.
4.5 Reconstruction vs. Classification Error
In this experiment, we show how the hyperparame-
ter ? influences accuracy on the development set of
one of the cross-validation splits of the MR dataset.
This parameter essentially trade-off the supervised
and unsupervised parts of the objective. Fig. 5 shows
that a larger focus on the supervised objective is im-
portant but that a weight of ? = 0.2 for the recon-
struction error prevents overfitting and achieves the
highest performance.
158
n Most negative n-grams Most positive n-grams
1 bad; boring; dull; flat; pointless; tv; neither; pretentious; badly;
worst; lame; mediocre; lack; routine; loud; bore; barely; stupid;
tired; poorly; suffers; heavy;nor; choppy; superficial
touching; enjoyable; powerful; warm; moving; culture; flaws;
provides; engrossing; wonderful; beautiful; quiet; socio-political;
thoughtful; portrait; refreshingly; chilling; rich; beautifully; solid;
2 how bad; by bad; dull .; for bad; to bad; boring .; , dull; are bad;
that bad; boring ,; , flat; pointless .; badly by; on tv; so routine; lack
the; mediocre .; a generic; stupid ,; abysmally pathetic
the beautiful; moving,; thoughtful and; , inventive; solid and; a
beautiful; a beautifully; and hilarious; with dazzling; provides the;
provides.; and inventive; as powerful; moving and; a moving; a
powerful
3 . too bad; exactly how bad; and never dull; shot but dull; is more
boring; to the dull; dull, UNK; it is bad; or just plain; by turns
pretentious; manipulative and contrived; bag of stale; is a bad; the
whole mildly; contrived pastiche of; from this choppy; stale mate-
rial.
funny and touching; a small gem; with a moving; cuts, fast; , fine
music; smart and taut; culture into a; romantic , riveting; ... a solid;
beautifully acted .; , gradually reveals; with the chilling; cast of
solid; has a solid; spare yet audacious; ... a polished; both the
beauty;
5 boring than anything else.; a major waste ... generic; nothing i
hadn?t already; ,UNK plotting;superficial; problem ? no laughs.;
,just horribly mediocre .; dull, UNK feel.; there?s nothing exactly
wrong; movie is about a boring; essentially a collection of bits
reminded us that a feel-good; engrossing, seldom UNK,; between
realistic characters showing honest; a solid piece of journalistic;
easily the most thoughtful fictional; cute, funny, heartwarming;
with wry humor and genuine; engrossing and ultimately tragic.;
8 loud, silly, stupid and pointless.; dull, dumb and derivative horror
film.; UNK?s film, a boring, pretentious; this film biggest problem
? no laughs.; film in the series looks and feels tired; do draw easy
chuckles but lead nowhere.; stupid, infantile, redundant, sloppy
shot in rich , shadowy black-and-white , devils an escapist con-
fection that ?s pure entertainment .; , deeply absorbing piece that
works as a; ... one of the most ingenious and entertaining; film is a
riveting , brisk delight .; bringing richer meaning to the story ?s;
Table 5: Examples of n-grams (n = 1, 2, 3, 5, 8) from the test data of the movie polarity dataset for which our model
predicts the most positive and most negative responses.
5 Related Work
5.1 Autoencoders and Deep Learning
Autoencoders are neural networks that learn a re-
duced dimensional representation of fixed-size in-
puts such as image patches or bag-of-word repre-
sentations of text documents. They can be used to
efficiently learn feature encodings which are useful
for classification. Recently, Mirowski et al (2010)
learn dynamic autoencoders for documents in a bag-
of-words format which, like ours, combine super-
vised and reconstruction objectives.
The idea of applying an autoencoder in a recursive
setting was introduced by Pollack (1990). Pollack?s
recursive auto-associative memories (RAAMs) are
similar to ours in that they are a connectionst, feed-
forward model. However, RAAMs learn vector
representations only for fixed recursive data struc-
tures, whereas our RAE builds this recursive data
structure. More recently, (Voegtlin and Dominey,
2005) introduced a linear modification to RAAMs
that is able to better generalize to novel combina-
tions of previously seen constituents. One of the
major shortcomings of previous applications of re-
cursive autoencoders to natural language sentences
was their binary word representation as discussed in
Sec. 2.1.
Recently, (Socher et al, 2010; Socher et al, 2011)
introduced a max-margin framework based on recur-
sive neural networks (RNNs) for labeled structure
prediction. Their models are applicable to natural
language and computer vision tasks such as parsing
or object detection. The current work is related in
that it uses a recursive deep learning model. How-
ever, RNNs require labeled tree structures and use a
supervised score at each node. Instead, RAEs learn
hierarchical structures that are trying to capture as
much of the the original word vectors as possible.
The learned structures are not necessarily syntacti-
cally plausible but can capture more of the semantic
content of the word vectors. Other recent deep learn-
ing methods for sentiment analysis include (Maas et
al., 2011).
5.2 Sentiment Analysis
Pang et al (2002) were one of the first to experiment
with sentiment classification. They show that sim-
ple bag-of-words approaches based on Naive Bayes,
MaxEnt models or SVMs are often insufficient for
predicting sentiment of documents even though they
work well for general topic-based document classi-
fication. Even adding specific negation words, bi-
grams or part-of-speech information to these mod-
els did not add significant improvements. Other
document-level sentiment work includes (Turney,
2002; Dave et al, 2003; Beineke et al, 2004; Pang
and Lee, 2004). For further references, see (Pang
and Lee, 2008).
Instead of document level sentiment classifica-
tion, (Wilson et al, 2005) analyze the contextual
polarity of phrases and incorporate many well de-
signed features including dependency trees. They
also show improvements by first distinguishing be-
159
tween neutral and polar sentences. Our model natu-
rally incorporates the recursive interaction between
context and polarity words in sentences in a unified
framework while simultaneously learning the neces-
sary features to make accurate predictions. Other ap-
proaches for sentence-level sentiment detection in-
clude (Yu and Hatzivassiloglou, 2003; Grefenstette
et al, 2004; Ikeda et al, 2008).
Most previous work is centered around a given
sentiment lexicon or building one via heuristics
(Kim and Hovy, 2007; Esuli and Sebastiani, 2007),
manual annotation (Das and Chen, 2001) or machine
learning techniques (Turney, 2002). In contrast, we
do not require an initial or constructed sentiment lex-
icon of positive and negative words. In fact, when
training our approach on documents or sentences, it
jointly learns such lexica for both single words and
n-grams (see Table 5). (Mao and Lebanon, 2007)
propose isotonic conditional random fields and dif-
ferentiate between local, sentence-level and global,
document-level sentiment.
The work of (Polanyi and Zaenen, 2006; Choi and
Cardie, 2008) focuses on manually constructing sev-
eral lexica and rules for both polar words and re-
lated content-word negators, such as ?prevent can-
cer?, where prevent reverses the negative polarity of
cancer. Like our approach they capture composi-
tional semantics. However, our model does so with-
out manually constructing any rules or lexica.
Recently, (Velikovich et al, 2010) showed how to
use a seed lexicon and a graph propagation frame-
work to learn a larger sentiment lexicon that also in-
cludes polar multi-word phrases such as ?once in a
life time?. While our method can also learn multi-
word phrases it does not require a seed set or a large
web graph. (Nakagawa et al, 2010) introduced an
approach based on CRFs with hidden variables with
very good performance. We compare to their state-
of-the-art system. We outperform them on the stan-
dard corpora that we tested on without requiring
external systems such as POS taggers, dependency
parsers and sentiment lexica. Our approach jointly
learns the necessary features and tree structure.
In multi-aspect rating (Snyder and Barzilay, 2007)
one finds several distinct aspects such as food or ser-
vice in a restaurant and then rates them on a fixed
linear scale such as 1-5 stars, where all aspects could
obtain just 1 star or all aspects could obtain 5 stars
independently. In contrast, in our method a single
aspect (a complex reaction to a human experience)
is predicted not in terms of a fixed scale but in terms
of a multinomial distribution over several intercon-
nected, sometimes mutually exclusive emotions. A
single story cannot simultaneously obtain a strong
reaction in different emotional responses (by virtue
of having to sum to one).
6 Conclusion
We presented a novel algorithm that can accurately
predict sentence-level sentiment distributions. With-
out using any hand-engineered resources such as
sentiment lexica, parsers or sentiment shifting rules,
our model achieves state-of-the-art performance on
commonly used sentiment datasets. Furthermore,
we introduce a new dataset that contains distribu-
tions over a broad range of human emotions. Our
evaluation shows that our model can more accu-
rately predict these distributions than other models.
Acknowledgments
We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of DARPA, AFRL, or
the US government. This work was also supported in part
by the DARPA Deep Learning program under contract
number FA8650-10-C-7020.
We thank Chris Potts for help with the EP data set, Ray-
mond Hsu, Bozhi See, and Alan Wu for letting us use
their system as a baseline and Jiquan Ngiam, Quoc Le,
Gabor Angeli and Andrew Maas for their feedback.
References
P. Beineke, T. Hastie, C. D. Manning, and
S. Vaithyanathan. 2004. Exploring sentiment
summarization. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning Re-
search., 3:993?1022.
160
Y. Choi and C. Cardie. 2008. Learning with composi-
tional semantics as structural inference for subsenten-
tial sentiment analysis. In EMNLP.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
S. Das and M. Chen. 2001. Yahoo! for Amazon: Ex-
tracting market sentiment from stock message boards.
In Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proceedings of
WWW, pages 519?528.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings of
the Conference on Web Search and Web Data Mining
(WSDM).
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3):195?225.
A. Esuli and F. Sebastiani. 2007. Pageranking word-
net synsets: An application to opinion mining. In
Proceedings of the Association for Computational Lin-
guistics (ACL).
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
G. Grefenstette, Y. Qu, J. G. Shanahan, and D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of Recherche d?Information Assiste?e par Ordinateur
(RIAO).
D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to shift the polarity of words for senti-
ment classification. In IJCNLP.
S. Kim and E. Hovy. 2007. Crystal: Analyzing predic-
tive opinions on the web. In EMNLP-CoNLL.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng,
and C. Potts. 2011. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of ACL.
Y. Mao and G. Lebanon. 2007. Isotonic Conditional
Random Fields and Local Sentiment Flow. In NIPS.
P. Mirowski, M. Ranzato, and Y. LeCun. 2010. Dynamic
auto-encoders for semantic indexing. In Proceedings
of the NIPS 2010 Workshop on Deep Learning.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115?124.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In EMNLP.
J. W. Pennebaker, R.J. Booth, and M. E. Francis. 2007.
Linguistic inquiry and word count: Liwc2007 opera-
tors manual. University of Texas.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46:77?105, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the Good Grief algorithm. In HLT-NAACL.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning.
2011. Parsing Natural Scenes and Natural Language
with Recursive Neural Networks. In ICML.
P. J. Stone. 1966. The General Inquirer: A Computer
Approach to Content Analysis. The MIT Press.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification of
reviews. In ACL.
L. Velikovich, S. Blair-Goldensohn, K. Hannan, and
R. McDonald. 2010. The viability of web-derived po-
larity lexicons. In NAACL, HLT.
T. Voegtlin and P. Dominey. 2005. Linear Recursive Dis-
tributed Representations. Neural Networks, 18(7).
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In HLT/EMNLP.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
EMNLP.
161
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1201?1211, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Semantic Compositionality through Recursive Matrix-Vector Spaces
Richard Socher Brody Huval Christopher D. Manning Andrew Y. Ng
richard@socher.org, {brodyh,manning,ang}@stanford.edu
Computer Science Department, Stanford University
Abstract
Single-word vector space models have been
very successful at learning lexical informa-
tion. However, they cannot capture the com-
positional meaning of longer phrases, prevent-
ing them from a deeper understanding of lan-
guage. We introduce a recursive neural net-
work (RNN) model that learns compositional
vector representations for phrases and sen-
tences of arbitrary syntactic type and length.
Our model assigns a vector and a matrix to ev-
ery node in a parse tree: the vector captures
the inherent meaning of the constituent, while
the matrix captures how it changes the mean-
ing of neighboring words or phrases. This
matrix-vector RNN can learn the meaning of
operators in propositional logic and natural
language. The model obtains state of the art
performance on three different experiments:
predicting fine-grained sentiment distributions
of adverb-adjective pairs; classifying senti-
ment labels of movie reviews and classifying
semantic relationships such as cause-effect or
topic-message between nouns using the syn-
tactic path between them.
1 Introduction
Semantic word vector spaces are at the core of many
useful natural language applications such as search
query expansions (Jones et al 2006), fact extrac-
tion for information retrieval (Pas?ca et al 2006)
and automatic annotation of text with disambiguated
Wikipedia links (Ratinov et al 2011), among many
others (Turney and Pantel, 2010). In these mod-
els the meaning of a word is encoded as a vector
computed from co-occurrence statistics of a word
and its neighboring words. Such vectors have been
shown to correlate well with human judgments of
word similarity (Griffiths et al 2007).
?      very                        good                           movie          ...       (  a  ,  A  )                (  b  ,  B  )                     (  c  ,  C  )  
Recursive Matrix-Vector Model
f(Ba, Ab)=
 Ba=                        Ab=
- vector
- matrix...
?
Figure 1: A recursive neural network which learns se-
mantic vector representations of phrases in a tree struc-
ture. Each word and phrase is represented by a vector
and a matrix, e.g., very = (a,A). The matrix is applied
to neighboring vectors. The same function is repeated to
combine the phrase very good with movie.
Despite their success, single word vector models
are severely limited since they do not capture com-
positionality, the important quality of natural lan-
guage that allows speakers to determine the meaning
of a longer expression based on the meanings of its
words and the rules used to combine them (Frege,
1892). This prevents them from gaining a deeper
understanding of the semantics of longer phrases or
sentences. Recently, there has been much progress
in capturing compositionality in vector spaces, e.g.,
(Mitchell and Lapata, 2010; Baroni and Zamparelli,
2010; Zanzotto et al 2010; Yessenalina and Cardie,
2011; Socher et al 2011c) (see related work). We
extend these approaches with a more general and
powerful model of semantic composition.
We present a novel recursive neural network
model for semantic compositionality. In our context,
compositionality is the ability to learn compositional
vector representations for various types of phrases
and sentences of arbitrary length. Fig. 1 shows an
illustration of the model in which each constituent
(a word or longer phrase) has a matrix-vector (MV)
1201
representation. The vector captures the meaning of
that constituent. The matrix captures how it modifies
the meaning of the other word that it combines with.
A representation for a longer phrase is computed
bottom-up by recursively combining the words ac-
cording to the syntactic structure of a parse tree.
Since the model uses the MV representation with a
neural network as the final merging function, we call
our model a matrix-vector recursive neural network
(MV-RNN).
We show that the ability to capture semantic com-
positionality in a syntactically plausible way trans-
lates into state of the art performance on various
tasks. The first experiment demonstrates that our
model can learn fine-grained semantic composition-
ality. The task is to predict a sentiment distribution
over movie reviews of adverb-adjective pairs such as
unbelievably sad or really awesome. The MV-RNN
is the only model that is able to properly negate sen-
timent when adjectives are combined with not. The
MV-RNN outperforms previous state of the art mod-
els on full sentence sentiment prediction of movie
reviews. The last experiment shows that the MV-
RNN can also be used to find relationships between
words using the learned phrase vectors. The rela-
tionship between words is recursively constructed
and composed by words of arbitrary type in the
variable length syntactic path between them. On
the associated task of classifying relationships be-
tween nouns in arbitrary positions of a sentence the
model outperforms all previous approaches on the
SemEval-2010 Task 8 competition (Hendrickx et al
2010). It outperforms all but one of the previous ap-
proaches without using any hand-designed semantic
resources such as WordNet or FrameNet. By adding
WordNet hypernyms, POS and NER tags our model
outperforms the state of the art that uses significantly
more resources. The code for our model is available
at www.socher.org.
2 MV-RNN: A Recursive Matrix-Vector
Model
The dominant approach for building representations
of multi-word units from single word vector repre-
sentations has been to form a linear combination of
the single word representations, such as a sum or
weighted average. This happens in information re-
trieval and in various text similarity functions based
on lexical similarity. These approaches can work
well when the meaning of a text is literally ?the sum
of its parts?, but fails when words function as oper-
ators that modify the meaning of another word: the
meaning of ?extremely strong? cannot be captured
as the sum of word representations for ?extremely?
and ?strong.?
The model of Socher et al(2011c) provided a
new possibility for moving beyond a linear combi-
nation, through use of a matrix W that multiplied
the word vectors (a, b), and a nonlinearity function
g (such as a sigmoid or tanh). They compute the
parent vector p that describes both words as
p = g
(
W
[
a
b
])
(1)
and apply this function recursively inside a binarized
parse tree so that it can compute vectors for multi-
word sequences. Even though the nonlinearity al-
lows to express a wider range of functions, it is al-
most certainly too much to expect a single fixed W
matrix to be able to capture the meaning combina-
tion effects of all natural language operators. After
all, inside the function g, we have the same linear
transformation for all possible pairs of word vectors.
Recent work has started to capture the behavior
of natural language operators inside semantic vec-
tor spaces by modeling them as matrices, which
would allow a matrix for ?extremely? to appropri-
ately modify vectors for ?smelly? or ?strong? (Ba-
roni and Zamparelli, 2010; Zanzotto et al 2010).
These approaches are along the right lines but so
far have been restricted to capture linear functions
of pairs of words whereas we would like nonlinear
functions to compute compositional meaning repre-
sentations for multi-word phrases or full sentences.
The MV-RNN combines the strengths of both of
these ideas by (i) assigning a vector and a matrix to
every word and (ii) learning an input-specific, non-
linear, compositional function for computing vector
and matrix representations for multi-word sequences
of any syntactic type. Assigning vector-matrix rep-
resentations to all words instead of only to words of
one part of speech category allows for greater flex-
ibility which benefits performance. If a word lacks
operator semantics, its matrix can be an identity ma-
trix. However, if a word acts mainly as an operator,
1202
such as ?extremely?, its vector can become close to
zero, while its matrix gains a clear operator mean-
ing, here magnifying the meaning of the modified
word in both positive and negative directions.
In this section we describe the initial word rep-
resentations, the details of combining two words as
well as the multi-word extensions. This is followed
by an explanation of our training procedure.
2.1 Matrix-Vector Neural Word Representation
We represent a word as both a continuous vector
and a matrix of parameters. We initialize all word
vectors x ? Rn with pre-trained 50-dimensional
word vectors from the unsupervised model of Col-
lobert and Weston (2008). Using Wikipedia text,
their model learns word vectors by predicting how
likely it is for each word to occur in its context. Sim-
ilar to other local co-occurrence based vector space
models, the resulting word vectors capture syntactic
and semantic information. Every word is also asso-
ciated with a matrix X . In all experiments, we ini-
tialize matrices as X = I+ , i.e., the identity plus a
small amount of Gaussian noise. If the vectors have
dimensionality n, then each word?s matrix has di-
mensionality X ? Rn?n. While the initialization is
random, the vectors and matrices will subsequently
be modified to enable a sequence of words to com-
pose a vector that can predict a distribution over se-
mantic labels. Henceforth, we represent any phrase
or sentence of length m as an ordered list of vector-
matrix pairs ((a,A), . . . , (m,M)), where each pair
is retrieved based on the word at that position.
2.2 Composition Models for Two Words
We first review composition functions for two
words. In order to compute a parent vector p from
two consecutive words and their respective vectors
a and b, Mitchell and Lapata (2010) give as their
most general function: p = f(a, b, R,K),where R
is the a-priori known syntactic relation and K is
background knowledge.
There are many possible functions f . For our
models, there is a constraint on p which is that it
has the same dimensionality as each of the input
vectors. This way, we can compare p easily with
its children and p can be the input to a composition
with another word. The latter is a requirement that
will become clear in the next section. This excludes
tensor products which were outperformed by sim-
pler weighted addition and multiplication methods
in (Mitchell and Lapata, 2010).
We will explore methods that do not require
any manually designed semantic resources as back-
ground knowledge K. No explicit knowledge about
the type of relation R is used. Instead we want the
model to capture this implicitly via the learned ma-
trices. We propose the following combination func-
tion which is input dependent:
p = fA,B(a, b) = f(Ba,Ab) = g
(
W
[
Ba
Ab
])
,
(2)
whereA,B are matrices for single words, the global
W ? Rn?2n is a matrix that maps both transformed
words back into the same n-dimensional space. The
element-wise function g could be simply the identity
function but we use instead a nonlinearity such as
the sigmoid or hyperbolic tangent tanh. Such a non-
linearity will allow us to approximate a wider range
of functions beyond purely linear functions. We can
also add a bias term before applying g but omit this
for clarity. Rewriting the two transformed vectors as
one vector z, we get p = g(Wz) which is a single
layer neural network. In this model, the word ma-
trices can capture compositional effects specific to
each word, whereas W captures a general composi-
tion function.
This function builds upon and generalizes several
recent models in the literature. The most related
work is that of (Mitchell and Lapata, 2010; Zan-
zotto et al 2010) who introduced and explored the
composition function p = Ba + Ab for word pairs.
This model is a special case of Eq. 2 when we set
W = [II] (i.e. two concatenated identity matri-
ces) and g(x) = x (the identity function). Baroni
and Zamparelli (2010) computed the parent vector
of adjective-noun pairs by p = Ab, where A is an
adjective matrix and b is a vector for a noun. This
cannot capture nouns modifying other nouns, e.g.,
disk drive. This model too is a special case of the
above model with B = 0n?n. Lastly, the models of
(Socher et al 2011b; Socher et al 2011c; Socher et
al., 2011a) as described above are also special cases
with bothA andB set to the identity matrix. We will
compare to these special cases in our experiments.
1203
???????????????   (a , A)     (b , B)    (c , C)  
Matrix-Vector Recursive Neural Network
(p1 , P1)
( p2, P2  ) p2  = g(W                )
P2 =  WM
Cp1 P1c[   ]
P1C[   ]
Figure 2: Example of how the MV-RNN merges a phrase
with another word at a nonterminal node of a parse tree.
2.3 Recursive Compositions of Multiple Words
and Phrases
This section describes how we extend a word-pair
matrix-vector-based compositional model to learn
vectors and matrices for longer sequences of words.
The main idea is to apply the same function f to
pairs of constituents in a parse tree. For this to
work, we need to take as input a binary parse tree
of a phrase or sentence and also compute matrices at
each nonterminal parent node. The function f can
be readily used for phrase vectors since it is recur-
sively compatible (p has the same dimensionality as
its children). For computing nonterminal phrase ma-
trices, we define the function
P = fM (A,B) = WM
[
A
B
]
, (3)
where WM ? Rn?2n, so P ? Rn?n just like each
input matrix.
After two words form a constituent in the parse
tree, this constituent can now be merged with an-
other one by applying the same functions f and
fM . For instance, to compute the vectors and ma-
trices depicted in Fig. 2, we first merge words a
and b and their matrices: p1 = f(Ba,Ab), P1 =
fM (A,B). The resulting vector-matrix pair (p1, P1)
can now be used to compute the full phrase when
combining it with word c and computing p2 =
f(Cp1, P1c), P2 = fM (P1, C). The model com-
putes vectors and matrices in a bottom-up fashion,
applying the functions f, fM to its own previous out-
put (i.e. recursively) until it reaches the top node of
the tree which represents the entire sentence.
For experiments with longer sequences we will
compare to standard RNNs and the special case of
the MV-RNN that computes the parent by p = Ab+
Ba, which we name the linear Matrix-Vector Re-
cursion model (linear MVR). Previously, this model
had not been trained for multi-word sequences. Sec.
6 talks about alternatives for compositionality.
2.4 Objective Functions for Training
One of the advantages of RNN-based models is that
each node of a tree has associated with it a dis-
tributed vector representation (the parent vector p)
which can also be seen as features describing that
phrase. We train these representations by adding on
top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or
relationship classes: d(p) = softmax(W labelp). If
there are K labels, then d ? RK is a K-dimensional
multinomial distribution. For the applications below
(excluding logic), the corresponding error function
E(s, t, ?) that we minimize for a sentence s and its
tree t is the sum of cross-entropy errors at all nodes.
The only other methods that use this type of ob-
jective function are (Socher et al 2011b; Socher
et al 2011c), who also combine it with either a
score or reconstruction error. Hence, for compar-
isons to other related work, we need to merge vari-
ations of computing the parent vector p with this
classifier. The main difference is that the MV-RNN
has more flexibility since it has an input specific re-
cursive function fA,B to compute each parent. In
the following applications, we will use the softmax
classifier to predict both sentiment distributions and
noun-noun relationships.
2.5 Learning
Let ? = (W,WM ,W label, L, LM ) be our model pa-
rameters and ? a vector with regularization hyperpa-
rameters for all model parameters. L andLM are the
sets of all word vectors and word matrices. The gra-
dient of the overall objective function J becomes:
?J
??
=
1
N
?
(x,t)
?E(x, t; ?)
??
+ ??. (4)
To compute this gradient, we first compute all tree
nodes (pi, Pi) from the bottom-up and then take
derivatives of the softmax classifiers at each node
in the tree from the top down. Derivatives are com-
puted efficiently via backpropagation through struc-
ture (Goller and Ku?chler, 1996). Even though the
1204
objective is not convex, we found that L-BFGS run
over the complete training data (batch mode) mini-
mizes the objective well in practice and convergence
is smooth. For more information see (Socher et al
2010).
2.6 Low-Rank Matrix Approximations
If every word is represented by an n-dimensional
vector and additionally by an n ? n matrix, the di-
mensionality of the whole model may become too
large with commonly used vector sizes of n = 100.
In order to reduce the number of parameters, we rep-
resent word matrices by the following low-rank plus
diagonal approximation:
A = UV + diag(a), (5)
where U ? Rn?r, V ? Rr?n, a ? Rn and we set
the rank for all experiments to r = 3.
2.7 Discussion: Evaluation and Generality
Evaluation of compositional vector spaces is a com-
plex task. Most related work compares similarity
judgments of unsupervised models to those of hu-
man judgments and aims at high correlation. These
evaluations can give important insights. However,
even with good correlation the question remains
how these models would perform on downstream
NLP tasks such as sentiment detection. We ex-
perimented with unsupervised learning of general
vector-matrix representations by having the MV-
RNN predict words in their correct context. Ini-
tializing the models with these general representa-
tions, did not improve the performance on the tasks
we consider. For sentiment analysis, this is not sur-
prising since antonyms often get similar vectors dur-
ing unsupervised learning from co-occurrences due
to high similarity of local syntactic contexts. In our
experiments, the high prediction performance came
from supervised learning of meaning representations
using labeled data. While these representations are
task-specific, they could be used across tasks in a
multi-task learning setup. However, in order to fairly
compare to related work, we use only the super-
vised data of each task. Before we describe our full-
scale experiments, we analyze the model?s expres-
sive powers.
3 Model Analysis
This section analyzes the model with two proof-of-
concept studies. First, we examine its ability to learn
operator semantics for adverb-adjective pairs. If a
model cannot correctly capture how an adverb op-
erates on the meaning of adjectives, then there?s lit-
tle chance it can learn operators for more complex
relationships. The second study analyzes whether
the MV-RNN can learn simple boolean operators of
propositional logic such as conjunctives or negation
from truth values. Again, if a model did not have this
ability, then there?s little chance it could learn these
frequently occurring phenomena from the noisy lan-
guage of real texts such as movie reviews.
3.1 Predicting Sentiment Distributions of
Adverb-Adjective Pairs
The first study considers the prediction of fine-
grained sentiment distributions of adverb-adjective
pairs and analyzes different possibilities for com-
puting the parent vector p. The results show that
the MV-RNN operators are powerful enough to cap-
ture the operational meanings of various types of ad-
verbs. For example, very is an intensifier, pretty is an
attenuator, and not can negate or strongly attenuate
the positivity of an adjective. For instance not great
is still pretty good and not terrible; see Potts (2010)
for details.
We use a publicly available IMDB dataset of ex-
tracted adverb-adjective pairs from movie reviews.1
The dataset provides the distribution over star rat-
ings: Each consecutive word pair appears a certain
number of times in reviews that have also associ-
ated with them an overall rating of the movie. After
normalizing by the total number of occurrences, one
gets a multinomial distribution over ratings. Only
word pairs that appear at least 50 times are kept. Of
the remaining pairs, we use 4211 randomly sampled
ones for training and a separate set of 1804 for test-
ing. We never give the algorithm sentiment distribu-
tions for single words, and, while single words over-
lap between training and testing, the test set consists
of never before seen word pairs.
The softmax classifier is trained to minimize the
cross entropy error. Hence, an evaluation in terms of
KL-divergence is the most reasonable choice. It is
1http://compprag.christopherpotts.net/reviews.html
1205
Method Avg KL
Uniform 0.327
Mean train 0.193
p = 12(a+ b) 0.103
p = a? b 0.103
p = [a; b] 0.101
p = Ab 0.103
RNN 0.093
Linear MVR 0.092
MV-RNN 0.091
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 fairly annoying
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 fairly awesome
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 fairly sad
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 not annoying
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 not awesome
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 not sad
 
 Training Pair
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 unbelievably annoying
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 unbelievably awesome
 
 MV?RNNRNN
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5 unbelievably sad
 
 MV?RNNRNN
Figure 3: Left: Average KL-divergence for predicting sentiment distributions of unseen adverb-adjective pairs of the
test set. See text for p descriptions. Lower is better. The main difference in the KL divergence comes from the few
negation pairs in the test set. Right: Predicting sentiment distributions (over 1-10 stars on the x-axis) of adverb-
adjective pairs. Each row has the same adverb and each column the same adjective. Many predictions are similar
between the two models. The RNN and linear MVR are not able to modify the sentiment correctly: not awesome is
more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying. Predictions of the
linear MVR model are almost identical to the standard RNN for these examples.
defined as KL(g||p) =
?
i gi log(gi/pi), where g is
the gold distribution and p is the predicted one.
We compare to several baselines and ablations of
the MV-RNN model. An (adverb,adjective) pair is
described by its vectors (a, b) and matrices (A,B).
1 p = 0.5(a+ b), vector average
2. p = a? b, element-wise vector multiplication
3. p = [a; b], vector concatenation
4. p = Ab, similar to (Baroni and Lenci, 2010)
5. p = g(W [a; b]), RNN, similar to Socher et al
6. p = Ab+Ba, Linear MVR, similar to (Mitchell
and Lapata, 2010; Zanzotto et al 2010)
7. p = g(W [Ba;Ab]), MV-RNN
The final distribution is always predicted by a
softmax classifier whose inputs p vary for each of
the models. This objective function (see Sec. 2.4)
is different to all previously published work except
that of (Socher et al 2011c).
We cross-validated all models over regulariza-
tion parameters for word vectors, the softmax clas-
sifier, the RNN parameter W and the word op-
erators (10?4, 10?3) and word vector sizes (n =
6, 8, 10, 12, 15, 20). All models performed best at
vector sizes of below 12. Hence, it is the model?s
power and not the number of parameters that deter-
mines the performance. The table in Fig. 3 shows
the average KL-divergence on the test set. It shows
that the idea of matrix-vector representations for all
words and having a nonlinearity are both impor-
tant. The MV-RNN which combines these two ideas
is best able to learn the various compositional ef-
fects. The main difference in KL divergence comes
from the few negation cases in the test set. Fig. 3
shows examples of predicted distributions. Many
of the predictions are accurate and similar between
the top models. However, only the MV-RNN has
enough expressive power to allow negation to com-
pletely shift the sentiment with respect to an adjec-
tive. A negated adjective carrying negative senti-
ment becomes slightly positive, whereas not awe-
some is correctly attenuated. All three top models
correctly capture the U-shape of unbelievably sad.
This pair peaks at both the negative and positive
spectrum because it is ambiguous. When referring
to the performance of actors, it is very negative, but,
when talking about the plot, many people enjoy sad
and thought-provoking movies. The p = Ab model
does not perform well because it cannot model the
fact that for an adjective like ?sad,? the operator of
?unbelievably? behaves differently.
1206
false
false
? false
false
true
? false
false
false
? true
true
true
? true
true
? false
false
? true
Figure 4: Training trees for the MV-RNN to learn propositional operators. The model learns vectors and operators for
? (and) and ? (negation). The model outputs the exact representations of false and true respectively at the top node.
Hence, the operators can be combined recursively an arbitrary number of times for more complex logical functions.
3.2 Logic- and Vector-based Compositionality
Another natural question is whether the MV-RNN
can, in general, capture some of the simple boolean
logic that is sometimes found in language. In other
words, can it learn some of the propositional logic
operators such as and, or, not in terms of vectors and
matrices from a few examples. Answering this ques-
tion can also be seen as a first step towards bridg-
ing the gap between logic-based, formal semantics
(Montague, 1974) and vector space models.
The logic-based view of language accounts nicely
for compositionality by directly mapping syntac-
tic constituents to lambda calculus expressions. At
the word level, the focus is on function words, and
nouns and adjectives are often defined only in terms
of the sets of entities they denote in the world. Most
words are treated as atomic symbols with no rela-
tion to each other. There have been many attempts
at automatically parsing natural language to a logi-
cal form using recursive compositional rules.
Conversely, vector space models have the attrac-
tive property that they can automatically extract
knowledge from large corpora without supervision.
Unlike logic-based approaches, these models allow
us to make fine-grained statements about the seman-
tic similarity of words which correlate well with hu-
man judgments (Griffiths et al 2007). Logic-based
approaches are often seen as orthogonal to distribu-
tional vector-based approaches. However, Garrette
et al(2011) recently introduced a combination of a
vector space model inside a Markov Logic Network.
One open question is whether vector-based mod-
els can learn some of the simple logic encountered
in language such as negation or conjunctives. To
this end, we illustrate in a simple example that our
MV-RNN model and its learned word matrices (op-
erators) have the ability to learn propositional logic
operators such as ?,?,? (and, or, not). This is a
necessary (though not sufficient) condition for the
ability to pick up these phenomena in real datasets
and tasks such as sentiment detection which we fo-
cus on in the subsequent sections.
Our setup is as follows. We train on 6 strictly
right-branching trees as in Fig. 4. We consider the 1-
dimensional case and fix the representation for true
to (t = 1, T = 1) and false to (f = 0, F = 1).
Fixing the operators to the 1 ? 1 identity matrix 1
is essentially ignoring them. The objective is then
to create a perfect reconstruction of (t, T ) or (f, F )
(depending on the formula), which we achieve by
the least squares error between the top vector?s rep-
resentation and the corresponding truth value, e.g.
for ?false: min ||ptop ? t||2 + ||Ptop ? T ||2.
As our function g (see Eq. 2), we use a linear
threshold unit: g(x) = max(min(x, 1), 0). Giving
the derivatives computed for the objective function
for the examples in Fig. 4 to a standard L-BFGS op-
timizer quickly yields a training error of 0. Hence,
the output of these 6 examples has exactly one of the
truth representations, making it recursively compati-
ble with further combinations of operators. Thus, we
can combine these operators to construct any propo-
sitional logic function of any number of inputs (in-
cluding xor). Hence, this MV-RNN is complete in
terms of propositional logic.
4 Predicting Movie Review Ratings
In this section, we analyze the model?s performance
on full length sentences. We compare to previous
state of the art methods on a standard benchmark
dataset of movie reviews (Pang and Lee, 2005; Nak-
agawa et al 2010; Socher et al 2011c). This
dataset consists of 10,000 positive and negative sin-
gle sentences describing movie sentiment. In this
and the next experiment we use binarized trees from
the Stanford Parser (Klein and Manning, 2003). We
use the exact same setup and parameters (regulariza-
tion, word vector size, etc.) as the published code of
Socher et al(2011c).2
2www.socher.org
1207
Method Acc.
Tree-CRF (Nakagawa et al 2010) 77.3
RAE (Socher et al 2011c) 77.7
Linear MVR 77.1
MV-RNN 79.0
Table 1: Accuracy of classification on full length movie
review polarity (MR).
S. C. Review sentence
1
?
The film is bright and flashy in all the right ways.
0
?
Not always too whimsical for its own good this
strange hybrid of crime thriller, quirky character
study, third-rate romance and female empowerment
fantasy never really finds the tonal or thematic glue
it needs.
0
?
Doesn?t come close to justifying the hype that sur-
rounded its debut at the Sundance film festival two
years ago.
0 x Director Hoffman, his writer and Kline?s agent
should serve detention.
1 x A bodice-ripper for intellectuals.
Table 2: Hard movie review examples of positive (1) and
negative (0) sentiment (S.) that of all methods only the
MV-RNN predicted correctly (C:
?
) or could not classify
as correct either (C: x).
Table 1 shows comparisons to the system of (Nak-
agawa et al 2010), a dependency tree based classifi-
cation method that uses CRFs with hidden variables.
The state of the art recursive autoencoder model of
Socher et al(2011c) obtained 77.7% accuracy. Our
new MV-RNN gives the highest performance, out-
performing also the linear MVR (Sec. 2.2).
Table 2 shows several hard examples that only the
MV-RNN was able to classify correctly. None of the
methods correctly classified the last two examples
which require more world knowledge.
5 Classification of Semantic Relationships
The previous task considered global classification of
an entire phrase or sentence. In our last experiment
we show that the MV-RNN can also learn how a syn-
tactic context composes an aggregate meaning of the
semantic relationships between words. In particular,
the task is finding semantic relationships between
pairs of nominals. For instance, in the sentence
?My [apartment]e1 has a pretty large [kitchen]e2.?,
we want to predict that the kitchen and apartment are
in a component-whole relationship. Predicting such
????[ m o v i e ]  s h o w e d  [ w ar s ]     ?
MV - R N N  f o r  R e l at i o n s h i p Cl as s i f i cat i o n
?
?
Cl as s i f i e r :  Me s s age - T o pi c
Figure 5: The MV-RNN learns vectors in the path con-
necting two words (dotted lines) to determine their se-
mantic relationship. It takes into consideration a variable
length sequence of various word types in that path.
semantic relations is useful for information extrac-
tion and thesaurus construction applications. Many
approaches use features for all words on the path
between the two words of interest. We show that
by building a single compositional semantics for the
minimal constituent including both terms one can
achieve a higher performance.
This task requires the ability to deal with se-
quences of words of arbitrary type and length in be-
tween the two nouns in question.Fig. 5 explains our
method for classifying nominal relationships. We
first find the path in the parse tree between the two
words whose relation we want to classify. We then
select the highest node of the path and classify the
relationship using that node?s vector as features. We
apply the same type of MV-RNN model as in senti-
ment to the subtree spanned by the two words.
We use the dataset and evaluation framework
of SemEval-2010 Task 8 (Hendrickx et al 2010).
There are 9 ordered relationships (with two direc-
tions) and an undirected other class, resulting in
19 classes. Among the relationships are: message-
topic, cause-effect, instrument-agency (etc. see Ta-
ble 3 for list). A pair is counted as correct if the
order of the words in the relationship is correct.
Table 4 lists results for several competing meth-
ods together with the resources and features used
by each method. We compare to the systems of
the competition which are described in Hendrickx
et al(2010) as well as the RNN and linear MVR.
Most systems used a considerable amount of hand-
designed semantic resources. In contrast to these
methods, the MV-RNN only needs a parser for the
tree structure and learns all semantics from unla-
beled corpora and the training data. Only the Se-
mEval training dataset is specific to this task, the re-
1208
Relationship Sentence with labeled nouns for which to predict relationships
Cause-Effect(e2,e1) Avian [influenza]e1 is an infectious disease caused by type a strains of the influenza [virus]e2.
Entity-Origin(e1,e2) The [mother]e1 left her native [land]e2 about the same time and they were married in that city.
Message-Topic(e2,e1) Roadside [attractions]e1 are frequently advertised with [billboards]e2 to attract tourists.
Product-Producer(e1,e2) A child is told a [lie]e1 for several years by their [parents]e2 before he/she realizes that ...
Entity-Destination(e1,e2) The accident has spread [oil]e1 into the [ocean]e2.
Member-Collection(e2,e1) The siege started, with a [regiment]e1 of lightly armored [swordsmen]e2 ramming down the gate.
Instrument-Agency(e2,e1) The core of the [analyzer]e1 identifies the paths using the constraint propagation [method]e2.
Component-Whole(e2,e1) The size of a [tree]e1 [crown]e2 is strongly correlated with the growth of the tree.
Content-Container(e1,e2) The hidden [camera]e1, found by a security guard, was hidden in a business card-sized [leaflet
box]e2 placed at an unmanned ATM in Tokyo?s Minato ward in early September.
Table 3: Examples of correct classifications of ordered, semantic relations between nouns by the MV-RNN. Note that
the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed
words. The paths vary in length and the words vary in type.
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, WordNet, stemming, syntactic
patterns
74.8
SVM POS, WordNet, morphological fea-
tures, thesauri, Google n-grams
77.6
MaxEnt POS, WordNet, morphological fea-
tures, noun compound system, the-
sauri, Google n-grams
77.6
SVM POS, WordNet, prefixes and other
morphological features, POS, depen-
dency parse features, Levin classes,
PropBank, FrameNet, NomLex-Plus,
Google n-grams, paraphrases, Tex-
tRunner
82.2
RNN - 74.8
Lin.MVR - 73.0
MV-RNN - 79.1
RNN POS,WordNet,NER 77.6
Lin.MVR POS,WordNet,NER 78.7
MV-RNN POS,WordNet,NER 82.4
Table 4: Learning methods, their feature sets and F1
results for predicting semantic relations between nouns.
The MV-RNN outperforms all but one method without
any additional feature sets. By adding three such features,
it obtains state of the art performance.
maining inputs and the training setup are the same
as in previous sentiment experiments.
The best method on this dataset (Rink and
Harabagiu, 2010) obtains 82.2% F1. In order to
see whether our system can improve over this sys-
tem, we added three features to the MV-RNN vec-
tor and trained another softmax classifier. The fea-
tures and their performance increases were POS tags
(+0.9); WordNet hypernyms (+1.3) and named en-
tity tags (NER) of the two words (+0.6). Features
were computed using the code of Ciaramita and Al-
tun (2006).3 With these features, the performance
improved over the state of the art system. Table 3
shows random correct classification examples.
6 Related work
Distributional approaches have become omnipresent
for the recognition of semantic similarity between
words and the treatment of compositionality has
seen much progress in recent years. Hence, we can-
not do justice to the large amount of literature. Com-
monly, single words are represented as vectors of
distributional characteristics ? e.g., their frequencies
in specific syntactic relations or their co-occurrences
with given context words (Pado and Lapata, 2007;
Baroni and Lenci, 2010; Turney and Pantel, 2010).
These representations have proven very effective in
sense discrimination and disambiguation (Schu?tze,
1998), automatic thesaurus extraction (Lin, 1998;
Curran, 2004) and selectional preferences.
There are several sophisticated ideas for com-
positionality in vector spaces. Mitchell and Lap-
ata (2010) present an overview of the most impor-
tant compositional models, from simple vector ad-
dition and component-wise multiplication to tensor
products, and convolution (Metcalfe, 1990). They
measured the similarity between word pairs such
as compound nouns or verb-object pairs and com-
pared these with human similarity judgments. Sim-
ple vector averaging or multiplication performed
best, hence our focus on related baselines above.
3sourceforge.net/projects/supersensetag/
1209
Other important models are tensor products (Clark
and Pulman, 2007), quantum logic (Widdows,
2008), holographic reduced representations (Plate,
1995) and the Compositional Matrix Space model
(Rudolph and Giesbrecht, 2010). RNNs are related
to autoencoder models such as the recursive autoas-
sociative memory (RAAM) (Pollack, 1990) or recur-
rent neural networks (Elman, 1991). Bottou (2011)
and Hinton (1990) discussed related models such as
recursive autoencoders for text understanding.
Our model builds upon and generalizes the mod-
els of (Mitchell and Lapata, 2010; Baroni and Zam-
parelli, 2010; Zanzotto et al 2010; Socher et al
2011c) (see Sec. 2.2). We compare to them in
our experiments. Yessenalina and Cardie (2011) in-
troduce a sentiment analysis model that describes
words as matrices and composition as matrix mul-
tiplication. Since matrix multiplication is associa-
tive, this cannot capture different scopes of nega-
tion or syntactic differences. Their model, is a spe-
cial case of our encoding model (when you ignore
vectors, fix the tree to be strictly branching in one
direction and use as the matrix composition func-
tion P = AB). Since our classifiers are trained on
the vectors, we cannot compare to this approach di-
rectly. Grefenstette and Sadrzadeh (2011) learn ma-
trices for verbs in a categorical model. The trained
matrices improve correlation with human judgments
on the task of identifying relatedness of subject-
verb-object triplets.
7 Conclusion
We introduced a new model towards a complete
treatment of compositionality in word vector spaces.
Our model builds on a syntactically plausible parse
tree and can handle compositional phenomena. The
main novelty of our model is the combination of
matrix-vector representations with a recursive neu-
ral network. It can learn both the meaning vectors of
a word and how that word modifies its neighbors (via
its matrix). The MV-RNN combines attractive the-
oretical properties with good performance on large,
noisy datasets. It generalizes several models in the
literature, can learn propositional logic, accurately
predicts sentiment and can be used to classify se-
mantic relationships between nouns in a sentence.
Acknowledgments
We thank for great discussions about the paper:
John Platt, Chris Potts, Josh Tenenbaum, Mihai Sur-
deanu, Quoc Le and Kevin Miller. The authors
gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181, and the DARPA Deep Learning program
under contract number FA8650-10-C-7020. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
DARPA, AFRL, or the US government.
References
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
M. Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP.
L. Bottou. 2011. From machine learning to machine
reasoning. CoRR, abs/1102.1808.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In EMNLP.
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the AAAI Spring Symposium on Quantum Interaction,
pages 52?55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: deep neural networks
with multitask learning. In ICML.
J. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
J. L. Elman. 1991. Distributed representations, simple
recurrent networks, and grammatical structure. Ma-
chine Learning, 7(2-3).
G. Frege. 1892. U?ber Sinn und Bedeutung. In Zeitschrift
fu?r Philosophie und philosophische Kritik, 100.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrat-
ing Logical Representations with Probabilistic Infor-
mation using Markov Logic. In Proceedings of the In-
ternational Conference on Computational Semantics.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
1210
E. Grefenstette and M. Sadrzadeh. 2011. Experimental
support for a categorical compositional distributional
model of meaning. In EMNLP.
T. L. Griffiths, J. B. Tenenbaum, and M. Steyvers. 2007.
Topics in semantic representation. Psychological Re-
view, 114.
I. Hendrickx, S.N. Kim, Z. Kozareva, P. Nakov,
D. O? Se?aghdha, S. Pado?, M. Pennacchiotti, L. Ro-
mano, and S. Szpakowicz. 2010. Semeval-2010 task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proceedings of the 5th
International Workshop on Semantic Evaluation.
G. E. Hinton. 1990. Mapping part-whole hierarchies into
connectionist networks. Artificial Intelligence, 46(1-
2).
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL, pages
768?774.
E. J. Metcalfe. 1990. A compositive holographic asso-
ciative recall model. Psychological Review, 88:627?
661.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388?1429.
R. Montague. 1974. English as a formal language. Lin-
guaggi nella Societa e nella Tecnica, pages 189?224.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. In ACL.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115?124.
T. A. Plate. 1995. Holographic reduced representations.
IEEE Transactions on Neural Networks, 6(3):623?
641.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
C. Potts. 2010. On the negativity of negation. In David
Lutz and Nan Li, editors, Proceedings of Semantics
and Linguistic Theory 20. CLC Publications, Ithaca,
NY.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
B. Rink and S. Harabagiu. 2010. UTD: Classifying se-
mantic relations by combining lexical and semantic re-
sources. In Proceedings of the 5th International Work-
shop on Semantic Evaluation.
S. Rudolph and E. Giesbrecht. 2010. Compositional
matrix-space models of language. In ACL.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24:97?124.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
In NIPS. MIT Press.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
D. Widdows. 2008. Semantic vector products: Some ini-
tial investigations. In Proceedings of the Second AAAI
Symposium on Quantum Interaction.
A. Yessenalina and C. Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
EMNLP.
F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-
andhar. 2010. Estimating linear models for composi-
tional distributional semantics. COLING.
1211
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393?1398,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Bilingual Word Embeddings for Phrase-Based Machine Translation
Will Y. Zou?, Richard Socher, Daniel Cer, Christopher D. Manning
Department of Electrical Engineering? and Computer Science Department
Stanford University, Stanford, CA 94305, USA
{wzou,danielcer,manning}@stanford.edu, richard@socher.org
Abstract
We introduce bilingual word embeddings: se-
mantic embeddings associated across two lan-
guages in the context of neural language mod-
els. We propose a method to learn bilingual
embeddings from a large unlabeled corpus,
while utilizing MT word alignments to con-
strain translational equivalence. The new em-
beddings significantly out-perform baselines
in word semantic similarity. A single semantic
similarity feature induced with bilingual em-
beddings adds near half a BLEU point to the
results of NIST08 Chinese-English machine
translation task.
1 Introduction
It is difficult to recognize and quantify semantic sim-
ilarities across languages. The Fr-En phrase-pair
{?un cas de force majeure?, ?case of absolute neces-
sity?}, Zh-En phrase pair {??????,?persist in a
stubborn manner?} are similar in semantics. If co-
occurrences of exact word combinations are rare in
the training parallel text, it can be difficult for classi-
cal statistical MT methods to identify this similarity,
or produce a reasonable translation given the source
phrase.
We introduce an unsupervised neural model
to learn bilingual semantic embedding for words
across two languages. As an extension to their
monolingual counter-part (Turian et al, 2010;
Huang et al, 2012; Bengio et al, 2003), bilin-
gual embeddings capture not only semantic infor-
mation of monolingual words, but also semantic re-
lationships across different languages. This prop-
erty allows them to define semantic similarity met-
rics across phrase-pairs, making them perfect fea-
tures for machine translation.
To learn bilingual embeddings, we use a new ob-
jective function which embodies both monolingual
semantics and bilingual translation equivalence. The
latter utilizes word alignments, a natural sub-task
in the machine translation pipeline. Through large-
scale curriculum training (Bengio et al, 2009), we
obtain bilingual distributed representations which
lie in the same feature space. Embeddings of di-
rect translations overlap, and semantic relationships
across bilingual embeddings were further improved
through unsupervised learning on a large unlabeled
corpus.
Consequently, we produce for the research com-
munity a first set of Mandarin Chinese word embed-
dings with 100,000 words trained on the Chinese
Gigaword corpus. We evaluate these embedding
on Chinese word semantic similarity from SemEval-
2012 (Jin and Wu, 2012). The embeddings sig-
nificantly out-perform prior work and pruned tf-idf
base-lines. In addition, the learned embeddings
give rise to 0.11 F1 improvement in Named Entity
Recognition on the OntoNotes dataset (Hovy et al,
2006) with a neural network model.
We apply the bilingual embeddings in an end-to-
end phrase-based MT system by computing seman-
tic similarities between phrase pairs. On NIST08
Chinese-English translation task, we obtain an im-
provement of 0.48 BLEU from a competitive base-
line (30.01 BLEU to 30.49 BLEU) with the Stanford
Phrasal MT system.
1393
2 Review of prior work
Distributed word representations are useful in NLP
applications such as information retrieval (Pas?ca et
al., 2006; Manning et al, 2008), search query ex-
pansions (Jones et al, 2006), or representing se-
mantics of words (Reisinger et al, 2010). A num-
ber of methods have been explored to train and ap-
ply word embeddings using continuous models for
language. Collobert et al (2008) learn embed-
dings in an unsupervised manner through a con-
trastive estimation technique. Mnih and Hinton (
2008), Morin and Bengio ( 2005) proposed efficient
hierarchical continuous-space models. To system-
atically compare embeddings, Turian et al (2010)
evaluated improvements they bring to state-of-the-
art NLP benchmarks. Huang et al (2012) intro-
duced global document context and multiple word
prototypes. Recently, morphology is explored to
learn better word representations through Recursive
Neural Networks (Luong et al, 2013).
Bilingual word representations have been ex-
plored with hand-designed vector space mod-
els (Peirsman and Pado? , 2010; Sumita, 2000),
and with unsupervised algorithms such as LDA and
LSA (Boyd-Graber and Resnik, 2010; Tam et al,
2007; Zhao and Xing, 2006). Only recently have
continuous space models been applied to machine
translation (Le et al, 2012). Despite growing in-
terest in these models, little work has been done
along the same lines to train bilingual distributioned
word represenations to improve machine translation.
In this paper, we learn bilingual word embeddings
which achieve competitive performance on seman-
tic word similarity, and apply them in a practical
phrase-based MT system.
3 Algorithm and methods
3.1 Unsupervised training with global context
Our method starts with embedding learning formu-
lations in Collobert et al (2008). Given a context
window c in a document d, the optimization mini-
mizes the following Context Objective for a word w
in the vocabulary:
J (c,d)CO =
?
wr?VR
max(0, 1? f(cw, d) + f(cwr , d))
(1)
Here f is a function defined by a neural network.
wr is a word chosen in a random subset VR of the
vocabulary, and cwr is the context window contain-
ing word wr. This unsupervised objective func-
tion contrasts the score between when the correct
word is placed in context with when a random word
is placed in the same context. We incorporate the
global context information as in Huang et al (2012),
shown to improve performance of word embed-
dings.
3.2 Bilingual initialization and training
In the joint semantic space of words across two lan-
guages, the Chinese word ???? is expected to be
close to its English translation ?government?. At the
same time, when two words are not direct transla-
tions, e.g. ?lake? and the Chinese word ??? (deep
pond), their semantic proximity could be correctly
quantified.
We describe in the next sub-sections the methods
to intialize and train bilingual embeddings. These
methods ensure that bilingual embeddings retain
their translational equivalence while their distribu-
tional semantics are improved during online training
with a monolingual corpus.
3.2.1 Initialization by MT alignments
First, we use MT Alignment counts as weighting
to initialize Chinese word embeddings. In our ex-
periments, we use MT word alignments extracted
with the Berkeley Aligner (Liang et al, 2006) 1.
Specifically, we use the following equation to com-
pute starting word embeddings:
Wt-init =
S
?
s=1
Cts + 1
Ct + S
Ws (2)
In this equation, S is the number of possible tar-
get language words that are aligned with the source
word. Cts denotes the number of times when word t
in the target and word s in the source are aligned in
the training parallel text; Ct denotes the total num-
ber of counts of word t that appeared in the target
language. Finally, Laplace smoothing is applied to
this weighting function.
1On NIST08 Zh-En training data and data from GALE MT
evaluation in the past 5 years
1394
Single-prototype English embeddings by Huang
et al (2012) are used to initialize Chinese em-
beddings. The initialization readily provides a set
(Align-Init) of benchmark embeddings in experi-
ments (Section 4), and ensures translation equiva-
lence in the embeddings at start of training.
3.2.2 Bilingual training
Using the alignment counts, we form alignment
matrices Aen?zh and Azh?en. For Aen?zh, each
row corresponds to a Chinese word, and each col-
umn an English word. An element aij is first as-
signed the counts of when the ith Chinese word is
aligned with the jth English word in parallel text.
After assignments, each row is normalized such that
it sums to one. The matrix Azh?en is defined sim-
ilarly. Denote the set of Chinese word embeddings
as Vzh, with each row a word embedding, and the
set of English word embeddings as Ven. With the
two alignment matrices, we define the Translation
Equivalence Objective:
JTEO-en?zh = ?Vzh ?Aen?zhVen?2 (3)
JTEO-zh?en = ?Ven ?Azh?enVzh?2 (4)
We optimize for a combined objective during train-
ing. For the Chinese embeddings we optimize for:
JCO-zh + ?JTEO-en?zh (5)
For the English embeddings we optimize for:
JCO-en + ?JTEO-zh?en (6)
During bilingual training, we chose the value of ?
such that convergence is achieved for both JCO and
JTEO. A small validation set of word similarities
from (Jin and Wu, 2012) is used to ensure the em-
beddings have reasonable semantics. 2
In the next sections, ?bilingual trained? embed-
dings refer to those initialized with MT alignments
and trained with the objective defined by Equa-
tion 5. ?Monolingual trained? embeddings refer to
those intialized by alignment but trained without
JTEO-en?zh.
2In our experiments, ? = 50.
3.3 Curriculum training
We train 100k-vocabulary word embeddings using
curriculum training (Turian et al, 2010) with Equa-
tion 5. For each curriculum, we sort the vocabu-
lary by frequency and segment the vocabulary by a
band-size taken from {5k, 10k, 25k, 50k}. Separate
bands of the vocabulary are trained in parallel using
minibatch L-BFGS on the Chinese Gigaword cor-
pus 3. We train 100,000 iterations for each curricu-
lum, and the entire 100k vocabulary is trained for
500,000 iterations. The process takes approximately
19 days on a eight-core machine. We show visual-
ization of learned embeddings overlaid with English
in Figure 1. The two-dimensional vectors for this vi-
sualization is obtained with t-SNE (van der Maaten
and Hinton, 2008). To make the figure comprehen-
sible, subsets of Chinese words are provided with
reference translations in boxes with green borders.
Words across the two languages are positioned by
the semantic relationships implied by their embed-
dings.
Figure 1: Overlaid bilingual embeddings: English words
are plotted in yellow boxes, and Chinese words in green;
reference translations to English are provided in boxes
with green borders directly below the original word.
4 Experiments
4.1 Semantic Similarity
We evaluate the Mandarin Chinese embeddings with
the semantic similarity test-set provided by the or-
3Fifth Edition. LDC catelog number LDC2011T13. We only
exclude cna cmn, the Traditional Chinese segment of the cor-
pus.
1395
Table 1: Results on Chinese Semantic Similarity
Method Sp. Corr. K. Tau
(?100) (?100)
Prior work (Jin and Wu, 2012) 5.0
Tf-idf
Naive tf-idf 41.5 28.7
Pruned tf-idf 46.7 32.3
Word Embeddings
Align-Init 52.9 37.6
Mono-trained 59.3 42.1
Biling-trained 60.8 43.3
ganizers of SemEval-2012 Task 4. This test-set con-
tains 297 Chinese word pairs with similarity scores
estimated by humans.
The results for semantic similarity are shown in
Table 1. We show two evaluation metrics: Spear-
man Correlation and Kendall?s Tau. For both, bilin-
gual embeddings trained with the combined objec-
tive defined by Equation 5 perform best. For pruned
tf-idf, we follow Reisinger et al (2010; Huang et
al. (2012) and count word co-occurrences in a 10-
word window. We use the best results from a
range of pruning and feature thresholds to compare
against our method. The bilingual and monolingual
trained embeddings4 out-perform pruned tf-idf by
14.1 and 12.6 Spearman Correlation (?100), respec-
tively. Further, they out-perform embeddings initial-
ized from alignment by 7.9 and 6.4. Both our tf-idf
implementation and the word embeddings have sig-
nificantly higher Kendall?s Tau value compared to
Prior work (Jin and Wu, 2012). We verified Tau cal-
culations with original submissions provided by the
authors.
4.2 Named Entity Recognition
We perform NER experiments on OntoNotes (v4.0)
(Hovy et al, 2006) to validate the quality of the
Chinese word embeddings. Our experimental set-
up is the same as Wang et al (2013). With em-
beddings, we build a naive feed-forward neural net-
work (Collobert et al, 2008) with 2000 hidden neu-
rons and a sliding window of five words. This naive
setting, without sequence modeling or sophisticated
4Due to variations caused by online minibatch L-BFGS, we
take embeddings from five random points out of last 105 mini-
batch iterations, and average their semantic similarity results.
Table 2: Results on Named Entity Recognition
Embeddings Prec. Rec. F1 Improve
Align-Init 0.34 0.52 0.41
Mono-trained 0.54 0.62 0.58 0.17
Biling-trained 0.48 0.55 0.52 0.11
Table 3: Vector Matching Alignment AER (lower is bet-
ter)
Embeddings Prec. Rec. AER
Mono-trained 0.27 0.32 0.71
Biling-trained 0.37 0.45 0.59
join optimization, is not competitive with state-of-
the-art (Wang et al, 2013). Table 2 shows that the
bilingual embeddings obtains 0.11 F1 improvement,
lagging monolingual, but significantly better than
Align-Init (as in Section3.2.1) on the NER task.
4.3 Vector matching alignment
Translation equivalence of the bilingual embeddings
is evaluated by naive word alignment to match word
embeddings by cosine distance.5 The Alignment Er-
ror Rates (AER) reported in Table 3 suggest that
bilingual training using Equation 5 produces embed-
dings with better translation equivalence compared
to those produced by monolingual training.
4.4 Phrase-based machine translation
Our experiments are performed using the Stan-
ford Phrasal phrase-based machine translation sys-
tem (Cer et al, 2010). In addition to NIST08 train-
ing data, we perform phrase extraction, filtering
and phrase table learning with additional data from
GALE MT evaluations in the past 5 years. In turn,
our baseline is established at 30.01 BLEU and rea-
sonably competitive relative to NIST08 results. We
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the decoder.
In the phrase-based MT system, we add one fea-
ture to bilingual phrase-pairs. For each phrase, the
word embeddings are averaged to obtain a feature
vector. If a word is not found in the vocabulary, we
disregard and assume it is not in the phrase; if no
word is found in a phrase, a zero vector is assigned
5This is evaluated on 10,000 randomly selected sentence
pairs from the MT training set.
1396
Table 4: NIST08 Chinese-English translation BLEU
Method BLEU
Our baseline 30.01
Embeddings
Random-Init Mono-trained 30.09
Align-Init 30.31
Mono-trained 30.40
Biling-trained 30.49
to it. We then compute the cosine distance between
the feature vectors of a phrase pair to form a seman-
tic similarity feature for the decoder.
Results on NIST08 Chinese-English translation
task are reported in Table 46. An increase of
0.48 BLEU is obtained with semantic similarity
with bilingual embeddings. The increase is modest,
just surpassing a reference standard deviation 0.29
BLEU Cer et al (2010)7 evaluated on a similar sys-
tem. We intend to publish further analysis on statis-
tical significance of this result as an appendix. From
these suggestive evidence in the MT results, random
initialized monolingual trained embeddings add lit-
tle gains to the baseline. Bilingual initialization and
training seem to be offering relatively more consis-
tent gains by introducing translational equivalence.
5 Conclusion
In this paper, we introduce bilingual word embed-
dings through initialization and optimization con-
straint using MT alignments The embeddings are
learned through curriculum training on the Chinese
Gigaword corpus. We show good performance on
Chinese semantic similarity with bilingual trained
embeddings. When used to compute semantic simi-
larity of phrase pairs, bilingual embeddings improve
NIST08 end-to-end machine translation results by
just below half a BLEU point. This implies that se-
mantic embeddings are useful features for improv-
ing MT systems. Further, our results offer sugges-
tive evidence that bilingual word embeddings act as
high-quality semantic features and embody bilingual
translation equivalence across languages.
6We report case-insensitive BLEU
7With 4-gram BLEU metric from Table 4
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Transla-
tion (BOLT) program through IBM. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
or the US government. We thank John Bauer and
Thang Luong for helpful discussions.
References
A. Klementiev, I. Titov and B. Bhattarai. 2012. Induc-
ing Crosslingual Distributed Representation of Words.
COLING.
Y. Bengio, J. Louradour, R. Collobert and J. Weston.
2009. Curriculum Learning. ICML.
Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research.
Y. Bengio and Y. LeCunn. 2007. Scaling learning algo-
rithms towards AI. Large-Scale Kernal Machines.
J. Boyd-Graber and P. Resnik. 2010. Holistic sentiment
analysis across languages: multilingual supervised la-
tent dirichlet alocation. EMNLP.
D. Cer, M. Galley, D. Jurafsky and C. Manning. 2010.
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Ar-
bitrary Model Features. In Proceedings of the North
American Association of Computational Linguistics -
Demo Session (NAACL-10).
D. Cer, C. Manning and D. Jurafsky. 2010. The Best
Lexical Metric for Phrase-Based Statistical MT Sys-
tem Optimization. NAACL.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. ICML.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proceedings of the Fourth Workshop on
Statistical Machine Translation.
M. Galley, P. Chang, D. Cer, J. R. Finkel and C. D. Man-
ning. 2008. NIST Open Machine Translation 2008
Evaluation: Stanford University?s System Description.
Unpublished working notes of the 2008 NIST Open
Machine Translation Evaluation Workshop.
S. Green, S. Wang, D. Cer and C. Manning. 2013. Fast
and adaptive online training of feature-rich translation
models. ACL.
G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N.
Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath
1397
and B. Kingsbury. 2012. Deep Neural Networks for
Acoustic Modeling in Speech Recognition. IEEE Sig-
nal Processing Magazine.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R.
Weischedel. 2006. OntoNotes: the 90% solution.
NAACL-HLT.
E. H. Huang, R. Socher, C. D. Manning and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. ACL.
P. Jin and Y. Wu. 2012. SemEval-2012 Task 4: Eval-
uating Chinese Word Similarity. Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation. Association for Computational Linguistics.
R. Jones. 2006. Generating query substitutions. In Pro-
ceedings of the 15th international conference on World
Wide Web.
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical
Phrase-Based Translation. HLT.
H. Le, A. Allauzen and F. Yvon 2012. Continuous space
translation models with neural networks. NAACL.
P. Liang, B. Taskar and D. Klein. 2006. Alignment by
agreement. NAACL.
M. Luong, R. Socher and C. Manning. 2013. Better
word representations with recursive neural networks
for morphology. CONLL.
L. van der Maaten and G. Hinton. 2008. Visualizing data
using t-SNE. Journal of Machine Learning Research.
A. Maas and R. E. Daly and P. T. Pham and D. Huang and
A. Y. Ng and C. Potts. 2011. Learning word vectors
for sentiment analysis. ACL.
C. Manning and P. Raghavan and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge Univer-
sity Press, New York, NY, USA.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky and S.
Khudanpur. 2010. Recurrent neural network based
language model. INTERSPEECH.
T. Mikolov, K. Chen, G. Corrado and J. Dean. 2013. Ef-
ficient Estimation of Word Representations in Vector
Space. arXiv:1301.3781v1.
A. Mnih and G. Hinton. 2008. A scalable hierarchical
distributed language model. NIPS.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. AISTATS.
F. Och. 2003. Minimum error rate training in statistical
machine translation. ACL.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits and A. Jain.
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. ACL.
Y. Peirsman and S. Pado?. 2010. Cross-lingual induction
of selectional preferences with bilingual vector spaces.
ACL.
J. Reisinger and R. J. Mooney. 2010. Multi-prototype
vector-space models of word meaning. NAACL.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Comput. Surv., 34:1-47, March.
R. Socher, J. Pennington, E. Huang, A. Y. Ng and
C. D. Manning. 2011. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
EMNLP.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
NIPS.
E. Sumita. 2000. Lexical transfer using a vector-space
model. ACL.
Y. Tam, I. Lane and T. Schultz. 2007. Bilingual-LSA
based LM adaptation for spoken language translation.
ACL.
S. Tellex and B. Katz and J. Lin and A. Fernandes and
G. Marton. 2003. Quantitative evaluation of passage
retrieval algorithms for question answering. In Pro-
ceedings of the 26th Annual International ACM SIGIR
Conference on Search and Development in Informa-
tion Retrieval, pages 41-47. ACM Press.
J. Turian and L. Ratinov and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. ACL.
M. Wang, W. Che and C. D. Manning. 2013. Joint Word
Alignment and Bilingual Named Entity Recognition
Using Dual Decomposition. ACL.
K. Yamada and K. Knight. 2001. A Syntax-based Statis-
tical Translation Model. ACL.
B. Zhao and E. P. Xing 2006. BiTAM: Bilingual topic
AdMixture Models for word alignment. ACL.
1398
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng and Christopher Potts
Stanford University, Stanford, CA 94305, USA
richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu
{jeaneis,manning,cgpotts}@stanford.edu
Abstract
Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics. It pushes the state of the art in single
sentence positive/negative classification from
80% up to 85.4%. The accuracy of predicting
fine-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.
1 Introduction
Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al, 2010;
Zanzotto et al, 2010; Yessenalina and Cardie, 2011;
Socher et al, 2012; Grefenstette et al, 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and
?
0
0This 0film
?
?
?
0does 0n?t
0
+care +0about ++
+
+
+cleverness 0,
0wit
0or
+
0
0any 00other +kind
+
0of ++intelligent + +humor
0.
Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive (? ?, ?, 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.
models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.
The Stanford Sentiment Treebank is the first cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset alows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of
1631
this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.
In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al, 2011b),
matrix-vector RNNs (Socher et al, 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a significant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting fine-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction ?but? dominates.
The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.
2 Related Work
This work is connected to five different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces. The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a
word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pado?, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al, 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al, 2012) but then also be
fine-tuned and trained to specific tasks such as sen-
timent detection (Socher et al, 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces. Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and define composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and find a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models. In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al, 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form. A related field that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned
1632
work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and first models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classifi-
cation (Sutskever et al, 2009; Jenatton et al, 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al, 2012).
Sentiment Analysis. Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al, 2010; Nakagawa et al, 2010).
3 Stanford Sentiment Treebank
Bag of words classifiers can work well in longer
documents by relying on a few words with strong
sentiment like ?awesome? or ?exhilarating.? How-
ever, sentiment accuracies even for binary posi-
tive/negative classification for single sentences has
not exceeded 80% for several years. For the more
difficult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al, 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.
We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-
nerdy ?folks
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
phenomenal ?fantasy ?best ?sellers
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
 ?
 ?
Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.
tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reflects the writer?s over-
all intention for this review. The normalized, lower-
cased text is first used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
influenced by what follows. For more details on the
dataset collection, see supplementary material.
Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the findings from
labeling sentences based on reader?s perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the five po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classification
into these categories captures the main variability
of the labels. We will name this fine-grained senti-
ment classification and our main experiment will be
to recover these five labels for phrases of all lengths.
1633
5 10 15 20 25 30 35 40 45
N-Gram Length
0%
20%
40%
60%
80%
100%
%
 o
f S
en
tim
en
t V
al
ue
s
Neutral
SomeZhat 3ositiYe
3ositiYe
Ver\ 3ositiYe
SomeZhat NegatiYe
NegatiYe
Ver\ NegatiYe
(a)
(a)
(b)
(b)
(c)
(c)
(d)
(d)
Distributions of sentiment values for (a) unigrams, 
(b) 10-grams, (c) 20-grams, and (d) full sentences.
Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.
4 Recursive Neural Models
The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classifier. For ease of exposition,
we will use the tri-gram in this figure to explain all
models.
We first describe the operations that the below re-
cursive neural models have in common: word vector
representations and classification. This is followed
by descriptions of two previous RNN models and
our RNTN.
Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(?r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L ?
Rd?|V |, where |V | is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.
We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classifier. For classification into five
classes, we compute the posterior probability over
    not      very       good ...
        a          b             c 
p1 =g(b,c)
p2 = g(a,p1)
0 0 +
+ +
-
Figure 4: Approach of Recursive Neural Network mod-
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classifier at that node. This func-
tion varies for the different models.
labels given the word vector via:
ya = softmax(Wsa), (1)
where Ws ? R5?d is the sentiment classification
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi ? Rd in a bottom up fashion.
4.1 RNN: Recursive Neural Network
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and Ku?chler, 1996; Socher et al,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two children?s vectors since both
are words. RNNs use the following equations to
compute the parent vectors:
1634
p1 = f
(
W
[
b
c
])
, p2 = f
(
W
[
a
p1
])
,
where f = tanh is a standard element-wise nonlin-
earity, W ? Rd?2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classifier of Eq. 1 to compute its
label probabilities.
This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we fix the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.
4.2 MV-RNN: Matrix-Vector RNN
The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al, 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.
Each word?s matrix is initialized as a d?d identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sification error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:
(p2,P2)
(a,A) (p1,P1)
(b,B) (c,C)
the MV-RNN computes the first parent vector and its
matrix via two equations:
p1 = f
(
W
[
Cb
Bc
])
, P1 = f
(
WM
[
B
C
])
,
where WM ? Rd?2d and the result is again a d ? d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a,A). The vectors are used
for classifying each phrase using the same softmax
classifier as in Eq. 1.
4.3 RNTN:Recursive Neural Tensor Network
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a fixed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specific ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We define the
output of a tensor product h ? Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i] ? Rd?d:
h =
[
b
c
]T
V [1:d]
[
b
c
]
;hi =
[
b
c
]T
V [i]
[
b
c
]
.
where V [1:d] ? R2d?2d?d is the tensor that defines
multiple bilinear forms.
1635
            Slices of       Standard   
                Tensor Layer          Layer
p = f             V[1:2]        +   W
Neural Tensor Layer
b
c
b
c
b
c
T
p = f                             +          
Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of influence a child can have
on its parent.
The RNTN uses this definition for computing p1:
p1 = f
([
b
c
]T
V [1:d]
[
b
c
]
+W
[
b
c
])
,
where W is as defined in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:
p2 = f
([
a
p1
]T
V [1:d]
[
a
p1
]
+W
[
a
p1
])
.
The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of compo-
sition.
An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.
4.4 Tensor Backprop through Structure
We describe in this section how to train the RNTN
model. As mentioned above, each node has a
softmax classifier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.
We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi ? RC?1 at
node i and the target distribution ti ? RC?1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters ? = (V,W,Ws, L) for a sentence is:
E(?) =
?
i
?
j
tij log y
i
j + ????
2 (2)
The derivative for the weights of the softmax clas-
sifier are standard and simply sum up from each
node?s error. We define xi to be the vector at node
i (in the example trigram, the xi ? Rd?1?s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V,W . Let ?i,s ? Rd?1
be the softmax error vector at node i:
?i,s =
(
W Ts (y
i ? ti)
)
? f ?(xi),
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f(xi).
The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We define the complete incoming error
messages for a node i as ?i,com. The top node, in
our case p2, only received errors from the top node?s
softmax. Hence, ?p2,com = ?p2,s which we can
use to obtain the standard backprop derivative for
W (Goller and Ku?chler, 1996; Socher et al, 2010).
For the derivative of each slice k = 1, . . . , d, we get:
?Ep2
?V [k]
= ?p2,comk
[
a
p1
] [
a
p1
]T
,
where ?p2,comk is just the k?th element of this vector.
Now, we can compute the error message for the two
1636
children of p2:
?p2,down =
(
W T ?p2,com + S
)
? f ?
([
a
p1
])
,
where we define
S =
d?
k=1
?p2,comk
(
V [k] +
(
V [k]
)T
)[
a
p1
]
The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete ?. In particular, we have
?p1,com = ?p1,s + ?p2,down[d+ 1 : 2d],
where ?p2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the final word vector derivative for a, it
will be ?p2,down[1 : d].
The full derivative for slice V [k] for this trigram
tree then is the sum at each node:
?E
?V [k]
=
Ep2
?V [k]
+ ?p1,comk
[
b
c
] [
b
c
]T
,
and similarly for W . For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al, 2011) which con-
verges in less than 3 hours to a local optimum.
5 Experiments
We include two types of analyses. The first type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.
For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours. Initial experiments
Model
Fine-grained Positive/Negative
All Root All Root
NB 67.2 41.0 82.6 81.8
SVM 64.3 40.7 84.6 79.4
BiNB 71.0 41.9 82.7 83.1
VecAvg 73.3 32.7 85.1 80.1
RNN 79.0 43.2 86.1 82.4
MV-RNN 78.7 44.4 86.8 82.9
RNTN 80.7 45.7 87.6 85.4
Table 1: Accuracy for fine grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.
showed that the recursive models worked signifi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.
We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).
The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
filters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.
5.1 Fine-grained Sentiment For All Phrases
The main novel experiment and evaluation metric
analyze the accuracy of fine-grained sentiment clas-
sification for all phrases. Fig. 2 showed that a fine
grained classification into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.
Table 1 (left) shows the overall accuracy numbers
for fine grained prediction at all phrase lengths and
full sentences.
1637
    
1*UDP/HQJWK





$
F
F
X
U
D
F
\
    
1*UDP/HQJWK

Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 633?644,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
A Neural Network for Factoid Question Answering over
Paragraphs
Mohit Iyyer
1
, Jordan Boyd-Graber
2
, Leonardo Claudino
1
,
Richard Socher
3
, Hal Daume? III
1
1
University of Maryland, Department of Computer Science and umiacs
2
University of Colorado, Department of Computer Science
3
Stanford University, Department of Computer Science
{miyyer,claudino,hal}@umiacs.umd.edu,
Jordan.Boyd.Graber@colorado.edu, richard@socher.org
Abstract
Text classification methods for tasks
like factoid question answering typi-
cally use manually defined string match-
ing rules or bag of words representa-
tions. These methods are ineffective
when question text contains very few
individual words (e.g., named entities)
that are indicative of the answer. We
introduce a recursive neural network
(rnn) model that can reason over such
input by modeling textual composition-
ality. We apply our model, qanta, to
a dataset of questions from a trivia
competition called quiz bowl. Unlike
previous rnn models, qanta learns
word and phrase-level representations
that combine across sentences to reason
about entities. The model outperforms
multiple baselines and, when combined
with information retrieval methods, ri-
vals the best human players.
1 Introduction
Deep neural networks have seen widespread
use in natural language processing tasks such
as parsing, language modeling, and sentiment
analysis (Bengio et al., 2003; Socher et al.,
2013a; Socher et al., 2013c). The vector spaces
learned by these models cluster words and
phrases together based on similarity. For exam-
ple, a neural network trained for a sentiment
analysis task such as restaurant review classifi-
cation might learn that ?tasty? and ?delicious?
should have similar representations since they
are synonymous adjectives.
These models have so far only seen success in
a limited range of text-based prediction tasks,
Later in its existence, this polity?s leader was chosen
by a group that included three bishops and six laymen,
up from the seven who traditionally made the decision.
Free imperial cities in this polity included Basel and
Speyer. Dissolved in 1806, its key events included the
Investiture Controversy and the Golden Bull of 1356.
Led by Charles V, Frederick Barbarossa, and Otto I,
for 10 points, name this polity, which ruled most of
what is now Germany through the Middle Ages and
rarely ruled its titular city.
Figure 1: An example quiz bowl question about
the Holy Roman Empire. The first sentence
contains no words or named entities that by
themselves are indicative of the answer, while
subsequent sentences contain more and more
obvious clues.
where inputs are typically a single sentence and
outputs are either continuous or a limited dis-
crete set. Neural networks have not yet shown
to be useful for tasks that require mapping
paragraph-length inputs to rich output spaces.
Consider factoid question answering: given
a description of an entity, identify the per-
son, place, or thing discussed. We describe a
task with high-quality mappings from natural
language text to entities in Section 2. This
task?quiz bowl?is a challenging natural lan-
guage problem with large amounts of diverse
and compositional data.
To answer quiz bowl questions, we develop
a dependency tree recursive neural network
in Section 3 and extend it to combine predic-
tions across sentences to produce a question
answering neural network with trans-sentential
averaging (qanta). We evaluate our model
against strong computer and human baselines
in Section 4 and conclude by examining the
latent space and model mistakes.
633
2 Matching Text to Entities: Quiz
Bowl
Every weekend, hundreds of high school and
college students play a game where they map
raw text to well-known entities. This is a trivia
competition called quiz bowl. Quiz bowl ques-
tions consist of four to six sentences and are
associated with factoid answers (e.g., history
questions ask players to identify specific battles,
presidents, or events). Every sentence in a quiz
bowl question is guaranteed to contain clues
that uniquely identify its answer, even without
the context of previous sentences. Players an-
swer at any time?ideally more quickly than
the opponent?and are rewarded for correct
answers.
Automatic approaches to quiz bowl based on
existing nlp techniques are doomed to failure.
Quiz bowl questions have a property called
pyramidality, which means that sentences early
in a question contain harder, more obscure
clues, while later sentences are ?giveaways?.
This design rewards players with deep knowl-
edge of a particular subject and thwarts bag
of words methods. Sometimes the first sen-
tence contains no named entities?answering
the question correctly requires an actual un-
derstanding of the sentence (Figure 1). Later
sentences, however, progressively reveal more
well-known and uniquely identifying terms.
Previous work answers quiz bowl ques-
tions using a bag of words (na??ve Bayes) ap-
proach (Boyd-Graber et al., 2012). These mod-
els fail on sentences like the first one in Figure 1,
a typical hard, initial clue. Recursive neural
networks (rnns), in contrast to simpler models,
can capture the compositional aspect of such
sentences (Hermann et al., 2013).
rnns require many redundant training exam-
ples to learn meaningful representations, which
in the quiz bowl setting means we need multiple
questions about the same answer. Fortunately,
hundreds of questions are produced during the
school year for quiz bowl competitions, yield-
ing many different examples of questions ask-
ing about any entity of note (see Section 4.1
for more details). Thus, we have built-in re-
dundancy (the number of ?askable? entities is
limited), but also built-in diversity, as difficult
clues cannot appear in every question without
becoming well-known.
3 Dependency-Tree Recursive
Neural Networks
To compute distributed representations for the
individual sentences within quiz bowl ques-
tions, we use a dependency-tree rnn (dt-rnn).
These representations are then aggregated and
fed into a multinomial logistic regression clas-
sifier, where class labels are the answers asso-
ciated with each question instance.
In previous work, Socher et al. (2014) use
dt-rnns to map text descriptions to images.
dt-rnns are robust to similar sentences with
slightly different syntax, which is ideal for our
problem since answers are often described by
many sentences that are similar in meaning
but different in structure. Our model improves
upon the existing dt-rnn model by jointly
learning answer and question representations
in the same vector space rather than learning
them separately.
3.1 Model Description
As in other rnn models, we begin by associ-
ating each word w in our vocabulary with a
vector representation x
w
? R
d
. These vectors
are stored as the columns of a d ? V dimen-
sional word embedding matrix W
e
, where V is
the size of the vocabulary. Our model takes
dependency parse trees of question sentences
(De Marneffe et al., 2006) and their correspond-
ing answers as input.
Each node n in the parse tree for a partic-
ular sentence is associated with a word w, a
word vector x
w
, and a hidden vector h
n
? R
d
of the same dimension as the word vectors. For
internal nodes, this vector is a phrase-level rep-
resentation, while at leaf nodes it is the word
vector x
w
mapped into the hidden space. Un-
like in constituency trees where all words reside
at the leaf level, internal nodes of dependency
trees are associated with words. Thus, the dt-
rnn has to combine the current node?s word
vector with its children?s hidden vectors to form
h
n
. This process continues recursively up to
the root, which represents the entire sentence.
We associate a separate d?d matrix W
r
with
each dependency relation r in our dataset and
learn these matrices during training.
1
Syntac-
tically untying these matrices improves com-
1
We had 46 unique dependency relations in our quiz
bowl dataset.
634
This city ?s economy depended on subjugated peasants called helots
ROOT
DET
POSSESSIVE
POSS
NSUBJ
PREP
POBJ
AMOD
VMOD
DOBJ
Figure 2: Dependency parse of a sentence from a question about Sparta.
positionality over the standard rnn model by
taking into account relation identity along with
tree structure. We include an additional d? d
matrix, W
v
, to incorporate the word vector x
w
at a node into the node vector h
n
.
Given a parse tree (Figure 2), we first com-
pute leaf representations. For example, the
hidden representation h
helots
is
h
helots
= f(W
v
? x
helots
+ b), (1)
where f is a non-linear activation function such
as tanh and b is a bias term. Once all leaves
are finished, we move to interior nodes with
already processed children. Continuing from
?helots? to its parent, ?called?, we compute
h
called
=f(W
DOBJ
? h
helots
+W
v
? x
called
+ b). (2)
We repeat this process up to the root, which is
h
depended
=f(W
NSUBJ
? h
economy
+W
PREP
? h
on
+W
v
? x
depended
+ b). (3)
The composition equation for any node n with
children K(n) and word vector x
w
is h
n
=
f(W
v
? x
w
+ b+
?
k?K(n)
W
R(n,k)
? h
k
), (4)
where R(n, k) is the dependency relation be-
tween node n and child node k.
3.2 Training
Our goal is to map questions to their corre-
sponding answer entities. Because there are
a limited number of possible answers, we can
view this as a multi-class classification task.
While a softmax layer over every node in the
tree could predict answers (Socher et al., 2011;
Iyyer et al., 2014), this method overlooks that
most answers are themselves words (features)
in other questions (e.g., a question on World
War II might mention the Battle of the Bulge
and vice versa). Thus, word vectors associated
with such answers can be trained in the same
vector space as question text,
2
enabling us to
model relationships between answers instead
of assuming incorrectly that all answers are
independent.
To take advantage of this observation, we
depart from Socher et al. (2014) by training
both the answers and questions jointly in a
single model, rather than training each sep-
arately and holding embeddings fixed during
dt-rnn training. This method cannot be ap-
plied to the multimodal text-to-image mapping
problem because text captions by definition are
made up of words and thus cannot include im-
ages; in our case, however, question text can
and frequently does include answer text.
Intuitively, we want to encourage the vectors
of question sentences to be near their correct
answers and far away from incorrect answers.
We accomplish this goal by using a contrastive
max-margin objective function described be-
low. While we are not interested in obtaining a
ranked list of answers,
3
we observe better per-
formance by adding the weighted approximate-
rank pairwise (warp) loss proposed in Weston
et al. (2011) to our objective function.
Given a sentence paired with its correct an-
swer c, we randomly select j incorrect answers
from the set of all incorrect answers and denote
this subset as Z. Since c is part of the vocab-
ulary, it has a vector x
c
? W
e
. An incorrect
answer z ? Z is also associated with a vector
x
z
?W
e
. We define S to be the set of all nodes
in the sentence?s dependency tree, where an
individual node s ? S is associated with the
2
Of course, questions never contain their own answer
as part of the text.
3
In quiz bowl, all wrong guesses are equally detri-
mental to a team?s score, no matter how ?close? a guess
is to the correct answer.
635
hidden vector h
s
. The error for the sentence is
C(S, ?) =
?
s?S
?
z?Z
L(rank(c, s, Z))max(0,
1? x
c
? h
s
+ x
z
? h
s
), (5)
where the function rank(c, s, Z) provides the
rank of correct answer c with respect to the
incorrect answers Z. We transform this rank
into a loss function
4
shown by Usunier et al.
(2009) to optimize the top of the ranked list,
L(r) =
r
?
i=1
1/i.
Since rank(c, s, Z) is expensive to compute,
we approximate it by randomly sampling K
incorrect answers until a violation is observed
(x
c
? h
s
< 1 + x
z
? h
s
) and set rank(c, s, Z) =
(|Z|?1)/K, as in previous work (Weston et al.,
2011; Hermann et al., 2014). The model mini-
mizes the sum of the error over all sentences T
normalized by the number of nodes N in the
training set,
J(?) =
1
N
?
t?T
C(t, ?). (6)
The parameters ? = (W
r?R
,W
v
,W
e
, b), where
R represents all dependency relations in the
data, are optimized using AdaGrad(Duchi et
al., 2011).
5
In Section 4 we compare perfor-
mance to an identical model (fixed-qanta)
that excludes answer vectors from W
e
and show
that training them as part of ? produces signif-
icantly better results.
The gradient of the objective function,
?C
??
=
1
N
?
t?T
?J(t)
??
, (7)
is computed using backpropagation through
structure (Goller and Kuchler, 1996).
3.3 From Sentences to Questions
The model we have just described considers
each sentence in a quiz bowl question indepen-
dently. However, previously-heard sentences
within the same question contain useful infor-
mation that we do not want our model to ignore.
4
Our experiments show that adding this loss term to
the objective function not only increases performance
but also speeds up convergence
5
We set the initial learning rate ? = 0.05 and reset
the squared gradient sum to zero every five epochs.
While past work on rnn models have been re-
stricted to the sentential and sub-sentential
levels, we show that sentence-level representa-
tions can be easily combined to generate useful
representations at the larger paragraph level.
The simplest and best
6
aggregation method
is just to average the representations of each
sentence seen so far in a particular question.
As we show in Section 4, this method is very
powerful and performs better than most of our
baselines. We call this averaged dt-rnn model
qanta: a question answering neural network
with trans-sentential averaging.
4 Experiments
We compare the performance of qanta against
multiple strong baselines on two datasets.
qanta outperforms all baselines trained only
on question text and improves an information
retrieval model trained on all of Wikipedia.
qanta requires that an input sentence de-
scribes an entity without mentioning that
entity, a constraint that is not followed by
Wikipedia sentences.
7
While ir methods can
operate over Wikipedia text with no issues,
we show that the representations learned by
qanta over just a dataset of question-answer
pairs can significantly improve the performance
of ir systems.
4.1 Datasets
We evaluate our algorithms on a corpus of over
100,000 question/answer pairs from two differ-
ent sources. First, we expand the dataset used
in Boyd-Graber et al. (2012) with publically-
available questions from quiz bowl tournaments
held after that work was published. This gives
us 46,842 questions in fourteen different cate-
gories. To this dataset we add 65,212 questions
from naqt, an organization that runs quiz
bowl tournaments and generously shared with
us all of their questions from 1998?2013.
6
We experimented with weighting earlier sentences
less than later ones in the average as well as learning an
additional RNN on top of the sentence-level representa-
tions. In the former case, we observed no improvements
over a uniform average, while in the latter case the
model overfit even with strong regularization.
7
We tried transforming Wikipedia sentences into
quiz bowl sentences by replacing answer mentions with
appropriate descriptors (e.g., ?Joseph Heller? with ?this
author?), but the resulting sentences suffered from a
variety of grammatical issues and did not help the final
result.
636
Because some categories contain substan-
tially fewer questions than others (e.g., astron-
omy has only 331 questions), we consider only
literature and history questions, as these two
categories account for more than 40% of the
corpus. This leaves us with 21,041 history ques-
tions and 22,956 literature questions.
4.1.1 Data Preparation
To make this problem feasible, we only consider
a limited set of the most popular quiz bowl an-
swers. Before we filter out uncommon answers,
we first need to map all raw answer strings to
a canonical set to get around formatting and
redundancy issues. Most quiz bowl answers are
written to provide as much information about
the entity as possible. For example, the follow-
ing is the raw answer text of a question on the
Chinese leader Sun Yat-sen: Sun Yat-sen; or
Sun Yixian; or Sun Wen; or Sun Deming; or
Nakayama Sho; or Nagao Takano. Quiz bowl
writers vary in how many alternate acceptable
answers they provide, which makes it tricky to
strip superfluous information from the answers
using rule-based approaches.
Instead, we use Whoosh,
8
an information re-
trieval library, to generate features in an active
learning classifier that matches existing answer
strings to Wikipedia titles. If we are unable
to find a match with a high enough confidence
score, we throw the question out of our dataset.
After this standardization process and manual
vetting of the resulting output, we can use the
Wikipedia page titles as training labels for the
dt-rnn and baseline models.
9
65.6% of answers only occur once or twice
in the corpus. We filter out all answers that
do not occur at least six times, which leaves
us with 451 history answers and 595 literature
answers that occur on average twelve times
in the corpus. These pruning steps result in
4,460 usable history questions and 5,685 liter-
ature questions. While ideally we would have
used all answers, our model benefits from many
training examples per answer to learn mean-
ingful representations; this issue can possibly
be addressed with techniques from zero shot
learning (Palatucci et al., 2009; Pasupat and
Liang, 2014), which we leave to future work.
8
https://pypi.python.org/pypi/Whoosh/
9
Code and non-naqt data available at http://cs.
umd.edu/
~
miyyer/qblearn.
We apply basic named entity recogni-
tion (ner) by replacing all occurrences of
answers in the question text with single
entities (e.g., Ernest Hemingway becomes
Ernest Hemingway). While we experimented
with more advanced ner systems to detect
non-answer entities, they could not handle
multi-word named entities like the book Love
in the Time of Cholera (title case) or battle
names (e.g., Battle of Midway). A simple
search/replace on all answers in our corpus
works better for multi-word entities.
The preprocessed data are split into folds
by tournament. We choose the past two na-
tional tournaments
10
as our test set as well
as questions previously answered by players in
Boyd-Graber et al. (2012) and assign all other
questions to train and dev sets. History results
are reported on a training set of 3,761 ques-
tions with 14,217 sentences and a test set of
699 questions with 2,768 sentences. Literature
results are reported on a training set of 4,777
questions with 17,972 sentences and a test set
of 908 questions with 3,577 sentences.
Finally, we initialize the word embedding
matrix W
e
with word2vec (Mikolov et al., 2013)
trained on the preprocessed question text in
our training set.
11
We use the hierarchical skip-
gram model setting with a window size of five
words.
4.2 Baselines
We pit qanta against two types of baselines:
bag of words models, which enable comparison
to a standard NLP baseline, and information
retrieval models, which allow us to compare
against traditional question answering tech-
niques.
BOW The bow baseline is a logistic regres-
sion classifier trained on binary unigram indi-
cators.
12
This simple discriminative model is
an improvement over the generative quiz bowl
answering model of Boyd-Graber et al. (2012).
10
The tournaments were selected because naqt does
not reuse any questions or clues within these tourna-
ments.
11
Out-of-vocabulary words from the test set are ini-
tialized randomly.
12
Raw word counts, frequencies, and TF-IDF
weighted features did not increase performance, nor
did adding bigrams to the feature set (possibly because
multi-word named entities are already collapsed into
single words).
637
BOW-DT The bow-dt baseline is identical
to bow except we augment the feature set with
dependency relation indicators. We include
this baseline to isolate the effects of the depen-
dency tree structure from our compositional
model.
IR-QB The ir-qb baseline maps questions to
answers using the state-of-the-art Whoosh ir
engine. The knowledge base for ir-qb consists
of ?pages? associated with each answer, where
each page is the union of training question text
for that answer. Given a partial question, the
text is first preprocessed using a query lan-
guage similar to that of Apache Lucene. This
processed query is then matched to pages uses
bm-25 term weighting, and the top-ranked page
is considered to be the model?s guess. We also
incorporate fuzzy queries to catch misspellings
and plurals and use Whoosh?s built-in query ex-
pansion functionality to add related keywords
to our queries. IR-WIKI The ir-wiki model
is identical to the ir-qb model except that each
?page? in its knowledge base also includes all
text from the associated answer?s Wikipedia
article. Since all other baselines and dt-rnn
models operate only on the question text, this
is not a valid comparison, but we offer it to
show that we can improve even this strong
model using qanta.
4.3 DT-RNN Configurations
For all dt-rnn models the vector dimension d
and the number of wrong answers per node j
is set to 100. All model parameters other than
W
e
are randomly initialized. The non-linearity
f is the normalized tanh function,
13
f(v) =
tanh(v)
?tanh(v)?
. (8)
qanta is our dt-rnn model with feature
averaging across previously-seen sentences in a
question. To obtain the final answer prediction
given a partial question, we first generate a
feature representation for each sentence within
that partial question. This representation is
computed by concatenating together the word
embeddings and hidden representations aver-
aged over all nodes in the tree as well as the
13
The standard tanh function produced heavy sat-
uration at higher levels of the trees, and corrective
weighting as in Socher et al. (2014) hurt our model
because named entities that occur as leaves are often
more important than non-terminal phrases.
root node?s hidden vector. Finally, we send
the average of all of the individual sentence fea-
tures
14
as input to a logistic regression classifier
for answer prediction.
fixed-qanta uses the same dt-rnn configu-
ration as qanta except the answer vectors are
kept constant as in the text-to-image model.
4.4 Human Comparison
Previous work provides human answers (Boyd-
Graber et al., 2012) for quiz bowl questions.
We use human records for 1,201 history guesses
and 1,715 literature guesses from twenty-two of
the quiz bowl players who answered the most
questions.
15
The standard scoring system for quiz bowl is
10 points for a correct guess and -5 points for
an incorrect guess. We use this metric to com-
pute a total score for each human. To obtain
the corresponding score for our model, we force
it to imitate each human?s guessing policy. For
example, Figure 3 shows a human answering
in the middle of the second sentence. Since our
model only considers sentence-level increments,
we compare the model?s prediction after the
first sentence to the human prediction, which
means our model is privy to less information
than humans.
The resulting distributions are shown in Fig-
ure 4?our model does better than the average
player on history questions, tying or defeat-
ing sixteen of the twenty-two players, but it
does worse on literature questions, where it
only ties or defeats eight players. The figure
indicates that literature questions are harder
than history questions for our model, which is
corroborated by the experimental results dis-
cussed in the next section.
5 Discussion
In this section, we examine why qanta im-
proves over our baselines by giving examples
of questions that are incorrectly classified by
all baselines but correctly classified by qanta.
We also take a close look at some sentences that
all models fail to answer correctly. Finally, we
visualize the answer space learned by qanta.
14
Initial experiments with L
2
regularization hurt per-
formance on a validation set.
15
Participants were skilled quiz bowl players and are
not representative of the general population.
638
History Literature
Model Pos 1 Pos 2 Full Pos 1 Pos 2 Full
bow 27.5 51.3 53.1 19.3 43.4 46.7
bow-dt 35.4 57.7 60.2 24.4 51.8 55.7
ir-qb 37.5 65.9 71.4 27.4 54.0 61.9
fixed-qanta 38.3 64.4 66.2 28.9 57.7 62.3
qanta 47.1 72.1 73.7 36.4 68.2 69.1
ir-wiki 53.7 76.6 77.5 41.8 74.0 73.3
qanta+ir-wiki 59.8 81.8 82.3 44.7 78.7 76.6
Table 1: Accuracy for history and literature at the first two sentence positions of each question
and the full question. The top half of the table compares models trained on questions only, while
the IR models in the bottom half have access to Wikipedia. qanta outperforms all baselines
that are restricted to just the question data, and it substantially improves an IR model with
access to Wikipedia despite being trained on much less data.
200
150
100
50
0
50
100
150
200
Sco
re D
iffe
ren
ce
History: Model vs. Human
Model losesModel wins 400300
200
100
0
100
200
Sco
re D
iffe
ren
ce
Literature: Model vs. Human
Model losesModel wins
Figure 4: Comparisons of qanta+ir-wiki to human quiz bowl players. Each bar represents an
individual human, and the bar height corresponds to the difference between the model score and
the human score. Bars are ordered by human skill. Red bars indicate that the human is winning,
while blue bars indicate that the model is winning. qanta+ir-wiki outperforms most humans
on history questions but fails to defeat the ?average? human on literature questions.
A minor character in this play can be summoned
by a bell that does not always work; that character
also doesn?t have eyelids. Near the end, a woman
who drowned her illegitimate child attempts to stab
another woman in the Second Empire-style
3
room
in which the entire play takes place. For 10 points,
Estelle and Ines are characters in which existentialist
play in which Garcin claims ?Hell is other people?,
written by Jean-Paul Sartre?
Figure 3: A question on the play ?No Exit?
with human buzz position marked as
3
. Since
the buzz occurs in the middle of the second
sentence, our model is only allowed to see the
first sentence.
5.1 Experimental Results
Table 1 shows that when bag of words and
information retrieval methods are restricted to
question data, they perform significantly worse
than qanta on early sentence positions. The
performance of bow-dt indicates that while
the dependency tree structure helps by itself,
the compositional distributed representations
learned by qanta are more useful. The signif-
icant improvement when we train answers as
part of our vocabulary (see Section 3.2) indi-
cates that our model uses answer occurrences
within question text to learn a more informa-
tive vector space.
The disparity between ir-qb and ir-wiki
indicates that the information retrieval models
need lots of external data to work well at all
sentence positions. ir-wiki performs better
than other models because Wikipedia contains
many more sentences that partially match spe-
cific words or phrases found in early clues than
the question training set. In particular, it is
impossible for all other models to answer clues
in the test set that have no semantically similar
639
or equivalent analogues in the training ques-
tion data. With that said, ir methods can
also operate over data that does not follow the
special constraints of quiz bowl questions (e.g.,
every sentence uniquely identifies the answer,
answers don?t appear in their corresponding
questions), which qanta cannot handle. By
combining qanta and ir-wiki, we are able to
leverage access to huge knowledge bases along
with deep compositional representations, giv-
ing us the best of both worlds.
5.2 Where the Attribute Space Helps
Answer Questions
We look closely at the first sentence from a
literature question about the author Thomas
Mann: ?He left unfinished a novel whose title
character forges his father?s signature to get
out of school and avoids the draft by feigning
desire to join?.
All baselines, including ir-wiki, are unable
to predict the correct answer given only this
sentence. However, qanta makes the correct
prediction. The sentence contains no named
entities, which makes it almost impossible for
bag of words or string matching algorithms to
predict correctly. Figure 6 shows that the plot
description associated with the ?novel? node
is strongly indicative of the answer. The five
highest-scored answers are all male authors,
16
which shows that our model is able to learn the
answer type without any hand-crafted rules.
Our next example, the first sentence in Ta-
ble 2, is from the first position of a question
on John Quincy Adams, which is correctly an-
swered by only qanta. The bag of words
model guesses Henry Clay, who was also a Sec-
retary of State in the nineteenth century and
helped John Quincy Adams get elected to the
presidency in a ?corrupt bargain?. However,
the model can reason that while Henry Clay
was active at the same time and involved in
the same political problems of the era, he did
not represent the Amistad slaves, nor did he
negotiate the Treaty of Ghent.
5.3 Where all Models Struggle
Quiz bowl questions are intentionally written to
make players work to get the answer, especially
at early sentence positions. Our model fails to
16
three of whom who also have well-known unfinished
novels
answer correctly more than half the time after
hearing only the first sentence. We examine
some examples to see if there are any patterns
to what makes a question ?hard? for machine
learning models.
Consider this question about the Italian ex-
plorer John Cabot: ?As a young man, this
native of Genoa disguised himself as a Muslim
to make a pilgrimage to Mecca?.
While it is obvious to human readers that
the man described in this sentence is not actu-
ally a Muslim, qanta has to accurately model
the verb disguised to make that inference. We
show the score plot of this sentence in Figure 7.
The model, after presumably seeing many in-
stances of muslim and mecca associated with
Mughal emperors, is unable to prevent this
information from propagating up to the root
node. On the bright side, our model is able to
learn that the question is expecting a human
answer rather than non-human entities like the
Umayyad Caliphate.
More examples of impressive answers by
qanta as well as incorrect guesses by all sys-
tems are shown in Table 2.
5.4 Examining the Attribute Space
Figure 5 shows a t-SNE visualization (Van der
Maaten and Hinton, 2008) of the 451 answers
in our history dataset. The vector space is
divided into six general clusters, and we focus
in particular on the us presidents. Zooming
in on this section reveals temporal clustering:
presidents who were in office during the same
timeframe occur closer together. This observa-
tion shows that qanta is capable of learning
attributes of entities during training.
6 Related Work
There are two threads of related work relevant
to this paper. First, we discuss previous ap-
plications of compositional vector models to
related NLP tasks. Then, we examine existing
work on factoid question-answering and review
the similarities and differences between these
tasks and the game of quiz bowl.
6.1 Recursive Neural Networks for
NLP
The principle of semantic composition states
that the meaning of a phrase can be derived
640
TSNE-1TSNE-2 Wars, rebellions, and battlesU.S. presidentsPrime ministersExplorers & emperorsPoliciesOthertammany_hall calvin_coolidgelollardy fourth_crusadesonghai_empire peace_of_westphaliainca_empireatahualpa charles_sumnerjohn_paul_jones wounded_knee_massacrehuldrych_zwingli darius_ibattle_of_ayacuchojohn_cabotghana ulysses_s._grant hartford_conventioncivilian_conservation_corpsroger_williams_(theologian)george_h._pendleton william_mckinleyvictoria_woodhullcredit_mobilier_of_america_scandal henry_cabot_lodge,_jr. mughal_empire john_marshallcultural_revolutionguadalcanallouisiana_purchasenight_of_the_long_kniveschandragupta_mauryasamuel_de_champlainthirty_years'_war compromise_of_1850battle_of_hastingsbattle_of_salamisakbar lewis_cassdawes_plan hernando_de_soto carthage joseph_mccarthymainesalvador_allende battle_of_gettysburgmikhail_gorbachev aaron_burrequal_rights_amendmentwar_of_the_spanish_successioncoxey's_army george_meadefourteen_pointsmapp_v._ohio sam_houston ming_ ynastyboxer_rebellionanti-masonic_partyporfirio_diaz treaty_of_portsmouththebes,_greece golden_hordefrancisco_i._madero hittitesjames_g._blaineschenck_v._united_states caligulawilliam_walker_(filibuster)henry_vii_of_ nglandkonrad_adenauerkellogg-briand_pact battle_of_cullodentreaty_of_brest-litovsk william_p nna._philip_randolphh nry_l._stimsonwhig_party_(united_states)caroline_affair clarence_darrowwhiskey_rebellionbattle_of_midwaybattle_of_lepantoadolf_eichmanngeorges_clemenceau battle_of_the_little_bighornpontiac_(person) black_hawk_warbattle_of_tannenbergclayton_antitrust_actprovisions_of_oxford battle_of_actiumsuez_crisis spartacusdorr_rebellion jay_treatytriangle_shirtwaist_factory_fire kamakura_shogunatejulius_nyerere frederick_douglasspierre_trudeaunagasaki suleiman_the_magnificentfalklands_war war_of_devolutioncharlemagnedaniel_boone edict_of_nantesharry_s._trumanshakapedro_alvares_cabralthomas_hart_benton_(politician)battle_of_the_coral_sea peterloo_massacrebattle_of_bosworth_fieldroger_b._taneybernardo_o'higginsneville_chamberlainhenry_hudson cyrus_the_great jane_addamsrough_ridersjames_a._garfieldnapoleon_iii missouri_compromisebattle_of_leyte_gulfambrose_burnsidetrent_affairmaria_theresawilliam_ewart_gladstone walter_mondalebarry_goldw terlouis_rielhideki_tojo marco_polobrian_mulroney truman_doctrineroald_amundsen tokugawa_shogunateeleanor_of_aquitaine louis_brandeisbattle_of_trentonkhmer_empirebenito_juarez battle_of_antietamwhiskey_ring otto_von_bismarckbook r_t._washingtonbattle_of_bannockburneugene_v._debs erie_canaljameson_raid green_mountain_boyshaymarket_affairfinlandfashoda_incident battle_of_shilohhannibal john_jayeaster_rising jamaicabrook_farm umayyad_caliph temuhammadfrancis_drakeclara_barton shays'_rebellion verdunhadrianvyacheslav_molotov oda_nobunagacanossasamuel_gompers battle_of_bunker_hi lbattle_of_plasseydavid_livingstonesol npericles tang_dynastyteutonic_knightssecond_vatican_councilalfred_dreyfushenry_the_navigatornelson_mandelapeasants'_revolt gaius_mariusgetulio_vargas horatio_gatesjohn_t._scopes league_of_nationsfirst_battle_of_bull_runalfred_the_greatleonid_brezhn cherokee long_marchemiliano_zapata james_monroewoodrow_wilsonvandals william_henry_harrisonbattle_of_puebla battle_of_zamajustinian_i thaddeus_stevenscecil_rhodeskwame_nkrumah diet_of_wormsgeorge_armstrong_custerbattle_of_agincourtseminole_wars shah_jahanamerigo_vespucci john_foster_dulleslester_b._pearson oregon_trail claudiuslateran_treatychester_a._arthuropium_wars treaty_of_utrechtknights_of_labor alexander_hamiltonplessy_v._ferguson hora e_greeleymary baker_eddyalex nder_kerensky jacquerie treaty_of_ghentb y_of_pigs_invasionantonio_lopez_de_santa_anna great_northern_warhenry_i_of_england council_of_trentchiang_kai-sheksamuel_j._tildenfidel_castro wilmot_proviso yu n_dynastybastille b njamin_harrisonwar_of_the_austrian_successioncrimean_warjohn_brown_(abolitionist)teapot_dome_scandal albert_b._fallmarcus_licinius_crassus earl_warrenwarren_g._harding gunpowder_plothomestead_strike samuel_adamsjohn_peter_zenger thomas_painefree_soil_partyst._barth lomew's_day_massacrearthur_wellesley,_1st_duke_of_wellingtoncharles_de_gaulleleon_trotsky hugh_capetal xander_h._stephens haile_selassieilliam_h._sewardrutherford_b._hayes safavid_dynastymuhammad_ali_jinnah kulturkampfmaximilien_de_robespierre hub rt_humphreyluddite hull_housephilip_ii_of_macedon guelphs_a d_ghibellines byzantine_empirealbigensian_crusade diocletianfort_ticonderoga parthian_empirecharles_martel william_jennings_bryanalexan er_ii_of_russiaferdinand_magellanstate_of_franklin ivan_the_terriblemartin_luther_(1953_film) millard_fillmorefrancisco_francoaethelred_th _unready ronald_reaganbenito_mussolini henry_claykitchen_cabinetblack_hole_of_calcuttaancient_corinth john_wilkes_b th john_tylerrobert_walpolehuey_long tokugawa_ieyasuthomas_nastnikita_khrushchev andrew_jacksonportug llabour_party_(uk) monroe_doctrine john_quincy_adamscongress_of_berlintecumsehjacques_cartier battle_ f_the_thamesspanis _civil_warethiopia fugitiv _slave_lawsjoh _a._macdonald council_of_chalcedonpancho_villa war_of_the_pacific george_ allacesusan_b._anthonymarcus_garvey grover_clevelandjohn_haygeorge_b._mcclellanoctober_manifestovitus_bering john_hanc ckwilliam_lloyd_garrisonplatt_amendmentmary,_queen_of_scotsfirst_triumviratefra cisc _vasquez_de_coronadomargaret_thatcher sherma _antitrust_acthanseatic_leaguehenry_morton_stanley ju y_revolutionstephen_a._douglas xyz_affair jimmy_ca terfrancisco_pizarrokublai_khanvasco_da_gama spartabattle_of_caporetto ostend_manifestomustafa_kemal_ataturk peter_the_greatgang_of_fourbattle_of_chance lorsvilledavid_lloyd_georgecardinal_mazarin embargo_act_of_1807 brigham_youngcharles_lindberghhudson's_bay_company attilaparis_commu e jefferson_davisamelia_earhart mali_empireadolf_hitler benedict_arnoldcamillo_benso,_count_of_cavour meiji_restorationblack_panther_party mark_antony f anklin_p ercemolly_maguires zachary_taylorhan_dynastyadlai_stevenson_iijames_k._polk douglas_macarthurboston_massacretoyotomi_hidey shigreenback_partysecond_boer_warthird_crusade j es_buchananjoh _she mangeorge_washingtonwars_of_the_rosesatlantic_charter eleanor_rooseveltcongress_ f_viennajohn_wycliffewinston_churchillmilio_aguinaldom guel_hidalgo_ costill second_bank_of_the_united_statescouncil_of constanceseneca_falls_convention first_crusade spiro_agnewtaiping_rebellionmao_zedong paul_von_hindenburgalbany_congressjawaharlal_nehru battle_of_blenheimethan_allenantonio_de_oliveira_salazar herbert_hooverpepin_the_shortindira_gandhi william_howard_taftthomas_jeffersonga a _abdel_nasser oliver_cromw llsalmon_p._chase battle_of_austerlitzbenjamin_disraeli gadsden purchasegirolamo_savonarola treaty_of_tordesillasbattle_of_marathon elizabeth_cady_stantonbattle_of_kings_mountainchristopher_colum uswilliam_the_conquerorbattle_of_trafalgar charles_evans_hughescleistheneswilliam_tecumseh_shermanmobutu_sese ek prague_spring babur peloponn sian_w rjacques_marquette neroparaguay hyksos artin_van_burenbonus rmycha les_stew rt_parnell edward_the_confessorbartolome _d ssalem_witch_trials battle of_ he_bulge john_ damsaginot_lin henry_cabot_logiuseppe_garib ldi daniel_websterjohn_c._calhoun treaty_of_waitangizebulon_pike genghis_khan
calvin_coolidgewilliam_mckinley
james_monroe
woodrow_wilson
william_henry_harrisonbenjamin_harrisonmillard_fillmoreronald_reagan
john_tyler andrew_jacksonjohn_quincy_adamsgrover_clevelandjimmy_carter
franklin_piercezachary_taylorjames_buchanangeorge_washington
herbert_hooverwilliam_howard_taft
thomas_jefferson
martin_van_buren
john_adams
Figure 5: t-SNE 2-D projections of 451 answer
vectors divided into six major clusters. The
blue cluster is predominantly populated by U.S.
presidents. The zoomed plot reveals temporal
clustering among the presidents based on the
years they spent in office.
from the meaning of the words that it con-
tains as well as the syntax that glues those
words together. Many computational models
of compositionality focus on learning vector
spaces (Zanzotto et al., 2010; Erk, 2012; Grefen-
stette et al., 2013; Yessenalina and Cardie,
2011). Recent approaches towards modeling
compositional vector spaces with neural net-
works have been successful, although simpler
functions have been proposed for short phrases
(Mitchell and Lapata, 2008).
Recursive neural networks have achieved
state-of-the-art performance in sentiment anal-
ysis and parsing (Socher et al., 2013c; Hermann
and Blunsom, 2013; Socher et al., 2013a). rnns
have not been previously used for learning at-
tribute spaces as we do here, although recursive
tensor networks were unsuccessfully applied to
a knowledge base completion task (Socher et
al., 2013b). More relevant to this work are the
dialogue analysis model proposed by Kalchbren-
ner & Blunsom (2013) and the paragraph vec-
tor model described in Le and Mikolov (2014),
both of which are able to generate distributed
representations of paragraphs. Here we present
a simpler approach where a single model is able
to learn complex sentence representations and
average them across paragraphs.
6.2 Factoid Question-Answering
Factoid question answering is often functionally
equivalent to information retrieval. Given a
knowledge base and a query, the goal is to
Thomas Mann
Joseph Conrad
Henrik Ibsen
Franz Kafka
Henry James
Figure 6: A question on the German novelist
Thomas Mann that contains no named entities,
along with the five top answers as scored by
qanta. Each cell in the heatmap corresponds
to the score (inner product) between a node
in the parse tree and the given answer, and
the dependency parse of the sentence is shown
on the left. All of our baselines, including ir-
wiki, are wrong, while qanta uses the plot
description to make a correct guess.
return the answer. Many approaches to this
problem rely on hand-crafted pattern matching
and answer-type classification to narrow down
the search space (Shen, 2007; Bilotti et al.,
2010; Wang, 2006). More recent factoid qa
systems incorporate the web and social media
into their retrieval systems (Bian et al., 2008).
In contrast to these approaches, we place the
burden of learning answer types and patterns
on the model.
7 Future Work
While we have shown that dt-rnns are effec-
tive models for quiz bowl question answering,
other factoid qa tasks are more challenging.
Questions like what does the aarp stand for?
from trec qa data require additional infras-
tructure. A more apt comparison would be to
IBM?s proprietary Watson system (Lally et al.,
2012) for Jeopardy, which is limited to single
sentences, or to models trained on Yago (Hof-
fart et al., 2013).
We would also like to fairly compare qanta
641
Akbar
Shah Jahan
Muhammad
Babur
Ghana
Figure 7: An extremely misleading question
about John Cabot, at least to computer models.
The words muslim and mecca lead to three
Mughal emperors in the top five guesses from
qanta; other models are similarly led awry.
with ir-wiki. A promising avenue for future
work would be to incorporate Wikipedia data
into qanta by transforming sentences to look
like quiz bowl questions (Wang et al., 2007) and
to select relevant sentences, as not every sen-
tence in a Wikipedia article directly describes
its subject. Syntax-specific annotation (Sayeed
et al., 2012) may help in this regard.
Finally, we could adapt the attribute space
learned by the dt-rnn to use information from
knowledge bases and to aid in knowledge base
completion. Having learned many facts about
entities that occur in question text, a dt-rnn
could add new facts to a knowledge base or
check existing relationships.
8 Conclusion
We present qanta, a dependency-tree recursive
neural network for factoid question answering
that outperforms bag of words and informa-
tion retrieval baselines. Our model improves
upon a contrastive max-margin objective func-
tion from previous work to dynamically update
answer vectors during training with a single
model. Finally, we show that sentence-level
representations can be easily and effectively
combined to generate paragraph-level represen-
Q he also successfully represented the amistad
slaves and negotiated the treaty of ghent and
the annexation of florida from spain during his
stint as secretary of state under james monroe
A john quincy adams, henry clay, andrew jack-
son
Q this work refers to people who fell on their
knees in hopeless cathedrals and who jumped
off the brooklyn bridge
A howl, the tempest, paradise lost
Q despite the fact that twenty six martyrs were
crucified here in the late sixteenth century it
remained the center of christianity in its coun-
try
A nagasaki, guadalcanal, ethiopia
Q this novel parodies freudianism in a chapter
about the protagonist ?s dream of holding a
live fish in his hands
A
billy budd, the ambassadors, all my sons
Q a contemporary of elizabeth i he came to power
two years before her and died two years later
A
grover cleveland, benjamin harrison, henry
cabot lodge
Table 2: Five example sentences occuring at
the first sentence position along with their top
three answers as scored by qanta; correct an-
swers are marked with blue and wrong answers
are marked with red. qanta gets the first
three correct, unlike all other baselines. The
last two questions are too difficult for all of
our models, requiring external knowledge (e.g.,
Freudianism) and temporal reasoning.
tations with more predictive power than those
of the individual sentences.
Acknowledgments
We thank the anonymous reviewers, Stephanie
Hwa, Bert Huang, and He He for their insight-
ful comments. We thank Sharad Vikram, R.
Hentzel, and the members of naqt for pro-
viding our data. This work was supported by
nsf Grant IIS-1320538. Boyd-Graber is also
supported by nsf Grant CCF-1018625. Any
opinions, findings, conclusions, or recommen-
dations expressed here are those of the authors
and do not necessarily reflect the view of the
sponsor.
642
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR.
Jiang Bian, Yandong Liu, Eugene Agichtein, and
Hongyuan Zha. 2008. Finding the right facts in
the crowd: factoid question answering over social
media. In WWW.
Matthew W. Bilotti, Jonathan Elsas, Jaime Carbonell,
and Eric Nyberg. 2010. Rank learning for factoid
question answering with linguistic and semantic con-
straints. In CIKM.
Jordan Boyd-Graber, Brianna Satinoff, He He, and
Hal Daume III. 2012. Besting the quiz master:
Crowdsourcing incremental classification games. In
EMNLP.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 999999:2121?
2159.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on, volume 1.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. CoRR.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In ACL.
Karl Moritz Hermann, Edward Grefenstette, and Phil
Blunsom. 2013. ?not not bad? is not ?bad?: A
distributional account of negation. Proceedings of the
ACL Workshop on Continuous Vector Space Models
and their Compositionality.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. Yago2: A spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, 194:28?61.
Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and
Philip Resnik. 2014. Political ideology detection
using recursive neural networks.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. Proceedings of the 2013 Workshop on
Continuous Vector Space Models and their Composi-
tionality.
Adam Lally, John M Prager, Michael C McCord,
BK Boguraev, Siddharth Patwardhan, James Fan,
Paul Fodor, and Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How watson reads a clue. IBM Journal
of Research and Development.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL.
Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning with
semantic output codes. In NIPS.
P. Pasupat and P. Liang. 2014. Zero-shot entity extrac-
tion from web pages. In ACL.
Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In NAACL.
Dan Shen. 2007. Using semantic role to improve ques-
tion answering. In EMNLP.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Predict-
ing Sentiment Distributions. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing With Composi-
tional Vector Grammars. In ACL.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013b. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In NIPS.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013c. Recursive deep models for
semantic compositionality over a sentiment treebank.
In EMNLP.
Richard Socher, Quoc V Le, Christopher D Manning,
and Andrew Y Ng. 2014. Grounded compositional
semantics for finding and describing images with
sentences. TACL.
Nicolas Usunier, David Buffoni, and Patrick Gallinari.
2009. Ranking with ordered weighted pairwise clas-
sification. In ICML.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP.
643
Mengqiu Wang. 2006. A survey of answer extraction
techniques in factoid question answering. Computa-
tional Linguistics, 1(1).
Jason Weston, Samy Bengio, and Nicolas Usunier. 2011.
Wsabie: Scaling up to large vocabulary image anno-
tation. In IJCAI.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In EMNLP.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In COLT.
644
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532?1543,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
GloVe: Global Vectors for Word Representation
Jeffrey Pennington, Richard Socher, Christopher D. Manning
Computer Science Department, Stanford University, Stanford, CA 94305
jpennin@stanford.edu, richard@socher.org, manning@stanford.edu
Abstract
Recent methods for learning vector space
representations of words have succeeded
in capturing fine-grained semantic and
syntactic regularities using vector arith-
metic, but the origin of these regularities
has remained opaque. We analyze and
make explicit the model properties needed
for such regularities to emerge in word
vectors. The result is a new global log-
bilinear regression model that combines
the advantages of the two major model
families in the literature: global matrix
factorization and local context window
methods. Our model efficiently leverages
statistical information by training only on
the nonzero elements in a word-word co-
occurrence matrix, rather than on the en-
tire sparse matrix or on individual context
windows in a large corpus. The model pro-
duces a vector space with meaningful sub-
structure, as evidenced by its performance
of 75% on a recent word analogy task. It
also outperforms related models on simi-
larity tasks and named entity recognition.
1 Introduction
Semantic vector space models of language repre-
sent each word with a real-valued vector. These
vectors can be used as features in a variety of ap-
plications, such as information retrieval (Manning
et al., 2008), document classification (Sebastiani,
2002), question answering (Tellex et al., 2003),
named entity recognition (Turian et al., 2010), and
parsing (Socher et al., 2013).
Most word vector methods rely on the distance
or angle between pairs of word vectors as the pri-
mary method for evaluating the intrinsic quality
of such a set of word representations. Recently,
Mikolov et al. (2013c) introduced a new evalua-
tion scheme based on word analogies that probes
the finer structure of the word vector space by ex-
amining not the scalar distance between word vec-
tors, but rather their various dimensions of dif-
ference. For example, the analogy ?king is to
queen as man is to woman? should be encoded
in the vector space by the vector equation king ?
queen = man ? woman. This evaluation scheme
favors models that produce dimensions of mean-
ing, thereby capturing the multi-clustering idea of
distributed representations (Bengio, 2009).
The two main model families for learning word
vectors are: 1) global matrix factorization meth-
ods, such as latent semantic analysis (LSA) (Deer-
wester et al., 1990) and 2) local context window
methods, such as the skip-gram model of Mikolov
et al. (2013c). Currently, both families suffer sig-
nificant drawbacks. While methods like LSA ef-
ficiently leverage statistical information, they do
relatively poorly on the word analogy task, indi-
cating a sub-optimal vector space structure. Meth-
ods like skip-gram may do better on the analogy
task, but they poorly utilize the statistics of the cor-
pus since they train on separate local context win-
dows instead of on global co-occurrence counts.
In this work, we analyze the model properties
necessary to produce linear directions of meaning
and argue that global log-bilinear regression mod-
els are appropriate for doing so. We propose a spe-
cific weighted least squares model that trains on
global word-word co-occurrence counts and thus
makes efficient use of statistics. The model pro-
duces a word vector space with meaningful sub-
structure, as evidenced by its state-of-the-art per-
formance of 75% accuracy on the word analogy
dataset. We also demonstrate that our methods
outperform other current methods on several word
similarity tasks, and also on a common named en-
tity recognition (NER) benchmark.
We provide the source code for the model as
well as trained word vectors at http://nlp.
stanford.edu/projects/glove/.
1532
2 Related Work
Matrix Factorization Methods. Matrix factor-
ization methods for generating low-dimensional
word representations have roots stretching as far
back as LSA. These methods utilize low-rank ap-
proximations to decompose large matrices that
capture statistical information about a corpus. The
particular type of information captured by such
matrices varies by application. In LSA, the ma-
trices are of ?term-document? type, i.e., the rows
correspond to words or terms, and the columns
correspond to different documents in the corpus.
In contrast, the Hyperspace Analogue to Language
(HAL) (Lund and Burgess, 1996), for example,
utilizes matrices of ?term-term? type, i.e., the rows
and columns correspond to words and the entries
correspond to the number of times a given word
occurs in the context of another given word.
A main problem with HAL and related meth-
ods is that the most frequent words contribute a
disproportionate amount to the similarity measure:
the number of times two words co-occur with the
or and, for example, will have a large effect on
their similarity despite conveying relatively little
about their semantic relatedness. A number of
techniques exist that addresses this shortcoming of
HAL, such as the COALS method (Rohde et al.,
2006), in which the co-occurrence matrix is first
transformed by an entropy- or correlation-based
normalization. An advantage of this type of trans-
formation is that the raw co-occurrence counts,
which for a reasonably sized corpus might span
8 or 9 orders of magnitude, are compressed so as
to be distributed more evenly in a smaller inter-
val. A variety of newer models also pursue this
approach, including a study (Bullinaria and Levy,
2007) that indicates that positive pointwise mu-
tual information (PPMI) is a good transformation.
More recently, a square root type transformation
in the form of Hellinger PCA (HPCA) (Lebret and
Collobert, 2014) has been suggested as an effec-
tive way of learning word representations.
Shallow Window-Based Methods. Another
approach is to learn word representations that aid
in making predictions within local context win-
dows. For example, Bengio et al. (2003) intro-
duced a model that learns word vector representa-
tions as part of a simple neural network architec-
ture for language modeling. Collobert and Weston
(2008) decoupled the word vector training from
the downstream training objectives, which paved
the way for Collobert et al. (2011) to use the full
context of a word for learning the word represen-
tations, rather than just the preceding context as is
the case with language models.
Recently, the importance of the full neural net-
work structure for learning useful word repre-
sentations has been called into question. The
skip-gram and continuous bag-of-words (CBOW)
models of Mikolov et al. (2013a) propose a sim-
ple single-layer architecture based on the inner
product between two word vectors. Mnih and
Kavukcuoglu (2013) also proposed closely-related
vector log-bilinear models, vLBL and ivLBL, and
Levy et al. (2014) proposed explicit word embed-
dings based on a PPMI metric.
In the skip-gram and ivLBL models, the objec-
tive is to predict a word?s context given the word
itself, whereas the objective in the CBOW and
vLBL models is to predict a word given its con-
text. Through evaluation on a word analogy task,
these models demonstrated the capacity to learn
linguistic patterns as linear relationships between
the word vectors.
Unlike the matrix factorization methods, the
shallow window-based methods suffer from the
disadvantage that they do not operate directly on
the co-occurrence statistics of the corpus. Instead,
these models scan context windows across the en-
tire corpus, which fails to take advantage of the
vast amount of repetition in the data.
3 The GloVe Model
The statistics of word occurrences in a corpus is
the primary source of information available to all
unsupervised methods for learning word represen-
tations, and although many such methods now ex-
ist, the question still remains as to how meaning
is generated from these statistics, and how the re-
sulting word vectors might represent that meaning.
In this section, we shed some light on this ques-
tion. We use our insights to construct a new model
for word representation which we call GloVe, for
Global Vectors, because the global corpus statis-
tics are captured directly by the model.
First we establish some notation. Let the matrix
of word-word co-occurrence counts be denoted by
X , whose entries Xi j tabulate the number of times
word j occurs in the context of word i. Let Xi =
?
k Xik be the number of times any word appears
in the context of word i. Finally, let Pi j = P( j |i) =
Xi j/Xi be the probability that word j appear in the
1533
Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6
billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion
cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and
small values (much less than 1) correlate well with properties specific of steam.
Probability and Ratio k = solid k = gas k = water k = fashion
P(k |ice) 1.9 ? 10
?4
6.6 ? 10
?5
3.0 ? 10
?3
1.7 ? 10
?5
P(k |steam) 2.2 ? 10
?5
7.8 ? 10
?4
2.2 ? 10
?3
1.8 ? 10
?5
P(k |ice)/P(k |steam) 8.9 8.5 ? 10
?2
1.36 0.96
context of word i.
We begin with a simple example that showcases
how certain aspects of meaning can be extracted
directly from co-occurrence probabilities. Con-
sider two words i and j that exhibit a particular as-
pect of interest; for concreteness, suppose we are
interested in the concept of thermodynamic phase,
for which we might take i = ice and j = steam.
The relationship of these words can be examined
by studying the ratio of their co-occurrence prob-
abilities with various probe words, k. For words
k related to ice but not steam, say k = solid, we
expect the ratio Pik/Pjk will be large. Similarly,
for words k related to steam but not ice, say k =
gas, the ratio should be small. For words k like
water or fashion, that are either related to both ice
and steam, or to neither, the ratio should be close
to one. Table 1 shows these probabilities and their
ratios for a large corpus, and the numbers confirm
these expectations. Compared to the raw probabil-
ities, the ratio is better able to distinguish relevant
words (solid and gas) from irrelevant words (water
and fashion) and it is also better able to discrimi-
nate between the two relevant words.
The above argument suggests that the appropri-
ate starting point for word vector learning should
be with ratios of co-occurrence probabilities rather
than the probabilities themselves. Noting that the
ratio Pik/Pjk depends on three words i, j, and k,
the most general model takes the form,
F (wi ,w j , w?k ) =
Pik
Pjk
, (1)
where w ? Rd are word vectors and w? ? Rd
are separate context word vectors whose role will
be discussed in Section 4.2. In this equation, the
right-hand side is extracted from the corpus, and
F may depend on some as-of-yet unspecified pa-
rameters. The number of possibilities for F is vast,
but by enforcing a few desiderata we can select a
unique choice. First, we would like F to encode
the information present the ratio Pik/Pjk in the
word vector space. Since vector spaces are inher-
ently linear structures, the most natural way to do
this is with vector differences. With this aim, we
can restrict our consideration to those functions F
that depend only on the difference of the two target
words, modifying Eqn. (1) to,
F (wi ? w j , w?k ) =
Pik
Pjk
. (2)
Next, we note that the arguments of F in Eqn. (2)
are vectors while the right-hand side is a scalar.
While F could be taken to be a complicated func-
tion parameterized by, e.g., a neural network, do-
ing so would obfuscate the linear structure we are
trying to capture. To avoid this issue, we can first
take the dot product of the arguments,
F
(
(wi ? w j )
T w?k
)
=
Pik
Pjk
, (3)
which prevents F from mixing the vector dimen-
sions in undesirable ways. Next, note that for
word-word co-occurrence matrices, the distinction
between a word and a context word is arbitrary and
that we are free to exchange the two roles. To do so
consistently, we must not only exchange w ? w?
but also X ? X
T
. Our final model should be in-
variant under this relabeling, but Eqn. (3) is not.
However, the symmetry can be restored in two
steps. First, we require that F be a homomorphism
between the groups (R,+) and (R>0,? ), i.e.,
F
(
(wi ? w j )
T w?k
)
=
F (wTi w?k )
F (wTj w?k )
, (4)
which, by Eqn. (3), is solved by,
F (wTi w?k ) = Pik =
Xik
Xi
. (5)
The solution to Eqn. (4) is F = exp, or,
wTi w?k = log(Pik ) = log(Xik ) ? log(Xi ) . (6)
1534
Next, we note that Eqn. (6) would exhibit the ex-
change symmetry if not for the log(Xi ) on the
right-hand side. However, this term is indepen-
dent of k so it can be absorbed into a bias bi for
wi . Finally, adding an additional bias ?bk for w?k
restores the symmetry,
wTi w?k + bi + ?bk = log(Xik ) . (7)
Eqn. (7) is a drastic simplification over Eqn. (1),
but it is actually ill-defined since the logarithm di-
verges whenever its argument is zero. One reso-
lution to this issue is to include an additive shift
in the logarithm, log(Xik ) ? log(1 + Xik ), which
maintains the sparsity of X while avoiding the di-
vergences. The idea of factorizing the log of the
co-occurrence matrix is closely related to LSA and
we will use the resulting model as a baseline in
our experiments. A main drawback to this model
is that it weighs all co-occurrences equally, even
those that happen rarely or never. Such rare co-
occurrences are noisy and carry less information
than the more frequent ones ? yet even just the
zero entries account for 75?95% of the data in X ,
depending on the vocabulary size and corpus.
We propose a new weighted least squares re-
gression model that addresses these problems.
Casting Eqn. (7) as a least squares problem and
introducing a weighting function f (Xi j ) into the
cost function gives us the model
J =
V
?
i, j=1
f
(
Xi j
) (
wTi w? j + bi + ?bj ? log Xi j
)
2
,
(8)
where V is the size of the vocabulary. The weight-
ing function should obey the following properties:
1. f (0) = 0. If f is viewed as a continuous
function, it should vanish as x ? 0 fast
enough that the limx?0 f (x) log
2
x is finite.
2. f (x) should be non-decreasing so that rare
co-occurrences are not overweighted.
3. f (x) should be relatively small for large val-
ues of x, so that frequent co-occurrences are
not overweighted.
Of course a large number of functions satisfy these
properties, but one class of functions that we found
to work well can be parameterized as,
f (x) =
{
(x/x
max
)
?
if x < x
max
1 otherwise .
(9)
0.2
0.4
0.6
0.8
1.0
0.0
Figure 1: Weighting function f with ? = 3/4.
The performance of the model depends weakly on
the cutoff, which we fix to x
max
= 100 for all our
experiments. We found that ? = 3/4 gives a mod-
est improvement over a linear version with ? = 1.
Although we offer only empirical motivation for
choosing the value 3/4, it is interesting that a sim-
ilar fractional power scaling was found to give the
best performance in (Mikolov et al., 2013a).
3.1 Relationship to Other Models
Because all unsupervised methods for learning
word vectors are ultimately based on the occur-
rence statistics of a corpus, there should be com-
monalities between the models. Nevertheless, cer-
tain models remain somewhat opaque in this re-
gard, particularly the recent window-based meth-
ods like skip-gram and ivLBL. Therefore, in this
subsection we show how these models are related
to our proposed model, as defined in Eqn. (8).
The starting point for the skip-gram or ivLBL
methods is a model Qi j for the probability that
word j appears in the context of word i. For con-
creteness, let us assume that Qi j is a softmax,
Qi j =
exp(wTi w? j )
?V
k=1 exp(w
T
i w?k )
. (10)
Most of the details of these models are irrelevant
for our purposes, aside from the the fact that they
attempt to maximize the log probability as a con-
text window scans over the corpus. Training pro-
ceeds in an on-line, stochastic fashion, but the im-
plied global objective function can be written as,
J = ?
?
i?corpus
j?context(i)
logQi j . (11)
Evaluating the normalization factor of the soft-
max for each term in this sum is costly. To al-
low for efficient training, the skip-gram and ivLBL
models introduce approximations to Qi j . How-
ever, the sum in Eqn. (11) can be evaluated much
1535
more efficiently if we first group together those
terms that have the same values for i and j,
J = ?
V
?
i=1
V
?
j=1
Xi j logQi j , (12)
where we have used the fact that the number of
like terms is given by the co-occurrence matrix X .
Recalling our notation for Xi =
?
k Xik and
Pi j = Xi j/Xi , we can rewrite J as,
J = ?
V
?
i=1
Xi
V
?
j=1
Pi j logQi j =
V
?
i=1
XiH (Pi ,Qi ) ,
(13)
where H (Pi ,Qi ) is the cross entropy of the dis-
tributions Pi and Qi , which we define in analogy
to Xi . As a weighted sum of cross-entropy error,
this objective bears some formal resemblance to
the weighted least squares objective of Eqn. (8).
In fact, it is possible to optimize Eqn. (13) directly
as opposed to the on-line training methods used in
the skip-gram and ivLBL models. One could inter-
pret this objective as a ?global skip-gram? model,
and it might be interesting to investigate further.
On the other hand, Eqn. (13) exhibits a number of
undesirable properties that ought to be addressed
before adopting it as a model for learning word
vectors.
To begin, cross entropy error is just one among
many possible distance measures between prob-
ability distributions, and it has the unfortunate
property that distributions with long tails are of-
ten modeled poorly with too much weight given
to the unlikely events. Furthermore, for the mea-
sure to be bounded it requires that the model dis-
tribution Q be properly normalized. This presents
a computational bottleneck owing to the sum over
the whole vocabulary in Eqn. (10), and it would be
desirable to consider a different distance measure
that did not require this property of Q. A natural
choice would be a least squares objective in which
normalization factors in Q and P are discarded,
?
J =
?
i, j
Xi
(
?
Pi j ? ?Qi j
)
2
(14)
where
?
Pi j = Xi j and ?Qi j = exp(wTi w? j ) are the
unnormalized distributions. At this stage another
problem emerges, namely that Xi j often takes very
large values, which can complicate the optimiza-
tion. An effective remedy is to minimize the
squared error of the logarithms of
?
P and
?
Q instead,
?
J =
?
i, j
Xi
(
log
?
Pi j ? log ?Qi j
)
2
=
?
i, j
Xi
(
wTi w? j ? log Xi j
)
2
. (15)
Finally, we observe that while the weighting factor
Xi is preordained by the on-line training method
inherent to the skip-gram and ivLBL models, it is
by no means guaranteed to be optimal. In fact,
Mikolov et al. (2013a) observe that performance
can be increased by filtering the data so as to re-
duce the effective value of the weighting factor for
frequent words. With this in mind, we introduce
a more general weighting function, which we are
free to take to depend on the context word as well.
The result is,
?
J =
?
i, j
f (Xi j )
(
wTi w? j ? log Xi j
)
2
, (16)
which is equivalent
1
to the cost function of
Eqn. (8), which we derived previously.
3.2 Complexity of the model
As can be seen from Eqn. (8) and the explicit form
of the weighting function f (X ), the computational
complexity of the model depends on the number of
nonzero elements in the matrix X . As this num-
ber is always less than the total number of en-
tries of the matrix, the model scales no worse than
O( |V |
2
). At first glance this might seem like a sub-
stantial improvement over the shallow window-
based approaches, which scale with the corpus
size, |C |. However, typical vocabularies have hun-
dreds of thousands of words, so that |V |
2
can be in
the hundreds of billions, which is actually much
larger than most corpora. For this reason it is im-
portant to determine whether a tighter bound can
be placed on the number of nonzero elements of
X .
In order to make any concrete statements about
the number of nonzero elements in X , it is neces-
sary to make some assumptions about the distribu-
tion of word co-occurrences. In particular, we will
assume that the number of co-occurrences of word
i with word j, Xi j , can be modeled as a power-law
function of the frequency rank of that word pair,
ri j :
Xi j =
k
(ri j )
?
. (17)
1
We could also include bias terms in Eqn. (16).
1536
The total number of words in the corpus is pro-
portional to the sum over all elements of the co-
occurrence matrix X ,
|C | ?
?
i j
Xi j =
|X |
?
r=1
k
r
?
= kH|X |,? , (18)
where we have rewritten the last sum in terms of
the generalized harmonic number Hn,m . The up-
per limit of the sum, |X |, is the maximum fre-
quency rank, which coincides with the number of
nonzero elements in the matrix X . This number is
also equal to the maximum value of r in Eqn. (17)
such that Xi j ? 1, i.e., |X | = k
1/?
. Therefore we
can write Eqn. (18) as,
|C | ? |X |
?
H|X |,? . (19)
We are interested in how |X | is related to |C | when
both numbers are large; therefore we are free to
expand the right hand side of the equation for large
|X |. For this purpose we use the expansion of gen-
eralized harmonic numbers (Apostol, 1976),
Hx,s =
x
1?s
1 ? s
+ ? (s) + O(x
?s
) if s > 0, s , 1 ,
(20)
giving,
|C | ?
|X |
1 ? ?
+ ? (?) |X |
?
+ O(1) , (21)
where ? (s) is the Riemann zeta function. In the
limit that X is large, only one of the two terms on
the right hand side of Eqn. (21) will be relevant,
and which term that is depends on whether ? > 1,
|X | =
{
O(|C |) if ? < 1,
O(|C |
1/?
) if ? > 1.
(22)
For the corpora studied in this article, we observe
that Xi j is well-modeled by Eqn. (17) with ? =
1.25. In this case we have that |X | = O(|C |
0.8
).
Therefore we conclude that the complexity of the
model is much better than the worst case O(V
2
),
and in fact it does somewhat better than the on-line
window-based methods which scale like O(|C |).
4 Experiments
4.1 Evaluation methods
We conduct experiments on the word analogy
task of Mikolov et al. (2013a), a variety of word
similarity tasks, as described in (Luong et al.,
2013), and on the CoNLL-2003 shared benchmark
Table 2: Results on the word analogy task, given
as percent accuracy. Underlined scores are best
within groups of similarly-sized models; bold
scores are best overall. HPCA vectors are publicly
available
2
; (i)vLBL results are from (Mnih et al.,
2013); skip-gram (SG) and CBOW results are
from (Mikolov et al., 2013a,b); we trained SG
?
and CBOW
?
using the word2vec tool
3
. See text
for details and a description of the SVD models.
Model Dim. Size Sem. Syn. Tot.
ivLBL 100 1.5B 55.9 50.1 53.2
HPCA 100 1.6B 4.2 16.4 10.8
GloVe 100 1.6B 67.5 54.3 60.3
SG 300 1B 61 61 61
CBOW 300 1.6B 16.1 52.6 36.1
vLBL 300 1.5B 54.2 64.8 60.0
ivLBL 300 1.5B 65.2 63.0 64.0
GloVe 300 1.6B 80.8 61.5 70.3
SVD 300 6B 6.3 8.1 7.3
SVD-S 300 6B 36.7 46.6 42.1
SVD-L 300 6B 56.6 63.0 60.1
CBOW
?
300 6B 63.6 67.4 65.7
SG
?
300 6B 73.0 66.0 69.1
GloVe 300 6B 77.4 67.0 71.7
CBOW 1000 6B 57.3 68.9 63.7
SG 1000 6B 66.1 65.1 65.6
SVD-L 300 42B 38.4 58.2 49.2
GloVe 300 42B 81.9 69.3 75.0
dataset for NER (Tjong Kim Sang and De Meul-
der, 2003).
Word analogies. The word analogy task con-
sists of questions like, ?a is to b as c is to ??
The dataset contains 19,544 such questions, di-
vided into a semantic subset and a syntactic sub-
set. The semantic questions are typically analogies
about people or places, like ?Athens is to Greece
as Berlin is to ??. The syntactic questions are
typically analogies about verb tenses or forms of
adjectives, for example ?dance is to dancing as fly
is to ??. To correctly answer the question, the
model should uniquely identify the missing term,
with only an exact correspondence counted as a
correct match. We answer the question ?a is to b
as c is to ?? by finding the word d whose repre-
sentation wd is closest to wb ? wa + wc according
to the cosine similarity.
4
2
http://lebret.ch/words/
3
http://code.google.com/p/word2vec/
4
Levy et al. (2014) introduce a multiplicative analogy
evaluation, 3COSMUL, and report an accuracy of 68.24% on
1537
0 100 200 300 400 500 60020
30
40
50
60
70
80
Vector Dimension
Accur
acy [%
]
 
SemanticSyntacticOverall
(a) Symmetric context
2 4 6 8 1040
50
55
60
65
70
45
Window Size
Accur
acy [%
]
 
 
 
SemanticSyntacticOverall
(b) Symmetric context
2 4 6 8 1040
50
55
60
65
70
45
Window Size
Accur
acy [%
]
 
 
 
SemanticSyntacticOverall
(c) Asymmetric context
Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are
trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.
Word similarity. While the analogy task is our
primary focus since it tests for interesting vector
space substructures, we also evaluate our model on
a variety of word similarity tasks in Table 3. These
include WordSim-353 (Finkelstein et al., 2001),
MC (Miller and Charles, 1991), RG (Rubenstein
and Goodenough, 1965), SCWS (Huang et al.,
2012), and RW (Luong et al., 2013).
Named entity recognition. The CoNLL-2003
English benchmark dataset for NER is a collec-
tion of documents from Reuters newswire articles,
annotated with four entity types: person, location,
organization, and miscellaneous. We train mod-
els on CoNLL-03 training data on test on three
datasets: 1) ConLL-03 testing data, 2) ACE Phase
2 (2001-02) and ACE-2003 data, and 3) MUC7
Formal Run test set. We adopt the BIO2 annota-
tion standard, as well as all the preprocessing steps
described in (Wang and Manning, 2013). We use a
comprehensive set of discrete features that comes
with the standard distribution of the Stanford NER
model (Finkel et al., 2005). A total of 437,905
discrete features were generated for the CoNLL-
2003 training dataset. In addition, 50-dimensional
vectors for each word of a five-word context are
added and used as continuous features. With these
features as input, we trained a conditional random
field (CRF) with exactly the same setup as the
CRF
join
model of (Wang and Manning, 2013).
4.2 Corpora and training details
We trained our model on five corpora of varying
sizes: a 2010 Wikipedia dump with 1 billion to-
kens; a 2014 Wikipedia dump with 1.6 billion to-
kens; Gigaword 5 which has 4.3 billion tokens; the
combination Gigaword5 + Wikipedia2014, which
the analogy task. This number is evaluated on a subset of the
dataset so it is not included in Table 2. 3COSMUL performed
worse than cosine similarity in almost all of our experiments.
has 6 billion tokens; and on 42 billion tokens of
web data, from Common Crawl
5
. We tokenize
and lowercase each corpus with the Stanford to-
kenizer, build a vocabulary of the 400,000 most
frequent words
6
, and then construct a matrix of co-
occurrence counts X . In constructing X , we must
choose how large the context window should be
and whether to distinguish left context from right
context. We explore the effect of these choices be-
low. In all cases we use a decreasing weighting
function, so that word pairs that are d words apart
contribute 1/d to the total count. This is one way
to account for the fact that very distant word pairs
are expected to contain less relevant information
about the words? relationship to one another.
For all our experiments, we set x
max
= 100,
? = 3/4, and train the model using AdaGrad
(Duchi et al., 2011), stochastically sampling non-
zero elements from X , with initial learning rate of
0.05. We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate). Unless otherwise noted, we use a context of
ten words to the left and ten words to the right.
The model generates two sets of word vectors,
W and
?
W . When X is symmetric, W and
?
W are
equivalent and differ only as a result of their ran-
dom initializations; the two sets of vectors should
perform equivalently. On the other hand, there is
evidence that for certain types of neural networks,
training multiple instances of the network and then
combining the results can help reduce overfitting
and noise and generally improve results (Ciresan
et al., 2012). With this in mind, we choose to use
5
To demonstrate the scalability of the model, we also
trained it on a much larger sixth corpus, containing 840 bil-
lion tokens of web data, but in this case we did not lowercase
the vocabulary, so the results are not directly comparable.
6
For the model trained on Common Crawl data, we use a
larger vocabulary of about 2 million words.
1538
the sum W +
?
W as our word vectors. Doing so typ-
ically gives a small boost in performance, with the
biggest increase in the semantic analogy task.
We compare with the published results of a va-
riety of state-of-the-art models, as well as with
our own results produced using the word2vec
tool and with several baselines using SVDs. With
word2vec, we train the skip-gram (SG
?
) and
continuous bag-of-words (CBOW
?
) models on the
6 billion token corpus (Wikipedia 2014 + Giga-
word 5) with a vocabulary of the top 400,000 most
frequent words and a context window size of 10.
We used 10 negative samples, which we show in
Section 4.6 to be a good choice for this corpus.
For the SVD baselines, we generate a truncated
matrix X
trunc
which retains the information of how
frequently each word occurs with only the top
10,000 most frequent words. This step is typi-
cal of many matrix-factorization-based methods as
the extra columns can contribute a disproportion-
ate number of zero entries and the methods are
otherwise computationally expensive.
The singular vectors of this matrix constitute
the baseline ?SVD?. We also evaluate two related
baselines: ?SVD-S? in which we take the SVD of
?
X
trunc
, and ?SVD-L? in which we take the SVD
of log(1+X
trunc
). Both methods help compress the
otherwise large range of values in X .
7
4.3 Results
We present results on the word analogy task in Ta-
ble 2. The GloVe model performs significantly
better than the other baselines, often with smaller
vector sizes and smaller corpora. Our results us-
ing the word2vec tool are somewhat better than
most of the previously published results. This is
due to a number of factors, including our choice to
use negative sampling (which typically works bet-
ter than the hierarchical softmax), the number of
negative samples, and the choice of the corpus.
We demonstrate that the model can easily be
trained on a large 42 billion token corpus, with a
substantial corresponding performance boost. We
note that increasing the corpus size does not guar-
antee improved results for other models, as can be
seen by the decreased performance of the SVD-
7
We also investigated several other weighting schemes for
transforming X ; what we report here performed best. Many
weighting schemes like PPMI destroy the sparsity of X and
therefore cannot feasibly be used with large vocabularies.
With smaller vocabularies, these information-theoretic trans-
formations do indeed work well on word similarity measures,
but they perform very poorly on the word analogy task.
Table 3: Spearman rank correlation on word simi-
larity tasks. All vectors are 300-dimensional. The
CBOW
?
vectors are from the word2vec website
and differ in that they contain phrase vectors.
Model Size WS353 MC RG SCWS RW
SVD 6B 35.3 35.1 42.5 38.3 25.6
SVD-S 6B 56.5 71.5 71.0 53.6 34.7
SVD-L 6B 65.7 72.7 75.1 56.5 37.0
CBOW
?
6B 57.2 65.6 68.2 57.0 32.5
SG
?
6B 62.8 65.2 69.7 58.1 37.2
GloVe 6B 65.8 72.7 77.8 53.9 38.1
SVD-L 42B 74.0 76.4 74.1 58.3 39.9
GloVe 42B 75.9 83.6 82.9 59.6 47.8
CBOW
?
100B 68.4 79.6 75.4 59.4 45.5
L model on this larger corpus. The fact that this
basic SVD model does not scale well to large cor-
pora lends further evidence to the necessity of the
type of weighting scheme proposed in our model.
Table 3 shows results on five different word
similarity datasets. A similarity score is obtained
from the word vectors by first normalizing each
feature across the vocabulary and then calculat-
ing the cosine similarity. We compute Spearman?s
rank correlation coefficient between this score and
the human judgments. CBOW
?
denotes the vec-
tors available on the word2vec website that are
trained with word and phrase vectors on 100B
words of news data. GloVe outperforms it while
using a corpus less than half the size.
Table 4 shows results on the NER task with the
CRF-based model. The L-BFGS training termi-
nates when no improvement has been achieved on
the dev set for 25 iterations. Otherwise all config-
urations are identical to those used by Wang and
Manning (2013). The model labeled Discrete is
the baseline using a comprehensive set of discrete
features that comes with the standard distribution
of the Stanford NER model, but with no word vec-
tor features. In addition to the HPCA and SVD
models discussed previously, we also compare to
the models of Huang et al. (2012) (HSMN) and
Collobert and Weston (2008) (CW). We trained
the CBOW model using the word2vec tool
8
.
The GloVe model outperforms all other methods
on all evaluation metrics, except for the CoNLL
test set, on which the HPCA method does slightly
better. We conclude that the GloVe vectors are
useful in downstream NLP tasks, as was first
8
We use the same parameters as above, except in this case
we found 5 negative samples to work slightly better than 10.
1539
Table 4: F1 score on NER task with 50d vectors.
Discrete is the baseline without word vectors. We
use publicly-available vectors for HPCA, HSMN,
and CW. See text for details.
Model Dev Test ACE MUC7
Discrete 91.0 85.4 77.4 73.4
SVD 90.8 85.7 77.3 73.7
SVD-S 91.0 85.5 77.6 74.3
SVD-L 90.5 84.8 73.6 71.5
HPCA 92.6 88.7 81.7 80.7
HSMN 90.5 85.7 78.7 74.7
CW 92.2 87.4 81.7 80.2
CBOW 93.1 88.2 82.2 81.1
GloVe 93.2 88.3 82.9 82.2
shown for neural vectors in (Turian et al., 2010).
4.4 Model Analysis: Vector Length and
Context Size
In Fig. 2, we show the results of experiments that
vary vector length and context window. A context
window that extends to the left and right of a tar-
get word will be called symmetric, and one which
extends only to the left will be called asymmet-
ric. In (a), we observe diminishing returns for vec-
tors larger than about 200 dimensions. In (b) and
(c), we examine the effect of varying the window
size for symmetric and asymmetric context win-
dows. Performance is better on the syntactic sub-
task for small and asymmetric context windows,
which aligns with the intuition that syntactic infor-
mation is mostly drawn from the immediate con-
text and can depend strongly on word order. Se-
mantic information, on the other hand, is more fre-
quently non-local, and more of it is captured with
larger window sizes.
4.5 Model Analysis: Corpus Size
In Fig. 3, we show performance on the word anal-
ogy task for 300-dimensional vectors trained on
different corpora. On the syntactic subtask, there
is a monotonic increase in performance as the cor-
pus size increases. This is to be expected since
larger corpora typically produce better statistics.
Interestingly, the same trend is not true for the se-
mantic subtask, where the models trained on the
smaller Wikipedia corpora do better than those
trained on the larger Gigaword corpus. This is
likely due to the large number of city- and country-
based analogies in the analogy dataset and the fact
that Wikipedia has fairly comprehensive articles
for most such locations. Moreover, Wikipedia?s
50
55
60
65
70
75
80
85 OverallSyntacticSemantic
Wiki20101B tokens
Accur
acy [%
]
Wiki20141.6B tokens Gigaword54.3B tokens Gigaword5 + Wiki20146B tokens Common Crawl 42B tokens
Figure 3: Accuracy on the analogy task for 300-
dimensional vectors trained on different corpora.
entries are updated to assimilate new knowledge,
whereas Gigaword is a fixed news repository with
outdated and possibly incorrect information.
4.6 Model Analysis: Run-time
The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across mul-
tiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 ma-
chine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X , the time it takes to train the model de-
pends on the vector size and the number of itera-
tions. For 300-dimensional vectors with the above
settings (and using all 32 cores of the above ma-
chine), a single iteration takes 14 minutes. See
Fig. 4 for a plot of the learning curve.
4.7 Model Analysis: Comparison with
word2vec
A rigorous quantitative comparison of GloVe with
word2vec is complicated by the existence of
many parameters that have a strong effect on per-
formance. We control for the main sources of vari-
ation that we identified in Sections 4.4 and 4.5 by
setting the vector length, context window size, cor-
pus, and vocabulary size to the configuration men-
tioned in the previous subsection.
The most important remaining variable to con-
trol for is training time. For GloVe, the rele-
vant parameter is the number of training iterations.
For word2vec, the obvious choice would be the
number of training epochs. Unfortunately, the
code is currently designed for only a single epoch:
1540
1 2 3 4 5 6
60
62
64
66
68
70
72
5 10 15 20 25
1357 10 15 20 25 30 40 50
Accu
racy 
[%]
Iterations (GloVe)
Negative Samples (CBOW)
Training Time (hrs)
 
GloVeCBOW
(a) GloVe vs CBOW
3 6 9 12 15 18 21 24
60
62
64
66
68
70
72
20 40 60 80 100
1 2 3 4 5 6 7 10 12 15 20
GloVeSkip-Gram
Accu
racy 
[%]
Iterations (GloVe)
Negative Samples (Skip-Gram)
Training Time (hrs)
(b) GloVe vs Skip-Gram
Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by
the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram
(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +
Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.
it specifies a learning schedule specific to a single
pass through the data, making a modification for
multiple passes a non-trivial task. Another choice
is to vary the number of negative samples. Adding
negative samples effectively increases the number
of training words seen by the model, so in some
ways it is analogous to extra epochs.
We set any unspecified parameters to their de-
fault values, assuming that they are close to opti-
mal, though we acknowledge that this simplifica-
tion should be relaxed in a more thorough analysis.
In Fig. 4, we plot the overall performance on
the analogy task as a function of training time.
The two x-axes at the bottom indicate the corre-
sponding number of training iterations for GloVe
and negative samples for word2vec. We note
that word2vec?s performance actually decreases
if the number of negative samples increases be-
yond about 10. Presumably this is because the
negative sampling method does not approximate
the target probability distribution well.
9
For the same corpus, vocabulary, window size,
and training time, GloVe consistently outperforms
word2vec. It achieves better results faster, and
also obtains the best results irrespective of speed.
5 Conclusion
Recently, considerable attention has been focused
on the question of whether distributional word
representations are best learned from count-based
9
In contrast, noise-contrastive estimation is an approxi-
mation which improves with more negative samples. In Ta-
ble 1 of (Mnih et al., 2013), accuracy on the analogy task is a
non-decreasing function of the number of negative samples.
methods or from prediction-based methods. Cur-
rently, prediction-based models garner substantial
support; for example, Baroni et al. (2014) argue
that these models perform better across a range of
tasks. In this work we argue that the two classes
of methods are not dramatically different at a fun-
damental level since they both probe the under-
lying co-occurrence statistics of the corpus, but
the efficiency with which the count-based meth-
ods capture global statistics can be advantageous.
We construct a model that utilizes this main ben-
efit of count data while simultaneously capturing
the meaningful linear substructures prevalent in
recent log-bilinear prediction-based methods like
word2vec. The result, GloVe, is a new global
log-bilinear regression model for the unsupervised
learning of word representations that outperforms
other models on word analogy, word similarity,
and named entity recognition tasks.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments. Stanford University gratefully
acknowledges the support of the Defense Threat
Reduction Agency (DTRA) under Air Force Re-
search Laboratory (AFRL) contract no. FA8650-
10-C-7020 and the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under AFRL
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DTRA,
AFRL, DEFT, or the US government.
1541
References
Tom M. Apostol. 1976. Introduction to Analytic
Number Theory. Introduction to Analytic Num-
ber Theory.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL.
Yoshua Bengio. 2009. Learning deep architectures
for AI. Foundations and Trends in Machine
Learning.
Yoshua Bengio, R?ejean Ducharme, Pascal Vin-
cent, and Christian Janvin. 2003. A neural prob-
abilistic language model. JMLR, 3:1137?1155.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods, 39(3):510?526.
Dan C. Ciresan, Alessandro Giusti, Luca M. Gam-
bardella, and J?urgen Schmidhuber. 2012. Deep
neural networks segment neuronal membranes
in electron microscopy images. In NIPS, pages
2852?2860.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language process-
ing: deep neural networks with multitask learn-
ing. In Proceedings of ICML, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural Language Processing (Al-
most) from Scratch. JMLR, 12:2493?2537.
Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for
Information Science, 41.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learn-
ing and stochastic optimization. JMLR, 12.
Lev Finkelstein, Evgenly Gabrilovich, Yossi Ma-
tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,
and Eytan Ruppin. 2001. Placing search in con-
text: The concept revisited. In Proceedings
of the 10th international conference on World
Wide Web, pages 406?414. ACM.
Eric H. Huang, Richard Socher, Christopher D.
Manning, and Andrew Y. Ng. 2012. Improving
Word Representations via Global Context and
Multiple Word Prototypes. In ACL.
R?emi Lebret and Ronan Collobert. 2014. Word
embeddings through Hellinger PCA. In EACL.
Omer Levy, Yoav Goldberg, and Israel Ramat-
Gan. 2014. Linguistic regularities in sparse and
explicit word representations. CoNLL-2014.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, In-
strumentation, and Computers, 28:203?208.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word represen-
tations with recursive neural networks for mor-
phology. CoNLL-2013.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. In ICLRWork-
shop Papers.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In NIPS, pages 3111?3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey
Zweig. 2013c. Linguistic regularities in con-
tinuous space word representations. In HLT-
NAACL.
George A. Miller and Walter G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and cognitive processes, 6(1):1?28.
Andriy Mnih and Koray Kavukcuoglu. 2013.
Learning word embeddings efficiently with
noise-contrastive estimation. In NIPS.
Douglas L. T. Rohde, Laura M. Gonnerman,
and David C. Plaut. 2006. An improved
model of semantic similarity based on lexical
co-occurence. Communications of the ACM,
8:627?633.
Herbert Rubenstein and John B. Goodenough.
1965. Contextual correlates of synonymy. Com-
munications of the ACM, 8(10):627?633.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing
Surveys, 34:1?47.
Richard Socher, John Bauer, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Parsing With
Compositional Vector Grammars. In ACL.
1542
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron
Fernandes, and Gregory Marton. 2003. Quanti-
tative evaluation of passage retrieval algorithms
for question answering. In Proceedings of the
SIGIR Conference on Research and Develop-
ment in Informaion Retrieval.
Erik F. Tjong Kim Sang and Fien De Meul-
der. 2003. Introduction to the CoNLL-2003
shared task: Language-independent named en-
tity recognition. In CoNLL-2003.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings of ACL, pages 384?394.
Mengqiu Wang and Christopher D. Manning.
2013. Effect of non-linear deep architecture in
sequence labeling. In Proceedings of the 6th
International Joint Conference on Natural Lan-
guage Processing (IJCNLP).
1543
Tutorials, NAACL-HLT 2013, pages 1?3,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Deep Learning for NLP (without Magic)
Richard Socher, Chris Manning
Stanford University
richard@socher.org
manning@stanford.edu
1 Overview
Machine learning is everywhere in today?s NLP, but by and large machine learning
amounts to numerical optimization of weights for human designed representations
and features. The goal of deep learning is to explore how computers can take ad-
vantage of data to develop features and representations appropriate for complex
interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models
and learning algorithms in deep learning for natural language processing. Recently,
these methods have been shown to perform very well on various NLP tasks such
as language modeling, POS tagging, named entity recognition, sentiment analysis
and paraphrase detection, among others. The most attractive quality of these tech-
niques is that they can perform well without any external hand-designed resources
or time-intensive feature engineering. Despite these advantages, many researchers
in NLP are not familiar with these methods. Our focus is on insight and understand-
ing, using graphical illustrations and simple, intuitive derivations. The goal of the
tutorial is to make the inner workings of these techniques transparent, intuitive and
their results interpretable, rather than black boxes labeled ?magic here?. The first
part of the tutorial presents the basics of neural networks, neural word vectors, sev-
eral simple models based on local windows and the math and algorithms of training
via backpropagation. In this section applications include language modeling and
POS tagging. In the second section we present recursive neural networks which
can learn structured tree outputs as well as vector representations for phrases and
sentences. We cover both equations as well as applications. We show how training
can be achieved by a modified version of the backpropagation algorithm intro-
duced before. These modifications allow the algorithm to work on tree structures.
Applications include sentiment analysis and paraphrase detection. We also draw
connections to recent work in semantic compositionality in vector spaces. The
principle goal, again, is to make these methods appear intuitive and interpretable
1
rather than mathematically confusing. By this point in the tutorial, the audience
members should have a clear understanding of how to build a deep learning system
for word-, sentence- and document-level tasks. The last part of the tutorial gives
a general overview of the different applications of deep learning in NLP, includ-
ing bag of words models. We will provide a discussion of NLP-oriented issues in
modeling, interpretation, representational power, and optimization.
2 Outline
Part I: The Basics
? Motivation
? From logistic regression to neural networks
? Theory: Backpropagation training
? Applications: Word vector learning, POS, NER
? Unsupervised pre-training, multi-task learning, and learning relations
PART II: Recursive Neural Networks
? Motivation
? Definition of RNNs
? Theory: Backpropagation through structure
? Applications: Sentiment Analysis, Paraphrase detection, Relation Classifi-
cation
PART III: Applications and Discussion
? Overview of various NLP applications
? Efficient reconstruction or prediction of high-dimensional sparse vectors
? Discussion of future directions, advantages and limitations
2
3 Speaker Bios
Richard Socher1 is a PhD student at Stanford working with Chris Manning and
Andrew Ng. His research interests are machine learning for NLP and vision. He
is interested in developing new models that learn useful features, capture composi-
tional and hierarchical structure in multiple modalities and perform well across dif-
ferent tasks. He was awarded the 2011 Yahoo! Key Scientific Challenges Award,
the Distinguished Application Paper Award at ICML 2011 and a Microsoft Re-
search PhD Fellowship in 2012.
Christopher Manning2 is an Associate Professor of Computer Science and
Linguistics at Stanford University (PhD, Stanford, 1995). Manning has coauthored
leading textbooks on statistical approaches to NLP (Manning and Schuetze 1999)
and information retrieval (Manning et al 2008). His recent work concentrates on
machine learning and natural language processing, including applications such as
statistical parsing and text understanding, joint probabilistic inference, clustering,
and deep learning over text and images.
1http://www.socher.org/
2http://nlp.stanford.edu/?manning/
3
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 873?882,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improving Word Representations via Global Context
and Multiple Word Prototypes
Eric H. Huang, Richard Socher?, Christopher D. Manning, Andrew Y. Ng
Computer Science Department, Stanford University, Stanford, CA 94305, USA
{ehhuang,manning,ang}@stanford.edu, ?richard@socher.org
Abstract
Unsupervised word representations are very
useful in NLP tasks both as inputs to learning
algorithms and as extra word features in NLP
systems. However, most of these models are
built with only local context and one represen-
tation per word. This is problematic because
words are often polysemous and global con-
text can also provide useful information for
learning word meanings. We present a new
neural network architecture which 1) learns
word embeddings that better capture the se-
mantics of words by incorporating both local
and global document context, and 2) accounts
for homonymy and polysemy by learning mul-
tiple embeddings per word. We introduce a
new dataset with human judgments on pairs of
words in sentential context, and evaluate our
model on it, showing that our model outper-
forms competitive baselines and other neural
language models. 1
1 Introduction
Vector-space models (VSM) represent word mean-
ings with vectors that capture semantic and syntac-
tic information of words. These representations can
be used to induce similarity measures by computing
distances between the vectors, leading to many use-
ful applications, such as information retrieval (Man-
ning et al, 2008), document classification (Sebas-
tiani, 2002) and question answering (Tellex et al,
2003).
1The dataset and word vectors can be downloaded at
http://ai.stanford.edu/?ehhuang/.
Despite their usefulness, most VSMs share a
common problem that each word is only repre-
sented with one vector, which clearly fails to capture
homonymy and polysemy. Reisinger and Mooney
(2010b) introduced a multi-prototype VSM where
word sense discrimination is first applied by clus-
tering contexts, and then prototypes are built using
the contexts of the sense-labeled words. However, in
order to cluster accurately, it is important to capture
both the syntax and semantics of words. While many
approaches use local contexts to disambiguate word
meaning, global contexts can also provide useful
topical information (Ng and Zelle, 1997). Several
studies in psychology have also shown that global
context can help language comprehension (Hess et
al., 1995) and acquisition (Li et al, 2000).
We introduce a new neural-network-based lan-
guage model that distinguishes and uses both local
and global context via a joint training objective. The
model learns word representations that better cap-
ture the semantics of words, while still keeping syn-
tactic information. These improved representations
can be used to represent contexts for clustering word
instances, which is used in the multi-prototype ver-
sion of our model that accounts for words with mul-
tiple senses.
We evaluate our new model on the standard
WordSim-353 (Finkelstein et al, 2001) dataset that
includes human similarity judgments on pairs of
words, showing that combining both local and
global context outperforms using only local or
global context alone, and is competitive with state-
of-the-art methods. However, one limitation of this
evaluation is that the human judgments are on pairs
873
Global ContextLocal Context
scorel scoreg
Document
he walks to the bank... ...
sum
score
river
water
shore
global semantic vector
?
play
weighted average
Figure 1: An overview of our neural language model. The model makes use of both local and global context to compute
a score that should be large for the actual next word (bank in the example), compared to the score for other words.
When word meaning is still ambiguous given local context, information in global context can help disambiguation.
of words presented in isolation, ignoring meaning
variations in context. Since word interpretation in
context is important especially for homonymous and
polysemous words, we introduce a new dataset with
human judgments on similarity between pairs of
words in sentential context. To capture interesting
word pairs, we sample different senses of words us-
ing WordNet (Miller, 1995). The dataset includes
verbs and adjectives, in addition to nouns. We show
that our multi-prototype model improves upon the
single-prototype version and outperforms other neu-
ral language models and baselines on this dataset.
2 Global Context-Aware Neural Language
Model
In this section, we describe the training objective of
our model, followed by a description of the neural
network architecture, ending with a brief description
of our model?s training method.
2.1 Training Objective
Our model jointly learns word representations while
learning to discriminate the next word given a short
word sequence (local context) and the document
(global context) in which the word sequence occurs.
Because our goal is to learn useful word representa-
tions and not the probability of the next word given
previous words (which prohibits looking ahead), our
model can utilize the entire document to provide
global context.
Given a word sequence s and document d in
which the sequence occurs, our goal is to discrim-
inate the correct last word in s from other random
words. We compute scores g(s, d) and g(sw, d)
where sw is swith the last word replaced by wordw,
and g(?, ?) is the scoring function that represents the
neural networks used. We want g(s, d) to be larger
than g(sw, d) by a margin of 1, for any other word
w in the vocabulary, which corresponds to the train-
ing objective of minimizing the ranking loss for each
(s, d) found in the corpus:
Cs,d =
?
w?V
max(0, 1? g(s, d) + g(sw, d)) (1)
Collobert and Weston (2008) showed that this rank-
ing approach can produce good word embeddings
that are useful in several NLP tasks, and allows
much faster training of the model compared to op-
timizing log-likelihood of the next word.
2.2 Neural Network Architecture
We define two scoring components that contribute
to the final score of a (word sequence, document)
pair. The scoring components are computed by two
neural networks, one capturing local context and the
other global context, as shown in Figure 1. We now
describe how each scoring component is computed.
The score of local context uses the local word se-
quence s. We first represent the word sequence s as
874
an ordered list of vectors x = (x1, x2, ..., xm) where
xi is the embedding of word i in the sequence, which
is a column in the embedding matrix L ? Rn?|V |
where |V | denotes the size of the vocabulary. The
columns of this embedding matrix L are the word
vectors and will be learned and updated during train-
ing. To compute the score of local context, scorel,
we use a neural network with one hidden layer:
a1 = f(W1[x1;x2; ...;xm] + b1) (2)
scorel = W2a1 + b2 (3)
where [x1;x2; ...;xm] is the concatenation of the
m word embeddings representing sequence s, f is
an element-wise activation function such as tanh,
a1 ? Rh?1 is the activation of the hidden layer with
h hidden nodes, W1 ? Rh?(mn) and W2 ? R1?h
are respectively the first and second layer weights of
the neural network, and b1, b2 are the biases of each
layer.
For the score of the global context, we represent
the document also as an ordered list of word em-
beddings, d = (d1, d2, ..., dk). We first compute the
weighted average of all word vectors in the docu-
ment:
c =
?k
i=1w(ti)di
?k
i=1w(ti)
(4)
where w(?) can be any weighting function that cap-
tures the importance of word ti in the document. We
use idf-weighting as the weighting function.
We use a two-layer neural network to compute the
global context score, scoreg, similar to the above:
a1
(g) = f(W (g)1 [c;xm] + b
(g)
1 ) (5)
scoreg = W
(g)
2 a
(g)
1 + b
(g)
2 (6)
where [c;xm] is the concatenation of the weighted
average document vector and the vector of the last
word in s, a1(g) ? Rh
(g)?1 is the activation of
the hidden layer with h(g) hidden nodes, W (g)1 ?
Rh
(g)?(2n) and W (g)2 ? R
1?h(g) are respectively the
first and second layer weights of the neural network,
and b(g)1 , b
(g)
2 are the biases of each layer. Note that
instead of using the document where the sequence
occurs, we can also specify a fixed k > m that cap-
tures larger context.
The final score is the sum of the two scores:
score = scorel + scoreg (7)
The local score preserves word order and syntactic
information, while the global score uses a weighted
average which is similar to bag-of-words features,
capturing more of the semantics and topics of the
document. Note that Collobert and Weston (2008)?s
language model corresponds to the network using
only local context.
2.3 Learning
Following Collobert and Weston (2008), we sample
the gradient of the objective by randomly choosing
a word from the dictionary as a corrupt example for
each sequence-document pair, (s, d), and take the
derivative of the ranking loss with respect to the pa-
rameters: weights of the neural network and the em-
bedding matrix L. These weights are updated via
backpropagation. The embedding matrix L is the
word representations. We found that word embed-
dings move to good positions in the vector space
faster when using mini-batch L-BFGS (Liu and No-
cedal, 1989) with 1000 pairs of good and corrupt ex-
amples per batch for training, compared to stochas-
tic gradient descent.
3 Multi-Prototype Neural Language
Model
Despite distributional similarity models? successful
applications in various NLP tasks, one major limi-
tation common to most of these models is that they
assume only one representation for each word. This
single-prototype representation is problematic be-
cause many words have multiple meanings, which
can be wildly different. Using one representa-
tion simply cannot capture the different meanings.
Moreover, using all contexts of a homonymous or
polysemous word to build a single prototype could
hurt the representation, which cannot represent any
one of the meanings well as it is influenced by all
meanings of the word.
Instead of using only one representation per word,
Reisinger and Mooney (2010b) proposed the multi-
prototype approach for vector-space models, which
uses multiple representations to capture different
senses and usages of a word. We show how our
875
model can readily adopt the multi-prototype ap-
proach. We present a way to use our learned
single-prototype embeddings to represent each con-
text window, which can then be used by clustering to
perform word sense discrimination (Schu?tze, 1998).
In order to learn multiple prototypes, we first
gather the fixed-sized context windows of all occur-
rences of a word (we use 5 words before and after
the word occurrence). Each context is represented
by a weighted average of the context words? vectors,
where again, we use idf-weighting as the weighting
function, similar to the document context represen-
tation described in Section 2.2. We then use spheri-
cal k-means to cluster these context representations,
which has been shown to model semantic relations
well (Dhillon and Modha, 2001). Finally, each word
occurrence in the corpus is re-labeled to its associ-
ated cluster and is used to train the word representa-
tion for that cluster.
Similarity between a pair of words (w,w?) us-
ing the multi-prototype approach can be computed
with or without context, as defined by Reisinger and
Mooney (2010b):
AvgSimC(w,w?) =
1
K2
k?
i=1
k?
j=1
p(c, w, i)p(c?, w?, j)d(?i(w), ?j(w
?))
(8)
where p(c, w, i) is the likelihood that word w is in
its cluster i given context c, ?i(w) is the vector rep-
resenting the i-th cluster centroid of w, and d(v, v?)
is a function computing similarity between two vec-
tors, which can be any of the distance functions pre-
sented by Curran (2004). The similarity measure can
be computed in absence of context by assuming uni-
form p(c, w, i) over i.
4 Experiments
In this section, we first present a qualitative analysis
comparing the nearest neighbors of our model?s em-
beddings with those of others, showing our embed-
dings better capture the semantics of words, with the
use of global context. Our model also improves the
correlation with human judgments on a word simi-
larity task. Because word interpretation in context is
important, we introduce a new dataset with human
judgments on similarity of pairs of words in senten-
tial context. Finally, we show that our model outper-
forms other methods on this dataset and also that the
multi-prototype approach improves over the single-
prototype approach.
We chose Wikipedia as the corpus to train all
models because of its wide range of topics and
word usages, and its clean organization of docu-
ment by topic. We used the April 2010 snapshot of
the Wikipedia corpus (Shaoul and Westbury, 2010),
with a total of about 2 million articles and 990 mil-
lion tokens. We use a dictionary of the 30,000 most
frequent words in Wikipedia, converted to lower
case. In preprocessing, we keep the frequent num-
bers intact and replace each digit of the uncommon
numbers to ?DG? so as to preserve information such
as it being a year (e.g. ?DGDGDGDG?). The con-
verted numbers that are rare are mapped to a NUM-
BER token. Other rare words not in the dictionary
are mapped to an UNKNOWN token.
For all experiments, our models use 50-
dimensional embeddings. We use 10-word windows
of text as the local context, 100 hidden units, and no
weight regularization for both neural networks. For
multi-prototype variants, we fix the number of pro-
totypes to be 10.
4.1 Qualitative Evaluations
In order to show that our model learns more seman-
tic word representations with global context, we give
the nearest neighbors of our single-prototype model
versus C&W?s, which only uses local context. The
nearest neighbors of a word are computed by com-
paring the cosine similarity between the center word
and all other words in the dictionary. Table 1 shows
the nearest neighbors of some words. The nearest
neighbors of ?market? that C&W?s embeddings give
are more constrained by the syntactic constraint that
words in plural form are only close to other words
in plural form, whereas our model captures that the
singular and plural forms of a word are similar in
meaning. Other examples show that our model in-
duces nearest neighbors that better capture seman-
tics.
Table 2 shows the nearest neighbors of our model
using the multi-prototype approach. We see that
the clustering is able to group contexts of different
876
Center
Word
C&W Our Model
markets firms, industries,
stores
market, firms,
businesses
American Australian,
Indian, Italian
U.S., Canadian,
African
illegal alleged, overseas,
banned
harmful, prohib-
ited, convicted
Table 1: Nearest neighbors of words based on cosine sim-
ilarity. Our model is less constrained by syntax and is
more semantic.
Center Word Nearest Neighbors
bank 1 corporation, insurance, company
bank 2 shore, coast, direction
star 1 movie, film, radio
star 2 galaxy, planet, moon
cell 1 telephone, smart, phone
cell 2 pathology, molecular, physiology
left 1 close, leave, live
left 2 top, round, right
Table 2: Nearest neighbors of word embeddings learned
by our model using the multi-prototype approach based
on cosine similarity. The clustering is able to find the dif-
ferent meanings, usages, and parts of speech of the words.
meanings of a word into separate groups, allowing
our model to learn multiple meaningful representa-
tions of a word.
4.2 WordSim-353
A standard dataset for evaluating vector-space mod-
els is the WordSim-353 dataset (Finkelstein et al,
2001), which consists of 353 pairs of nouns. Each
pair is presented without context and associated with
13 to 16 human judgments on similarity and re-
latedness on a scale from 0 to 10. For example,
(cup,drink) received an average score of 7.25, while
(cup,substance) received an average score of 1.92.
Table 3 shows our results compared to previous
methods, including C&W?s language model and the
hierarchical log-bilinear (HLBL) model (Mnih and
Hinton, 2008), which is a probabilistic, linear neu-
ral model. We downloaded these embeddings from
Turian et al (2010). These embeddings were trained
on the smaller corpus RCV1 that contains one year
of Reuters English newswire, and show similar cor-
relations on the dataset. We report the result of
Model Corpus ?? 100
Our Model-g Wiki. 22.8
C&W RCV1 29.5
HLBL RCV1 33.2
C&W* Wiki. 49.8
C&W Wiki. 55.3
Our Model Wiki. 64.2
Our Model* Wiki. 71.3
Pruned tf-idf Wiki. 73.4
ESA Wiki. 75
Tiered Pruned tf-idf Wiki. 76.9
Table 3: Spearman?s ? correlation on WordSim-353,
showing our model?s improvement over previous neural
models for learning word embeddings. C&W* is the
word embeddings trained and provided by C&W. Our
Model* is trained without stop words, while Our Model-
g uses only global context. Pruned tf-idf (Reisinger and
Mooney, 2010b) and ESA (Gabrilovich and Markovitch,
2007) are also included.
our re-implementation of C&W?s model trained on
Wikipedia, showing the large effect of using a dif-
ferent corpus.
Our model is able to learn more semantic word
embeddings and noticeably improves upon C&W?s
model. Note that our model achieves higher corre-
lation (64.2) than either using local context alone
(C&W: 55.3) or using global context alone (Our
Model-g: 22.8). We also found that correlation can
be further improved by removing stop words (71.3).
Thus, each window of text (training example) con-
tains more information but still preserves some syn-
tactic information as the words are still ordered in
the local context.
4.3 New Dataset: Word Similarity in Context
The many previous datasets that associate human
judgments on similarity between pairs of words,
such as WordSim-353, MC (Miller and Charles,
1991) and RG (Rubenstein and Goodenough, 1965),
have helped to advance the development of vector-
space models. However, common to all datasets is
that similarity scores are given to pairs of words in
isolation. This is problematic because the mean-
ings of homonymous and polysemous words depend
highly on the words? contexts. For example, in the
two phrases, ?he swings the baseball bat? and ?the
877
Word 1 Word 2
Located downtown along the east bank of the Des
Moines River ...
This is the basis of all money laundering , a track record
of depositing clean money before slipping through dirty
money ...
Inside the ruins , there are bats and a bowl with Pokeys
that fills with sand over the course of the race , and the
music changes somewhat while inside ...
An aggressive lower order batsman who usually bats at
No. 11 , Muralitharan is known for his tendency to back
away to leg and slog ...
An example of legacy left in the Mideast from these
nobles is the Krak des Chevaliers ? enlargement by the
Counts of Tripoli and Toulouse ...
... one should not adhere to a particular explanation ,
only in such measure as to be ready to abandon it if it
be proved with certainty to be false ...
... and Andy ?s getting ready to pack his bags and head
up to Los Angeles tomorrow to get ready to fly back
home on Thursday
... she encounters Ben ( Duane Jones ) , who arrives
in a pickup truck and defends the house against another
pack of zombies ...
In practice , there is an unknown phase delay between
the transmitter and receiver that must be compensated
by ? synchronization ? of the receivers local oscillator
... but Gilbert did not believe that she was dedicated
enough , and when she missed a rehearsal , she was
dismissed ...
Table 4: Example pairs from our new dataset. Note that words in a pair can be the same word and have different parts
of speech.
bat flies?, bat has completely different meanings. It
is unclear how this variation in meaning is accounted
for in human judgments of words presented without
context.
One of the main contributions of this paper is the
creation of a new dataset that addresses this issue.
The dataset has three interesting characteristics: 1)
human judgments are on pairs of words presented in
sentential context, 2) word pairs and their contexts
are chosen to reflect interesting variations in mean-
ings of homonymous and polysemous words, and 3)
verbs and adjectives are present in addition to nouns.
We now describe our methodology in constructing
the dataset.
4.3.1 Dataset Construction
Our procedure of constructing the dataset consists
of three steps: 1) select a list a words, 2) for each
word, select another word to form a pair, 3) for each
word in a pair, find a sentential context. We now
describe each step in detail.
In step 1, in order to make sure we select a diverse
list of words, we consider three attributes of a word:
frequency in a corpus, number of parts of speech,
and number of synsets according to WordNet. For
frequency, we divide words into three groups, top
2,000 most frequent, between 2,000 and 5,000, and
between 5,000 to 10,000 based on occurrences in
Wikipedia. For number of parts of speech, we group
words based on their number of possible parts of
speech (noun, verb or adjective), from 1 to 3. We
also group words by their number of synsets: [0,5],
[6,10], [11, 20], and [20, max]. Finally, we sam-
ple at most 15 words from each combination in the
Cartesian product of the above groupings.
In step 2, for each of the words selected in step
1, we want to choose the other word so that the pair
captures an interesting relationship. Similar to Man-
andhar et al (2010), we use WordNet to first ran-
domly select one synset of the first word, we then
construct a set of words in various relations to the
first word?s chosen synset, including hypernyms, hy-
ponyms, holonyms, meronyms and attributes. We
randomly select a word from this set of words as the
second word in the pair. We try to repeat the above
twice to generate two pairs for each word. In addi-
tion, for words with more than five synsets, we allow
the second word to be the same as the first, but with
different synsets. We end up with pairs of words as
well as the one chosen synset for each word in the
pairs.
In step 3, we aim to extract a sentence from
Wikipedia for each word, which contains the word
and corresponds to a usage of the chosen synset.
We first find all sentences in which the word oc-
curs. We then POS tag2 these sentences and filter out
those that do not match the chosen POS. To find the
2We used the MaxEnt Treebank POS tagger in the python
nltk library.
878
Model ?? 100
C&W-S 57.0
Our Model-S 58.6
Our Model-M AvgSim 62.8
Our Model-M AvgSimC 65.7
tf-idf-S 26.3
Pruned tf-idf-S 62.5
Pruned tf-idf-M AvgSim 60.4
Pruned tf-idf-M AvgSimC 60.5
Table 5: Spearman?s ? correlation on our new
dataset. Our Model-S uses the single-prototype approach,
while Our Model-M uses the multi-prototype approach.
AvgSim calculates similarity with each prototype con-
tributing equally, while AvgSimC weighs the prototypes
according to probability of the word belonging to that
prototype?s cluster.
word usages that correspond to the chosen synset,
we first construct a set of related words of the chosen
synset, including hypernyms, hyponyms, holonyms,
meronyms and attributes. Using this set of related
words, we filter out a sentence if the document in
which the sentence appears does not include one of
the related words. Finally, we randomly select one
sentence from those that are left.
Table 4 shows some examples from the dataset.
Note that the dataset alo includes pairs of the same
word. Single-prototype models would give the max
similarity score for those pairs, which can be prob-
lematic depending on the words? contexts. This
dataset requires models to examine context when de-
termining word meaning.
Using Amazon Mechanical Turk, we collected 10
human similarity ratings for each pair, as Snow et
al. (2008) found that 10 non-expert annotators can
achieve very close inter-annotator agreement with
expert raters. To ensure worker quality, we only
allowed workers with over 95% approval rate to
work on our task. Furthermore, we discarded all
ratings by a worker if he/she entered scores out of
the accepted range or missed a rating, signaling low-
quality work.
We obtained a total of 2,003 word pairs and their
sentential contexts. The word pairs consist of 1,712
unique words. Of the 2,003 word pairs, 1328 are
noun-noun pairs, 399 verb-verb, 140 verb-noun, 97
adjective-adjective, 30 noun-adjective, and 9 verb-
adjective. 241 pairs are same-word pairs.
4.3.2 Evaluations on Word Similarity in
Context
For evaluation, we also compute Spearman corre-
lation between a model?s computed similarity scores
and human judgments. Table 5 compares different
models? results on this dataset. We compare against
the following baselines: tf-idf represents words in
a word-word matrix capturing co-occurrence counts
in all 10-word context windows. Reisinger and
Mooney (2010b) found pruning the low-value tf-idf
features helps performance. We report the result
of this pruning technique after tuning the thresh-
old value on this dataset, removing all but the top
200 features in each word vector. We tried the
same multi-prototype approach and used spherical
k-means3 to cluster the contexts using tf-idf repre-
sentations, but obtained lower numbers than single-
prototype (55.4 with AvgSimC). We then tried using
pruned tf-idf representations on contexts with our
clustering assignments (included in Table 5), but still
got results worse than the single-prototype version
of the pruned tf-idf model (60.5 with AvgSimC).
This suggests that the pruned tf-idf representations
might be more susceptible to noise or mistakes in
context clustering.
By utilizing global context, our model outper-
forms C&W?s vectors and the above baselines on
this dataset. With multiple representations per
word, we show that the multi-prototype approach
can improve over the single-prototype version with-
out using context (62.8 vs. 58.6). Moreover, using
AvgSimC4 which takes contexts into account, the
multi-prototype model obtains the best performance
(65.7).
5 Related Work
Neural language models (Bengio et al, 2003; Mnih
and Hinton, 2007; Collobert and Weston, 2008;
Schwenk and Gauvain, 2002; Emami et al, 2003)
have been shown to be very powerful at language
modeling, a task where models are asked to ac-
curately predict the next word given previously
seen words. By using distributed representations of
3We first tried movMF as in Reisinger and Mooney (2010b),
but were unable to get decent results (only 31.5).
4probability of being in a cluster is calculated as the inverse
of the distance to the cluster centroid.
879
words which model words? similarity, this type of
models addresses the data sparseness problem that
n-gram models encounter when large contexts are
used. Most of these models used relative local con-
texts of between 2 to 10 words. Schwenk and Gau-
vain (2002) tried to incorporate larger context by
combining partial parses of past word sequences and
a neural language model. They used up to 3 previ-
ous head words and showed increased performance
on language modeling. Our model uses a similar
neural network architecture as these models and uses
the ranking-loss training objective proposed by Col-
lobert and Weston (2008), but introduces a new way
to combine local and global context to train word
embeddings.
Besides language modeling, word embeddings in-
duced by neural language models have been use-
ful in chunking, NER (Turian et al, 2010), parsing
(Socher et al, 2011b), sentiment analysis (Socher et
al., 2011c) and paraphrase detection (Socher et al,
2011a). However, they have not been directly eval-
uated on word similarity tasks, which are important
for tasks such as information retrieval and summa-
rization. Our experiments show that our word em-
beddings are competitive in word similarity tasks.
Most of the previous vector-space models use a
single vector to represent a word even though many
words have multiple meanings. The multi-prototype
approach has been widely studied in models of cat-
egorization in psychology (Rosseel, 2002; Griffiths
et al, 2009), while Schu?tze (1998) used clustering
of contexts to perform word sense discrimination.
Reisinger and Mooney (2010b) combined the two
approaches and applied them to vector-space mod-
els, which was further improved in Reisinger and
Mooney (2010a). Two other recent papers (Dhillon
et al, 2011; Reddy et al, 2011) present models
for constructing word representations that deal with
context. It would be interesting to evaluate those
models on our new dataset.
Many datasets with human similarity ratings on
pairs of words, such as WordSim-353 (Finkelstein
et al, 2001), MC (Miller and Charles, 1991) and
RG (Rubenstein and Goodenough, 1965), have been
widely used to evaluate vector-space models. Moti-
vated to evaluate composition models, Mitchell and
Lapata (2008) introduced a dataset where an intran-
sitive verb, presented with a subject noun, is com-
pared to another verb chosen to be either similar or
dissimilar to the intransitive verb in context. The
context is short, with only one word, and only verbs
are compared. Erk and Pado? (2008), Thater et al
(2011) and Dinu and Lapata (2010) evaluated word
similarity in context with a modified task where sys-
tems are to rerank gold-standard paraphrase candi-
dates given the SemEval 2007 Lexical Substitution
Task dataset. This task only indirectly evaluates sim-
ilarity as only reranking of already similar words are
evaluated.
6 Conclusion
We presented a new neural network architecture that
learns more semantic word representations by us-
ing both local and global context in learning. These
learned word embeddings can be used to represent
word contexts as low-dimensional weighted average
vectors, which are then clustered to form different
meaning groups and used to learn multi-prototype
vectors. We introduced a new dataset with human
judgments on similarity between pairs of words in
context, so as to evaluate model?s abilities to capture
homonymy and polysemy of words in context. Our
new multi-prototype neural language model outper-
forms previous neural models and competitive base-
lines on this new dataset.
Acknowledgments
The authors gratefully acknowledges the support of
the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181, and the DARPA Deep
Learning program under contract number FA8650-
10-C-7020. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA, AFRL, or the US government.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent,
Christian Jauvin, Jaz K, Thomas Hofmann, Tomaso
Poggio, and John Shawe-taylor. 2003. A neural prob-
abilistic language model. Journal of Machine Learn-
ing Research, 3:1137?1155.
880
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, ICML ?08, pages 160?167, New York, NY, USA.
ACM.
James Richard Curran. 2004. From distributional to se-
mantic similarity. Technical report.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42:143?175, January.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems (NIPS), volume 24.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 1162?1172,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmad Emami, Peng Xu, and Frederick Jelinek. 2003.
Using a connectionist model in a syntactical based lan-
guage model. In Acoustics, Speech, and Signal Pro-
cessing, pages 372?375.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, WWW ?01, pages
406?414, New York, NY, USA. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thomas L Griffiths, Kevin R Canini, Adam N Sanborn,
and Daniel J Navarro. 2009. Unifying rational models
of categorization via the hierarchical dirichlet process.
Brain, page 323328.
David J Hess, Donald J Foss, and Patrick Carroll. 1995.
Effects of global and local context on lexical process-
ing during language comprehension. Journal of Ex-
perimental Psychology: General, 124(1):62?82.
Ping Li, Curt Burgess, and Kevin Lund. 2000. The ac-
quisition of word meaning through global lexical co-
occurrences.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach,
and Sameer S Pradhan. 2010. Semeval-2010 task
14: Word sense induction & disambiguation. Word
Journal Of The International Linguistic Association,
(July):63?68.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
George A Miller and Walter G Charles. 1991. Contextual
correlates of semantic similarity. Language & Cogni-
tive Processes, 6(1):1?28.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39?41.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings of
ACL-08: HLT, pages 236?244.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of the 24th international conference on
Machine learning, ICML ?07, pages 641?648, New
York, NY, USA. ACM.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In In NIPS.
Ht Ng and J Zelle. 1997. Corpus-based approaches to
semantic interpretation in natural language processing.
AI Magazine, 18(4):45?64.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static proto-
type vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural Lan-
guage Processing, pages 705?713, Chiang Mai, Thai-
land, November. Asian Federation of Natural Lan-
guage Processing.
Joseph Reisinger and Raymond Mooney. 2010a. A mix-
ture model with sharing for lexical semantics. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 1173?1182, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joseph Reisinger and Raymond J. Mooney. 2010b.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 109?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yves Rosseel. 2002. Mixture models of categorization.
Journal of Mathematical Psychology, 46:178?210.
881
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627?633, October.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
Holger Schwenk and Jean-luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary con-
tinuous speech recognition. In In International Con-
ference on Acoustics, Speech and Signal Processing,
pages 765?768.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Comput. Surv., 34:1?
47, March.
Cyrus Shaoul and Chris Westbury. 2010. The westbury
lab wikipedia corpus.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 254?263, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative evalu-
ation of passage retrieval algorithms for question an-
swering. In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Search and Devel-
opment in Information Retrieval, pages 41?47. ACM
Press.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: a simple and effec-
tive vector model. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language Process-
ing, IJCNLP ?11.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, pages 384?394, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
882
Tutorial Abstracts of ACL 2012, page 5,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Deep Learning for NLP (without Magic)
Richard Socher Yoshua Bengio? Christopher D. Manning
richard@socher.org bengioy@iro.umontreal.ca, manning@stanford.edu
Computer Science Department, Stanford University
? DIRO, Universite? de Montre?al, Montre?al, QC, Canada
1 Abtract
Machine learning is everywhere in today?s NLP, but
by and large machine learning amounts to numerical
optimization of weights for human designed repre-
sentations and features. The goal of deep learning
is to explore how computers can take advantage of
data to develop features and representations appro-
priate for complex interpretation tasks. This tuto-
rial aims to cover the basic motivation, ideas, mod-
els and learning algorithms in deep learning for nat-
ural language processing. Recently, these methods
have been shown to perform very well on various
NLP tasks such as language modeling, POS tag-
ging, named entity recognition, sentiment analysis
and paraphrase detection, among others. The most
attractive quality of these techniques is that they can
perform well without any external hand-designed re-
sources or time-intensive feature engineering. De-
spite these advantages, many researchers in NLP are
not familiar with these methods. Our focus is on
insight and understanding, using graphical illustra-
tions and simple, intuitive derivations. The goal of
the tutorial is to make the inner workings of these
techniques transparent, intuitive and their results in-
terpretable, rather than black boxes labeled ?magic
here?.
The first part of the tutorial presents the basics of
neural networks, neural word vectors, several simple
models based on local windows and the math and
algorithms of training via backpropagation. In this
section applications include language modeling and
POS tagging.
In the second section we present recursive neural
networks which can learn structured tree outputs as
well as vector representations for phrases and sen-
tences. We cover both equations as well as applica-
tions. We show how training can be achieved by a
modified version of the backpropagation algorithm
introduced before. These modifications allow the al-
gorithm to work on tree structures. Applications in-
clude sentiment analysis and paraphrase detection.
We also draw connections to recent work in seman-
tic compositionality in vector spaces. The princi-
ple goal, again, is to make these methods appear in-
tuitive and interpretable rather than mathematically
confusing. By this point in the tutorial, the audience
members should have a clear understanding of how
to build a deep learning system for word-, sentence-
and document-level tasks.
The last part of the tutorial gives a general
overview of the different applications of deep learn-
ing in NLP, including bag of words models. We will
provide a discussion of NLP-oriented issues in mod-
eling, interpretation, representational power, and op-
timization.
5
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455?465,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing with Compositional Vector Grammars
Richard Socher John Bauer Christopher D. Manning Andrew Y. Ng
Computer Science Department, Stanford University, Stanford, CA 94305, USA
richard@socher.org, horatio@gmail.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Natural language parsing has typically
been done with small sets of discrete cat-
egories such as NP and VP, but this rep-
resentation does not capture the full syn-
tactic nor semantic richness of linguistic
phrases, and attempts to improve on this
by lexicalizing phrases or splitting cate-
gories only partly address the problem at
the cost of huge feature spaces and sparse-
ness. Instead, we introduce a Compo-
sitional Vector Grammar (CVG), which
combines PCFGs with a syntactically un-
tied recursive neural network that learns
syntactico-semantic, compositional vector
representations. The CVG improves the
PCFG of the Stanford Parser by 3.8% to
obtain an F1 score of 90.4%. It is fast
to train and implemented approximately as
an efficient reranker it is about 20% faster
than the current Stanford factored parser.
The CVG learns a soft notion of head
words and improves performance on the
types of ambiguities that require semantic
information such as PP attachments.
1 Introduction
Syntactic parsing is a central task in natural lan-
guage processing because of its importance in me-
diating between linguistic expression and mean-
ing. For example, much work has shown the use-
fulness of syntactic representations for subsequent
tasks such as relation extraction, semantic role la-
beling (Gildea and Palmer, 2002) and paraphrase
detection (Callison-Burch, 2008).
Syntactic descriptions standardly use coarse
discrete categories such as NP for noun phrases
or PP for prepositional phrases. However, recent
work has shown that parsing results can be greatly
improved by defining more fine-grained syntactic
(riding,V,       )    (a,Det,       )        (bike,NN,       )
(a bike,NP,       )
(riding a bike,VP,       )
Discrete Syntactic ? Continuous Semantic 
Representations in the Compositional Vector Grammar
Figure 1: Example of a CVG tree with (cate-
gory,vector) representations at each node. The
vectors for nonterminals are computed via a new
type of recursive neural network which is condi-
tioned on syntactic categories from a PCFG.
categories, which better capture phrases with simi-
lar behavior, whether through manual feature engi-
neering (Klein and Manning, 2003a) or automatic
learning (Petrov et al, 2006). However, subdi-
viding a category like NP into 30 or 60 subcate-
gories can only provide a very limited represen-
tation of phrase meaning and semantic similarity.
Two strands of work therefore attempt to go fur-
ther. First, recent work in discriminative parsing
has shown gains from careful engineering of fea-
tures (Taskar et al, 2004; Finkel et al, 2008). Fea-
tures in such parsers can be seen as defining effec-
tive dimensions of similarity between categories.
Second, lexicalized parsers (Collins, 2003; Char-
niak, 2000) associate each category with a lexical
item. This gives a fine-grained notion of semantic
similarity, which is useful for tackling problems
like ambiguous attachment decisions. However,
this approach necessitates complex shrinkage esti-
mation schemes to deal with the sparsity of obser-
vations of the lexicalized categories.
In many natural language systems, single words
and n-grams are usefully described by their distri-
butional similarities (Brown et al, 1992), among
many others. But, even with large corpora, many
455
n-grams will never be seen during training, espe-
cially when n is large. In these cases, one cannot
simply use distributional similarities to represent
unseen phrases. In this work, we present a new so-
lution to learn features and phrase representations
even for very long, unseen n-grams.
We introduce a Compositional Vector Grammar
Parser (CVG) for structure prediction. Like the
above work on parsing, the model addresses the
problem of representing phrases and categories.
Unlike them, it jointly learns how to parse and how
to represent phrases as both discrete categories and
continuous vectors as illustrated in Fig. 1. CVGs
combine the advantages of standard probabilistic
context free grammars (PCFG) with those of re-
cursive neural networks (RNNs). The former can
capture the discrete categorization of phrases into
NP or PP while the latter can capture fine-grained
syntactic and compositional-semantic information
on phrases and words. This information can help
in cases where syntactic ambiguity can only be re-
solved with semantic information, such as in the
PP attachment of the two sentences: They ate udon
with forks. vs. They ate udon with chicken.
Previous RNN-based parsers used the same
(tied) weights at all nodes to compute the vector
representing a constituent (Socher et al, 2011b).
This requires the composition function to be ex-
tremely powerful, since it has to combine phrases
with different syntactic head words, and it is hard
to optimize since the parameters form a very deep
neural network. We generalize the fully tied RNN
to one with syntactically untied weights. The
weights at each node are conditionally dependent
on the categories of the child constituents. This
allows different composition functions when com-
bining different types of phrases and is shown to
result in a large improvement in parsing accuracy.
Our compositional distributed representation al-
lows a CVG parser to make accurate parsing de-
cisions and capture similarities between phrases
and sentences. Any PCFG-based parser can be im-
proved with an RNN. We use a simplified version
of the Stanford Parser (Klein and Manning, 2003a)
as the base PCFG and improve its accuracy from
86.56 to 90.44% labeled F1 on all sentences of the
WSJ section 23. The code of our parser is avail-
able at nlp.stanford.edu.
2 Related Work
The CVG is inspired by two lines of research:
Enriching PCFG parsers through more diverse
sets of discrete states and recursive deep learning
models that jointly learn classifiers and continuous
feature representations for variable-sized inputs.
Improving Discrete Syntactic Representations
As mentioned in the introduction, there are several
approaches to improving discrete representations
for parsing. Klein and Manning (2003a) use
manual feature engineering, while Petrov et
al. (2006) use a learning algorithm that splits
and merges the syntactic categories in order
to maximize likelihood on the treebank. Their
approach splits categories into several dozen
subcategories. Another approach is lexicalized
parsers (Collins, 2003; Charniak, 2000) that
describe each category with a lexical item, usually
the head word. More recently, Hall and Klein
(2012) combine several such annotation schemes
in a factored parser. We extend the above ideas
from discrete representations to richer continuous
ones. The CVG can be seen as factoring discrete
and continuous parsing in one model. Another
different approach to the above generative models
is to learn discriminative parsers using many well
designed features (Taskar et al, 2004; Finkel et
al., 2008). We also borrow ideas from this line of
research in that our parser combines the generative
PCFG model with discriminatively learned RNNs.
Deep Learning and Recursive Deep Learning
Early attempts at using neural networks to de-
scribe phrases include Elman (1991), who used re-
current neural networks to create representations
of sentences from a simple toy grammar and to
analyze the linguistic expressiveness of the re-
sulting representations. Words were represented
as one-on vectors, which was feasible since the
grammar only included a handful of words. Col-
lobert and Weston (2008) showed that neural net-
works can perform well on sequence labeling lan-
guage processing tasks while also learning appro-
priate features. However, their model is lacking
in that it cannot represent the recursive structure
inherent in natural language. They partially cir-
cumvent this problem by using either independent
window-based classifiers or a convolutional layer.
RNN-specific training was introduced by Goller
and Ku?chler (1996) to learn distributed represen-
tations of given, structured objects such as logi-
cal terms. In contrast, our model both predicts the
structure and its representation.
456
Henderson (2003) was the first to show that neu-
ral networks can be successfully used for large
scale parsing. He introduced a left-corner parser to
estimate the probabilities of parsing decisions con-
ditioned on the parsing history. The input to Hen-
derson?s model consists of pairs of frequent words
and their part-of-speech (POS) tags. Both the orig-
inal parsing system and its probabilistic interpre-
tation (Titov and Henderson, 2007) learn features
that represent the parsing history and do not pro-
vide a principled linguistic representation like our
phrase representations. Other related work in-
cludes (Henderson, 2004), who discriminatively
trains a parser based on synchrony networks and
(Titov and Henderson, 2006), who use an SVM to
adapt a generative parser to different domains.
Costa et al (2003) apply recursive neural net-
works to re-rank possible phrase attachments in
an incremental parser. Their work is the first to
show that RNNs can capture enough information
to make correct parsing decisions, but they only
test on a subset of 2000 sentences. Menchetti et
al. (2005) use RNNs to re-rank different parses.
For their results on full sentence parsing, they re-
rank candidate trees created by the Collins parser
(Collins, 2003). Similar to their work, we use the
idea of letting discrete categories reduce the search
space during inference. We compare to fully tied
RNNs in which the same weights are used at every
node. Our syntactically untied RNNs outperform
them by a significant margin. The idea of untying
has also been successfully used in deep learning
applied to vision (Le et al, 2010).
This paper uses several ideas of (Socher et al,
2011b). The main differences are (i) the dual
representation of nodes as discrete categories and
vectors, (ii) the combination with a PCFG, and
(iii) the syntactic untying of weights based on
child categories. We directly compare models with
fully tied and untied weights. Another work that
represents phrases with a dual discrete-continuous
representation is (Kartsaklis et al, 2012).
3 Compositional Vector Grammars
This section introduces Compositional Vector
Grammars (CVGs), a model to jointly find syntac-
tic structure and capture compositional semantic
information.
CVGs build on two observations. Firstly, that a
lot of the structure and regularity in languages can
be captured by well-designed syntactic patterns.
Hence, the CVG builds on top of a standard PCFG
parser. However, many parsing decisions show
fine-grained semantic factors at work. Therefore
we combine syntactic and semantic information
by giving the parser access to rich syntactico-
semantic information in the form of distributional
word vectors and compute compositional semantic
vector representations for longer phrases (Costa
et al, 2003; Menchetti et al, 2005; Socher et
al., 2011b). The CVG model merges ideas from
both generative models that assume discrete syn-
tactic categories and discriminative models that
are trained using continuous vectors.
We will first briefly introduce single word vec-
tor representations and then describe the CVG ob-
jective function, tree scoring and inference.
3.1 Word Vector Representations
In most systems that use a vector representa-
tion for words, such vectors are based on co-
occurrence statistics of each word and its context
(Turney and Pantel, 2010). Another line of re-
search to learn distributional word vectors is based
on neural language models (Bengio et al, 2003)
which jointly learn an embedding of words into an
n-dimensional feature space and use these embed-
dings to predict how suitable a word is in its con-
text. These vector representations capture inter-
esting linear relationships (up to some accuracy),
such as king?man+woman ? queen (Mikolov
et al, 2013).
Collobert and Weston (2008) introduced a new
model to compute such an embedding. The idea
is to construct a neural network that outputs high
scores for windows that occur in a large unla-
beled corpus and low scores for windows where
one word is replaced by a random word. When
such a network is optimized via gradient ascent the
derivatives backpropagate into the word embed-
ding matrix X . In order to predict correct scores
the vectors in the matrix capture co-occurrence
statistics.
For further details and evaluations of these em-
beddings, see (Turian et al, 2010; Huang et al,
2012). The resulting X matrix is used as follows.
Assume we are given a sentence as an ordered list
of m words. Each word w has an index [w] = i
into the columns of the embedding matrix. This
index is used to retrieve the word?s vector repre-
sentation aw using a simple multiplication with a
binary vector e, which is zero everywhere, except
457
at the ith index. So aw = Lei ? Rn. Henceforth,
after mapping each word to its vector, we represent
a sentence S as an ordered list of (word,vector)
pairs: x = ((w1, aw1), . . . , (wm, awm)).
Now that we have discrete and continuous rep-
resentations for all words, we can continue with
the approach for computing tree structures and
vectors for nonterminal nodes.
3.2 Max-Margin Training Objective for
CVGs
The goal of supervised parsing is to learn a func-
tion g : X ? Y , where X is the set of sentences
and Y is the set of all possible labeled binary parse
trees. The set of all possible trees for a given sen-
tence xi is defined as Y (xi) and the correct tree
for a sentence is yi.
We first define a structured margin loss ?(yi, y?)
for predicting a tree y? for a given correct tree.
The loss increases the more incorrect the proposed
parse tree is (Goodman, 1998). The discrepancy
between trees is measured by counting the number
of nodes N(y) with an incorrect span (or label) in
the proposed tree:
?(yi, y?) =
?
d?N(y?)
?1{d /? N(yi)}. (1)
We set ? = 0.1 in all experiments. For a given
set of training instances (xi, yi), we search for the
function g?, parameterized by ?, with the smallest
expected loss on a new sentence. It has the follow-
ing form:
g?(x) = arg max
y??Y (x)
s(CVG(?, x, y?)), (2)
where the tree is found by the Compositional Vec-
tor Grammar (CVG) introduced below and then
scored via the function s. The higher the score of
a tree the more confident the algorithm is that its
structure is correct. This max-margin, structure-
prediction objective (Taskar et al, 2004; Ratliff
et al, 2007; Socher et al, 2011b) trains the CVG
so that the highest scoring tree will be the correct
tree: g?(xi) = yi and its score will be larger up to
a margin to other possible trees y? ? Y(xi):
s(CVG(?, xi, yi)) ? s(CVG(?, xi, y?)) + ?(yi, y?).
This leads to the regularized risk function for m
training examples:
J(?) = 1m
m?
i=1
ri(?) +
?
2 ???
2
2, where
ri(?) = max
y??Y (xi)
(
s(CVG(xi, y?)) + ?(yi, y?)
)
? s(CVG(xi, yi)) (3)
Intuitively, to minimize this objective, the score of
the correct tree yi is increased and the score of the
highest scoring incorrect tree y? is decreased.
3.3 Scoring Trees with CVGs
For ease of exposition, we first describe how to
score an existing fully labeled tree with a standard
RNN and then with a CVG. The subsequent sec-
tion will then describe a bottom-up beam search
and its approximation for finding the optimal tree.
Assume, for now, we are given a labeled
parse tree as shown in Fig. 2. We define
the word representations as (vector, POS) pairs:
((a,A), (b, B), (c, C)), where the vectors are de-
fined as in Sec. 3.1 and the POS tags come from
a PCFG. The standard RNN essentially ignores all
POS tags and syntactic categories and each non-
terminal node is associated with the same neural
network (i.e., the weights across nodes are fully
tied). We can represent the binary tree in Fig. 2
in the form of branching triplets (p ? c1c2).
Each such triplet denotes that a parent node p has
two children and each ck can be either a word
vector or a non-terminal node in the tree. For
the example in Fig. 2, we would get the triples
((p1 ? bc), (p2 ? ap1)). Note that in order
to replicate the neural network and compute node
representations in a bottom up fashion, the parent
must have the same dimensionality as the children:
p ? Rn.
Given this tree structure, we can now compute
activations for each node from the bottom up. We
begin by computing the activation for p1 using
the children?s word vectors. We first concatenate
the children?s representations b, c ? Rn?1 into a
vector
[
b
c
]
? R2n?1. Then the composition
function multiplies this vector by the parameter
weights of the RNN W ? Rn?2n and applies an
element-wise nonlinearity function f = tanh to
the output vector. The resulting output p(1) is then
given as input to compute p(2).
p(1) = f
(
W
[
b
c
])
, p(2) = f
(
W
[
a
p1
])
458
(A , a=       )        ( B , b=       )       ( C, c=       )
P(1 ), p(1 )=       
 P(2 ), p(2 )=        
Standard Recursive Neural Network
= f   W bc
= f   W ap(1 )
Figure 2: An example tree with a simple Recursive
Neural Network: The same weight matrix is repli-
cated and used to compute all non-terminal node
representations. Leaf nodes are n-dimensional
vector representations of words.
In order to compute a score of how plausible of
a syntactic constituent a parent is the RNN uses a
single-unit linear layer for all i:
s(p(i)) = vT p(i),
where v ? Rn is a vector of parameters that need
to be trained. This score will be used to find the
highest scoring tree. For more details on how stan-
dard RNNs can be used for parsing, see Socher et
al. (2011b).
The standard RNN requires a single composi-
tion function to capture all types of compositions:
adjectives and nouns, verbs and nouns, adverbs
and adjectives, etc. Even though this function is
a powerful one, we find a single neural network
weight matrix cannot fully capture the richness of
compositionality. Several extensions are possible:
A two-layered RNN would provide more expres-
sive power, however, it is much harder to train be-
cause the resulting neural network becomes very
deep and suffers from vanishing gradient prob-
lems. Socher et al (2012) proposed to give ev-
ery single word a matrix and a vector. The ma-
trix is then applied to the sibling node?s vector
during the composition. While this results in a
powerful composition function that essentially de-
pends on the words being combined, the number
of model parameters explodes and the composi-
tion functions do not capture the syntactic com-
monalities between similar POS tags or syntactic
categories.
Based on the above considerations, we propose
the Compositional Vector Grammar (CVG) that
conditions the composition function at each node
on discrete syntactic categories extracted from a
(A , a=       )        ( B , b=       )       ( C, c=       )
P(1 ), p(1 )=       
 P(2 ), p(2 )=        
Syntactically Untied Recursive Neural Network
= f   W (B ,C) bc
= f   W (A ,P  ) ap(1 )
(1 )
Figure 3: Example of a syntactically untied RNN
in which the function to compute a parent vector
depends on the syntactic categories of its children
which we assume are given for now.
PCFG. Hence, CVGs combine discrete, syntactic
rule probabilities and continuous vector composi-
tions. The idea is that the syntactic categories of
the children determine what composition function
to use for computing the vector of their parents.
While not perfect, a dedicated composition func-
tion for each rule RHS can well capture common
composition processes such as adjective or adverb
modification versus noun or clausal complementa-
tion. For instance, it could learn that an NP should
be similar to its head noun and little influenced by
a determiner, whereas in an adjective modification
both words considerably determine the meaning of
a phrase. The original RNN is parameterized by a
single weight matrixW . In contrast, the CVG uses
a syntactically untied RNN (SU-RNN) which has
a set of such weights. The size of this set depends
on the number of sibling category combinations in
the PCFG.
Fig. 3 shows an example SU-RNN that com-
putes parent vectors with syntactically untied
weights. The CVG computes the first parent vec-
tor via the SU-RNN:
p(1) = f
(
W (B,C)
[
b
c
])
,
where W (B,C) ? Rn?2n is now a matrix that de-
pends on the categories of the two children. In
this bottom up procedure, the score for each node
consists of summing two elements: First, a single
linear unit that scores the parent vector and sec-
ond, the log probability of the PCFG for the rule
that combines these two children:
s
(
p(1)
)
=
(
v(B,C)
)T p(1) + logP (P1 ? B C),
(4)
459
where P (P1 ? B C) comes from the PCFG.
This can be interpreted as the log probability of a
discrete-continuous rule application with the fol-
lowing factorization:
P ((P1, p1)? (B, b)(C, c)) (5)
= P (p1 ? b c|P1 ? B C)P (P1 ? B C),
Note, however, that due to the continuous nature
of the word vectors, the probability of such a CVG
rule application is not comparable to probabilities
provided by a PCFG since the latter sum to 1 for
all children.
Assuming that node p1 has syntactic category
P1, we compute the second parent vector via:
p(2) = f
(
W (A,P1)
[
a
p(1)
])
.
The score of the last parent in this trigram is com-
puted via:
s
(
p(2)
)
=
(
v(A,P1)
)T p(2) + logP (P2 ? A P1).
3.4 Parsing with CVGs
The above scores (Eq. 4) are used in the search for
the correct tree for a sentence. The goodness of a
tree is measured in terms of its score and the CVG
score of a complete tree is the sum of the scores at
each node:
s(CVG(?, x, y?)) =
?
d?N(y?)
s
(
pd
)
. (6)
The main objective function in Eq. 3 includes a
maximization over all possible trees maxy??Y (x).
Finding the global maximum, however, cannot be
done efficiently for longer sentences nor can we
use dynamic programming. This is due to the fact
that the vectors break the independence assump-
tions of the base PCFG. A (category, vector) node
representation is dependent on all the words in its
span and hence to find the true global optimum,
we would have to compute the scores for all bi-
nary trees. For a sentence of length n, there are
Catalan(n) many possible binary trees which is
very large even for moderately long sentences.
One could use a bottom-up beam search, keep-
ing a k-best list at every cell of the chart, possibly
for each syntactic category. This beam search in-
ference procedure is still considerably slower than
using only the simplified base PCFG, especially
since it has a small state space (see next section for
details). Since each probability look-up is cheap
but computing SU-RNN scores requires a matrix
product, we would like to reduce the number of
SU-RNN score computations to only those trees
that require semantic information. We note that
labeled F1 of the Stanford PCFG parser on the test
set is 86.17%. However, if one used an oracle to
select the best tree from the top 200 trees that it
produces, one could get an F1 of 95.46%.
We use this knowledge to speed up inference via
two bottom-up passes through the parsing chart.
During the first one, we use only the base PCFG to
run CKY dynamic programming through the tree.
The k = 200-best parses at the top cell of the
chart are calculated using the efficient algorithm
of (Huang and Chiang, 2005). Then, the second
pass is a beam search with the full CVG model (in-
cluding the more expensive matrix multiplications
of the SU-RNN). This beam search only consid-
ers phrases that appear in the top 200 parses. This
is similar to a re-ranking setup but with one main
difference: the SU-RNN rule score computation at
each node still only has access to its child vectors,
not the whole tree or other global features. This
allows the second pass to be very fast. We use this
setup in our experiments below.
3.5 Training SU-RNNs
The full CVG model is trained in two stages. First
the base PCFG is trained and its top trees are
cached and then used for training the SU-RNN
conditioned on the PCFG. The SU-RNN is trained
using the objective in Eq. 3 and the scores as ex-
emplified by Eq. 6. For each sentence, we use the
method described above to efficiently find an ap-
proximation for the optimal tree.
To minimize the objective we want to increase
the scores of the correct tree?s constituents and
decrease the score of those in the highest scor-
ing incorrect tree. Derivatives are computed via
backpropagation through structure (BTS) (Goller
and Ku?chler, 1996). The derivative of tree i has
to be taken with respect to all parameter matrices
W (AB) that appear in it. The main difference be-
tween backpropagation in standard RNNs and SU-
RNNs is that the derivatives at each node only add
to the overall derivative of the specific matrix at
that node. For more details on backpropagation
through RNNs, see Socher et al (2010)
460
3.6 Subgradient Methods and AdaGrad
The objective function is not differentiable due to
the hinge loss. Therefore, we generalize gradient
ascent via the subgradient method (Ratliff et al,
2007) which computes a gradient-like direction.
Let ? = (X,W (??), v(??)) ? RM be a vector of all
M model parameters, where we denote W (??) as
the set of matrices that appear in the training set.
The subgradient of Eq. 3 becomes:
?J
?? =
?
i
?s(xi, y?max)
?? ?
?s(xi, yi)
?? + ?,
where y?max is the tree with the highest score. To
minimize the objective, we use the diagonal vari-
ant of AdaGrad (Duchi et al, 2011) with mini-
batches. For our parameter updates, we first de-
fine g? ? RM?1 to be the subgradient at time step
? and Gt = ?t?=1 g?gT? . The parameter update at
time step t then becomes:
?t = ?t?1 ? ? (diag(Gt))?1/2 gt, (7)
where ? is the learning rate. Since we use the di-
agonal of Gt, we only have to store M values and
the update becomes fast to compute: At time step
t, the update for the i?th parameter ?t,i is:
?t,i = ?t?1,i ?
???t
?=1 g2?,i
gt,i. (8)
Hence, the learning rate is adapting differ-
ently for each parameter and rare parameters get
larger updates than frequently occurring parame-
ters. This is helpful in our setting since some W
matrices appear in only a few training trees. This
procedure found much better optima (by ?3% la-
beled F1 on the dev set), and converged more
quickly than L-BFGS which we used previously
in RNN training (Socher et al, 2011a). Training
time is roughly 4 hours on a single machine.
3.7 Initialization of Weight Matrices
In the absence of any knowledge on how to com-
bine two categories, our prior for combining two
vectors is to average them instead of performing a
completely random projection. Hence, we initial-
ize the binary W matrices with:
W (??) = 0.5[In?nIn?n0n?1] + ,
where we include the bias in the last column and
the random variable is uniformly distributed:  ?
U [?0.001, 0.001]. The first block is multiplied by
the left child and the second by the right child:
W (AB)
?
?
a
b
1
?
? =
[
W (A)W (B)bias
]
?
?
a
b
1
?
?
= W (A)a+W (B)b+ bias.
4 Experiments
We evaluate the CVG in two ways: First, by a stan-
dard parsing evaluation on Penn Treebank WSJ
and then by analyzing the model errors in detail.
4.1 Cross-validating Hyperparameters
We used the first 20 files of WSJ section 22
to cross-validate several model and optimization
choices. The base PCFG uses simplified cate-
gories of the Stanford PCFG Parser (Klein and
Manning, 2003a). We decreased the state split-
ting of the PCFG grammar (which helps both by
making it less sparse and by reducing the num-
ber of parameters in the SU-RNN) by adding
the following options to training: ?-noRightRec -
dominatesV 0 -baseNP 0?. This reduces the num-
ber of states from 15,276 to 12,061 states and 602
POS tags. These include split categories, such as
parent annotation categories like VP?S. Further-
more, we ignore all category splits for the SU-
RNN weights, resulting in 66 unary and 882 bi-
nary child pairs. Hence, the SU-RNN has 66+882
transformation matrices and scoring vectors. Note
that any PCFG, including latent annotation PCFGs
(Matsuzaki et al, 2005) could be used. However,
since the vectors will capture lexical and semantic
information, even simple base PCFGs can be sub-
stantially improved. Since the computational com-
plexity of PCFGs depends on the number of states,
a base PCFG with fewer states is much faster.
Testing on the full WSJ section 22 dev set (1700
sentences) takes roughly 470 seconds with the
simple base PCFG, 1320 seconds with our new
CVG and 1600 seconds with the currently pub-
lished Stanford factored parser. Hence, increased
performance comes also with a speed improve-
ment of approximately 20%.
We fix the same regularization of ? = 10?4
for all parameters. The minibatch size was set to
20. We also cross-validated on AdaGrad?s learn-
ing rate which was eventually set to ? = 0.1 and
word vector size. The 25-dimensional vectors pro-
vided by Turian et al (2010) provided the best
461
Parser dev (all) test? 40 test (all)
Stanford PCFG 85.8 86.2 85.5
Stanford Factored 87.4 87.2 86.6
Factored PCFGs 89.7 90.1 89.4
Collins 87.7
SSN (Henderson) 89.4
Berkeley Parser 90.1
CVG (RNN) 85.7 85.1 85.0
CVG (SU-RNN) 91.2 91.1 90.4
Charniak-SelfTrain 91.0
Charniak-RS 92.1
Table 1: Comparison of parsers with richer state
representations on the WSJ. The last line is the
self-trained re-ranked Charniak parser.
performance and were faster than 50-,100- or 200-
dimensional ones. We hypothesize that the larger
word vector sizes, while capturing more seman-
tic knowledge, result in too many SU-RNN matrix
parameters to train and hence perform worse.
4.2 Results on WSJ
The dev set accuracy of the best model is 90.93%
labeled F1 on all sentences. This model re-
sulted in 90.44% on the final test set (WSJ sec-
tion 23). Table 1 compares our results to the
two Stanford parser variants (the unlexicalized
PCFG (Klein and Manning, 2003a) and the fac-
tored parser (Klein and Manning, 2003b)) and
other parsers that use richer state representations:
the Berkeley parser (Petrov and Klein, 2007),
Collins parser (Collins, 1997), SSN: a statistical
neural network parser (Henderson, 2004), Fac-
tored PCFGs (Hall and Klein, 2012), Charniak-
SelfTrain: the self-training approach of McClosky
et al (2006), which bootstraps and parses addi-
tional large corpora multiple times, Charniak-RS:
the state of the art self-trained and discrimina-
tively re-ranked Charniak-Johnson parser combin-
ing (Charniak, 2000; McClosky et al, 2006; Char-
niak and Johnson, 2005). See Kummerfeld et al
(2012) for more comparisons. We compare also
to a standard RNN ?CVG (RNN)? and to the pro-
posed CVG with SU-RNNs.
4.3 Model Analysis
Analysis of Error Types. Table 2 shows a de-
tailed comparison of different errors. We use
the code provided by Kummerfeld et al (2012)
and compare to the previous version of the Stan-
ford factored parser as well as to the Berkeley
and Charniak-reranked-self-trained parsers (de-
fined above). See Kummerfeld et al (2012) for
details and comparisons to other parsers. One of
Error Type Stanford CVG Berkeley Char-RS
PP Attach 1.02 0.79 0.82 0.60
Clause Attach 0.64 0.43 0.50 0.38
Diff Label 0.40 0.29 0.29 0.31
Mod Attach 0.37 0.27 0.27 0.25
NP Attach 0.44 0.31 0.27 0.25
Co-ord 0.39 0.32 0.38 0.23
1-Word Span 0.48 0.31 0.28 0.20
Unary 0.35 0.22 0.24 0.14
NP Int 0.28 0.19 0.18 0.14
Other 0.62 0.41 0.41 0.50
Table 2: Detailed comparison of different parsers.
the largest sources of improved performance over
the original Stanford factored parser is in the cor-
rect placement of PP phrases. When measuring
only the F1 of parse nodes that include at least one
PP child, the CVG improves the Stanford parser
by 6.2% to an F1 of 77.54%. This is a 0.23 re-
duction in the average number of bracket errors
per sentence. The ?Other? category includes VP,
PRN and other attachments, appositives and inter-
nal structures of modifiers and QPs.
Analysis of Composition Matrices. An analy-
sis of the norms of the binary matrices reveals
that the model learns a soft vectorized notion of
head words: Head words are given larger weights
and importance when computing the parent vec-
tor: For the matrices combining siblings with cat-
egories VP:PP, VP:NP and VP:PRT, the weights in
the part of the matrix which is multiplied with the
VP child vector dominates. Similarly NPs dom-
inate DTs. Fig. 5 shows example matrices. The
two strong diagonals are due to the initialization
described in Sec. 3.7.
Semantic Transfer for PP Attachments. In this
small model analysis, we use two pairs of sen-
tences that the original Stanford parser and the
CVG did not parse correctly after training on
the WSJ. We then continue to train both parsers
on two similar sentences and then analyze if the
parsers correctly transferred the knowledge. The
training sentences are He eats spaghetti with a
fork. and She eats spaghetti with pork. The very
similar test sentences are He eats spaghetti with a
spoon. and He eats spaghetti with meat. Initially,
both parsers incorrectly attach the PP to the verb
in both test sentences. After training, the CVG
parses both correctly, while the factored Stanford
parser incorrectly attaches both PPs to spaghetti.
The CVG?s ability to transfer the correct PP at-
tachments is due to the semantic word vector sim-
ilarity between the words in the sentences. Fig. 4
shows the outputs of the two parsers.
462
(a) Stanford factored parserS
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
DT
a
NN
spoon
S
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
PRP
meat
(b) Compositional Vector GrammarS
NP
PRP
He
VP
VBZ
eats
NP
NNS
spaghetti
PP
IN
with
NP
DT
a
NN
spoon
S
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
NN
meat
Figure 4: Test sentences of semantic transfer for PP attachments. The CVG was able to transfer se-
mantic word knowledge from two related training sentences. In contrast, the Stanford parser could not
distinguish the PP attachments based on the word semantics.
 
 
10 20 30 40 50
5
10
15
20
25 ?0.2
0
0.2
0.4
0.6
0.8
DT-NP
 
 
10 20 30 40 50
5
10
15
20
25
?0.4
?0.2
0
0.2
0.4
0.6
VP-NP
 
 
10 20 30 40 50
5
10
15
20
25 ?0.2
0
0.2
0.4
0.6
ADJP-NP
Figure 5: Three binary composition matrices
showing that head words dominate the composi-
tion. The model learns to not give determiners
much importance. The two diagonals show clearly
the two blocks that are multiplied with the left and
right children, respectively.
5 Conclusion
We introduced Compositional Vector Grammars
(CVGs), a parsing model that combines the speed
of small-state PCFGs with the semantic richness
of neural word representations and compositional
phrase vectors. The compositional vectors are
learned with a new syntactically untied recursive
neural network. This model is linguistically more
plausible since it chooses different composition
functions for a parent node based on the syntac-
tic categories of its children. The CVG obtains
90.44% labeled F1 on the full WSJ test set and is
20% faster than the previous Stanford parser.
Acknowledgments
We thank Percy Liang for chats about the paper.
Richard is supported by a Microsoft Research PhD
fellowship. The authors gratefully acknowledge
the support of the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under Air Force
Research Laboratory (AFRL) prime contract no.
FA8750-13-2-0040, and the DARPA Deep Learn-
ing program under contract number FA8650-10-
C-7020. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA, AFRL, or the US govern-
ment.
463
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Lin-
guistics, 18.
C. Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, pages 196?205.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking.
In ACL.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of ACL, pages 132?139.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language us-
ing recursive neural networks. Applied Intelligence.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12, July.
J. L. Elman. 1991. Distributed representations, sim-
ple recurrent networks, and grammatical structure.
Machine Learning, 7(2-3):195?225.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In Proceedings of ACL, pages 959?967.
D. Gildea and M. Palmer. 2002. The necessity of pars-
ing for predicate argument recognition. In Proceed-
ings of ACL, pages 239?246.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backprop-
agation through structure. In Proceedings of the In-
ternational Conference on Neural Networks.
J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
MIT.
D. Hall and D. Klein. 2012. Training factored pcfgs
with expectation propagation. In EMNLP.
J. Henderson. 2003. Neural network probability esti-
mation for broad coverage parsing. In Proceedings
of EACL.
J. Henderson. 2004. Discriminative training of a neu-
ral network statistical parser. In ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT 2005).
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2012. A
unified sentence space for categorical distributional-
compositional semantics: Theory and experiments.
Proceedings of 24th International Conference on
Computational Linguistics (COLING): Posters.
D. Klein and C. D. Manning. 2003a. Accurate un-
lexicalized parsing. In Proceedings of ACL, pages
423?430.
D. Klein and C.D. Manning. 2003b. Fast exact in-
ference with a factored model for natural language
parsing. In NIPS.
J. K. Kummerfeld, D. Hall, J. R. Curran, and D. Klein.
2012. Parser showdown at the wall street corral: An
empirical investigation of error types in parser out-
put. In EMNLP.
Q. V. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, and
A. Y. Ng. 2010. Tiled convolutional neural net-
works. In NIPS.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL.
S. Menchetti, F. Costa, P. Frasconi, and M. Pon-
til. 2005. Wide coverage natural language pro-
cessing using kernel methods and neural networks
for structured data. Pattern Recognition Letters,
26(12):1896?1906.
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguis-
tic regularities in continuous spaceword representa-
tions. In HLT-NAACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of ACL, pages 433?440.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2007. (On-
line) subgradient methods for structured prediction.
In Eleventh International Conference on Artificial
Intelligence and Statistics (AIStats).
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learn-
ing continuous phrase representations and syntactic
parsing with recursive neural networks. In Proceed-
ings of the NIPS-2010 Deep Learning and Unsuper-
vised Feature Learning Workshop.
464
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. In NIPS. MIT Press.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic Compositionality Through Recur-
sive Matrix-Vector Spaces. In EMNLP.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
EMNLP, pages 1?8.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In CoNLL-X.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In ACL.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
465
Grounded Compositional Semantics
for Finding and Describing Images with Sentences
Richard Socher, Andrej Karpathy, Quoc V. Le*, Christopher D. Manning, Andrew Y. Ng
Stanford University, Computer Science Department, *Google Inc.
richard@socher.org, karpathy@cs.stanford.edu,
qvl@google.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Previous work on Recursive Neural Networks
(RNNs) shows that these models can produce
compositional feature vectors for accurately
representing and classifying sentences or im-
ages. However, the sentence vectors of previ-
ous models cannot accurately represent visu-
ally grounded meaning. We introduce the DT-
RNN model which uses dependency trees to
embed sentences into a vector space in order
to retrieve images that are described by those
sentences. Unlike previous RNN-based mod-
els which use constituency trees, DT-RNNs
naturally focus on the action and agents in
a sentence. They are better able to abstract
from the details of word order and syntactic
expression. DT-RNNs outperform other re-
cursive and recurrent neural networks, kernel-
ized CCA and a bag-of-words baseline on the
tasks of finding an image that fits a sentence
description and vice versa. They also give
more similar representations to sentences that
describe the same image.
1 Introduction
Single word vector spaces are widely used (Turney
and Pantel, 2010) and successful at classifying sin-
gle words and capturing their meaning (Collobert
and Weston, 2008; Huang et al., 2012; Mikolov et
al., 2013). Since words rarely appear in isolation,
the task of learning compositional meaning repre-
sentations for longer phrases has recently received a
lot of attention (Mitchell and Lapata, 2010; Socher
et al., 2010; Socher et al., 2012; Grefenstette et al.,
2013). Similarly, classifying whole images into a
fixed set of classes also achieves very high perfor-
mance (Le et al., 2012; Krizhevsky et al., 2012).
However, similar to words, objects in images are of-
ten seen in relationships with other objects which are
not adequately described by a single label.
In this work, we introduce a model, illustrated in
Fig. 1, which learns to map sentences and images
into a common embedding space in order to be able
to retrieve one from the other. We assume word and
image representations are first learned in their re-
spective single modalities but finally mapped into a
jointly learned multimodal embedding space.
Our model for mapping sentences into this space
is based on ideas from Recursive Neural Networks
(RNNs) (Pollack, 1990; Costa et al., 2003; Socher
et al., 2011b). However, unlike all previous RNN
models which are based on constituency trees (CT-
RNNs), our model computes compositional vector
representations inside dependency trees. The com-
positional vectors computed by this new dependency
tree RNN (DT-RNN) capture more of the meaning
of sentences, where we define meaning in terms of
similarity to a ?visual representation? of the textual
description. DT-RNN induced vector representa-
tions of sentences are more robust to changes in the
syntactic structure or word order than related mod-
els such as CT-RNNs or Recurrent Neural Networks
since they naturally focus on a sentence?s action and
its agents.
We evaluate and compare DT-RNN induced rep-
resentations on their ability to use a sentence such as
?A man wearing a helmet jumps on his bike near a
beach.? to find images that show such a scene. The
goal is to learn sentence representations that capture
207
Transactions of the Association for Computational Linguistics, 2 (2014) 207?218. Action Editor: Alexander Clark.
Submitted 10/2013; Revised 3/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
A man wearing a helmet jumps on his bike near a beach.
Compositional Sentence Vectors
Two airplanes parked in an airport.
A man jumping his downhill bike.
Image Vector Representation
A small child sits on a cement wall near white flower.
Multi-Modal 
Representations
Figure 1: The DT-RNN learns vector representations for sentences based on their dependency trees. We learn to
map the outputs of convolutional neural networks applied to images into the same space and can then compare both
sentences and images. This allows us to query images with a sentence and give sentence descriptions to images.
the visual scene described and to find appropriate
images in the learned, multi-modal sentence-image
space. Conversely, when given a query image, we
would like to find a description that goes beyond a
single label by providing a correct sentence describ-
ing it, a task that has recently garnered a lot of at-
tention (Farhadi et al., 2010; Ordonez et al., 2011;
Kuznetsova et al., 2012). We use the dataset intro-
duced by (Rashtchian et al., 2010) which consists of
1000 images, each with 5 descriptions. On all tasks,
our model outperforms baselines and related mod-
els.
2 Related Work
The presented model is connected to several areas of
NLP and vision research, each with a large amount
of related work to which we can only do some justice
given space constraints.
Semantic Vector Spaces and Their Composition-
ality. The dominant approach in semantic vec-
tor spaces uses distributional similarities of single
words. Often, co-occurrence statistics of a word and
its context are used to describe each word (Turney
and Pantel, 2010; Baroni and Lenci, 2010), such
as tf-idf. Most of the compositionality algorithms
and related datasets capture two-word compositions.
For instance, (Mitchell and Lapata, 2010) use two-
word phrases and analyze similarities computed by
vector addition, multiplication and others. Compo-
sitionality is an active field of research with many
different models and representations being explored
(Grefenstette et al., 2013), among many others. We
compare to supervised compositional models that
can learn task-specific vector representations such as
constituency tree recursive neural networks (Socher
et al., 2011b; Socher et al., 2011a), chain structured
recurrent neural networks and other baselines. An-
other alternative would be to use CCG trees as a
backbone for vector composition (K.M. Hermann,
2013).
Multimodal Embeddings. Multimodal embed-
ding methods project data from multiple sources
such as sound and video (Ngiam et al., 2011) or im-
ages and text. Socher et al. (Socher and Fei-Fei,
2010) project words and image regions into a com-
mon space using kernelized canonical correlation
analysis to obtain state of the art performance in an-
notation and segmentation. Similar to our work, they
use unsupervised large text corpora to learn seman-
tic word representations. Among other recent work
is that by Srivastava and Salakhutdinov (2012) who
developed multimodal Deep Boltzmann Machines.
Similar to their work, we use techniques from the
broad field of deep learning to represent images and
words.
Recently, single word vector embeddings have
been used for zero shot learning (Socher et al.,
2013c). Mapping images to word vectors enabled
their system to classify images as depicting objects
such as ?cat? without seeing any examples of this
class. Related work has also been presented at NIPS
(Socher et al., 2013b; Frome et al., 2013). This work
moves zero-shot learning beyond single categories
per image and extends it to unseen phrases and full
length sentences, making use of similar ideas of se-
mantic spaces grounded in visual knowledge.
208
Detailed Image Annotation. Interactions be-
tween images and texts is a growing research field.
Early work in this area includes generating single
words or fixed phrases from images (Duygulu et al.,
2002; Barnard et al., 2003) or using contextual in-
formation to improve recognition (Gupta and Davis,
2008; Torralba et al., 2010).
Apart from a large body of work on single object
image classification (Le et al., 2012), there is also
work on attribute classification and other mid-level
elements (Kumar et al., 2009), some of which we
hope to capture with our approach as well.
Our work is close in spirit with recent work in de-
scribing images with more detailed, longer textual
descriptions. In particular, Yao et al. (2010) describe
images using hierarchical knowledge and humans in
the loop. In contrast, our work does not require hu-
man interactions. Farhadi et al. (2010) and Kulkarni
et al. (2011), on the other hand, use a more automatic
method to parse images. For instance, the former ap-
proach uses a single triple of objects estimated for an
image to retrieve sentences from a collection written
to describe similar images. It forms representations
to describe 1 object, 1 action, and 1 scene. Kulkarni
et al. (2011) extends their method to describe an im-
age with multiple objects. None of these approaches
have used a compositional sentence vector repre-
sentation and they require specific language gener-
ation techniques and sophisticated inference meth-
ods. Since our model is based on neural networks in-
ference is fast and simple. Kuznetsova et al. (2012)
use a very large parallel corpus to connect images
and sentences. Feng and Lapata (2013) use a large
dataset of captioned images and experiments with
both extractive (search) and abstractive (generation)
models.
Most related is the very recent work of Hodosh et
al. (2013). They too evaluate using a ranking mea-
sure. In our experiments, we compare to kernelized
Canonical Correlation Analysis which is the main
technique in their experiments.
3 Dependency-Tree Recursive Neural
Networks
In this section we first focus on the DT-RNN model
that computes compositional vector representations
for phrases and sentences of variable length and syn-
tactic type. In section 5 the resulting vectors will
then become multimodal features by mapping im-
ages that show what the sentence describes to the
same space and learning both the image and sen-
tence mapping jointly.
The most common way of building representa-
tions for longer phrases from single word vectors is
to simply linearly average the word vectors. While
this bag-of-words approach can yield reasonable
performance in some tasks, it gives all the words the
same weight and cannot distinguish important dif-
ferences in simple visual descriptions such as The
bike crashed into the standing car. vs. The car
crashed into the standing bike..
RNN models (Pollack, 1990; Goller and Ku?chler,
1996; Socher et al., 2011b; Socher et al., 2011a) pro-
vided a novel way of combining word vectors for
longer phrases that moved beyond simple averag-
ing. They combine vectors with an RNN in binary
constituency trees which have potentially many hid-
den layers. While the induced vector representations
work very well on many tasks, they also inevitably
capture a lot of syntactic structure of the sentence.
However, the task of finding images from sentence
descriptions requires us to be more invariant to syn-
tactic differences. One such example are active-
passive constructions which can collapse words such
as ?by? in some formalisms (de Marneffe et al.,
2006), relying instead on the semantic relationship
of ?agent?. For instance, The mother hugged her
child. and The child was hugged by its mother.
should map to roughly the same visual space. Cur-
rent Recursive and Recurrent Neural Networks do
not exhibit this behavior and even bag of words rep-
resentations would be influenced by the words was
and by. The model we describe below focuses more
on recognizing actions and agents and has the po-
tential to learn representations that are invariant to
active-passive differences.
3.1 DT-RNN Inputs: Word Vectors and
Dependency Trees
In order for the DT-RNN to compute a vector repre-
sentation for an ordered list of m words (a phrase or
sentence), we map the single words to a vector space
and then parse the sentence.
First, we map each word to a d-dimensional vec-
tor. We initialize these word vectors with the un-
209
A man wearing a helmet jumps on his bike near a beach
det
nsubj
partmod detdobj
root
prep posspobj
prep
detpobj
Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations
at every word that represents that word and an arbitrary number of child nodes. The final representation is computed
at the root node, here at the verb jumps. Note that more important activity and object words are higher up in this tree
structure.
supervised model of Huang et al. (2012) which can
learn single word vector representations from both
local and global contexts. The idea is to construct a
neural network that outputs high scores for windows
and documents that occur in a large unlabeled corpus
and low scores for window-document pairs where
one word is replaced by a random word. When
such a network is optimized via gradient descent the
derivatives backpropagate into a word embedding
matrix A which stores word vectors as columns. In
order to predict correct scores the vectors in the ma-
trix capture co-occurrence statistics. We use d = 50
in all our experiments. The embedding matrix X
is then used by finding the column index i of each
word: [w] = i and retrieving the corresponding col-
umn xw from X . Henceforth, we represent an input
sentence s as an ordered list of (word,vector) pairs:
s = ((w1, xw1), . . . , (wm, xwm)).
Next, the sequence of words (w1, . . . , wm) is
parsed by the dependency parser of de Marneffe
et al. (2006). Fig. 2 shows an example. We can
represent a dependency tree d of a sentence s as
an ordered list of (child,parent) indices: d(s) =
{(i, j)}, where every child word in the sequence
i = 1, . . . ,m is present and has any word j ?
{1, . . . ,m} ? {0} as its parent. The root word has
as its parent 0 and we notice that the same word can
be a parent between zero and m number of times.
Without loss of generality, we assume that these in-
dices form a tree structure. To summarize, the input
to the DT-RNN for each sentence is the pair (s, d):
the words and their vectors and the dependency tree.
3.2 Forward Propagation in DT-RNNs
Given these two inputs, we now illustrate how the
DT-RNN computes parent vectors. We will use the
following sentence as a running example: Students1
ride2 bikes3 at4 night5. Fig. 3 shows its tree
and computed vector representations. The depen-
Students                 bikes           night
ride 
at          x 1
x 2
x 3
x 4
x 5
h1
h2
h3
h4
h5
Figure 3: Example of a DT-RNN tree structure for com-
puting a sentence representation in a bottom up fashion.
dency tree for this sentence can be summarized by
the following set of (child, parent) edges: d =
{(1, 2), (2, 0), (3, 2), (4, 2), (5, 4)}.
The DT-RNN model will compute parent vectors
at each word that include all the dependent (chil-
dren) nodes in a bottom up fashion using a com-
positionality function g? which is parameterized by
all the model parameters ?. To this end, the algo-
rithm searches for nodes in a tree that have either
(i) no children or (ii) whose children have already
been computed and then computes the correspond-
ing vector.
In our example, the words x1, x3, x5 are leaf
nodes and hence, we can compute their correspond-
ing hidden nodes via:
hc = g?(xc) = f(Wvxc) for c = 1, 3, 5, (1)
where we compute the hidden vector at position c
via our general composition function g?. In the case
of leaf nodes, this composition function becomes
simply a linear layer, parameterized by Wv ? Rn?d,
followed by a nonlinearity. We cross-validate over
using no nonlinearity (f = id), tanh, sigmoid or
rectified linear units (f = max(0, x), but generally
find tanh to perform best.
The final sentence representation we want to com-
pute is at h2, however, since we still do not have h4,
210
we compute that one next:
h4 = g?(x4, h5) = f(Wvx4 +Wr1h5), (2)
where we use the same Wv as before to map the
word vector into hidden space but we now also have
a linear layer that takes as input h5, the only child
of the fourth node. The matrix Wr1 ? Rn?n is used
because node 5 is the first child node on the right
side of node 4. Generally, we have multiple matri-
ces for composing with hidden child vectors from
the right and left sides: Wr? = (Wr1, . . . ,Wrkr) and
Wl? = (Wl1, . . . ,Wlkl). The number of needed ma-
trices is determined by the data by simply finding
the maximum numbers of left kl and right kr chil-
dren any node has. If at test time a child appeared
at an even large distance (this does not happen in
our test set), the corresponding matrix would be the
identity matrix.
Now that all children of h2 have their hidden vec-
tors, we can compute the final sentence representa-
tion via:
h2 = g?(x2, h1, h3, h4) = (3)
f(Wvx2 +Wl1h1 +Wr1h3 +Wr2h4).
Notice that the children are multiplied by matrices
that depend on their location relative to the current
node.
Another modification that improves the mean
rank by approximately 6 in image search on the dev
set is to weight nodes by the number of words under-
neath them and normalize by the sum of words under
all children. This encourages the intuitive desidera-
tum that nodes describing longer phrases are more
important. Let `(i) be the number of leaf nodes
(words) under node i and C(i, y) be the set of child
nodes of node i in dependency tree y. The final com-
position function for a node vector hi becomes:
hi = f
?
? 1
`(i)
?
?Wvxi +
?
j?C(i)
`(j)Wpos(i,j)hj
?
?
?
? ,
(4)
where by definition `(i) = 1 + ?j?C(i) `(j) and
pos(i, j) is the relative position of child j with re-
spect to node i, e.g. l1 or r2 in Eq. 3.
3.3 Semantic Dependency Tree RNNs
An alternative is to condition the weight matrices
on the semantic relations given by the dependency
parser. We use the collapsed tree formalism of
the Stanford dependency parser (de Marneffe et al.,
2006). With such a semantic untying of the weights,
the DT-RNN makes better use of the dependency
formalism and could give active-passive reversals
similar semantic vector representation. The equation
for this semantic DT-RNN (SDT-RNN) is the same
as the one above except that the matrices Wpos(i,j)
are replaced with matrices based on the dependency
relationship. There are a total of 141 unique such
relationships in the dataset. However, most are very
rare. For examples of semantic relationships, see
Fig. 2 and the model analysis section 6.7.
This forward propagation can be used for com-
puting compositional vectors and in Sec. 5 we will
explain the objective function in which these are
trained.
3.4 Comparison to Previous RNN Models
The DT-RNN has several important differences to
previous RNN models of Socher et al. (2011a) and
(Socher et al., 2011b; Socher et al., 2011c). These
constituency tree RNNs (CT-RNNs) use the follow-
ing composition function to compute a hidden par-
ent vector h from exactly two child vectors (c1, c2)
in a binary tree: h = f
(
W
[
c1
c2
])
, where W ?
Rd?2d is the main parameter to learn. This can be
rewritten to show the similarity to the DT-RNN as
h = f(Wl1c1 +Wr1c2). However, there are several
important differences.
Note first that in previous RNN models the par-
ent vectors were of the same dimensionality to be
recursively compatible and be used as input to the
next composition. In contrast, our new model first
maps single words into a hidden space and then par-
ent nodes are composed from these hidden vectors.
This allows a higher capacity representation which
is especially helpful for nodes that have many chil-
dren.
Secondly, the DT-RNN allows for n-ary nodes in
the tree. This is an improvement that is possible even
for constituency tree CT-RNNs but it has not been
explored in previous models.
Third, due to computing parent nodes in con-
stituency trees, previous models had the problem
that words that are merged last in the tree have a
larger weight or importance in the final sentence rep-
211
Figure 4: The architecture of the visual model. This model has 3 sequences of filtering, pooling and local contrast
normalization layers. The learnable parameters are the filtering layer. The filters are not shared, i.e., the network is
nonconvolutional.
resentation. This can be problematic since these are
often simple non-content words, such as a leading
?But,?. While such single words can be important for
tasks such as sentiment analysis, we argue that for
describing visual scenes the DT-RNN captures the
more important effects: The dependency tree struc-
tures push the central content words such as the main
action or verb and its subject and object to be merged
last and hence, by construction, the final sentence
representation is more robust to less important ad-
jectival modifiers, word order changes, etc.
Fourth, we allow some untying of weights de-
pending on either how far away a constituent is from
the current word or what its semantic relationship is.
Now that we can compute compositional vector
representations for sentences, the next section de-
scribes how we represent images.
4 Learning Image Representations with
Neural Networks
The image features that we use in our experiments
are extracted from a deep neural network, replicated
from the one described in (Le et al., 2012). The net-
work was trained using both unlabeled data (random
web images) and labeled data to classify 22,000 cat-
egories in ImageNet (Deng et al., 2009). We then
used the features at the last layer, before the classi-
fier, as the feature representation in our experiments.
The dimension of the feature vector of the last layer
is 4,096. The details of the model and its training
procedures are as follows.
The architecture of the network can be seen in
Figure 4. The network takes 200x200 pixel images
as inputs and has 9 layers. The layers consist of
three sequences of filtering, pooling and local con-
trast normalization (Jarrett et al., 2009). The pooling
function is L2 pooling of the previous layer (taking
the square of the filtering units, summing them up
in a small area in the image, and taking the square-
root). The local contrast normalization takes inputs
in a small area of the lower layer, subtracts the mean
and divides by the standard deviation.
The network was first trained using an unsuper-
vised objective: trying to reconstruct the input while
keeping the neurons sparse. In this phase, the net-
work was trained on 20 million images randomly
sampled from the web. We resized a given image
so that its short dimension has 200 pixels. We then
cropped a fixed size 200x200 pixel image right at the
center of the resized image. This means we may dis-
card a fraction of the long dimension of the image.
After unsupervised training, we used Ima-
geNet (Deng et al., 2009) to adjust the features in the
entire network. The ImageNet dataset has 22,000
categories and 14 million images. The number of
images in each category is equal across categories.
The 22,000 categories are extracted from WordNet.
To speed up the supervised training of this net-
work, we made a simple modification to the algo-
rithm described in Le et al. (2012): adding a ?bottle-
neck? layer in between the last layer and the classi-
fier. to reduce the number of connections. We added
one ?bottleneck? layer which has 4,096 units in be-
tween the last layer of the network and the softmax
layer. This newly-added layer is fully connected to
the previous layer and has a linear activation func-
tion. The total number of connections of this net-
work is approximately 1.36 billion.
212
The network was trained again using the super-
vised objective of classifying the 22,000 classes in
ImageNet. Most features in the networks are local,
which allows model parallelism. Data parallelism
by asynchronous SGD was also employed as in Le
et al. (2012). The entire training, both unsupervised
and supervised, took 8 days on a large cluster of ma-
chines. This network achieves 18.3% precision@1
on the full ImageNet dataset (Release Fall 2011).
We will use the features at the bottleneck layer as
the feature vector z of an image. Each scaled and
cropped image is presented to our network. The net-
work then performs a feedforward computation to
compute the values of the bottleneck layer. This
means that every image is represented by a fixed
length vector of 4,096 dimensions. Note that during
training, no aligned sentence-image data was used
and the ImageNet classes do not fully intersect with
the words used in our dataset.
5 Multimodal Mappings
The previous two sections described how we can
map sentences into a d = 50-dimensional space and
how to extract high quality image feature vectors of
4096 dimensions. We now define our final multi-
modal objective function for learning joint image-
sentence representations with these models. Our
training set consists of N images and their feature
vectors zi and each image has 5 sentence descrip-
tions si1, . . . , si5 for which we use the DT-RNN to
compute vector representations. See Fig. 5 for ex-
amples from the dataset. For training, we use a max-
margin objective function which intuitively trains
pairs of correct image and sentence vectors to have
high inner products and incorrect pairs to have low
inner products. Let vi = WIzi be the mapped image
vector and yij = DTRNN?(sij) the composed sen-
tence vector. We define S to be the set of all sentence
indices and S(i) the set of sentence indices corre-
sponding to image i. Similarly, I is the set of all im-
age indices and I(j) is the image index of sentence
j. The set P is the set of all correct image-sentence
training pairs (i, j). The ranking cost function to
minimize is then: J(WI , ?) =
?
(i,j)?P
?
c?S\S(i)
max(0,?? vTi yj + vTi yc)
+
?
(i,j)?P
?
c?I\I(j)
max(0,?? vTi yj + vTc yj), (5)
where ? are the language composition matrices,
and both second sums are over other sentences com-
ing from different images and vice versa. The hyper-
parameter ? is the margin. The margin is found via
cross validation on the dev set and usually around 1.
The final objective also includes the regulariza-
tion term ?/left(???22 + ?WI?F ). Both the visual
model and the word vector learning require a very
large amount of training data and both have a huge
number of parameters. Hence, to prevent overfitting,
we assume their weights are fixed and only train the
DT-RNN parameters WI . If larger training corpora
become available in the future, training both jointly
becomes feasible and would present a very promis-
ing direction. We use a modified version of Ada-
Grad (Duchi et al., 2011) for optimization of both
WI and the DT-RNN as well as the other baselines
(except kCCA). Adagrad has achieved good perfor-
mance previously in neural networks models (Dean
et al., 2012; Socher et al., 2013a). We modify it
by resetting all squared gradient sums to 1 every 5
epochs. With both images and sentences in the same
multimodal space, we can easily query the model for
similar images or sentences by finding the nearest
neighbors in terms of negative inner products.
An alternative objective function is based on the
squared loss J(WI , ?) = ?(i,j)?P ?vi ? yj?22. This
requires an alternating minimization scheme that
first trains only WI , then fixes WI and trains the
DT-RNN weights ? and then repeats this several
times. We find that the performance with this ob-
jective function (paired with finding similar images
using Euclidean distances) is worse for all models
than the margin loss of Eq. 5. In addition kCCA
also performs much better using inner products in
the multimodal space.
6 Experiments
We use the dataset of Rashtchian et al. (2010) which
consists of 1000 images, each with 5 sentences. See
Fig. 5 for examples.
We evaluate and compare the DT-RNN in three
different experiments. First, we analyze how well
the sentence vectors capture similarity in visual
meaning. Then we analyze Image Search with
Query Sentences: to query each model with a sen-
tence in order to find an image showing that sen-
213
1 . A woman and her dog watch the cameraman in their living with wooden floors .
2 . A woman sitting on the couch while a black faced dog runs across the floor.
3 . A woman wearing a backpack sits on a couch while a small dog runs on the hardwood floor next to her .
4 . A women sitting on a sofa while a small Jack Russell walks towards the camera .
5 . White and black small dog walks toward the camera while woman sits on couch , desk and computer seen 
    in the background as well as a pillow, teddy bear and moggie toy on the wood floor .
1 . A man in a cowboy hat check approaches a small red sports car .
2 . The back and left side of a red Ferrari and two men admiring it .
3 . The sporty car is admired by passer by .
4 . Two men next to a red sports car in a parking lot .
5 . Two men stand beside a red sports car.
Figure 5: Examples from the dataset of images and their sentence descriptions (Rashtchian et al., 2010). Sentence
length varies greatly and different objects can be mentioned first. Hence, models have to be invariant to word ordering.
tence?s visual ?meaning.? The last experiment De-
scribing Images by Finding Suitable Sentences does
the reverse search where we query the model with an
image and try to find the closest textual description
in the embedding space.
In our comparison to other methods we focus on
those models that can also compute fixed, continu-
ous vectors for sentences. In particular, we compare
to the RNN model on constituency trees of Socher
et al. (2011a), a standard recurrent neural network;
a simple bag-of-words baseline which averages the
words. All models use the word vectors provided by
Huang et al. (2012) and do not update them as dis-
cussed above. Models are trained with their corre-
sponding gradients and backpropagation techniques.
A standard recurrent model is used where the hidden
vector at word index t is computed from the hidden
vector at the previous time step and the current word
vector: ht = f(Whht?1 + Wxxt). During training,
we take the last hidden vector of the sentence chain
and propagate the error into that. It is also this vector
that is used to represent the sentence.
Other possible comparisons are to the very differ-
ent models mentioned in the related work section.
These models use a lot more task-specific engineer-
ing, such as running object detectors with bounding
boxes, attribute classifiers, scene classifiers, CRFs
for composing the sentences, etc. Another line of
work uses large sentence-image aligned resources
(Kuznetsova et al., 2012), whereas we focus on eas-
ily obtainable training data of each modality sepa-
rately and a rather small multimodal corpus.
In our experiments we split the data into 800 train-
ing, 100 development and 100 test images. Since
there are 5 sentences describing each image, we
have 4000 training sentences and 500 testing sen-
tences. The dataset has 3020 unique words, half of
which only appear once. Hence, the unsupervised,
pre-trained semantic word vector representations are
crucial. Word vectors are not fine tuned during train-
ing. Hence, the main parameters are the DT-RNN?s
Wl?,Wr? or the semantic matrices of which there are
141 and the image mappingWI . For both DT-RNNs
the weight matrices are initialized to block identity
matrices plus Gaussian noise. Word vectors and hid-
den vectors are set o length 50. Using the develop-
ment split, we found ? = 0.08 and the learning rate
of AdaGrad to 0.0001. The best model uses a mar-
gin of ? = 3.
Inspired by Socher and Fei-Fei (2010) and Ho-
dosh et al. (2013) we also compare to kernelized
Canonical Correlation Analysis (kCCA). We use the
average of word vectors for describing sentences and
the same powerful image vectors as before. We
use the code of Socher and Fei-Fei (2010). Tech-
nically, one could combine the recently introduced
deep CCA Andrew et al. (2013) and train the re-
cursive neural network architectures with the CCA
objective. We leave this to future work. With lin-
ear kernels, kCCA does well for image search but
is worse for sentence self similarity and describing
images with sentences close-by in embedding space.
All other models are trained by replacing the DT-
RNN function in Eq. 5.
6.1 Similarity of Sentences Describing the
Same Image
In this experiment, we first map all 500 sentences
from the test set into the multi-modal space. Then
for each sentence, we find the nearest neighbor sen-
214
Sentences Similarity for Image
Model Mean Rank
Random 101.1
BoW 11.8
CT-RNN 15.8
Recurrent NN 18.5
kCCA 10.7
DT-RNN 11.1
SDT-RNN 10.5
Image Search
Model Mean Rank
Random 52.1
BoW 14.6
CT-RNN 16.1
Recurrent NN 19.2
kCCA 15.9
DT-RNN 13.6
SDT-RNN 12.5
Describing Images
Model Mean Rank
Random 92.1
BoW 21.1
CT-RNN 23.9
Recurrent NN 27.1
kCCA 18.0
DT-RNN 19.2
SDT-RNN 16.9
Table 1: Left: Comparison of methods for sentence similarity judgments. Lower numbers are better since they indicate
that sentences describing the same image rank more highly (are closer). The ranks are out of the 500 sentences in the
test set. Center: Comparison of methods for image search with query sentences. Shown is the average rank of the
single correct image that is being described. Right: Average rank of a correct sentence description for a query image.
tences in terms of inner products. We then sort
these neighbors and record the rank or position of
the nearest sentence that describes the same im-
age. If all the images were very unique and the vi-
sual descriptions close-paraphrases and consistent,
we would expect a very low rank. However, usually
a handful of images are quite similar (for instance,
there are various images of airplanes flying, parking,
taxiing or waiting on the runway) and sentence de-
scriptions can vary greatly in detail and specificity
for the same image.
Table 1 (left) shows the results. We can see that
averaging the high quality word vectors already cap-
tures a lot of similarity. The chain structure of a
standard recurrent neural net performs worst since
its representation is dominated by the last words in
the sequence which may not be as important as ear-
lier words.
6.2 Image Search with Query Sentences
This experiment evaluates how well we can find im-
ages that display the visual meaning of a given sen-
tence. We first map a query sentence into the vector
space and then find images in the same space using
simple inner products. As shown in Table 1 (center),
the new DT-RNN outperforms all other models.
6.3 Describing Images by Finding Suitable
Sentences
Lastly, we repeat the above experiments but with
roles reversed. For an image, we search for suitable
textual descriptions again simply by finding close-
by sentence vectors in the multi-modal embedding
space. Table 1 (right) shows that the DT-RNN again
outperforms related models. Fig. 2assigned to im-
Image Search
Model mRank
BoW 24.7
CT-RNN 22.2
Recurrent NN 28.4
kCCA 13.7
DT-RNN 13.3
SDT-RNN 15.8
Describing Images
Model mRank
BoW 30.7
CT-RNN 29.4
Recurrent NN 31.4
kCCA 38.0
DT-RNN 26.8
SDT-RNN 37.5
Table 2: Results of multimodal ranking when models are
trained with a squared error loss and using Euclidean dis-
tance in the multimodal space. Better performance is
reached for all models when trained in a max-margin loss
and using inner products as in the previous table.
ages. The average ranking of 25.3 for a correct sen-
tence description is out of 500 possible sentences. A
random assignment would give an average ranking
of 100.
6.4 Analysis: Squared Error Loss vs. Margin
Loss
We analyze the influence of the multimodal loss
function on the performance. In addition, we com-
pare using Euclidean distances instead of inner prod-
ucts. Table 2 shows that performance is worse for all
models in this setting.
6.5 Analysis: Recall at n vs Mean Rank
Hodosh et al. (2013) and other related work use re-
call at n as an evaluation measure. Recall at n cap-
tures how often one of the top n closest vectors were
a correct image or sentence and gives a good intu-
ition of how a model would perform in a ranking
task that presents n such results to a user. Below, we
compare three commonly used and high performing
models: bag of words, kCCA and our SDT-RNN on
215
A gray convertible sports car is parked in front of the trees .
A close-up view of the headlights of a blue old -fashioned car.
Black shiny sports car parked on concrete driveway .
Five cows grazing on a patch of grass between two roadways .
A jockey rides a brown and white horse in a dirt corral .
A young woman is riding a Bay hose in a dirt riding -ring.
A white bird pushes a miniature teal shopping cart.
A person rides a brown horse.
A motocross bike with rider flying through the air .
White propeller plane parked in middle of grassy field .
The white jet with its landing gear down flies in the blue sky .
An elderly woman catches a ride on the back of the bicycle .
A green steam train running down the tracks.
Steamy locomotive speeding thou the forest .
A steam engine comes down a train track near trees.
A double decker bus is driving by Big Ben in London .
People in an outrigger canoe sail on emerald green water .
Two people sailing a small white sail boat.
behind a cliff, a boat sails away
Tourist move in on Big Ben on a typical overcast London day .
A group of people sitting around a table on a porch.
A group of four people walking past a giant mushroom.
A man and women smiling for the camera in a kitchen.
A group of men sitting around a table drinking while a man behind 
stands pointing.
Figure 6: Images and their sentence descriptions assigned by the DT-RNN.
Image Search
Model mRank 4 R@1 5 R@5 5 R@10 5
BoW 14.6 15.8 42.2 60.0
kCCA 15.9 16.4 41.4 58.0
SDT-RNN 12.5 16.4 46.6 65.6
Describing Images
BoW 21.1 19.0 38.0 57.0
kCCA 18.0 21.0 47.0 61.0
SDT-RNN 16.9 23.0 45.0 63.0
Table 3: Evaluation comparison between mean rank of
the closest correct image or sentence (lower is better 4)
with recall at different thresholds (higher is better, 5).
With one exception (R@5, bottom table), the SDT-RNN
outperforms the other two models and all other models
we did not include here.
this different metric. Table 3 shows that the mea-
sures do correlate well and the SDT-RNN also per-
forms best on the multimodal ranking tasks when
evaluated with this measure.
6.6 Error Analysis
In order to understand the main problems with the
composed sentence vectors, we analyze the sen-
tences that have the worst nearest neighbor rank be-
tween each other. We find that the main failure mode
of the SDT-RNN occurs when a sentence that should
describe the same image does not use a verb but the
other sentences of that image do include a verb. For
example, the following sentence pair has vectors that
are very far apart from each other even though they
are supposed to describe the same image:
1. A blue and yellow airplane flying straight down
while emitting white smoke
2. Airplane in dive position
Generally, as long as both sentences either have a
verb or do not, the SDT-RNN is more robust to dif-
ferent sentence lengths than bag of words represen-
tations.
6.7 Model Analysis: Semantic Composition
Matrices
The best model uses composition matrices based on
semantic relationships from the dependency parser.
We give some insights into what the model learns
by listing the composition matrices with the largest
Frobenius norms. Intuitively, these matrices have
learned larger weights that are being multiplied with
the child vector in the tree and hence that child will
have more weight in the final composed parent vec-
tor. In decreasing order of Frobenius norm, the re-
lationship matrices are: nominal subject, possession
modifier (e.g. their), passive auxiliary, preposition
at, preposition in front of, passive auxiliary, passive
nominal subject, object of preposition, preposition
in and preposition on.
The model learns that nouns are very important as
well as their spatial prepositions and adjectives.
7 Conclusion
We introduced a new recursive neural network
model that is based on dependency trees. For eval-
uation, we use the challenging task of mapping sen-
tences and images into a common space for finding
one from the other. Our new model outperforms
baselines and other commonly used models that can
compute continuous vector representations for sen-
tences. In comparison to related models, the DT-
RNN is more invariant and robust to surface changes
such as word order.
216
References
G. Andrew, R. Arora, K. Livescu, and J. Bilmes. 2013.
Deep canonical correlation analysis. In ICML, At-
lanta, Georgia.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language using
recursive neural networks. Applied Intelligence.
M. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin,
Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker,
K. Yang, and A.Y. Ng. 2012. Large scale distributed
deep networks. In NIPS.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic op-
timization. JMLR, 12, July.
P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth.
2002. Object recognition as machine translation. In
ECCV.
A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. 2010.
Every picture tells a story: Generating sentences from
images. In ECCV.
Y. Feng and M. Lapata. 2013. Automatic caption gen-
eration for news images. IEEE Trans. Pattern Anal.
Mach. Intell., 35.
A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,
M. Ranzato, and T. Mikolov. 2013. Devise: A deep
visual-semantic embedding model. In NIPS.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks.
E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning for
compositional distributional semantics. In IWCS.
A. Gupta and L. S. Davis. 2008. Beyond nouns: Exploit-
ing prepositions and comparative adjectives for learn-
ing visual classifiers. In ECCV.
M. Hodosh, P. Young, and J. Hockenmaier. 2013. Fram-
ing image description as a ranking task: Data, mod-
els and evaluation metrics. J. Artif. Intell. Res. (JAIR),
47:853?899.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
K. Jarrett, K. Kavukcuoglu, M.A. Ranzato, and Y. Le-
Cun. 2009. What is the best multi-stage architecture
for object recognition? In ICCV.
P. Blunsom. K.M. Hermann. 2013. The role of syntax
in vector space models of compositional semantics. In
ACL.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
Imagenet classification with deep convolutional neural
networks. In NIPS.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understanding
and generating image descriptions. In CVPR.
N. Kumar, A. C. Berg, P. N. Belhumeur, , and S. K. Na-
yar. 2009. Attribute and simile classifiers for face ver-
ification. In ICCV.
P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Yejin Choi. 2012. Collective generation of natural
image descriptions. In ACL.
Q. V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen,
G.S. Corrado, J. Dean, and A. Y. Ng. 2012. Build-
ing high-level features using large scale unsupervised
learning. In ICML.
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguistic
regularities in continuous spaceword representations.
In HLT-NAACL.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388?1429.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y.
Ng. 2011. Multimodal deep learning. In ICML.
V. Ordonez, G. Kulkarni, and T. L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
C. Rashtchian, P. Young, M. Hodosh, and J. Hocken-
maier. 2010. Collecting image annotations using
Amazon?s Mechanical Turk. In Workshop on Creat-
ing Speech and Language Data with Amazon?s MTurk.
R. Socher and L. Fei-Fei. 2010. Connecting modalities:
Semi-supervised segmentation and annotation of im-
ages using unaligned text corpora. In CVPR.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
217
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Unfold-
ing Recursive Autoencoders for Paraphrase Detection.
In NIPS.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In EMNLP.
R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. 2013a.
Parsing With Compositional Vector Grammars. In
ACL.
R. Socher, M. Ganjoo, C. D. Manning, and A. Y. Ng.
2013b. Zero-Shot Learning Through Cross-Modal
Transfer. In NIPS.
R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, and
A. Y. Ng. C. D. Manning and. 2013c. Zero-shot learn-
ing through cross-modal transfer. In Proceedings of
the International Conference on Learning Representa-
tions (ICLR, Workshop Track).
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In NIPS.
A. Torralba, K. P. Murphy, and W. T. Freeman. 2010.
Using the forest to see the trees: exploiting context for
visual object detection and localization. Communica-
tions of the ACM.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
B. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. 2010.
I2t:image parsing to text description. IEEE Xplore.
218
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104?113,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Better Word Representations with Recursive Neural Networks for
Morphology
Minh-Thang Luong Richard Socher Christopher D. Manning
Computer Science Department Stanford University, Stanford, CA, 94305
{lmthang, manning}@stanford.edu richard@socher.org
Abstract
Vector-space word representations have
been very successful in recent years at im-
proving performance across a variety of
NLP tasks. However, common to most
existing work, words are regarded as in-
dependent entities without any explicit re-
lationship among morphologically related
words being modeled. As a result, rare and
complex words are often poorly estimated,
and all unknown words are represented
in a rather crude way using only one or
a few vectors. This paper addresses this
shortcoming by proposing a novel model
that is capable of building representations
for morphologically complex words from
their morphemes. We combine recursive
neural networks (RNNs), where each mor-
pheme is a basic unit, with neural language
models (NLMs) to consider contextual
information in learning morphologically-
aware word representations. Our learned
models outperform existing word repre-
sentations by a good margin on word sim-
ilarity tasks across many datasets, includ-
ing a new dataset we introduce focused on
rare words to complement existing ones in
an interesting way.
1 Introduction
The use of word representations or word clusters
pretrained in an unsupervised fashion from lots of
text has become a key ?secret sauce? for the suc-
cess of many NLP systems in recent years, across
tasks including named entity recognition, part-of-
speech tagging, parsing, and semantic role label-
ing. This is particularly true in deep neural net-
work models (Collobert et al, 2011), but it is also
true in conventional feature-based models (Koo et
al., 2008; Ratinov and Roth, 2009).
Deep learning systems give each word a
distributed representation, i.e., a dense low-
dimensional real-valued vector or an embedding.
The main advantage of having such a distributed
representation over word classes is that it can cap-
ture various dimensions of both semantic and syn-
tactic information in a vector where each dimen-
sion corresponds to a latent feature of the word. As
a result, a distributed representation is compact,
less susceptible to data sparsity, and can implicitly
represent an exponential number of word clusters.
However, despite the widespread use of word
clusters and word embeddings, and despite much
work on improving the learning of word repre-
sentations, from feed-forward networks (Bengio
et al, 2003) to hierarchical models (Morin, 2005;
Mnih and Hinton, 2009) and recently recurrent
neural networks (Mikolov et al, 2010; Mikolov et
al., 2011), these approaches treat each full-form
word as an independent entity and fail to cap-
ture the explicit relationship among morphologi-
cal variants of a word.1 The fact that morphologi-
cally complex words are often rare exacerbates the
problem. Though existing clusterings and embed-
dings represent well frequent words, such as ?dis-
tinct?, they often badly model rare ones, such as
?distinctiveness?.
In this work, we use recursive neural networks
(Socher et al, 2011b), in a novel way to model
morphology and its compositionality. Essentially,
we treat each morpheme as a basic unit in the
RNNs and construct representations for morpho-
logically complex words on the fly from their mor-
phemes. By training a neural language model
(NLM) and integrating RNN structures for com-
plex words, we utilize contextual information in
1An almost exception is the word clustering of (Clark,
2003), which does have a model of morphology to encour-
age words ending with the same suffix to appear in the same
class, but it still does not capture the relationship between a
word and its morphologically derived forms.
104
an interesting way to learn morphemic semantics
and their compositional properties. Our model
has the capability of building representations for
any new unseen word comprised of known mor-
phemes, giving the model an infinite (if still in-
complete) covered vocabulary.
Our learned representations outperform pub-
licly available embeddings by a good margin on
word similarity tasks across many datasets, which
include our newly released dataset focusing on
rare words (see Section 5). The detailed analysis
in Section 6 reveals that our models can blend well
syntactic information, i.e., the word structure, and
the semantics in grouping related words.2
2 Related Work
Neural network techniques have found success in
several NLP tasks recently such as sentiment anal-
ysis at the sentence (Socher et al, 2011c) and
document level (Glorot et al, 2011), language
modeling (Mnih and Hinton, 2007; Mikolov and
Zweig, 2012), paraphrase detection (Socher et al,
2011a), discriminative parsing (Collobert, 2011),
and tasks involving semantic relations and compo-
sitional meaning of phrases (Socher et al, 2012).
Common to many of these works is use of a
distributed word representation as the basic input
unit. These representations usually capture lo-
cal cooccurrence statistics but have also been ex-
tended to include document-wide context (Huang
et al, 2012). Their main advantage is that they
can both be learned unsupervisedly as well as be
tuned for supervised tasks. In the former training
regiment, they are evaluated by how well they can
capture human similarity judgments. They have
also been shown to perform well as features for
supervised tasks, e.g., NER (Turian et al, 2010).
While much work has focused on different ob-
jective functions for training single and multi-
word vector representations, very little work has
been done to tackle sub-word units and how they
can be used to compute syntactic-semantic word
vectors. Collobert et al (2011) enhanced word
vectors with additional character-level features
such as capitalization but still can not recover
more detailed semantics for very rare or unseen
words, which is the focus of this work.
This is somewhat ironic, since working out cor-
2The rare word dataset and trained word vectors can be
found at http://nlp.stanford.edu/?lmthang/
morphoNLM.
rect morphological inflections was a very central
problem in early work in the parallel distributed
processing paradigm and criticisms of it (Rumel-
hart and McClelland, 1986; Plunkett and March-
man, 1991), and later work developed more so-
phisticated models of morphological structure and
meaning (Gasser and Lee, 1990; Gasser, 1994),
while not providing a compositional semantics nor
working at the scale of what we present.
To the best of our knowledge, the work clos-
est to ours in terms of handing unseen words are
the factored NLMs (Alexandrescu and Kirchhoff,
2006) and the compositional distributional seman-
tic models (DSMs) (Lazaridou et al, 2013). In
the former work, each word is viewed as a vec-
tor of features such as stems, morphological tags,
and cases, in which a single embedding matrix is
used to look up all of these features.3 Though
this is a principled way of handling new words in
NLMs, the by-product word representations, i.e.
the concatenations of factor vectors, do not en-
code in them the compositional information (they
are stored in the NN parameters). Our work does
not simply concatenate vectors of morphemes, but
rather combines them using RNNs, which cap-
tures morphological compositionality.
The latter work experimented with different
compositional DSMs, originally designed to learn
meanings of phrases, to derive representations for
complex words, in which the base unit is the mor-
pheme similar to ours. However, their models can
only combine a stem with an affix and does not
support recursive morpheme composition. It is,
however, interesting to compare our neural-based
representations with their DSM-derived ones and
cross test these models on both our rare word
similarity dataset and their nearest neighbor one,
which we leave as future work.
Mikolov et al (2013) examined existing word
embeddings and showed that these representations
already captured meaningful syntactic and seman-
tic regularities such as the singular/plural relation
that xapple - xapples ? xcar - xcars. However,we believe that these nice relationships will not
hold for rare and complex words when their vec-
tors are poorly estimated as we analyze in Sec-
tion 6. Our model, on the other hand, explicitly
represents these regularities through morphologi-
cal structures of words.
3(Collobert et al, 2011) used multiple embeddings, one
per discrete feature type, e.g., POS, Gazeteer, etc.
105
 
	

	




Figure 1: Morphological Recursive Neural Net-
work. A vector representation for the word ?un-
fortunately? is constructed from morphemic vec-
tors: unpre, fortunatestm, lysuf. Dotted nodes are
computed on-the-fly and not in the lexicon.
3 Morphological RNNs
Our morphological Recursive Neural Network
(morphoRNN) is similar to (Socher et al, 2011b),
but operates at the morpheme level instead of at
the word level. Specifically, morphemes, the mini-
mum meaning-bearing unit in languages, are mod-
eled as real-valued vectors of parameters, and are
used to build up more complex words. We assume
access to a dictionary of morphemic analyses of
words, which will be detailed in Section 4.
Following (Collobert and Weston, 2008), dis-
tinct morphemes are encoded by column vectors
in a morphemic embedding matrix We ? Rd?|M|,
where d is the vector dimension and M is an or-
dered set of all morphemes in a language.
As illustrated in Figure 1, vectors of morpho-
logically complex words are gradually built up
from their morphemic representations. At any lo-
cal decision (a dotted node), a new parent word
vector (p) is constructed by combining a stem vec-
tor (xstem) and an affix vector (xaffix) as follow:
p = f(Wm[xstem;xaffix] + bm) (1)
Here, Wm ? Rd?2d is a matrix of morphemic pa-
rameters while bm ? Rd?1 is an intercept vector.
We denote an element-wise activation function as
f , such as tanh. This forms the basis of our mor-
phoRNNmodels with ? = {We,Wm, bm} being
the parameters to be learned.
3.1 Context-insensitive Morphological RNN
Our first model examines how well morphoRNNs
could construct word vectors simply from the mor-
phemic representation without referring to any
context information. Input to the model is a refer-
ence embedding matrix, i.e. word vectors trained
by an NLM such as (Collobert and Weston, 2008)
and (Huang et al, 2012). By assuming that these
reference vectors are right, the goal of the model
is to construct new representations for morpholog-
ically complex words from their morphemes that
closely match the corresponding reference ones.
Specifically, the structure of the context-
insensitive morphoRNN (cimRNN) is the same as
the basic morphoRNN. For learning, we first de-
fine a cost function s for each word xi as the
squared Euclidean distance between the newly-
constructed representation pc(xi) and its refer-
ence vector pr(xi): s (xi) = ?pc(xi) ? pr(xi)?22.
The objective function is then simply the sum of
all individual costs over N training examples, plus
a regularization term, which we try to minimize:
J(?) =
N?
i=1
s (xi) +
?
2 ???
2
2 (2)
3.2 Context-sensitive Morphological RNN
The cimRNN model, though simple, is interesting
to attest if morphemic semantics could be learned
solely from an embedding. However, it is lim-
ited in several aspects. Firstly, the model has
no chance of improving representations for rare
words which might have been poorly estimated.
For example, ?distinctness? and ?unconcerned?
are very rare, occurring only 141 and 340 times
in Wikipedia documents, even though their corre-
sponding stems ?distinct? and ?concern? are very
frequent (35323 and 26080 respectively). Trying
to construct exactly those poorly-estimated word
vectors might result in a bad model with parame-
ters being pushed in wrong directions.
Secondly, though word embeddings learned
from an NLM could, in general, blend well both
the semantic and syntactic information, it would
be useful to explicitly model another kind of syn-
tactic information, the word structure, as we train
our embeddings. Motivated by these limitations,
we propose a context-sensitive morphoRNN (csm-
RNN) which integrates RNN structures into NLM
training, allowing for contextual information be-
ing taken into account in learning morphemic
compositionality. Specifically, we adopt the NLM
training approach proposed in (Collobert et al,
2011) to learn word embeddings, but build rep-
resentations for complex words from their mor-
phemes. During learning, updates at the top level
of the neural network will be back-propagated all
the way till the morphemic layer.
106
	
    	
 
 
 	
 	 

 



Figure 2: Context-sensitive morphological RNN
has two layers: (a) themorphological RNN, which
constructs representations for words from their
morphemes and (b) the word-based neural lan-
guage which optimizes scores for relevant ngrams.
Structure-wise, we stack the NLM on top of our
morphoRNN as illustrated in Figure 2. Complex
words like ?unfortunately? and ?closed? are con-
structed from their morphemic vectors, unpre +
fortunatestm + lysuf and closestm + dsuf, whereas
simple words4, i.e. stems, and affixes could be
looked up from the morphemic embedding ma-
trix We as in standard NLMs. Once vectors of all
complex words have been built, the NLM assigns
a score for each ngram ni consisting of words
x1, . . . , xn as follows:
s (ni) = ??f(W [x1; . . . ;xn] + b)
Here, xj is the vector representing the word xj .
We follow (Huang et al, 2012) to use a sim-
ple feed-forward network with one h-dimensional
hidden layer. W ? Rh?nd, b ? Rh?1, and
? ? Rh?1 are parameters of the NLM, and f is
an element-wise activation function as in Eq. (1).
We adopt a ranking-type cost in defining our ob-
jective function to minimize as below:
J(?) =
N?
i=1
max{0, 1? s (ni) + s (ni)} (3)
Here, N is the number of all available ngrams in
the training corpus, whereas ni is a ?corrupted?
ngram created from ni by replacing its last word
with a random word similar in spirit to (Smith
and Eisner, 2005). Our model parameters are
? = {We,Wm, bm,W , b,?}.
Such a ranking criterion influences the model
to assign higher scores to valid ngrams than to
4?fortunate?, ?the?, ?bank?, ?was?, and ?close?.
invalid ones and has been demonstrated in (Col-
lobert et al, 2011) to be both efficient and effective
in learning word representations.
3.3 Learning
Our models alternate between two stages: (1) for-
ward pass ? recursively construct morpheme trees
(cimRNN, csmRNN) and language model struc-
tures (csmRNN) to derive scores for training ex-
amples and (2) back-propagation pass ? compute
the gradient of the corresponding object function
with respect to the model parameters.
For the latter pass, computing the objective gra-
dient amounts to estimating the gradient for each
individual cost ?s(x)?? , where x could be either aword (cimRNN) or an ngram (csmRNN). We have
the objective gradient for the cimRNN derived as:
?J(?)
?? =
N?
i=1
?s (xi)
?? + ??
In the case of csmRNN, since the objective
function in Eq. (3) is not differentiable, we use the
subgradient method (Ratliff et al, 2007) to esti-
mate the objective gradient as:
?J(?)
?? =
?
i:1?s(ni)+s(ni)>0
??s (ni)?? +
?s (ni)
??
Back-propagation through structures (Goller
and Ku?chler, 1996) is employed to compute the
gradient for each individual cost with similar for-
mulae as in (Socher et al, 2010). Unlike their
RNN structures over sentences, where each sen-
tence could have an exponential number of deriva-
tions, our morphoRNN structure per word is, in
general, deterministic. Each word has a single
morphological tree structure which is constructed
from the main morpheme (the stem) and gradu-
ally appended affixes in a fixed order (see Sec-
tion 4 for more details). As a result, both our
forward and backward passes over morphologi-
cal structures are efficient with no recursive calls
implementation-wise.
4 Unsupervised Morphological
Structures
We utilize an unsupervised morphological seg-
mentation toolkit, named Morfessor by Creutz and
Lagus (2007), to obtain segmentations for words
in our vocabulary. Morfessor segments words in
107
two stages: (a) it recursively splits words to min-
imize an objective inspired by the minimum de-
scription length principle and (b) it labels mor-
phemes with tags pre (prefixes), stm (stems),
and suf (suffixes) using hidden Markov models.
Morfessor captures a general word structure of
the form (pre? stm suf?)+, which is handy
for words in morphologically rich languages like
Finnish or Turkish. However, such general form is
currently unnecessary in our models as the mor-
phoRNNs assume input of the form pre? stm
suf? for efficient learning of the RNN structures:
a stem is always combined with an affix to yield a
new stem.5 We, thus, postprocess as follows:
(1) Restrict segmentations to the form pre?
stm{1, 2} suf?: allow us to capture compounds.
(2) Split hyphenated words A-B as Astm Bstm.
(3) For a segmentation with two stems, pre?
Astm Bstm suf?, we decide if one could be a main
stem while the other could functions as an affix.6
Otherwise, we reject the segmentation. This will
provide us with more interesting morphemes such
as alpre in Arabic names (al-jazeera, al-salem) and
relatedsuf in compound adjectives (health-related,
government-related).
(4) To enhance precision, we reject a segmen-
tation if it has either an affix or an unknown stem
(not a word by itself) whose type count is below a
predefined threshold7.
The final list of affixes produced is given in Ta-
ble 1. Though generally reliable, our final seg-
mentations do contain errors, most notably non-
compositional ones, e.g. depre faultstm edsuf or
repre turnstm ssuf. With a sufficiently large num-
ber of segmentation examples, we hope that the
model would be able to pick up general trends
from the data. In total, we have about 22K com-
plex words out of a vocabulary of 130K words.
Examples of words with interesting affixes are
given in Table 2. Beside conventional affixes, non-
conventional ones like ?0? or ?mc? help further
categorize rare or unknown words into meaningful
groups such as measurement words or names.
5When multiple affixes are present, we use a simple
heuristic to first merge suffixes into stems and then combine
prefixes. Ideally, we would want to learn and generate an
order for such combination, which we leave for future work.
6We first aggregate type counts of pairs (A, left) and (B,
right) across all segmentations with two stems. Once done,
we label A as stm and B as suf if count (B, right) > 2 ?
count (A, left), and conversely, we label them as Apre Bstm if
count (A, left) > 2 ? count(B, right). Our rationale was that
Prefixes Suffixes
0 al all anti auto co
counter cross de dis
electro end ex first five
focus four half high hy-
per ill im in inter ir jan
jean long low market mc
micro mid multi neuro
newly no non off one
over post pre pro re sec-
ond self semi seven short
six state sub super third
three top trans two un
under uni well
able al ally american ance
ate ation backed bank
based born controlled d
dale down ed en er es field
ford free ful general head
ia ian ible ic in ing isation
ise ised ish ism ist ity ive
ization ize ized izing land
led less ling listed ly made
making man ment ness off
on out owned related s ship
shire style ton town up us
ville wood
Table 1: List of prefixes and suffixes discovered ?
conventional affixes in English are italicized.
Affix Words
0 0-acre, 0-aug, 0-billion, 0-centistoke
anti anti-immigrant, antipsychotics
counter counterexample, counterinsurgency
hyper hyperactivity, hypercholesterolemia
mc mcchesney, mcchord, mcdevitt
bank baybank, brockbank, commerzbank
ford belford, blandford, carlingford
land adventureland, bodoland, bottomland
less aimlessly, artlessness, effortlessly
owned bank-owned, city-owned disney-owned
Table 2: Sample affixes and corresponding words.
5 Experiments
As our focus is in learning morphemic seman-
tics, we do not start training from scratch, but
rather, initialize our models with existing word
representations. In our experiments, we make
use of two publicly-available embeddings (50-
dimensional) provided by(Collobert et al, 2011)
(denoted as C&W)8 and Huang et al (2012) (re-
ferred as HSMN)9.
Both of these representations are trained on
Wikipedia documents using the same ranking-type
cost function as in Eq. (3). The latter further uti-
lizes global context and adopts a multi-prototype
approach, i.e. each word is represented by mul-
tiple vectors, to better capture word semantics in
various contexts. However, we only use their
single-prototype embedding10 and as we train, we
affixes occur more frequently than stems.
7Set to 15 and 3 for affixes and stems respectively.
8http://ronan.collobert.com/senna/.
9http://www-nlp.stanford.edu/?ehhuang/.10The embedding obtained just before the clustering step
to build multi-prototype representation.
108
do not consider the global sentence-level context
information. It is worth to note that these aspects
of the HSMN embedding ? incorporating global
context and maintaining multiple prototypes ? are
orthogonal to our approach, which would be inter-
esting to investigate in future work.
For the context-sensitive morphoRNN model,
we follow Huang et al (2012) to use the April
2010 snapshot of the Wikipedia corpus (Shaoul
and Westbury, 2010). All paragraphs containing
non-roman characters are removed while the re-
maining text are lowercased and then tokenized.
The resulting clean corpus contains about 986 mil-
lion tokens. Each digit is then mapped into 0, i.e.
2013 will become 0000. Other rare words not in
the vocabularies of C&W and HSMN are mapped
to an UNKNOWN token, and we use <s> and
</s> for padding tokens representing the begin-
ning and end of each sentence.
Follow (Huang et al, 2012)?s implementation,
which our code is based on initially, we use 50-
dimensional vectors to represent morphemic and
word embeddings. For cimRNN, the regulariza-
tion weight ? is set to 10?2. For csmRNN, we use
10-word windows of text as the local context, 100
hidden units, and no weight regularization.
5.1 Word Similarity Task
Similar to (Reisinger and Mooney, 2010) and
(Huang et al, 2012), we evaluate the quality of our
morphologically-aware embeddings on the popu-
lar WordSim-353 dataset (Finkelstein et al, 2002),
WS353 for short. In this task, we compare corre-
lations between the similarity scores given by our
models and those rated by human.
To avoid overfitting our models to a single
dataset, we benchmark our models on a vari-
ety of others including MC (Miller and Charles,
1991), RG (Rubenstein and Goodenough, 1965),
SCWS?11 (Huang et al, 2012), and our new rare
word (RW) dataset (details in ?5.1.1). Information
about these datasets are summarized in Table 3
We also examine these datasets from the
?rareness? aspect by looking at distributions of
words across frequencies as in Table 4. The first
bin counts unknown words in each dataset, while
the remaining bins group words based on their
11SCWS? is a modified version of the Stanford?s contex-
tual word similarities dataset. The original one utilizes sur-
rounding contexts in judging word similarities and includes
pairs of identical words, e.g. financial bank vs. river bank.
We exclude these pairs and ignore the provided contexts.
pairs type raters scale Complex wordstoken type
WS353 353 437 13-16 0-10 24 17
MC 30 39 38 0-4 0 0
RG 65 48 51 0-4 0 0
SCWS? 1762 1703 10 0-10 190 113
RW (new) 2034 2951 10 0-10 987 686
Table 3: Word similarity datasets and their
statistics: number of pairs/raters/type counts as
well as rating scales. The number of complex
words are shown as well (both type and token
counts). RW denotes our new rare word dataset.
frequencies extracted from Wikipedia documents.
It is interesting to observe that WS353, MC, RG
contain very frequent words and have few complex
words (only WS353 has).12 SCWS? and RW have
a more diverse set of words in terms of frequencies
and RW has the largest number of unknown and
rare words, which makes it a challenging dataset.
All words Complex words
WS353 0 | 0 / 9 / 87 / 341 0 | 0 / 1 / 6 / 10
MC 0 | 0 / 1 / 17 / 21 0 | 0 / 0 / 0 / 0
RG 0 | 0 / 4 / 22 / 22 0 | 0 / 0 / 0 / 0
SCWS? 26 | 2 / 140 / 472 / 1063 8 | 2 / 22 / 44 / 45
RW 801 | 41 / 676 / 719 / 714 621 | 34 / 311 / 238 / 103
Table 4: Word distribution by frequencies ? dis-
tinct words in each dataset are grouped based on
frequencies and counts are reported for the fol-
lowing bins : unknown | [1, 100] / [101, 1000] /
[1001, 10000] / [10001, ?). We report counts for
all words in each dataset as well as complex ones.
5.1.1 Rare Word Dataset
As evidenced in Table 4, most existing word sim-
ilarity datasets contain frequent words and few of
them possesses enough rare or morphologically
complex words that we could really attest the ex-
pressiveness of our morphoRNN models. In fact,
we believe a good embedding in general should be
able to learn useful representations for not just fre-
quent words but also rare ones. That motivates us
to construct another dataset focusing on rare words
to complement existing ones.
Our dataset construction proceeds in three
stages: (1) select a list of rare words, (2) for each
of the rare words, find another word (not neces-
sarily rare) to form a pair, and (3) collect human
judgments on how similar each pair is.
12All these counts are with respect to the vocabulary list in
the C&W embedding (we obtain similar figures for HSMN).
109
(5, 10] (10, 100] (100, 1000]
un- untracked unrolls undissolved unrehearsed unflagging unfavourable unprecedented unmarried uncomfortable-al apocalyptical traversals bestowals acoustical extensional organismal directional diagonal spherical-ment obtainment acquirement retrenchments discernment revetment rearrangements confinement establishment management
word1 untracked unflagging unprecedented apocalyptical organismal diagonal obtainment discernment confinementword2 inaccessible constant new prophetic system line acquiring knowing restraint
Table 5: Rare words (top) ? word1 by affixes and frequencies and sample word pairs (bottom).
Rare word selection: our choices of rare words
(word1) are based on their frequencies ? based on
five bins (5, 10], (10, 100], (100, 1000], (1000,
10000], and the affixes they possess. To create a
diverse set of candidates, we randomly select 15
words for each configuration (a frequency bin, an
affix). At the scale of Wikipedia, a word with
frequency of 1-5 is most likely a junk word, and
even restricted to words with frequencies above
five, there are still many non-English words. To
counter such problems, each word selected is re-
quired to have a non-zero number of synsets in
WordNet(Miller, 1995).
Table 5 (top) gives examples of rare words se-
lected and organized by frequencies and affixes. It
is interesting to find out that words like obtainment
and acquirement are extremely rare (not in tradi-
tional dictionaries) but are perfectly understand-
able. We also have less frequent words like revet-
ment from French or organismal from biology.
Pair construction: following (Huang et al,
2012), we create pairs with interesting relation-
ships for each word1 as follow. First, a Word-
Net synset of word1 is randomly selected, and we
construct a set of candidates which connect to that
synset through various relations, e.g., hypernyms,
hyponyms, holonyms, meronyms, and attributes.
A word2 is then randomly selected from these can-
didates, and the process is repeated another time
to generate a total of two pairs for each word1.
Sample word pairs are given in Table 5 in which
word2 includes mostly frequent words, implying
a balance of words in terms of frequencies in our
dataset. We collected 3145 pairs after this stage
Human judgment: we use Amazon Mechani-
cal Turk to collect 10 human similarity ratings on
a scale of [0, 10] per word pair.13 Such procedure
has been demonstrated by Snow et al (2008) in
replicating ratings for the MC dataset, achieving
close inter-annotator agreement with expert raters.
Since our pairs contain many rare words which are
13We restrict to only US-based workers with 95% approval
rate and ask for native speakers to rate 20 pairs per hit.
challenging even to native speakers, we ask raters
to indicate for each pair if they do not know the
first word, the second word, or both. We use such
information to collect reliable ratings by either dis-
card pairs which many people do not know or col-
lect additional ratings to ensure we have 10 rat-
ings per pair.14 As a result, only 2034 pairs are
retained.
5.2 Results
We evaluate the quality of our morphoRNN em-
beddings through the word similarity task dis-
cussed previously. The Spearman?s rank correla-
tion is used to gauge how well the relationship be-
tween two variables, the similarity scores given by
the NLMs and the human annotators, could be de-
scribed using a monotonic function.
Detailed performance of the morphoRNN em-
beddings trained from either the HSMN or the
C&W embeddings are given in Table 7 for
all datasets. We also report baseline results
(rows HSMN, C&W) using these initial embed-
dings alone, which interestingly reveals strengths
and weaknesses of existing embeddings. While
HSMN is good for datasets with frequent words
(WS353, MC, and RG), its performances for those
with more rare and complex words (SCWS? and
RW) are much inferior than those of C&W, and
vice versa. Additionally, we consider two slightly
more competitive baselines (rows +stem) based
on the morphological segmentation of unknown
words: instead of using a universal vector repre-
senting all unknown words, we use vectors rep-
resenting the stems of unknown words. These
baselines yield slightly better performance for the
SCWS? and RW datasets while the trends we men-
tioned earlier remain the same.
Our first model, the context-insensitive mor-
phoRNN (cimRNN), outperforms its correspond-
ing baseline significantly over the rare word
14In our later experiments, an aggregated rating is derived
for each pair. We first discard ratings not within one standard
deviation of the mean, and then estimate a new mean from
the remaining ones to use as an aggregated rating.
110
Words C&W C&W + cimRNN C&W + csmRNNcommenting insisting insisted focusing hinted republishing accounting expounding commented comments criticizingcomment commentary rant statement remark commentary rant statement remark rant commentary statement anecdote
distinctness morphologies pesawat clefts modality indistinct tonality spatiality indistinct distinctiveness largeness uniquenessdistinct different distinctive broader narrower different distinctive broader divergent divergent diverse distinctive homogeneous
unaffected unnoticed dwarfed mitigated disaffected unconstrained uninhibited undesired unhindered unrestrictedaffected caused plagued impacted damaged disaffected unaffected mitigated disturbed complicated desired constrained reasonedunaffect ? affective affecting affectation unobserved affective affecting affectation restrictiveaffect exacerbate impacts characterize affects affectation exacerbate characterize decrease arise complicate exacerbateheartlessness ? fearlessness vindictiveness restlessness depersonalization terrorizes sympathizesheartless merciless sadistic callous mischievous merciless sadistic callous mischievous sadistic callous merciless hideousheart death skin pain brain life blood death skin pain brain life blood death brain blood skin lung mouth
saudi-owned avatar mohajir kripalani fountainhead saudi-based somaliland al-jaber saudi-based syrian-controlled syrian-backedshort-changed kindled waylaid endeared peopled conformal conformist unquestionable short-termism short-positions self-sustainable
Table 6: Nearest neighbors. We showmorphologically related words and their closest words in different
representations (?unaffect? is a pseudo-word; ? marks no results due to unknown words).
WS353 MC RG SCWS? RW
HSMN 62.58 65.90 62.81 32.11 1.97
+stem 62.58 65.90 62.81 32.11 3.40
+cimRNN 62.81 65.90 62.81 32.97 14.85
+csmRNN 64.58 71.72 65.45 43.65 22.31
C&W 49.77 57.37 49.30 48.59 26.75
+stem 49.77 57.37 49.30 49.05 28.03
+cimRNN 51.76 57.37 49.30 47.00 33.24
+csmRNN 57.01 60.20 55.40 48.48 34.36
Table 7: Word similarity task ? shown are Spear-
man?s rank correlation coefficient (? ? 100) be-
tween similarity scores assigned by neural lan-
guage models and by human annotators. stem in-
dicates baseline systems in which unknown words
are represented by their stem vectors. cimRNN and
csmRNN refer to our context insensitive and sensi-
tive morphological RNNs respectively.
dataset. The performance is constant for MC and
RG (with no complex words) and modestly im-
proved for MC (with some complex words ? see
Table 4). This is expected since the cimRNN
model only concerns about reconstructing the
original embedding (while learning word struc-
tures), and the new representation mostly differs
at morphologically complex words. For SCWS?,
the performance, however, decreases when train-
ing with C&W, which perhaps is due to: (a) the
baseline performance of C&W for SCWS? is com-
petitive and (b) the model trades off between learn-
ing syntactics (the word structure) and capturing
semantics, which requires context information.
On the other hand, the context-sensitive mor-
phoRNN (csmRNN) consistently improves corre-
lations over the cimRNN model for all datasets,
demonstrating the effectiveness of using surround-
ing contexts in learning both morphological syn-
tactics and semantics. It also outperforms the
corresponding baselines by a good margin for all
datasets (except for SCWS?). This highlights the
fact that our method is reliable and potentially ap-
plicable for other embeddings.
6 Analysis
To gain a deeper understanding of how our mor-
phoRNN models have ?moved? word vectors
around, we look at nearest neighbors of sev-
eral complex words given by various embed-
dings, where cosine similarity is used as a dis-
tance metric. Examples are shown in Table 6
for three representations: C&W and the context-
insensitive/sensitive morphoRNN models trained
on the C&W embedding.15
Syntactically, it is interesting to observe that
the cimRNN model could well enforce structural
agreement among related words. For example, it
returns V-ing as nearest neighbors for ?comment-
ing? and similarly, JJ-ness for ?fearlessness?, an
unknown word that C&W cannot handle. How-
ever, for those cases, the nearest neighbors are
badly unrelated.
On the semantic side, we notice that when
structural agreement is not enforced, the cimRNN
model tends to cluster words sharing the same
stem together, e.g., rows with words of the form
affect .16 This might be undesirable when we
want to differentiate semantics of words sharing
the same stem, e.g. ?affected? and ?unaffected?.
The csmRNN model seems to balance well be-
tween the two extremes (syntactic and seman-
tic) by taking into account contextual information
15Results of HSMN-related embeddings are not shown, but
similar trends follow.
16?unaffect? is a pseudo-word that we inserted.
111
when learning morphological structures. It returns
neighbors of the same structure un ed for ?unaf-
fected?, but does not include any negation of ?af-
fected? in the top 10 results when ?affected? is
queried.17 Even better, the answers for ?distinct-
ness? have blended well both types of results.
7 Conclusion
This paper combines recursive neural networks
(RNNs) and neural language models (NLMs) in
a novel way to learn better word representa-
tions. Each of these components contributes to
the learned syntactic-semantic word vectors in a
unique way. The RNN explicitly models the mor-
phological structures of words, i.e., the syntactic
information, to learn morphemic compositional-
ity. This allows for better estimation of rare and
complex words and a more principled way of han-
dling unseen words, whose representations could
be constructed from vectors of knownmorphemes.
The NLMs, on the other hand, utilize surround-
ing word contexts to provide further semantics
to the learned morphemic representations. As
a result, our context-sensitive morphoRNN em-
beddings could significantly outperform existing
embeddings on word similarity tasks for many
datasets. Our analysis reveals that the model could
blend well both the syntactic and semantic infor-
mation in clustering related words. We have also
made available a word similarity dataset focusing
on rare words to complement existing ones which
tend to include frequent words.
Lastly, as English is still considered limited
in terms of morphology, our model could poten-
tially yield even better performance when applied
to other morphologically complex languages such
as Finnish or Turkish, which we leave for future
work. Also, even within English, we expect our
model to be value to other domains, such as bio-
NLP with complicated but logical taxonomy.
Acknowledgements
We thank the anonymous reviewers for their feed-
back and Eric Huang for making his various pieces
of code available to us as well as answering our
questions on different datasets. Stanford Uni-
versity gratefully acknowledges the support of
the Defense Advanced Research Projects Agency
(DARPA) Deep Exploration and Filtering of Text
17?disaffected? is ranked 5th for the first query while ?af-
fecting? occurs at position 8 for the latter.
(DEFT) Program under Air Force Research Lab-
oratory (AFRL) contract no. FA8750-13-2-0040
and the DARPA Broad Operational Language
Translation (BOLT) program through IBM. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
Factored neural language models. In NAACL.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In EACL.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
R. Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In AISTATS.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4:3:1?3:34.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Michael Gasser and Chan-Do Lee. 1990. A short-
term memory architecture for the learning of mor-
phophonemic rules. In NIPS.
Michael Gasser. 1994. Acquiring receptive morphol-
ogy: A connectionist model. In ACL.
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In ICML.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backprop-
agation through structure. IEEE Transactions on
Neural Networks, 1:347?352.
112
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving word representations via global
context and multiple word prototypes. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In ACL.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT.
Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In NAACL-HLT.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
G.A. Miller. 1995. WordNet: A Lexical Database for
English. Communications of the ACM.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In ICML.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In NIPS.
Frederic Morin. 2005. Hierarchical probabilistic neu-
ral network language model. AIstats?05. In AIS-
TATS.
K. Plunkett and V. Marchman. 1991. U-shaped learn-
ing and frequency effects in a multi-layered percep-
tron: implications for child language acquisition.
Cognition, 38(1):43?102.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2007. Online subgradient methods for
structured prediction.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In NAACL.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
David E. Rumelhart and James L. McClelland. 1986.
On learning the past tenses of English verbs. In J. L.
McClelland, D. E. Rumelhart, and PDP Research
Group, editors, Parallel Distributed Processing. Vol-
ume 2: Psychological and Biological Models, pages
216?271. MIT Press.
Cyrus Shaoul and Chris Westbury. 2010. The West-
bury lab wikipedia corpus. Edmonton, AB: Univer-
sity of Alberta.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In ACL.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In EMNLP.
Richard Socher, Christopher Manning, and Andrew
Ng. 2010. Learning continuous phrase represen-
tations and syntactic parsing with recursive neural
networks. In NIPS*2010 Workshop on Deep Learn-
ing and Unsupervised Feature Learning.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic pooling and un-
folding recursive autoencoders for paraphrase detec-
tion. In NIPS.
R. Socher, Cliff C. Lin, A. Y. Ng, and C. D. Manning.
2011b. Parsing natural scenes and natural language
with recursive neural networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011c. Semi-supervised recursive
autoencoders for predicting sentiment distributions.
In EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. In EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In ACL.
113
