Statistical Language Modeling with Performance Benchmarks
using Various Levels of Syntactic-Semantic Information
Dharmendra KANEJIYA?, Arun KUMAR?, Surendra PRASAD?
?Department of Electrical Engineering
?Centre for Applied Research in Electronics
Indian Institute of Technology
New Delhi 110016 INDIA
kanejiya@hotmail.com, arunkm@care.iitd.ernet.in, sprasad@ee.iitd.ernet.in
Abstract
Statistical language models using n-gram
approach have been under the criticism of
neglecting large-span syntactic-semantic in-
formation that influences the choice of the
next word in a language. One of the ap-
proaches that helped recently is the use of
latent semantic analysis to capture the se-
mantic fabric of the document and enhance
the n-gram model. Similarly there have
been some approaches that used syntactic
analysis to enhance the n-gram models. In
this paper, we explain a framework called
syntactically enhanced latent semantic anal-
ysis and its application in statistical lan-
guage modeling. This approach augments
each word with its syntactic descriptor in
terms of the part-of-speech tag, phrase type
or the supertag. We observe that given this
syntactic knowledge, the model outperforms
LSA based models significantly in terms of
perplexity measure. We also present some
observations on the effect of the knowledge
of content or function word type in language
modeling. This paper also poses the prob-
lem of better syntax prediction to achieve
the benchmarks.
1 Introduction
Statistical language models consist of estimat-
ing the probability distributions of a word given
the history of words so far used. The standard
n-gram language model considers two histories
to be equivalent if they end in the same n ? 1
words. Due to the tradeoff between predictive
power and reliability of estimation, n is typically
chosen to be 2 (bi-gram) or 3 (tri-gram). Even
tri-gram model suffers from sparse-data estima-
tion problem, but various smoothing techniques
(Goodman, 2001) have led to significant im-
provements in many applications. But still the
criticism that n-grams are unable to capture the
long distance dependencies that exist in a lan-
guage, remains largely valid.
In order to model the linguistic structure that
spans a whole sentence or a paragraph or even
more, various approaches have been taken re-
cently. These can be categorized into two main
types : syntactically motivated and semanti-
cally motivated large span consideration. In
the first type, probability of a word is decided
based on a parse-tree information like grammat-
ical headwords in a sentence (Charniak, 2001)
(Chelba and Jelinek, 1998), or based on part-
of-speech (POS) tag information (Galescu and
Ringger, 1999). Examples of the second type
are (Bellegarda, 2000) (Coccaro and Jurafsky,
1998), where latent semantic analysis (LSA)
(Landauer et al, 1998) is used to derive large-
span semantic dependencies. LSA uses word-
document co-occurrence statistics and a matrix
factorization technique called singular value de-
composition to derive semantic similarity mea-
sure between any two text units - words or doc-
uments. Each of these approaches, when inte-
grated with n-gram language model, has led to
improved performance in terms of perplexity as
well as speech recognition accuracy.
While each of these approaches has been stud-
ied independently, it would be interesting to see
how they can be integrated in a unified frame-
work which looks at syntactic as well as seman-
tic information in the large span. Towards this
direction, we describe in this paper a mathemat-
ical framework called syntactically enhanced la-
tent syntactic-semantic analysis (SELSA). The
basic hypothesis is that by considering a word
alongwith its syntactic descriptor as a unit of
knowledge representation in the LSA-like frame-
work, gives us an approach to joint syntactic-
semantic analysis of a document. It also pro-
vides a finer resolution in each word?s seman-
tic description for each of the syntactic con-
texts it occurs in. Here the syntactic descriptor
can come from various levels e.g. part-of-speech
tag, phrase type, supertag etc. This syntactic-
semantic representation can be used in language
modeling to allocate the probability mass to
words in accordance with their semantic simi-
larity to the history as well as syntactic fitness
to the local context.
In the next section, we present the mathe-
matical framework. Then we describe its ap-
plication to statistical language modeling. In
section 4 we explain the the use of various lev-
els of syntactic information in SELSA. That is
followed by experimental results and conclusion.
2 Syntactically Enhanced LSA
Latent semantic analysis (LSA) is a statistical,
algebraic technique for extracting and inferring
relations of expected contextual usage of words
in documents (Landauer et al, 1998). It is
based on word-document co-occurrence statis-
tics, and thus is often called a ?bag-of-words?
approach. It neglects the word-order or syn-
tactic information in a language which if prop-
erly incorporated, can lead to better language
modeling. In an effort to include syntactic in-
formation in the LSA framework, we have de-
veloped a model which characterizes a word?s
behavior across various syntactic and semantic
contexts. This can be achieved by augment-
ing a word with its syntactic descriptor and
considering as a unit of knowledge representa-
tion. The resultant LSA-like analysis is termed
as syntactically enhanced latent semantic anal-
ysis (SELSA). This approach can better model
the finer resolution in a word?s usage compared
to an average representation by LSA. This finer
resolution can be used to better discriminate
semantically ambiguous sentences for cognitive
modeling as well as to predict a word using
syntactic-semantic history for language model-
ing. We explain below, a step-by-step procedure
for building this model.
2.1 Word-Tag-Document Structure
The syntactic description of a word can be in
many forms like part-of-speech tag, phrase type
or supertags. In the description hereafter we
call any such syntactic information as tag of the
word. Now, consider a tagged training corpus
of sufficient size in the domain of interest. The
first step is to construct a matrix whose rows
correspond to word-tag pairs and columns cor-
respond to documents in the corpus. A docu-
ment can be a sentence, a paragraph or a larger
unit of text. If the vocabulary V consists of
I words, tagset T consists of J tags and the
number of documents in corpus is K, then the
matrix will be IJ ? K. Let ci j,k denote the
frequency of word wi with tag tj in the docu-
ment dk. The notation i j (i underscore j) in
subscript is used for convenience and indicates
word wi with tag tj i.e., (i ? 1)J + jth row of
the matrix. Then we find entropy ?i j of each
word-tag pair and scale the corresponding row
of the matrix by (1??i j). The document length
normalization to each column of the matrix is
also applied by dividing the entries of kth doc-
ument by nk, the number of words in document
dk. Let ci j be the frequency of i jth word-tag
pair in the whole corpus i.e. ci j =
?K
k=1 ci j,k.
Then,
xi j,k = (1? ?i j)
ci j,k
nk
(1)
?i j = ?
1
logK
K?
k=1
ci j,k
ci j
log
ci j,k
ci j
(2)
Once the matrix X is obtained, we perform
its singular value decomposition (SVD) and ap-
proximate it by keeping the largest R singular
values and setting the rest to zero. Thus,
X ? X? = USVT (3)
where, U(IJ ?R) and V(K ?R) are orthonor-
mal matrices and S(R?R) is a diagonal matrix.
It is this dimensionality reduction step through
SVD that captures major structural associa-
tions between words-tags and documents, re-
moves ?noisy? observations and allows the same
dimensional representation of words-tags and
documents (albeit, in different bases).This same
dimensional representation is used (eq. 12) to
find syntactic-semantic correlation between the
present word and the history of words and then
to derive the language model probabilities. This
R-dimensional space can be called either syntac-
tically enhanced latent semantic space or latent
syntactic-semantic space.
2.2 Document Projection in SELSA
Space
After the knowledge is represented in the latent
syntactic-semantic space, we can project any
new document as a R dimensional vector v?selsa
in this space. Let the new document consist
of a word sequence wi1 , wi2 , . . . , win and let the
corresponding tag sequence be tj1 , tj2 , . . . , tjn ,
where ip and jp are the indices of the pth word
and its tag in the vocabulary V and the tagset
T respectively. Let d be the IJ ? 1 vector rep-
resenting this document whose elements di j are
the frequency counts i.e. number of times word
wi occurs with tag pj , weighted by its corre-
sponding entropy measure (1 ? ?i j). It can be
thought of as an additional column in the ma-
trix X, and therefore can be thought of as hav-
ing its corresponding vector v in the matrix V.
Then, d = USvT and
v?selsa=vS=d
TU=
1
n
n?
p=1
(1? ?ip jp)uip jp (4)
which is a 1?R dimensional vector representa-
tion of the document in the latent space. Here
uip jp represents the row vector of the SELSA
U matrix corresponding to the word wip and
tag tjp in the current document.
We can also define a syntactic-semantic simi-
larity measure between any two text documents
as the cosine of the angle between their pro-
jection vectors in the latent syntactic-semantic
space. With this measure we can address the
problems that LSA has been applied to, namely
natural language understanding, cognitive mod-
eling, statistical language modeling etc.
3 Statistical Language Modeling
using SELSA
3.1 Framework
We follow the framework in (Bangalore, 1996)
to define a class-based language model where
classes are defined by the tags. Here probability
of a sequence Wn of n words is given by
P (Wn) =
?
t1
. . .
?
tn
n?
q=1
P (wq|tq,Wq?1, Tq?1)
P (tq|Wq?1, Tq?1) (5)
where ti is a tag variable for the word wi. To
compute this probability in realtime based on
local information, we make certain assumptions:
P (wq|tq,Wq?1, Tq?1) ? P (wq|tq, wq?1, wq?2)
P (tq|Wq?1, Tq?1) ? P (tq|tq?1) (6)
where probability of a word is calculated by
renormalizing the tri-gram probability to those
words which are compatible with the tag in con-
text. Similarly, tag probability is modeled using
a bi-gram model. Other models like tag based
likelihood probability of a word or tag tri-grams
can also be used. Similarly there is a motiva-
tion for using the syntactically enhanced latent
semantic analysis method to derive the word
probability given the syntax of tag and seman-
tics of word-history.
The calculation of perplexity is based on con-
ditional probability of a word given the word
history, which can be derived in the following
manner using recursive computation.
P (wq|Wq?1)
=
?
tq
P (wq|tq,Wq?1)P (tq|Wq?1)
?
?
tq
P (wq|tq, wq?1, wq?2)
?
tq?1
P (tq|tq?1)P (tq?1|Wq?1)
=
?
tq
P (wq|tq, wq?1, wq?2)
?
tq?1
P (tq|tq?1)
P (Wq?1, tq?1)
?
tq?1 P (Wq?1, tq?1)
(7)
where,
P (Wq, tq)
=
?
?
?
tq?1
P (Wq?1, tq?1)P (tq|tq?1)
?
?
P (wq|tq, wq?1, wq?2) (8)
A further reduction in computation is
achieved by restricting the summation over only
those tags which the target word can anchor. A
similar expression using the tag tri-gram model
can be derived which includes double summa-
tion. The efficiency of this model depends upon
the prediction of tag tq using the word his-
tory Wq?1. When the target tag is correctly
known, we can derive a performance bench-
mark in terms of lower bound on the perplexity
achievable. Furthermore, if we assume tagged
corpus, then tq?s and Tq?s become deterministic
variables and (5) and (7) can be written as,
P (Wn) =
n?
q=1
P (wq|tq,Wq?1, Tq?1) (9)
P (wq|Wq?1) = P (wq|tq,Wq?1, Tq?1) (10)
respectively in which case the next described
SELSA language model can be easily applied to
calculate the benchmarks.
3.2 SELSA Language Model
SELSA model using tag information for each
word can also be developed and used along the
line of LSA based language model. We can
observe in the above framework the need for
the probability of the form P (wq|tq,Wq?1, Tq?1)
which can be evaluated using the SELSA rep-
resentation of the word-tag pair corresponding
to wq and tq and the history Wq?1Tq?1. The
former is given by the row uiq jq of SELSA U
matrix and the later can be projected onto the
SELSA space as a vector ??vq?1 using (4). The
length of history can be tapered to reduce the
effect of far distant words using the exponential
forgetting factor 0 < ? < 1 as below:
??vq?1 =
1
q ? 1
q?1?
p=1
?q?1?p(1? ?ip jp)uip jp (11)
The next step is to calculate the cosine mea-
sure reflecting the syntactic-semantic ?closeness?
between the word wq and the history Wq?1 as
below:
K(wq,Wq?1) =
uiq jq ??v
T
q?1
? uiq jqS
1
2 ?? ??vq?1S?
1
2 ?
(12)
Then SELSA based probability
P (sel)(wq|Wq?1) is calculated by allocating
total probability mass in proportion to this
closeness measure such that least likely word
has a probability of 0 and all probabilities sum
to 1:
Kmin(Wq?1) = min
wi?V
K(wi,Wq?1) (13)
P? (wq|Wq?1)=
K(wq,Wq?1)?Kmin(Wq?1)
?
wi?V
(K(wi,Wq?1)?Kmin(Wq?1))
(14)
But this results in a very limited dynamic range
for SELSA probabilities which leads to poor
performance. This is alleviated by raising the
above derived probability to a power ? > 1 and
then normalizing as follows(Coccaro and Juraf-
sky, 1998):
P (sel)(wq|Wq?1) =
P? (wq|Wq?1)
?
?
wi?V
P? (wi|Wq?1)
? (15)
This probability gives more importance to the
large span syntactic-semantic dependencies and
thus would be higher for those words which are
syntactic-semantically regular in the recent his-
tory as compared to others. But it will not
predict very well certain locally regular words
like of, the etc whose main role is to support
the syntactic structure in a sentence. On the
other hand, n-gram language models are able
to model them well because of maximum likeli-
hood estimation from training corpus and var-
ious smoothing techniques. So the best perfor-
mance can be achieved by integrating the two.
One way to derive the ?SELSA + N-gram? joint
probability P (sel+ng)(wq|Wq?1) is to use the ge-
ometric mean based integration formula given
for LSA in (Coccaro and Jurafsky, 1998) as fol-
lows:
P (sel+ng)(wq|Wq?1)=
[P (sel)(wq |Wq?1)]
?iq [P (wq |wq?1,...,wq?n+1)]
1??iq
?
wi?V
[P (sel)(wi|Wq?1)]
?i [P (wi|wq?1,...,wq?n+1)]
1??i
(16)
where, ?iq =
1??iq jq
2 and ?i =
1??i jq
2 are the ge-
ometric mean weights for SELSA probabilities
for the current word wq and any word wi ? V
respectively.
4 Various Levels of Syntactic
Information
In this section we explain various levels of syn-
tactic information that can be incorporated
within SELSA framework. They are supertags,
phrase type and content/fuction word type.
These are in decreasing order of complexity and
provide finer to coarser levels of syntactic infor-
mation.
4.1 Supertags
Supertags are the elementary structures of Lex-
icalized Tree Adjoining Grammars (LTAGs)
(Bangalore and Joshi, 1999). They are com-
bined by the operations of substitution and ad-
junction to yield a parse for the sentence. Each
supertag is lexicalized i.e. associated with at
least one lexical item - the anchor. Further,
all the arguments of the anchor of a supertag
are localized within the same supertag which al-
lows the anchor to impose syntactic and seman-
tic (predicate-argument) constraints directly on
its arguments. As a result, a word is typically
associated with one supertag for each syntac-
tic configuration the word may appear in. Su-
pertags can be seen as providing a much more
refined set of classes than do part-of-speech tags
and hence we expect supertag-based language
models to be better than part-of-speech based
language models.
4.2 Phrase-type
Words in a sentence are not just strung to-
gether as a sequence of parts of speech, but
rather they are organized into phrases, group-
ing of words that are clumped as a unit. A
sentence normally rewrites as a subject noun
phrase (NP) and a verb phrase (VP) which are
the major types of phrases apart from proposi-
tional phrases, adjective phrases etc (Manning
and Schutze, 1999). Using the two major phrase
types and the rest considered as other type, we
constructed a model for SELSA. This model as-
signs each word three syntactic descriptions de-
pending on its frequency of occurrence in each
of three phrase types across a number of doc-
uments. This model captures the semantic be-
haviour of each word in each phrase type. Gen-
erally nouns accur in noun phrases and verbs
occur in verb phrases while prepositions occur
in the other type. So this framework brings in
the finer syntactic resolution in each word?s se-
mantic description as compared to LSA based
average description. This is particularly more
important for certain words occurring as both
noun and verb.
4.3 Content or Function Word Type
If a text corpus is analyzed by counting word
frequencies, it is observed that there are cer-
tain words which occur with very high frequen-
cies e.g. the, and, a, to etc. These words have
a very important grammatical behaviour, but
they do not convey much of the semantics. Thse
words are called function or stop words. Sim-
ilarly in a text corpus, there are certain words
with frequencies in moderate to low range e.g.
car, wheel, road etc. They each play an impor-
tant role in deciding the semantics associated
with the whole sentence or document. Thus
they are known as content words. Generally
a list of vocabulary consists of a few hundred
function words and a few tens of thousands of
content words. However, they span more or less
the same frequency space of a corpora. So it is
also essential to give them equal importance by
treating them separately in a language model-
ing framework as they both convey some sort
of orthogonal information - syntactic vs seman-
tic. LSA is better at predicting topic bearing
content words while parsing based models are
better for function words. Even n-gram mod-
els are quite better at modeling function words,
but they lack the large-span semantic that can
be achieved by LSA. On the other hand, SELSA
model is suitable for both types of words as it
captures semantics of a word in a syntactic con-
text.
We performed experiments with LSA and
SELSA with various levels of syntactic informa-
tion in both the situations - content words only
vs content and function words together. In the
former case, the function words are treated by
n-gram model only.
5 Experiments and Discussion
A statistical language model is evaluated by
how well it predicts some hitherto unseen text
- test data - generated by the source to be
modeled. A commonly used quality measure
for a given model M is related to the en-
tropy of the underlying source and is known
as perplexity(PPL). Given a word sequence
w1, w2, . . . , wN to be used as a test corpus, the
perplexity of a language model M is given by:
PPL =exp
?
??
1
N
N?
q=1
logP (M)(wq|Wq?1)
?
? (17)
Perplexity also indicates the (geometric) aver-
age branching factor of the language according
to the modelM and thus indicates the difficulty
of a speech recognition task(Jelinek, 1999). The
lower the perplexity, the better the model; usu-
ally a reduction in perplexity translates into a
reduction in word error rate of a speech recog-
nition system.
We have implemented both the LSA and
SELSA models using the BLLIP corpus1 which
consists of machine-parsed English new stories
from the Wall Street Journal (WSJ) for the
years 1987, 1988 and 1989. We used the su-
pertagger (Bangalore and Joshi, 1999) to su-
pertag each word in the corpus. This had a tag-
ging acuracy of 92.2%. The training corpus con-
sisted of about 40 million words from the WSJ
1987, 1988 and some portion of 1989. This con-
sists of about 87000 documents related to news
stories. The test corpus was a section of WSJ
1989 with around 300, 000 words. The baseline
tri-gram model had a perplexity of 103.12 and
bi-gram had 161.06. The vocabulary size for
words was 20106 and for supertags was 449.
5.1 Perplexity Results
In the first experiment, we performed SELSA
using supertag information for each word. The
word-supertag vocabulary was about 60000.
This resulted in a matrix of about 60000X87000
for which we performed SVD at various dimen-
sions. Similarly we trained LSA matrix and per-
formed its SVD. Then we used this knowledge to
calculate language model probability and then
1Available from the Linguistic Data Consor-
tium(LDC) www.ldc.upenn.edu
integrated with tri-gram probability using geo-
metric interpolation method (Coccaro and Ju-
rafsky, 1998). In the process, we had assumed
the knowledge of the content/function word
type for the next word being predicted. Fur-
thermore, in this experiment, we had used only
content words for LSA as well as SELSA repre-
sentation, while the function words were treated
by tri-gram model only. We also used the su-
pertagged test corpus, thus we knew the su-
pertag of the next word being predicted. These
results thus sets benchmarks for content word
based SELSA model. With these assumptions,
we obtained the perplexity values as shown in
Table 1.
SVD dimensions LSA+ SELSA+
R tri-gram tri-gram
tri-gram only 103.12 103.12
0 (uniform prob) 78.92 60.83
2 78.05 60.88
10 74.92 57.88
20 72.91 56.15
50 69.85 52.80
125 68.42 50.39
200 67.79 49.50
300 67.34 48.84
Table 1: Perplexity at different SVD dimensions
with content/function word type knowledge as-
sumed. For SELSA, these are benchmarks with
correct supertag knowledge.
These benchmark results show that given the
knowledge of the content or function word as
well as the supertag of the word being predicted,
SELSA model performs far better than the LSA
model. This improvement in the performance is
attributed to the finer level of syntactic infor-
mation available now in the form of supertag.
Thus given the supertag, the choice of the word
becomes very limited and thus perplexity de-
creases. The decrease in perplexity across the
SVD dimension shows that the SVD also plays
an important role and thus for SELSA it is tru-
ely a latent syntactic-semantic analysis. Thus if
we devise an algorithm to predict the supertag
of the next word with a very high accuracy, then
there is a gurantee of performance improvement
by this model compared to LSA.
Our next experiment, was based on no knowl-
edge of content or function word type of the
next word. Thus the LSA and SELSA matrices
had all the words in the vocabulary. We also
kept the SVD dimensions for both SELSA and
LSA to 125. The results are shown in Table
2. In this case, we observe that LSA achieves
the perplexity of 88.20 compared to the base-
line tri-gram 103.12. However this is more than
LSA perplexity of 68.42 when the knowledge of
content/function words was assumed. This rel-
ative increase is mainly due to poor modeling
of function words in the LSA-space. However
for SELSA, we can observe that its perplexity
of 36.37 is less than 50.39 value in the case of
knowledge about content/function words. This
is again attributed to better modeling of syntac-
tically regular function words in SELSA. This
can be better understood from the observation
that there were 305 function words compared
to 19801 content words in the vocabulary span-
ning 19.8 and 20.3 million words respectively in
the training corpus. Apart from this, there were
152, 145 and 147 supertags anchoring function
word only, content word only and both types
of words respectively. Thus given a supertag
belonging to function word specific supertags,
the ?vocabulary? for the target word is reduced
by orders of magnitude compared to the case for
content word specific supertags. It is also worth
observing that the 125-dimensional SVD case of
SELSA is better than the 0-dimensional SVD
or uniform SELSA case. Thus the SVD plays
a role in deciphering the syntactic-semantically
important dimensions of the information space.
Model Perplexity
tri-gram only 103.12
LSA(125)+tri-gram 88.20
SELSA(125)+tri-gram 36.37
uniform-SELSA+tri-gram 41.79
Table 2: Perplexity without content/function
word knowledge. For SELSA, these are bench-
marks with correct supertag knowledge.
We also performed experiments using the
phrase-type (NP, VP, others) knowledge and
incorporated them within SELSA framework.
The resultant model was also used to calcu-
late perplexity values and the results on con-
tent/function type assumption set compares
favourably with LSA by improving the perfor-
mance. In another experiment we used the part-
of-speech tag of the previous word (prevtag)
within SELSA, but it couldn?t improve against
the plain LSA. These results shows that phrase
level information is somewhat useful if it can be
predicted correctly, but previous POS tags are
not useful.
Model Perplexity
tri-gram only 103.12
LSA(125)+tri-gram 68.42
phrase-SELSA(125)+tri-gram 64.78
prevtag-SELSA(125)+tri-gram 69.12
Table 3: Perplexity of phrase/prevtag based
SELSA with the knowledge of content/function
word type and the correct phrase/prevtag
Finally the utility of this language model can
be tested in a speech recognition experiment.
Here it can be most suitably applied in a second-
pass rescoring framework where the output of
first-pass could be the N-best list of either joint
word-tag sequences (Wang and Harper, 2002) or
word sequences which are then passed through
a syntax tagger. Both these approaches allow a
direct application of the results shown in above
experiments, however there is a possibility of
error propagation if some word is incorrectly
tagged. The other approach is to predict the
tag left-to-right from the word-tag partial prefix
followed by word prediction and then repeating
the procedure for the next word.
6 Conclusions and Research
Direction
We presented the effect of incorporating vari-
ous levels of syntactic information in a statisti-
cal language model that uses the mathematical
framework called syntactically enhanced LSA.
SELSA is an attempt to develop a unified frame-
work where syntactic and semantic dependen-
cies can be jointly represented. It general-
izes the LSA framework by incorporating var-
ious levels of the syntactic information along
with the current word. This provides a mech-
anism for statistical language modeling where
the probability of a word given the semantics
of the preceding words is constrained by the
adjacent syntax. The results on WSJ corpus
sets a set of benchmarks for the performance
improvements possible with these types of syn-
tactic information. The supertag based infor-
mation is very fine-grained and thus leads to a
large reduction in perplexity if correct supertag
is known. It is also observed that the knowledge
of the phrase type also helps to reduce the per-
plexity compared to LSA. Even the knowledge
of the content/function word type helps addi-
tionally in each of the SELSA based language
models. These benchmarks can be approached
with better algorithms for predicting the nec-
essary syntactic information. Our experiments
are still continuing in this direction as well as
toward better understanding of the overall sta-
tistical language modeling problem with appli-
cations to speech recognition.
References
S. Bangalore and A. K. Joshi. 1999. Supertag-
ging:an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
S. Bangalore. 1996. ?almost parsing? technique
for language modeling. In Proc. Int. Conf.
Spoken Language Processing, Philadeplphia,
PA, USA.
J. R. Bellegarda. 2000. Exploiting la-
tent semantic information in statistical lan-
guage modeling. Proceedings of the IEEE,
88(8):1279?1296.
E. Charniak. 2001. Immediate-head parsing for
language models. In Proc. 39th Annual Meet-
ing of the Association for Computational Lin-
guistics.
C. Chelba and F. Jelinek. 1998. Exploiting syn-
tactic structure for language modeling. In
Proc. COLING-ACL, volume 1, Montreal,
Canada.
N. Coccaro and D. Jurafsky. 1998. To-
wards better integration of semantic predic-
tors in statistical language modeling. In Proc.
ICSLP-98, volume 6, pages 2403?2406, Syd-
ney.
L. Galescu and E. R. Ringger. 1999. Aug-
menting words with linguistic information for
n-gram language models. In Proc. 6th Eu-
roSpeech, Budapest, Hungary.
J. T. Goodman. 2001. A bit of progress in lan-
guage modeling. Microsoft Technical Report
MSR-TR-2001-72.
F. Jelinek. 1999. Statistical methods for speech
recognition. The MIT Press.
T. K. Landauer, P. W. Foltz, and D. Laham.
1998. Introduction to latent semantic analy-
sis. Discourse Processes, 25:259?284.
C. Manning and H. Schutze. 1999. Foundations
of statistical natural language processing. The
MIT Press.
W. Wang and M. P. Harper. 2002. The super-
ARV language model: Investigating the effec-
tiveness of tightly integrating multiple knowl-
edge sources. In Proc. Conf. Empirical Meth-
ods in Natural Language Processing, pages
238?247, Philadelphia.
Automatic Evaluation of Students? Answers using
Syntactically Enhanced LSA
Dharmendra Kanejiya? and Arun Kumar? and Surendra Prasad?
?Department of Electrical Engineering
?Centre for Applied Research in Electronics
Indian Institute of Technology
New Delhi 110016 INDIA
kanejiya@hotmail.com arunkm@care.iitd.ernet.in sprasad@ee.iitd.ernet.in
Abstract
Latent semantic analysis (LSA) has been used
in several intelligent tutoring systems(ITS?s)
for assessing students? learning by evaluat-
ing their answers to questions in the tutoring
domain. It is based on word-document co-
occurrence statistics in the training corpus and
a dimensionality reduction technique. How-
ever, it doesn?t consider the word-order or
syntactic information, which can improve the
knowledge representation and therefore lead to
better performance of an ITS. We present here
an approach called Syntactically Enhanced
LSA (SELSA) which generalizes LSA by con-
sidering a word along with its syntactic neigh-
borhood given by the part-of-speech tag of its
preceding word, as a unit of knowledge repre-
sentation. The experimental results on Auto-
Tutor task to evaluate students? answers to ba-
sic computer science questions by SELSA and
its comparison with LSA are presented in terms
of several cognitive measures. SELSA is able
to correctly evaluate a few more answers than
LSA but is having less correlation with human
evaluators than LSA has. It also provides bet-
ter discrimination of syntactic-semantic knowl-
edge representation than LSA.
1 Introduction
Computer based education systems are useful in dis-
tance learning as well as for class-room learning envi-
ronment. These systems are based on intelligent tutor-
ing systems(ITS?s) which provide an interactive learning
environment to students. These systems first familiarize
a student with a topic and then ask questions to assess
her knowledge. Automatic evaluation of students? an-
swers is thus central to design of an ITS that can func-
tion without the need of continuous monitoring by a hu-
man. Examples of ITS?s that use natural language pro-
cessing to understand students? contribution are CIRC-
SIM (Glass, 2001), Atlas (Freedman et al, 2000), PACT
(Aleven et al, 2001) etc. These systems use a parser to
derive various levels of syntactic and semantic informa-
tion and rules to determine the next dialog move. They
perform quite well with short answers in a limited do-
main, but are limited to take arbitrarily long free-text in-
put and are difficult to port across domains. These limi-
tations can be alleviated by using latent semantic analy-
sis(LSA), a recently developed technique for information
retrieval (Deerwester et al, 1990), knowledge represen-
tation (Landauer et al, 1998), natural language under-
standing and cognitive modeling (Graesser et al, 1999;
Graesser et al, 2000) etc. LSA has been used in vari-
ous ITS?s like AutoTutor (Wiemer-Hastings et al, 1998),
Intelligent Essay Assessor (Foltz et al, 1999), Summary
Street (Kintsch et al, 2000), Apex (Dessus et al, 2000)
etc.
LSA is a statistical corpus-based natural language un-
derstanding technique that supports semantic similarity
measurement between texts. Given a set of documents
in the tutoring domain, LSA uses the frequency of oc-
currence of each word in each document to construct a
word-document co-occurrence matrix. After preprocess-
ing, singular value decomposition is performed to repre-
sent the domain knowledge into a 200 to 400 dimensional
space. This space is then used for evaluating the semantic
similarity between any two text units.
In an ITS, LSA is used to evaluate students? answers
with respect to the ideal answers to questions in the do-
main (Graesser et al, 2000). This is done by finding the
match between a student?s answer and the ideal answer by
calculating the cosine similarity measure between their
projections in LSA space. This information is used to
provide interactive response to the student in terms of
hint, prompt,question etc.
It has been found that LSA performs as good as an
intermediate expert human evaluator but not so well as an
accomplished expert of the domain. This may be because
LSA is a ?bag-of-words? approach and so lacks the word-
order or syntactic information in a text document. But
for correct automatic evaluation of students? answers, a
model should consider both syntax and semantics in the
answer. So, one obvious way to improve the performance
of LSA is to incorporate some syntactic information in it.
In order to add syntactic information to LSA, recently
there has been an effort in (Wiemer-Hastings and Zipitria,
2001), where a word along with its part-of-speech (POS)
tag was used to construct the LSA matrix, thus capturing
multiple syntactic senses of a word. But this approach,
called tagged LSA, deteriorated the performance. In an-
other attempt (Wiemer-Hastings and Zipitria, 2001), sim-
ilarity between two sentences was calculated by averag-
ing the LSA based similarity of sub-sentence structures
like noun phrase, verb phrase, object phrase etc. This
approach, called as structured LSA (SLSA), could im-
prove the performance in terms of sentence-pair similar-
ity judgment. But its performance in terms of evaluating
students? answers was poorer than that of LSA(Wiemer-
Hastings, 2000).
We propose here a model called Syntactically En-
hanced LSA (SELSA), where we augment each word with
the part-of-speech (POS) tag of the preceding word. Thus
instead of word-document co-occurence matrix, we gen-
erate a matrix in which rows correspond to all possible
word - POS tag combinations and columns correspond to
documents. A preceding tag indicates some kind of syn-
tactic neighbourhood around the focus word. Depending
on the preceding tag, the syntactic-semantic sense of a
word can vary. Thus SELSA captures finer resolution of
syntactic-semantic information compared to mere seman-
tics of LSA. This finer information can therefore be used
to evaluate a student?s answer more accurately than LSA.
We compare the performance of SELSA with LSA
for the AutoTutor cognitive modeling task (Graesser et
al., 1999). This involves evaluating students? answers to
questions in three areas of computer science viz. hard-
ware, operating system and networking. The perfor-
mance is measured in terms of various criteria like cor-
relation, mean absolute difference and number of correct
/emphvs false evaluations by humans and by computer.
SELSA is found better than LSA in terms of robustness
across thresholds as well as in terms of evaluating more
answers correctly, but it is having less correlation mea-
sure with human than LSA.
The organization of this paper is as follows. The next
section describes LSA and its applications in ITS?s. In
section 3, we describe the proposed SELSA model. The
experimental details are given in section 4 followed by
discussion on results in section 5.
2 LSA in Intelligent Tutoring Systems
2.1 A Brief Introduction to LSA
LSA is a statistical-algebraic technique for extracting and
inferring contextual usage of words in documents (Lan-
dauer et al, 1998). A document can be a sentence, a para-
graph or even a larger unit of text. It consists of first con-
structing a word-document co-occurrence matrix, scaling
and normalizing it with a view to discriminate the impor-
tance of words across documents and then approximating
it using singular value decomposition(SVD) in R dimen-
sions (Bellegarda, 2000). It is this dimensionality reduc-
tion step through SVD that captures mutual implications
of words and documents and allows us to project any text
unit whether a word, a sentence or a paragraph as a vector
on the latent ?semantic? space. Then any two documents
can be compared by calculating the cosine measure be-
tween their projection vectors in this space.
LSA has been applied to model various ITS related
phenomena in cognitive science e.g. judgment of es-
say quality scores (Landauer et al, 1998), assessing stu-
dent knowledge by evaluating their answers to questions
etc (Graesser et al, 2000), deciding tutoring strategy
(Lemaire, 1999). It has been also used to derive a sta-
tistical language model for large vocabulary continuous
speech recognition task (Bellegarda, 2000).
2.2 LSA based ITS?s
Researchers have long been attempting to develop a com-
puter tutor that can interact naturally with students to
help them understand a particular subject. Unfortunately,
however, language and discourse have constituted a seri-
ous barrier in these efforts. But recent technological ad-
vances in the areas of latent semantic processing of natu-
ral language, world knowledge representation, multime-
dia interfaces etc have made it possible for various teams
of researchers to develop ITS?s that approach human per-
formance. Some of these are briefly reviewed below.
2.2.1 AutoTutor
AutoTutor task (Graesser et al, 1999) was developed
at Tutoring Research Group of University of Memphis.
AutoTutor is a fully automated computer tutor that as-
sists students in learning about hardware, operating sys-
tems and the Internet in an introductory computer literacy
course. AutoTutor presents questions and problems from
a curriculum script, attempts to comprehend learner con-
tributions that are entered by keyboard, formulates dia-
log moves that are sensitive to the learner?s contributions
(such as prompts, elaborations, corrections and hints),
and delivers the dialog moves with a talking head. LSA is
a major component of the mechanism that evaluates the
quality of student contributions in the tutorial dialog. It
was found that the performance of LSA in terms of evalu-
ating answers from college students was equivalent to an
intermediate expert human evaluator.
2.2.2 Intelligent Essay Assessor
Intelligent essay assessor (Foltz et al, 1999) uses LSA
for automatic scoring of short essays that would be used
in any kind of content-based courses. Student essays are
characterized by LSA representations of the meaning of
their contained words and compared with pre-graded es-
says on degree of conceptual relevance and amount of
relevant content by means of two kinds of scores: (1) the
holistic score, the score of the closest pre-graded essay
and (2) the gold standard, the LSA proximity between
the student essay and a standard essay.
2.2.3 Summary Street
Summary Street (Kintsch et al, 2000) is also built on
top of LSA. It helps students to write good summaries.
First of all, a student is provided with a general advice on
how to write a summary, then the student selects a topic,
reads the text and writes out a summary. LSA procedures
are then applied to give a holistic grade to the summary.
2.2.4 Apex
Apex (Dessus et al, 2000) is a web-based learning en-
vironment which manages student productions, assess-
ments and courses. Once connected to the system, a stu-
dent selects a topic or a question that he or she wishes to
work on. The student then types a text about this topic
into a text editor. At any time, she can get a three-part
evaluation of the essay based on content, outline and co-
herence. At the content level, the system identifies how
well the notions are covered by requesting LSA to mea-
sure a semantic similarity between the student text and
each notion of the selected topic and correspondingly
provides a message to the student.
3 Syntactically Enhanced LSA (SELSA)
LSA is based on word-document co-occurrence, also
called a ?bag-of-words? approach. It is therefore blind
to word-order or syntactic information. This puts limita-
tions on LSA?s ability to capture the meaning of a sen-
tence which depends upon both syntax and semantics.
The syntactic information in a text can be characterized in
various ways like a full parse tree, a shallow parse, POS
tag sequence etc. In an effort to generalize the LSA, we
present here a concept of word-tag-document structure,
which captures the behavior of a word within each syn-
tactic context across various semantic contexts. The idea
behind this is that the syntactic-semantic sense of a word
is specified by the syntactic neighborhood in which it oc-
curs. So representation of each such variation in an LSA-
like space gives us a finer resolution in a word?s behavior
compared to an average behavior captured by LSA. This
then allows to compare two text documents based on their
syntactic-semantic regularity and not based on semantics-
only. So it can be used in high quality text evaluation
applications.
This approach is quite similar to the tagged LSA
(Wiemer-Hastings and Zipitria, 2001) which considered a
word along with its POS tag to discriminate multiple syn-
tactic senses of a word. But our approach is an extension
of this work towards a more general framework where
a word along with the syntactic context specified by its
adjacent words is considered as a unit of knowledge rep-
resentation. We define the syntactic context as the POS
tag information around a focus word. In particular, we
look at the POS tag of the preceding word also called
prevtag for convenience. The motivation for this comes
from statistical language modeling and left-to-right pars-
ing literature where a word is predicted or tagged using its
preceding words and their POS tags. Moreover, prevtag
is used as an approximation to the notion of a preceding
parse tree characterizing the word sequence before the
focus word. But in general, we can also use the syntactic
information from the words following the current word,
e.g. posttag, the POS tag of the next word. However, one
of the concerns while incorporating syntactic information
in LSA is that of sparse data estimation problem. So it is
very important to choose a robust characterization of syn-
tactic neighbourhood as well as apply smoothing either at
the matrix formation level or at the time of projecting a
document in the latent space.
The approach consists of first identifying a sufficiently
large corpus representing the domain of tutoring. Then a
POS tagger is used to convert it to a POS tagged corpus.
The next step is to construct a matrix whose rows corre-
spond to word-prevtag pairs and columns correspond to
documents in the corpus. Again, a document can be a
sentence, a paragraph or a larger unit of text. If the vo-
cabulary size is I , POS tag vocabulary size is J and num-
ber of documents in corpus is K, then the matrix will
be IJ ? K. Let ci j,k denote the frequency of word wi
with prevtag pj in the document dk. The notation i j (i
underscore j) in subscript is used for convenience and in-
dicates word wi with prevtag pj i.e., (i ? 1)J + jth row
of the matrix. Then as in LSA (Bellegarda, 2000), we
find entropy ?i j of each word-prevtag pair and scale the
corresponding row of the matrix by (1? ?i j). The doc-
ument length normalization to each column of the matrix
is also applied by dividing the entries of kth document by
nk, the number of words in document dk. Let ti j be the
frequency of i jth word-prevtag pair in the whole corpus
i.e. ti j =
?K
k=1 ci j,k. Then ?i j and the matrix element
xi j,k are given as:
?i j = ?
1
logK
K?
k=1
ci j,k
ti j
log
ci j,k
ti j
(1)
xi j,k = (1? ?i j)
ci j,k
nk
(2)
Once the matrix X is obtained, we perform its singular
value decomposition (SVD) and approximate it by keep-
ing the largest R singular values and setting the rest to
zero. Thus,
X ? X? = USVT (3)
where, U(IJ ?R) and V(K ?R) are orthonormal ma-
trices and S(R?R) is a diagonal matrix. It is this dimen-
sionality reduction step through SVD that captures major
structural associations between words-prevtags and docu-
ments, removes ?noisy? observations and allows the same
dimensional representation of words-prevtags and docu-
ments (albeit, in different bases). This R-dimensional
space can be called either syntactically enhanced latent
semantic space or latent syntactic-semantic space.
After the knowledge is represented in the latent
syntactic-semantic space, we can project any new docu-
ment as aR dimensional vector d?L in this space. Let d be
the IJ ? 1 vector representing this document whose ele-
ments di j are the frequency counts i.e. number of times
word wi occurs with prevtag pj , weighted by its corre-
sponding entropy measure (1 ? ?i j). It can be thought
of as an additional column in the matrix X, and therefore
can be thought of as having its corresponding vector v in
the matrix V. Then, d = USvT and
d?L = SvT = UTd (4)
which is a R?1 dimensional vector representation of the
document in the latent space.
We can also define a syntactic-semantic similarity
measure between any two text documents as the cosine
of the angle between their projection vectors in the latent
syntactic-semantic space. With this measure we can ad-
dress the problems that LSA has been applied to, namely
natural language understanding, cognitive modeling, sta-
tistical language modeling etc.
4 Experiment - Evaluating Students?
Answers
We have studied the performance of SELSA and com-
pared it with LSA in the AutoTutor task (section 2.2.1)
for natural language understanding and cognitive model-
ing performance. The details of the experiment are pre-
sented below.
4.1 Corpus
The tutoring research group at the University of Memphis
has developed the training as well as testing corpus for
the AutoTutor task. The training corpus consisted of two
complete computer literacy textbooks, and ten articles on
each of the tutoring topics viz. hardware, operating sys-
tem and the Internet. The test corpus was formed in the
following manner : eight questions from each of the three
topics were asked to a number of students. Then eight
answers per question, 192 in total, were selected as test
database. There were also around 20 good answers per
question which were used in training and testing. Using
this corpus, we have implemented LSA and SELSA.
4.2 Human Evaluation of Answers
For comparing the performance of SELSA and LSA with
humans, we selected four human evaluators from com-
puter related areas. Three of them were doctorate candi-
dates and one had completed it, thus they were expert hu-
man evaluators. Each of them were given the 192 student-
answers and a set of good answers to each of the question.
They were asked to evaluate the answers on the basis of
compatibility score i.e. the fraction of the number of sen-
tences in a student-answer that matches any of the good
answers. Thus, the score for each answer ranged between
0 to 1. They were not told what constitutes a ?match?, but
were to decide themselves.
4.3 Syntactic Information
We approximated the syntactic neighborhood by the POS
tag of preceding word. POS tagging was performed
by the LTPOS software from the Language Technology
Group of University of Edinburgh1. We also mapped the
45 tags from Penn tree-bank tagset to 12 tags so as to con-
sider major syntactic categories and also to keep the size
of resulting matrix manageable.
4.4 LSA and SELSA Training
We considered a paragraph as a unit of document. Af-
ter removing very small documents consisting less than
four words, we had 5596 documents. The vocabulary
size, after removing words with frequency less than two
and some stopwords, was 9194. The density of LSA and
SELSA matrices were 0.27% and 0.025% respectively.
SVD was performed using the MATLAB sparse matrix
toolbox. We performed SVD with dimensions R varying
from 200 to 400 in steps of 50.
4.5 Evaluation Measure
In order to evaluate the performance of SELSA and LSA
on AutoTutor task, we need to define an appropriate mea-
sure. The earlier studies on this task used a correlation
coefficient measure between the LSA?s rating and human
rating of the 192 answers. We have also used this as one
of the three measures for comparison. But for a task hav-
ing small sample size, the correlation coefficient is not
reliably estimated, so we defined two new performance
1http://www.ltg.ed.ac.uk
measures. The first one was the mean absolute difference
between the human and SELSA (correspondingly LSA)
evaluations. In the other measure we used the compari-
son of how many answers were correctly evaluated versus
how many were falsely evaluated by SELSA (LSA) as
compared to human evaluations. A detailed explanation
of these measures is given in the following section.
5 Results and Discussions
We calculated the compatibility score evaluation using
SELSA (LSA) in an analogous way to the human evalua-
tion. Thus SELSA (LSA) would evaluate the answers in
the following manner. It would first break each student-
answer into a number of sentences and then evaluate each
sentence against the good answers for that question. If
the cosine measure between the SELSA (LSA) represen-
tation of the sentence and any good answer exceeded a
predefined threshold then that part was considered cor-
rect. Thus it would find the fraction of the number of
sentences in a student-answer that exceeded the thresh-
old. We performed the experiments by varying threshold
between 0.05 to 0.95 with a step of 0.05. We also varied
the number of singular values R from 200 to 400 with a
step of 50. In the following, we present our results using
the three evaluation measures.
5.1 Correlation Analysis
For each of the five SVD dimensions R and each value
of the thresholds, we calculated the correlation coeffi-
cient between the SELSA (LSA) evaluation and each hu-
man rater?s evaluation. Then we averaged this across the
four human evaluators. The resulting average correlation
curves for SELSA and LSA are shown in figs. (1) and (2)
respectively.
From these two figures we observe that maximum cor-
relation between SELSA and human raters is 0.47 and
that between LSA and human is 0.51 while the average
inter-human correlation was 0.59. Thus LSA seems to be
closer to human than SELSA in this particular tutoring
task. This seems to support the arguments from (Lan-
dauer et al, 1997) that syntax plays little role, if any, in
semantic similarity judgments and text comprehension.
But the likely reason behind this could be that the corpus,
particularly the student answers, contained very poor syn-
tactic structure and also that human evaluators might not
have paid attention to grammatical inaccuracies in this
technical domain of computer literacy.
But it is also worth noting that SELSA is closer to
LSA than a previous approach of adding syntactic in-
formation to LSA (Wiemer-Hastings, 2000), which had
a correlation of 0.40 compared to 0.49 of LSA on the
same task of evaluating students? answers, where aver-
age inter-human correlation was 0.78 between the expert
raters and 0.51 between the intermediate experts. SELSA
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5 Correlation between evaluations by Human and SELSA
Threshold
Corr
elatio
n
R 200R 250R 300R 350R 400
Figure 1: Correlation between SELSA and human evalu-
ators
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55 Correlation between evaluations by Human and LSA
Threshold
Corr
elatio
n
R 200R 250R 300R 350R 400
Figure 2: Correlation between LSA and human evalua-
tors
is also comparable to tagged LSA (Wiemer-Hastings and
Zipitria, 2001), which used the current POS tag instead
of prevtag. It had a correlation of 0.27 compared to 0.36
of LSA in a modified evaluation task of judging similar-
ity between two sentences where the correlation between
skilled raters was 0.45 and that between non-proficient
raters was 0.35.
If we look at these curves more carefully, especially,
their behavior across thresholds, then it is interesting to
note that SELSA has wider threshold-widths(TW) than
LSA across all the cases of SVD dimension R. In ta-
ble (1) and (2) we have shown the 10% and 20% TW
of SELSA and LSA respectively. This is calculated by
finding the range over thresholds for which the correla-
tion is within 10% and 20% of the maximum correlation.
This observation shows that SELSA is much more robust
across thresholds than LSA in the sense that semantic in-
formation is discriminated better in SELSA space than in
LSA space.
R Cormax Tmax 10% TW 20% TW
200 0.46 0.45 0.48 0.63
250 0.47 0.40 0.42 0.61
300 0.47 0.40 0.41 0.62
350 0.45 0.50 0.55 0.65
400 0.46 0.50 0.45 0.64
Table 1: Threshold Width of SELSA
R Cormax Tmax 10% TW 20% TW
200 0.49 0.65 0.33 0.44
250 0.51 0.65 0.29 0.44
300 0.51 0.60 0.26 0.41
350 0.50 0.60 0.32 0.44
400 0.50 0.50 0.32 0.50
Table 2: Threshold Width of LSA
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
Corr
elatio
n
Threshold
Correlation for LSA and SELSA at R=250
LSA250SELSA250
Figure 3: LSA vs SELSA for SVD dimensions 250
Another interesting observation occurs when we plot
the two curves simultaneously as shown in fig. (3). Here
we plotted the SELSA and LSA performances for 250 di-
mensions of latent space. We can easily see that SELSA
performs better than LSA for thresholds less than 0.5 and
viceversa. This observation along with the previous ob-
servation about TW can be understood in the following
manner. When comparing two document vectors for a
cosine measure exceeding a threshold, we can consider
one of the vectors to be the axis of a right circular cone
with a semi-vertical angle decided by the threshold. If the
other vector falls within this cone, we say the two docu-
ments are matching. Now if the human raters emphasized
semantic similarity, which is most likely the case, then
this means that LSA could best capture the same infor-
mation in a narrower cone while SELSA required a wider
cone. This is quite intuitive in the sense that SELSA has
zoomed the document similarity measure axis by putting
finer resolution of syntactic information. Thus mere se-
mantically similar documents are placed wider apart in
SELSA space than syntactic-semantically similar docu-
ments. This concept can be best used in a language mod-
eling task where a word is to be predicted from the his-
tory. It is observed in (Kanejiya et al, 2003) that SELSA
assigns better probabilities to syntactic-semantically reg-
ular words than LSA, although the overall perplexity re-
duction over a bi-gram language model was less than that
by LSA.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65 Mean Absolute Difference between evaluations by Human and SELSA
Threshold
me
an 
abso
lute 
diffe
renc
e
R 200R 250R 300R 350R 400
Figure 4: Mean absolute difference between SELSA and
human evaluators
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65 Mean Absolute Difference between evaluations by Human and LSA
Threshold
me
an 
abso
lute 
diffe
renc
e
R 200R 250R 300R 350R 400
Figure 5: Mean absolute difference between LSA and hu-
man evaluators
5.2 Mean Absolute Difference Analysis
Here we calculated the mean absolute difference(MAD)
between a human rater?s evaluation and SELSA (LSA)
evaluations as follow:
MAD =
1
192
192?
i=1
|hi ? li| (5)
where, hi and li correspond to human and SELSA(LSA)
evaluation of ith answer. This was then averaged across
human evaluators. These results are plotted in figs. (4)
and (5). These two curves show that SELSA and LSA are
almost equal to each other. Again SELSA has the advan-
tage of more robustness and in most cases it is even better
than LSA in terms of minimum MAD with human. Ta-
bles (3) and (4) show values of minimum MAD at various
values of SVD dimensions R. The best minimum MAD
for SELSA is 0.2412 at 250 dimensional space while that
for LSA is 0.2475 at 400 dimensions. The average MAD
among human evaluators is 0.2050.
R minMAD maxCorrect minFalse
200 0.2449 125 31
250 0.2412 125 30
300 0.2422 126 30
350 0.2484 125 31
400 0.2504 124 32
Table 3: SELSA - MAD, correct and false evaluation
R minMAD maxCorrect minFalse
200 0.2497 122 29
250 0.2523 120 31
300 0.2555 121 32
350 0.2525 122 32
400 0.2475 123 30
Table 4: LSA - MAD, correct and false evaluation
5.3 Correct vs False Evaluations Analysis
We define an evaluation li by SELSA (LSA) to be correct
or false as below:
li CORRECT if |li ? hi| < CT
li FALSE if |li ? hi| > FT
where CT and FT are correctness and falsehood thresh-
olds which were set to 0.05 and 0.95 respectively for strict
measures. Number of such correct as well as false eval-
uations were then averaged across the four human evalu-
ators. They are plotted in figs. (6) and (7) for SELSA
and LSA respectively (the upper curves corresponding
to correct and the lower ones to false evaluations). The
maximum number of correct (maxCorrect) and the min-
imum number of false (minFalse) evaluations across the
thresholds for each value of SVD dimensions are calcu-
lated and shown in tables (3) and (4). We observe that the
best performance for SELSA is achieved at 300 dimen-
sions with 126 correct and 30 false evaluations, while for
LSA it is at 400 dimensions with 123 correct and 30 false
evaluations. The average correct and false evaluations
among all human-human evaluator pairs were 132 and 23
respectively. Thus here also SELSA is closer to human
evaluators than LSA. In fact, for the cognitive task like
AutoTutor, this is a more appealing and explicit measure
than the previous two. Apart from these three measures,
one can also calculate precision, recall and F-measure
(Burstein et al, 2003) to evaluate the performance.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 130
40
50
60
70
80
90
100
110
120
130 Correct and False evaluations using SELSA
Threshold
Num
ber o
f Cor
rect 
and 
False
 Eva
luatio
ns
R 200R 250R 300R 350R 400
Figure 6: Correct and false evaluations by SELSA as
compared to human evaluators
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
30
40
50
60
70
80
90
100
110
120
130 Correct and False evaluations using LSA
Threshold
Num
ber o
f Cor
rect 
and 
False
 Eva
luatio
ns
R 200R 250R 300R 350R 400
Figure 7: Correct and False evaluations by LSA as com-
pared to human evaluators
6 Conclusion
Automatic evaluation of students? answers in an intelli-
gent tutoring system can be performed using LSA. But
LSA lacks syntactic information which can be also use-
ful for meaning representation of a text document. So, we
have developed and implemented a model called syntacti-
cally enhanced LSA which generalizes LSA by augment-
ing a word with the POS tag of the preceding word to de-
rive a latent syntactic-semantic information. Experimen-
tal results on the AutoTutor task of evaluating students?
answers to computer science questions show a range of
performance comparison between SELSA and LSA. In
terms of the correlation measure with human raters, LSA
is slightly better than SELSA. But SELSA is at least as
good as LSA in terms of the mean absolute difference
measure. On the other end, SELSA is able to correctly
evaluate a few more answers than LSA is. SELSA can
do better if the training and testing corpora have a good
syntactic structure.
From the correlation performance analysis, it is ob-
served that SELSA is more robust in discriminating the
semantic information across a wider threshold width than
LSA. It is also found that SELSA uses the syntactic infor-
mation to expand the document similarity measure i.e.,
mere semantically similar documents are placed wider
apart than syntactic-semantically similar documents in
SELSA space.
These initial results are part of an ongoing research to-
wards an overall improvement of natural language under-
standing and modeling. Although the present version of
SELSA has limited improvements over LSA, it leads to
future experiments with robust characterization of syn-
tactic neighbourhood in terms of headwords or phrase
structure as well as applying smoothing across syntax to
tackle the problem of sparse data estimation.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001. A
tutorial dialogue system with knowledge-based under-
standing and classification of student explanations. In
Working notes of the 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Seattle.
J. R. Bellegarda. 2000. Exploiting latent semantic infor-
mation in statistical language modeling. Proceedings
of the IEEE, 88(8):1279?1296.
J. Burstein, C. Leacock, and M. Chodorow. 2003. Crite-
rion: Online essay evaluation: An application for au-
tomated evaluation of student essays. In Proc. of the
Fifteenth Annual Conf. on Innovative Applications of
Artificial Intelligence, Acapulco, Mexico. (in press).
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshmann. 1990. Indexing by latent
semantic analysis. Journal of the American Society for
Information Science, 41(6):391?407.
P. Dessus, B. Lemaire, and A. Vernier. 2000. Free-text
assessment in a virtual campus. In Proc. 3rd Int. Conf.
on Human Systems Learning (CAPS?2000), Paris.
P. W. Foltz, D. Laham, and T. K. Landauer. 1999. Auto-
mated essay scoring: applications to educational tech-
nology. In Proc. of the ED-MEDIA?99 conference,
Charlottesville. AACE.
R. Freedman, C. P. Ros, M. A. Ringenberg, and K. Van-
Lehn. 2000. ITS tools for natural language dialogue:
A domain-independent parser and planner. In Fifth In-
ternational Conference on Intelligent Tutoring Systems
(ITS 2000), Montreal. Springer-Verlag.
M. Glass. 2001. Processing language input in the
CIRCSIM-tutor intelligent tutoring system. In J. D.
Moore, C. L. Redfield, and W. L. Johnson, editors, Ar-
tificial Intelligence in Education, pages 210?221. IOS
Press, San Antonio.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, R. Kreuz, and Tutoring Research Group.
1999. Autotutor: A simulation of a human tutor. Jour-
nal of Cognitive Systems Research, 1:35?51.
A. C. Graesser, P. Wiemer-Hastings, K. Wiemer-
Hastings, D. Harter, Tutoring Research Group, and
N. Person. 2000. Using latent semantic analysis to
evaluate the contributions of students in autotutor. In-
teractive Learning Environments, 8(2):129?147.
D. P. Kanejiya, A. Kumar, and S. Prasad. 2003. Statis-
tical language modeling using syntactically enhanced
LSA. In Proc. TIFR Workshop on Spoken Language
Processing, pages 93?100, Mumbai, India.
E. Kintsch, D. Steinhart, G. Stahl, and the LSA Re-
search Group. 2000. Developing summarization skills
through the use of lsa-based feedback. Interactive
Learning Environments, 8(2):87?109.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1997. How well can passage meaning be
derived without using word order? a comparison of
latent semantic analysis and humans. In Proc. 9th an-
nual meeting of the Cognitive Science Society, pages
412?417, Mawhwah, NJ. Erlbaum.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. In-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
B. Lemaire. 1999. Tutoring systems based on latent se-
mantic analysis. In S. Lajoie and M. Vivet, editors, Ar-
tificial Intelligence in Education, pages 527?537. IOS
Press, Amsterdam.
P. Wiemer-Hastings and I. Zipitria. 2001. Rules for syn-
tax, vectors for semantics. In Proc. 23rd Annual Conf.
of the Cognitive Science Society, Mawhwah, NJ. Erl-
baum.
P. Wiemer-Hastings, A. C. Graesser, D. Harter, and Tu-
toring Research Group. 1998. The foundation and ar-
chitecture of autotutor. In Proc. 4th Int. Conf. on Intel-
ligent Tutoring Systems, Berlin. Springer-Verlag.
P. Wiemer-Hastings. 2000. Adding syntactic informa-
tion to lsa. In Proc. 22nd Annual Conf. of the Cog-
nitive Science Society, pages 988?993, Mawhwah, NJ.
Erlbaum.
