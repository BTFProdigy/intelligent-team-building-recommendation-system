Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Statistical Phrase Alignment Model
Using Dependency Relation Probability
Toshiaki Nakazawa Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
nakazawa@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
When aligning very different language pairs,
the most important needs are the use of struc-
tural information and the capability of gen-
erating one-to-many or many-to-many corre-
spondences. In this paper, we propose a
novel phrase alignment method which models
word or phrase dependency relations in depen-
dency tree structures of source and target lan-
guages. The dependency relation model is a
kind of tree-based reordering model, and can
handle non-local reorderings which sequen-
tial word-based models often cannot handle
properly. The model is also capable of esti-
mating phrase correspondences automatically
without any heuristic rules. Experimental re-
sults of alignment show that our model could
achieve F-measure 1.7 points higher than the
conventional word alignment model with sym-
metrization algorithms.
1 Introduction
We consider that there are two important needs in
aligning parallel sentences written in very differ-
ent languages such as Japanese and English. One
is to adopt structural or dependency analysis into
the alignment process to overcome the difference in
word order. The other is that the method needs to
have the capability of generating phrase correspon-
dences, that is, one-to-many or many-to-many word
correspondences. Most existing alignment methods
simply consider a sentence as a sequence of words
(Brown et al, 1993), and generate phrase correspon-
dences using heuristic rules (Koehn et al, 2003).
Some studies incorporate structural information into
the alignment process after this simple word align-
ment (Quirk et al, 2005; Cowan et al, 2006). How-
ever, this is not sufficient because the basic word
alignment itself is not good.
On the other hand, a few models have been pro-
posed which use structural information from the be-
ginning of the alignment process. Watanabe et al
(2000) and Menezes and Richardson (2001) pro-
posed a structural alignment methods. These meth-
ods use heuristic rules when resolving correspon-
dence ambiguities. Yamada and Knight (2001) and
Gildea (2003) proposed a tree-based probabilistic
alignment methods. These methods reorder, insert
or delete sub-trees on one side to reproduce the other
side, but the constraints of using syntactic informa-
tion is often too rigid. Yamada and Knight flat-
tened the trees by collapsing nodes. Gildea cloned
sub-trees to deal with the problem. Cherry and Lin
(2003) proposed a model which uses a source side
dependency tree structure and constructs a discrim-
inative model. However, there is the defect that its
alignment unit is a word, so it can only find one-
to-one alignments. Nakazawa and Kurohashi (2008)
also proposed a model focusing on the dependency
relations. Their model has the constraint that content
words can only correspond to content words on the
other side, and the same applies for function words.
This sometimes leads to an incorrect alignment. We
have removed this constraint to make more flexi-
ble alignments possible. Moreover, in their model,
some function words are brought together, and thus
they cannot handle the situation where each func-
tion word corresponds to a different part. The small-
est unit of our model is a single word, which should
solve this problem.
10
In this paper, we propose a novel phrase align-
ment method which models word or phrase de-
pendency relations in dependency tree structures of
source and target languages. For a pair of correspon-
dences which has a parent-child relation on one side,
the dependency relation on the other side is defined
as the relation between the two correspondences.
It is a kind of tree-based reordering model, and
can capture non-local reorderings which sequential
word-based models often cannot handle properly.
The model is also capable of estimating phrase cor-
respondences automatically without heuristic rules.
The model is trained in two steps: Step 1 estimates
word translation probabilities, and Step 2 estimates
phrase translation probabilities and dependency re-
lation probabilities. Both Step 1 and Step 2 are per-
formed iteratively by the EM algorithm. During the
Step 2 iterations, word correspondences are grown
into phrase correspondences.
2 Proposed Model
We suppose that Japanese is the source language and
English is the target language in the description of
our model. Note that the model is not specialized
for this language pair, and it can be applied to any
language pair.
Because our model uses dependency tree struc-
tures, both source and target sentences are parsed
beforehand. Japanese sentences are converted into
dependency structures using the morphological ana-
lyzer JUMAN (Kurohashi et al, 1994), and the de-
pendency analyzer KNP (Kawahara and Kurohashi,
2006). MSTparser (McDonald et al, 2005) is used
to convert English sentences. Figure 1 shows an ex-
ample of dependency structures. The root of a tree is
placed at the extreme left and words are placed from
top to bottom.
2.1 Overview
This section outlines our proposed model in compar-
ison to the IBM models, which are the conventional
statistical alignment models.
In the IBM models (Brown et al, 1993), the best
alignment a? between a given source sentence f and
its target sentence e is acquired by the following
equation:
a? = argmax
a
p(f ,a|e)
= argmax
a
p(f |e,a) ? p(a|e) (1)
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
(accept)
(light)
(device)
(photo)
(gate)
(used)
(ni)
(ha)
(wo)
Figure 1: An example of a dependency tree and its align-
ment.
where p(f |e,a) is called lexicon probability and
p(a|e) is called alignment probability.
Suppose f consists of nwords f1, f2, ..., fn, and e
consists ofmwords e1, e2, ..., em and a NULL word
(e0). The alignment mapping a consists of associa-
tions j ? i = aj from source position j to target
position i = aj . The two probabilities above are
broken down as:
p(f |e,a) =
J?
j=1
p(fj |eaj ) (2)
p(a|e) =
I?
i=1
p(?j|ei) (3)
where ?j is a relative position of words in the
source side which corresponds to ei. Equation 2 is
the product of the word translation probabilities, and
Equation 3 is the product of relative position proba-
bilities.
In the proposed model, we refine the IBM models
in three ways. First, as for Equation 2, we consider
phrases instead of words. Second, as for Equation 3,
we consider dependencies of words instead of their
positions in a sentence.
Finally, the proposed model can find the best
alignment a? by not using f -to-e alone, but simulta-
neously with e-to-f . That is, Equation 1 is modified
as follows:
a? = argmax
a
p(f |e,a) ? p(a|e) ?
p(e|f ,a) ? p(a|f) (4)
Since our model regards a phrase as a basic unit,
the above formula is calculated in a straightforward
way. In contrast, the IBM models can consider
a many-to-one alignment by combining one-to-one
11
alignments, but they cannot consider a one-to-many
or many-to-many alignment.
The models are estimated by EM-like algorithm
which is very similar to (Liang et al, 2006). The
important difference is that we are using tree struc-
tures.
We maximize the data likelihood:
max
?ef ,?fe
?
f ,e
(log pef (f , e; ?ef ) + log pfe(f , e; ?fe))
(5)
In the E-step, we compute the posterior distribution
of the alignments with the current parameter ?:
q(a; f , e) := pef (a|f , e; ?ef ) ? pfe(a|f , e; ?fe) (6)
In the M-step, we update the parameter ?:
?? := argmax
?
?
a,f ,e
q(a; f , e) log pef (a, f , e; ?ef )
+?
a,f ,e
q(a; f , e) log pfe(a, f , e; ?fe)
= argmax
?
?
a,f ,e
q(a; f , e) log p(e) ? pef (a, f |e; ?ef )
+?
a,f ,e
q(a; f , e) log p(f) ? pfe(a, e|f ; ?fe)
(7)
Note that p(e) and p(f) have no effect on maxi-
mization, and pef (a, f |e; ?ef ) and pfe(a, e|f ; ?fe)
appeared in Equation 1 or Equation 4.
In the following sections, we decompose the lexi-
con probability and alignment probability.
2.2 Phrase Translation Probability
Suppose f consists of N phrases F1, F2, ..., FN , and
e consists of M phrases E1, E2, ..., EM . The align-
ment mapping a consists of associations j ? i =
Aj from source phrase j to target phrase i = Aj .
We consider phrase translation probability
p(Fj |Ei) instead of word translation probability.
There is one restriction: that phrases composed of
more than one word cannot be aligned to NULL.
Only a single word can be aligned to NULL.
We denote a phrase which the word fj belongs to
as Fs(j), and a phrase which the word ei belongs to
as Et(i). With these notations, we refine Equation 2
as follows:
p(f |e,a) =
J?
j=1
p(Fs(j)|EAs(j)) (8)
Suppose phrase Fj and Ei are aligned where the
number of words in Fj is denoted by |Fj | and that
number in Ei is |Ei|, the probability mass related to
this alignment in Equation 8 is as follows:
p(Fj |Ei)|Fj | ? p(Ei|Fj)|Ei| (9)
We call this probability for the link between Fj and
Ei phrase alignment probability. The upper part of
Table 1 shows phrase alignment probabilities for the
alignment in Figure 1.
2.3 Dependency Relation Probability
The reordering model in the IBM Models is defined
on the relative position between an alignment and
its previous alignment, as shown in Equation 3. Our
model, on the other hand, considers dependencies of
words instead of positional relations.
We start with a dependency relation where fc de-
pends on fp in the source sentence. In a possible
alignment, fc belongs to Fs(c), fp belongs to Fs(p),
and Fs(c) depends on Fs(p). In this situation, we con-
sider the relation between EAs(p) and EAs(c) . Even
if two languages have different word order, their de-
pendency structures are similar in many cases, and
EAs(c) tends to depend on EAs(p) . Our model takes
this tendency into consideration. In order to de-
note the relationship between phrases, we introduce
rel(EAs(p) , EAs(c)). This is defined as the path from
EAs(p) to EAs(c) . It is represented by applying the
notations below:
? ?c? if going down to the child node
? ?p? if going down to the parent node
For example, in Figure 1, the path from ?for? to
?photodetector? is ?c?, from ?the? to ?for? is ?p;p?
because it travels across two nodes. All the phrases
are considered as a single node, so the path from
?photogate? to ?the? is ?p;c;c;c? with the alignment
in Figure 1.
We refine Equation 3 using rel as follows:
p(a|e) = ?
(p,c)?Ds-pc
pt(rel(EAs(p) , EAs(c))|pc)
(10)
where Ds-pc denotes a set of parent-child
word pairs in the source sentence. We call
pt(rel(EAs(p) , EAs(c))|pc) target side dependency
relation probability. pt is a kind of tree-based
reordering model.
12
Table 1: A probability calculation example.
Source Target Phrase alignment probability
???? photodetector p(???? |photodetector)3 ? p(photodetector|????)
?? for p(?? |for)2 ? p(for|??)
?????? photogate p(?????? |a photogate)2 ? p(a photogate|??????)2
???? is used p(???? |is used)2 ? p(is used|????)2
NULL the p(the|NULL)
Source Target dependency Target Source dependency
c p relation probability c p relation probability
? ? pt(SAME|pc) A photogate ps(SAME|pc)
? ?? pt(SAME|pc) photogate is ps(c|pc)
?? ? pt(c|pc) used is ps(SAME|pc)
? ? pt(SAME|pc) for used ps(c|pc)
? ??? pt(c|pc) the photodetector ps(NULL c|pc)
??? ??? pt(SAME|pc) photodetector for ps(c|pc)
??? ? pt(c|pc)
? ??? pt(SAME|pc)
There are some special cases for rel. When Fs(c)
and Fs(p) are the same, that is, fc and fp belong
to the same phrase, rel is represented as ?SAME?.
When fp is aligned to NULL, fc is aligned to NULL,
and both of them are aligned to NULL, rel is repre-
sented as ?NULL p?, ?NULL c?, and ?NULL b?, re-
spectively. The lower part of Table 1 shows depen-
dency relation probabilities corresponding to Figure
1.
Actually, we extend the dependency relation
probability to consider a wider relation, i.e, the
grandparent-child relation, as follows:
p(a|e) = ?
(p,c)?Ds-pc
pt(rel(EAs(p) , EAs(c))|pc) ?
?
(g,c)?Ds-gc
pt(rel(EAs(g) , EAs(c))|gc)
(11)
where Ds-gc denotes a set of grandparent-child word
pairs in the source sentence.
3 Model Training
Our model is trained in two steps. In Step 1, word
translation probability is estimated. Then, in Step 2,
possible phrases are acquired, and both phrase trans-
lation probability and dependency relation probabil-
ity are estimated. In both steps, parameter estima-
tion is done with the EM algorithm.
3.1 Step 1
In Step 1, word translation probability in each di-
rection is estimated independently. This is done in
exactly the same way as in IBM Model 1.
In this process, the alignment unit is a word.
When we consider f -to-e alignment, each word on
the source side fj can correspond to a word on the
target side ei or a NULL word, independently of
other source words. The probability of one possible
alignment a is calculated as follows:
p(a, f |e) =
J?
j=1
p(fj |eaj ) (12)
By considering all possible alignments, p(f |e) is
calculated as:
p(f |e) =?
a
p(a, f |e) (13)
As initial parameters of p(f |e), we use uniform
probabilities. Then, after calculating Equation 12
and 13, we give the fractional count p(a,f |e)p(f |e) to all
word alignments in a, and we estimate p(f |e) by
MLE. We perform this estimation iteratively.
The inverse model e-to-f can be calculated in the
same manner.
3.2 Step 2
Both phrase translation probability and dependency
relation probability are estimated, and one undi-
rected alignment is found using the e-to-f and f -to-e
probabilities simultaneously in this step. In contrast
to Step 1, it is impossible to enumerate all the possi-
ble alignments. To find the best alignment, we first
create an initial alignment based on phrase trans-
lation probability only, and then gradually revise it
13
by considering the dependency relation probability
with a hill-climbing algorithm.
The initial parameters of Step 2 are calculated
as follows. The dependency relation probability is
calculated using the final alignment result of Step
1, and we use the word translation probability esti-
mated in Step 1 as the initial phrase translation prob-
ability.
3.2.1 Initial Alignment
We first create an initial alignment based on the
phrase translation probability without considering
the dependency relation probabilities.
For all the combinations of possible phrases
(including NULL), phrase alignment probabilities
are calculated (equation 9). Correspondences are
adopted one by one in descending order of geomet-
ric mean of the phrase alignment probabilities. All
the words should be aligned only once, that is, the
correspondences are adopted exclusively. Genera-
tion of possible phrases is explained in Section 3.2.3.
3.2.2 Hill-climbing
To find better alignments, the initial alignment is
gradually revised with a hill-climbing algorithm. We
use four kinds of revising operations:
Swap: Focusing on any two correspondences, the
partners are swapped. In the first step in
Figure2, the correspondences ?? ? photo-
gate? and ???????? photodetector? are
swapped to ??? photodetector? and ????
??? ? photogate?.
Extend: Focusing on one correspondence, the
source or target phrase is extended to include
its neighboring (parent or child) NULL-aligned
word.
Add: A new correspondence is added between a
source word and a target word both of which
are aligned to NULL.
Reject: A correspondence is rejected and the source
and target phrase are aligned to NULL.
Figure 2 shows an illustrative example of hill
climbing. The alignment is revised only if the align-
ment probability gets increased. It is repeated un-
til no operation can improve the alignment probabil-
ity, and the final state is the best approximate align-
ment. As a by-product of hill-climbing, pseudo n-
best alignment can be acquired. It is used in collect-
ing fractional counts.
3.2.3 Phrase Generation
If there is a word which is aligned to NULL in the
best approximate alignment, a new possible phrase
is generated by merging the word into a neighbor-
ing phrase which is not aligned to NULL. In the last
alignment result in Figure 2, for example, ????
is treated as being included in the correspondence
between ?? ?? and ?photodetector? and the cor-
respondence between ??? and ?for?. As a result,
we consider the correspondence between ?? ? ?
?? and ?photodetector? and the correspondence be-
tween ????? and ?for? existing in parallel sen-
tences. The new possible phrase is taken into con-
sideration from the next iteration.
3.2.4 Model Estimation
Collecting all the alignment results, we estimate
phrase alignment probabilities and dependency rela-
tion probabilities.
One way of estimating parameters of phrase
alignment probabilities is using the following equa-
tions:
p(Fj |Ei) = C(Fj , Ei)?
k C(Fk, Ei)
p(Ei|Fj) = C(Fj , Ei)?
k C(Ek, Fj)
(14)
where C(Fj , Ei) is a frequency of Fj and Ei is
aligned.
However, if we use this in our model, the phrase
translation probability of the new possible phrase
can become extremely high (often it becomes 1).
To avoid this problem, we use the equations below
for the estimation of phrase translation probability
in place of Equation 14:
p(Fj |Ei) = C(Fj , Ei)C(Ei) , p(Ei|Fj) =
C(Fj , Ei)
C(Fj) (15)
C(Ei) is the frequency of the phrase Ei in the train-
ing corpus which can be pre-counted. This definition
can resolve the problem where the phrase translation
probability of the new possible phrase becomes too
high.
As for the NULL, we use Equation 14 because we
cannot pre-count the frequency of NULL.
Using the estimated phrase alignment probabil-
ities and dependency relation probabilities, we go
back to the initial alignment described in Section
3.2.1 iteratively.
14
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
?
?
??
?
?
???
???
?
???
?
A
photogate
is
used
for
the
photodetector
.
Ini al alignment
Swap Reject
Add Extend
(accept)
(light)
(device)
(ni)
(ha)
(photo)
(gate)
(used)
(wo)
Figure 2: An example of hill-climbing.
4 Experimental Results
We conducted alignment experiments. A JST1
Japanese-English paper abstract corpus consisting
of 1M parallel sentences was used for the model
training. This corpus was constructed from a 2M
Japanese-English paper abstract corpus by NICT2
using the method of Uchiyama and Isahara (2007).
As gold-standard data, we used 475 sentence pairs
which were annotated by hand. The annotations
were only sure (S) alignments (there were no possi-
ble (P ) alignments) (Och and Ney, 2003). The unit
of evaluation was word-base for both Japanese and
English. We used precision, recall, and F-measure
as evaluation criteria.
We conducted two experiments to reveal 1) the
contribution of our proposed model compared to the
existing models, and 2) the effectiveness of using
dependency tree structure and phrases, which are
larger alignment units than words. Trainings were
run on the original forms of words for both the pro-
posed model and the models used for comparison.
4.1 Comparison with Word Sequential Model
For comparison, we used GIZA++ (Och and Ney,
2003) which implements the prominent sequential
word-base statistical alignment model of IBM Mod-
els. We conducted word alignment bidirectionally
with its default parameters and merged them using
three types of symmetrization heuristics (Koehn et
al., 2003). The results are shown in Table 2.
1http://www.jst.go.jp/
2http://www.nict.go.jp/
The result of ?Step 1? uses parameters estimated
after 5 iterations of Step 1. The alignment is ob-
tained by the method of initial alignment shown in
Section 3.2.1. In ?Step 2-1?, the phrase translation
probabilities are the same as those in ?Step 1?. In ad-
dition, dependency relation probabilities estimated
from the ?Step 1? alignment result are used. By com-
paring ?Step 1? and ?Step 2-1?, we can see the ef-
fectiveness of dependency relation probability. We
performed 5 iterations for Step 2 and calculated the
alignment accuracy each time. As a result, the pro-
posed model could achieve a higher F-measure by
1.7 points compared to the sequential model. ?In-
tersection? achieved best Precision, but its Recall is
quite low. ?grow-diag-final-and? achieved best Re-
call, but its Precision is lower than our best result
where the Recall is almost same. Thus, we can say
our result is better than sequential word alignment
models.
4.2 Effectiveness of Dependency Trees and
Phrases
To confirm the effectiveness of dependency trees and
phrases, we conducted alignment experiments on the
following four conditions:
? Using both dependency trees and phrases (re-
ferred to as ?proposed?).
? Using dependency trees only.
? Using phrases only.
? Not using dependency trees or phrases (referred
to as ?none?)
For the conditions which do not use dependency
trees, we used positional relations of a sentence as
15
Table 2: Results of alignment experiment.
Precision Recall F
Step 1 77.55 33.92 47.20
Step 2-1 83.46 40.03 54.11
Step 2-2 87.74 45.37 59.81
Step 2-3 87.62 48.92 62.79
Step 2-4 86.87 50.42 63.81
Step 2-5 85.90 50.75 63.80
Step 2-6 85.54 51.00 63.90
Step 2-7 85.18 50.87 63.70
Step 2-8 84.66 50.75 63.46
intersection 90.34 34.28 49.71
grow-final-and 81.32 48.85 61.04
grow-diag-final-and 79.39 51.15 62.22
Table 3: Effectiveness of dependency trees and phrases
(results after 5 iterations in Step 2.)
Precision Recall F
proposed 85.54 51.00 63.90
dependency tree only 89.77 39.47 54.83
phrase only 84.41 47.33 60.65
none 85.07 38.06 52.59
a sequence of words instead of dependency tree re-
lations. The results are shown in Table 3. All the
results are the alignment accuracy after 5 iterations
of Step 2.
5 Discussion
Table 2 shows that our proposed model could
achieve reasonably high accuracy of alignment, and
is better than sequential word-base models. As
an example, alignment results of a word sequen-
tial model are shown in Figure 3. The gray col-
ored cells are the gold-standard alignments, and the
black boxes are the outputs of the sequential model.
The model failed to resolve the correspondence am-
biguities between ?? (not) ?? (castrated) ??
? (mice)?, and ??? ????; and ?non-castrated
mice?, and ?castrated mice? respectively. This is
because these words are placed close to each other
and are also close to the correspondence ????
? as? which can be a clue to the word order. Us-
ing the tree structure in Figure 4, these words were
correctly aligned. This is because in the English
tree, the phrase ?castrated mice? does not depend
on ?as?, and ?non-castrated mice? does. Similarly
in the Japanese tree, ???????? depends on ?
???? and ??????? does not.
As mentioned in Section 1, sequential statistical
???exhibited ? ?astrong ?inhibitory ?effect ?on ? ?tumor ?growth ?in ?the ?castrated ?mice ?as ? ?in ?thenon-castrated ?mice ????? ?? ???
? ??
?
?? ???
? ???
? ?? ? ???? ?? ?? ? ???
Figure 3: An alignment example of the word sequential
model (grow-diag-final-and).
???exhibited ? ?? ??a? ??strong ?? ??inhibitory ???effect ???on? ? ??tumor ?? ??growth ???in ?? ? ??the? ? ??castrated ?? ??mice ???as ???in ?? ??the? ??non-castrated ? ???mice ?????
??
?
?
??
??
?
??
???
?
??
?
??
??
?
?
??
??
?
??
???
?
??
?
?
??
???
?
??
?
?
??
??
?
??
?
??
??
?
??
??
?
??
??
?
??
??
??
?
???
Figure 4: An alignment example of the proposed model.
methods, which regard a sentence as a sequence of
words, work well for language pairs that are not too
different in their language structure. Japanese and
English have significantly different structures. One
of the issues is that Japanese sentences have a SOV
word order, but in English, the word order is SVO, so
the dependency relations are often turned over. For
language pairs such as Japanese and English, deeper
sentence analysis using NLP resources is necessary
and useful. Our method is therefore suitable for such
language pairs.
As another example of an alignment failure by
the sequential model, Figure 5 shows the phrase cor-
respondence ?? ? ?? ? photodetector?, which
was correctly found as shown in Figure 6. The pro-
16
Aphotogate ? ?is ?used ?for ?the ?photodetector ?
?? ?? ? ? ???
???
? ???
Figure 5: An unsuccessful example of phrase detection in
the sequential model (grow-diag-final-and).
??A ????photogate ? ?
is ? ???used ? ???for ? ?? ??the??photodetector ? ? ?
??
?
??
?
??
??
??
?
??
?
?
??
???
?
??
???
??
?
???
Figure 6: An example of phrase detection in the proposed
model.
posed method of generating possible phrases during
iterations works well and improves alignment.
From the result of our second experiment, we can
see the following points:
1. Phrasal alignment improves the recall, but low-
ers the precision.
2. By using dependency trees, precision can be
improved.
3. We can find a balance point by using both
phrasal alignment and dependency trees.
The causes of alignment errors in our model can
be summarized into categories. The biggest one is
parsing errors. Since our model is highly dependent
on the parsing result, the alignments would easily
turn out wrong if the parsing result was incorrect.
Sometimes the hill-climbing algorithm could not
revise the initial alignment. Most of these cases
would happen when one word occurred several
times on one side, but some of those occurrences
were omitted on the other side. Let?s suppose there
are two identical words on the source side, but the
target side has only one corresponding word. Initial
alignment is created without considering the depen-
dencies at all, so it cannot judge which source word
should be aligned to the corresponding target word.
In this case, the best alignment searching sometimes
gets the local solution. This problem could be re-
solved by considering local dependencies for am-
biguous words.
One difficulty is how to handle function words.
Function words often do not have exactly corre-
sponding words in the opposite language. Japanese
case markers such as ?? (ha)?, ?? (ga)? (subjec-
tive case), ?? (wo)? (objective case) and so on, and
English articles are typical examples of words, that
do not have corresponding parts. There is a differ-
ence between alignment criteria for function words
of gold-standard and our outputs, and it is somewhat
difficult to improve alignment accuracy.
6 Conclusion
In this paper, we have proposed a linguistically-
motivated probabilistic phrase alignment model
based on dependency tree structures. The model in-
corporates the tree-based reordering model. Experi-
mental results show that the word sequential model
does not work well for linguistically different lan-
guage pairs, and this can be resolved by using syn-
tactic information. We have conducted the experi-
ments only on Japanese-English corpora. To firmly
support our claim that syntactic information is im-
portant, it is necessary to do more investigation on
other language pairs.
Most frequent alignment errors are derived from
parsing errors. Because our method depends heavily
on structural information, parsing errors easily make
the alignment accuracy worse. Although the parsing
accuracy is high in general for both Japanese and
English, it sometimes outputs incorrect dependency
structures because technical or unknown words of-
ten appears in scientific papers. This problem could
be resolved by introducing parsing probabilities into
our model using parsing tools which can output n-
best parsing with their parsing probabilities. This
will not only improve the alignment accuracy, it will
allow revision of the parsing result. Moreover, we
need to investigate the contribution of our alignment
result to the translation quality.
17
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Association for Computational Linguistics,
19(2):263?312.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association of Compu-
tational Linguistics, pages 88?95.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
EMNLP, pages 232?241, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting on ACL, pages 80?87.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 176?183, New York City,
USA, June. Association for Computational Linguis-
tics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127?133.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proceedings of
The International Workshop on Sharable Natural Lan-
guage, pages 22?28.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL) Workshop on Data-
Driven Machine Translation, pages 39?46.
Toshiaki Nakazawa and Sadao Kurohashi. 2008.
Linguistically-motivated tree-based probabilistic
phrase alignment. In In Proceedings of the Eighth
Conference of the Association for Machine Translation
in the Americas (AMTA2008).
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Association for Computational Linguistics, 29(1):19?
51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 271?279.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In MT summit XI, pages
475?482.
Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki.
2000. Finding structural correspondences from bilin-
gual parsed corpus for corpus-based translation. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 906?912.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the ACL, pages 523?530.
18
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 682 ? 693, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Automatic Acquisition of Basic Katakana Lexicon  
from a Given Corpus 
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao Kurohashi 
University of Tokyo, 7-3-1 Hongo Bunkyo-ku, Tokyo, 113-8656, Japan 
{nakazawa, kawahara, kuro}@kc.t.u-tokyo.ac.jp 
Abstract. Katakana, Japanese phonogram mainly used for loan words, is a 
trou-blemaker in Japanese word segmentation. Since Katakana words are heavily 
domain-dependent and there are many Katakana neologisms, it is almost 
impossible to construct and maintain Katakana word dictionary by hand. This 
paper proposes an automatic segmentation method of Japanese Katakana 
compounds, which makes it possible to construct precise and concise Katakana 
word dictionary automati-cally, given only a medium or large size of Japanese 
corpus of some domain. 
1   Introduction
Handling words properly is very important for Natural Language Processing. Words 
are basic unit to assign syntactic/semantic information manually, basic unit to acquire 
knowledge based on frequencies and co-occurrences, and basic unit to access texts in 
Information Retrieval.  
Languages with explicit word boundaries, like white spaces in English, do not suffer 
from this issue so severely, though it is a bit troublesome to handle compounds and 
hyphenation appropriately. On the other hand, languages without explicit boundaries 
such as Japanese always suffer from this issue.  
Japanese character set and their usage. Here, we briefly explain Japanese character set 
and their usage. Japanese uses about 6,000 ideogram, Kanji characters, 83 phonogram, 
Hiragana, and another 86 phonogram, Katakana.  
Kanji is used for Japanese time-honored nouns (including words imported from 
China ancient times) and stems of verbs and adjectives; Hiragana is used for function 
words such as postpositions and auxiliary verbs, and endings of verbs and adjectives; 
Katakana is used for loan words, mostly from the West, as transliterations.  
Japanese is very active to naturalize loan words. Neologisms in special/technical 
domains are often transliterated into Katakana words without translations, or even if 
there are translations, Katakana transliterations are more commonly used in many 
cases. For example,  , transliteration of ?computer? is more commonly 
used than the translation, (keisanki).  
Even for some time-honored Japanese nouns, both Japanese nouns and 
translitera-tions of their English translations are used together these days, and the use of 
        Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus 683 
translit-erations is increasing, such as  , transliteration of ?desk work? vs. 
 (tsukue shigoto). Furthermore, some Japanese nouns, typically the names of 
animals, plants, and food, which can be written in Kanji or Hiragana, are also written in 
Katakana sometimes [4, 6].  
Word segmentation and Katakana words. Let us go back to the word segmentation 
issue. Japanese word segmentation is performed like this: Japanese words are registered 
into the dictionary; given an input sentence, all possible words embedded in the sentence 
and their connections are checked by looking up the dictionary and some connectivity 
grammar; then the most plausible word sequence is selected. The criteria of selecting the 
best word sequence were simple heuristic rules preferring longer words in earlier times, 
and some cost calculation based on manual rules or using some training data, these days.  
Such a segmentation process is in practice not so difficult for Kanji-Hiragana string. 
First of all, since Kanji words and Hiragana words are fairly stable excepting proper 
nouns, they are most perfectly registered in the dictionary. Then, the orthogonal usage of 
Kanji and Hiragana mentioned above makes the segmentation rather simple, as follows:  
                   
(Kare      wa  daigaku      ni  kayou) 
he     postp.   Univ.    postp.     go 
Kanji compound words can cause a segmentation problem. However, since large 
num-ber of Kanji characters lead fairly sparse space of Kanji words, most Kanji 
compounds can be segmented unambiguously.  
A real troublemaker is Katakana words, which are sometimes very long compounds 
such as 	
?extra vergin olive oil? and 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 146?149,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation and Named Entity Recognition by 
Character Tagging 
 
Kun Yu1      Sadao Kurohashi2     Hao Liu1     Toshiaki Nakazawa1 
 Graduate School of Information Science and Technology, The University of 
Tokyo, Tokyo, Japan, 113-86561 
Graduate School of Informatics, Kyoto University, Kyoto, Japan, 606-85012 
 {kunyu, liuhao, nakazawa}@kc.t.u-tokyo.ac.jp1 
kuro@i.kyoto-u.ac.jp2 
  
Abstract 
This paper describes our word segmenta-
tion system and named entity recognition 
(NER) system for participating in the 
third SIGHAN Bakeoff. Both of them are 
based on character tagging, but use dif-
ferent tag sets and different features. 
Evaluation results show that our word 
segmentation system achieved 93.3% and 
94.7% F-score in UPUC and MSRA open 
tests, and our NER system got 70.84% 
and 81.32% F-score in LDC and MSRA 
open tests. 
1 Introduction 
Dealing with word segmentation as character 
tagging showed good results in last SIGHAN 
Bakeoff (J.K.Low et al,2005). It is good at un-
known word identification, but only using char-
acter-level features sometimes makes mistakes 
when identifying known words (T.Nakagawa, 
2004). Researchers use word-level features 
(J.K.Low et al,2005) to solve this problem. 
Based on this idea, we develop a word segmenta-
tion system based on character-tagging, which 
also combine character-level and word-level fea-
tures. In addition, a character-based NER module 
and a rule-based factoid identification module 
are developed for post-processing.  
Named entity recognition based on character-
tagging has shown better accuracy than word-
based methods (H.Jing et al,2003). But the small 
window of text makes it difficult to recognize the 
named entities with many characters, such as 
organization names (H.Jing et al,2003). Consid-
ering about this, we developed a NER system 
based on character-tagging, which combines 
word-level and character-level features together. 
In addition, in-NE probability is defined in this 
system to remove incorrect named entities and 
create new named entities as post-processing. 
2 Character Tagging for Word 
Segmentation and NER 
2.1 Basic Model 
We look both word segmentation and NER as 
character tagging, which is to find the tag se-
quence T* with the highest probability given a 
sequence of characters S=c1c2?cn.  
)|(maxarg* STPT
T
=  (1) 
Then we assume that the tagging of one char-
acter is independent of each other, and modify 
formula 1 as 
?
=
=
=
=
=
n
i
ii
tttT
nn
tttT
ctP
ccctttPT
n
n
1...
2121
...
*
)|(maxarg     
)...|...(maxarg
21
21
 (2) 
 Beam search (n=3) (Ratnaparkhi,1996) is ap-
plied for tag sequence searching, but we only 
search the valid sequences to ensure the validity 
of searching result. SVM is selected as the basic 
classification model for tagging because of its 
robustness to over-fitting and high performance 
(Sebastiani, 2002). To simplify the calculation, 
the output of SVM is regarded as P(ti|ci). 
2.2 Tag Definition 
Four tags ?B, I, E, S? are defined for the word 
segmentation system, in which ?B? means the 
character is the beginning of one word, ?I? means 
the character is inside one word, ?E? means the 
character is at the end of one word and ?S? means 
the character is one word by itself. 
For the NER system, different tag sets are de-
fined for different corpuses. Table 1 shows the 
146
tag set defined for MSRA corpus. It is the prod-
uct of Segment-Tag set and NE-Tag set, because 
not only named entities but also words are seg-
mented in this corpus. Here NE-Tag ?O? means 
the character does not belong to any named enti-
ties. For LDC corpus, because there is no seg-
mentation information, we delete NE-Tag ?O? 
but add tag ?NONE? to indicate the character 
does not belong to any named entities (Table 2). 
Table 1 Tags of NER for MSRA corpus 
Segment-Tag NE-Tag 
B, I, E, S ? PER, LOC, ORG, O 
Table 2 Tags of NER for LDC corpus 
Segment Tag NE Tag 
B, I, E, S ? PER, LOC, ORG, GPE + NONE 
2.3 Feature Definition 
First, some features based on characters are 
defined for the two tasks, which are: 
(a) Cn (n=-2,-1,0,1,2) 
(b) Pu(C0) 
Feature Cn (n=-2,-1,0,1,2) mean the Chinese 
characters appearing in different positions (the 
current character and two characters to its left 
and right), and they are binary features. A char-
acter list, which contains all the characters in the 
lexicon introduced later, is used to identify them.
 
Besides of that, feature Pu(C0) means whether C0 
is in a punctuation character list. It is also binary 
feature and all the punctuations in the punctua-
tion character list come from Penn Chinese Tree-
bank 5.1 (N.Xue et al,2002). 
In addition, we define some word-level fea-
tures based on a lexicon to enlarge the window 
size of text in the two tasks, which are:  
(c) Wn (n=-1,0,1) 
Feature Wn (n=-1,0,1) mean the lexicon words 
in different positions (the word containing C0 
and one word to its left and right) and they are 
also binary features. We select all the possible 
words in the lexicon that satisfy the requirements, 
not like only selecting the longest one in 
(J.K.Low et al,2005). To create the lexicon, we 
use following steps. First, a lexicon from NICT 
(National Institute of Information and Communi-
cations Technology, Japan) is used as the basic 
lexicon, which is extracted from Peking Univer-
sity Corpus of the second SIGHAN Bakeoff 
(T.Emerson, 2005), Penn Chinese Treebank 4.0 
(N.Xue et al,2002), a Chinese-to-English Word-
list1  and part of NICT corpus (K.Uchimoto et 
al.,2004; Y.J.Zhang et al,2005). Then, all the 
words containing digits and letters are removed 
                                                 
1
 http://projects.ldc.upenn.edu/Chinese/  
from this lexicon. At last, all the punctuations in 
Penn Chinese Treebank 5.1 (N.Xue et al,2002) 
and all the words in the training data of UPUC 
and MSRA corpuses are added into the lexicon.  
Besides of above features, some extra features 
are defined only for NER task. 
First, we add some character-based features to 
improve the accuracy of person name recognition, 
which are CNn (n=-2,-1,0,1,2). They mean 
whether C
 n (n=-2,-1,0,1,2) belong to a Chinese 
surname list. All of them are binary features. The 
Chinese surname list contains the most famous 
100 Chinese surnames, such as ?, ?, ?, ? 
(Zhao, Qian, Sun, Li). 
Then, we add some word-based features to 
help identify the organization name, which are 
WORGn (n=-1,0,1). They mean whether W n (n= 
-1,0,1) belong to an organization suffix list. All 
of them are also binary features. The organiza-
tion suffix list is created by extracting the last 
word from all the organization names in the 
training data of both MSRA and LDC corpuses. 
3 Post-processing 
Besides of the basic model, a NER module 
and a factoid identification module are developed 
in our word segmentation system for post-
processing. In addition, we define in-NE prob-
ability to delete the incorrect named entities and 
identify new named entities in the post-
processing phrase of our NER system. 
3.1 Named Entity Recognition for Word 
Segmentation 
In this module, if two or more segments in the 
outputs of basic model are recognized as one 
named entity, we combine them as one segment.  
This module uses the same basic NER model 
as what we introduced in the previous section. 
But it only identifies person and location names, 
because organization names often contain more 
than one word. In addition, to keep the high ac-
curacy of person name recognition, the features 
about organization suffixes are not used here.  
3.2 Factoid Identification for Word Seg-
mentation 
Rules are used to identify the following fac-
toids among the segments from the basic word 
segmentation model:  
NUMBER: Integer, decimal, Chinese number 
PERCENT: Percentage and fraction 
DATE: Date 
FOREIGN: English words 
147
Table 3 shows some rules defined here. 
Table 3 Some Rules for Factoid Identification 
Factoid Rule 
NUMBER If previous segment ends with DIGIT and current 
segment starts with DIGIT, then combine them. 
PERCENT If previous segment is composed of DIGIT and 
current segment equals ?%?, then combine them. 
DATE 
If previous segment is composed of DIGIT and 
current segment is in the list of ??, ?, ?, ? 
(Year, Month, Day, Day)?, then combine them. 
FOREIGN Combine the consequent letters as one segment. 
(DIGIT means both Arabic and Chinese numerals) 
3.3 NER Deletion and Creation 
In-word probability has been used in unknown 
word identification successfully (H.Q.Li et al, 
2004). Accordingly, we define in-NE probability 
to help delete and create named entities (NE). 
Formula 3 shows the definition of in-NE prob-
ability for character sequence cici+1?ci+n. Here ?# 
of cici+1?ci+n as NE? is defined as TimeInNE and 
the occurrence of cici+1?ci+n in different type of 
NE is treated differently. 
data in testing ... of #
NE as ... of #)...(
1
1
1
niii
niii
niiiInNE
ccc
ccc
cccP
++
++
++ =
 (3) 
Then, we use some criteria to delete the incor-
rect NE and create new possible NE, in which 
different thresholds are set for different tasks. 
Criterion 1: If PInNE(cici+1?ci+n) of one NE 
type is lower than TDel, and TimeInNE(cici+1?ci+n) 
of the same NE type is also lower than TTime, then 
delete this type of NE composed of cici+1?ci+n.  
Criterion 2: If PInNE(cici+1?ci+n) of one NE 
type is higher than TCre, and in other places the 
character sequence cici+1?ci+n does not belong to 
any NE, then create a new NE containing 
cici+1?ci+n with this NE type.  
4 Evaluation Results and Discussion 
4.1 Evaluation Setting 
SVMlight (T.Joachims, 1999) was used as 
SVM tool. In addition, we used the MSRA train-
ing corpus of NER task in this Bakeoff to train 
our NER post-processing module. 
4.2 Results of Word Segmentation 
We attended the open track of word segmenta-
tion task for two corpuses: UPUC and MSRA. 
Table 4 shows the evaluation results. 
Table 4 Results of Word Segmentation Task (in percentage %) 
Corpus Pre. Rec. F-score Roov Riv 
UPUC 94.4 92.2 93.3 68.0 97.0 
MSRA 94.0 95.3 94.7 50.3 96.9 
The F-score of our word segmentation system 
in UPUC corpus ranked 4th (same as that of the 
3rd group) among all the 8 participants. And it 
was only 1.1% lower than the highest one and 
0.2% lower than the second one. It showed that 
our character-tagging approach was feasible. But 
the F-score of MSRA corpus was only higher 
than one participant in all the 10 groups (the 
highest one was 97.9%). Error analysis shows 
that there are two main reasons.  
First, in MSRA corpus, they tend to segment 
one organization name as one word, such as ?
?????(China Chamber of Commerce in 
USA). But our basic segmentation model seg-
mented such word into several words, e.g. ??/
??/??(USA/China/Chamber of Commerce), 
and our post-processing NER module does not 
consider about organization names.  
Second, our factoid identification rule did not 
combine the consequent DATE factoids into one 
word, but they are combined in MSRA corpus. 
For example, our system segmented the word?
? 9?? (9 o?clock in the evening) into three 
parts ??/9 ?/? (Evening/9 o?clock/Exact). 
This error can be solved by revising the rules for 
factoid identification. 
Besides of that, we also found although our 
large lexicon helped identify the known word 
successfully, it also decreased the recall of OOV 
words (our Riv of UPUC corpus ranked 2nd, with 
only 0.6% decrease than the highest one, but 
Roov ranked 4th, with 8.8% decrease than the 
highest one). The large size of this lexicon is 
looked as the main reason.  
Our lexicon contains 221,407 words, in which 
6,400 words are single-character words. It made 
our system easy to segment one word into sev-
eral words, for example word ??? (Economy 
Group) in UPUC corpus was segmented into?
?  (Economy) and ? (Group). Moreover, the 
large size of this lexicon also brought errors of 
combining two words into one word if the word 
was in the lexicon. For example, words ? (Only) 
and ? (Have) in MSRA corpus were identified 
as one word because there existed the word?? 
(Only) in our lexicon. We will reduce our lexi-
con to a reasonable size to solve these problems. 
4.3 Results of NER 
We also attended the open track of NER task 
for both LDC corpus and MSRA corpus. Table 5 
and Table 6 give the evaluation results.  
There were only 3 participants in the open 
track of LDC corpus and our group got the best 
F-score. In addition, among all the 11 partici-
pants for MSRA corpus, our system ranked 6th 
148
by F-score. It showed the validity of our charac-
ter-tagging method for NER. But for location 
name (LOC) in LDC corpus, both the precision 
and recall of our NER system were very low. It 
was because there were too few location names 
in the training data (there were only 476 LOC in 
the training data, but 5648 PER, 5190 ORG and 
9545 GPE in the same data set). 
Table 5 Results of NER Task for LDC corpus (in percentage %) 
 PER LOC ORG GPE Overall 
Pre. 83.29 58.52 61.48 78.66 76.16 
Rec. 66.93 18.87 45.19 79.94 66.21 
F-score 74.22 28.57 52.09 79.30 70.84 
Table 6 Results of NER Task for MSRA corpus (in percentage %) 
 PER LOC ORG Overall 
Pre. 90.76 85.62 73.90 84.68 
Rec. 76.13 85.41 65.74 78.22 
F-score 82.80 85.52 69.58 81.32 
Besides of that, error analysis shows there are 
four types of main errors in the NER results. 
First, some organization names were very long 
and can be divided into several words, in which 
parts of them can also be looked as named enti-
ties. In such case, our system only recognized the 
small parts as named entities. For example,  ??
???????????  (Fei Zhengqing 
Eastern Asia Research Center of Harvard Univ.) 
was an organization name. But our system rec-
ognized it as????(Harvard Univ.)/ORG+?
? ? (Fei Zheng Qing)/PER+ ? ? (Eastern 
Asia)/LOC+ ????(Research Center)/ORG. 
Adding more context features may be useful to 
resolve this issue. 
In addition, our system was not good at recog-
nizing foreign person names, such as ??? 
(Riordan), and abbreviations, such as ?? (Los 
Angeles), if they seldom or never appeared in 
training corpus. It is because the use of the large 
lexicon decreased the unknown word identifica-
tion ability of our NER system simultaneously. 
Third, the in-NE probability used in post-
processing is helpful to identify named entities 
which cannot be recognized by the basic model. 
But it also recognized some words which can 
only be regarded as named entities in the local 
context incorrectly. For example, our system 
recognized?? (Najing) as GPE in ?????
? (Send to Najing for remedy) in LDC corpus. 
We will consider about adding the in-NE prob-
ability as one feature into the basic model to 
solve this problem. 
At last, in LDC corpus, they combine the at-
tributive of one named entity (especially person 
and organization names) with the named entity 
together. But our system only recognized the 
named entity by itself. For example, our system 
only recognized ??? (Liu Gui Fang) as PER 
in the reference person name ???????? 
(Liu Gui Fang who does not know the inside). 
5 Conclusion and Future Work 
Through the participation of the third 
SIGHAN Bakeoff, we found that tagging charac-
ters with both character-level and word-level fea-
tures was effective for both word segmentation 
and NER. While, this work is only our 
preliminary attempt and there are still many 
works needed to do in the future, such as the 
control of lexicon size, the use of extra 
knowledge (e.g. pos-tag), the feature definition, 
and so on. In addition, our word segmentation 
system only combined the NER module as post-
processing, which resulted in that lots of infor-
mation from NER module cannot be used by the 
basic model. We will consider about combining 
the NER and factoid identification modules into 
the basic word segmentation model by defining 
new tag sets in our future work. 
Acknowledgement 
We would like to thank Dr. Kiyotaka Uchi-
moto for providing the NICT lexicon. 
Reference 
T.Emerson. 2005. The Second International Chinese Word Seg-
mentation Bakeoff. In the 4th SIGHAN Workshop. pp. 123-133. 
H.Jing et al 2003. HowtogetaChineseName(Entity): Segmentation 
and Combination Issues. In EMNLP 2003. pp. 200-207. 
T.Joachims. 1999. Making large-scale SVM learning practical. 
Advances in Kernel Methods - Support Vector Learning. MIT-
Press. 
H.Q.Li et al 2004. The Use of SVM for Chinese New Word Identi-
fication. In IJCNLP 2004. pp. 723-732. 
J.K.Low et al 2005. A Maximum Entropy Approach to Chinese 
Word Segmentation. In the 4th SIGHAN Workshop. pp. 161-164. 
T.Nakagawa. 2004. Chinese and Japanese Word Segmentation 
Using Word-level and Character-level Information. In COLING 
2004. pp. 466-472. 
A.Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-
Speech Tagging. In EMNLP 1996. 
F.Sebastiani. 2002. Machine learning in automated text categoriza-
tion. ACM Computing Surveys. 34(1): 1-47. 
K.Uchimoto et al 2004. Multilingual Aligned Parallel Treebank 
Corpus Reflecting Contextual Information and its Applications. 
In Proceedings of the MLR 2004. pp. 63-70. 
N.Xue et al 2002. Building a Large-Scale Annotated Chinese Cor-
pus. In COLING 2002. 
Y.J.Zhang et al 2005. Building an Annotated Japanese-Chinese 
Parallel Corpus ? A part of NICT Multilingual Corpora. In Pro-
ceedings of the MT SummitX. pp. 71-78. 
149
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 79?84,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
KyotoEBMT: An Example-Based Dependency-to-Dependency
Translation Framework
John Richardson? Fabien Cromi?res? Toshiaki Nakazawa? Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University, Kyoto 606-8501
?Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012
john@nlp.ist.i.kyoto-u.ac.jp, {fabien, nakazawa}@pa.jst.jp,
kuro@i.kyoto-u.ac.jp
Abstract
This paper introduces the Ky-
otoEBMT Example-Based Machine
Translation framework. Our system
uses a tree-to-tree approach, employing
syntactic dependency analysis for
both source and target languages
in an attempt to preserve non-local
structure. The effectiveness of our
system is maximized with online ex-
ample matching and a flexible decoder.
Evaluation demonstrates BLEU scores
competitive with state-of-the-art SMT
systems such as Moses. The current
implementation is intended to be
released as open-source in the near
future.
1 Introduction
Corpus-based approaches have become a ma-
jor focus of Machine Translation research.
We present here a fully-fledged Example-
Based Machine Translation (EBMT) plat-
form making use of both source-language
and target-language dependency structure.
This paradigm has been explored compar-
atively less, as studies on Syntactic-based
SMT/EBMT tend to focus on constituent
trees rather than dependency trees, and
on tree-to-string rather than tree-to-tree ap-
proaches. Furthermore, we employ separate
dependency parsers for each language rather
than projecting the dependencies from one lan-
guage to another, as in (Quirk et. al, 2005).
The dependency structure information is
used end-to-end: for improving the quality
of the alignment of the translation examples,
for constraining the translation rule extraction
and for guiding the decoding. We believe that
dependency structure, which considers more
than just local context, is important in order
to generate fluent and accurate translations
of complex sentences across distant language
pairs.
Our experiments focus on technical do-
main translation for Japanese-Chinese and
Japanese-English, however our implementa-
tion is applicable to any domain and language
pair for which there exist translation examples
and dependency parsers.
A further unique characteristic of our sys-
tem is that, again contrary to the majority of
similar systems, it does not rely on precompu-
tation of translation rules. Instead it matches
each input sentence to the full database of
translation examples before extracting trans-
lation rules online. This has the merit of max-
imizing the information available when creat-
ing and combining translation rules, while re-
taining the ability to produce excellent trans-
lations for input sentences similar to an exist-
ing translation example.
The system is mostly developed in C++ and
incorporates a web-based translation interface
for ease of use. The web interface (see Fig-
ure 1) also displays information useful for error
analysis such as the list of translation exam-
ples used. Experiments are facilitated through
the inclusion of a curses-based graphical in-
terface for performing tuning and evaluation.
The decoder supports multiple threads.
We are currently making preparations for
the project to be released with an open-
source license. The code will be available at
http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.
2 System Overview
Figure 2 shows the basic structure of the pro-
posed translation pipeline.
The training process begins with parsing
and aligning parallel sentences from the train-
79
Figure 1: A screenshot of the web interface
showing a Japanese-English translation. The
interface provides the source and target side
dependency tree, as well as the list of exam-
ples used with their alignments. The web in-
terface facilitates easy and intuitive error anal-
ysis, and can be used as a tool for computer-
aided translation.
ing corpus. Alignment uses a Bayesian sub-
tree alignment model based on dependency
trees. This contains a tree-based reorder-
ing model and can capture non-local reorder-
ings, which sequential word-based models of-
ten cannot handle effectively. The alignments
are then used to build an example database
(?translation memory?) containing ?examples?
or ?treelets? that form the hypotheses to be
combined during decoding.
Translation is performed by first parsing
an input sentence then searching for treelets
matching entries in the example database.
The retrieved treelets are combined by a de-
coder that optimizes a log linear model score.
The example retrieval and decoding steps are
explained in more detail in sections 3 and 4
respectively. The choice of features and the
tuning of the log linear model is described in
section 5.
Figure 3 shows the process of combining ex-
amples matching the input tree to create an
output sentence.
Figure 2: Translation pipeline. An example
database is first trained from a parallel cor-
pus. Translation is performed by the decoder,
which combines initial hypotheses generated
by the example retrieval module. Weights can
be improved with batch tuning.
3 Example retrieval and translation
hypothesis construction
An important characteristic of our system is
that we do not extract and store translation
rules in advance: the alignment of translation
examples is performed offline. However, for a
given input sentence i, the steps for finding
examples partially matching i and extracting
their translation hypotheses is an online pro-
cess. This approach could be considered to be
more faithful to the original EBMT approach
advocated by Nagao (1984). It has already
been proposed for phrase-based (Callison-
Burch et al., 2005), hierarchical (Lopez, 2007),
and syntax-based (Cromi?res and Kurohashi,
2011) systems. It does not however, seem to
be very commonly integrated in syntax-based
MT.
This approach has several benefits. The first
is that we are not required to impose a limit
on the size of translation hypotheses. Systems
extracting rules in advance typically restrict
the size and number of extracted rules for fear
of becoming unmanageable. In particular, if
an input sentence is the same or very similar
to one of our translation examples, we will be
able to retrieve a perfect translation. A second
advantage is that we can make use of the full
context of the example to assign features and
scores to each translation hypothesis.
The main drawback of our approach is that
it can be computationally more expensive to
retrieve arbitrarily large matchings in the ex-
80
!"
#$%&"
!""
#$%#&''("
')"
#*!)+,!-')"
.!/01)$"
'()234"
*+2546,-.274"
!284"
/2946 02:4"
#$%&2;4"
*$!<"
!"
=0$6 #$%#&''("
')"
#*!)+,!-')"
.!/01)$"
>)?@#6A*$$6 B@#?@#6A*$$6
C%!.?,$6D!#!&!+$6
,-."
'()"
*6 1"
/6 06
*$!<"
!"
E$6 &''(6
2F54" 2F74"
2F54" 2F74"
23" ?!?$*+"
2F84" 2F84"2F84"
*+6 =0$6
2GBBA4"
2F54"
2F74"
2F84"
Figure 3: The process of translation. The source sentence is parsed and matching subtrees from
the example database are retrieved. From the examples, we extract translation hypotheses than
can contain optional target words and several position for each non-terminals. For example the
translation hypothesis containing ?textbook? has three possible position for the non-terminal X3
(as a left-child before ?a?, as a left-child after ?a? or as a right-child). The translation hypotheses
are then combined during decoding. Choice of optional words and final Non-Terminal positions
is also done during decoding.
ample database online than it is to match pre-
computed rules. We use the techniques de-
scribed in (Cromi?res and Kurohashi, 2011)
to perform this step as efficiently as possible.
Once we have found an example translation
(s, t) for which s partially matches i, we pro-
ceed to extract a translation hypothesis from
it. A translation hypothesis is defined as a
generic translation rule for a part p of the in-
put sentence that is represented as a target-
language treelet, with non-terminals repre-
senting the insertion positions for the transla-
tions of other parts of the sentence. A trans-
lation hypothesis is created from a translation
example as follows:
1. We project the part of s that is matched
into the target side t using the alignment
of s and t. This is trivial if each word of
s and t is aligned, but this is not typi-
cally the case. Therefore our translation
hypotheses will often have some target
words/nodes marked as optionals: this
means that we will decide if they should
be added to the final translation only at
the moment of combination.
2. We insert the non-terminals as child
nodes of the projected subtree. This is
simple if i, s and t have the same struc-
ture and are perfectly aligned, but again
this is not typically the case. A conse-
quence is that we will sometimes have sev-
eral possible insertion positions for each
non-terminal. The choice of insertion po-
sition is again made during combination.
4 Decoding
After having extracted translation hypotheses
for as many parts of the input tree as possible,
we need to decide how to select and combine
them. Our approach here is similar to what
81
Figure 4: A translation hypothesis endoded
as a lattice. This representation allows us to
handle efficiently the ambiguities of our trans-
lation rules. Note that each path in this lat-
tice corresponds to different choices of inser-
tion position for X2, morphological forms of
?be?, and the optional insertion of ?at?.
has been proposed for Corpus-Based Machine
Translation. We first choose a number of fea-
tures and create a linear model scoring each
possible combination of hypotheses (see Sec-
tion 5). We then attempt to find the combi-
nation that maximizes this model score.
The combination of rules is constrained by
the structure of the input dependency tree. If
we only consider local features1, then a simple
bottom-up dynamic programming approach
can efficiently find the optimal combination
with linear O(|H|) complexity2. However,
non-local features (such as language models)
will force us to prune the search space. This
pruning is done efficiently through a varia-
tion of cube-pruning (Chiang, 2007). We
use KenLM3 (Heafield, 2011) for computing
the target language model score. Decoding
is made more efficient by using some of the
more advanced features of KenLM such as
state-reduction ((Li and Khudanpur, 2008),
(Heafield et al., 2011)) and rest-cost estima-
tions(Heafield et al., 2012).
Compared with the original cube-pruning
algorithm, our decoder is designed to handle
an arbitrary number of non-terminals. In ad-
dition, as we have seen in Section 3, the trans-
lation hypotheses we initially extract from ex-
amples are ambiguous in term of which target
word is going to be used and which will be the
final position of each non-terminal. In order to
handle such ambiguities, we use a lattice-based
internal representation that can encode them
efficiently (see Figure 4). This lattice represen-
tation also allows the decoder to make choices
between various morphological variations of a
1The score of a combination will be the sum of the
local scores of each translation hypothesis.
2
H = set of translation hypotheses
3http://kheafield.com/code/kenlm/
word (e.g. be/is/are).
5 Features and Tuning
During decoding we use a linear model to score
each possible combination of hypotheses. This
linear model is based on a linear combination
of both local features (local to each translation
hypothesis) and non-local features (such as a
5-gram language model score of the final trans-
lation). The decoder considers in total a com-
bination of 34 features, a selection of which are
given below.
? Example penalty and example size
? Translation probability
? Language model score
? Optional words added/removed
The optimal weights for each feature are
estimated using the Pairwise Ranking Op-
timization (PRO) algorithm (Hopkins and
May, 2011) and parameter optimization with
MegaM4. We use the implementation of PRO
that is provided with the Moses SMT system
and the default settings of MegaM.
6 Experiments
In order to evaluate our system, we conducted
translation experiments on four language
pairs: Japanese-English (JA?EN), English-
Japanese (EN?JA), Japanese-Chinese (JA?
ZH) and Chinese-Japanese (ZH?JA).
For Japanese-English, we evaluated on the
NTCIR-10 PatentMT task data (patents)
(Goto et al., 2013) and compared our system
with the official baseline scores. For Japanese-
Chinese, we used parallel scientific paper ex-
cerpts from the ASPEC5 corpus and com-
pared against the same baseline system as for
Japanese-English. The corpora contain 3M
parallel sentences for Japanese-English and
670K for Japanese-Chinese.
The two baseline systems are based on the
open-source GIZA++/Moses pipeline. The
baseline labeled ?Moses? uses the classic
phrase-based engine, while ?Moses-Hiero? uses
the Hierarchical Phrase-Based decoder. These
4http://www.umiacs.umd.edu/~hal/megam/
5http://orchid.kuee.kyoto-u.ac.jp/ASPEC/
82
System JA?EN EN?JA JA?ZH ZH?JA
Moses 28.86 33.61 32.90 42.79
Moses-Hiero 28.56 32.98 ? ?
Proposed 29.00 32.15 32.99 37.64
Table 1: Scores
System BLEU Translation
Moses 31.09 Further, the expansion stroke, the sectional area of the inner tube 12,
and the oil is supplied to the lower oil chamber S2 from the oil reservoir
chamber R ? stroke.
Moses-
Hiero
21.49 Also, the expansion stroke, the cross-sectional area of the inner tube
12 ? stroke of oil supplied from the oil reservoir chamber R lower oil
chamber S2.
Proposed 44.99 Further in this expansion stroke, the oil at an amount obtained by mul-
tiplying cross sectional area of the inner tube 12 from the oil reservoir
chamber R is resupplied to the lower oil chamber S2.
Reference 100.00 In this expansion stroke, oil in an amount obtained by multiplying the
cross sectional area of the inner tube 12 by the stroke is resupplied from
the upper oil reservoir chamber R to the lower oil chamber S2.
Table 2: Example of JA?EN translation with better translation quality than baselines.
correspond to the highest performing official
baselines for the NTCIR-10 PatentMT task.
As it appeared Moses was giving similar
and slightly higher BLEU scores than Moses-
Hiero for Japanese-English, we restricted eval-
uation to the standard settings for Moses for
our Japanese-Chinese experiments.
The following dependency parsers were
used. The scores in parentheses are the ap-
proximate parsing accuracies (micro-average),
which were evaluated by hand on a random
subset of sentences from the test data. The
parsers were trained on domains different to
those used in the experiments.
? English: NLParser6 (92%) (Charniak and
Johnson, 2005)
? Japanese: KNP (96%) (Kawahara and
Kurohashi, 2006)
? Chinese: SKP (88%) (Shen et al., 2012)
6.1 Results
The results shown are for evaluation on the
test set after tuning. Tuning was conducted
over 50 iterations on the development set using
an n-best list of length 500.
Table 2 shows an example sentence showing
significant improvement over the baseline. In
6Converted to dependency parses with in-house
tool.
particular, non-local structure has been pre-
served by the proposed system, such as the
modification of ?oil? by the ?in an amount... by
the stroke? phrase. Another example is the in-
correct location of ?? stroke? in the Moses out-
put. The proposed system produces a much
more fluent output than the hierarchical-based
baseline Moses-Hiero.
The proposed system also outperforms the
baseline for JA?ZH, however falls short for
ZH?JA. We believe this is due to the low qual-
ity of parsing for Chinese input.
The decoder requires on average 0.94 sec-
onds per sentence when loading from precom-
piled hypothesis files. As a comparison, Moses
(default settings) takes 1.78 seconds per sen-
tence, loading from a binarized and filtered
phrase table.
7 Conclusion
This paper introduces an example-based
translation system exploiting both source and
target dependency analysis and online exam-
ple retrieving, allowing the availability of full
translation examples at translation time.
We believe that the use of dependency pars-
ing is important for accurate translation across
distant language pairs, especially in settings
such as ours with many long sentences. We
have designed a complete translation frame-
83
work around this idea, using dependency-
parsed trees at each step from alignment to
example retrieval to example combination.
The current performance (BLEU) of our
system is similar to (or even slightly bet-
ter than) state-of-the-art open-source SMT
systems. As we have been able to obtain
steady performance improvements during de-
velopment, we are hopeful that this trend will
continue and we will shortly obtain even bet-
ter results. Future plans include enriching
the feature set, adding a tree-based language
model and considering forest input for multi-
ple parses to provide robustness against pars-
ing errors. When the code base is sufficiently
stable, we intend to release the entire system
as open-source, in the hope of providing a
more syntactically-focused alternative to ex-
isting open-source SMT engines.
Acknowledgements
This work was partially supported by the
Japan Science and Technology Agency. The
first author is supported by a Japanese Gov-
ernment (MEXT) research scholarship. We
would like to thank the anonymous reviewers.
References
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Dis-
criminative Reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, ACL 2005.
Fabien Cromi?res and Sadao Kurohashi. 2011. Ef-
ficient retrieval of tree translation examples for
syntax-based machine translation. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita
and Benjamin Tsou. 2013. Overview of
the Patent Machine Translation Task at the
NTCIR-10 Workshop. In Proceedings of the 10th
NTCIR Workshop Meeting on Evaluation of In-
formation Access Technologies (NTCIR-10).
Mark Hopkins and Jonathan May. 2011. Tuning
as Ranking. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing.
Daisuke Kawahara and Sadao Kurohashi. 2006.
A Fully-Lexicalized Probabilistic Model for
Japanese Syntactic and Case Structure Anal-
ysis. In Proceedings of the Human Language
Technology Conference of the NAACL.
Makoto Nagao. 1984. A framework of a mechan-
ical translation between Japanese and English
by analogy principle. In A. Elithorn and R.
Banerji. Artificial and Human Intelligence.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by bilingual generation and mono-
lingual derivation. In Proceedings of COLING
2012.
Mo Shen, Daisuke Kawahara and Sadao Kuro-
hashi. 2012. A Reranking Approach for De-
pendency Parsing with Variable-sized Subtree
Features. In Proceedings of 26th Pacific Asia
Conference on Language Information and Com-
puting.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. Scaling phrase-based statistical ma-
chine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 255?262. Association for Compu-
tational Linguistics, 2005.
David Chiang. 2007. Hierarchical phrase-based
translation. In Computational Linguistics.
Kenneth Heafield. 2011. KenLM: faster and
smaller language model queries. In Proceedings
of the EMNLP 2011 Sixth Workshop on Statis-
tical Machine Translation, 2011.
Kenneth Heafield, Hieu Hoang, Philipp Koehn,
Tetsuo Kiso, and Marcello Federico. 2011.
Left language model state for syntactic ma-
chine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Trans-
lation, 2011.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-
efficient storage. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natu-
ral Language Learning, 2012.
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine transla-
tion with equivalent language model state main-
tenance. In Proceedings of the Second Workshop
on Syntax and Structure in Statistical Transla-
tion. Association for Computational Linguistics,
2008.
Adam Lopez. 2007. Hierarchical phrase-based
translation with suffix arrays. In EMNLP-
CoNLL 2007.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency Treelet Translation: Syn-
tactically Informed Phrasal SMT. In Proceed-
ings of the 43rd Annual Meeting on Association
for Computational Linguistics. Association for
Computational Linguistics, 2005.
84
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34?42,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Chinese?Japanese Parallel Sentence Extraction
from Quasi?Comparable Corpora
Chenhui Chu, Toshiaki Nakazawa, Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
{chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
Parallel sentences are crucial for statistical
machine translation (SMT). However, they
are quite scarce for most language pairs,
such as Chinese?Japanese. Many studies
have been conducted on extracting parallel
sentences from noisy parallel or compara-
ble corpora. We extract Chinese?Japanese
parallel sentences from quasi?comparable
corpora, which are available in far larger
quantities. The task is significantly more
difficult than the extraction from noisy
parallel or comparable corpora. We ex-
tend a previous study that treats parallel
sentence identification as a binary classifi-
cation problem. Previous method of clas-
sifier training by the Cartesian product is
not practical, because it differs from the
real process of parallel sentence extrac-
tion. We propose a novel classifier train-
ing method that simulates the real sentence
extraction process. Furthermore, we use
linguistic knowledge of Chinese character
features. Experimental results on quasi?
comparable corpora indicate that our pro-
posed approach performs significantly bet-
ter than the previous study.
1 Introduction
In statistical machine translation (SMT) (Brown
et al, 1993; Koehn et al, 2007), the quality
and quantity of the parallel sentences are cru-
cial, because translation knowledge is acquired
from a sentence?level aligned parallel corpus.
However, except for a few language pairs, such
as English?French, English?Arabic and English?
Chinese, parallel corpora remain a scarce re-
source. The cost of manual construction for paral-
lel corpora is high. As non?parallel corpora are far
more available, constructing parallel corpora from
non?parallel corpora is an attractive research field.
Non?parallel corpora include various levels of
comparability: noisy parallel, comparable and
quasi?comparable. Noisy parallel corpora con-
tain non?aligned sentences that are nevertheless
mostly bilingual translations of the same docu-
ment, comparable corpora contain non?sentence?
aligned, non?translated bilingual documents that
are topic?aligned, while quasi?comparable cor-
pora contain far more disparate very?non?parallel
bilingual documents that could either be on the
same topic (in?topic) or not (out?topic) (Fung and
Cheung, 2004). Most studies focus on extracting
parallel sentences from noisy parallel corpora or
comparable corpora, such as bilingual news ar-
ticles (Zhao and Vogel, 2002; Utiyama and Isa-
hara, 2003; Munteanu andMarcu, 2005; Tillmann,
2009; Abdul-Rauf and Schwenk, 2011), patent
data (Utiyama and Isahara, 2007; Lu et al, 2010)
and Wikipedia (Adafre and de Rijke, 2006; Smith
et al, 2010). Few studies have been conducted
on quasi?comparable corpora. Quasi?comparable
corpora are available in far larger quantities than
noisy parallel or comparable corpora, while the
parallel sentence extraction task is significantly
more difficult.
While most studies are interested in language
pairs between English and other languages, we
focus on Chinese?Japanese, where parallel cor-
pora are very scarce. This study extracts
Chinese?Japanese parallel sentences from quasi?
comparable corpora. We adopt a system pro-
posed by Munteanu and Marcu (2005), which is
for parallel sentence extraction from comparable
corpora. We extend the system in several aspects
to make it even suitable for quasi?comparable cor-
pora. The core component of the system is a clas-
sifier which can identify parallel sentences from
non?parallel sentences. Previous method of clas-
sifier training by the Cartesian product is not prac-
tical, because it differs from the real process of
parallel sentence extraction. We propose a novel
34
Translated Ja
sentences as 
queriesSMT
IR: top N results
Common
Chinese
characters
Candidate 
sentence 
pairs
Parallel
sentencesFiltering 
Chinese 
corpora
Japanese 
corpora
ClassifierProbabilisticdictionary
(2)
(1)
(3)
(4)
Zh-Ja parallel 
corpus
5k sentences
Figure 1: Parallel sentence extraction system.
method of classifier training and testing that sim-
ulates the real sentence extraction process, which
guarantees the quality of the extracted sentences.
Since Chinese characters are used both in Chi-
nese and Japanese, they can be powerful linguistic
clues to identify parallel sentences. Therefore, we
use Chinese character features, which significantly
improve the accuracy of the classifier. We con-
duct parallel sentence extraction experiments on
quasi?comparable corpora, and evaluate the qual-
ity of the extracted sentences from the perspective
of MT performance. Experimental results show
that our proposed system performs significantly
better than the previous study.
2 Parallel Sentence Extraction System
The overview of our parallel sentence extraction
system is presented in Figure 1. Source sentences
are translated to target language using a SMT sys-
tem (1). We retrieve the top N documents from tar-
get language corpora with a information retrieval
(IR) framework, using the translated sentences as
queries (2). For each source sentence, we treat
all target sentences in the retrieved documents as
candidates. Then, we pass the candidate sentence
pairs through a sentence ratio filter and a word?
overlap?based filter based on a probabilistic dic-
tionary, to reduce the candidates keeping more re-
liable sentences (3). Finally, a classifier trained on
a small number of parallel sentences, is used to
identify the parallel sentences from the candidates
(4). A parallel corpus is needed to train the SMT
system, generate the probabilistic dictionary and
train the classifier.
Our system is inspired by Munteanu and Marcu
(2005), however, there are several differences. The
first difference is query generation. Munteanu and
Marcu (2005) generate queries by taking the top
N translations of each source word according to
the probabilistic dictionary. This method is im-
precise due to the noise in the dictionary. In-
stead, we adopt a method proposed by Abdul?
Rauf and Schwenk (2011). We translate the source
sentences to target language with a SMT system
trained on the parallel corpus. Then use the trans-
lated sentences as queries. This method can gen-
erate more precise queries, because phrase?based
MT is better than word?based translation.
Another difference is that we do not conduct
document matching. The reason is that docu-
ments on the same topic may not exist in quasi?
comparable corpora. Instead, we retrieve the top
N documents for each source sentence. In com-
parable corpora, it is reasonable to only use the
best target sentence in the retrieved documents as
candidates (Abdul-Rauf and Schwenk, 2011). In
quasi?comparable corpora, it is important to fur-
ther guarantee the recall. Therefore, we keep all
target sentences in the retrieved documents as can-
didates.
Our system also differs by the way of classi-
fier training and testing, which is described in Sec-
tion 3 in detail.
3 Binary Classification of Parallel
Sentence Identification
Parallel sentence identification from non?parallel
sentences can be seen as a binary classification
problem (Munteanu and Marcu, 2005; Tillmann,
2009; Smith et al, 2010; S?tefa?nescu et al, 2012).
35
Since the quality of the extracted sentences is de-
termined by the accuracy of the classifier, the clas-
sifier becomes the core component of the extrac-
tion system. In this section, we first describe the
training and testing process, then introduce the
features we use for the classifier.
3.1 Training and Testing
Munteanu and Marcu (2005) propose a method of
creating training and test instances for the classi-
fier. They use a small number of parallel sentences
as positive instances, and generate non?parallel
sentences from the parallel sentences as negative
instances. They generate all the sentence pairs
except the original parallel sentence pairs in the
Cartesian product, and discard the pairs that do not
fulfill the condition of a sentence ratio filter and a
word?overlap?based filter. Furthermore, they ran-
domly discard some of the non?parallel sentences
when necessary, to guarantee the ratio of negative
to positive instances smaller than five for the per-
formance of the classifier.
Creating instances by using the Cartesian prod-
uct is not practical, because it differs from the real
process of parallel sentence extraction. Here, we
propose a novel method of classifier training and
testing that simulates the real parallel sentence ex-
traction process. For training, we first select 5k
parallel sentences from a parallel corpus. Then
translate the source side of the selected sentences
to target language with a SMT system trained on
the parallel corpus excluding the selected parallel
sentences. We retrieve the top N documents from
the target language side of the parallel corpus, us-
ing the translated sentences as queries. For each
source sentence, we consider all target sentences
in the retrieved documents as candidates. Finally,
we pass the candidate sentence pairs through a
sentence ratio filter and a word?overlap?based fil-
ter, and get the training instances. We treat the
sentence pairs that exist in the original 5k parallel
sentences as positive instances, while the remain-
der as negative instances. Note that positive in-
stances may be less than 5k, because some of the
parallel sentences do not pass the IR framework
and the filters. For the negative instances, we also
randomly discard some of them when necessary,
to guarantee the ratio of negative to positive in-
stances smaller than five. Test instances are gen-
erated by another 5k parallel sentences from the
parallel corpus using the same method.
There are several merits of the proposed
method. It can guarantee the quality of the ex-
tracted sentences, because of the similarity be-
tween the real sentence extraction process. Also,
features from the IR results can be used to further
improve the accuracy of the classifier. The pro-
posed method can be evaluated not only on the
test sentences that passed the IR framework and
the filters, but also on all the test sentences, which
is similar to the evaluation for the real extraction
process. However, there is a limitation of our
method that a both sentence?level and document?
level aligned parallel corpus is needed.
3.2 Features
3.2.1 Basic Features
The following features are the basic features we
use for the classifier, which are proposed by
Munteanu and Marcu (2005):
? Sentence length, length difference and length
ratio.
? Percentage of words on each side that have a
translation on the other side (according to the
probabilistic dictionary).
? Alignment features:
? Percentage and number of words that
have no connection.
? The top three largest fertilities.
? Length of the longest contiguous con-
nected span.
? Length of the longest unconnected sub-
string.
Alignment features are extracted from the align-
ment results of the parallel and non?parallel sen-
tences used as instances for the classifier. Note
that alignment features may be unreliable when
the quantity of non?parallel sentences is signifi-
cantly larger than parallel sentences.
3.2.2 Chinese Character Features
Different from other language pairs, Chinese and
Japanese share Chinese characters. In Chinese
the Chinese characters are called Hanzi, while in
Japanese they are called Kanji. Hanzi can be di-
vided into two groups, Simplified Chinese (used
in mainland China and Singapore) and Traditional
Chinese (used in Taiwan, Hong Kong and Macau).
The number of strokes needed to write characters
36
????????????????????
????????????????????????????????
Wash ether phase with saturated saline,  and dry it with anhydrous magnesium.
Zh:
Ja:
Ref:
Figure 2: Example of common Chinese characters in a Chinese?Japanese parallel sentence pair.
Meaning snow love begin
TC ? (U+96EA) ? (U+611B) ? (U+767C)
SC ? (U+96EA) ?(U+7231) ?(U+53D1)
Kanji ? (U+96EA) ? (U+611B) ? (U+767A)
Table 1: Examples of common Chinese characters
(TC denotes Traditional Chinese and SC denotes
Simplified Chinese).
has been largely reduced in Simplified Chinese,
and the shapes may be different from those in Tra-
ditional Chinese. Because Kanji characters origi-
nated from ancient China, many common Chinese
characters exist between Hanzi and Kanji. Table 1
gives some examples of common Chinese char-
acters in Traditional Chinese, Simplified Chinese
and Japanese with their Unicode.
Since Chinese characters contain significant se-
mantic information, and common Chinese charac-
ters share the same meaning, they can be valuable
linguistic clues for many Chinese?Japanese NLP
tasks. Many studies have exploited common Chi-
nese characters. Tan et al (1995) used the occur-
rence of identical common Chinese characters in
Chinese and Japanese (e.g. ?snow? in Table 1) in
automatic sentence alignment task for document?
level aligned text. Goh et al (2005) detected com-
mon Chinese characters where Kanji are identical
to Traditional Chinese, but different from Simpli-
fied Chinese (e.g. ?love? in Table 1). Using a Chi-
nese encoding converter1 that can convert Tradi-
tional Chinese into Simplified Chinese, they built
a Japanese?Simplified Chinese dictionary partly
using direct conversion of Japanese into Chinese
for Japanese Kanji words. Chu et al (2011) made
use of the Unihan database2 to detect common
Chinese characters which are visual variants of
each other (e.g. ?begin? in Table 1), and proved
the effectiveness of common Chinese characters
in Chinese?Japanese phrase alignment. Chu et
al. (2012a) exploited common Chinese charac-
ters in Chinese word segmentation optimization,
which improved the translation performance.
In this study, we exploit common Chinese char-
1http://www.mandarintools.com/zhcode.html
2http://unicode.org/charts/unihan.html
acters in parallel sentence extraction. Chu et
al. (2011) investigated the coverage of common
Chinese characters on a scientific paper abstract
parallel corpus, and showed that over 45% Chi-
nese Hanzi and 75% Japanese Kanji are common
Chinese characters. Therefore, common Chinese
characters can be powerful linguistic clues to iden-
tify parallel sentences.
We make use of the Chinese character map-
ping table created by Chu et al (2012b) to de-
tect common Chinese characters. Following fea-
tures are used. We use an example of Chinese?
Japanese parallel sentence presented in Figure 2 to
explain the features in detail, where common Chi-
nese characters are in bold and linked with dotted
lines.
? Number of Chinese characters on each side
(Zh: 18, Ja: 14).
? Percentage of Chinese characters out of all
characters on each side (Zh: 18/20=90%, Ja:
14/32=43%).
? Ratio of Chinese character numbers on both
sides (18/14=128%).
? Number of n?gram common Chinese charac-
ters (1?gram: 12, 2?gram: 6, 3?gram: 2, 4?
gram: 1).
? Percentage of n?gram common Chinese char-
acters out of all n?gram Chinese characters
on each side (Zh: 1?gram: 12/18=66%, 2?
gram: 6/16=37%, 3?gram: 2/14=14%, 4?
gram: 1/12=8%; Ja: 1?gram: 12/14=85%,
2?gram: 6/9=66%, 3?gram=: 2/5=40%, 4?
gram: 1/3=33%).
Note that Chinese character features are only
applicable to Chinese?Japanese. However, since
Chinese and Japanese character information is a
kind of cognates (words or languages which have
the same origin), the similar idea can be applied to
other language pairs by using cognates. Cognates
among European languages have been shown ef-
fective in word alignments (Kondrak et al, 2003).
We also can use cognates for parallel sentence ex-
traction.
37
3.3 Rank Feature
One merit of our classifier training and testing
method is that features from the IR results can be
used. Here, we use the ranks of the retrieved doc-
uments returned by the IR framework as feature.
4 Experiments
We conducted classification and translation exper-
iments to evaluate the effectiveness of our pro-
posed parallel sentence extraction system.
4.1 Data
4.1.1 Parallel Corpus
The parallel corpus we used is a scientific
paper abstract corpus provided by JST3 and
NICT4. This corpus was created by the Japanese
project ?Development and Research of Chinese?
Japanese Natural Language Processing Technol-
ogy?, containing various domains such as chem-
istry, physics, biology and agriculture etc. This
corpus is aligned in both sentence?level and
document?level, containing 680k sentences and
100k articles.
4.1.2 Quasi?Comparable Corpora
The quasi?comparable corpora we used are scien-
tific paper abstracts collected from academic web-
sites. The Chinese corpora were collected from
CNKI5, containing 420k sentences and 90k arti-
cles. The Japanese corpora were collected from
CiNii6 web portal, containing 5M sentences and
880k articles. Note that since the paper abstracts
in these two websites were written by Chinese and
Japanese researchers respectively through differ-
ent periods, documents on the same topic may not
exist in the collected corpora. We investigated
the domains of the Chinese and Japanese corpora
in detail. We found that most documents in the
Chinese corpora belong to the domain of chem-
istry. While the Japanese corpora contain various
domains such as chemistry, physics, biology and
computer science etc. However, the domain infor-
mation is unannotated in both corpora.
4.2 Classification Experiments
We conducted experiments to evaluate the accu-
racy of the proposed method of classification, us-
3http://www.jst.go.jp
4http://www.nict.go.jp
5http://www.cnki.net
6http://ci.nii.ac.jp
ing different 5k parallel sentences from the paral-
lel corpus as training and test data.
4.2.1 Settings
? Probabilistic dictionary: We took the top
5 translations with translation probability
larger than 0.1 created from the parallel cor-
pus.
? IR tool: Indri7 with the top 10 results.
? Segmenter: For Chinese, we used a
segmenter optimized for Chinese?Japanese
SMT (Chu et al, 2012a). For Japanese, we
used JUMAN (Kurohashi et al, 1994).
? Alignment: GIZA++8.
? SMT: We used the state?of?the?art phrase?
based SMT toolkit Moses (Koehn et al,
2007) with default options, except for the dis-
tortion limit (6?20).
? Classifier: LIBSVM9 with 5?fold cross?
validation and radial basis function (RBF)
kernel.
? Sentence ratio filter threshold: 2.
? Word?overlap?based filter threshold: 0.25.
? Classifier probability threshold: 0.5.
4.2.2 Evaluation
We evaluate the performance of classification by
computing precision, recall and F?value, defined
as:
precision = 100? classified well
classified parallel
, (1)
recall = 100? classified well
true parallel
, (2)
F ? value = 2? precision ? recall
precision + recall
. (3)
Where classified well is the number of pairs
that the classifier correctly identified as parallel,
classified parallel is the number of pairs that
the classifier identified as parallel, true parallel
is the number of real parallel pairs in the test set.
Note that we only use the top 1 result identified as
parallel by the classifier for evaluation.
7http://www.lemurproject.org/indri
8http://code.google.com/p/giza-pp
9http://www.csie.ntu.edu.tw/?cjlin/libsvm
38
Features Precision Recall F?value
Munteanu+ 2005 88.43 85.20/79.76 86.78/83.87
+Chinese character 91.62 93.63/87.66 92.61/89.60
+Rank 92.15 94.53/88.50 93.32/90.29
Table 2: Classification results for the filtered test
sentences (before ?/?) and all the test sentences
(after ?/?).
4.2.3 Results
We conducted classification experiments, compar-
ing the following three experimental settings:
? Munteanu+ 2005: Only using the features
proposed by Munteanu and Marcu (2005).
? +Chinese character: Add the Chinese charac-
ter features.
? +Rank: Further add the rank feature.
Results evaluated for the test sentences that
passed the IR framework and the filters, and all
the test sentences are shown in Table 2. We can
see that the Chinese character features can signifi-
cantly improve the accuracy. The accuracy can be
further improved by the rank feature.
4.3 Translation Experiments
We extracted parallel sentences from the quasi?
comparable corpora, and evaluated Chinese?to?
Japanese MT performance by appending the ex-
tracted sentences to two baseline settings.
4.3.1 Settings
? Baseline: Using all the 680k parallel sen-
tences in the parallel corpus as training data
(containing 11k sentences of chemistry do-
main).
? Tuning: Using another 368 sentences of
chemistry domain.
? Test: Using another 367 sentences of chem-
istry domain.
? Language model: 5?gram LM trained on the
Japanese side of the parallel corpus (680k
sentences) using SRILM toolkit10.
? Classifier probability threshold: 0.6.
10http://www.speech.sri.com/projects/srilm
Classifier # sentences
Munteanu+ 2005 (Cartesian) 27,077
Munteanu+ 2005 (Proposed) 5,994
+Chinese character (Proposed) 3,936
+Rank (Proposed) 3,516
Table 3: Number of extracted sentences.
The reason we evaluate on chemistry domain is
the one we described in Section 4.1.2 that most
documents in the Chinese corpora belong to the
domain of chemistry. We keep all the sentence
pairs rather than the top 1 result (used in the clas-
sification evaluation) identified as parallel by the
classifier. The other settings are the same as the
ones used in the classification experiments.
4.3.2 Results
Numbers of extracted sentences using different
classifiers are shown in Table 3, where
? Munteanu+ 2005 (Cartesian): Classifier
trained using the Cartesian product, and only
using the features proposed by Munteanu and
Marcu (2005).
? Munteanu+ 2005 (Proposed): Classifier
trained using the proposed method, and only
using the features proposed by Munteanu and
Marcu (2005).
? +Chinese character (Proposed): Add the Chi-
nese character features.
? +Rank (Proposed): Further add the rank fea-
ture.
We can see that the extracted number is signif-
icantly decreased by the proposed method com-
pared to the Cartesian product, which may indi-
cate the quality improvement of the extracted sen-
tences. Adding more features further decreases the
number.
We conducted Chinese?to?Japanese translation
experiments by appending the extracted sentences
to the baseline. BLEU?4 scores for experiments
are shown in Table 4. We can see that our proposed
method of classifier training performs better than
the Cartesian product. Adding the Chinese charac-
ter features and rank feature further improves the
translation performance significantly.
39
Example 1
Zh: ??????????????????
(Finally, this article explains the physical meaning of the optical operator.)
Ja: ?????????????????????????????
(Finally, briefly explain the physical meaning of the chemical potential.)
Example 2
Zh: ?????????????????????
(Discussion of detection limit and measurement methods of emission spectral  analysis method.)
Ja: ??????????????????????
(Detection limit of emission spectral analysis method by photoelectric photometry.)
Figure 3: Examples of extracted sentences (parallel subsentential fragments are in bold).
System BLEU
Baseline 38.64
Munteanu+ 2005 (Cartesian) 38.10
Munteanu+ 2005 (Proposed) 38.54
+Chinese character (Proposed) 38.87?
+Rank (Proposed) 39.47??
Table 4: BLEU scores for Chinese?to?Japanese
translation experiments (??? and ??? denotes the
result is better than ?Munteanu+ 2005 (Cartesian)?
significantly at p < 0.05 and p < 0.01 respec-
tively, ?*? denotes the result is better than ?Base-
line? significantly at p < 0.01).
4.3.3 Discussion
The translation results indicate that compared to
the previous study, our proposed method can ex-
tract sentences with better qualities. However,
when we investigated the extracted sentences, we
found that most of the extracted sentences are
not sentence?level parallel. Instead, they contain
many parallel subsentential fragments. Figure 3
presents two examples of sentence pairs extracted
by ?+Rank (Proposed)?, where parallel subsenten-
tial fragments are in bold. We investigated the
alignment results of the extracted sentences. We
found that most of the parallel subsentential frag-
ments were correctly aligned with the help of the
parallel sentences in the baseline system. There-
fore, translation performance was improved by ap-
pending the extracted sentences. However, it also
led to many wrong alignments among the non?
parallel fragments which are harmful to transla-
tion. In the future, we plan to further extract
these parallel subsentential fragments, which can
be more effective for SMT (Munteanu and Marcu,
2006).
5 Related Work
As parallel sentences trend to appear in similar
document pairs, many studies first conduct doc-
ument matching, then identify the parallel sen-
tences from the matched document pairs (Utiyama
and Isahara, 2003; Fung and Cheung, 2004;
Munteanu and Marcu, 2005). Approaches with-
out document matching also have been proposed
(Tillmann, 2009; Abdul-Rauf and Schwenk, 2011;
S?tefa?nescu et al, 2012). These studies directly re-
trieve candidate sentence pairs, and select the par-
allel sentences using some filtering methods. We
adopt a moderate strategy, which retrieves candi-
date documents for sentences.
The way of parallel sentence identification can
be specified with two different approaches: bi-
nary classification (Munteanu and Marcu, 2005;
Tillmann, 2009; Smith et al, 2010; S?tefa?nescu
et al, 2012) and translation similarity measures
(Utiyama and Isahara, 2003; Fung and Cheung,
2004; Abdul-Rauf and Schwenk, 2011). We adopt
the binary classification approach with a novel
classifier training and testing method and Chinese
character features.
Few studies have been conducted for extract-
ing parallel sentences from quasi?comparable cor-
pora. We are aware of only two previous efforts.
Fung and Cheung (2004) proposed a multi-level
bootstrapping approach. Wu and Fung (2005) ex-
ploited generic bracketing Inversion Transduction
Grammars (ITG) for this task. Our approach dif-
fers from the previous studies that we extend the
approach for comparable corpora in several as-
pects to make it work well for quasi?comparable
corpora.
6 Conclusion and Future Work
In this paper, we proposed a novel method of clas-
sifier training and testing that simulates the real
parallel sentence extraction process. Furthermore,
we used linguistic knowledge of Chinese charac-
ter features. Experimental results of parallel sen-
tence extraction from quasi?comparable corpora
indicated that our proposed system performs sig-
nificantly better than the previous study.
40
Our approach can be improved in several as-
pects. One is bootstrapping, which has been
proven effective in some related works (Fung and
Cheung, 2004; Munteanu and Marcu, 2005). In
our system, bootstrapping can be done not only
for extension of the probabilistic dictionary, but
also for improvement of the SMT system used to
translate the source language to target language for
query generation. Moreover, as parallel sentences
rarely exist in quasi?comparable corpora, we plan
to extend our system to parallel subsentential frag-
ment extraction. Our study showed that Chi-
nese character features are helpful for Chinese?
Japanese parallel sentence extraction. We plan to
apply the similar idea to other language pairs by
using cognates.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Par-
allel sentence generation from comparable corpora
for improved smt. Machine Translation, 25(4):341?
375.
Sisay Fissaha Adafre and Maarten de Rijke. 2006.
Finding similar sentences across multiple languages
in wikipedia. In Proceedings of EACL, pages 62?69.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Association for Computational
Linguistics, 19(2):263?312.
Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2011. Japanese-chinese phrase alignment
using common chinese characters information. In
Proceedings of MT Summit XIII, pages 475?482, Xi-
amen, China, September.
Chenhui Chu, Toshiaki Nakazawa, Daisuke Kawahara,
and Sadao Kurohashi. 2012a. Exploiting shared
Chinese characters in Chinese word segmentation
optimization for Chinese-Japanese machine transla-
tion. In Proceedings of the 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT?12), Trento, Italy, May.
Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2012b. Chinese characters mapping table of
Japanese, Traditional Chinese and Simplified Chi-
nese. In Proceedings of the Eighth Conference on
International Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, May.
Dan S?tefa?nescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from com-
parable corpora. In Proceedings of the 16th Annual
Conference of the European Association for Ma-
chine Translation (EAMT?12), Trento, Italy, May.
Pascale Fung and Percy Cheung. 2004. Multi-level
bootstrapping for extracting parallel sentences from
a quasi-comparable corpus. In Proceedings of Col-
ing 2004, pages 1051?1057, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Building a Japanese-Chinese dic-
tionary using kanji/hanzi conversion. In Proceed-
ings of the International Joint Conference on Natu-
ral Language Processing, pages 670?681.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical transla-
tion models. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 46?48.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22?28.
Bin Lu, Tao Jiang, Kapo Chow, and Benjamin K. Tsou.
2010. Building a large english-chinese parallel cor-
pus from comparable patents and its experimental
application to smt. In Proceedings of the 3rd Work-
shop on Building and Using Comparable Corpora,
LREC 2010, pages 42?49, Valletta, Malta, May.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504, December.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81?88, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
403?411, Los Angeles, California, June. Associa-
tion for Computational Linguistics.
41
Chew Lim Tan and Makoto Nagao. 1995. Automatic
alignment of Japanese-Chinese bilingual texts. IE-
ICE Transactions on Information and Systems, E78-
D(1):68?76.
Christoph Tillmann. 2009. A beam-search extrac-
tion algorithm for comparable data. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 225?228, Suntec, Singapore, August. Associ-
ation for Computational Linguistics.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning japanese-english news articles
and sentences. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 72?79, Sapporo, Japan, July. Associ-
ation for Computational Linguistics.
Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT summit XI, pages 475?482.
Dekai Wu and Pascale Fung. 2005. Inversion trans-
duction grammar constraints for mining parallel sen-
tences from quasi-comparable corpora. In IJCNLP,
pages 257?268.
Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web abilingual news col-
lections. In Proceedings of the 2002 IEEE Interna-
tional Conference on Data Mining, pages 745?748,
Maebashi City, Japan. IEEE Computer Society.
42
