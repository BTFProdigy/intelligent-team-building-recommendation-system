Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 49?52,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Archivus: A multimodal system for multimedia meeting browsing and
retrieval
Marita Ailomaa, Miroslav Melichar,
Martin Rajman
Artificial Intelligence Laboratory
?Ecole Polytechnique Fe?de?rale de Lausanne
CH-1015 Lausanne, Switzerland
marita.ailomaa@epfl.ch
Agnes Lisowska,
Susan Armstrong
ISSCO/TIM/ETI
University of Geneva
CH-1211 Geneva, Switzerland
agnes.lisowska@issco.unige.ch
Abstract
This paper presents Archivus, a multi-
modal language-enabled meeting brows-
ing and retrieval system. The prototype
is in an early stage of development, and
we are currently exploring the role of nat-
ural language for interacting in this rela-
tively unfamiliar and complex domain. We
briefly describe the design and implemen-
tation status of the system, and then focus
on how this system is used to elicit useful
data for supporting hypotheses about mul-
timodal interaction in the domain of meet-
ing retrieval and for developing NLP mod-
ules for this specific domain.
1 Introduction
In the past few years, there has been an increasing
interest in research on developing systems for effi-
cient recording of and access to multimedia meet-
ing data1. This work often results in videos of
meetings, transcripts, electronic copies of docu-
ments referenced, as well as annotations of various
kinds on this data. In order to exploit this work, a
user needs to have an interface that allows them to
retrieve and browse the multimedia meeting data
easily and efficiently.
In our work we have developed a multimodal
(voice, keyboard, mouse/pen) meeting browser,
Archivus, whose purpose is to allow users to ac-
cess multimedia meeting data in a way that is most
natural to them. We believe that since this is a new
domain of interaction, users can be encouraged to
1The IM2 project http://www.im2.ch, the AMI project
www.amiproject.org, The Meeting Room Project at Carnegie
Mellon University, http://www.is.cs.cmu.edu/mie, and rich
transcription of natural and impromptu meetings at ICSI,
Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html
try out and consistently use novel input modalities
such as voice, including more complex natural lan-
guage, and that in particular in this domain, such
multimodal interaction can help the user find in-
formation more efficiently.
When developing a language interface for an in-
teractive system in a new domain, the Wizard of
Oz (WOz) methodology (Dahlba?ck et al, 1993;
Salber and Coutaz, 1993) is a very useful tool.
The user interacts with what they believe to be a
fully automated system, when in fact another per-
son, a ?wizard? is simulating the missing or incom-
plete NLP modules, typically the speech recogni-
tion, natural language understanding and dialogue
management modules. The recorded experiments
provide valuable information for implementing or
fine-tuning these parts of the system.
However, the methodology is usually applied
to unimodal (voice-only or keyboard-only) sys-
tems, where the elicitation of language data is not
a problem since this is effectively the only type of
data resulting from the experiment. In our case, we
are developing a complex multimodal system. We
found that when the Wizard of Oz methodology
is extended to multimodal systems, the number of
variables that have to be considered and controlled
for in the experiment increases substantially. For
instance, if it is the case that within a single inter-
face any task that can be performed using natural
language can also be performed with other modal-
ities, for example a mouse, the user may prefer
to use the other ? more familiar ? modality for
a sizeable portion of the experiment. In order to
gather a useful amount of natural language data,
greater care has to be taken to design the system
in a way that encourages language use. But, if
the goal of the experiment is also to study what
modalities users find more useful in some situa-
49
Figure 1: The Archivus Interface
tions compared to others, language use must be
encouraged without being forced, and finding this
balance can be very hard to achieve in practice.
2 Design and implementation
The Archivus system has been designed to sat-
isfy realistic user needs based on a user require-
ment analysis (Lisowska, 2003), where subjects
were asked to formulate queries that would enable
them to find out ?what happened at a meeting?.
The design of the user interface is based on the
metaphor of a person interacting in an archive or
library (Lisowska et al, 2004).
Furthermore, Archivus is flexibly multimodal,
meaning that users can interact unimodally choos-
ing one of the available modalities exclusively,
or multimodally, using any combination of the
modalities. In order to encourage natural lan-
guage interaction, the system gives textual and vo-
cal feedback to the user. The Archivus Interface
is shown in Figure 1. A detailed description of all
of the components can be found in Lisowska et al
(2004).
Archivus was implemented within a software
framework for designing multimodal applications
with mixed-initiative dialogue models (Cenek et
al., 2005). Systems designed within this frame-
work handle interaction with the user through
a multimodal dialogue manager. The dialogue
manager receives user input from all modalities
(speech, typing and pointing) and provides mul-
timodal responses in the form of graphical, textual
and vocal feedback.
The dialogue manager contains only linguistic
knowledge and interaction algorithms. Domain
knowledge is stored in an SQL database and is ac-
cessed by the dialogue manager based on the con-
straints expressed by the user during interaction.
The above software framework provides sup-
port for remote simulation or supervision of
some of the application functionalities. This fea-
ture makes any application developed under this
methodology well suited for WOz experiments. In
the case of Archivus, pilot experiments strongly
suggested the use of two wizards ? one supervising
the user?s input (Input Wizard) and the other con-
trolling the natural language output of the system
(Output Wizard). Both wizards see the user?s in-
put, but their actions are sequential, with the Out-
put Wizard being constrained by the actions of the
Input Wizard.
The role of the Input Wizard is to assure that
the user?s input (in any modality combination)
is correctly conveyed to the system in the form
of sets of semantic pairs. A semantic pair (SP)
is a qualified piece of information that the dia-
50
logue system is able to understand. For exam-
ple, a system could understand semantic pairs such
as date:Monday or list:next. A user?s
utterance ?What questions did this guy ask in
the meeting yesterday?? combined with point-
ing on the screen at a person called ?Raymond?
could translate to dialogact:Question,
speaker:Raymond, day:Monday.
In the current version of Archivus, user clicks
are translated into semantic pairs automatically by
the system. Where written queries are concerned,
the wizard sometimes needs to correct automat-
ically generated pairs due to the currently low
performance of our natural language understand-
ing module. Finally since the speech recognition
engine has not been implemented yet, the user?s
speech is fully processed by a wizard. The Input
Wizard also assures that the fusion of pairs coming
from different modalities is done correctly.
The role of the Output Wizard is to monitor, and
if necessary change the default prompts that are
generated by the system. Changes are made for
example to smooth the dialogue flow, i.e. to bet-
ter explain the dialogue situation to the user or to
make the response more conversational. The wiz-
ard can select a prompt from a predefined list, or
type a new one during interaction.
All wizards? actions are logged and afterwards
used to help automate the correct behavior of the
system and to increase the overall performance.
3 Collecting natural language data
In order to obtain a sufficient amount of language
data from the WOz experiments, several means
have been used to determine what encourages
users to speak to the system. These include giving
users different types of documentation before the
experiment ? lists of possible voice commands, a
user manual, and step-by-step tutorials. We found
that the best solution was to give users a tutorial
in which they worked through an example using
voice alone or in combination with other modali-
ties, explaining in each step the consequences of
the user?s actions on the system. The drawback of
this approach is that the user may be biased by the
examples and continue to interact according to the
interaction patterns that are provided, rather than
developing their own patterns. These influences
need to be considered both in the data analysis,
and in how the tutorials are written and structured.
The actual experiment consists of two parts in
which the user gets a mixed set of short-answer
and true-false questions to solve using the system.
First they are only allowed to use a subset of the
available modalities, e.g. voice and pen, and then
the full set of modalities. By giving the users dif-
ferent subsets in the first part, we can compare if
the enforcement of certain modalities has an im-
pact on how they choose to use language when all
modalities are available.
On the backend, the wizards can also to some
extent have an active role in encouraging language
use. The Input Wizard is rather constrained in
terms of what semantic pairs he can produce, be-
cause he is committed to selecting from a set of
pairs that are extracted from the meeting data.
For example if ?Monday? is not a meeting date
in the database, the input is interpreted as having
?no match?, which generates the system prompt
?I don?t understand?. Here, the Output Wizard
can intervene by replacing that prompt by one that
more precisely specifies the nature of the problem.
The Output Wizard can also decide to replace
default prompts in situations when they are too
general in a given context. For instance, when
the user is browsing different sections of a meeting
book (cover page, table of contents, transcript and
referenced documents) the default prompt gives
general advice on how to access the different parts
of the book, but can be changed to suggest a spe-
cific section instead.
4 Analysis of elicited language data
The data collected with Archivus through WOz
experiments provide useful information in several
ways. One aspect is to see the complexity of the
language used by users ? for instance whether they
use more keywords, multi-word expressions or
full-sentence queries. This is important for choos-
ing the appropriate level of language processing,
for instance for the syntactic analysis. Another as-
pect is to see the types of actions performed us-
ing language. On one hand, users can manipulate
elements in the graphical interface by expressing
commands that are semantically equivalent with
pointing, e.g. ?next page?. On the other hand,
they can freely formulate queries relating to the
information they are looking for, e.g. ?Did they
decide to put a sofa in the lounge??. Commands
are interface specific rather than domain specific.
From the graphical interface the user can easily
predict what they can say and how the system will
51
Part 1 condition Pointing Language
Experiment set 1
voice only 91% 9%
voice+keyboard 88% 12%
keyboard+pointing 66% 34%
voice+keyb.+pointing 79% 21%
Experiment set 2
voice only 68% 32%
voice+pointing 62% 38%
keyboard+pointing 39% 61%
pointing 76% 24%
Table 1: Use of each modality in part 2.
respond. Queries depend on the domain and the
data, and are more problematic for the user be-
cause they cannot immediately see what types of
queries they can ask and what the coverage of
the data is. But, using queries can be very use-
ful, because it allows the user to express them-
selves in their own terms. An important goal of the
data analysis is to determine if the language inter-
face enables the user to interact more successfully
than if they are limited to pointing only. In addi-
tion, the way in which the users use language in
these two dimensions has important implications
for the dialogue strategy and for the implementa-
tion of the language processing modules, for in-
stance the speech recognition engine. A speech
recognizer can be very accurate when trained on a
small, fixed set of commands whereas it may per-
form poorly when faced with a wide variety of lan-
guage queries.
Thus far, we have performed 3 sets of pilot
WOz experiments with 40 participants. The pri-
mary aim was to improve and finetune the system
and the WOz environment as a preparation for the
data-collection experiments that we plan to do in
the future. In these experiments we compared how
frequently users used voice and keyboard in rela-
tion to pointing as we progressively changed fea-
tures in the system and the experimental setup to
encourage language use. The results between the
first and the third set of experiments can be seen
in table 1, grouped by the subset of modalities that
the users had in the first part of the experiment.
From the table we can see that changes made
between the different iterations of the system
achieved their goal ? by the third experiment set
we were managing to elicit larger amounts of nat-
ural language data. Moreover, we noticed that the
modality conditions that are available to the user
in the first part play a role in the amount of use of
language modalities in the second part.
5 Conclusions and future work
We believe that the work presented here (both the
system and the WOz environment and experimen-
tal protocol) has now reached a stable stage that
allows for the elicitation of sufficient amounts of
natural language and interaction data. The next
step will be to run a large-scale data collection.
The results from this collection should provide
enough information to allow us to develop and in-
tegrate fairly robust natural language processing
into the system. Ideally, some of the components
used in the software framework will be made pub-
licly available at the end of the project.
References
Pavel Cenek, Miroslav Melichar, and Martin Rajman.
2005. A Framework for Rapid Multimodal Appli-
cation Design. In Va?clav Matous?ek, Pavel Mautner,
and Toma?s? Pavelka, editors, Proceedings of the 8th
International Conference on Text, Speech and Dia-
logue (TSD 2005), volume 3658 of Lecture Notes
in Computer Science, pages 393?403, Karlovy Vary,
Czech Republic, September 12-15. Springer.
Nils Dahlba?ck, Arne Jo?nsson, and Lars Ahrenberg.
1993. Wizard of Oz Studies ? Why and How. In
Dianne Murray Wayne D. Gray, William Hefley, ed-
itor, International Workshop on Intelligent User In-
terfaces 1993, pages 193?200. ACM Press.
Agnes Lisowska, Martin Rajman, and Trung H. Bui.
2004. ARCHIVUS: A System for Accessing the
Content of Recorded Multimodal Meetings. In
In Procedings of the JOINT AMI/PASCAL/IM2/M4
Workshop on Multimodal Interaction and Related
Machine Learning Algorithms, Bourlard H. & Ben-
gio S., eds. (2004), LNCS, Springer-Verlag, Berlin.,
Martigny, Switzerland, June.
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary in-
dications from a query analysis study. Project re-
port IM2.MDM-11, University of Geneva, Geneva,
Switzerland, November.
Daniel Salber and Joe?lle Coutaz. 1993. Applying
the wizard of oz technique to the study of multi-
modal systems. In EWHCI ?93: Selected papers
from the Third International Conference on Human-
Computer Interaction, pages 219?230, London, UK.
Springer-Verlag.
52
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 9?16
Manchester, August 2008
Making Speech Look Like Text
in the Regulus Development Environment
Elisabeth Kron
3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
Manny Rayner, Marianne Santaholma, Pierrette Bouillon, Agnes Lisowska
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
Agnes.Lisowska@issco.unige.ch
Abstract
We present an overview of the de-
velopment environment for Regulus, an
Open Source platform for construction of
grammar-based speech-enabled systems,
focussing on recent work whose goal has
been to introduce uniformity between text
and speech views of Regulus-based appli-
cations. We argue the advantages of be-
ing able to switch quickly between text and
speech modalities in interactive and offline
testing, and describe how the new func-
tionalities enable rapid prototyping of spo-
ken dialogue systems and speech transla-
tors.
1 Introduction
Sex is not love, as Madonna points out at the be-
ginning of her 1992 book Sex, and love is not
sex. None the less, even people who agree with
Madonna often find it convenient to pretend that
these two concepts are synonymous, or at least
closely related. Similarly, although text is not
speech, and speech is not text, it is often conve-
nient to pretend that they are both just different as-
pects of the same thing.
In this paper, we will explore the similarities and
differences between text and speech, in the con-
crete setting of Regulus, a development environ-
ment for grammar based spoken dialogue systems.
Our basic goal will be to make text and speech
processing as similar as possible from the point
of view of the developer. Specifically, we arrange
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
things so that the developer is able to develop her
system using a text view; she will write text-based
rules, and initially test the system using text in-
put and output. At any point, she will be able to
switch to a speech view, compiling the text-based
processing rules into corresponding speech-based
versions, and test the resulting speech-based sys-
tem using speech input and output.
Paradoxically, the reason why it is so important
to be able to switch seamlessly between text and
speech viewpoints is that text and speech are in
fact not the same. For example, a pervasive prob-
lem in speech recognition is that of easily confus-
able pairs of words. This type of problem is of-
ten apparent after just a few minutes when running
the system in speech mode (the recogniser keeps
recognising one word as the other), but is invis-
ible in text mode. More subtly, some grammar
problems can be obvious in text mode, but hard
to see in speech mode. For instance, articles like
?the? and ?a? are short, and usually pronounced
unstressed, which means that recognisers can be
reasonably forgiving about whether or not to hy-
pothesise them when they are required or not re-
quired by the recognition grammar. In text mode, it
will immediately be clear if the grammar requires
an article in a given NP context: incorrect vari-
ants will fail to parse. In speech mode, the symp-
toms are far less obvious, and typically amount to
no more than a degradation in recognition perfor-
mance.
The rest of the paper is structured as follows.
Sections 2 and 3 provide background on the Reg-
ulus platform and development cycle respectively.
Section 4 describes speech and text support in the
interactive development environment, and 5 de-
scribes how the framework simplifies the task of
9
switching between modalities in regression testing.
Section 6 concludes.
2 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled
systems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Univer-
sity, and is freely available for download from the
SourceForge website1. In terms of ideas (though
not code), Regulus is a descendent of SRI Inter-
national?s CLE and Gemini platforms (Alshawi,
1992; Dowding et al, 1993); other related systems
are LKB (Copestake, 2002), XLE (Crouch et al,
2008) and UNIANCE (Bos, 2002).
Regulus has already been used to build sev-
eral large applications. Prominent examples
are Geneva University?s MedSLT medical speech
translator (Bouillon et al, 2005), NASA?s Clarissa
procedure browser (Rayner et al, 2005) and Ford
Research?s experimental SDS in-car spoken dia-
logue system, which was awarded first prize at
the 2007 Ford internal demo fair. Regulus is de-
scribed at length in (Rayner et al, 2006), the first
half of which consists of an extended tutorial in-
troduction. The release includes a command-line
development environment, extensive online docu-
mentation, and several example applications.
The core functionality offered by Regulus is
compilation of typed unification grammars into
parsers, generators, and Nuance-formatted CFG
language models, and hence also into Nuance
recognition packages. These recognition packages
produced by Regulus can be invoked through the
Regulus SpeechServer (?Regserver?), which pro-
vides an interface to the underlying Nuance recog-
nition engine. The value added by the Regserver
is to provide a view of the recognition process
based on the Regulus unification grammar frame-
work. In particular, recognition results, originally
produced in the Nuance recognition platform?s in-
ternal format, are reformatted into the semantic no-
tation used by the Regulus grammar formalism.
There is extensive support within the Regulus
toolkit for development of both speech translation
and spoken dialogue applications. Spoken dia-
1http://sourceforge.net/projects/
regulus/
logue applications (Rayner et al, 2006, Chapter 5)
use a rule-based side-effect free state update model
similar in spirit to that described in (Larsson and
Traum, 2000). Very briefly, there are three types of
rules: state update rules, input management rules,
and output management rules. State update rules
take as input the current state, and a ?dialogue
move?; they produce as output a new state, and an
?abstract action?. Dialogue moves are abstract rep-
resentations of system inputs; these inputs can ei-
ther be logical forms produced by the grammar, or
non-speech inputs (for example, mouse-clicks in a
GUI). Similarly, abstract actions are, as the name
suggests, abstract representations of the concrete
actions the dialogue system will perform, for ex-
ample speaking or updating a visual display. Input
management rules map system inputs to dialogue
moves; output management rules map abstract ac-
tions to system outputs.
Speech translation applications are also rule-
based, using an interlingua model (Rayner et al,
2006, Chapter 6). The developer writes a second
grammar for the target language, using Regulus
tools to compile it into a generator; mappings from
source representation to interlingua, and from in-
terlingua to target representation, are defined by
sets of translation rules. The interlingua itself is
specified using a third Regulus grammar (Bouillon
et al, 2008).
To summarise, the core of a Regulus application
consists of several different linguistically oriented
rule-sets, some of which can be interpreted in ei-
ther a text or a speech modality, and all of which
need to interact correctly together. In the next sec-
tion, we describe how this determines the nature of
the Regulus development cycle.
3 The Regulus development cycle
Small unification grammars can be compiled di-
rectly into executable forms. The central idea
of Regulus, however, is to base as much of
the development work as possible on large,
domain-independent, linguistically motivated re-
source grammars. A resource grammar for En-
glish is available from the Regulus website; similar
grammars for several other languages have been
developed under the MedSLT project at Geneva
University, and can be downloaded from the Med-
SLT SourceForge website2. Regulus contains
2http://sourceforge.net/projects/
medslt
10
an extensive set of tools that permit specialised
domain-specific grammars to be extracted from the
larger resource grammars, using example-based
methods driven by small corpora (Rayner et al,
2006, Chapter 7). At the beginning of a project,
these corpora can consist of just a few dozen exam-
ples; for a mature application, they will typically
have grown to something between a few hundred
and a couple of thousand sentences. Specialised
grammars can be compiled by Regulus into effi-
cient recognisers and generators.
As should be apparent from the preceding de-
scription, the Regulus architecture is designed to
empower linguists to the maximum possible ex-
tent, in terms of increasing their ability directly
to build speech enabled systems; the greater part
of the core development teams in the large Reg-
ulus projects mentioned in Section 1 have indeed
come from linguistics backgrounds. Experience
with Regulus has however shown that linguists are
not quite as autonomous as they are meant to be,
and in particular are reluctant to work directly with
the speech view of the application. There are sev-
eral reasons.
First, non-toy Regulus projects require a range
of competences, including both software engineer-
ing and linguistics. In practice, linguist rule-
writers have not been able to test their rules in
the speech view without writing glue code, scripts,
and other infrastructure required to tie together the
various generated components. These are not nec-
essarily things that they want to spend their time
doing. The consequence can easily be that the lin-
guists end up working exclusively in the text view,
and over-refine the text versions of the rule-sets.
From a project management viewpoint, this results
in bad prioritisation decisions, since there are more
pressing issues to address in the speech view.
A second reason why linguist rule-writers have
been unhappy working in the speech view is the
lack of reproducibility associated with speech in-
put. One can type ?John loves Mary? into a text-
processing system any number of times, and ex-
pect to get the same result. It is much less reason-
able to expect to get the same result each time if
one says ?John loves Mary? to a speech recogniser.
Often, anomalous results occur, but cannot be de-
bugged in a systematic fashion, leading to general
frustration. The result, once again, is that linguists
have preferred to stick with the text view, where
they feel at home.
Yet another reason why rule-writers tend to
limit themselves to the text view is simply the
large number of top-level commands and inter-
mediate compilation results. The current Regulus
command-line environment includes over 110 dif-
ferent commands, and compilation from the initial
resource grammar to the final Nuance recognition
package involves creating a sequence of five com-
pilation steps, each of which requires the output
created by the preceding one. This makes it diffi-
cult for novice users to get their bearings, and in-
creases their cognitive load. Additionally, once the
commands for the text view have been mastered,
there is a certain temptation to consider that these
are enough, since the text and speech views can
reasonably be perceived as fairly similar.
In the next two sections, we describe an en-
hanced development environment for Regulus,
which addresses the key problems we have just
sketched. From the point of view of the linguist
rule-writer, we want speech-based development to
feel more like text-based development.
4 Speech and text in the online
development environment
The Regulus GUI (Kron et al, 2007) is intended
as a complete redesign of the development envi-
ronment, which simultaneously attacks all of the
central issues. Commands are organised in a struc-
tured set of functionality-based windows, each of
which has an appropriate set of drop-down menus.
Following normal GUI design practice (Dix et al,
1998, Chapters 3 and 4); (Jacko and Sears, 2003,
Chapter 13), only currently meaningful commands
are executable in each menu, with the others shown
greyed out.
Both compile-time and run-time speech-related
functionality can be invoked directly from the
command menus, with no need for external scripts,
Makefiles or glue code. Focussing for the moment
on the specific case of developing a speech transla-
tion application, the rule-writer will initially write
and debug her rules in text mode. She will be able
to manipulate grammar rules and derivation trees
using the Stepper window (Figure 1; cf. also (Kron
et al, 2007)), and load and test translation rules
in the Translate window (Figure 2). As soon as
the grammar is consistent, it can at any point be
compiled into a Nuance recognition package us-
ing the command menus. The resulting recogniser,
together with other speech resources (license man-
11
Figure 1: Using the Stepper window to browse trees in the Toy1 grammar from (Rayner et al, 2006,
Chapter 4). The upper left window shows the analysis tree for ?switch on the light in the kitchen?; the
lower left window shows one of the subtrees created by cutting the first tree at the higher NP node. Cut
subtrees can be recombined for debugging purposes (Kron et al, 2007).
Figure 2: Using the Translate window to test the toy English ? French translation application from
(Rayner et al, 2006, Chapter 6). The to- and from-interlingua rules used in the example are shown in the
two pop-up windows at the top of the figure.
12
regulus_config(regulus_grammar,
[toy1_grammars(toy1_declarations),
toy1_grammars(toy1_rules),
toy1_grammars(toy1_lexicon)]).
regulus_config(top_level_cat, ?.MAIN?).
regulus_config(nuance_grammar, toy1_runtime(recogniser)).
regulus_config(to_interlingua_rules,
toy1_prolog(?eng_to_interlingua.pl?)).
regulus_config(from_interlingua_rules,
toy1_prolog(?interlingua_to_fre.pl?)).
regulus_config(generation_rules, toy1_runtime(?generator.pl?)).
regulus_config(nuance_language_pack,
?English.America?).
regulus_config(nuance_compile_params, [?-auto_pron?, ?-dont_flatten?]).
regulus_config(translation_rec_params,
[package=toy1_runtime(recogniser), grammar=?.MAIN?]).
regulus_config(tts_command,
?vocalizer -num_channels 1 -voice juliedeschamps -voices_from_disk?).
Figure 3: Config file for a toy English ? French speech translation application, showing items relevant
to the speech view. Some declarations have been omitted for expositional reasons.
ager, TTS engine etc), can then be started using a
single menu command.
In accordance with the usual Regulus design
philosophy of declaring all the resources associ-
ated with a given application in its config file, the
speech resources are also specified here. Figure 3
shows part of the config file for a toy translation
application, in particular listing all the declara-
tions relevant to the speech view. If we needed to
change the speech resources, this would be done
just by modifying the last four lines. For example,
the config file as shown specifies construction of
a recogniser using acoustic models appropriate to
American English. We could change this to British
English by replacing the entry
regulus_config(nuance_language_pack,
?English.America?).
with
regulus_config(nuance_language_pack,
?English.UK?).
When the speech resources have been loaded,
the Translate window can take input equally easily
in text or speech mode; the Translate button pro-
cesses written text from the input pane, while the
Recognise button asks for spoken input. In each
case, the input is passed through the same process-
ing stages of source-to-interlingua and interlingua-
to-target translation, followed by target-language
generation. If a TTS engine or a set of recorded
target language wavfiles is specified, they are used
to realise the final result in spoken form (Figure 4).
Every spoken utterance submitted to recognition
is logged as a SPHERE-headed wavfile, in a time-
stamped directory started at the beginning of the
current session; this directory also contains a meta-
data file, which associates each recorded wavfile
with the recognition result it produced. The Trans-
late window?s History menu is constructed using
the meta-data file, and allows the user to select any
recorded utterance, and re-run it through the sys-
tem as though it were a new speech input. The
consequence is that speech input becomes just as
reproducible as text, with corresponding gains for
interactive debugging in speech mode.
5 Speech and text in regression testing
In earlier versions of the Regulus development
environment (Rayner et al, 2006, ?6.6), regres-
sion testing in speech mode was all based on
Nuance?s batchrec utility, which permits of-
fline recognition of a set of recorded wavfiles.
A test suite for spoken regression testing conse-
quently consisted of a list of wavfiles. These
were first passed through batchrec; outputs
were then post-processed into Regulus form, and
finally passed through Regulus speech understand-
ing modules, such as translation or dialogue man-
agement.
As Regulus applications grow in complexity,
this model has become increasingly inadequate,
since system input is very frequently not just a
list of monolingual speech events. In a multi-
modal dialogue system, input can consist of either
speech or screen events (text/mouse-clicks); con-
text is generally important, and the events have to
be processed in the order in which they occurred.
Dialogue systems which control real or simulated
robots, like the Wheelchair application of (Hockey
13
Figure 4: Speech to speech translation from the GUI, using a Japanese to Arabic translator built from
MedSLT components (Bouillon et al, 2008). The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
and Miller, 2007) will also receive asynchronous
inputs from the robot control and monitoring pro-
cess; once again, all inputs have to be processed in
the appropriate temporal order. A third example is
contextual bidirectional speech translation (Bouil-
lon et al, 2007). Here, the problem is slightly
different ? we have only speech inputs, but they
are for two different languages. The basic issue,
however, remains the same, since inputs have to be
processed in the right order to maintain the correct
context at each point.
With examples like these in mind, we have also
effected a complete redesign of the Regulus envi-
ronment?s regression testing facilities. A test suite
is now allowed to consist of a list of items of any
type ? text, wavfile, or non-speech input ? in any
order. Instead of trying to fit processing into the
constraints imposed by the batchrec utility, of-
fline processing now starts up speech resources in
the same way as the interactive environment, and
submits each item for appropriate processing in the
order in which it occurs. By adhering to the prin-
ciple that text and speech should be treated uni-
formly, we arrive at a framework which is simpler,
less error-prone (the underlying code is less frag-
ile) and above all much more flexible.
6 Summary and conclusions
The new functionality offered by the redesigned
Regulus top-level is not strikingly deep. In the
context of any given application, it could all
have been duplicated by reasonably simple scripts,
which linked together existing Regulus compo-
nents. Indeed, much of this new functionality is
implemented using code derived precisely from
such scripts. Our observation, however, has been
that few developers have actually taken the time
to write these scripts, and that when they have
been developed inside one project they have usu-
ally not migrated to other ones. One of the things
we have done, essentially, is to generalise previ-
ously ad hoc application-dependent functionality,
and make it part of the top-level development en-
vironment. The other main achievements of the
new Regulus top-level are to organise the existing
functionality in a more systematic way, so that it is
easier to find commands, and to package it all as a
normal-looking Swing-based GUI.
Although none of these items sound dramatic,
they make a large difference to the platform?s over-
14
all usability, and to the development cycle it sup-
ports. In effect, the Regulus top-level becomes
a generic speech-enabled application, into which
developers can plug their grammars, rule-sets and
derived components. Applications can be tested in
the speech view much earlier, giving a correspond-
ingly better chance of catching bad design deci-
sions before they become entrenched. The mecha-
nisms used to enable this functionality do not de-
pend on any special properties of Regulus, and
could readily be implemented in other grammar-
based development platforms, such as Gemini and
UNIANCE, which support compilation of feature
grammars into grammar-based language models.
At risk of stating the obvious, it is also worth
pointing out that many users, particularly younger
ones who have grown up using Windows and Mac
environments, expect as a matter of course that de-
velopment platforms will be GUI-based rather than
command-line. Addressing this issue, and sim-
plifying the transition between text- and speech-
based, views has the pleasant consequence of im-
proving Regulus as a vehicle for introducing lin-
guistics students to speech technology. An initial
Regulus-based course at the University of Santa
Cruz, focussing on spoken dialogue systems, is de-
scribed in (Hockey and Christian, 2008); a similar
one, but oriented towards speech translation and
using the new top-level described here, is currently
under way at the University of Geneva. We expect
to present this in detail in a later paper.
References
Alshawi, H., editor. 1992. The Core Language Engine.
MIT Press, Cambridge, Massachusetts.
Bos, J. 2002. Compilation of unification grammars
with compositional semantics to speech recognition
packages. In Proceedings of the 19th International
Conference on Computational Linguistics, Taipei,
Taiwan.
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Copestake, A. 2002. Implementing Typed Feature
Structure Grammars. CSLI Press, Chicago.
Crouch, R., M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman, 2008. XLE Documenta-
tion. http://www2.parc.com/isl/groups/nltt/xle/doc.
As of 29 Apr 2008.
Dix, A., J.E. Finlay, G.D. Abowd, and R. Beale, edi-
tors. 1998. Human Computer Interaction. Second
ed. Prentice Hall, England.
Dowding, J., M. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proceedings of the Thirty-First Annual Meeting of
the Association for Computational Linguistics.
Hockey, B.A. and G. Christian. 2008. Zero to spoken
dialogue system in one quarter: Teaching computa-
tional linguistics to linguists using regulus. In Pro-
ceedings of the Third ACL Workshop on Teaching
Computational Linguistics (TeachCL-08), Colum-
bus, OH.
Hockey, B.A. and D. Miller. 2007. A demonstration of
a conversationally guided smart wheelchair. In Pro-
ceedings of the 9th international ACM SIGACCESS
conference on Computers and accessibility, pages
243?244, Denver, CO.
Jacko, J.A. and A. Sears, editors. 2003. The
human-computer interaction handbook: Fundamen-
tals, evolving technologies and emerging applica-
tions. Lawerence Erlbaum Associates, Mahwah,
New Jersey.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Larsson, S. and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Di-
alogue Systems Engineering, pages 323?340.
Rayner, M., B.A. Hockey, J.M. Renders,
N. Chatzichrisafis, and K. Farrell. 2005. A
voice enabled procedure browser for the interna-
tional space station. In Proceedings of the 43rd
15
Annual Meeting of the Association for Compu-
tational Linguistics (interactive poster and demo
track), Ann Arbor, MI.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
16
