Proceedings of the ACL 2010 System Demonstrations, pages 72?77,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Demonstration of a prototype for a Conversational Companion for  reminiscing about images  Yorick Wilks IHMC, Florida ywilks@ihmc.us Roberta Catizone University of Sheffield, UK r.catizone@dcs.shef.ac.uk Alexiei Dingli University of Malta, Malta alexiei.dingli@um.edu.mt Weiwei Cheng University of Sheffield, UK w.cheng@dcs.shef.ac.uk  Abstract 
This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following:  1) The use of Informa-tion Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF)  to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and net-work virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning. 1. Introduction This demonstrator Senior Companion (SC) was built during the initial phase of the Companions project and  aims to change the way we think about the relationships of people to computers and the internet by developing a virtual conver-sational 'Companion that will be an agent or 'presence' that stays with the user for long peri-ods of time, developing a relationship and 'know-ing its owners? preferences and wishes. The Companion communicates with the user primar-ily through speech, but also using other tech-nologies such as touch screens and sensors. This paper describes the functionality and system modules of the Senior Companion, one of two initial prototypes built in the first two years of the project. The SC provides a multimodal inter-face for eliciting, retrieving and inferring per-sonal information from elderly users by means of conversation about their photographs. The Com-panion, through conversation, elicits life memo-
ries and reminiscences, often prompted by dis-cussion of their photographs; the aim is that the Companion should come to know a great deal about its user, their tastes, likes, dislikes, emo-tional reactions etc, through long periods of con-versation. It is assumed that most life informa-tion will soon be stored on the internet (as in the Memories for Life project: http://www.memoriesforlife.org/) and we have linked the SC directly to photo inventories in Facebook (see below). The overall aim of the SC project (not yet achieved) is to produce a coher-ent life narrative for its user from conversations about personal photos, although its short-term goals, reported here, are to assist, amuse and en-tertain the user.  The technical content of the project is to use a number of types of machine learning  (ML) to achieve these ends in original ways, initially us-ing a methodology developed in earlier research: first, by means of an Information Extraction (IE) approach to deriving content from user input ut-terances; secondly, using a training method for attaching Dialogue Acts to these utterance and, lastly, using a specific type of dialogue manager (DM) that uses Dialogue Action Forms  (DAF) to determine the context of any utterance. A stack of these DAFs is the virtual machine that models the ongoing dialogue by means of shared user and Companion initiative and generates ap-propriate responses. In this description of the demo, we shall: 
? describe the current SC prototype?s func-tionality; 
? set out its architecture and modules, fo-cusing on the Natural Language Under-standing module and the Dialogue Man-ager. A mini-version of the demo running in real time can be seen at:  URL   
72
http://www.youtube.com/watch?v=-Xx5hgjD-Mw 2. The Senior Companion System The Senior Companion prototype (Wilks, 2010)  was designed to make a rapid advance in the first two years of the project so as to be basis for a second round of prototypes embodying more advanced ML. This strategy was deliberately chosen to avoid a well-known problem with ex-perimental AI systems: that a whole project is spent in design so that a prototype never emerges until the very end, but which is then never fully evaluated and, most importantly, nothing is ever built on the experience obtained in its construc-tion. The central function of the SC is engaging the user in discussion about their photographs: where and when they were taken, details about the people in them and their relationship to the user and each other.  Once a photo is loaded, it is processed with face recognition software to identify any faces in it. The recognition software, OpenCV, provides positional information by identifying the face coordinates and this information is exploited in the Dialogue Manager by making explicit refer-ence to the position of people in the photograph (the person on the left, right, center, etc.) as well as recognizing when there are groups of people. The system discusses properties of the photo as well as properties and relationships of the people in the photos.   The news reading feature adds an interesting ac-companiment to the photo domain and demon-strates the ability of the system to handle more than one kind of application at a time, and news has, of course, an unconstrained vocabulary. The following is a fairly typical example of its cur-rent capacity, depending of course on the images loaded, and comes from the middle part of a sample dialogue generated by the system when discussing a group photo from a vacation in Zan-zibar, and after the system had already stored, from an earlier conversation, that the user has a daughter Zoe (using researchers? children to avoid any problems):  SC: What is your relationship to the doctor? User: There is no relationship. SC: Please tell me how you know the doctor U: Uh, he's a friend. SC: How old was the doctor when this photo was taken? U: I'm not sure but I think about 30. 
SC: How do you know Octavia? U: She's my daughter. SC: Aha! I see. Octavia's sister is Zoe. SC: How old was your daughter Octavia when this photo was taken? U: Octavia was 19 years old. SC: What was the occasion of this photo? U: This was another family holiday. SC: Where was this photo taken? U: This photo was taken in Tanzania. SC: have you ever been to Zanzibar? U: Yes, I have. SC: Great place, I hope you saw the beaches. When was this photo taken? U: last year.  It is a key feature of the SC that it can make sim-ple inferences from family relationships it al-ready knows (e.g. that daughters of the same par-ent are siblings) and that it can access real-time information about places to show that it has some knowledge of what is being talked about, in this case the beaches on Zanzibar, and how this is done is discussed below. This real time access to unconstrained place information on the internet is an attempt to break out of classic AI systems that only know the budget of facts they have been primed with.  This basic system provides the components for future development of the SC, as well as its main use as a device to generate more conversation data for machine learning research in the future. Key features of the SC are listed below followed by a description of the system architecture and modules. The SC: ? Contains a visually appealing multi-modal interface with a character avatar to mediate the system?s functionality to the user. ? Interacts with the user using multiple modalities ? speech and touch. ? Includes face detection software for identifying the position of faces in the photos. ? Accepts pre-annotated (XML) photo in-ventories as a means for creating richer dialogues more quickly.  ? Engages in conversation with the user about topics within the photo domain: when and where the photo was taken, discussion of the people in the photo in-cluding their relationships to the user. ? Reads news from three categories: poli-tics, business and sports. 
73
? Tells jokes taken from an internet-based  joke website. ? Retains all user input for reference in re-peat user sessions, in addition to the knowledge base that has been updated by the Dialogue Manager on the basis of what was said. ? Contains a fully integrated Knowledge Base for maintaining user information including: o Ontological information which is exploited by the Dialogue Manager and provides domain-specific relations between fun-damental concepts.  o A mechanism for storing infor-mation in a triple store (Subject-Predicate-Object) - the RDF Semantic Web format - for han-dling unexpected user input that falls outside of the photo do-main, e.g. arbitrary locations in which photos might have been taken. o A reasoning module for reason-ing over the Knowledge Base and world knowledge obtained in RDF format from the internet; the SC is thus a primitive Se-mantic Web device (see refernce8, 2008) ? Contains basic photo management capa-bility allowing the user, in conversation, to select photos as well as display a set of photos with a particular feature.  
 Figure 1: The Senior Companion Interface 
 3. System Architecture  In this section we will review the components of the SC architecture. As can be seen from Figure 2, the architecture contains three abstract level components ? Connectors, Input Handlers and Application Services ?together with the Dialogue Manager and the Natural Language Understander (NLU). 
  Figure 2: Senior Companion system architecture  Connectors form a communication bridge be-tween the core system and external applications. The external application refers to any modules or systems which provide a specific set of function-alities that might be changed in the future. There is one connector for each external application. It hides the underlying complex communication protocol details and provides a general interface for the main system to use. This abstraction de-couples the connection of external and internal modules and makes changing and adding new external modules easier. At this moment, there are two connectors in the system ? Napier Inter-face Connector and CrazyTalk Avatar Connec-tor. Both of them are using network sockets to send/receive messages.  Input Handlers are a set of modules for process-ing messages according to message types. Each handler deals with a category of messages where categories are coarse-grained and could include one or more message types. The handlers sepa-rate the code handling inputs into different places and make the code easier to locate and change. Three handlers have been implemented in the Senior Companion system ? Setup Handler, Dragon Events Handler and General Handler. The Setup Handler is responsible for loading the photo annotations if any, performing face detec-tion if no annotation file is associated with the photo and checking the Knowledge Base in case 
74
the photo being processed has been discussed in earlier sessions. Dragon Event Handler deals with dragon speech recognition commands sent from the interface while the General Handler processes user utterances and photo change events of the interface.  Application Services are a group of internal modules which provide interfaces for the Dia-logue Action Forms (DAF) to use. It has an easy-to-use high-level interface for general DAF de-signers to code associated tests and actions as well as a low level interface for advanced DAFs. It also provides the communication link between DAFs and the internal system and enables DAFs to access system functionalities. Following is a brief summary of modules grouped into Applica-tion Services.  News Feeders are a set of RSS Feeders for fetch-ing news from the internet. Three different news feeders have been implemented for fetching news from BBC website Sports, Politics and Business channels. There is also a Jokes Feeder to fetch Jokes from internet in a similar way. During the conversation, the user can request news about particular topics and the SC simply reads the news downloaded through the feeds.  The DAF Repository is a list of DAFs loaded from files generated by the DAF Editor.   The Natural Language Generation (NLG) mod-ule is responsible for randomly selecting a sys-tem utterance from a template. An optional vari-able can be passed when calling methods on this module. The variable will be used to replace spe-cial symbols in the text template if applicable.   Session Knowledge is the place where global information for a particular running session is stored. For example, the name of the user who is running the session, the list of photos being dis-cussed in this session and the list of user utter-ances etc.  The Knowledge Base is the data store of persis-tent knowledge. It is implemented as an RDF triplestore using a Jena implementation. The tri-plestore API is a layer built upon a traditional relational database. The application can save/retrieve information as RDF triples rather than table records. The structure of knowledge represented in RDF triples is discussed later.  
The Reasoner is used to perform inference on existing knowledge in the Knowledge Base (see example in next section).  The Output Manager deals with sending mes-sages to external applications. It has been im-plemented in a publisher/subscriber fashion. There are three different channels in the system: the text channel, the interface command channel and the avatar command channel. Those chan-nels could be subscribed to by any connectors and handled respectively.  4. Dialogue understanding and inference Every utterance is passed through the Natural Language Understanding (NLU) module for processing. This module uses a set of well-established natural language processing tools such as those found in the GATE (Cunningham, et al, 1997) system. The basic processes carried out by GATE are: tokenizing, sentence splitting, POS tagging, parsing and Named Entity Recog-nition. These components have been further en-hanced for the SC system by adding 1) new and improved gazetteers including family relations and 2) accompanying extraction rules .The Named Entity (NE) recognizer is a key part of the NLU module and recognizes the significant entities required to process dialogue in the photo domain: PERSON NAMES, LOCATION NAMES, FAMILY RELATIONS and DATES. Although GATE recognizes basic entities, more complex entities are not handled. Apart from the gazetteers mentioned earlier and the hundreds of extraction rules already present in GATE, about 20 new extraction rules using the JAPE rule lan-guage were also developed for the SC module. These included rules which identify complex dates, family relationships, negations and other information related to the SC domain. The fol-lowing is an example of a simple rule used to identify relationship in utterances such as ?Mary is my sister?:  Macro: RELATIONSHIP_IDENTIFIER ( ({To-ken.category=="PRP$"}|{Token.category=="PRP"}|{Lookup.majorType=="person_first"}):person2 ({Token.string=="is"}) ({Token.string=="my"}):person1  ({Lookup.minorType=="Relationship"}):relationship) 
75
Using this rule with the example mentioned ear-lier, the rule interprets person1 as referring to the speaker so, if the name of the user speaking is John (which was known from previous conversa-tions), it is utilized. Person 2 is then the name of the person mentioned, i.e. Mary. This name is recognised by using the gazetteers we have in the system (which contain about 40,000 first names). The relationship is once again identified using the almost 800 unique relationships added to the gazetteer.  With this information, the NLU mod-ule identifies Information Extraction patterns in the dialogue that represent significant content with respect to a user's life and photos.   The information obtained (such as Mary=sister-of John) is passed to the Dialogue Manager (DM) and then stored in the knowledge base (KB). The DM filters what to include and ex-clude from the KB. Given, in the example above, that Mary is the sister of John, the NLU knows that sister is a relationship between two people and is a key relationship. However, the NLU also discovers syntactical information such as the fact the both Mary and John are nouns. Even though this information is important, it is too low level to be of any use by the SC with respect to the user, i.e. the user is not interested in the parts-of-speech of a word. Thus, this information is dis-carded by the DM and not stored in the KB. The NLU module also identifies a Dialogue Act Tag for each user utterance based on the DAMSL set of DA tags and prior work done jointly with the University of Albany (Webb et al, 2008).  The KB is a long-term store of information which makes it possible for the SC to retrieve information stored between different sessions. The information can be accessed anytime it is needed by simply invoking the relevant calls. The structure of the data in the database is an RDF triple, and the KB is more commonly re-ferred to as a triple store. In mathematical terms, a triple store is nothing more than a large data-base of interconnected graphs. Each triple is made up of a subject, a predicate and an object. So, if we took the previous example, Mary sister-of John; Mary would be the subject, sister-of would be the predicate and John would be the object. The inference engine is an important part of the system because it allows us to discover new facts beyond what is elicited from the con-versation with the user.    
Uncle Inference Rule:   (?a sisterOf ?b), (?x sonOf ?a), (?b gender male) -> (?b uncleOf ?x)    Triples: (Mary  sisterOf  John) (Tom   sonOf   Mary)  Triples produced automatically by ANNIE (the semantic tagger): (John  gender   male)  Inference: (Mary  sisterOf  John) (Tom   sonOf   Mary) (John  gender   male)  ->  (John uncleOf Tom)  This kind of inference is already used by the SC and we have about 50 inference rules aimed at producing new data on the relationships domain. This combination of triple store, inference engine and inference rules makes a system which is weak but powerful enough to mimic human rea-soning in this domain and thus simulate basic intelligence in the SC. For our prototype, we are using the JENA Semantic Web Framework for the inference engine together with a MySQL da-tabase as the knowledge base. However, this sys-tem of family relationships is not enough to cover all the possible topics which can crop up during a conversation and, in such circum-stances, the DM switches to an open-world model and instructs the NLU to seek further in-formation online.  5. The Hybrid-world approach When the DM requests further information on a particular topic, the NLU first checks with the KB whether the topic is about something known. At this stage, we have to keep in mind that any topic requested by the DM should be already in the KB since it was preprocessed by the NLU when it was mentioned in the utterance. So, if the user informs the system that the photograph was taken in Paris, (in response to a system question asking where the photo was taken), the utterance is first processed by the NLU which discovers that ?Paris? is a location using its semantic tag-ger ANNIE (A Nearly New Information Extrac-tion engine). The semantic tagger makes use of gazetteers and IE rules in order to accomplish 
76
this task. It also goes through the KB and re-trieves any triples related to ?Paris?. Inference is then performed on this data and the new informa-tion generated by this process is stored back in the KB.   Once the type of information is identified, the NLU can use various predefined strategies: In the case of LOCATIONS, one of the strategies used is to seek for information in Wiki-Travel or Virtual Tourists. The system already knows how to query these sites and interpret their output by using predefined wrappers. This is then used to extract relevant information from the mentioned sites webpages by sending an online query to these sites and storing the information retrieved in the triple-store. This information is then used by the DM to generate a reply. In the previous example, the system manages to extract the best sightseeing spots in Paris. The NLU would then store in the KB triples such as [Paris, sight-seeing, Eiffel Tower] and the DM with the help of the NLG would ask the user ?I?ve heard that the X is a very famous spot. Have you seen it while you were there?? Obviously in this case, X would  be replaced by the ?Eiffel Tower?.  On the other hand, if the topic requested by the DM is unknown, or the semantic tagger is not capable of understanding the semantic category, the system uses a normal search engine (and this is what we call ?hybrid-world?: the move outside the world the system already knows). A query containing the unknown term in context is sent to standard engines and the top pages are retrieved. These pages are then processed using ANNIE and their tagged attributes are analyzed. The standard attributes returned by ANNIE include information about Dialogue Acts, Polarity (i.e. whether a sentence has positive, negative or neu-tral connotations), Named Entities, Semantic Categories (such as dates and currency), etc. The system then filters the information collected by using more generic patterns and generates a reply from the resultant information. ANNIE?s polarity methods have been shown to be an adequate im-plementation of the general word-based polarity methods pioneered by Wiebe and her colleagues (see e.g. Akkaya et al, 2009). 6. Evaluation  The notion of companionship is not yet one with any agreed evaluation strategy or metric, though developing one is part of the main project itself.  
Again, there are established measures for the as-sessment of dialogue programs but they have all been developed for standard task-based dia-logues and the SC is not of that type: there is no specific task either in reminiscing conversations, nor in the elicitation of the content of photos, that can be assessed in standard ways, since there is no clear point at which an informal dialogue need stop, having been completed.  Conventional dialogue evaluations often use measures like ?stickiness? to determine how much a user will stay with or stick with a dialogue system and not leave it, presumably because they are disap-pointed or find it lacking in some feature. But it is hard to separate that feature out from a task rapidly and effectively completed, where sticki-ness would be low not high. Traum (Traum et al, 2004) has developed a methodology for dialogue evaluation based on ?appropriateness? of re-sponses and the Companions project has devel-oped a model of evaluation for the SC based on that (Benyon et al, 2008).  Acknowledgement  This work was funded by the Companions project  (2006-2009) sponsored by the European Commission as part of the Information Society Technologies (IST) programme under EC grant number IST-FP6-034434.  References David Benyon, Prem Hansen and Nick Webb, 2008. Evaluating Human-Computer Conversation in Companions. In: Proc.4th International Workshop on Human-Computer Conversation, Bellagio, Italy. Cem Akkaya, Jan Wiebe, and Rada Mihalcea,. 2009. Subjectivity Word Sense Disambiguation, In:  EMNLP 2009. Hamish Cunningham, Kevin Humphreys, Robert Gai-zauskas, and Yorick Wilks, 1997. GATE -- a TIP-STER based General Architecture for Text Engi-neering. In: Proceedings of the TIPSTER Text Pro-gram (Phase III) 6 Month Workshop. Morgan Kaufmann, CA. David Traum, Susan Robinson, and Jens Stephan. 2004.  Evaluation of multi-party virtual reality dia-logue interaction, In: Proceedings of Fourth International Conference on Language Resources and Evaluation (LREC 2004), pp.1699-1702 Yorick Wilks (ed.) 2010. Artificial Companions in Society: scientific, economic, psychological and philosophical perspectives. John Benjamins: Am-sterdam.  
77
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 231?239,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Scaling up WSD with Automatically Generated Examples
Weiwei Cheng, Judita Preiss and Mark Stevenson
Department of Computer Science,
Sheffield University,
Regent Court, 211 Portobello,
Sheffield, S1 4DP
United Kingdom
{W.Cheng, J.Preiss, M.Stevenson}@dcs.shef.ac.uk
Abstract
The most accurate approaches to Word Sense
Disambiguation (WSD) for biomedical docu-
ments are based on supervised learning. How-
ever, these require manually labeled training
examples which are expensive to create and
consequently supervised WSD systems are
normally limited to disambiguating a small set
of ambiguous terms. An alternative approach
is to create labeled training examples automat-
ically and use them as a substitute for manu-
ally labeled ones. This paper describes a large
scale WSD system based on automatically la-
beled examples generated using information
from the UMLS Metathesaurus. The labeled
examples are generated without any use of la-
beled training data whatsoever and is therefore
completely unsupervised (unlike some previ-
ous approaches). The system is evaluated on
two widely used data sets and found to outper-
form a state-of-the-art unsupervised approach
which also uses information from the UMLS
Metathesaurus.
1 Introduction
The information contained in the biomedical liter-
ature that is available in electronic formats is use-
ful for health professionals and researchers (West-
brook et al, 2005). The amount is so vast that
it is difficult for researchers to identify informa-
tion of interest without the assistance of automated
tools (Krallinger and Valencia, 2005). However,
processing these documents automatically is made
difficult by the fact that they contain terms that
are ambiguous. For example, ?culture? can mean
?laboratory procedure? (e.g. ?In peripheral blood
mononuclear cell culture?) or ?anthropological cul-
ture? (e.g. ?main accomplishments of introducing a
quality management culture?). These lexical ambi-
guities are problematic for language understanding
systems.
Word sense disambiguation (WSD) is the process
of automatically identifying the meanings of am-
biguous terms. Some WSD systems for the biomed-
ical domain are only able to disambiguate a small
number of ambiguous terms (see Section 2). How-
ever, for WSD systems to be useful in applications
they should be able to disambiguate all ambiguous
terms. One way to create such a WSD system is to
automatically create the labeled data that is used to
train supervised WSD systems. Several approaches
(Liu et al, 2002; Stevenson and Guo, 2010; Jimeno-
Yepes and Aronson, 2010) have used information
from the UMLS Metathesaurus1 to create labeled
training data that have successfully been used to cre-
ate WSD systems.
A key decision for any system that automatically
generates labeled examples is the number of exam-
ples of each sense to create, known as the bias of the
data set. It has been shown that the bias of a set of la-
beled examples affects the performance of the WSD
system it is used to train (Mooney, 1996; Agirre and
Mart??nez, 2004b). Some of the previous approaches
to generating labeled data relied on manually anno-
tated examples to determine the bias of the data sets
and were therefore not completely unsupervised.
This paper describes the development of a large
scale WSD system that is able to disambiguate all
1http://www.nlm.nih.gov/research/umls/
231
terms that are ambiguous in the UMLS Metathe-
saurus. The system relies on labeled examples that
are created using information from UMLS. Various
bias options are explored, including ones that do not
make use of information from manually labeled ex-
amples, and thus we can create a completely unsu-
pervised system. Evaluation is carried out on two
standard datasets (the NLM-WSD and MSH-WSD
corpora). We find that WSD systems can be cre-
ated without using any information from manually
labeled examples and that their performance is bet-
ter than a state-of-the-art unsupervised approach.
The remainder of this paper is organized as fol-
lows. Previous approaches to WSD in biomedical
documents are described in the next Section. Section
3 presents the methods used to identify bias in the
labeled examples and WSD system. Experiments in
which these approaches are compared are described
in Section 4 and their results in Section 5.
2 Background
Many WSD systems for the biomedical domain are
based on supervised learning (McInnes et al, 2007;
Xu et al, 2007; Stevenson et al, 2008; Yepes and
Aronson, 2011). These systems require labeled
training data, examples of an ambiguous term la-
beled with the correct meaning. Some sets of labeled
data have been developed for the biomedical domain
(Weeber et al, 2001; Savova et al, 2008; Jimeno-
Yepes et al, 2011). However, these data sets only
contain examples for a few hundred terms and can
only be used to develop WSD systems to identify
the meanings of those terms. The process of creat-
ing labeled examples is extremely time-consuming
and difficult (Artstein and Poesio, 2008), making it
impractical to create labeled examples of all possible
ambiguous terms found in biomedical documents.
Two alternative approaches have been explored to
develop systems which are able to disambiguate all
ambiguous terms in biomedical documents. The first
makes use of unsupervised WSD algorithms (see
Section 2.1) and the second creates labeled data au-
tomatically and uses it to train a supervised WSD
system (see Section 2.2).
2.1 Unsupervised WSD
Unsupervised WSD algorithms make use of infor-
mation from some knowledge source, rather than re-
lying on training data.
Humphrey et al (2006) describe an unsupervised
system which uses semantic types in UMLS to dis-
tinguish between the possible meanings of ambigu-
ous words. However, it cannot disambiguate be-
tween senses with the same semantic type, i.e., it
is not possible for the system to recognise all sense
distinctions.
The Personalised Page Rank (PPR) system
(Agirre et al, 2010; Jimeno-Yepes and Aronson,
2010) relies on a a graph-based algorithm similar
to the Page Rank algorithm originally developed for
use in search engines (Brin, 1998). It performs
WSD by converting the UMLS Metathesaurus into
a graph in which the possible meanings of ambigu-
ous words are nodes and relations between them are
edges. Disambiguation is carried out by providing
the algorithm with a list of senses that appear in the
text that is being disambiguated. This information is
then combined with the graph and a ranked list of the
possible senses for each ambiguous word generated.
Unsupervised systems have the advantage of be-
ing able to disambiguate all ambiguous terms. How-
ever, the performance of unsupervised systems that
have been developed for biomedical documents is
lower than that of supervised ones.
2.2 Automatic Generation of Labeled Data
Automatic generation of labeled data for WSD com-
bines the accuracy of supervised approaches with
the ability of unsupervised approaches to disam-
biguate all ambiguous terms. It was first suggested
by Leacock et al (1998). Their approach is based
on the observation that some terms in a lexicon oc-
cur only once and, consequently, there is no doubt
about their meaning. These are referred to as being
monosemous. Examples for each possible meaning
of an ambiguous term are generated by identifying
the closest monosemous term (the monosemous rel-
ative) in the lexicon and using examples of that term.
Variants of the approach have been applied to the
biomedical domain using the UMLS Metathesaurus
as the sense inventory.
232
Liu et al (2002) were the first to apply the
monosemous relatives approach to biomedical WSD
and use it to disambiguate a set of 35 abbreviations.
They reported high precision but low recall, indicat-
ing that labeled examples could not be created for
many of the abbreviations. Jimeno-Yepes and Aron-
son (2010) applied a similar approach and found
that it performed better than a number of alternative
approaches on a standard evaluation resource (the
NLM-WSD corpus) but did not perform as well as
supervised WSD. Stevenson and Guo (2010) com-
pared two techniques for automatically creating la-
beled data, including the monosemous relatives ap-
proach. They found that the examples which were
generated were as good as manually labeled exam-
ples when used to train a supervised WSD system.
However, Stevenson and Guo (2010) relied on la-
beled data to determine the number of examples of
each sense to create, and therefore the bias of the
data set. Consequently their approach is not com-
pletely unsupervised since it could not be applied to
ambiguous terms that do not have labeled training
data available.
3 Approach
3.1 WSD System
The WSD system is based on a supervised approach
that has been adapted for the biomedical domain
(Stevenson et al, 2008). The system was tested on
the NLM-WSD corpus (see Section 4.1) and found
to outperform alternative approaches.
The system can exploit a wide range of fea-
tures, including several types of linguistic informa-
tion from the context of an ambiguous term, MeSH
codes and Concept Unique Identifiers (CUIs) from
the UMLS Metathesaurus. However, computing
these features for every example is a time consum-
ing process and to make the system suitable for large
scale WSD it was restricted to using a smaller set
of features. Previous experiments (Stevenson et al,
2008) showed that this only leads to a small drop in
disambiguation accuracy while significantly reduc-
ing the computational cost of generating features.
3.1.1 Features
Two types of context words are used as features:
the lemmas of all content words in the same sen-
tence as the ambiguous word and the lemmas of all
content words in a?4-word window around the am-
biguous term. A list of corpus-specific stopwords
was created containing terms that appear frequently
in Medline abstracts but which are not useful for dis-
ambiguation (e.g. ?abstract?, ?conclusion?). Any
lemmas found in this list were not used as features.
3.1.2 Learning algorithm
Disambiguation is carried out using the Vector
Space Model, a memory-based learning algorithm
in which each occurrence of an ambiguous word is
represented as a vector created using the features ex-
tracted to represent it (Agirre and Mart??nez, 2004a).
The Vector Space Model was found to outperform
other learning algorithms when evaluated using the
NLM-WSD corpus (Stevenson et al, 2008).
During the algorithm?s training phase a single
centroid vector, ~Csj , is generated for each possible
sense, sj . This is shown in equation 1 where T is
the set of training examples for a particular term and
sense(~t) is the sense associated with the vector ~t.
~Csj =
?
~ti  T :sense(~ti)=sj
~ti
|~ti  T : sense(~ti) = sj |
(1)
Disambiguation is carried out by comparing the
vector representing the ambiguous word, ~a, against
the centroid of each sense using the cosine metric,
shown in equation 2, and choosing the one with the
highest score.
score(sj ,~a) = cos( ~Csj ,~a) =
~Csj .~a
| ~Csj ||~a|
(2)
Note that the learning algorithm does not ex-
plicitly model the prior probability of each possi-
ble sense, unlike alternative approaches (e.g. Naive
Bayes), since it was found that including this infor-
mation did not improve performance.
3.2 Automatically generating training
examples
The approaches used for generating training exam-
ples used here are based on the work of Stevenson
and Guo (2010), who describe two approaches:
1. Monosemous relatives
2. Co-occurring concepts
233
Both approaches are provided with a set of ambigu-
ous CUIs from the UMLS Metathesaurus, which
represent the possible meanings of an ambiguous
term, and a target number of training examples to be
generated for each CUI. Each CUI is associated with
at least one term and each term is labeled with a lex-
ical unique identifier (LUI) which represents a range
of lexical variants for a particular term. The UMLS
Metathesaurus contains a number of data files which
are exploited within these techniques, including:
AMBIGLUI: a list of cases where a LUI is linked
to multiple CUIs.
MRCON: every string or concept name in the
Metathesaurus appears in this file.
MRCOC: co-occuring concepts.
For the monosemous relatives approach, the
strings of monosemous LUIs of the target CUI
and its relatives are used to search Medline to re-
trieve training examples. The monosemous LUIs re-
lated to a CUI are defined as any LUIs associated
with the CUI in the MRCON table and not listed in
AMBIGLUI table.
The co-occurring concept approach works differ-
ently. Instead of using strings of monosemous LUIs
of the target CUI and its relatives, the strings associ-
ated with LUIs of a number of co-occurring CUIs of
the target CUI and its relatives found in MRCOC ta-
ble are used. The process starts by finding the LUIs
of the top n co-occurring CUIs of the target CUI.
These LUIs are then used to form search queries.
The query is quite restrictive in the beginning and re-
quires all terms appear in the Medline citations files.
Subsequently queries are made less restrictive by re-
ducing the number of required terms in the query.
These techniques were used to generate labeled
examples for all terms that are ambiguous in the
2010 AB version of the UMLS Metathesaurus.2 The
set of all ambiguous terms was created by analysing
the AMBIGLUI table, to identify CUIs that are asso-
ciated with multiple LUIs. The Medline Baseline
Repository (MBR)3 was also analysed and it was
found that some terms were ambiguous in this re-
source, in the sense that more than one CUI had been
2Stevenson and Guo (2010) applied them to a small set of
examples from the NLM-WSD data set (see Section 4.1).
3http://mbr.nlm.nih.gov
assigned to an instance of a term, but could not be
identified from the AMBIGLUI table. The final list
of ambiguous CUIs was created by combining those
identified from the AMBIGLUI table and those find
in the MBR. This list contained a total of 103,929
CUIs.
Both techniques require large number of searches
over the Medline database and to carry this out ef-
ficiently the MBR was indexed using the Lucene
Information Retrieval system4 and all searches ex-
ecuted locally.
Examples were generated using both approaches.
The monosemous relatives approach generated ex-
amples for 98,462 CUIs and the co-occurring con-
cepts for 98,540. (Examples generated using the
monosemous relatives approach were preferred for
the experiments reported later.) However, neither
technique was able to generate examples for 5,497
CUIs, around 5% of the total. This happened when
none of the terms associated with a CUI returned
any documents when queried against the MBR and
that CUI does not have any monosemous relatives.
An example is C1281723 ?Entire nucleus pulpo-
sus of intervertebral disc of third lumbar vertebra?.
The lengthy terms associated with this CUI do not
return any documents when used as search terms
and, in addition, it is only related to one other CUI
(C0223534 ?Structure of nucleus pulposus of inter-
vertebral disc of third lumbar vertebra?) which is it-
self only connected to C1281723. Fortunately there
are relatively few CUIs for which no examples could
be generated and none of them appear in the MBR,
suggesting they refer to UMLS concepts that do not
tend to be mentioned in documents.
3.3 Generating Bias
Three different techniques for deciding the number
of training examples to be generated for each CUI
(i.e. the bias) were explored.
Uniform Bias (UB) uses an equal number of
training examples to generate centroid vectors for
each of the possible senses of the ambiguous term.
Gold standard bias (GSB) is similar to the uni-
form bias but instead of being the same for all pos-
sible CUIs the number of training examples for each
CUI is determined by the number of times it appears
4http://lucene.apache.org/
234
in a manually labeled gold standard corpus. Assume
t is an ambiguous term and Ct is the set of possible
meanings (CUIs). The number of training examples
used to generate the centroid for that CUI, Ec, is
computed according to equation 3 where Gc is the
number of instances in the gold standard corpus an-
notated with CUI c and n is a constant which is set
to 100 for these experiments.5
Ec =
Gc
?
ci  Ct
Gci,t
.n (3)
The final technique, Metamap Baseline Repos-
itory Bias (MBB), is based on the distribution of
CUIs in the MBR. The number of training examples
are generated in a similar way to the gold standard
bias with MBR being used instead of a manually la-
beled corpus and is shown in equation 4 whereMc is
the number of times the CUI c appears in the MBR.
Ec =
Mc
?
ci  Ct
Mci
.n (4)
For example, consider the three possible CUIs as-
sociated with term ?adjustment? in the NLM-WSD
corpus: C0376209, C0456081 and C06832696.
The corpus contains 18 examples of C0376209,
62 examples of C0456081 and 13 of C0683269.
Using equation 3, the number of training exam-
ples when GSB is applied for C0376209 is 20,
67 for C0456081 and 14 for C0683269. In the
Metamap Baseline Repository files, C0376209 has
a frequency count of 98046, C0456081 a count of
292809 and C0683269 a count of 83530. Therefore
the number of training examples used for the three
senses when applying MBB is: 21 for C0376209, 62
for C0456081 and 18 for C0683269.
4 Evaluation
4.1 Data sets
We evaluate our system on two datasets: the NLM-
WSD and MSH-WSD corpora.
5Small values for Ec are rounded up to ensure that any rare
CUIs have at least one training example.
6These CUIs are obtained using the mappings from NLM-
WSD senses to CUIs available on the NLM website: http:
//wsd.nlm.nih.gov/collaboration.shtml
The NLM-WSD corpus7 (Weeber et al, 2001) has
been widely used for experiments on WSD in the
biomedical domain, for example (Joshi et al, 2005;
Leroy and Rindflesch, 2005; McInnes et al, 2007;
Savova et al, 2008). It contains 50 ambiguous terms
found in Medline with 100 examples of each. These
examples were manually disambiguated by 11 an-
notators. The guidelines provided to the annotators
allowed them to label a senses as ?None? if none
of the concepts in the UMLS Metathesaurus seemed
appropriate. These instances could not be mapped
onto UMLS Metathesaurus and were ignored for our
experiments.
The larger MSH-WSD corpus (Jimeno-Yepes et
al., 2011) contains 203 strings that are associated
with more than one possible MeSH code in the
UMLS Metathesaurus. 106 of these are ambiguous
abbreviations, 88 ambiguous terms and 9 a combi-
nation of both. The corpus contains up to 100 ex-
amples for each possible sense and a total of 37,888
examples of ambiguous strings taken from Medline.
Unlike the NLM-WSD corpus, all of the instances
can be mapped to the UMLS Metathesaurus and
none was removed from the dataset for our exper-
iments.
The two data sets differ in the way the number
of instances of each sense was determined. For
the NLM-WSD corpus manual annotation is used to
decide the number of instances that are annotated
with each sense of an ambiguous term. However,
the NLM-MSH corpus was constructed automati-
cally and each ambiguous term has roughly the same
number of examples of each possible sense.
4.2 Experiments
The WSD system described in Section 3 was tested
using each of the three techniques for determining
the bias, i.e. number of examples generated for each
CUI. Performance is compared against various alter-
native approaches.
Two supervised approaches are included. The
first, most frequent sense (MFS) (McCarthy et al,
2004), is widely used baseline for supervised WSD
systems. It consists of assigning each ambiguous
term the meaning that is more frequently observed
in the training data. The second supervised approach
7http://wsd.nlm.nih.gov
235
is to train the WSD system using manually labeled
examples from the NLM-WSD and MSH-WSD cor-
pora. 10-fold cross validation is applied to evaluate
this approach.
Performance of the Personalised Page Rank ap-
proach described in Section 2.1 is also provided to
allow comparison with an unsupervised algorithm.
Both Personalised Page Rank and the techniques
we employ to generate labeled data, base disam-
biguation decisions on information from the UMLS
Metathesaurus.
The performance of all approaches is measured
in terms of the percentage of instances which are
correctly disambiguated for each term with the av-
erage across all terms reported. Confidence inter-
vals (95%) computed using bootstrap resampling
(Noreen, 1989) are also shown.
5 Results
Results of the experiments are shown in Table 1
where the first three rows show performance of the
approach described in Section 3 using the three
methods for computing the bias (UB, MMB and
GSB). MFS and Sup refer to the Most Frequent
Sense supervised baseline and using manually la-
beled examples, respectively, and PPR to the Per-
sonalised PageRank approach.
When the performance of the approaches us-
ing automatically labeled examples (UB, MMB and
GSB) is compared it is not surprising that the best re-
sults are obtained using the gold standard bias since
this is obtained from manually labeled data. Results
using this technique for computing bias always out-
perform the other two, which are completely unsu-
pervised and do not make use of any information
from manually labeled data. However, the improve-
ment in performance varies according to the corpus,
for the NLM-WSD corpus there is an improvement
of over 10% in comparison to UB while the corre-
sponding improvement for the MSH-WSD corpus is
less than 0.5%.
A surprising result is that performance obtained
using the uniform bias (UB) is consistently better
than using the bias obtained by analysis of the MBR
(MMB). It would be reasonable to expect that in-
formation about the distribution of CUIs in this cor-
pus would be helpful for WSD but it turns out that
making no assumptions whatsoever about their rel-
ative frequency, i.e., assigning a uniform baseline,
produces better results.
The relative performance of the supervised (MFS,
Sup and GSB) and unsupervised approaches (UB,
MMB and PPR) varies according to the corpus. Un-
surprisingly using manually labeled data (Sup) out-
performs all other approaches on both corpora. The
supervised approaches also outperform the unsuper-
vised ones on the NLM-WSD corpus. However, for
the MSH-WSD corpus all of the unsupervised ap-
proaches outperform the MFS baseline.
A key reason for the differences in these results is
the different distributions of senses in the two cor-
pora, as shown by the very different performance of
the MFS approach on the two corpora. This is dis-
cussed in more detail later (Section 5.2).
Comparison of the relative performance of the un-
supervised approaches (UB, MMB and PPR) shows
that training a supervised system with the automat-
ically labeled examples using a uniform bias (UB)
always outperforms PPR. This demonstrates that
this approach outperforms a state-of-the-art unsu-
pervised algorithm that relies on the same infor-
mation used to generate the examples (the UMLS
Metathesaurus).
5.1 Performance by Ambiguity Type
The MSH-WSD corpus contains both ambiguous
terms and abbreviations (see Section 4.1). Perfor-
mance of the approaches on both types of ambiguity
are shown in Table 2.
MSH-WSD Ambiguity Type
Approach Abbreviation Term
UB 91.40 [91.00, 91.75] 72.68 [72.06, 73.32]
MMB 84.43 [83.97, 84.89] 69.45 [68.86, 70.10]
GSB 90.82 [90.45, 91.22] 73.96 [73.40, 74.62]
MFS 52.43 [51.73, 53.05] 51.76 [51.11, 52.36]
Sup. 97.41 [97.19, 97.62] 91.54 [91.18, 91.94]
PPR 86.40 [86.00, 86.85] 68.40 [67.80, 69.14]
Table 2: WSD evaluation results for abbreviations and
terms in the MSH-WSD data set.
The relative performance of the different ap-
proaches on the terms and abbreviations is similar to
the entire MSH-WSD data set (see Table 1). In par-
236
Corpus
Approach Type NLM-WSD MSH-WSD
UB Unsup. 74.00 [72.80, 75.29] 83.19 [82.87, 83.54]
MMB Unsup. 71.18 [69.94, 72.38] 78.09 [77.70, 78.46]
GSB Sup. 84.28 [83.12, 85.36] 83.39 [83.08, 83.67]
MFS Sup. 84.70 [83.67, 85.81] 52.01 [51.50, 52.45]
Sup Sup. 90.69 [89.87, 91.52] 94.83 [94.63, 95.02]
PPR Unsup. 68.10 [66.80, 69.23] 78.60 [78.23, 78.90]
Table 1: WSD evaluation results on NLM-WSD and MSH-WSD data sets.
ticular using automatically generated examples with
a uniform bias (UB) outperforms using the bias de-
rived from the Medline Baseline Repository (MBR)
while using the gold standard baseline (GSB) im-
proves results slightly for terms and actually reduces
them for abbreviations.
Results for all approaches are higher when disam-
biguating abbreviations than terms which is consis-
tent with previous studies that have suggested that
in biomedical text abbreviations are easier to disam-
biguate than terms.
5.2 Analysis
An explanation of the reason for some of the re-
sults can be gained by looking at the distributions
of senses in the various data sets used for the ex-
periments. Kullback-Leibler divergence (or KL di-
vergence) (Kullback and Leibler, 1951) is a com-
monly used measure for determining the difference
between two probability distributions. For each term
t, we define S as the set of possible senses of t,
the sense probability distributions of t as D and D?.
Then the KL divergence between the sense probabil-
ity distributions D and D? can be calculated accord-
ing to equation 5.
KL(D||D?) =
?
s  S
D(s). log
D(s)
D?(s)
(5)
The three techniques for determining the bias de-
scribed in Section 3.3 each generate a probability
distribution over senses. Table 2 shows the average
KL divergence when the gold standard distribution
obtained from the manually labeled data (GSB) is
compared with the uniform bias (UB) and bias ob-
tained by analysing the Medline Baseline Reposi-
tory (MMB).
Corpus
Avg. KL Divergence NLM-WSD MSH-WSD
KL(GSB||MMB) 0.5649 0.4822
KL(GSB||UB) 0.4600 0.0406
Table 3: Average KL divergence of sense probability dis-
tributions in the NLM-WSD and MSH-WSD data sets.
The average KL divergence scores in the table
are roughly similar with the exception of the much
lower score obtained for the gold-standard and uni-
form bias for the MSH-WSD corpus (0.0406). This
is due to the fact that the MSH-WSD corpus was
designed to have roughly the same number of ex-
amples for each sense, making the sense distribu-
tion close to uniform (Jimeno-Yepes et al, 2011).
This is evident from the MFS scores for the MSH-
WSD corpus which are always close to 50%. This
also provides as explanation of why performance us-
ing automatically generated examples on the MSH-
WSD corpus only improves by a small amount when
the gold standard bias is used (see Table 1). The gold
standard bias simply does not provide much addi-
tional information to the WSD system. The situa-
tion is different in the NLM-WSD corpus, where the
MFS score is much higher. In this case the additional
information available in the gold standard sense dis-
tribution is useful for the WSD system and leads to
a large improvement in performance.
In addition, this analysis demonstrates why per-
formance does not improve when the bias gener-
ated from the MBR is used. The distributions which
are obtained are different from the gold standard
and are therefore mislead the WSD system rather
than providing useful information. The difference
between these distributions would be expected for
237
the MSH-WSD corpus, since it contains roughly the
same number of examples for each possible sense
and does not attempt to represent the relative fre-
quency of the different senses. However, it is sur-
prising to observe a similar difference for the NLM-
WSD corpus, which does not have this constraint.
The difference suggests the information about CUIs
in the MBR, which is generated automatically, has
some limitations.
Table 4 shows a similar analysis for the MSH-
WSD corpus when abbreviations and terms are con-
sidered separately and supports this analysis. The
figures in this table show that the gold standard and
uniform distributions are very similar for both ab-
breviations and terms, which explains the similar re-
sults for UB and GSB in Table 2. However, the gold
standard distribution is different from the one ob-
tained from the MBR. The drop in performance of
MMB compared with GBS in Table 2 is a conse-
quence of this.
Ambiguity Type
Avg. KL Divergence Abbreviation Term
KL(GSB||MMB) 0.4554 0.4603
KL(GSB||UB) 0.0544 0.0241
Table 4: Average KL divergence for abbreviations and
terms in the MSH-WSD data set.
6 Conclusion
This paper describes the development of a large
scale WSD system based on automatically labeled
examples. We find that these examples can be gener-
ated for the majority of CUIs in the UMLS Metathe-
saurus. Evaluation on the NLM-WSD and MSH-
WSD data sets demonstrates that the WSD system
outperforms the PPR approach without making any
use of labeled data.
Three techniques for determining the number of
examples to use for training are explored. It is
found that a supervised approach (which makes use
of manually labeled data) provides the best results.
Surprisingly it was also found that using information
from the MBR did not improve performance. Anal-
ysis showed that the sense distributions extracted
from the MBR were different from those observed
in the evaluation data, providing an explanation for
this result.
Evaluation showed that accurate information
about the bias of training examples is useful for
WSD systems and future work will explore other un-
supervised ways of obtaining this information. Al-
ternative techniques for generating labeled examples
will also be explored. In addition, further evaluation
of the WSD system will be carried out, such as ap-
plying it to an all words task and within applications.
Acknowledgements
This research has been supported by the Engineer-
ing and Physical Sciences Research Council and a
Google Research Award.
References
E. Agirre and D. Mart??nez. 2004a. The Basque Country
University system: English and Basque tasks. In Rada
Mihalcea and Phil Edmonds, editors, Proceedings of
Senseval-3, pages 44?48, Barcelona, Spain.
E. Agirre and D. Mart??nez. 2004b. Unsupervised WSD
Based on Automatically Retrieved Examples: The
Importance of Bias. In Proceedings of EMNLP-04,
Barcelona, Spain.
E. Agirre, A. Sora, and M. Stevenson. 2010. Graph-
based word sense disambiguation of biomedical docu-
ments. Bioinformatics, 26(22):2889?2896.
R. Artstein and M. Poesio. 2008. Inter-Coder Agree-
ment for Computational Linguistics. Computational
Linguistics, 34(4):555?596.
S. Brin. 1998. Extracting Patterns and relations from the
Word-Wide Web. In Proceedings of WebDB?98.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense
Disambiguation by Selecting the Best Semantic Type
Based on Journal Descriptor Indexing: Preliminary ex-
periment. Journal of the American Society for Infor-
mation Science and Technology, 57(5):96?113.
A. Jimeno-Yepes and A. Aronson. 2010. Knowledge-
based biomedical word sense disambiguation: com-
parison of approaches. BMC Bioinformatics,
11(1):569.
A. Jimeno-Yepes, B. McInnes, and A. Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to generate
a data set for word sense disambiguation. BMC Bioin-
formatics, 12(1):223.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Com-
parative Study of Support Vector Machines Applied to
the Word Sense Disambiguation Problem for the Med-
ical Domain. In Proceedings of IICAI-05, pages 3449?
3468, Pune, India.
238
M. Krallinger and A. Valencia. 2005. Text mining and
information retrieval services for molecular biology.
Genome Biology, 6(7):224.
S. Kullback and R. A. Leibler. 1951. On Information and
Sufficiency. The Annals of Mathematical Statistics,
22(1):79?86.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
165.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with Small Datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, S. Johnson, and C. Friedman. 2002. Au-
tomatic Resolution of Ambiguous Terms Based on
Machine Learning and Conceptual Relations in the
UMLS. Journal of the American Medical Informatics
Association, 9(6):621?636.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding Predominant Word Senses in Untagged
Text. In Proceedings of ACL-2004, pages 280?287,
Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the AMIA Symposium, pages 533?537,
Chicago, IL.
R. Mooney. 1996. Comparative Experiments on Disam-
biguating Word Senses: An Illustration of the Role of
Bias in Machine Learning. In Proceedings of EMNLP-
96, pages 82?91, Philadelphia, PA.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
G. Savova, A. Coden, I. Sominsky, R. Johnson, P. Ogren,
C. de Groen, and C. Chute. 2008. Word Sense Disam-
biguation across Two Domains: Biomedical Literature
and Clinical Notes. Journal of Biomedical Informat-
ics, 41(6):1088?1100.
M. Stevenson and Y. Guo. 2010. Disambiguation of Am-
biguous Biomedical Terms using Examples Generated
from the UMLS Metathesaurus. Journal of Biomedi-
cal Informatics, 43(5):762?773.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMIA Symposium, pages
746?50, Washington, DC.
J. Westbrook, E. Coiera, and A. Gosling. 2005. Do On-
line Information Retrieval Systems Help Experienced
Clinicians Answer Clinical Questions? Journal of the
American Medical Informatics Association, 12:315?
321.
H. Xu, J. Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,
and Friedman C. 2007. Gene symbol disambigua-
tion using knowledge-based profiles. Bioinformatics,
23(8):1015?22.
A. Jimeno Yepes and A. Aronson. 2011. Self-training
and co-training in biomedical word sense disambigua-
tion. In Proceedings of BioNLP 2011 Workshop, pages
182?183, Portland, Oregon, USA, June.
239
