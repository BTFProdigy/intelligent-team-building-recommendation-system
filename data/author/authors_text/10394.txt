Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 513?520
Manchester, August 2008
Authorship Attribution and Verification with Many Authors and Limited
Data
Kim Luyckx and Walter Daelemans
CNTS Language Technology Group
University of Antwerp
Prinsstraat 13, 2000 Antwerp, Belgium
{kim.luyckx,walter.daelemans}@ua.ac.be
Abstract
Most studies in statistical or machine
learning based authorship attribution focus
on two or a few authors. This leads to
an overestimation of the importance of the
features extracted from the training data
and found to be discriminating for these
small sets of authors. Most studies also
use sizes of training data that are unreal-
istic for situations in which stylometry is
applied (e.g., forensics), and thereby over-
estimate the accuracy of their approach in
these situations. A more realistic interpre-
tation of the task is as an authorship ver-
ification problem that we approximate by
pooling data from many different authors
as negative examples. In this paper, we
show, on the basis of a new corpus with
145 authors, what the effect is of many
authors on feature selection and learning,
and show robustness of a memory-based
learning approach in doing authorship at-
tribution and verification with many au-
thors and limited training data when com-
pared to eager learning methods such as
SVMs and maximum entropy learning.
1 Introduction
In traditional studies on authorship attribution, the
focus is on small sets of authors. Trying to classify
an unseen text as being written by one of two or
of a few authors is a relatively simple task, which
c
?Kim Luyckx & Walter Daelemans, 2008. Li-
censed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
in most cases can be solved with high reliabil-
ity and accuracies over 95%. An early statistical
study by Mosteller and Wallace (1964) adopted
distributions of function words as a discriminat-
ing feature to settle the disputed authorship of the
Federalist Papers between three candidate authors
(Alexander Hamilton, James Madison, and John
Jay). The advantage of distributions of function
words and syntactic features is that they are not
under the author?s conscious control, and there-
fore provide good clues for authorship (Holmes,
1994). Frequencies of rewrite rules (Baayen et
al., 1996), n-grams of syntactic labels from par-
tial parsing (Hirst and Feiguina, 2007), n-grams of
parts-of-speech (Diederich et al, 2000), function
words (Miranda Garc??a and Calle Mart??n, 2007),
and functional lexical features (Argamon et al,
2007) have all been claimed to be reliable markers
of style. There is of course a difference between
claims about types of features and claims about in-
dividual features of that type. E.g., it may be cor-
rect to claim that distributions of function words
are important markers of author identity, but the
distribution of a particular function word, while
useful to distinguish between one particular pair
of authors, may be irrelevant when comparing an-
other pair of authors.
The field of authorship attribution is however
dominated by studies potentially overestimating
the importance of these specific predictive features
in experiments discriminating between only two or
a few authors. Taking into account a larger set of
authors allows the computation of the degree of
variability encountered in text on a single topic of
different (types of) features. Recently, research has
started to focus on authorship attribution on larger
sets of authors: 8 (Van Halteren, 2005), 20 (Arga-
mon et al, 2003), 114 (Madigan et al, 2005), or up
513
to thousands of authors (Koppel et al, 2006) (see
Section 5).
A second problem in traditional studies are the
unrealistic sizes of training data, which also makes
the task considerably easier. Researchers tend to
use over 10,000 words per author (Argamon et al,
2007; Burrows, 2007; Gamon, 2004; Hirst and
Feiguina, 2007; Madigan et al, 2005; Stamatatos,
2007), which is regarded to be ?a reliable mini-
mum for an authorial set? (Burrows, 2007). When
no long texts are available, for example in po-
ems (Coyotl-Morales et al, 2006) or student es-
says (Van Halteren, 2005), a large number of short
texts is selected for training for each author. One of
the few studies focusing on small texts is Feiguina
and Hirst (2007), but they select hundreds of these
short texts (here 100, 200 or 500 words). The ac-
curacy of any of these studies with unrealistic sizes
of training data is overestimated when compared
to realistic situations. When only limited data is
available for a specific author, the author attribu-
tion task becomes much more difficult. In foren-
sics, where often only one small text per candidate
author is available, traditional approaches are less
reliable than expected from reported results.
In this paper, we present a more realistic inter-
pretation of the authorship attribution task, viz. as
a problem of authorship verification. This is a
much more natural task, since the group of poten-
tial authors for a document is essentially unknown.
Forensic experts not only want to identify the au-
thor given a small set of suspects, they also want
to make sure the author is not someone else not
under investigation. They often deal with short e-
mails or letters and have only limited data avail-
able. The central question in authorship verifica-
tion is Did candidate author x write the document?
Of the three basic approaches to authorship veri-
fication - also including a one-class learning ap-
proach (Koppel et al, 2007) - we selected a one vs.
all approach. This approach is similar to the one
investigated by Argamon et al (2003), which al-
lows for a better comparison of results. With only
few positive instances and a large number of neg-
ative instances to learn from, we are dealing with
highly skewed class distributions.
We show, on the basis of a new corpus with 145
authors, what the effect is of many authors on fea-
ture selection and learning, and show robustness
of a memory-based learning approach in doing au-
thorship attribution and verification with many au-
thors and limited training data when compared to
eager learning methods such as SVMs and max-
imum entropy learning. As far as feature selec-
tion is concerned, we find that similar types of fea-
tures tend to work well for small and large sets of
authors, but that no generalisations can be made
about individual features. Classification accuracy
is clearly overestimated in authorship attribution
with few authors. Experiments in authorship veri-
fication with a one vs. all approach reveal that ma-
chine learning methods are able to correctly clas-
sify up to 56% of the positive instances in test data.
For our experiments, we use the Personae cor-
pus, a collection of student essays by 145 authors
(see Section 2). Most studies in stylometry focus
on English, whereas our focus is on Dutch written
language. Nevertheless, the techniques used are
transferable to other languages.
2 Corpus
The 200,000-word Personae corpus
1
used in this
study consists of 145 student (BA level) essays of
about 1400 words about a documentary on Artifi-
cial Life, thereby keeping markers of genre, regis-
ter, topic, age, and education level relatively con-
stant. These essays contain a factual description of
the documentary and the students? opinion about
it. The task was voluntary and students produc-
ing an essay were rewarded with two cinema tick-
ets. The students also took an online Myers-Briggs
Type Indicator (MBTI) (Briggs Myers and Myers,
1980) test and submitted their profile, the text and
some user information via a website. All students
released the copyright of their text and explicitly
allowed the use of their text and associated per-
sonality profile for research, which makes it pos-
sible to distribute the corpus. The corpus cannot
only be used for authorship attribution and verifi-
cation experiments, but also for personality predic-
tion. More information about the motivation be-
hind the corpus and results from exploratory ex-
periments in personality prediction can be found
in Luyckx & Daelemans (2008).
3 Methodology
We approach authorship attribution and verifica-
tion as automatic text categorization tasks that la-
bel documents according to a set of predefined cat-
egories (Sebastiani, 2002, 3). Like in most text cat-
1
The Personae corpus can be downloaded from
http://www.cnts.ua.ac.be/?kim/Personae.html
514
egorization systems, we take a two-step approach
in which our system (i) achieves automatic selec-
tion of features that have high predictive value for
the categories to be learned (see Section 3.1), and
(ii) uses machine learning algorithms to learn to
categorize new documents by using the features se-
lected in the first step (see Section 3.2).
3.1 Feature Extraction
Syntactic features have been proposed as more re-
liable style markers than for example token-level
features since they are not under the conscious
control of the author (Baayen et al, 1996; Arga-
mon et al, 2007). To allow the selection of lin-
guistic features rather than (n-grams of) terms, ro-
bust and accurate text analysis tools such as lem-
matizers, part of speech taggers, chunkers etc.,
are needed. We use the Memory-Based Shallow
Parser (MBSP) (Daelemans and van den Bosch,
2005), which gives an incomplete parse of the
input text, to extract reliable syntactic features.
MBSP tokenizes the input, performs a part-of-
speech analysis, looks for noun phrase, verb phrase
and other phrase chunks and detects subject and
object of the sentence and a number of other gram-
matical relations.
Word or part-of-speech (n-grams) occurring
more often than expected with either of the cate-
gories are extracted automatically for every docu-
ment. We use the ?
2
metric (see Figure 1), which
calculates the expected and observed frequency for
every item in every category, to identify features
that are able to discriminate between the categories
under investigation.
?
2
=
k
?
i=1
(?
i
? ?
i
)
2
?
i
Figure 1: Chi-square formula
Distributions of n-grams of lexical features (lex)
are represented numerically in the feature vectors,
as well as of n-grams of both fine-grained (pos)
and coarse-grained parts-of-speech (cgp). The
most predictive function words are present in the
fwd feature set. For all of these features, the ?
2
value is calculated.
An implementation of the Flesch-Kincaid met-
ric indicating the readability of a text, along with
its components (viz., mean word and sentence
length) and the type-token ratio (which indicates
vocabulary richness) are also represented (tok).
3.2 Experimental Set-Up
This paper focuses on three topics, each with their
own experimental set-up:
(a) the effect of many authors on feature selection
and learning;
(b) the effect of limited data in authorship attri-
bution;
(c) the results of authorship verification using
many authors and limited data on learning.
For (a), we perform experiments in authorship
attribution while gradually increasing the number
of authors. First, we select a hundred random sam-
ples of 2, 5 and 10 authors in order to minimize the
effect of chance, then select one random sample of
20, 50, 100 authors and finally experiment with all
145 authors (Section 5.1).
We investigate (b) by performing authorship at-
tribution on 2 and 145 authors while gradually in-
creasing the amount of training data, keeping test
set size constant at 20% of the entire corpus. The
resulting learning curve will be used to compare
performance of eager and lazy learners (see Sec-
tion 5.1).
The authorship verification task (c) - which is
closer to a realistic situation in e.g. forensics -
using limited data and many authors is approxi-
mated as a skewed binary classification task (one
vs. all). For each of the 145 authors, we have 80%
of the text in training and 20% in test. The neg-
ative class contains 80% of each of the other 144
author?s training data in training and 20% in test
(see Section 5.2).
All experiments for (a), (b) and (c) are per-
formed using 5-fold cross-validation. This allows
us to get a reliable indication of how well the
learner will do when it is asked to make new pre-
dictions on the held-out test set. The data set is di-
vided into five subsets containing two fragments of
equal size per author. Five times one of the subsets
is used as test set and the other subsets as training
set.
The feature vectors that are fed into the ma-
chine learning algorithm contain the top-n features
(n=50) with highest ?
2
value. Every text fragment
is split into ten equal parts, each part being repre-
sented by means of a feature vector, resulting in
1450 vectors per fold (divided over training and
test).
515
For classification, we experimented with both
lazy and eager supervised learning methods.
As an implementation of the lazy learning ap-
proach we used TiMBL (Tilburg Memory-Based
Learner) (Daelemans et al, 2007), a supervised in-
ductive algorithm for learning classification tasks
based on the k-nn algorithm with various exten-
sions for dealing with nominal features and fea-
ture relevance weighting. Memory-based learning
stores feature representations of training instances
in memory without abstraction and classifies new
instances by matching their feature representation
to all instances in memory. From these ?nearest
neighbors?, the class of the test item is extrapo-
lated.
As eager learners, we selected SMO, an im-
plementation of Support-Vector Machines (SVM)
using Sequential Minimal Optimization (Platt,
1998), and Maxent, an implementation of Maxi-
mum Entropy learning (Le, 2006). SMO is em-
bedded in the WEKA (Waikato Environment for
Knowledge Analysis) software package (Witten
and Frank, 1999).
Our expectation is that eager learners will tend
to overgeneralize for this task when dealing with
limited training data, while lazy learners, by de-
laying generalization over training data until the
test phase, will be at an advantage when dealing
with limited data. Unlike eager learners, they will
not ignore - i.e. not abstract away from - the fre-
quently occurring infrequent or untypical patterns
in the training data, that will nevertheless be useful
in generalization.
4 Results and Discussion
In this section, we present results of experiments
concerning the three main issues of this paper (see
Section 3.2 for the experimental set-up):
(a) the effect of many authors on feature selection
and learning;
(b) the effect of limited data in authorship attri-
bution;
(c) the results of authorship verification using
many authors and limited data on learning.
4.1 Authorship Attribution
(a) Figure 2 shows the effect of many authors in
authorship attribution experiments using memory-
based learning (TiMBL) (k=1) and separate fea-
ture sets. Most authorship attribution studies fo-
Number of authors
Accur
acy in
 %
20
40
60
80
100x2 100x5 100x10 20 50 100 145
cgp1cgp2cgp3 fwd1lex1lex2 lex3pos1pos2 pos3tok
Figure 2: The effect of many authors using single
feature sets
cus on a small set of authors and report good re-
sults, but systematically increasing the amount of
authors under investigation leads to a significant
decrease in performance. In the 2-author task (100
experiments with random samples of 2 authors),
we achieve an average accuracy of 96.90%, which
is in line with results reported in other studies on
small sets of authors. The 5-, 10- (both in 100
experiments with random samples) and 20-author
tasks show a gradual decrease in performance with
results up to 88%, 82% and 76% accuracy, respec-
tively. A significant fall in accuracy comes with the
50- and 100-author attribution task, where accu-
racy drops below 52% for the best performing fea-
ture sets. Experiments with all 145 authors from
the corpus (as a multiclass problem) show an ac-
curacy up to 34%. Studies reporting on accuracies
over 95% are clearly overestimating their perfor-
mance on a small set of authors.
Incremental combinations of feature sets per-
forming well in authorship attribution lead to an
accuracy of almost 50% in the 145-author case, as
is shown in Figure 3. This indicates that provid-
ing a more heterogeneous set of features improves
the system significantly. Memory-based learning
shows robustness for a large set of authors in au-
thorship attribution.
As far as feature selection is concerned, we find
that similar types of features tend to work well for
small and large sets of authors in our corpus, but
that no generalisations can be made about individ-
ual features towards other corpora or studies, since
this is highly dependent of the specific authors se-
lected.
516
Number of authors
Accur
acy in
 %
20
40
60
80
100x2 100x5 100x10 20 50 100 145
lex1+pos1lex1+pos1+toklex1+pos1+tok+fwdlex1+pos2
lex1+pos2+toklex1+pos2+tok+fwdlex1+tok
Figure 3: The effect of many authors using combi-
nations of feature sets
Number of authors
Evolu
tion o
f chi?
squar
e valu
e
0
10
20
30
40
100x2 100x5 100x10 20 50 100 145
?conclusieerg iklangsman miermijnstrengen was
Figure 4: The effect of many authors on ?
2
value
for 2-author discriminating features
Figure 4 shows the top-ten features with highest
?
2
value in one of the randomly selected 2-author
samples. In 5-author cases, we see that some of
these features have some discriminating power, but
with the increase of the number of authors comes
a decrease in importance.
(b) The effect of limited data is demonstrated
by means of a learning curve. The performance
of lazy learner TiMBL is compared to that of ea-
ger learners Maxent (Maximum Entropy Learning)
and SMO (Support-Vector Machines) when com-
paring different training set sizes. Figure 5 shows
the evolution of learning in authorship attribution
using the lex1 feature set. Although memory-based
learning does show robustness when dealing with
limited data, we cannot show a clear superiority
on this aspect to the eager learning methods in this
experiment. However, results are good enough to
Percentage of training data
Accur
acy in
 %
20
40
60
80
20 40 60 80
TiMBL 2 authorsMaxentSMO TiMBL 145 authorsMaxentSMO
Figure 5: The effect of limited data in authorship
attribution on lex1
warrant continuing the experiments on authorship
verification with this method.
4.2 Authorship Verification
(c) We now focus on a more realistic interpreta-
tion of the authorship attribution task, viz. as a
authorship verification problem. Forensic experts
want to answer both questions of authorship attri-
bution (Which of the n candidate authors wrote
the document?) and verification (Did candidate
author x write the document?). They often deal
with limited data like short e-mails or letters, and
the amount of candidate authors is essentially un-
known. With only few positive instances (of 1 au-
thor) and a large amount of negative instances (of
144 authors in our corpus), we are dealing with
highly skewed class distributions.
We approximate the author verification problem
by defining a binary classification task with the au-
thor fragments as positive training data, and the
fragments of all the other authors as negative train-
ing data. A more elegant formulation would be as
a one-class problem (providing only positive train-
ing data), but in exploratory experiments, these
one-class learning approaches did not yield useful
results.
We evaluate authorship verification experiments
by referring to precision and recall of the positive
class. Recall represents the proportional number
of times an instance of the positive class has cor-
rectly been classified as positive. Precision shows
the proportion of test instances predicted by the
system to be positive that was correctly classified
as such.
517
Feature set Precision Recall F-score
tok 20.66% 15.93% 17.99%
fwd 37.89% 8.41% 13.76%
lex1 56.04% 7.03% 12.49%
lex2 47.95% 5.66% 10.12%
lex3 34.05% 8.73% 13.90%
cgp1 25.70% 24.55% 25.11%
cgp2 36.35% 18.28% 24.33%
cgp3 33.13% 3.79% 6.80%
pos1 42.42% 0.97% 1.90%
pos2 42.66% 4.21% 7.66%
pos3 38.75% 2.14% 4.06%
Table 1: Results of one vs. all Authorship Verifi-
cation experiments using MBL
Table 1 shows the results for the positive class of
one vs. all authorship verification using memory-
based learning. We see that memory-based learn-
ing on the authorship verification task is able to
correctly classify up to 56% of the positive class
which is highly underrepresented in both training
and test data. Despite the very skewed class distri-
butions, memory-based learning scores reasonably
well on this approximation of authorship verifica-
tion with limited data. The most important lesson
is that in a realistic set-up of the task of authorship
verification, the accuracy to be expected is much
lower than what in general can be found in the pub-
lished literature.
5 Related Research
As mentioned earlier, most research in author-
ship attribution starts from unrealistic assumptions
about numbers of authors and amount of training
data available. We list here the exceptions to this
general rule. These studies partially agree with our
own results. Argamon et al (2003) report on re-
sults in authorship attribution on twenty authors in
a corpus of Usenet newsgroups on a variety of top-
ics. Depending on the topic, results vary from 25%
(books, computer theory) to 45% accuracy (com-
puter language) for the 20-author task. Linguis-
tic profiling, a technique presented by Van Hal-
teren (2005), takes large numbers of linguistic fea-
tures to compare separate authors to average pro-
files. In a set of eight authors, a linguistic pro-
filing system correctly classifies 97% of the test
documents. Madigan et al (2005) use a collec-
tion of data released by Reuters consisting of 114
authors, each represented by a minimum of 200
texts. Results of Bayesian multinomial logistic re-
gression on this corpus show error rates between
97% and 20%, depending on the type of features
applied. This is only partially comparable to the
authorship attribution results on 145 authors pre-
sented in this paper because of the large amount of
data in the Madigan et al (2005) study, while our
system works on limited data. In a study of weblog
corpora, Koppel et al (2006) show that authorship
attribution with thousands of candidate authors is
reasonably reliable, since the system gave an an-
swer in 31.3% of the cases, while the answer is
correct in almost 90% of the cases. Whereas these
cases show similar results as ours, we believe this
study is the first to study the effect of training
set size and number of authors involved system-
atically.
When applied to author verification on eight
authors, the linguistic profiling system (Van Hal-
teren, 2005) has a False Reject Rate (FRR) of 0%
and a False Accept Rate (FAR) of 8.1%. Argamon
et al (2003) also report on one vs. all learning in
a set of twenty authors. Results vary from 19%
(books, computer theory) to 43% (computer lan-
guage) accuracy, depending on the topics. Madi-
gan et al (2005) also did authorship verification
experiments on their corpus of 114 authors we de-
scribed above. They vary the number of target,
decoy, and test authors to find that the ideal split
is 10-50-54, which produces an error rate of 24%.
Koppel et al (2007) also report on results in one
vs. all experiments. Using a corpus of 21 books
by 10 authors in different genres (including essays,
plays, and novels), their system scores a precision
of 22.30% and recall of 95%. Our system performs
better in precision and worse in recall. Their cor-
pus nevertheless consists of 21 books (each rep-
resented by more than forty 500-word chunks) by
10 authors, which makes the task considerably less
difficult.
6 Conclusions and Further Research
A lot of the research in authorship attribution is
performed on a small set of authors and unrealistic
sizes of data, which is an artificial situation. Most
of these studies not only overestimate the perfor-
mance of their system, but also the importance of
linguistic features in experiments discriminating
between only two or a small number of authors.
In this paper, we have shown the effect of many
authors and limited data in authorship attribution
518
and verification. When systematically increasing
the number of authors in authorship attribution, we
see that performance drops significantly. Similar
types of features work well for different amounts
of authors in our corpus, but generalizations about
individual features are not useful.
Memory-based learning shows robustness when
dealing with limited data, which is essential in e.g.
forensics. Results from experiments in authorship
attribution on 145 authors indicate that in almost
50% of the cases, a text from one of the 145 au-
thors is classified correctly. Using combinations of
good working lexical and syntactic features leads
to significant improvements. The authorship veri-
fication task is a much more difficult task, which,
in our approximation of it, leads to a correct clas-
sification in 56% of the test cases. It is clear that
studies reporting over 95% accuracy on a 2-author
study are overestimating their performance and the
importance of the features selected.
Further research with the 145-author corpus will
involve a study of handling with imbalanced data
and experimenting with other machine learning al-
gorithms for authorship attribution and verifica-
tion and a more systematic study of the behavior
of different types of learning methods (including
feature selection and other optimization issues) on
this problem.
Acknowledgements
This study has been carried out in the framework
of the ?Computational Techniques for Stylometry
for Dutch? project, funded by the National Fund
for Scientific Research (FWO) in Belgium. We
would like to thank the reviewers for their com-
ments which helped improve this paper.
References
Argamon, Shlomo, Marin Saric, and Sterling S. Stein.
2003. Style mining of electronic messages for mul-
tiple authorship discrimination: First results. In Pro-
ceedings of the 2003 Association for Computing Ma-
chinery Conference on Knowledge Discovery and
Data Mining (ACM SIGKDD), pages 475?480.
Argamon, Shlomo, Casey Whitelaw, Paul Chase,
Sushant Dawhle, Sobhan R. Hota, Navendu Carg,
and Shlomo Levitan. 2007. Stylistic text classifica-
tion using functional lexical features. Journal of the
American Society of Information Science and Tech-
nology, 58(6):802?822.
Baayen, Harald R., Hans Van Halteren, and Fiona
Tweedie. 1996. Outside the cave of shadows: Using
syntactic annotation to enhance authorship attribu-
tion. Literary and Linguistic Computing, 11(3):121?
131.
Briggs Myers, Isabel and Peter B. Myers. 1980. Gifts
differing: Understanding personality type. Moun-
tain View, CA: Davies-Black Publishing.
Burrows, John. 2007. All the way through: Testing for
authorship in different frequency data. Literary and
Linguistic Computing, 22(1):27?47.
Coyotl-Morales, Rosa M., Luis Villase?nor Pineda,
Manuel Montes-y G?omez, and Paolo Rosso. 2006.
Authorship attribution using word sequences. In
Proceedings of the Iberoamerican Congress on Pat-
tern Recognition (CIARP), pages 844?853.
Daelemans, Walter and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies
in Natural Language Processing. Cambridge, UK:
Cambridge University Press.
Daelemans, Walter, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2007. TiMBL: Tilburg Mem-
ory Based Learner, version 6.1, Reference Guide.
Technical Report ILK Research Group Technical Re-
port Series no. 07-07, ILK Research Group, Univer-
sity of Tilburg.
Diederich, Joachim, J?org. Kindermann, Edda Leopold,
and Gerhard Paass. 2000. Authorship attribution
with Support Vector Machines. Applied Intelligence,
19(1-2):109?123.
Feiguina, Ol?ga and Graeme Hirst. 2007. Authorship
attribution for small texts: literary and forensic ex-
periments. In Proceedings of the 30th International
Conference of the Special Interest Group on Infor-
mation Retrieval: Workshop on Plagiarism Analysis,
Authorship Identification, and Near-Duplicate De-
tection (SIGIR).
Gamon, Michael. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic anal-
ysis features. In Proceedings of the 2004 Inter-
national Conference on Computational Linguistics
(COLING), pages 611?617.
Hirst, Graeme and Ol?ga Feiguina. 2007. Bigrams
of syntactic labels for authorship discrimination of
short texts. Literary and Linguistic Computing,
22(4):405?417.
Holmes, D. 1994. Authorship Attribution. Computers
and the Humanities, 28(2):87?106.
Koppel, Moshe, Jonathan Schler, Shlomo Argamon,
and Eran Messeri. 2006. Authorship attribution
with thousands of candidate authors. In Proceed-
ings of the 29th International Conference of the Spe-
cial Interest Group on Information Retrieval (SI-
GIR), pages 659?660.
519
Koppel, Moshe, Jonathan Schler, and Elisheva
Bonchek-Dokow. 2007. Measuring differentiabil-
ity: Unmasking pseudonymous authors. Journal of
Machine Learning Research, 8:1261?1276.
Le, Zhang. 2006. Maximum Entropy Modeling Toolkit
for Python and C++. Version 20061005.
Luyckx, Kim and Walter Daelemans. 2008. Per-
sonae: a corpus for author and personality prediction
from text. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC).
Madigan, David, Alexander Genkin, David D. Lewis,
Shlomo Argamon, Dmitriy Fradkin, and Li Ye.
2005. Author identification on the large scale. In
Proceedings of the 2005 Meeting of the Classifica-
tion Society of North America (CSNA).
Miranda Garc??a, Antonio and Javier Calle Mart??n.
2007. Function words in authorship attribution stud-
ies. Literary and Linguistic Computing, 22(1):49?
66.
Mosteller, F. and D. Wallace. 1964. Inference and dis-
puted authorship: the Federalist. Series in Behav-
ioral Science: Quantitative Methods Edition.
Platt, John, 1998. Advances in Kernel Methods - Sup-
port Vector Learning, chapter Fast training of Sup-
port Vector Machines using Sequential Minimal Op-
timization, pages 185?208.
Sebastiani, Fabrizio. 2002. Machine learning in auto-
mated text categorization. Association for Comput-
ing Machinery (ACM) Computing Surveys, 34(1):1?
47.
Stamatatos, Efstathios. 2007. Author identification us-
ing imbalanced and limited training texts. In Pro-
ceedings of the 18th International Conference on
Database and Expert Systems Applications (DEXA),
pages 237?241.
Van Halteren, Hans. 2005. Linguistic profiling for au-
thor recognition and verification. In Proceedings of
the 2005 Meeting of the Association for Computa-
tional Linguistics (ACL).
Witten, Ian and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools with Java Implemen-
tations. San Fransisco: Morgan Kaufmann.
520
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
CNTS: Memory-Based Learning of Generating Repeated References
Iris Hendrickx, Walter Daelemans, Kim Luyckx, Roser Morante, Vincent Van Asch
CNTS, Department of Linguistics
University of Antwerp
Prinsstraat 13, 2000, Antwerp, Belgium
firstname.lastname@ua.ac.be
Abstract
In this paper we describe our machine learning
approach to the generation of referring expres-
sions. As our algorithm we use memory-based
learning. Our results show that in case of pre-
dicting the TYPE of the expression, having one
general classifier gives the best results. On the
contrary, when predicting the full set of prop-
erties of an expression, a combined set of spe-
cialized classifiers for each subdomain gives
the best performance.
1 Introduction
In this paper we describe the systems with which
we participated in the GREC task of the REG 2008
challenge (Belz and Varges, 2007). The GREC task
concerns predicting which expression is appropriate
to refer to a particular discourse referent in a certain
position in a text, given a set of alternative referring
expressions for selection. The organizers provided
the GREC corpus that consists of 2000 texts col-
lected from Wikipedia, from 5 different subdomains
(people, cities, countries, mountains and rivers) .
One of the main goals of the task is to discover
what kind of information is useful in the input to
make the decision between candidate referring ex-
pressions. We experimented with a pool of features
and several machine learning algorithms in order to
achieve this goal.
2 Method
We apply a standard machine learning approach
to the task. We train a classifier to predict the
correct label for each mention. As our machine
learning algorithm we use memory-based learn-
ing as implemented in the Timbl package (Daele-
mans et al, 2007). To select the optimal algorith-
mic parameter setting for each classifier we used
a heuristic optimization method called paramsearch
(Van den Bosch, 2004). We also tried several other
machine learning algorithms implemented in the
Weka package (Witten and Frank, 2005), but these
experiments did not lead to better results and are not
further discussed here.
We developed four systems: a system that only
predicts the TYPE of each expression (Type), so it
predicts four class labels; and a system that pre-
dicts the four properties (TYPE, EMPATHIC, HEAD,
CASE) of each expression simultaneously (Prop).
The class labels predicted by this system are con-
catenated strings: ?common no nominal plain?, and
these concatenations lead to 14 classes, which
means that not all combinations appear in the train-
ing set. For both Type an Prop we created two vari-
ants: one general classifer (g) that is trained on all
subdomains, and a set of combined specialized clas-
sifiers (s) that are optimized for each domain sepa-
rately.
3 System description
To build the feature representations, we first prepro-
cessed the texts performing the following actions:
rule-based tokenization, memory-based part-of-
speech tagging, NP-chunking, Named entity recog-
nition, and grammatical relation finding (Daelemans
and van den Bosch, 2005). We create an instance for
each mention, using the following features to repre-
194
sent each instance:
? Positional features: the sentence number, the
NP number, a boolean feature that indicates if
the mention appears in the first sentence.
? Syntactic and semantic category given of the
entity (SEMCAT, SYNCAT).
? Local context of 3 words and POS tags left and
right of the entity.
? Distance to the previous mention measured in
sentences and in NPs.
? Trigram pattern of the given syntactic cate-
gories of 3 previous mentions.
? Boolean feature indicating if the previous sen-
tence contains another named entity than the
entity in focus.
? the main verb of the sentence.
We do not use any information about the given set
of alternative expressions except for post process-
ing. In a few cases our classifier predicts a label that
is not present in the set of alternatives. For those
cases we choose the most frequent class label (as es-
timated on the training set).
We experimented with predicting all subdomains
with the same classifier and with creating separate
classifiers for each subdomains. We expected that
semantically different domains would have different
preferences for expressions.
4 Results
We provide results for the four systems Type-g,
Type-s, Prop-g and Prop-s in Table 1. The evalua-
tion script was provided by the organisers. The vari-
ant Type-g performs best with a score of 76.52% on
the development set.
5 Conclusions
In this paper we described our machine learning ap-
proach to the generation of referring expressions.
We reported results of four memory-based systems.
Predicting all subdomains with the same classifier
is more efficient when predicting the coarse-grained
TYPE class. On the contrary, training specialized
classifiers for each subdomain works better for the
Data Type-g Type-s
Cities 64.65 60.61
Countries 75.00 71.74
Mountains 75.42 77.07
People 85.37 72.50
Rivers 65.00 80.00
All 76.52 72.26
Data Prop-g Prop-s
Cities 63.64 65.66
Countries 72.83 69.57
Mountains 72.08 74.58
People 79.51 79.51
Rivers 65.00 70.00
All 73.02 73.93
Table 1: Accuracy on GREC development set.
more fine-grained prediction of all properties simu-
laneously. For the test set we will present results the
two best systems: CNTS-Type-g and CNTS-Prop-s.
Acknowledgments
This research is funded by FWO, IWT, GOA BOF UA,
and the STEVIN programme funded by the Dutch and
Flemish Governments.
References
A. Belz and S. Varges. 2007. Generation of repeated ref-
erences to discourse entities. In In Proceedings of the
11th European Workshop on Natural Language Gen-
eration (ENLG?07), pages 9?16.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2007. TiMBL: Tilburg Memory
Based Learner, version 6.1, reference manual. Techni-
cal Report 07-07, ILK, Tilburg University.
A. Van den Bosch. 2004. Wrapped progressive sampling
search for optimizing learning algorithm parameters.
In Proceedings of the 16th Belgian-Dutch Conference
on Artificial Intelligence, pages 219?226.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, second
edition. Morgan Kaufmann, San Francisco.
195
