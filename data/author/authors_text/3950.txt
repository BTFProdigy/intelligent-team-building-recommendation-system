Word Selection for EBMT based on Monolingual Similarity and Translation
Confidence
Eiji Aramaki, Sadao Kurohashi, Hideki Kashioka and Hideki Tanaka
 Graduate School of Information Science and Tech. University of Tokyo
Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki, kuro@kc.t.u-tokyo.ac.jp
 ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seika, Soraku, Kyoto 619-0288, Japan
hideki.kashioka, hideki.tanaka@atr.co.jp
Abstract
We propose a method of constructing an
example-based machine translation (EBMT)
system that exploits a content-aligned bilingual
corpus. First, the sentences and phrases in the
corpus are aligned across the two languages,
and the pairs with high translation confidence
are selected and stored in the translation mem-
ory. Then, for a given input sentences, the
system searches for fitting examples based on
both the monolingual similarity and the transla-
tion confidence of the pair, and the obtained re-
sults are then combined to generate the transla-
tion. Our experiments on translation selection
showed the accuracy of 85% demonstrating the
basic feasibility of our approach.
1 Introduction
The basic idea of example-based machine translation, or
EBMT, is that translation examples similar to a part of
an input sentence are retrieved and combined to produce
a translation(Nagao, 1984). In order to make a practi-
cal MT system based on this approach, a large number
of translation examples with structural correspondences
are required. This naturally presupposes high-accuracy
parsers and well-aligned large bilingual corpora.
Over the last decade, the accuracy of the parsers im-
proved significantly. The availability of well-aligned
bilingual corpora, however, has not increased despite our
expectations. In reality, the number of bilingual cor-
pora that share the same content, such as newspapers and
broadcast news, has increased steadily. We call this type
of corpus a content-aligned corpus. With these observa-
tions, we started a research project that covered all as-
pects of constructing EBMT systems starting from using
Figure 1: Translation Example (TE).
a content-aligned corpus, i.e., a bilingual broadcast news
corpus.
First, the sentences and phrases in the corpus are
aligned across the two languages, and the pairs with high
translation confidence are selected and stored in the trans-
lation memory. Then, translation examples are retrieved
based on both the monolingual similarity and the trans-
lation confidence of the pair. Finally, these examples are
combined to generate the translation.
This paper is organized as follows. The next sec-
tion presents how to build the translation memory from
a content-aligned corpus. Section 3 describes our EBMT
system, paying special attention to the selection of trans-
lation examples. Section 4 reports experimental results
of word selection, Section 5 describes related works, and
Section 6 gives our conclusions.
* Underlined phrases and sentences have no parallel expressions in the other language.
Figure 2: NHK News Corpus.
2 Building Translation Memory
In EBMT, an input sentence can hardly be translated by
a single translation example, except when an input is ex-
tremely short or is a typical domain-dependent sentence.
Therefore, two or more translation examples are used to
translate parts of the input and are then combined to gen-
erate a whole translation. Syntactic information is useful
for composing example fragments.
In this paper, we call a structurally aligned bilingual
sentence pair a translation example or TE (Figure 1).
This section presents our method for building TEs from a
content-aligned corpus.
Since the bilingual corpus used in our project does not
contain literal translations, automatic parsing and align-
ment inevitably contain errors. Therefore, we selected
highly likely TEs to make a translation memory.
2.1 NHK News Corpus
We used a bilingual news corpus compiled by the NHK
broadcasting service (NHK News Corpus), which con-
sists of about 40,000 Japanese-English article pairs cov-
ering a five-year period. The average number of Japanese
sentences in an article is 5.2, and that of English sentence
is 7.4. Table 2 shows an example of an article pair.
As shown in Table 2, an English article is not a literal
translation of a Japanese article, although their contents
are almost parallel.
2.2 Sentence Alignment
We used a DP matching for bilingual sentence alignment,
where we allow the matching of 1-to-1, 1-to-2, 1-to-3, 2-
to-1 and 2-to-2 Japanese and English sentence pairs. This
matching covered 84% of the following evaluation set.
We selected 96 article pairs for the evaluation of sentence
and phrase alignment, and we call this the evaluation set.
We use the following score for matching, which is based
on a ratio of corresponding content words (WCR: content
Word Corresponding Ratio).
WCR  




 (1)
where 

is the number of Japanese content words in a
unit, 

is the number of English content words, and 

is the number of content words whose translation is also
in the unit, which is found by translation dictionaries?
We used the EDR electronic dictionary, EDICT,
ENAMDICT, the ANCHOR translation dictionary, and
Figure 3: Handling of Remaining Phrases.
Figure 4: WCR and Precision.
the EIJIRO translation dictionary. These dictionaries
have about two million entries in total.
On the evaluation data, the precision of the sentence
alignment (defined as follows) was 60.7%.
precision  # of correct system outputs
# of system outputs
(2)
Among types of a corresponding unit, the precision of
1-to-1 correspondence was the best, at 77.5%. Since a 1-
to-1 correspondence is suitable for the following phrase
alignment, we decided to use only the 1-to-1 correspon-
dence results.
2.3 Phrase Alignment
The 1-to-1 sentence pairs obtained in the previous sec-
tion are then aligned at phrase level by the method based
on (Aramaki et al, 2001). The method consists of the
following pre-process and two aligning steps.
Pre-process: Conversion to phrasal dependency struc-
tures.
First, the phrasal dependency structures of the sen-
tence pair are estimated. The English parser returns
a word-based phrase structure, which is merged into
a phrase sequence by the following rules and con-
verted into a dependency structure by lifting up head
phrases.
Table 1: Number of TEs.
Corpus WCR # of TEs
0.3?0.4 18290
NHK News 0.4?0.5 6975
0.5? 2314
White Paper ? 2225
SENSEVAL ? 6920
1. Function words are grouped with the following
content word.
2. Adjoining nouns are grouped into one phrase.
3. Auxiliary verbs are grouped with the following
verb.
The Japanese parser outputs the phrasal dependency
structure of an input, and that is used as is. We used
The Japanese parser KNP (Kurohashi and Nagao,
1994) and The English nl-parser (Charniak, 2000).
Step 1: Estimation of basic phrasal correspondences.
We started with the word-level alignment to get the
basic phrasal alignment. We used translation dictio-
naries for this process. The word sense ambiguity
in the dictionaries is resolved with a heuristics that
the most plausible correspondence is near other cor-
respondences.
Step 2: Expansion of phrasal correspondences.
Finally, the remaining phrases, which were not han-
dled in the step 1, are merged into a neighboring
phrase correspondence or are used to establish a new
correspondence, depending on the surrounding ex-
isting correspondences. Figure 3 shows an example
of a new correspondence established by a structural
pattern.
These procedures can detect the phrasal alignments in
a pair of sentences as shown in Figure 1.
For phrase alignment evaluation, we selected all of the
145 sentence pairs that had 1-to-1 correspondences form
the evaluation set and gave correct content word corre-
spondences to these pairs. The phrase correspondences
detected by the system were judged correct when the cor-
respondences include the manually given content word
correspondences.
Based on this criterion, the precision of phrase align-
ment was 50%. Then, we found a correlation between
the phrase alignment precision and WCR of parallel sen-
tences as shown in Figure 4. Furthermore, the precision
of sentence alignment and WCR also have a correlation.
Since their performances nearly reaches their limits when
WCR is 0.3, we decided to use parallel sentences whose
WCR is 0.3 or greater as TEs.
Figure 5: Example of Translation.
2.4 Building Translation Memory
As explained in the preceding sections, among sentence-
aligned and phrase-aligned NHK News articles, TEs with
a 1-to-1 sentence correspondence and whose WCR is 0.3
or greater are registered in the translation memory. Table
1 shows the number of TEs for each WCR range.
In addition, the Bilingual White Paper and Translation
Memory of SENSEVAL2 (Kurohashi, 2001) were also
phrase-aligned and registered in the translation memory.
Sentence alignments are already given for these corpora.
Since their parallelism are fairly high and the accuracies
of their phrase alignments are more than 70%, we utilized
all phrase-aligned sentence pairs as TEs (Table 1).
3 EBMT System
Our EBMT system translates a Japanese sentence into
English. A Japanese input sentence is parsed and trans-
formed into a phrase-based dependency structure. Then,
for each phrase, an appropriate TE is retrieved from the
translation memory that is most suitable for translating
Figure 6: Selection of a TE.
the phrase (and its neighboring phrases). Finally, the En-
glish expressions of the TEs are combined to produce the
final English translation (Figure 5).
This section describes our EBMT system, mainly the
TE selection part.
3.1 Basic Idea of TE Selection
The basic idea of TE selection is shown in Figure 6.
When a part of the input sentence and a part of the TE
source language sentence have an equal expression, the
part of the input sentence is called I and the part of the
TE source language sentence is called S. A part of the TE
target language corresponding to S is called T. The pair S
and T is called fragment of TE (FTE).
I, S and T have to meet the following conditions, as a
natural consequence of the fact that S-T is used for trans-
lating I.
1. I, S and T are each structurally connected phrases.
2. I is equal to S except for function words at the
boundaries.
3. S corresponds to T completely, that is, all phrases in
S and T are aligned.
It might be the case that for an I, two or more FTEs that
meet the above conditions exist in the translation mem-
ory. Our method takes into account the following rela-
tions among I-S-T to select the best FTE:
1. The largest pair of I and S.
2. The similarity between the surroundings of I and
these of S.
3. The confidence of alignment between S and T.
The following sections concretely present how to cal-
culate these criteria. For simplicity of explanation, we
call a set of phrasal correspondences between S and T,
EQ; that neighboring EQ, CONTEXT; that between S and
T, ALIGN (Figure 6).
3.2 Monolingual Similarity between Japanese
Expressions
The equality between I and S is a sum of the equality
score of each phrase correspondence in EQ, which is cal-
culated as follows:
EQUAL 



 





	
 

	
 (3)
where

is the number of content words in the phrase
correspondence, 
	
is the number of function words,


is the equality between content words, and 
	
is the equality between function words. 

and 
	
are given in Table 2.1
Usually, the equality score between I and S is equal to
the number of phrases in I (the number of phrase corre-
spondences in EQ), but sometimes these are slightly dif-
ferent, depending on the conjugation type and function
words.
1All constant values in Table 2 and formulas were decided
based on preliminary experiments.
On the other hand, the similarity between the surround-
ings of I and those of S is a sum of the similarity score of
each phrase correspondence in CONTEXT, which is cal-
culated as follows:
SIM 




 






 






(4)
Basically the calculation of SIM and EQUAL is the
same, except that SIM considers the relation type between
the phrase in I and its outer phrase by 

. When
the relation is the same, the influence of the surrounding
phrases must be large, so 

is set to 1.0; when the
relation is not the same, 

is set to 0.5. The rela-
tions between phases are estimated by the function word
or conjugation type of the dependent phrase.
The monolingual similarity between Japanese expres-
sions I and S is calculated as follows:



EQUAL 



SIM (5)
3.3 Translation Confidence of Japanese-to-English
Alignment
The translation confidence of phrase alignment between
S and T is the sum of the confidence score of each phrase
correspondence in ALIGN, CONF() in Table 2, and it is
weighted by the WCR of the parallel sentences.
As a final measure, the score of I-S-T is calculated as
follows:




EQUAL 



SIM






CONF

WCR (6)
3.4 Search Algorithm of FTE
For each phrase (P) in an input sentence, the most plausi-
ble FTE is retrieved by the following algorithm:
1. FTEs are retrieved from the translation memory, in
which a Japanese phrase matches P, and it is aligned
to an English phrase. (that is, these are FTEs that
meet the basic conditions for translation in Section
3.1).
2. For each FTE obtained in the previous step, it is
checked whether the surrounding phrase of P and
that of FTE are the same or similar, phrase by
phrase, and the largest I-S-T that meets the basic
conditions is detected.
Table 2: Parameters for Similarity and Confidence Calculation.
1.1 exact match
1.0 stem match


0.5  

+ 0.3 thesaurus match
0.3 POS match
0 otherwise
* 

is a similarity calculated based on NTT thesaurus(Ikehara et al, 1997) (max = 1).
1.1 exact match


1.0 stem match
0 otherwise
1.0 all content words in alignment  correspond to each other in dic
CONF() 0.8 some content words in alignment  correspond to each other in dic
0.5 otherwise
3. The score of each I-S-T is calculated, and the best
I-S-T (S-T is the FTE) is selected as the FTE for P.
As a result of detecting FTEs for phrases in the input,
two FTEs starting from the different phrase might over-
lap each other. In such a case, we employed a greedy
search algorithm that adopts the higher score FTE one
by one; therefore, each previously adopted FTE is only
partly used for translation.
On the other hand, when no FTE is obtained for an in-
put phrase, a translation dictionary is utilized (when the
phrase contains two or more content words, the longest
matching strategy is used for dictionary look-up). When
two or more possible translations are given from the dic-
tionary, the most frequent phrase/word in the NHK News
Corpus is adopted.
Figure 5 shows examples of FTEs detected by our
method.2
3.5 Generating a Target Sentence
The English expressions in the selected FTEs are com-
bined, and the English dependency structure is con-
structed. The dependency relations in FTEs are pre-
served, and the relation between the two FTEs is esti-
mated based on the relation of the input sentences. Figure
5 shows an example of a combined English dependency
structure.
When a surface expression is generated from its depen-
dency structure, its word order must be selected properly.
This can be done by preserving the word order in FTEs
and by ordering FTEs by a set of rules governing both the
dependency relation and the word-order.
The module for controlling conjugation, determiner,
and singular/plural is not yet implemented in our current
MT system.
2As the bottom example in Figure 5 shows, EBMT can eas-
ily handle head-switching translation by using an FTE that con-
tains all of the head-switching phenomena in it.
4 Experiments
For evaluation, we selected 50 sentence pairs from the
NHK News Corpus that were not used for the translation
memory. Their source (Japanese) sentences were trans-
lated by our EBMT system, and the selected FTEs were
evaluated by hand, referring to the target (English) sen-
tences.
A phrase by phrase evaluation was done to judge
whether the English expression of the selected FTE was
good or bad. The accuracy was 85.0%.
In order to investigate the effectiveness of each com-
ponent of FTE selection, we compared the following four
methods:
1. EQCONTEXTALIGN: The proposed method.
2. EQALIGN: FTE score is calculated as follows, with-
out the CONTEXT similarity:



EQUAL()



CONF()WCR (7)
3. EQCONTEXT: FTE score is calculated as follows,
without the ALIGN confidence:



EQUAL() 



SIM() (8)
4. DICONLY: Word selection is based only on dictio-
naries and frequency in the corpus.
The accuracy of each method is shown in Table 3,
and the results indicate that the proposed method, EQ-
CONTEXTALIGN, is the best, that is, using context sim-
ilarity and align confidence works effectively. Figure 7
Figure 7: Word Selection by EQCONTEXTALIGN and DICONLY.
Table 3: Experimental Results.
Good Bad Accuracy
EQCONTEXTALIGN 268 47 85.0%
(246) (35) (87.5%)
EQALIGN 254 61 80.6 %
(233) (48) (82.9%)
EQCONTEXT 234 80 74.2%
(213) (68) (75.8%)
DICONLY 232 83 73.6%
* Values in brackets indicate the accuracy only for FTEs,
excluding cases in which the dictionary was used as a
backup.
shows examples of EQCONTEXTALIGN and DICONLY.
EQCONTEXTALIGN usually selects appropriate words,
compared to DICONLY.
When there are no plausible translation examples in the
translationmemory, the system selects a low-similarity or
low-confidence FTE. However we believe this problem
will be resolved as the number of translation examples
increases, since the News Corpus is increasing day by
day.
5 Related Work
The idea of example based machine translation systems
was first proposed by (Nagao, 1984), and preliminary
systems that appeared about ten years (Sato and Na-
gao, 1990; Sadler and Vendelmans, 1990; Maruyama and
Watanabe, 1992; Furuse and Iida, 1994) showed the basic
feasibility of the idea.
Recent studies have focused on the practical aspects
of EBMT, and this technology has even been applied
to some restricted domains. The work in (Richardson
et al, 2001; Menezes and Richardson, 2001) addressed
the problem of technical manual translation in several
languages, and the work of (Imamura, 2002) dealt with
dialogues translation in the travel arrangement domain.
These works select the translation example pairs based
solely on the source language similarity. We believe this
is partly due to the high parallelism found in their cor-
pora.
Our work targets a more general corpus of wider cover-
age, i.e., the broadcast news collection. Generally avail-
able corpora like the one we use tend to be more freely
translated and suffer from lower parallelism. This com-
pelled us to use the criterion of translation confidence,
together with the criterion of monolingual similarity used
in the previous works. As we showed in this paper, this
metric succeeded in meeting our expectations.
6 Conclusion
In this paper, we described operations of the entire EBMT
process while using a content-aligned corpus, i.e., the
NHK Broadcast Corpus. In this process, one of the key
problems is how to select plausible translation examples.
We proposed a new method to select translation exam-
ples based on source language similarity and translation
confidence. In the word selection task, the performance
is highly accurate.
Acknowledgements
This work was supported in part by the 21st Century COE
program ?Information Science and Technology Strate-
gic Core? at University of Tokyo and by a contract with
the Telecommunications Advancement Organization of
Japan, entitled ?A study of speech dialogue translation
technology based on a large corpus?.
References
Eiji Aramaki, Sadao Kurohashi, Satoshi Sato, and Hideo
Watanabe. 2001. Finding translation correspondences
from parallel parsed corpus for example-based transla-
tion. In Proceedings of MT Summit VIII, pages 27?32.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In In Proceedings of NAACL 2000, pages 132?
139.
Osamu Furuse and Hitoshi Iida. 1994. Constituent
boundary parsing for example-based machine transla-
tion. In Proceedings of the 15th COLING, pages 105?
111.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, and Yoshi-
fumi Oyama Yoshihiko Hayashi, editors. 1997.
Japanese Lexicon. Iwanami Publishing.
Kenji Imamura. 2002. Application of translation knowl-
edgeacquired by hierarchical phrase alignment for
pattern-based mt. In Proceedings of TMI-2002, pages
74?84.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
Sadao Kurohashi. 2001. Senseval2 Japanese translation
task. In Proceedings of SENSEVAL2, pages 37?40.
Hiroshi Maruyama and Hideo Watanabe. 1992. The
cover search algorithm for example-based translation.
In Proceedings of TMI-1992, pages 173?184.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the ACL 2001 Workshop on Data-Driven Meth-
ods in Machine Translation, pages 39?46.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and english by analogy
principle. In In Artificial and Human Intelligence,
pages 173?180.
Stephen D. Richardson, William B. Dolan, Arul
Menezes, and Monica Corston-Oliver. 2001. Over-
coming the customization bottleneck using example-
based mt. In Proceedings of the ACL 2001 Work-
shop onData-DrivenMethods in Machine Translation,
pages 9?16.
V. Sadler and R. Vendelmans. 1990. Pilot implementa-
tion of a bilingual knowledge bank. In Proeedings of
the 13th COLING, pages 449?451.
Satoshi Sato andMakoto Nagao. 1990. Toward memory-
based translation. InProceedings of the 13th COLING,
pages 247?252.
Construction and Analysis of Japanese-English Broadcast News Corpus
with Named Entity Tags
Tadashi Kumano, Hideki Kashioka and Hideki Tanaka
ATR Spoken Language Translation Research Laboratories
2?2?2, Hikaridai, Keihanna Science City, Kyoto 619?0288, Japan
{tadashi.kumano, hideki.kashioka, hideki.tanaka}@atr.co.jp
Takahiro Fukusima
Otemon Gakuin University
1?15, Nishiai 2-chome, Ibaraki, Osaka 567?8502, Japan
fukusima@res.otemon.ac.jp
Abstract
We are aiming to acquire named entity
(NE) translation knowledge from non-
parallel, content-aligned corpora, by uti-
lizing NE extraction techniques. For this
research, we are constructing a Japanese-
English broadcast news corpus with NE
tags. The tags represent not only NE
class information but also coreference in-
formation within the same monolingual
document and between corresponding
Japanese-English document pairs. Anal-
ysis of about 1,100 annotated article pairs
has shown that if NE occurrence informa-
tion, such as classes, number of occur-
rence and occurrence order, is given for
each language, it may provide a good clue
for corresponding NEs across languages.
1 Introduction
Studies on named entity (NE) extraction are mak-
ing progress for various languages, such as En-
glish and Japanese. A number of evaluation work-
shops have been held, including the Message Under-
standing Conference (MUC)1 for English and other
languages, and the Information Retrieval and Ex-
traction Exercise (IREX)2 for Japanese. Extraction
accuracy for English has reached a nearly practi-
cal level (Marsh and Perzanowski, 1998). As for
Japanese, it is more difficult to find NE bound-
1http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/
2http://nlp.cs.nyu.edu/irex/
aries, however, NE extraction is relatively accurate
(Sekine and Isahara, 2000).
Most of the past research on NE extraction used
monolingual corpora, but the application of NE ex-
traction techniques to bilingual (or multilingual) cor-
pora is expected to obtain NE translation pairs. We
are developing a Japanese-English machine trans-
lation system for documents including many NEs,
such as news articles or documents about current
topics. Translating NE correctly is indispensable for
conveying information correctly. NE translations,
however, are not listed in conventional dictionaries.
It is necessary to retrieve NE translation knowledge
from the latest bilingual documents.
When extracting translation knowledge from
bilingual corpora, using literally translated parallel
corpora, such as official documents written in sev-
eral languages makes it easier to get the desired in-
formation. However, not many of such corpora con-
tain the latest NEs. There are few Japanese-English
corpora which are translated literally. Therefore,
we decided to extract NE translation pairs from
content-aligned corpora, such as multilingual broad-
cast news articles including new NEs daily, which
are not literally translated.
Sentential alignment (Brown et al, 1991; Gale
and Church, 1993; Kay and Ro?scheisen, 1993; Ut-
suro et al, 1994; Haruno and Yamazaki, 1996) is
commonly used as a starting point for finding the
translations of words or expressions from bilingual
corpora. However, it is not always possible to cor-
respond non-parallel corpora in sentences. Past sta-
tistical methods for non-parallel corpora (Fung and
Yee, 1998) are not valid for finding translations of
words or expressions with low frequency. These
methods have a problem in covering NEs because
there are many NEs that appear only once in a cor-
pus. So we need a specialized method for extract-
ing NE translation pairs. Transliteration is used for
finding the translations of NE in the source language
from texts in the target language (Stalls and Knight,
1998; Goto et al, 2001; Al-Onazian and Knight,
2002). Transliteration is useful for the names of per-
sons and places; however, it is not applicable to all
sorts of NEs.
Content-aligned documents, such as a bilingual
news corpus, are made to convey the same top-
ics. Since NEs are the essential element of docu-
ment contents, content-aligned documents are likely
to share NEs pointing to the same objects. Con-
sequently, when extracting all NEs with NE class
information from each of a pair of bilingual docu-
ments separately by applying monolingual NE ex-
traction techniques, the distribution of the NEs in
each document may be similar enough to recognize
correspondences between the NE translation pairs.
A technique for finding bilingual NE correspon-
dences will have a wide range of applications other
than NE translation-pair extraction. For example,
? Bilingual NE correspondences have clues for
identifying corresponding parts in a pair of
noisy bilingual documents.
? The similarity of any two documents in dif-
ferent languages can be estimated by NE
translation-pair correspondence.
For this research, we obtained a Japanese-English
broadcast news corpus (Kumano et al, 2002) by
the Japanese broadcast company NHK3, and we are
manually tagging NEs in the corpus to analyze it
and to conduct NE translation-pair extraction exper-
iments.
The tag specifications are based on the IREX
NE task (Sekine and Isahara, 1999), the evaluation
workshop of Japanese NE extraction. We extended
the specifications to English NEs. In addition, coref-
erence information between NEs, within the same
monolingual document and between the correspond-
ing Japanese-English document pairs (henceforth,
3Nippon Hoso Kyokai (Japan Broadcasting Corporation)
(http://www.nhk.or.jp/englishtop/)
we call these in a language and across languages,
respectively), is added to each of the tagged NEs,
for NE translation-pair extraction studies.
In Section 2, we will introduce the bilingual cor-
pus used in this study and describe its characteris-
tics. Then, we will discuss tag design for NE extrac-
tion studies, and explain the tag specifications and
existing problems. The current status of corpus an-
notation under these specifications will also be in-
troduced. We analyzed an annotated part of the cor-
pus in terms of NE occurrence and translation. This
analysis will be shown in Section 3. In Section 4,
we will mention future plans for the extraction of
NE translation-pairs.
2 Constructing a Japanese-English
broadcast news corpus with NE tags
2.1 Characteristics of the NHK
Japanese-English broadcast news corpus
We are annotating an NHK broadcast news corpus
with NE tags. The corpus is composed of Japanese
news articles for domestic programs and English
news articles translated for international broadcast-
ing4 and domestic bilingual programs5.
Figure 1 shows an example of a Japanese news
article and its translation in English. The original
Japanese article and the translated English article
deal with the same topic, but they differ much in de-
tails. The difference arises from the following rea-
sons (Kumano et al, 2002).
Audience Content might be added or deleted, ac-
cording to the audience, especially for interna-
tional broadcasting.
Broadcasting date The broadcasting of English
news is often delayed compared to the origi-
nal Japanese news. The time expressions might
be changed sometimes or new facts might be
added to the articles.
News styles / languages Comparing news articles
of two languages reveals that they have differ-
ent presentation styles, for example, facts are
sometimes introduced in a different order. The
4NHK WORLD (http://www.nhk.or.jp/
nhkworld/)
5http://www.nhk.or.jp/englishtop/
program_list/
Original article in Japanese (and its literal translation in English by authors):
1: ????????????????????????????????????
????????????
(There was a strong earthquake at 6:42 this morning in Izu Islands, the site of recent
numerous earthquakes. An earthquake of a little less than five in seismic intensity was
observed at Shikine Island.)
2: ????????????????????????????????????
????????????????????
(In addition, an event of seismic intensity four was observed for Niijima and Kozu Is-
land, events seismic intensity three for Toshima Island and Miyake Island, and events of
seismic intensity two and one for various parts of Kanto Area and Shizuoka Prefecture.)
3: ???????????????????
(There is no risk of tsunamis resulting from this earthquake.)
4: ?????????????????????????????????????
????????????????????????????
(According to observations by the Meteorological Agency, the earthquake epicenter
was located in the sea at a depth of ten kilometers near Niijima and Kozu Island. The
magnitude of the earthquakes was estimated to be five point one.)
5: ????????????????????????????????????
????????????????????????????????????
??????????????????
(In Izu Islands, where seismic activity has been observed from the end of June, repeated
cycles of seismic activity and dormancy have been observed. On the 30th of the previ-
ous month, a single strong earthquake having seismic intensity of a little less than six
was observed at Miyake Island, while two earthquakes having seismic intensity of five
were also observed there.)
6: ????????????????????????????????????
????????????????????????????????????
????
(In a series of seismic events, seventeen earthquakes having seismic intensity over five
have been observed up to this point, including strong tremors with a seismic intensity
of a little less than six observed four times at Kozu Island, Niijima, and Miyake Island.)
Translated article in English:
1: A strong earthquake jolted
Shikine Island, one of the Izu
islands south of Tokyo, early
on Thursday morning.
2: The Meteorological Agency
says the quake measured five-
minus on the Japanese scale of
seven.
3: The quake affected other is-
lands nearby.
4: Seismic activity began in the
area in late July, and 17 quakes
of similar or stronger intensity
have occurred.
5: Officials are warning of more
similar or stronger earthquakes
around Niijima and Kozu Is-
lands.
6: Tokyo police say there have
been no reports of damage
from the latest quake.
Figure 1: An article pair in an NHK broadcast news corpus
difference is due to language and socio-cultural
backgrounds.
2.2 NE tag design
We designed NE tags for NE translation-pair extrac-
tion research and working efficiency for manual an-
notation. The specifications are shown below.
? It is desirable that NE recognition guidelines
be consistent with NE tags of existing corpora.
Past guidelines of MUC and IREX should be
respected because they were configured as a re-
sult of many discussions. Consistent guidelines
enable us to utilize existing annotated corpora
and systems designated for the corpora.
? Within each bilingual document pair, corefer-
ence between NEs in a language and across lan-
guages will be specified. When several NEs
exist for the same referent in a document, it
is not always possible to determine the actual
translation for each instance of the NEs from
the counterpart document, because our corpus
is not composed of literal translations. There-
fore, coreference between NEs in a language
should be marked so that the coreference across
languages can be assigned between NE groups
that have the same referent. Coreference be-
tween NE groups is sufficient for our purpose.
? Assignment of coreference in a language is lim-
ited between NEs only. Although NEs may
have the same referent with pronouns or non-
NE expressions, these elements are ignored to
avoid complicating the annotation work.
2.3 Tag specifications
1. The tag specifications conform to IREX NE
tag specifications (IREX Committee, 1999) (an
English description in (Sekine and Isahara,
1999)) as regards the markup form, NE classes,
and NE recognition guidelines.
Japanese:
????????<LOCATION ID=?1? COR=?2?>
(Izu Islands)
????</LOCATION>
?<DATE ID=?2? COR=?4?>
(today)
???</DATE><TIME ID=?3? COR=?5?>
(a.m.)
??
(6:42)
??????</TIME>????????<LOCATION ID=?4? COR=?1?>
(Shikine Island)
???</LOCATION>????????????? ? ? ?
English:
A strong earthquake jolted <LOCATION ID=?1? COR=?4?>
Shikine Island</LOCATION>, one of the <LOCATION ID=?2?
COR=?1?>Izu islands</LOCATION> south of <LOCATION ID=?3?>
Tokyo</LOCATION>, early on <DATE ID=?4? COR=?2?>Thursday
</DATE> <TIME ID=?5? COR=?3?>morning</TIME>. ? ? ?
Figure 2: An annotation example
NE Class Example
Named entities (in the narrow sense):
ORGANIZATION The Diet; IREX Committee
PERSON (Mr.) Obuchi; Wakanohana
LOCATION Japan; Tokyo; Mt. Fuji
ARTIFACT Pentium Processor; Nobel Prize
Temporal expressions:
DATE September 2, 1999; Yesterday
TIME 11 PM; midnight
Number expressions:
MONEY 100 yen; $12,345
PERCENT 10%; a half
Table 1: NE Classes
Eight NE classes were defined at the IREX NE
task ? the same 7 classes as MUC-7 (3 types
of named entities in the narrow sense, 2 types
of temporal expressions, and 2 types of number
expressions), and ARTIFACT (concrete objects
like commercial products and abstract objects
such as laws or intellectual properties). Table 1
shows a list of these.
2. IREX?s NE classes and NE recognition guide-
lines are applied to English for consistency be-
tween Japanese and English NEs. For English-
specific annotation, such as prepositions or de-
terminers in NE, the MUC-7 Named Entity
Task Definition (Chinchor, 1997) is consulted6.
3. The SGML markup form of the IREX tag is
extended by adding the following two tag at-
tributes, which represent coreference informa-
tion in a language, and across languages.
ID=?NE group ID? (mandatory)
Each NE is assigned an attribute ID and
an ID number as its value. All corefer-
ent NEs in each language document are
6The tag specifications of IREX NE and those of MUC-7 do
not differ radically, because IREX NE tags are designed based
on the discussions of MUC.
given the same ID number7. The same
ID number is assigned to NEs that have
different forms, such as the full name and
the first name or the official name and the
abbreviated form, in addition to NEs with
the same form. Basically, NE are assigned
the same ID number when they belong to
an NE class and have the identical surface
form8.
COR=?ID for corresponding NE groups in
the other language? (optional)
When there exists a corresponding NE
(group) belonging to the same NE class
in the other language, an attribute COR
is given to each NE (group) in both lan-
guages, and the ID number for the coun-
terpart is assigned as a value to each other.
Annotations by the specifications are illustrated
in Figure 2.
2.4 Current status of the corpus annotation
Annotators who have experience in translation work
and in the production of linguistic data are engaging
in the tag annotation. Plans call for a total of 2,000
article pairs to be annotated, and about 1,100 pairs
have been finished up to the present.
2.5 Problems
Some problems became obvious in the course of
discussions of tag specifications and tag annotation
work. They confuse annotators and make the result
inaccurate. Typical cases are shown below.
2.5.1 The granularity difference between
Japanese and English
In Japanese, a unit smaller than a morpheme may
be accepted as an NE according to IREX guidelines.
7ID numbers do not maintain uniqueness across the docu-
ments.
8There are some exceptions. See Section 2.5.3.
(last Sunday and this Sunday)
sensyuu-no nichiyou -to konsyuu-no nichiyou
J: ???<DATE ID=?1?>??</DATE>????<DATE ID=?2?>??</DATE>
E: <DATE COR=?1?>last Sunday</DATE> and <DATE COR=?2?>this Sunday</DATE>
Figure 3: Assignment of different group IDs with NEs having the same surface form
On the other hand, English does not accept any unit
smaller than a word by MUC-7 guidelines. Some
Japanese NEs cannot have a counterpart English
NE, even if they have a corresponding English ex-
pression because of the difference in the segmenta-
tion granularity. For example, ????? (amerika;
America)? in the Japanese morpheme ??????
(amerika-jin; America-people)? is treated as an NE,
while no NE can be tagged to ?American?, the En-
glish counterpart of ??????.?
2.5.2 Translation problems
NEs have the same problem that translation in
general has: What is the exact translation word(s)
for an expression?
? Semantically corresponding expressions may
not be assigned corresponding NE relations,
because they belong to different NE classes or
an expression in a language is not recognized
as an NE. For example, a non-NE word ???
(seifu; government)? which means Japanese
government in Japanese articles is often trans-
lated as the English NE: ?Japan.?
? A non-literal translation of an NE may cause
difficulty in recognizing corresponding rela-
tions. Correspondences for some expressions
cannot be decided with the information repre-
sented in documents: Relative temporal expres-
sions in Japanese are often translated as ab-
solute expressions in English and those corre-
spondences cannot be identified without con-
sulting the calendar; Money expressions are
generally converted to dollars and the ex-
change rate at the relative time is needed to
confirm correspondences. For example, we
found a translation pair of money expressions
????? (sanzen-oku-en; three hundred bil-
lion yen)? and ?three billion U-S dollars? in our
corpus, which constitutes a rough conversion
from yen into dollars when the articles were
produced.
2.5.3 Assigning NE group IDs
We defined NEs that have the identical surface
form and the same NE class to be coreferent and
assigned the same NE group ID, in order to make
coreference judgment easier. There are some cases
where we cannot apply this rule, especially to tem-
poral expressions or number expressions.
The example in Figure 3 shows the translation
pair ???????????? (last Sunday and this
Sunday)? and ?last Sunday and this Sunday? anno-
tated with NE tags. Japanese temporal expressions
?????? (last Sunday)? and ?????? (this
Sunday)? are translated into English as ?last Sun-
day? and ?this Sunday? respectively. When anno-
tating NE tags for this translation pair, only ???
(Sunday)? in those temporal expressions in Japanese
is regarded as an NE according to the IREX?s NE
specifications. This causes a problem in which the
two NEs of the same surface form that are assigned
the same NE class have different referents. Each of
them should assign correspondence to different NEs
in the counterpart: the former to ?last Sunday? and
the latter to ?this Sunday.?
Tentatively, we allowed a different NE group ID to
be assigned to an NE with the identical surface form
in an NE class, as shown in Figure 3. It would be
better reexamine the consistency of the NE tag spec-
ification between Japanese and English, and the ne-
cessity of coreference information for temporal ex-
pressions and number expressions.
3 Analysis
We conducted an elementary investigation into
1,096 pairs of annotated Japanese and English ar-
ticles.
3.1 Corpus size
Table 2 shows the content size of our corpus by
the number of sentences and the morphemes/words.
The content decreases significantly when translating
from Japanese to English. This fact points out that
NE class
Japanese English
tokens avr. per types avr. per tokens avr. per types avr. per( art. / sent.) ( art. / sent.) ( art. / sent.) ( art. / sent.)
Total 24,147 (22.03 / 4.13) 12,809 (11.69 / 2.19) 15,844 (14.46 / 2.03) 10,353 ( 9.45 / 1.32)
ORGANIZATION 5,160 ( 4.71 / 0.88) 2,558 ( 2.33 / 0.44) 2,882 ( 2.63 / 0.37) 1,863 ( 1.70 / 0.24)
PERSON 3,525 ( 3.22 / 0.60) 1,628 ( 1.49 / 0.28) 2,800 ( 2.55 / 0.36) 1,410 ( 1.29 / 0.18)
LOCATION 8,737 ( 7.97 / 1.49) 3,752 ( 3.42 / 0.64) 5,792 ( 5.28 / 0.74) 3,302 ( 3.01 / 0.42)
ARTIFACT 455 ( 0.42 / 0.08) 282 ( 0.26 / 0.05) 241 ( 0.22 / 0.03) 193 ( 0.18 / 0.02)
DATE 4,342 ( 3.96 / 0.74) 2,959 ( 2.70 / 0.51) 2,990 ( 2.73 / 0.38) 2,620 ( 2.39 / 0.34)
TIME 854 ( 0.78 / 0.15) 740 ( 0.68 / 0.13) 245 ( 0.22 / 0.03) 232 ( 0.21 / 0.03)
MONEY 577 ( 0.53 / 0.10) 462 ( 0.42 / 0.08) 517 ( 0.47 / 0.07) 375 ( 0.34 / 0.05)
PERCENT 497 ( 0.45 / 0.08) 428 ( 0.39 / 0.07) 377 ( 0.34 / 0.05) 358 ( 0.33 / 0.05)
Table 3: NE frequency
articles sentences morphemes/words(avr. per article) (avr. per sent.)
J 1,096 5,851 (5.34) 321,204 (54.90)E 7,815 (7.13) 181,180 (23.18)
Table 2: Corpus size
the content tends to be lost through the translation
process.
3.2 In-language characteristics of NE
occurrences
3.2.1 Frequency
The number of occurrences for each NE class is
listed in Table 3. The distribution of NE classes is
almost the same as that in the data for MUC-7 or
IREX.
By comparing the decrease in content (cf. Ta-
ble 2), the number of NE tokens also decreases for
translations. However, the degree of the NE de-
crease is less than that of the morphemes/words. It
is also remarkable that the number of NE types is
fairly well preserved. Notice that only a small num-
ber of tokens in the NE class TIME appear in En-
glish. The reason may be that detailed time infor-
mation may become less important for English ar-
ticles, which are intended for audiences outside of
Japan and broadcast later than the original Japanese
articles.
3.2.2 NE characteristics within NE groups
To examine the surface form distribution in the
same NE groups, we counted the number of mem-
bers ( freq) and sorts of surface form (sort) for each
NE group in each article. The probability that a
given member has a unique surface form in a group
NE class
Japanese English
freq sort uniq freq sort uniq
Average 1.89 1.10 0.131 1.53 1.14 0.332
ORG. 2.02 1.12 0.144 1.55 1.16 0.345
PERSON 2.17 1.12 0.121 1.99 1.49 0.655
LOCATION 2.33 1.14 0.114 1.75 1.07 0.105
ARTIFACT 1.61 1.05 0.072 1.25 1.05 0.216
DATE 1.47 1.08 0.175 1.14 1.03 0.200
TIME 1.15 1.02 0.098 1.06 1.01 0.182
MONEY 1.25 1.03 0.109 1.38 1.35 0.936
PERCENT 1.16 1.00 0.008 1.05 1.06 0.278
Table 4: Surface form distribution in the same NE
groups
that has two or more members (uniq) has also been
calculated as follows:
uniq = freq? 2Csort? 2freq? 1Csort? 1 =
sort ? 1
freq ? 1 ( freq ? 2).
Table 4 shows the values averaged for all the NE
groups that appeared in all articles.
In English, a repetition of the same expression is
not conventionally desirable. Therefore, pronouns
or paraphrases are used frequently. On the other
hand, Japanese does not have such a convention.
This difference is considered to be the reason for the
result shown in Table 4: freq in English is smaller
than that in Japanese, and sort in English is larger
than that in Japanese. As a result, uniq in English is
higher than that in Japanese. These tendencies differ
slightly according to the NE classes.
? The sort of English PERSON is notably large. In
English, the name of a person is usually first ex-
pressed in full, and after that, it tends to be ex-
pressed only by the family name. In Japanese,
only the family name is generally used from the
beginning, especially for well-known persons.
NE class
J? E J? E
token type token type
Average 0.742 0.639 0.842 0.786
ORGANIZATION 0.684 0.612 0.877 0.837
PERSON 0.881 0.777 0.938 0.898
LOCATION 0.799 0.673 0.833 0.753
ARTIFACT 0.701 0.628 0.925 0.912
DATE 0.717 0.656 0.761 0.742
TIME 0.207 0.184 0.596 0.591
MONEY 0.593 0.595 0.781 0.733
PERCENT 0.712 0.692 0.830 0.827
Table 5: Cross-language corresponding rate
NE class
Japanese English
freq sort uniq freq sort uniq
Average 2.19 1.14 0.134 1.64 1.17 0.342
ORG. 2.25 1.17 0.164 1.62 1.19 0.364
PERSON 2.45 1.14 0.110 2.07 1.53 0.645
LOCATION 2.77 1.19 0.117 1.94 1.10 0.112
ARTIFACT 1.80 1.06 0.075 1.27 1.05 0.222
DATE 1.60 1.10 0.167 1.17 1.04 0.211
TIME 1.30 1.04 0.106 1.07 1.01 0.250
MONEY 1.24 1.04 0.138 1.47 1.43 0.934
PERCENT 1.20 1.00 0.010 1.06 1.01 0.250
Table 6: Surface form distribution in the same NE
groups (only for those having cross-language corre-
spondences)
? The uniq of English MONEY is quite high. A
money expression in Japanese tends to be trans-
lated into English as both the original currency
(usually yen) and dollars.
? The freq of temporal and number expressions
are smaller than those of named entities in the
narrow sense.
3.3 Cross-language characteristics of NE
occurrences
3.3.1 Correspondence across languages
We calculated the rates for a given NE in a doc-
ument to have a corresponding NE in the counter-
part language. The units of NE correspondences we
used for these calculations are both NE token and
NE group (type). The results, shown in Table 5,
show that an NE that appeared in English will have
a Japanese NE correspondent with a high rate.
We also conducted the same survey as we did in
Table 4 for only NEs having cross-language corefer-
ences, whose results are shown in Table 6. A com-
parison of both results shows that the freq for only
NEs having cross-language coreferences is larger,
NE class
J? E J? E
All Corr. only All Corr. only
All NEs 0.291 0.774 0.483 0.774
Average 0.304 0.790 0.494 0.790
ORG. 0.269 0.808 0.568 0.809
PERSON 0.403 0.877 0.671 0.875
LOCATION 0.318 0.746 0.461 0.745
ARTIFACT 0.410 0.725 0.662 0.710
DATE 0.307 0.805 0.428 0.805
TIME 0.033 0.815 0.227 0.815
MONEY 0.170 0.829 0.407 0.829
PERCENT 0.509 0.903 0.658 0.903
Table 7: Preservation ratio of NE order
especially in Japanese. An NE occurring more times
in an article may have more important information
and is more likely to appear in the translation.
3.3.2 Preservation of NE order
We investigated how well the order of NEs oc-
curring in an article is preserved in the counterpart
language as follows:
1. In every article, we eliminated all NEs except
the first occurrence of every NE group.
2. We calculated the ratio between all of the pos-
sible NE pairs in the source language and those
translated into the target language with the
same order of occurrence.
Table 7 lists the average preservation ratios of the
NE order for all NEs (?All?) and for NEs having
corresponding NEs in the counterpart (?Corr. only?).
The scores labeled ?All NEs? express ratios for the
order of all NEs. The preservation ratio for each
NE class is listed below in the table. The NE or-
ders are preserved so well even for all NEs that they
can be used for determining cross-language corre-
spondences.
4 Conclusion
In this paper, in which we aimed to acquire NE trans-
lation knowledge, we described our construction of
a Japanese-English broadcast news corpus with NE
tags for NE translation-pair extraction. The tags rep-
resent NE characteristics and coreference informa-
tion in a language and across languages. Analysis
of the annotated 1,097 article pairs has shown that
if NE occurrence information, such as classes, num-
ber of occurrences and occurrence order, is given for
each language side, it may provide a good clue for
determining NE correspondence across languages.
Our future plans are listed below.
? The problems in Section 2.5 need to be reex-
amined from the point of view of what infor-
mation bilingual corpora should have for NE
translation-pair extraction research.
? The proposed analysis in Section 3 pointed out
that identifying coreferences in a language is
very important for achieving NE translation-
pair extraction. Richer coreference information
should be annotated in our corpus for coref-
erence identification studies. We are planning
to annotate coreference information for pro-
nouns and some other non-NE expressions, re-
ferring to the MUC-7 coreference task defini-
tion (Hirschman and Chinchor, 1997).
? Corpora with different characteristics, such as a
bilingual newspaper corpus, will be annotated
and analyzed.
Acknowledgments This research was supported
in part by the Telecommunications Advancement
Organization of Japan.
References
Yaser Al-Onazian and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02), pages 400?408.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th Annual Meeting of the Association
for Computational Linguistics (ACL-91), pages 169?
176.
Nancy Chinchor. 1997. MUC-7 named entity task
definition. http://www.itl.nist.gov/iaui/
894.02/related_projects/muc/proceedings/
ne_task.html.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 36th Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
(COLING-ACL ?98), volume I, pages 414?420.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Isao Goto, Noriyoshi Uratani, and Terumasa Ehara.
2001. Cross-language information retrieval of proper
nouns using context information. In Proceedings of
the 6th Natural Language Processing Pacific Rim Sym-
posium (NLPRS 2001), pages 571?578.
Masahiko Haruno and Takefumi Yamazaki. 1996. High-
performance bilingual text alignment using statistical
and dictionary information. In Proceedings of the 34th
International Conference on Computational Linguis-
tics (ACL ?96), pages 131?138.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 coreference task definition. http:
//www.itl.nist.gov/iaui/894.02/related_
projects/muc/proceedings/co_task.html.
IREX Committee. 1999. Named entity extraction task
definition (version 990214). http://nlp.cs.nyu.
edu/irex/NE/df990214.txt. (In Japanese).
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Tadashi Kumano, Isao Goto, Hideki Tanaka, Noriyoshi
Uratani, and Terumasa Ehara. 2002. A translation
aid system by retrieving bilingual news database. Sys-
tems and Computers in Japan, 33(8):19?29. (Original
written in Japanese is in Transactions of the Institute
of Electronics, Information and Communication Engi-
neers, J85-D-II(6):1175?1184. 2001).
Elaine Marsh and Dennis Perzanowski. 1998. MUC-7
evaluation of IE technology: Overview and results.
http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/proceedings/muc_7_
proceedings/marsh_slides.pdf.
Satoshi Sekine and Hitoshi Isahara. 1999. IREX
project overview. http://nlp.cs.nyu.edu/
irex/Paper/irex-e.ps. (Original written in
Japanese is in Proceedings of the IREX Workshop,
pages 1?5).
Satoshi Sekine and Hitoshi Isahara. 2000. IREX: IR
and IE evaluation project in Japanese. In Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation (LREC-2000), pages 1475?
1480.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text. In
Proceedings of the Workshop on Computational Ap-
proaches of the Semitic Languages, pages 34?41.
Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statistics.
In Proceedings of the 32th International Conference
on Computational Linguistics (ACL-94), pages 1076?
1082.
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32?39,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Dialogue Acts to Construct Dialogue Systems for Consulting
Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi Nakamura
MASTAR Project, National Institute of Information and Communications Technology
Hikaridai, Keihanna Science City, JAPAN
kiyonori.ohtake (at) nict.go.jp
Abstract
This paper introduces a new corpus of con-
sulting dialogues, which is designed for
training a dialogue manager that can han-
dle consulting dialogues through sponta-
neous interactions from the tagged dia-
logue corpus. We have collected 130 h
of consulting dialogues in the tourist guid-
ance domain. This paper outlines our tax-
onomy of dialogue act annotation that can
describe two aspects of an utterances: the
communicative function (speech act), and
the semantic content of the utterance. We
provide an overview of the Kyoto tour
guide dialogue corpus and a preliminary
analysis using the dialogue act tags.
1 Introduction
This paper introduces a new dialogue corpus for
consulting in the tourist guidance domain. The
corpus consists of speech, transcripts, speech act
tags, morphological analysis results, dependency
analysis results, and semantic content tags. In this
paper, we describe the current status of a dialogue
corpus that is being developed by our research
group, focusing on two types of tags: speech act
tags and semantic content tags. These speech act
and semantic content tags were designed to ex-
press the dialogue act of each utterance.
Many studies have focused on developing spo-
ken dialogue systems. Their typical task do-
mains included the retrieval of information from
databases or making reservations, such as airline
information e.g., DARPA Communicator (Walker
et al, 2001) and train information e.g., ARISE
(Bouwman et al, 1999) and MASK (Lamel et al,
2002). Most studies assumed a definite and con-
sistent user objective, and the dialogue strategy
was usually designed to minimize the cost of in-
formation access. Other target tasks include tutor-
ing and trouble-shooting dialogues (Boye, 2007).
In such tasks, dialogue scenarios or agendas are
usually described using a (dynamic) tree structure,
and the objective is to satisfy all requirements.
In this paper, we introduce our corpus, which is
being developed as part of a project to construct
consulting dialogue systems, that helps the user in
making a decision. So far, several projects have
been organized to construct speech corpora such
as CSJ (Maekawa et al, 2000) for Japanese. The
size of CSJ is very large, and a great part of the
corpus consists of monologues. Although, CSJ
includes some dialogues, the size of dialogues is
not enough to construct a dialogue system via re-
cent statistical techniques. In addition, relatively
to consulting dialogues, the existing large dialogue
corpora covered very clear tasks in limited do-
mains.
However, consulting is a frequently used and
very natural form of human interaction. We of-
ten consult with a sales clerk while shopping or
with staff at a concierge desk in a hotel. Such dia-
logues usually form part of a series of information
retrieval dialogues that have been investigated in
many previous studies. They also contains various
exchanges, such as clarifications and explanations.
The user may explain his/her preferences vaguely
by listing examples. The server would then sense
the user?s preferences from his/her utterances, pro-
vide some information, and then request a deci-
sion.
It is almost impossible to handcraft a scenario
that can handle such spontaneous consulting dia-
logues; thus, the dialogue strategy should be boot-
strapped from a dialogue corpus. If an extensive
dialogue corpus is available, we can model the
dialogue using machine learning techniques such
as partially observable Markov decision processes
(POMDPs) (Thomson et al, 2008). Hori et al
(2008) have also proposed an efficient approach to
organize a dialogue system using weighted finite-
state transducers (WFSTs); the system obtains the
32
Table 2: Overview of Kyoto tour guide dialogue
corpus
dialogue type F2F WOZ TEL
# of dialogues 114 80 62
# of guides 3 2 2
avg. # of utterance 365.4 165.2 324.5/ dialogue (guide)
avg. # of utterance 301.7 112.9 373.5/ dialogue (tourist)
structure of the transducers and the weight for
each state transitions from an annotated corpus.
Thus, the corpus must be sufficiently rich in in-
formation to describe the consulting dialogue to
construct the statistical dialogue manager via such
techniques.
In addition, a detailed description would be
preferable when developing modules that focus
on spoken language understanding and generation
modules. In this study, we adopt dialogue acts
(DAs) (Bunt, 2000; Shriberg et al, 2004; Banga-
lore et al, 2006; Rodriguez et al, 2007; Levin et
al., 2002) for this information and annotate DAs in
the corpus.
In this paper, we describe the design of the Ky-
oto tour guide dialogue corpus in Section 2. Our
design of the DA annotation is described in Sec-
tion 3. Sections 4 and 5 respectively describe two
types of the tag sets, namely, the speech act tag
and the semantic content tag.
2 Kyoto Tour Guide Dialogue Corpus
We are currently developing a dialogue corpus
based on tourist guidance for Kyoto City as the tar-
get domain. Thus far, we have collected itinerary
planning dialogues in Japanese, in which users
plan a one-day visit to Kyoto City. There are
three types of dialogues in the corpus: face-to-
face (F2F), Wizard of OZ (WOZ), and telephonic
(TEL) dialogues. The corpus consists of 114 face-
to-face dialogues, 80 dialogues using the WOZ
system, and 62 dialogues obtained from telephone
conversations with the interface of the WOZ sys-
tem.
The overview of these three types of dialogues
is shown in Table 2. Each dialogue lasts for almost
30 min. Most of all the dialogues have been man-
ually transcribed. Table 2 also shows the average
number of utterances per a dialogue.
Each face-to-face dialogue involved a profes-
sional tour guide and a tourist. Three guides, one
male and two females, were employed to collect
the dialogues. All three guides were involved in
almost the same number of dialogues. The guides
used maps, guidebooks, and a PC connected to the
internet.
In the WOZ dialogues, two female guides were
employed. Each of them was participated in 40
dialogues. The WOZ system consists of two in-
ternet browsers, speech synthesis program, and
an integration program for the collaborative work.
Collaboration was required because in addition to
the guide, operators were employed to operate the
WOZ system and support the guide. Each of the
guide and operators used own computer connected
each other, and they collaboratively operate the
WOZ system to serve a user (tourist).
In the telephone dialogues, two female guides
who are the same for the WOZ dialogues were
employed. In these dialogues, we used the WOZ
system, but we did not need the speech synthesis
program. The guide and a tourist shared the same
interface in different rooms, and they could talk to
each other through the hands-free headset.
Dialogues to plan a one-day visit consist of sev-
eral conversations for choosing places to visit. The
conversations usually included sequences of re-
quests from the users and provision of information
by the guides as well as consultation in the form of
explanation and evaluation. It should be noted that
in this study, enabling the user to access informa-
tion is not an objective in itself, unlike information
kiosk systems such as those developed in (Lamel
et al, 2002) or (Thomson et al, 2008). The objec-
tive is similar to the problem-solving dialogue of
the study by Ferguson and Allen (1998), in other
words, accessing information is just an aspect of
consulting dialogues.
An example of dialogue via face-to-face com-
munication is shown in Table 1. This dialogue is
a part of a consultation to decide on a sightseeing
spot to visit. The user asks about the location of a
spot, and the guide answers it. Then, the user pro-
vides a follow-up by evaluating the answer. The
task is challenging because there are many utter-
ances that affect the flow of the dialogue during a
consultation. The utterances are listed in the order
of their start times with the utterance ids (UID).
From the column ?Time? in the table, it is easy to
see that there are many overlaps.
33
Table 1: Example dialogue from the Kyoto tour guide dialogue corpus
UID Time (ms) Speaker Transcript Speech act tag** Semantic content tag
56 76669?78819 User
Ato (And,)
WH?Question Where
null
Ohara ga (Ohara is) (activity),location
dono henni (where) (activity),(demonstrative),interr
narimasuka (I?d like to know) (activity),predicate
57 80788?81358 Guide kono (here) State Answer?56 (demonstrative),kosoahendesune (is around) (demonstrative),noun
58 81358?81841 Guide Ohara ha (Ohara) State Inversion location
59 81386?82736 User Chotto (a bit) State Evaluation?57 (transp),(cost),(distance),adverb-phrasehanaresugitemasune (is too far) (transp),(cost),(distance),predicate
60 83116?83316 Guide A (Yeah,) Pause Grabber null
61 83136?85023 User
Kore demo (it)
Y/N?Question
null
ichinichi dewa (in a day) (activity),(planning),duration
doudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr
62 83386?84396 Guide Soudesune (right.) State Acknowledgment?59 null
63 85206?87076 Guide
Ichinichi (One day)
State AffirmativeAnswer?61
(activity),(planning),(entity),day-window
areba (is) (activity),(planning),predicate
jubuN (enough) (consulting),(activity),adverb-phrase
ikemasu (to enjoy it.) (consulting),(activity),action
64 88392?90072 Guide
Oharamo (Ohara is)
State Opinion
(activity),location
sugoku (very) (recommendation),(activity),adverb-phrase
kireidesuyo (a beautiful spot) (recommendation),(activity),predicate
65 89889?90759 User Iidesune (that would be nice.) State Acknowledgment?64 (consulting),(activity),predicateEvaluation?64
* Tags are concatenated using a delimiter ? ? and omitting null values.
The number following the ??? symbol denotes the target utterance of the function.
3 Annotation of Communicative
Function and Semantic Content in DA
We annotate DAs in the corpus in order to de-
scribe a user?s intention and a system?s (or the tour
guide?s) action. Recently, several studies have ad-
dressed multilevel annotation of dialogues (Levin
et al, 2002; Bangalore et al, 2006; Rodriguez et
al., 2007); in our study, we focus on the two as-
pects of a DA indicated by Bunt (2000). One is the
communicative function that corresponds to how
the content should be used in order to update the
context, and the other is a semantic content that
corresponds to what the act is about. We consider
both of them important information to handle the
consulting dialogue. We designed two different
tag sets to annotate DAs in the corpus. The speech
act tag is used to capture the communicative func-
tions of an utterance using domain-independent
multiple function layers. The semantic content tag
is used to describe the semantic contents of an ut-
terance using domain-specific hierarchical seman-
tic classes.
4 Speech Act Tags
In this section, we introduce the speech act (SA)
tag set that describes communicative functions of
utterances. As the base units for tag annotation,
we adopt clauses that are detected by applying
the clause boundary annotation program (Kash-
ioka and Maruyama, 2004) to the transcript of the
dialogue. Thus, in the following discussions, ?ut-
terance? denotes a clause.
4.1 Tag Specifications
There are two major policies in SA annotation.
One is to select exactly one label from the tag set
(e.g., the AMI corpus1). The other is to annotate
with as many labels as required. MRDA (Shriberg
et al, 2004) and DIT++ (Bunt, 2000) are defined
on the basis of the second policy. We believe that
utterances are generally multifunctional and this
multifunctionality is an important aspect for man-
aging consulting dialogues through spontaneous
interactions. Therefore, we have adopted the latter
policy.
By extending the MRDA tag set and DIT++, we
defined our speech act tag set that consists of six
layers to describe six groups of function: Gen-
eral, Response, Check, Constrain, ActionDiscus-
sion, and Others. A list of the tag sets (excluding
the Others layer is shown in Table 3. The General
layer has two sublayers under the labels, Pause
and WH-Question, respectively. The two sublay-
ers are used to elaborate on the two labels, respec-
tively. A tag of the General layer must be labeled
to an utterance, but the other layer?s tags are op-
tional, in other words, layers other than the Gen-
eral layer can take null values when there is no tag
which is appropriate to the utterance. In the practi-
cal annotation, the most appropriate tag is selected
from each layer, without taking into account any
of the other layers.
The descriptions of the layers are as follows:
General: It is used to represent the basic form
1http://corpus.amiproject.org
34
Table 3: List of speech act tags and their occurrence in the experiment
Tag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)User Guide User Guide User Guide User Guide
(General) (Response) (ActionDiscussion) (Constrain)
Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52
Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09
Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00
Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01
WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)
Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03
OR?after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20
OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15
Statement== 9.91 27.79 ApprovalRequest 2.01 1.07
of the unit. Most of the tags in this layer
are used to describe forward-looking func-
tions. The tags are classified into three large
groups: ?Question,? ?Fragment,? and ?State-
ment.? ?Statement==? denotes the continua-
tion of the utterance.
Response: It is used to label responses directed
to a specific previous utterance made by the
addressee.
Check: It is used to label confirmations that are
along a certain expected response.
Constrain: It is used to label utterances that re-
strict or complement the target of the utter-
ance.
ActionDiscussion: It is used to label utterances
that pertain to a future action.
Others: It is used to describe various functions of
the utterance, e.g., Greeting, SelfTalk, Wel-
come, Apology, etc.
In the General layer, there are two sublayers:? (1)
the Pause sublayer that consists of Hold, Grabber,
Holder, and Releaser and (2) the WH sublayer that
labels the WH-Question type.
It should be noted that this taxonomy is in-
tended to be used for training spoken dialogue sys-
tems. Consequently, it contains detailed descrip-
tions to elaborate on the decision-making process.
For example, checks are classified into four cat-
egories because they should be treated in various
ways in a dialogue system. UnderstandingCheck
is often used to describe clarifications; thus, it
should be taken into account when creating a di-
alogue scenario. In contrast, RepetitionRequest,
which is used to request that the missed portions
of the previous utterance be repeated, is not con-
cerned with the overall dialogue flow.
An example of an annotation is shown in Table
1. Since the Response and Constrain layers are not
necessarily directed to the immediately preceding
utterance, the target utterance ID is specified.
4.2 Evaluation
We performed a preliminary annotation of the
speech act tags in the corpus. Thirty dialogues
(900 min, 23,169 utterances) were annotated by
three labellers. When annotating the dialogues, we
took into account textual information, audio infor-
mation, and contextual information The result was
cross-checked by another labeller.
4.2.1 Distributional Statistics
The frequencies of the tags, expressed as a per-
centages, are shown in Table 3. In the General
layer, nearly half of the utterances were Statement.
This bias is acceptable because 66% of the utter-
ances had tag(s) of other layers.
The percentages of tags in the Constrain layer
are relatively higher than those of tags in the other
layers. They are also higher than the percentages
of the corresponding tags of MRDA (Shriberg
et al, 2004) and SWBD-DAMSL(Jurafsky et al,
1997).
These statistics characterize the consulting dia-
logue of sightseeing planning, where explanations
and evaluations play an important role during the
decision process.
4.2.2 Reliability
We investigated the reliability of the annotation.
Another two dialogues (2,087 utterances) were an-
notated by three labelers and the agreement among
them was examined. These results are listed in Ta-
ble 4. The agreement ratio is the average of all the
combinations of the three individual agreements.
In the same way, we also computed the average
Kappa statistic, which is often used to measure the
agreement by considering the chance rate.
A high concordance rate was obtained for the
General layer. When the specific layers and sub-
layers are taken into account, Kappa statistic was
35
Table 4: Agreement among labellers
General layer All layers
Agreement ratio 86.7% 74.2%
Kappa statistic 0.74 0.68
0.68, which is considered a good result for this
type of task. (cf. (Shriberg et al, 2004) etc.)
4.2.3 Analysis of Occurrence Tendency
during Progress of Episode
We then investigated the tendencies of tag occur-
rence through a dialogue to clarify how consult-
ing is conducted in the corpus. We annotated the
boundaries of episodes that determined the spots
to visit in order to carefully investigate the struc-
ture of the decision-making processes. In our cor-
pus, users were asked to write down their itinerary
for a practical one day tour. Thus, the beginning
and ending of an episode can be determined on the
basis of this itinerary.
As a result, we found 192 episodes. We selected
122 episodes that had more than 50 utterances,
and analyzed the tendency of tag occurrence. The
episodes were divided into five segments so that
each segment had an equal number of utterances.
The tendency of tag occurrence is shown in Figure
1. The relative occurrence rate denotes the number
of times the tags appeared in each segment divided
by the total number of occurrences throughout the
dialogues. We found three patterns in the tendency
of occurrence. The tags corresponding to the first
pattern frequently appear in the early part of an
episode; this typically applies to Open-Question,
WH-Question, and Wish. The tags of the sec-
ond pattern frequently appear in the later part, this
typically applies to Evaluation, Commitment, and
Opinion. The tags of the third pattern appear uni-
formly over an episode, e.g., Y/N-Question, Ac-
cept, and Elaboration. These statistics characterize
the dialogue flow of sightseeing planning, where
the guide and the user first clarify the latter?s in-
terests (Open, WH-Questions), list and evaluate
candidates (Evaluation), and then the user makes
a decision (Commitment).
This progression indicates that a session (or di-
alogue phase) management is required within an
episode to manage the consulting dialogue, al-
though the test-set perplexity2 , which was calcu-
2The perplexity was calculated by 10-fold cross validation
of the 30 dialogues.







    
	






	



	



	

	
Translation using Information on Dialogue Participants 
Setsuo Yamada, E i i ch i ro  Sumi ta  and  H idek i  Kashioka 
ATR Interpreting Telecommunications Research Laboratories* 
2-2, Hikaridai, Seika-cho, Soraku-gun, 
Kyoto, 619-0288, JAPAN 
{ syamada, sumita, kashioka} @itl.atr.co.jp t 
Abstract 
This paper proposes a way to improve the trans- 
lation quality by using information on dialogue 
participants that is easily obtained from out- 
side the translation component. We incorpo- 
rated information on participants' ocial roles 
and genders into transfer ules and dictionary 
entries. An experiment with 23 unseen dia- 
logues demonstrated a recall of 65% and a preci- 
sion of 86%. These results howed that our sim- 
ple and easy-to-implement method is effective, 
and is a key technology enabling smooth con- 
versation with a dialogue translation system. 
1 I n t roduct ion  
Recently, various dialogue translation systems 
have been proposed (Bub and others, 1997; 
Kurematsu and Morimoto, 1996; Rayner and 
Carter, 1997; Ros~ and Levin, 1998; Sumita 
and others, 1999; Yang and Park, 1997; Vi- 
dal, 1997). If we want to make a conversation 
proceed smoothly using these translation sys- 
tems, it is important o use not only linguis- 
tic information, which comes from the source 
language, but also extra-linguistic nformation, 
which does not come from the source language, 
but, is shared between the participants of the 
conversation. 
Several dialogue translation methods that 
use extra-linguistic information have been pro- 
posed. Horiguchi outlined how "spoken lan- 
guage pragmatic information" can be trans- 
lated (Horiguchi, 1997). However, she did not 
apply this idea to a dialogue translation system. 
LuperFoy et al proposed a software architec- 
*Current affiliation is ATR Spoken Language Trans- 
lation Research Laboratories 
Current mail addresses are 
{ setsuo.yarnada, eiichiro.sumita, hideki.kashioka} 
@slt. atr. co.jp 
ture that uses '% pragmatic adaptation" (Lu- 
perFoy and others, 1998), and Mima et al pro- 
posed a method that uses "situational informa- 
tion" (Mima and others, 1997). LuperFoy et al 
simulated their method on man-machine inter- 
faces and Mima et al preliminarily evaluated 
their method. Neither study, however, applied 
its proposals to an actual dialogue translation 
system. 
The above mentioned methods will need time 
to work in practice, since it is hard to obtain 
the extra-linguistic nformation on which they 
depend. 
We have been paying special attention to "po- 
liteness," because a lack of politeness can inter- 
fere with a smooth conversation between two 
participants, uch as a clerk and a customer. It 
is easy for a dialogue translation system to know 
which participant is the clerk and which is the 
customer from the interface (such as the wires 
to the microphones). 
This paper describes a method of "polite- 
ness" selection according to a participant's so- 
cial role (a clerk or a customer), which is eas- 
ily obtained from the extra-linguistic environ- 
ment. We incorporated each participant's so- 
cial role into transfer ules and transfer dictio- 
nary entries. We then conducted an experiment 
with 23 unseen dialogues (344 utterances). Our 
method achieved a recall of 65% and a preci- 
sion of 86%. These rates could be improved to 
86% and 96%, respectively (see Section 4). It 
is therefore possible to use a "participant's so- 
cial role" (a clerk or a customer in this case) 
to appropriately make the translation results 
"polite," and to make the conversation proceed 
smoothly with a dialogue translation system. 
Section 2 analyzes the relationship between a
particular participant's social role (a clerk) and 
politeness in Japanese. Section 3 describes our 
proposal in detail using an English-to-Japanese 
37 
translation system. Section 4 shows an exper- 
iment and results, followed by a discussion in 
Section 5. Finally, Section 6 concludes this pa- 
per. 
2 A Par t i c ipant ' s  Soc ia l  Ro le  and  
Po l i teness  
This section focuses on one participant's social 
role. We investigated Japanese outputs of a di- 
alogue translation system to see how many ut- 
terances hould be polite expressions in a cur- 
rent translation system for travel arrangement. 
We input 1,409 clerk utterances into a Transfer 
Driven Machine Translation system (Sumita 
and others, 1999) (TDMT for short). The in- 
puts were closed utterances, meaning the sys- 
tem already knew the utterances, enabling the 
utterances to be transferred at a good quality. 
Therefore, we used closed utterances as the in- 
puts to avoid translation errors. 
As a result, it was shown that about 70% 
(952) of all utterances should be improved to use 
polite expressions. This result shows that a cur- 
rent translation system is not enough to make 
a conversation smoothly. Not surprisingly, if all 
expressions were polite, some Japanese speakers 
would feel insulted. Therefore, Japanese speak- 
ers do not have to use polite expression in all 
utterances. 
We classified the investigated ata into dif- 
ferent ypes of English expressions for Japanese 
politeness, i.e., into honorific titles, parts of 
speech such as verbs, and canned phrases, 
as shown in Table 1; however, not all types 
appeared in the data. For example, when 
the clerk said "How will you be paying, Mr. 
Suzuki," the Japanese translation was made 
polite as "donoyouni oshiharaininarimasu-ka 
suzuki-sama" in place of the standard expres- 
sion "donoyouni shiharaimasu-ka suzuki-san." 
Table 1 shows that there is a difference in 
how expressions should be made more polite ac- 
cording to the type, and that many polite ex- 
pressions can be translated by using only local 
information, i.e., transfer rules and dictionary 
entries. In the next section, we describe how to 
incorporate the information on dialogue partic- 
ipants, such as roles and genders, into transfer 
rules and dictionary entries in a dialogue trans- 
lation system. 
3 A Method  of  Us ing  In fo rmat ion  
on  D ia logue  Par t i c ipants  
This section describes how to use information 
on dialogue participants, such as participants' 
social roles and genders. First, we describe 
TDMT, which we also used in our experiment. 
Second, we mention how to modify transfer 
rules and transfer dictionary entries according 
to information on dialogue participants. 
3.1 Transfer  Dr iven  Mach ine  
Trans la t ion  
TDMT uses bottom-up left-to-right chart pars- 
ing with transfer rules as shown in Figure 1. 
The parsing determines the best structure and 
best transferred result locally by performing 
structural disambiguation using semantic dis- 
tance calculations, in parallel with the deriva- 
tion of possible structures. The semantic dis- 
tance is defined by a thesaurus. 
(source pattern) 
==~ 
J ((target pattern 1) 
((source xample 1) 
(source xample 2) 
? "- ) 
(target pattern 2) 
?o* ) 
Figure 1: Transfer ule format 
A transfer ule consists of a source pattern, 
a target pattern, and a source example. The 
source pattern consists of variables and con- 
stituent boundaries (Furuse and Iida, 1996). 
A constituent boundary is either a functional 
word or the part-of-speech of a left constituent's 
last word and the part-of-speech of a right con- 
stituent's first word. In Example (1), the con- 
stituent boundary IV-CN) is inserted between 
"accept" and "payment," because "accept" is 
a Verb and "payment" is a Common Noun. 
The target pattern consists of variables that cor- 
respond to variables in the source pattern and 
words of the target language. The source exam- 
ple consists of words that come from utterances 
referred to when a person creates transfer ules 
(we call such utterances closed utterances). 
Figure 2 shows a transfer ule whose source 
pattern is (X (V-CN) Y). Variable X corre- 
sponds to x, which is used in the target pat- 
tern, and Y corresponds to y, which is also 
38 
Table 1: Examples of polite expressions 
Type: verb, title 
Eng: How will you be paying, Mr. Suzuki 
Standard: donoyouni shiharaimasu-ka suzuki-san 
Polite: donoyouni o_shiharaininarimasu-ka suzuki-sama 
Gloss: How pay-QUESTION suzuki-Mr. 
Type: verb, common noun 
Eng: We have two types of rooms available 
Standard: aiteiru ni-shurui-no heya-ga ariraasu 
Polite: aiteiru ni-shurui-no oheya-ga gozaimasu 
Gloss: available two-types-of room-TOP have 
Type: auxiliary verb 
Eng: You can shop for hours 
Standard: suujikan kaimono-wo surukotogadekimasu 
Polite: suujikan kaimono-wo shiteitadakemasu 
Gloss: for hours make-OBJ can 
Type: pronoun 
Eng: Your room number, please 
Standard: anatano heya bangou-wo 
Polite: okyakusamano heya bangou-wo 
Gloss: Your room number-so obj 
onegaishirnasu 
onegaishimasu 
please 
Type: canned phrase 
Eng: How can I help you 
Standard: dou shimashitaka 
Polite: douitta goyoukendeshouka 
Gloss: How can I help you 
Example (1) 
Eng: We accept payment by credit card 
Standard: watashitachi-wa kurejitlo-kaado-deno shiharai-wo ukelsukemasu 
Polite: watashidomo-wa kurejitto-kaado-deno o_shiharai-wo ukeshimasu 
Gloss: We-TOP credit-card-by payment-OBJ accept 
used in the target pattern. The source exam- 
ple (("accept") ("payment")) comes from Ex- 
ample (1), and the other source examples come 
from the other closed utterances. This transfer 
rule means that if the source pattern is (X (V- 
CN) Y) then (y "wo" x) or (y "ni" x) is selected 
as the target pattern, where an input word pair 
corresponding to X and Y is semantically the 
most similar in a thesaurus to, or exactly the 
same as, the source example. For example, if 
an input word pair corresponding to X and Y 
is semantically the most similar in a thesaurus 
to, or exactly the same as, (("accept") ("pay- 
ment")), then the target pattern (y "wo" x) is 
selected in Figure 2. As a result, an appropriate 
target pattern is selected. 
After a target pattern is selected, TDMT cre- 
ates a target structure according to the pattern 
(X (V-CN) Y) 
((y "wo" x) 
((("accept") ("payment")) 
(("take") ("picture"))) 
(y "hi" x) 
((("take") ("bus")) 
(("get") ("sunstroke"))) 
) 
Figure 2: Transfer ule example 
by referring to a transfer dictionary, as shown 
in Figure 3. If the input is "accept (V -CN)  
payment," then this part is translated into "shi- 
harai wo uketsukeru." "wo" is derived from the 
target pattern (y "wo" x), and "shiharai" and 
"uketsukeru" are derived from the transfer dic- 
tionary, as shown in Figure 4. 
39 
(source pattern) 
(((target pattern 11) :pattern-cond 11
(target pattern 12) :pattern-cond 12 
itarget pattern In) :default) 
((source xample 1) 
? oo ) 
(((source xample 1) ~ (target word lt) :word-cond 11 
(source example 1) --* (target word 12) :word-cond 12 
?? .  
(source example 1) --* (target word lm) :default) 
o . "  ) 
(((target pattern 21) :pattern-cond 21 
. . .  ) ) )  
Figure 5: Transfer ule format with information on dialogue participants 
(((source word 1) --* (target word 11) :cond 11 I 
(source word 1) -* (target word 12) :cond 12 I 
I . . .  
(source word 1) -~ (target word lk) :default)\[ 
o*.  ) I 
Figure 6: Dictionary format with information on dialogue participants 
((source word) ~ (target word) 
? " .  ) 
Figure 3: Transfer dictionary format 
(("accept") --* ("uketsukeru') I ("payment") --* ("shiharai"))  
Figure 4: Transfer dictionary example 
(X "sama") 
((("Mr." x) :h-gender male 
("Ms." x) :h-gender female 
("Mr-ms." x)) 
(("room number"))) 
) 
Figure 7: Transfer ule example with the par- 
ticipant's gender 
3.2 Transfer Rules and Entr ies 
according to Information on 
Dialogue Part ic ipants 
For this research, we modified the transfer ules 
and the transfer dictionary entries, as shown in 
Figures 5 and 6. In Figure 5, the target pattern 
"target pattern 11" and the source word "source 
example 1" are used to change the translation 
according to information on dialogue partici- 
pants. For example, if ":pattern-cond 11" is de- 
fined as ":h-gender male" as shown in Figure 7, 
then "target pattern 11" is selected when the 
hearer is a male, that is, "("Mr." x)" is selected. 
Moreover, if ":word-cond 11" is defined as ":s- 
role clerk" as shown in Figure 8, then "source 
example 1" is translated into "target word 11" 
when the speaker is a clerk, that is, "accept" is 
translated into "oukesuru." Translations uch 
as "target word 11" are valid only in the source 
pattern; that is, a source example might not 
always be translated into one of these target 
words. If we always want to produce transla- 
tions according to information on dialogue par- 
ticipants, then we need to modify the entries 
in the transfer dictionary like Figure 6 shows. 
Conversely, if we do not want to always change 
the translation, then we should not modify the 
entries but modify the transfer ules. Several 
conditions can also be given to ":word-cond" 
and ":pattern-cond." For example, ":s-role cus- 
tomer and :s-gender female," which means the 
speaker is a customer and a female, can be 
given. In Figure 5, ":default" means the de- 
40 
fault target pattern or word if no condition is 
matched. The condition is checked from up to 
down in order; that is, first, ":pattern-cond 11," 
second, ":pattern-cond 1~," ... and so on. 
(X (V-CN) Y) 
((y "wo" x) 
((("accept") ("payment")) 
(("take") ("picture"))) 
((("accept") -~ ("oukesuru"):s-role clerk 
( "accept" ) --+ ( "uketsukeru" ) )) 
) 
Figure 8: Transfer ule example with a partici- 
pant's role 
((("payment") --~ ("oshiharai") :s-role clerk 
( "payment" ) ---* ( "shiharai" )) 
(("we") --* ("watashidomo") :s-role clerk 
("we") --~ ("watashltachi"))) 
Figure 9: Transfer dictionary example with a 
speaker's role 
Even though we do not have rules and en- 
tries for pattern conditions and word condi- 
tions according to another participant's infor- 
mation, such as ":s-role customer'(which means 
the speaker's role is a customer) and ":s-gender 
male" (which means the speaker's gender is 
male), TDMT can translate xpressions corre- 
sponding to this information too. For example, 
"Very good, please let me confirm them" will 
be translated into "shouchiitashimasita kakunin 
sasete itadakimasu" when the speaker is a clerk 
or "soredekekkoudesu kakunin sasete kudasai" 
when the speaker is a customer, as shown in 
Example (2). 
By making a rule and an entry like the ex- 
amples shown in Figures 8 and 9, the utter- 
ance of Example (1) will be translated into 
"watashidomo wa kurejitto kaado deno oshi- 
harai wo oukeshimasu" when the speaker is a 
clerk. 
4 An  Exper iment  
The TDMT system for English-to-Japanese at 
the time Of the experiment had about 1,500 
transfer ules and 8,000 transfer dictionary en- 
tries. In other words, this TDMT system was 
capable of translating 8,000 English words into 
Japanese words. About 300 transfer ules and 
40 transfer dictionary entries were modified to 
improve the level of "politeness." 
We conducted an experiment using the trans- 
fer rules and transfer dictionary for a clerk with 
23 unseen dialogues (344 utterances). Our input 
was off-line, i.e., a transcription of dialogues, 
which was encoded with the participant's social 
role. In the on-line situation, our system can 
not infer whether the participant's social role is 
a clerk or a customer, but can instead etermine 
the role without error from the interface (such 
as a microphone or a button). 
In order to evaluate the experiment, we clas- 
sifted the Japanese translation results obtained 
for the 23 unseen dialogues (199 utterances from 
a clerk, and 145 utterances from a customer, 
making 344 utterances in total) into two types: 
expressions that had to be changed to more po- 
lite expressions, and expressions that did not. 
Table 2 shows the number of utterances that in- 
cluded an expression which had to be changed 
into a more polite one (indicated by "Yes") and 
those that did not (indicated by "No"). We ne- 
glected 74 utterances whose translations were 
too poor to judge whether to assign a "Yes" or 
"No." 
Table 2: The number of utterances to be 
changed or not 
Necessity | The number 
of change I of utterances 
Yes 104 
No 166 
Out of scope 74 
Total \[ 344 
* 74 translations were too poor to handle for the 
"politeness" problem, and so they are ignored in this 
paper. 
The translation results were evaluated to see 
whether the impressions of the translated re- 
sults were improved or not with/without mod- 
ification for the clerk from the viewpoint of 
"politeness." Table 3 shows the impressions 
obtained according to the necessity of change 
shown in Table 2. 
The evaluation criteria are recall and preci- 
sion, which are defined as follows: 
Recall = 
number of utterances whose impression is better 
number of utterances which should be more polite 
41 
Example (2) 
Eng: Very good, please let me confirm them 
Standard: wakarimasita kakunin sasete 
Clerk: shouchiitashimasita kakunin sase~e 
Customer: soredekekkoudesu kakunin sasete 
Gloss: very good con:firm let me 
kudasai 
itadakimasu 
kudasai 
please 
Table 3: Evaluation on using the speaker's role 
Necessity 
of change 
Yes 
(lo4) 
No 
(166) 
~ Impression 
better 
same 
worse  
no-diff 
better 
s alTle 
worse  
no-diff 
The number 
of utterances 
68 
5 
3 
28 
0 
3 
0 
163 
bet ter :  Impression of a translation is better. 
same:  Impression of a translation has not changed. 
worse: Impression of a translation is worse. 
no-diff: There is no difference between the two 
translations. 
Precision = 
number of utterances whose impression is better 
number of utterances whose expression has been 
changed by the modified rules and entries 
The recall was 65% (= 68 - (68 + 5 + 3 + 28)) 
and the precision was 86% (= 68 -: (68 + 5 + 3 + 
0+3+0)).  
There are two main reasons which bring down 
these rates. One reason is that TDMT does not 
know who or what the agent of the action in 
the utterance is; agents are also needed to se- 
lect polite expressions. The other reason is that 
there are not enough rules and transfer dictio- 
nary entries for the clerk. 
It is easier to take care of the latter problem 
than the former problem. If we resolve the lat- 
ter problem, that is, if we expand the transfer 
rules and the transfer dictionary entries accord- 
ing to the "participant's social role" (a clerk and 
a customer), then the recall rate and the preci- 
sion rate can be improved (to 86% and 96%, 
respectively, as we have found). As a result, we 
can say that our method is effective for smooth 
conversation with a dialogue translation system. 
5 D iscuss ion  
In general, extra-linguistic information is hard 
to obtain. However, some extra-linguistic infor- 
mation can be easily obtained: 
(1) One piece of information is the participant's 
social role, which can be obtained from the in- 
terface such as the microphone used. It was 
proven that a clerk and customer as the social 
roles of participants are useful for translation 
into Japanese. However, more research is re- 
quired on another participant's social role. 
(2) Another piece of information is the par- 
ticipant's gender, which can be obtained by a 
speech recognizer with high accuracy (Takezawa 
and others, 1998; Naito and others, 1998). We 
have considered how expressions can be useful 
by using the hearer's gender for Japanese-to- 
English translation. 
Let us consider the Japanese honorific title 
"sama" or "san." If the heater's gender is male, 
then it should be translated "Mr." and if the 
hearer's gender is female, then it should be 
translated "Ms." as shown in Figure 7. Ad- 
ditionally, the participant's gender is useful for 
translating typical expressions for males or fe- 
males. For example, Japanese "wa" is often at- 
tached at the end of the utterance by females. 
It is also important for a dialogue translation 
system to use extra-linguistic information which 
the system can obtain easily, in order to make 
a conversation proceed smoothly and comfort- 
ably for humans using the translation system. 
We expect hat other pieces of usable informa- 
tion can be easily obtained in the future. For 
example, age might be obtained from a cellular 
telephone if it were always carried by the same 
person and provided with personal information. 
In this case, if the system knew the hearer was a 
child, it could change complex expressions into 
easier ones. 
6 Conc lus ion  
We have proposed a method of translation us- 
ing information on dialogue participants, which 
42 
is easily obtained from outside the translation 
component, and applied it to a dialogue trans- 
lation system for travel arrangement. This 
method can select a polite expression for an 
utterance according to the "participant's social 
role," which is easily determined by the inter- 
face (such as the wires to the microphones). For 
example, if the microphone is for the clerk (the 
speaker is a clerk), then the dialogue translation 
system can select a more polite expression. 
In an English-to-Japanese translation system, 
we added additional transfer ules and transfer 
dictionary entries for the clerk to be more po- 
lite than the customer. Then, we conducted an 
experiment with 23 unseen dialogues (344 ut- 
terances). We evaluated the translation results 
to see whether the impressions of the results im- 
proved or not. Our method achieved a recall of 
65% and a precision of 86%. These rates could 
easily be improved to 86% and 96%, respec- 
tively. Therefore, we can say that our method 
is effective for smooth conversation with a dia- 
logue translation system. 
Our proposal has a limitation in that if the 
system does not know who or what the agent 
of an action in an utterance is, it cannot ap- 
propriately select a polite expression. We are 
considering ways to enable identification of the 
agent of an action in an utterance and to ex- 
pand the current framework to improve the level 
of politeness even more. In addition, we intend 
to apply other extra-linguistic nformation to a 
dialogue translation system. 
References  
Thomas Bub et al 1997. Verbmobih The 
combination of deep and shallow processing 
for spontaneous speech translation. In the 
1997 International Conference on Acoustics, 
Speech, and Signal Processing: ICASSP 97, 
pages 71-74, Munich. 
Osamu Furuse and Hitoshi Iida. 1996. In- 
cremental translation utilizing constituent 
boundary patterns. In Proceedings of 
COLING-96, pages 412-417, Copenhagen. 
Keiko Horiguchi. 1997. Towards translating 
spoken language pragmatics in an analogical 
framework. In Proceedings ofA CL/EA CL-97 
workshop on Spoken Language Translation, 
pages 16-23, Madrid. 
Akira Kurematsu and Tsuyoshi Morimoto. 
1996. Automatic Speech Translation. Gordon 
and Breach Publishers. 
Susann LuperFoy et al 1998. An architecture 
for dialogue management, context tracking, 
and pragmatic adaptation i  spoken dialogue 
system. In Proceedings of COLING-A CL'98, 
pages 794-801, Montreal. 
Hideki Mima et al 1997. A situation-based 
approach to spoken dialogue translation be- 
tween different social roles. In Proceedings of
TMI-97, pages 176-183, Santa Fe. 
Masaki Naito et al 1998. Acoustic and lan- 
guage model for speech translation system 
ATR-MATRIX. In the Proceedings of the 
1998 Spring Meeting of the Acoustical Soci- 
ety of Japan, pages 159-160 (in Japanese). 
Manny Rayner and David Carter. 1997. Hy- 
brid language processing in the spoken lan- 
guage translator. In the 1997 International 
Conference on Acoustics, Speech, and Signal 
Processing: ICASSP 97, pages 107-110, Mu- 
nich. 
Carolyn Penstein Ros~ and Lori S. Levin. 1998. 
An interactive domain independent approach 
to robust dialogue interpretation. In Proceed- 
ings of COLING-ACL'98, pages 1129-1135, 
Montreal. 
Eiichiro Sumita et al 1999. Solutions to prob- 
lems inherent in spoken-language translation: 
The ATR-MATRIX approach. In the Ma- 
chine Translation Summit VII, pages 229- 
235, Singapore. 
Toshiyuki Takezawa et al 1998. A Japanese- 
to-English speech translation system: ATR- 
MATRIX. In the 5th International Con- 
ference On Spoken Language Processing: 
ICSLP-98, pages 2779-2782, Sydney. 
Enrique Vidal. 1997. Finite-state speech-to- 
speech translation. In the 1997 International 
Conference on Acoustics, Speech, and Signal 
Processing: ICASSP 97, pages 111-114, Mu- 
nich. 
Jae-Woo Yang and Jun Park. 1997. An exper- 
iment on Korean-to-English and Korean-to- 
Japanese spoken language translation. In the 
1997 International Conference on Acoustics, 
Speech, and Signal Processing: ICASSP 97, 
pages 87-90, Munich. 
43 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 33?41, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Unsupervised SVM Classifier for Answer Selection in Web
Question Answering
Youzheng Wu, Ruiqiang Zhang, Xinhui Hu, and Hideki Kashioka
National Institute of Information and Communications Technology (NICT),
ATR Spoken Language Communication Research Labs.
2-2-2 Hikaridai ?Keihanna Science City? Kyoto 619-0288 Japan
{Youzheng.wu,Ruiqiang.zhang,Xinhui.hu,Hideki.kashioka}@atr.jp
Abstract
Previous machine learning techniques for
answer selection in question answering
(QA) have required question-answer train-
ing pairs. It has been too expensive and
labor-intensive, however, to collect these
training pairs. This paper presents a novel
unsupervised support vector machine (U-
SVM) classifier for answer selection, which
is independent of language and does not re-
quire hand-tagged training pairs. The key
ideas are the following: 1. unsupervised
learning of training data for the classifier by
clustering web search results; and 2. select-
ing the correct answer from the candidates
by classifying the question. The compara-
tive experiments demonstrate that the pro-
posed approach significantly outperforms
the retrieval-based model (Retrieval-M), the
supervised SVM classifier (S-SVM), and the
pattern-based model (Pattern-M) for answer
selection. Moreover, the cross-model com-
parison showed that the performance rank-
ing of these models was: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
1 Introduction
The purpose of answer selection in QA is to se-
lect the exact answer to the question from the ex-
tracted candidate answers. In recent years, many
supervised machine learning techniques for answer
selection in open-domain question answering have
been investigated in some pioneering studies [Itty-
cheriah et al 2001; Ng et al 2001; Suzuki et al
2002; Sasaki, et al 2005; and Echihabi et al 2003].
Compared with retrieval-based [Yang et al 2003],
pattern-based [Ravichandran et al 2002 and Soub-
botin et al 2002], and deep NLP-based [Moldovan
et al 2002, Hovy et al 2001; and Pasca et al 2001]
answer selection, machine learning techniques are
more effective in constructing QA components from
scratch. These techniques suffer, however, from the
problem of requiring an adequate number of hand-
tagged question-answer training pairs. It is too ex-
pensive and labor intensive to collect such training
pairs for supervised machine learning techniques.
To tackle this knowledge acquisition bottleneck,
this paper presents an unsupervised SVM classifier
for answer selection, which is independent of lan-
guage and question type, and avoids the need for
hand-tagged question-answer pairs. The key ideas
are as follows:
1. Regarding answer selection as a kind of classi-
fication task and adopting an SVM classifier;
2. Applying unsupervised learning of pseudo-
training data for the SVM classifier by cluster-
ing web search results;
3. Training the SVM classifier by using three
types of features extracted from the pseudo-
training data; and
4. Selecting the correct answer from the candidate
answers by classifying the question. Note that
this means classifying a question into one of
the clusters learned by clustering web search
results. Therefore, our classifying the question
33
Figure 1: Web Question Answering Architecture
is different from conventional question classifi-
cation (QC) [Li et al 2002] that determines the
answer type of the question.
The proposed approach is fully unsupervised and
starts only from a user question. It does not require
richly annotated corpora or any deep linguistic tools.
To the best of our knowledge, no research on this
kind of study we discuss here has been reported.
Figure 1 illustrates the architecture of our web QA
approach. The S-SVM and Pattern-M models are
included for comparison.
Because the focus of this paper just evaluates the
answer selection part, our approach requires knowl-
edge of the answer type to the question in order to
find candidate answers, and that the answer must be
a NE for convenience in candidate extraction. Ex-
periments using Chinese versions of the TREC 2004
and 2005 test data sets show that our approach sig-
nificantly outperforms the S-SVM for answer selec-
tion, with a top 1 score improvement of more than
20%. Results obtained with the test data set in [Wu
et al 2004] show that the U-SVM increases the
top 1/mrr 5/top 5 scores by 5.95%/6.06%/8.68%
as compared with the Pattern-M. Moreover, our
cross-model comparison demonstrates that the per-
formance ranking of all models considered is: U-
SVM > Pattern-M > S-SVM > Retrieval-M.
2 Comparison among Models
Related researches on answer selection in QA can be
classified into four categories. The retrieval-based
model [Yang et al 2003] selects a correct answer
from the candidates according to the distance be-
tween a candidate and all question keywords. This
model does not work, however, if the question and
the answer-bearing sentences do not match on the
surface. The pattern-based model [Ravichandran
et al 2002 and Soubbotin et al 2002] first clas-
sifies the question into predefined categories, and
then extracts the exact answer by using answer pat-
terns learned off-line. Although the pattern-based
model can obtain high precision for some prede-
fined types of questions, it is difficult to define ques-
tion types in advance for open-domain question an-
swering. Furthermore, this model is not suitable for
all types of questions. The deep NLP-based model
[Moldovan et al 2002; Hovy et al 2001; and Pasca
et al 2001] usually parses the user question and an
answer-bearing sentence into a semantic represen-
tation, and then semantically matches them to find
the answer. This model has performed very well at
TREC workshops, but it heavily depends on high-
performance NLP tools, which are time consuming
and labor intensive for many languages. Finally, the
machine learning-based model has also been inves-
tigated. current models of this type are based on su-
pervised approaches [Ittycheriah et al 2001; Ng et
al. 2001; Suzuki et al 2002; and Sasaki et al 2005]
that are heavily dependent on hand-tagged question-
answer training pairs, which not readily available.
In response to this situation, this paper presents
the U-SVM for answer selection in open-domain
web question answering system. Our U-SVM has
the following advantages over supervised machine
learning techniques. First, the U-SVM classifies
questions into a question-dependent set of clusters,
and the answer is the name of a question cluster.
In contrast, most previous models have classified
candidates into positive and negative. Second, the
U-SVM automatically learns the unique question-
dependent clusters and the pseudo-training for each
34
Table 1: Comparison of Various Machine Learning Techniques
System Model Key Idea Training Data
[Ittycheriah et al 2001] ME Classifier Classifying candidates into positive
and negative
5,000 English
Q-A pairs
[Suzuki et al 2002] SVM Classifier Classifying candidates into positive
and negative
1358 Japanese
Q-A pairs
[Echihabi et al 2003] N-C Model Selecting correct answer by aligning
question with sentences
90,000 English
Q-A pairs
[Sasaki et al 2005] ME Classifier Classifying words in sentences into an-
swer and non-answer words
2,000 Japanese
Q-A pairs
Our U-SVM Model SVM Classifier Classifying question into a set of
question-dependent clusters
No Q-A pairs
question. This differs from the supervised tech-
niques, in which a large number of hand-tagged
training pairs are shared by all of the test ques-
tions. In addition, supervised techniques indepen-
dently process the answer-bearing sentences, so the
answers to the questions may not always be ex-
tractable because of algorithmic limitations. On the
other hand, the U-SVM can use the interdependence
between answer-bearing sentences to select the an-
swer to a question.
Table 1 compares the key idea and training data
used in the U-SVM with those used in the supervised
machine learning techniques. Here, ME means the
maximum entropy model, and N-C means the noisy-
channel model.
3 The U-SVM
The essence of the U-SVM is to regard answer selec-
tion as a kind of text categorization-like classifica-
tion task, but with no training data available. In the
U-SVM, the steps of ?clustering web search results?,
?classifying the question?, and ?training SVM clas-
sifier? play very important roles.
3.1 Clustering Web Search Results
Web search results, such as snippets returned by
Google, usually include a mixture of multiple
subtopics (called clusters in this paper) related to
the user question. To group the web search results
into clusters, we assume that the candidate answer in
each Google snippet can represent the ?signature? of
its cluster. In other words, the Google snippets con-
taining the same candidate are regarded as aligned
snippets, and thus belong to the same cluster. Web
search results are clustered in two phases.
? A first-stage Google search (FGS) is ap-
plied to extract n candidate answers
{c1, c2, . . . , cn} from the top m Google
snippets {s1, s2, . . . , sm} by a NER tool
[Wu et al 2005]. Those snippets containing
the candidates {ci} and at least one ques-
tion keyword {qi} are retained. Finally,
the retained snippets {s1, s2, . . . , sm} are
clustered into n clusters {C1, C2, . . . , Cn}
by clustering web search results, that is,
If a snippet includes L different candidates,
the snippet belongs to L different clusters.
If the candidates of different snippets are
the same, these snippets belong to the same
clusters.
Consequently, the number of clusters {Ci} is
fully determined by the number of candidates
{ci}, and the cluster name of a cluster Ci is the
candidate answer ci. Up to this point, we have
obtained clusters and sample snippets for each
cluster that will be used as training data for the
SVM classifier. Because this training data is
learned automatically, rather than hand-tagged,
we call it pseudo-training data.
? A second-stage Google search (SGS) is ap-
plied to resolve data sparseness in the pseudo-
training samples learned through the FGS. The
FGS data may have very few training snip-
pets in some clusters, so more snippets must
be collected. Note that this step just learns new
35
Google snippets into the clusters learned by the
FGS, but does not add new clusters.
For each candidate answer ci:
Combine the original query q = {qi} and
the candidate ci to form a new query q? =
{q, ci}.
Submit q? to Google and download the top
50 Google snippets.
Retain the snippets containing the candi-
date ci and at least one keyword qi.
Group the retained snippets into n clusters
to form the new pseudo-training data.
End
Here, we give an example illustrating the prin-
ciple of clustering web search results in the
FGS. In submitting TREC 2004 test question 1.1
?when was the first Crip gang started?? to Google
(http://www.google.com/apis), we extract n(= 8)
different candidates from the top m(= 30) Google
snippets. The Google snippets containing the same
candidates are aligned snippets, and thus the 12 re-
tained snippets are grouped into 8 clusters, as listed
in Table 2. This table roughly indicates that the snip-
pets with the same candidate answers contain the
same sub-meanings, so these snippets are considered
as aligned snippets. For example, all Google snip-
pets that contain the candidate answer 1969 express
the time of establishment of ?the first Crip gang?.
In summary, the U-SVM uses the result of ?clus-
tering web search results? as the pseudo-training
data of the SVM classifier, and then classifies user
question into one of the clusters for answer selec-
tion. On the one hand, the clusters and their names
are based on candidate answers to question; on the
other hand, candidates are dependent on question.
Therefore, the clusters are question-dependent.
3.2 Classifying Question
Using the pseudo-training data obtained by cluster-
ing web search results to train the SVM classifier,
we classify user questions into a set of question-
dependent clusters and assume that the correct an-
swer is the name of the question cluster that is as-
signed by the trained U-SVM classifier. For the
above example, if the U-SVM classifier, trained on
the pseudo-training data listed in Table 2, classifies
the above test question into a cluster whose name is
1969, then the cluster name 1969 is the answer to
the question.
This paper selects LIBSVM toolkit1 to implement
the SVM classifier. The kernel is the radical basis
function with the parameter ? = 0.001 in the exper-
iments.
3.3 Feature Extraction
To classify the question into a question-dependent
set of clusters, the U-SVM classifier extracts three
types of features.
? A similarity-based feature set (SBFS) is
extracted from the Google snippets. The SBFS
attempts to capture the word overlap between
a question and a snippet. The possible values
range from 0 to 1.
SBFS Features
percentage of matched keywords (KWs)
percentage of mismatched KWs
percentage of matched bi-grams of KWs
percentage of matched thesauruses
normalized distance between candidate and
KWs
To compute the matched thesaurus feature, we
adopt TONGYICICILIN 2 in the experiments.
? A Boolean match-based feature set (BMFS) is
also extracted from the Google snippets. The
BMFS attempts to capture the specific key-
word Boolean matches between a question and
a snippet. The possible values are true or false.
BMFS Features
person names are matched or not
location names are matched or not
organization names are matched or not
time words are matched or not
number words are matched or not
root verb is matched or not
candidate has or does not have bi-gram in
snippet matching bi-gram in question
candidate has or does not have desired
named entity type
? A window-based word feature set (WWFS)
is a set of words consisting of the words
1http://www.csie.ntu.edu.tw/ cjlin/libsvm/
2A Chinese Thesaurus Lexicon
36
Table 2: Clustering Web Search Results
Cluster Name Google Snippet
1969 It is believed that the first Crip gang was formed in late 1969. During this time in
Los Angeles there were ...
... the first Bloods and Crips gangs started forming in Los Angeles in late 1969, the
Island Bloods sprung up in north Pomona ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
August 8, 2005 High Country News ? August 8, 2005: The Gangs of Zion
2004 2004 main 1 Crips 1.1 FACTOID When was the first Crip gang started? 1.2 FAC-
TOID What does the name mean or come...
1972 One of the first-known and publicized killings by Crip gang members occurred at
the Hollywood Bowl in March 1972.
1971 Williams joined Washington in 1971, forming the westside faction of what had
come to be called the Crips.
The Crips gang formed as a kind of community watchdog group in 1971 after the
demise of the Black Panthers. ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
1982 Oceanside police first started documenting gangs in 1982, when five known gangs
were operating in the city: the Posole Locos...
mid-1990s Street Locos; Deep Valley Bloods and Deep Valley Crips. By the mid-1990s, gang
violence had ...
1970s The Blood gangs started up as opposition to the Crips gangs, also in the 1970s, and
the rivalry stands to this day ...
preceding {wi?5, . . . , wi?1} and following
{wi+1, . . . , wi+5} the candidate answer. The
WWFS features can be regarded as a kind of
relevant snippets-based question keywords ex-
pansion. By extracting the WWFS feature set,
the feature space in the U-SVM becomes ques-
tion dependent, which may be more suitable for
classifying the question. The number of classi-
fication features in the S-SVM must be fixed,
however, because all questions share the same
training data. This is one difference between
the U-SVM and the supervised SVM classifier
for answer selection. Each word feature in the
WWFS is weighted using its ISF value.
ISF (wj , Ci) =
N(wj , Ci) + 0.5
N(wj) + 0.5
(1)
where N(wj) is the total number of the
snippets containing word feature wj , and
N(wj , Ci) is the number of snippets in cluster
Ci containing word feature wj .
When constructing question vector, we assume
that the question is an ideal question that con-
tains all the extracted WWFS words. There-
fore, the values of the WWFS word features in
question vector are 1. Similarly, the values of
the SBFS and BMFS features in question vec-
tor are also estimated by self-similarity calcu-
lation.
4 Experiments
4.1 Data Sets
For the experiments, no English named entity recog-
nition (NER) tool is in our hand at the time of
the experiments; therefore, we validate the U-SVM
37
in terms of Chinese web QA using three test data
sets, which will be published with this paper3. Al-
though the U-SVM is independent of the question
types, for convenience in candidate extraction, only
those questions whose answers are named entities
are selected. The three test data sets are CTREC04,
CTREC05 and CTEST05. CTREC04 is a set of
178 Chinese questions translated from TREC 2004
FACTOID testing questions. CTREC05 is a set of
279 Chinese questions translated from TREC 2005
FACTOID testing questions. CTEST05 is a set of
178 Chinese questions found in [Wu et al 2004]
that are similar to TREC testing questions except
that they are written in Chinese. Figure 2 breaks
down the types of questions (manually assigned) in
the CTREC04 and CTREC05 data sets. Here, PER,
LOC, ORG, TIM, NUM, and CR refer to questions
whose answers are a person, location, organization,
time, number, and book or movie, respectively.
Figure 2: Statistics of CTEST05
To collect the question-answer training data for
the S-SVM, we submitted 807 Chinese questions to
Google and extracted the candidates for each ques-
tion from the top 50 Google snippets. We then man-
ually selected the snippets containing the correct
answers as positive snippets, and designated all of
the other snippets as negative snippets. Finally, we
collected 807 hand-tagged Chinese question-answer
pairs as the training data of S-SVM called CTRAIN-
DATA.
4.2 Evaluation Method
In the experiments, the top m(= 50) Google snip-
pets are adopted to extract candidates by using a
3Currently no public testing question set for simplified Chi-
nese QA is available.
Chinese NER tool [Wu et al 2005]. The number of
the candidates extracted from the top m(= 50) snip-
pets, n, is adaptive for different questions but it does
not exceed 30. The results are evaluated in terms
of two scores, top n and mrr 5. Here, top n is the
rate at which at least one correct answer is included
in the top n answers, while mrr 5 is the average re-
ciprocal rank (1/n) of the highest rank n(n ? 5) of
a correct answer to each question.
4.3 U-SVM vs. Retrieval-M
The Retrieval-M selects the candidate with the short-
est distances to all question keywords as the cor-
rect answer. In this experiment, the Retrieval-M
is implemented based on the snippets returned by
Google, while the U-SVM is based on the SGS data,
the SBFS and BMFS feature. Table 3 summarizes
the comparative performance.
Table 3: Comparison of Retrieval-M and U-SVM
Retrieval-M U-SVM
top 1 27.84% 53.61%
CTREC04 mrr 5 43.67% 66.25%
top 5 71.13% 88.66%
top 1 34.00% 50.00%
CTREC05 mrr 5 48.20% 62.38%
top 5 71.33% 82.67%
The table shows that the U-SVM greatly improves
the performance of the Retrieval-M: the top 1 im-
provements for CTREC04 and CTREC05 are about
25.8% and 16.0%, respectively. This experiment
demonstrates that the assumptions used here in clus-
tering web search results and in classifying the ques-
tion are effective in many cases, and that the U-SVM
benefits from these assumptions.
4.4 U-SVM vs. S-SVM
To explore the effectiveness of our unsupervised
model as compared with the supervised model, we
conduct a cross-model comparison of the S-SVM
and the U-SVM with the SBFS and BMFS feature
sets. The U-SVM results are compared with the S-
SVM results for CTREC04 and CTREC05 in Ta-
bles 4 and 5, respectively. The S-SVM is trained
on CTRAINDATA.
These tables show the following:
38
Table 4: Comparison of U-SVM and S-SVM on
CTREC04
FGS SGS
top 1 S-SVM 30.93% 39.18%
U-SVM 45.36% 53.61%
mrr 1 S-SVM 45.36% 53.54%
U-SVM 57.44% 66.25%
top 5 S-SVM 71.13% 79.38%
U-SVM 76.29% 88.66%
Table 5: Comparison of U-SVM and S-SVM on
CTREC05
FGS SGS
top 1 S-SVM 30.00% 33.33%
U-SVM 48.00% 50.00%
mrr 1 S-SVM 45.59% 48.67%
U-SVM 58.01% 62.38%
top 5 S-SVM 72.00% 74.67%
U-SVM 75.33% 82.67%
? The proposed U-SVM significantly outper-
forms the S-SVM for all measurements and
all test data sets. For the CTREC04 test data
set, the top1 improvements for the FGS and
SGS data are about 14.5% and 14.4%, respec-
tively. For the CTREC05 test data set, the top1
score for the FGS data increases from 30.0%
to 48.0%, and the top 1 score for the SGS data
increases from 33.3% to 50.0%. Note that the
SBFS and BMFS features here is fewer than the
features in [Ittycheriah et al 2001; Suzuki et
al. 2002], but the comparison is still effective
because the models are compared in terms of
the same features. In the S-SVM, all questions
share the same training data, while the U-SVM
uses the unique pseudo-training data for each
question. This is the main reason why the U-
SVM performs better than the S-SVM does.
? The SGS data is greatly helpful for both
the U-SVM and the S-SVM. Compared with
the FGS data, the top 1/mrr 5/top 5 im-
provements for the S-SVM and the U-SVM
on CTREC04 are 8.25%/8.18%/8.25% and
7.25%/8.81%/12.37%. The SGS can be re-
garded as a kind of query expansion. The rea-
sons for this improvement are: the data sparse-
ness in FGS data is partially resolved; and the
use of the Web to introduce data redundancy
is helpful. [Clarke et al 2001; Magnini et al
2002; and Dumais et al 2002].
In the S-SVM, all of the test questions share the
same hand-tagged training data, so the WWFS fea-
tures cannot be easily used [Ittycheriah et al 2002;
Suzuki, et al 2002]. Tables 6 and 7 compare
the performances of the U-SVM with the (SBFS +
BMFS) features, the WWFS features, and combina-
tion of three types of features for the CTREC04 and
CTREC05 test data sets, respectively.
Table 6: Performances of U-SVM for Different Fea-
tures on CTREC04
SBFS+BMFS WWFS Combination
top 1 53.61% 46.39% 60.82%
mrr 5 66.25% 59.19% 71.31%
top 5 88.66% 81.44% 88.66%
Table 7: Performances of U-SVM for Different Fea-
tures on CTREC05
SBFS+BMFS WWFS Combination
top 1 50.00% 49.33% 57.33%
mrr 5 62.38% 59.26% 65.61%
top 5 82.67% 74.00% 80.00%
These tables report that combining three types
of features can improve the performance of
the U-SVM. Using a combination of features
with the CTREC04 test data set results in the
best performances: 60.82%/71.31%/88.66% for
top 1/mrr 5/top 5. Similarly, as compared with
using the (SBFS + BMFS) and WWFS features, the
improvements from using a combination of features
with the CTREC05 test data set are 7.33%/3.23%/-
2.67% and 8.00%/6.35%/6.00%, respectively. The
results also demonstrate that the (SBFS + BMFS)
features are more important than the WWFS fea-
tures.
These comparative experiments indicate that the
U-SVM performs better than the S-SVM does, even
though the U-SVM is an unsupervised technique and
no hand-tagged training data is provided. The aver-
39
age top 1 improvements for both test data sets are
both more than 20%.
4.5 U-SVM vs. Pattern-M vs. S-SVM
To compare the U-SVM with the Pattern-M and
the S-SVM, we use the CTEST05 data set, shown
in Figure 3. The CTEST05 includes 14 different
question types, for example, Inventor Stuff (with
question like ?Who invented telephone??), Event-
Day (with question like ?when is World Day for Wa-
ter??), and so on. The Pattern-M uses the depen-
dency syntactic answer patterns learned in [Wu et
al. 2007] to extract the answer, and named entities
are also used to filter noise from the candidates.
Figure 3: Statistics of CTEST05
Table 8 summarizes the performances of the U-
SVM, Pattern-M, and S-SVM models on CTEST05.
Table 8: Comparison of U-SVM, Pattern-M and S-
SVM on CTEST05
S-SVM Pattern-M U-SVM
top 1 44.89% 53.14% 59.09%
mrr 5 56.49% 61.28% 67.34%
top 5 74.43% 73.14% 81.82%
The results in the table show that the U-SVM
significantly outperforms the S-SVM and Pattern-
M, while the S-SVM underperforms the Pattern-
M. Compared with the Pattern-M, the U-SVM in-
creases the top 1/mrr 5/top 5 scores by 5.95%/
6.06%/8.68%, respectively. The reasons may lie in
the following:
? The Chinese dependency parser influences de-
pendency syntactic answer-pattern extraction,
and thus degrades the performance of the
Pattern-M model.
? The imperfection of Google snippets affects
pattern matching, and thus adversely influences
the Pattern-M model. From the cross-model
comparison, we conclude that the performance
ranking of these models is: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
5 Conclusion and Future Work
This paper presents an unsupervised machine learn-
ing technique (called the U-SVM) for answer selec-
tion that is validated in Chinese open-domain web
QA. Regarding answer selection as a kind of classifi-
cation task, the U-SVM automatically learns clusters
and pseudo-training data for each cluster by cluster-
ing web search results. It then selects the correct
answer from the candidates according to classifying
the question. The contribution of this paper is that
it presents an unsupervised machine learning tech-
nique for web QA that starts with only a user ques-
tion. The results of our experiments with three test
data sets are encouraging. As compared with the
S-SVM, the top 1 performances of the U-SVM for
the CTREC04 and CTREC05 data sets are signifi-
cantly improved, at more than 20%. Moreover, the
U-SVM performs better than the Retrieval-M and
the Pattern-M.
These experiments have only validated the U-
SVM on named entity types of questions that ac-
count for about 82% of all TREC2004 and 2005
FACTOID test questions. In fact, our technique is
independent of question types only if the candidates
can be extracted. In the future, we will explore the
effectiveness of our technique for the other types of
questions. The web search results clustering in the
U-SVM defines that a candidate in a Google snip-
pet can represent the ?signature? of its cluster. This
definition, however, is not always effective. To fil-
ter noise in the pseudo-training data, we will extract
relations between the candidates and the keywords
as the cluster signatures of Google snippets. More-
over, applying the U-SVM to QA systems in other
languages, like English and Japanese, will also be
included in our future work.
40
References
Abdessamad Echihabi, and Daniel Marcu. 2003. A
Noisy-Channel Approach to Question Answering. In
Proc. of ACL-2003, Japan.
Abraham Ittycheriah, Salim Roukos. 2002. IBM?s Sta-
tistical Question Answering System-TREC 11. In Proc.
of TREC-11, Gaithersburg, Maryland.
Bernardo Magnini, Matteo Negri, Roberto Prevete,
Hristo Tanev. 2002. Is It the Right Answer? Exploit-
ing Web Redundancy for Answer Validation. In Proc.
of ACL-2002, Philadelphia, pp. 425 432.
Charles L. A. Clarke, Gordon V. Cormack, Thomas R.
Lynam. Exploiting Redundancy in Question Answer-
ing In Proc. of SIGIR-2001, pp 358?365, 2001.
Christopher Pinchak, Dekang Lin. 2006. A Probabilistic
Answer Type Model. In Proc. of EACL-2006, Trento,
Italy, pp. 393-400.
Dan Moldovan, Sanda Harabagiu, Roxana Girju, et al
2002. LCC Tools for Question Answering. NIST Spe-
cial Publication: SP 500-251, TREC-2002.
Deepak Ravichandran, Eduard Hovy. 2002. Learning
Surface Text Patterns for a Question Answering Sys-
tem. In Proc. of the 40th ACL, Philadelphia, July
2002.
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin. 2001. The
Use of External Knowledge of Factoid QA. In Proc.
of TREC 2001, Gaithersburg, MD, U.S.A., November
13-16, 2001.
Hui Yang, Tat-Seng Chua. 2003. QUALIFIER: Question
Answering by Lexical Fabric and External Resources.
In Proc. of EACL-2003, page 363-370.
Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia. 2001.
Question Answering Using a Large Text Database: A
Machine Learning Approach. In Proc. of EMNLP-
2001, pp66-73 (2001).
Jun Suzuki, Yutaka Sasaki, Eisaku Maeda. 2002. SVM
Answer Selection for Open-Domain Question Answer-
ing. In Proc. of Coling-2002, pp. 974 980 (2002).
Marius Pasca. 2001. A Relational and Logic Represen-
tation for Open-Domain Textual Question Answering.
In Proc. of ACL (Companion Volume) 2001: 37-42.
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use of
Patterns for Detection of Likely Answer Strings: A Sys-
tematic Approach. In Proc. of TREC-2002, Gaithers-
burg, Maryland, November 2002.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andre Ng. Web Question Answering: Is More
Always Better?. In Proc. SIGIR-2002, pp 291?298,
2002.
Xin Li, and Dan Roth. 2002. Learning Question Classi-
fication. In Proc. of the 19th International Conference
on Computational Linguistics, Taibai, 2002.
Youzheng Wu, Hideki Kashioka, Jun Zhao. 2007. Us-
ing Clustering Approaches to Open-domain Question
Answering. In Proc. of CICLING-2007, Mexico City,
Mexico, pp506 517, 2007.
Youzheng Wu, Jun Zhao and Bo Xu. 2005. Chinese
Named Entity Recognition Model Based on Multiple
Features. In Proc. of HLT/EMNLP-2005, Vancouver,
Canada, pp.427-434.
Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu.
2004. Building an Evaluation Platform for Chinese
Question Answering Systems. In Proc. of the First
NCIRCS, China, December, 2004.
Yutaka Sasaki. 2005. Question Answering as Question-
Biased Term Extraction: A New Approach toward
Multilingual QA. In Proc. of ACL-2005, pp.215-222.
41
Corpus-oriented Acquisition of Chinese Grammar
Yan Zhang 
ATR Spoken language 
Communication Research 
Laboratories
2-2-2 Keihanna Science City, 
Kyoto, 619-0288 
yan.zhang@atr.jp
Hideki Kashioka  
ATR Spoken language 
Communication Research 
Laboratories
2-2-2 Keihanna Science City, 
Kyoto, 619-0288 
Hideki.kashioka@atr.jp
Abstract
The acquisition of grammar from a 
corpus is a challenging task in the 
preparation of a knowledge bank. In 
this paper, we discuss the extraction of 
Chinese grammar oriented to a re-
stricted corpus. First, probabilistic con-
text-free grammars (PCFG) are 
extracted automatically from the Penn 
Chinese Treebank and are regarded as 
the baseline rules. Then a corpus-
oriented grammar is developed by add-
ing specific information including head 
information from the restricted corpus. 
Then, we describe the peculiarities and 
ambiguities, particularly between the 
phrases ?PP? and ?VP? in the extracted 
grammar. Finally, the parsing results of 
the utterances are used to evaluate the 
extracted grammar.  
1 Introduction 
Research and development work on spoken lan-
guage systems for special domains has been 
gaining more attention in recent years. Many 
approaches to spoken language processing re-
quire a grammar system for parsing the input 
utterances in order to obtain the structures, espe-
cially for rule-based approaches.  
Manually developing grammars based on lin-
guistics theories is a very difficult task. Lan-
guage phenomena are usually described as being 
symbolic systems such as lexical, syntactic, se-
mantic and common sense. Grammar develop-
ment has to depend on linguistic knowledge and 
the characteristics of the corpus to explicate a 
system of linguistic entities. However, it is ex-
pensive and time-consuming to maintain a ro-
bust grammar system by manual writing.  
Recently some researchers (H. Meng et al, 
2002; S. Dipper, 2004 and Y. Ding, 2004) have 
presented a methodology to semi-automatically 
capture different grammar inductions from an-
notated corpora within restricted domains. A 
corpus-oriented approach (Y. Miyao, 2004) pro-
vides a way to extract grammars automatically 
from an annotated corpus. The specific language 
knowledge and knowledge relations need to be 
constructed and oriented to different corpora and 
tasks (K. Chen, 2004).
The Chinese treebank is a useful resource for 
acquiring grammar rules and the context rela-
tions. Currently there are several Chinese tree-
banks on a scale of size. In the Penn Chinese 
Treebank (F. Xia, 2000), each structural tree is 
annotated with words, parts-of-speech and syn-
tactic structure brackets. In the Sinica Treebank 
(CKIP), thematic roles are also labeled in the 
CKIP to provide deeper information.
In this paper, we discuss the extraction of Chi-
nese grammar oriented to a restricted corpus. 
First, probabilistic context-free grammars 
(PCFG) are extracted automatically from the 
Penn Chinese Treebank and are regarded as the 
baseline rules. Then a corpus-oriented grammar 
is developed by adding specific information in-
cluding head information from the restricted 
corpus. We then describe the peculiarities and 
ambiguities, especially between the phrases 
?PP? and ?VP? in the extracted grammar. Fi-
17
nally, the parsing results of the utterances are
used to evaluate the extracted grammar.
The outline of this paper is as follows: Section 2 
gives the process of acquiring the baseline Chi-
nese grammar and the extension of the current 
grammar oriented to the corpus. Section 3 ex-
plains the grammar properties in our corpus and
our approach to disambiguating some special 
phrase rules, such as ?PP? and ?VP? and the 
word ??(ZAI)? in different categories. Section 
4 describes the evaluation results of the ex-
tracted Chinese grammar. Finally section 5 of-
fers some concluding remarks and outlines our
future work. 
2 Grammar Acquisition
There are two parts to acquiring grammar in our 
system. The baseline grammar is extracted
automatically from the Penn Chinese Treebank. 
We define a suitable parts-of-speech and phrase
categories and extend them by introducing spe-
cific information from our corpus.
2.1 Grammar Extraction from Penn Chinese
Treebank
The University of Pennsylvania (Upenn) has 
released a scale of Chinese treebanks as a kind 
of resource since 2000 (Xia Fei et al, 2000).
Each structural tree includes parts-of-speech and
syntactic structure brackets, which provides a 
good way to extract Chinese probabilistic con-
text-free grammars (PCFG). There are a total of 
325 files collected from the Xinhua newswire in
this treebank. The majority of these documents
focus on economic development and are organ-
ized in written formats as opposed to spoken 
utterances, so the grammars extracted from it are
seen as the baseline bank.
The probabilistic context-free grammars have 
proven to be very effective for parsing natural 
language. The produced rules are learnt by
matching the bracketed structures automatically
from the trees, and the rule probabilities are cal-
culated based on the maximum likelihood esti-
mation (MLE), presented in the following 
formula (Charniak, 1996):
? o
o o
k
ki
ji
ji
NC
NCNP
)(
)()( ]
]]   (1)
The baseline grammar includes about 400 PCFG 
rules after cleaning and merging some rules with 
low probabilities (Imamula et al, 2003).
2.2 Extension of the Extracted Grammar
Different corpora produce different grammars 
that have some specific information. In baseline 
grammars, many grammars are not suitable for 
spoken corpora. Therefore, we need to build an 
appropriate grammar by using specific informa-
tion in our corpus to improve the parsing results 
and machine translation systems that operates in
a restricted field. The data we used in this sys-
tem is from the ATR Basic Travel Expression 
Corpus (BTEC) in which the format of utter-
ances is different from the sentences in Upenn.
Consequently, an appropriate phrase category is
required to be constructed by analyzing the 
knowledge characteristics in BTEC. We define 
it by comparing three Chinese structure category
systems: Sinica, University of Pennsylvania, and 
HIT (Harbin Institute of Technology). A phrase 
category should be not too complicated as but 
cover language phenomenon in the corpus. Our
phrase category is defined and explained in table
1.
Categories Explanation
NNP Noun Phrase
TNP Temporal Noun Phrase 
LP Localizer Phrase
NSP Location Phrase
VP Verb Phrase
AP Adjective Phrase
DP Adverbial Phrase
QP Quantifier Phrase
PP Preposition Phrase
VBAP Phrase with ?? (BA)? 
DENP Nominal Phrase Ending 
by ?? (DE)? 
DEP Attributive Phrase formed
by ?? (DE)? 
       Table 1 Phrase Categories
In BTEC, Chinese utterances are segmented and 
labeled as parts-of-speech. We not only con-
struct corpus-oriented grammar rules differently
from the baseline grammars but also add head 
information for each rule.
In the above Table 1, the phrase category 
?VBAP? is a phrase name including the preposi-
tion ??(BA)? and its following noun or verb
phrase. The phrase ?DENP? is a special nominal
phrase which has no word after the auxiliary 
18
word ?? (DE)?, and it is usually put at the end 
of the utterance. Following are some examples 
of our grammars.  
1. PP ? p(sem"?") (head)n  
2. DENP ? (head)a y(sem"?")
3. PP ? p(sem"?") (head)r  
4. DEP ? (head)DP de
In above rules, the mark ?sem? means its fol-
lowing word is a terminal node.   
3 Grammar Annotation and Disam-
biguation
Above constructed Chinese grammars some-
times bring out conflicts when parsing utter-
ances because of the ambiguity phenomenon. 
Grammar annotation is done to make the gram-
matical relations of an utterance more explicit. 
Thus, some ideas are proposed to deal with these 
ambiguities that are tightly related to Chinese 
language.
3.1 Annotation and Analysis of Grammar 
Plenty of prepositions are rooted in verbs in 
Chinese language, and most of them still keep 
the function of verbs. This phenomenon pro-
duces ambiguous problems not only between 
categories preposition ?p? and verb ?v? but be-
tween the phrases ?VP? and ?PP? in the struc-
tures of the utterances. PP-attachment ambiguity 
is a big problem related to the construction of 
grammar (S. Zhao. 2004).  
Firstly, we extract a lexicon of Chinese preposi-
tions, which have other categories at the same 
time, such as the category ?v?, adjective ?a?, and 
so on. The following table shows the colloca-
tions of these words and their frequencies.  
Word Category Frequency  
p 226?
v 85
p 2423?
vt 4857
p 579?
a 1058
p 6422?
v 4309
p 1270?
v 1226
p 11115 
v 2381
?
d 39
 Table 2 Some Examples in the Preposition 
Lexicon
We construct some particular grammar rules for 
these preposition words showed in Table 2 in 
order to deal with the conflicts among these 
words. For example, following rules are related 
to the word ???.
PP? p(sem"?") (head)n
VP? p(sem"?") (head)V
VP? v(sem"?") NNP (head)VP 
In order to represent the function of the ex-
tracted grammar, we compare the coverage of 
the grammar in different layers between a termi-
nal node and a phrase layer. The different struc-
tural trees of the same utterance in Figure 1 are 
listed as follows.
1.Sentence (??/r ?/de ?/n ?/r ?/q ?/v1
??/n??/n?/w ) 
|__ NNP__NNP(head)__DEP__r(head) ??
|     |              |                       | ___ de ?
|     |              | _________NNP __ n(head) ?
|     | 
|     | ___ QP __ r(sem"?")
|               | ____q(head) ?
|
| __ v1(sem"?")
|
| __ NNP(head) __ NNP _____n(head) ??
|         | _________NNP(head)__n(head) ??
| __ w ?
 2. Sentence (??/r ?/de ?/n ?/r ?/q ?/v1
??/n??/n?/w ) 
|__NNP__NNP(head)*__ r ??
|     |                       | ___ de ?
|     |                       | ____  n(head) ?
|     | 
|     | ___ QP __ r(sem"?")
|                | __ q(head) ?
|
| __ v1(sem"?")
|
| __ NNP(head)** __ n ??
|           | __________ n(head) ??
| __ w ?
Figure 1 Annotation of Different trees in the 
same sentence  
The same utterance obtains different structural 
trees from different levels of grammar rules by 
parsing results, although these two trees are cor-
19
rect and acceptable. The grammar plays an im-
portant role in the machine translation system 
when we build the mapping relations with the 
goal languages by transform rules. This problem 
is also called Granularity (K. Chen, 2004). 
Symbol ?**? in Figure 1 denotes that the phrase 
?NNP? is produced by the rule ?NNP ? n 
(head)n? rather than ?NNP ? NNP (head)NNP?.  
3.2 Grammar Disambiguation 
A grammar inevitably includes ambiguities 
among its rules. To some extent, certain kinds of 
ambiguities are produced by the same ambigu-
ous problems found among part-of-speech tags. 
As with the expression in Section 2, the ambigu-
ity between the phrases ?PP? and ?VP? is partly 
produced by the multiple categories ?p? and ?v? 
of the words. This is a common case where the 
phrases ?PP? and ?VP? are nested against each 
other. For example, the rule ?PP ? p (head)v? 
and ?VP ? PP (head)VP?. This situation is de-
scribed in the following two utterances in Figure 
2.
1. Sentence (???/n ?/d ?/p ??/v ?/de 
??/n?/v?)
|__NNP__ n???
|
|__VP__d?
|      |___VP__PP*__p?
|               |        |____NNP__v??
|               |                    |____de ?
|               |                    |_____n??
|               | 
|               |___VP__v?
|
| ___ w ?
2. sentence (?/vw?/p?/r?/v?/q??/n?)
|__VP__vw?
|      | 
|      |___VP**__PP__p?
|                |          |___r?
|                | 
|                |___VP__VP__v?
|                         | 
|                         |___NNP__q?
|                                   |____n??
|
|__w?
Figure 2 The Correct Trees of Utterances In-
cluding phrases ?PP? and ?VP? 
In sentence 1 of figure 2, the phrase ?PP? (?/p
??/v ?/de ??/n) contains the verb ????,
and is produced by the rule ?DEP ? v de? and 
?NNP? DEP n? firstly. Likewise, in sentence 2, 
the phrase ?VP(????)? is produced firstly 
rather than phrase ????? is got by rule ?VP 
? PP v?. That is to say, the phrase ?VP? has 
higher priority to be produced than ?PP?.  
The Chinese word ??? is a special individual 
word in our corpus. Its correlative disambigua-
tion rules are constructed by the knowledge rela-
tions listed in the following table:
Category 
of ? ?
(ZAI)?
The order of 
rules
Ambiguity parts 
bracketed in  ut-
terance
P
(preposi-
tion)
1. VP ?
V(sem? ? ?)
(head)r
2. VP ? PP 
(head)VP
?/r ??/d [[?/p 
?? /r] [? /v ?
?/n] ]?
V (verb) 1. VP ?
V(sem? ? ?)
(head)r
2. VP D  
(head)VP
?/r [??/d ?/v
??/n] 
D (ad-
verb)
1. VP ?
D(sem? ? ?)
(head)VP
2. DP ? D 
(head)d
?/r [??/d ?/d
?/v?/u?/n] 
      Table 3 The characteristics of word ???
The following steps are used to identify the am-
biguities between the phrases ?PP? and ?VP?:  
1. The first step is to look up the preposition 
lexicon based on the category of the word and 
find the relative rules from the extracted gram-
mar.
2. When the ?PP? rules conflict with the ?VP? 
rules, we firstly consider the verb and then select 
an appropriate rule by comparing the relation-
ship to neighboring preposition words.  
3. Long distance rules have priority. For in-
stance, rule ?PP ? p v nd? is preferred to rule 
?PP? p  v?.  
4. It is clear that the fine-grained rules have less 
representational ambiguity than the coarse-
20
grained grammar rules in relation to the tree 
presentations.
5. The head information in the rules is viewed as
being types of reference knowledge because of 
their own ambiguities.
4 Evaluation for Grammar 
We use the extracted grammar described in sec-
tion 3 to parse Chinese utterances in BTEC and
to evaluate the roles of the grammar.
4.1 Parsing with Grammar
The parser adopts a bottom-up parsing algorithm
in order to obtain the phrase structures of utter-
ances. There are 200 Chinese utterances selected
in our experiment. The number of rules totals 
682 that are constructed manually except base-
line rules from Upenn Chinese treebank. Table 4 
lists the number of PCFG rules which have dif-
ferent left-side phrases.
Left-side
phrase
fre-
quency
Proportion as 
head information
AP 42 15
DENP 20 2
DEP 15 2
DP 9 5
LP 10 3
NNP 240 114
NSP 18 2
PP 39 1
QP 50 17
TNP 28 15
VBAP 7 0
VP 162 106
sentence 40 --
Table 4 The number of rules with different left-
side phrases 
In our current experiment, the evaluation is lim-
ited to obtaining several special phrase struc-
tures including ?NNP?, ?VP?, ?PP?, and 
?DENP? by using the extracted grammar. There-
fore, the parsing results are calculated using the 
precision of these phrases in the following for-
mula (2) and are listed in Table 5. We give the 
evaluation results of the word ??? separately in 
Table 6.
%100)(Pr u 
t
c
N
Nphraseec (2)
where denotes the number of correct
phrases in the parsing results, and is the total 
number of the phrases in the utterances.
cN
tN
Phrase Precision
without dis-
ambiguation
Precision
with disam-
biguation
Prec(NNP) 70.03 70.43
Prec(PP) 81.51 84.17
Prec(VP) 69.01 70.13
Prec(DENP) 82.61 82.81
      Table 5 The evaluation results
Phrase
with ???
Precision with-
out disam-
biguation
Precision
with disam-
biguation
Prec(PP) 79.12 83.67
Prec(VP) 89.34 91.72
Prec(DP) 87.71 88.02
    Table 6 The evaluation results of ???
From the evaluation results, we found that the 
precisions of the phrases ?NNP? and ?VP? were
not high due to the diversity and complexity. We
only processed the ambiguity between ?VP? and
?PP? and improve the precision of phrase ?PP?.
From the condition of the word ???, it is very
useful for the grammar extraction to construct
information on high-frequency words and word-
to-word collocation relations.
4.2 Discussion
The Chinese language is one of the most diffi-
cult languages to process. There is still no uni-
form standard for acquiring Chinese grammar
that covers all domains. Hence, a grammar
should be constructed from the view of point of
real research requirements in real corpora. It is 
the most important to maintain consistency and 
satisfy the actual requirements of a real corpus.
One of the main purposes in constructing a Chi-
nese grammar is to improve its validity and ro-
bustness to machine translation in a restricted
corpus. The development of a robust grammar
based on linguistics is difficult because of the
complexity of deep linguistic analysis. For ex-
ample, how many annotated grammars are suit-
able for the parsing system and a real machine
translation? What is the balance between the 
granularity of grammar structures and grammar
21
coverage including the ambiguities? In general, 
the coarse-grained grammar rules have a higher 
coverage rate compared with fine-grained rules, 
which contain more terminal nodes. There is 
also the major problem of determining which 
Treebank size is required to acquire the gram-
mar rules.
5 Conclusion and Future Work 
Corpus-oriented grammar extraction is con-
ducted for the purpose of constructing more ex-
plicit grammar knowledge and improving the 
machine translation system in a restricted corpus. 
Treebanks provide a useful resource for acquir-
ing grammar rules. However, it is time consum-
ing to construct a much larger size Treebank, 
which is better for grammar extraction. It would 
be better if the knowledge extraction process 
could be carried out iteratively. The parser could 
use the initial grammar to produce a large 
amount of structural trees. These new trees 
would provide more information on the gram-
mar to improve the robustness of the grammar 
and the power of the parsing system. This whole 
process can be regarded as an automatic knowl-
edge learning system.  
The principal idea in this paper was to acquire 
Chinese grammar from a restricted corpus for a 
machine translation system. The extracted 
grammar was not only from the Penn Chinese 
treebank but also from new information added to 
our experimental corpus. The corpus-oriented 
Chinese grammar was evaluated by parsing the 
phrase structures that includes ?NNP?, ?VP?, 
?PP?, ?DENP?, and the phrases relative to the 
word ???.
Currently, we only focus on a few limited 
phrases, and the disambiguation process has 
been explored with specific rules manually. 
Therefore, to improve grammar extraction in the 
future, we will aim at increasing the robustness 
and coverage of the rules and try to automati-
cally reduce the ambiguity rate by constructing 
more knowledge relations. The word-to-word 
collocation relations provided useful informa-
tion on grammar extraction for the detailed 
processing.
Acknowledgment  
This research was supported by a contract with 
the National Institute of Information and Com-
munication Technology (NICT) of Japan.  
References
Helen M. Meng and Kai-Chung Siu. 2002. Semi-
Automatic Acquisition of Domain-Specific Se-
mantic Structures, IEEE Transactions on Knowl-
edge and Data Engineering, vol 14, n 1, 
January/February, pp. 172-180 
Stefanie Dipper. Grammar Modularity and its Impact 
on Grammar Documentation. In Proceedings of 
the 20th International Conference on Computa-
tional Linguistics (COLING), pp. 1-7, Geneva, 
Switzerland, 2004  
Claire Gardent, Marilisa Amoia and Evelyne Jacquey. 
Paraphrastic Grammars. ACL Workshop on text 
meaning, Barcelona, July 2004  
Yuan Ding and Martha Palmer. Automatic Learning 
of Parallel Dependency Treelet Pairs. In  Proceed-
ings of the First International Joint Conference on 
Natural Language Processing (IJCNLP2004). 
March, Sanya, pp. 30-37, 2004 
Shaojun Zhao and Dekang Lin. A nearest-neighbor 
method for resolving pp-attachment ambiguity. In  
Proceedings of the First International Joint Con-
ference on Natural Language Processing 
(IJCNLP2004). March, Sanya, pp. 428-434, 2004 
Kenji Imamura, Eiichiro Sumita and Yuji Matsumoto. 
2003. Feedback Cleaning of Machine Translation 
Rules Using Automatic Evaluation. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL 2003), pp. 
447-454.  
Keh-Jian Chen and Yu-Ming Hsieh. Chinese Tree-
bank and Grammar Extraction. In  Proceedings of 
the First International Joint Conference on Natural 
Language Processing (IJCNLP2004). March, 
Sanya, pp. 560-565, 2004 
CKIP (Chinese Knowledge Information Processing). 
1993. The Categorical Analysis of Chinese. [In 
Chinese]. CKIP Technical Report 93-05. 
Nankang: Academic Sinica.  
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen 
Okurowski, John Kovarik, Fudong Chiou, Shizhe 
Huang, Tony Kroch, and Mitch Marcus. 2000. 
Developing Guidelines and Ensuring Consistency 
for Chinese Text Annotation. Proceeding of the 
second International Conference on Language Re-
22
sources and Evaluation (LREC-2000), Athens, 
Greece.  
Rashmi Prasad, Elini Miltsahaki, Aravind Joshi and 
Bonnie Webber. Annotation and Data Mining of 
the Penn Discourse Treebank. In Proceedings of 
the ACL 2004 Workshop on Discourse Annotation, 
Barcelona. 2004. 
Yusuke Miyao, Takashi Ninomiya and Jun?ichi Tsu-
jii. Corpus-oriented Grammar Development for 
Acquiring a Head-driven Phrase Structure Gram-
mar from the Penn Treebank. In Proceedings of 
the First International Joint Conference on Natural 
Language Processing (IJCNLP2004). March, 
Sanya, pp. 390-397, 2004 
E. Charniak. 1996. Treebank Grammars. Technical 
Report CS-96-02, Department of Computer Sci-
ence, Brown University. 
23
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 169?176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Dependency Parsing of Japanese Spoken Monologue
Based on Clause Boundaries
Tomohiro Ohno?a) Shigeki Matsubara? Hideki Kashioka?
Takehiko Maruyama] and Yasuyoshi Inagaki\
?Graduate School of Information Science, Nagoya University, Japan
?Information Technology Center, Nagoya University, Japan
?ATR Spoken Language Communication Research Laboratories, Japan
]The National Institute for Japanese Language, Japan
\Faculty of Information Science and Technology, Aichi Prefectural University, Japan
a)ohno@el.itc.nagoya-u.ac.jp
Abstract
Spoken monologues feature greater sen-
tence length and structural complexity
than do spoken dialogues. To achieve high
parsing performance for spoken mono-
logues, it could prove effective to sim-
plify the structure by dividing a sentence
into suitable language units. This paper
proposes a method for dependency pars-
ing of Japanese monologues based on sen-
tence segmentation. In this method, the
dependency parsing is executed in two
stages: at the clause level and the sen-
tence level. First, the dependencies within
a clause are identified by dividing a sen-
tence into clauses and executing stochastic
dependency parsing for each clause. Next,
the dependencies over clause boundaries
are identified stochastically, and the de-
pendency structure of the entire sentence
is thus completed. An experiment using
a spoken monologue corpus shows this
method to be effective for efficient depen-
dency parsing of Japanese monologue sen-
tences.
1 Introduction
Recently, monologue data such as a lecture and
commentary by a professional have been consid-
ered as human valuable intellectual property and
have gathered attention. In applications, such as
automatic summarization, machine translation and
so on, for using these monologue data as intel-
lectual property effectively and efficiently, it is
necessary not only just to accumulate but also to
structure the monologue data. However, few at-
tempts have been made to parse spoken mono-
logues. Spontaneously spoken monologues in-
clude a lot of grammatically ill-formed linguistic
phenomena such as fillers, hesitations and self-
repairs. In order to robustly deal with their extra-
grammaticality, some techniques for parsing of di-
alogue sentences have been proposed (Core and
Schubert, 1999; Delmonte, 2003; Ohno et al,
2005b). On the other hand, monologues also have
the characteristic feature that a sentence is gen-
erally longer and structurally more complicated
than a sentence in dialogues which have been dealt
with by the previous researches. Therefore, for
a monologue sentence the parsing time would in-
crease and the parsing accuracy would decrease. It
is thought that more effective, high-performance
spoken monologue parsing could be achieved by
dividing a sentence into suitable language units for
simplicity.
This paper proposes a method for dependency
parsing of monologue sentences based on sen-
tence segmentation. The method executes depen-
dency parsing in two stages: at the clause level
and at the sentence level. First, a dependency rela-
tion from one bunsetsu1 to another within a clause
is identified by dividing a sentence into clauses
based on clause boundary detection and then ex-
ecuting stochastic dependency parsing for each
clause. Next, the dependency structure of the en-
tire sentence is completed by identifying the de-
pendencies over clause boundaries stochastically.
An experiment on monologue dependency pars-
ing showed that the parsing time can be drasti-
1A bunsetsu is the linguistic unit in Japanese that roughly
corresponds to a basic phrase in English. A bunsetsu con-
sists of one independent word and more than zero ancillary
words. A dependency is a modification relation in which a
dependent bunsetsu depends on a head bunsetsu. That is, the
dependent bunsetsu and the head bunsetsu work as modifier
and modifyee, respectively.
169
??
???
??
???
??? ????
???
??
????
?????
?
???
????
???
?
????
????
??
?Dependency relation whose dependent bunsetsu is not the last bunsetsu of a clause
?Dependency relation whose dependent bunsetsu is the last bunsetsu of a clause
?Bunsetsu
?Clause boundary
?Clause
The public opinion poll that the Prime Minister?s Office announced the other day indicates that 
the ratio of people advocating capital punishment is nearly 80%
the other
day
that the 
Prime
Minister?s
Office
announced The 
public
opinion
poll
indicates
that
capital
punishment
advocating the ratio 
of people
nearly
80%
is
Figure 1: Relation between clause boundary and
dependency structure
cally shortened and the parsing accuracy can be
increased.
This paper is organized as follows: The next
section describes a parsing unit of Japanese mono-
logue. Section 3 presents dependency parsing
based on clause boundaries. The parsing experi-
ment and the discussion are reported in Sections
4 and 5, respectively. The related works are de-
scribed in Section 6.
2 Parsing Unit of Japanese Monologues
Our method achieves an efficient parsing by adopt-
ing a shorter unit than a sentence as a parsing unit.
Since the search range of a dependency relation
can be narrowed by dividing a long monologue
sentence into small units, we can expect the pars-
ing time to be shortened.
2.1 Clauses and Dependencies
In Japanese, a clause basically contains one verb
phrase. Therefore, a complex sentence or a com-
pound sentence contains one or more clauses.
Moreover, since a clause constitutes a syntacti-
cally sufficient and semantically meaningful lan-
guage unit, it can be used as an alternative parsing
unit to a sentence.
Our proposed method assumes that a sentence
is a sequence of one or more clauses, and every
bunsetsu in a clause, except the final bunsetsu,
depends on another bunsetsu in the same clause.
As an example, the dependency structure of the
Japanese sentence:
????????????????????
?????????????????????
?????????????The public opinion
poll that the Prime Minister?s Office announced
the other day indicates that the ratio of people
advocating capital punishment is nearly 80%)
is presented in Fig. 1. This sentence consists of
four clauses:
? ?????????????? (that the
Prime Minister?s Office announced the other
day)
? ?????????? (The public opinion
poll indicates that)
? ?????????? (advocating capital
punishment)
? ???????????????????
(the ratio of people is nearly 80%)
Each clause forms a dependency structure (solid
arrows in Fig. 1), and a dependency relation from
the final bunsetsu links the clause with another
clause (dotted arrows in Fig. 1).
2.2 Clause Boundary Unit
In adopting a clause as an alternative parsing unit,
it is necessary to divide a monologue sentence
into clauses as the preprocessing for the follow-
ing dependency parsing. However, since some
kinds of clauses are embedded in main clauses,
it is fundamentally difficult to divide a mono-
logue into clauses in one dimension (Kashioka and
Maruyama, 2004).
Therefore, by using a clause boundary anno-
tation program (Maruyama et al, 2004), we ap-
proximately achieve the clause segmentation of
a monologue sentence. This program can iden-
tify units corresponding to clauses by detecting
the end boundaries of clauses. Furthermore, the
program can specify the positions and types of
clause boundaries simply from a local morpho-
logical analysis. That is, for a sentence mor-
phologically analyzed by ChaSen (Matsumoto et
al., 1999), the positions of clause boundaries are
identified and clause boundary labels are inserted
there. There exist 147 labels such as ?compound
clause? and ?adnominal clause.? 2
In our research, we adopt the unit sandwiched
between two clause boundaries detected by clause
boundary analysis, were called the clause bound-
ary unit, as an alternative parsing unit. Here, we
regard the label name provided for the end bound-
ary of a clause boundary unit as that unit?s type.
2The labels include a few other constituents that do not
strictly represent clause boundaries but can be regarded as be-
ing syntactically independent elements, such as ?topicalized
element,? ?conjunctives,? ?interjections,? and so on.
170
Table 1: 200 sentences in ?Asu-Wo-Yomu?
sentences 200
clause boundary units 951
bunsetsus 2,430
morphemes 6,017
dependencies over clause boundaries 94
2.3 Relation between Clause Boundary Units
and Dependency Structures
To clarify the relation between clause boundary
units and dependency structures, we investigated
the monologue corpus ?Asu-Wo-Yomu 3.? In the
investigation, we used 200 sentences for which
morphological analysis, bunsetsu segmentation,
clause boundary analysis, and dependency pars-
ing were automatically performed and then modi-
fied by hand. Here, the specification of the parts-
of-speech is in accordance with that of the IPA
parts-of-speech used in the ChaSen morphologi-
cal analyzer (Matsumoto et al, 1999), the rules
of the bunsetsu segmentation with those of CSJ
(Maekawa et al, 2000), the rules of the clause
boundary analysis with those of Maruyama et
al. (Maruyama et al, 2004), and the dependency
grammar with that of the Kyoto Corpus (Kuro-
hashi and Nagao, 1997).
Table 1 shows the results of analyzing the 200
sentences. Among the 1,479 bunsetsus in the dif-
ference set between all bunsetsus (2,430) and the
final bunsetsus (951) of clause boundary units,
only 94 bunsetsus depend on a bunsetsu located
outside the clause boundary unit. This result
means that 93.6% (1,385/1,479) of all dependency
relations are within a clause boundary unit. There-
fore, the results confirmed that the assumption
made by our research is valid to some extent.
3 Dependency Parsing Based on Clause
Boundaries
In accordance with the assumption described in
Section 2, in our method, the transcribed sentence
on which morphological analysis, clause bound-
ary detection, and bunsetsu segmentation are per-
formed is considered the input 4. The dependency
3Asu-Wo-Yomu is a collection of transcriptions of a TV
commentary program of the Japan Broadcasting Corporation
(NHK). The commentator speaks on some current social is-
sue for 10 minutes.
4It is difficult to preliminarily divide a monologue into
sentences because there are no clear sentence breaks in mono-
logues. However, since some methods for detecting sentence
boundaries have already been proposed (Huang and Zweig,
2002; Shitaoka et al, 2004), we assume that they can be de-
tected automatically before dependency parsing.
parsing is executed based on the following proce-
dures:
1. Clause-level parsing: The internal depen-
dency relations of clause boundary units are
identified for every clause boundary unit in
one sentence.
2. Sentence-level parsing: The dependency
relations in which the dependent unit is the fi-
nal bunsetsu of the clause boundary units are
identified.
In this paper, we describe a sequence of clause
boundary units in a sentence as C1 ? ? ?Cm, a se-
quence of bunsetsus in a clause boundary unit Ci
as bi1 ? ? ? bini , a dependency relation in which the
dependent bunsetsu is a bunsetsu bik as dep(bik),
and a dependency structure of a sentence as
{dep(b11), ? ? ? , dep(bmnm?1)}.
First, our method parses the dependency struc-
ture {dep(bi1), ? ? ? , dep(bini?1)} within the clause
boundary unit whenever a clause boundary unit
Ci is inputted. Then, it parses the dependency
structure {dep(b1n1), ? ? ? , dep(bm?1nm?1)}, which is a
set of dependency relations whose dependent bun-
setsu is the final bunsetsu of each clause boundary
unit in the input sentence. In addition, in both of
the above procedures, our method assumes the fol-
lowing three syntactic constraints:
1. No dependency is directed from right to left.
2. Dependencies don?t cross each other.
3. Each bunsetsu, except the final one in a sen-
tence, depends on only one bunsetsu.
These constraints are usually used for Japanese de-
pendency parsing.
3.1 Clause-level Dependency Parsing
Dependency parsing within a clause boundary
unit, when the sequence of bunsetsus in an input
clause boundary unit Ci is described as Bi (=
bi1 ? ? ? bini), identifies the dependency structure
Si (= {dep(bi1), ? ? ? , dep(bini?1)}), which max-
imizes the conditional probability P (Si|Bi). At
this level, the head bunsetsu of the final bunsetsu
bini of a clause boundary unit is not identified.
Assuming that each dependency is independent
of the others, P (Si|Bi) can be calculated as fol-
lows:
P (Si|Bi) =
ni?1?
k=1
P (bik rel? bil|Bi), (1)
171
where P (bik
rel? bil|Bi) is the probability that a bun-
setsu bik depends on a bunsetsu bil when the se-
quence of bunsetsus Bi is provided. Unlike the
conventional stochastic sentence-by-sentence de-
pendency parsing method, in our method, Bi is
the sequence of bunsetsus that constitutes not a
sentence but a clause. The structure Si, which
maximizes the conditional probability P (Si|Bi),
is regarded as the dependency structure of Bi and
calculated by dynamic programming (DP).
Next, we explain the calculation of P (bik
rel?
bil|Bi). First, the basic form of independent words
in a dependent bunsetsu is represented by hik, its
parts-of-speech tik, and type of dependency rik,
while the basic form of the independent word in
a head bunsetsu is represented by hil , and its parts-
of-speech til . Furthermore, the distance between
bunsetsus is described as diikl. Here, if a dependent
bunsetsu has one or more ancillary words, the type
of dependency is the lexicon, part-of-speech and
conjugated form of the rightmost ancillary word,
and if not so, it is the part-of-speech and conju-
gated form of the rightmost morpheme. The type
of dependency rik is the same attribute used in
our stochastic method proposed for robust depen-
dency parsing of spoken language dialogue (Ohno
et al, 2005b). Then diikl takes 1 or more than 1,
that is, a binary value. Incidentally, the above
attributes are the same as those used by the con-
ventional stochastic dependency parsing methods
(Collins, 1996; Ratnaparkhi, 1997; Fujio and Mat-
sumoto, 1998; Uchimoto et al, 1999; Charniak,
2000; Kudo and Matsumoto, 2002).
Additionally, we prepared the attribute eil to in-
dicate whether bil is the final bunsetsu of a clause
boundary unit. Since we can consider a clause
boundary unit as a unit corresponding to a sim-
ple sentence, we can treat the final bunsetsu of a
clause boundary unit as a sentence-end bunsetsu.
The attribute that indicates whether a head bun-
setsu is a sentence-end bunsetsu has often been
used in conventional sentence-by-sentence parsing
methods (e.g. Uchimoto et al, 1999).
By using the above attributes, the conditional
probability P (bik
rel? bil|Bi) is calculated as fol-
lows:
P (bik rel? bil|Bi) (2)
?= P (bik rel? bil|hik, hil, tik, til, rik, diikl, eil)
= F (b
i
k
rel? bil, hik, hil, tik, til, rik, diikl, eil)
F (hik, hil, tik, til, rik, diikl, eil)
.
Note that F is a co-occurrence frequency function.
In order to resolve the sparse data problems
caused by estimating P (bik
rel? bil|Bi) with formula
(2), we adopted the smoothing method described
by Fujio and Matsumoto (Fujio and Matsumoto,
1998): if F (hik, hil, tik, til, rik, diikl, eil) in formula (2)
is 0, we estimate P (bik
rel? bil|Bi) by using formula
(3).
P (bik rel? bil|Bi) (3)
?= P (bik rel? bil|tik, til, rik, diikl, eil)
= F (b
i
k
rel? bil, tik, til, rik, diikl, eil)
F (tik, til, rik, diikl, eil)
3.2 Sentence-level Dependency Parsing
Here, the head bunsetsu of the final bunsetsu
of a clause boundary unit is identified. Let
B (=B1 ? ? ?Bn) be the sequence of bunset-
sus of one sentence and Sfin be a set of de-
pendency relations whose dependent bunsetsu is
the final bunsetsu of a clause boundary unit,
{dep(b1n1), ? ? ? , dep(bm?1nm?1)}; then Sfin, which
makes P (Sfin|B) the maximum, is calculated by
DP. The P (Sfin|B) can be calculated as follows:
P (Sfin|B) =
m?1?
i=1
P (bini
rel? bjl |B), (4)
where P (bini
rel? bjl |B) is the probability that a
bunsetsu bini depends on a bunsetsu bjl when the
sequence of the sentence?s bunsetsus, B, is pro-
vided. Our method parses by giving consideration
to the dependency structures in each clause bound-
ary unit, which were previously parsed. That is,
the method does not consider all bunsetsus lo-
cated on the right-hand side as candidates for a
head bunsetsu but calculates only dependency re-
lations within each clause boundary unit that do
not cross any other relation in previously parsed
dependency structures. In the case of Fig. 1,
the method calculates by assuming that only three
bunsetsus ??? (the ratio of people),? or ???
????? (is)? can be the head bunsetsu of the
bunsetsu ???????? (advocating).?
In addition, P (bini
rel? bjl |B) is calculated as in
Eq. (5). Equation (5) uses all of the attributes used
in Eq. (2), in addition to the attribute sjl , which
indicates whether the head bunsetsu of bjl is the
final bunsetsu of a sentence. Here, we take into
172
Table 2: Size of experimental data set (Asu-Wo-
Yomu)
test data learning data
programs 8 95
sentences 500 5,532
clause boundary units 2,237 26,318
bunsetsus 5,298 65,821
morphemes 13,342 165,129
Note that the commentator of each program is different.
Table 3: Experimental results on parsing time
our method conv. method
average time (msec) 10.9 51.9
programming language: LISP
computer used: Pentium4 2.4 GHz, Linux
account the analysis result that about 70% of the
final bunsetsus of clause boundary units depend on
the final bunsetsu of other clause boundary units 5
and also use the attribute ejl at this phase.
P (bini
rel? bjl |B) (5)
?= P (bini
rel?bjl |hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
= F (b
ini
rel?bjl , hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
F (hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
4 Parsing Experiment
To evaluate the effectiveness of our method for
Japanese spoken monologue, we conducted an ex-
periment on dependency parsing.
4.1 Outline of Experiment
We used the spoken monologue corpus? Asu-
Wo-Yomu,?annotated with information on mor-
phological analysis, clause boundary detection,
bunsetsu segmentation, and dependency analy-
sis6. Table 2 shows the data used for the ex-
periment. We used 500 sentences as the test
data. Although our method assumes that a depen-
dency relation does not cross clause boundaries,
there were 152 dependency relations that contra-
dicted this assumption. This means that the depen-
dency accuracy of our method is not over 96.8%
(4,646/4,798). On the other hand, we used 5,532
sentences as the learning data.
To carry out comparative evaluation of our
method?s effectiveness, we executed parsing for
5We analyzed the 200 sentences described in Section 2.3
and confirmed 70.6% (522/751) of the final bunsetsus of
clause boundary units depended on the final bunsetsu of other
clause boundary units.
6Here, the specifications of these annotations are in accor-
dance with those described in Section 2.3.
0
50
100
150
200
250
300
350
400
0 5 10 15 20 25 30
Pa
rs
in
g 
tim
e 
[m
se
c]
Length of sentence [number of bunsetsu]
our method
conv. method
Figure 2: Relation between sentence length and
parsing time
the above-mentioned data by the following two
methods and obtained, respectively, the parsing
time and parsing accuracy.
? Our method: First, our method provides
clause boundaries for a sequence of bunset-
sus of an input sentence and identifies all
clause boundary units in a sentence by per-
forming clause boundary analysis (CBAP)
(Maruyama et al, 2004). After that, our
method executes the dependency parsing de-
scribed in Section 3.
? Conventional method: This method parses
a sentence at one time without dividing it into
clause boundary units. Here, the probability
that a bunsetsu depends on another bunsetsu,
when the sequence of bunsetsus of a sentence
is provided, is calculated as in Eq. (5), where
the attribute e was eliminated. This conven-
tional method has been implemented by us
based on the previous research (Fujio and
Matsumoto, 1998).
4.2 Experimental Results
The parsing times of both methods are shown in
Table 3. The parsing speed of our method im-
proves by about 5 times on average in comparison
with the conventional method. Here, the parsing
time of our method includes the time taken not
only for the dependency parsing but also for the
clause boundary analysis. The average time taken
for clause boundary analysis was about 1.2 mil-
lisecond per sentence. Therefore, the time cost of
performing clause boundary analysis as a prepro-
cessing of dependency parsing can be considered
small enough to disregard. Figure 2 shows the re-
lation between sentence length and parsing time
173
Table 4: Experimental results on parsing accuracy
our method conv. method
bunsetsu within a clause boundary unit (except final bunsetsu) 88.2% (2,701/3,061) 84.7% (2,592/3,061)
final bunsetsu of a clause boundary unit 65.6% (1,140/1,737) 63.3% (1,100/1,737)
total 80.1% (3,841/4,798) 76.9% (3,692/4,798)
Table 5: Experimental results on clause boundary
analysis (CBAP)
recall 95.7% (2,140/2,237)
precision 96.9% (2,140/2,209)
for both methods, and it is clear from this figure
that the parsing time of the conventional method
begins to rapidly increase when the length of a
sentence becomes 12 or more bunsetsus. In con-
trast, our method changes little in relation to pars-
ing time. Here, since the sentences used in the
experiment are composed of 11.8 bunsetsus on av-
erage, this result shows that our method is suitable
for improving the parsing time of a monologue
sentence whose length is longer than the average.
Table 4 shows the parsing accuracy of both
methods. The first line of Table 4 shows the
parsing accuracy for all bunsetsus within clause
boundary units except the final bunsetsus of the
clause boundary units. The second line shows
the parsing accuracy for the final bunsetsus of
all clause boundary units except the sentence-end
bunsetsus. We confirmed that our method could
analyze with a higher accuracy than the conven-
tional method. Here, Table 5 shows the accu-
racy of the clause boundary analysis executed by
CBAP. Since the precision and recall is high, we
can assume that the clause boundary analysis ex-
erts almost no harmful influence on the following
dependency parsing.
As mentioned above, it is clear that our method
is more effective than the conventional method in
shortening parsing time and increasing parsing ac-
curacy.
5 Discussions
Our method assumes that dependency relations
within a clause boundary unit do not cross clause
boundaries. Due to this assumption, the method
cannot correctly parse the dependency relations
over clause boundaries. However, the experi-
mental results indicated that the accuracy of our
method was higher than that of the conventional
method.
In this section, we first discuss the effect of our
method on parsing accuracy, separately for bun-
Table 6: Comparison of parsing accuracy between
conventional method and our method (for bunsetsu
within a clause boundary unit except final bun-
setsu)``````````conv. method
our method
correct incorrect total
correct 2,499 93 2,592
incorrect 202 267 469
total 2,701 360 3,061
setsus within clause boundary units (except the fi-
nal bunsetsus) and the final bunsetsus of clause
boundary units. Next, we discuss the problem of
our method?s inability to parse dependency rela-
tions over clause boundaries.
5.1 Parsing Accuracy for Bunsetsu within a
Clause Boundary Unit (except final
bunsetsu)
Table 6 compares parsing accuracies for bunsetsus
within clause boundary units (except the final bun-
setsus) between the conventional method and our
method. There are 3,061 bunsetsus within clause
boundary units except the final bunsetsu, among
which 2,499 were correctly parsed by both meth-
ods. There were 202 dependency relations cor-
rectly parsed by our method but incorrectly parsed
by the conventional method. This means that our
method can narrow down the candidates for a head
bunsetsu.
In contrast, 93 dependency relations were cor-
rectly parsed solely by the conventional method.
Among these, 46 were dependency relations over
clause boundaries, which cannot in principle be
parsed by our method. This means that our method
can correctly parse almost all of the dependency
relations that the conventional method can cor-
rectly parse except for dependency relations over
clause boundaries.
5.2 Parsing Accuracy for Final Bunsetsu of a
Clause Boundary Unit
We can see from Table 4 that the parsing accuracy
for the final bunsetsus of clause boundary units by
both methods is much worse than that for bunset-
sus within the clause boundary units (except the
final bunsetsus). This means that it is difficult
174
Table 7: Comparison of parsing accuracy between
conventional method and our method (for final
bunsetsu of a clause boundary unit)``````````conv. method
our method
correct incorrect total
correct 1037 63 1,100
incorrect 103 534 637
total 1,140 597 1,737
Table 8: Parsing accuracy for dependency rela-
tions over clause boundaries
our method conv. method
recall 1.3% (2/152) 30.3% (46/152)
precision 11.8% (2/ 17) 25.3% (46/182)
to identify dependency relations whose dependent
bunsetsu is the final one of a clause boundary unit.
Table 7 compares how the two methods parse
the dependency relations when the dependent bun-
setsu is the final bunsetsu of a clause bound-
ary unit. There are 1,737 dependency relations
whose dependent bunsetsu is the final bunsetsu of
a clause boundary unit, among which 1,037 were
correctly parsed by both methods. The number
of dependency relations correctly parsed only by
our method was 103. This number is higher than
that of dependency relations correctly parsed by
only the conventional method. This result might
be attributed to our method?s effect; that is, our
method narrows down the candidates internally for
a head bunsetsu based on the first-parsed depen-
dency structure for clause boundary units.
5.3 Dependency Relations over Clause
Boundaries
Table 8 shows the accuracy of both methods for
parsing dependency relations over clause bound-
aries. Since our method parses based on the as-
sumption that those dependency relations do not
exist, it cannot correctly parse anything. Al-
though, from the experimental results, our method
could identify two dependency relations over
clause boundaries, these were identified only be-
cause dependency parsing for some sentences was
performed based on wrong clause boundaries that
were provided by clause boundary analysis. On
the other hand, the conventional method correctly
parsed 46 dependency relations among 152 that
crossed a clause boundary in the test data. Since
the conventional method could correctly parse
only 30.3% of those dependency relations, we can
see that it is in principle difficult to identify the
dependency relations.
6 Related Works
Since monologue sentences tend to be long and
have complex structures, it is important to con-
sider the features. Although there have been
very few studies on parsing monologue sentences,
some studies on parsing written language have
dealt with long-sentence parsing. To resolve the
syntactic ambiguity of a long sentence, some of
them have focused attention on the ?clause.?
First, there are the studies that focused atten-
tion on compound clauses (Agarwal and Boggess,
1992; Kurohashi and Nagao, 1994). These tried
to improve the parsing accuracy of long sentences
by identifying the boundaries of coordinate struc-
tures. Next, other research efforts utilized the three
categories into which various types of subordinate
clauses are hierarchically classified based on the
?scope-embedding preference? of Japanese subor-
dinate clauses (Shirai et al, 1995; Utsuro et al,
2000). Furthermore, Kim et al (Kim and Lee,
2004) divided a sentence into ?S(ubject)-clauses,?
which were defined as a group of words containing
several predicates and their common subject. The
above studies have attempted to reduce the pars-
ing ambiguity between specific types of clauses in
order to improve the parsing accuracy of an entire
sentence.
On the other hand, our method utilizes all types
of clauses without limiting them to specific types
of clauses. To improve the accuracy of long-
sentence parsing, we thought that it would be more
effective to cyclopaedically divide a sentence into
all types of clauses and then parse the local de-
pendency structure of each clause. Moreover,
since our method can perform dependency pars-
ing clause-by-clause, we can reasonably expect
our method to be applicable to incremental pars-
ing (Ohno et al, 2005a).
7 Conclusions
In this paper, we proposed a technique for de-
pendency parsing of monologue sentences based
on clause-boundary detection. The method can
achieve more effective, high-performance spoken
monologue parsing by dividing a sentence into
clauses, which are considered as suitable language
units for simplicity. To evaluate the effectiveness
of our method for Japanese spoken monologue, we
conducted an experiment on dependency parsing
of the spoken monologue sentences recorded in
the ?Asu-Wo-Yomu.? From the experimental re-
175
sults, we confirmed that our method shortened the
parsing time and increased the parsing accuracy
compared with the conventional method, which
parses a sentence without dividing it into clauses.
Future research will include making a thorough
investigation into the relation between dependency
type and the type of clause boundary unit. After
that, we plan to investigate techniques for identi-
fying the dependency relations over clause bound-
aries. Furthermore, as the experiment described in
this paper has shown the effectiveness of our tech-
nique for dependency parsing of long sentences
in spoken monologues, so our technique can be
expected to be effective in written language also.
Therefore, we want to examine the effectiveness
by conducting the parsing experiment of long sen-
tences in written language such as newspaper arti-
cles.
8 Acknowledgements
This research was supported in part by a contract
with the Strategic Information and Communica-
tions R&D Promotion Programme, Ministry of In-
ternal Affairs and Communications and the Grand-
in-Aid for Young Scientists of JSPS. The first au-
thor is partially supported by JSPS Research Fel-
lowships for Young Scientists.
References
R. Agarwal and L. Boggess. 1992. A simple but use-
ful approach to conjunct indentification. In Proc. of
30th ACL, pages 15?21.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of 1st NAACL, pages 132?139.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. of 34th ACL,
pages 184?191.
Mark G. Core and Lenhart K. Schubert. 1999. A syn-
tactic framework for speech repairs and other dis-
ruptions. In Proc. of 37th ACL, pages 413?420.
R. Delmonte. 2003. Parsing spontaneous speech. In
Proc. of 8th EUROSPEECH, pages 1999?2004.
M. Fujio and Y. Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statis-
tics. In Proc. of 3rd EMNLP, pages 87?96.
J. Huang and G. Zweig. 2002. Maximum entropy
model for punctuation annotation from speech. In
Proc. of 7th ICSLP, pages 917?920.
H. Kashioka and T. Maruyama. 2004. Segmentation
of semantic unit in Japanese monologue. In Proc. of
ICSLT-O-COCOSDA 2004, pages 87?92.
M. Kim and J. Lee. 2004. Syntactic analysis of long
sentences based on s-clauses. In Proc. of 1st IJC-
NLP, pages 420?427.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc.
of 6th CoNLL, pages 63?69.
S. Kurohashi and M. Nagao. 1994. A syntactic analy-
sis method of long Japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
S. Kurohashi and M. Nagao. 1997. Building a
Japanese parsed corpus while improving the parsing
system. In Proc. of 4th NLPRS, pages 451?456.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000.
Spontaneous speech corpus of Japanese. In Proc. of
2nd LREC, pages 947?952.
T. Maruyama, H. Kashioka, T. Kumano, and
H. Tanaka. 2004. Development and evaluation
of Japanese clause boundaries annotation program.
Journal of Natural Language Processing, 11(3):39?
68. (In Japanese).
Y. Matsumoto, A. Kitauchi, T. Yamashita, and Y. Hi-
rano, 1999. Japanese Morphological Analysis Sys-
tem ChaSen version 2.0 Manual. NAIST Technical
Report, NAIST-IS-TR99009.
T. Ohno, S. Matsubara, H. Kashioka, N. Kato, and
Y. Inagaki. 2005a. Incremental dependency pars-
ing of Japanese spoken monologue based on clause
boundaries. In Proc. of 9th EUROSPEECH, pages
3449?3452.
T. Ohno, S. Matsubara, N. Kawaguchi, and Y. Inagaki.
2005b. Robust dependency parsing of spontaneous
Japanese spoken language. IEICE Transactions on
Information and Systems, E88-D(3):545?552.
A. Ratnaparkhi. 1997. A liner observed time statistical
parser based on maximum entropy models. In Proc.
of 2nd EMNLP, pages 1?10.
S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995.
A new dependency analysis method based on se-
mantically embedded sentence structures and its per-
formance on Japanese subordinate clause. Jour-
nal of Information Processing Society of Japan,
36(10):2353?2361. (In Japanese).
K. Shitaoka, K. Uchimoto, T. Kawahara, and H. Isa-
hara. 2004. Dependency structure analysis and sen-
tence boundary detection in spontaneous Japanese.
In Proc. of 20th COLING, pages 1107?1113.
K. Uchimoto, S. Sekine, and K. Isahara. 1999.
Japanese dependency structure analysis based on
maximum entropy models. In Proc. of 9th EACL,
pages 196?203.
T. Utsuro, S. Nishiokayama, M. Fujio, and Y. Mat-
sumoto. 2000. Analyzing dependencies of Japanese
subordinate clauses based on statistics of scope em-
bedding preference. In Proc. of 6th ANLP, pages
110?117.
176
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 19?24,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Training Data Modification for SMT  
Considering Groups of Synonymous Sentences 
 
Hideki KASHIOKA 
Spoken Language Communication Research Laboratories, ATR  
2-2-2 Hikaridai, Keihanna Science City 
Kyoto, 619-0288, Japan 
hideki.kashioka@atr.jp  
 
Abstract 
Generally speaking, statistical machine 
translation systems would be able to attain 
better performance with more training sets. 
Unfortunately, well-organized training sets 
are rarely available in the real world. Con-
sequently, it is necessary to focus on modi-
fying the training set to obtain high 
accuracy for an SMT system. If the SMT 
system trained the translation model, the 
translation pair would have a low probabil-
ity when there are many variations for tar-
get sentences from a single source sentence. 
If we decreased the number of variations 
for the translation pair, we could construct 
a superior translation model. This paper de-
scribes the effects of modification on the 
training corpus when consideration is given 
to synonymous sentence groups. We at-
tempt three types of modification: com-
pression of the training set, replacement of 
source and target sentences with a selected 
sentence from the synonymous sentence 
group, and replacement of the sentence on 
only one side with the selected sentence 
from the synonymous sentence group. As a 
result, we achieve improved performance 
with the replacement of source-side sen-
tences. 
1 Introduction 
Recently, many researchers have focused their in-
terest on statistical machine translation (SMT) sys-
tems, with particular attention given to models and 
decoding algorithms. The quantity of the training 
corpus has received less attention, although of 
course the earlier reports do address the quantity 
issue. In most cases, the larger the training corpus 
becomes, the higher accuracy is achieved. Usually, 
the quantity problem of the training corpus is dis-
cussed in relation to the size of the training corpus 
and system performance; therefore, researchers 
study line graphs that indicate the relationship be-
tween accuracy and training corpus size.  
On the other hand, needless to say, a single sen-
tence in the source language can be used to trans-
late several sentences in the target language. Such 
various possibilities for translation make MT sys-
tem development and evaluation very difficult. 
Consequently, here we employ multiple references 
to evaluate MT systems like BLEU (Papineni et al, 
2002) and NIST (Doddington, 2002). Moreover, 
such variations in translation have a negative effect 
on training in SMT because when several sen-
tences of input-side language are translated into the 
exactly equivalent output-side sentences, the prob-
ability of correct translation decreases due to the 
large number of possible pairs of expressions. 
Therefore, if we can restrain or modify the training 
corpus, the SMT system might achieve high accu-
racy. 
 As an example of modification, different out-
put-side sentences paired with the exactly equiva-
lent input-side sentences are replaced with one 
target sentence. These sentence replacements are 
required for synonymous sentence sets. Kashioka 
(2004) discussed synonymous sets of sentences. 
Here, we employ a method to group them as a way 
of modifying the training corpus for use with SMT. 
This paper focuses on how to control the corpus 
while giving consideration to synonymous sen-
tence groups.  
19
2 Target Corpus 
In this paper, we use a multilingual parallel corpus 
called BTEC (Takezawa et al, 2002) for our ex-
periments. BTEC was used in IWSLT (Akiba et al, 
2004). This parallel corpus is a collection of Japa-
nese sentences and their translations into English, 
Korean and Chinese that are often found in phrase 
books for foreign tourists. These parallel sentences 
cover a number of situations (e.g., hotel reserva-
tions, troubleshooting) for Japanese going abroad, 
and most of the sentences are rather short. Since 
the scope of its topics is quite limited, some very 
similar sentences can be found in the corpus, mak-
ing BTEC appropriate for modification with com-
pression or replacement of sentences. We use only 
a part of BTEC for training data in our experiments.  
The training data we employ contain 152,170 
Japanese sentences, with each sentence combined 
with English and Chinese translations. In Japanese, 
each sentence has 8.1 words on average, and the 
maximum sentence length is 150 words. In English, 
each sentence contains an average of 7.4 words, 
with a maximum sentence length of 117 words. In 
Chinese, each sentence has an average of 6.7 
words and maximum length of 122 words. Some 
sentences appear twice or more in the training cor-
pus. In total, our data include 94,268 different 
Japanese sentences, 87,061 different Chinese sen-
tences, and 91,750 different English sentences. 
Therefore, there are some sentence pairs that con-
sist of exactly the same sentence in one language 
but a different sentence in another language, as Fig. 
1 shows. This relationship can help in finding the 
synonymous sentence group.  
The test data contain 510 sentences from differ-
ent training sets in the BTEC. Each source sen-
tence in the test data has 15 target sentences for 
evaluations. For the evaluation, we do not use any 
special process for the grouping process. Conse-
quently, our results can be compared with those of 
other MT systems. 
Figure 1.  Sample sentence pairs 
 
3 Modification Method  
When an SMT system learns the translation model, 
variations in the translated sentences of the pair are 
critical for determining whether the system obtains 
a good model. If the same sentence appears twice 
in the input-side language and these sentences 
form pairs with two different target sentences in 
the output-side language, then broadly speaking 
the translation model defines almost the same 
probability for these two target sentences.   
In our model, the translation system features the 
ability to generate an output sentence with some 
variations; however, for the system to generate the 
most appropriate output sentence, sufficient infor-
mation is required. Thus, it is difficult to prepare a 
sufficiently large training corpus.  
3.1 Synonymous Sentence Group 
Kashioka (2004) reported two steps for making a 
synonymous sentence group. The first is a con-
catenation step, and the second is a decomposition 
step. In this paper, to form a synonymous sentence 
group, we performed only the concatenation step, 
which has a very simple idea. When the expression 
?Exp_A1? in language A is translated into the ex-
pressions ?Exp_B1, Exp_BB2, ..., Exp_Bn? in lan-
guage B, that set of expressions form one 
synonymous group. Furthermore, when the sen-
tence ?Exp_A2? in language A is translated into the 
sentences ?Exp_B1, Exp_Bn+1, ..., Exp_Bm? in lan-
guage B, ?Exp_B1, Exp_Bn+1,  ..., Exp_Bm (n < m)? 
form one synonymous group. In this situation, 
?Exp_A1? and ?Exp_A2? form a synonymous 
group because both ?Exp_A1? and ?Exp_A2? have 
a relationship with the translation pairs of 
?Exp_B1.? Thus, ?Exp_A1, Exp_A2? in language A 
and ?Exp_B1, ..., Exp_Bm? in language B form a 
synonymous group. If other language information 
is available, we can extend this synonymous group 
using information on translation pairs for other 
languages. 
In this paper, we evaluate an EJ/JE system and a 
CJ/JC system, and our target data include three 
languages, i.e., Japanese, English, and Chinese. 
We make synonymous sentence groups in two dif-
ferent environments. One is a group using Japanese 
and English data, and other is a group that uses 
Japanese and Chinese data. 
S1 ? T1 
S2 ? T1 
S1 ? T2 
S3 ? T1 
The JE group contained 72,808 synonymous sentence 
groups, and the JC group contained 83,910 synonymous 
sentence groups as shown in Table 1. 
20
 
 # of Groups # of Sent per Group 
JE 72,808 2.1 
JC 83,910 1.8 
Table 1 Statistics used in BTEC data 
3.2 Modification 
We prepared the three types of modifications for 
training data. 
1. Compress the training corpus based on the 
synonymous sentence group (Fig. 2). 
2. Replace the input and output sides? sen-
tences with the selected sentence, consider-
ing the synonymous sentence group (Fig. 3). 
3. Replace one side?s sentences with a se-
lected sentence, considering the synony-
mous sentence group (Figs. 4, 5).  
We describe these modifications in more detail 
in the following subsections.  
3.2.1 Modification with Compression 
Here, a training corpus is constructed with several 
groups of synonymous sentences. Then, each 
group keeps only one pair of sentences and the 
other pairs are removed from each group, thereby 
decreasing the total number of sentences and nar-
rowing the variation of expressions. Figure 2 
shows an example of modification in this way. In 
the figure, S1, S2, and S3 indicate the input-side 
sentences while T1 and T2 indicate the output-side 
sentences. The left-hand side box shows a syn-
onymous sentence group in the original training 
corpus, where four sentence pairs construct one 
synonymous sentence group. The right-hand side 
box shows a part of the modified training corpus. 
In this case, we keep the S1 and T1 sentences, and 
this resulting pair comprises a modified training 
corpus.  
The selection of what sentences to keep is an im-
portant issue. In our current experiment, we select 
the most frequent sentence in each side?s language 
from within each group. In Fig. 2, S1 appeared 
twice, while S2 and S3 appeared only once in the 
input-side language. As for the output-side lan-
guage, T1 appeared three times and T2 appeared 
once. Thus, we keep the pair consisting of S1 and 
T1. When attempting to separately select the most 
frequent sentence in each language, we may not 
find suitable pairs in the original training corpus; 
however, we can make a new pair with the ex-
tracted sentences for the modified training corpus. 
 
S1?T1 
S2?T1 
S1?T2 
S3?T1 
? S1?T1 
Figure 2. Modification sample for compression 
3.2.2 Modification of replacing the sentences 
of both sides 
In the compression stage, the total number of sen-
tences in the modified training corpus is decreased, 
and it is clear that fewer sentences in the training 
corpus leads to diminished accuracy. In order to 
make a comparison between the original training 
corpus and a modified training corpus with the 
same number of sentences, we extract one pair of 
sentences from each group, and each pair appears 
in the modified training corpus in the same number 
of sentences. Figure 3 shows an example of this 
modification. The original training data are the 
same as in Fig. 2. Then we extract S1 and T1 by 
the same process from each side with this group, 
and replacing all of the input-side sentences with 
S1 in this group. The output side follows the same 
process. In this case, the modified training corpus 
consists of four pairs of S1 and T1. 
S1?T1 
S2?T1 
S1?T2 
S3?T1 
? 
S1?T1 
S1?T1 
S1?T1 
S1?T1 
 Figure 3. Sample modifications for replacement of 
both sentences 
3.2.3 Modification to replace only one side?s 
sentence 
With the previous two modifications, the lan-
guage variations in both sides decrease. Next, we 
propose the third modification, which narrows the 
range of one side?s variations.  
The sentences of one side are replaced with the 
selected sentence from that group. The sentence for 
replacement is selected by following the same 
process used in the previous modifications. As a 
result, two modified training corpora are available 
21
as shown in Figs. 4 and 5. Figure 4 illustrates the 
output side?s decreasing variation, while Fig. 5 
shows the input side?s decreasing variation.  
Figure 4. Modification example of replacing the 
output side?s sentence 
Figure 5. Modification example of replacing the 
input side?s sentence 
4 SMT System and Evaluation method 
In this section, we describe the SMT systems used 
in these experiments. The SMT systems? decoder 
is a graph-based decoder (Ueffing et al, 2002; 
Zhang et al, 2004). The first pass of the decoder 
generates a word-graph, a compact representation 
of alternative translation candidates, using a beam 
search based on the scores of the lexicon and lan-
guage models. In the second pass, an A* search 
traverses the graph. The edges of the word-graph, 
or the phrase translation candidates, are generated 
by the list of word translations obtained from the 
inverted lexicon model. The phrase translations 
extracted from the Viterbi alignments of the train-
ing corpus also constitute the edges. Similarly, the 
edges are also created from dynamically extracted 
phrase translations from the bilingual sentences 
(Watanabe and Sumita, 2003). The decoder used 
the IBM Model 4 with a trigram language model 
and a five-gram part-of-speech language model. 
Training of the IBM model 4 was implemented by 
the GIZA++ package (Och and Ney, 2003). All 
parameters in training and decoding were the same 
for all experiments. Most systems with this training 
can be expected to achieve better accuracy when 
we run the parameter tuning processes. However, 
our purpose is to compare the difference in results 
caused by modifying the training corpus. 
We performed experiments for JE/EJ and JC/CJ 
systems and four types of training corpora: 
1) Original BTEC corpus; 
2) Compressed BTEC corpus (see 3.2.1); 
3) Replace both languages (see 3.2.2); 
4) Replace one side language (see 3.2.3) 
4-1) replacement on the input side 
4-2) replacement on the output side. 
 For the evaluation, we use BLEU, NIST, WER, 
and PER as follows: 
S1?T1 
S2?T1 
S1?T2 
S3?T1 
? 
S1?T1 
S2?T1 
S1?T1 
S3?T1 
BLEU: A weighted geometric mean of the n-
gram matches between test and reference 
sentences multiplied by a brevity penalty 
that penalizes short translation sentences. 
S1?T1 
S2?T1 
S1?T2 
S3?T1 
? 
S1?T1 
S1?T1 
S1?T2 
S1?T1 
NIST: An arithmetic mean of the n-gram 
matches between test and reference sen-
tences multiplied by a length factor, which 
again penalizes short translation sentences. 
mWER (Niessen et al, 2000): Multiple refer-
ence word-error rate, which computes the 
edit distance (minimum number of inser-
tions, deletions, and substitutions) between 
test and reference sentences. 
mPER: Multiple reference position-independent 
word-error rate, which computes the edit 
distance without considering the word order. 
5 Experimental Results 
In this section, we show the experimental results 
for the JE/EJ and JC/CJ systems. 
5.1 EJ/JE-system-based JE group 
Tables 2 and 3 show the evaluation results for the 
EJ/JE system.  
EJ BLEU NIST mWER mPER
Original 0.36 3.73 0.55 0.51
Compress 0.47 5.83 0.47 0.44
Replace Both 0.42 5.71 0.50 0.47
Replace J. 0.44 2.98 0.60 0.58
Replace E. 0.48 6.05 0.44 0.41
Table 2. Evaluation results for EJ System 
 
 JE BLEU NIST mWER mPER
Original 0.46 3.96 0.52 0.49 
Compress 0.53 8.53 0.42 0.38 
Replace Both 0.49 8.10 0.46 0.41 
Replace J. 0.54 8.64 0.42 0.38 
Replace E. 0.51 6.10 0.52 0.49 
Table 3. Evaluation results for JE system 
22
Modification of the training data is based on the 
synonymous sentence group with the JE pair. 
The EJ system performed at 0.55 in mWER with 
the original data set, and the system replacing the 
Japanese side achieved the best performance of 
0.44 in mWER. The system then gained 0.11 in 
mWER. On the other hand, the system replacing 
the English side lost 0.05 in mWER. The mPER 
score also indicates a similar result. For the BLEU 
and NIST scores, the system replacing the Japa-
nese side also attained the best performance. 
The JE system attained a score of 0.52 in mWER 
with the original data set, while the system with 
English on the replacement side gave the best per-
formance of 0.42 in mWER, a gain of 0.10. On the 
other hand, the system with Japanese on the re-
placement side showed no change in mWER, and 
the case of compression achieved good perform-
ance. The ratios of mWER and mPER are nearly 
the same for replacing Japanese. Thus, in both di-
rections replacement of the input-side language 
derives a positive effect for translation modeling. 
5.2 CJ/JC system-based JC group 
Tables 4 and 5 show the evaluation results for the 
EJ/JE system based on the group with a JC lan-
guage pair. 
CJ BLEU NIST mWER mPER
Original 0.51 6.22 0.41 0.38
Compress 0.52 6.43 0.43 0.40
Replace both 0.53 5.99 0.40 0.37
Replace J. 0.50 5.98 0.41 0.39
Replace C.  0.51 6.22 0.41 0.38
Table 4. Evaluation results for CJ based on the JC 
language pair 
 
JC BLEU NIST mWER mPER
Original 0.56 8.45 0.38 0.34 
Compress 0.55 8.22 0.41 0.36 
Replace both 0.56 8.32 0.39 0.35 
Replace J. 0.56 8.25 0.40 0.36 
Replace C. 0.57 8.33 0.38 0.35 
Table 5. Evaluation results for JC based on the JC 
language pair 
 
The CJ system achieved a score of 0.41 in 
mWER with the original data set, with the other 
cases similar to the original; we could not find a 
large difference among the training corpus modifi-
cations. Furthermore, the JC system performed at 
0.38 in mWER with the original data, although the 
other cases? results were not as good. These results 
seem unusual considering the EJ/JE system, indi-
cating that they derive from the features of the 
Chinese part of the BTEC corpus.  
6 Discussion  
Our EJ/JE experiment indicated that the system 
with input-side language replacement achieved 
better performance than that with output-side lan-
guage replacement. This is a reasonable result be-
cause the system learns the translation model with 
fewer variations for input-side language.   
In the experiment on the CJ/JC system based on 
the JC group, we did not provide an outline of the 
EJ/JE system due to the features of BTEC. Initially, 
BTEC data were created from pairs of Japanese 
and English sentences in the travel domain. Japa-
nese-English translation pairs have variation as 
shown in Fig. 1. However, when Chinese data was 
translated, BTEC was controlled so that the same 
Japanese sentence has only one Chinese sentence. 
Accordingly, there is no variation in Chinese sen-
tences for the pair with the same Japanese sentence. 
Therefore, the original training data would be simi-
lar to the situation of replacing Chinese. Moreover, 
replacing the Japanese data was almost to the same 
as replacing both sets of data. Considering this fea-
ture of the training corpus, i.e. the results for the 
CJ/JC system based on the group with JC language 
pairs, there are few differences between keeping 
the original data and replacing the Chinese data, or 
between replacing both side?s data and replacing 
only the Japanese data. These results demonstrate 
the correctness of the hypothesis that reducing the 
input side?s language variation makes learning 
models more effective.  
Currently, our modifications only roughly proc-
ess sentence pairs, though the process of making 
groups is very simple. Sometimes a group may 
include sentences or words that have slightly dif-
ferent meanings, such as. fukuro (bag), kamibukuro 
(paper bag), shoppingu baggu (shopping bag), 
tesagebukuro (tote bag), and biniiru bukuro (plas-
tic bag). In this case if we select tesagebukuro 
from the Japanese side and ?paper bag? from the 
English side, we have an incorrect word pair in the 
translation model. To handle such a problem, we 
would have to arrange a method to select the sen-
23
tences from a group. This problem is discussed in 
Imamura et al (2003). As one solution to this 
problem, we borrowed the measures of literalness, 
context freedom, and word translation stability in 
the sentence-selection process.  
In some cases, the group includes sentences with 
different meanings, and this problem was men-
tioned in Kashioka (2004). In an attempt to solve 
the problem, he performed a secondary decomposi-
tion step to produce a synonymous group. How-
ever, in the current training corpus, each 
synonymous group before the decomposition step 
is small, so there would not be enough difference 
for modifications after the decomposition step.   
The replacement of a sentence could be called 
paraphrasing. Shimohata et al (2004) reported a 
paraphrasing effect in MT systems, where if each 
group would have the same meaning, the variation 
in the phrases that appeared in the other groups 
would reduce the probability. Therefore, consider-
ing our results in light of their discussion, if the 
training corpus could be modified with the module 
for paraphrasing in order to control phrases, we 
could achieve better performance.  
7 Conclusion  
This paper described the modification of a training 
set based on a synonymous sentence group for a 
statistical machine translation system in order to 
attain better performance. In an EJ/JE system, we 
confirmed a positive effect by replacing the input-
side language. Because the Chinese data was spe-
cific in our modification, we observed an inconclu-
sive result for the modification in the CJ/JC system 
based on the synonymous sentence group with a JC 
language pair. However, there was still some effect 
on the characteristics of the training corpus. In this 
paper, the modifications of the training set are 
based on the synonymous sentence group, and we 
replace the sentence with rough processing. If we 
paraphrased the training set and controlled the 
phrase pair, we could achieve better performance 
with the same training set. 
Acknowledgements 
This research was supported in part by the National 
Institute of Information and Communications 
Technology. 
References 
Yasuhiro AKIBA, Marcello FEDERICO, Noriko 
KANDO, Hiromi NAKAIWA, Michael PAUL, and 
Jun'ichi TSUJII, 2004. Overview of the IWSLT04 
Evaluation Campaign, In Proc. of IWSLT04, 1 ? 12. 
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence 
statistics. In Proceedings of the HLT Conference, San 
Diego, California. 
Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto, 
2003. Automatic Construction of Machine Translation 
Knowledge Using Translation Literalness, in Proc. of 
EACL 2003, 155 ? 162. 
Hideki Kashioka, 2004. Grouping Synonymous Sen-
tences from a Parallel Corpus. In Proc. of LREC 2004, 
391 - 394. 
Sonja Niessen, Franz J. Och, Gregor Leusch, and 
Hermann Ney. 2000. An evaluation tool for machine 
translation: Fast evaluation for machine translation 
research. In Proc.of LREC 2000, 39 ? 45. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19 - 51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL 2002, 
311?318. 
Mitsuo Shimohata, Eiichiro Sumita, and Yuji Matsu-
moto, 2004. Building a Paraphrase Corpus for Speech 
Translation. In Proc. of LREC 2004, 1407 - 1410. 
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, 
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. 
Toward a broad-coverage bilingual corpus for speech 
translation of travel conversations in the real world, 
In Proc. of LREC 2002, 147?152. 
Nicola Ueffing, Franz Josef Och, and Hermann Ney. 
2002. Generation of word graphs in statistical ma-
chine translation. In Proc. of the Conference on 
Empirical Methods for Natural Language Proc-
essing (EMNLP02), 156 ? 163. 
Taro Watanabe and Eiichiro Sumita. 2003. Example-
based decoding for statistical machine translation. In 
Machine Translation Summit IX, 410 ? 417. 
Ruiqiang Zhang, Genichiro Kikui, Hirofumi Yamamoto, 
Frank Soong, Taro Watanabe and Wai Kit Lo, 2004. 
A Unified Approach in Speech-to-Speech Translation: 
Integrating Features of Speech recognition and Ma-
chine Translation, In Proc. of COLING 2004, 1168 - 
1174.  
24
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221?224,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Modeling Spoken Decision Making Dialogue
and Optimization of its Dialogue Strategy
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi Nakamura
MASTAR Project, NICT
Kyoto, Japan.
teruhisa.misu@nict.go.jp
Abstract
This paper presents a spoken dialogue frame-
work that helps users in making decisions.
Users often do not have a definite goal or cri-
teria for selecting from a list of alternatives.
Thus the system has to bridge this knowledge
gap and also provide the users with an appro-
priate alternative together with the reason for
this recommendation through dialogue. We
present a dialogue state model for such deci-
sion making dialogue. To evaluate this model,
we implement a trial sightseeing guidance sys-
tem and collect dialogue data. Then, we opti-
mize the dialogue strategy based on the state
model through reinforcement learning with a
natural policy gradient approach using a user
simulator trained on the collected dialogue
corpus.
1 Introduction
In many situations where spoken dialogue interfaces
are used, information access by the user is not a goal in
itself, but a means for decision making (Polifroni and
Walker, 2008). For example, in a restaurant retrieval
system, the user?s goal may not be the extraction of
price information but to make a decision on candidate
restaurants based on the retrieved information.
This work focuses on how to assist a user who is
using the system for his/her decision making, when
he/she does not have enough knowledge about the tar-
get domain. In such a situation, users are often un-
aware of not only what kind of information the sys-
tem can provide but also their own preference or fac-
tors that they should emphasize. The system, too, has
little knowledge about the user, or where his/her inter-
ests lie. Thus, the system has to bridge such gaps by
sensing (potential) preferences of the user and recom-
mend information that the user would be interested in,
considering a trade-off with the length of the dialogue.
We propose a model of dialogue state that consid-
ers the user?s preferences as well as his/her knowledge
about the domain changing through a decision making
dialogue. A user simulator is trained on data collected
with a trial sightseeing system. Next, we optimize
the dialogue strategy of the system via reinforcement
learning (RL) with a natural policy gradient approach.
2 Spoken decision making dialogue
We assume a situation where a user selects from a given
set of alternatives. This is highly likely in real world
situations; for example, the situation wherein a user se-
lects one restaurant from a list of candidates presented
Choose the optimal spot
1. Cherry 
Blossoms
2. Japanese
Garden
3. Easy
Access
Kinkakuji-
Temple
Ryoanji-
Temple
Nanzenji-
Temple
?
?????
Goal
Criteria
Alternatives
(choices)
?????
p1 p2 p3
v11 v12 v13
? ?
Figure 1: Hierarchy structure for sightseeing guidance
dialogue
by a car navigation system. In this work, we deal with
a sightseeing planning task where the user determines
the sightseeing spot to visit, with little prior knowledge
about the target domain. The study of (Ohtake et al,
2009), which investigated human-human dialogue in
such a task, reported that such consulting usually con-
sists of a sequence of information requests from the
user, presentation and elaboration of information about
certain spots by the guide followed by the user?s evalu-
ation. We thus focus on these interactions.
Several studies have featured decision support sys-
tems in the operations research field, and the typical
method that has been employed is the Analytic Hierar-
chy Process (Saaty, 1980) (AHP). In AHP, the problem
is modeled as a hierarchy that consists of the decision
goal, the alternatives for achieving it, and the criteria
for evaluating these alternatives. An example hierarchy
using these criteria is shown in Figure 1.
For the user, the problem of making an optimal de-
cision can be solved by fixing a weight vector P
user
=
(p
1
, p
2
, . . . , p
M
) for criteria and local weight matrix
V
user
= (v
11
, v
12
, . . . , v
1M
, . . . , v
NM
) for alterna-
tives in terms of the criteria. The optimal alternative
is then identified by selecting the spot k with the maxi-
mum priority of
?
M
m=1
p
m
v
km
. In typical AHP meth-
ods, the procedure of fixing these weights is often con-
ducted through pairwise comparisons for all the possi-
ble combinations of criteria and spots in terms of the
criteria, followed by weight tuning based on the re-
sults of these comparisons (Saaty, 1980). However, this
methodology cannot be directly applied to spoken dia-
logue systems. The information about the spot in terms
of the criteria is not known to the users, but is obtained
only via navigating through the system?s information.
In addition, spoken dialogue systems usually handle
several candidates and criteria, making pairwise com-
parison a costly affair.
We thus consider a spoken dialogue framework that
estimates the weights for the user?s preference (po-
tential preferences) as well as the user?s knowledge
221
about the domain through interactions of information
retrieval and navigation.
3 Decision support system with spoken
dialogue interface
The dialogue system we built has two functions: an-
swering users? information requests and recommend-
ing information to them. When the system is requested
to explain about the spots or their determinants, it ex-
plains the sightseeing spots in terms of the requested
determinant. After satisfying the user?s request, the
system then provides information that would be helpful
in making a decision (e.g., instructing what the system
can explain, recommending detailed information of the
current topic that the user might be interested in, etc.).
Note that the latter is optimized via RL (see Section 4).
3.1 Knowledge base
Our back-end DB consists of 15 sightseeing spots as al-
ternatives and 10 determinants described for each spot.
We select determinants that frequently appear in the di-
alogue corpus of (Ohtake et al, 2009) (e.g. cherry blos-
soms, fall foliage). The spots are annotated in terms of
these determinants if they apply to them. The value of
the evaluation e
nm
is ?1? when the spot n applies to the
determinant m and ?0? when it does not.
3.2 System initiative recommendation
The content of the recommendation is determined
based on one of the following six methods:
1. Recommendation of determinants based on the
currently focused spot (Method 1)
This method is structured on the basis of the user?s
current focus on a particular spot. Specifically, the
system selects several determinants related to the
current spot whose evaluation is ?1? and presents
them to the user.
2. Recommendation of spots based on the cur-
rently focused determinant (Method 2)
This method functions on the basis of the focus on
a certain specific determinant.
3. Open prompt (Method 3)
The system does not make a recommendation, and
presents an open prompt.
4. Listing of determinants 1 (Method 4)
This method lists several determinants to the user in
ascending order from the low level user knowledge
K
sys
(that the system estimates). (K
sys
, P
sys
, p
m
and Pr(p
m
= 1) are defined and explained in Sec-
tion 4.2.)
5. Listing of determinants 2 (Method 5)
This method also lists the determinants, but the or-
der is based on the user?s high preference P
sys
(that
the system estimates).
6. Recommendation of user?s possibly preferred
spot (Method 6)
The system recommends a spot as well as the de-
terminants that the users would be interested in
based on the estimated preference P
sys
. The sys-
tem selects one spot k with a maximum value of
?
M
m=1
Pr(p
m
= 1) ? e
k,m
. This idea is based
on collaborative filtering which is often used for
recommender systems (Breese et al, 1998). This
method will be helpful to users if the system suc-
cessfully estimates the user?s preference; however,
it will be irrelevant if the system does not.
We will represent these recommendations
through a dialogue act expression, (ca
sys
{sc
sys
}),
which consists of a communicative act ca
sys
and the semantic content sc
sys
. (For exam-
ple Method1{(Spot
5
), (Det
3
,Det
4
,Det5)},
Method3{NULL,NULL}, etc.)
4 Optimization of dialogue strategy
4.1 Models for simulating a user
We introduce a user model that consists of a tuple of
knowledge vector K
user
, preference vector P
user
, and
local weight matrix V
user
. In this paper, for simplic-
ity, a user?s preference vector or weight for determi-
nants P
user
= (p
1
, p
2
, . . . , p
M
) is assumed to con-
sist of binary parameters. That is, if the user is in-
terested in (or potentially interested in) the determi-
nant m and emphasizes it when making a decision,
the preference p
m
is set to ?1?. Otherwise, it is set
to ?0?. In order to represent a state that the user has
potential preference, we introduce a knowledge param-
eter K
user
= (k
1
, k
2
, . . . , k
M
) that shows if the user
has the perception that the system is able to handle or
he/she is interested in the determinants. k
m
is set to
?1? if the user knows (or is listed by system?s recom-
mendations) that the system can handle determinant m
and ?0? when he/she does not. For example, the state
that the determinant m is the potential preference of a
user (but he/she is unaware of that) is represented by
(k
m
= 0, p
m
= 1). This idea is in contrast to previous
research which assumes some fixed goal observable by
the user from the beginning of the dialogue (Schatz-
mann et al, 2007). A user?s local weight v
nm
for spot
n in terms of determinant m is set to ?1?, when the
system lets the user know that the evaluation of spots is
?1? through recommendation Methods 1, 2 and 6.
We constructed a user simulator that is based on
the statistics calculated through an experiment with the
trial system (Misu et al, 2010) as well as the knowl-
edge and preference of the user. That is, the user?s com-
municative act cat
user
and the semantic content sct
user
for the system?s recommendation at
sys
are generated
based on the following equation:
Pr(cat
user
, sct
user
|cat
sys
, sct
sys
,K
user
,P
user
)
= Pr(cat
user
|cat
sys
)
?Pr(sct
user
|K
user
,P
user
, cat
user
, cat
sys
, sct
sys
)
This means that the user?s communicative act ca
user
is sampled based on the conditional probability of
Pr(cat
user
|cat
sys
) in (Misu et al, 2010). The seman-
tic content sc
user
is selected based on the user?s pref-
erence P
user
under current knowledge about the de-
terminants K
user
. That is, the sc is sampled from the
determinants within the user?s knowledge (k
m
= 1)
based on the probability that the user requests the de-
terminant of his/her preference/non-preference, which
is also calculated from the dialogue data of the trial sys-
tem.
4.2 Dialogue state expression
We defined the state expression of the user in the pre-
vious section. However the problem is that for the
system, the state (P
user
,K
user
,V
user
) is not observ-
able, but is only estimated from the interactions with
the user. Thus, this model is a partially observable
Markov decision process (POMDP) problem. In or-
der to estimate unobservable properties of a POMDP
222
 
Priors of the estimated state:
- Knowledge: K
sys
= (0.22, 0.01, 0.02, 0.18, . . . )
- Preference: P
sys
= (0.37, 0.19, 0.48, 0.38, . . . )
Interactions (observation):
- System recommendation:
a
sys
= Method1{(Spot
5
), (Det
1
, Det
3
, Det4)}
- User query:
a
user
= Accept{(Spot
5
), (Det
3
)}
Posterior of the estimated state:
- Knowledge: K
sys
= (1.00, 0.01, 1.00, 1.00, . . . )
- Preference: P
sys
= (0.26, 0.19, 0.65, 0.22, . . . )
User?s knowledge acquisition:
- Knowledge: K
user
? {k
1
= 1, k
3
= 1, k
4
= 1}
- Local weight: V
user
? {v
51
= 1, v
53
= 1, v
54
=
1}
 
Figure 2: Example of state update
and handle the problem as an MDP, we introduce
the system?s inferential user knowledge vector K
sys
or probability distribution (estimate value) K
sys
=
(Pr(k
1
= 1), P r(k
2
= 1), . . . , P r(k
M
= 1)) and
that of preference P
sys
= (Pr(p
1
= 1), P r(p
2
=
1), . . . , P r(p
M
= 1)).
The dialogue state DSt+1 or estimated user?s dia-
logue state of the step t+1 is assumed to be dependent
only on the previous state DSt, as well as the interac-
tions It = (at
sys
, at
user
).
The estimated user?s state is represented as a prob-
ability distribution and is updated by each interac-
tion. This corresponds to representing the user types
as a probability distribution, whereas the work of (Ko-
matani et al, 2005) classifies users to several discrete
user types. The estimated user?s preference P
sys
is up-
dated when the system observes the interaction It. The
update is conducted based on the following Bayes? the-
orem using the previous state DSt as a prior.
Pr(p
m
= 1|It) =
Pr(I
t
|p
m
=1)Pr(p
m
=1)
Pr(I
t
|p
m
=1)Pr(p
m
=1)+Pr(I
t
|(p
m
=0))Pr(1?Pr(p
m
=1))
Here, Pr(It|p
m
= 1), P r(It|(p
m
= 0) to the right
side was obtained from the dialogue corpus of (Misu et
al., 2010). This posterior is then used as a prior in the
next state update using interaction It+1. An example
of this update is illustrated in Figure 2.
4.3 Reward function
The reward function that we use is based on the num-
ber of agreed attributes between the user preference
and the decided spot. Users are assumed to determine
the spot based on their preference P
user
under their
knowledge K
user
(and local weight for spots V
user
)
at that time, and select the spot k with the maximum
priority of
?
m
k
k
? p
k
? v
km
. The reward R is then
calculated based on the improvement in the number of
agreed attributes between the user?s actual (potential)
preferences and the decided spot k over the expected
agreement by random spot selection.
R =
M
?
m=1
p
m
? e
k,m
?
1
N
N
?
n=1
M
?
m=1
p
m
? e
n,m
For example, if the decided spot satisfies three prefer-
ences and the average agreement of the agreement by
random selection is 1.3, then the reward is 1.7.
4.4 Optimization by reinforcement learning
The problem of system recommendation generation is
optimized through RL. The MDP (S, A, R) is defined
as follows. The state parameter S = (s
1
, s
2
, . . . , s
I
) is
generated by extracting the features of the current dia-
logue state DSt. We use the following 29 features 1.
1. Parameters that indicate the # of interactions from
the beginning of the dialogue. This is approximated by
five parameters using triangular functions. 2. User?s
previous communicative act (1 if at?1
user
= x
i
, other-
wise 0). 3. System?s previous communicative act (1 if
at?1
sys
= y
j
, otherwise 0). 4. Sum of the estimated user
knowledge about determinants (?N
n=1
Pr(k
n
= 1)).
5. Number of presented spot information. 6. Expecta-
tion of the probability that the user emphasizes the de-
terminant in the current state (Pr(k
n
= 1)? Pr(p
n
=
1)) (10 parameters). The action set A consists of the
six recommendation methods shown in subsection 3.2.
Reward R is given by the reward function of subsection
4.3.
A system action a
sys
(ca
sys
) is sampled based on the
following soft-max (Boltzmann) policy.
?(a
sys
= k|S) = Pr(a
sys
= k|S,?)
=
exp(
?
I
i=1
s
i
? ?
ki
)
?
J
j=1
exp(
?
I
i=1
s
i
? ?
ji
)
Here, ? = (?
11
, ?
12
, . . . ?
1I
, . . . , ?
JI
) consists of J (#
actions) ? I (# features) parameters. The parameter
?
ji
works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. This ? is the target of optimization by RL.
We adopt the Natural Actor Critic (NAC) (Peters and
Schaal, 2008), which adopts a natural policy gradient
method as the policy optimization method.
4.5 Experiment by dialogue simulation
For each simulated dialogue session, a simulated user
(P
user
,K
user
,V
user
) is sampled. A preference vector
P
user
of the user is generated so that he/she has four
preferences. As a result, four parameters in P
user
are
?1? and the others are ?0?. This vector is fixed through-
out the dialogue episode. This sampling is conducted
based on the rate proportional to the percentage of users
who emphasize it for making decisions (Misu et al,
2010). The user?s knowledge K
user
is also set based
on the statistics of the ?percentage of users who stated
the determinants before system recommendation?. For
each determinant, we sample a random valuable r that
ranges from ?0? to ?1?, and k
m
is set to ?1? if r is
smaller than the percentage. All the parameters of
local weights V
user
are initialized to ?0?, assuming
that users have no prior knowledge about the candi-
date spots. As for system parameters, the estimated
user?s preference P
sys
and knowledge K
sys
are ini-
tialized based on the statistics of our trial system (Misu
et al, 2010).
We assumed that the system does not misunderstand
the user?s action. Users are assumed to continue a di-
alogue session for 20 turns2, and episodes are sampled
using the policy ? at that time and the user simulator
1Note that about half of them are continuous variables and
that the value function cannot be denoted by a lookup table.
2In practice, users may make a decision at any point once
they are satisfied collecting information. And this is the rea-
son why we list the rewards in the early dialogue stage in
223
Table 1: Comparison of reward with baseline methods
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)
B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)
Table 2: Comparison of reward with discrete dialogue
state expression
Reward (?std)
State T = 5 T = 10 T = 15 T = 20
PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)
Table 3: Effect of estimated preference and knowledge
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
Pref+Know0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)
Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)
No Pref or
Know
0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)
of subsection 4.1. In each turn, the system is rewarded
using the reward function of subsection 4.3. The pol-
icy (parameter ?) is updated using NAC in every 2,000
dialogues.
4.6 Experimental result
The policy was fixed at about 30,000 dialogue
episodes. We analyzed the learned dialogue policy by
examining the value of weight parameter ?. We com-
pared the parameters of the trained policy between ac-
tions3. The weight of the parameters that represent the
early stage of the dialogue was large in Methods 4 and
5. On the other hand, the weight of the parameters that
represent the latter stage of the dialogue was large in
Methods 2 and 6. This suggests that in the trained pol-
icy, the system first bridges the knowledge gap between
the user, estimates the user?s preference, and then, rec-
ommends specific information that would be useful to
the user.
Next, we compared the trained policy with the fol-
lowing baseline methods.
1. No recommendation (B1)
The system only provides the requested informa-
tion and does not generate any recommendations.
2. Random recommendation (B2)
The system randomly chooses a recommendation
from six methods.
The comparison of the average reward between the
baseline methods is listed in Table 1. Note that the ora-
cle average reward that can be obtained only when the
user knows all knowledge about the knowledge base
(it requires at least 50 turns) was 1.45. The reward by
the strategy optimized by NAC was significantly better
than that of baseline methods (n = 500, p < .01).
We then compared the proposed method with the
case where estimated user?s knowledge and preference
are represented as discrete binary parameters instead of
probability distributions (PDs). That is, the estimated
user?s preference p
m
of determinant m is set to ?1?
when the user requested the determinant, otherwise it
is ?0?. The estimated user?s knowledge k
m
is set to
the following subsections. In our trial system, the dialogue
length was 16.3 turns with a standard deviation of 7.0 turns.
3The parameters can be interpreted as the size of the con-
tribution for selecting the action.
?1? when the system lets the user know the determi-
nant, otherwise it is ?0?. Another dialogue strategy was
trained using this dialogue state expression. This result
is shown in Table 2. The proposed method that rep-
resents the dialogue state as a probability distribution
outperformed (p < .01 (T=15,20)) the method using a
discrete state expression.
We also compared the proposed method with the
case where either one of estimated preference or
knowledge was used as a feature for dialogue state in
order to carefully investigate the effect of these factors.
In the proposed method, expectation of the probabil-
ity that the user emphasizes the determinant (Pr(k
n
=
1) ? Pr(p
n
= 1)) was used as a feature of dialogue
state. We evaluated the performance of the cases where
the estimated knowledge Pr(k
n
= 1) or estimated
preference Pr(p
n
= 1) was used instead of the expec-
tation of the probability that the user emphasizes the
determinant. We also compared with the case where
no preference/knowledge feature was used. This result
is shown in Table 3. We confirmed that significant im-
provement (p < .01 (T=15,20)) was obtained by taking
into account the estimated knowledge of the user.
5 Conclusion
In this paper, we presented a spoken dialogue frame-
work that helps users select an alternative from a list of
alternatives. We proposed a model of dialogue state for
spoken decision making dialogue that considers knowl-
edge as well as preference of the user and the system,
and its dialogue strategy was trained by RL. We con-
firmed that the learned policy achieved a better recom-
mendation strategy over several baseline methods.
Although we dealt with a simple recommendation
strategy with a fixed number of recommendation com-
ponents, there are many possible extensions to this
model. The system is expected to handle a more com-
plex planning of natural language generation. We also
need to consider errors in speech recognition and un-
derstanding when simulating dialogue.
References
J. Breese, D. Heckerman, and C. Kadie. 1998. ?empirical
analysis of predictive algorithms for collaborative filter-
ing?. In ?Proc. the 14th Annual Conference on Uncer-
tainty in Artificial Intelligence?, pages 43?52.
K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005.
User Modeling in Spoken Dialogue Systems to Generate
Flexible Guidance. User Modeling and User-Adapted In-
teraction, 15(1):169?183.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and
S. Nakamura. 2010. Construction and Experiment of a
Spoken Consulting Dialogue System. In Proc. IWSDS.
K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.
2009. Annotating Dialogue Acts to Construct Dialogue
Systems for Consulting. In Proc. The 7th Workshop on
Asian Language Resources, pages 32?39.
J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neuro-
computing, 71(7-9):1180?1190.
J. Polifroni and M. Walker. 2008. Intensional Summaries
as Cooperative Responses in Dialogue: Automation and
Evaluation. In Proc. ACL/HLT, pages 479?487.
T. Saaty. 1980. The Analytic Hierarchy Process: Planning,
Priority Setting, Resource Allocation. Mcgraw-Hill.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
HLT/NAACL.
224
