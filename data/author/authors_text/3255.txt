Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 438?441,
Prague, June 2007. c?2007 Association for Computational Linguistics
USFD: Preliminary Exploration of Features
and Classifiers for the TempEval-2007 Tasks
Mark Hepple
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
hepple@dcs.shef.ac.uk
Andrea Setzer
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
andrea@dcs.shef.ac.uk
Rob Gaizauskas
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
robertg@dcs.shef.ac.uk
Abstract
We describe the Sheffield system used
in TempEval-2007. Our system takes
a machine-learning (ML) based approach,
treating temporal relation assignment as a
simple classification task and using features
easily derived from the TempEval data, i.e.
which do not require ?deeper? NLP analy-
sis. We aimed to explore three questions:
(1) How well would a ?lite? approach of
this kind perform? (2) Which features con-
tribute positively to system performance?
(3) Which ML algorithm is better suited for
the TempEval tasks? We used the Weka
ML workbench to facilitate experimenting
with different ML algorithms. The paper de-
scribes our system and supplies preliminary
answers to the above questions.
1 Introduction
The Sheffield team were involved in TempEval as
co-proposers/co-organisers of the task.1 For our par-
ticipation in the task, we decided to pursue an ML-
based approach, the benefits of which have been ex-
plored elsewhere (Boguraev and Ando, 2005; Mani
et al, 2006). For the TempEval tasks, this is easily
done by treating the assignment of temporal relation
types as a simple classification task, using readily
available information for the instance features. More
specifically, the features used were ones provided as
1We maintained a strict separation between persons assisting
in annotation of the test corpus and those involved in system
development.
attributes in the TempEval data annotation for the
events/times being related, plus some additional fea-
tures that could be straightforwardly computed from
documents, i.e. without the use of more heavily ?en-
gineered? NLP components. The aims of this work
were three-fold. First, we wanted to see whether a
?lite? approach of this kind could yield reasonable
performance, before pursuing possibilities that re-
lied on using ?deeper? NLP analysis methods. Sec-
ondly, we were interested to see which of the fea-
tures considered would contribute positively to sys-
tem performance. Thirdly, rather than selecting a
single ML approach (e.g. one of those currently in
vogue within NLP), we wanted to look across ML
algorithms to see if any approach was better suited
to the TempEval tasks than any other, and conse-
quently we used the Weka workbench (Witten and
Frank, 2005) in our ML experiments.
In what follows, we will first describe how our
system was constructed, before going on to discuss
our main observations around the key aims men-
tioned above. For example, in regard to our ?lite? ap-
proach, we would observe (c.f. the results reported
in the Task Description paper) that although some
other systems scored more highly, the score differ-
ences were relatively small. Regarding features, we
found for example that the system performed better
for Task A when, surprisingly, the tense attribute
of EVENTs was excluded. Regarding ML algo-
rithms, we found not only that there was substantial
variation between the effectiveness of different algo-
rithms for assigning relations (as one might expect),
but also that there was considerable differences in
the relative effectiveness of algorithms across tasks,
438
i.e. so that an algorithm performing well on one task
(compared to the alternatives), might perform rather
poorly on another task. The paper closes with some
comments about future research directions.
2 System Description
The TempEval training and test data is marked up
to identify all event and time expressions occurring
within documents, and also to record the TLINK re-
lations that are relevant for each task (except that
TLINK relation types are absent in the test data).
These annotations provide additional information
about these entities in the form of XML attributes,
e.g. for EVENT annotations we find attributes such
as tense, aspect, part-of-speech and so on.
Our system consists of a suite of Perl scripts
that create the input files required for Weka, and
handle its output. These include firstly an ?ex-
traction? script, which extracts information about
EVENT, TIMEXs and TLINKs from the data files, and
secondly a ?feature selection/reformatting? script,
which allows the information that is to be supplied
to Weka to be selected, and recasts it into the format
that Weka requires for its training/test files. A final
script takes Weka?s output over the test files and con-
nects it back to the original test documents to pro-
duce the final output files required for scoring.
The information that the first extraction script ex-
tracts for each EVENT, TIMEX and TLINK largely
corresponds to attributes/values associated with the
annotations of these items in the initial data files
(although not all such attributes are of use for ma-
chine learning purposes). In addition, the script de-
termines for each EVENT expression whether it is
one deemed relevant by the Event Target List (ETL)
for Tasks A and B. This script also maps EVENTs
and TIMEXs into sequential order ? intra-sentential
order for task A and inter-sentential order for task
C. This information can be used to compute various
?order? features, such as:
event-first: do a related EVENT and TIMEX
(for Task A) appear with the EVENT before or after
the TIMEX?
adjacent: do a related EVENT and TIMEX (again
for Task A) appear adjacently in the sequence of
temporal entities or not? (Note that this allows an
EVENT and TIMEX to be adjacent if there tokens
Task
Type Attribute A B C
EVENT aspect X X X
EVENT polarity X X ?
EVENT POS X X X
EVENT stem X ? ?
EVENT string ? ? ?
EVENT class ? X X
EVENT tense ? X X
ORDER adjacent X N/A N/A
ORDER event-first X N/A N/A
ORDER event-between ? N/A N/A
ORDER timex-between ? N/A N/A
TIMEX3 mod X ? N/A
TIMEX3 type X ? N/A
TLINK reltype X X X
Table 1: Features
that intervene, but not any other temporal entities.)
event-between: for a related EVENT/TIMEX
pair, do any other events appear between them?
timex-between: for a related EVENT/TIMEX
pair, do any other timexes appear between them?
Table 1 lists all the features that we tried using
for any of the three tasks. Aside from the OR-
DER features (as designated in the leftmost col-
umn), which were computed as just described, and
the EVENT string feature (which is the literal
tagged expression from the text), all other features
correspond to annotation attributes. Note that the
TLINKreltype is extracted from the training data
to provide the target attribute for training (a dummy
value is provided for this in test data).
The output of the extraction script is converted to
a format suitable for use by Weka by a second script.
This script also allows a manual selection to be made
as to the features that are included. For each of the
three tasks, a rough-and-ready process was followed
to find a ?good? set of features for use with that
task, which proceeded as follows. Firstly, the maxi-
mal set of features considered for the task was tried
with a few ML algorithms in Weka (using a 10-fold
cross-validation over the training data) to find one
that seemed to work quite well for the task. Then
using only that algorithm, we checked whether the
string feature could be dropped (since this fea-
439
ture?s value set was always of quite high cardinality),
i.e. if its omission improved performance, which for
all three tasks was the case. Next, we tried dropping
each of the remaining features in turn, to identify
those whose exclusion improved performance, and
then for those features so identified, tried dropping
them in combination to arrive at a final ?optimal? fea-
ture set. Table 1 shows for each of the tasks which
of the features were considered for inclusion (those
marked N/A were not), and which of these remained
in the final optimal feature set (X).
Having determined the set of features for use with
each task, we tried out a range of ML algorithms
(again with a 10-fold cross-validation over the train-
ing data), to arrive at the final feature-set/ML algo-
rithm combination that was used for the task in the
competitive evaluation. This was trained over the
entire training data and applied to the test data to
produce the final submitted results.
3 Discussion
Looking to Table 1, and the features that were con-
sidered for each task and then included in the final
set, various observations can be made. First, note
that the string feature was omitted for all tasks,
which is perhaps not surprising, since its values will
be sparsely distributed, so that there will be very few
training instances for most of its individual values.
However, the stem feature was found to be use-
ful for Task A, which can be interpreted as evidence
for a ?lexical effect? on local event-timex relations,
e.g. perhaps with different verbs displaying different
trends in how they relate to timexes. No correspond-
ing effects were observed for Tasks B and C.
The use of ORDER features for Task A was found
to be useful ? specifically the features indicating
whether the event or timex appeared linearly first in
the sentence and whether the two were adjacent or
not. The more elaborate ORDER features, address-
ing more specific cases of what might intervene be-
tween the related timex and event expression, were
not found to be helpful.
Perhaps the most striking observation to be made
regarding the table is that it was found beneficial to
exclude the feature tense for Task A, whilst the
feature aspect was retained. We have no expla-
nation to offer for this result. Likewise, the event
Task
Algorithm A B C
baseline 49.8 62.1 42.0
lazy.KStar 58.2 76.7 54.0
rules.DecisionTable 53.3 79.0 52.9
functions.SMO (svm) 55.1 78.1 55.5
rules.JRip 50.7 78.6 53.4
bayes.NaiveBayes 56.3 76.2 50.7
Table 2: Comparing different algorithms (%-acc.
scores, from cross-validation over training data)
class feature, which distinguishes e.g. perception
vs. reporting vs. aspectual etc verbs, was excluded
for Task A, although it was retained for Task B.
In regard to the use of different ML algorithms for
the classification tasks addressed in TempEval, we
observed considerable variation between algorithms
as to their performance, and this was not unexpected.
However, given the seemingly high similarity of the
three tasks, we were rather more surprised to see that
there was considerable variation between the perfor-
mance of algorithms across tasks, i.e. so that an al-
gorithm performing well on one task (compared to
the alternatives), might perform rather poorly on an-
other task. This is illustrated by the results in Table 2
for a selected subset of the algorithms considered,
which shows %-accuracy scores that were computed
by cross-validation over the training data, using the
feature set chosen as ?optimal? for each task.2 The
algorithm names in the left-hand column are the
ones used in WEKA (of which functions.SMO
is the WEKA implementation of support-vector ma-
chines or SVM). The first row of results give a ?base-
line? for performance, corresponding to the assign-
ment of the most common label for the task. (These
were produced using WEKA?s rules.ZeroR al-
gorithm, which does exactly that.)
The best results observed for each task are shown
in bold in the table. These best performing al-
gorithms were used for the corresponding tasks in
the competition. Observe that the lazy.KStar
2These scores are computed under the ?strict? requirement
that key and response labels should be identical. The TempE-
val competition also uses a ?relaxed? metric which gives par-
tial credit when one (or both) label is disjunctive and there is a
partial match, e.g. between labels AFTER and OVERLAP-OR-
AFTER. See (Verhagen et al, 2007) for details.
440
Task A Task B Task C
FS FR FS FR FS FR
USFD 0.59 0.60 0.73 0.74 0.54 0.59
ave. 0.56 0.59 0.74 0.75 0.51 0.60
max. 0.62 0.64 0.80 0.81 0.55 0.66
Table 3: Competition task scores for Sheffield sys-
tem (USFD), plus average/max scores across all
competing systems
method, which gives the best performance for Task
A, gives a rather ?middling? performance for Task
B. Similarly, the SVM method that gives the best
results for Task C falls quite a way below the per-
formance of KStar on Task A. A more extreme
case is seen with the results for rules.JRip
(Weka?s implementation of the RIPPER algorithm),
whose score for Task B is close to that of the best-
performing system, but which scores only slightly
above baseline on Task A.
The competition scores for our system are given
in Table 3, shown as (harmonic) F-measures under
both strict (FS) and relaxed (FR) metrics (see foot-
note 2). The table also shows the average score for
each task/metric across all systems taking part in the
competition, as well as the maximum score returned
by any system. See (Verhagen et al, 2007) for a full
tabulation of results for all systems.3
4 Future Directions
SIGNALs and SLINKs are possible candidates as
additional features ? signals obviously so, whereas
the benefits of exploiting subordination information
are less clear. Our initial exploratory efforts in
this direction involved pulling information regard-
ing SIGNALs and SLINKs across from TimeBank4
(Pustejovsky et al, 2003) so as to make this avail-
3The TempEval test data identifies precisely the temporal
entity pairs to which a relation label must be assigned. When
a fixed set of items is classified, the scores for precision, recall
and F-measure will be identical, being the same as the score for
simple accuracy. However, not all the participating systems fol-
low this pattern of assigning labels to ?all and only? the entity
pairs identified in the test data, i.e. some systems decide which
entity pairs to label, as well as which label to assign. Accord-
ingly, the performance results given in (Verhagen et al, 2007)
are reported using metrics of precision, recall and F-measure.
4This was possible because both the trial and training data
were derived from TimeBank.
able for use with the TempEval tasks, in the hope
that this would allow us to determine if this informa-
tion would be useful without first facing the cost of
developing SIGNAL and SLINK recognisers. Re-
garding SIGNALs, however, we ran into the prob-
lem that there are many TLINKs in the TempEval
data for which no corresponding TLINK appears
in TimeBank, and hence for which SIGNAL infor-
mation could not be imported. We were unable to
progress this work sufficiently in the time available
for there to be any useful results to report here.
5 Conclusion
We have explored using a ML-based approach to
the TempEval tasks, which does not rely on the use
of deeper NLP-analysis components. We observe
that although some other systems in the competi-
tion have produced higher scores for the tasks, the
score differences are relatively small. In the course
of this work, we have made some interesting ob-
servations regarding the performance variability of
different ML algorithms when applied to the diffent
TempEval tasks, and regarding the features that con-
tribute to the system?s performance.
References
B. Boguraev and R. Kubota Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. In
Proceedings of IJCAI-05, pages 997?1003.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In ACL ?06: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 753?760, Morristown, NJ, USA. Association for
Computational Linguistics.
J. Pustejovsky, D. Day, L. Ferro, R. Gaizauskas, P. Hanks,
M. Lazo, D. Radev, R. Saur??, A. See, A. Setzer, and
B. Sundheim. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. SemEval-2007
Task 15: TempEval Temporal Relation Identification.
In Proceedings of SemEval-2007: 4th International
Workshop on Semantic Evaluations.
I.H. Witten and E. Frank, editors. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, second edition.
441
 	
A Large Scale Terminology Resource for Biomedical Text Processing
Henk Harkema, Robert Gaizauskas, Mark Hepple, Angus Roberts,
Ian Roberts, Neil Davis, Yikun Guo
Department of Computer Science, University of Sheffield, UK
biomed@dcs.shef.ac.uk
Abstract
In this paper we discuss the design, implemen-
tation, and use of Termino, a large scale termi-
nological resource for text processing. Dealing
with terminology is a difficult but unavoidable
task for language processing applications, such
as Information Extraction in technical domains.
Complex, heterogeneous information must be
stored about large numbers of terms. At the
same time term recognition must be performed
in realistic times. Termino attempts to recon-
cile this tension by maintaining a flexible, ex-
tensible relational database for storing termino-
logical information and compiling finite state
machines from this database to do term look-
up. While Termino has been developed for
biomedical applications, its general design al-
lows it to be used for term processing in any
domain.
1 Introduction
It has been widely recognized that the biomedical litera-
ture is now so large, and growing so quickly, that it is be-
coming increasingly difficult for researchers to access the
published results that are relevant to their research. Con-
sequently, any technology that can facilitate this access
should help to increase research productivity. This has
led to an increased interest in the application of natural
language processing techniques for the automatic capture
of biomedical content from journal abstracts, complete
papers, and other textual documents (Gaizauskas et al,
2003; Hahn et al, 2002; Pustejovsky et al, 2002; Rind-
flesch et al, 2000).
An essential processing step in these applications is
the identification and semantic classification of techni-
cal terms in text, since these terms often point to enti-
ties about which information should be extracted. Proper
semantic classification of terms also helps in resolving
anaphora and extracting relations whose arguments are
restricted semantically.
1.1 Challenge
Any technical domain generates very large numbers of
terms ? single or multiword expressions that have some
specialised use or meaning in that domain. For exam-
ple, the UMLS Metathesaurus (Humphreys et al, 1998),
which provides a semantic classification of terms from a
wide range of vocabularies in the clinical and biomedical
domain, currently contains well over 2 million distinct
English terms.
For a variety of reasons, recognizing these terms in
text is not a trivial task. First of all, terms are often
long multi-token sequences, e.g. 3-methyladenine-DNA
glycosylase I. Moreover, since terms are referred to re-
peatedly in discourses there is a benefit in their being
short and unambiguous, so they are frequently abbre-
viated and acronymized, e.g. CvL for chromobacterium
viscosum lipase. However, abbreviations may not al-
ways occur together with their full forms in a text, the
method of abbreviation is not predictable in all cases, and
many three letter abbreviations are highly overloaded.
Terms are also subject to a high degree of orthographic
variation as a result of the representation of non-Latin
characters, e.g. a-helix vs. alpha-helix, capitalization,
e.g. DNA vs. dna, hyphenation, e.g. anti-histamine vs. an-
tihistamine, and British and American spelling variants,
e.g. tumour vs. tumor. Furthermore, biomedical science
is a dynamic field: new terms are constantly being in-
troduced while old ones fall into disuse. Finally, certain
classes of biomedical terms exhibit metonomy, e.g. when
a protein is referred to by the gene that expresses it.
To begin to address these issues in term recognition, we
are building a large-scale resource for storing and recog-
nizing technical terminology, called Termino. This re-
source must store complex, heterogeneous information
about large numbers of terms. At the same time term
recognition must be performed in realistic times. Ter-
mino attempts to reconcile this tension by maintaining a
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 53-60.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
flexible, extensible relational database for storing termi-
nological information and compiling finite state machines
from this database to do term look-up.
1.2 Context
Termino is being developed in the context of two ongoing
projects: CLEF, for Clinical E-Science Framework (Rec-
tor et al, 2003) and myGrid (Goble et al, 2003). Both
these projects involve an Information Extraction compo-
nent. Information Extraction is the activity of identifying
pre-defined classes of entities and relationships in natural
language texts and storing this information in a structured
format enabling rapid and effective access to the informa-
tion, e.g. Gaizauskas and Wilks (1998), Grishman (1997).
The goal of the CLEF project is to extract information
from patient records regarding the treatment of cancer.
The treatment of cancer patients may extend over several
years and the resulting clinical record may include many
documents, such as clinic letters, case notes, lab reports,
discharge summaries, etc. These documents are gener-
ally full of medical terms naming entities such as body
parts, drugs, problems (i.e. symptoms and diseases), in-
vestigations and interventions. Some of these terms are
particular to the hospital from which the document origi-
nates. We aim to identify these classes of entities, as well
as relationships between such entities, e.g. that an investi-
gation has indicated a particular problem, which, in turn,
has been treated with a particular intervention. The infor-
mation extracted from the patient records is potentially of
value for immediate patient care, but can also be used to
support longitudinal and epidemiological medical stud-
ies, and to assist policy makers and health care managers
in regard to planning and clinical governance.
The myGrid project aims to present research biolo-
gists with a unified workbench through which component
bioinformatic services can be accessed using a workflow
model. These services may be remotely located from the
user and will be exploited via grid or web-service chan-
nels. A text extraction service will form one of these ser-
vices and will facilitate access to information in the sci-
entific literature. This text service comprises an off-line
and an on-line component. The off-line component in-
volves pre-processing a large biological sciences corpus,
in this case the contents of Medline, in order to identify
various biological entities such as genes, enzymes, and
proteins, and relationships between them such as struc-
tural and locative relations. These entities and relation-
ships are referred to in Medline abstracts by a very large
number of technical terms and expressions, which con-
tributes to the complexity of processing these texts. The
on-line component supports access to the extracted infor-
mation, as well as to the raw texts, via a SOAP interface
to an SQL database.
Despite the different objectives for text extraction
within the CLEF and myGrid projects, many of the tech-
nical challenges they face are the same, such as the
need for extensive capabilities to recognize and classify
biomedical entities as described using complex techni-
cal terminology in text. As a consequence we are con-
structing a general framework for the extraction of infor-
mation from biomedical text: AMBIT, a system for ac-
quiring medical and biological information from text. An
overview of the AMBIT logical architecture is shown in
figure 1.
The AMBIT system contains several engines, of which
Termino is one. The Information Extraction Engine pulls
selected information out of natural language text and
pushes this information into a set of pre-defined tem-
plates. These are structured objects which consists of one
or more slots for holding the extracted entities and rela-
tions. The Query Engine allows users to access informa-
tion through traditional free text search and search based
on the structured information produced by the Informa-
tion Extraction Engine, so that queries may refer to spe-
cific entities and classes of entities, and specific kinds of
relations that are recognised to hold between them. The
Text Indexing Engine is used to index text and extracted,
structured information for the purposes of information re-
trieval. The AMBIT system contains two further compo-
nents: an interface layer, which provides a web or grid
channel to allow user and program access to the system;
and a database which holds free text and structured infor-
mation that can be searched through the Query Engine.
Termino interacts with the Query Engine and the Text
Indexing Engine to provide terminological support for
query formulation and text indexation. It also provides
knowledge for the Information Extraction Engine to use
in identifying and classifying biomedical entities in text.
The Terminology Engine can furthermore be called by
users and remote programs to access information from
the various lexical resources that are integrated in the ter-
minological database.
2 Related Work
Since identification and classification of technical terms
in biomedical text is an essential step in information
extraction and other natural language processing tasks,
most natural language processing systems contain a
terminological resource of some sort. Some systems
make use of existing terminological resources, notably
the UMLS Metathesaurus, e.g. Rindflesch et al (2000),
Pustejovski et al (2002); other systems rely on re-
sources that have been specifically built for the applica-
tion, e.g. Humphreys et al (2000), Thomas et al (2000).
The UMLS Metathesaurus provides a semantic classi-
fication of terms drawn from a wide range of vocabularies
in the clinical and biomedical domain (Humphreys et al,
1998). It does so by grouping strings from the source vo-
from Hospital 1
Clinical Records
Journals
On?line
Abstracts
Medline
Literature
Biomedical
Engine
Indexing
Text
Engine
Extraction
...
(Termino)
Engine
Terminology
Ambit
from Hospital 2
Clinical Records
Web GRID
Interface layer
Raw text
(entities / relations)
Structured InfoFree text
  search
Engine
Query
Information
SOAP /
     HTTP
& Annotations
Structured Info
Figure 1: AMBIT Architecture
cabularies that are judged to have the same meaning into
concepts, and mapping these concepts onto nodes or se-
mantic types in a semantic network. Although the UMLS
Metathesaurus is used in a number of biomedical natural
language processing applications, we have decided not to
adopt the UMLS Metathesaurus as the primary terminol-
ogy resource in AMBIT for a variety of reasons.
One of the reasons for this decision is that the Metathe-
saurus is a closed system: strings are classified in terms
of the concepts and the semantic types that are present
in the Metathesaurus and the semantic network, whereas
we would like to be able to link our terms into multi-
ple ontologies, including in-house ontologies that do not
figure in any of the Metathesaurus? source vocabularies
and hence are not available through the Metathesaurus.
Moreover, we would also like to be able to have access to
additional terminological information that is not present
in the Metathesaurus, such as, for example, the annota-
tions in the Gene Ontology (The Gene Ontology Con-
sortium, 2001) assigned to a given human protein term.
While the terms making up the the tripartite Gene On-
tology are present in the UMLS Metathesaurus, assign-
ments of these terms to gene products are not recorded
in the Metathesaurus. Furthermore, as new terms appear
constantly in the biomedical field we would like to be
able to instantly add these to our terminological resource
and not have to wait until they have been included in the
UMLS Metathesaurus. Additionally, some medical terms
appearing in patient notes are hospital-specific and are
unlikely to be included in the Metathesaurus at all.
With regard to systems that do not use the UMLS
Metathesaurus, but rather depend on terminological re-
sources that have been specifically built for an applica-
tion, we note that these terminological resources tend to
be limited in the following two respects. First, the struc-
ture of these resources is often fixed and in some cases
amounts to simple gazetteer lists. Secondly, because of
their fixed structure, these resources are usually popu-
lated with content from just a few sources, leaving out
many other potentially interesting sources of terminolog-
ical information.
Instead, we intend for Termino to be an exten-
sible resource that can hold diverse kinds of termi-
nological information. The information in Termino
is either imported from existing, outside knowledge
sources, e.g. the Enzyme Nomenclature (http://www.
chem.qmw.ac.uk/iubmb/enzyme/), the Structural Classi-
fication of Proteins database (Murzin et al, 1995), and
the UMLS Metathesaurus, or it is induced from on-line
raw text resources, e.g. Medline abstracts. Termino thus
provides uniform access to terminological information
aggregated across many sources. Using Termino re-
moves the need for multiple, source-specific terminolog-
ical components within text processing systems that em-
ploy multiple terminological resources.
3 Architecture
Termino consists of two components: a database holding
terminological information and a compiler for generating
term recognizers from the contents of the database. These
two components will be discussed in the following two
sections.
STRINGS
string str id
. . . . . .
neurofibromin str728
abdomen str056
mammectomy str176
mastectomy str183
. . . . . .
TERMOID STRINGS
trm id str id
. . . . . .
trm023 str056
trm656 str056
trm924 str728
trm369 str728
trm278 str176
trm627 str183
. . . . . .
PART OF SPEECH
trm id pos
. . . . . .
trm023 N
. . . . . .
SYNONYMY
syn id trm id scl id
. . . . . . . . .
syn866 trm278 syn006
syn435 trm627 syn006
. . . . . . . . .
GO ANNOTATIONS
trm id annotation version
. . . . . . . . .
trm924 GO:0004857 9/2003
trm369 GO:0008285 9/2003
. . . . . . . . .
UMLS
trm id cui lui sui version
. . . . . . . . . . . . . . .
trm278 C0024881 L0024669 S0059711 2003AC
trm656 C0000726 L0000726 S0414154 2003AC
. . . . . . . . . . . . . . .
Figure 2: Structure of the terminological database
3.1 Terminological Database
The terminological database is designed to meet three re-
quirements. First of all, it must be capable of storing large
numbers of terms. As we have seen, the UMLS Metathe-
saurus contains over 2 million distinct terms. However,
as UMLS is just one of many resources whose terms may
need to be stored, many millions of terms may need to
be stored in total. Secondly, Termino?s database must
also be flexible enough to hold a variety of information
about terms, including information of a morpho-syntactic
nature, such as part of speech and morphological class;
information of a semantic nature, such as quasi-logical
form and links to concepts in ontologies; and provenance
information, such as the sources of the information in the
database. The database will also contain links to connect
synonyms and morphological and orthographic variants
to one another and to connect abbreviations and acronyms
to their full forms. Finally, the database must be orga-
nized in such a way that it allows for fast and efficient
recognition of terms in text.
As mentioned above, the information in Termino?s
database is either imported from existing, outside knowl-
edge sources or induced from text corpora. Since these
sources are heterogeneous in both information content
and format, Termino?s database is ?extensional?: it stores
strings and information about strings. Higher-order con-
cepts such as ?term? emerge as the result of interconnec-
tions between strings and information in the database.
The database is organized as a set of relational tables,
each storing one of the types of information mentioned
above. In this way, new information can easily be in-
cluded in the database without any global changes to the
structure of the database.
Terminological information about any given string is
usually gathered from multiple sources. As information
about a string accumulates in the database, we must make
sure that co-dependencies between various pieces of in-
formation about the string are preserved. This considera-
tion leads to the fundamental element of the terminologi-
cal database, a termoid. A termoid consists of a string to-
gether with associated information of various kinds about
the string. Information in one termoid holds conjunc-
tively for the termoid?s string, while multiple termoids
for the same string express disjunctive alternatives.
For instance, taking an example from UMLS, we may
learn from one source that the string cold as an adjective
refers to a temperature, whereas another source may tell
us that cold as a noun refers to a disease. This informa-
tion is stored in the database as two termoids: abstractly,
?cold, adjective, temperature? and ?cold, noun, disease?.
A single termoid ?cold, adjective, noun, temperature, dis-
ease? would not capture the co-dependency between the
part of speech and the ?meaning? of cold.1 This example
illustrates that a string can be in more than one termoid.
1Note that the UMLS Metathesaurus has no mechanism for
storing this co-dependency between grammatical and semantic
information.
Each termoid, however, has one and only one string.
Figure 2 provides a detailed example of part of the
structure of the terminological database. In the table
STRINGS every unique string is assigned a string iden-
tifier (str id). In the table TERMOID STRINGS each string
identifier is associated with one or more termoid iden-
tifiers (trm id). These termoid identifiers then serve as
keys into the tables holding terminological information.
Thus, in this particular example, the database includes
the information that in the Gene Ontology the string
neurofibromin has been assigned the terms with identi-
fiers GO:0004857 and GO:0008285. Furthermore, in the
UMLS Metathesaurus version 2003AC, the string mam-
mectomy has been assigned the concept-unique identifier
C0024881 (CUI), the lemma-unique identifier L0024669
(LUI), and the string-unique identifier S0059711 (SUI).
Connections between termoids such as those arising
from synonymy and orthographic variation are recorded
in another set of tables. For example, the table SYN-
ONYMY in figure 2 indicates that termoids 278 and
627 are synonymous, since they have the same syn-
onymy class identifier (scl id).2 The synonymy identifier
(syn id) identifies the assignment of a termoid to a partic-
ular synonymy class. This identifier is used to record the
source on which the assignment is based. This can be a
reference to a knowledge source from which synonymy
information has been imported into Termino, or a refer-
ence to both an algorithm by which and a corpus from
which synonyms have been extracted. Similarly there are
tables containing provenance information for strings, in-
dexed by str id, and termoids, indexed by trm id. These
tables are not shown in he example.
With regard to the first requirement for the design of
the terminological database mentioned at the beginning
of this section ? scalability ?, an implementation of Ter-
mino in MySQL has been loaded with 427,000 termoids
for 363,000 strings (see section 4 for more details). In it
the largest table, STRINGS, measures just 16MB, which is
nowhere near the default limit of 4GB that MySQL im-
poses on the size of tables. Hence, storing a large num-
ber of terms in Termino is not a problem size-wise. The
second requirement, flexibility of the database, is met by
distributing terminological information over a set of rela-
tively small tables and linking the contents of these tables
to strings via termoid identifiers. In this way we avoid the
strictures of any one fixed representational scheme, thus
making it possible for the database to hold information
from disparate sources. The third requirement on the de-
sign of the database, efficient recognition of terms, will
2The function of synonymy class identifiers in Termino is
similar to the function of CUIs in the UMLS Metathesaurus.
However, since we are not bound to a classification into UMLS
CUIs, we can assert synonymy between terms coming from ar-
bitrary sources.
be addressed in the next section.
3.2 Term Recognition
To ensure fast term recognition with Termino?s vast ter-
minological database, the system comes equipped with
a compiler for generating finite state machines from the
strings in the terminological database discussed in the
previous section. Direct look-up of strings in the database
is not an option, because it is unknown in advance at
which positions in a text terms will start and end. In order
to be complete, one would have to look up all sequences
of words or tokens in the text, which is very inefficient.
Compilation of a finite state recognizer proceeds in
the following way. First, each string in the database is
broken into tokens, where a token is either a contigu-
ous sequence of alpha-numeric characters or a punctu-
ation symbol. Next, starting from a single initial state, a
path through the machine is constructed, using the tokens
of the string to label transitions. For example, for the
string Graves? disease the machine will include a path
with transitions on Graves, ?, and disease. New states are
only created when necessary. The state reached on the fi-
nal token of a string will be labeled final and is associated
with the identifiers of the termoids for that string.
To recognize terms in text, the text is tokenized and the
finite state machine is run over the text, starting from the
initial state at each token in the text. For each sequence
of tokens leading to a final state, the termoid identifiers
associated with that state are returned. These identifiers
are then used to access the terminological database and
retrieve the information contained in the termoids. Where
appropriate the machine will produce multiple termoid
identifiers for strings. It will also recognize overlapping
and embedded strings.
Figure 3 shows a small terminological database and a
finite state recognizer derived from it. Running this rec-
ognizer over the phrase . . . thyroid dysfunction, such as
Graves? disease . . . produces four annotations: thyroid
is assigned the termoid identifiers trm1 and trm2; thyroid
dysfunction, trm3; and Graves? disease, trm4.
It should be emphasised at this point that term recog-
nition as performed by Termino is in fact term look-up
and not the end point of term processing. Term look-up
might return multiple possible terms for a given string,
or for overlapping strings, and subsequent processes may
apply to filter these alternatives down to the single option
that seems most likely to be correct in the given context.
Furthermore, more flexible processes of term recognition
might apply over the results of look-up. For example, a
term grammar might be provided for a given domain, al-
lowing longer terms to be built from shorter terms that
have been identified by term look-up.
The compiler can be parameterized to produce finite
state machines that match exact strings only, or that ab-
STRINGS
string str id
thyroid str12
thyroid disfunction str15
Graves? disease str25
TERMOID STRINGS
trm id str id
trm1 str12
trm2 str12
trm3 str15
trm4 str25
? trm4disease
thyroid
Graves
trm3
trm2
trm1
disfunction
Figure 3: Sample terminological database and finite state term recognizer
stract away from morphological and orthographical vari-
ation. At the moment, morphological information about
strings is supplied by a component outside Termino. In
our current term recognition system, this component ap-
plies to a text before the recognition process and asso-
ciates all verbs and nouns with their base form. Similarly,
the morphological component applies to the strings in the
terminological database before the compilation process.
The set-up in which term recognizers are compiled
from the contents of the terminological database turns
Termino into a general terminological resource which is
not restricted to any single domain or application. The
database can be loaded with terms from multiple domains
and compilation can be restricted to particular subsets of
strings by selecting termoids from the database based on
their source, for example. In this way one can produce
term recognizers that are tailored towards specific do-
mains or specific applications within domains.
4 Implementation & Performance
A first version of Termino has been implemented. It uses
a database implemented in MySQL and currently con-
tains over 427,000 termoids for around 363,000 strings.
Content has been imported from various sources by
means of source-specific scripts for extracting relevant
information from sources and a general script for load-
ing this extracted information into Termino. More specif-
ically, to support information extraction from patient
records, we have included in Termino strings from the
UMLS Metathesaurus falling under the following seman-
tic types: pharmacologic substances, anatomical struc-
tures, therapeutic procedure, diagnostic procedure, and
several others. We have also loaded a list of hu-
man proteins and their assignments to the Gene Ontol-
ogy as produced by the European Bioinformatics Insti-
tute (http://www.ebi.ac.uk/GOA/) into Termino. Further-
more, we have included several gazetteer lists containing
terms in the fields of molecular biology and pharmacol-
ogy that were assembled for previous information extrac-
tion projects in our NLP group. A web services (SOAP)
API to the database is under development. We plan to
make the resource available to researchers as a web ser-
vice or in downloadable form.3
The compiler to construct finite state recognizers from
the database is fully implemented, tested, and integrated
into AMBIT. The compiled recognizer for the 363,000
strings of Termino has 1.2 million states and an on-disk
size of around 80MB. Loading the matcher from disk
into memory requires about 70 seconds (on an UltraSparc
900MHz), but once loaded recognition is a very fast pro-
cess. We have been able to annotate a corpus of 114,200
documents, drawn from electronic patient records from
the Royal Marsden NHS Trust in London and each ap-
proximately 1kB of text, in approximately 44 hours ? an
average rate of 1.4 seconds per document, or 42 docu-
ments per minute. On average, about 30 terms falling un-
der the UMLS ?clinical? semantic types mentioned above
were recognized in each document. We are currently an-
notating a bench-mark corpus in order to obtain precision
and recall figures. We are also planning to compile rec-
ognizers for differently sized subsets of the terminologi-
cal database and measure their recognition speed over a
given collection of texts. This will provide some indica-
tion as to the scalability of the system.
Since Termino currently contains many terms imported
from the UMLS Metathesaurus, it is interesting to com-
pare its term recognition performance against the per-
formance of MetaMap. MetaMap is a program avail-
able from at the National Library of Medicine ? the de-
velopers of UMLS ? specifically designed to discover
UMLS Metathesaurus concepts referred to in text (Aron-
son, 2001). An impressionistic comparison of the per-
formance of Termino and MetaMap on the CLEF patient
records shows that the results differ in two ways. First,
MetaMap recognizes more terms than Termino. This
is simply because MetaMap draws on a comprehensive
version of UMLS, whereas Termino just contains a se-
lected subset of the strings in the Metathesaurus. Sec-
ondly, MetaMap is able to recognize variants of terms,
e.g. it will map the verb to treat and its inflectional forms
onto the term treatment, whereas Termino currently does
not do this. To recognize term variants MetaMap re-
lies on UMLS?s SPECIALIST lexicon, which provides
3Users may have to sign license agreements with third par-
ties in order to be able to use restricted resources that have been
integrated into Termino.
syntactic, morphological, and orthographic information
for many of the terms occurring in the Metathesaurus.
While the performance of both systems differs in favor
of MetaMap, it is important to note that the source of
these differences is unrelated to the actual design of Ter-
mino?s terminological database or Termino?s use of fi-
nite state machines to do term recognition. Rather, the
divergence in performance follows from a difference in
breadth of content of both systems at the moment. With
regard to practical matters, the comparison showed that
term recognition with Termino is much faster than with
MetaMap. Also, compiling a finite state recognizer from
the terminological database in Termino is a matter of min-
utes, whereas setting up MetaMap can take several hours.
However, since MetaMap?s processing is more involved
than Termino?s, e.g. MetaMap parses the input first, and
hence requires more resources, these remarks should be
backed up with a more rigorous comparison between Ter-
mino and MetaMap, which is currently underway.
The advantage of term recognition with Termino over
MetaMap and UMLS or any other recognizer with a sin-
gle source, is that it provides immediate entry points
into a variety of outside ontologies and other knowledge
sources, making the information in these sources avail-
able to processing steps subsequent to term recognition.
For example, for a gene or protein name recognized in a
text, Termino will return the database identifiers of this
term in the HUGO Nomenclature database (Wain et al,
2002) and the OMIM database (Online Mendelian Inher-
itance in Man, OMIM (TM), 2000). These identifiers
give access to the information stored in these databases
about the gene or protein, including alternative names,
gene map locus, related disorders, and references to rele-
vant papers.
5 Conclusions & Future Work
Dealing with terminology is an essential step in natural
language processing in technical domains. In this paper
we have described the design, implementation, and use of
Termino, a large scale terminology resource for biomedi-
cal language processing.
Termino includes a relational database which is de-
signed to store a large number of terms together with
complex, heterogeneous information about these terms,
such as morpho-syntactic information, links to concepts
in ontologies, and other kinds of annotations. The
database is also designed to be extensible: it is easy to
include terms and information about terms found in out-
side biological databases and ontologies. Term look-up
in text is done via finite state machines that are compiled
from the contents of the database. This approach allows
the database to be very rich without sacrificing speed at
look-up time. These three features make Termino a flexi-
ble tool for inclusion in a biomedical text processing sys-
tem.
As noted in section 3.2, Termino has not been designed
to be used as a stand-alone term recognition system but
rather as the first component, the lexical look-up com-
ponent, in a multi-component term processing system.
Since Termino may return multiple terms for a given
string, or for overlapping strings, some post-filtering of
these alternatives is necessary. Secondly, it is likely that
better term recognition performance will be obtained by
supplementing Termino look-up with a term parser which
uses a grammar to give a term recognizer the generative
capacity to recognize previously unseen terms. For ex-
ample, many terms for chemical compounds conform to
grammars that allow complex terms to be built out of sim-
pler terms prefixed or suffixed with numerals separated
from the simpler term with hyphens. It does not make
sense to attempt to store in Termino all of these variants.
Termino provides a firm basis on which to build large-
scale biomedical text processing applications. However,
there are a number of directions where further work can
be done. First, as noted in 3.2, morphological informa-
tion is currently not held in Termino, but rather resides
in an external morphological analyzer. We are working
to extend the Termino data model to enable information
about morphological variation to be stored in Termino,
so that Termino serves as a single source of information
for the terms it contains. Secondly, we are working to
build term induction modules to allow Termino content
to be automatically acquired from corpora, in addition
to deriving it from manually created resources such as
UMLS. Finally, while we have already incorporated Ter-
mino into the AMBIT system where it collaborates with
a term parser to perform more complete term recogni-
tion, more work can be done to with respect to such an
integration. For example, probabilities could be incorpo-
rated into Termino to assist with probabilistic parsing of
terms; or, issues of trade-off between what should be in
the term lexicon versus the term grammar could be fur-
ther explored by looking to see which compound terms
in the lexicon contain other terms as substrings and at-
tempt to abstract away from these to grammar rules. For
example, in the example thyroid disfunction above, both
thyroid and disfunction are terms, the first of class ?body
part?, the second of class ?problem?. Their combination
thyroid disfunction is a term of class ?problem?, suggest-
ing a rule of the form ?problem?   ?body part? ?problem?.
References
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of the American Medical Infor-
matics Association Symposium, pages 17?21.
R. Gaizauskas and Y. Wilks. 1998. Information extrac-
tion: Beyond document retrieval. Journal of Docu-
mentation, 54(1):70?105.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Protein structures and information extrac-
tion from biological texts: The PASTA system. Jour-
nal of Bioinformatics, 19(1):135?143.
C.A. Goble, C.J. Wroe, R. Stevens, and the my-
Grid consortium. 2003. The myGrid project:
Services, architecture and demonstrator. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
R. Grishman. 1997. Information extraction: Techniques
and challenges. In Maria Teresa Pazienza, editor, In-
formation Extraction, pages 10?27. Springer Verlag.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: the
medSynDiKATe text mining system. In Proceedings
of the Pacific Symposium on Biocomputing, pages 338?
349.
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman, and
G.O. Barnett. 1998. The Unified Medical Language
System: An informatics research collaboration. Jour-
nal of the American Medical Informatics Association,
1(5):1?13.
K. Humphreys, G. Demetriou, and R. Gaizauskas. 2000.
Two applications of information extraction to biolog-
ical science journal articles: Enzyme interactions and
protein structures. In Proceedings of the Pacific Sym-
posium on Biocomputing, pages 505?516.
A.G. Murzin, S.E. Brenner, T. Hubbard, and C. Chothia.
1995. SCOP: A structural classification of proteins
database for the investigation of sequences and struc-
tures. Journal of Molecular Biology, (247):536?540.
(http://scop.mrc-lmb.cam.ac.uk/scop/).
Online Mendelian Inheritance in Man, OMIM (TM).
2000. McKusick-Nathans Institute for Genetic
Medicine, Johns Hopkins University (Baltimore, MD)
and National Center for Biotechnology Informa-
tion, National Library of Medicine (Bethesda, MD).
http://www.ncbi.nlm.nih.gov/omim/.
J. Pustejovsky, J. Castan?o, R. Saur??, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the Workshop on Natural
Language Processing in the Biomedical Domain, As-
sociation for Computational Linguistics 40th Anniver-
sary Meeting (ACL-02), pages 85?92.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, R. Gaizauskas, M. Hepple, D. Scott,
and R. Power. 2003. Joining up health care
with clinical and post-genomic research. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
C.T. Rindflesch, J.V. Rajan, and L. Hunter. 2000. Ex-
tracting molecular binding relationships from biomed-
ical text. In Proceedings of the 6th Applied Natu-
ral Language Processing conference / North American
chapter of the Association for Computational Linguis-
tics annual meeting, pages 188?915.
The Gene Ontology Consortium. 2001. Creating the
gene ontology resource: design and implementation.
Genome Research, 11(8):1425?1433.
J. Thomas, D. Milward, C. Ouzounis, and S. Pulman.
2000. Automatic extraction of protein interactions
from scientific abstracts. In Proceedings of the Pacific
Symposium on Biocomputing, pages 538?549.
H.M. Wain, M. Lush, F. Ducluzeau, and S. Povey.
2002. Genew: The human nomenclature
database. Nucleic Acids Research, 30(1):169?171.
(http://www.gene.ucl.ac.uk/nomenclature/).
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 200?201,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
SUPPLE: A Practical Parser for Natural Language Engineering
Applications
Robert Gaizauskas, Mark Hepple, Horacio Saggion,
Mark A. Greenwood and Kevin Humphreys?
Department of Computer Science
University of Sheffield, Sheffield, UK
{robertg|hepple|saggion|m.greenwood|-}@dcs.shef.ac.uk
Abstract
We describe SUPPLE, a freely-available,
open source natural language parsing sys-
tem, implemented in Prolog, and designed
for practical use in language engineering
(LE) applications. SUPPLE can be run as
a stand-alone application, or as a compo-
nent within the GATE General Architec-
ture for Text Engineering. SUPPLE is dis-
tributed with an example grammar that has
been developed over a number of years
across several LE projects. This paper de-
scribes the key characteristics of the parser
and the distributed grammar.
1 Introduction
In this paper we describe SUPPLE1 ? the Sheffield
University Prolog Parser for Language Engineering
? a general purpose parser that produces both syn-
tactic and semantic representations for input sen-
tences, which is well-suited for a range of LE ap-
plications. SUPPLE is freely available, and is dis-
tributed with an example grammar for English that
was developed across a number of LE projects. We
will describe key characteristics of the parser and the
grammar in turn.
2 The SUPPLE Parser
SUPPLE is a general purpose bottom-up chart parser
for feature-based context free phrase structure gram-
?At Microsoft Corporation since 2000 (Speech and Natural
Language Group). Email: kevinhum@microsoft.com.
1In previous published materials and in the current GATE
release the parser is referred to as buChart. This is name is now
deprecated.
mars (CF-PSGs), written in Prolog, that has a num-
ber of characteristics making it well-suited for use
in LE applications. It is available both as a language
processing resource within the GATE General Ar-
chitecture for Text Engineering (Cunningham et al,
2002) and as a standalone program requiring vari-
ous preprocessing steps to be applied to the input.
We will here list some of its key characteristics.
Firstly, the parser allows multiword units identi-
fied by earlier processing components, e.g. named
entity recognisers (NERs), gazetteers, etc, to be
treated as non-decomposable units for syntactic pro-
cessing. This is important as the identification of
such items is an essential part of analyzing real text
in many domains.
The parser allows a layered parsing process, with
a number of separate grammars being applied in se-
ries, one on top of the other, with a ?best parse? se-
lection process between stages so that only a sub-
set of the constituents constructed at each stage is
passed forward to the next. While this may make
the parsing process incomplete with respect to the
total set of analyses licensed by the grammar rules,
it makes the parsing process much more efficient and
allows a modular development of sub-grammars.
Facilities are provided to simplify handling
feature-based grammars. The grammar representa-
tion uses flat, i.e. non-embedded, feature represen-
tations which are combined used Prolog term uni-
fication for efficiency. Features are predefined and
source grammars compiled into a full form repre-
sentation, allowing grammar writers to include only
relevant features in any rule, and to ignore feature or-
dering. The formalism also permits disjunctive and
optional right-hand-side constituents.
The chart parsing algorithm is simple but very
200
efficient, exploiting the characteristics of Prolog to
avoid the need for active edges or an agenda. In in-
formal testing, this approach was roughly ten times
faster than a related Prolog implementation of stan-
dard bottom-up active chart parsing.
The parser does not fail if full sentential parses
cannot be found, but instead outputs partial anal-
yses as syntactic and semantic fragments for user-
selectable syntactic categories. This makes the
parser robust in applications which deal with large
volumes of real text.
3 The Sample Grammar
The sample grammar distributed with SUPPLE has
been developed over several years, across a number
LE projects. We here list some key characteristics.
The morpho-syntactic and semantic information
required for individual lexical items is minimal ?
inflectional root and word class only, where the word
class inventory is basically the PTB tagset.
A conservative philosophy is adopted regarding
identification of verbal arguments and attachment of
nominal and verbal post-modifiers, such as preposi-
tional phrases and relative clauses. Rather than pro-
ducing all possible analyses or using probabilities to
generate the most likely analysis, the preference is to
offer a single analysis that spans the input sentence
only if it can be relied on to be correct, so that in
many cases only partial analyses are produced. The
philosophy is that it is more useful to produce par-
tial analyses that are correct than full analyses which
may well be wrong or highly disjunctive. Output
from the parser can be passed to further processing
components which may bring additional information
to bear in resolving attachments.
An analysis of verb phrases is adopted in which
a core verb cluster consisting of verbal head plus
auxiliaries and adverbials is identified before any at-
tempt to attach any post-verbal arguments. This con-
trasts with analyses where complements are attached
to the verbal head at a lower level than auxiliaries
and adverbials, e.g. as in the Penn TreeBank. This
decision is again motivated by practical concerns: it
is relatively easy to recognise verbal clusters, much
harder to correctly attach complements.
A semantic analysis, or simplified quasi-logical
form (SQLF), is produced for each phrasal con-
stituent, in which tensed verbs are interpreted as re-
ferring to unique events, and noun phrases as refer-
ring to unique objects. Where relations between syn-
tactic constituents are identified in parsing, semantic
relations between associated objects and events are
asserted in the SQLF.
While linguistically richer grammatical theories
could be implemented in the grammar formalism
of SUPPLE, the emphasis in our work has been on
building robust wide-coverage tools ? hence the re-
quirement for only minimal lexical morphosyntac-
tic and semantic information. As a consequence the
combination of parser and grammars developed to
date results in a tool that, although capable of return-
ing full sentence analyses, more commonly returns
results that include chunks of analysis with some,
but not all, attachment relations determined.
4 Downloading SUPPLE Resources
SUPPLE resources, including source code and the
sample grammar, and also a longer paper providing
a more detailed account of both the parser and gram-
mar, are available from the supple homepage at:
http://nlp.shef.ac.uk/research/supple
5 Conclusion
The SUPPLE parser has served as a component in
numerous LE research projects, and is currently in
use in a Question Answering system which partic-
ipated in recent TREC/QA evaluations. We hope
its availability as a GATE component will facilitate
its broader use by NLP researchers, and by others
building applications exploiting NL technology.
Acknowledgements
The authors would like to acknowledge the sup-
port of the UK EPSRC under grants R91465 and
K25267, and also the contributions of Chris Huyck
and Sam Scott to the parser code and grammars.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. GATE: A framework and graphical devel-
opment environment for robust NLP tools and applica-
tions. Proceedings of the 40th Anniversary Meeting of
the Association for Computational Linguistics, 2002.
201
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 15: TempEval Temporal Relation Identification
Marc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,
Graham Katz? and James Pustejovsky?
? Brandeis University, {marc,jamesp}@cs.brandeis.edu
? University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
? Thomson Legal & Regulatory, frank.schilder@thomson.com,
? Stanford University, egkatz@stanford.edu
Abstract
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
1 Introduction
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al, 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al, 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al, 2006; Boguraev et al, forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
2 Task Description
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks ? A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
75
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified ?
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
3 Data Description and Data Preparation
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
? TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
? EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
? TLINK. This is a simplified version of the
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen?s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
76
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
4 Evaluating Temporal Relations
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
Precision = Rc/R
Recall = Rc/K
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
Precision = Rcw/R
Recall = Rcw/K
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
Table 1: Evaluation weights
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
77
5 Participants
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs ? a TIMEX3 and an EVENT
? are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI?s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
6 Results
The results for the six teams are presented in tables
2, 3, and 4.
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
Table 2: Results for Task A
78
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
Table 3: Results for Task B
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
Table 4: Results for Task C
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI?s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
elements that needed to be linked for the TempEval
task.
7 Conclusion: the Future of Temporal
Evaluation
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ?...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...?. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
79
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
8 Acknowledgements
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Saur??, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
Hageg`e and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus?cas?u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, BenWellner, Marc Verhagen, ChongMin
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
80
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting Clinical Relationships from Patient Narratives
Angus Roberts, Robert Gaizauskas, Mark Hepple
Department of Computer Science, University of Sheffield,
Regent Court, 211 Portobello, Sheffield S1 4DP
{initial.surname}@dcs.shef.ac.uk
Abstract
The Clinical E-Science Framework (CLEF)
project has built a system to extract clin-
ically significant information from the tex-
tual component of medical records, for clin-
ical research, evidence-based healthcare and
genotype-meets-phenotype informatics. One
part of this system is the identification of rela-
tionships between clinically important entities
in the text. Typical approaches to relationship
extraction in this domain have used full parses,
domain-specific grammars, and large knowl-
edge bases encoding domain knowledge. In
other areas of biomedical NLP, statistical ma-
chine learning approaches are now routinely
applied to relationship extraction. We report
on the novel application of these statistical
techniques to clinical relationships.
We describe a supervised machine learning
system, trained with a corpus of oncology nar-
ratives hand-annotated with clinically impor-
tant relationships. Various shallow features
are extracted from these texts, and used to
train statistical classifiers. We compare the
suitability of these features for clinical re-
lationship extraction, how extraction varies
between inter- and intra-sentential relation-
ships, and examine the amount of training data
needed to learn various relationships.
1 Introduction
The application of Natural Language Processing
(NLP) is widespread in biomedicine. Typically, it
is applied to improve access to the ever-burgeoning
research literature. Increasingly, biomedical re-
searchers need to relate this literature to pheno-
typic data: both to populations, and to individ-
ual clinical subjects. The computer applications
used in biomedical research, including NLP appli-
cations, therefore need to support genotype-meets-
phenotype informatics and the move towards trans-
lational biology. Such support will undoubtedly in-
clude linkage to the information held in individual
medical records: both the structured portion, and the
unstructured textual portion.
The Clinical E-Science Framework (CLEF)
project (Rector et al, 2003) is building a frame-
work for the capture, integration and presentation of
this clinical information, for research and evidence-
based health care. The project?s data resource is a
repository of the full clinical records for over 20000
cancer patients from the Royal Marsden Hospital,
Europe?s largest oncology centre. These records
combine structured information, clinical narratives,
and free text investigation reports. CLEF uses infor-
mation extraction (IE) technology to make informa-
tion from the textual portion of the medical record
available for integration with the structured record,
and thus available for clinical care and research. The
CLEF IE system analyses the textual records to ex-
tract entities, events and the relationships between
them. These relationships give information that is
often not available in the structured record. Why
was a drug given? What were the results of a physi-
cal examination? What problems were not present?
We have previously reported entity extraction in the
CLEF IE system (Roberts et al, 2008b). This paper
examines relationship extraction.
Extraction of relationships from clinical text is
usually carried out as part of a full clinical IE sys-
tem. Several such systems have been described.
They generally use a syntactic parse with domain-
specific grammar rules. The Linguistic String
project (Sager et al, 1994) used a full syntactic and
10
clinical sublanguage parse to fill template data struc-
tures corresponding to medical statements. These
were mapped to a database model incorporating
medical facts and the relationships between them.
MedLEE (Friedman et al, 1994), and more recently
BioMedLEE (Lussier et al, 2006) used a semantic
lexicon and grammar of domain-specific semantic
patterns. The patterns encode the possible relation-
ships between entities, allowing both entities and the
relationships between them to be directly matched
in the text. Other systems have incorporated large-
scale domain-specific knowledge bases. MEDSYN-
DIKATE (Hahn et al, 2002) employed a rich dis-
course model of entities and their relationships, built
using a dependency parse of texts and a descrip-
tion logic knowledge base re-engineered from exist-
ing terminologies. MENELAS (Zweigenbaum et al,
1995) also used a full parse, a conceptual represen-
tation of the text, and a large scale knowledge base.
In other applications of biomedical NLP, a sec-
ond paradigm has become widespread: the appli-
cation of statistical machine learning techniques to
feature-based models of the text. Such approaches
have typically been applied to journal texts. They
have been used both for entity recognition and ex-
traction of various relations, such as protein-protein
interactions (see, for example, Grover et al(2007)).
This follows on from the success of these methods
in general NLP (see for example Zhou et al(2005)).
Statistical machine learning has also been applied to
clinical text, but its use has generally been limited
to entity recognition. The Mayo Clinic text analysis
system (Pakhomov et al, 2005), for example, uses a
combination of dictionary lookup and a Na??ve Bayes
classifier to identify entities for information retrieval
applications. To the best of our knowledge, statisti-
cal methods have not been previously applied to ex-
traction of clinical relationships from text.
This paper describes experiments in the statistical
machine learning of relationships from a novel text
type: oncology narratives. The set of relationships
extracted are considered to be of interest for clinical
and research applications down line of IE, such as
querying to support clinical research. We apply Sup-
port Vector Machine (SVM) classifiers to learn these
relationships. The classifiers are trained and eval-
uated using novel data: a gold standard corpus of
clinical text, hand-annotated with semantic entities
and relationships. In order to test the applicability
of this method to the clinical domain, we train clas-
sifiers using a number of comparatively simple text
features, and look at the contribution of these fea-
tures to system performance. Clinically interesting
relationships may span several sentences, and so we
compare classifiers trained for both intra- and inter-
sentential relationships (spanning one or more sen-
tence boundaries). We also examine the influence of
training corpus size on performance, as hand anno-
tation of training data is the major expense in super-
vised machine learning.
2 Relationship Schema
Relationship Argument 1 Argument 2
has target Investigation Locus
Intervention Locus
has finding Investigation Condition
Investigation Result
has indication Drug or device Condition
Intervention Condition
Investigation Condition
has location Condition Locus
negation modifies Negation modifier Condition
laterality modifies Laterality modifier Intervention
Laterality modifier Locus
sub-location modifies Sub-location modifier Locus
Table 1: Relationship types and their argument type con-
straints.
The CLEF application extracts entities, relation-
ships and modifiers from text. By entity, we mean
some real-world thing, event or state referred to in
the text: the drugs that are mentioned, the tests that
were carried out, etc. Modifiers are words that qual-
ify an entity in some way, referring e.g. to the lat-
erality of an anatomical locus, or the negation of a
condition (?no sign of inflammation?). Entities are
connected to each other and to modifiers by rela-
tionships: e.g. linking a drug entity to the condition
entity for which it is indicated, linking an investiga-
tion to its results, or linking a negating phrase to a
condition.
The entities, modifiers, and relationships are de-
scribed by both a formal XML schema, and by a
set of detailed definitions. These were developed by
a group of clinical experts through an iterative pro-
cess, until acceptable agreement was reached. Entity
types are mapped to types from the UMLS seman-
tic network (Lindberg et al, 1993), each CLEF en-
11
tity type covering several UMLS types. Relationship
types are those that were felt necessary to capture the
essential clinical dependencies between entities re-
ferred to in patient documents, and to support CLEF
end user applications.
Each relationship type is constrained to exist be-
tween limited pairs of entity types. For example,
the has location relationship can only exist be-
tween a Condition entity and a Locus entity.
Some relationships can exist between multiple type
pairs. The full set of relationships and their argu-
ment type constraints are shown in Table 1. Ex-
amples of each relationship are given in Roberts et
al (2008a).
Some of the relationships considered important
by the clinical experts were not obvious without do-
main knowledge. For example,
He is suffering from nausea and severe
headaches. Dolasteron was prescribed.
Without domain knowledge, it is not clear that there
is a has indication relationship between the
?Dolasteron? Drug or device entity and the
?nausea? Condition entity. As in this example,
many of this type of relationship are intra-sentential.
A single real-world entity may be referred to sev-
eral times in the same text. Each of these co-
referring expressions is a mention of the entity. The
gold standard includes annotation of co-reference
between different textual mentions of the same en-
tity. For the work reported in this paper, however,
co-reference is not considered. Each entity is as-
sumed to have a single mention. Relationships be-
tween entities can be considered, by extension, as
relationships between the single mentions of those
entities. The implications of this are discussed fur-
ther below.
3 Gold Standard Corpus
The schema and definitions were used to hand-
annotate the entities and relationships in 77 oncol-
ogy narratives, to provide a gold standard for sys-
tem training and evaluation. Corpora of this size
are typical in supervised machine learning, and re-
flect the expense of hand annotation. Narratives
were carefully selected and annotated according to
a best practice methodology, as described in Roberts
et al(2008a). Narratives were annotated by two in-
dependent, clinically trained, annotators, and a con-
sensus created by a third. We will refer to this corpus
as C77.
Annotators were asked to first mark the mentions
of entities and modifiers, and then to go through
each of these in turn, deciding if any had relation-
ships with mentions of other entities. Although the
annotators were marking co-reference between men-
tions of the same entity, they were asked to ignore
this with respect to relationship annotation. Both
the annotation tool that they were using and their
annotation guidelines, enforced the creation of rela-
tionships between mentions, and not between enti-
ties. The gold standard is thus analogous to the style
of relationship extraction reported here, in which
we extract relations between single mention entities,
and do not consider co-reference. Annotators were
further told that relationships could span multiple
sentences, and that it was acceptable to use clini-
cal domain knowledge to infer that a relationship
existed between two mentions. Counts of all rela-
tionships annotated in C77 are shown in Table 2,
sub-divided by the number of sentence boundaries
spanned by a relationship.
4 Relationship Extraction
The system we have built uses the GATE NLP
toolkit (Cunningham et al, 2002) 1. The system is
shown in Figure 1, and is described below.
Narratives are first pre-processed using standard
GATE modules. Narratives were tokenised, sen-
tences found with a regular expression-based sen-
tence splitter, part-of-speech (POS) tagged, and
morphological roots found for tokens. Each to-
ken was also labelled with a generalised POS tag,
the first two characters of the full POS tag. This
takes advantage of the Penn Treebank tagset used
by GATE?s POS tagger, in which related POS tags
share the first two characters. For example, all six
verb POS tags start with the letters ?VB?.
After pre-processing, mentions of entities within
the text are annotated. In the experiments reported,
we assume perfect entity recognition, as given by
the entities in the human annotated gold standard
1We used a development build of GATE 4.0, downloadable
from http://gate.ac.uk
12
Sentence boundaries between arguments
0 1 2 3 4 5 6 7 8 9 >9 Total
has finding 265 46 25 7 5 4 3 2 2 2 0 361
has indication 139 85 35 32 14 11 6 4 5 5 12 348
has location 360 4 1 1 1 1 1 0 0 0 4 373
has target 122 14 4 2 2 4 3 1 0 1 0 153
laterality modifies 128 0 0 0 0 0 0 0 0 0 0 128
negation modifies 100 1 0 0 0 0 0 0 0 0 0 101
sub location modifies 76 0 0 0 0 0 0 0 0 0 0 76
Total 1190 150 65 42 22 20 13 7 7 8 16 1540
Cumulative total 1190 1340 1405 1447 1469 1489 1502 1509 1516 1524 1540
Table 2: Count of relationships in 77 gold standard documents.
described above. Our results are therefore higher
than would be expected in a system with automatic
entity recognition. It is useful and usual to fix en-
tity recognition in this way, to allow tuning specific
to relationship extraction, and to allow the isolation
of relation-specific problems. We accept, however,
that ultimately, relation extraction does depend on
the quality of entity recognition. The relation extrac-
tion described here is used as part of an operational
IE system in which clinical entity recognition is per-
formed by a combination of lexical lookup and su-
pervised machine learning. We have described our
entity extraction system elsewhere (Roberts et al,
2008b).
4.1 Classification
We treat clinical relationship extraction as a classi-
fication task, training classifiers to assign a relation-
ship type to an entity pair. An entity pair is a pairing
of entities that may or may not be the arguments of
a relation. For a given document, we create all pos-
sible entity pairs within two constraints. First, en-
tities that are paired must be within n sentences of
each other. For all of the work reported here, unless
stated, n ? 1 (crossing 0 or 1 sentence boundaries).
Second, we can constrain the entity pairs created
by argument type (Rindflesch and Fiszman, 2003).
For example, there is little point in creating an en-
tity pair between a Drug or device entity and
a Result entity, as no relationships, as specified
by the schema, exist between entities of these types.
Entity pairing is carried out by a GATE component
developed specifically for clinical relationship ex-
traction. In addition to pairing entities according to
the above constraints, this component also assigns
features to each pair that characterise its lexical and
syntactic qualities (described further in Section 4.2).
Entity pairs correspond to classifier training and
test instances. In classifier training, if an entity
pair corresponds to the arguments of a relationship
present in the gold standard, then it is assigned a
class of that relationship type. If it does not corre-
spond to such a relation, then it is assigned the class
null. The classifier builds a model of these entity
pair training instances, from their features. In classi-
fier application, entity pairs are created from unseen
text, under the above constraints. The classifier as-
signs one of our seven relationship types, or null,
to each entity pair.
We use Support Vector machines (SVMs) as train-
able classifiers, as these have proved to be robust and
efficient for a range of NLP tasks, including relation
extraction. We use an SVM implementation devel-
oped within our own group, and provided as part
of the GATE toolkit. This is a variant on the orig-
inal SVM algorithm, SVM with uneven margins, in
which classification may be biased towards positive
training examples. This is particularly suited to NLP
applications, in which positive training examples are
often rare. Full details of the classifier are given in
Li et al(2005). We used the implementation ?out of
the box?, with default parameters as determined in
experiments with other data sets.
SVMs are binary classifiers: the multi-class prob-
lem of classifying entity pairs must therefore be
mapped to a number of binary classification prob-
lems. There are several ways in which a multi-
class problem can be recast as binary problems. The
commonest are one-against-one in which one classi-
fier is trained for every possible pair of classes, and
one-against-all in which a classifier is trained for
a binary decision between each class and all other
13
classes, including null, combined. We have car-
ried out extensive experiments (not reported here),
with these two strategies, and have found little dif-
ference between them for our data. We have chosen
to use one-against-all, as it needs fewer classifiers
(for an n class problem, it needs n classifiers, as op-
posed to (n?1)!2 for one-against-one).
The resultant class assignments by multiple bi-
nary classifiers must be post-processed to deal with
ambiguity. In application to unseen text, it is possi-
ble that several classifiers assign different classes to
an entity pair (test instance). To disambiguate these
cases, the output of each one-against-all classifier is
transformed into a probability, and the class with
the highest probability is assigned. Re-casting the
multi-class relation problem as a number of binary
problems, and post-processing to resolve ambigui-
ties, is handled by the GATE Learning API.
Figure 1: The relationship extraction system.
4.2 Features for Classification
The SVM classification model is built from lexical
and syntactic features assigned to tokens and en-
tity pairs prior to classification. We use features
developed in part from those described in Zhou et
al (2005) and Wang et al(2006). These features are
split into 11 sets, as described in Table 3.
The tokN features are POS and surface string
taken from a window of N tokens on each side of
each paired entity?s mention. For N = 6, this
gives 48 features. The rationale behind these sim-
ple features is that there is useful information in the
words surrounding two mentions, that helps deter-
mine any relationship between them. The gentokN
features generalise tokN to use morphological root
and generalised POS. The str features are a set
of 14 surface string features, encoding the full sur-
face strings of both entity mentions, their heads,
their heads combined, the surface strings of the first,
last and other tokens between the mentions, and
of the two tokens immediately before and after the
leftmost and rightmost mentions respectively. The
pos, root, and genpos feature sets are similarly
constructed from the POS tags, roots, and gener-
alised POS tags of the entity mentions and their sur-
rounding tokens. These four feature sets differ from
tokN and gentokN, in that they provide more fine-
grained information about the position of features
relative to the paired entity mentions.
For the event feature set, the main entities
were divided into events (Investigation and
Intervention) and non-events (all others). Fea-
tures record whether the entity pair consists of two
events, two non-events, one of each, and whether
there are any intervening events and non-events.
This feature set gives similar information to atype
(semantic types of arguments) and inter (inter-
vening entities), but at a coarser level of typing.
5 Evaluation
We used a standard ten-fold cross validation
methodology and standard evaluation metrics. Met-
rics are defined in terms of true positive, false pos-
itive and false negative matches between relation-
ships in a system annotated response document and
a gold standard key document. A response relation-
ship is a true positive if a relationship of the same
type, and with the exact same arguments, exists in
the key. Corresponding definitions apply for false
positive and false negative. Counts of these matches
are used to calculate standard metrics of Recall (R),
Precision (P ) and F1 measure.
The metrics do not say how hard relationship ex-
traction is. We therefore provide a comparison with
Inter Annotator Agreement (IAA) scores from the
gold standard. The IAA score gives the agreement
between the two independent double annotators. It
is equivalent to scoring one annotator against the
other using the F1 metric. IAA scores are not di-
rectly comparable here, as relationship annotation is
14
Feature set Size Description
tokN 8N Surface string and POS of tokens surrounding the arguments, windowed ?N to +N , N = 6 by default
gentokN 8N Root and gerenalised POS of tokens surrounding the argument entities, windowed ?N to +N , N = 6 by default
atype 1 Concatenated semantic type of arguments, in arg1-arg2 order
dir 1 Direction: linear text order of the arguments (is arg1 before arg2, or vice versa?)
dist 2 Distance: absolute number of sentence and paragraph boundaries between arguments
str 14 Surface string features based on Zhou et al(2005), see text for full description
pos 14 POS features, as above
root 14 Root features, as above
genpos 14 Generalised POS features, as above
inter 11 Intervening mentions: numbers and types of intervening entity mentions between arguments
event 5 Events: are any of the arguments, or intevening entities, events?
allgen 96 All features in root and generalised POS forms, i.e. gentok6+atype+dir+dist+root+genpos+inter+event
notok 48 All except tokN features, others in string and POS forms, i.e. atype+dir+dist+str+pos+inter+event
Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.
a slightly different task for the human annotators.
The relationship extraction system is given entities,
and finds relationships between them. Human an-
notators must find both the entities and the relation-
ships. Therefore, were one human annotator to fail
to find a particular entity, they could never find rela-
tionships with that entity. The raw IAA score does
not take this into account: if an annotator fails to
find an entity, then they will also be penalised for
all relationships with that entity. We therefore give a
Corrected IAA, CIAA, in which annotators are only
compared on those relations for which they have
both found the entities involved. Both forms of IAA
are shown in Table 4. It is clear that it is hard for
annotators to reach agreement on relationships, and
that this is compounded massively by lack of perfect
agreement on entities. Note that the gold standard
used in training and evaluation reflects a further con-
sensus annotation, to correct this poor agreement.
6 Results
6.1 Feature Selection
The first group of experiments reported looks at the
performance of relation extraction with various fea-
ture sets. We followed an additive strategy for fea-
ture selection. Starting with basic features, we added
further features one set at a time. We measured the
performance of the resulting classifier each time we
added a new feature set. Results are shown in Ta-
ble 4. The initial classifier used a tok6+atype
feature set. Addition of both dir and dist fea-
tures give significant improvements in all metrics, of
around 10% F1 overall, in each case. This suggests
that the linear text order of arguments, and whether
relations are intra- or inter-sentential is important to
classification. Addition of the str features also give
good improvement in most metrics, again 10% F1
overall. Addition of part-of-speech information, in
the form of pos features, however, leads to a drop
in some metrics, overall F1 dropping by 1%. Unex-
pectedly, POS seems to provide little extra informa-
tion above that in surface string. Errors in POS tag-
ging cannot be dismissed, and could be the cause of
this. The existence of intervening entities, as coded
in feature set inter, provides a small benefit. The
inclusion of information about events, in the event
feature set, is less clear-cut.
We were interested to see if generalising features
could improve performance, as this had benefited
our previous work in entity extraction. We replaced
all surface string features with their root form, and
POS features with their generalised POS form. This
gave the results shown in column allgen. Results
are not clear cut, in some cases better and in some
worse than the previous best. Overall, there is no
difference in F1. There is a slight increase in over-
all recall, and a corresponding drop in precision ?
as might be expected.
Both the tokN, and the str and pos feature sets
provide surface string and POS information about
tokens surrounding and between relationship argu-
ments. The former gives features from a window
around each argument. The latter two give a greater
amount of positional information. Do these two pro-
vide enough information on their own, without the
windowed features? To test this, we removed the
tokN features from the full cumulative feature set,
from column +event. Results are given in column
15
Relation Metric tok6+atype +dir +dist +str +pos +inter +event allgen notok IAA CIAA
has finding P 44 49 58 63 62 64 65 63 63
R 39 63 78 80 80 81 81 82 82
F1 39 54 66 70 69 71 72 71 71 46 80
has indication P 37 23 38 42 40 41 42 37 44
R 14 14 46 44 44 47 47 45 47
F1 18 16 39 39 38 41 42 38 41 26 50
has location P 36 36 50 68 71 72 72 73 73
R 28 28 74 79 79 81 81 83 83
F1 30 30 58 72 74 76 75 77 76 55 80
has target P 9 9 32 63 57 60 62 60 59
R 11 11 51 68 67 67 66 68 68
F1 9 9 38 64 60 63 63 63 62 42 63
laterality modifies P 21 38 73 84 83 84 84 86 86
R 9 55 82 89 86 88 88 87 89
F1 12 44 76 85 83 84 84 84 85 73 94
negation modifies P 19 54 85 81 80 79 79 77 81
R 12 82 97 98 93 92 93 93 93
F1 13 63 89 88 85 84 85 83 85 66 93
sub location modifies P 2 2 55 88 86 86 88 88 87
R 1 1 62 94 92 95 95 95 95
F1 1 1 56 90 86 89 91 91 90 49 96
Overall P 33 38 50 63 62 64 65 64 64
R 22 36 70 74 73 75 75 76 76
F1 26 37 58 68 67 69 69 69 70 47 75
Table 4: Variation in performance by feature set. Features sets are abbreviated as in Table 3. For the first seven
columns, features were added cumulatively to each other. The next two columns, allgen and notok, are as de-
scribed in Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for
comparison.
notok. There is no clear change in performance,
some relationships improving, and some worsening.
Overall, there is a 1% improvement in F1.
It appears that the bulk of performance is attained
through entity type and distance features, with some
contribution from positional surface string informa-
tion. Performance is between 1% and 9% lower than
CIAA for the same relationship, with a best overall
F1 of 70%, compared to a CIAA of 75%.
6.2 Sentences Spanned
Table 2 shows that although most relationships are
intra-sentential, 23% are inter-sentential, 10% of all
relationships being between arguments in adjacent
sentences. If we consider a relationship to cross n
sentence boundaries, then the classifiers described in
the previous section were all trained on relationships
crossing n ? 1 sentence boundaries, i.e. with argu-
ments in the same or adjacent sentences. What effect
does including more distant relationships have on
performance? We trained classifiers on only intra-
sentential relationships, and on relationships span-
ning up to n sentence boundaries, for n ? {1...5}.
We also trained a classifier on relationships with
1 ? n ? 5, comprising 85% of all inter-sentential
relationships. In each case, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. It is clear from the results that
the feature sets used do not perform well on inter-
sentential relationships. There is a 6% drop in over-
all F1 when including relationships with n = 1 to-
gether with n < 1. Performance continues to drop as
more inter-sentential relationships are included, and
is very poor for just inter-sentential relationships.
A preliminary error analysis suggests that the
more distant relationship arguments are from each
other, the more likely clinical knowledge is required
to extract the relationship. This raises additional dif-
ficulties for extraction, which the simple features de-
scribed here are unable to address.
6.3 Size of Training Corpus
The provision of sufficient training data for super-
vised learning algorithms is a limitation on their use.
We examined the effect of training corpus size on
relationship extraction. The C77 corpus, compris-
16
Number of sentence boundaries between arguments
inter- intra- inter- and intra-sentential Corpus size
Relation Metric 1 ? n ? 5 n < 1 n ? 1 n ? 2 n ? 3 n ? 4 n ? 5 C25 C50 C77
has finding P 24 68 65 62 60 61 61 66 63 65
R 18 89 81 79 78 78 77 74 74 81
F1 18 76 72 69 67 68 67 67 67 72
has indication P 18 49 42 42 36 32 30 22 25 42
R 17 59 47 42 42 39 38 30 31 47
F1 16 51 42 39 37 34 33 23 25 42
has location P 0 74 72 73 72 72 72 72 71 72
R 0 83 81 81 81 82 82 76 80 81
F1 0 77 75 76 75 76 76 73 74 75
has target P 3 64 62 59 60 59 58 65 49 62
R 1 75 66 64 62 61 61 60 65 66
F1 2 68 63 61 60 60 59 59 54 63
laterality modifies P 0 86 84 86 86 86 87 77 78 84
R 0 89 88 88 88 87 88 69 68 88
F1 0 85 84 85 86 85 86 72 69 84
negation modifies P 0 80 79 79 80 80 80 78 79 79
R 0 94 93 91 93 93 93 80 93 93
F1 0 86 85 84 85 86 85 78 84 85
sub location modifies P 0 89 88 88 89 89 89 64 91 88
R 0 95 95 95 95 95 95 64 85 95
F1 0 91 91 91 91 91 91 64 86 91
Overall P 22 69 65 64 62 61 60 62 63 65
R 17 83 75 73 71 70 70 65 71 75
F1 19 75 69 68 66 65 65 63 66 69
Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.
ing 77 narratives and used in the previous experi-
ments, was subsetted to give corpora of 25 and 50
narratives, which will be referred to as C25 and C50
respectively. We trained two further classifiers on
these new corpora. Again, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. Overall, performance improves as
training corpus size increases (F1 rising from 63%
to 69%). We were struck however, by the fact that
increasing from 50 to 77 documents has little effect
on a few relationships (negation modifies and
has location). It may well be that the amount
of training data required has plateaued for those re-
lationships.
7 Conclusion
We have shown that it is possible to extract clini-
cal relationships from text, using shallow features,
and supervised statistical machine learning. Judg-
ing from poor inter annotator agreement, the task
is hard. Our system achieves a reasonable perfor-
mance, with an overall F1 just 5% below a cor-
rected inter annotator agreement. This performance
is reached largely by using features of the text that
encode entity type, distance between arguments, and
some surface string information. Performance does,
however, vary with the number of sentences spanned
by the relationships. Learning inter-sentential rela-
tionships does not seem amenable to this approach,
and may require the use of domain knowledge.
A major concern when using supervised learning
algorithms is the expense and availability of training
data. We have shown that while this concern is jus-
tified in some cases, larger training corpora may not
improve performance for all relationships.
The technology used has proved scalable. The
full CLEF IE system, including automatic entity
recognition, is able to process a document in sub-
second time on a commodity workstation. We
have used the system to extract 6 million relations
from over half a million patient documents, for use
in downstream CLEF applications (Roberts et al,
2008a). Our future work on relationship extrac-
tion in CLEF includes integration of a dependency
parse into the feature set, further analysis to deter-
mine what knowledge may be required to learn inter-
sentential relations, and integration of relationship
extraction with a co-reference algorithm.
17
Availability All of the software described here
is open source and can be downloaded as part of
GATE, with the exception of the entity pairing com-
ponent, which will be released shortly. We are cur-
rently preparing a UK research ethics committee ap-
plication, requesting permission to release our anno-
tated corpus.
Acknowledgements
CLEF is funded by the UK Medical Research Coun-
cil. We would like to thank the Royal Marsden
Hospital for providing the corpus, and our clinical
partners in CLEF for assistance in developing the
schema, and for gold standard annotation.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics, pages 168?175, Philadelphia, PA, USA, July.
C. Friedman, P. Alderson, J. Austin, J. Cimino, and
S. Johnson. 1994. A general natural-language text
processor for clinical radiology. Journal of the Amer-
ican Medical Informatics Association, 1(2):161?174,
March.
C. Grover, B. Haddow, E. Klein, M. Matthews,
L. Nielsen, R. Tobin, and X. Wang. 2007. Adapting
a relation extraction pipeline for the BioCreAtIvE II
task. In Proceedings of the BioCreAtIvE II Workshop
2007, Madrid, Spain.
U. Hahn, M. Romacker, and S. Schulz. 2002. MEDSYN-
DIKATE ? a natural language system for the ex-
traction of medical information from findings reports.
International Journal of Medical Informatics, 67(1?
3):63?74, December.
Y. Li, K. Bontcheva, and H. Cunningham. 2005.
SVM based learning system for information extrac-
tion. In Deterministic and statistical methods in ma-
chine learning: first international workshop, number
3635 in Lecture Notes in Computer Science, pages
319?339. Springer.
D. Lindberg, B. Humphreys, and A. McCray. 1993. The
Unified Medical Language System. Methods of Infor-
mation in Medicine, 32(4):281?291.
Y. Lussier, T. Borlawsky, D. Rappaport, Y. Liu, and
C. Friedman. 2006. PhenoGO: Assigning phenotypic
context to Gene Ontology annotations with natural lan-
guage processing. In Biocomputing 2006, Proceed-
ings of the Pacific Symposium, pages 64?75, Hawaii,
USA, January.
S. Pakhomov, J. Buntrock, and P. Duffy. 2005. High
throughput modularized NLP system for clinical text.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
interactive poster and demonstration sessions, pages
25?28, Ann Arbor, MI, USA, June.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, P. Singleton, R. Gaizauskas, M. Hepple,
D. Scott, and R. Power. 2003. CLEF ? joining up
healthcare with clinical and post-genomic research. In
Proceedings of UK e-Science All Hands Meeting 2003,
pages 264?267, Nottingham, UK.
T. Rindflesch and M. Fiszman. 2003. The interaction of
domain knowledge and linguistic structure in natural
language processing: interpreting hypernymic propo-
sitions in biomedical text. Journal of Biomedical In-
formatics, 36(6):462?477.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,
Y. Guo, A. Setzer, and I. Roberts. 2008a. Seman-
tic annotation of clinical text: The CLEF corpus. In
Proceedings of Building and evaluating resources for
biomedical text mining: workshop at LREC 2008,
Marrakech, Morocco, May. In press.
A. Roberts, R. Gaizauskas, M. Hepple, and Y. Guo.
2008b. Combining terminology resources and statis-
tical methods for entity recognition: an evaluation.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco, May. In press.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick.
1994. Natural language processing and the representa-
tion of clinical data. Journal of the American Medical
Informatics Association, 1(2):142?160, March-April.
T. Wang, Y. Li, K. Bontcheva, H. Cunningham, and
J. Wang. 2006. Automatic extraction of hierarchical
relations from text. In The Semantic Web: Research
and Applications. 3rd European Semantic Web Con-
ference, ESWC 2006, number 4011 in Lecture Notes
in Computer Science, pages 215?229. Springer.
G. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring Various Knowledge in Relation Extraction. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
427?434, Ann Arbor, MI, USA, June.
P. Zweigenbaum, B. Bachimont, J. Bouaud, J. Charlet,
and J-F. Boisvieux. 1995. A multi-lingual architec-
ture for building a normalised conceptual representa-
tion from medical language. In Proceedings of the An-
nual Symposium on Computer Applications in Medical
Care, pages 357?361, New York, NY, USA.
18
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 262?272,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Storing the Web in Memory: Space Efficient Language Models with
Constant Time Retrieval
David Guthrie
Computer Science Department
University of Sheffield
D.Guthrie@dcs.shef.ac.uk
Mark Hepple
Computer Science Department
University of Sheffield
M.Hepple@dcs.shef.ac.uk
Abstract
We present three novel methods of compactly
storing very large n-gram language models.
These methods use substantially less space
than all known approaches and allow n-gram
probabilities or counts to be retrieved in con-
stant time, at speeds comparable to modern
language modeling toolkits. Our basic ap-
proach generates an explicit minimal perfect
hash function, that maps all n-grams in a
model to distinct integers to enable storage of
associated values. Extensions of this approach
exploit distributional characteristics of n-gram
data to reduce storage costs, including variable
length coding of values and the use of tiered
structures that partition the data for more effi-
cient storage. We apply our approach to stor-
ing the full Google Web1T n-gram set and all
1-to-5 grams of the Gigaword newswire cor-
pus. For the 1.5 billion n-grams of Gigaword,
for example, we can store full count informa-
tion at a cost of 1.66 bytes per n-gram (around
30% of the cost when using the current state-
of-the-art approach), or quantized counts for
1.41 bytes per n-gram. For applications that
are tolerant of a certain class of relatively in-
nocuous errors (where unseen n-grams may
be accepted as rare n-grams), we can reduce
the latter cost to below 1 byte per n-gram.
1 Introduction
The availability of very large text collections, such
as the Gigaword corpus of newswire (Graff, 2003),
and the Google Web1T 1-5gram corpus (Brants and
Franz, 2006), have made it possible to build mod-
els incorporating counts of billions of n-grams. The
storage of these language models, however, presents
serious problems, given both their size and the need
to provide rapid access. A prevalent approach for
language model storage is the use of compact trie
structures, but these structures do not scale well and
require space proportional to both to the number
of n-grams and the vocabulary size. Recent ad-
vances (Talbot and Brants, 2008; Talbot and Os-
borne, 2007b) involve the development of Bloom fil-
ter based models, which allow a considerable reduc-
tion in the space required to store a model, at the cost
of allowing some limited extent of false positives
when the model is queried with previously unseen
n-grams. The aim is to achieve sufficiently compact
representation that even very large language models
can be stored totally within memory, avoiding the
latencies of disk access. These Bloom filter based
models exploit the idea that it is not actually neces-
sary to store the n-grams of the model, as long as,
when queried with an n-gram, the model returns the
correct count or probability for it. These techniques
allow the storage of language models that no longer
depend on the size of the vocabulary, but only on the
number of n-grams.
In this paper we give three different models for
the efficient storage of language models. The first
structure makes use of an explicit perfect hash func-
tion that is minimal in that it maps n keys to in-
tegers in the range 1 to n. We show that by us-
ing a minimal perfect hash function and exploit-
ing the distributional characteristics of the data we
produce n-gram models that use less space than all
know approaches with no reduction in speed. Our
two further models achieve even more compact stor-
age while maintaining constant time access by us-
ing variable length coding to compress the n-grams
values and by using tiered hash structures to parti-
262
tion the data into subsets requiring different amounts
of storage. This combination of techniques allows
us, for example, to represent the full count informa-
tion of the Google Web1T corpus (Brants and Franz,
2006) (where count values range up to 95 billion) at
a cost of just 2.47 bytes per n-gram (assuming 8-
bit fingerprints, to exclude false positives) and just
1.41 bytes per n-gram if we use 8-bit quantization
of counts. These costs are 36% and 57% respec-
tively of the space required by the Bloomier Filter
approach of Talbot and Brants (2008). For the Gi-
gaword dataset, we can store full count information
at a cost of only 1.66 bytes per n-gram. We re-
port empirical results showing that our approach al-
lows a look-up rate which is comparable to existing
modern language modeling toolkits, and much faster
than a competitor approach for space-efficient stor-
age. Finally, we propose the use of variable length
fingerprinting for use in contexts which can tolerate
a higher rate of ?less damaging? errors. This move
allows, for example, the cost of storing a quantized
model to be reduced to 1 byte per n-gram or less.
2 Related Work
A range of lossy methods have been proposed, to
reduce the storage requirements of LMs by discard-
ing information. Methods include the use of entropy
pruning techniques (Stolcke, 1998) or clustering (Je-
linek et al, 1990; Goodman and Gao, 2000) to re-
duce the number of n-grams that must be stored.
A key method is quantization (Whittaker and Raj,
2001), which reduces the value information stored
with n-grams to a limited set of discrete alternatives.
It works by grouping together the values (probabil-
ities or counts) associated with n-grams into clus-
ters, and replacing the value to be stored for each
n-gram with a code identifying its value?s cluster.
For a scheme with n clusters, codes require log2n
bits. A common case is 8-bit quantization, allow-
ing up to 256 distinct ?quantum? values. Differ-
ent methods of dividing the range of values into
clusters have been used, e.g. Whittaker and Raj
(2001) used the Lloyd-Max algorithm, whilst Fed-
erico and Bertoldi (2006) use the simpler Binning
method to quantize probabilities, and show that the
LMs so produced out-perform those produced us-
ing the Lloyd-Max method on a phrase-based ma-
chine translation task. Binning partitions the range
of values into regions that are uniformly populated,
i.e. producing clusters that contain the same num-
ber of unique values. Functionality to perform uni-
form quantization of this kind is provided as part of
various LM toolkits, such as IRSTLM. Some of the
empirical storage results reported later in the paper
relate to LMs recording n-gram count values which
have been quantized using this uniform binning ap-
proach. In the rest of this section, we turn to look
at some of the approaches used for storing language
models, irrespective of whether lossy methods are
first applied to reduce the size of the model.
2.1 Language model storage using Trie
structures
A widely used approach for storing language mod-
els employs the trie data structure (Fredkin, 1960),
which compactly represents sequences in the form
of a prefix tree, where each step down from the
root of the tree adds a new element to the sequence
represented by the nodes seen so far. Where two
sequences share a prefix, that common prefix is
jointly represented by a single node within the trie.
For language modeling purposes, the steps through
the trie correspond to words of the vocabulary, al-
though these are in practice usually represented by
24 or 32 bit integers (that have been uniquely as-
signed to each word). Nodes in the trie correspond-
ing to complete n-grams can store other informa-
tion, e.g. a probability or count value. Most mod-
ern language modeling toolkits employ some ver-
sion of a trie structure for storage, including SRILM
(Stolcke, 2002), CMU toolkit (Clarkson and Rosen-
feld, 1997), MITLM (Hsu and Glass, 2008), and
IRSTLM (Federico and Cettolo, 2007) and imple-
mentations exist which are very compact (Germann
et al, 2009). An advantage of this structure is that it
allows the stored n-grams to be enumerated. How-
ever, although this approach achieves a compact of
representation of sequences, its memory costs are
still such that very large language models require
very large storage space, far more than the Bloom
filter based methods described shortly, and far more
than might be held in memory as a basis for more
rapid access. The memory costs of such models
have been addressed using compression methods,
see Harb et al (2009), but such extensions of the
263
approach present further obstacles to rapid access.
2.2 Bloom Filter Based Language Models
Recent randomized language models (Talbot and
Osborne, 2007b; Talbot and Osborne, 2007a; Tal-
bot and Brants, 2008; Talbot and Talbot, 2008; Tal-
bot, 2009) make use of Bloom filter like structures
to map n-grams to their associated probabilities or
counts. These methods store language models in
relatively little space by not actually keeping the n-
gram key in the structure and by allowing a small
probability of returning a false positive, i.e. so that
for an n-gram that is not in the model, there is a
small risk that the model will return some random
probability instead of correctly reporting that the n-
gram was not found. These structures do not allow
enumeration over the n-grams in the model, but for
many applications this is not a requirement and their
space advantages make them extremely attractive.
Two major approaches have been used for storing
language models: Bloom Filters and Bloomier Fil-
ters. We give an overview of both in what follows.
2.2.1 Bloom Filters
A Bloom filter (Bloom, 1970) is a compact data
structure for membership queries, i.e. queries of the
form ?Is this key in the Set??. This is a weaker struc-
ture than a dictionary or hash table which also asso-
ciates a value with a key. Bloom filters use well be-
low the information theoretic lower bound of space
required to actually store the keys and can answer
queries in O(1) time. Bloom filters achieve this feat
by allowing a small probability of returning a false
positive. A Bloom filter stores a set S of n elements
in a bit array B of size m. Initially B is set to con-
tain all zeros. To store an item x from S in B we
compute k random independent hash functions on
x that each return a value in the range [0 . .m? 1].
These values serve as indices to the bit array B and
the bits at those positions are set to 1. We do this
for all elements in S, storing to the same bit array.
Elements may hash to an index inB that has already
been set to 1 and in this case we can think of these
elements as ?sharing? this bit. To test whether set S
contains a key w, we run our k hash functions on w
and check if all those locations in B are set to 1. If
w ? S then the bloom filter will always declare that
w belongs to S, but if x /? S then the filter can only
say with high probability that w is not in S. This er-
ror rate depends on the number of k hash functions
and the ratio of m/n. For instance with k = 3 hash
functions and a bit array of size m = 20n, we can
expect to get a false positive rate of 0.0027.
Talbot and Osborne (2007b) and Talbot and Os-
borne (2007a) adapt Bloom filters to the requirement
of storing values for n-grams by concatenating the
key (n-gram) and value to form a single item that is
inserted into the filter. Given a quantization scheme
allowing values in the range [1 . . V ], a quantized
value v is stored by inserting into the filter all pair-
ings of the n-gram with values from 1 up to v. To re-
trieve the value for a given key, we serially probe the
filter for pairings of the key with each value from 1
upwards, until the filter returns false. The last value
found paired with the key in the filter is the value re-
turned. Talbot and Osborne use a simple logarithmic
quantization of counts that produce limited quan-
tized value ranges, where most items will have val-
ues that are low in the range, so that the serial look-
up process will require quite a low number of steps
on average. For alternative quantization schemes
that involve greater value ranges (e.g. the 256 values
of a uniform 8-bit scheme) and/or distribute n-grams
more evenly across the quantized values, the average
number of look-up steps required will be higher and
hence the speed of access per n-gram accordingly
lower. In that case also, the requirement of insert-
ing n-grams more than once in the filter (i.e. with
values from 1 up to the actual value v being stored)
could substantially reduce the space efficiency of the
method, especially if low false positive rates are re-
quired, e.g. the case k = 3,m = 20n produces a
false positive rate of 0.0027, as noted above, but in a
situation where 3 key-value items were being stored
per n-gram on average, this error rate would in fact
require a storage cost of 60 bits per original n-gram.
2.2.2 Bloomier Filters
More recently, Talbot and Brants (2008) have pro-
posed an approach to storing large language mod-
els which is based on the Bloomier Filter technique
of Chazelle et al (2004). Bloomier Filters gener-
alize the Bloom Filter to allow values for keys to
be stored in the filter. To test whether a given key
is present in a populated Bloomier filter, we apply
k hash functions to the key and use the results as
264
indices for retrieving the data stored at k locations
within the filter, similarly to look-up in a Bloom fil-
ter. In this case, however, the data retrieved from the
filter consists of k bit vectors, which are combined
with a fingerprint of the key, using bitwise XOR, to
return the stored value. The risk of false positives
is managed by making incorporating a fingerprint of
the n-gram, and by making bit vectors longer than
the minimum length required to store values. These
additional error bits have a fairly predictable impact
on error rates, i.e. with e error bits, we anticipate the
probability of falsely construing an unseen n-gram
as being stored in the filter to be 2?e. The algo-
rithm required to correctly populate the Bloomier fil-
ter with stored data is complicated, and we shall not
consider its details here. Nevertheless, when using v
bits to represent values and e bits for error detection,
this approach allows a language model to be stored
at a cost of is 1.23 ? (v + e) bits per n-gram.
3 Single Minimal Perfect Hash Ranking
Approach
We first describe our basic structure we call Single
Minimal Perfect Hash Rank (S-MPHR) that is more
compact than that of Talbot and Brants (2008) while
still keeping a constant look up time. In the next
two sections we describe variations on this model to
further reduce the space required while maintaining
a constant look up time. The S-MPHR structure can
be divided into 3 parts as shown in Figure 1: Stage
1 is a minimal perfect hash function; Stage 2 is a
fingerprint and rank array; and Stage 3 is a unique
value array. We discuss each stage in turn.
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function
Array of K distinct probability values / frequency counts
p
1
p
2
p
3
p
4
p
5
p
6
... p
K
rank(key
5
) rank(key) rank(key
1
) rank(key
3
) rank(key
2
) rank(key) rank(key
4
) ...
Figure 1: The Single MPHR structure
3.1 Minimal Perfect Hash Function
The first part of the structure is a minimal perfect
hash function that maps every n-gram in the training
data to a distinct integer in the range 0 to N ? 1,
whereN is the total number of n-grams to store. We
use these integers as indices into the array of Stage
2 of our structure.
We use the Hash, displace, and compress (CHD)
(Belazzougui et al, 2009) algorithm to generate a
minimal perfect hash function that requires 2.07 bits
per key and has O(1) access. The algorithm works
as follows. Given a set S that contains N = |S|
keys (in our case n-grams) that we wish to map to
integers in the range 0 to N ? 1, so that every key
maps to a distinct integer (no collisions).
The first step is to use a hash function g(x), to
map every key to a bucket B in the range 0 to r.
(For this step we use a simple hash function like the
one used for generating fingerprints in the pervious
section.)
Bi = {x ? S|g(x) = i} 0 ? i ? r
The function g(x) is not perfect so several keys can
map to the same bucket. Here we choose r ? N ,
so that the number of buckets is less than or equal
to the number of keys (to achieve 2.07 bits per key
we use r = N5 , so that the average bucket size is 5).
The buckets are then sorted into descending order
according to the number of keys in each bucket |Bi|.
For the next step, a bit array, T , of size N is ini-
tialized to contain all zeros T [0 . . . N ? 1]. This bit
array is used during construction to keep track of
which integers in the range 0 to N ? 1 the minimal
perfect hash has already mapped keys to. Next we
must assume we have access to a family of random
and independent hash functions h1, h2, h3, . . . that
can be accessed using an integer index. In practice
it sufficient to use functions that behave similarly to
fully random independent hash functions and Belaz-
zougui et al (2009) demonstrate how such functions
can be generated easily by combining two simple
hash functions.
Next is the ?displacement? step. For each bucket,
in the sorted order from largest to smallest, they
search for a random hash function that maps all ele-
ments of the bucket to values in T that are currently
set to 0. Once this function has been found those
265
positions in T are set to 1. So, for each bucket Bi,
it is necessary to iteratively try hash functions, h`
for ` = 1, 2, 3, . . . to hash every element of Bi to a
distinct index j in T that contains a zero.
{h`(x)|x ? Bi} ? {j|T [j] = 1} = ?
where the size of {h`(x)|x ? Bi} is equal to the size
of Bi. When such a hash function is found we need
only to store the index, `, of the successful function
in an array ? and set T [j] = 1 for all positions j that
h` hashed to. Notice that the reason the largest buck-
ets are handled first is because they have the most el-
ements to displace and this is easier when the array
T contains more empty positions (zeros).
The final step in the algorithm is to compress the ?
array (which has length equal to the number of buck-
ets |B|), retaining O(1) access. This compression is
achieved using simple variable length encoding with
an index array (Fredriksson and Nikitin, 2007).
3.2 Fingerprint and Rank Array
The hash function used in Stage 1 is perfect, so it
is guaranteed to return unique integers for seen n-
grams, but our hash function will also return inte-
ger values in the range 0 to N ? 1 for n-grams that
have not been seen before (were not used to build the
hash function). To reduce the probability of these
unseen n-grams giving false positives results from
our model we store a fingerprint of each n-gram in
Stage 2 of our structure that can be compared against
the fingerprints of unseen n-grams when queried.
If these fingerprints of the queried n-gram and the
stored n-gram do not match then the model will
correctly report that the n-gram has not been seen
before. The size of this fingerprint determines the
rate of false positives. Assuming that the finger-
print is generated by a random hash function, and
that the returned integer of an unseen key from the
MPH function is also random, expected false posi-
tive rate for the model is the same as the probabil-
ity of two keys randomly hashing to the same value,
1
2m , where m is the number of bits of the finger-
print. The fingerprint can be generated using any
suitably random hashing algorithm. We use Austin
Appleby?s Murmurhash21 implementation to finger-
print each n-gram and then store the m highest or-
der bits. Stage 2 of the MPHR structure also stores
1http://murmurhash.googlepages.com/
a rank for every n-gram along with the fingerprint.
This rank is an index into the array of Stage 3 of
our structure that holds the unique values associated
with any n-gram.
3.3 Unique Value Array
We describe our storage of the values associated
with n-grams in our model assuming we are storing
frequency ?counts? of n-grams, but it applies also to
storing quantized probabilities. For every n-gram,
we store the ?rank? of the frequency count r(key),
(r(key) ? [0...R ? 1]) and use a separate array in
Stage 3 to store the frequency count value. This is
similar to quantization in that it reduces the num-
ber of bits required for storage, but unlike quanti-
zation it does not require a loss of any information.
This was motivated by the sparsity of n-gram fre-
quency counts in corpora in the sense that if we take
the lowest n-gram frequency count and the high-
est n-gram frequency count then most of the inte-
gers in that range do not occur as a frequency count
of any n-grams in the corpus. For example in the
Google Web1T data, there are 3.8 billion unique n-
grams with frequency counts ranging from 40 to 95
Billion yet these n-grams only have 770 thousand
distinct frequency counts (see Table 2). Because
we only store the frequency rank, to keep the pre-
cise frequency information we need only dlog2Ke
bits per n-gram, where K is the number of distinct
frequency counts. To keep all information in the
Google Web1T data we need only dlog2 771058e =
20 bits per n-gram. Rather than the bits needed
to store the maximum frequency count associated
with an n-gram, dlog2 maxcounte, which for Google
Web1T would be dlog2 95119665584e = 37 bits per
n-gram.
unique maximum n-gram unique
n-grams frequency count counts
1gm 1, 585, 620 71, 363, 822 16, 896
2gm 55, 809, 822 9, 319, 466 20, 237
3gm 250, 928, 598 829, 366 12, 425
4gm 493, 134, 812 231, 973 6, 838
5gm 646, 071, 143 86, 943 4, 201
Total 1, 447, 529, 995 71, 363, 822 60, 487
Table 1: n-gram frequency counts from Gigaword corpus
266
unique maximum n-gram unique
n-grams frequency count counts
1gm 13, 588, 391 95, 119, 665, 584 238, 592
2gm 314, 843, 401 8, 418, 225, 326 504, 087
3gm 977, 069, 902 6, 793, 090, 938 408, 528
4gm 1, 313, 818, 354 5, 988, 622, 797 273, 345
5gm 1, 176, 470, 663 5, 434, 417, 282 200, 079
Total 3, 795, 790, 711 95, 119, 665, 584 771, 058
Table 2: n-gram frequency counts from Google Web1T
corpus
3.4 Storage Requirements
We now consider the storage requirements of our S-
MPHR approach, and how it compares against the
Bloomier filter method of Talbot and Brants (2008).
To start with, we put aside the gains that can come
from using the ranking method, and instead con-
sider just the costs of using the CHD approach for
storing any language model. We saw that the stor-
age requirements of the Talbot and Brants (2008)
Bloomier filter method are a function of the number
of n-grams n, the bits of data d to be stored per n-
gram (with d = v + e: v bits for value storage, and
e bits for error detection), and a multiplying factor
of 1.23, giving an overall cost of 1.23d bits per n-
gram. The cost for our basic approach is also easily
computed. The explicit minimal PHF computed us-
ing the CHD algorithm brings a cost of 2.07 bits per
n-gram for the PHF itself, and so the comparable
overall cost to store a S-MPHR model is 2.07 + d
bits per n-gram. For small values of d, the Bloomier
filter approach has the smaller cost, but the ?break-
even? point occurs when d = 9. When d is greater
than 9 bits (as it usually will be), our approach wins
out, being up to 18% more efficient.
The benefits that come from using the ranking
method (Stage 3), for compactly storing count val-
ues, can only be evaluated in relation to the distribu-
tional characteristics specific corpora, for which we
show results in Section 6.
4 Compressed MPHR Approach
Our second approach, called Compressed MPHR,
further reduces the size of the model whilst main-
taining O(1) time to query the model. Most com-
pression techniques work by exploiting the redun-
dancy in data. Our fingerprints are unfortunately
random sequences of bits, so trying to compress
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function
Array of K distinct probability values / frequency counts
p
1
p
2
p
3
p
4
p
5
p
6
... p
K
rank(key
5
) rank(key) rank(key
1
) rank(key
3
) rank(key
2
) rank(key) rank(key
4
) ...
Fingerprint Array
Compressed Rank Array
Figure 2: Compressed MPHR structure
these is fruitless, but the ranks associated with n-
grams contain much redundancy and so are likely to
compress well. We therefore modify our original ar-
chitecture to put the ranks and fingerprints into sep-
arate arrays, of which the ranks array will be com-
pressed, as shown in Figure 2.
Much like the final stage of the CHD minimal
perfect hash algorithm we employ a random access
compression algorithm of Fredriksson and Nikitin
(2007) to reduce the size required by the array of
ranks. This method allows compression while re-
taining O(1) access to query the model.
The first step in the compression is to encode
the ranks array using a dense variable length cod-
ing. This coding works by assigning binary codes
with different lengths to each number in the rank ar-
ray, based on how frequent that number occurs. Let
s1, s2, s3, . . . , sK be the ranks that occur in the rank
array sorted by there frequency. Starting with most
frequent number in the rank array (clearly 1 is the
most common frequency count in the data unless it
has been pruned) s1 we assign it the bit code 0 and
then assign s2 the bit code 1, we then proceed by as-
signing bit codes of two bits, so s3 is assigned 00, s4
is assigned 01, etc. until all two bit codes are used
up. We then proceed to assign 3 bit codes and so on.
All of the values from the rank array are coded in
this form and concatenated to form a large bit vector
retaining their original ordering. The length in bits
for the ith number is thus blog2 (i+ 2)c and so the
number of bits required for the whole variable length
coded rank array is: b =
?K
i=0 f(si)blog2 (i+ 2)c.
Where f() gives the frequency that the rank occurs
267
andK is the total number of distinct ranks. The code
for the ith number is the binary representation with
length blog2 (i+ 2)c of the number obtained using
the formula:
code = i+ 2? 2blog2 (i+2)c
This variable length coded array is not useful by it-
self because we do not know where each number be-
gins and ends, so we also store an index array hold
this information. We create an additional bit array
D of the same size b as the variable length coded ar-
ray that simply contains ones in all positions that a
code begins in the rank array and zeros in all other
positions. That is the ith rank in the variable length
coded array occurs at position select1(D, i), where
select1 gives the position of the ith one in the ar-
ray. We do not actually store theD array, but instead
we build a more space efficient structure to answer
select1 queries. Due the distribution of n-gram fre-
quencies, the D array is typically dense in contain-
ing a large proportion of ones, so we build a rank9sel
dictionary structure (Vigna, 2008) to answer these
queries in constant time. We can use this structure
to identify the ith code in our variable length en-
coded rank array by querying for its starting posi-
tion, select1(D, i), and compute its length using its
ending position, select1(D, i+1)?1. The code and
its length can then be decoded to obtain the original
rank:
rank = code + 2(length in bits) ? 2
5 Tiered MPHR
In this section we describe an alternative route to ex-
tending our basic S-MPHR model to achieve better
space efficiency, by using multiple hash stores. The
method exploits distributional characteristics of the
data, i.e. that lower rank values (those assigned to
values shared by very many n-grams) are sufficient
for representing the value information of a dispro-
portionately large subset of the data. For the Google
Web 1T data, for example, we find that the first 256
ranks account for nearly 85% of distinct n-grams, so
if we could store ranks for these n-grams using only
the 8 bits they require, whilst allowing perhaps 20
bits per n-gram for the remaining 15%, we would
achieve an average of just under 10 bits per n-gram
to store all the rank values.
To achieve this gain, we might partition the n-
gram data into subsets requiring different amounts
of space for value storage, and put these subsets in
separate MPHRs, e.g. for the example just men-
tioned, with two MPHRs having 8 and 20 bit value
storage respectively. Partitioning to a larger number
h of MPHRs might further reduce this average cost.
This simple approach has several problems. Firstly,
it potentially requires a series of look up steps (i.e.
up to h) to retrieve the value for any n-gram, with
all hashes needing to be addressed to determine the
unseen status of an unseen n-gram. Secondly, mul-
tiple look ups will produce a compounding of error
rates, since we have up to h opportunities to falsely
construe an unseen n-gram as seen, or to construe
a seen n-gram as being stored in the wrong MPHR
and so return an incorrect count for it.
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function #1
rank(key
5
) Redirect 1 Redirect 2 rank(key
3
) rank(key
2
) Redirect 1 Redirect 2 ...
Minimal Perfect Hash Function  #2 Minimal Perfect Hash Function #3
rank(key) rank(key) ... rank(key) rank(key) rank(key) ... rank(key)
Figure 3: Tiered minimal perfect hash data structure
We will here explore an alternative approach that
we call Tiered MPHR, which avoids this compound-
ing of errors, and which limits the number of looks
ups to a maximum of 2, irrespective of how many
hashes are used. This approach employs a single
top-level MPHR which has the full set of n-grams
for its key-set, and stores a fingerprint for each. In
addition, space is allocated to store rank values, but
with some possible values being reserved to indicate
redirection to other secondary hashes where values
can be found. Each secondary hash has a minimal
perfect hash function that is computed only for the
n-grams whose values it stores. Secondary hashes
do not need to record fingerprints, as fingerprint test-
ing is done in the top-level hash.
For example, we might have a configuration of
268
three hashes, with the top-level MPHR having 8-bit
storage, and with secondary hashes having 10 and 20
bit storage respectively. Two values of the 8-bit store
(e.g. 0 and 1) are reserved to indicate redirection
to the specific secondary hashes, with the remaining
values (2 . . 255) representing ranks 1 to 254. The
10-bit secondary hash can store 1024 different val-
ues, which would then represent ranks 255 to 1278,
with all ranks above this being represented in the
20-bit hash. To look up the count for an n-gram,
we begin with the top-level hash, where fingerprint
testing can immediately reject unseen n-grams. For
some seen n-grams, the required rank value is pro-
vided directly by the top-level hash, but for others
a redirection value is returned, indicating precisely
the secondary hash in which the rank value will be
found by simple look up (with no additional finger-
print testing). Figure 3 gives a generalized presenta-
tion of the structure of tiered MPHRs. Let us repre-
sent a configuration for a tiered MPHR as a sequence
of bit values for their value stores, e.g. (8,10,20)
for the example above, or H = (b1, . . . .bh) more
generally (with b1 being the top-level MPHR).
The overall memory cost of a particular config-
uration depends on distributional characteristics of
the data stored. The top-level MPHR of config-
uration (b1, . . . .bh) stores all n-grams in its key-
set, so its memory cost is calculated as before as
N ? (2.07 +m + b1) (m the fingerprint size). The
top-level MPHR must reserve h? 1 values for redi-
rection, and so covers ranks [1 . . (2b1 ?h+1)]. The
second MPHR then covers the next 2b2 ranks, start-
ing at (2b1 ? h+2), and so on for further secondary
MPHRs. This range of ranks determines the pro-
portion ?i of the overall n-gram set that each sec-
ondary MPHR bi stores, and so the memory cost of
each secondary MPHR is N ??i? (2.07+ bi). The
optimal T-MPHR configuration for a given data set
is easily determined from distributional information
(of the coverage of each rank), by a simple search.
6 Results
In this section, we present some results comparing
the performance of our new storage methods to some
of the existing methods, regarding the costs of stor-
ing LMs, and regarding the data access speeds that
alternative systems allow.
Method
Gigaword Web1T
full quantized full quantized
Bloomier 6.00 3.08 7.53 3.08
S-MPHR 3.76 2.76 4.26 2.76
C-MPHR 2.19 2.09 3.40 2.09
T-MPHR 2.16 1.91 2.97 1.91
Table 3: Space usage in bytes/ngram using 12-bit finger-
prints and storing all 1 to 5 grams
Method
Gigaword Web1T
full quantized full quantized
Bloomier 5.38 2.46 6.91 2.46
S-MPHR 3.26 2.26 3.76 2.26
C-MPHR 1.69 1.59 2.90 1.59
T-MPHR 1.66 1.41 2.47 1.41
Table 4: Space usage in bytes/n-gram using 8-bit finger-
prints and storing all 1 to 5 grams
6.1 Comparison of memory costs
To test the effectiveness of our models we built mod-
els storing n-grams and full frequency counts for
both the Gigaword and Google Web1T corpus stor-
ing all 1,2,3,4 and 5 grams. These corpora are very
large, e.g. the Google Web1T corpus is 24.6GB
when gzip compressed and contains over 3.7 bil-
lion n-grams, with frequency counts as large as 95
billion, requiring at least 37 bits to be stored. Us-
ing the Bloomier algorithm of Talbot and Brants
(2008) with 37 bit values and 12 bit fingerprints
would require 7.53 bytes/n-gram, so we would need
26.63GB to store a model for the entire corpus.
In comparison, our S-MPHR method requires
only 4.26 bytes per n-gram to store full frequency
count information and stores the entire Web1T cor-
pus in just 15.05GB or 57% of the space required by
the Bloomier method. This saving is mostly due to
the ranking method allowing values to be stored at a
cost of only 20 bits per n-gram. Applying the same
rank array optimization to the Bloomier method sig-
nificantly reduces its memory requirement, but S-
MPHR still uses only 86% of the space that the
Bloomier approach requires. Using T-MPHR in-
stead, again with 12-bit fingerprints, we can store
full counts for the Web 1T corpus in 10.50GB,
which is small enough to be held in memory on
many modern machines. Using 8-bit fingerprints, T-
269
Method bytes/
ngram
SRILM Full, Compact 33.6
IRSTLM, 8-bit Quantized 9.1
Bloomier 12bit fp, 8bit Quantized 3.08
S-MPHR 12bit fp, 8bit Quantized 2.76
C-MPHR 12bit fp, 8bit Quantized 2.09
T-MPHR 12bit fp, 8bit Quantized 1.91
Table 5: Comparison between approaches for storing all
1 to 5 grams of the Gigaword Corpus
MPHR can store this data in just 8.74GB.
Tables 3, 4 and 5 show results for all methods2 on
both corpora, for storing full counts, and for when
8-bit binning quantization of counts is used.
6.2 Access speed comparisons
The three models we present in this paper perform
queries in O(1) time and are thus asymptotically
optimal, but this does not guarantee they perform
well in practice, therefore in this section we mea-
sure query speed on a large set of n-grams and com-
pare it to that of modern language modeling toolk-
its. We build a model of all unigrams and bigrams
in the Gigaword corpus (see Table 1) using the C-
MPHR method, SRILM (Stolcke, 2002), IRSTLM
(Federico and Cettolo, 2007), and randLM3 (Talbot
and Osborne, 2007a) toolkits. RandLM is a mod-
ern language modeling toolkit that uses Bloom filter
based structures to store large language models and
has been integrated so that it can be used as the lan-
guage model storage for the Moses statistical ma-
chine translation system (Koehn et al, 2007). We
use randLM with the BloomMap (Talbot and Tal-
bot, 2008) storage structure option with 8 bit quan-
tized values and an error rate equivalent to using 8
bit fingerprints (as recommended in the Moses doc-
umentation). All methods are implemented in C++
and are run on a machine with 2.80GHz Intel Xeon
E5462 processor and 64 GB of RAM. In addition
we show a comparison to using a modern database,
MySQL 5.0, to store the same data. We measure
the speed of querying all models for the 55 mil-
lion distinct bigrams that occur in the Gigaword,
2All T-MPHR results are for optimal configurations: Gi-
gaword full:(2,3,16), Gigaword quant:(1,8), Web1T
full:(8,6,7,8,9,10,13,20), Web1T quant:(1,8).
3http://sourceforge.net/projects/randlm/
Test Time Speed
(hr :min:sec) queries/sec
C-MPHR 00 : 01 : 50 507362
IRSTLM 00 : 02 : 12 422802
SRILM 00 : 01 : 29 627077
randLM 00 : 27 : 28 33865
MySQL 5 29 : 25 : 01 527
Table 6: Look-up speed performance comparison for C-
MPHR and several other LM storage methods
these results are shown in Table 6. Unsurprisingly
all methods perform significantly faster than using a
database as they build models that reside completely
in memory. The C-MPHR method tested here is
slower than both S-MPHR and T-MPHR models due
to the extra operations required for access to the vari-
able length encoded array yet still performs similarly
to SRILM and IRSTLM and is 14.99 times faster
than using randLM.
7 Variable Length Fingerprints
To conclude our presentation of new methods for
space-efficient language model storage, we suggest
an additional possibility for reducing storage costs,
which involves using different sizes of fingerprint
for different n-grams. Recall that the only errors al-
lowed by our approach are false-positives, i.e. where
an unseen n-gram is falsely construed as being part
of the model and a value returned for it. The idea be-
hind using different sizes of fingerprint is that, intu-
itively, some possible errors seem worse than others,
and in particular, it seems likely to be less damaging
if we falsely construe an unseen n-gram as being a
seen n-gram that has a low count or probability than
as being one with a high count or probability.
False positives arise when our perfect hashing
method maps an unseen n-gram to position where
the stored n-gram fingerprint happens to coincide
with that computed for the unseen n-gram. The risk
of this occurring is a simple function of the size
of fingerprints. To achieve a scheme that admits a
higher risk of less damaging errors, but enforces a
lower risk of more damaging errors, we need only
store shorter fingerprints for n-grams in our model
that have low counts or probabilities, and longer
fingerprints for n-grams with higher values. This
270
idea could be implemented in different ways, e.g.
by storing fingerprints of different lengths contigu-
ously within a bit array, and constructing a ?selection
structure? of the kind described in Section 4 to allow
us to locate a given fingerprint within the bit array.
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function
rank(key
5
) Redirect 1 Redirect 2 rank(key
3
) rank(key
2
) Redirect 1 Redirect 2 ...
Minimal Perfect Hash Function
rank(key) rank(key) ... rank(key)
Minimal Perfect Hash Function
first j bits of 
fingerprint
FP FP
...
FP
last m - j 
bits of 
fingerprint
rank(key) rank(key) ... rank(key)
FP FP
...
FP
Figure 4: Variable length fingerprint T-MPHR structure
using j bit fingerprints for the n-grams which are most
rare and m bit fingerprints for all others.
We here instead consider an alternative imple-
mentation, based on the use of tiered structures. Re-
call that for T-MPHR, the top-level MPHR has all
n-grams of the model as keys, and stores a fin-
gerprint for each, plus a value that may represent
an n-gram?s count or probability, or that may redi-
rect to a second-level hash where that information
can be found. Redirection is done for items with
higher counts or probabilities, so we can achieve
lower error rates for precisely these items by stor-
ing additional fingerprint information for them in
the second-level hash (see Figure 4). For example,
we might have a top-level hash with only 4-bit fin-
gerprints, but have an additional 8-bits of fingerprint
for items also stored in a second-level hash, so there
is quite a high risk (close to 116 ) of returning a low
count for an unseen n-gram, but a much lower risk
of returning any higher count. Table 7 applies this
idea to storing full and quantized counts of the Gi-
gaword and Web 1T models, when fingerprints in the
top-level MPHR have sizes in the range 1 to 6 bits,
with the fingerprint information for items stored in
secondary hashes being ?topped up? to 12 bits. This
approach achieves storage costs of around 1 byte per
n-gram or less for the quantized models.
Bits in
lowest
finger-
print
Giga-
word
Quan-
tized
Web1T
Quan-
tized
Giga-
word
All
Web1T
All
1 0.55 0.55 1.00 1.81
2 0.68 0.68 1.10 1.92
3 0.80 0.80 1.21 2.02
4 0.92 0.92 1.31 2.13
5 1.05 1.04 1.42 2.23
6 1.17 1.17 1.52 2.34
Table 7: Bytes per fingerprint for T-MPHR model using 1
to 6 bit fingerprints for rarest n-grams and 12 bit (in total)
fingerprints for all other n-grams. (All configurations are
as in Footnote 2.)
8 Conclusion
We have presented novel methods of storing large
language models, consisting of billions of n-grams,
that allow for quantized values or frequency counts
to be accessed quickly and which require far less
space than all known approaches. We show that it
is possible to store all 1 to 5 grams in the Gigaword
corpus, with full count information at a cost of just
1.66 bytes per n-gram, or with quantized counts for
just 1.41 bytes per n-gram. We have shown that our
models allow n-gram look-up at speeds comparable
to modern language modeling toolkits (which have
much greater storage costs), and at a rate approxi-
mately 15 times faster than a competitor approach
for space-efficient storage.
References
Djamal Belazzougui, Fabiano Botelho, and Martin Diet-
zfelbinger. 2009. Hash, displace, and compress. Al-
gorithms - ESA 2009, pages 682?693.
Burton H. Bloom. 1970. Space/time trade-offs in
hash coding with allowable errors. Commun. ACM,
13(7):422?426.
Thorsten Brants and Alex Franz. 2006. Google Web
1T 5-gram Corpus, version 1. Linguistic Data Con-
sortium, Philadelphia, Catalog Number LDC2006T13,
September.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: an efficient
data structure for static support lookup tables. In
SODA ?04, pages 30?39, Philadelphia, PA, USA.
271
Philip Clarkson and Ronald Rosenfeld. 1997. Statis-
tical language modeling using the CMU-cambridge
toolkit. In Proceedings of ESCA Eurospeech 1997,
pages 2707?2710.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In StatMT ?06: Proceedings of the
Workshop on Statistical Machine Translation, pages
94?101, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Marcello Federico and Mauro Cettolo. 2007. Efficient
handling of n-gram language models for statistical ma-
chine translation. In StatMT ?07: Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 88?95, Morristown, NJ, USA. Association for
Computational Linguistics.
Edward Fredkin. 1960. Trie memory. Commun. ACM,
3(9):490?499.
Kimmo Fredriksson and Fedor Nikitin. 2007. Simple
compression code supporting random access and fast
string matching. In Proc. of the 6th International
Workshop on Efficient and Experimental Algorithms
(WEA?07), pages 203?216.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly packed tries: How to fit large models into
memory, and make them load fast, too. Proceedings of
the Workshop on Software Engineering, Testing, and
Quality Assurance for Natural Language (SETQA-
NLP 2009), pages 31?39.
Joshua Goodman and Jianfeng Gao. 2000. Language
model size reduction by pruning and clustering. In
Proceedings of ICSLP?00, pages 110?113.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium, catalog number LDC2003T05.
Boulos Harb, Ciprian Chelba, Jeffrey Dean, and Sanjay
Ghemawat. 2009. Back-off language model compres-
sion. In Proceedings of Interspeech, pages 352?355.
Bo-June Hsu and James Glass. 2008. Iterative language
model estimation:efficient data structure & algorithms.
In Proceedings of Interspeech, pages 504?511.
F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss I.
1990. Self-organized language modeling for speech
recognition. In Readings in Speech Recognition, pages
450?506. Morgan Kaufmann.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In ACL ?07:
Proceedings of the 45th Annual Meeting of the ACL on
Interactive Poster and Demonstration Sessions, pages
177?180, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904, Denver.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. Proceed-
ings of ACL-08 HLT, pages 505?513.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL 07, pages 512?519, Prague,
Czech Republic, June.
David Talbot and Miles Osborne. 2007b. Smoothed
bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of EMNLP, pages 468?476.
David Talbot and John M. Talbot. 2008. Bloom maps.
In 4th Workshop on Analytic Algorithmics and Com-
binatorics 2008 (ANALCO?08), pages 203?212, San
Francisco, California.
David Talbot. 2009. Succinct approximate counting of
skewed data. In IJCAI?09: Proceedings of the 21st
international jont conference on Artifical intelligence,
pages 1243?1248, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Sebastiano Vigna. 2008. Broadword implementation of
rank/select queries. In WEA?08: Proceedings of the
7th international conference on Experimental algo-
rithms, pages 154?168, Berlin, Heidelberg. Springer-
Verlag.
Edward Whittaker and Bhinksha Raj. 2001.
Quantization-based language model compres-
sion. Technical report, Mitsubishi Electric Research
Laboratories, TR-2001-41.
272
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 289?292,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Evaluation Metrics for the Lexical Substitution Task
Sanaz Jabbari Mark Hepple Louise Guthrie
Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield, S1 4DP, UK
{S.Jabbari,M.Hepple,L.Guthrie}@dcs.shef.ac.uk
Abstract
We identify some problems of the evaluation
metrics used for the English Lexical Substitu-
tion Task of SemEval-2007, and propose al-
ternative metrics that avoid these problems,
which we hope will better guide the future de-
velopment of lexical substitution systems.
1 Introduction
The English Lexical Substitution task at SemEval-
2007 (here called ELS07) requires systems to find
substitutes for target words in a given sentence (Mc-
Carthy & Navigli, 2007: M&N). For example, we
might replace the target word match with game in
the sentence they lost the match. System outputs are
evaluated against a set of candidate substitutes pro-
posed by human subjects for test items. Targets are
typically sense ambiguous (e.g. match in the above
example), and so task performance requires a com-
bination of word sense disambiguation (by exploit-
ing the given sentential context) and (near) synonym
generation. In this paper, we discuss some problems
of the evaluation metrics used in ELS07, and then
propose some alternative measures that avoid these
problems, and which we believe will better serve to
guide the development of lexical substitution sys-
tems in future work.1 The subtasks within ELS07
divide into two groups, in terms of whether they fo-
cus on a system?s ?best? answer for a test item, or ad-
dress the broader set of answer candidates a system
1We consider here only the case of substituting for single
word targets. Subtasks of ELS07 involving multi-word substi-
tutions are not addressed.
can produce. In what follows, we address these two
cases in separate sections, and then present some re-
sults for applying our new metrics for the second
case. We begin by briefly introducing the test ma-
terials that were created for the ELS07 evaluation.
2 Evaluation Materials
Briefly stated, the ELS07 dataset comprises around
2000 sentences, providing 10 test sentences each
for some 201 preselected target words, which were
required to be sense ambiguous and have at least
one synonym, and which include nouns, verbs, ad-
jectives and adverbs. Five human annotators were
asked to suggest up to three substitutes for the tar-
get word of each test sentence, and their collected
suggestions serve as the gold standard against which
system outputs are compared. Around 300 sentences
were distributed as development data, and the re-
mainder retained for the final evaluation.
To assist defining our metrics, we formally de-
scribe this data as follows.2 For each sentence t
i
in the test data (1 ? i ? N , N the number of test
items), let H
i
denote the set of human proposed sub-
stitutes. A key aspect of the data is the count of hu-
man annotators that proposed each candidate (since
a term appears a stronger candidate if proposed by
annotators). For each t
i
, there is a function freq
i
which returns this count for each term within H
i
(and 0 for any other term), and a value maxfreq
i
corresponding to the maximal count for any term in
H
i
. The pairing of H
i
and freq
i
in effect provides a
multiset representation of the human answer set. We
2For consistency, we also restate the original ELS07 metrics
in these terms, whilst preserving their essential content.
289
use |S|i in what follows to denote the multiset car-
dinality of S according to freq
i
, i.e. ?
a?S
freq
i
(a).
Some of the ELS07 metrics use a notion of mode
answer m
i
, which exists only for test items that
have a single most-frequent human response, i.e.
a unique a ? H
i
such that freq
i
(a) = maxfreq
i
.
To adapt an example from M&N, an item with tar-
get word happy (adj) might have human answers
{glad ,merry , sunny , jovial , cheerful } with counts
(3,3,2,1,1) respectively. We will abbreviate this an-
swer set as H
i
= {G:3,M:3,S:2,J:1,Ch:1} where it
is used later in the paper.
3 Best Answer Measures
Two of the ELS07 tasks address how well systems
are able to find a ?best? substitute for a test item, for
which individual test items are scored as follows:
best(i) =
?
a?A
i
freq
i
(a)
|H
i
|
i
? |A
i
|
mode(i) =
{
1 if bg
i
= m
i
0 otherwise
For the first task, a system can return a set of an-
swers A
i
(the answer set for item i), but since the
score achieved is divided by |A
i
|, returning multiple
answers only serves to allow a system to ?hedge its
bets? if it is uncertain which candidate is really the
best. The optimal score on a test item is achieved by
returning a single answer whose count is maxfreq
i
,
with proportionately lesser credit being received for
any answer in H
i
with a lesser count. For the sec-
ond task, which uses the mode metric, only a single
system answer ? its ?best guess? bg
i
? is allowed,
and the score is simply 0 or 1 depending on whether
the best guess is the mode. Overall performance is
computed by averaging across a broader set of test
items (which for the second task includes only items
having a mode value). M&N distinguish two over-
all performance measures: Recall, which averages
over all relevant items, and Precision, which aver-
ages only over those items for which the system gave
a non-empty response.
We next discuss these measures and make an al-
ternative proposal. The task for the first measure
seems a reasonable one, i.e. assessing the ability of
systems to provide a ?best? answer for a test item,
but allowing them to offer multiple candidates (to
?hedge their bets?). However, the metric is unsatis-
factory in that a system that performs optimally in
terms of this task (i.e. which, for every test item, re-
turns a single correct ?most frequent? response) will
get a score that is well below 1, because the score is
also divided by |H
i
|
i, the multiset cardinality of H
i
,
whose size varies between test items (being a con-
sequence of the number of alternatives suggested by
the human annotators), but which is typically larger
than the numerator value maxfreq
i
of an optimal an-
swer (unless H
i
is singleton). This problem is fixed
in the following modified metric definition, by di-
viding instead by maxfreq
i
, as then a response con-
taining a single optimal answer will score 1.
best(i) =
?
a?A
i
freq
i
(a)
maxfreq
i
? |A
i
|
best
1
(i) =
freq
i
(bg
i
)
maxfreq
i
With H
i
= {G:3,M:3,S:2,J:1,Ch:1}, for example,
an optimal response A
i
= {M} receives score 1,
where the original metric gives score 0.3. Singleton
responses containing a correct but non-optimal an-
swer receive proportionately lower credit, e.g. for
A
i
= {S} we score 0.66 (vs. 0.2 for the origi-
nal metric). For a non-singleton answer set includ-
ing, say, a correct answer and an incorrect one, the
credit for the correct answer will be halved, e.g. for
A
i
= {S,X} we score 0.33.
Regarding the second task, we think it reasonable
to have a task where systems may offer only a single
?best guess? response, but argue that the mode met-
ric used has two key failings: it is too brittle in being
applicable only to items that have a mode answer,
and it loses information valuable to system rank-
ing, in assigning no credit to a response that might
be good but not optimal. We propose instead the
best
1
metric above, which assigns score 1 to a best
guess answer with count maxfreq
i
, but applies to all
test items irrespective of whether or not they have
a unique mode. For answers having lesser counts,
proportionately less credit is assigned. This metric
is equivalent to the new best metric shown beside it
for the case where |A
i
| = 1.
For assessing overall performance, we suggest
just taking the average of scores across all test items,
c.f. M&N?s Recall measure. Their Precision met-
ric is presumably intended to favour a system that
can tell whether it does or does not have any good
answers to return. However, the ability to draw a
290
boundary between good vs. poor candidates will be
reflected widely in a system?s performance and cap-
tured elsewhere (not least by the coverage metrics
discussed later) and so, we argue, does not need to
be separately assessed in this way. Furthermore, the
fact that a system does not return any answers may
have other causes, e.g. that its lexical resources have
failed to yield any substitution candidates for a term.
4 Measures of Coverage
A third task of ELS07 assesses the ability of systems
to field a wider set of good substitution candidates
for a target, rather than just a ?best? candidate. This
?out of ten? (oot) task allows systems to offer a set
A
i
of upto 10 guesses per item i, and is scored as:
oot(i) =
?
a?A
i
freq
i
(a)
|H
i
|
i
Since the score is not divided by the answer set
size |A
i
|, no benefit derives from offering less than
10 candidates.3 When systems are asked to field a
broader set of candidates, we suggest that evalua-
tion should assess if the response set is good in con-
taining as many correct answers as possible, whilst
containing as few incorrect answers as possible. In
general, systems will tackle this problem by com-
bining a means of ranking candidates (drawn from
lexical resources) with a means of drawing a bound-
ary between good and bad candidates, e.g. thresh-
old setting.4 Since the oot metric does not penalise
incorrect answers, it does not encourage systems to
develop such boundary methods, even though this is
important to their ultimate practical utility.
The view of a ?good? answer set described above
suggests a comparison of A
i
to H
i
using versions
of ?recall? and ?precision? metrics, that incorporate
the ?weighting? of human answers via freq
i
. Let us
begin by noting the obvious definitions for recall and
3We do not consider here a related task which assesses
whether the mode answer m
i
is found within an answer set of
up to 10 guesses. We do not favour the use of this metric for
reasons parallel to those discussed for the mode metric of the
previous section, i.e. brittleness and information loss.
4In Jabbari et al (2010), we define a metric that directly
addresses the ability of systems to achieve good ranking of sub-
stitution candidates. This is not itself a measure of lexical sub-
stitution task performance, but addresses a component ability
that is key to the achievement of lexical substitution tasks.
precision metrics without count-weighting:
R(i) =
|H
i
?A
i
|
|H
i
|
P (i) =
|H
i
?A
i
|
|A
i
|
Our definitions of these metrics, given below, do
include count-weighting, and require some explana-
tion. The numerator of our recall definition is |A
i
|
i
not |H
i
? A
i
|
i as |A
i
|
i
= |H
i
? A
i
|
i (as freq
i
as-
signs 0 to any term not in H
i
), an observation which
also affects the numerator of our P definition. Re-
garding the latter?s denominator, merely dividing by
|A
i
|
i would not penalise incorrect terms (as, again,
freq
i
(a) = 0 for any a /? H
i
), so this is done di-
rectly by adding k|A
i
?H
i
|, where |A
i
?H
i
| is the
number of incorrect answers, and k some penalty
factor, which might be k = 1 in the simplest case.
(Note that our weighted R metric is in fact equiv-
alent to the oot definition above.) As usual, an F-
score can be computed as the harmonic mean of
these values (i.e. F = 2PR/(P + R)). For as-
sessing overall performance, we might average P ,
R and F scores across all test items.
R(i) =
|A
i
|
i
|H
i
|
i
P (i) =
|A
i
|
i
|A
i
|
i
+ k|A
i
?H
i
|
With H
i
= {G:3,M:3,S:2,J:1,Ch:1}, for example,
the perfect response set A
i
= {G,M,S, J,Ch}
gives P and R scores of 1. The response
A
i
= {G,M,S, J,Ch,X, Y, Z, V,W}, containing
all correct answers plus 5 incorrect ones, gets R =
1, but only P = 0.66 (assuming k = 1, giving
10/(10 + 5)). The response A
i
= {G,S, J,X, Y },
with 3 out of 5 correct answers, plus 2 incorrect
ones, gets R = 0.6 (6/10) and P = 0.75 (6/6 + 2))
5 Applying the Coverage measure
Although the ?best guess? task is a valuable indicator
of the likely utility of a lexical substitution system
within various broader applications, we would argue
that the core task for lexical substitution is coverage,
i.e. the ability to field a broad set of correct substi-
tution candidates. This task requires systems both to
field and rank promising candidates, and to have a
means of drawing a boundary between the good and
bad candidates, i.e. a boundary strategy.
In this section, we apply the coverage metrics to
the outputs of some lexical substitution systems, and
291
Model 1 2 3 4 5 6 7 8 9 10
bow .067 .114 .151 .173 .191 .201 .212 .219 .222 .225
lm .119 .192 .228 .246 .256 .267 .271 .272 .271 .271
cmlc .139 .205 .251 .271 .284 .288 .291 .290 .289 .286
KU .173 .244 .287 .307 .318 .321 .320 .318 .314 .311
Table 3: Coverage F-scores (macro-avgd), for simple boundary strategies (with penalty factor k = 1).
All By part-of-speech
Model words nouns adj verb adv
bow .326 .343 .334 .205 .461
lm .393 .372 .442 .252 .562
cmlc .414 .404 .447 .311 .534
KU .462 .408 .511 .398 .567
Table 1: Out-of-ten recall scores for all the systems (with
a subdivision by pos of target item).
All By part-of-speech
Model words nouns adj verb adv
bow .298 .315 .302 .189 .422
lm .371 .35 .408 .24 .539
cmlc .395 .383 .419 .31 .506
KU .435 .379 .477 .385 .536
Table 2: Optimal F-scores (macro-avgd) for coverage,
computed over the (oot) ranked outputs of the systems
(with penalty factor k = 1).
compare the indication it provides of relative sys-
tem performance to that of the oot metric. We con-
sider three systems described in Jabbari (2010), de-
veloped as part of an investigation into the means
and benefits of combining models of lexical context:
(i) bow: a system using a bag-of-words model to
rank candidates, (ii) lm: using a (simple) n-gram lan-
guage model, and (iii) cmlc: using a model that com-
bines bow and lm models into one. We also consider
the system KU, which uses a very large language
model and an advanced treatment of smoothing, and
which performed well at ELS07 (Yuret, 2007).5 Ta-
ble 1 shows the oot scores for these systems, includ-
ing a breakdown by part-of-speech, which indicate a
performance ranking: bow < lm < cmlc < KU
Our first problem is that these systems are devel-
oped for the oot task, not coverage, so after rank-
5We thank Deniz Yuret for allowing us to use his system?s
outputs in this analysis.
ing their candidates, they do not attempt to draw
a boundary between the candidates worth returning
and those not. Instead, we here use the oot out-
puts to compute an optimal performance for each
system, i.e. we find, for the ranked candidates of
each question, the cut-off position giving the high-
est F-score, and then average these scores across
questions, which tells us the F-score the system
could achieve if it had an optimal boundary strategy.
These scores, shown in Table 2, indicate a ranking of
systems in line with that in Table 1, which is not sur-
prising as both will ultimately reflect the quality of
candidate ranking achieved by the systems.
Table 3 shows the coverage results achieved by
applying a naive boundary strategy to the system
outputs. The strategy is just to always return the
top n candidates for each question, for a fixed value
n. Again, performance correlates straightforwardly
with the underlying quality of ranking. Comparing
tables, we see, for example, that by always returning
6 candidates, the system KU could achieve a cover-
age of .32 as compared to the .435 optimal score.
References
D. McCarthy and R. Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task.
Proc. of the 4th Int. Workshop on Semantic Eval-
uations (SemEval-2007), Prague.
S. Jabbari. 2010. A Statistical Model of Lexical Con-
text, PhD Thesis, University of Sheffield.
S. Jabbari, M. Hepple and L.Guthrie. 2010. Evaluat-
ing Lexical Substitution: Analysis and NewMea-
sures. Proc. of the 7th Int. Conf. on Language
Resources and Evaluation (LREC-2010). Malta.
D. Yuret. 2007. KU: Word Sense Disambiguation by
Substitution. In Proc. of the 4th Int. Workshop on
Semantic Evaluations (SemEval-2007), Prague.
292
LAW VIII - The 8th Linguistic Annotation Workshop, pages 93?98,
Dublin, Ireland, August 23-24 2014.
Part-of-speech Tagset and Corpus Development for Igbo, an African
Language
Ikechukwu E. Onyenwe
Dept. of Computer Science,
University of Sheffield
Sheffield S1 4DP, UK
i.onyenwe@shef.ac.uk
Dr. Chinedu Uchechukwu
Dept. of Linguistics
Nnamdi Azikiwe University
Anambra State, Nigeria
neduchi@yahoo.com
Dr. Mark Hepple
Dept. of Computer Science,
University of Sheffield
Sheffield S1 4DP, UK
m.r.hepple@shef.ac.uk
Abstract
This project aims to develop linguistic resources to support computational NLP research on the
Igbo language. The starting point for this project is the development of a new part-of-speech tag-
ging scheme based on the EAGLES tagset guidelines, adapted to incorporate additional language
internal features. The tags are currently being used in a part-of-speech annotation task for the
development of POS tagged Igbo corpus. The proposed tagset has 59 tags.
1 Introduction
Supervised machine learning methods in NLP require an adequate amount of training data. The first
crucial step for a part-of-speech (POS) tagging system for a language is a well designed, consistent, and
complete tagset (Bamba Dione et al., 2010) which must be preceded by a detailed study and analysis of
the language. Our tagset was developed from scratch through the study of linguistics and electronic texts
in Igbo, using the EAGLES recommendations.
This initial manual annotation is important. Firstly, information dealing with challenging phenomena
in a language is expressed in the tagging guideline; secondly, computational POS taggers require anno-
tated text as training data. Even in unsupervised methods, some annotated texts are still required as a
benchmark in evaluation. With this in mind, our tagset design follows three main goals: to determine
the tagset size, since a smaller granularity provides higher accuracy and less ambiguity (de Pauwy et al.,
2012); to use a sizeable scheme to capture the grammatical distinctions at a word level suited for further
grammatical analysis, such as parsing; and to deliver good accuracy for automatic tagging, using the
manually tagged data. We discuss the development of the tagset and corpus for Igbo. This work is, to the
best of our knowledge, the first published work attempting to develop statistical NLP resources for Igbo.
2 Some Grammatical Features of the Igbo Language
2.1 Language family and speakers
The Igbo language has been classified as a Benue-Congo language of the Kwa sub-group of the Niger-
Congo family
1
and is one of the three major languages in Nigeria, spoken in the eastern part of Nigeria,
with about 36 million speakers
2
. Nigeria is a multilingual country having around 510 living languages
1
,
but English serves as the official language.
2.2 Phonology
Standard Igbo has eight vowels and thirty consonants. The 8 vowels are divided into two harmony groups
that are distinguished on the basis of the Advanced Tongue Root (ATR) phenomenon. They are -ATR: i
.
[I], u
.
[U], a [A], o
.
[O] and +ATR: i [i], u [u], e [e], o [o] (Uchechukwu, 2008). Many Igbo words select
their vowels from the same harmony group. Also, Igbo is a tonal language. There are three distinct tones
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://nigerianwiki.com/wiki/Languages
2
http://en.wikipedia.org/wiki/Igbo_people
93
recognized in the language viz; High, Low, and Downstep. The tones are represented as High [H] =
[? ], Low [L] = [` ], downstep = [? ] (Emenanjo, 1978; Ikekeonwu, 1999) and are placed above the tone
bearing units (TBU) of the language.
There are two tone marking systems, either: all high tones are left unmarked and all low tones and
downsteps are marked (Green and Igwe, 1963; Emenanjo, 1978), or only contrastive tones are marked
(Welmers and Welmers, 1968; Nwachukwu, 1995). We used the first system to illustrate the importance
of tonal feature in the language?s lexical or grammatical structure. For example, at the lexical level the
word akwa without a tone mark can be given the equivalent of ?bed/bridge?, ?cry?, ?cloth?, or ?egg?.
But these equivalents can be properly distinguished when tone marked, as follows: akwa ?cry?, akw?
?cloth?, ?kw? ?bed or brigde?, ?kwa ?egg?. At the grammatical level, an interrogative sentence can be
distinguished from a declarative sentence through a change in tone of the person pronouns from a high
tone (e.g. O
.
n?-?bi
.
a ?He is coming?) to a low tone (e.g. ?
.
n?-?bi
.
a ?Is he coming??). Also, there are
syllabic nasal consonants, which are tone bearing units in the language. The nasal consonants always
occur before a consonant. For example: `ndo ?Sorry? or explicitly tone marked as `nd?.
2.3 Writing System
The Igbo orthography is based on the Standard Igbo by the O
.
nwu
.
Committee (O
.
nwu
.
Committee, 1961).
There are 28 consonants: b gb ch d f g gh gw h j k kw kp l m n nw ny ?n p r s sh t v w y z, and 8 vowels
(see phonology section). Nine of the consonants are digraphs: ch, gb, gh, gw, kp, kw, nw, ny, sh.
Igbo is an agglutinative language in which its lexical categories undergo affixation, especially the
verbs, to form a lexical unit. For example, the word form erichari
.
ri
.
is a verbal structure with four
morphemes: verbal vowel prefix e-, verb root -ri-, extensional suffix -cha-, and a second extensional
suffix -ri
.
ri
.
. Its occurrence in the sentence ?Obi must eat up that food? is Obi ga-erichari
.
ri
.
nri ahu
.
,
that is, Obi aux-eat.completely.must food DET. Igbo word order is Subject-Verb-Object (SVO), with a
complement to the right of the head.
2.4 Grammatical Classes
Generally, Emenanjo (1978) identified the following broad word classes for Igbo: verbal, nominal, nom-
inal modifier, conjunction, preposition, suffixes, and enclitics. The verbal is made up of verbs, auxiliaries
and participles, while the nominal is made up of nouns, numerals, pronouns and interrogatives. Nouns are
further classified into five lexical classes, viz; proper, common, qualificative, adverbial and ideophones.
However, we identified extra five in the tagset design phase (see the appendix). Nominal modifiers occur
in a noun phrase. Its four classes are adjectives, demonstratives, quantifiers and pronominal modifiers.
Conjunctions link words or sentences together, while prepositions are found preceding nominals and ver-
bals and cannot be found in isolation. Suffixes and enclitics are the only bound elements in the language.
Suffixes are primarily affixed to verbals only, while enclitics are used with both verbals and other word
classes. Suffixes are found in verb phrase slots and enclitics can be found in both verb phrase and noun
phrase slots. The language does not have a grammatical gender system.
3 Language Resources
The development of NLP resources for any language is based on the linguistics resources available for the
language. This includes appropriate fonts and text processing software as well as the available electronic
texts for the work. The font and software problems of the language have been addressed through the
Unicode development (Uchechukwu, 2005; Uchechukwu, 2006). The next is the availability of Igbo
texts.
Any effort towards the Igbo corpus development is a non-trivial task. There are basic issues connected
with the nature of the language. The first major surprise is that Igbo texts ?by native speakers? written
?for native speakers? vary in forms due to dialectal difference and are usually not tone-marked. Indeed,
the tone marking used in the sections above are usually found in academic articles. It would be strange
to find an Igbo text (literary work) that is fully tone marked and no effort has been made to undertake a
tone marking of existing Igbo texts. Such an effort looks impossible as more Igbo texts are written and
94
published. Such is the situation that confronts any effort to develop an Igbo corpus. Hence, developing
NLP resources for the language has to start with the available resources; otherwise, such an endeavour
would have to first take a backward step of tone marking all the texts to be added to its corpus and
normalizing the dialectal differences. This is a no mean task.
It is for this reason that we chose the New World Translation (NWT) Bible version for Igbo corpus
with its English parallel text
3
. The NWT Bible does not adopt a particular tone marking system, neither
is there a consistent use of tone marks for all the sentences in the Bible. Instead, there is narrow use
of tone marks in specific and restricted circumstances throughout the book. An example is when there
is a need to disambiguate a particular word. For instance, ihe without tone mark could mean ?thing? or
?light?. These two are always tone marked in the Bible to avoid confusion; hence ?h? ?light? and ?h?
?thing?. The same applies to many other lexical items. Another instance is the placement of a low tone
on the person pronouns to indicate the onset of an interrogative sentence, which otherwise would be
read as a declarative sentence. This particular example has already been cited as one of the uses of tone
mark in the language. Apart from such instances, the sentences in the Bible are not tone marked. As
such, one cannot rely on such restricted use of tone marks for any major conclusions on the grammar of
the language. With regard to corpus work in general, the Bible has been described as consistent in its
orthography, most easily accessible, carefully translated (most translators believe it is the word of God),
and well structured (books, chapters, verses), etc. (Resnik et al., 1999; Kanungo and Resnik, 1999; Chew
et al., 2006). The NWT Bible is generally written in standard Igbo.
4 Tokenization
We outline here the method we used in the tokenization of the text. For the sake of a start-up, we
tokenized based on the whitespace. The Igbo language uses whitespace to represent lexical boundaries;
we used the following regex:
Separate characters if the string matches:
? ?ga-? or ?n?? or ?N?? or ?na-? or ?Na-? or ?ana-? or ?i
.
na-?; for example, the following
samples n?elu, na?erughari
.
, i
.
na-akwa, ana-egbu in the Bible will be separated into n?,
elu, na?, erughari
.
, i
.
na-, akwa, ana-, egbu tokens.
? Any non-zero length sequence consisting of a?z, A?Z, 0?9, combining grave accent
(` ), combining acute accent (? ), combining dot below (
.
); for example, these words
?h?, ah?
.
, ?j? in the corpus will be separated as tokens with their diacritics.
? Any single character from: left double-quotation mark (?), right double-quotation
mark (?), comma (,), colon (:), semicolon (;), exclamation (!), question (?), dot (.).
? Any single non-whitespace character.
In place of sentence splitting, we use verses since all 66 books of the Bible is written in verse level. Our
major aim is to use this Igbo corpus to implement our new tagset, which will capture all the inflected and
non-inflected tokens in the corpus. For lack of space, issues with tokenization with respect to morphemes,
manual annotation implemetations and platform used will not be discussed in this paper.
5 Tagset Design
We adopt the (Leech, 1997) definition of a POS tagset as a set of word categories to be applied to the
tokens of a text. We designed our tagset following the standard EAGLES guidelines, diverging where
necessary (e.g. EAGLES, which favours European languages, specifies articles at the obligatory level,
but this category does not apply for Igbo). A crucial question in tagset design is the extent of fine-grained
distinctions to encode within the tagset. A too coarsely grained tagset may fail to capture distinctions that
would be valuable for subsequent analysis, e.g. syntactic parsing; too fine-grained may make automatic
(and manual) POS tagging difficult, resulting in errors that lead to different problems for later processing.
In what follows, we introduce a sizeable tagset granularity with the intention of providing a basis for
practical POS tagging.
3
Obtained from jw.org.
95
NNM Number marking nouns NNT Instrumental nouns
NNQ Qualificative nouns VrV ?rV implies suffix
NND Adverbial nouns VCJ Conjunctional verbs
NNH Inherent complement nouns ?_XS any POS tag with affixes
NNA Agentive nouns
Table 1: Selected distinctive tags from the tagset scheme
The tagset is intended to strike an appropriate balance for practical purposes regarding granularity,
capturing what we believe will be the key lexico-grammatical distinctions of value for subsequent pro-
cessing, such as parsing. Further subcategorization of the grammatical classes, as described in section
2.4, results in 59 tags which apply to whole tokens (produced by the tokenisation stage described above).
An important challenge comes from the complex morphological behaviour of Igbo. Thus, a verb such
as bi
.
a, which we assign the tag VSI (a verb in its simple or base form), can combine with extensional
suffixes, such as ghi
.
and kwa, to produce variants such as bi
.
aghi
.
, bi
.
akwa and bi
.
aghi
.
kwa, which exhibit
similar grammatical behaviour to the base form. As such, we might have assigned these variants the VSI
tag also, but have instead chosen to assign VSI_XS, which serves to indicate both the core grammatical
behaviour and the presence of extensional suffixes. In abi
.
akwa, we find the same base form bi
.
a, plus a
verbal vowel prefix a, resulting in the verb being a participle, which we assign the tag VPP_XS. For the
benefit of cross-lingual training and other NLP tasks, a smaller tagset that captures only the grammatical
distinctions between major classes is required. The present 59 tags can easily be simplified to a coarse-
grained tagset of 15 tags, which will principally preserve just the core distinctions between word classes,
such as nouns, verb, adjective, etc.
Athough Emenanjo (1978) classified ideophones as a form of noun, we have assigned them a sepa-
rate tag IDEO, as these items can be found performing many grammatical functions. For instance, the
ideophone ko
.
i
.
, ?to say that someone walks ko
.
i
.
ko
.
i
.
? has no nominal meaning, rather its function here is
adverbial. A full enumeration of this scheme is given in the appendix.
5.1 The developement of an POS tagged Igbo Corpus
Here we analyse the manual POS tagging process that is ongoing based on the tagset scheme. The
Bible books were allocated randomly to six groups, producing six corpora portions of approximately
45,000 tokens each. Our plan was for each human annotator to tag at least 1000 tokens per day, resulting
in complete POS tagging in 45 days. The overall corpus size allocated is 264,795 tokens of the new
testament Bible. There are six human annotators, who are students of the Department of Linguistics at
Nnamdi Azikiwe University, Awka, supervised by a senior lecturer in the same department; giving an
effective total of seven human annotators. Additionally, a common portion of the corpus (38,093 tokens)
was given to all the annotators, as a basis for calculating inter-annotator agreement.
6 Conclusions
We have outlined our current progress in the development of a POS tagging scheme for Igbo from scratch.
Our project aims to build linguistic computational resources to support research in natural language
processing (NLP) for Igbo. It is important to note that these tags are applicable on unmarked, not fully
marked, and fully tone marked Igbo texts, since the fully tone marked tokens play the same grammatical
roles as in the none tone marked texts, written by native speakers for fellow native speakers.
Our method of tagset design could be used for other African or under-resourced languages. African
languages are morphologically rich, and of around 2000 languages in the continent, only a small number
have featured in NLP research.
Acknowledgements
We acknowledge the support of Tertiary Education Trust Fund (TETFund) in Nigeria, and would like to
thank Mark Tice for his useful comments and help in preparing this paper.
96
References
Cheikh M. Bamba Dione, Jonas Kuhn, and Sina Zarrie?. 2010. Design and Development of Part-of-Speech-
Tagging Resources for Wolof (Niger-Congo, spoken in Senegal). In Proceedings of the Seventh International
Conference on Language Resources and Evaluation (LREC?10). ELRA).
Peter A. Chew, Steve J. Verzi, Travis L. Bauer, and Jonathan T. McClain. 2006. Evaluation of the Bible as a
Resource for Cross-Language Information Retrieval. In Proceedings of the Workshop on Multilingual Language
Resources and Interoperability. Association for Computational Linguistics.
E. No
.
lue Emenanjo. 1978. Elements of Modern Igbo Grammar: A Descriptive Approach. Ibadan Ox. Uni. Press.
Margaret M. Green and G. Egemba Igwe. 1963. A descriptive grammar of Igbo. London: Oxford University Press
and Berlin: Akademie-Verlag.
Clara Ikekeonwu. 1999. ?Igbo?, Handbook of the International Phonetic Association. C. U. Press.
Tapas Kanungo and Philip Resnik. 1999. The Bible, Truth, and Multilingual OCR Evaluation. In Proceedings of
SPIE Conf. on Document Recognition and Retrieval, pages 86?96.
Geoffrey Leech. 1997. Introducing Corpus Annotation. Longman, London.
P. Akujuoobi Nwachukwu. 1995. Tone in Igbo Syntax. Technical report, Nsukka: Igbo Language Association.
Guy de Pauwy, Gilles-Maurice de Schryverz, and Janneke van de Loo. 2012. Resource-Light Bantu Part-of-
Speech Tagging. In Proceedings of the Workshop on Language Technology for Normalisation of Less-Resourced
Languages, pages 85?92.
Philip Resnik, Mari Broman Olsen, and Mona Diab. 1999. The Bible as a Parallel Corpus: Annotating the ?Book
of 2000 Tongues?. Computers and the Humanities, 33.
Chinedu Uchechukwu. 2005. The Representation of Igbo with the Appropriate Keyboard. In Clara Ikekeonwu
and Inno Nwadike, editors, Igbo Lang. Dev.: The Metalanguage Perspective, pages 26?38. CIDJAP Enugu.
Chinedu Uchechukwu. 2006. Igbo Language and Computer Linguistics: Problems and Prospects. In Proceedings
of the Lesser Used Languages and Computer Linguistics Conference. European Academy (EURAC).
Chinedu Uchechukwu. 2008. African Language Data Processing: The Example of the Igbo Language. In 10th
International pragmatics conference, Data processing in African languages.
Beatrice F. Welmers and William E. Welmers. 1968. Igbo: A Learner?s Manual. Published by authors.
O
.
nwu
.
Committee. 1961. The Official Igbo Orthography.
A A Tagset Design for the Igbo Language
Noun Class
Tag Description/Example
NNP Noun Proper. Chineke ?God?, Onyeka, Okonkwo, Osita.
NNC Noun Common. Oku
.
?fire?, u
.
wa ?earth?, osisi ?tree, stick?, ala ?ground?, eluigwe ?sky, heaven?
NNM Number Marking Noun. Ndi
.
?people?, nwa ?child?, u
.
mu
.
?children?. ndi
.
is classified as a common noun with
an attached phrase of ?thing/person associated with? (Emenanjo, 1978). ndi
.
preceding a noun marks plurality
of that noun, nwa marks it singular (e.g. nwa agbo
.
gho
.
?a maiden?), and u
.
mu
.
also indicate plurality (e.g. u
.
mu
.
agbo
.
gho
.
?maidens?).
NNQ Qualificative noun. Nouns that are inherently semantically descriptive. E.g. ogologo [height, long, tall]
NND Adverbial noun. This lexical class function to modify verbals, e.g. O ji nwayo
.
o
.
eri nri ya
NNH Inherent Complement. Igbo verb has a [verb + NP/PP] structure. NP/PP are the verb complement. They
cooccur with the verb, at times quite distant from the verb, e.g. (1) i
.
gu
.
egwu ?to sing? , (2) iti i
.
gba ?to drum?, (3)
igwu ji ?harvest yam?.
NNA Agentive Noun. Nouns are formed through verbs nominalization. Compare (1) with o
.
go
.
egwu ?singer? and (2)
with oti i
.
gba ?drummer?. For links NNAV . . . NNAC.
NNT Instrumental Noun. Refer to instruments and are formed via nominalization. Compare (3) with ngwu ji
?digger?. For links NNTV . . . NNTC.
NOTE: We introduced link indicators in NNA and NNT, V and C, Where V and C stand for verbal and Complementary
respectively. So, NNAV indicates derivation from the verbal component of the inherent complement verb and NNAC is the
inherent complement of the whole verbal complex. E.g., o
.
gu
.
/NNAV egwu/NNAC. Also, NNTV and NNTC, where NNTV
is derived from the verbal component of the inherent complement verb and NNTC is the inherent complement of the whole
verbal complex. E.g. , ngwu/NNTV ji/NNTC
97
Verb Class
VIF Infinitive. Marked through the addition of the vowel [i] or [i
.
] to the verb root.
VSI Simple verb. Has only one verb root.
VCO Compound Verb. Involves a combination of two verb roots.
VIC Inherent Complement Verb (ICV). Involves the combination of a simple or compound verb with a noun phrase
or a prepositional phrase. It gives rise to the structures (1) V + NP, or (2) V + PP
VMO Modal Verb. Its formed by inherent complement verbs and simple verbs. [See the section on suffixes]
VAX Auxiliary Verb. ga [Future marking], na [progressive]
VPP Participle. Always occurs after the auxiliary, and prefixed e/a to the verb root using vowel harmony.
VCJ Conjunctional Verb. A verb that has a conjuntional meaning, especially in narratives: wee
VBC
(BVC)
Bound Verb Complment or Bound Cognate Noun. Its formed by harmonizing prefix a/e to the verb root. It
looks like the participle but occurs after the participle in same sentence as the verb. It can be formed from every
verb.
VGD Gerund. Reduplication of the verb root plus harmonizing vowel o/o
.
. Also, internal vowel changes can occur.
E.g. ba ?enter? [o
.
+ bu
.
+ ba ]=o
.
bu
.
ba ?the entering?
Inflectional Class
VrV ?rV (e.g. -ra). If attached to an active verb, it means simple past; but a stative meaning with a stative verb.
VPERF Perfect (e.g. -la/-le, -go). Describes the ?perfect tense?. -la/-le obeys vowel harmony and the variant -go does
not.
Other part-of-speech tags
ADJ Adjective. The traditional part of speech ?adjective? that qualifies a noun. Igbo has very few of them.
PRN Pronoun. The 3 persons are 1st (sing + pl), 2nd (sing + pl), and 3rd (sing + pl) person Pronouns.
PRNREF Reflexive Pronoun. Formed by combination of the personal pronouns with the noun onwe ?self?.
PRNEMP Emphatic pronoun. This involves the structure [pronoun+onwe+pronoun].
ADV Adverb. Changes or simplifies the meaning of a verb. They are few in Igbo.
CJN Conjunction. There are complex and simple conjunctions distinguish based on grammatical functions viz;
co-rodinators, sub-ordinators and correlatives. Link indicators CJN1...CJN2 are for ?correlative CJN?. E.g.
ma/CJN1...ma/CJN2.
PREP Preposition. The preposition na is realised as n? if the modified word begins with a vowel.
WH Interrogative. Questions that return useful data through explanation. ?nye, gi
.
ni
.
, olee, ...
PRNYNQ Pronoun question. Questions that return YES or NO answer. E.g. `m, ?, h?, ?, `o
.
, ...
IDEO Ideophone. This is used for sound-symbolic realization of various lexico-grammatical function. E.g. ni
.
gani
.
ga,
mu
.
ri
.
i
.
, ko
.
i
.
, etc.
QTF Quantifier. This can be found after their nominals in the NP structure. E.g. dum, naabo
.
, nille.
DEM Demonstrative. This is made up of only two deictics and always used after their nominals. E.g. a, ahu
.
.
INTJ Interjection.Ee
FW Borrowed word. amen.
SYM Punctuation. It includes all symbols.
CD Number. This includes all digits 1,2,3, ... and otu, mbu
.
, abu
.
a, ato
.
, ...
DIGR Digraph. All combined graphemes that represent a character in Igbo, which occur in the text. gb, gw, kp, nw, ...
TTL Title . Includes foreign and Igbo titles. E.g. Maazi
.
.
CURN Currency.
ABBR Abbreviation.
Any type of suffixes
?_XS any POS tag with affixes. for ? ? {VIF, VSI, VCO, VPP, VGD, VAX, CJN, WH, VPERF, VrV, PREP, DEM,
QTF, ADJ,ADV}. See verb, other POS, inflectional classes.
NOTE: Tags with affixes identify inflected token forms in the corpus for use in further analysis, e.g. morphology. For
practical POS tagging, such tags may be simplified, i.e. ?_XS? ?.
Any type of Enclitics
ENC Collective. cha, si
.
nu
.
, ko
.
? means all, totality forming a whole or aggregate.
Negative Interrogative. di
.
, ri
.
, du
.
? indicates scorn or disrespect and are mainly used in Rhetorical Interroga-
tives.
Adverbial ?Immediate present and past?. fo
.
/hu
.
? it indicates action that is just/has just taking/taken place. ri
.
i
.
? indicates that an action/event has long taken place
Adverbial ?Additive?. kwa (kwo
.
), kwu ? mean ?also?, ?in addition to?, ?denoting?, ?repetition or emphasis?.
Adverbial ?Confirmative?. no
.
o
.
(no
.
o
.
; nno
.
o
.
) ? this means really or quite.
B The Major Classes of the Tagset
ADJ adjective FW foreign word QTF quantifier ADV adverb NNC common noun
INTJ interjection SYM symbol CJN conjunction NNP proper noun PREP preposition
WH interrogative PRN pronoun V verb CD number DEM demonstration
There is no article in the language.
98
