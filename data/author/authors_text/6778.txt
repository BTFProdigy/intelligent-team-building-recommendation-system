Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 9?16
Manchester, August 2008
A Supervised Algorithm for Verb Disambiguation into VerbNet Classes
Omri Abend1 Roi Reichart2 Ari Rappoport1
1Institute of Computer Science , 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
VerbNet (VN) is a major large-scale En-
glish verb lexicon. Mapping verb instances
to their VN classes has been proven use-
ful for several NLP tasks. However, verbs
are polysemous with respect to their VN
classes. We introduce a novel supervised
learning model for mapping verb instances
to VN classes, using rich syntactic features
and class membership constraints. We
evaluate the algorithm in both in-domain
and corpus adaptation scenarios. In both
cases, we use the manually tagged Sem-
link WSJ corpus as training data. For in-
domain (testing on Semlink WSJ data), we
achieve 95.9% accuracy, 35.1% error re-
duction (ER) over a strong baseline. For
adaptation, we test on the GENIA corpus
and achieve 72.4% accuracy with 10.7%
ER. This is the first large-scale experimen-
tation with automatic algorithms for this
task.
1 Introduction
The organization of verbs into classes whose mem-
bers exhibit similar syntactic and semantic behav-
ior has been discussed extensively in the linguistics
literature (see e.g. (Levin and Rappaport Hovav,
2005; Levin, 1993)). Such an organization helps
in avoiding lexicon representation redundancy and
enables generalizations across similar verbs. It
can also be of great practical use, e.g. in com-
pensating NLP statistical models for data sparse-
ness. Indeed, Levin?s seminal work had motivated
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
much research aimed at automatic discovery of
verb classes (see Section 2).
VerbNet (VN) (Kipper et al, 2000; Kipper-
Schuler, 2005) is a large scale, publicly available
domain independent verb lexicon that builds on
Levin classes and extends them with new verbs,
new classes, and additional information such as
semantic roles and selectional restrictions. VN
classes were proven beneficial for Semantic Role
Labeling (SRL) (Swier and Stevenson, 2005), Se-
mantic Parsing (Shi and Mihalcea, 2005) and
building conceptual graphs (Hensman and Dun-
nion, 2004). Levin-inspired classes have been
used in several NLP tasks, such as Machine Trans-
lation (Dorr, 1997) and Document Classification
(Klavans and Kan, 1998).
Many applications that use VN need to map verb
instances onto their VN classes. However, verbs
are polysemous with respect to VN classes. Sem-
link (Loper et al, 2007) is a dataset that maps each
verb instance in the WSJ Penn Treebank to its VN
class. The mapping has been created using a com-
bination of automatic and manual methods. Yi et
al. (2007) have used Semlink to improve SRL.
In this paper we present the first large-scale ex-
perimentation with a supervised machine learning
classification algorithm for disambiguating verb
instances to their VN classes. We use rich syntactic
features extracted from a treebank-style parse tree,
and utilize a learning algorithm capable of impos-
ing class membership constraints, thus taking ad-
vantage of the nature of our task. We use Semlink
as the training set.
We evaluate our algorithm in both in-domain
and corpus adaptation scenarios. In the former,
we test on the WSJ (using Semlink again), ob-
taining 95.9% accuracy with 35.1% error reduc-
tion (ER) over a strong baseline (most frequent
9
class) when using a modern statistical parser. In
the corpus adaptation scenario, we disambiguate
verbs in sentences taken from outside the train-
ing domain. Since the manual annotation of new
corpora is costly, and since VN is designed to be
a domain independent resource, adaptation results
are important to the usability in NLP in practice.
We manually annotated 400 sentences from GE-
NIA (Kim et al, 2003), a medical domain cor-
pus1. Testing on these, we achieved 72.4% ac-
curacy with 10.7% ER. Our adaptation scenario
is complete in the sense that the parser we use
was also trained on a different corpus (WSJ). We
also report experiments done using gold-standard
(manually created) parses.
The most relevant previous works addressing
verb instance class classification are (Lapata and
Brew, 2004; Li and Brew, 2007; Girju et al, 2005).
The former two do not use VerbNet and their ex-
periments were narrower than ours, so we can-
not compare to their results. The latter mapped to
VN, but used a preliminary highly restricted setup
where most instances were monosemous. For
completeness, we compared our method to theirs2,
achieving similar results.
We review related work in Section 2, and dis-
cuss the task in Section 3. Section 4 introduces the
model, Section 5 describes the experimental setup,
and Section 6 presents our results.
2 Related Work
VerbNet. VN is a major electronic English verb
lexicon. It is organized in a hierarchical struc-
ture of classes and sub-classes, each sub-class in-
heriting the full characterization of its super-class.
VN is built on a refinement of the Levin classes,
the intersective Levin classes (Dang et al, 1998),
aimed at achieving more coherent classes both se-
mantically and syntactically. VN was also sub-
stantially extended (Kipper et al, 2006) using the
Levin classes extension proposed in (Korhonen
and Briscoe, 2004). VN today contains 3626 verb
lemmas (forms), organized in 237 main classes
having 4991 verb types (we refer to a lemma with
an ascribed class as a type). Of the 3626 lem-
mas, 912 are polysemous (i.e., appear in more
than a single class). VN?s significant coverage of
the English verb lexicon is demonstrated by the
1Our annotations will be made available to the community.
2Using the same sentences and instances, obtained from
the authors.
75.5% coverage of VN classes over PropBank3
instances (Loper et al, 2007). Each class con-
tains rich semantic information, including seman-
tic roles of the arguments augmented with se-
lectional restrictions, and possible subcategoriza-
tion frames consisting of a syntactic description
and semantic predicates with temporal informa-
tion. VN thematic roles are relatively coarse, vs.
the situation-specific FrameNet role system or the
verb-specific PropBank role system, enabling gen-
eralizations across a wide semantic scope. Swier
and Stevenson (2005) and Yi et al (2007) used VN
for SRL.
Verb type classification. Quite a few works
have addressed the issue of verb type classification
and in particular classification to ?Levin inspired?
classes (e.g., (Schulte im Walde, 2000; Merlo and
Stevenson, 2001)). Such work is not comparable
to ours, as it deals with verb type (sense) rather
than verb token (instance) classification.
Verb token classification. Lapata and Brew
(2004) dealt with classification to Levin classes of
polysemous verbs. They established a prior from
the BNC in an unsupervised manner. They also
showed that this prior helps in the training of a
naive Bayes classifier employed to distinguish be-
tween possible verb classes of a given verb in a
given frame (when the ambiguity is not solved by
knowing the frame alone). Li and Brew (2007) ex-
tended this model by proposing a method to train
the class disambiguator without using hand-tagged
data. While these papers have good results, their
experimental setup was rather narrow and used
only at most 67 polysemous verbs (in 4 frames).
VN includes 912 polysemous verbs, of which 695
appeared in our in-domain experiments.
Girju et al (2005) performed the only previous
work we are aware of that addresses the problem of
token level verb disambiguation into VN classes.
They treated the task as a supervised learning prob-
lem, proposing features based on a POS tagger, a
Chunker and a named entity classifier. In order
to create the data4, they used a mapping between
Propbank rolesets and VN classes, and took the in-
stances in WSJ sections 15-18,20,21 that were an-
notated by Propbank and for which the roleset de-
termines the VN class uniquely. This resulted in
most instances being in fact monosemous. Their
3Propbank (Palmer et al, 2005) is a corpus annotation of
the WSJ sections of the Penn Treebank with semantic roles of
each verbal proposition.
4Semlink was not available then.
10
experiment was conducted in a WSJ in-domain
scenario, and in a much narrower scope than in
this paper. They had 870 (39 polysemous) unique
verb lemmas, compared to 2091 (695 polysemous)
in our in-domain scenario. They did not test their
model in an adaptation scenario. The scope and
difficulty contrast between our setup and theirs are
demonstrated by the large differences in the num-
ber of instances and in the percentage of polyse-
mous instances: 972/12431 (7.8%) in theirs, com-
pared to 49571/84749 (58.5%) in our in-domain
scenario (training+test). We compared our method
to theirs for completeness and achieved similar re-
sults.
Semlink. The Semlink project (Yi et al, 2007;
Loper et al, 2007) aims to create a mapping of
PropBank, FrameNet (Baker et al, 1998), Word-
Net (henceforth WN) and VN to one another, thus
allowing these resources to synergize. In addition,
the project includes the most extensive token map-
ping of verbs to their VN classes available today.
It covers all verbs in the WSJ sections of the Penn
Treebank within VN coverage (out of 113K verb
instances, 97K have lemmas present in VN).
3 Nature of the Task
Polysemy is a major issue in NLP. Verbs are not an
exception, resulting in a single verb form (lemma)
appearing in more than a single class. This pol-
ysemy is also present in the original Levin clas-
sification, where polysemous classes account for
more than 48% of the BNC verb instances (Lapata
and Brew, 2004).
Given a verb instance whose lemma is within
the coverage of VN, given the sentence in which
it appears, given a parse tree of this sentence (see
below), and given the VN resource, our task is to
classify the verb instance to its correct VN class.
There are currently 237 possible classes5. Each
verb has only a few possible classes (no more than
10, but only about 2.5 on the average over the poly-
semous verbs). Depending on the application, the
parse tree for the sentence may be either a gold
standard parse or a parse tree generated by a parser.
We have experimented with both options.
The task can be viewed in two complemen-
tary ways: per-class and per-verb type. The per-
class perspective takes into consideration the small
5We ignore sub-class distinctions. This is justified since in
98.2% of the in-coverage instances in Semlink, knowing the
verb and its class suffices for knowing its exact sub-class.
number of classes relative to the number of types6.
A classifier may gather valuable information for all
members of a certain VN class, without seeing all
of its members in the training data. From this per-
spective the task resembles POS tagging. In both
tasks there are many dozens (or more) of possible
labels, while each word has only a small subset of
possible labels. Different words may receive the
same label.
The per-verb perspective takes into consider-
ation the special properties of every verb type.
Even the best lexicons necessarily ignore certain
idiosyncratic characteristics of the verb when as-
signing it to a certain class. If a verb appears
many times in the corpus, it is possible to estimate
its parameters to a reasonable reliability, and thus
to use its specific distributional properties for dis-
ambiguation. Viewed in this manner, the task re-
sembles a word sense disambiguation (WSD) task:
each verb has a small distinct set of senses (types),
and no two different verbs have the same sense.
The similarity to WSD suggests that our task
might be solved by WN sense disambiguation fol-
lowed by a mapping from WN to VN. However,
good results are not to be expected, due to the
medium quality of today?s WSD algorithms and
because the mapping between WN and VN is both
incomplete and many-to-many7. Even for a perfect
WN WSD algorithm, the resulting WN synset may
not be mapped to VN at all or may be mapped onto
multiple VN classes. We experimented with this
method and obtained results below the MF base-
line we used8.
The above discussion does not rule out the pos-
sibility of obtaining reasonable results through ap-
plying a high quality WSD engine followed by a
WN to VN mapping. However, there are much
fewer VN classes than WN classes per verb. This
may result in the WSD engine learning many dis-
tinctions that are not useful in this context, which
may in turn jeopardize its performance with re-
spect to our task. Moreover, a word sense may
belong to a single verb only while a VN class con-
tains many verbs. Consequently, the performance
6237 classes vs. 4991 types.
7In the WN to VN mapping built into VN, 14.69% of the
covered WN synsets were mapped to more than a single VN
class.
8We used the publicly available SenseLearner 2.0, the VB-
Collocations model. We chose VN classes containing the
lemma in random when a single mapping is not specified. We
obtained 67.74% accuracy on section 00 of the WSJ, which is
less than the MF baseline. See Sections 5 and 7.
11
on a certain lemma may benefit from training in-
stances of other lemmas.
Note that our task is not reducible to VN frame
identification (a non-trivial task given the rich-
ness of the information used to define a frame
in VN). Although the categorizing criterion for
Levin?s classification is the subset of frames the
verb may appear in (equivalently, the diathesis al-
ternations the verbal proposition may perform),
knowing only the frame in which an instance ap-
pears does not suffice, as frames are shared among
classes.
4 The Learning Model
As common in supervised learning models, we en-
code the verb instances into feature vectors and
then apply a learning algorithm to induce a clas-
sifier. We first discuss the feature set and then the
learning algorithm.
Features. Our feature set heavily relies on syn-
tactic annotation. Dorr and Jones (1996) showed
that perfect knowledge of the allowable syntactic
frames for a verb allows 98% accuracy in type as-
signment to Levin classes. This motivates the en-
coding of the syntactic structure of the sentence
as features, since we have no access to all frames,
only to the one appearing in the sentence.
Since some verbs may appear in the same syn-
tactic frame in different VN classes, a model rely-
ing on the syntactic frame alone would not be able
to disambiguate instances of these verbs when ap-
pearing in those frames. Hence our features in-
clude lexical context words. The parse tree en-
ables us to use words that appear in specific syn-
tactic slots rather than in a linear window around
the verb. To this end, we use the head words of
the neighboring constituents. The definition of the
head of a constituent is given in (Collins, 1999).
Our feature set is comprised of two parallel sets
of features. The first contains features extracted
from the parse tree and the verb?s lemma as a stan-
dalone feature. In the second set, each feature is a
conjunction of a feature from the first set with the
verb?s lemma. By doing so we created a general
feature space shared by all verbs, and replications
of it for each and every verb. This feature selection
strategy was chosen in view of the two perspec-
tives on the task (per-class and per-verb) discussed
in Section 3.
Our first set of features encodes the verb?s con-
text as inferred from the sentence?s parse tree (Fig-
First Feature Set
The stemmed head words, POS, parse tree labels,
function tags, and ordinals of the verb?s right k
r
siblings (k
r
is the maximum number of right sib-
lings in the corpus. These are at most 5k
r
differ-
ent features).
The stemmed head words, POS, labels, function
tags and ordinals of the verb?s left k
l
siblings, as
above.
The stemmed head word & POS of the ?second
head word? nodes on the left and right (see text
for precise definition).
All of the above features employed on the sib-
lings of the parent of the verb (only if the verb?s
parent is the head constituent of its grandparent)
The number of right/left siblings of the verb.
The number of right/left siblings of the verb?s
parent.
The parse tree label of the verb?s parent.
The verb?s voice (active or passive).
The verb?s lemma.
Figure 1: The first set of features in our model. All
of them are binary. The final feature set includes
two sets: the set here, and a set obtained by its
conjunction with the verb?s lemma.
ure 1). We attempt to encode both the syntactic
frame, by encoding the tree structure, and the ar-
gument preferences, by encoding the head words
of the arguments and their POS. The restriction on
the verb?s parent being the head constituent of its
grandparent is done in order to focus on the correct
verb in verb series such as ?intend to run?.
The 3rd cell in the table makes use of a ?sec-
ond head word? node, defined as follows. Consider
a left sibling (right siblings are addressed analo-
gously) M of the verb?s node. Take the node H
in the subtree of M where M ?s head appears. H
is a descendent of a node J which is a child of
M . The ?second head word? node is J?s sibling on
the right. For example, in the sentence We went to
school (see Figure 2) the head word of the PP ?to
school? is ?to?, and the ?second head word? node is
?school?. The rationale is that ?school? could be a
useful feature for ?went?, in addition to ?to?, which
is highly polysemous (note that it is also a feature
for ?went?, in the 1st and 2nd cells of the table).
The voice feature was computed using a simple
heuristic based on the verb?s POS tag (past partici-
ple) and presence of auxiliary verbs to its left.
12
SNP
PRP
We
VP
VBD
went
PP
TO
to
NP
NN
school
Figure 2: An example parse tree for the ?second
head word? feature.
The current set of features does not detect verb
particle constructions. We leave this for future re-
search.
Learning Algorithm. Our learning task can be
formulated as follows. Let x
i
denote the feature
vector of an instance i, and let X denote the space
of all such feature vectors. The subset of possi-
ble labels for x
i
is denoted by C
i
, and the correct
label by c
i
? C
i
. We denote the label space by
S. Let T be the training set of instances T = {<
x
1
, C
1
, c
1
>,< x
2
, C
2
, c
2
>, ..., < x
n
, C
n
, c
n
>
} ? (X ? 2
S
? S)
n
, where n is the size of the
training set. Let < x
n+1
, C
n+1
>? (X ? 2
S
) be
a new instance. Our task is to select which of the
labels in C
n+1
is its correct label c
n+1
(x
n+1
does
not have to be a previously observed lemma, but
its lemma must appear in a VN class).
The structure of the task lets us apply a learn-
ing algorithm that is especially appropriate for it.
What we need is an algorithm that allows us to re-
strict the possible labels of each instance, both in
training and in testing. The sequential model algo-
rithm presented by Even-Zohar and Roth (2001)
directly supports this requirement. We use the
SNOW learning architecture for multi-class clas-
sification (Roth, 1998), which contains an imple-
mentation of that algorithm 9.
5 Experimental Setup
We used SemLink VN annotations and parse trees
on sections 02-21 of the WSJ Penn Treebank for
training, and section 00 as a development set, as
is common in the parsing community. We per-
formed two parallel sets of experiments, one us-
ing manually created gold standard parse trees and
one using parse trees created by a state-of-the-art
9Experiments on development data revealed that for verbs
for which almost all of the training instances are mapped to
the same VN class, it is most beneficial to select that class.
Thus, where more than 90% of the training instances of a verb
are mapped to the same class, our algorithm mapped the in-
stances of the verb to that class regardless of the context.
parser (Charniak and Johnson, 2005) (Note that
this parser does not output function tags). The
parser was also trained on sections 02-21 and tuned
on section 0010. Consequently, our adaptation sce-
nario is a full adaptation situation in which both the
parser and the VerbNet training data are not in the
test domain. Note that generative parser adaptation
results are known to be of much lower quality than
in-domain results (Lease and Charniak, 2005). The
quality of the discriminative parser we used did
indeed decrease in our adaptation scenario (Sec-
tion 7).
The training data included 71209 VN in-scope
instances (of them 41753 polysemous) and the de-
velopment 3624 instances (2203 polysemous). An
?in-scope? instance is one that appears in VN and
is tagged with a verb POS. The same trained model
was used in both the in-domain and adaptation sce-
narios, which only differ in their test sets.
In-Domain. Tests were held on sections
01,22,23,24 of WSJ PTB. Test data includes all in-
scope instances for which there is a SemLink anno-
tation, yielding 13540 instances, 7798 (i.e., 57.6%)
of them polysemous.
Adaptation. For the testing we annotated sen-
tences from GENIA (Kim et al, 2003) (version
3.0.2). The GENIA corpus is composed of MED-
LINE abstracts related to transcription factors in
human blood cells. We annotated 400 sentences
from the corpus, each including at least one in-
scope verb instance. We took the first 400 sen-
tences from the corpus that met that criterion11 .
After cleaning some GENIA POS inconsistencies,
this amounts to 690 in-scope instances (380 of
them polysemous). The tagging was done by two
annotators with an inter-annotator agreement rate
of 80.35% and Kappa 67.66%.
Baselines. We used two baselines, random and
most frequent (MF). The random baseline selects
uniformly and independently one of the possible
classes of the verb. The most frequent (MF) base-
line selects the most frequent class of the verb in
the training data for verbs seen while training, and
selects in random for the unseen ones. Conse-
quently, it obtains a perfect score over the monose-
mous verbs. This baseline is a strong one and is
common in disambiguation tasks.
We repeated all of the setup above in two sce-
10For the very few sentences out of coverage for the parser,
we used the MF baseline (see below).
11Discarding the first 120 sentences, which were used to
design the annotator guidelines.
13
narios. In the first (main) scenario, in-scope in-
stances were always mapped to VN classes, while
in the second (?other is possible? (OIP)) scenario,
in-scope instances were allowed to be tagged (dur-
ing training) and classified (during test) as not be-
longing to any existing VN class12. In all cases,
out-of-scope instances (verbs whose lemmas do
not appear in VN) were ignored. For the OIP sce-
nario, we used a different ?other? label for each of
the lemmas, not a single label shared by them all.
6 Results
Table 1 shows our results. In addition to the over-
all results, we also show results for the polysemous
ones alone, since the task is trivial for the monose-
mous ones. The results using gold standard parses
effectively set an upper bound on our model?s per-
formance, while those using statistical parser out-
put demonstrate its current usability.
In-Domain. Results are shown in the WSJ ?
WSJ columns of Table 1. Using gold standard
parses (top), we achieve 96.42% accuracy over-
all. Over the polysemous verbs, the accuracy is
93.68%. This translates to an error reduction over
the MF baseline of 43.35% overall and 43.22% for
the polysemous verbs. In the ?other is possible?
scenario (right), we obtained 36.67% error reduc-
tion. Using a state-of-the-art parser (Charniak and
Johnson, 2005) (bottom), we experienced some
degradation of the results (as expected), but they
remained significantly above baseline. We achieve
95.9% accuracy overall and 92.77% for the polyse-
mous verbs, which translates to about 35.13% and
35.04% error reduction respectively. In the OIP
scenario, we obtained 28.95% error reduction.
The results of the random baseline for the in-
domain scenario are substantially worse than the
MF baseline. On the WSJ the random baseline
scored 66.97% (37.51%) accuracy in the main
(OIP) scenarios.
Adaptation. Here we test our model?s ability
to generalize across domains. Since VN is sup-
posed to be a domain independent resource, we
hope to acquire statistics that are relevant across
domains as well and so to enable us to automati-
cally map verbs in domains of various genres. The
results are shown in the WSJ ? GENIA columns
of Table 1. When using gold standard parses, our
model scored 73.16%. This translates to about
13.17% ER on GENIA. We interestingly experi-
12i.e., including instances tagged by SemLink as ?none?.
enced very little degradation in the results when
moving to parser output, achieving 72.4% accu-
racy which translates to 10.71% error reduction
over the MF baseline. The random baseline on GE-
NIA was again worse than MF, obtaining 66.04%
accuracy as compared to 69.09% of MF (in the OIP
scenario, 39.12% compared to 46.41%).
Run-time performance. Given a parsed cor-
pus, our main model trains and runs in no more
than a few minutes for a training set of ?60K in-
stances and a test set of ?11K instances, using a
Pentium 4 CPU 2.40GHz with 1GB main mem-
ory. The bottleneck in tagging large corpora using
our model is thus most likely the running time of
current parsers.
7 Discussion
In this paper we introduced a new statistical model
for automatically mapping verb instances into
VerbNet classes, and presented the first large-scale
experiments for this task, for both in-domain and
corpus adaptation scenarios.
Using gold standard parse trees, we achieved
96.42% accuracy on WSJ test data, showing
43.35% error reduction over a strong baseline.
For adaptation to the GENIA corpus, we showed
13.1% error reduction over the baseline. A sur-
prising result in the context of adaptation is the lit-
tle influence of using gold standard parses versus
using parser output, especially given the relatively
low performance of today?s parsers in the adapta-
tion task (91.4% F-score for the WSJ in-domain
scenario compared to 81.24% F-score when pars-
ing our GENIA test set). This is an interesting di-
rection for future work.
In addition, we conducted some additional pre-
liminary experiments in order to shed light on
some aspects of the task. The experiments reported
below were conducted on the development data,
given gold standard parse trees.
First, motivated by the close connection be-
tween WSD and our task (see Section 3), we con-
ducted an experiment to test the applicability of
using a WSD engine. In addition to the experi-
ments listed above, we also attempted to encode
the output of a modern WSD engine (the VBCollo-
cations Model of SenseLearner 2.0 (Mihalcea and
Csomai, 2005)), both by encoding the synset (if
exists) of the verb instance as a feature, and by en-
coding each possible mapped class of the WSD
engine output synset as a feature. There are k
14
Main Scenario ?Other is Possible? (OIP) Scenario
WSJ?WSJ WSJ?GENIA WSJ?WSJ WSJ?GENIA
MF Model MF Model MF Model MF Model
Gold Std Total 93.68 96.42 69.09 73.16 88.6 92.78 46.41 52.46
ER 43.35 13.17 36.67 11.29
Poly. 88.87 93.68 48.58 55.35 ? ? ? ?
ER 43.22 13.17 ? ?
Parser Total 93.68 95.9 69.09 72.4 88.6 91.9 46.41 52.46
ER 35.13 10.71 28.95 11.29
Poly. 88.87 92.77 48.58 55.35 ? ? ? ?
ER 35.04 10.72 ? ?
Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline.
Error reduction is computed as MODEL?MF
100?MF
. Results are given for the WSJ and GENIA corpora test
sets. The top table is for a model receiving gold standard parses of the test data. The bottom is for a
model using (Charniak and Johnson, 2005) state-of-the-art parses of the test data. In the main scenario
(left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (during
both training and test) to map instances as not belonging to any existing class. For the latter, no results
are displayed for polysemous verbs, since each verb can be mapped both to ?other? and to at least one
class.
features if there are k possible classes13. There
was no improvement over the previous model. A
possible reason for this is the performance of the
WSD engine (e.g. 56.1% precision on the verbs in
Senseval-3 all-words task data). Naturally, more
research is needed to establish better methods of
incorporating WSD information to assist in this
task.
Second, we studied the relative usability of class
information as opposed to verb idiosyncratic infor-
mation in the VN disambiguation task. By mea-
suring the accuracy of our model, first given the
per-class features (the first set of features exclud-
ing the verb?s lemma feature) and second given the
per-verb features (the conjunction of the first set
with the verb?s lemma), we tried to address this
question. We obtained 94.82% accuracy for the
per-class experiment, and 95.51% for the per-verb
experiment, compared to 95.95% when using both
in the in-domain gold standard scenario. The MF
baseline scored 92.45% on this development set.
These results, which are close in the per-class ex-
periment to those of the MF baseline, indicate that
combining both approaches in the construction of
the classifier is justified.
Third, we studied the importance of having a
learning algorithm utilizing the task?s structure
(mapping into a large label space where each in-
13The mapping is many-to-many and partial. To overcome
the first issue, given a WN sense of the verb, we encoded all
possible VN classes that correspond to it. To overcome the
second, we treated a verb in a certain VN class, for which the
mapping to WN was available, as one that can be mapped to
all WN senses of the verb.
stance can be mapped to only a small subspace).
Our choice of the algorithm in (Even-Zohar and
Roth, 2001) was done in light of this requirement.
We conducted an experiment in which we omitted
these per-instance restrictions on the label space,
effectively allowing each verb to take every label
in the label space. We obtained 94.54% accuracy,
which translates to 27.68% error reduction, com-
pared to 95.95% accuracy (46.36% error reduc-
tion) when using the restrictions. These results in-
dicate that although our feature set keeps us sub-
stantially above baseline even without the above
algorithm, using it boosts our results even further.
This result is different from the results obtained
in (Girju et al, 2005), where the results of the un-
constrained (flat) model were significantly below
baseline.
As noted earlier, the field of instance level
verb classification into Levin-inspired classes is far
from being exhaustively explored. We intend to
make our implementation of the model available
to the community, to enable others to engage in
further research on this task.
Acknowledgements. We would like to thank Dan
Roth, Mark Sammons and Ran Luria for their help.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. Proc. of the
36th Meeting of the ACL and the 17th COLING.
Eugene Charniak and Mark Johnson, 2005. Coarse-
15
to-fine n-best parsing and maxent discriminative
reranking. Proc. of the 43rd Meeting of the ACL.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Hoa Trang Dang, Karin Kipper, Martha Palmer and
Joseph Rosenzweig, 1998. Investigating regular
sense extensions based on intersective Levin classes.
Proc. of the 36th Meeting of the ACL and the 17th
COLING.
Bonnie J. Dorr, 1997. Large-Scale Dictionary Con-
struction for Foreign Language Tutoring and Inter-
lingual Machine Translation. Machine Translation,
12:1-55.
Bonnie J. Dorr and Douglas Jones, 1996. Role of Word
Sense Disambiguation in Lexical Acquisition: Pre-
dicting Semantics from Syntactic Cues. Proc. of the
16th COLING.
Yair Even-Zohar and Dan Roth, 2001. A Sequential
Model for Multi-Class Classification. Proc. of the
2001 Conference on Empirical Methods in Natural
Language Processing.
Roxana Girju, Dan Roth and Mark Sammons, 2005.
Token-level Disambiguation of VerbNet classes. The
Interdisciplinary Workshop on Verb Features and
Verb Classes.
Svetlana Hensman and John Dunnion, 2004. Automat-
ically building conceptual graphs using VerbNet and
WordNet. International Symposium on Information
and Communication Technologies (ISICT).
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford U. Press 2003.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
Proc. of the 17th National Conference on Artificial
Intelligence.
Karin Kipper-Schuler, 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph. D. the-
sis, University of Pennsylvania.
Karin Kipper, Anna Korhonen, Neville Ryant and
Martha Palmer, 2006. Extending VerbNet with
Novel Verb Classes. Proc. of the 5th International
Conference on Language Resources and Evaluation.
Judith Klavans and Min-Yen Kan, 1998. Role of verbs
in document analysis. Proc. of the 36th Meeting of
the ACL and the 17th International Conference on
Computational Linguistics.
Anna Korhonen and Ted Briscoe, 2004. Extended
Lexical-Semantic Classification of English Verbs.
Proc. of the 42nd Meeting of the ACL, Workshop on
Computational Lexical Semantics.
Mirella Lapata and Chris Brew, 2004. Verb Class
Disambiguation using Informative Priors. Compu-
tational Linguistics, 30(1):45-73
Matthew Lease and Eugene Charniak, 2005. Towards
a Syntactic Account of Punctuation. Proc. of the 2nd
International Joint Conference on Natural Language
Processing.
Beth Levin, 1993. English Verb Classes And Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Beth Levin and Malka Rappaport Hovav, 2005. Argu-
ment Realization. Cambridge University Press.
Juanguo Li and Chris Brew, 2007. Disambiguating
Levin Verbs Using Untagged Data. Proc. of the
2007 International Conference on Recent Advances
in Natural Language Processing.
Edward Loper, Szu-ting Yi and Martha Palmer, 2007.
Combining Lexical Resources: Mapping Between
PropBank and VerbNet. Proc. of the 7th Inter-
national Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb-Classification Based On Statistical Distribu-
tion Of Argument Structure. Computational Linguis-
tics, 27(3):373?408.
Rada Mihalcea and Andras Csomai 2005. Sense-
Learner: word sense disambiguation for all words
in unrestricted text. Proc. of the 43rd Meeting of the
ACL , Poster Session.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1).
Dan Roth, 1998. Learning to resolve natural language
ambiguities: A unified approach. Proc. of the 15th
National Conference on Artificial Intelligence
Sabine Schulte im Walde, 2000. Clustering verbs se-
mantically according to their alternation behavior.
Proc. of the 18th COLING.
Lei Shi and Rada Mihalcea, 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and WordNet
for robust semantic parsing. Proc. of the Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. Proc. of the 2005 conference on empirical
methods in natural language processing.
Szu-ting Yi, Edward Loper and Martha Palmer, 2007.
Can Semantic Roles Generalize Across Genres?
Proc. of the 2007 conference of the north american
chapter of the association for computational linguis-
tics.
16
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 721?728
Manchester, August 2008
Unsupervised Induction of Labeled Parse Trees
by Clustering with Syntactic Features
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present an algorithm for unsupervised
induction of labeled parse trees. The al-
gorithm has three stages: bracketing, ini-
tial labeling, and label clustering. Brack-
eting is done from raw text using an un-
supervised incremental parser. Initial la-
beling is done using a merging model that
aims at minimizing the grammar descrip-
tion length. Finally, labels are clustered
to a desired number of labels using syn-
tactic features extracted from the initially
labeled trees. The algorithm obtains 59%
labeled f-score on the WSJ10 corpus, as
compared to 35% in previous work, and
substantial error reduction over a random
baseline. We report results for English,
German and Chinese corpora, using two
label mapping methods and two label set
sizes.
1 Introduction
Unsupervised learning of grammar from text
(?grammar induction?) is of great theoretical and
practical importance. It can shed light on language
acquisition by humans and on the general structure
of language, and it can potentially assist NLP ap-
plications that utilize parser output. The problem
has attracted researchers for decades, and interest
has greatly increased recently, in part due to the
availability of huge corpora, computation power,
and new learning algorithms (see Section 2).
A fundamental issue in this research direction is
the representation of the resulting induced gram-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mar. Most recent work (e.g., (Klein and Manning,
2004; Dennis, 2005; Bod, 2006a; Smith and Eis-
ner, 2006; Seginer, 2007)) annotates text sentences
using a hierarchical bracketing (constituents) or a
dependency structure, and thus represents the in-
duced grammar through its behavior in a parsing
task. Solan et al (2005) uses a graph representa-
tion, while (Nakamura, 2006) simply uses a gram-
mar formalism such as PCFG. When the bracket-
ing approach is taken, some algorithms label the
resulting constituents, while most do not.
Each of these approaches can be justified or crit-
icized; a detailed discussion of this issue is be-
yond the scope of this paper. The algorithm pre-
sented here belongs to the first group, annotating
given sentences with labeled bracketing structures.
The main theoretical justification for this approach
is that many linguistic and psycho-linguistic theo-
ries posit some kind of a hierarchical labeled con-
stituent (or constructional) structure, arguing that it
has a measurable psychological (cognitive) reality
(e.g., (Goldberg, 2006)). The main practical argu-
ments in favor of this approach are that it enables
a detailed and large-scale evaluation using anno-
tated corpora, as is done in this paper, and that the
output format is suitable for many applications.
When an algorithm generates labeled structures,
the number of labels is an important issue. From a
theoretical point of view, the algorithm should also
discover the appropriate number of labels. How-
ever, for evaluation and application purposes it is
useful to base the number of labels on a specific
target grammar. In previous work, the number was
set to be equal to that in the target grammar. This
is a reasonable approach that we experiment with
in this paper. In addition, to reduce the possible
arbitrariness in this approach, we also experiment
with the number of prominent labels in the target
721
grammar, determined according to their coverage
of corpus constituents.
Another issue relates to the nature of the in-
put. In most cases (e.g., in the Klein, Smith, Den-
nis and Bod papers above), the input consists of
part-of-speech (POS) sequences, derived from text
corpora by manual or automatic POS tagging. In
some cases (e.g., in the Seginer and Solan papers
above) it can consist of plain text. Again, each
approach has its pros and cons. The algorithm
we present here requires POS tags for its labeling
stages. Parts-of-speech are widely considered to
have a psychological reality (at least in English,
including when they are viewed as low-level con-
structions as in (Croft, 2001)), so this kind of input
is reasonable for theoretical research. Moreover, as
POS induction is of medium quality (Clark, 2003),
using a manually POS tagged corpus enables us to
measure the performance of other induction stages
in a controlled manner. Since supervised POS tag-
ging is of very high quality and very efficient com-
putationally (Brants, 2000), this requirement does
not seriously limit the practical applicability of a
grammar induction algorithm.
Our labeled bracketings induction algorithm
consists of three stages. We first induce unla-
beled bracketing trees using the algorithm given in
(Seginer, 2007)1. We then induce initial labels us-
ing a Bayesian Model Merging (BMM) labeling al-
gorithm (Borensztajn and Zuidema, 2007), which
aims at minimizing the description length of the
input data and the induced grammar. Finally, the
initial labels are clustered to a desired number of
labels using syntactic features extracted from the
initially labeled trees. Previous work on labeled
brackets induction (Section 2) did not differentiate
the unlabeled structure induction phase from the
labeling phase, applying a single phase approach.
To evaluate labeled bracketings, we need a map-
ping between the label symbols of the induced and
target grammars. Previous work used a ?greedy?,
many to one, mapping. We used both the greedy
mapping and a label-to-label (LL) mapping, since
greedy mapping is highly forgiving to structural
problems in the induced labeling. We report results
for two cases: one in which the number of labels in
the induced and target grammars is the same, and
one in which the former is the number of promi-
nent labels in the target grammar. We discuss how
this number can be defined and determined. We
1The algorithm uses raw (not POS tagged) sentences.
experimented with English (WSJ10, Brown10),
German (NEGRA10) and Chinese (CTB10) cor-
pora.
When comparing to previous work that used
manually annotated corpora in its evaluation
(Haghighi and Klein, 2006)2, we obtained 59.5%
labeled f-score on the WSJ10 setup vs. their 35.3%
(Section 5). We also show substantial improve-
ment over a random baseline, and that the cluster-
ing stage of our algorithm improves the results of
the second merging stage.
Section 2 discusses previous work. In Section 3
we detail our algorithm. The experimental setup
and results are presented in Sections 4 and 5.
2 Previous Work
Unsupervised parsing has attracted researchers for
decades (see (Clark, 2001; Klein, 2005) for recent
reviews). Many types of input, syntax formalisms,
search procedures, and success criteria were used.
Among the theoretical and practical motivations to
this problem are the study of human language ac-
quisition (in particular, an empirical study of the
poverty of stimulus hypothesis), preprocessing for
constructing large treebanks (Van Zaanen, 2001),
and improving language models (Chen, 1995).
In recent years efforts have been made to eval-
uate the algorithms on manually annotated cor-
pora such as the WSJ PennTreebank. Recently,
works along this line have for the first time out-
performed the right branching heuristic baseline
for English. These include the constituent?context
model (CCM) (Klein and Manning, 2002), its
extension using a dependency model (Klein and
Manning, 2004), (U)DOP based models (Bod,
2006a; Bod, 2006b; Bod, 2007), an exemplar?
based approach (Dennis, 2005), guiding EM using
contrastive estimation (Smith and Eisner, 2006),
and the incremental parser of (Seginer, 2007). All
of these use as input POS tag sequences, except
of Seginer?s algorithm, which uses plain text. All
of these papers induce unlabeled bracketing or de-
pendencies.
There are other algorithmic approaches to the
problem (e.g., (Adriaans, 1992; Daelemans, 1995;
Van Zaanen, 2001)). None of these had evaluated
labeled bracketing on annotated corpora.
In this paper we focus on the induction of
labeled bracketing. Bayesian Model Merging
2Using, as they did, a greedy mapping with an equal num-
ber of labels in the induced and target grammars.
722
(BMM) (Stolcke, 1994; Stolcke and Omohundro,
1994) is a framework for inducing PCFG contain-
ing both a bracketing and a labeling. The charac-
teristics of this framework (separating prior prob-
ability, data likelihood and heuristic search proce-
dures) can also be found in the grammar induction
models of (Wolf, 1982; Langley and Stromsten,
2000; Petasis et al, 2004; Solan et al, 2005). The
BMM model used here (Borensztajn and Zuidema,
2007) combines features of (Petasis et al, 2004)
and Stolcke?s algorithm, applying the minimum
description length (MDL) principle. We use it here
only for initial labeling of existing bracketings.
The MDL principle was also used in (Grunwald,
1994; de Marcken, 1995; Clark, 2001).
There are only two previous papers we are
aware of that induce labeled bracketing and eval-
uate on corpora annotated with a similar repre-
sentation (Haghighi and Klein, 2006; Borensztajn
and Zuidema, 2007). We utilize and extend the
latter?s labeling algorithm. However, the evalu-
ation done by the latter dealt only with labeling,
using gold-standard (manually annotated) bracket-
ings. Thus, we can directly compare our results
only to (Haghighi and Klein, 2006), where two
models (PCFG ? NONE and PCFG ? CCM) are fully un-
supervised. These models use the inside-outside
and EM algorithms to induce bracketing and label-
ing simultaneously, as opposed to our three step
method3.
3 Algorithm
Our model consists of three stages: bracketing, ini-
tial labeling, and label clustering.
3.1 Induction of Unlabeled Bracketing
In this step, we apply the algorithm of (Seginer,
2007) to induce bracketing from plain text4. We
have chosen that algorithm because it is very fast
(both learning and parsing) and its code is publicly
available. We could have chosen any of the algo-
rithms mentioned above producing a similar output
format.
3.2 Initial Constituent Labeling
Our label clustering stage uses syntactic fea-
tures. To obtain these, we need an initial label-
ing on the bracketings computed in the previous
3Their other models, which were the core of their paper,
are semi-supervised.
4http://www.seggu.net/ccl
stage. To do that we modify the Bayesian Model
Merging (BMM) algorithm of (Borensztajn and
Zuidema, 2007), which induces context-free gram-
mars (bracketing and labeling) from POS tags,
combining features of the models of (Stolcke and
Omohundro, 1994) and (Petasis et al, 2004).
The BMM algorithm (Borensztajn and
Zuidema, 2007) uses an iterative heuristic
greedy search for an optimal PCFG according to
the Bayesian criterion of maximum posterior prob-
ability. Two operators define possible transitions
between grammars: MERGE creates generaliza-
tions by replacing two existing non-terminals
X
1
and X
2
that occur in the same contexts by a
single new non-terminal Y ; CHUNK concatenates
repeating patterns by taking a sequence of two
non-terminals X
1
and X
2
and creating a new
non-terminal Y that expands to X
1
X
2
.
We have used the algorithm to deal only with
labeling. It reads the initial rules of the grammar
from all productions implicit in the bracketed cor-
pus induced in the previous step. Every constituent
(except of the start symbol) is given a unique label.
Since only labeling is required, only MERGE oper-
ations are performed.
The objective function the algorithm tries to op-
timize at each step is the posterior probability cal-
culated according to Bayes? Law:
M
MAP
= argmax
M
P (M|X) = argmax
M
P (X|M) ? P (M)
(1)
where P (X|M) is the likelihood of the data X
given the grammar M and P (M) is the prior prob-
ability of the grammar. This is equivalent to mini-
mizing the function
?log(P (X|M)) ? logP (m) := DDL+ GDL := DL. (2)
Using a Minimal Description Length (MDL)
principle, BMM interprets this function as total de-
scription length (DL): The Grammar Description
Length GDL = ?logP (M) is the space needed
to encode the model, and the Data Description
Length DDL = ?logP (X|M) is the space re-
quired to describe the data given the model. The
rationale for MDL is to prefer smaller grammars
that describe the data well. DDL and GDL are
computed as in (Stolcke, 1994; Stolcke and Omo-
hundro, 1994). In order to reduce the number of
grammars considered at each step, which naively
is quadratic in the number of non-terminals, a
method based on (Petasis et al, 2004) for effi-
ciently predicting DL gain is applied. The process
723
is iterated until no additional merge operation im-
proves the objective function. Full details are given
in (Borensztajn and Zuidema, 2007).
3.3 Label Clustering
Label set size. BMM produces quite a large num-
ber of labels (4944 for WSJ105). In the third step
of our algorithm we reduce that number. We first
discuss the issue of the number of labels in induced
grammars, which is an important issue.
In many situations, it is reasonable to use a num-
ber T identical to the number of labels in a given
target grammar, for example when that grammar
is used for applications or evaluation. This is the
approach in (Haghighi and Klein, 2006) for their
unsupervised models6, and we use it in part of our
evaluation. However, it is also reasonable to argue
that the granularity of syntactic categories (labels)
in the gold standard annotation of the corpora we
experiment with is somewhat arbitrary. For exam-
ple, in the WSJ Penn Treebank noun phrases are
annotated with the symbol NP, but there is no dis-
tinction between subject and object NPs. Incorpo-
rating such a distinction into the WSJ10 grammar
would result in a 27 labels grammar instead of 26.
To examine this issue, consider Figure 1, which
shows the amount of constituent coverage obtained
by a certain number of labels in the four corpora
we use (see Section 4). In all of them, about 95%
of the constituents are covered by 23% ? 37% of
the labels, and the curve rises very sharply until
that 95% value. Motivated by this observation,
given a corpus annotated using a certain hierarchi-
cal labeled grammar, we refer to the set of P labels
that cover at least 95% of the constituents in the
corpus as the grammar?s prominent labels.
The prominent labels are not only the most
frequent in the corpus; each of them substan-
tially contributes to constituent labeling, while the
saliency of other labels is much smaller. It is
thus reasonable to assume that by addressing only
prominent labels, we address a level of granularity
that is uniform and basic (to the annotation scheme
used). As a result, by asking the induced grammar
to produce P labels, we reduce arbitrariness and
enable our testing to focus on our success in iden-
tifying the basic phenomena in the target grammar.
5For completeness, in Section 5 we provide results for this
grammar using greedy mapping evaluation. LL mapping eval-
uation cannot be performed when the numbers of induced and
target labels differ.
6Personal communication with the authors.
0 5 10 15 20 25 30
20
30
40
50
60
70
80
90
100
K most frequent labels
%
 o
f c
on
st
itu
en
ts
 
 
NEGRA10
BROWN10
WSJ10
CTB10
Figure 1: For each k, the fraction of constituents
labeled with the k most frequent labels, for WSJ10
(solid), Brown10 (triangles), NEGRA10 (dashed)
and CTB10 (dotted). In all corpora, more than
95% of the constituents are labeled using less than
10 prominent labels.
As a result, we generated two grammars for each
corpus we experimented with, one having T labels
and the other having P labels.
Clustering. we stop BMM when no improvement
to its objective function is possible, and cluster the
labels to conform to the size constraint. 7
Denote the number of labels in the induced
grammar with M , the set of D most frequent in-
duced labels with A, and the set consisting of the
other induced labels with B (|B| = M ? D). If
M 6> D, there is nothing to do since the con-
straint holds. Otherwise, we map each label in
B to the label in A that exhibits the most simi-
lar syntactic behavior, as follows. We construct
a feature vector representation of each of the la-
bels, using 3M + |K| features, where K is the set
of POS tags in the corpus. The first M features
correspond to parent-child relationships between
each of the induced labels and the represented la-
bel. The i-th feature (i ? [1,M ]) is the number of
times the i-th label is the parent of the represented
label. Similarly, the next M features correspond
to child-parent relationships, the next M features
correspond to sibling relationships and the last |K|
features correspond to the number of times each
POS tag is the leftmost POS tag in a constituent
labeled by the represented label. Note that in order
to compute the values of the first 3M features, we
needed an initial labeling on the induced bracket-
ings; this is the main reason for using the BMM
stage.
For each label b
i
? B, we compute the cosine
7It is possible to force BMM to iterate until a desired num-
ber of induced labels (T or P ) is achieved. However, the in-
duced grammars are of very low quality (see Section 5).
724
metric between its vector bv
i
and that of every a
j
?
A, mapping b
i
to the label a
j
with which it obtains
the highest score:
Map(b
i
) = argmax
j
b
v
i
? a
v
j
|b
v
i
||a
v
j
|
(3)
The cosine metric grows when the same coordi-
nates (features) in both vectors have higher values.
As a result, vectors with high values of the same
features (corresponding to similar syntactic behav-
ior) get high scores.
4 Experimental Setup
We evaluated our algorithm on English, German
and Chinese corpora: the WSJ Penn Treebank,
containing economic English newspaper articles,
the Brown corpus, containing various English gen-
res, the Negra corpus (Brants, 1997) of German
newspaper text, and version 5.0 of the Chinese
Penn Treebank (Xue et al, 2002). In each cor-
pus, we used the sentences of length at most 108,
numbering 7422 (WSJ10), 9117 (Brown10), 7542
(NEGRA10) and 4626 (CTB10).
For each corpus the following T and P values
were used: WSJ10: 26, 8; Brown10: 28, 7; NE-
GRA10: 22, 6; CTB10: 24, 9. Each number pro-
duces a different grammar.
For labeled f-score evaluation, the induced la-
bels should be mapped to the target labels9. We
evaluated with two different mapping schemes.
For each pair (X
i
, Y
j
) of induced and target labels,
let C
X
i
,Y
j
be the number of times they label a con-
stituent having the same span in the same sentence.
Following (Haghighi and Klein, 2006) we applied
a greedy (many to one) mapping where the map-
ping is given by Map(X
i
) = argmax
Y
j
C
X
i
,Y
j
.
This greedy mapping tends to map many induced
labels to the same target label, and is therefore
highly forgiving of large mismatches between the
structures of the induced and target grammars.
Hence, we also applied a label-to-label (LL) map-
ping, computed by reducing this problem to op-
timal assignment in a weighted complete bipar-
tite graph, formally defined as follows. Given a
weighted complete bipartite graph G = (X ?
Y ;X ? Y ) where edge (X
i
, Y
j
) has weight w
ij
,
8Excluding punctuation and null elements, according to
the scheme of (Klein, 2005).
9There are many possible methods for evaluating cluster-
ing quality (Rosenberg and Hirschberg, 2007). For our task,
overall f-score is a very natural one. We will address other
methods in future papers.
find a (one-to-one) matching M from X to Y hav-
ing a maximal weight. In our case, X is the set of
model symbols, Y is the set of T or P most fre-
quent target symbols (depending on the desired la-
bel set size used), and w
ij
:= C
X
i
,Y
j
, computed as
in greedy mapping (the number of times x
i
and y
j
share a constituent). To make the graph complete,
we add zero weight edges between induced and
target labels that do not share any constituent. The
Kuhn-Munkres algorithm (Kuhn, 1955; Munkres,
1957) solves this problem, and we used it to per-
form the LL mapping (see also (Luo, 2005)).
We assessed the overall quality of our algorithm,
the quality of its labeling stage and the quality of
the syntactic clustering (SC) stage. For the over-
all quality of the induced grammar (both brack-
eting and labeling) we compare our results with
(Haghighi and Klein, 2006), using their setup10.
That setup was used for all numbers reported in
this paper. Note that a random baseline would
yield very poor results, so there is nothing to be
gained from comparing to it.
We assessed the quality of the labeling (MDL
and SC) stages alone, using only the correct brack-
etings produced by the first stage of the algorithm.
We compare to a random baseline on these correct
constituents that randomly selects (using a uniform
distribution) a label for each constituent among the
set of labels allowed to the algorithm.
To asses the quality of the third stage (SC)
we compare the f-score performance of our three
stages labeled trees induction algorithm (bracket-
ing, MDL, SC) to an algorithm consisting of the
first two stages only (bracketing and MDL) and
the accuracy of the two stages labeling algorithm
(MDL, SC) to an algorithm where the syntactic
clustering stage is replaced by a simpler method
(MDL, random clustering).
5 Results
We start with comparing our algorithm with
(Haghighi and Klein, 2006), the only previous
work that produces labeled bracketing and was
tested on large manually annotated corpora. Their
relevant models are PCFG ? NONE and PCFG ? CCM11.
10Brackets covering a single word are not counted, multi-
ple labels and the sentence level constituent are counted. Two
sentence level constituents are usually used: one for the root
symbol at the top (which was not counted), and one real sym-
bol (in WSJ10 it is usually, but not always, S), which was
counted. We had verified the setup with the authors.
11They focused on a different, semi-supervised, setting.
725
This Paper PCFG? CCM PCFG ? NONE
WSJ10 59.5 35.3 26.3
Table 1: F-scores of our algorithm and of the unsu-
pervised models in (Haghighi and Klein, 2006) on
WSJ10 (they did not test these models on the other
corpora we experimented with).
The number of labels in their induced grammar
equals the number of labels in the target grammar
(26 for WSJ10), and they had used a greedy map-
ping. Table 1 shows that our algorithm achieves
a superior f-score of 59.5% over their 35.3%.
Haghighi and Klein (2006) did not experiment
with the NEGRA10 and Brown10 corpora, and had
used version 3.0 of CTB10 while we have used the
substantially different version 5.0, so we can only
compare our results on WSJ10.
Table 2 shows the labeled recall, precision and f-
score of our algorithm on the various corpora and
mappings we use. On Brown10, NEGRA10 and
CTB10 (version 5.0) these are the first reported
results for this task. For reference, the table also
shows the unlabeled f-score results of Seginer?s
bracketing algorithm (our first stage)12.
We can see that greedy mapping is indeed more
forgiving than LL mapping, for both T labels and
P labels. WSJ results are generally higher than for
the other corpora, probably because WSJ bracket-
ing results are higher than for the other corpora.
Comparing the left and right columns in each
of the table sections reveals that for greedy map-
ping, mapping to a higher number of labels results
in higher scores than mapping to a lower number.
LL mapping behaves in exactly the opposite way.
The explanation for this is that when we force the
mapping to cover all of the target labels (as done
by LL mapping for T labels), we move probabil-
ity mass from the correct, heavy labels to smaller
ones, thereby magnifying errors.
Table 4 addresses the quality of the whole la-
beling stage (MDL and SC) and of the SC stage.
We report the quality of our labels (top line for
each corpus in the table) the random baseline la-
bels (third line) and the labels of an algorithm
where MDL is performed and the syntactic clus-
tering is replaced by a random clustering (RC) al-
gorithm that, given a label L that is not one of the
T or P most frequent labels, randomly selects one
of the most frequent labels and adds L to its clus-
12The numbers slightly differ from those in Seginer?s paper,
since we use the (Haghighi and Klein, 2006) setup.
Greedy LL
T P T P
WSJ10
MDL,SC 80 67 47 59
MDL,RC 67 61 37 42
Rand. Base. 30 30 5 14
Error Reduction 39%,71% 15%,53% 16%, 44% 29%, 52%
Brown10
MDL,SC 73 61 48 60
MDL,RC 68 59 46 51
Rand. Base. 27 27 4 14
Error Reduction 16%,63% 5%, 47% 4%, 46% 18%, 53%
NEGRA10
MDL,SC 79 72 65 72
MDL,RC 73 69 54 58
Rand. Base. 39 39 5 17
Error Reduction 22%,66% 10%,34% 24%,63% 33%,66%
CTB10
MDL,SC 70 67 44 55
MDL,RC 36 32 40 45
Rand. Base. 29 29 5 12
Error Reduction 53%,58% 51%, 54% 7%,41% 18%,49%
Table 4: Pure labeling results (taking into account
only the correct bracketings produced at stage 1),
compared to the random and (MDL,RC) baselines.
The left number in the Error Reduction lines slots
compares (MDL,SC) to (MDL,RC) and the right
number compares (MDL,SC) to random labeling.
(MDL,SC) algorithm is substantially superior.
ter (second line).13 All three labeling algorithms
used Seginer?s bracketing and results are reported
only for labels of correctly bracketed constituents.
Reported are the algorithm and baselines accuracy
(percentage of correctly labeled constituents after
the mapping has been performed) and the error re-
duction of the algorithm over the baselines (bottom
line). (MDL,SC) substantially outperforms both
the random baseline, demonstrating the power of
the whole labeling stage, and the (MDL,RC) algo-
rithm, demonstrating the power of the SC stage.
We compared our grammars to the grammars in-
duced by the first two stages (bracketing and then
MDL that stops when no DL improvement is pos-
sible) alone. Since the number of labels in these
grammars is much larger than in the target gram-
mar, only the evaluation with the greedy, many to
one, mapping is performed. Using greedy map-
ping, the F-score of these grammars constitutes an
upper bound on the F-score after the subsequent
SC stage. For WSJ10 (4944 labels), NEGRA10
(5557 labels), CTB10 (2298 labels) and Brown10
(3314 labels) F-score values are 64.6, 49.9, 38.7
and 52.5 compared to F-score values of 59.5(50.2),
45.6(42), 36.4(34.7) and 49.4(41.3) after mapping
all induced labels to the T (P ) most frequent la-
bels with SC (Table 2, ?greedy? section). The frac-
13Our algorithm?s numbers can be deduced from Table 2.
Results for all random baselines are averaged over 10 runs.
726
Greedy Mapping LL Mapping Seginer
T labels P labels T labels P labels (unlabeled)
Corpus R P F R P F R P F R P F F
WSJ10 58 61 59.5 48.9 51.5 50.2 34.2 36.1 35.2 42.7 44.9 43.8 74.6
NEGRA10 54.2 39.3 45.6 50 36.2 42 44.7 32.4 37.6 49.5 35.9 41.7 58.1
CTB10 35.1 37.8 36.4 33.4 36 34.7 21.9 23.6 22.7 27.4 29.5 28.4 51.8
Brown10 47.6 51.3 49.4 39.9 43 41.3 31.3 33.7 32.4 38.9 41.9 40.3 67.8
Table 2: Labeled recall, precision and f-score of our algorithm, mapping model labels into target labels
greedily (left) and using LL mapping (right). The number of induced labels was set to be the total
number T of target labels or the number P of prominent labels in the target grammar (WSJ10: 26, 8;
Brown10: 28, 7; NEGRA10: 22, 6; CTB10: 24, 9). Also shown are Seginer?s unlabeled bracketing
results (rightmost column), which constitute an upper bound on the quality of subsequent labeling steps.
WSJ10 Brown10
Label T labels P labels T labels P labels
R P F R P F R P F R P F
S 77.1 77.6 77.3 75.4 67.9 71.5 72.3 60.9 66.1 69.3 63.2 66.1
NP 8.5 79.5 15.4 19.8 61.6 30 10.7 79.3 18.9 15.6 78 26
VP 20.4 67.6 31.3 64.2 36.7 46.7 9.8 72.5 17.3 14.1 59 22.8
PP 40.8 63.5 49.7 8 8.9 8.4 17.4 59.2 26.9 75.5 14.4 24.2
Table 3: Recall, Precision and F-score for constituents labeled with the 4 most frequent labels in the
WSJ10 and Brown10 test sets. LL mapping is used for evaluation.
tion of constituents covered by the T (P ) most fre-
quent labels before mapping with SC is 0.42(0.29),
0.33(0.23), 0.58(0.45) and 0.66(0.42), emphasiz-
ing the effect of SC on the final result.
MDL finds the best merge at each iteration. In-
stead of stopping it when no DL gains are possi-
ble, we can keep merging after the deltas become
worse than the total DL, stopping only when the
desired number of labels (T or P ) is achieved. We
tried this version of a (bracketing and MDL) algo-
rithm and obtained grammars of very low quality.
This further demonstrates the importance of the SC
stage.
Table 3 shows results for the four most frequent
labels of WSJ10 and Brown10 .
6 Conclusion
Unsupervised grammar induction is a central re-
search problem, possessing both theoretical and
practical significance. There is great value in pro-
ducing an output format consistent with and evalu-
ated against formats used in large human annotated
corpora. Most previous work of that kind produces
unlabeled bracketing or dependencies. In this pa-
per we presented an algorithm that induces labeled
bracketing. The labeling stages of the algorithm
use the MDL principle to induce an initial, rela-
tively large, set of labels, which are then clustered
using syntactic features. We discussed the issue of
the desired number of labels, and introduced the
concept of prominent labels, which allows us cov-
erage of the basic and most salient level of a target
grammar. Labels are clearly an important aspect of
grammar induction. Future work will explore their
significance for applications.
Evaluating induced labels is a complex issue.
We applied greedy mapping as in previous work,
and showed that our algorithm significantly out-
performs it. In addition, we introduced LL map-
ping, which overcomes some of the shortcomings
of greedy mapping. There are several other possi-
ble methods for evaluating labeled induced gram-
mars, and we plan to explore them in future work.
We evaluated on large human annotated corpora
of different English domains and three languages,
and showed that our labeling stages, and specif-
ically the SC stage, outperform several baselines
for all corpora and mapping methods.
Acknowledgments. We thank Gideon Borensz-
tajn and Yoav Seginer for their help.
References
Pieter Adriaans, 1992. Learning Language from a
Categorical Perspective. Ph.D. thesis, University of
Amsterdam.
Rens Bod, 2006a. An All-Subtrees Approach to Un-
supervised Parsing. Proc. of the 44th Meeting of the
ACL.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
Proc. of CoNLL X.
727
Rens Bod, 2007. Is the End of Supervised Parsing in
Sight? Proc. of the 45th Meeting of the ACL.
Gideon Borensztajn and Willem Zuidema, 2007.
Bayesian Model Merging for Unsupervised Con-
stituent Labeling and Grammar Induction. Technical
Report, ILLC . http: //staff.science.uva.nl/?gideon/
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Thorsten Brants, 2000. TnT: A Statistical Part-Of-
Speech Tagger. Proc. of the 6th Applied Natural
Language Processing Conference.
Stanley F. Chen, 1995. Bayesian grammar induction
for language modeling. Proc. of the 33th Meeting of
the ACL.
Alexander Clark, 2001. Unsupervised Language Ac-
quisition: Theory and Practice. Ph.D. thesis, Uni-
versity of Sussex.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. Proc. of the 10th Meeting of the European
Chapter of the ACL.
Willliam A. Croft, 2001. Radical Construction Gram-
mar. Cambridge University Press.
Carl G. de Marcken, 1995. Unsupervised Language
Acquisition. Ph.D. thesis, MIT.
Walter Daelemans, 1995. Memory-based lexical ac-
quisition and processing. Lecture Notes In Artificial
Intelligence, 898:85?98.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Adele E. Goldberg, 2006. Constructions at Work. Ox-
ford University Press.
Peter Grunwald, 1994. A minimum description length
approach to grammar inference. Lecture Notes In
Artificial Intelligence, 1004 : 203-216.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
grammar induction. Proc. of the 44th Meeting of the
ACL.
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford University Press,
2003.
Dan Klein and Christopher Manning, 2002. A gener-
ative constituent-context model for improved gram-
mar induction. Proc. of the 40th Meeting of the ACL.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. Proc. of the 42nd Meet-
ing of the ACL.
Dan Klein, 2005. The unsupervised learning of natu-
ral language structure. Ph.D. thesis, Stanford Uni-
versity.
Harold W. Kuhn, 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83-97.
Pat Langley and Sean Stromsten, 2000. Learning
context-free grammars with a simplicity bias. Proc.
of the 11th European Conference on Machine Learn-
ing.
Xiaoqiang Luo, 2005. On coreference resolution per-
formance metrics. Proc. of the 2005 Conference on
Empirical Methods in Natural Language Processing.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32?38.
Katsuhiko Nakamura, 2006. Incremental learning of
context free grammars by bridging rule generation
and search for semi-optimum rule sets. Proc. of the
8th ICGI.
Georgios Petasis, Georgios Paliouras and Vangelis
Karkaletsis, 2004. E-grids: Computationally effi-
cient grammatical inference from positive examples.
Grammars, 7:69?110.
Andrew Rosenberg and Julia Hirschberg, 2007.
Entropy-based external cluster evaluation measure.
Proc. of the 2007 Conference on Empirical Methods
in Natural Language Processing.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. Proc. of the 45th Meeting of the ACL.
Noah A. Smith and Jason Eisner, 2006. Annealing
Structural Bias in Multilingual Weighted Grammar
Induction . Proc. of the 44th Meeting of the ACL.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman, 2005. Unsupervised learning of natural
languages. Proceedings of the National Academy of
Sciences, 102 : 11629?11634.
Andreas Stolcke. 1994. Bayesian Learning of Proba-
bilistic Language Models. Ph.D. thesis, University
of of California at Berkeley.
Andreas Stolcke and Stephen M. Omohundro, 1994.
Inducing probabilistic grammars by Bayesian model
merging . Proc. of the 2nd ICGI.
Menno van Zaanen, 2001. Bootstrapping Structure
into Language: Alignment-Based Learning. Ph.D.
thesis, University of Leeds.
J. Gerard Wolff, 1982. Language acquisition, data
compression and generalization. Language and
Communication, 2(1): 57?89.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer,
2002. Building a large?scale annotated Chinese cor-
pus. Proc. of the 40th Meeting of the ACL.
728
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 408?415,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
An Ensemble Method for Selection of High Quality Parses
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
While the average performance of statisti-
cal parsers gradually improves, they still at-
tach to many sentences annotations of rather
low quality. The number of such sentences
grows when the training and test data are
taken from different domains, which is the
case for major web applications such as in-
formation retrieval and question answering.
In this paper we present a Sample Ensem-
ble Parse Assessment (SEPA) algorithm for
detecting parse quality. We use a function
of the agreement among several copies of
a parser, each of which trained on a differ-
ent sample from the training data, to assess
parse quality. We experimented with both
generative and reranking parsers (Collins,
Charniak and Johnson respectively). We
show superior results over several baselines,
both when the training and test data are from
the same domain and when they are from
different domains. For a test setting used by
previous work, we show an error reduction
of 31% as opposed to their 20%.
1 Introduction
Many algorithms for major NLP applications such
as information extraction (IE) and question answer-
ing (QA) utilize the output of statistical parsers
(see (Yates et al, 2006)). While the average per-
formance of statistical parsers gradually improves,
the quality of many of the parses they produce is
too low for applications. When the training and test
data are taken from different domains (the parser
adaptation scenario) the ratio of such low quality
parses becomes even higher. Figure 1 demonstrates
these phenomena for two leading models, Collins
(1999) model 2, a generative model, and Charniak
and Johnson (2005), a reranking model. The parser
adaptation scenario is the rule rather than the excep-
tion for QA and IE systems, because these usually
operate over the highly variable Web, making it very
difficult to create a representative corpus for manual
annotation. Medium quality parses may seriously
harm the performance of such systems.
In this paper we address the problem of assess-
ing parse quality, using a Sample Ensemble Parse
Assessment (SEPA) algorithm. We use the level of
agreement among several copies of a parser, each of
which trained on a different sample from the training
data, to predict the quality of a parse. The algorithm
does not assume uniformity of training and test data,
and is thus suitable to web-based applications such
as QA and IE.
Generative statistical parsers compute a probabil-
ity p(a, s) for each sentence annotation, so the im-
mediate technique that comes to mind for assess-
ing parse quality is to simply use p(a, s). Another
seemingly trivial method is to assume that shorter
sentences would be parsed better than longer ones.
However, these techniques produce results that are
far from optimal. In Section 5 we show the superi-
ority of our method over these and other baselines.
Surprisingly, as far as we know there is only one
previous work explicitly addressing this problem
(Yates et al, 2006). Their WOODWARD algorithm
filters out high quality parses by performing seman-
408
80 85 90 95 1000.2
0.4
0.6
0.8
1
F score
Fr
ac
tio
n 
of
 p
ar
se
s
 
 Collins, ID
Collins, Adap.
Charniak, ID
Charniak,Adap.
Figure 1: F-score vs. the fraction of parses whose
f-score is at least that f-score. For the in-domain
scenario, the parsers are tested on sec 23 of the WSJ
Penn Treebank. For the parser adaptation scenario,
they are tested on the Brown test section. In both
cases they are trained on sections 2-21 of WSJ.
tic analysis. The present paper provides a detailed
comparison between the two algorithms, showing
both that SEPA produces superior results and that
it operates under less restrictive conditions.
We experiment with both the generative parsing
model number 2 of Collins (1999) and the reranking
parser of Charniak and Johnson (2005), both when
the training and test data belong to the same domain
(the in-domain scenario) and in the parser adapta-
tion scenario. In all four cases, we show substantial
improvement over the baselines. The present paper
is the first to use a reranking parser and the first to
address the adaptation scenario for this problem.
Section 2 discusses relevant previous work, Sec-
tion 3 describes the SEPA algorithm, Sections 4 and
5 present the experimental setup and results, and
Section 6 discusses certain aspects of these results
and compares SEPA to WOODWARD.
2 Related Work
The only previous work we are aware of that explic-
itly addressed the problem of detecting high quality
parses in the output of statistical parsers is (Yates et
al., 2006). Based on the observation that incorrect
parses often result in implausible semantic interpre-
tations of sentences, they designed the WOODWARD
filtering system. It first maps the parse produced by
the parser to a logic-based representation (relational
conjunction (RC)) and then employs four methods
for semantically analyzing whether a conjunct in the
RC is likely to be reasonable. The filters use seman-
tic information obtained from the Web. Measuring
errors using filter f-score (see Section 3) and using
the Collins generative model, WOODWARD reduces
errors by 67% on a set of TREC questions and by
20% on a set of a 100 WSJ sentences. Section 5
provides a detailed comparison with our algorithm.
Reranking algorithms (Koo and Collins, 2005;
Charniak and Johnson, 2005) search the list of best
parses output by a generative parser to find a parse of
higher quality than the parse selected by the genera-
tive parser. Thus, these algorithms in effect assess
parse quality using syntactic and lexical features.
The SEPA algorithm does not use such features, and
is successful in detecting high quality parses even
when working on the output of a reranker. Rerank-
ing and SEPA are thus relatively independent.
Bagging (Breiman, 1996) uses an ensemble of in-
stances of a model, each trained on a sample of the
training data1. Bagging was suggested in order to
enhance classifiers; the classification outcome was
determined using a majority vote among the mod-
els. In NLP, bagging was used for active learning
for text classification (Argamon-Engelson and Da-
gan, 1999; McCallum and Nigam, 1998). Specif-
ically in parsing, (Henderson and Brill, 2000) ap-
plied a constituent level voting scheme to an en-
semble of bagged models to increase parser perfor-
mance, and (Becker and Osborne, 2005) suggested
an active learning technique in which the agreement
among an ensemble of bagged parsers is used to pre-
dict examples valuable for human annotation. They
reported experiments with small training sets only
(up to 5,000 sentences), and their agreement func-
tion is very different from ours. Both works experi-
mented with generative parsing models only.
Ngai and Yarowsky (2000) used an ensemble
based on bagging and partitioning for active learning
for base NP chunking. They select top items with-
out any graded assessment, and their f-complement
function, which slightly resembles our MF (see the
next section), is applied to the output of a classifier,
while our function is applied to structured output.
A survey of several papers dealing with mapping
1Each sample is created by sampling, with replacement, L
examples from the training pool, where L is the size of the train-
ing pool. Conversely, each of our samples is smaller than the
training set, and is created by sampling without replacement.
See Section 3 (?regarding S?) for a discussion of this issue.
409
predictors in classifiers? output to posterior proba-
bilities is given in (Caruana and Niculescu-Mizil,
2006). As far as we know, the application of a sam-
ple based parser ensemble for assessing parse qual-
ity is novel.
Many IE and QA systems rely on the output of
parsers (Kwok et al, 2001; Attardi et al, 2001;
Moldovan et al, 2003). The latter tries to address
incorrect parses using complex relaxation methods.
Knowing the quality of a parse could greatly im-
prove the performance of such systems.
3 The Sample Ensemble Parse Assessment
(SEPA) Algorithm
In this section we detail our parse assessment algo-
rithm. Its input consists of a parsing algorithm A, an
annotated training set TR, and an unannotated test
set TE. The output provides, for each test sentence,
the parse generated for it by A when trained on the
full training set, and a grade assessing the parse?s
quality, on a continuous scale between 0 to 100. Ap-
plications are then free to select a sentence subset
that suits their needs using our grades, e.g. by keep-
ing only high-quality parses, or by removing low-
quality parses and keeping the rest. The algorithm
has the following stages:
1. Choose N random samples of size S from the
training set TR. Each sample is selected with-
out replacement.
2. Train N copies of the parsing algorithm A,
each with one of the samples.
3. Parse the test set with each of the N models.
4. For each test sentence, compute the value of an
agreement function F between the models.
5. Sort the test set according to F ?s value.
The algorithm uses the level of agreement among
several copies of a parser, each trained on a different
sample from the training data, to predict the qual-
ity of a parse. The higher the agreement, the higher
the quality of the parse. Our approach assumes that
if the parameters of the model are well designed to
annotate a sentence with a high quality parse, then
it is likely that the model will output the same (or
a highly similar) parse even if the training data is
somewhat changed. In other words, we rely on the
stability of the parameters of statistical parsers. Al-
though this is not always the case, our results con-
firm that strong correlation between agreement and
parse quality does exist.
We explored several agreement functions. The
one that showed the best results is Mean F-score
(MF)2, defined as follows. Denote the models by
m1 . . .mN , and the parse provided by mi for sen-
tence s as mi(s). We randomly choose a model ml,
and compute
MF (s) = 1N ? 1
?
i?[1...N ],i6=l
fscore(mi, ml) (1)
We use two measures to evaluate the quality of
SEPA grades. Both measures are defined using a
threshold parameter T , addressing only sentences
whose SEPA grades are not smaller than T . We refer
to these sentences as T-sentences.
The first measure is the average f-score of the
parses of T-sentences. Note that we compute the
f-score of each of the selected sentences and then
average the results. This stands in contrast to the
way f-score is ordinarily calculated, by computing
the labeled precision and recall of the constituents
in the whole set and using these as the arguments of
the f-score equation. The ordinary f-score is com-
puted that way mostly in order to overcome the fact
that sentences differ in length. However, for appli-
cations such as IE and QA, which work at the single
sentence level and which might reach erroneous de-
cision due to an inaccurate parse, normalizing over
sentence lengths is less of a factor. For this reason,
in this paper we present detailed graphs for the aver-
age f-score. For completeness, Table 4 also provides
some of the results using the ordinary f-score.
The second measure is a generalization of the fil-
ter f-score measure suggested by Yates et al (2006).
They define filter precision as the ratio of correctly
parsed sentences in the filtered set (the set the algo-
rithm choose) to total sentences in the filtered set and
filter recall as the ratio of correctly parsed sentences
in the filtered set to correctly parsed sentences in the
2Recall that sentence f-score is defined as: f = 2?P?RP+R ,
where P and R are the labeled precision and recall of the con-
stituents in the sentence relative to another parse.
410
whole set of sentences parsed by the parser (unfil-
tered set or test set). Correctly parsed sentences are
sentences whose parse got f-score of 100%.
Since requiring a 100% may be too restrictive, we
generalize this measure to filter f-score with param-
eter k. In our measure, the filter recall and precision
are calculated with regard to sentences that get an
f-score of k or more, rather than to correctly parsed
sentences. Filtered f-score is thus a special case of
our filtered f-score, with parameter 100.
We now discuss the effect of the number of mod-
els N and the sample size S. The discussion is based
on experiments (using development data, see Sec-
tion 4) in which all the parameters are fixed except
for the parameter in question, using our development
sections.
Regarding N (see Figure 2): As the number of
models increases, the number of T-sentences se-
lected by SEPA decreases and their quality im-
proves, in terms of both average f-score and filter
f-score (with k = 100). The fact that more mod-
els trained on different samples of the training data
agree on the syntactic annotation of a sentence im-
plies that this syntactic pattern is less sensitive to
perturbations in the training data. The number of
such sentences is small and it is likely the parser will
correctly annotate them. The smaller T-set size leads
to a decrease in filter recall, while the better quality
leads to an increase in filter precision. Since the in-
crease in filter precision is sharper than the decrease
in filter recall, filter f-score increases with the num-
ber of models N .
Regarding S3: As the sample size increases, the
number of T-sentences increases, and their qual-
ity degrades in terms of average f-score but im-
proves in terms of filter f-score (again, with param-
eter k = 100). The overlap among smaller sam-
ples is small and the data they supply is sparse. If
several models trained on such samples attach to a
sentence the same parse, this syntactic pattern must
be very prominent in the training data. The num-
ber of such sentences is small and it is likely that
the parser will correctly annotate them. Therefore
smaller sample size leads to smaller T-sets with high
average f-score. As the sample size increases, the T-
set becomes larger but the average f-score of a parse
3Graphs are not shown due to lack of space.
5 10 15 20
90
91
92
93
94
A
ve
ra
ge
 f 
sc
or
e
Number of models ? N
54
56
58
60
62
F
ilt
e
r 
f 
sc
o
re
, 
k 
=
 1
0
0
0 5 10 15 20
65
70
75
80
85
90
F
ilt
e
r 
re
ca
ll,
 k
 =
 1
0
0
35
40
45
50
55
60
F
ilt
e
r 
pr
ec
is
io
n,
 k
 =
 1
00
Number of models ? N
Figure 2: The effect of the number of models N on
SEPA (Collins? model). The scenario is in-domain,
sample size S = 33, 000 and T = 100. We see:
average f-score of T-sentences (left, solid curve and
left y-axis), filter f-score with k = 100 (left, dashed
curve and right y-axis), filter recall with k = 100
(right, solid curve and left y-axis), and filter preci-
sion with k = 100 (right, dashed curve and right
y-axis).
decreases. The larger T-set size leads to increase in
filter recall, while the lower average quality leads
to decrease in filter precision. Since the increase in
filter recall is sharper than the decrease in filter pre-
cision, the result is that filter f-score increases with
the sample size S.
This discussion demonstrates the importance of
using both average f-score and filter f-score, since
the two measures reflect characteristics of the se-
lected sample that are not necessarily highly (or pos-
itively) correlated.
4 Experimental Setup
We performed experiments with two parsing mod-
els, the Collins (1999) generative model number
2 and the Charniak and Johnson (2005) reranking
model. For the first we used a reimplementation
(?). We performed experiments with each model
in two scenarios, in-domain and parser adaptation.
In both experiments the training data are sections
02-21 of the WSJ PennTreebank (about 40K sen-
tences). In the in-domain experiment the test data
is section 23 (2416 sentences) of WSJ and in the
parser adaptation scenario the test data is Brown test
section (2424 sentences). Development sections are
WSJ section 00 for the in-domain scenario (1981
sentences) and Brown development section for the
adaptation scenario (2424 sentences). Following
411
(Gildea, 2001), the Brown test and development sec-
tions consist of 10% of Brown sentences (the 9th and
10th of each 10 consecutive sentences in the devel-
opment and test sections respectively).
We performed experiments with many configu-
rations of the parameters N (number of models),
S (sample size) and F (agreement function). Due
to space limitations we describe only experiments
where the values of the parameters N, S and F are
fixed (F is MF , N and S are given in Section 5)
and the threshold parameter T is changed.
5 Results
We first explore the quality of the selected set in
terms of average f-score. In Section 3 we reported
that the quality of a selected T-set of parses increases
as the number of models N increases and sample
size S decreases. We therefore show the results for
relatively high N (20) and relatively low S (13,000,
which is about a third of the training set). Denote
the cardinality of the set selected by SEPA by n (it
is actually a function of T but we omit the T in order
to simplify notations).
We use several baseline models. The first, confi-
dence baseline (CB), contains the n sentences hav-
ing the highest parser assigned probability (when
trained on the whole training set). The second, min-
imum length (ML), contains the n shortest sentences
in the test set. Since many times it is easier to parse
short sentences, a trivial way to increase the aver-
age f-score measure of a set is simply to select short
sentences. The third, following (Yates et al, 2006),
is maximum recall (MR). MR simply predicts that all
test set sentences should be contained in the selected
T-set. The output set of this model gets filter recall of
1 for any k value, but its precision is lower. The MR
baseline is not relevant to the average f-score mea-
sure, because it selects all of the sentences in a set,
which leads to the same average as a random selec-
tion (see below). In order to minimize visual clutter,
for the filter f-score measure we use the maximum
recall (MR) baseline rather than the minimum length
(ML) baseline, since the former outperforms the lat-
ter. Thus, ML is only shown for the average f-score
measure. We have also experimented with a random
baseline model (containing n randomly selected test
sentences), whose results are the worst and which is
shown for reference.
Readers of this section may get confused between
the agreement threshold parameter T and the param-
eter k of the filter f-score measure. Please note: as to
T , SEPA sorts the test set by the values of the agree-
ment function. One can then select only sentences
whose agreement score is at least T . T ?s values are
on a continuous scale from 0 to 100. As to k, the fil-
ter f-score measure gives a grade. This grade com-
bines three values: (1) the number of sentences in
the set (selected by an algorithm) whose f-score rel-
ative to the gold standard parse is at least k, (2) the
size of the selected set, and (3) the total number of
sentences with such a parse in the whole test set. We
did not introduce separate notations for these values.
Figure 3 (top) shows average f-score results where
SEPA is applied to Collins? generative model in the
in-domain (left) and adaptation (middle) scenarios.
SEPA outperforms the baselines for all values of the
agreement threshold parameter T . Furthermore, as
T increases, not only does the SEPA set quality in-
crease, but the quality differences between this set
and the baseline sets increases as well. The graphs
on the right show the number of sentences in the sets
selected by SEPA for each T value. As expected,
this number decreases as T increases.
Figure 3 (bottom) shows the same pattern of re-
sults for the Charniak reranking parser in the in-
domain (left) and adaptation (middle) scenarios. We
see that the effects of the reranker and SEPA are rel-
atively independent. Even after some of the errors of
the generative model were corrected by the reranker
by selecting parses of higher quality among the 50-
best, SEPA can detect parses of high quality from
the set of parsed sentences.
To explore the quality of the selected set in terms
of filter f-score, we recall that the quality of a se-
lected set of parses increases as both the number of
models N and the sample size S increase, and with
T . Therefore, for k = 85 . . . 100 we show the value
of filter f-score with parameter k when the parame-
ters configuration is a relatively high N (20), rela-
tively high S (33,000, which are about 80% of the
training set), and the highest T (100).
Figure 4 (top) shows filter f-score results for
Collins? generative model in the in-domain (left)
and adaptation (middle) scenarios. As these graphs
show, SEPA outperforms CB and random for all val-
412
ues of the filter f-score parameter k, and outper-
forms the MR baseline where the value of k is 95 or
more. Although for small k values MR gets a higher
f-score than SEPA, the filter precision of SEPA is
much higher (right, shown for adaptation. The in-
domain pattern is similar and not shown). This stems
from the definition of the MR baseline, which sim-
ply predicts any sentence to be in the selected set.
Furthermore, since the selected set is meant to be
the input for systems that require high quality parses,
what matters most is that SEPA outperforms the MR
baseline at the high k ranges.
Figure 4 (bottom) shows the same pattern of re-
sults for the Charniak reranking parser in the in-
domain (left) and adaptation (middle) scenarios. As
for the average f-score measure, it demonstrates that
the effects of the reranker and SEPA algorithm are
relatively independent.
Tables 1 and 2 show the error reduction achieved
by SEPA for the filter f-score measure with param-
eters k = 95, 97, 100 (Table 1) and for the aver-
age f-score measure with several SEPA agreement
threshold (T ) values (Table 2) . The error reductions
achieved by SEPA for both measures are substantial.
Table 3 compares SEPA and WOODWARD on the
exact same test set used by (Yates et al, 2006)
(taken from WSJ sec 23). SEPA achieves error re-
duction of 31% over the MR baseline on this set,
compared to only 20% achieved by WOODWARD.
Not shown in the table, in terms of ordinary f-score
WOODWARD achieves error reduction of 37% while
SEPA achieves 43%. These numbers were the only
ones reported in (Yates et al, 2006).
For completeness of reference, Table 4 shows the
superiority of SEPA over CB in terms of the usual f-
score measure used by the parsing community (num-
bers are counted for constituents first). Results for
other baselines are even more impressive. The con-
figuration is similar to that of Figure 3.
6 Discussion
In this paper we introduced SEPA, a novel algorithm
for assessing parse quality in the output of a statis-
tical parser. SEPA is the first algorithm shown to
be successful when a reranking parser is considered,
even though such models use a reranker to detect
and fix some of the errors made by the base gener-
Filter f-score
In-domain Adaptation
k value 95 97 100 95 97 100
Coll. MR 3.5 20.1 29.2 22.8 29.8 33.6
Coll. CB 11.6 11.7 3.4 14.2 9.9 7.4
Char. MR 1.35 13.6 23.44 21.9 30 32.5
Char. CB 21.9 16.8 11.9 25 20.2 16.2
Table 1: Error reduction in the filter f-score mea-
sure obtained by SEPA with Collins? (top two lines)
and Charniak?s (bottom two lines) model, in the
two scenarios (in-domain and adaptation), vs. the
maximum recall (MR lines 1 and 3) and confi-
dence (CB, lines 2 and 4) baselines, using N =
20, T = 100 and S = 33, 000. Shown are pa-
rameter values k = 95, 97, 100. Error reduction
numbers were computed by 100?(fscoreSEPA?
fscorebaseline)/(1? fscorebaseline).
Average f-score
In-domain Adaptation
T 95 97 100 95 97 100
Coll. ML 32.6 37.2 60.8 46.8 52.7 70.7
Coll. CB 26.5 31.4 53.9 46.9 53.6 70
Char. ML 25.1 33.2 58.5 46.9 58.4 77.1
Char. CB 20.4 30 52 44.4 55.5 73.5
Table 2: Error reduction in the average f-score mea-
sure obtained by SEPA with Collins (top two lines)
and Charniak (bottom two lines) model, in the two
scenarios (in-domain and adaptation), vs. the min-
imum length (ML lines 1 and 3) and confidence
(CB, lines 2 and 4) baselines, using N = 20 and
S = 13, 000. Shown are agreement threhsold pa-
rameter values T = 95, 97, 100. Error reduction
numbers were computed by 100?(fscoreSEPA?
fscorebaseline)/(1? fscorebaseline).
SEPA WOODWARD CB
ER 31% 20% -31%
Table 3: Error reduction compared to the MR base-
line, measured by filter f-score with parameter 100.
The data is the WSJ sec 23 test set usd by (Yates
et al, 2006). All three methods use Collins? model.
SEPA uses N = 20, S = 33, 000, T = 100.
ative model. WOODWARD, the only previously sug-
gested algorithm for this problem, was tested with
Collins? generative model only. Furthermore, this is
the first time that an algorithm for this problem suc-
ceeds in a domain adaptation scenario, regardless of
413
85 90 95 10088
90
92
94
96
98
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 
SEPA
CB
ML
Rand.
85 90 95 10080
85
90
95
100
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 SEPA
CB
ML
Rand.
85 90 95 1000
500
1000
1500
2000
2500
Agreement threshold
N
um
be
r o
f s
en
te
nc
es
 
 
In domain
Adaptation
85 90 95 10092
93
94
95
96
97
98
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 
SEPA
CB
ML
Rand.
85 90 95 10085
90
95
100
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 
SEPA
CB
ML
Rand.
85 90 95 100500
1000
1500
2000
2500
Agreement threshold
N
um
be
r o
f s
en
te
nc
es
 
 
In domain
Adaptation
Figure 3: Agreement threshold T vs. average f-score (left and middle) and number of sentences in the se-
lected set (right), for SEPA with Collins? generative model (top) and the Charniak reranking model (bottom).
SEPA parameters are S = 13, 000, N = 20. In both rows, SEPA results for the in-domain (left) and adap-
tation (middle) scenarios are compared to the confidence (CB) and minimum length (ML) baselines. The
graphs on the right show the number of sentences in the selected set for both scenarios.
85 90 95 1000.3
0.4
0.5
0.6
0.7
0.8
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand. 85 90 95 100
0.4
0.5
0.6
0.7
0.8
0.9
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand.
85 90 95 1000.2
0.4
0.6
0.8
1
K
Fi
lte
r p
re
cis
io
n 
wi
th
 p
ar
am
et
er
 k
 
 SEPA
CB
MR
Rand.
85 90 95 1000.4
0.5
0.6
0.7
0.8
0.9
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand.
85 90 95 1000.4
0.5
0.6
0.7
0.8
0.9
1
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand.
85 90 95 1000.2
0.4
0.6
0.8
1
K
Fi
lte
r p
re
cis
io
n 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA CB MR Rand.
Figure 4: Parameter k vs. filter f-score (left and middle) and filter precision (right) with that parameter, for
SEPA with Collins? generative model (top) and the Charniak reranking model (bottom). SEPA parameters
are S = 33, 000, N = 20, T = 100. In both rows, results for the in-domain (left) and adaptation (middle)
scenarios. In two leftmost graphs, the performance of the algorithm is compared to the confidence baseline
(CB) and maximum recall (MR). The graphs on the right compare the filter precision of SEPA with that of
the MR and CB baselines.
414
the parsing model. In the Web environment this is
the common situation.
The WSJ and Brown experiments performed with
SEPA are much broader than those performed with
WOODWARD, considering all sentences of WSJ sec
23 and Brown test section rather than a subset
of carefully selected sentences from WSJ sec 23.
However, we did not perform a TREC experiment,
as (Yates et al, 2006) did. Our WSJ and Brown
results outperformed several baselines. Moreover,
WSJ (or Brown) sentences that contain conjunctions
were avoided in the experiments of (Yates et al,
2006). We have verified that our algorithm shows
substantial error reduction over the baselines for this
type of sentences (in the ranges 13 ? 46% for the
filter f-score with k = 100, and 30 ? 60% for the
average f-score).
As Table 3 shows, on a WSJ sec 23 test set similar
to that used by (Yates et al, 2006), SEPA achieves
31% error reduction compared to 20% of WOOD-
WARD.
WOODWARD works under several assumptions.
Specifically, it requires a corpus whose content over-
laps at least in part with the content of the parsed
sentences. This corpus is used to extract semanti-
cally related statistics for its filters. Furthermore, the
filters of this algorithm (except of the QA filter) are
focused on verb and preposition relations. Thus, it
is more natural for it to deal with mistakes contained
in such relations. This is reflected in the WSJ based
test set on which it is tested. SEPA does not make
any of these assumptions. It does not use any exter-
nal information source and is shown to select high
quality parses from diverse sets.
In-domain Adaptation
F ER F ER
SEPA Collins 97.09 44.36% 95.38 66.38%
CB Collins 94.77 ? 86.3 ?
SEPA Char-
niak
97.21 35.69% 96.3 54.66%
CB Charniak 95.6 ? 91.84 ?
Table 4: SEPA error reduction vs. the CB base-
line in the in-domain and adaptation scenarios, us-
ing the traditional f-score of the parsing literature.
N = 20, S = 13, 000, T = 100.
For future work, integrating SEPA into the rerank-
ing process seems a promising direction for enhanc-
ing overall parser performance.
Acknowledgement. We would like to thank Dan
Roth for his constructive comments on this paper.
References
Shlomo Argamon-Engelson and Ido Dagan, 1996.
committee-based sample selection for probabilistic
classifiers. Journal of Artificial Intelligence Research,
11:335?360.
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi and Alessandro Tommasi, 2001.
PiQASso: Pisa question answering system. TREC
?01.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ?05.
Daniel Bikel, 2004. Code developed at University of
Pennsylvania. http://www.cis.upenn.edu.bikel.
Leo Breiman, 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Rich Caruana and Alexandru Niculescu-Mizil, 2006.
An empirical comparison of supervised learning algo-
rithms. ICML ?06.
Eugene Charniak and Mark Johnson, 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. ACL ?05.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Daniel Gildea, 2001. Corpus variation and parser perfor-
mance. EMNLP ?01.
John C. Henderson and Eric Brill, 2000. Bagging and
boosting a treebank parser. NAACL ?00.
Terry Koo and Michael Collins, 2005. Hidden-variable
models for discriminative reranking. EMNLP ?05.
Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.
Scaling question answering to the web. WWW ?01.
Andrew McCallum and Kamal Nigam, 1998. Employing
EM and pool-based active learning for text classifica-
tion. ICML ?98.
Dan Moldovan, Christine Clark, Sanda Harabagiu and
Steve Maiorano, 2003. Cogex: A logic prover for
question answering. HLT-NAACL ?03.
Grace Ngai and David Yarowsky, 2000. Rule writing or
annotation: cost-efficient resource usage for base noun
phrase chunking. ACL ?00.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting parser errors using web-based
semantic filters. EMNLP ?06.
415
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616?623,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Self-Training for Enhancement and Domain Adaptation of
Statistical Parsers Trained on Small Datasets
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Creating large amounts of annotated data to
train statistical PCFG parsers is expensive,
and the performance of such parsers declines
when training and test data are taken from
different domains. In this paper we use self-
training in order to improve the quality of
a parser and to adapt it to a different do-
main, using only small amounts of manually
annotated seed data. We report significant
improvement both when the seed and test
data are in the same domain and in the out-
of-domain adaptation scenario. In particu-
lar, we achieve 50% reduction in annotation
cost for the in-domain case, yielding an im-
provement of 66% over previous work, and a
20-33% reduction for the domain adaptation
case. This is the first time that self-training
with small labeled datasets is applied suc-
cessfully to these tasks. We were also able
to formulate a characterization of when self-
training is valuable.
1 Introduction
State of the art statistical parsers (Collins, 1999;
Charniak, 2000; Koo and Collins, 2005; Charniak
and Johnson, 2005) are trained on manually anno-
tated treebanks that are highly expensive to create.
Furthermore, the performance of these parsers de-
creases as the distance between the genres of their
training and test data increases. Therefore, enhanc-
ing the performance of parsers when trained on
small manually annotated datasets is of great impor-
tance, both when the seed and test data are taken
from the same domain (the in-domain scenario) and
when they are taken from different domains (the out-
of-domain or parser adaptation scenario). Since the
problem is the expense in manual annotation, we de-
fine ?small? to be 100-2,000 sentences, which are the
sizes of sentence sets that can be manually annotated
by constituent structure in a few hours1.
Self-training is a method for using unannotated
data when training supervised models. The model is
first trained using manually annotated (?seed?) data,
then the model is used to automatically annotate a
pool of unannotated (?self-training?) data, and then
the manually and automatically annotated datasets
are combined to create the training data for the fi-
nal model. Self-training of parsers trained on small
datasets is of enormous potential practical impor-
tance, due to the huge amounts of unannotated data
that are becoming available today and to the high
cost of manual annotation.
In this paper we use self-training to enhance the
performance of a generative statistical PCFG parser
(Collins, 1999) for both the in-domain and the parser
adaptation scenarios, using only small amounts of
manually annotated data. We perform four experi-
ments, examining all combinations of in-domain and
out-of-domain seed and self-training data.
Our results show that self-training is of substantial
benefit for the problem. In particular, we present:
? 50% reduction in annotation cost when the seed
and test data are taken from the same domain,
which is 66% higher than any previous result
with small manually annotated datasets.
1We note in passing that quantitative research on the cost of
annotation using various annotation schemes is clearly lacking.
616
? The first time that self-training improves a gen-
erative parser when the seed and test data are
from the same domain.
? 20-33% reduction in annotation cost when the
seed and test data are from different domains.
? The first time that self-training succeeds in
adapting a generative parser between domains
using a small manually annotated dataset.
? The first formulation (related to the number of
unknown words in a sentence) of when self-
training is valuable.
Section 2 discusses previous work, and Section 3
compares in-depth our protocol to a previous one.
Sections 4 and 5 present the experimental setup and
our results, and Section 6 analyzes the results in an
attempt to shed light on the phenomenon of self-
training.
2 Related Work
Self-training might seem a strange idea: why should
a parser trained on its own output learn anything
new? Indeed, (Clark et al, 2003) applied self-
training to POS-tagging with poor results, and
(Charniak, 1997) applied it to a generative statisti-
cal PCFG parser trained on a large seed set (40K
sentences), without any gain in performance.
Recently, (McClosky et al, 2006a; McClosky et
al., 2006b) have successfully applied self-training to
various parser adaptation scenarios using the rerank-
ing parser of (Charniak and Johnson, 2005). A
reranking parser (see also (Koo and Collins, 2005))
is a layered model: the base layer is a generative sta-
tistical PCFG parser that creates a ranked list of k
parses (say, 50), and the second layer is a reranker
that reorders these parses using more detailed fea-
tures. McClosky et al(2006a) use sections 2-21 of
the WSJ PennTreebank as seed data and between
50K to 2,500K unlabeled NANC corpus sentences
as self-training data. They train the PCFG parser and
the reranker with the manually annotated WSJ data,
and parse the NANC data with the 50-best PCFG
parser. Then they proceed in two directions. In
the first, they reorder the 50-best parse list with the
reranker to create a new 1-best list. In the second,
they leave the 1-best list produced by the genera-
tive PCFG parser untouched. Then they combine the
1-best list (each direction has its own list) with the
WSJ training set, to retrain the PCFG parser. The
final PCFG model and the reranker (trained only on
annotated WSJ material) are then used to parse the
test section (23) of WSJ.
There are two major differences between these pa-
pers and the current one, stemming from their usage
of a reranker and of large seed data. First, when
their 1-best list of the base PCFG parser was used
as self training data for the PCFG parser (the sec-
ond direction), the performance of the base parser
did not improve. It had improved only when the 1-
best list of the reranker was used. In this paper we
show how the 1-best list of a base (generative) PCFG
parser can be used as a self-training material for the
base parser itself and enhance its performance, with-
out using any reranker. This reveals a noteworthy
characteristic of generative PCFG models and offers
a potential direction for parser improvement, since
the quality of a parser-reranker combination criti-
cally depends on that of the base parser.
Second, these papers did not explore self-training
when the seed is small, a scenario whose importance
has been discussed above. In general, PCFG mod-
els trained on small datasets are less likely to parse
the self-training data correctly. For example, the f-
score of WSJ data parsed by the base PCFG parser
of (Charniak and Johnson, 2005) when trained on
the training sections of WSJ is between 89% to
90%, while the f-score of WSJ data parsed with the
Collins? model that we use, and a small seed, is be-
tween 40% and 80%. As a result, the good results of
(McClosky et al 2006a; 2006b) with large seed sets
do not immediately imply success with small seed
sets. Demonstration of such success is a contribu-
tion of the present paper.
Bacchiani et al(2006) explored the scenario of
out-of-domain seed data (the Brown training set
containing about 20K sentences) and in-domain
self-training data (between 4K to 200K sentences
from the WSJ) and showed an improvement over
the baseline of training the parser with the seed data
only. However, they did not explore the case of small
seed datasets (the effort in manually annotating 20K
is substantial) and their work addresses only one of
our scenarios (OI, see below).
617
A work closely related to ours is (Steedman et
al., 2003a), which applied co-training (Blum and
Mitchell, 1998) and self-training to Collins? pars-
ing model using a small seed dataset (500 sentences
for both methods and 1,000 sentences for co-training
only). The seed, self-training and test datasets they
used are similar to those we use in our II experi-
ment (see below), but the self-training protocols are
different. They first train the parser with the seed
sentences sampled from WSJ sections 2-21. Then,
iteratively, 30 sentences are sampled from these sec-
tions, parsed by the parser, and the 20 best sentences
(in terms of parser confidence defined as probability
of top parse) are selected and combined with the pre-
viously annotated data to retrain the parser. The co-
training protocol is similar except that each parser
is trained with the 20 best sentences of the other
parser. Self-training did not improve parser perfor-
mance on the WSJ test section (23). Steedman et
al (2003b) followed a similar co-training protocol
except that the selection function (three functions
were explored) considered the differences between
the confidence scores of the two parsers. In this pa-
per we show a self-training protocol that achieves
better results than all of these methods (Table 2).
The next section discusses possible explanations for
the difference in results. Steedman et al(2003b) and
Hwa et al (2003) also used several versions of cor-
rected co-training which are not comparable to ours
and other suggested methods because their evalua-
tion requires different measures (e.g. reviewed and
corrected constituents are separately counted).
As far as we know, (Becker and Osborne, 2005)
is the only additional work that tries to improve a
generative PCFG parsers using small seed data. The
techniques used are based on active learning (Cohn
et al, 1994). The authors test two novel methods,
along with the tree entropy (TE) method of (Hwa,
2004). The seed, the unannotated and the test sets,
as well as the parser used in that work, are similar
to those we use in our II experiment. Our results are
superior, as shown in Table 3.
3 Self-Training Protocols
There are many possible ways to do self-training.
A main goal of this paper is to identify a self-
training protocol most suitable for enhancement and
domain adaptation of statistical parsers trained on
small datasets. No previous work has succeeded in
identifying such a protocol for this task. In this sec-
tion we try to understand why.
In the protocol we apply, the self-training set con-
tains several thousand sentences A parser trained
with a small seed set parses the self-training set, and
then the whole automatically annotated self-training
set is combined with the manually annotated seed
set to retrain the parser. This protocol and that of
Steedman et al(2003a) were applied to the problem,
with the same seed, self-training and test sets. As
we show below (see Section 4 and Section 5), while
Steedman?s protocol does not improve over the base-
line of using only the seed data, our protocol does.
There are four differences between the protocols.
First, Steedman et als seed set consists of consecu-
tive WSJ sentences, while we select them randomly.
In the next section we show that this difference is
immaterial. Second, Steedman et als protocol looks
for sentences of high quality parse, while our pro-
tocol prefers to use many sentences without check-
ing their parse quality. Third, their protocol is itera-
tive while ours uses a single step. Fourth, our self-
training set is orders of magnitude larger than theirs.
To examine the parse quality issue, we performed
their experiment using their setting but selecting the
high quality parse sentences using their f-score rel-
ative to the gold standard annotation from secs 2-
21 rather than a quality estimate. No improvement
over the baseline was achieved even with this or-
acle. Thus the problem with their protocol does
not lie with the parse quality assessment function;
no other function would produce results better than
the oracle. To examine the iteration issue, we per-
formed their experiment in a single step, selecting at
once the oracle-best 2,000 among 3,000 sentences2,
which produced only a mediocre improvement. We
thus conclude that the size of the self-training set is a
major factor responsible for the difference between
the protocols.
4 Experimental Setup
We used a reimplementation of Collins? parsing
model 2 (Bikel, 2004). We performed four experi-
ments, II, IO, OI, and OO, two with in-domain seed
2Corresponding to a 100 iterations of 30 sentences each.
618
(II, IO) and two with out-of-domain seed (OI, OO),
examining in-domain self-training (II, OI) and out-
of-domain self-training (IO, OO). Note that being
?in? or ?out? of domain is determined by the test data.
Each experiment contained 19 runs. In each run a
different seed size was used, from 100 sentences on-
wards, in steps of 100. For statistical significance,
we repeated each experiment five times, in each rep-
etition randomly sampling different manually anno-
tated sentences to form the seed dataset3.
The seed data were taken from WSJ sections 2-
21. For II and IO, the test data is WSJ section 23
(2416 sentences) and the self-training data are either
WSJ sections 2-21 (in II, excluding the seed sen-
tences) or the Brown training section (in IO). For
OI and OO, the test data is the Brown test section
(2424 sentences), and the self-training data is either
the Brown training section (in OI) or WSJ sections
2-21 (in OO). We removed the manual annotations
from the self-training sections before using them.
For the Brown corpus, we based our division
on (Bacchiani et al, 2006; McClosky et al, 2006b).
The test and training sections consist of sentences
from all of the genres that form the corpus. The
training division consists of 90% (9 of each 10 con-
secutive sentences) of the data, and the test section
are the remaining 10% (We did not use any held out
data). Parsing performance is measured by f-score,
f = 2?P?RP+R , where P, R are labeled precision and
recall.
To further demonstrate our results for parser adap-
tation, we also performed the OI experiment where
seed data is taken from WSJ sections 2-21 and both
self-training and test data are taken from the Switch-
board corpus. The distance between the domains of
these corpora is much greater than the distance be-
tween the domains of WSJ and Brown. The Brown
and Switchboard corpora were divided to sections in
the same way.
We have also performed all four experiments with
the seed data taken from the Brown training section.
3 (Steedman et al, 2003a) used the first 500 sentences of
WSJ training section as seed data. For direct comparison, we
performed our protocol in the II scenario using the first 500 or
1000 sentences of WSJ training section as seed data and got
similar results to those reported below for our protocol with ran-
dom selection. We also applied the protocol of Steedman et al
to scenario II with 500 randomly selected sentences, getting no
improvement over the random baseline.
The results were very similar and will not be detailed
here due to space constraints.
5 Results
5.1 In-domain seed data
In these two experiments we show that when the
seed and test data are taken from the same domain, a
very significant enhancement of parser performance
can be achieved, whether the self-training material
is in-domain (II) or out-of-domain (IO). Figure 1
shows the improvement in parser f-score when self-
training data is used, compared to when it is not
used. Table 1 shows the reduction in manually an-
notated seed data needed to achieve certain f-score
levels. The enhancement in performance is very im-
pressive in the in-domain self-training data scenario
? a reduction of 50% in the number of manually an-
notated sentences needed for achieving 75 and 80 f-
score values. A significant improvement is achieved
in the out-of-domain self-training scenario as well.
Table 2 compares our results with self-training
and co-training results reported by (Steedman et al
20003a; 2003b). As stated earlier, the experimental
setup of these works is similar to ours, but the self-
training protocols are different. For self-training,
our II improves an absolute 3.74% over their 74.3%
result, which constitutes a 14.5% reduction in error
(from 25.7%).
The table shows that for both seed sizes our
self training protocol outperforms both the self-
training and co-training protocols of (Steedman et
al, 20003a; 2003b). Results are not included in the
table only if they are not reported in the relevant pa-
per. The self-training protocol of (Steedman et al,
2003a) does not actually improve over the baseline
of using only the seed data. Section 3 discussed a
possible explanation to the difference in results.
In Table 3 we compare our results to the results of
the methods tested in (Becker and Osborne, 2005)
(including TE)4. To do that, we compare the reduc-
tion in manually annotated data needed to achieve
an f-score value of 80 on WSJ section 23 achieved
by each method. We chose this measure since it is
4The measure is constituents and not sentences because this
is how results are reported in (Becker and Osborne, 2005).
However, the same reduction is obtained when sentences are
counted, because the number of constituents is averaged when
taking many sentences.
619
f-score 75 80
Seed data only 600(0%) 1400(0%)
II 300(50%) 700(50%)
IO 500(17%) 1200(14.5%)
Table 1: Number of in-domain seed sentences
needed for achieving certain f-scores. Reductions
compared to no self-training (line 1) are given in
parentheses.
Seed
size
our
II
our
IO
Steedman
ST
Steedman
CT
Steedman
CT
2003a 2003b
500
sent.
78.04 75.81 74.3 76.9 ?-
1,000
sent.
81.43 79.49 ?- 79 81.2
Table 2: F-scores of our in-domain-seed self-
training vs. self-training (ST) and co-training (CT)
of (Steedman et al 20003a; 2003b).
the only explicitly reported number in that work. As
the table shows, our method is superior: our reduc-
tion of 50% constitutes an improvement of 66% over
their best reduction of 30.6%.
When applying self-training to a parser trained
with a small dataset we expect the coverage of the
parser to increase, since the combined training set
should contain items that the seed dataset does not.
On the other hand, since the accuracy of annota-
tion of such a parser is poor (see the no self-training
curve in Figure 1) the combined training set surely
includes inaccurate labels that might harm parser
performance. Figure 2 (left) shows the increase in
coverage achieved for in-domain and out-of-domain
self-training data. The improvements induced by
both methods are similar. This is quite surpris-
ing given that the Brown sections we used as self-
training data contain science, fiction, humor, ro-
mance, mystery and adventure texts while the test
section in these experiments, WSJ section 23, con-
tains only news articles.
Figure 2 also compares recall (middle) and preci-
sion (right) for the different methods. For II there
is a significant improvement in both precision and
recall even though many more sentences are parsed.
For IO, there is a large gain in recall and a much
smaller loss in precision, yielding a substantial im-
provement in f-score (Figure 1).
F -
score
This
work - II
Becker
unparsed
Becker en-
tropy/unparsed
Hwa
TE
80 50% 29.4% 30.6% -5.7%
Table 3: Reduction of the number of manually anno-
tated constituents needed for achieving f score value
of 80 on section 23 of the WSJ. In all cases the seed
and additional sentences selected to train the parser
are taken from sections 02-21 of WSJ.
5.2 Out-of-domain seed data
In these two experiments we show that self-training
is valuable for adapting parsers from one domain to
another. Figure 3 compares out-of-domain seed data
used with in-domain (OI) or out-of-domain (OO)
self-training data against the baseline of training
only with the out-of-domain seed data.
The left graph shows a significant improvement
in f-score. In the middle and right graphs we exam-
ine the quality of the parses produced by the model
by plotting recall and precision vs. seed size. Re-
garding precision, the difference between the three
conditions is small relative to the f-score difference
shown in the left graph. The improvement in the
recall measure is much greater than the precision
differences, and this is reflected in the f-score re-
sult. The gain in coverage achieved by both meth-
ods, which is not shown in the figure, is similar to
that reported for the in-domain seed experiments.
The left graph along with the increase in coverage
show the power of self-training in parser adaptation
when small seed datasets are used: not only do OO
and OI parse many more sentences than the baseline,
but their f-score values are consistently better.
To see how much manually annotated data can
be saved by using out-of-domain seed, we train the
parsing model with manually annotated data from
the Brown training section, as described in Sec-
tion 4. We assume that given a fixed number of
training sentences the best performance of the parser
without self-training will occur when these sen-
tences are selected from the domain of the test sec-
tion, the Brown corpus. We compare the amounts of
manually annotated data needed to achieve certain f-
score levels in this condition with the corresponding
amounts of data needed by OI and OO. The results
are summarized in Table 4. We compare to two base-
lines using in- and out-of-domain seed data without
620
0 200 400 600 800 100040
50
60
70
80
90
number of manually annotated sentences
f s
co
re
 
 
no self training
wsj self?training
brown self?training
1000 1200 1400 1600 1800 200078
79
80
81
82
83
84
number of manually annotated sentences
f s
co
re
 
 
no self?training
wsj self?training
brown self?training
Figure 1: Number of seed sentences vs. f-score, for the two in-domain seed experiments: II (triangles) and
IO (squares), and for the no self-training baseline. Self-training provides a substantial improvement.
0 500 1000 1500 20001000
1500
2000
2500
number of manually annotated sentences
n
u
m
be
r o
f c
ov
er
ed
 s
en
te
nc
es
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200020
40
60
80
100
number of manually annotated sentences
re
c
a
ll
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200065
70
75
80
85
number of manually annotated sentences
pr
ec
is
io
n
 
 
no self?training
wsj self?training
brown self?training
Figure 2: Number of seed sentences vs. coverage (left), recall (middle) and precision (right) for the two
in-domain seed experiments: II (triangles) and IO (squares), and for the no self-training baseline.
any self-training. The second line (ID) serves as a
reference to compute how much manual annotation
of the test domain was saved, and the first line (OD)
serves as a reference to show by how much self-
training improves the out-of-domain baseline. The
table stops at an f-score of 74 because that is the
best that the baselines can do.
A significant reduction in annotation cost over the
ID baseline is achieved where the seed size is be-
tween 100 and 1200. Improvement over the OD
baseline is for the whole range of seed sizes. Both
OO and OI achieve 20-33% reduction in manual an-
notation compared to the ID baseline and enhance
the performance of the parser by as much as 42.9%.
The only previous work that adapts a parser
trained on a small dataset between domains is that
of (Steedman et al, 2003a), which used co-training
(no self-training results were reported there or else-
where). In order to compare with that work, we per-
formed OI with seed taken from the Brown corpus
and self-training and test taken from WSJ, which
is the setup they use, obtaining a similar improve-
ment to that reported there. However, co-training is
a more complex method that requires an additional
parser (LTAG in their case).
To further substantiate our results for the parser
adaptation scenario, we used an additional corpus,
Switchboard. Figure 4 shows the results of an OI
experiment with WSJ seed and Switchboard self-
training and test data. Although the domains of these
two corpora are very different (more so than WSJ
and Brown), self-training provides a substantial im-
provement.
We have also performed all four experiments with
Brown and WSJ trading places. The results obtained
were very similar to those reported here, and will not
be detailed due to lack of space.
6 Analysis
In this section we try to better understand the ben-
efit in using self-training with small seed datasets.
We formulate the following criterion: the number of
words in a test sentence that do not appear in the
seed data (?unknown words?) is a strong indicator
621
0 500 1000 1500 200030
40
50
60
70
80
number of manually annotated sentences
f s
co
re
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200020
30
40
50
60
70
80
number of manually annotated sentences
re
c
a
ll
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200072
74
76
78
80
82
number of manually annotated sentences
pr
ec
is
io
n
 
 
no self?training
wsj self?training
brown self?training
Figure 3: Number of seed sentences vs. f-score (left), recall (middle) and precision (right), for the two
out-of-domain seed data experiments: OO (triangles) and OI (squares), and for the no self-training baseline.
f-sc. 66 68 70 72 74
OD 600 800 1, 000 1, 400 ?
ID 600 700 800 1, 000 1, 200
OO 400 500 600 800 1100
33, 33 28.6, 37.5 33, 40 20, 42.9 8, ?
OI 400 500 600 800 1, 300
33, 33 28.6, 37.5 33, 40 20, 42.9 ?8, ?
Table 4: Number of manually annotated seed sen-
tences needed for achieving certain f-score values.
The first two lines show the out-of-domain and in-
domain seed baselines. The reductions compared to
the baselines is given as ID, OD.
0 500 1000 1500 200010
20
30
40
50
number of manually annotated sentences
f s
co
re
 
 
switchboard self?training
no self?training
Figure 4: Number of seed sentences vs. f-score,
for the OI experiment using WSJ seed data and
SwitchBoard self-training and test data. In spite of
the strong dissimilarity between the domains, self-
training provides a substantial improvement.
to whether it is worthwhile to use small seed self-
training. Figure 5 shows the number of unknown
words in a sentence vs. the probability that the self-
training model will parse a sentence no worse (up-
per curve) or better (lower curve) than the baseline
model.
The upper curve shows that regardless of the
0 10 20 30 40 500
0.2
0.4
0.6
0.8
1
number of unknown words
pr
ob
ab
ili
ty
 
 
ST > baseline
ST >= baseline
Figure 5: For sentences having the same number of
unknown words, we show the probability that the
self-training model parses a sentence from the set
no worse (upper curve) or better (lower curve) than
the baseline model.
number of unknown words in the sentence, there is
more than 50% chance that the self-training model
will not harm the result. This probability decreases
from almost 1 for a very small number of unknown
words to about 0.55 for 50 unknown words. The
lower curve shows that when the number of un-
known words increases, the probability that the
self-training model will do better than the baseline
model increases from almost 0 (for a very small
number of unknown words) to about 0.55. Hence,
the number of unknown words is an indication for
the potential benefit (value on the lower curve)
and risk (1 minus the value on the upper curve) in
using the self-training model compared to using the
baseline model. Unknown words were not identified
in (McClosky et al, 2006a) as a useful predictor for
the benefit of self-training.
622
We also identified a length effect similar to that
studied by (McClosky et al, 2006a) for self-training
(using a reranker and large seed, as detailed in Sec-
tion 2). Due to space limitations we do not discuss
it here.
7 Discussion
Self-training is usually not considered to be a valu-
able technique in improving the performance of gen-
erative statistical parsers, especially when the man-
ually annotated seed sentence dataset is small. In-
deed, in the II scenario, (Steedman et al, 2003a;
McClosky et al, 2006a; Charniak, 1997) reported
no improvement of the base parser for small (500
sentences, in the first paper) and large (40K sen-
tences, in the last two papers) seed datasets respec-
tively. In the II, OO, and OI scenarios, (McClosky et
al, 2006a; 2006b) succeeded in improving the parser
performance only when a reranker was used to re-
order the 50-best list of the generative parser, with a
seed size of 40K sentences. Bacchiani et al(2006)
improved the parser performance in the OI scenario
but their seed size was large (about 20K sentences).
In this paper we have shown that self-training
can enhance the performance of generative parsers,
without a reranker, in four in- and out-of-domain
scenarios using a small seed dataset. For the II, IO
and OO scenarios, we are the first to show improve-
ment by self-training for generative parsers. We
achieved a 50% (20-33%) reduction in annotation
cost for the in-domain (out-of-domain) seed data
scenarios. Previous work with small seed datasets
considered only the II and OI scenarios. Our results
for the former are better than any previous method,
and our results for the latter (which are the first
reported self-training results) are similar to previ-
ous results for co-training, a more complex method.
We demonstrated our results using three corpora of
varying degrees of domain difference.
A direction for future research is combining
self-training data from various domains to enhance
parser adaptation.
Acknowledgement. We would like to thank Dan
Roth for his constructive comments on this paper.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat, 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ?05.
Daniel Bikel, 2004. Code developed at University of
Pennsylvania. http://www.cis.upenn.edu.bikel.
Avrim Blum and Tom M. Mitchell, 1998. Combining la-
beled and unlabeled data with co-training. COLT ?98.
Eugene Charniak, 1997. Statistical parsing with a
context-free grammar and word statistics. AAAI ?97.
Eugene Charniak, 2000. A maximum-entropy-inspired
parser. ANLP ?00.
Eugene Charniak and Mark Johnson, 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. ACL ?05.
Stephen Clark, James Curran, and Miles Osborne,
2003. Bootstrapping pos taggers using unlabelled
data. CoNLL ?03.
David A. Cohn, Les Atlas, and Richard E. Ladner, 1994.
Improving generalization with active learning. Ma-
chine Learning, 15(2):201?221.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Rebecca Hwa, Miles Osborne, Anoop Sarkar and Mark
Steedman, 2003. Corrected co-training for statistical
parsers. In ICML ?03, Workshop on the Continuum
from Labeled to Unlabeled Data in Machine Learning
and Data Mining.
Rebecca Hwa, 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Terry Koo and Michael Collins, 2005. Hidden-variable
models for discriminative reranking. EMNLP ?05.
David McClosky, Eugene Charniak, and Mark John-
son, 2006a. Effective self-training for parsing. HLT-
NAACL ?06.
David McClosky, Eugene Charniak, and Mark Johnson,
2006b. Reranking and self-training for parser adapta-
tion. ACL-COLING ?06.
Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca
Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim, 2003a. Bootstrap-
ping statistical parsers from small datasets. EACL ?03.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles
Osborne, Anoop Sarkar, Julia Hockenmaier, Paul
Ruhlen,Steven Baker, and Jeremiah Crim, 2003b. Ex-
ample selection for bootstrapping statistical parsers.
NAACL ?03.
623
Proceedings of ACL-08: HLT, pages 861?869,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multi-Task Active Learning for Linguistic Annotations
Roi Reichart1? Katrin Tomanek2? Udo Hahn2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem, Israel
{roiri|arir}@cs.huji.ac.il
2Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We extend the classical single-task active
learning (AL) approach. In the multi-task ac-
tive learning (MTAL) paradigm, we select ex-
amples for several annotation tasks rather than
for a single one as usually done in the con-
text of AL. We introduce two MTAL meta-
protocols, alternating selection and rank com-
bination, and propose a method to implement
them in practice. We experiment with a two-
task annotation scenario that includes named
entity and syntactic parse tree annotations on
three different corpora. MTAL outperforms
random selection and a stronger baseline, one-
sided example selection, in which one task is
pursued using AL and the selected examples
are provided also to the other task.
1 Introduction
Supervised machine learning methods have success-
fully been applied to many NLP tasks in the last few
decades. These techniques have demonstrated their
superiority over both hand-crafted rules and unsu-
pervised learning approaches. However, they re-
quire large amounts of labeled training data for every
level of linguistic processing (e.g., POS tags, parse
trees, or named entities). When, when domains
and text genres change (e.g., moving from common-
sense newspapers to scientific biology journal arti-
cles), extensive retraining on newly supplied train-
ing material is often required, since different do-
mains may use different syntactic structures as well
as different semantic classes (entities and relations).
? Both authors contributed equally to this work.
Consequently, with an increasing coverage of a
wide variety of domains in human language tech-
nology (HLT) systems, we can expect a growing
need for manual annotations to support many kinds
of application-specific training data.
Creating annotated data is extremely labor-
intensive. The Active Learning (AL) paradigm
(Cohn et al, 1996) offers a promising solution to
deal with this bottleneck, by allowing the learning
algorithm to control the selection of examples to
be manually annotated such that the human label-
ing effort be minimized. AL has been successfully
applied already for a wide range of NLP tasks, in-
cluding POS tagging (Engelson and Dagan, 1996),
chunking (Ngai and Yarowsky, 2000), statistical
parsing (Hwa, 2004), and named entity recognition
(Tomanek et al, 2007).
However, AL is designed in such a way that it se-
lects examples for manual annotation with respect to
a single learning algorithm or classifier. Under this
AL annotation policy, one has to perform a separate
annotation cycle for each classifier to be trained. In
the following, we will refer to the annotations sup-
plied for a classifier as the annotations for a single
annotation task.
Modern HLT systems often utilize annotations re-
sulting from different tasks. For example, a machine
translation system might use features extracted from
parse trees and named entity annotations. For such
an application, we obviously need the different an-
notations to reside in the same text corpus. It is not
clear how to apply the single-task AL approach here,
since a training example that is beneficial for one
task might not be so for others. We could annotate
861
the same corpus independently by the two tasks and
merge the resulting annotations, but that (as we show
in this paper) would possibly yield sub-optimal us-
age of human annotation efforts.
There are two reasons why multi-task AL, and
by this, a combined corpus annotated for various
tasks, could be of immediate benefit. First, annota-
tors working on similar annotation tasks (e.g., con-
sidering named entities and relations between them),
might exploit annotation data from one subtask for
the benefit of the other. If for each subtask a sepa-
rate corpus is sampled by means of AL, annotators
will definitely lack synergy effects and, therefore,
annotation will be more laborious and is likely to
suffer in terms of quality and accuracy. Second, for
dissimilar annotation tasks ? take, e.g., a compre-
hensive HLT pipeline incorporating morphological,
syntactic and semantic data ? a classifier might re-
quire features as input which constitute the output
of another preceding classifier. As a consequence,
training such a classifier which takes into account
several annotation tasks will best be performed on
a rich corpus annotated with respect to all input-
relevant tasks. Both kinds of annotation tasks, simi-
lar and dissimilar ones, constitute examples of what
we refer to as multi-task annotation problems.
Indeed, there have been efforts in creating re-
sources annotated with respect to various annotation
tasks though each of them was carried out indepen-
dently of the other. In the general language UPenn
annotation efforts for the WSJ sections of the Penn
Treebank (Marcus et al, 1993), sentences are anno-
tated with POS tags, parse trees, as well as discourse
annotation from the Penn Discourse Treebank (Milt-
sakaki et al, 2008), while verbs and verb arguments
are annotated with Propbank rolesets (Palmer et al,
2005). In the biomedical GENIA corpus (Ohta et
al., 2002), scientific text is annotated with POS tags,
parse trees, and named entities.
In this paper, we introduce multi-task active
learning (MTAL), an active learning paradigm for
multiple annotation tasks. We propose a new AL
framework where the examples to be annotated are
selected so that they are as informative as possible
for a set of classifiers instead of a single classifier
only. This enables the creation of a single combined
corpus annotated with respect to various annotation
tasks, while preserving the advantages of AL with
respect to the minimization of annotation efforts.
In a proof-of-concept scenario, we focus on two
highly dissimilar tasks, syntactic parsing and named
entity recognition, study the effects of multi-task AL
under rather extreme conditions. We propose two
MTAL meta-protocols and a method to implement
them for these tasks. We run experiments on three
corpora for domains and genres that are very differ-
ent (WSJ: newspapers, Brown: mixed genres, and
GENIA: biomedical abstracts). Our protocols out-
perform two baselines (random and a stronger one-
sided selection baseline).
In Section 2 we introduce our MTAL framework
and present two MTAL protocols. In Section 3 we
discuss the evaluation of these protocols. Section
4 describes the experimental setup, and results are
presented in Section 5. We discuss related work in
Section 6. Finally, we point to open research issues
for this new approach in Section 7.
2 A Framework for Multi-Task AL
In this section we introduce a sample selection
framework that aims at reducing the human anno-
tation effort in a multiple annotation scenario.
2.1 Task Definition
To measure the efficiency of selection methods, we
define the training quality TQ of annotated mate-
rial S as the performance p yielded with a reference
learner X trained on that material: TQ(X, S) = p.
A selection method can be considered better than an-
other one if a higher TQ is yielded with the same
amount of examples being annotated.
Our framework is an extension of the Active
Learning (AL) framework (Cohn et al, 1996)). The
original AL framework is based on querying in an it-
erative manner those examples to be manually anno-
tated that are most useful for the learner at hand. The
TQ of an annotated corpus selected by means of AL
is much higher than random selection. This AL ap-
proach can be considered as single-task AL because
it focuses on a single learner for which the examples
are to be selected. In a multiple annotation scenario,
however, there are several annotation tasks to be ac-
complished at once and for each task typically a sep-
arate statistical model will then be trained. Thus, the
goal of multi-task AL is to query those examples for
862
human annotation that are most informative for all
learners involved.
2.2 One-Sided Selection vs. Multi-Task AL
The naive approach to select examples in a multiple
annotation scenario would be to perform a single-
task AL selection, i.e., the examples to be annotated
are selected with respect to one of the learners only.1
In a multiple annotation scenario we call such an ap-
proach one-sided selection. It is an intrinsic selec-
tion for the reference learner, and an extrinsic selec-
tion for all the other learners also trained on the an-
notated material. Obviously, a corpus compiled with
the help of one-sided selection will have a good TQ
for that learner for which the intrinsic selection has
taken place. For all the other learners, however, we
have no guarantee that their TQ will not be inferior
than the TQ of a random selection process.
In scenarios where the different annotation tasks
are highly dissimilar we can expect extrinsic selec-
tion to be rather poor. This intuition is demonstrated
by experiments we conducted for named entity (NE)
and parse annotation tasks2 (Figure 1). In this sce-
nario, extrinsic selection for the NE annotation task
means that examples where selected with respect
to the parsing task. Extrinsic selection performed
about the same as random selection for the NE task,
while for the parsing task extrinsic selection per-
formed markedly worse. This shows that examples
that were very informative for the NE learner were
not that informative for the parse learner.
2.3 Protocols for Multi-Task AL
Obviously, we can expect one-sided selection to per-
form better for the reference learner (the one for
which an intrinsic selection took place) than multi-
task AL selection, because the latter would be a
compromise for all learners involved in the multi-
ple annotation scenario. However, the goal of multi-
task AL is to minimize the annotation effort over all
annotation tasks and not just the effort for a single
annotation task.
For a multi-task AL protocol to be valuable in a
specific multiple annotation scenario, the TQ for all
considered learners should be
1Of course, all selected examples would be annotated w.r.t.
all annotation tasks.
2See Section 4 for our experimental setup.
1. better than the TQ of random selection,
2. and better than the TQ of any extrinsic selec-
tion.
In the following, we introduce two protocols for
multi-task AL. Multi-task AL protocols can be con-
sidered meta-protocols because they basically spec-
ify how task-specific, single-task AL approaches can
be combined into one selection decision. By this,
the protocols are independent of the underlying task-
specific AL approaches.
2.3.1 Alternating Selection
The alternating selection protocol alternates one-
sided AL selection. In sj consecutive AL iterations,
the selection is performed as one-sided selection
with respect to learning algorithm Xj . After that,
another learning algorithm is considered for selec-
tion for sk consecutive iterations and so on. Depend-
ing on the specific scenario, this enables to weight
the different annotation tasks by allowing them to
guide the selection in more or less AL iterations.
This protocol is a straight-forward compromise be-
tween the different single-task selection approaches.
In this paper we experiment with the special case
of si = 1, where in every AL iteration the selection
leadership is changed. More sophisticated calibra-
tion of the parameters si is beyond the scope of this
paper and will be dealt with in future work.
2.3.2 Rank Combination
The rank combination protocol is more directly
based on the idea to combine single-task AL selec-
tion decisions. In each AL iteration, the usefulness
score sXj (e) of each unlabeled example e from the
pool of examples is calculated with respect to each
learner Xj and then translated into a rank rXj (e)
where higher usefulness means lower rank number
(examples with identical scores get the same rank
number). Then, for each example, we sum the rank
numbers of each annotation task to get the overall
rank r(e) = ?nj=1 rXj (e). All examples are sorted
by this combined rank and b examples with lowest
rank numbers are selected for manual annotation.3
3As the number of ranks might differ between the single an-
notation tasks, we normalize them to the coarsest scale. Then
we can sum up the ranks as explained above.
863
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
random selection
extrinsic selection (PARSE?AL)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
random selection
extrinsic selection (NE?AL)
Figure 1: Learning curves for random and extrinsic selection on both tasks: named entity annotation (left) and syntactic
parse annotation (right), using the WSJ corpus scenario
This protocol favors examples which are good for
all learning algorithms. Examples that are highly in-
formative for one task but rather uninformative for
another task will not be selected.
3 Evaluation of Multi-Task AL
The notion of training quality (TQ) can be used to
quantify the effectiveness of a protocol, and by this,
annotation costs in a single-task AL scenario. To ac-
tually quantify the overall training quality in a multi-
ple annotation scenario one would have to sum over
all the single task?s TQs. Of course, depending on
the specific annotation task, one would not want to
quantify the number of examples being annotated
but different task-specific units of annotation. While
for entity annotations one does typically count the
number of tokens being annotated, in the parsing
scenario the number of constituents being annotated
is a generally accepted measure. As, however, the
actual time needed for the annotation of one exam-
ple usually differs for different annotation tasks, nor-
malizing exchange rates have to be specified which
can then be used as weighting factors. In this paper,
we do not define such weighting factors4, and leave
this challenging question to be discussed in the con-
text of psycholinguistic research.
We could quantify the overall efficiency score E
of a MTAL protocol P by
E(P ) =
n
?
j=1
?j ? TQ(Xj , uj)
where uj denotes the individual annotation task?s
4Such weighting factors not only depend on the annotation
level or task but also on the domain, and especially on the cog-
nitive load of the annotation task.
number of units being annotated (e.g., constituents
for parsing) and the task-specific weights are defined
by ?j . Given weights are properly defined, such a
score can be applied to directly compare different
protocols and quantify their differences.
In practice, such task-specific weights might also
be considered in the MTAL protocols. In the alter-
nating selection protocol, the numbers of consecu-
tive iterations si each single task protocol can be
tuned according to the ? parameters. As for the
rank combination protocol, the weights can be con-
sidered when calculating the overall rank: r(e) =
?n
j=1 ?j ? rXj (e) where the parameters ?1 . . . ?n re-
flect the values of ?1 . . . ?n (though they need not
necessarily be the same).
In our experiments, we assumed the same weight
for all annotation schemata, thus simply setting si =
1, ?i = 1. This was done for the sake of a clear
framework presentation. Finding proper weights for
the single tasks and tuning the protocols accordingly
is a subject for further research.
4 Experiments
4.1 Scenario and Task-Specific Selection
Protocols
The tasks in our scenario comprise one semantic
task (annotation with named entities (NE)) and one
syntactic task (annotation with PCFG parse trees).
The tasks are highly dissimilar, thus increasing the
potential value of MTAL. Both tasks are subject to
intensive research by the NLP community.
The MTAL protocols proposed are meta-
protocols that combine the selection decisions of
the underlying, task-specific AL protocols. In
our scenario, the task-specific AL protocols are
864
committee-based (Freund et al, 1997) selection
protocols. In committee-based AL, a committee
consists of k classifiers of the same type trained
on different subsets of the training data.5 Each
committee member then makes its predictions on
the unlabeled examples, and those examples on
which the committee members disagree most are
considered most informative for learning and are
thus selected for manual annotation. In our scenario
the example grain-size is the sentence level.
For the NE task, we apply the AL approach of
Tomanek et al (2007). The committee consists of
k1 = 3 classifiers and the vote entropy (VE) (Engel-
son and Dagan, 1996) is employed as disagreement
metric. It is calculated on the token-level as
V Etok(t) = ?
1
log k
c
?
i=0
V (li, t)
k log
V (li, t)
k (1)
where V (li,t)k is the ratio of k classifiers where the
label li is assigned to a token t. The sentence level
vote entropy V Esent is then the average over all to-
kens tj of sentence s.
For the parsing task, the disagreement score is
based on a committee of k2 = 10 instances of Dan
Bikel?s reimplementation of Collins? parser (Bickel,
2005; Collins, 1999). For each sentence in the un-
labeled pool, the agreement between the committee
members was calculated using the function reported
by Reichart and Rappoport (2007):
AF (s) = 1N
?
i,l?[1...N ],i6=l
fscore(mi, ml) (2)
Where mi and ml are the committee members and
N = k2?(k2?1)2 is the number of pairs of different
committee members. This function calculates the
agreement between the members of each pair by cal-
culating their relative f-score and then averages the
pairs? scores. The disagreement of the committee on
a sentence is simply 1 ? AF (s).
4.2 Experimental settings
For the NE task we employed the classifier described
by Tomanek et al (2007): The NE tagger is based on
Conditional Random Fields (Lafferty et al, 2001)
5We randomly sampled L = 34 of the training data to create
each committee member.
and has a rich feature set including orthographical,
lexical, morphological, POS, and contextual fea-
tures. For parsing, Dan Bikel?s reimplementation of
Collins? parser is employed, using gold POS tags.
In each AL iteration we select 100 sentences for
manual annotation.6 We start with a randomly cho-
sen seed set of 200 sentences. Within a corpus we
used the same seed set in all selection scenarios. We
compare the following five selection scenarios: Ran-
dom selection (RS), which serves as our baseline;
one-sided AL selection for both tasks (called NE-AL
and PARSE-AL); and multi-task AL selection with
the alternating selection protocol (alter-MTAL) and
the rank combination protocol (ranks-MTAL).
We performed our experiments on three dif-
ferent corpora, namely one from the newspaper
genre (WSJ), a mixed-genre corpus (Brown), and a
biomedical corpus (Bio). Our simulation corpora
contain both entity annotations and (constituent)
parse annotations. For each corpus we have a pool
set (from which we select the examples for annota-
tion) and an evaluation set (used for generating the
learning curves). The WSJ corpus is based on the
WSJ part of the PENN TREEBANK (Marcus et al,
1993); we used the first 10,000 sentences of section
2-21 as the pool set, and section 00 as evaluation set
(1,921 sentences). The Brown corpus is also based
on the respective part of the PENN TREEBANK. We
created a sample consisting of 8 of any 10 consec-
utive sentences in the corpus. This was done as
Brown contains text from various English text gen-
res, and we did that to create a representative sample
of the corpus domains. We finally selected the first
10,000 sentences from this sample as pool set. Every
9th from every 10 consecutive sentences package
went into the evaluation set which consists of 2,424
sentences. For both WSJ and Brown only parse an-
notations though no entity annotations were avail-
able. Thus, we enriched both corpora with entity
annotations (three entities: person, location, and or-
ganization) by means of a tagger trained on the En-
glish data set of the CoNLL-2003 shared task (Tjong
Kim Sang and De Meulder, 2003).7 The Bio corpus
6Manual annotation is simulated by just unveiling the anno-
tations already contained in our corpora.
7We employed a tagger similar to the one presented by Set-
tles (2004). Our tagger has a performance of ? 84% f-score on
the CoNLL-2003 data; inspection of the predicted entities on
865
is based on the parsed section of the GENIA corpus
(Ohta et al, 2002). We performed the same divi-
sions as for Brown, resulting in 2,213 sentences in
our pool set and 276 sentences for the evaluation set.
This part of the GENIA corpus comes with entity an-
notations. We have collapsed the entity classes an-
notated in GENIA (cell line, cell type, DNA, RNA,
protein) into a single, biological entity class.
5 Results
In this section we present and discuss our results
when applying the five selection strategies (RS, NE-
AL, PARSE-AL, alter-MTAL, and ranks-MTAL) to
our scenario on the three corpora. We refrain from
calculating the overall efficiency score (Section 3)
here due to the lack of generally accepted weights
for the considered annotation tasks. However, we
require from a good selection protocol to exceed the
performance of random selection and extrinsic se-
lection. In addition, recall from Section 3 that we
set the alternate selection and rank combination pa-
rameters to si = 1, ?i = 1, respectively to reflect a
tradeoff between the annotation efforts of both tasks.
Figures 2 and 3 depict the learning curves for
the NE tagger and the parser on WSJ and Brown,
respectively. Each figure shows the five selection
strategies. As expected, on both corpora and both
tasks intrinsic selection performs best, i.e., for the
NE tagger NE-AL and for the parser PARSE-AL.
Further, random selection and extrinsic selection
perform worst. Most importantly, both MTAL pro-
tocols clearly outperform extrinsic and random se-
lection in all our experiments. This is in contrast
to NE-AL which performs worse than random se-
lection for all corpora when used as extrinsic selec-
tion, and for PARSE-AL that outperforms the ran-
dom baseline only for Brown when used as extrin-
sic selection. That is, the MTAL protocols suggest a
tradeoff between the annotation efforts of the differ-
ent tasks, here.
On WSJ, both for the NE and the parse annotation
tasks, the performance of the MTAL protocols is
very similar, though ranks-MTAL performs slightly
better. For the parser task, up to 30,000 constituents
MTAL performs almost as good as does PARSE-
AL. This is different for the NE task where NE-AL
WSJ and Brown revealed a good tagging performance.
clearly outperforms MTAL. On Brown, in general
we see the same results, with some minor differ-
ences. On the NE task, extrinsic selection (PARSE-
AL) performs better than random selection, but it is
still much worse than intrinsic AL or MTAL. Here,
ranks-MTAL significantly outperforms alter-MTAL
and almost performs as good as intrinsic selection.
For the parser task, we see that extrinsic and ran-
dom selection are equally bad. Both MTAL proto-
cols perform equally well, again being quite similar
to the intrinsic selection. On the BIO corpus8 we ob-
served the same tendencies as in the other two cor-
pora, i.e., MTAL clearly outperforms extrinsic and
random selection and supplies a better tradeoff be-
tween annotation efforts of the task at hand than one-
sided selection.
Overall, we can say that in all scenarios MTAL
performs much better than random selection and ex-
trinsic selection, and in most cases the performance
of MTAL (especially but not exclusively, ranks-
MTAL) is even close to intrinsic selection. This is
promising evidence that MTAL selection can be a
better choice than one-sided selection in multiple an-
notation scenarios. Thus, considering all annotation
tasks in the selection process (even if the selection
protocol is as simple as the alternating selection pro-
tocol) is better than selecting only with respect to
one task. Further, it should be noted that overall the
more sophisticated rank combination protocol does
not perform much better than the simpler alternating
selection protocol in all scenarios.
Finally, Figure 4 shows the disagreement curves
for the two tasks on the WSJ corpus. As has already
been discussed by Tomanek and Hahn (2008), dis-
agreement curves can be used as a stopping crite-
rion and to monitor the progress of AL-driven an-
notation. This is especially valuable when no anno-
tated validation set is available (which is needed for
plotting learning curves). We can see that the dis-
agreement curves significantly flatten approximately
at the same time as the learning curves do. In the
context of MTAL, disagreement curves might not
only be interesting as a stopping criterion but rather
as a switching criterion, i.e., to identify when MTAL
could be turned into one-sided selection. This would
be the case if in an MTAL scenario, the disagree-
8The plots for the Bio are omitted due to space restrictions.
866
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
0.
85
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000
0.
55
0.
60
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 2: Learning curves for NE task on WSJ (left) and Brown (right)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000 35000
0.
65
0.
70
0.
75
0.
80
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 3: Learning curves for parse task on WSJ (left) and Brown (right)
ment curve of one task has a slope of (close to) zero.
Future work will focus on issues related to this.
6 Related Work
There is a large body of work on single-task AL ap-
proaches for many NLP tasks where the focus is
mainly on better, task-specific selection protocols
and methods to quantify the usefulness score in dif-
ferent scenarios. As to the tasks involved in our
scenario, several papers address AL for NER (Shen
et al, 2004; Hachey et al, 2005; Tomanek et al,
2007) and syntactic parsing (Tang et al, 2001; Hwa,
2004; Baldridge and Osborne, 2004; Becker and Os-
borne, 2005). Further, there is some work on ques-
tions arising when AL is to be used in real-life anno-
tation scenarios, including impaired inter-annotator
agreement, stopping criteria for AL-driven annota-
tion, and issues of reusability (Baldridge and Os-
borne, 2004; Hachey et al, 2005; Zhu and Hovy,
2007; Tomanek et al, 2007).
Multi-task AL is methodologically related to ap-
proaches of decision combination, especially in the
context of classifier combination (Ho et al, 1994)
and ensemble methods (Breiman, 1996). Those ap-
proaches focus on the combination of classifiers in
order to improve the classification error rate for one
specific classification task. In contrast, the focus of
multi-task AL is on strategies to select training ma-
terial for multi classifier systems where all classifiers
cover different classification tasks.
7 Discussion
Our treatment of MTAL within the context of the
orthogonal two-task scenario leads to further inter-
esting research questions. First, future investiga-
tions will have to focus on the question whether
the positive results observed in our orthogonal (i.e.,
highly dissimilar) two-task scenario will also hold
for a more realistic (and maybe more complex) mul-
tiple annotation scenario where tasks are more sim-
ilar and more than two annotation tasks might be
involved. Furthermore, several forms of interde-
pendencies may arise between the single annotation
tasks. As a first example, consider the (functional)
interdependencies (i.e., task similarity) in higher-
level semantic NLP tasks of relation or event recog-
nition. In such a scenario, several tasks including
entity annotations and relation/event annotations, as
well as syntactic parse data, have to be incorporated
at the same time. Another type of (data flow) inter-
867
10000 20000 30000 40000
0.
01
0
0.
01
4
0.
01
8
tokens
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
10000 20000 30000 40000
5
10
15
20
25
30
35
40
constituents
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 4: Disagreement curves for NE task (left) and parse task (right) on WSJ
dependency occurs in a second scenario where ma-
terial for several classifiers that are data-dependent
on each other ? one takes the output of another clas-
sifier as input features ? has to be efficiently anno-
tated. Whether the proposed protocols are beneficial
in the context of such highly interdependent tasks is
an open issue. Even more challenging is the idea
to provide methodologies helping to predict in an
arbitrary application scenario whether the choice of
MTAL is truly advantageous.
Another open question is how to measure and
quantify the overall annotation costs in multiple an-
notation scenarios. Exchange rates are inherently
tied to the specific task and domain. In practice, one
might just want to measure the time needed for the
annotations. However, in a simulation scenario, a
common metric is necessary to compare the perfor-
mance of different selection strategies with respect
to the overall annotation costs. This requires stud-
ies on how to quantify, with a comparable cost func-
tion, the efforts needed for the annotation of a textual
unit of choice (e.g., tokens, sentences) with respect
to different annotation tasks.
Finally, the question of reusability of the anno-
tated material is an important issue. Reusability in
the context of AL means to which degree corpora
assembled with the help of any AL technique can be
(re)used as a general resource, i.e., whether they are
well suited for the training of classifiers other than
the ones used during the selection process.This is
especially interesting as the details of the classifiers
that should be trained in a later stage are typically
not known at the resource building time. Thus, we
want to select samples valuable to a family of clas-
sifiers using the various annotation layers. This, of
course, is only possible if data annotated with the
help of AL is reusable by modified though similar
classifiers (e.g., with respect to the features being
used) ? compared to the classifiers employed for the
selection procedure.
The issue of reusability has already been raised
but not yet conclusively answered in the context of
single-task AL (see Section 6). Evidence was found
that reusability up to a certain, though not well-
specified, level is possible. Of course, reusability
has to be analyzed separately in the context of var-
ious MTAL scenarios. We feel that these scenarios
might both be more challenging and more relevant
to the reusability issue than the single-task AL sce-
nario, since resources annotated with multiple lay-
ers can be used to the design of a larger number of a
(possibly more complex) learning algorithms.
8 Conclusions
We proposed an extension to the single-task AL ap-
proach such that it can be used to select examples for
annotation with respect to several annotation tasks.
To the best of our knowledge this is the first paper on
this issue, with a focus on NLP tasks. We outlined
a problem definition and described a framework for
multi-task AL. We presented and tested two proto-
cols for multi-task AL. Our results are promising as
they give evidence that in a multiple annotation sce-
nario, multi-task AL outperforms naive one-sided
and random selection.
Acknowledgments
The work of the second author was funded by the
German Ministry of Education and Research within
the STEMNET project (01DS001A-C), while the
work of the third author was funded by the EC
within the BOOTSTREP project (FP6-028099).
868
References
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
EMNLP?04, pages 9?16.
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proceedings of IJCAI?05, pages 991?996.
Daniel M. Bickel. 2005. Code developed at the Univer-
sity of Pennsylvania, http://www.cis.upenn.
edu/
?
dbikel/software.html.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL?96, pages 319?326.
Yoav Freund, Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133?168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of CoNLL?05, pages
144?151.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 16(1):66?75.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of ICML?01, pages 282?289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind K. Joshi. 2008. Sense annotation in the penn
discourse treebank. In Proceedings of CICLing?08,
pages 275?286.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL?00,
pages 117?125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA corpus: An annotated research abstract
corpus in molecular biology domain. In Proceedings
of HLT?02, pages 82?86.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Roi Reichart and Ari Rappoport. 2007. An ensemble
method for selection of high quality parses. In Pro-
ceedings of ACL?07, pages 408?415, June.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of JNLPBA?04, pages 107?110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceedings
of ACL?04, pages 589?596.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
Proceedings of ACL?02, pages 120?127.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL?03, pages 142?147.
Katrin Tomanek and Udo Hahn. 2008. Approximating
learning curves for active-learning-driven annotation.
In Proceedings of LREC?08.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains corpus reusabil-
ity of annotated data. In Proceedings of EMNLP-
CoNLL?07, pages 486?495.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
EMNLP-CoNLL?07, pages 783?790.
869
Proceedings of ACL-08: HLT, pages 1030?1038,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extraction of Entailed Semantic Relations Through
Syntax-based Comma Resolution
Vivek Srikumar 1 Roi Reichart2 Mark Sammons1 Ari Rappoport2 Dan Roth1
1University of Illinois at Urbana-Champaign
{vsrikum2|mssammon|danr}@uiuc.edu
2Institute of Computer Science, Hebrew University of Jerusalem
{roiri|arir}@cs.huji.ac.il
Abstract
This paper studies textual inference by inves-
tigating comma structures, which are highly
frequent elements whose major role in the ex-
traction of semantic relations has not been
hitherto recognized. We introduce the prob-
lem of comma resolution, defined as under-
standing the role of commas and extracting the
relations they imply. We show the importance
of the problem using examples from Tex-
tual Entailment tasks, and present A Sentence
Transformation Rule Learner (ASTRL), a ma-
chine learning algorithm that uses a syntac-
tic analysis of the sentence to learn sentence
transformation rules that can then be used to
extract relations. We have manually annotated
a corpus identifying comma structures and re-
lations they entail and experimented with both
gold standard parses and parses created by a
leading statistical parser, obtaining F-scores of
80.2% and 70.4% respectively.
1 Introduction
Recognizing relations expressed in text sentences is
a major topic in NLP, fundamental in applications
such as Textual Entailment (or Inference), Question
Answering and Text Mining. In this paper we ad-
dress this issue from a novel perspective, that of un-
derstanding the role of the commas in a sentence,
which we argue is a key component in sentence
comprehension. Consider for example the following
three sentences:
1. Authorities have arrested John Smith, a retired
police officer.
2. Authorities have arrested John Smith, his friend
and his brother.
3. Authorities have arrested John Smith, a retired
police officer announced this morning.
Sentence (1) states that John Smith is a retired
police officer. The comma and surrounding sen-
tence structure represent the relation ?IsA?. In (2),
the comma and surrounding structure signifies a list,
so the sentence states that three people were ar-
rested: (i) John Smith, (ii) his friend, and (iii) his
brother. In (3), a retired police officer announced
that John Smith has been arrested. Here, the comma
and surrounding sentence structure indicate clause
boundaries.
In all three sentences, the comma and the sur-
rounding sentence structure signify relations essen-
tial to comprehending the meaning of the sentence,
in a way that is not easily captured using lexical-
or even shallow parse-level information. As a hu-
man reader, we understand them easily, but auto-
mated systems for Information Retrieval, Question
Answering, and Textual Entailment are likely to en-
counter problems when comparing structures like
these, which are lexically similar, but whose mean-
ings are so different.
In this paper we present an algorithm for comma
resolution, a task that we define to consist of (1) dis-
ambiguating comma type and (2) determining the
relations entailed from the sentence given the com-
mas? interpretation. Specifically, in (1) we assign
each comma to one of five possible types, and in
(2) we generate a set of natural language sentences
that express the relations, if any, signified by each
comma structure. The algorithm uses information
extracted from parse trees. This work, in addition to
having immediate significance for natural language
processing systems that use semantic content, has
potential applications in improving a range of auto-
1030
mated analysis by decomposing complex sentences
into a set of simpler sentences that capture the same
meaning. Although there are many other widely-
used structures that express relations in a similar
way, commas are one of the most commonly used
symbols1. By addressing comma resolution, we of-
fer a promising first step toward resolving relations
in sentences.
To evaluate the algorithm, we have developed an-
notation guidelines, and manually annotated sen-
tences from the WSJ PennTreebank corpus. We
present a range of experiments showing the good
performance of the system, using gold-standard and
parser-generated parse trees.
In Section 2 we motivate comma resolution
through Textual Entailment examples. Section 3 de-
scribes related work. Sections 4 and 5 present our
corpus annotation and learning algorithm. Results
are given in Section 6.
2 Motivating Comma Resolution Through
Textual Entailment
Comma resolution involves not only comma dis-
ambiguation but also inference of the arguments
(and argument boundaries) of the relationship repre-
sented by the comma structure, and the relationships
holding between these arguments and the sentence
as a whole. To our knowledge, this is the first pa-
per that deals with this problem, so in this section
we motivate it in depth by showing its importance
to the semantic inference task of Textual Entailment
(TE) (Dagan et al, 2006), which is increasingly rec-
ognized as a crucial direction for improving a range
of NLP tasks such as information extraction, ques-
tion answering and summarization.
TE is the task of deciding whether the meaning
of a text T (usually a short snippet) can be inferred
from the meaning of another text S. If this is the
case, we say that S entails T . For example2, we say
that sentence (1) entails sentence (2):
1. S: Parviz Davudi was representing Iran at a
meeting of the Shanghai Co-operation Orga-
nization (SCO), the fledgling association that
1For example, the WSJ corpus has 49K sentences, among
which 32K with one comma or more, 17K with two or more,
and 7K with three or more.
2The examples of this section are variations of pairs taken
from the Pascal RTE3 (Dagan et al, 2006) dataset.
binds two former Soviet republics of central
Asia, Russia and China to fight terrorism.
2. T: SCO is the fledgling association that binds
several countries.
To see that (1) entails (2), one must understand
that the first comma structure in sentence (1) is an
apposition structure, and does not indicate the begin-
ning of a list. The second comma marks a boundary
between entities in a list. To make the correct infer-
ence one must determine that the second comma is a
list separator, not an apposition marker. Misclassify-
ing the second comma in (1) as an apposition leads
to the conclusion that (1) entails (3):
3. T: Russia and China are two former Soviet re-
publics of central Asia .
Note that even to an educated native speaker of
English, sentence 1 may be initially confusing; dur-
ing the first reading, one might interpret the first
comma as indicating a list, and that ?the Shanghai
Co-operation Organization? and ?the fledgling asso-
ciation that binds...? are two separate entities that are
meeting, rather than two representations of the same
entity.
From these examples we draw the following con-
clusions: 1. Comma resolution is essential in com-
prehending natural language text. 2. Explicitly rep-
resenting relations derived from comma structures
can assist a wide range of NLP tasks; this can be
done by directly augmenting the lexical-level rep-
resentation, e.g., by bringing surface forms of two
text fragments with the same meaning closer to-
gether. 3. Comma structures might be highly am-
biguous, nested and overlapping, and consequently
their interpretation is a difficult task. The argument
boundaries of the corresponding extracted relations
are also not easy to detect.
The output of our system could be used to aug-
ment sentences with an explicit representation of en-
tailed relations that hold in them. In Textual Entail-
ment systems this can increase the likelihood of cor-
rect identification of entailed sentences, and in other
NLP systems it can help understanding the shallow
lexical/syntactic content of a sentence. A similar ap-
proach has been taken in (Bar-Haim et al, 2007; de
Salvo Braz et al, 2005), which augment the source
sentence with entailed relations.
1031
3 Related Work
Since we focus on extracting the relations repre-
sented by commas, there are two main strands of
research with similar goals: 1) systems that directly
analyze commas, whether labeling them with syn-
tactic information or correcting inappropriate use in
text; and 2) systems that extract relations from text,
typically by trying to identify paraphrases.
The significance of interpreting the role of com-
mas in sentences has already been identified by (van
Delden and Gomez, 2002; Bayraktar et al, 1998)
and others. A review of the first line of research is
given in (Say and Akman, 1997).
In (Bayraktar et al, 1998) the WSJ PennTreebank
corpus (Marcus et al, 1993) is analyzed and a very
detailed list of syntactic patterns that correspond to
different roles of commas is created. However, they
do not study the extraction of entailed relations as
a function of the comma?s interpretation. Further-
more, the syntactic patterns they identify are unlexi-
calized and would not support the level of semantic
relations that we show in this paper. Finally, theirs
is a manual process completely dependent on syn-
tactic patterns. While our comma resolution system
uses syntactic parse information as its main source
of features, the approach we have developed focuses
on the entailed relations, and does not limit imple-
mentations to using only syntactic information.
The most directly comparable prior work is that
of (van Delden and Gomez, 2002), who use fi-
nite state automata and a greedy algorithm to learn
comma syntactic roles. However, their approach dif-
fers from ours in a number of critical ways. First,
their comma annotation scheme does not identify
arguments of predicates, and therefore cannot be
used to extract complete relations. Second, for each
comma type they identify, a new Finite State Au-
tomaton must be hand-encoded; the learning com-
ponent of their work simply constrains which FSAs
that accept a given, comma containing, text span
may co-occur. Third, their corpus is preprocessed by
hand to identify specialized phrase types needed by
their FSAs; once our system has been trained, it can
be applied directly to raw text. Fourth, they exclude
from their analysis and evaluation any comma they
deem to have been incorrectly used in the source
text. We include all commas that are present in the
text in our annotation and evaluation.
There is a large body of NLP literature on punctu-
ation. Most of it, however, is concerned with aiding
syntactic analysis of sentences and with developing
comma checkers, much based on (Nunberg, 1990).
Pattern-based relation extraction methods (e.g.,
(Davidov and Rappoport, 2008; Davidov et al,
2007; Banko et al, 2007; Pasca et al, 2006; Sekine,
2006)) could in theory be used to extract relations
represented by commas. However, the types of
patterns used in web-scale lexical approaches cur-
rently constrain discovered patterns to relatively
short spans of text, so will most likely fail on
structures whose arguments cover large spans (for
example, appositional clauses containing relative
clauses). Relation extraction approaches such as
(Roth and Yih, 2004; Roth and Yih, 2007; Hirano
et al, 2007; Culotta and Sorenson, 2004; Zelenko et
al., 2003) focus on relations between Named Enti-
ties; such approaches miss the more general apposi-
tion and list relations we recognize in this work, as
the arguments in these relations are not confined to
Named Entities.
Paraphrase Acquisition work such as that by (Lin
and Pantel, 2001; Pantel and Pennacchiotti, 2006;
Szpektor et al, 2004) is not constrained to named
entities, and by using dependency trees, avoids the
locality problems of lexical methods. However,
these approaches have so far achieved limited accu-
racy, and are therefore hard to use to augment exist-
ing NLP systems.
4 Corpus Annotation
For our corpus, we selected 1,000 sentences con-
taining at least one comma from the Penn Treebank
(Marcus et al, 1993) WSJ section 00, and manu-
ally annotated them with comma information3. This
annotated corpus served as both training and test
datasets (using cross-validation).
By studying a number of sentences from WSJ (not
among the 1,000 selected), we identified four signif-
icant types of relations expressed through commas:
SUBSTITUTE, ATTRIBUTE, LOCATION, and LIST.
Each of these types can in principle be expressed us-
ing more than a single comma. We define the notion
3The guidelines and annotations are available at http://
L2R.cs.uiuc.edu/
?
cogcomp/data.php.
1032
of a comma structure as a set of one or more commas
that all relate to the same relation in the sentence.
SUBSTITUTE indicates an IS-A relation. An ex-
ample is ?John Smith, a Renaissance artist, was fa-
mous?. By removing the relation expressed by the
commas, we can derive three sentences: ?John Smith
is a Renaissance artist?, ?John Smith was famous?,
and ?a Renaissance artist was famous?. Note that in
theory, the third relation will not be valid: one exam-
ple is ?The brothers, all honest men, testified at the
trial?, which does not entail ?all honest men testified
at the trial?. However, we encountered no examples
of this kind in the corpus, and leave this refinement
to future work.
ATTRIBUTE indicates a relation where one argu-
ment describes an attribute of the other. For ex-
ample, from ?John, who loved chocolate, ate with
gusto?, we can derive ?John loved chocolate? and
?John ate with gusto?.
LOCATION indicates a LOCATED-IN relation. For
example, from ?Chicago, Illinois saw some heavy
snow today? we can derive ?Chicago is located in
Illinois? and ?Chicago saw some heavy snow today?.
LIST indicates that some predicate or property
is applied to multiple entities. In our annotation,
the list does not generate explicit relations; instead,
the boundaries of the units comprising the list are
marked so that they can be treated as a single unit,
and are considered to be related by the single rela-
tion ?GROUP?. For example, the derivation of ?John,
James and Kelly all left last week? is written as
?[John, James, and Kelly] [all left last week]?.
Any commas not fitting one of the descriptions
above are designated as OTHER. This does not in-
dicate that the comma signifies no relations, only
that it does not signify a relation of interest in this
work (future work will address relations currently
subsumed by this category). Analysis of 120 OTHER
commas show that approximately half signify clause
boundaries, which may occur when sentence con-
stituents are reordered for emphasis, but may also
encode implicit temporal, conditional, and other re-
lation types (for example, ?Opening the drawer, he
found the gun.?). The remainder comprises mainly
coordination structures (for example, ?Although he
won, he was sad?) and discourse markers indicating
inter-sentence relations (such as ?However, he soon
cheered up.?). While we plan to develop an anno-
Rel. Type Avg. Agreement # of Commas # of Rel.s
SUBSTITUTE 0.808 243 729
ATTRIBUTE 0.687 193 386
LOCATION 0.929 71 140
LIST 0.803 230 230
OTHER 0.949 909 0
Combined 0.869 1646 1485
Table 1: Average inter-annotator agreement for identify-
ing relations.
tation scheme for such relations, this is beyond the
scope of the present work.
Four annotators annotated the same 10% of the
WSJ sentences in order to evaluate inter-annotator
agreement. The remaining sentences were divided
among the four annotators. The resulting corpus was
checked by two judges and the annotation corrected
where appropriate; if the two judges disagreed, a
third judge was consulted and consensus reached.
Our annotators were asked to identify comma struc-
tures, and for each structure to write its relation type,
its arguments, and all possible simplified version(s)
of the original sentence in which the relation implied
by the comma has been removed. Arguments must
be contiguous units of the sentence and will be re-
ferred to as chunks hereafter. Agreement statistics
and the number of commas and relations of each
type are shown in Table 4. The Accuracy closely ap-
proximates Kappa score in this case, since the base-
line probability of chance agreement is close to zero.
5 A Sentence Tranformation Rule Learner
(ASTRL)
In this section, we describe a new machine learning
system that learns Sentence Transformation Rules
(STRs) for comma resolution. We first define the
hypothesis space (i.e., STRs) and two operations ?
substitution and introduction. We then define the
feature space, motivating the use of Syntactic Parse
annotation to learn STRs. Finally, we describe the
ASTRL algorithm.
5.1 Sentence Transformation Rules
A Sentence Transformation Rule (STR) takes a
parse tree as input and generates new sentences. We
formalize an STR as the pair l ? r, where l is a
tree fragment that can consist of non-terminals, POS
tags and lexical items. r is a set {ri}, each ele-
ment of which is a template that consists of the non-
1033
terminals of l and, possibly, some new tokens. This
template is used to generate a new sentence, called a
relation.
The process of applying an STR l ? r to a parse
tree T of a sentence s begins with finding a match for
l in T . A match is said to be found if l is a subtree
of T . If matched, the non-terminals of each ri are
instantiated with the terminals that they cover in T .
Instantiation is followed by generation of the output
relations in one of two ways: introduction or sub-
stitution, which is specified by the corresponding ri.
If an ri is marked as an introductory one, then the
relation is the terminal sequence obtained by replac-
ing the non-terminals in ri with their instantiations.
For substitution, firstly, the non-terminals of the ri
are replaced by their instantiations. The instantiated
ri replaces all the terminals in s that are covered by
the l-match. The notions of introduction and substi-
tution were motivated by ideas introduced in (Bar-
Haim et al, 2007).
Figure 1 shows an example of an STR and Figure
2 shows the application of this STR to a sentence. In
the first relation, NP1 and NP2 are instantiated with
the corresponding terminals in the parse tree. In the
second and third relations, the terminals of NP1 and
NP2 replace the terminals covered by NPp.
LHS: NPp
NP1 , NP2 ,
RHS:
1. NP1 be NP2 (introduction)
2. NP1 (substitution)
3. NP2 (substitution)
Figure 1: Example of a Sentence Transformation Rule. If
the LHS matches a part of a given parse tree, then the
RHS will generate three relations.
5.2 The Feature Space
In Section 2, we discussed the example where there
could be an ambiguity between a list and an apposi-
tion structure in the fragment two former Soviet re-
publics, Russia and China. In addition, simple sur-
face examination of the sentence could also identify
the noun phrases ?Shanghai Co-operation Organi-
zation (SCO)?, ?the fledgling association that binds
S
NPp
NP1
John Smith
, NP2
a renaissance
artist
,
V P
was
famous
RELATIONS:
1 [John Smith]/NP1 be [a renaissance artist]/NP2
2 [John Smith] /NP1 [was famous]
3 [a renaissance artist]/NP2 [was famous]
Figure 2: Example of application of the STR in Figure 1.
In the first relation, an introduction, we use the verb ?be?,
without dealing with its inflections. NP1 and NP2 are
both substitutions, each replacing NPp to generate the
last two relations.
two former Soviet Republics?, ?Russia? and ?China?
as the four members of a list. To resolve such ambi-
guities, we need a nested representation of the sen-
tence. This motivates the use of syntactic parse trees
as a logical choice of feature space. (Note, however,
that semantic and pragmatic ambiguities might still
remain.)
5.3 Algorithm Overview
In our corpus annotation, the relations and their ar-
gument boundaries (chunks) are explicitly marked.
For each training example, our learning algorithm
first finds the smallest valid STR ? the STR with the
smallest LHS in terms of depth. Then it refines the
LHS by specializing it using statistics taken from
the entire data set.
5.4 Generating the Smallest Valid STR
To transform an example into the smallest valid
STR, we utilize the augmented parse tree of the
sentence. For each chunk in the sentence, we find
the lowest node in the parse tree that covers the
chunk and does not cover other chunks (even par-
tially). It may, however, cover words that do not
belong to any chunk. We refer to such a node as
a chunk root. We then find the lowest node that cov-
ers all the chunk roots, referring to it as the pat-
tern root. The initial LHS consists of the sub-
tree of the parse tree rooted at the pattern root and
whose leaf nodes are all either chunk roots or nodes
that do not belong to any chunk. All the nodes are
labeled with the corresponding labels in the aug-
1034
mented parse tree. For example, if we consider the
parse tree and relations shown in Figure 2, then do-
ing the above procedure gives us the initial LHS
as S (NPp(NP1, NP2, ) V P ). The three relations
gives us the RHS with three elements ?NP1 be
NP2?, ?NP1 V P ? and ?NP1 V P ?, all three being
introduction.
This initial LHS need not be the smallest one that
explains the example. So, we proceed by finding the
lowest node in the initial LHS such that the sub-
tree of the LHS at that node can form a new STR
that covers the example using both introduction and
substitution. In our example, the initial LHS has a
subtree, NPp(NP1, NP2, ) that can cover all the re-
lations with the RHS consisting of ?NP1 be NP2?,
NP1 and NP2. The first RHS is an introduction,
while the second and the third are both substitutions.
Since no subtree of this LHS can generate all three
relations even with substitution, this is the required
STR. The final step ensures that we have the small-
est valid STR at this stage.
5.5 Statistical Refinement
The STR generated using the procedure outlined
above explains the relations generated by a single
example. In addition to covering the relations gen-
erated by the example, we wish to ensure that it does
not cover erroneous relations by matching any of the
other comma types in the annotated data.
Algorithm 1 ASTRL: A Sentence Transformation
Rule Learning.
1: for all t: Comma type do
2: Initialize STRList[t] = ?
3: p = Set of annotated examples of type t
4: n = Annotated examples of all other types
5: for all x ? p do
6: r = Smallest Valid STR that covers x
7: Get fringe of r.LHS using the parse tree
8: S = Score(r,p,n)
9: Sprev = ??
10: while S 6= Sprev do
11: if adding some fringe node to r.LHS causes a signifi-
cant change in score then
12: Set r = New rule that includes that fringe node
13: Sprev = S
14: S = Score(r,p,n)
15: Recompute new fringe nodes
16: end if
17: end while
18: Add r to STRList[t]
19: Remove all examples from p that are covered by r
20: end for
21: end for
For this purpose, we specialize the LHS so that it
covers as few examples from the other comma types
as possible, while covering as many examples from
the current comma type as possible. Given the most
general STR, we generate a set of additional, more
detailed, candidate rules. Each of these is obtained
from the original rule by adding a single node to
the tree pattern in the rule?s LHS, and updating the
rule?s RHS accordingly. We then score each of the
candidates (including the original rule). If there is
a clear winner, we continue with it using the same
procedure (i.e., specialize it). If there isn?t a clear
winner, we stop and use the current winner. After
finishing with a rule (line 18), we remove from the
set of positive examples of its comma type all exam-
ples that are covered by it (line 19).
To generate the additional candidate rules that we
add, we define the fringe of a rule as the siblings
and children of the nodes in its LHS in the original
parse tree. Each fringe node defines an additional
candidate rule, whose LHS is obtained by adding
the fringe node to the rule?s LHS tree. We refer to
the set of these candidate rules, plus the original one,
as the rule?s fringe rules. We define the score of an
STR as
Score(Rule,p,n) = Rp|p| ?
Rn
|n|
where p and n are the set of positive and negative
examples for this comma type, and Rp and Rn are
the number of positive and negative examples that
are covered by the STR. For each example, all exam-
ples annotated with the same comma type are pos-
itive while all examples of all other comma types
are negative. The score is used to select the win-
ner among the fringe rules. The complete algorithm
we have used is listed in Algorithm 1. For conve-
nience, the algorithm?s main loop is given in terms
of comma types, although this is not strictly nec-
essary. The stopping criterion in line 11 checks
whether any fringe rule has a significantly better
score than the rule it was derived from, and exits the
specialization loop if there is none.
Since we start with the smallest STR, we only
need to add nodes to it to refine it and never have
to delete any nodes from the tree. Also note that the
algorithm is essentially a greedy algorithm that per-
forms a single pass over the examples; other, more
1035
complex, search strategies could also be used.
6 Evaluation
6.1 Experimental Setup
To evaluate ASTRL, we used the WSJ derived cor-
pus. We experimented with three scenarios; in two
of them we trained using the gold standard trees
and then tested on gold standard parse trees (Gold-
Gold), and text annotated using a state-of-the-art sta-
tistical parser (Charniak and Johnson, 2005) (Gold-
Charniak), respectively. In the third, we trained and
tested on the Charniak Parser (Charniak-Charniak).
In gold standard parse trees the syntactic cate-
gories are annotated with functional tags. Since cur-
rent statistical parsers do not annotate sentences with
such tags, we augment the syntactic trees with the
output of a Named Entity tagger. For the Named
Entity information, we used a publicly available NE
Recognizer capable of recognizing a range of cat-
egories including Person, Location and Organiza-
tion. On the CoNLL-03 shared task, its f-score is
about 90%4. We evaluate our system from different
points of view, as described below. For all the eval-
uation methods, we performed five-fold cross vali-
dation and report the average precision, recall and
f-scores.
6.2 Relation Extraction Performance
Firstly, we present the evaluation of the performance
of ASTRL from the point of view of relation ex-
traction. After learning the STRs for the different
comma types using the gold standard parses, we
generated relations by applying the STRs on the test
set once. Table 2 shows the precision, recall and
f-score of the relations, without accounting for the
comma type of the STR that was used to generate
them. This metric, called the Relation metric in fur-
ther discussion, is the most relevant one from the
point of view of the TE task. Since a list does not
generate any relations in our annotation scheme, we
use the commas to identify the list elements. Treat-
ing each list in a sentence as a single relation, we
score the list with the fraction of its correctly identi-
fied elements.
In addition to the Gold-Gold and Gold-Charniak
4A web demo of the NER is at http://L2R.cs.uiuc.
edu/
?
cogcomp/demos.php.
settings described above, for this metric, we also
present the results of the Charniak-Charniak setting,
where both the train and test sets were annotated
with the output of the Charniak parser. The improve-
ment in recall in this setting over the Gold-Charniak
case indicates that the parser makes systematic er-
rors with respect to the phenomena considered.
Setting P R F
Gold-Gold 86.1 75.4 80.2
Gold-Charniak 77.3 60.1 68.1
Charniak-Charniak 77.2 64.8 70.4
Table 2: ASTRL performance (precision, recall and f-
score) for relation extraction. The comma types were
used only to learn the rules. During evaluation, only the
relations were scored.
6.3 Comma Resolution Performance
We present a detailed analysis of the performance of
the algorithm for comma resolution. Since this paper
is the first one that deals with the task, we could not
compare our results to previous work. Also, there
is no clear baseline to use. We tried a variant of
the most frequent baseline common in other disam-
biguation tasks, in which we labeled all commas as
OTHER (the most frequent type) except when there
are list indicators like and, or and but in adjacent
chunks (which are obtained using a shallow parser),
in which case the commas are labeled LIST. This
gives an average precision 0.85 and an average recall
of 0.36 for identifying the comma type. However,
this baseline does not help in identifying relations.
We use the following approach to evaluate the
comma type resolution and relation extraction per-
formance ? a relation extracted by the system is con-
sidered correct only if both the relation and the type
of the comma structure that generated it are correctly
identified. We call this metric the Relation-Type
metric. Another way of measuring the performance
of comma resolution is to measure the correctness of
the relations per comma type. In both cases, lists are
scored as in the Relation metric. The performance of
our system with respect to these two metrics are pre-
sented in Table 3. In this table, we also compare the
performance of the STRs learned by ASTRL with
the smallest valid STRs without further specializa-
tion (i.e., using just the procedure outlined in Sec-
tion 5.4).
1036
Type Gold-Gold Setting Gold-Charniak Setting
Relation-Type metric
Smallest Valid STRs ASTRL Smallest Valid STRs ASTRL
P R F P R F P R F P R F
Total 66.2 76.1 70.7 81.8 73.9 77.6 61.0 58.4 59.5 72.2 59.5 65.1
Relations Metric, Per Comma Type
ATTRIBUTE 40.4 68.2 50.4 70.6 59.4 64.1 35.5 39.7 36.2 56.6 37.7 44.9
SUBSTITUTE 80.0 84.3 81.9 87.9 84.8 86.1 75.8 72.9 74.3 78.0 76.1 76.9
LIST 70.9 58.1 63.5 76.2 57.8 65.5 58.7 53.4 55.6 65.2 53.3 58.5
LOCATION 93.8 86.4 89.1 93.8 86.4 89.1 70.3 37.2 47.2 70.3 37.2 47.2
Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types and
generating relations.
There is an important difference between the Re-
lation metric (Table 2) and the Relation-type met-
ric (top part of Table 3) that depends on the seman-
tic interpretation of the comma types. For example,
consider the sentence ?John Smith, 59, went home.?
If the system labels the commas in this as both AT-
TRIBUTE and SUBSTITUTE, then, both will gener-
ate the relation ?John Smith is 59.? According to
the Relation metric, there is no difference between
them. However, there is a semantic difference be-
tween the two sentences ? the ATTRIBUTE relation
says that being 59 is an attribute of John Smith while
the SUBSTITUTE relation says that John Smith is the
number 59. This difference is accounted for by the
Relation-Type metric.
From this standpoint, we can see that the special-
ization step performed in the full ASTRL algorithm
greatly helps in disambiguating between the AT-
TRIBUTE and SUBSTITUTE types and consequently,
the Relation-Type metric shows an error reduction
of 23.5% and 13.8% in the Gold-Gold and Gold-
Charniak settings respectively. In the Gold-Gold
scenario the performance of ASTRL is much better
than in the Gold-Charniak scenario. This reflects the
non-perfect performance of the parser in annotating
these sentences (parser F-score of 90%).
Another key evaluation question is the per-
formance of the method in identification of the
OTHER category. A comma is judged to be as
OTHER if no STR in the system applies to it.
The performance of ASTRL in this aspect is pre-
sented in Table 4. The categorization of this cate-
gory is important if we wish to further classify the
OTHER commas into finer categories.
Setting P R F
Gold-Gold 78.9 92.8 85.2
Gold-Charniak 72.5 92.2 81.2
Table 4: ASTRL performance (precision, recall and f-
score) for OTHER identification.
7 Conclusions
We defined the task of comma resolution, and devel-
oped a novel machine learning algorithm that learns
Sentence Transformation Rules to perform this task.
We experimented with both gold standard and parser
annotated sentences, and established a performance
level that seems good for a task of this complexity,
and which will provide a useful measure of future
systems developed for this task. When given au-
tomatically parsed sentences, performance degrades
but is still much higher than random, in both sce-
narios. We designed a comma annotation scheme,
where each comma unit is assigned one of four types
and an inference rule mapping the patterns of the
unit with the entailed relations. We created anno-
tated datasets which will be made available over the
web to facilitate further research.
Future work will investigate four main directions:
(i) studying the effects of inclusion of our approach
on the performance of Textual Entailment systems;
(ii) using features other than those derivable from
syntactic parse and named entity annotation of the
input sentence; (iii) recognizing a wider range of im-
plicit relations, represented by commas and in other
ways; (iv) adaptation to other domains.
Acknowledgement
The UIUC authors were supported by NSF grant
ITR IIS-0428472, DARPA funding under the Boot-
strap Learning Program and a grant from Boeing.
1037
References
M. Banko, M. Cafarella, M. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proc. of IJCAI, pages 2670?2676.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In Proc. of AAAI, pages 871?876.
M. Bayraktar, B. Say, and V. Akman. 1998. An analysis
of english punctuation: The special case of comma.
International Journal of Corpus Linguistics, 3(1):33?
57.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
of the Annual Meeting of the ACL, pages 173?180.
A. Culotta and J. Sorenson. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of the Annual
Meeting of the ACL, pages 423?429.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In Proc. of the Annual Meeting of the
ACL.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the Annual Meeting
of the ACL, pages 232?239.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailment in natural language. In Proc. of AAAI,
pages 1678?1679.
T. Hirano, Y. Matsuo, and G. Kikui. 2007. Detecting
semantic relations between named entities in text using
contextual features. In Proc. of the Annual Meeting of
the ACL, pages 157?160.
D. Lin and P. Pantel. 2001. DIRT: discovery of inference
rules from text. In Proc. of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining 2001,
pages 323?328.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G. Nunberg. 1990. CSLI Lecture Notes 18: The Lin-
guistics of Punctuation. CSLI Publications, Stanford,
CA.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proc. of the Annual Meeting of the
ACL, pages 113?120.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proc. of the Annual Meeting of
the ACL, pages 809?816.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
B. Say and V. Akman. 1997. Current approaches to
punctuation in computational linguistics. Computers
and the Humanities, 30(6):457?469.
S. Sekine. 2006. On-demand information extraction. In
Proc. of the Annual Meeting of the ACL, pages 731?
738.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based of entailment relations. In Proc. of
EMNLP, pages 49?56.
S. van Delden and F. Gomez. 2002. Combining finite
state automata and a greedy learning algorithm to de-
termine the syntactic roles of commas. In Proc. of IC-
TAI, pages 293?300.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106.
1038
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 28?36,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Argument Identification for Semantic Role Labeling
Omri Abend1 Roi Reichart2 Ari Rappoport1
1Institute of Computer Science , 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
The task of Semantic Role Labeling
(SRL) is often divided into two sub-tasks:
verb argument identification, and argu-
ment classification. Current SRL algo-
rithms show lower results on the identifi-
cation sub-task. Moreover, most SRL al-
gorithms are supervised, relying on large
amounts of manually created data. In
this paper we present an unsupervised al-
gorithm for identifying verb arguments,
where the only type of annotation required
is POS tagging. The algorithm makes use
of a fully unsupervised syntactic parser,
using its output in order to detect clauses
and gather candidate argument colloca-
tion statistics. We evaluate our algorithm
on PropBank10, achieving a precision of
56%, as opposed to 47% of a strong base-
line. We also obtain an 8% increase in
precision for a Spanish corpus. This is
the first paper that tackles unsupervised
verb argument identification without using
manually encoded rules or extensive lexi-
cal or syntactic resources.
1 Introduction
Semantic Role Labeling (SRL) is a major NLP
task, providing a shallow sentence-level semantic
analysis. SRL aims at identifying the relations be-
tween the predicates (usually, verbs) in the sen-
tence and their associated arguments.
The SRL task is often viewed as consisting of
two parts: argument identification (ARGID) and ar-
gument classification. The former aims at identi-
fying the arguments of a given predicate present
in the sentence, while the latter determines the
type of relation that holds between the identi-
fied arguments and their corresponding predicates.
The division into two sub-tasks is justified by
the fact that they are best addressed using differ-
ent feature sets (Pradhan et al, 2005). Perfor-
mance in the ARGID stage is a serious bottleneck
for general SRL performance, since only about
81% of the arguments are identified, while about
95% of the identified arguments are labeled cor-
rectly (Ma`rquez et al, 2008).
SRL is a complex task, which is reflected by the
algorithms used to address it. A standard SRL al-
gorithm requires thousands to dozens of thousands
sentences annotated with POS tags, syntactic an-
notation and SRL annotation. Current algorithms
show impressive results but only for languages and
domains where plenty of annotated data is avail-
able, e.g., English newspaper texts (see Section 2).
Results are markedly lower when testing is on a
domain wider than the training one, even in En-
glish (see the WSJ-Brown results in (Pradhan et
al., 2008)).
Only a small number of works that do not re-
quire manually labeled SRL training data have
been done (Swier and Stevenson, 2004; Swier and
Stevenson, 2005; Grenager and Manning, 2006).
These papers have replaced this data with the
VerbNet (Kipper et al, 2000) lexical resource or
a set of manually written rules and supervised
parsers.
A potential answer to the SRL training data bot-
tleneck are unsupervised SRL models that require
little to no manual effort for their training. Their
output can be used either by itself, or as training
material for modern supervised SRL algorithms.
In this paper we present an algorithm for unsu-
pervised argument identification. The only type of
annotation required by our algorithm is POS tag-
28
ging, which needs relatively little manual effort.
The algorithm consists of two stages. As pre-
processing, we use a fully unsupervised parser to
parse each sentence. Initially, the set of possi-
ble arguments for a given verb consists of all the
constituents in the parse tree that do not contain
that predicate. The first stage of the algorithm
attempts to detect the minimal clause in the sen-
tence that contains the predicate in question. Us-
ing this information, it further reduces the possible
arguments only to those contained in the minimal
clause, and further prunes them according to their
position in the parse tree. In the second stage we
use pointwise mutual information to estimate the
collocation strength between the arguments and
the predicate, and use it to filter out instances of
weakly collocating predicate argument pairs.
We use two measures to evaluate the perfor-
mance of our algorithm, precision and F-score.
Precision reflects the algorithm?s applicability for
creating training data to be used by supervised
SRL models, while the standard SRL F-score mea-
sures the model?s performance when used by it-
self. The first stage of our algorithm is shown to
outperform a strong baseline both in terms of F-
score and of precision. The second stage is shown
to increase precision while maintaining a reason-
able recall.
We evaluated our model on sections 2-21 of
Propbank. As is customary in unsupervised pars-
ing work (e.g. (Seginer, 2007)), we bounded sen-
tence length by 10 (excluding punctuation). Our
first stage obtained a precision of 52.8%, which is
more than 6% improvement over the baseline. Our
second stage improved precision to nearly 56%, a
9.3% improvement over the baseline. In addition,
we carried out experiments on Spanish (on sen-
tences of length bounded by 15, excluding punctu-
ation), achieving an increase of over 7.5% in pre-
cision over the baseline. Our algorithm increases
F?score as well, showing an 1.8% improvement
over the baseline in English and a 2.2% improve-
ment in Spanish.
Section 2 reviews related work. In Section 3 we
detail our algorithm. Sections 4 and 5 describe the
experimental setup and results.
2 Related Work
The advance of machine learning based ap-
proaches in this field owes to the usage of large
scale annotated corpora. English is the most stud-
ied language, using the FrameNet (FN) (Baker et
al., 1998) and PropBank (PB) (Palmer et al, 2005)
resources. PB is a corpus well suited for evalu-
ation, since it annotates every non-auxiliary verb
in a real corpus (the WSJ sections of the Penn
Treebank). PB is a standard corpus for SRL eval-
uation and was used in the CoNLL SRL shared
tasks of 2004 (Carreras and Ma`rquez, 2004) and
2005 (Carreras and Ma`rquez, 2005).
Most work on SRL has been supervised, requir-
ing dozens of thousands of SRL annotated train-
ing sentences. In addition, most models assume
that a syntactic representation of the sentence is
given, commonly in the form of a parse tree, a de-
pendency structure or a shallow parse. Obtaining
these is quite costly in terms of required human
annotation.
The first work to tackle SRL as an indepen-
dent task is (Gildea and Jurafsky, 2002), which
presented a supervised model trained and evalu-
ated on FrameNet. The CoNLL shared tasks of
2004 and 2005 were devoted to SRL, and stud-
ied the influence of different syntactic annotations
and domain changes on SRL results. Computa-
tional Linguistics has recently published a special
issue on the task (Ma`rquez et al, 2008), which
presents state-of-the-art results and surveys the lat-
est achievements and challenges in the field.
Most approaches to the task use a multi-level
approach, separating the task to an ARGID and an
argument classification sub-tasks. They then use
the unlabeled argument structure (without the se-
mantic roles) as training data for the ARGID stage
and the entire data (perhaps with other features)
for the classification stage. Better performance
is achieved on the classification, where state-
of-the-art supervised approaches achieve about
81% F-score on the in-domain identification task,
of which about 95% are later labeled correctly
(Ma`rquez et al, 2008).
There have been several exceptions to the stan-
dard architecture described in the last paragraph.
One suggestion poses the problem of SRL as a se-
quential tagging of words, training an SVM clas-
sifier to determine for each word whether it is in-
side, outside or in the beginning of an argument
(Hacioglu and Ward, 2003). Other works have in-
tegrated argument classification and identification
into one step (Collobert and Weston, 2007), while
others went further and combined the former two
along with parsing into a single model (Musillo
29
and Merlo, 2006).
Work on less supervised methods has been
scarce. Swier and Stevenson (2004) and Swier
and Stevenson (2005) presented the first model
that does not use an SRL annotated corpus. How-
ever, they utilize the extensive verb lexicon Verb-
Net, which lists the possible argument structures
allowable for each verb, and supervised syntac-
tic tools. Using VerbNet alng with the output of
a rule-based chunker (in 2004) and a supervised
syntactic parser (in 2005), they spot instances in
the corpus that are very similar to the syntactic
patterns listed in VerbNet. They then use these as
seed for a bootstrapping algorithm, which conse-
quently identifies the verb arguments in the corpus
and assigns their semantic roles.
Another less supervised work is that
of (Grenager and Manning, 2006), which presents
a Bayesian network model for the argument
structure of a sentence. They use EM to learn
the model?s parameters from unannotated data,
and use this model to tag a test corpus. However,
ARGID was not the task of that work, which dealt
solely with argument classification. ARGID was
performed by manually-created rules, requiring a
supervised or manual syntactic annotation of the
corpus to be annotated.
The three works above are relevant but incom-
parable to our work, due to the extensive amount
of supervision (namely, VerbNet and a rule-based
or supervised syntactic system) they used, both in
detecting the syntactic structure and in detecting
the arguments.
Work has been carried out in a few other lan-
guages besides English. Chinese has been studied
in (Xue, 2008). Experiments on Catalan and Span-
ish were done in SemEval 2007 (Ma`rquez et al,
2007) with two participating systems. Attempts
to compile corpora for German (Burdchardt et al,
2006) and Arabic (Diab et al, 2008) are also un-
derway. The small number of languages for which
extensive SRL annotated data exists reflects the
considerable human effort required for such en-
deavors.
Some SRL works have tried to use unannotated
data to improve the performance of a base su-
pervised model. Methods used include bootstrap-
ping approaches (Gildea and Jurafsky, 2002; Kate
and Mooney, 2007), where large unannotated cor-
pora were tagged with SRL annotation, later to
be used to retrain the SRL model. Another ap-
proach used similarity measures either between
verbs (Gordon and Swanson, 2007) or between
nouns (Gildea and Jurafsky, 2002) to overcome
lexical sparsity. These measures were estimated
using statistics gathered from corpora augmenting
the model?s training data, and were then utilized
to generalize across similar verbs or similar argu-
ments.
Attempts to substitute full constituency pars-
ing by other sources of syntactic information have
been carried out in the SRL community. Sugges-
tions include posing SRL as a sequence labeling
problem (Ma`rquez et al, 2005) or as an edge tag-
ging problem in a dependency representation (Ha-
cioglu, 2004). Punyakanok et al (2008) provide
a detailed comparison between the impact of us-
ing shallow vs. full constituency syntactic infor-
mation in an English SRL system. Their results
clearly demonstrate the advantage of using full an-
notation.
The identification of arguments has also been
carried out in the context of automatic subcatego-
rization frame acquisition. Notable examples in-
clude (Manning, 1993; Briscoe and Carroll, 1997;
Korhonen, 2002) who all used statistical hypothe-
sis testing to filter a parser?s output for arguments,
with the goal of compiling verb subcategorization
lexicons. However, these works differ from ours
as they attempt to characterize the behavior of a
verb type, by collecting statistics from various in-
stances of that verb, and not to determine which
are the arguments of specific verb instances.
The algorithm presented in this paper performs
unsupervised clause detection as an intermedi-
ate step towards argument identification. Super-
vised clause detection was also tackled as a sepa-
rate task, notably in the CoNLL 2001 shared task
(Tjong Kim Sang and De`jean, 2001). Clause in-
formation has been applied to accelerating a syn-
tactic parser (Glaysher and Moldovan, 2006).
3 Algorithm
In this section we describe our algorithm. It con-
sists of two stages, each of which reduces the set
of argument candidates, which a-priori contains all
consecutive sequences of words that do not con-
tain the predicate in question.
3.1 Algorithm overview
As pre-processing, we use an unsupervised parser
that generates an unlabeled parse tree for each sen-
30
tence (Seginer, 2007). This parser is unique in that
it is able to induce a bracketing (unlabeled pars-
ing) from raw text (without even using POS tags)
achieving state-of-the-art results. Since our algo-
rithm uses millions to tens of millions sentences,
we must use very fast tools. The parser?s high
speed (thousands of words per second) enables us
to process these large amounts of data.
The only type of supervised annotation we
use is POS tagging. We use the taggers MX-
POST (Ratnaparkhi, 1996) for English and Tree-
Tagger (Schmid, 1994) for Spanish, to obtain POS
tags for our model.
The first stage of our algorithm uses linguisti-
cally motivated considerations to reduce the set of
possible arguments. It does so by confining the set
of argument candidates only to those constituents
which obey the following two restrictions. First,
they should be contained in the minimal clause
containing the predicate. Second, they should be
k-th degree cousins of the predicate in the parse
tree. We propose a novel algorithm for clause de-
tection and use its output to determine which of
the constituents obey these two restrictions.
The second stage of the algorithm uses point-
wise mutual information to rule out constituents
that appear to be weakly collocating with the pred-
icate in question. Since a predicate greatly re-
stricts the type of arguments with which it may
appear (this is often referred to as ?selectional re-
strictions?), we expect it to have certain character-
istic arguments with which it is likely to collocate.
3.2 Clause detection stage
The main idea behind this stage is the observation
that most of the arguments of a predicate are con-
tained within the minimal clause that contains the
predicate. We tested this on our development data
? section 24 of the WSJ PTB, where we saw that
86% of the arguments that are also constituents
(in the gold standard parse) were indeed contained
in that minimal clause (as defined by the tree la-
bel types in the gold standard parse that denote
a clause, e.g., S, SBAR). Since we are not pro-
vided with clause annotation (or any label), we at-
tempted to detect them in an unsupervised manner.
Our algorithm attempts to find sub-trees within the
parse tree, whose structure resembles the structure
of a full sentence. This approximates the notion of
a clause.
L
L
DT
The
NNS
materials
L
L
IN
in
L
DT
each
NN
set
L
VBP
reach
L
L
IN
about
CD
90
NNS
students
L
L L
L L
VBP L
L
VBP L
Figure 1: An example of an unlabeled POS tagged
parse tree. The middle tree is the ST of ?reach?
with the root as the encoded ancestor. The bot-
tom one is the ST with its parent as the encoded
ancestor.
Statistics gathering. In order to detect which
of the verb?s ancestors is the minimal clause, we
score each of the ancestors and select the one that
maximizes the score. We represent each ancestor
using its Spinal Tree (ST ). The ST of a given
verb?s ancestor is obtained by replacing all the
constituents that do not contain the verb by a leaf
having a label. This effectively encodes all the k-
th degree cousins of the verb (for every k). The
leaf labels are either the word?s POS in case the
constituent is a leaf, or the generic label ?L? de-
noting a non-leaf. See Figure 1 for an example.
In this stage we collect statistics of the occur-
rences of ST s in a large corpus. For every ST in
the corpus, we count the number of times it oc-
curs in a form we consider to be a clause (positive
examples), and the number of times it appears in
other forms (negative examples).
Positive examples are divided into two main
types. First, when the ST encodes the root an-
cestor (as in the middle tree of Figure 1); second,
when the ancestor complies to a clause lexico-
syntactic pattern. In many languages there is a
small set of lexico-syntactic patterns that mark a
clause, e.g. the English ?that?, the German ?dass?
and the Spanish ?que?. The patterns which were
used in our experiments are shown in Figure 2.
For each verb instance, we traverse over its an-
31
English
TO + VB. The constituent starts with ?to? followed by
a verb in infinitive form.
WP. The constituent is preceded by a Wh-pronoun.
That. The constituent is preceded by a ?that? marked
by an ?IN? POS tag indicating that it is a subordinating
conjunction.
Spanish
CQUE. The constituent is preceded by a word with the
POS ?CQUE? which denotes the word ?que? as a con-
junction.
INT. The constituent is preceded by a word with the
POS ?INT? which denotes an interrogative pronoun.
CSUB. The constituent is preceded by a word with one
of the POSs ?CSUBF?, ?CSUBI? or ?CSUBX?, which
denote a subordinating conjunction.
Figure 2: The set of lexico-syntactic patterns that
mark clauses which were used by our model.
cestors from top to bottom. For each of them we
update the following counters: sentence(ST ) for
the root ancestor?s ST , patterni(ST ) for the ones
complying to the i-th lexico-syntactic pattern and
negative(ST ) for the other ancestors1.
Clause detection. At test time, when detecting
the minimal clause of a verb instance, we use
the statistics collected in the previous stage. De-
note the ancestors of the verb with A1 . . . Am.
For each of them, we calculate clause(STAj )
and total(STAj ). clause(STAj ) is the sum
of sentence(STAj ) and patterni(STAj ) if this
ancestor complies to the i-th pattern (if there
is no such pattern, clause(STAj ) is equal to
sentence(STAj )). total(STAj ) is the sum of
clause(STAj ) and negative(STAj ).
The selected ancestor is given by:
(1) Amax = argmaxAj
clause(STAj )
total(STAj )
An ST whose total(ST ) is less than a small
threshold2 is not considered a candidate to be the
minimal clause, since its statistics may be un-
reliable. In case of a tie, we choose the low-
est constituent that obtained the maximal score.
1If while traversing the tree, we encounter an ancestor
whose first word is preceded by a coordinating conjunction
(marked by the POS tag ?CC?), we refrain from performing
any additional counter updates. Structures containing coor-
dinating conjunctions tend not to obey our lexico-syntactic
rules.
2We used 4 per million sentences, derived from develop-
ment data.
If there is only one verb in the sentence3 or if
clause(STAj ) = 0 for every 1 ? j ? m, we
choose the top level constituent by default to be
the minimal clause containing the verb. Other-
wise, the minimal clause is defined to be the yield
of the selected ancestor.
Argument identification. For each predicate in
the corpus, its argument candidates are now de-
fined to be the constituents contained in the min-
imal clause containing the predicate. However,
these constituents may be (and are) nested within
each other, violating a major restriction on SRL
arguments. Hence we now prune our set, by keep-
ing only the siblings of all of the verb?s ancestors,
as is common in supervised SRL (Xue and Palmer,
2004).
3.3 Using collocations
We use the following observation to filter out some
superfluous argument candidates: since the argu-
ments of a predicate many times bear a semantic
connection with that predicate, they consequently
tend to collocate with it.
We collect collocation statistics from a large
corpus, which we annotate with parse trees and
POS tags. We mark arguments using the argu-
ment detection algorithm described in the previous
two sections, and extract all (predicate, argument)
pairs appearing in the corpus. Recall that for each
sentence, the arguments are a subset of the con-
stituents in the parse tree.
We use two representations of an argument: one
is the POS tag sequence of the terminals contained
in the argument, the other is its head word4. The
predicate is represented as the conjunction of its
lemma with its POS tag.
Denote the number of times a predicate x
appeared with an argument y by nxy. Denote
the total number of (predicate, argument) pairs
by N . Using these notations, we define the
following quantities: nx = ?ynxy, ny = ?xnxy,
p(x) = nxN , p(y) =
ny
N and p(x, y) =
nxy
N . The
pointwise mutual information of x and y is then
given by:
3In this case, every argument in the sentence must be re-
lated to that verb.
4Since we do not have syntactic labels, we use an approx-
imate notion. For English we use the Bikel parser default
head word rules (Bikel, 2004). For Spanish, we use the left-
most word.
32
(2) PMI(x, y) = log p(x,y)p(x)?p(y) = log
nxy
(nx?ny)/N
PMI effectively measures the ratio between
the number of times x and y appeared together and
the number of times they were expected to appear,
had they been independent.
At test time, when an (x, y) pair is observed, we
check if PMI(x, y), computed on the large cor-
pus, is lower than a threshold ? for either of x?s
representations. If this holds, for at least one rep-
resentation, we prune all instances of that (x, y)
pair. The parameter ? may be selected differently
for each of the argument representations.
In order to avoid using unreliable statistics,
we apply this for a given pair only if nx?nyN >
r, for some parameter r. That is, we consider
PMI(x, y) to be reliable, only if the denomina-
tor in equation (2) is sufficiently large.
4 Experimental Setup
Corpora. We used the PropBank corpus for de-
velopment and for evaluation on English. Section
24 was used for the development of our model,
and sections 2 to 21 were used as our test data.
The free parameters of the collocation extraction
phase were tuned on the development data. Fol-
lowing the unsupervised parsing literature, multi-
ple brackets and brackets covering a single word
are omitted. We exclude punctuation according
to the scheme of (Klein, 2005). As is customary
in unsupervised parsing (e.g. (Seginer, 2007)), we
bounded the lengths of the sentences in the cor-
pus to be at most 10 (excluding punctuation). This
results in 207 sentences in the development data,
containing a total of 132 different verbs and 173
verb instances (of the non-auxiliary verbs in the
SRL task, see ?evaluation? below) having 403 ar-
guments. The test data has 6007 sentences con-
taining 1008 different verbs and 5130 verb in-
stances (as above) having 12436 arguments.
Our algorithm requires large amounts of data
to gather argument structure and collocation pat-
terns. For the statistics gathering phase of the
clause detection algorithm, we used 4.5M sen-
tences of the NANC (Graff, 1995) corpus, bound-
ing their length in the same manner. In order
to extract collocations, we used 2M sentences
from the British National Corpus (Burnard, 2000)
and about 29M sentences from the Dmoz cor-
pus (Gabrilovich and Markovitch, 2005). Dmoz
is a web corpus obtained by crawling and clean-
ing the URLs in the Open Directory Project
(dmoz.org). All of the above corpora were parsed
using Seginer?s parser and POS-tagged by MX-
POST (Ratnaparkhi, 1996).
For our experiments on Spanish, we used 3.3M
sentences of length at most 15 (excluding punctua-
tion) extracted from the Spanish Wikipedia. Here
we chose to bound the length by 15 due to the
smaller size of the available test corpus. The
same data was used both for the first and the sec-
ond stages. Our development and test data were
taken from the training data released for the Se-
mEval 2007 task on semantic annotation of Span-
ish (Ma`rquez et al, 2007). This data consisted
of 1048 sentences of length up to 15, from which
200 were randomly selected as our development
data and 848 as our test data. The development
data included 313 verb instances while the test
data included 1279. All corpora were parsed us-
ing the Seginer parser and tagged by the ?Tree-
Tagger? (Schmid, 1994).
Baselines. Since this is the first paper, to our
knowledge, which addresses the problem of unsu-
pervised argument identification, we do not have
any previous results to compare to. We instead
compare to a baseline which marks all k-th degree
cousins of the predicate (for every k) as arguments
(this is the second pruning we use in the clause
detection stage). We name this baseline the ALL
COUSINS baseline. We note that a random base-
line would score very poorly since any sequence of
terminals which does not contain the predicate is
a possible candidate. Therefore, beating this ran-
dom baseline is trivial.
Evaluation. Evaluation is carried out using
standard SRL evaluation software5. The algorithm
is provided with a list of predicates, whose argu-
ments it needs to annotate. For the task addressed
in this paper, non-consecutive parts of arguments
are treated as full arguments. A match is consid-
ered each time an argument in the gold standard
data matches a marked argument in our model?s
output. An unmatched argument is an argument
which appears in the gold standard data, and fails
to appear in our model?s output, and an exces-
sive argument is an argument which appears in
our model?s output but does not appear in the gold
standard. Precision and recall are defined accord-
ingly. We report an F-score as well (the harmonic
mean of precision and recall). We do not attempt
5http://www.lsi.upc.edu/?srlconll/soft.html#software.
33
to identify multi-word verbs, and therefore do not
report the model?s performance in identifying verb
boundaries.
Since our model detects clauses as an interme-
diate product, we provide a separate evaluation
of this task for the English corpus. We show re-
sults on our development data. We use the stan-
dard parsing F-score evaluation measure. As a
gold standard in this evaluation, we mark for each
of the verbs in our development data the minimal
clause containing it. A minimal clause is the low-
est ancestor of the verb in the parse tree that has
a syntactic label of a clause according to the gold
standard parse of the PTB. A verb is any terminal
marked by one of the POS tags of type verb ac-
cording to the gold standard POS tags of the PTB.
5 Results
Our results are shown in Table 1. The left section
presents results on English and the right section
presents results on Spanish. The top line lists re-
sults of the clause detection stage alone. The next
two lines list results of the full algorithm (clause
detection + collocations) in two different settings
of the collocation stage. The bottom line presents
the performance of the ALL COUSINS baseline.
In the ?Collocation Maximum Precision? set-
ting the parameters of the collocation stage (? and
r) were generally tuned such that maximal preci-
sion is achieved while preserving a minimal recall
level (40% for English, 20% for Spanish on the de-
velopment data). In the ?Collocation Maximum F-
score? the collocation parameters were generally
tuned such that the maximum possible F-score for
the collocation algorithm is achieved.
The best or close to best F-score is achieved
when using the clause detection algorithm alone
(59.14% for English, 23.34% for Spanish). Note
that for both English and Spanish F-score im-
provements are achieved via a precision improve-
ment that is more significant than the recall degra-
dation. F-score maximization would be the aim of
a system that uses the output of our unsupervised
ARGID by itself.
The ?Collocation Maximum Precision?
achieves the best precision level (55.97% for
English, 21.8% for Spanish) but at the expense
of the largest recall loss. Still, it maintains a
reasonable level of recall. The ?Collocation
Maximum F-score? is an example of a model that
provides a precision improvement (over both the
baseline and the clause detection stage) with a
relatively small recall degradation. In the Spanish
experiments its F-score (23.87%) is even a bit
higher than that of the clause detection stage
(23.34%).
The full two?stage algorithm (clause detection
+ collocations) should thus be used when we in-
tend to use the model?s output as training data for
supervised SRL engines or supervised ARGID al-
gorithms.
In our algorithm, the initial set of potential ar-
guments consists of constituents in the Seginer
parser?s parse tree. Consequently the fraction
of arguments that are also constituents (81.87%
for English and 51.83% for Spanish) poses an
upper bound on our algorithm?s recall. Note
that the recall of the ALL COUSINS baseline is
74.27% (45.75%) for English (Spanish). This
score emphasizes the baseline?s strength, and jus-
tifies the restriction that the arguments should be
k-th cousins of the predicate. The difference be-
tween these bounds for the two languages provides
a partial explanation for the corresponding gap in
the algorithm?s performance.
Figure 3 shows the precision of the collocation
model (on development data) as a function of the
amount of data it was given. We can see that
the algorithm reaches saturation at about 5M sen-
tences. It achieves this precision while maintain-
ing a reasonable recall (an average recall of 43.1%
after saturation). The parameters of the colloca-
tion model were separately tuned for each corpus
size, and the graph displays the maximum which
was obtained for each of the corpus sizes.
To better understand our model?s performance,
we performed experiments on the English cor-
pus to test how well its first stage detects clauses.
Clause detection is used by our algorithm as a step
towards argument identification, but it can be of
potential benefit for other purposes as well (see
Section 2). The results are 23.88% recall and 40%
precision. As in the ARGID task, a random se-
lection of arguments would have yielded an ex-
tremely poor result.
6 Conclusion
In this work we presented the first algorithm for ar-
gument identification that uses neither supervised
syntactic annotation nor SRL tagged data. We
have experimented on two languages: English and
Spanish. The straightforward adaptability of un-
34
English (Test Data) Spanish (Test Data)
Precision Recall F1 Precision Recall F1
Clause Detection 52.84 67.14 59.14 18.00 33.19 23.34
Collocation Maximum F?score 54.11 63.53 58.44 20.22 29.13 23.87
Collocation Maximum Precision 55.97 40.02 46.67 21.80 18.47 20.00
ALL COUSINS baseline 46.71 74.27 57.35 14.16 45.75 21.62
Table 1: Precision, Recall and F1 score for the different stages of our algorithm. Results are given for English (PTB, sentences
length bounded by 10, left part of the table) and Spanish (SemEval 2007 Spanish SRL task, right part of the table). The results
of the collocation (second) stage are given in two configurations, Collocation Maximum F-score and Collocation Maximum
Precision (see text). The upper bounds on Recall, obtained by taking all arguments output by our unsupervised parser, are
81.87% for English and 51.83% for Spanish.
0 2 4 6 8 10
42
44
46
48
50
52
Number of Sentences (Millions)
Pr
ec
isi
on
 
 
Second Stage
First Stage
Baseline
Figure 3: The performance of the second stage on English
(squares) vs. corpus size. The precision of the baseline (trian-
gles) and of the first stage (circles) is displayed for reference.
The graph indicates the maximum precision obtained for each
corpus size. The graph reaches saturation at about 5M sen-
tences. The average recall of the sampled points from there
on is 43.1%. Experiments were performed on the English
development data.
supervised models to different languages is one
of their most appealing characteristics. The re-
cent availability of unsupervised syntactic parsers
has offered an opportunity to conduct research on
SRL, without reliance on supervised syntactic an-
notation. This work is the first to address the ap-
plication of unsupervised parses to an SRL related
task.
Our model displayed an increase in precision of
9% in English and 8% in Spanish over a strong
baseline. Precision is of particular interest in this
context, as instances tagged by high quality an-
notation could be later used as training data for
supervised SRL algorithms. In terms of F?score,
our model showed an increase of 1.8% in English
and of 2.2% in Spanish over the baseline.
Although the quality of unsupervised parses is
currently low (compared to that of supervised ap-
proaches), using great amounts of data in identi-
fying recurring structures may reduce noise and
in addition address sparsity. The techniques pre-
sented in this paper are based on this observation,
using around 35M sentences in total for English
and 3.3M sentences for Spanish.
As this is the first work which addressed un-
supervised ARGID, many questions remain to be
explored. Interesting issues to address include as-
sessing the utility of the proposed methods when
supervised parses are given, comparing our model
to systems with no access to unsupervised parses
and conducting evaluation using more relaxed
measures.
Unsupervised methods for syntactic tasks have
matured substantially in the last few years. No-
table examples are (Clark, 2003) for unsupervised
POS tagging and (Smith and Eisner, 2006) for un-
supervised dependency parsing. Adapting our al-
gorithm to use the output of these models, either to
reduce the little supervision our algorithm requires
(POS tagging) or to provide complementary syn-
tactic information, is an interesting challenge for
future work.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. ACL-
COLING ?98.
Daniel M. Bikel, 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Ted Briscoe, John Carroll, 1997. Automatic Extraction
of Subcategorization from Corpora. Applied NLP
1997.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad and Manfred Pinkal, 2006
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. LREC ?06.
Lou Burnard, 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University.
Xavier Carreras and Llu?`s Ma`rquez, 2004. Intro-
duction to the CoNLL?2004 Shared Task: Semantic
Role Labeling. CoNLL ?04.
35
Xavier Carreras and Llu?`s Ma`rquez, 2005. Intro-
duction to the CoNLL?2005 Shared Task: Semantic
Role Labeling. CoNLL ?05.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Ronan Collobert and Jason Weston, 2007. Fast Se-
mantic Extraction Using a Novel Neural Network
Architecture. ACL ?07.
Mona Diab, Aous Mansouri, Martha Palmer, Olga
Babko-Malaya, Wajdi Zaghouani, Ann Bies and
Mohammed Maamouri, 2008. A pilot Arabic Prop-
Bank. LREC ?08.
Evgeniy Gabrilovich and Shaul Markovitch, 2005.
Feature Generation for Text Categorization using
World Knowledge. IJCAI ?05.
Daniel Gildea and Daniel Jurafsky, 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Elliot Glaysher and Dan Moldovan, 2006. Speed-
ing Up Full Syntactic Parsing by Leveraging Partial
Parsing Decisions. COLING/ACL ?06 poster ses-
sion.
Andrew Gordon and Reid Swanson, 2007. Generaliz-
ing Semantic Role Annotations across Syntactically
Similar Verbs. ACL ?07.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Trond Grenager and Christopher D. Manning, 2006.
Unsupervised Discovery of a Statistical Verb Lexi-
con. EMNLP ?06.
Kadri Hacioglu, 2004. Semantic Role Labeling using
Dependency Trees. COLING ?04.
Kadri Hacioglu and Wayne Ward, 2003. Target Word
Detection and Semantic Role Chunking using Sup-
port Vector Machines. HLT-NAACL ?03.
Rohit J. Kate and Raymond J. Mooney, 2007. Semi-
Supervised Learning for Semantic Parsing using
Support Vector Machines. HLT?NAACL ?07.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Dan Klein, 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Anna Korhonen, 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Christopher D. Manning, 1993. Automatic Acquisition
of a Large Subcategorization Dictionary. ACL ?93.
Llu?`s Ma`rquez, Xavier Carreras, Kenneth C. Lit-
tkowski and Suzanne Stevenson, 2008. Semantic
Role Labeling: An introdution to the Special Issue.
Computational Linguistics, 34(2):145?159
Llu?`s Ma`rquez, Jesus Gime`nez Pere Comas and Neus
Catala`, 2005. Semantic Role Labeling as Sequential
Tagging. CoNLL ?05.
Llu?`s Ma`rquez, Lluis Villarejo, M. A. Mart?` and Mar-
iona Taule`, 2007. SemEval?2007 Task 09: Multi-
level Semantic Annotation of Catalan and Spanish.
The 4th international workshop on Semantic Evalu-
ations (SemEval ?07).
Gabriele Musillo and Paula Merlo, 2006. Accurate
Parsing of the proposition bank. HLT-NAACL ?06.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky,
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning, 60(1):11?
39.
Sameer Pradhan, Wayne Ward, James H. Martin, 2008.
Towards Robust Semantic Role Labeling. Computa-
tional Linguistics, 34(2):289?310.
Adwait Ratnaparkhi, 1996. Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?96.
Helmut Schmid, 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees International Confer-
ence on New Methods in Language Processing.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2006. Annealing
Structural Bias in Multilingual Weighted Grammar
Induction. ACL ?06.
Robert S. Swier and Suzanne Stevenson, 2004. Unsu-
pervised Semantic Role Labeling. EMNLP ?04.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. EMNLP ?05.
Erik F. Tjong Kim Sang and Herve? De?jean, 2001. In-
troduction to the CoNLL-2001 Shared Task: Clause
Identification. CoNLL ?01.
Nianwen Xue and Martha Palmer, 2004. Calibrating
Features for Semantic Role Labeling. EMNLP ?04.
Nianwen Xue, 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2):225?255.
36
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 3?11,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sample Selection for Statistical Parsers: Cognitively Driven
Algorithms and Evaluation Measures
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Creating large amounts of manually annotated
training data for statistical parsers imposes
heavy cognitive load on the human annota-
tor and is thus costly and error prone. It
is hence of high importance to decrease the
human efforts involved in creating training
data without harming parser performance. For
constituency parsers, these efforts are tradi-
tionally evaluated using the total number of
constituents (TC) measure, assuming uniform
cost for each annotated item. In this paper, we
introduce novel measures that quantify aspects
of the cognitive efforts of the human annota-
tor that are not reflected by the TC measure,
and show that they are well established in the
psycholinguistic literature. We present a novel
parameter based sample selection approach
for creating good samples in terms of these
measures. We describe methods for global op-
timisation of lexical parameters of the sam-
ple based on a novel optimisation problem, the
constrained multiset multicover problem, and
for cluster-based sampling according to syn-
tactic parameters. Our methods outperform
previously suggested methods in terms of the
new measures, while maintaining similar TC
performance.
1 Introduction
State of the art statistical parsers require large
amounts of manually annotated data to achieve good
performance. Creating such data imposes heavy
cognitive load on the human annotator and is thus
costly and error prone. Statistical parsers are ma-
jor components in NLP applications such as QA
(Kwok et al, 2001), MT (Marcu et al, 2006) and
SRL (Toutanova et al, 2005). These often oper-
ate over the highly variable Web, which consists of
texts written in many languages and genres. Since
the performance of parsers markedly degrades when
training and test data come from different domains
(Lease and Charniak, 2005), large amounts of train-
ing data from each domain are required for using
them effectively. Thus, decreasing the human efforts
involved in creating training data for parsers without
harming their performance is of high importance.
In this paper we address this problem through
sample selection: given a parsing algorithm and a
large pool of unannotated sentences S, select a sub-
set S1 ? S for human annotation such that the hu-
man efforts in annotating S1 are minimized while
the parser performance when trained with this sam-
ple is maximized.
Previous works addressing training sample size
vs. parser performance for constituency parsers
(Section 2) evaluated training sample size using the
total number of constituents (TC). Sentences differ
in length and therefore in annotation efforts, and it
has been argued (see, e.g, (Hwa, 2004)) that TC re-
flects the number of decisions the human annotator
makes when syntactically annotating the sample, as-
suming uniform cost for each decision.
In this paper we posit that important aspects of
the efforts involved in annotating a sample are not
reflected by the TC measure. Since annotators ana-
lyze sentences rather than a bag of constituents, sen-
tence structure has a major impact on their cognitive
efforts. Sizeable psycholinguistic literature points
to the connection between nested structures in the
syntactic structure of a sentence and its annotation
efforts. This has motivated us to introduce (Sec-
tion 3) three sample size measures, the total and av-
3
erage number of nested structures of degree k in the
sample, and the average number of constituents per
sentence in the sample.
Active learning algorithms for sample selection
focus on sentences that are difficult for the parsing
algorithm when trained with the available training
data (Section 2). In Section 5 we show that active
learning samples contain a high number of complex
structures, much higher than their number in a ran-
domly selected sample that achieves the same parser
performance level. To avoid that, we introduce (Sec-
tion 4) a novel parameter based sample selection
(PBS) approach which aims to select a sample that
enables good estimation of the model parameters,
without focusing on difficult sentences. In Section 5
we show that the methods derived from our approach
select substantially fewer complex structures than
active learning methods and the random baseline.
We propose two different methods. In cluster
based sampling (CBS), we aim to select a sample
in which the distribution of the model parameters is
similar to their distribution in the whole unlabelled
pool. To do that we build a vector representation for
each sentence in the unlabelled pool reflecting the
distribution of the model parameters in this sentence,
and use a clustering algorithm to divide these vectors
into clusters. In the second method we use the fact
that a sample containing many examples of a certain
parameter yields better estimation of this parameter.
If this parameter is crucial for model performance
and the selection process does not harm the distri-
bution of other parameters, then the selected sam-
ple is of high quality. To select such a sample we
introduce a reduction between this selection prob-
lem and a variant of the NP-hard multiset-multicover
problem (Hochbaum, 1997). We call this problem
the constrained multiset multicover (CMM) problem,
and present an algorithm to approximate it.
We experiment (Section 5) with the WSJ Pen-
nTreebank (Marcus et al, 1994) and Collins? gen-
erative parser (Collins, 1999), as in previous work.
We show that PBS algorithms achieve good results
in terms of both the traditional TC measure (signifi-
cantly better than the random selection baseline and
similar to the results of the state of the art tree en-
tropy (TE) method of (Hwa, 2004)) and our novel
cognitively driven measures (where PBS algorithms
significantly outperform both TE and the random
baseline). We thus argue that PBS provides a way to
select a sample that imposes reduced cognitive load
on the human annotator.
2 Related Work
Previous work on sample selection for statistical
parsers applied active learning (AL) (Cohn and Lad-
ner, 1994) to corpora of various languages and syn-
tactic annotation schemes and to parsers of different
performance levels. In order to be able to compare
our results to previous work targeting high parser
performance, we selected the corpus and parser
used by the method reporting the best results (Hwa,
2004), WSJ and Collins? parser.
Hwa (2004) used uncertainty sampling with the
tree entropy (TE) selection function1 to select train-
ing samples for the Collins parser. In each it-
eration, each of the unlabelled pool sentences is
parsed by the parsing model, which outputs a list
of trees ranked by their probabilities. The scored
list is treated as a random variable and the sentences
whose variable has the highest entropy are selected
for human annotation. Sample size was measured
in TC and ranged from 100K to 700K WSJ con-
stituents. The initial size of the unlabelled pool was
800K constituents (the 40K sentences of sections 2-
21 of WSJ). A detailed comparison between the re-
sults of TE and our methods is given in Section 5.
The following works addressed the task of sam-
ple selection for statistical parsers, but in signifi-
cantly different experimental setups. Becker and
Osborne (2005) addressed lower performance lev-
els of the Collins parser. Their uncertainty sam-
pling protocol combined bagging with the TE func-
tion, achieving a 32% TC reduction for reaching a
parser f-score level of 85.5%. The target sample size
set contained a much smaller number of sentences
(?5K) than ours. Baldridge and Osborne (2004) ad-
dressed HPSG parse selection using a feature based
log-linear parser, the Redwoods corpus and commit-
tee based active learning, obtaining 80% reduction
in annotation cost. Their annotation cost measure
was related to the number of possible parses of the
sentence. Tang et al (2002) addressed a shallow
parser trained on a semantically annotated corpus.
1Hwa explored several functions in the experimental setup
used in the present work, and TE gave the best results.
4
They used an uncertainty sampling protocol, where
in each iteration the sentences of the unlabelled pool
are clustered using a distance measure defined on
parse trees to a predefined number of clusters. The
most uncertain sentences are selected from the clus-
ters, the training taking into account the densities of
the clusters. They reduced the number of training
sentences required for their parser to achieve its best
performance from 1300 to 400.
The importance of cognitively driven measures of
sentences? syntactic complexity has been recognized
by Roark et al (2007) who demonstrated their utility
for mild cognitive impairment diagnosis. Zhu et al
(2008) used a clustering algorithm for sampling the
initial labeled set in an AL algorithm for word sense
disambiguation and text classification. In contrast to
our CBS method, they proceeded with iterative un-
certainty AL selection. Melville et al (2005) used
parameter-based sample selection for a classifier in
a classic active learning setting, for a task very dif-
ferent from ours.
Sample selection has been applied to many NLP
applications. Examples include base noun phrase
chunking (Ngai, 2000), named entity recognition
(Tomanek et al, 2007) and multi?task annotation
(Reichart et al, 2008).
3 Cognitively Driven Evaluation Measures
While the resources, capabilities and constraints of
the human parser have been the subject of extensive
research, different theories predict different aspects
of its observed performance. We focus on struc-
tures that are widely agreed to impose a high cog-
nitive load on the human annotator and on theories
considering the cognitive resources required in pars-
ing a complete sentence. Based on these, we derive
measures for the cognitive load on the human parser
when syntactically annotating a set of sentences.
Nested structures. A nested structure is a parse
tree node representing a constituent created while
another constituent is still being processed (?open?).
The degree K of a nested structure is the number of
such open constituents. In this paper, we enumer-
ate the constituents in a top-down left-right order,
and thus when a constituent is created, only its an-
cestors are processed2. A constituent is processed
2A good review on node enumeration of the human parser
in given in (Abney and Johnson, 1991).
S
NP1
JJ
Last
NN
week
NP2
NNP
IBM
VP
VBD
bought
NP3
NNP
Lotus
Figure 1: An example parse tree.
until the processing of its children is completed. For
example, in Figure 1, when the constituent NP3 is
created, it starts a nested structure of degree 2, since
two levels of its ancestors (VP, S) are still processed.
Its parent (VP) starts a nested structure of degree 1.
The difficulty of deeply nested structures for the
human parser is well established in the psycholin-
guistics literature. We review here some of the vari-
ous explanations of this phenomenon; for a compre-
hensive review see (Gibson, 1998).
According to the classical stack overflow theory
(Chomsky and Miller, 1963) and its extension, the
incomplete syntactic/thematic dependencies theory
(Gibson, 1991), the human parser should track the
open structures in its short term memory. When the
number of these structures is too large or when the
structures are nested too deeply, the short term mem-
ory fails to hold them and the sentence becomes un-
interpretable.
According to the perspective shifts theory
(MacWhinney, 1982), processing deeply nested
structures requires multiple shifts of the annotator
perspective and is thus more difficult than process-
ing shallow structures. The difficulty of deeply
nested structured has been demonstrated for many
languages (Gibson, 1998).
We thus propose the total number of nested struc-
tures of degree K in a sample (TNSK) as a measure
of the cognitive efforts that its annotation requires.
The higher K is, the more demanding the structure.
Sentence level resources. In the psycholinguis-
tic literature of sentence processing there are many
theories describing the cognitive resources required
during a complete sentence processing. These re-
sources might be allocated during the processing of
a certain word and are needed long after its con-
stituent is closed. We briefly discuss two lines of
theory, focusing on their predictions that sentences
consisting of a large number of structures (e.g., con-
5
stituents or nested structures) require more cognitive
resources for longer periods.
Levelt (2001) suggested a layered model of the
mental lexicon organization, arguing that when one
hears or reads a sentence s/he activates word forms
(lexemes) that in turn activate lemma information.
The lemma information contains information about
syntactic properties of the word (e.g., whether it is
a noun or a verb) and about the possible sentence
structures that can be generated given that word. The
process of reading words and retrieving their lemma
information is incremental and the lemma informa-
tion for a given word is used until its syntactic struc-
ture is completed. The information about a word in-
clude all syntactic predictions, obligatory (e.g., the
prediction of a noun following a determiner) and op-
tional (e.g., optional arguments of a verb, modifier
relationships). This information might be relevant
long after the constituents containing the word are
closed, sometimes till the end of the sentence.
Another line of research focuses on working
memory, emphasizing the activation decay princi-
ple. It stresses that words and structures perceived
during sentence processing are forgotten over time.
As the distance between two related structures in a
sentence grows, it is more demanding to reactivate
one when seeing the other. Indeed, supported by
a variety of observations, many of the theories of
the human parser (see (Lewis et al, 2006) for a sur-
vey) predict that processing items towards the end of
longer sentences should be harder, since they most
often have to be integrated with items further back.
Thus, sentences with a large number of structures
impose a special cognitive load on the annotator.
We thus propose to use the number of structures
(constituents or nested structures) in a sentence as a
measure of its difficulty for human annotation. The
measures we use for a sample (a sentence set) are the
average number of constituents (AC) and the aver-
age number of nested structures of degree k (ANSK)
per sentence in the set. Higher AC or ANSK values
of a set imply higher annotation requirements3.
Pschycolinguistics research makes finer observa-
3The correlation between the number of constituents and
sentence length is very strong (e.g., correlation coefficient of
0.93 in WSJ section 0). We could use the number of words, but
we prefer the number of structures since the latter better reflects
the arguments made in the literature.
tions about the human parser than those described
here. A complete survey of that literature is beyond
the scope of this paper. We consider the proposed
measures a good approximation of some of the hu-
man parser characteristics.
4 Parameter Based Sampling (PBS)
Our approach is to sample the unannotated pool with
respect to the distribution of the model parameters
in its sentences. In this paper, in order to compare to
previous works, we apply our methods to the Collins
generative parser (Collins, 1999). For any sentence
s and parse tree t it assigns a probability p(s, t),
and finds the tree for which this probability is maxi-
mized. To do that, it writes p(t, s) as a product of the
probabilities of the constituents in t and decomposes
the latter using the chain rule. In simplified notation,
it uses:
p(t, s) =
?
P (S1 ? S2 . . . Sn) =
?
P (S1)?. . .?P (Sn|?(S1 . . . Sn))
(1)
We refer to the conditional probabilities as the model
parameters.
Cluster Based Sampling (CBS). We describe
here a method for sampling subsets that leads to a
parameter estimation that is similar to the parame-
ter estimation we would get if annotating the whole
unannotated set.
To do that, we randomly select M sentences from
the unlabelled pool N , manually annotate them,
train the parser with these sentences and parse the
rest of the unlabelled pool (G = N ? M ). Using
this annotation we build a syntactic vector repre-
sentation for each sentence in G. We then cluster
these sentences and sample the clusters with respect
to their weights to preserve the distribution of the
syntactic features. The selected sentences are man-
ually annotated and combined with the group of M
sentences to train the final parser. The size of this
combined sample is measured when the annotation
efforts are evaluated.
Denote the left hand side nonterminal of a con-
stituent by P and the unlexicalized head of the con-
stituent by H . The domain of P is the set of non-
terminals (excluding POS tags) and the domain of H
is the set of nonterminals and POS tags of WSJ. In
all the parameters of the Collins parser P and H are
conditioned upon. We thus use (P,H) pairs as the
6
features in the vector representation of each sentence
in G. The i-th coordinate is given by the equation:
?
c?t(s)
?
i
Fi(Q(c) == i) ? L(c) (2)
Where c are the constituents of the sentence parse
t(s), Q is a function that returns the (P,H) pair
of the constituent c, Fi is a predicate that returns 1
iff it is given pair number i as an argument and 0
otherwise, and L is the number of modifying non-
terminals in the constituent plus 1 (for the head),
counting the number of parameters that condition
on (P,H). Following equation (2), the ith coordi-
nate of the vector representation of a sentence in G
contains the number of parameters that will be cal-
culated conditioned on the ith (P,H) pair.
We use the k-means clustering algorithm, with the
L2 norm as a distance metric (MacKay, 2002), to di-
vide vectors into clusters. Clusters created by this
algorithm contain adjacent vectors in a Euclidean
space. Clusters represent sentences with similar fea-
tures values. To initialize k-means, we sample the
initial centers values from a uniform distribution
over the data points.
We do not decide on the number of clusters in ad-
vance but try to find inherent structure in the data.
Several methods for estimating the ?correct? num-
ber of clusters are known (Milligan and Cooper,
1985). We used a statistical heuristic called the
elbow test. We define the ?within cluster disper-
sion? Wk as follows. Suppose that the data is di-
vided into k clusters C1 . . . Ck with |Cj | points in
the jth cluster. Let Dt = ?i,j?Ct di,j where
di,j is the squared Euclidean distance, then Wk :=?k
t=1
1
2|Ct|Dt. Wk tends to decrease monotonically
as k increases. In many cases, from some k this de-
crease flattens markedly. The heuristic is that the
location of such an ?elbow? indicates the appropriate
number of clusters. In our experiments, an obvious
elbow occurred for 15 clusters.
ki sentences are randomly sampled from each
cluster, ki = D |Ci|?
j |Cj |
, where D is the number
of sentences to be sampled from G. That way we
ensure that in the final sample each cluster is repre-
sented according to its size.
CMM Sampling. All of the parameters in the
Collins parser are conditioned on the constituent?s
head word. Since word statistics are sparse, sam-
pling from clusters created according to a lexical
vector representation of the sentences does not seem
promising4.
Another way to create a sample from which the
parser can extract robust head word statistics is to
select a sample containing many examples of each
word. More formally, we denote the words that oc-
cur in the unlabelled pool at least t times by t-words,
where t is a parameter of the algorithm. We want to
select a sample containing at least t examples of as
many t-words as possible.
To select such a sample we introduce a novel op-
timisation problem. Our problem is a variant of the
multiset multicover (MM) problem, which we call
the constrained multiset multicover (CMM) prob-
lem. The setting of the MM problem is as fol-
lows (Hochbaum, 1997): Given a set I of m ele-
ments to be covered each bi times, a collection of
multisets Sj ? I , j ? J = {1, . . . , n} (a multiset is
a set in which members? multiplicity may be greater
than 1), and weights wj , find a subcollection C of
multisets that covers each i ? I at least bi times, and
such that
?
j?C wj is minimized.
CMM differs from MM in that in CMM the sum
of the weights (representing the desired number of
sentences to annotate) is bounded, while the num-
ber of covered elements (representing the t-words)
should be maximized. In our case, I is the set of
words that occur at least t times in the unlabelled
pool, bi = t,?i ? I , the multisets are the sentences
in that pool and wj = 1,?j ? J .
Multiset multicover is NP-hard. However, there is
a good greedy approximation algorithm for it. De-
fine a(sj , i) = min(R(sj , i), di), where di is the
difference between bi and the number of instances
of item i that are present in our current sample, and
R(sj , i) is the multiplicity of the i-th element in the
multiset sj . Define A(sj) to be the multiset contain-
ing exactly a(sj , i) copies of any element i if sj is
not already in the set cover and the empty set if it
is. The greedy algorithm repeatedly adds a set mini-
mizing wj|A(sj)| . This algorithm provenly achieves an
approximation ratio between ln(m) and ln(m) + 1.
In our case all weights are 1, so the algorithm would
4We explored CBS with several lexical features schemes and
got only marginal improvement over random selection.
7
simply add the sentence that maximizes A(sj) to the
set cover.
The problem in directly applying the algorithm to
our case is that it does not take into account the de-
sired sample size. We devised a variant of the algo-
rithm where we use a binary tree to ?push? upwards
the number of t-words in the whole batch of unan-
notated sentences that occurs at least t times in the
selected one. Below is a detailed description. D de-
notes the desired number of items to sample.
The algorithm has two steps. First, we iter-
atively sample (without replacement) D multisets
(sentences) from a uniform distribution over the
multisets. In each iteration we calculate for the se-
lected multiset its ?contribution? ? the number of
items that cross the threshold of t occurrences with
this multiset minus the number of items that cross
the t threshold without this multiset (i.e. the contri-
bution of the first multiset is the number of t-words
occurring more than t times in it). For each multiset
we build a node with a key that holds its contribu-
tion, and insert these nodes in a binary tree. Inser-
tion is done such that all downward paths are sorted
in decreasing order of key values.
Second, we iteratively sample (from a uniform
distribution, without replacement) the rest of the
multisets pool. For each multiset we perform two
steps. First, we prepare a node with a key as de-
scribed above. We then randomly choose Z leaves5
in the binary tree (if the number of leaves is smaller
than Z all of the leaves are chosen). For each leaf we
find the place of the new node in the path from the
root to the leaf (paths are sorted in decreasing order
of key values). We insert the new node to the high-
est such place found (if the new key is not smaller
than the existing paths), add its multiset to the set of
selected multisets, and remove the multiset that cor-
responds to the leaf of this path from the batch and
the leaf itself from the binary tree. We finally choose
the multisets that correspond to the highest D nodes
in the tree.
An empirical demonstration of the quality of ap-
proximation that the algorithm provides is given in
Figure 2. We ran our algorithm with the threshold
parameter set to t ? [2, 14] and counted the num-
5We tried Z values from 10 to 100 in steps of 10 and ob-
served very similar results. We report results for Z = 100.
0 100 200 300 400 500 600 700 800
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
number of training constituents (thousands)
n
u
m
be
r  
of
  t
?w
or
ds
t=2
t=8
t=11
t=14
t=5
random
Figure 2: Number of t-words for t = 5 in samples selected
by CMM runs with different values of the threshold pa-
rameter t and in a randomly selected sample. CMM with
t = 5 is significantly higher. All the lines except for the
line for t = 5 are unified. For clarity, we do not show all t
values: their curves are also similar to the t 6= 5 lines.
Method 86% 86.5% 87% 87.5% 88%
TE 16.9% 27.1% 26.9% 14.8% 15.8%
(152K) (183K) (258K) (414K) (563 K)
CBS 19.6% 16.8% 19% 21.1% 9%
(147K) (210K) (286K) (382K) (610K)
CMM 9% 10.4% 8.9% 10.3% 14%
(167K) (226K) (312K) (433K) (574K)
Table 1: Reduction in annotation cost in TC terms com-
pared to the random baseline for tree entropy (TE), syn-
tactic clustering (CBS) and CMM. The compared samples
are the smallest samples selected by each of the methods
that achieve certain f-score levels. Reduction is calcu-
lated by: 100 ? 100 ? (TCmethod/TCrandom).
ber of words occurring at least 5 times in the se-
lected sample. We followed the same experimen-
tal protocol as in Section 5. The graph shows that
the number of words occurring at least 5 times in a
sample selected by our algorithm when t = 5 is sig-
nificantly higher (by about a 1000) than the number
of such words in a randomly selected sample and in
samples selected by our algorithm with other t pa-
rameter values. We got the same pattern of results
when counting words occurring at least t times for
the other values of the t parameter ? only the run of
the algorithm with the corresponding t value created
a sample with significantly higher number of words
not below threshold. The other runs and random se-
lection resulted in samples containing significantly
lower number of words not below threshold.
In Section 5 we show that the parser performance
when it is trained with a sample selected by CMM
is significantly better than when it is trained with a
randomly selected sample. Improvement is similar
across the t parameter values.
8
86% 87% 88%
Method TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK
(1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22)
TE 34.9% 3.6% - 8.9% - 61.3% 42.2% 14.4% - 9.9% - 62.7% 25% 8.1% - 6.3% - 30%
CBS 21.3% 18.6% - 0.5% - 3.5% 19.6% 24.2% - 0.3% - 1.8% 8.9% 8.6 % 0% - 0.3%
CMM 10.18% 8.87% -0.82% -3.39% 11% 16.22% -0.34% -1.8% 14.65% 14.11% -0.02% - 0.08%
Table 2: Annotation cost reduction in TNSK and ANSK compared to the random baseline for tree entropy (TE), syntactic
clustering (CBS) and CMM. The compared samples are the smallest selected by each of the methods that achieve certain
f-score levels. Each column represents the reduction in total or average number of structures of degree 1?6 or 7?22.
Reduction for each measure is calculated by: 100? 100? (measuremethod/measurerandom). Negative reduction
is an addition. Samples with a higher reduction in a certain measure are better in terms of that measure.
0 5 10 15 20 25?1
0
1
2
3
x 104
K
T
N
S
K
 (K
)
 
 
CMM(t=8) ? TE
CBS ? TE
0 line
0 5 10 15 20 251
1.1
1.2
1.3
1.4
1.5
1.6
1.7
K
A
N
S
K
 m
e
th
o
d
/A
N
S
K
 r
a
n
d
o
m
 
 
TE
CMM,t=8
CBS
86  86.5 87  87.5 88  18
20
22
24
26
28
F score
A
v
e
ra
g
e
 n
u
m
b
e
r 
o
f 
c
o
n
s
ti
tu
e
n
ts
 
 
TE
CMM,  t = 8
CBS
0 1 2 3 3.5
x 104
1  
1.25
1.5
Number  of  sentences
A
C
 m
e
th
o
d
/A
C
 r
a
n
d
o
m
 
 
TE
CMM, t=8
CBS
Figure 3: Left to right: First: The difference between the number of nested structures of degree K of CMM and TE and
of CBS and TE. The curves are unified. The 0 curve is given for reference. Samples selected by CMM and CBS have
more nested structures of degrees 1?6 and less nested structures of degrees 7?22. Results are presented for the smallest
samples required for achieving f-score of 88. Similar patterns are observed for other f-score values. Second: Average
number of nested structures of degree K as a function of K for the smallest sample required for achieving f-score of
88. Results for each of the methods are normalized by the average number of nested structures of degree K in the
smallest randomly selected sample required for achieving f-score of 88. The sentences in CMM and CBS samples are
not more complex than sentences in a randomly selected sample. In TE samples sentences are more complex. Third:
Average number of constituents (AC) for the smallest sample of each of the methods that is required for achieving a
given f-score. CMM and CBS samples contain sentences with a smaller number of constituents. Fourth: AC values for
the samples created by the methods (normalized by AC values of a randomly selected sample). The sentences in TE
samples, but not in CMM and CBS samples, are more complex than sentences in a randomly selected sample.
5 Results
Experimental setup. We used Bikel?s reimplemen-
tation of Collins? parsing model 2 (Bikel, 2004).
Sections 02-21 and 23 of the WSJ were stripped
from their annotation. Sections 2-21 (39832 sen-
tences, about 800K constituents) were used for train-
ing, Section 23 (2416 sentences) for testing. No
development set was used. We used the gold stan-
dard POS tags in two cases: in the test section (23)
in all experiments, and in Sections 02-21 in the
CBS method when these sections are to be parsed
in the process of vector creation. In active learn-
ing methods the unlabelled pool is parsed in each
iteration and thus should be tagged with POS tags.
Hwa (2004) (to whom we compare our results) used
the gold standard POS tags for the same sections
in her work6. We implemented a random baseline
6Personal communication with Hwa. Collins? parser uses an
where sentences are uniformly selected from the un-
labelled pool for annotation. For reliability we re-
peated each experiment with the algorithms and the
random baseline 10 times, each time with different
random selections (M sentences for creating syntac-
tic tagging and k-means initialization for CBS, sen-
tence order in CMM), and averaged the results.
Each experiment contained 38 runs. In each run
a different desired sample size was selected, from
1700 onwards, in steps of 1000. Parsing perfor-
mance is measured in terms of f-score
Results. We compare the performance of our
CBS and CMM algorithms to the TE method (Hwa,
2004)7, which is the only sample selection work ad-
input POS tag only if it cannot tag its word using the statistics
learned from the training set.
7Hwa has kindly sent us the samples selected by her TE. We
evaluated these samples with TC and the new measures. The TC
of the minimal sample she sent us needed for achieving f-score
9
dressing our experimental setup. Unless otherwise
stated, we report the reduction in annotation cost:
100? 100? (measuremethod/measurerandom).
CMM results are very similar for t ? {2, 3, . . . , 14},
and presented for t = 8.
Table 1 presents reduction in annotation cost in
TC terms. CBS achieves greater reduction for f =
86, 87.5, TE for f = 86.5, 87, 88. For f = 88, TE
and CMM performance are almost similar. Examin-
ing the f-score vs. TC sample size over the whole
constituents range (not shown due to space con-
straints) reveals that CBS, CMM and TE outperform
random selection over the whole range. CBS and
TE performance are quite similar with TE being bet-
ter in the ranges of 170?300K and 520?650K con-
stituents (42% of the 620K constituents compared)
and CBS being better in the ranges of 130?170K and
300?520K constituents (44% of the range). CMM
performance is worse than CBS and TE until 540K
constituents. From 650K constituents on, where
the parser achieves its best performance, the perfor-
mance of CMM and TE methods are similar, outper-
forming CBS.
Table 2 shows the annotation cost reduction in
ANSK and TNSK terms. TE achieves remarkable
reduction in the total number of relatively shallow
structures (TNSK K = 1?6). Our methods, in con-
trast, achieve remarkable reduction in the number of
deep structures (TNSK K = 7?22)8. This is true for
all f-score values. Moreover, the average number of
nested structures per sentence, for every degree K
(ANSK for every K) in TE sentences is much higher
than in sentences of a randomly selected sample. In
samples selected by our methods, the ANSK values
are very close to the ANSK values of randomly se-
lected samples. Thus, sentences in TE samples are
much more complex than in CBS and CMM samples.
The two leftmost graphs in Figure 3 demonstrate
(for the minimal samples required for f-score of 88)
that these reductions hold for each K value (ANSK)
and for each K ? [7, 22] (TNSK) not just on the av-
of 88 is different from the number reported in (Hwa, 2004). We
compare our TC results with the TC result in the sample sent us
by Hwa.
8We present results where the border between shallow and
deep structures is set to be Kborder = 6. For every Kborder ?
{7, 8, . . . , 22} TNSK reductions with CBS and CMM are much
more impressive than with TE for structures whose degree is
K ? [Kborder, 22].
erage over these K values. We observed similar re-
sults for other f-score values.
The two rightmost graphs of Figure 3 demon-
strates AC results. The left of them shows that for
every f-score value, the AC measure of the minimal
TE sample required to achieve that f-score is higher
than the AC value of PBS samples (which are very
similar to the AC values of randomly selected sam-
ples). The right graph demonstrates that for every
sample size, the AC value of TE samples is higher
than that of PBS samples.
All AL based previous work (including TE) is it-
erative. In each iteration thousands of sentences
are parsed, while PBS algorithms perform a single
iteration. Consequently, PBS computational com-
plexity is dramatically lower. Empirically, using a
Pentium 4 2.4GHz machine, CMM requires about an
hour and CBS about 16.5 hours, while the TE parsing
steps alone take 662 hours (27.58 days).
6 Discussion and Future Work
We introduced novel evaluation measures: AC,
TNSK and ANSK for the task of sample selection
for statistical parsers. Based on the psycholinguis-
tic literature we argue that these measures reflect as-
pects of the cognitive efforts of the human annota-
tor that are not reflected by the traditional TC mea-
sure. We introduced the parameter based sample se-
lection (PBS) approach and its CMM and CBS algo-
rithms that do not deliberately select difficult sen-
tences. Therefore, our intuition was that they should
select a sample that leads to an accurate parameter
estimation but does not contain a high number of
complex structures. We demonstrated that CMM and
CBS achieve results that are similar to the state of the
art TE method in TC terms and outperform it when
the cognitively driven measures are considered.
The measures we suggest do not provide a full
and accurate description of human annotator efforts.
In future work we intend to extend and refine our
measures and to revise our algorithms accordingly.
We also intend to design stopping criteria for the
PBS methods. These are criteria that decide when
the selected sample suffices for the parser best per-
formance and further annotation is not needed.
10
References
Steven Abney and Mark Johnson, 1991. Memory re-
quirements and local ambiguities of parsing strategies.
Psycholinguistic Research, 20(3):233?250.
Daniel M. Biken, 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Jason Baldridge and Miles Osborne, 2004. Active learn-
ing and the total cost of annotation. EMNLP ?04.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI 05.
Markus Becker, 2008. Active learning ? an explicit treat-
ment of unreliable parameters. Ph.D. thesis, The Uni-
versity of Edinburgh.
Noam Chomsky and George A. Miller, 1963. Fini-
tary models of language users. In R. Duncan Luce,
Robert R. Bush, and Eugene Galanter, editors, Hand-
book of Mathematical Psychology, volume II. John
Wiley, New York, 419?491.
David Cohn, Les Atlas and Richard E. Ladner, 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201?221.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Edward Gibson, 1991. A computational theory of hu-
man linguistic processing: memory limitations and
processing breakdowns. Ph.D. thesis, Carnegie Mel-
lon University, Pittsburg, PA.
Edward Gibson, 1998. Linguistic complexity: locality
of syntactic dependencies. Cognition, 68:1?76.
Dorit Hochbaum (ed), 1997. Approximation algorithms
for NP-hard problems. PWS Publishing, Boston.
Rebecca Hwa, 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Cody Kwok, Oren Etzioni, Daniel S. Weld, 2001. Scal-
ing question answering to the Web. WWW ?01.
Matthew Lease and Eugene Charniak, 2005. Parsing
biomedical literature. IJCNLP ?05.
Willem J.M. Levelt, 2001. Spoken word production: A
theory of lexical access. PNAS, 98(23):13464?13471.
Richard L. Lewis, Shravan Vasishth and Julie Van Dyke,
2006. Computational principles of working memory
in sentence comprehension. Trends in Cognitive Sci-
ence, October:447-454.
David MacKay, 2002. Information theory, inference and
learning algorithms. Cambridge University Press.
Brian MacWhinney, 1982. The competition model. In
B. MacWhinney, editor, Mechanisms of language ac-
quisition. Hillsdale, NJ: Lawrence Erlbaum, 249?308.
Daniel Marcu, Wei Wang, Abdessamabad Echihabi, and
Kevin Knight, 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
EMNLP ?06.
P. Melville, M. Saar-Tsechansky, F. Provost and R.J.
Mooney, 2005. An expected utility approach to ac-
tive feature-value acquisition. 5th IEEE Intl. Conf. on
Data Mining ?05.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz, 1994. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
G.W. Milligan and M.C Cooper, 1985. An examination
of procedures for determining the number of clusters
in a data set. Psychometrika, 58(2):159?157.
Grace Ngai and David Yarowski, 2000. Rule writing
or annotation: cost?efficient resource usage for base
noun phrase chunking. ACL ?00.
Roi Reichart, Katrin Tomanek, Udo Hahn and Ari Rap-
poport, 2008. Multi-task active learning for linguistic
annotations. ACL ?08.
Brian Roark, Margaret Mitchell and Kristy Hollingshead,
2007. Syntactic complexity measures for detecting
mild cognitive impairment. BioNLP workshop, ACL
?07.
Min Tang, Xiaoqiang Luo, and Salim Roukos, 2002. Ac-
tive learning for statistical natural language parsing.
ACL ?02.
Katrin Tomanek, Joachim Wermtre, and Udo Hahn,
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. EMNLP ?07.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning, 2005. Joint learning improves semantic role
labeling. ACL ?05.
Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin
K. Tsou, 2008. Active learning with sampling by un-
certainty and density for word sense disambiguation
and text classification. COLING ?08.
11
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 48?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Superior and Efficient Fully Unsupervised Pattern-based Concept
Acquisition Using an Unsupervised Parser
Dmitry Davidov1 Roi Reichart1 Ari Rappoport2
1ICNC , 2Institute of Computer Science
Hebrew University of Jerusalem
{dmitry@alice.nc|roiri@cs|arir@cs}.huji.ac.il
Abstract
Sets of lexical items sharing a significant
aspect of their meaning (concepts) are fun-
damental for linguistics and NLP. Unsuper-
vised concept acquisition algorithms have
been shown to produce good results, and are
preferable over manual preparation of con-
cept resources, which is labor intensive, er-
ror prone and somewhat arbitrary. Some ex-
isting concept mining methods utilize super-
vised language-specific modules such as POS
taggers and computationally intensive parsers.
In this paper we present an efficient fully
unsupervised concept acquisition algorithm
that uses syntactic information obtained from
a fully unsupervised parser. Our algorithm
incorporates the bracketings induced by the
parser into the meta-patterns used by a sym-
metric patterns and graph-based concept dis-
covery algorithm. We evaluate our algorithm
on very large corpora in English and Russian,
using both human judgments and WordNet-
based evaluation. Using similar settings as
the leading fully unsupervised previous work,
we show a significant improvement in con-
cept quality and in the extraction of multiword
expressions. Our method is the first to use
fully unsupervised parsing for unsupervised
concept discovery, and requires no language-
specific tools or pattern/word seeds.
1 Introduction
Comprehensive lexical resources for many domains
and languages are essential for most NLP applica-
tions. One of the most utilized types of such re-
sources is a repository of concepts: sets of lexical
items sharing a significant aspect of their meanings
(e.g., types of food, tool names, etc).
While handcrafted concept databases (e.g., Word-
Net) are extensively used in NLP, manual compila-
tion of such databases is labor intensive, error prone,
and somewhat arbitrary. Hence, for many languages
and domains great efforts have been made for au-
tomated construction of such databases from avail-
able corpora. While language-specific and domain-
specific studies show significant success in develop-
ment of concept discovery frameworks, the majority
of domains and languages remain untreated. Hence
there is a need for a framework that performs well
for many diverse settings and is as unsupervised and
language-independent as possible.
Numerous methods have been proposed for seed-
based concept extraction where a set of concept pat-
terns (or rules), or a small set of seed words for each
concept, is provided as input to the concept acqui-
sition system. However, even simple definitions for
concepts are not always available.
To avoid requiring this type of input, a number of
distributional and pattern-based methods have been
proposed for fully unsupervised seed-less acquisi-
tion of concepts from text. Pattern-based algorithms
were shown to obtain high quality results while be-
ing highly efficient in comparison to distributional
methods. Such fully unsupervised methods do not
incorporate any language-specific parsers or taggers,
so can be successfully applied to diverse languages.
However, unsupervised pattern-based methods
suffer from several weaknesses. Thus they are fre-
quently restricted to single-word terms and are un-
able to discover multiword expressions in efficient
and precise manner. They also usually ignore poten-
tially useful part-of-speech and other syntactic in-
formation. In order to address these weaknesses,
several studies utilize language-specific parsing or
48
tagging systems in concept acquisition. Unfortu-
nately, while improving results, this heavily affects
the language- and domain- independence of such
frameworks, and severely impacts efficiency since
even shallow parsing is computationally demanding.
In this paper we present a method to utilize the in-
formation induced by unsupervised parsers in an un-
supervised pattern-based concept discovery frame-
work. With the recent development of fast fully un-
supervised parsers, it is now possible to add parser-
based information to lexical patterns while keep-
ing the language-independence of the whole frame-
work and still avoiding heavy computational costs.
Specifically, we incorporate the bracketings induced
by the parser into the meta-patterns used by a sym-
metric patterns and graph-based unsupervised con-
cept discovery algorithm.
We performed a thorough evaluation on two En-
glish corpora (the BNC and a 68GB web corpus)
and on a 33GB Russian corpus. Evaluations were
done using both human judgments and WordNet, in
similar settings as that of the leading unsupervised
previous work. Our results show that utilization of
unsupervised parser both improves the assignment
of single-word terms to concepts and allows high-
precision discovery and assignment of of multiword
expressions to concepts.
2 Previous Work
Much work has been done on lexical acquisition of
all sorts and the acquisition of concepts in particu-
lar. Concept acquisition methods differ in the type of
corpus annotation and other human input used, and
in their basic algorithmic approach. Some methods
directly aim at concept acquisition, while the direct
goal in some is the construction of hyponym (?is-a?)
hierarchies. A subtree in such a hierarchy can be
viewed as defining a concept.
A major algorithmic approach is to represent
word contexts as vectors in some space and use dis-
tributional measures and clustering in that space.
Pereira (1993), Curran (2002) and Lin (1998) use
syntactic features in the vector definition. (Pantel
and Lin, 2002) improves on the latter by clustering
by committee. Caraballo (1999) uses conjunction
and appositive annotations in the vector representa-
tion. Several studies avoid requiring any syntactic
annotation. Some methods are based on decompo-
sition of a lexically-defined matrix (by SVD, PCA
etc), e.g. (Schu?tze, 1998; Deerwester et al, 1990).
While great effort has been made for improv-
ing the computational complexity of distributional
methods (Gorman and Curran, 2006), they still re-
main highly computationally intensive in compari-
son to pattern approaches (see below), and most of
them do not scale well for very large datasets.
The second main approach is to use lexico-
syntactic patterns. Patterns have been shown to pro-
duce more accurate results than feature vectors, at
a lower computational cost on large corpora (Pan-
tel et al, 2004). Since (Hearst, 1992), who used a
manually prepared set of initial lexical patterns, nu-
merous pattern-based methods have been proposed
for the discovery of concepts from seeds. Other
studies develop concept acquisition for on-demand
tasks where concepts are defined by user-provided
seeds. Many of these studies utilize information ob-
tained by language-specific parsing and named en-
tity recognition tools (Dorow et al, 2005). Pantel et
al. (2004) reduce the depth of linguistic data used,
but their method requires POS tagging.
TextRunner (Banko et al, 2007) utilizes a set
of pattern-based seed-less strategies in order to ex-
tract relational tuples from text. However, this sys-
tem contains many language-specific modules, in-
cluding the utilization of a parser in one of the pro-
cessing stages. Thus the majority of the existing
pattern-based concept acquisition systems rely on
pattern/word seeds or supervised language-specific
tools, some of which are very inefficient.
Davidov and Rappoport (2006) developed a
framework which discovers concepts based on high
frequency words and symmetry-based pattern graph
properties. This framework allows a fully unsuper-
vised seed-less discovery of concepts without rely-
ing on language-specific tools. However, it com-
pletely ignores potentially useful syntactic or mor-
phological information.
For example, the pattern ?X and his Y? is useful
for acquiring the concept of family member types,
as in ?his siblings and his parents?. Without syn-
tactic information, it can capture noise, as in ?... in
ireland) and his wife)? (parentheses denote syntac-
tic constituent boundaries). As another example, the
useful symmetric pattern ?either X or Y? can appear
in both good examples (?choose either Chihuahua
49
or Collie.?) and bad ones (?either Collie or Aus-
tralian Bulldog?). In the latter case, the algorithm
both captures noise (?Australlian? is now consid-
ered as a candidate for the ?dog type? concept), and
misses the discovery of a valid multiword candidate
(?Australlian Bulldog?). While symmetry-based fil-
tering greatly reduces such noise, the basic problem
remains. As a result, incorporating at least some
parsing information in a language-independent and
efficient manner could be beneficial.
Unsupervised parsing has been explored for sev-
eral decades (see (Clark, 2001; Klein, 2005) for re-
cent reviews). Recently, unsupervised parsers have
for the first time outperformed the right branch-
ing heuristic baseline for English. These include
CCM (Klein and Manning, 2002), the DMV and
DMV+CCM models (Klein and Manning, 2004),
(U)DOP based models (Bod, 2006a; Bod, 2006b;
Bod, 2007), an exemplar based approach (Den-
nis, 2005), guiding EM using contrastive estimation
(Smith and Eisner, 2006), and the incremental parser
of Seginer (2007) which we use here. These works
learn an unlabeled syntactic structure, dependency
or constituency. In this work we use constituency
trees as our syntactic representation.
Another important factor in concept acquisition
is the source of textual data used. To take advan-
tage of the rapidly expanding web, many of the pro-
posed frameworks utilize web queries rather than
local corpora (Etzioni et al, 2005; Davidov et al,
2007; Pasca and Van Durme, 2008; Davidov and
Rappoport, 2009). While these methods have a defi-
nite practical advantage of dealing with the most re-
cent and comprehensive data, web-based evaluation
has some methodological drawbacks such as limited
repeatability (Kilgarriff, 2007). In this study we ap-
ply our framework on offline corpora in settings sim-
ilar to that of previous work, in order to be able to
make proper comparisons.
3 Efficient Unsupervised Parsing
Our method utilizes the information induced by un-
supervised parsers. Specifically, we make use of the
bracketings induced by Seginer?s parser1 (Seginer,
2007). This parser has advantages in three major as-
1The parser is freely available at
http://staff.science.uva.nl/?yseginer/ccl
pects relevant to this paper.
First, it achieves state of the art unsupervised
parsing performance: its F-score2 is 75.9% for sen-
tences of up to 10 words from the PennTreebank
Wall Street Journal corpus (WSJ) (Marcus, 1993),
and 59% for sentences of the same length from the
German NEGRA (Brants, 1997) corpus. These cor-
pora consists of newspaper texts.
Second, to obtain good results, manually created
POS tags are used as input in all the unsupervised
parsers mentioned above except of Seginer?s, which
uses raw sentences as input. (Headden et al, 2008)
have shown that the performance of algorithms that
require POS tags substantially decreases when using
POS tags induced by unsupervised POS taggers in-
stead of manually created ones. Seginer?s incremen-
tal parser is therefore the only fully unsupervised
parser providing high quality parses.
Third, Seginer?s parser is extremely fast. During
its initial stage, the parser builds a lexicon. Our Pen-
tium 2.8GHB machines with 4GHB RAM can store
in memory the lexicon created by up to 0.2M sen-
tences. We thus divided our corpora to batches of
0.2M sentences and parsed each of them separately.
Note that in this setup parsing quality might be even
better than the quality reported in (Seginer, 2007),
since in the setup reported in that paper the parser
was applied to a few thousand sentences only. On
average, the parsing time of a single batch was 5
minutes (run time did not significantly differ across
batches and corpora).
Parser description. The parser utilizes the novel
common-cover link representation for syntactic
structure. This representation resembles depen-
dency structure but unlike the latter, it can be trans-
lated into a constituency tree, which is the syntactic
representation we use in this work.
The parsing algorithm creates the common-cover
links structure of a sentence in an incremental man-
ner. This means that the parser reads the words of
a sentence one after the other and, as each word is
read, it is only allowed to add links that have one of
their ends at that words (and update existing ones).
Words which have not yet been read are not avail-
2F = 2?R?PR+P , where R and P are the recall and precision of
the parsers? bracketing compared to manually created bracket-
ing of the same text. This is the accepted measure for parsing
performance (see (Klein, 2005)).
50
able to the parser at this stage. This restriction is
inspired by psycholinguistics research which sug-
gests that humans process language incrementally.
This results in a significant restriction of the parser?s
search space, which is the reason it is so fast.
During its initial stage the parser builds a lexicon
containing, for each word, statistics helping the deci-
sion of whether to link that word to other words. The
lexicon is updated as any new sentence is read. Lex-
icon updating is also done in an incremental manner
so this stage is also very fast.
4 Unsupervised Pattern Discovery
In the first stage of our algorithm, we run the unsu-
pervised parser on the corpus in order to produce a
bracketing structure for each sentence. In the sec-
ond stage, described here, we use these bracketings
in order to discover, in a fully unsupervised manner,
patterns that could be useful for concept mining.
Our algorithm is based on the concept acquisition
method of (Davidov and Rappoport, 2006). We dis-
cover patterns that connect terms belonging to the
same concept in two main stages: discovery of pat-
tern candidates, and identification of the symmetric
patterns among the candidates.
Pattern candidates. A major idea of (Davidov
and Rappoport, 2006) is that a few dozen high fre-
quency words (HFW) such as ?and? and ?is? con-
nect other, less frequent content terms into relation-
ships. They define meta-patterns, which are short
sequences of H?s and C?s, where H is a slot for
a HFW and C is a slot for a content word (later
to become a word belonging to a discovered con-
cept). Their method was shown to produce good
results. However, the fact that it does not consider
any syntactic information causes problems. Specif-
ically, it does not consider the constituent structure
of the sentence. Meta-patterns that cross constituent
boundaries are likely to generate noise ? two content
words (C?s) in a meta-pattern that belong to differ-
ent constituents are likely to belong to different con-
cepts as well. In addition, meta-patterns that do not
occupy a full constituent are likely to ?cut? multi-
word expressions (MWEs) into two parts, one part
that gets treated as a valid C word and one part that
is completely ignored.
The main idea in the present paper is to use the
bracketings induced by unsupervised parsers in or-
der to avoid the problems above. We utilize brack-
eting boundaries in our meta-patterns in addition
to HFW and C slots. In other words, their origi-
nal meta-patterns are totally lexical, while ours are
lexico-syntactic meta-patterns. We preserve the at-
tractive properties of meta-patterns, because both
HFWs and bracketings can be found or computed in
a language independent manner and very efficiently.
Concretely, we define a HFW as a word appearing
more than TH times per million words, and a C as
a word or multiword expression containing up to 4
words, appearing less than TC times per million.
We require that our patterns include two slots for
C?s, separated by at least a single HFW or bracket.
We allow separation by a single bracket because the
lowest level in the induced bracketing structure usu-
ally corresponds to lexical items, while higher levels
correspond to actual syntactic constituents.
In order to avoid truncation of multiword expres-
sions, we also require the meta pattern to start and
end by a HFW or bracket. Thus our meta-patterns
match the following regular expression:
{H|B}? C1 {H|B}+ C2 {H|B}?
where ?*? means zero or more times, and ?+? means
one or more time and B can be ?(?,?)? brackets pro-
duced by the parser (in these patterns we do not
need to guarantee that brackets match properly). Ex-
amples of such patterns include ?((C1)in C2))?,
?(C1)(such(as(((C2)?, and ?(C1)and(C2)?3. We
dismiss rare patterns that appear less than TP times
per million words.
Symmetric patterns. Many of the pattern candi-
dates discovered in the previous stage are not usable.
In order to find a usable subset, we focus on the sym-
metric patterns. We define a symmetric pattern as a
pattern in which the same pair of terms (C words)
is likely to appear in both left-to-right and right-to-
left orders. In order to identify symmetric patterns,
for each pattern we define a pattern graph G(P ), as
proposed by (Widdows and Dorow, 2002). If term
pair (C1, C2) appears in pattern P in some context,
3This paper does not use any punctuation since the parser
is provided with sentences having all non-alphabetic characters
removed. We assume word separation. C1,2 can be a word or a
multiword expression.
51
we add nodes c1, c2 to the graph and a directed edge
EP (c1, c2) between them. In order to select sym-
metric patterns, we create such a pattern graph for
every discovered pattern, and create a symmetric
subgraph SymG(P) in which we take only bidirec-
tional edges from G(P ). Then we compute three
measures for each pattern candidate as proposed by
(Davidov and Rappoport, 2006):
M1(P ) := |{c1|?c2EP (c1, c2) ? ?c3EP (c3, c1)}||Nodes(G(P ))|
M2(P ) := |Nodes(SymG(P ))||Nodes(G(P ))|
M3(P ) := |Edges(SymG(P ))||Edges(G(P ))|
For each measure, we prepare a sorted list of all can-
didate patterns. We remove patterns that are not in
the top ZT (we use 100, see Section 6) in any of the
three lists, and patterns that are in the bottom ZB in
at least one of the lists.
5 Concept Discovery
At the end of the previous stage we have a set of
symmetric patterns. We now use them in order to
discover concepts. The concept discovery algorithm
is essentially the same as used by (Davidov and Rap-
poport, 2006) and has some similarity with the one
used by (Widdows and Dorow, 2002). In this section
we outline the algorithm.
The clique-set method. The utilized approach to
concept discovery is based on connectivity struc-
tures in the all-pattern term relationship graph G,
resulting from merging all of the single-pattern
graphs for symmetric patterns selected in the previ-
ous stage. The main observation regarding G is that
highly interconnected words are good candidates to
form a concept. We find all strong n-cliques (sub-
graphs containing n nodes that are all interconnected
in both directions). A clique Q defines a concept that
contains all of the nodes in Q plus all of the nodes
that are (1) at least unidirectionally connected to all
nodes in Q, and (2) bidirectionally connected to at
least one node in Q. Using this definition, we create
a concept for each such clique.
Note that a single term can be assigned to several
concepts. Thus a clique based on a connection of the
word ?Sun? to ?Microsoft? can lead to a concept of
computer companies, while the connection of ?Sun?
to ?Earth? can lead to a concept of celestial bodies.
Reducing noise: merging and windowing. Since
any given term can participate in many cliques, the
algorithm creates overlapping categories, some of
which redundant. In addition, due to the nature of
language and the imperfection of the corpus some
noise is obviously to be expected. We enhance the
quality of the obtained concepts by merging them
and by windowing on the corpus. We merge two
concepts Q,R, iff there is more than a 50% overlap
between them: (|Q?R| > |Q|/2) ? (|Q?R| >
|R|/2). In order to increase concept quality and re-
move concepts that are too context-specific, we use
a simple corpus windowing technique. Instead of
running the algorithm of this section on the whole
corpus, we divide the corpus into windows of equal
size and perform the concept discovery algorithm of
this section (without pattern discovery) on each win-
dow independently. We now have a set of concepts
for each window. For the final set, we select only
those concepts that appear in at least two of the win-
dows. This technique reduces noise at the potential
cost of lowering coverage.
A decrease in the number of windows should pro-
duce more noisy results, while discovering more
concepts and terms. In the next section we show that
while windowing is clearly required for a large cor-
pus, incorporation of parser data increases the qual-
ity of the extracted corpus to the point where win-
dowing can be significantly reduced.
6 Results
In order to estimate the quality of concepts and to
compare it to previous work, we have performed
both automatic and human evaluation. Our basic
comparison was to (Davidov and Rappoport, 2006)
(we have obtained their data and utilized their al-
gorithm), where we can estimate if incorporation of
parser data can solve some fundamental weaknesses
of their framework. In the following description, we
call their algorithm P and our parser-based frame-
work P+. We have also performed an indirect com-
parison to (Widdows and Dorow, 2002).
While there is a significant number of other re-
lated studies4 on concept acquisition (see Section 2),
4Most are supervised and/or use language-specific tools.
52
direct or even indirect comparison to these works is
problematic due to difference in corpora, problem
definitions and evaluation strategies. Below we de-
scribe the corpora and parameters used in our evalu-
ation and then show and discuss WordNet-based and
Human evaluation settings and results.
Corpora. We performed in-depth evaluation in
two languages, English and Russian, using three
corpora, two for English and one for Russian.
The first English corpus is the BNC, containing
about 100M words. The second English corpus,
DMOZ(Gabrilovich and Markovitch, 2005), is a
web corpus obtained by crawling URLs in the Open
Directory Project (dmoz.org), resulting in 68GB
containing about 8.2G words from 50M web pages.
The Russian corpus (Davidov and Rappoport, 2006)
was assembled from web-based Russian reposito-
ries, to yield 33GB and 4G words. All of these cor-
pora were also used by (Davidov and Rappoport,
2006) and BNC was used in similar settings by
(Widdows and Dorow, 2002).
Algorithm parameters. The thresholds
TH , TC , TP , ZT , ZB , were determined mostly
by practical memory size considerations: we com-
puted thresholds that would give us the maximal
number of terms, while enabling the pattern access
table to reside in main memory. The resulting
numbers are 100, 50, 20, 100, 100. Corpus window
size was determined by starting from a small
window size, extracting at random a single window,
running the algorithm, and iterating this process
with increased ?2 window sizes until reaching a
desired vocabulary concept participation percentage
(before windowing) (i.e., x% of the different words
in the corpus participate in terms assigned into
concepts. We used 5%.). We also ran the algorithm
without windowing in order to check how well the
provided parsing information can help reduce noise.
Among the patterns discovered are the ubiquitous
ones containing ?and?,?or?, e.g. ?((X) or (a Y))?,
and additional ones such as ?from (X) to (Y)?.
Influence of parsing data on number of discov-
ered concepts. Table 1 compares the concept ac-
quisition framework with (P+) and without (P) uti-
lization of parsing data.
We can see that the amount of different words
V W C AS
P P+ P P+ P P+
DMOZ 16 330 504 142 130 12.8 16.0
BNC 0.3 25 42 9.6 8.9 10.2 15.6
Russ. 10 235 406 115 96 11.6 15.1
Table 1: Results for concept discovery with (P+) and
without (P) utilization of parsing data. V is the total num-
ber (millions) of different words in the corpus. W is the
number (thousands) of words belonging to at least one of
the terms for one of the concepts. C is the number (thou-
sands) of concepts (after merging and windowing). AS
is the average(words) category size.
covered by discovered concepts raises nearly 1.5-
fold when we utilize patterns based on parsing data
in comparison to pure HFW patterns used in previ-
ous work. We can also see nearly the same increase
in average concept size. At the same time we ob-
serve about 15% reduction in the total number of
discovered concepts.
There are two opposite factors in P+ which may
influence the number of concepts, their size and cov-
erage in comparison to P. On one hand, utilization of
more restricted patterns that include parsing infor-
mation leads to a reduced number of concept term
instances being discovered. Thus, the P+ pattern ?(X
(or (a Y))? will recognize ?(TV (or (a movie))? in-
stance and will miss ?(lunch) or (a snack))?, while
the P pattern ?X or a Y? will capture both. This leads
to a decrease in the number of discovered concepts.
On the other hand, P+ patterns, unlike P ones, al-
low the extraction of multiword expressions5, and
indeed more than third of the discovered terms us-
ing P+ were MWEs. Utilization of MWEs not only
allows to cover a greater amount of different words,
but also increases the number of discovered concepts
since new concepts can be found using cliques of
newly discovered MWEs. From the results, we can
see that for a given concept size and word coverage,
the ability to discover MWEs overcomes the disad-
vantage of ignoring potentially useful concepts.
Human judgment evaluation. Our human judge-
ment evaluation closely followed the protocol (Davi-
dov and Rappoport, 2006).
We used 4 subjects for evaluation of the English
5While P method can potentially be used to extract MWEs,
preliminary experimentation shows that without significant
modification, quality of MWEs obtained by P is very low in
comparison to P+
53
concepts and 4 subjects for Russian ones. In order
to assess subjects? reliability, we also included ran-
dom concepts (see below). The goal of the exper-
iment was to examine the differences between the
P+ and P concept acquisition frameworks. Subjects
were given 50 triplets of words and were asked to
rank them using the following scale: (1) the words
definitely share a significant part of their meaning;
(2) the words have a shared meaning but only in
some context; (3) the words have a shared mean-
ing only under a very unusual context/situation; (4)
the words do not share any meaning; (5) I am not
familiar enough with some/all of the words.
The 50 triplets were obtained as follows. We have
randomly selected 40 concept pairs (C+,C): C+ in
P+ and C in P using five following restrictions: (1)
concepts should contain at least 10 words; (2) for
a selected pair, C+ should share at least half of its
single-word terms with C, and C should share at
least half of its words with C+; (3) C+ should con-
tain at least 3 MWEs; (4) C should contain at least 3
words not appearing in C+; (5) C+ should contain at
least 3 single-word terms not appearing in C.
These restrictions allow to select concept pairs
such that C+ is similar to C while they still carry
enough differences which can be examined. We se-
lected the triplets as following: for pairs (C+, C) ten
triplets include terms appearing in both C+ and C
(Both column in Table 2), ten triplets include single-
word terms appearing in C+ but not C (P+ single
column), ten triplets include single-word terms ap-
pearing in C but not C+ (P column), ten triplets in-
clude MWEs appearing in C+ (P+ mwe column) and
ten triplets include random terms obtained from P+
concepts (Rand column).
P+ P Both Rand
mwe single
% shared
meaning
DMOZ 85 88 68 81 6
BNC 85 90 61 88 0
Russ. 89 95 70 93 11
triplet
score (1-4)
DMOZ 1.7 1.4 2.5 1.7 3.8
BNC 1.6 1.3 2.1 1.5 4.0
Russ. 1.5 1.1 2.0 1.3 3.7
Table 2: Results of evaluation by human judgment of
three data sets. P+ single/mwe: single-word/MWE terms
existing only in P+ concept; P: single-word terms existing
only in P concept; Both: terms existing in both concepts;
Rand: random terms. See text for detailed explanations.
The first part of Table 2 gives the average per-
centage of triplets that were given scores of 1 or 2
(that is, ?significant shared meaning?). The second
part gives the average score of a triplet (1 is best).
In these lines scores of 5 were not counted. Inter-
evaluator Kappa between scores are 0.68/0.75/0.76
for DMOZ, BNC and Russian respectively. We can
see that terms selected by P and skipped by P+
receive low scores, at the same time even single-
word terms selected by P+ and skipped by P show
very high scores. This shows that using parser data,
the proposed framework can successfully avoid se-
lection of erroneous terms, while discovering high-
quality terms missed by P. We can also see that P+
performance on MWEs, while being slightly infe-
rior to the one for single-word terms, still achieves
results comparable to those of single-word terms.
Thus our algorithm can greatly improve the re-
sults not only by discovering of MWEs but also by
improving the set of single word concept terms.
WordNet-based evaluation. The major guideline
in this part of the evaluation was to compare our re-
sults with previous work (Davidov and Rappoport,
2006; Widdows and Dorow, 2002) without the pos-
sible bias of human evaluation. We have followed
their methodology as best as we could, using the
same WordNet (WN) categories and the same cor-
pora. This also allows indirect comparison to several
other studies, thus (Widdows and Dorow, 2002) re-
ports results for an LSA-based clustering algorithm
that are vastly inferior to the pattern-based ones.
The evaluation method is as follows. We took
the exact 10 WN subsets referred to as ?subjects? in
(Widdows and Dorow, 2002), and removed all multi-
word items. We then selected at random 10 pairs of
words from each subject. For each pair, we found
the largest of our discovered concepts containing it.
The various morphological forms or clear typos of
the same word were treated as one in the evaluation.
We have improved the evaluation framework for
Russian by using the Russian WordNet (Gelfenbey-
nand et al, 2003) instead of back-translations as
done in (Davidov and Rappoport, 2006). Prelim-
inary examination shows that this has no apparent
effect on the results.
For each found concept C containing N words,
we computed the following: (1) Precision: the num-
54
ber of words present in both C and WN divided by
N ; (2) Precision*: the number of correct words di-
vided by N . Correct words are either words that
appear in the WN subtree, or words whose entry in
the American Heritage Dictionary or the Britannica
directly defines them as belonging to the given class
(e.g., ?murder? is defined as ?a crime?). This was
done in order to overcome the relative poorness of
WN; (3) Recall: the number of words present in
both C and WN divided by the number of words
in WN; (4) The percentage of correctly discovered
words (according to Precision*) that are not in WN.
Table 3 compares the macro-average of these 10
categories to corresponding related work. We do not
Prec. Prec.* Rec. %New
DMOZ
P 79.8 86.5 22.7 2.5
P+ 79.5 91.3 28.6 3.7
BNC
P 92.76 95.72 7.22 0.4
P+ 93.0 96.1 14.6 1.7
Widdows 82.0 - - -
Russian
P 82.39 89.64 20.03 2.1
P+ 83.5 92.6 29.6 4.0
Table 3: WordNet evaluation in comparison to P (Davi-
dov and Rappoport, 2006) and to Widdows(Widdows and
Dorow, 2002). Columns show average precision, preci-
sion* (as defined in text), recall, and % of new words
added to corresponding WN subtree.
observe apparent rise in precision when comparing
P+ and P, but we can see significant improvement
in both recall and precision* for all of three cor-
pora. In combination with human judgement results,
this suggests that the P+ framework successfully dis-
covers more correct terms not present in WN. This
causes precision to remain constant while precision*
improves significantly. Rise in recall also shows that
the P+ framework can discover significantly more
correct terms from the same data.
Windowing requirement. As discussed in Sec-
tion 5, windowing is required for successful noise
reduction. However, due to the increase in pattern
quality with parser data, it is likely that less noise
will be captured by the discovered patterns. Hence,
windowing could be relaxed allowing to obtain more
data with sufficiently high precision.
In order to test this issue we applied our algo-
rithms on the DMOZ corpus with 3 different win-
dowing settings: (1) choosing window size as de-
scribed above; (2) using ?4 larger window; (3)
avoiding windowing altogether. Each time we ran-
domly sampled a set of 100 concepts and tagged (by
the authors) noisy ones. A concept is considered to
be noisy if it has at least 3 words unrelated to each
other. Table 4 shows results of this test.
Reg. Window ?4 Window No windowing
P 4 18 33
P+ 4 5 21
Table 4: Percentage of noisy concepts as a function of
windowing.
We can see that while windowing is still essential
even with available parser data, using this data we
can significantly reduce windowing requirements,
allowing us to discover more concepts from the
same data.
Timing requirements are modest, considering we
parsed such large amounts of data. BNC pars-
ing took 45 minutes, and the total single-machine
processing time for the 68Gb DMOZ corpus was
4 days6. In comparison, a state-of-art supervised
parser (Charniak and Johnson, 2005) would process
the same amount of data in 1.3 years7.
7 Discussion
We have presented a framework which utilizes an
efficient fully unsupervised parser for unsupervised
pattern-based discovery of concepts. We showed
that utilization of unsupervised parser in pattern ac-
quisition not only allows successful extraction of
MWEs but also improves the quality of obtained
concepts, avoiding noise and adding new terms
missed by the parse-less approach. At the same time,
the framework remains fully unsupervised, allowing
its straightforward application to different languages
as supported by our bilingual evaluation.
This research presents one more step towards the
merging of fully unsupervised techniques for lex-
ical acquisition, allowing to extract semantic data
without strong assumptions on domain or language.
While we have aimed for concept acquisition, the
proposed framework can be also useful for extrac-
tion of different types of lexical relationships, both
among concepts and between concept terms.
6In fact, we used a PC cluster, and all 3 corpora were parsed
in 15 hours.
7Considering the reported parsing rate of 10 sentences per
second
55
References
Mishele Banko, Michael J Cafarella , Stephen Soderland,
Matt Broadhead, Oren Etzioni, 2007. Open Informa-
tion Extraction from the Web. IJCAI ?07.
Rens Bod, 2006a. An All-Subtrees Approach to Unsu-
pervised Parsing. ACL ?06.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
CoNLL X.
Rens Bod, 2007. Is the End of Supervised Parsing in
Sight? ACL ?07.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Sharon Caraballo, 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. ACL
?99.
Eugene Charniak and Mark Johnson, 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. ACL ?05.
Alexander Clark, 2001. Unsupervised Language Acqui-
sition: Theory and Practice. Ph.D. thesis, University
of Sussex.
James R. Curran, Marc Moens, 2002. Improvements in
Automatic Thesaurus Extraction SIGLEX 02?, 59?66.
Dmitry Davidov, Ari Rappoport, 2006. Efficient Un-
supervised Discovery of Word Categories using Sym-
metric Patterns and High Frequency Words. COLING-
ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2009. Translation and
Extension of Concepts Across Languages. EACL ?09.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. Index-
ing by Latent Semantic Analysis. J. of the American
Society for Info. Science, 41(6):391?407.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in Graphs for
Lexical Acquisition and Word Sense Discrimination.
MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey, S. Kok,
Ana-Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel Weld, Alexander Yates, 2005. Unsupervised
Named-entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91134.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture Generation for Text Categorization Using World
Knowledge. IJCAI ?05.
Ilya Gelfenbeyn, Artem Goncharuk, Vladislav Lehelt,
Anton Lipatov, Victor Shilo, 2003. Automatic Trans-
lation of WordNet Semantic Network to Russian Lan-
guage (in Russian) International Dialog 2003 Work-
shop.
James Gorman, James R. Curran, 2006. Scaling Distri-
butional Similarity to Large Corpora. COLING-ACL
?06.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech tagging for Grammar Induction. COLING ?08.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Adam Kilgarriff, 2007. Googleology is Bad Science.
Computational Linguistics ?08, Vol.33 No. 1,pp147-
151. .
Dan Klein and Christopher Manning, 2002. A genera-
tive constituent-context model for improved grammar
induction. Proc. of the 40th Meeting of the ACL.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. ACL ?04.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Hang Li, Naoki Abe, 1996. Clustering Words with the
MDL Principle. COLING ?96.
Dekang Lin, 1998. Automatic Retrieval and Clustering
of Similar Words. COLING ?98.
Marcus Mitchell P., Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
supervised Acquisition of Open-domain Classes and
Class Attributes from Web Documents and Query
Logs. ACL 08.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993. Dis-
tributional Clustering of English Words. ACL ?93.
Hinrich Schu?tze, 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics , 24(1):97?123.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2006. Annealing Struc-
tural Bias in Multilingual Weighted Grammar Induc-
tion . ACL ?06.
Dominic Widdows, Beate Dorow, 2002. A Graph Model
for Unsupervised Lexical Acquisition. COLING ?02.
56
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156?164,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Selection of High Quality Parses Created By a Fully
Unsupervised Parser
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
The average results obtained by unsupervised
statistical parsers have greatly improved in the
last few years, but on many specific sentences
they are of rather low quality. The output of
such parsers is becoming valuable for vari-
ous applications, and it is radically less expen-
sive to create than manually annotated training
data. Hence, automatic selection of high qual-
ity parses created by unsupervised parsers is
an important problem.
In this paper we present PUPA, a POS-based
Unsupervised Parse Assessment algorithm.
The algorithm assesses the quality of a parse
tree using POS sequence statistics collected
from a batch of parsed sentences. We eval-
uate the algorithm by using an unsupervised
POS tagger and an unsupervised parser, se-
lecting high quality parsed sentences from En-
glish (WSJ) and German (NEGRA) corpora.
We show that PUPA outperforms the leading
previous parse assessment algorithm for su-
pervised parsers, as well as a strong unsuper-
vised baseline. Consequently, PUPA allows
obtaining high quality parses without any hu-
man involvement.
1 Introduction
In unsupervised parsing an algorithm should un-
cover the syntactic structure of an input sentence
without using any manually created structural train-
ing data. The last decade has seen significant
progress in this field of research (Klein and Man-
ning, 2002; Klein and Manning, 2004; Bod, 2006a;
Bod, 2006b; Smith and Eisner, 2006; Seginer,
2007).
Many NLP systems use the output of supervised
parsers (e.g., (Kwok et al, 2001) for QA, (Moldovan
et al, 2003) for IE, (Punyakanok et al, 2008) for
SRL, (Srikumar et al, 2008) for Textual Inference
and (Avramidis and Koehn, 2008) for MT). To
achieve good performance, these parsers should be
trained on large amounts of manually created train-
ing data from a domain similar to that of the sen-
tences they parse (Lease and Charniak, 2005; Mc-
Closky and Charniak, 2008). In the highly variable
Web, where many of these systems are used, it is
very difficult to create a representative corpus for
manual annotation. The high cost of manual annota-
tion of training data for supervised parsers imposes
a significant burden on their usage.
A possible answer to this problem can be pro-
vided by high quality parses produced by unsuper-
vised parsers that require little to no manual efforts
for their training. These parses can be used either
as input for applications, or as training material for
modern supervised parsers whose output will in turn
be used by applications.
Although unsupervised parser results improve,
the quality of many of the parses they produce is still
too low for such goals. For example, the Seginer
(2007) parser achieves an F-score of 75.9% on the
WSJ10 corpus and 59% on the NEGRA10 corpus,
but the percentage of individual sentences with an
F-score of 100% is 21.5% for WSJ10 and 11% for
NEGRA10. When requirements are relaxed, only
asking for an F-score higher than 85%, percentage
is still low, 42% for WSJ10 and 15% for NEGRA10.
In this paper we address the task of a fully un-
supervised assessment of high quality parses cre-
156
ated by an unsupervised parser. The assessment
should be unsupervised in order to avoid the prob-
lems mentioned above with manually trained super-
vised parsers. Assessing the quality of a learning al-
gorithm?s output and selecting high quality instances
has been addressed for supervised algorithms (Caru-
ana and Niculescu-Mizil, 2006) and specifically for
supervised parsers (Yates et al, 2006; Reichart and
Rappoport, 2007; Kawahara and Uchimoto, 2008;
Ravi et al, 2008). Moreover, it has been shown
to be valuable for supervised parser adaptation be-
tween domains (Sagae and Tsujii, 2007; Kawahara
and Uchimoto, 2008; Chen et al, 2008). However,
as far as we know the present paper is the first to
address the task of unsupervised assessment of the
quality of parses created by unsupervised parsers.
Our POS-based Unsupervised Parse Assessment
(PUPA) algorithm uses statistics about POS tag se-
quences in a batch of parsed sentences1. The con-
stituents in the batch are represented using the POS
sequences of their yield and of the yields of neigh-
boring constituents. Constituents whose representa-
tion is frequent in the output of the parser are con-
sidered to be of a high quality. A score for each
range of constituent length is calculated, reflecting
the robustness of statistics used for the creation of
the constituents of that length. The final sentence
score is a weighted average of the scores calculated
for each constituent length. The score thus integrates
the quality of short and long constituents into one
score reflecting the quality of the whole parse tree.
PUPA provides a quality score for every sentence
in a parsed sentences set. An NLP application can
then decide if to use a parse or not, according to
its own definition of a high quality parse. For ex-
ample, it can select every sentence whose score is
above some threshold, or the k top scored sentences.
The selection strategy is application dependent and
is beyond the scope of this paper.
The unsupervised parser we use is the Seginer
(2007) incremental parser2, which achieves state-of-
1The algorithm can be used with supervised POS taggers
and parsers, but we focus here on the fully unsupervised sce-
nario, which is novel and more useful. For completeness of
analysis, we experimented with PUPA using a supervised POS
tagger (see Section 5). Using PUPA with supervised parsers is
left for future work.
2www.seggu.net/ccl.
the-art results without using manually created POS
tags. The POS tags we use are induced by the un-
supervised tagger of (Clark, 2003)3. Since both tag-
ger and parser do not require any manual annotation,
PUPA identifies high quality parses without any hu-
man involvement.
The incremental parser of (Seginer, 2007) does
not give any prediction of its output quality, and
extracting such a prediction from its internal data
structures is not straightforward. Such a predic-
tion can be given by supervised parsers in terms
of the parse likelihood, but this was shown to be
of medium quality (Reichart and Rappoport, 2007).
While the algorithms of Yates et al (2006), Kawa-
hara and Uchimoto (2008) and Ravi et al (2008) are
supervised (Section 3), the ensemble based SEPA al-
gorithm (Reichart and Rappoport, 2007) can be ap-
plied to unsupervised parsers in a way that preserves
the unsupervised nature of the selection task.
To compare between two algorithms, we use each
of them to assess the quality of the sentences in En-
glish and German corpora (WSJ and NEGRA)4. We
show that for every sentence length (up to 20) the
quality of the top scored k sentences according to
PUPA is higher than the quality of SEPA?s list (for
every k). As in (Reichart and Rappoport, 2007), the
quality of a set selected from the parser?s output is
evaluated using two measures: constituent F-score5
and average sentence F-score.
Section 2 describes the PUPA algorithm, Sec-
tion 3 discusses previous work, and Sections 4 and
5 present the evaluation setup and results.
2 The POS-based Unsupervised Parse
Assessment (PUPA) Algorithm
In this section we detail our parse assessment algo-
rithm. Its input consists of a set I of parsed sen-
tences, which in our evaluation scenario are pro-
duced by an unsupervised parser. The algorithm
assigns each parsed sentence a score reflecting its
quality.
3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html,
the neyessenmorph model.
4This is in contrast to algorithms for selection from the re-
sults of supervised constituency parsers, which were evaluated
only for English (Yates et al, 2006; Reichart and Rappoport,
2007; Ravi et al, 2008).
5This is the traditional parsing F-score.
157
The algorithm has three steps. First, the words in
I are POS tagged (in our case, using the fully unsu-
pervised POS induction algorithm of Clark (2003)).
Second, POS statistics about the constituents in I
are collected. Finally, a quality score is calculated
for each parsed sentence in I using the POS statis-
tics. In the following we detail the last two steps.
Collecting POS statistics. In its second step, the
algorithm collects statistics about the constituents in
the input set I . Recall that the yield of a constituent
is the set of words covered by it. The PUPA con-
stituent representation (PCR) consists of three fea-
tures: (1) the ordered POS tag sequence of the con-
stituent?s yield, (2) the constituents? right context,
and (3) the constituents? left context.
We define context to be the leftmost and rightmost
POS tags in the yield of the neighbor of the con-
stituent (if there is only one POS tag in the neigh-
bor?s yield, this POS tag is the context). For the
right and left contexts we consider the right and left
neighbors respectively. A constituent C1 is the right
neighbor of a constituent C2 if C1 is the highest level
constituent such that the first word in the yield of C1
comes immediately after the last word in the yield of
C2. A constituent C1 is the left neighbor of a con-
stituent C2 if C1 is the highest level constituent such
that the first word in the yield of C2 comes immedi-
ately after the last word in the yield of C1.
Figure 1 shows an example, an unlabeled tree for
the sentence ?I will give you the ball?. The tree has
6 constituents (C0-C5). C3 and C4 have both right
and left neighbors. For C3, the POS sequence of its
yield is POS2, POS3 , the left neighbor is C1 and thus
the left context is POS1, and the right neighbor is C4
and thus the right context is POS4. Note that the
left and right neighbors of C3 have only one POS
tag in their yield and therefore this POS tag is the
context. For C4 the yield is POS4, the left neighbor
is C3 (and thus the left context is POS2,POS3), and
the right neighbor is C5 (and thus the right context
is POS5,POS6). C1, whose yield is POS1, has only
a right neighbor, C2, and thus its right context is
POS2,POS6 and its left context is NULL. C2 and C5
(whose yields are POS2, POS3, POS4, POS5, POS6 for
C2 and POS5, POS6 for C5) have only a left neigh-
bor. For C2, this is C1 (and the context is POS1)
while for C5 this is C4 (with the context POS4).
0
1
POS1
I
2
3
POS2
will
POS3
give
4
POS4
you
5
POS5
the
POS6
ball
Figure 1: An example parse tree for contexts and neigh-
bors (see text).
The right context of both constituents is NULL. As
all sentence level constituents, C0 has no neighbors,
and thus both its left and right contexts are NULL.
We have also explored other representations of
left and right contexts based on the POS tags of their
yields. In these, we represented the left/right neigh-
bor using only the leftmost/rightmost POS tags of
its yield or other subsets of the yield?s POS tags.
These variations produced lower quality results than
the main variant above in our experiments, which
were for English and German. Exploring the suit-
ability of our representation for other languages is
left for future research.
Score computation. The third and last step of the
algorithm is a second pass over I for computing a
quality score for each parse tree.
Short constituents tend to be more frequent than
long ones. In order not to distort our score due to
parsing errors in short constituents, PUPA computes
the grade using a division into lengths, in three steps.
First, constituents are assigned to bins according to
their length, each bin containing the constituents of
a certain range of lengths. Denote this range by
W (for width), and the number of bins by N(W ).
For example, in our experiments the longest possible
constituent is of length 20, so we can take W = 5,
resulting in N(W ) = 4: bin 1 for constituents of
length 1-5, bin 2 for constituents of length 6-10, and
so on for bins 3, 4.
The score of bini is given by
(1) BinScore(bini) =?t=Xt=2 (X ? t + 2) ? |C
i
t |
|Ci|
Where X is the maximal number of occurrences
of constituents in the bin that we consider as impor-
tant for the score (see below for its selection), |Cit |
is the number of constituents in bin i occurring at
158
least t times in the batch of parsed sentences, and
|Ci| is the number of constituents in bin i. In words,
the score is a weighted average: the fraction of the
constituents in the bin occuring at least 2 times (with
weight X), plus the fraction of the constituents in the
bin occuring at least 3 times (with weight X ? 1),
etc, until the fraction of the constituents in the bin
occuring at least X times (with weight 2).
A score for the division into N bins is given by
(2) Score(N(W )) =
?N(W )
i=1 BinScore(bini)
Z?M
Where Z is the maximum bin score (according to
(1)) and M is the number of bins containing at least
one constituent. If, for example, N(W ) = 4 and
there is no constituent whose length is between 11
and 15 then bin number 3 is empty. If every other
bin contains at least one constituent, M = 3.
To get a final score for the parse tree of sentence
S that is independent of a specific bin division, we
sum the scores of the various bin division:
(3) PupaScore(S) =
?W=Y
W=1 Score(N(W ))
Y
where Y is the length of S (which is also its max-
imum bin width). PupaScore thus takes values in
the [0, 1] range.
In equation (1), if, for example, X = 20 then
the weight of the fraction of the bin?s constituents
occurring at least 2 times is 20 while the weight of
the fraction of the constituents occurring at least 10
times is 12 and of the fraction of constituents occur-
ring at least 20 times is 2. We consider the number
of times a constituent appears in a batch to be an in-
dication of its correctness. The difference between 3
and 2 occurrences is therefore more indicative than
the difference between 20 and 19 occurrences. More
generally, the more times a constituent occurs, the
less indicative any additional appearance is.
In equation (2) we give all bins the same weight.
Short constituents are more frequent and are gener-
ally more likely to be correct. However, the cor-
rectness of long constituents is an indication that the
parser has a correct interpretation of the tree struc-
ture and that it is likely to create a high quality tree.
The usage of equal bin weights was done to balance
the tendency of parse trees to have more short con-
stituents.
Parameters. PUPA has two parameters: X , the
maximal number of occurrences considered in equa-
tion (1), and P , the number of POS tags induced by
the unsupervised POS tagger. In the following we
present the unsupervised technique we used to tune
these parameters.
Figure 2 shows nc(t), the number of constituents
appearing at least t times in WSJ20 (left) and NE-
GRA20 (right). For both corpora, the pattern is
shown when using 5 POS tags (P = 5, solid line)
and 50 POS tags (P = 50, dashed line). The distri-
bution obeys Zipf?s law: many constituents appear a
small number of times while a few constituents ap-
pear a large number of times. We denote the t value
where the slope changes from steep to moderate by
telbow. Practically, we approximate the ?real? elbow
value and define telbow to be the smallest t for which
nc(t + 1) ? nc(t) = 1. When P = 5, telbow is 32
for WSJ and 19 for NEGRA. When P = 50, telbow is
15 for WSJ and 9 for NEGRA.
The number of constituents appearing more than
telbow times is considerably smaller than the number
of constituents appearing telbow times or less. There-
fore, the fact that a constituent appears telbow + S
times (for a positive integer S) is not a better indica-
tion of its quality than the fact that it appears telbow
times. We thus select X to be telbow.
The graphs also demonstrate that for both cor-
pora, telbow for P = 50 is smaller than telbow for
P = 5. Generally, telbow is a monotonically decreas-
ing function of P . Lower telbow values imply that
PUPA would be less distinctive between constituents
quality (see equation (1); recall that X = telbow).
We thus want to select the P value that maximizes
telbow. We therefore minimize P . telbow values for
P ? {3, . . . , 10} are very similar. Indeed, PUPA
achieves its best performance for P ? {3, . . . , 10}
and it is insensitive to the selection of P in this
range. In Section 5 we report results with P = 5.
3 Related Work
Unsupervised parsing has been explored for several
decades (see (Klein, 2005) for a recent review). Re-
cently, unsupervised parsing algorithms have for the
first time outperformed the right branching heuristic
baseline for English. These include CCM (Klein and
Manning, 2002), the DMV and DMV+CCM models
(Klein and Manning, 2004), (U)DOP based mod-
159
0 50 100
0
5000
10000
15000
t
# o
f co
nst
itue
nts
 ap
pea
ring
 at 
lea
st t
 tim
es
 
 
0 50 100
0
1000
2000
3000
4000
5000
6000
7000
8000
t
 
 
P = 5
P = 50
P = 5
P = 50
Figure 2: Number of constituents appearing at least t
times (nc(t)) as a function of t. Shown are WSJ (left)
and NEGRA (right), where constituents are represented
according to PUPA?s PCR with 5 POS tags (P = 5, solid
line) or 50 POS tags (P = 50, dashed line).
els (Bod, 2006a; Bod, 2006b), an exemplar based
approach (Dennis, 2005), guiding EM using con-
trastive estimation (Smith and Eisner, 2006), and the
incremental parser of Seginer (2007) that we use in
this work. To obtain good results, manually created
POS tags are used as input in all of these algorithms
except Seginer?s, which uses plain text.
Quality assessment of a learning algorithm?s out-
put and selection of high quality instances have been
addressed for supervised algorithms (see (Caruana
and Niculescu-Mizil, 2006) for a survey) and specif-
ically for supervised constituency parsers (Yates et
al., 2006; Reichart and Rappoport, 2007; Ravi et al,
2008). For dependency parsing in a corpus adapta-
tion scenario, (Kawahara and Uchimoto, 2008) built
a binary classifier that classifies each parse in the
parser?s output as reliable or not. To do that, they
selected 2500 sentences from the parser?s output,
compared them to their manually created gold stan-
dard, and used accurate (inaccurate) parses as posi-
tive (negative) examples for the classifier. Their ap-
proach is supervised and the features used by the
classifier are dependency motivated .
As far as we know, the present paper is the first to
address the task of selecting high quality parses from
the output of unsupervised parsers. The algorithms
of Yates et al (2006), Kawahara and Uchimoto
(2008) and Ravi et al (2008) are supervised, per-
forming semantic analysis of the parse tree and gold
standard-based calssification, respectively. How-
ever, the SEPA algorithm of Reichart and Rappoport
(2007), an algorithm for supervised constituency
parsers, can be applied to unsupervised parsers in
a way that preserves the unsupervised nature of the
selection task. In Section 5 we provide a detailed
comparison between PUPA and SEPA showing the
first to be superior. Below is a brief description of
the SEPA algorithm.
The input of the SEPA algorithm consists of a
parsing algorithm A, a training set, and a test set
(which in the unsupervised case might be the same
set). The algorithm provides, for each of the test
set?s parses generated by A when trained on the full
training set, a grade assessing the parse quality, on
a continuous scale between 0 to 100. The qual-
ity grade is calculated in the following way: N ran-
dom samples of size S are sampled from the train-
ing data and used for training the parsing algorithm
A. In that way N committee members are created.
Then, each of the test sentences is parsed by each of
the N committee members and an agreement score
ranging from 0 to 100 between the committee mem-
bers is calculated. All unsupervised parsers men-
tioned above (including the Seginer parser), have a
training phase where parameter values are estimated
from unlabeled data. SEPA can thus be applied to the
unsupervised case.
Automatic selection of high quality parses has
been shown to improve parser adaptation. Sagae and
Tsujii (2007) and Kawahara and Uchimoto (2008)
applied a self-training protocol to a parser adaptation
scenario but used only high quality parses to retrain
the parser. In the first work, high quality parses were
selected using an ensemble method, while in the sec-
ond a binary classifier was used (see above). The
first system achieved the highest score in the CoNLL
2007 shared task on domain adaptation of depen-
dency parsers, and the second system improved over
the basic self-training protocol. Chen et al (2008)
parsed target domain sentences and used short de-
pendencies information, which is often accurate, to
adapt a dependency parser to the Chinese language.
Automatic quality assessment has been exten-
sively explored for machine translation (Ueffing and
Ney, 2007) and speech recognition (Koo et al,
2001). Other NLP tasks where it has been explored
include semi-supervised relation extraction (Rosen-
feld and Feldman, 2007), IE (Culotta and McCal-
lum, 2004), QA (Chu-Carroll et al, 2003), and dia-
log systems (Lin and Weng, 2008).
The idea of representing a constituent by its yield
160
and (a different definition of) context is used by the
CCM unsupervised parsing model (Klein and Man-
ning, 2002). As far as we know the current work is
the first to use unsupervised POS tags for the selec-
tion of high quality parses.
4 Evaluation Setup
We experiment with sentences of up to 20 words
from the English WSJ Penn Treebank (WSJ20,
25236 sentences, 225126 constituents) and the Ger-
man NEGRA corpus (Brants, 1997) (NEGRA20,
15610 sentences, 108540 constiteunts), both con-
taining newspaper texts.
The unsupervised parsers of the kind addressed
in this paper output unlabeled parse trees. To eval-
uate the quality of a single parse tree with respect
to another, we use the unlabeled F-score (UF =
2?UR?UP
UR+UP ), where UR and UP are unlabeled recall
and unlabeled precision respectively.
Following the unsupervised parsing literature,
multiple brackets and brackets covering a single
word are not counted, but the sentence level bracket
is. We exclude punctuation and null elements ac-
cording to the scheme of (Klein, 2005).
The performance of unsupervised parsers
markedly degrades as sentence length increases.
For example, the Average sentence F?score for WSJ
sentences of length 10 is 71.4% compared to 58.5
for sentences of length 20 (the numbers for NEGRA
are 48.2% and 36.9%). We therefore evaluate PUPA
(and the baseline) for sentences of a given length.
We do this for every sentence of length 2-20 in
WSJ20 and NEGRA20.
For every sentence length L, we use PUPA and the
baseline algorithm (SEPA) to give a quality score to
each of the sentences of that length in the experi-
mental corpus. We then compare the quality of the
top k parsed sentences according to each algorithm.
We do this for every k from 1 to the number of sen-
tences of length L.
Following Reichart and Rappoport (2007), we use
two measures to evaluate the quality of a set of
parses: the constituent F-score (the traditional F-
score used in the parsing literature), and the average
F-score of the parses in the set. In the first mea-
sure we treat the whole set as a bag of constituents.
Each constituent is marked as correct (if it appears
in the gold standard parses of the set) or erroneous
(if it does not). Then, recall, precision and F-score
are calculated over these constituents. In the sec-
ond measure, the constituent F-score of each of the
parses in the set is computed, and then results are
averaged.
There are applications that use individual con-
stituents from the output of a parser while others
need the whole parse tree. For example, if the se-
lected set is used for training supervised parsers such
as the Collins parser (Collins, 1999), which collects
constituent statistics, the constituent F-score of the
selected set is the important measure. In applica-
tions such as the syntax based machine translation
model of (Yamada and Knight, 2001), a low qual-
ity tree might lead to errorenous translation of the
sentence. For such applications the average F-score
is more indicative. These measures thus represent
complementary aspects of a set quality and we con-
sider both of them.
The parser we use is the incremental parser of
(Seginer, 2007), POS tags are induced using the un-
supervised POS tagger of ((Clark, 2003), neyessen-
morph model). In each experiment, the tagger was
trained with the raw sentences of the experiment cor-
pus, and then the corpus words were POS tagged.
The output of the unsupervised POS tagger de-
pends on a random initialization. We ran the tagger
5 times, each time with a different random initializa-
tion, and then ran PUPA with its output. The results
we report for PUPA are the average over these 5 runs.
Random selection results (given for reference) were
also averages over 5 samples.
PUPA ?s parameter estimation is completely unsu-
pervised (see Section 2). No development data was
used to tune its parameters.
A 200 sentences development set from each cor-
pus was used for calibrating the parameters of the
SEPA algorithm. Based on the analysis of SEPA per-
formance with different assignments of its param-
eters given by Reichart and Rappoport (2007) (see
Section 3), we ran the SEPA algorithm with sam-
ple size (SEPA parameter S) of 30% and 80%, and
with 2 ? 10 committee members (N )6. The optimal
parameters were N = 10,S = 80 for WSJ20, and
6We tried higherN values but observed no improvements in
SEPA?s performance.
161
0 200 400 60070
75
80
85
90
95
100
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(a) WSJ, length 5
0 500 1000
60
65
70
75
80
85
90
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(b) WSJ, length 10
0 500 1000 1500
50
55
60
65
70
75
80
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(c) WSJ, length 15
0 500 1000 1500 2000
48
50
52
54
56
58
60
62
64
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(d) WSJ, length 20
0 200 400 600 800
50
55
60
65
70
75
80
Number of Sentneces
A
ve
ra
ge
 F
 S
co
re
(e) NEGRA, length5
0 200 400 600 800 1000
40
45
50
55
60
65
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(f) NEGRA, length 10
0 200 400 600 800
30
35
40
45
50
55
60
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(g) NEGRA, length 15
0 200 400 600 800
50
55
60
65
70
75
80
Number of Sentneces
A
ve
ra
ge
 F
 S
co
re
(h) NEGRA, length 20
0 500 1000 1500 2000
70
75
80
85
90
95
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(i) WSJ, length 5
0 2000 4000 6000 8000 10000
60
65
70
75
80
85
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(j) WSJ, length 10
0 0.5 1 1.5 2
x 104
55
60
65
70
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(k) WSJ, length 15
0 0.5 1 1.5 2 2.5
x 104
54
56
58
60
62
64
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(l) WSJ, length 20
0 500 1000 1500 2000 2500 3000
62
63
64
65
66
67
68
69
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(m) NEGRA, length5
0 1000 2000 3000 4000 5000
44
46
48
50
52
54
56
58
Number of Constituents
C
on
st
itu
en
t F
 S
co
re
(n) NEGRA, length 10
0 2000 4000 6000 8000
40
42
44
46
48
50
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(o) NEGRA, length 15
0 500 1000 1500 2000 2500 3000
62
63
64
65
66
67
68
69
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(p) NEGRA, length 20
Figure 3: In all graphs: PUPA: solid line. SEPA: line with triangles. MC: line with circles. Random selection is
presented for reference as a dotted line. Top two rows: Average F-score for PUPA, SEPA and MC for sentences from
WSJ (top row) and NEGRA (bottom row). Bottom two rows: Constituents F-score for PUPA, SEPA and MC for
sentences from WSJ (top row) and NEGRA (bottom row). Results are presented for sentence lengths of 5,10,15 and
20 (patterns for other sentence lengths between 2 and 20 are very similar). PUPA is superior in all cases. The graphs
for PUPA and SEPA show a downward trend because parsed sentences were sorted according to score, which correlates
positively with F-score (unlike MC). The graphs converge because on the extreme right all test sentences were selected.
N = 10, S = 30 for NEGRA20.
We also compare PUPA to a baseline selecting the
sentences with the lowest number of constituents.
Since the number of constituents is an indication of
the complexity of the syntactic structure of a sen-
tence, it is reasonable to assume that selecting the
sentences with the lowest number of constituents is
a good selection strategy. We denote this baseline by
MC (for minimum constituents).
The incremental parser does not give any predic-
tion of its output quality as supervised generative
parsers do. We are thus not able to compare to such
a score.
5 Results
Figure 3 shows Average F-score and Constituents F-
score results for PUPA SEPA and MC, for sentences
162
of lengths 5,10,15 and 20 in WSJ20 and NEGRA20.
The top two rows are for Average F-score (top row:
WSJ, bottom row: NEGRA), while the bottom two
rows are for Constituents F-score (top row: WSJ,
bottom row: NEGRA).
PUPA and SEPA are both better than random selec-
tion for both corpora for every sentence length. The
MC baseline is better than random selection only for
NEGRA (in which case it outperforms SEPA). For
WSJ, however, random selection is a better strategy
than MC.
It is clear from the graphs that PUPA outperforms
SEPA and MC in all experimental conditions. We
observed very similar patterns in all other sentence
lengths in WSJ20 and NEGRA20 for both Average
F-score and Constituent F-score. In other words, for
every sentence length in both corpora, PUPA outper-
forms SEPA and MC in terms of both measures. we
present our results per sentence length to deprive the
possibility that PUPA is useful only for short sen-
tences or that it prefers sentences whose syntactic
structure is not complex (i.e. with a small number of
constituents, like MC).
Table 1 shows that the same pattern of results
holds when evaluating on the whole corpus (WSJ20
or NEGRA20) without any sentence length restric-
tion.
Note that while PUPA is a fully unsupervised al-
gorithm, SEPA requires a few hundreds of sentences
for its parameters tuning.
The main result of this paper is for sentences
whose length is up to 20 words (note that most un-
supervised parser literature reports numbers for sen-
tences up to length 10). We have also ran the exper-
iments for the remaining length range, 20-40. For
NEGRA, PUPA is superior over MC up to length 36,
and both are much better than SEPA. For WSJ, PUPA
and SEPA both outperform MC, but SEPA is a bit bet-
ter than PUPA. When evaluating on the whole corpus
(i.e. without sentence length restriction, like in Ta-
ble 1) PUPA is superior over both SEPA and MC for
WSJ40 and NEGRA40.
For completeness of analysis we also experi-
mented in the condition where PUPA uses gold stan-
dard POS tags as input. The number of these tags is
35 for WSJ and 57 for NEGRA. Interestingly, PUPA
achieves in this condition the same performance as
when using the same number of POS tags induced
by an unsupervised POS tagger. Since PUPA?s per-
formance for a smaller number of POS tags is better
(see our parameter tuning discussion above), the bot-
tom line is that PUPA pefers using induced POS tags
over gold POS tags.
5% 10% 20% 30% 40% 50%
WSJ20
PUPA 82.75 79.34 75.77 73.46 71.68 70.3
SEPA 78.68 75.7 72.64 70.72 69.54 68.58
MC 76.75 74.6 72.1 70.35 68.97 67.77
NEGRA20
PUPA 70.66 67.06 61.89 58.75 56.6 54.73
SEPA 66.19 62.75 59.41 57.16 55.23 53.7
MC 69.41 65.79 60.87 58.08 55.9 54.36
Table 1: Average F?score for the top k% of constituents
selected from WSJ20 (up) and NEGRA20 (down). No sen-
tence length restriction is imposed. Results presented for
PUPA , SEPA and MC. Average F?score of random se-
lection is 66.55 (WSJ20) and 47.05 (NEGRA20). PUPA is
superior over all methods.
6 Conclusions
We introduced PUPA, an algorithm for unsupervised
parse assessment that utilizes POS sequence statis-
tics. PUPA is a fully unsupervised algorithm whose
parameters can be tuned in an unsupervised man-
ner. Experimenting with the Seginer unsupervised
parser and Clark?s unsupervised POS tagger on En-
glish and German corpora, PUPA was shown to out-
perform both the leading parse assessment algorithm
for supervised parsers (SEPA, even when its param-
eters are tuned on manually annotated development
data) and a strong baseline (MC).
Using PUPA, we extracted high quality parses
from the output of a parser which requires raw text
as input, using POS tags induced by an unsupervised
tagger. PUPA thus provides a way of obtaining high
quality parses without any human involvement.
For future work, we intend to use parses selected
by PUPA from the output of unsupervised parsers
as training data for supervised parsers, and in NLP
applications that use parse trees. A challenge for
the first direction is the fact that state of the art su-
pervised parsers require labeled parse trees, while
modern unsupervised parsers create unlabeled trees.
Combining PUPA with algorithms for labeled parse
trees induction (Haghighi and Klein, 2006; Reichart
and Rappoport, 2008) is a one direction to overcome
this challenge. We also intend to use PUPA to assess
the quality of parses created by supervised parsers.
163
References
Eleftherios Avramidis and Philipp Koehn, 2008. En-
riching Morphologically Poor Languages for Statisti-
cal Machine Translation. ACL ?08.
Rens Bod, 2006a. An All-Subtrees Approach to Unsu-
pervised Parsing. ACL ?06.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
CoNLL X.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Rich Caruana and Alexandru Niculescu-Mizil, 2006. An
Empirical Comparison of Supervised Learning Algo-
rithms. ICML ?06.
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager and
Abraham Ittycheriah, 2003. In Question Answering,
Two Heads Are Better Than One. HLT-NAACL ?03.
Wenliang Chen, Youzheng Wu and Hitoshi Isahara,
2008. Learning Reliable Information for Dependency
Parsing Adaptation. Coling ?08.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech Induc-
tion. EACL ?03.
Michael Collins, 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Aron Culotta and Andrew McCallum, 2004. Confidence
Estimation for Information Extraction. HLT-NAACL
?04.
Simon Dennis, 2005. An Exemplar-based Approach to
Unsupervised Parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Grammar Induction. ACL ?06.
Daisuke Kawahara and Kiyotaka Uchimoto 2008.
Learning Reliability of Parses for Domain Adaptation
of Dependency Parsing. IJCNLP ?08.
Dan Klein and Christopher Manning, 2002. A Gener-
ative Constituent-Context Model for Improved Gram-
mar Induction. ACL ?02.
Dan Klein and Christopher Manning, 2004. Corpus-
based Induction of Syntactic Structure: Models of De-
pendency and Constituency. ACL ?04.
Dan Klein, 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford University.
Myoung?Wan Koo, Chin-Hui Lee and Biing?Hwang
Juang 2001. Speech Recognition and Utterance Ver-
ification Based on a Generalized Confidence Score.
IEEE Transactions on Speech and Audio Processing,
9(8):821?832.
Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.
Scaling Question Answering to the Web. WWW ?01.
Matthew Lease and Eugene Charniak, 2005. Towards a
Syntactic Account of Punctuation. IJCNLP ?05.
Feng Lin and Fuliang Weng, 2008. Computing Confi-
dence Scores for All Sub Parse Trees. ACL ?08, short
paper.
David McClosky and Eugene Charniak, 2008. Self-
Training for Biomedical Parsing. ACL ?08, short pa-
per.
Dan Moldovan, Christine Clark, Sanda Harabagiu and
Steve Maiorano, 2003. Cogex: A Logic Prover for
Question Answering. HLT-NAACL ?03.
Vasin Punyakanok and Dan Roth and Wen-tau Yih, 2008.
The Importance of Syntactic Parsing and Inference in
Semantic Role Labeling. Computational Linguistics,
34(2):257-287.
Sujith Ravi, Kevin Knight and Radu Soricut, 2008. Au-
tomatic Prediction of Parser Accuracy. EMNLP ?08.
Roi Reichart and Ari Rappoport, 2007. An Ensemble
Method for Selection of High Quality Parses. ACL
?07.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Benjamin Rosenfeld and Ronen Feldman, 2007. Us-
ing Corpus Statistics on Entities to Improve Semi?
Supervised Relation Extraction From The WEB. ACL
?07.
Kenji Sagae and Junichi Tsujii, 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensemble. EMNLP-CoNLL ?07.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rap-
poport and Dan Roth, 2008. Extraction of Entailed
Semantic Relations Through Syntax-based Comma
Resolution. ACL ?08.
Noah A. Smith and Jason Eisner, 2006. Annealing Struc-
tural Bias in Multilingual Weighted Grammar Induc-
tion. ACL ?06.
Nicola Ueffing and Hermann Ney, 2007. Word-
Level Confidence Estimation for Machine Translation.
Computational Linguistics, 33(1):9?40.
Kenji Yamada and Kevin Knight, 2001. A Syntax-Based
Statistical Translation Model. ACL ?01.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting Parser Errors Using Web-
based Semantic Filters . EMNLP ?06.
164
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 165?173,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The NVI Clustering Evaluation Measure
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Clustering is crucial for many NLP tasks and
applications. However, evaluating the results
of a clustering algorithm is hard. In this paper
we focus on the evaluation setting in which a
gold standard solution is available. We discuss
two existing information theory based mea-
sures, V and VI, and show that they are both
hard to use when comparing the performance
of different algorithms and different datasets.
The V measure favors solutions having a large
number of clusters, while the range of scores
given by VI depends on the size of the dataset.
We present a new measure, NVI, which nor-
malizes VI to address the latter problem. We
demonstrate the superiority of NVI in a large
experiment involving an important NLP appli-
cation, grammar induction, using real corpus
data in English, German and Chinese.
1 Introduction
Clustering is a major technique in machine learn-
ing and its application areas. It lies at the heart
of unsupervised learning, which has great potential
advantages over supervised learning. This is es-
pecially true for NLP, due to the high efforts and
costs incurred by the human annotations required for
training supervised algorithms. Recent NLP prob-
lems addressed by clustering include POS induction
(Clark, 2003; Goldwater and Griffiths, 2007), word
sense disambiguation (Shin and Choi, 2004), seman-
tic role labeling (Baldewein et al, 2004), pitch ac-
cent type disambiguation (Levow, 2006) and gram-
mar induction (Klein, 2005).
Evaluation of clustering results is a challenging
task. In this paper we address the external measures
setting, where a correct assignment of elements to
classes is available and is used for evaluating the
quality of another assignment of the elements into
clusters. Many NLP works have used external clus-
tering evaluation measures (see Section 2).
Recently, two measures have been proposed that
avoid many of the weaknesses of previous measures
and exhibit several attractive properties (see Sec-
tions 2 and 3): the VI measure (Meila, 2007) and
the V measure (Rosenberg and Hirschberg, 2007).
However, each of these has a serious drawback. The
possible values of VI lie in [0, 2log N ], where N is
the size of the clustered dataset. Hence it has lim-
ited use when comparing performance on different
datasets. V measure values lie in [0, 1] regardless of
the dataset, but the measure strongly favors a cluster-
ing having many small clusters. In addition, V does
not have many of the attractive properties of VI.
This paper has two contributions. First, we pro-
pose the NVI measure, a normalization of VI which
guarantees that the score of clusterings that VI con-
siders good lies in [0,1], regardless of dataset size.
Most of VI?s attractive properties are retained by
NVI.
Second, we compare the behavior of V, VI and
NVI in various situations to the desired behavior and
to each other. In particular, we show that V gives
high scores to clusterings with a large number of
clusters even when they are of low quality. We
demonstrate this both in a synthetic example (Sec-
tion 5) and in the evaluation (in three languages) of
a difficult NLP problem, labeled parse tree induc-
165
tion (Section 6). We show that in both cases, NVI
constitutes a better clustering evaluation measure.
2 Previous Evaluation Measures
A large number of clustering quality measures have
been proposed. Here we briefly survey the three
main types, mapping based measures, counting pairs
measures and information theory based measures.
We first review some terminology (Meila, 2007;
Rosenberg and Hirschberg, 2007). In a homoge-
neous clustering, every cluster contains only ele-
ments from a single class. In a complete cluster-
ing, all elements of each class are assigned to the
same cluster. The perfect solution is the fully ho-
mogeneous and complete clustering. We will illus-
trate the behavior of some measures using three ex-
treme cases: the single cluster case, in which all
data elements are put in the same single cluster; the
singletons case, in which each data element is put
in a cluster of its own; and the no knowledge case,
in which the class distribution within each cluster
is identical to the class distribution in the entire
dataset. If the single cluster solution is not the per-
fect one, the no knowledge solution is the worst pos-
sible solution. Throughout the paper, the number of
data elements to be clustered is denoted by N.
Mapping based measures are based on a post-
processing step in which each cluster is mapped to a
class. Among these are: L (Larsen, 1999), D (Van
Dongen, 2000), misclassification index (MI) (Zeng
et al, 2002), H (Meila, 2001), clustering F-measure
(Fung et al, 2003) and micro-averaged precision
and recall (Dhillon et al, 2003). As noted in (Rosen-
berg and Hirschberg, 2007), these measures evalu-
ate not only the quality of the proposed clustering
but also of the mapping scheme. Different mapping
schemes can lead to different quality scores for the
same clustering. Moreover, even when the mapping
scheme is fixed, it can lead to not evaluating the en-
tire membership of a cluster and not evaluating every
cluster (Meila, 2007).
Counting pairs measures are based on a com-
binatorial approach which examines the number of
pairs of data elements that are clustered similarly in
the reference and proposed clustering. Among these
are Rand Index (Rand, 1971), Adjusted Rand In-
dex (Hubert and Arabie, 1985), ? statistic (Hubert
and Schultz, 1976), Jaccard (Milligan et al, 1983),
Fowlkes-Mallows (Fowlkes and Mallows, 1983) and
Mirkin (Mirkin, 1996).
Meila (2007) described a number of problems
with such measures. The most acute one is that their
values are unbounded, making it hard to interpret
their results. The problem can be solved by transfor-
mations adjusting their values to lie in [0, 1], but the
adjusted measures suffer from severe distributional
problems, again limiting their usability in practice.
Information-theoretic (IT) based measures are
those addressed in this work. The measures in this
family suffer neither from the problems associated
with mappings, since they evaluate the entire mem-
bership of each cluster and not just a mapped por-
tion, nor from the distributional problems of the
counting pairs measures.
Zhao and Karypis (2001) define Purity and En-
tropy as follows:
Purity = ?kr=1 1Nmaxi(nir)
Entropy = ?kr=1 nrN (? 1logq
?q
i=1
nir
nr log(n
i
r
nr ))
where q is the number of classes, k the number of
clusters, nr cluster r?s size, and nir is the number of
elements in class i assigned to cluster r.
Both measures are good measures for homogene-
ity (Purity increases and Entropy decreases when
homogeneity increases). However, they do not eval-
uate completeness at all. The singletons solution is
thus considered optimal even if in fact it is of very
low quality.
Dom (2001) proposed the Q measure, the sum of
a homogeneity term H(C|K) and a model cost term
calculated using a coding theory argument:
Q(C,K) = H(C|K) + 1N
?|k|
k=1 log
(h(k)+|C|?1
|C|?1
)
where C are the correct classes, K are the induced
clusters and h(k) is the number of elements in clus-
ter k. Dom also presented a normalized version of
the Q measure (called Q2) whose range is (0, 1] and
gives higher scores to clusterings that are preferable.
As noted by (Rosenberg and Hirschberg, 2007), the
Q measure does not explicitly address the complete-
ness of the suggested clustering. Due to the cost
term, if two clusterings have the same H(C|K)
value, the model prefers the one with the lower num-
ber of clusters, but the trade-off between homogene-
ity and completeness is not explicitly addressed.
In the next section we describe the V and VI mea-
166
sures, which are IT measures that explicitly assess
both the homogeneity and completeness of the clus-
tering solution.
BCubed (Bagga and Baldwin, 1998) is an attrac-
tive measure that addresses both completeness and
homogeneity. It does not explicitly use IT concepts
and avoids mapping. In this paper we focus on V
and VI; a detailed comparison with BCubed is out of
our scope here and will be done in future work.
Several recent NLP papers used clustering tech-
niques and evaluation measures. Examples include
(Finkel and Manning, 2008), using VI, Rand in-
dex and clustering F-score for evaluating corefer-
ence resolution; (Headden et al, 2008), using VI, V,
greedy 1-to-1 and many-to-1 mapping for evaluating
unsupervised POS induction; (Walker and Ringger,
2008), using clustering F-score, the adjusted Rand
index, V, VI and Q2 for document clustering; and
(Reichart and Rappoport, 2008), using greedy 1-to-
1 and many-to-1 mappings for evaluating labeled
parse tree induction.
Schulte im Walde (2003) used clustering to in-
duce semantic verb classes and extensively dis-
cussed non-IT based clustering evaluation measures.
Pfitzner et al (2008) presented a comparison of clus-
tering evaluation measures (IT based and others).
While their analysis is extensive, their experiments
were confined to artificial data. In this work, we
experiment with a complex NLP application using
large real datasets.
3 The V and VI Measures
The V (Rosenberg and Hirschberg, 2007) and VI
(Meila, 2007) measures are IT based measures. In
this section we give a detailed description of these
measures and analyze their properties.
Notations. The partition of the N data elements
into classes is denoted by C = {c1, . . . , c|C|}.
The clustering solution is denoted by K =
{k1, . . . , k|K|}. A = {aij} is a |C| ? |K| contin-
gency matrix such that aij is the number of data ele-
ments that are members of class ci and are assigned
by the algorithm to cluster kj .
As other IT measures, V and VI assume that the
elements in the dataset are taken from a known dis-
tribution (both assume the uniform distribution), and
thus the classes and clusters can be treated as ran-
dom variables. When assuming the uniform distri-
bution, the probability of an event (a class or a clus-
ter) is its relative size, so p(c) = ?|K|k=1 ackN and
p(k) = ?|C|c=1 ackN . Under this assumption we can
talk about the entropies H(C) and H(K) and the
conditional entropies H(C|K) and H(K|C):
H(C) = ??|C|c=1
P|K|
k=1 ack
N log
P|K|
k=1 ack
N
H(K) = ??|K|k=1
P|C|
c=1 ack
N log
P|C|
c=1 ack
N
H(C|K) = ??|K|k=1
?|C|
c=1
ack
N log
ack
P|C|
c=1 ack
H(K|C) = ??|K|k=1
?|C|
c=1
ack
N log
ack
P|K|
k=1 ack
In Section 2 we defined the concepts of homo-
geneity and completeness. In order to satisfy the ho-
mogeneity criterion, each cluster must be contained
in a certain class. This results in the minimization
of the conditional entropy of the classes given the
clusters, H(C|K) = 0. In the least homogeneous
solution, the conditional entropy is maximized, and
H(C|K) = H(C). Similarly, in order to satisfy the
completeness criterion, each class must be contained
in a certain cluster, which results in the minimiza-
tion of the conditional entropy of the clusters given
the classes, H(K|C) = 0. In the least complete
solution, the conditional entropy is maximized, and
H(K|C) = H(K).
The VI measure. Variation of information (VI) is
defined as follows:
V I(C,K) = H(C|K) + H(K|C).
In the least homogeneous (complete) clustering, the
values of H(C|K) (H(K|C)) are maximal. As
a clustering solution becomes more homogeneous
(complete), the values of H(C|K) (H(K|C)) de-
crease to zero. Consequently, lower VI values im-
ply better clustering solutions. In the perfect so-
lution, both H(C|K) = 0 and H(K|C) = 0 and
thus V I = 0. For the least homogeneous and com-
plete clustering solution, where knowing the cluster
tells nothing about the class and vise versa, V I =
H(C) + H(K).
As a result, the range of values that VI takes is
dataset dependent, and the numbers themselves tell
167
us nothing about the quality of the clustering solu-
tion (apart from a score of 0, which is given to the
best possible solution).
A bound for VI values is a function of the maxi-
mum number of clusters in C or K, denoted by k?.
This is obtained when each cluster contains a sin-
gle element, and k? = N . Thus, V I ? [0, 2logN ].
Consequently, the range of VI values is dataset de-
pendent and unbounded when datasets change. This
means that it is hard to use VI to compare the perfor-
mance of a clustering algorithm across datasets.
An apparent simple solution to this problem
would be to normalize VI by 2logk? or 2logN , so
that its values would lie in [0, 1]. We discuss this at
the end of the next section.
VI has two useful properties. First, it satis-
fies the metric axioms, that is: V I(C,K) ?
0, V I(C,K) = V I(K,C), V I(C1, C2) +
V I(C2, C3) ? V I(C1, C3). This gives an intuitive
understanding of the relation between VI values.
Second, it is convexly additive. This
means that if K is obtained from C by
splitting Cj into clusters K1j , . . . ,Kmj ,
H?(Kj) = ??mi=1 P (Kij |Cj)logP (Kij |Cj),
then V I(C,K) = P (Cj)H?(Kj). This property
guarantees that all changes to VI are local; the
impact of splitting or merging clusters is limited
only to those clusters involved, and its size is
relative to the size of these clusters.
The V measure. The V measure uses homogeneity
(h) and completeness (c) terms as follows:
h =
{1 H(C) = 0
1? H(C|K)H(C) H(C) 6= 0
c =
{1 H(K) = 0
1? H(K|C)H(K) H(K) 6= 0
V = 2hch + c
In the least homogeneous clustering, H(C|K) is
maximal, at H(C|K) = H(C). In this case h
reaches its minimum value, which is 0. As homo-
geneity increases H(C|K) values decrease. For the
most homogeneous clustering, H(C|K) = 0 and
h = 1. The same considerations hold for c, which
ranges between 0 (for the least complete clustering)
and 1 (for a complete clustering). Since V is de-
fined to be the harmonic mean of h and c, V values
lie in [0, 1]. Consequently, it can be used to com-
pare the performance of clustering algorithms across
datasets. Higher V values imply better clusterings.
Unlike VI, V does not satisfy the metric axioms
and is not convexly additive. The range of values it
can get does not depend on dataset size.
Extreme cases for the two measures. In the
single cluster solution H(C|K) = H(C) and
H(K|C) = 0, and thus V = 0 (the worst possi-
ble score) and V I = H(C). If there is indeed only
a single class, then V I = 0, the best possible score,
which is the correct behavior. VI behaves better than
V here.
The singletons solution is a fully homogeneous
clustering in which H(C|K) = 0. The score of each
measure depends on the completeness of the solu-
tion. The completeness of a singletons clustering in-
creases with the number of classes. In the extreme
case where every element is assigned to a unique
class (|C| = |K| = N ) singletons is also complete,
H(K|C) = 0, and V (C,K) = 1, V I(C,K) = 0.
Both measures exhibit the correct behavior.
If there are classes that contain many elements,
singletons is far from being complete and should be
treated as a low quality solution. Again, in the sin-
gletons solution V I = H(K|C). Suppose that the
number of clusters is fixed. When the number of
classes increases, this value decreases, which is what
we want. When the number of classes decreases, the
score increases, which is again the correct behav-
ior. In Section 5 we show that this desired behavior
shown by VI is not shown by V.
Both measures treat the no knowledge solution as
the worst one possible: V = 0, and V I = H(C) +
H(K).
4 Normalized Variation of Information
In this section we define NVI, a normalization of
VI. NVI is N -independent and its values for clus-
terings considered as good by VI lie in [0, 1]. Hence,
NVI can be used to compare clustering performance
across datasets. We show that NVI keeps the convex
additivity property of VI but not its metric axioms.
168
Definition. We define NVI to be:
NV I(C,K) =
{H(C|K)+H(K|C)
H(C) H(C) 6= 0
H(K) H(C) = 0
We define NVI to be H(K) when H(C) = 0 to sat-
isfy the requirements that NVI values decrease as C
and K become more similar and that NVI would be
0 when they are identical1.
Range and extreme cases. Like VI, NVI decreases
as the clustering becomes more complete and more
homogeneous. For the perfect solution, NV I = 0.
In both the single cluster and the no knowledge so-
lutions, H(C|K) = H(C). Thus, in the former case
NV I = 1, and in the latter NV I = 1 + H(K)HC ? 1.
For the singletons clustering case, NV I =
H(K|C)
H(C) . Suppose that the number of clusters is
fixed. When the number of classes increases, the
numerator decreases and the denominator increases,
and hence the score decreases. In other words, as the
real solution gets closer to the singletons solution,
the score decreases, which is the correct behavior.
When the number of classes decreases, the score in-
creases, which is again the correct behavior.
For any pair of clusterings K1 and K2,
V I(C,K1) > V I(C,K2) iff NV I(C,K1) >
V I(C,K2). This implies that only clustering solu-
tions whose VI scores are better (i.e., numerically
lower) than the score of the single cluster solution
will be scored lower than 1 by NVI.
Note that NVI is meant to be used when there is
a ?correct? reference solution. In this case H(C) is
constant, so the property above holds. In this sense,
VI is more general, allowing us to compare any three
clustering solutions even when we do not have a cor-
rect reference one.
To summarize:
1. All clusterings considered by VI to be of high
quality (i.e., better than the single cluster solu-
tion) are scored by NVI in the range of [0, 1].
2. All clusterings considered by VI to be of lower
quality than the single cluster solution are
scored higher than 1 by NVI.
1H(C) = 0 iff C consists of a single class, and therefore
H(C) = H(K) = 0 iff C (K) consists of a single class (clus-
ter).
3. The ordering of scores between solutions given
by VI is preserved by NVI.
4. The behavior of NVI on the extreme cases is the
desired one.
Useful properties. In Section 3 we saw that VI has
two useful properties, satisfying the metric axioms
and being convexly additive. NVI is not symmetric
since the term in its denominator is H(C), the en-
tropy of the correct class assignment. Thus, it does
not satisfy the metric axioms. Being convexly addi-
tive, however, is preserved. In the class splitting sce-
nario (see convex additivity definition in Section 3)
it holds that NV I(C,K) = P (Cj)H?(Kj)H(C) . That is,
like for VI, the impact of splitting or merging a clus-
ter on NVI is limited only to those clusters involved,
and its size is relative to the size of these clusters.
Meila (2007) derived various interesting properties
of VI from the convex additivity property. These
properties generally hold for NVI as well.
H(K) normalization. Normalizing by H(C)
takes into consideration the complexity of the cor-
rect clustering. Another normalization option would
be to normalize by H(K), which represents the in-
duced clustering complexity. This normalization
does not guarantee that the scores of the ?good? clus-
terings lie in a data-independent range.
Let us define NVIK(C,K) to be V I(C,K)H(K) if
H(K) > 0 and H(C) if H(K) = 0. Recall that
in order for NVIK to be 0 iff C and K are identi-
cal, we must require that NV IK = H(C) when
H(K) = 0. In the no knowledge case, NV IK =
H(C)+H(K)
H(K) = H(C)H(K) + 1 > 1. In the single cluster
solution, however, NV IK = H(C) (since in this
case H(K) = 0) which ranges in [0, logN ]. This is
a serious drawback of NVIK. In Section 6 we empir-
ically show an additional drawback of NVIK.
logN normalization. Another possible normal-
ization of VI is by 2logN (or 2logk?), which is an
upper bound on VI values. However, this results in
the values of the measure being dependent on dataset
size, so results on datasets with different sizes again
cannot be compared. For example, take any C and
K and split each element into two. All entropy val-
ues, and the quality of the solution, are preserved,
but the scores given to the two K?s (before and after
169
7 1 1 1 0 0 0 0 0 0
0 7 1 1 1 0 0 0 0 0
0 0 7 1 1 1 0 0 0 0
0 0 0 7 1 1 1 0 0 0
0 0 0 0 7 1 1 1 0 0
0 0 0 0 0 7 1 1 1 0
0 0 0 0 0 0 7 1 1 1
1 0 0 0 0 0 0 7 1 1
1 1 0 0 0 0 0 0 7 1
1 1 1 0 0 0 0 0 0 7
V VI NVI NVIK
Singletons 0.667 2.303 1 0.5
Solution R 0.587 1.88 0.81 0.81
Table 1: The clustering matrix of solution R (top), and
the scores given to it and to the singletons solution by the
four measures (bottom). Although solution R is superior,
the score given by V to the singletons solution is much
higher. NVI exhibits the most preferable behavior (recall
that higher V values are better, as opposed to the other
three measures).
the split) by such a normalized VI would be differ-
ent. Since H(C) is preserved, the scores given by
NVI to the two K?s are identical.
5 Problematic V Behavior Example
In this section we provide a synthetic example that
demonstrates an undesireable behavior of V (and
NVIK) not manifested by VI and NVI. Specifically,
V favors solutions with a large number of clusters,
giving them higher scores than to solutions that are
evidently superior. In addition, the score given to the
singletons solution is high in absolute terms.
To present the example, we use the matrix repre-
sentation A of a clustering solution defined in Sec-
tion 3. The entries in row i sum to the number of
elements in class i, while those in column j sum to
the number of elements in cluster j.
Suppose that we have 100 elements assigned to 10
classes such that there are 10 elements in each class.
We consider two clustering solutions: the singletons
solution, and solution R whose matrix is shown in
Table 1 (top). Like the real solution, solution R also
has 10 clusters each having 10 elements. Solution
R is not very far from the correct solution, since
each cluster has 7 elements of the same class, and
the three other elements in a cluster are taken from
a different class each and can be viewed as ?noise?.
Solution R is thus much better than the singletons
solution. In order not to rely on our own opinion,
we have performed a simple human judgment ex-
periment with 30 subjects (university graduates in
different fields), all of whom preferred solution R2.
The scores given by V, VI, NVI and NVIK to the
two solutions are shown in Table 1 (bottom). V
scores solution R as being worse than the single-
tons solution, and gives the latter a number that?s
relatively high in absolute terms (0.667). VI ex-
hibits qualitatively correct behavior, but the num-
bers it uses are hard to interpret since they are N-
dependent. NVI scores solution R as being better
than singletons, and its score is less than 1, indicat-
ing that it might be a good solution.
6 Grammar Induction Experiment
In this section we analyse the behavior of V, VI,
NVI and NVIK using a highly non-trivial NLP ap-
plication with large real datasets, the unsupervised
labeled parse tree induction (LTI) algorithm of (Re-
ichart and Rappoport, 2008). We focus on the label-
ing that the algorithm finds for parsing constituents,
which is a clustering of constituents.
Summary of result. We show that V gives about
the same score to a labeling that uses thousands of
labels and to labelings in which the number of la-
bels (dozens) is identical or smaller than the number
of labels in the reference evaluation set (an anno-
tated corpus). Contrary to V, both NVI and VI give
much better scores to the solutions having a smaller
number of labels.
It could be argued that the total number of ?real?
labels in the data is indeed large (e.g., because every
verb exhibits its own syntactic patterns) and that a
small number of labels is just an arbitrary decision of
the corpus annotators. However, most linguistic the-
ories agree that there is a prototypical level of gen-
eralization that uses concepts such as Noun Phrase
and Verb Phrase, a level which consists of at most
dozens of labels and is strongly manifested by real
language data. Under these accepted assumptions,
the scoring behavior of V is unreasonable.
2We must rely on people?s expectations, since the whole
point in this area is that clustering quality cannot be formalized
in an objective, application-independent way.
170
MDL+SC (T labels) MDL+SC (P labels) MDL labels
Corpus L = 1 < 10 < 102 ? 102 L = 1 < 10 < 102 ? 102 L = 1 < 10 < 102 ? 102
WSJ10 26 0 0 3 23 8 0 0 0 8 2916 2282 2774 2864 52
NEGRA10 22 0 2 12 10 6 0 0 1 5 1202 902 1114 1191 11
CTB10 24 1 4 11 13 9 1 2 4 5 1050 816 993 1044 6
Table 2: The number of elements (constituents) covered by the clusters (labels) produced by the MDL+SC (T or P
labels) and MDL clusterings. L is the total number of labels. Shown are the number of clusters having one element,
less than 10 elements, less than 100 elements, and more than 100 elements. It is evident that MDL induces a sparse
clustering with many clusters that annotate very few constituents.
V VI NVI NVIK
Corpus MDL T P MDL T P MDL T P MDL T P
WSJ10 0.4 0.44 0.41 3.83 2.32 1.9 2.21 1.34 1.1 0.81 0.86 1.2
NEGRA10 0.47 0.5 0.5 2.56 1.8 1.4 1.51 1.1 0.83 0.76 0.96 1.1
CTB10 0.42 0.42 0.45 3 2.22 1.85 1.72 1.26 1.1 0.87 1.1 1.25
Table 3: V, VI, NVI and NVIK values for MDL and MDL+SC with T or P labels. V gives the three clusterings
very similar scores. NVIK prefers MDL labeling. NVI and VI both show the expected qualitative behavior, favoring
MDL+SC clustering with P labels. The most preferable scores are those of NVI, whose numbers are also the easiest
to interpret.
The experiment. The LTI algorithm has three
stages: bracketing, initial labeling, and label clus-
tering. Bracketing is done from raw text using
the unsupervised incremental parser of (Seginer,
2007). Initial labeling is done using the BMM model
(Borensztajn and Zuidema, 2007), which aims at
minimizing the grammar description length (MDL).
Finally, labels are clustered to a desired number of
labels using the k-means algorithm with syntactic
features extracted from the initially labeled trees.
We refer to this stage as MDL+SC (for ?syntactic
clustering?). Using a mapping-based evaluation with
two different mapping functions, the LTI algorithm
was shown to outperform previous work on unsu-
pervised labeled parse tree induction.
The MDL clustering step induces several thou-
sand labels for corpora of several tens of thousands
of constituents. The role of the SC step is to gen-
eralize these labels using syntactic features. There
are two versions of the SC step. In one, the num-
ber of clusters is identical to the number of labels
in the gold standard annotation of the experimental
corpus. This set of labels is called T (for target)
labels. In the other SC version, the number of la-
bels is the minimum number of labels required to
annotate more than 95% of the constituents in the
gold standard annotation of the corpus. This set of
labels is called P (for prominent) labels. Since con-
stituent labels follow the Zipfian distribution, P is
much smaller than T .
In this paper we run the LTI algorithm and evalu-
ate its labeling quality using V, VI, NVI and NVIK.
We compare the quality of the clustering induced by
the first clustering step alone (the MDL clustering)
to the quality of the clustering induced by the full
algorithm (i.e., first applying MDL and then clus-
tering its output using the SC algorithm for T or P
labels)3.
We follow the experimental setup in (Reichart
and Rappoport, 2008), running the algorithm on En-
glish, German and Chinese corpora: the WSJ Penn
Treebank (English), the Negra corpus (Brants, 1997)
(German), and version 5.0 of the Chinese Penn Tree-
bank (Xue et al, 2002). In each corpus, we used
the sentences of length at most 10,4 numbering 7422
(WSJ10), 7542 (NEGRA10) and 4626 (CTB10).
The characteristics of the induced clusterings are
shown in Table 25. The table demonstrates the
fact that MDL labeling, while perhaps capturing the
3Note that our evaluation here has nothing to do with the
evaluation done in (Reichart and Rappoport, 2008), which pro-
vided a comparison of the full grammar induction results be-
tween different algorithms, using mapping-based measures. We
evaluate the labeling stages alone.
4Excluding punctuation and null elements, according to the
scheme of (Klein, 2005).
5The number of MDL labels in the table differs from their
numbers, since we report the number of unique MDL labels
used for annotating correct constituents in the parser?s output,
while they report the number of unique labels used for annotat-
ing all constituents in the parser?s output.
171
salient level of generalization of the data in its lead-
ing clusters, is extremely noisy. For WSJ10, for ex-
ample, 2282 of the 2916 unique labels annotate only
one constituent, and 2774 labels label less than 10
constituents. These 2774 labels annotate 14.4% of
compared constituents, and the 2864 labels that an-
notate less than 100 constituents each, cover 30.7%
of the compared constituents (these percentages are
not shown in the table). In other words, MDL is not
a solution in which almost all of the mass is concen-
trated in the few leading clusters; its tail occupies a
large percentage of its mass.
MDL patterns for NEGRA10 and CTB10 are very
similar. For MDL+SC with T or P labels, most
of the induced labels annotate 100 constituents or
more. We thus expect MDL+SC to provide better
clustering than MDL; a good clustering evaluation
measure should reflect this expectation.
Table 3 shows V, VI, NVI and NVIK scores for
MDL and MDL+SC (with T or P labels). For all
three corpora, V values are almost identical for the
MDL and the MDL+SC schemes. This is in con-
trast to VI and NVI values that strongly prefer the
MDL+SC clusterings, fitting our expectations (re-
call that for these measures, the lower the score, the
better the clustering). Moreover, VI and NVI pre-
fer MDL+SC with P labels, which again accords
with our expectations, since P labels were defined
as those that are more salient in the data (see above).
The patterns of NVI and VI are identical, since
NV I = V IH(C) and H(C) is independent of the
induced clustering. However, the numbers given
by NVI are easier to interpret than those given by
VI. The latter are basically meaningless, convey-
ing nothing about clustering quality. The former are
quite close to 1, telling us that clustering quality is
not that good but not horrible either. This makes
sense, because the overall quality of the labeling in-
duction algorithm is indeed not that high: using one-
to-one mapping (the more forgiving mapping), the
accuracy of the labels induced by MDL+SC is only
45?72% (Reichart and Rappoport, 2008).
NVIK, the normalization of VI with H(K), is
worse even than V. This measure (which also gives
lower scores to better clusterings) prefers the MDL
over MDL+SC labels. This is a further justification
of our decision to define NVI by normalizing VI by
H(C) rather than by H(K).
Corpus H(C) H(K)
MDL T P
WSJ10 1.73 4.72 2.7 1.58
NEGRA10 1.69 3.36 1.87 1.29
CTB10 1.76 3.45 2.1 1.48
Table 4: Class (H(C)) and cluster (H(K)) entropy for
MDL and MDL+SC with T or P labels. H(C) is cluster
independent. H(K) increases with the number of clus-
ters.
Table 4 shows the H(C) and H(K) values in the
experiment. While H(C) is independent of the in-
duced clustering and is thus constant for a given
annotated corpus, H(K) monotonically increases
with the number of induced clusters. Since both
NVIK and the completeness term of V are normalized
by H(K), these measures prefer clusterings with a
large number of clusters even when many of these
clusters provide useless information.
7 Conclusion
Unsupervised clustering evaluation is important for
various NLP tasks and applications. Recently, the
importance of the completeness and homogeneity as
evaluation criteria for such clusterings has been rec-
ognized. In this paper we addressed the two mea-
sures that address these criteria: VI (Meila, 2007)
and V (Rosenberg and Hirschberg, 2007).
While VI has many useful properties, the range of
values it can take is dataset dependent, which makes
it unsuitable for comparing clusterings of different
datasets. This imposes a serious restriction on the
measure usage. We presented NVI, a normalized ver-
sion of VI, which does not have this restriction and
still retains some of its useful properties.
Using experiments with both synthetic data and
a complex NLP application, we showed that the V
measure prefers clusterings with many clusters even
when these are clearly of low quality. VI and NVI do
not exhibit such behavior, and the numbers given by
NVI are easier to interpret than those given by VI.
In future work we intend to explore more of the
properties of NVI and use it in other real NLP appli-
cations.
References
Amit Bagga and Breck Baldwin, 1998. Entity-based
cross-document coreferencing using the vector space
172
model. ACL 98.
Ulrike Baldewein, Katrin Erk, Sebastian Pado, and Detlef
Prescher 2004. Semantic role labeling with similarity
based generalization using EM?based clustering. Sen-
seval ?04.
Thorsten Brants, 1997. The NEGRA export format.
CLAUS Report, Saarland University.
Gideon Borensztajn and Willem Zuidema, 2007.
Bayesian model merging for unsupervised constituent
labeling and grammar induction. Technical Report,
ILLC. http: //staff.science.uva.nl/?gideon/
Alexander Clark, 2003. Combining distributional and
morphological information for part of speech induc-
tion. EACL ?03.
I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. Infor-
mation theoretic co-clustering. KDD ?03.
Byron E. Dom, 2001. An information-theoretic external
cluster validity measure . Journal of American statis-
tical Association,78:553?569.
Jenny Rose Finkel and Christopher D. Manning, 2008.
Enforcing transitivity in coreference resolution. ACL
?08.
E.B Fowlkes and C.L. Mallows, 1983. A method for
comparing two hierarchical clusterings. Journal of
American statistical Association,78:553?569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester, 2003.
Hierarchical document clustering using frequent item-
sets. SIAM International Conference on Data Mining
?03.
Sharon Goldwater and Thomas L. Griffiths, 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. ACL ?07.
William P. Headden, David McClosky, and Eugene Char-
niak, 2008. Evaluating unsupervised part-of-speech
tagging for grammar induction. COLING ?08.
L. Hubert and P. Arabie, 1985. Comparing partitions.
Journal of Classification, 2:193?218.
L. Hubert and J. Schultz, 1976. Quadratic assignment
as a general data analysis strategy. British Journal
of Mathematical and Statistical Psychology, 29:190?
241.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Bjornar Larsen and Chinatsu Aone, 1999. Fast and effec-
tive text mining using linear-time document clustering.
KDD ?99.
Gina-Anne Levow, 2006. Unsupervised and semi-
supervised learning of tone and pitch accent. HLT-
NAACL ?06.
Marina Meila and David Heckerman, 2001. An exper-
imental comparison of model-based clustering meth-
ods. Machine Learning, 42(1/2):9-29.
Marina Meila, 2007. Comparing clustering ? an infor-
mation based distance. Journal of Multivariate Analy-
sis, 98:873?895.
C.W Milligan, S.C Soon and L.M Sokol, 1983. The
effect of cluster size, dimensionality and the number
of clusters on recovery of true cluster structure. IEEE
transactions on Pattern Analysis and Machine Intelli-
gence, 5:40?47.
Boris G. Mirkin, 1996. Mathematical classification and
clustering. Kluwer Academic Press.
Darius M. Pfitzner, Richard E. Leibbrandt and David
M.W Powers, 2008. Characterization and evaluation
of similarity measures for pairs of clusterings. Knowl-
edge and Information Systems: An International Jour-
nal, DOI 10.1007/s10115-008-0150-6.
William Rand, 1971. Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statstical Association, 66(336):846?850.
Roi Reichart and Ari Rappoport, 2008. Unsupervised in-
duction of labeled parse trees by clustering with syn-
tactic features. COLING ?08.
Andrew Rosenberg and Julia Hirschberg, 2007. V?
Measure: a conditional entropy?based external cluster
evaluation measure. EMNLP ?07.
Sabine Schulte im Walde, 2003. Experiments on the
automatic induction of German semantic verb classes.
Ph.D. thesis, Universitat Stuttgart.
Yoav Seginer, 2007. Fast unsupervised incremental pars-
ing. ACL 07.
Sa-Im Shin and Key-Sun Choi, 2004. Automatic word
sense clustering using collocation for sense adaptation.
The Second Global WordNet Conference.
Stijn van Dongen, 2000. Performance criteria for graph
clustering and markov cluster experiments. Technical
report CWI, Amsterdam
Daniel D. Walker and Eric K. Ringger, 2008. Model-
based document clustering with a collapsed Gibbs
sampler. KDD ?08.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer, 2002.
Building a large?scale annotated Chinese corpus. ACL
?02.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and
Guang R. Gao, 2002. An adaptive meta-clustering
approach: combining the information from different
clustering results. IEEE Computer Society Bioinfor-
matics Conference (CSB ?02) .
Ying Zhao and George Karypis, 2001. Criterion func-
tions for document clustering: experiments and analy-
sis. Technical Report TR 01-40, Department of Com-
puter Science, University of Minnesota.
173
Coling 2010: Poster Volume, pages 1274?1282,
Beijing, August 2010
A Multi-Domain Web-Based Algorithm for
POS Tagging of Unknown Words
Shulamit Umansky-Pesin
Institute of computer science
The Hebrew University
pesin@cs.huji.ac.il
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
We present a web-based algorithm for the
task of POS tagging of unknown words
(words appearing only a small number
of times in the training data of a super-
vised POS tagger). When a sentence s
containing an unknown word u is to be
tagged by a trained POS tagger, our algo-
rithm collects from the web contexts that
are partially similar to the context of u in
s, which are then used to compute new
tag assignment probabilities for u. Our
algorithm enables fast multi-domain un-
known word tagging, since, unlike pre-
vious work, it does not require a corpus
from the new domain. We integrate our
algorithm into the MXPOST POS tagger
(Ratnaparkhi, 1996) and experiment with
three languages (English, German and
Chinese) in seven in-domain and domain
adaptation scenarios. Our algorithm pro-
vides an error reduction of up to 15.63%
(English), 18.09% (German) and 13.57%
(Chinese) over the original tagger.
1 Introduction
Part-of-speech (POS) tagging is a fundamental
NLP task that has attracted much research in the
last decades. While supervised POS taggers have
achieved high accuracy (e.g., (Toutanova et al,
2003) report a 97.24% accuracy in the WSJ Penn
Treebank), tagger performance on words appear-
ing a small number of times in their training
corpus (unknown words) is substantially lower.
This effect is especially pronounced in the do-
main adaptation scenario, where the training and
test corpora are from different domains. For ex-
ample, when training the MXPOST POS tagger
(Ratnaparkhi, 1996) on sections 2-21 of the WSJ
Penn Treebank it achieves 97.04% overall accu-
racy when tested on WSJ section 24, and 88.81%
overall accuracy when tested on the BNC cor-
pus, which contains texts from various genres.
For unknown words (test corpus words appearing
8 times or less in the training corpus), accuracy
drops to 89.45% and 70.25% respectively.
In this paper we propose an unknown word POS
tagging algorithm based on web queries. When a
new sentence s containing an unknown word u is
to be tagged by a trained POS tagger, our algo-
rithm collects from the web contexts that are par-
tially similar to the context of u in s. The collected
contexts are used to compute new tag assignment
probabilities for u.
Our algorithm is particularly suitable for multi-
domain tagging, since it requires no information
about the domain from which the sentence to be
tagged is drawn. It does not need domain specific
corpora or external dictionaries, and it requires
no preprocessing step. The information required
for tagging an unknown word is very quickly col-
lected from the web.
This behavior is unlike previous works for the
task (e.g (Blitzer et al, 2006)), which require a
time consuming preprocessing step and a corpus
collected from the target domain. When the target
domain is heterogeneous (as is the web itself), a
corpus representing it is very hard to assemble. To
the best of our knowledge, ours is the first paper to
provide such an on-the-fly unknown word tagging
algorithm.
To demonstrate the power of our algorithm as a
1274
fast multi-domain learner, we experiment in three
languages (English, German and Chinese) and
several domains. We implemented the MXPOST
tagger and integrated it with our algorithm. We
show error reduction in unknown word tagging
of up to 15.63% (English), 18.09% (German) and
13.57% (Chinese) over MXPOST. The run time
overhead is less than 0.5 seconds per an unknown
word in the English and German experiments, and
less than a second per unknown word in the Chi-
nese experiments.
Section 2 reviews previous work on unknown
word Tagging. Section 3 describes our web-query
based algorithm. Section 4 and Section 5 describe
experimental setup and results.
2 Previous Work
Most supervised POS tagging works address the
issue of unknown words. While the general meth-
ods of POS tagging vary from study to study
? Maximum Entropy (Ratnaparkhi, 1996), con-
ditional random fields (Lafferty et al, 2001),
perceptron (Collins, 2002), Bidirectional Depen-
dency Network (Toutanova et al, 2003) ? the
treatment of unknown words is more homoge-
neous and is generally based on additional fea-
tures used in the tagging of the unknown word.
Brants (2000) used only suffix features. Rat-
naparkhi (1996) used orthographical data such as
suffixes, prefixes, capital first letters and hyphens,
combined with a local context of the word. In this
paper we show that we improve upon this method.
Toutanova and Manning (2000), Toutanova et al
(2003), Lafferty et al (2001) and Vadas and Cur-
ran (2005) used additional language-specific mor-
phological or syntactic features. Huihsin et al
(2005) combined orthographical and morpholog-
ical features with external dictionaries. Naka-
gawa and Matsumoto (2006) used global and local
information by considering interactions between
POS tags of unknown words with the same lexical
form.
Unknown word tagging has also been explored
in the context of domain adaptation of POS tag-
gers. In this context two directions were explored:
a supervised method that requires a manually an-
notated corpus from the target domain (Daume III,
2007), and a semi-supervised method that uses an
unlabeled corpus from the target domain (Blitzer
et al, 2006).
Both methods require the preparation of a cor-
pus of target domain sentences and re-training
the learning algorithm. Blitzer et al (2006) used
100K unlabeled sentences from the WSJ (source)
domain as well as 200K unlabeled sentences from
the biological (target) domain. Daume III (2007)
used an 11K words labeled corpus from the target
domain.
There are two serious problems with these ap-
proaches. First, it is not always realistically pos-
sible to prepare a corpus representing the target
domain, for example when that domain is the web
(e.g., when the POS tagger serves an application
working on web text). Second, preparing a cor-
pus is time consuming, especially when it needs
to be manually annotated. Our algorithm requires
no corpus from the target data domain, no prepro-
cessing step, and it doesn?t even need to know the
identity of the target domain. Consequently, the
problem we address here is more difficult (and ar-
guably more useful) than that addressed in previ-
ous work1.
The domain adaptation techniques above have
not been applied to languages other than English,
while our algorithm is shown to perform well in
seven scenarios in three languages.
Qiu et al (2008) explored Chinese unknown
word POS tagging using internal component and
contextual features. Their work is not directly
comparable to ours since they did not test a do-
main adaptation scenario, and used substantially
different corpora and evaluation measures in their
experiments.
Numerous works utilized web resources for
NLP tasks. Most of them collected corpora us-
ing data mining techniques and used them off-
line. For example, Keller et al, (2002) and Keller
and Lapata (2003) described a method to obtain
frequencies for unseen adjective-noun, noun-noun
and verb-object bigrams from the web by query-
1We did follow their experimental procedure as much as
we could. Like (Blitzer et al, 2006), we compare our algo-
rithm to the performance of the MXPOST tagger trained on
sections 2-21 of WSJ. Like both papers, we experimented
in domain adaptation from WSJ to a biological domain. We
used the freely available Genia corpus, while they used data
from the Penn BioIE project (PennBioIE, 2005).
1275
ing a Web engine.
On-line usage of web queries is less frequent
and was used mainly in semantic acquisition ap-
plications: the discovery of semantic verb rela-
tions (Chklovski and Pantel, 2004), the acquisi-
tion of entailment relations (Szpektor et al, 2004),
and the discovery of concept-specific relation-
ships (Davidov et al, 2007). Chen et al (2007)
used web queries to suggest spelling corrections.
Our work is related to self-training (McClosky
et al, 2006a; Reichart and Rappoport, 2007) as
the algorithm used its own tagging of the sen-
tences collected from the web in order to produce
a better final tagging. Unlike most self-training
works, our algorithm is not re-trained using the
collected data but utilizes it at test time. More-
over, unlike in these works, in this work the data
is collected from the web and is used only dur-
ing unknown words tagging. Interestingly, previ-
ous works did not succeed in improving POS tag-
ging performance using self-training (Clark et al,
2003).
3 The Algorithm
Our algorithm utilizes the correlation between the
POS of a word and the contexts in which the word
appears. When tackling an unknown word, the al-
gorithm searches the web to find contexts similar
to the one in which the word appears in the sen-
tence. A new tag assignment is then computed for
the unknown word based on the extracted contexts
as well as the original ones.
We start with a description of the web-based
context searching algorithm. We then describe
how we combine the context information col-
lected by our algorithm with the statistics of the
MXPOST tagger. While in this paper we imple-
mented this tagger and used it in our experiments,
the context information collected by our web-
query based algorithm can be integrated into any
POS tagger.
3.1 Web-Query Based Context Collection
An unknown word usually appears in a given sen-
tence with other words on its left and on its right.
We use three types of contexts. The first includes
all of these neighboring words, the second in-
cludes the words on the left, and the third includes
the words on the right.
For each context type we define a web query us-
ing two common features supported by the major
search engines: wild-card search, expressed using
the ?*? character, and exact sentence search, ex-
pressed by quoted characters. The retrieved sen-
tences contain the parts enclosed in quotes in the
exact same place they appear in the query, while
an asterisk can be replaced by any single word.
For a word u we execute the following three
queries for each of its test contexts:
1. Replacement: "u?2u?1 ?u+1u+2". This re-
trieves words that appear in the same context
as u.
2. Left-side: "? ? u u+1 u+2". This retrieves
alternative left-side contexts for the word u
and its original right-side context.
3. Right-side: query "u?2 u?1 u ? ?". This
retrieves alternative right-side contexts for u
and its original left-side context.
Query Type Query Matches (Counts)
Replacement "irradiation and * heat (15)
treatment of" chemical (7)
the (6)
radiation (1)
pressure (1)
Left-side "* * H2O2 by an (9)
treatment of" indicated that (5)
enhanced by (4)
familiar with (3)
observed after (3)
Right-side "irradiation and in comparison (3)
H2O2 * *" on Fe (1)
treatment by (1)
cause an (1)
does not (1)
Table 1: Top 5 matches of each query type for the word
?H2O2? in the GENIA sentence: ?UV irradiation and H2O2
treatment of T lymphocytes induce protein tyrosine phospho-
rylation and Ca2+ signals similar to those observed following
biological stimulation.?. For each query the matched words
(matches) are ranked by the number of times they occur in
the query results (counts).
An example is given in Table 1, presenting the
top 5 matches of every query type for the word
?H2O2?, which does not appear in the English
WSJ corpus, in a sentence taken from the English
Genia corpus. Since matching words can appear
1276
multiple times in the results, the algorithm main-
tains for each match a counter denoting the num-
ber of times it appeared in the results, and sorts
the results according to this number.
Seeing the table, readers might think of the fol-
lowing algorithm: take the leading match in the
Replacement query, and tag the unknown word us-
ing its most frequent tag (assuming it is a known
word). We have experimented with this method,
and it turned out that its results are worse than
those given by MXPOST, which we use as a base-
line.
The web queries are executed by Yahoo!
BOSS2, and the resulting XML containing up to a
1000 results (a limit set by BOSS) is processed for
matches. A list of matches is extracted from the
abstract and title nodes of the web results along
with counts of the number of times they appear.
The matches are filtered to include only known
words (words that appear in the training data of
the POS tagger more than a threshold) and to ex-
clude the original word or context.
Our algorithm uses a positive integer parameter
Nweb: only the Nweb top-scoring unique results
of each query type are used for tagging. If a left-
side or right-side query returns less than Nweb re-
sults, the algorithm performs a ?reduced? query:
"? ? u u+1" for left-side and "u?1 u ? ?" for the
right side. These queries should produce more re-
sults than the original ones due to the reduced con-
text. If these reduced queries do not produce Nweb
results, the web query algorithm is not used to as-
sist the tagger for the unknown word u at hand.
If a replacement query does not produce at least
Nweb unique results, only the left-side and right-
side queries are used.
For Chinese queries, search engines do their
own word segmentation so the semantics of the
?*? operator is supposedly the same as for English
and German. However, the answer returned by
the search engine does not provide this segmen-
tation. To obtain the words filling the ?*? slots in
our queries, we take all possible segmentations in
which the two words appears in our training data.
The queries we use in our algorithm are not the
only possible ones. For example, a possible query
2http://developer.yahoo.com/search/boss/
we do not use for the word u is "??u?1uu+1u+2".
The aforementioned set of queries gave the best
results in our English, German and Chinese de-
velopment data and is therefore the one we used.
3.2 Final Tagging
The MXPOST Tagger. We integrated our algo-
rithm into the maximum entropy tagger of (Rat-
naparkhi, 1996). The tagger uses a set h of con-
texts (?history?) for each word wi (the index i is
used to allow an easy notation of the previous and
next words, whose lexemes and POS tags are used
as features). For each such word, the tagger com-
putes the following conditional probability for the
tag tr:
p(tr|h) =
p(h, tr)?
t?r?T p(h, t
?r)
(1)
where T is the tag set, and the denominator is sim-
ply p(h). The joint probability of a history h and
a tag t is defined by:
p(h, t) = Z
k?
j=1
?fj(h,t)j (2)
where ?1, . . . , ?k are the model parameters,
f1, . . . , fk are the model?s binary features (indica-
tor functions), and Z is a normalization term for
ensuring that p(h, t) is a probability.
In the training phase the algorithm performs
maximum likelihood estimation for the ? param-
eters. These parameters are then used when the
model tags a new sentence (the test phase). For
words that appear 5 times or less in the training
data, the tagger extracts special features based on
the morphological properties of the word.
Combining Models. In general, we use the
same equation as MXPOST to compute joint prob-
abilities, and our training phase is identical to its
training phase. What we change are two things.
First, we add new contexts to the ?history? of a
word when it is considered as unknown (so Equa-
tion (2) is computed using different histories).
Second, we use a different equation for comput-
ing the conditional probability (below).
When the algorithm encounters an unknown
word wi in the context h during tagging, it per-
forms the web queries defined in Section 3.1. For
1277
each of the Nweb top resulting matches for each
query, {h?n|n ? [1, Nweb]}, the algorithm creates
its corresponding history representation hn. Con-
verting h?n to hn is required since in MXPOST a
history consists of an ordered set of words to-
gether with their POS tags, while h?n is an ordered
set of words without POS tags. Consequently, we
define hn to consist of the same ordered set of
words as h?n, and we tag each word using its most
frequent POS tag in the training corpus. If wi?1 or
wi?2 are unknown words, we do not tag them, let-
ting MXPOST use its back-off technique for such a
case (which is simply to compute the features that
it can and ignore those it cannot).
For each possible tag t ? T , its final assign-
ment probability to wi is computed as an average
between its probability given the various contexts:
p?(tr|h) =
porg(tr|h) +
?QNweb
n=1 pn(tr|hn)
QNweb + 1
(3)
where Q is the number of query types used (1, 2
or 3, see Section 3.1).
During inference, we use the two search space
constraints applied by the original MXPOST. First,
we apply a beam search procedure that consid-
ers the 10 most probable different tag sequences
of the tagged sentence at any point in the tagging
process. Second, known words are constrained to
be annotated only by tags with which they appear
in the training corpus.
4 Experimental Setup
Languages and Datasets. We experimented
with three languages, English, German and Chi-
nese, in various combinations of training and test-
ing domains (see Table 2). For English we used
the Penn Treebank WSJ corpus (WSJ) (Marcus
et al, 1993) from the economics newspapers do-
main, the GENIA corpus version 3.02p (GENIA)
(Kim et al, 2003) from the biological domain
and the British National Corpus version 3 (BNC)
(Burnard, 2000) consisting of various genres. For
German we used two different corpora from the
newspapers domain: NEGRA (Brants, 1997) and
TIGER (Brants et al, 2002). For Chinese we
used the Penn Chinese Treebank corpus version
5.0 (CTB) (Xue et al, 2002).
All corpora except of WSJ were split using
random sampling. For the NEGRA and TIGER
corpora we used the Stuttgart-Tuebingen Tagset
(STTS).
According to the annotation policy of the GE-
NIA corpus, only the names of journals, authors,
research institutes, and initials of patients are an-
notated by the ?NNP? (Proper Name) tag. Other
proper names such as general people names, tech-
nical terms (e.g. ?Epstein-Barr virus?) genes, pro-
teins, etc. are tagged by other noun tags (?NN? or
?NNS?). This is in contrast to the WSJ corpus, in
which every proper name is tagged by the ?NNP?
tag. We therefore omitted cases where ?NNP?
is replaced by another noun tag from the accu-
racy computation of the GENIA domain adapta-
tion scenario (see analysis in (Lease and Charniak,
2005)).
In all experimental setups except of WSJ-BNC
the training and test corpora are tagged with the
same POS tag set. In order to evaluate the WSJ-
BNC setup, we converted the BNC tagset to the
Penn Treebank tagset using the comparison table
provided in (Manning and Schuetze, 1999) (pages
141?142).
Baseline. As a baseline we implemented the
MXPOST tagger. An executable code for MXPOST
written by its author is available on the internet,
but we needed to re-implement it in order to in-
tegrate our technique. We made sure that our
implementation does not degrade results by run-
ning it on our WSJ scenario (see Table 2), which
is very close to the scenario reported in (Ratna-
parkhi, 1996). The accuracy of our implementa-
tion is 97.04%, a bit better than the numbers re-
ported in (Ratnaparkhi, 1996) for a WSJ scenario
using different sections.
Parameter Tuning. We ran experiments with
three values of the unknown word threshold T : 0
(only words that do not appear in the training data
are considered unknown), 5 and 8. That is, the al-
gorithm performs the web context queries and uti-
lizes the tag probabilities of equation 3 for words
that appear up to 0 ,5 or 8 times in the training
data.
Our algorithm has one free parameter Nweb, the
number of query results for each context type used
1278
Language Expe. name Training Development Test
English WSJ sections 2-21 (WSJ) section 22 (WSJ) section 23 (WSJ)
(2.4%,6.7%,8.4%)
English WSJ-BNC sections 2-21 (WSJ) 2000 BNC sentence 2000 BNC sentences
(8.4%,14.9%,17%)
English WSJ-GENIA WSJ sections 2-21 2000 GENIA sentences 2000 GENIA sentences
(22.7%,30.65%,32.9%)
German NEGRA 15689 NEGRA sentences 1746 NEGRA sentences 2096 NEGRA sentences
(11.1%,24.7%,28.7%)
German NEGRA-TIGER 15689 NEGRA sentences 2000 TIGER sentences 2000 TIGER sentences
(16%,27.3%,30.6%)
German TIGER-NEGRA 15689 TIGER sentences 1746 NEGRA sentences 2096 NEGRA sentence
(16.2%,27.9%,31.6%)
Chinese CTB 14903 CTB sentences 1924 CTB sentences 1945 CTB senteces
(7.4%,15.7%,18.1%)
Table 2: Details of the experimental setups. In the ?Test? column the numbers in parentheses are the
fraction of the test corpus words that are considered unknown, when the unknown word threshold is set
to 0, 5 and 8 respectively.
T = 0 T = 5 T = 8
WSJ WSJ- WSJ- WSJ WSJ- WSJ- WSJ WSJ- WSJ-
BNC GENIA BNC GENIA BNC GENIA
Baseline 83.56 61.22 80.05 88.79 68.71 80.12 89.45 70.25 80.8
Unlimited (-) 84.85 63.51 82.50 89.86 71.12 82.51 90.47 72.77 83.16
Top 5 (-) 84.25 64.24 82.75 89.73 71.21 82.78 90.36 72.74 83.46
Top 10 (-) 84.42 64.10 83.17 89.70 71.36 83.00 90.29 72.87 83.70
Top 10 (+) 84.67 64.47 82.60 89.83 72.12 82.54 90.29 73.53 83.22
best imp. 1.19 3.25 3.12 1.07 3.41 2.88 1.02 3.28 2.9
7.23% 8.38% 15.63% 9.54% 10.89% 14.48% 9.66% 11.02% 15.1%
T = 0 T = 5 T = 8
NEGRA NEGRA- TIGER- NEGRA NEGRA- TIGER- NEGRA NEGRA- TIGER-
TIGER NEGRA TIGER NEGRA TIGER NEGRA
Baseline 90.26 85.71 87.18 91.06 87.88 87.86 91.45 88.22 88.18
Unlimited (-) 91.22 86.60 89.49 91.66 88.22 89.84 92.25 89.08 90.23
Top 5 (-) 91.41 86.68 89.32 91.95 89.01 89.72 92.38 89.33 90.26
Top 10 (-) 91.06 86.83 89.50 91.25 88.36 89.84 92.33 89.38 90.26
Top 10 (+) 90.58 86.86 89.43 91.25 88.36 89.84 91.53 88.35 89.71
best imp. 1.15 1.15 2.32 0.89 1.13 1.98 0.93 1.16 2.08
11.8% 8.04% 18.09% 9.95% 9.32% 16.3% 10.87% 9.84% 17.59%
CTB
T = 0 T = 5 T = 8
Baseline 74.99 78.03 79.81
Unlimited (-) 77.01 80.46 81.94
Top 5 (-) 77.58 80.75 82.19
Top 10 (-) 77.43 80.68 82.45
Top 10 (+) 77.43 80.68 82.35
best imp. 2.59 2.72 2.74
10.35% 12.28% 13.57%
Table 3: Accuracy of unknown word tagging in the English (top table), German (middle table) and Chi-
nese (bottom table) experiments. Results are presented for three values of the unknown word threshold
parameter T : 0, 5 and 8. For all setups our models improves over the MXPOST baseline of (Ratnaparkhi,
1996). The bottom line of each table (?best imp.?) presents the improvement (top number) and error
reduction (bottom number) of the best performing model over the baseline. The best improvement is in
domain adaptation scenarios.
1279
in the probability computation of equation 3. For
each setup (Table 2) we ran several combinations
of query types and values of Nweb. We report re-
sults for the four leading combinations:
? Nweb = 5, left-side and right-side queries
(Top 5 (-)).
? Nweb = 10, left-side and right-side queries
(Top 10 (-)).
? Nweb = 10, replacement, left-side and right-
side queries (Top 10 (+)).
? Nweb = Unlimited (in practice, this means
1000, the maximum number of results pro-
vided by Yahoo! Boss), left-side and right-
side queries (Unlimited (-) ).
The order of the models with respect to their
performance was identical for the development
and test data. That is, the best parameter/queries
combination for each scenario can be selected us-
ing the development data. We experimented with
other parameter/queries combinations and addi-
tional query types but got worse results.
5 Results
The results of the experiments are shown in Ta-
ble 3. Our algorithm improves the accuracy of the
MXPOST tagger for all three languages and for all
values of the unknown word parameter.
Our experimental scenarios consist of three in-
domain setups in which the model is trained and
tested on the same corpus (the WSJ, NEGRA
and CTB experiments), and four domain adap-
tation setups: WSJ-GENIA, WSJ-BNC, TIGER-
NEGRA and NEGRA-TIGER.
Table 3 shows that our model is relatively
more effective in the domain adaptation scenar-
ios. While in the in-domain setups the error reduc-
tion values are 7.23% ? 9.66% (English), 9.95% ?
11.8% (German) and 10.35% ? 13.57% (Chinese),
in the domain adaptation scenarios they are 8.38%
? 11.02% (WSJ-BNC), 14.48% ? 15.63% (WSJ-
GENIA), 8.04% ? 9.84% (NEGRA-TIGER) and
16.3% ? 18.09% (TIGER-NEGRA).
Run Time. As opposed to previous approaches
to unknown word tagging (Blitzer et al, 2006;
Daume III, 2007), our algorithm does not contain
a step in which the base tagger is re-trained with a
corpus collected from the target domain. Instead,
when an unknown word is tackled at test time, a
set of web queries is run. This is an advantage for
flexible multi-domain POS tagging because pre-
processing times are minimized, but might cause
an issue of overhead per test word.
To show that the run time overhead created by
our algorithm is small, we measured its time per-
formance (using an Intel Xeon 3.06GHz, 3GB
RAM computer). The average time it took the best
configuration of our algorithm to process an un-
known word and the resulting total addition to the
run time of the base tagger are given in Table 4.
The average time added to an unknown word tag-
ging is less than half a second for English, even
less for German, and less than a second for Chi-
nese. This is acceptable for interactive applica-
tions that need to examine a given sentence with-
out being provided with any knowledge about its
domain.
Error Analysis. In what follows we try to ana-
lyze the cases in which our algorithm is most ef-
fective and the cases where further work is still
required. Due to space limitations we focus only
on the (Top 10 (+), T = 5) parameters setting,
and report the patterns for one English setup. The
corresponding patterns of the other parameter set-
tings, languages and setups are similar.
We report the errors of the base tagger that our
algorithm most usually fixes and the errors that
our algorithm fails to fix. We describe the base
tagger errors of the type ?POS tag ?a? is replaced
with POS tag ?b? (denoted by: a -> b)? using
the following data: (1) total number of unknown
words whose correct tag is ?a? that were assigned
?b? by the base tagger; (2) the percentage of un-
known words whose correct tag is ?a? that were
assigned ?b? by the base tagger; (3) the percentage
of unknown words whose correct tag is ?a? that
were assigned ?b? by our algorithm; (4) the per-
centage of mistakes of type (1) that were corrected
by our algorithm.
In the English WSJ-BNC setup, the base tagger
mistakes that our algorithm handles well (accord-
ing to the percentage of corrected mistakes) are:
(1) NNS -> VBZ (23, 3.73%, 0.8%, 65.2%); (2)
CD -> JJ (19 ,13.2% ,9.7% ,37.5%); (3) NN ->
1280
WSJ WSJ- WSJ- NEGRA NEGRA- TIGER- CTB
BNC GENIA TIGER NEGRA
Total addition 00:28:26 00:31:53 1:37:32 00:57:03 00:19:10 00:36:54 2:29:13
Avg. time per word 0.42 0.32 0.33 0.36 0.11 0.21 0.95
Table 4: The processing time added by the web based algorithm to the base tagger. For each setup results are presented for
the best performing model and for the unknown word threshold of 8. Results for the other models and threshold parameters
are very similar. The top line presents the total time added in the tagging of the full test data (hours:minutes:seconds). The
bottom line presents the average processing time of an unknown word by the web based algorithm (in seconds).
JJ (97, 6.17%, 5.3%, 27.8%); (4) JJ -> NN (69,
9.73%, 7.76%, 33.3%). The errors that were not
handled well by our algorithm are: (1) IN -> JJ
(70, 46.36% , 41%, 8.57%); (2) VBP -> NN (25,
19.5%, 21.9% , 0%).
In this setup, ?CD? is a cardinal number, ?IN? is
a preposition, ?JJ? is an adjective, ?NN? is a noun
(singular or mass), ?NNS? is a plural noun, ?VBP?
is a verb in non-third person singular present tense
and ?VBZ? is a verb in third person, singular
present tense.
We can see that no single factor is responsible
for the improvement over the baseline. Rather,
it is due to correcting many errors of different
types. The same general behavior is exhibited in
the other setups for all languages.
Multiple Unknown Words. Our method is ca-
pable of handling sentences containing several un-
known words. Query results in which ?*? is re-
placed by an unknown word are filtered. For
queries in which an unknown word appears as part
of the query (when it is one of the two right or left
non-?*? words), we let MXPOST invoke its own
unknown word heuristics if needed3.
In fact, the relative improvement of our algo-
rithm over the baseline is better for adjacent un-
known words than for single words. For ex-
ample, consider a sequence of consecutive un-
known words as correctly tagged if all of its
words are assigned their correct tag. In the
WSJ-GENIA scenario (Top 10 (+), T = 5), the
error reduction for sequences of length 1 (un-
known words surrounded by known words, 8767
sequences) is 8.26%, while for 2-words (2620
sequences) and 3-words (614 sequences) it is
11.26% and 19.11% respectively. Similarly, for
TIGER-NEGRA (same parameters setting) the er-
3They are needed only if the word is on the left of the
word to be tagged.
ror reduction is 6.85%, 8.07% and 18.18% for se-
quences of length 1 (4819) ,2 (1126) and 3 (223)
respectively.
6 Conclusions and Future Work
We presented a web-based algorithm for POS tag-
ging of unknown words. When an unknown word
is tackled at test time, our algorithm collects web
contexts of this word that are then used to improve
the tag probability computations of the POS tag-
ger.
In our experiments we used our algorithm to en-
hance the unknown word tagging quality of the
MXPOST tagger (Ratnaparkhi, 1996), a leading
state-of-the-art tagger, which we implemented for
this purpose. We showed significant improvement
(error reduction of up to 18.09%) for three lan-
guages (English, German and Chinese) in seven
experimental setups. Our algorithm is especially
effective in domain-adaptation scenarios where
the training and test data are from different do-
mains.
Our algorithm is fast (requires less than a sec-
ond for processing an unknown word) and can
handle test sentences coming from any desired un-
known domain without the costs involved in col-
lecting domain-specific corpora and retraining the
tagger. These properties makes it particularly ap-
propriate for applications that work on the web,
which is highly heterogeneous.
In future work we intend to integrate our al-
gorithm with additional POS taggers, experiment
with additional corpora and domains, and improve
our context extraction mechanism so that our al-
gorithm will be able to fix more error types.
References
Blitzer, John, Ryan McDonald, and Fernando Pereira,
2006. Domain adaptation with structural correspon-
1281
dence learning. EMNLP ?06.
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius and George Smith, 2002. The TIGER
Treebank. Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Brants, Thorsten, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Brants, Thorsten, 2000. Tnt: a statistical part-of-
speech tagger. In The Sixth Conference on Applied
Natural Language Processing.
Burnard, Lou, 2000. The British National Corpus
User Reference Guide. Technical Report, Oxford
University.
Chen, Qing, Mu Li, and Ming Zhou. 2007. Improving
query spelling correction using web search results.
In EMNLP-CoNLL ?07.
Chklovski, Timothy and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained semantic
verb relations. EMNLP ?04.
Clark, Stephen, James Curran and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabeled
data. CoNLL ?03.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. EMNLP ?02.
Daume III, Hal. 2007. Frustratingly easy domain
adaptation. ACL ?07.
Davidov, Dmitry, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ?07.
Huihsin, Tseng, Daniel Jurafsky, and Christopher
Manning. 2005. Morphological features help pos
tagging of unknown words across language vari-
eties. The Fourth SIGHAN Workshop on Chinese
Language Processing.
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford University Press,
2003.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. The Eighteenth International Con-
ference on Machine Learning.
Keller, Frank, Mirella Lapata, and Olga Ourioupina.
2002. Using the Web to Overcome Data Sparseness.
EMNLP ?02.
Keller, Frank, Mirella Lapata. 2003. . Computational
Linguistics, 29(3):459?484.
Lease, Matthew and Eugene Charniak. 2005. Pars-
ing Biomedical Literature. Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing..
Manning Chris and Hinrich Schuetze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press..
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
McClosky, David, Eugene Charniak, and Mark John-
son, 2006a. Effective self-training for parsing.
HLT-NAACL ?06.
Nakagawa, Tetsuji and Yuji Matsumoto. 2006. Guess-
ing parts-of-speech of unknown words using global
information. ACL-COLING ?06 .
PennBioIE. 2005. Mining the Bibliome Project..
http://bioie.ldc.upenn.edu.
Qiu, Likun, Changjian Hu and Kai Zhao. 2008. A
method for automatic POS guessing of Chinese un-
known words. COLING ?08.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. EMNLP ?96.
Reichart, Roi and Ari Rappoport. 2007. Self-Training
for Enhancement and Domain Adaptation of Statis-
tical Parsers Trained on Small Datasets. ACL ?07.
Reynolds, Sheila M. and Jeff A. Bilmes. 2005. Part-
of-speech tagging using virtual evidence and nega-
tive training. EMNLP ?06.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. EMNLP ?04.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. EMNLP
?00.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
NAACL ?03.
Vadas, David and James R. Curran. 2005. Tagging un-
known words with raw text features. Australasian
Language Technology Workshop 2005.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer,
2002. Building a large?scale annotated Chinese
corpus. ACL ?02.
1282
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1612?1623, Dublin, Ireland, August 23-29 2014.
Minimally Supervised Classification to Semantic Categories using
Automatically Acquired Symmetric Patterns
Roy Schwartz
1
Roi Reichart
2
1
Institute of Computer Science, The Hebrew University
{roys02|arir}@cs.huji.ac.il
2
Technion IIT
roiri@ie.technion.ac.il
Ari Rappoport
1
Abstract
Classifying nouns into semantic categories (e.g., animals, food) is an important line of research
in both cognitive science and natural language processing. We present a minimally supervised
model for noun classification, which uses symmetric patterns (e.g., ?X and Y?) and an iterative
variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a
predefined set of symmetric patterns, but extract them automatically from plain text, in an unsu-
pervised manner. We experiment with four semantic categories and show that symmetric patterns
constitute much better classification features compared to leading word embedding methods. We
further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-
the-art label propagation alternatives for this task. In experiments, our model obtains 82%-94%
accuracy using as few as four labeled examples per category, emphasizing the effectiveness of
simple search and representation techniques for this task.
1 Introduction
The role of language is to express meaning. In the field of NLP, there has been an increasingly grow-
ing number of approaches that deal with semantics. Among these are vector space models (Turney and
Pantel, 2010; Baroni and Lenci, 2010), lexical acquisition (Hearst, 1992; Dorow et al., 2005; Davidov
and Rappoport, 2006), universal cognitive conceptual annotation (Abend and Rappoport, 2013) and au-
tomatic induction of feature representations (Collobert et al., 2011). In this paper, we utilize extremely
weak supervision to classify words into fundamental cognitive semantic categories.
There are several types of semantic categories expressed by languages, e.g., objects, actions, and
properties. We follow human development, acquiring coarse-grained categories and distinctions before
detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete ?things? (Langacker,
2008, Ch. 4), roughly corresponding to nouns ? the main participants in linguistic clauses ? that are
universally present in the semantics of virtually all languages (Dixon, 2005).
Most works on noun classification to semantic categories require large amounts of human annotation
to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or
rely on language-specific resources such as WordNet (Evans and Or?asan, 2000; Or?asan and Evans, 2007).
Such heavy supervision is labor intensive and makes these models domain and language dependent.
Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can com-
pensate for the lack of input from the senses in text corpora. Our model therefore performs semantic
category classification using only a small number of labeled seed words per category. The experiments
we conduct show that such weak supervision is sufficient to construct a high quality classifier.
A key component of our model is the application of symmetric patterns. We define patterns to be
sequences of words and wildcards (e.g., ?X is a dog?, ?both X and Y?, etc.). Accordingly, symmet-
ric patterns are patterns that contain exactly two wildcards, where both wildcards are interchangeable.
Examples of symmetric patterns include ?X and Y?, ?X as well as Y? and ?neither X nor Y?.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/.
1612
Works that apply symmetric patterns in their model generally require expert knowledge in the form of a
pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract
symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This
algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about
high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain
and language independent.
Our model addresses semantic classification in a transductive setup. It takes advantage of word sim-
ilarity scores that are computed based on symmetric pattern features, and propagates information from
concepts with known classes to the rest of the concepts. For this aim we apply an iterative variant of the
k-Nearest Neighbors algorithm (denoted with I-k-NN) to a graph in which vertices correspond to nouns
and word pairs are connected with edges based on their participation in symmetric patterns.
We experiment with a subset of 450 nouns from the CSLB dataset (Devereux et al., 2013), which were
annotated with semantic categories by thirty human subjects. From the set of semantic categories in this
dataset, we select categories that are both frequent and have a high inter-annotator agreement (Section 2).
This results in a set of four semantic categories ? animacy, edibility, is a tool and is worn.
Our experiments show that our model performs very well even when only a small number of labeled
seed words are available. For example, on the task of binary classification with respect to a single
category, when using as few as four labeled seed words, classification accuracy reaches 82%-94%.
Furthermore, our model outperforms several strong baselines for this task. First, we compare our
model against a model that uses a deep neural network word embedding baseline (Collobert et al., 2011)
instead of our symmetric pattern based features, and applies the exact same I-k-NN algorithm. In recent
years, deep networks word embeddings obtained state-of-the-art results in several NLP tasks (Collobert
and Weston, 2008; Socher et al., 2013). However, in our task, features based on simple, intuitive and
easy to compute symmetric patterns, lead to substantially better performance (average improvement of
0.15 F1 points). Second, our model outperforms two baseline models that utilize the same symmetric
pattern classification features as in our model, but replace our simple I-k-NN algorithm with two leading
label propagation alternatives (the normalized graph cut (N-Cut) algorithm (Yu and Shi, 2003) and the
Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)). The average improvement over
these two baselines is 0.21 and 0.03 F1 points .
The rest of the paper is organized as follows. Section 2 describes our semantic classification task
and, particularly, the semantic classes that we aim to learn. Section 3 presents our method for automatic
symmetric patterns acquisition. Sections 4, 5 and 6 describe our model, experimental setup and results,
respectively. Related work is finally surveyed in Section 7.
2 Task Definition
The task we tackle in this paper is the classification of nouns into semantic categories. This section
defines the categories we address and the dataset we use.
Semantic Categorization of Concrete Nouns. We focus on concrete ?things? (Langacker, 2008),
which correspond to noun categories. Nouns are interesting because they are the most basic lexical
semantic categories. Specifically, children acquire nouns before any other category (Clark, 2009). More-
over, noun categories are generally not subjective. For example, it is hard to argue that a dog is not
an animal, or that an apple is inedible, in most reasonable contexts. The context independent nature of
nouns makes them appropriate for a type level classification task, such as the one we tackle. In order to
provide a better description of the categories we aim to predict, we now turn to discuss the CSLB dataset,
with which we experiment.
Dataset. We experiment with the CSLB property norms dataset (Devereux et al., 2013). In order to
prepare this data set, thirty human subjects were presented with 638 concrete nouns and were asked to
write the categories associated with each concept. Table 1 presents the top five categories for the nouns
apple and horse.
1613
Noun Categories
Apple is a fruit, does grow on trees, is green, is red, has pips seeds
Horse is ridden, is an animal, has four legs, has legs, has hooves
Table 1: Five most frequent semantic categories for the words apple and horse in the CSLB dataset.
Category Selection. The CSLB dataset consists of a total of 2725 semantic categories. We apply
a selection mechanism that provides us with a dataset in which (1) only noun categories (things) are
included; and (2) only semantic categories that are prominent across humans are considered. For this,
we apply the following filtering stages. First, since the vast majority of annotated categories are rare (for
example, 1691 categories are assigned to a single noun only), we set a minimum threshold of 35 nouns
per category (5% of the nouns). After removing highly infrequent categories, 28 are left. We then apply
an inter-annotator agreement criterion: for each semantic category c, we compute the average number
of human annotators that associated this category with a given noun, across the nouns annotated with c.
We select the category c only if the value of this statistic is higher than 10 subjects (1/3 of the subjects),
which results in a semantic category set of size 18. Finally, we discard categories, such as color and size,
that do not correspond to things. We are left with four noun semantic categories: animacy (animals),
edibility (food items), is a tool (tools), and is worn (clothes).
Interestingly, the resulting semantic categories can also be justified from a cognitive perspective. There
is a large body of work indicating that our categories relate to brain organization principles. For example,
Just et al. (2010) showed that food products and tools arouse different brain activation patterns. More-
over, a number of works showed that both animate objects and tools are represented in specific brain re-
gions. These works used neuroimaging methods such as functional magnetic resonance imaging (fMRI)
(Naselaris et al., 2012), electroencephalography (EEG) (Chan et al., 2011) and magnetoencephalogra-
phy (MEG) (Sudre et al., 2012). See (Martin, 2007) for a detailed survey. This parallel evidence to the
prominence of our categories provides substance for intriguing future research.
3 Symmetric Patterns
Patterns. In this work, patterns are combinations of words and wildcards, which provide a structural
phrase representation. Examples of patterns include ?X and Y?, ?X such as Y?, ?X is a country?, etc.
Patterns can be used to extract various relations between words. For example, patterns such as ?X of a
Y? (?basement of a building?) can be useful for detecting the meronymy (part-of) relation (Berland and
Charniak, 1999). Symmetric patterns (e.g., ?X and Y?, ?France and Holland?), which we use in this
paper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002).
Symmetric Patterns. Symmetric patterns are patterns that contain exactly two wildcards, and where
these wildcards are interchangeable. Examples of symmetric patterns include ?X and Y?, ?X or Y? and
?X as well as Y?. Previous works have shown that word pairs that participate in symmetric patterns bare
strong semantic resemblance, and consequently, that these patterns can be used to cluster words into
semantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorow
et al., 2005; Davidov and Rappoport, 2006). A key observation of this paper is that symmetric patterns
can be also used for semantic classification, where recall is as important as precision.
Flexible Patterns. It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008;
Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsu-
pervised manner. The key idea that makes this procedure possible is the concept of ?flexible patterns?,
which are composed of high frequency words (HFW) and content words (CW). Every word in the lan-
guage is defined as either HFW or CW, based on the number of times this word appears in a large corpus.
This clustering procedure is applied by traversing a large corpus, and marking words that appear with
corpus frequency higher than a predefined threshold t
1
as HFWs, and words with corpus frequency lower
than t
2
as CWs.
1
1
We follow (Davidov and Rappoport, 2006) and set t
1
= 10
?5
, t
2
= 10
?3
. Note that some words are marked both as HFW
and as CW. See (Davidov and Rappoport, 2008) for discussion.
1614
The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepo-
sitions, determiners, etc.) while CWs are comprised mostly of content words (nouns, verbs, adjectives
and adverbs). This coarse grained clustering is useful for pattern extraction from plain text, since lan-
guage patterns tend to use fixed function words, while content words change from one instance of the
pattern to another (Davidov and Rappoport, 2006).
Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words to
CWs and HFWs, extracting all pattern instances. An extracted pattern instance consists of CW wildcards
and the actual words replacing the HFWs in the pattern type. Consider the sentence ?The boy is happy
and joyful?. Replacing the content words with the CW wildcard results in ?The CW is CW and CW?.
From this intermediate representation, we extract word sequences of a given length constraint and denote
them as flexible patterns.
2
The flexible patterns of length 5 extracted from this sentence are ?The CW is
CW and? and ?CW is CW and CW?. The reader is referred to (Davidov and Rappoport, 2006) for more
details.
Automatically Extracted Symmetric Patterns. Most models that incorporate symmetric patterns use
a predefined set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we apply
an automatic, completely unsupervised procedure for symmetric pattern extraction. This procedure,
described in Algorithm 1, is adopted from (Davidov and Rappoport, 2006).
The procedure first extracts flexible patterns that contain exactly two CW wildcards. It then selects
those flexible patterns in which both CWs are interchangeable. That is, it selects a pattern p if every
word pair CW
1
, CW
2
that participates in p indicates with high probability that the word pair C
2
, C
1
also participates in p. For example, for the symmetric pattern ?CW and CW?, both ?cats and dogs?
and ?dogs and cats? are semantically plausible expressions, and are therefore likely to appear in a large
corpus. On the other hand, the flexible pattern ?CW such as CW? is asymmetric, as exemplified in
expressions like ?countries such as France?, where replacing the CWs does not result in a semantically
plausible expression (# ?France such as countries?). The selection process is done by computing the
proportion of CW
1
, CW
2
pairs that participate in p for which CW
2
, CW
1
also participates in p. Patterns
for which this proportion exceed a certain threshold are selected.
We apply Algorithm 1 on the google books 5-gram corpus (Michel et al., 2011)
3
and extract 20 sym-
metric patterns. Some of the more interesting symmetric patterns extracted using this algorithm include
?CW and the CW?, ?from CW to CW?, ?CW rather than CW? and ?CW versus CW?. In the next section
we present our approach to semantic classification, which makes use of automatically acquired symmet-
ric patterns for word similarity computations.
4 Model
In this section we present our model for binary word classification according to a single semantic category
in a minimally-supervised, transductive setup. Given a set of words, we label a small number of words
with their correct label according to the category at hand (+1 for words that belong to the category, -1
for words that do not belong to it). Our model is based on an undirected weighted graph, in which
vertices correspond to words, and edges correspond to relations between words. Our goal is to classify
the unlabeled words (vertices) in the graph through a label propagation process. We now turn to describe
our model in detail.
Graph Construction. We construct our graph such that an edge is added between two words (vertices)
if both words participate in a symmetric pattern. The edge generation process is performed as follows.
We first apply our symmetric pattern extraction procedure (Algorithm 1), and denote the set of selected
symmetric patterns with P . We then traverse a large corpus
4
and extract all word pairs that participate
in any pattern p ? P . We denote the number of occurrences of a word pair (w
1
, w
2
) in such patterns
with f
w
1
,w
2
. Finally, we select all word pairs (w
1
, w
2
) for which min(f
w
1
,w
2
, f
w
2
,w
1
) > ?. Each such
2
We set the maximal flexible pattern length to be 5.
3
https://books.google.com/ngrams
4
We use google books 5-grams (Michel et al., 2011).
1615
Algorithm 1 Symmetric pattern extraction
1: procedure EXTRACT SYMMETRIC PATTERNS(C,W )
2: . C is a large corpus, W is a lexicon
3: . Traverse C and extract all flexible patterns of length 3-5 that appear in C and contain exactly two content words
4: P ? extract flexible patterns(C,W )
5: for p ? P do
6: if p appears in <10
?6
of the sentences in C then
7: Discard p and continue
8: end if
9: G
p
? a directed graph s.t. V (G
p
)?W ,E(G
p
)?{(w
1
, w
2
)?W
2
:w
1
,w
2
participate in at least one instance of p}
10: . An undirected graph based on the bidirectional edges of the G
p
11: symG
p
? an undirected graph: {(w
1
), (w
1
,w
2
) : (w
1
,w
2
) ? E(G
p
) ? (w
2
, w
1
) ? E(G
p
)}
12: . Two measures of symmetry
13: M
1
?
|
V (symG
p
)
|
|
V (G
p
)
|
,M
2
?
|
E(symG
p
)
|
|
E(G
p
)
|
14: . Symmetric pattern candidates are those with high M
1
and M
2
values
15: if min (M
1
,M
2
) < 0.05 then
16: Discard p
17: end if
18: end for
19: for p ? P do
20: . E.g., ?CW and CW? is contained in ?both CW and CW?
21: if ?p
?
? P s.t. p
?
is contained in p then
22: Discard p
23: end if
24: end for
25: return The top 20 members of P with the highest M
1
value
26: end procedure
pair is connected with an edge e
w
1
,w
2
in the graph, where the edge weight (denoted with w
w
1
,w
2
) is the
geometric mean between f
w
1
,w
2
and f
w
2
,w
1
.
Label Propagation. Given a small number of annotated words (vertices), our goal is to propagate the
information these words convey to other words in the graph. To do so, we apply an iterative variant of the
k-Nearest Neighbors algorithm (I-k-NN). This iterative variant is required due to graph sparsity; when
starting with a small set of labeled vertices, most unlabeled vertices do not have any labeled neighbor, and
thus running the standard k-NN algorithm would result in classifying a very small number of vertices.
Our approach is to run iterations of the k-NN algorithm, and thus propagate information to additional
vertices at each iteration. At each k-NN step, the algorithm selects words that have at least one labeled
neighbor. From this set, only the words that have the highest ratio of neighbors with the same label are
selected, and are assigned with this label.
Consider a simple example. Say we have three candidate vertices a, b and c, where a has one neighbor
with label +1 (ratio(a) = 1/1 = 1.0), b has two neighbors with label -1 (ratio(b) = 2/2 = 1.0) and
c has three neighbors with label +1 and one neighbor with label -1 (ratio(c) = max(3, 1)/4 = 3/4).
Then, a and b are selected and are assigned with +1 and ?1, respectively.
Seed Expansion. In minimally supervised setups like ours, the model is initialized with a small set of
labeled seed examples. A natural approach in such settings is to apply a seed expansion step, in order to
obtain a larger set of labeled seeds. Our method uses the same graph construction procedure described
above, but uses a larger edge generation threshold ? >> ?.
5
We then apply an iterative procedure that
labels a vertex v with a label l if either (a) v is directly connected to ? of the vertices labeled with l or (b)
v is connected to ?
l
of the neighbors of vertices labeled with l.
6
This procedure is run iteratively until no
more vertices meet any of the criteria (a) or (b).
5
Using a larger threshold results in a sparser graph. Nevertheless, each edge in this graph is more likely to represent a real
semantic relation.
6
? and ?
l
are hyperparameters tuned on our development set (see Section 5.2).
1616
5 Experimental Setup
5.1 Baselines
We compare our model to two types of baselines. The first (Classification Features Baselines) utilizes
the I-k-NN algorithm, along with a different set of classification features. The second (Label Propa-
gation Baselines) utilizes the same classification features as we do, but replaces I-k-NN with a more
sophisticated label propagation algorithm.
5.1.1 Classification Features Baselines
In this set of baselines, we use different methods for building our graph. Concretely, instead of adding
edges for pairs of words that appear in the same symmetric pattern, we use word similarity measures
based on different feature sets as described below. The process of building the graph using the baseline
word similarity measures is described in Section 5.2.
SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word
representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,
7
a deep network based
word embedding method, which has been used to produce state-of-the-art results in several NLP tasks,
including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine
similarity between two word embeddings as a word similarity measure.
Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al.,
1992).
8
This clustering, in which words share a cluster if they tend to appear in the same lexical con-
text, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al.,
2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a
simple contextual preference similarity correlates with similarity in semantic categorization better than
symmetric pattern features.
The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph
distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word simi-
larity measure for building our graph.
5.1.2 Label Propagation Baselines
In this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still using
the symmetric pattern features for word similarity computations.
N-Cut. This baseline applies the normalized graph cut algorithm (Yu and Shi, 2003)
9
for label propa-
gation. Given a graphG = (V,E) and two sets of verticesA,B ? V , this algorithm defines links(A,B)
to be the sum of edge weights between A and B. The objective of the algorithm is to find the clusters
A, V \ A that minimize
links(A,V \A)
links(A,V )
. The algorithm of (Yu and Shi, 2003) is particularly efficient for
this problem as it avoids eigenvector computations which may become computationally prohibitive for
large graphs (for more details, see their paper). In order to encode information about our labeled seed
words, we hard-code a large negative value (-100000) to the weights of edges between seed words with
different labels (positive and negative).
MAD. The Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)
10
is an extension
of the Adsorption algorithm (Baluja et al., 2008). MAD is a stochastic graph-based label propagation
algorithm which has shown to have a number of attractive theoretical properties and demonstrated good
experimental results.
7
The word embeddings were downloaded from http://ml.nec-labs.com/senna/
8
We use the clusters induced by (Koo et al., 2008), who applied the Brown algorithm implementation of (Liang,
2005) to the BLLIP corpus (Charniak et al., 2000). http://www.people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
9
http://www.cis.upenn.edu/
?
jshi/software/Ncut_9.zip
10
http://github.com/parthatalukdar/junto
1617
5.2 Experiments
Graph Construction. We experiment with the CSLB dataset (Devereux et al., 2013), consisting of 638
nouns, annotated with their semantic categories by thirty human subjects. We first omit all nouns that
are annotated as having more than one sense, and use the remaining 603 nouns to build our graph. From
these nouns, 146 nouns are annotated as animate, 115 as edible, 50 as wearable and 35 as tools.
11
We
then discard nouns that have less than two neighbors, which results in a final set of 450 nouns (vertices).
The graphs used in the classification features baselines are different than those used by the models that
use our symmetric pattern classification features, since the features define the graph structure (Section 4).
In order to provide a meaningful comparison, we build graphs with the same number of vertices for each
of these baselines. We do so by selecting the n edges with the highest weight, together with the set of
vertices connected by these edges, such that the resulting graph has 450 vertices. Working with these
sets of vertices is the optimal setting for these baselines, as the resulting graphs are the ones with the
highest possible edge weights for graphs with 450 vertices.
12
Parameter Tuning. In order to avoid adding additional labeled examples for the sake of parameter
tuning, we set the hyperparameter values to the ones for which each model performs best on an auxiliary
semantic classification task. Concretely, we experiment with a fifth semantic category (audibility),
13
which is not part of our evaluation setting, for parameter tuning. Note that this results in our model
having the same hyperparamter values for all four classification tasks.
In order to ensure that the models assign all participating words with labels, we set ?=3, where ? is
the minimal number of times a word pair should appear in the same symmetric pattern in order to have
an edge in our graph (See Section 4). In our seed expansion procedure, where we search for seeds whose
label is predicted with high confidence, only word pairs that appear at least ?=50 times in the same
symmetric pattern are assigned an edge in the graph. We set the seed expansion procedure parameters to
be ? = 0.6, ?
+1
= 0.5, ?
?1
= 0.2.
Evaluation. For each classification task, we run experiments with 4, 10, 20 and 40 labeled seed words.
In each setting, half of the labeled seed words are assigned a positive label and the other half are assigned
a negative label. For each semantic category and labeled seed set size, we repeat our experiment 1000
times, each of which with a different set of randomly selected labeled seed examples, and report the
average results. We report both accuracy (number of correct labels divided by number of vertices in
the graph) and F1 score, which is the harmonic mean of p (the average precision across labels) and r
(average recall across labels).
These two measures represent complementary aspects of our results. On the one hand, accuracy is
the most natural classification performance measure. On the other hand, the number of positive labels is
substantially smaller than the number of negative labels,
14
and thus this measure can be manipulated: a
dummy model that always assigns the negative label gets a high accuracy. The F1 score controls against
such models by assigning them low scores.
6 Results
Our experiments are designed to explore two main questions: (a) the value of symmetric patterns as
semantic classification features, compared to state-of-the-art word clustering and embedding methods;
and (b) the required complexity of an algorithm that can propagate information about semantic simi-
larity. Particularly, we test the value of our simple I-k-NN algorithm compared to more sophisticated
alternatives.
A Minimally Supervised Setting. Our first set of experiments is in a minimally supervised setting
where only two positive and two negative examples are available for each binary classification task. This
11
Some words are classified as belonging to more than one category (e.g., ?chicken? is both animate and edible).
12
The resulting graphs are actually denser than the symmetric patterns-based graph: 14K and 9K edges for the Brown and
SENNA graphs, respectively, compared to < 5K edges in the symmetric patterns graph.
13
We used four labeled seed words in these experiments.
14
Only 6-25% of the nouns have a positive label.
1618
Animacy Edibility is worn is a tool
SP SENNA Brown SP SENNA Brown SP SENNA Brown SP SENNA Brown
Acc.
MAD 80.4% 77.7% 12.0% 75.0% 56.5% 14.8% 82.7% 66.8% 14.7% 73.3% 67.7% 12.2%
N-Cut 71.4% 60.4% 51.2% 75.5% 59.4% 50.9% 83.3% 71.5% 51.4% 82.7% 77.1% 52.0%
I-k-NN 85.1% 76.0% 55.5% 82.2% 56.8% 68.0% 94.1% 70.9% 66.7% 82.0% 75.7% 65.0%
F1
MAD 0.77 0.76 0.18 0.69 0.55 0.24 0.71 0.56 0.22 0.58 0.47 0.17
N-Cut 0.49 0.45 0.46 0.51 0.44 0.45 0.61 0.56 0.41 0.56 0.50 0.38
I-k-NN 0.78 0.70 0.48 0.71 0.53 0.62 0.86 0.59 0.55 0.64 0.52 0.51
Table 2: Accuracy and F1 score comparison between our model and the baselines. The columns cor-
respond to the type of classification features used by the model: SP ? symmetric patterns, SENNA ?
word embeddings extracted using deep networks (Collobert et al., 2011), Brown ? Brown word clus-
tering (Brown et al., 1992). The rows correspond to the algorithms applied by the model: N-Cut ? the
normalized graph cut algorithm (Yu and Shi, 2003), MAD ? the modified adsorption algorithm (Talukdar
and Crammer, 2009), I-k-NN ? our iterative k-NN algorithm. Our model (I-k-NN + SP) is superior in all
cases, except for the accuracy of the ?is a tool? semantic category, where it is second only to N-Cut+SP.
5 10 15 200.55
0.6
0.65
0.7
0.75
0.8
0.85
#training_samples
F1 Sco
re
 
 
SENNA (MAD)Brown (I?k?NN)Symmetric Patterns (I?k?NN)
(a) Classification Features Comparison
5 10 15 200.45
0.50.55
0.60.65
0.70.75
0.80.85
#training_samples
F1 Sco
re
 
 
N?Cut (Symmetric Patterns)MAD (Symmetric Patterns)I?k?NN (Symmetric Patterns)
(b) Algorithm Comparison
5 10 15 200.5
0.550.6
0.650.7
0.750.8
0.85
#training_samples
F1 Sco
re
 
 
I?k?NN, SENNAMAD, SENNAMAD, Symmetric PatternsI?k?NN, Symmetric Patterns
(c) Top four Best Models
Figure 1: (a) Comparison of the different classification features. The figure shows the F1 scores of the
best model that uses each of the feature sets (the label propagation algorithm used in each model appears
in parentheses). (b) Comparison of the different label propagation algorithms. The figure shows the F1
scores of the best model that uses each of the algorithms (the classification feature sets used in each model
appears in parentheses. It is always symmetric patterns). (c) The four best overall models (algorithm +
classification feature set). The figures show that the symmetric pattern feature set is superior to the other
feature sets, and that I-k-NN is superior or comparable to the other label propagation algorithms.
setup enables us to explore the performance of our model when the amounts of labeled training data is
taken to the possible minimum.
Table 2 presents our results. With respect to objective (a), the table clearly demonstrates that symmetric
patterns lead to much better results compared to the alternatives. Particularly, for all four semantic
categories, and across both evaluation measures, it is a model that utilizes symmetric pattern classification
features that achieves the best results. The average difference between the best model that uses symmetric
patterns and the best model that does not is 12.5% accuracy and 0.13 F1 points. The dominance of
symmetric pattern classification features is further demonstrated by the fact that a model that uses these
features always performs better than a model that uses the same algorithm but different features.
With respect to objective (b) the table shows that I-k-NN provides a large improvement in seven out
of eight (category ? evaluation measure) settings. The average difference between the best model that
utilizes I-k-NN and the best model that applies a different algorithm is 5.4% accuracy and 0.06 F1 points.
Analysis of Labeled Seed Set Size. In order to get a wider perspective on our model, we repeated our
experiments with various sizes of the labeled seed set: 5,10 and 20 positive and negative labeled examples
per semantic category. For brevity, only the F1 score results of the edibility category are presented. The
trends observed on the other semantic categories (as well as when using the accuracy measure) are very
similar.
Figure 1a compares the different classification features. For each feature f , results of the best per-
forming model that uses f are shown. The results reveal that symmetric patterns clearly outperform the
other features. The average differences between the best symmetric patterns-based model and the best
1619
models that use the other features are 0.15 (SENNA) and 0.16 (Brown) F1 points.
Figure 1b compares the different label propagation algorithms. For each algorithm a, results for the
best performing model that uses a are presented. The results reveal that the I-k-NN algorithm outper-
forms both algorithms by 0.03 (MAD) and 0.21 (N-Cut) F1 points. The results also show that for all
algorithms, the best performing model uses symmetric patterns classification features, which further
demonstrates the dominance of these features.
Finally, Figure 1c presents the four top performing models (algorithm + classification feature). In
accordance with the other findings presented in this section, the top two models, which outperform the
other models by a large margin, apply symmetric pattern classification features.
Seed Expansion Effect. Our model uses a seed expansion procedure in order to expand a small set of
labeled seed words to a larger set (see Section 4). In order to assess the quality of this procedure we
compute, for each semantic category, the average size of the expanded set and the accuracy of the new
seeds (i.e., the proportion of new seeds that are labeled correctly). Results show that the initial set is
increased from four seeds (two positive + two negative) to 48-52, and that the accuracy of the new seeds
is as high as 88-99%. Our experiments also show that this procedure provides a substantial performance
boost to our I-k-NN algorithm, which obtains a 7.2% accuracy and 0.05 F1 points improvement (averaged
over the four semantic categories) when applied with the expanded set of labeled seed words compared
to the original set of size four.
7 Related Work
Classification into Semantic Categories. Several works tackled the task of semantic classification,
mostly focusing on animacy, concreteness and countability. The vast majority of these works are either
supervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005;
?vrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) or
make use of external, language-specific resources such as WordNet (Or?asan and Evans, 2001; Or?asan
and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a
small set of labeled seed words.
Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences
in instances of hand-crafted patterns such as ?X who Y? and ?X and his Y?. While their model uses
patterns that are tailored to the animacy and gender categories, our model uses automatically induced
patterns and is thus applicable to a range of semantic categories.
Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais,
1997) based classification features. They used their model to classify nouns into the concrete/abstract
category using 40 labeled seed words . Unlike our model, which requires only a small set of labeled seeds,
their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting the
seed set of labeled words that are used for propagation. Our model, on the other hand, does not require
any seed selection procedure, and utilizes a randomly selected set of labeled seed words.
Lexical Acquisition. Another line of work focused on the acquisition of semantic categories. In this
setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for
precision. Our model tackles a different task, namely the classification of words according to a given
category where both recall and precision are to be optimized.
Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of
symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language
specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et
al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised
parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model
automatically induces symmetric patterns, obtaining high quality results without relying on any type of
language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and
Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label
1620
seeds to achieve good performance; in contrast, our work performs very well with a randomly selected
set of labeled seed words.
8 Conclusion
We presented a minimally supervised model for noun classification into coarse grained semantic cate-
gories. Our model obtains 82%-94% accuracy on four semantic categories even when using only four
labeled seed words per category. We showed that our modeling decisions ? using symmetric patterns as
classification features and a simple iterative k-NN algorithm for label propagation ? lead to a substantial
performance gain compared to state-of-the-art, more sophisticated, alternatives. Our results demonstrate
the applicability of minimally supervised methods for semantic classification tasks. Future work will
include modifying our model to support other, more fine-grained types of semantic categories, includ-
ing adjectival categories (properties). We also plan to work on token-level word classification, and thus
support multi-sense words, as well as demonstrate the power of unsupervised patterns acquisition for
multilingual setups.
Acknowledgments
This research was funded (in part) by the Harry and Sylvia Hoffman leadership and responsibility pro-
gram (for the first author), the Google Faculty research award (for the second author), the Intel Collab-
orative Research Institute for Computational Intelligence (ICRI-CI) and the Israel Ministry of Science
and Technology Center of Knowledge in Machine Learning and Artificial Intelligence (Grant number
3-9243).
References
O. Abend and A. Rappoport. 2013. Universal conceptual cognitive annotation (UCCA). In Proc. of ACL.
T. Baldwin and F. Bond. 2003. A plethora of methods for learning English countability. In Proc. of EMNLP.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random walks through the view graph. In Proc. of WWW, pages
895?904. ACM.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Proc. of ACL.
S. R. Bowman and H. Chopra. 2012. Automatic Animacy Classification. In Proc. of NAACL-HLT Student
Research Workshop.
P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4):467?479.
A. M. Chan, J. M. Baker, E. Eskandar, D. Schomer, I. Ulbert, K. Marinkovic, S. S. Cash, and E. Halgren. 2011.
First-pass selectivity for semantic categories in human anteroventral temporal lobe. The Journal of Neuro-
science, 31(49):18119?18129.
E. Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and M. Johnson. 2000. BLLIP 198789 WSJ Corpus Release 1,
LDC No. LDC2000T43. Linguistic Data Consortium.
A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proc. of CoNLL.
E. V. Clark. 2009. First language acquisition. Cambridge University Press.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks
with multitask learning. In Proc. of ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing
(almost) from scratch. JMLR, 12:2493?2537.
1621
D. Davidov and A. Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric pat-
terns and high frequency words. In Proc. of ACL-Coling.
D. Davidov and A. Rappoport. 2008. Unsupervised discovery of generic relationships using pattern clusters and
its evaluation by automatically generated SAT analogy questions. In Proc. of ACL-HLT.
B. J. Devereux, L. K. Tyler, J. Geertzen, and B. Randall. 2013. The centre for speech, language and the brain
(CSLB) concept property norms. Behavior research methods, pages 1?9.
R. M. Dixon. 2005. A semantic approach to English grammar. Oxford University Press.
B. Dorow, D. Widdows, K. Ling, J. P. Eckmann, D. Sergi, and E. Moses. 2005. Using Curvature and Markov
Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination.
R. Evans and C. Or?asan. 2000. Improving anaphora resolution by identifying animate entities in texts. In Proc. of
DAARC.
V. Hatzivassiloglou and K. R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of ACL.
M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of Coling ? Volume 2.
H. Ji and D. Lin. 2009. Gender and Animacy Knowledge Discovery from Web-Scale N-Grams for Unsupervised
Person Mention Detection. In Proc. of PACLIC.
M. A. Just, V. L. Cherkassky, S. Aryal, and T. M. Mitchell. 2010. A neurosemantic theory of concrete noun
representation based on the underlying brain codes. PloS one, 5(1):e8622.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL-HLT.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage
graphs. In Proc. of ACL-HLT.
O. Y. Kwong. 2011. Measuring concept concreteness from the lexicographic perspective. In Proc. of PACLIC.
T. K. Landauer and S. T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.
R. W. Langacker. 2008. Cognitive grammar: A basic introduction. Oxford University Press.
P. Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, Massachusetts Institute of Tech-
nology.
J. M. Mandler. 2004. The foundations of mind: Origins of conceptual thought. Oxford University Press New
York.
A. Martin. 2007. The representation of object concepts in the brain. Annual Review of Psychology, 58:25?45.
J. B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Or-
want, et al. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176?
182.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In
Proc. of NAACL.
J. L. Moore, C. J. Burges, E. Renshaw, and W.-t. Yih. 2013. Animacy Detection with Voting Models. In Proc. of
EMNLP.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. Reinforcing English countability prediction with one
countability per discourse property. Proc. of ACL-Coling.
T. Naselaris, D. E. Stansbury, and J. L. Gallant. 2012. Cortical representation of animate and inanimate objects in
complex natural scenes. Journal of Physiology-Paris, 106(5):239?249.
C. Or?asan and R. Evans. 2001. Learning to identify animate references. In Proc. of the Workshop on Computa-
tional Natural Language.
C. Or?asan and R. Evans. 2007. NP Animacy Identification for Anaphora Resolution. JAIR, 29:79?103.
1622
L. ?vrelid. 2005. Animacy classification based on morphosyntactic corpus frequencies: some experiments with
Norwegian nouns. In Proc. of the Workshop on Exploring Syntactically Annotated Corpora, pages 1?11.
J. Peng and K. Araki. 2005. Detecting the countability of english compound nouns using web-based models. In
Proc. of IJCNLP.
E. Riloff and J. Shepherd. 1997. A corpus-based approach for building semantic lexicons. In Proc. of EMNLP.
R. Schwartz, O. Tsur, A. Rappoport, and M. Koppel. 2013. Authorship Attribution of Micro-Messages. In Proc.
of EMNLP.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proc.
of ACL-Coling.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank. In Proc. of EMNLP.
G. Sudre, D. Pomerleau, M. Palatucci, L. Wehbe, A. Fyshe, R. Salmelin, and T. Mitchell. 2012. Tracking neural
coding of perceptual and semantic features of concrete nouns. NeuroImage, 62(1):451?463.
P. P. Talukdar and K. Crammer. 2009. New regularized algorithms for transductive learning. In ECML-PKDD,
pages 442?457. Springer.
O. Tsur, D. Davidov, and A. Rappoport. 2010. ICWSM ? a great catchy name: Semi-supervised recognition of
sarcastic sentences in online product reviews. In Proc. of ICWSM.
P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of
artificial intelligence research, 37(1):141?188.
P. Turney, Y. Neuman, D. Assaf, and Y. Cohen. 2011. Literal and metaphorical sense identification through
concrete and abstract context. In Proc. of EMNLP.
P. D. Turney. 2008. The latent relation mapping engine: Algorithm and experiments. Journal of Artificial Intelli-
gence Research, 33:615?655.
R. C. Wang and W. W. Cohen. 2009. Automatic set instance extraction using the web. In Proc. of ACL-IJCNLP.
D. Widdows and B. Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. of Coling.
X. Xing, Y. Zhang, and M. Han. 2010. Query difficulty prediction for contextual image retrieval. In Advances in
Information Retrieval, pages 581?585. Springer.
S. X. Yu and J. Shi. 2003. Multiclass spectral clustering. In Proc. of ICCV.
1623
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 325?334,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Tense Sense Disambiguation: a New Syntactic Polysemy Task
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Polysemy is a major characteristic of natu-
ral languages. Like words, syntactic forms
can have several meanings. Understanding the
correct meaning of a syntactic form is of great
importance to many NLP applications. In this
paper we address an important type of syn-
tactic polysemy ? the multiple possible senses
of tense syntactic forms. We make our dis-
cussion concrete by introducing the task of
Tense Sense Disambiguation (TSD): given a
concrete tense syntactic form present in a sen-
tence, select its appropriate sense among a
set of possible senses. Using English gram-
mar textbooks, we compiled a syntactic sense
dictionary comprising common tense syntac-
tic forms and semantic senses for each. We an-
notated thousands of BNC sentences using the
defined senses. We describe a supervised TSD
algorithm trained on these annotations, which
outperforms a strong baseline for the task.
1 Introduction
The function of syntax is to combine words to ex-
press meanings, using syntactic devices such as
word order, auxiliary words, and morphology (Gold-
berg, 1995). Virtually all natural language devices
used for expressing meanings (e.g., words) exhibit
polysemy. Like words, concrete syntactic forms (the
sentence words generated by specific syntactic de-
vices) can have several meanings. Consider the fol-
lowing sentences:
(a) They are playing chess in the park.
(b) They are playing chess next Tuesday.
Both contain the concrete syntactic form ?are play-
ing?, generated by the abstract syntactic form usu-
ally known as ?present progressive? (am/is/are + V-
ing). In (a), the meaning is ?something happening
now?, while in (b) it is ?a plan to do something in the
future?. Note that the polysemy is of the syntactic
form as a unit, not of individual words. In particu-
lar, the verb ?play? is used in the same sense in both
cases.
In this paper we address a prominent type of syn-
tactic form polysemy: the multiple possible senses
that tense syntactic forms can have. Disambiguat-
ing the polysemy of tense forms is of theoretical
and practical importance (Section 2). To make our
discussion concrete, we introduce the task of Tense
Sense Disambiguation (TSD): given a concrete tense
syntactic form in a sentence, select its correct sense
among a given set of possible senses (Section 3).
The disambiguation of polysemy is a fundamental
problem in NLP. For example, Word Sense Disam-
biguation (WSD) continues to attract a large number
of researchers (Agirre and Edmonds, 2006). TSD
has the same structure as WSD, with different dis-
ambiguated entities.
For experimenting with the TSD task, we com-
piled an English syntactic sense dictionary based
on a thorough study of three major English gram-
mar projects (Section 4). We selected 3000 sen-
tences from the British National Corpus containing
4702 concrete syntactic forms, and annotated each
of these by its sense (Section 5).We developed a su-
pervised learning TSD algorithm that uses various
feature types and takes advantage of the task struc-
ture (Section 6). Our algorithm substantially outper-
325
forms the ?most frequent sense? baseline (Section 7).
TSD is fundamental to sentence understanding
and thus to NLP applications such as textual infer-
ence, question answering and information retrieval.
To the best of our knowledge, this is the first paper to
address this task. In Section 8 we discuss research
directions relevant to TSD placing the new task in
the context of the previous research of syntactic am-
biguity resolution.
2 TSD Motivation
In this work we follow linguistics theories that posit
that tense does not directly reflect conceptual time as
one might think. Dinsmore (1991) and Cutrer (1994)
explain that the same tense may end up indicating
very different objective time relations relative to the
sentence production time.
Fauconnier (2007) exemplifies such phenomena.
In the following sentences, the present tense corre-
sponds to the future time: (1) The boat leaves next
week. (2) When he comes tomorrow, I will tell him
about the party. (3) If I see him next week, I will ask
him to call you.
In contrast, the following present tense sentences
talk about events that happened in the past: (1) I am
walking down the street one day when suddenly this
guy walks up to me. (2) He catches the ball. He
runs. He makes a touchdown. (morning-after sports
report).
Another set of examples is related to the past
tense. In the following sentences it corresponds to
a present time: (1) Do you have a minute? I wanted
to ask you a question. (2) I wish I lived closer to my
family now. In contrast, in the following two sen-
tences, it corresponds to a future time: (1) If I had
the time next week, I would go to your party. (2) I
cannot go to the concert tonight. You will have to
tell me how it was.
Fauconnier explains these phenomena by a model
for the grammar of tense. According to this model,
the grammar specifies partial constraints on time and
fact/prediction status that hold locally between men-
tal spaces within a discourse configuration. We may
obtain actual information about time by combining
this with other available pragmatic information. Ac-
cordingly, the same tense may end up indicating
very different objective time relations relative to the
speech event.
TSD fits well with modern linguistics theories.
For example, in the construction grammar frame-
work (Goldberg, 1995), the ?construction? is the ba-
sic unit, comprised of a form and a meaning. Words,
multiword expressions, and syntactic forms are all
valid constructions. It is thus very natural to address
the sense disambiguation problem for all of these. In
this paper we focus on tense constructions.
For many NLP applications, it is very important
to disambiguate the tense forms of the sentence.
Among these applications are: (1) machine transla-
tion, as the actual time described by one tense form
in the source language may be described by a dif-
ferent tense form in the target language; (2) under-
standing the order of events in a text; (3) textual en-
tailment, when the optional entailed sentences refer
to the time and/or order of events of the source sen-
tence. Many more examples also exist.
3 The TSD Task
In this section we formally define the TSD task, dis-
cuss its nature vs. WSD, and describe various con-
crete task variants.
Task definition. First, some essential terminol-
ogy. The function of syntax is to combine lexi-
cal items (words, multiword expressions) to express
meanings. This function is achieved through syntac-
tic devices. The most common devices in English
are word order, morphology, and the usage of auxil-
iary words. An Abstract Syntactic Form (ASF) is a
particular set of devices that can be used to express a
set of meanings. A Concrete Syntactic Form (CSF)
is a concrete set of words generated by an ASF for
expressing a certain meaning in an utterance1. A
CSF is ambiguous if its generating ASF has more
than one meaning, which is the usual case. In this
case we also say that the ASF is ambiguous.
Here are a few examples. The ?present progres-
sive? ASF has the form ?am/is/are V-ing?2, which
employs all three main devices. It is ambiguous,
1In some linguistic theories, the central notion is the con-
struction, which combines an ASF (referred to as the form of
the construction) with a single meaning (Goldberg, 1995).
2Note that strictly speaking, these are three different ASFs.
We refer to this ASF family by a single name because they have
the same set of meanings and because it is standard to treat them
as a single ASF.
326
as shown in Section 1. The ?present simple? ASF
has the form ?V(+s)?3, and is ambiguous as well: in
the sentence ?My Brother arrives this evening?, the
CSF ?arrives? conveys the meaning of ?a future event
arranged for a definite time?, while in the sentence
?The sun rises in the East? the meaning is that of a
repeated event.
TSD vs. WSD. The TSD task is to disambiguate
the semantic sense of a tense syntactic form. TSD
is clearly different from WSD. This is obvious when
the CSF comprises two words that are not a multi-
word expression, and is usually also the case when it
comprises a single word. Consider the ?My Brother
arrives this evening? example above. While the verb
?arrive? has two main senses: ?reach a place?, and
?begin?, as in ?Summer has arrived?, in that example
we focused on the disambiguation of the tense sense
of the ?arrives? construction.
Concrete task variants. Unlike with words, the
presence of a particular CSF in a sentence is not
trivially recognizable. Consequently, there are three
versions of the TSD task: (1) we are given the sen-
tence, a marked subset of its words comprising a
CSF, and the ASF that has generated these words;
(2) we are given the sentence and a marked subset
of its words comprising a CSF, without knowing the
generating ASF; (3) we are given only the sentence
and we need to find the contained CSFs and their
ASFs. In all cases, we need to disambiguate the
sense of the ASFs. We feel that the natural granu-
larity of the task is captured by version (2). How-
ever, since the ASF can usually be identified using
relatively simple features, we also report results for
version (1). The main difficulty in all versions is
identifying the appropriate sense, as is the case with
WSD.
4 The Syntactic Sense Dictionary
A prerequisite to any concrete experimentation with
the TSD task is a syntactic sense dictionary. Based
on a thorough examination of three major English
grammar projects, we compiled a set of 18 com-
mon English tense ASFs and their possible senses.
The projects are (1) the Cambridge University Press
3Again, these are two ASFs, one adding an ?s? and one using
the verb as is.
English Grammar In Use series, comprising three
books (essential, intermediate and advanced) (Mur-
phy, 2007; Murphy, 1994; Hewings, 2005); (2)
the English grammar texts resulting from the sem-
inal corpus-based Cobuild project (elementary, ad-
vanced) (Willis and Wright, 2003; Willis, 2004); (3)
the Longman Grammar of Spoken and Written En-
glish (Biber et al, 1999).
As in any sense dictionary, in many cases it is hard
to draw the line between senses. In order to be able
to explore the computational limits of the task, we
have adopted a policy of fine sense granularity. For
example, senses 1 and 3 of the ?present simple? ASF
in Table 1 can be argued to be quite similar to each
other, having a very fine semantic distinction. A spe-
cific application may choose to collapse some senses
into one.
We used the conventional ASF names, which
should not be confused with their meanings (e.g., the
?present simple? ASF can be used to refer to future,
not present, events, as in Table 1, sense 4).
The ASF set thus obtained is: real conditionals,
hypothetical conditionals, wishes, reported speech,
present simple, present progressive, present perfect,
present perfect progressive, past simple, past pro-
gressive, past perfect, past perfect progressive, ?be
+ going + to + infinitive?, future progressive, future
perfect, future perfect progressive, ?would? tense
forms, and ?be + to + infinitive?. Note that the first
four ASFs are not direct tense forms; we include
them because they involve tensed sub-sentences
whose disambiguation is necessary for disambigua-
tion of the whole ASF. The total number of possible
senses for these 18 ASFs is 103.
Table 1 shows the complete senses set for the
?present simple? and ?be + to + infinitive? ASFs, plus
an example sentence for each sense. Space limita-
tions prevent us from listing all form senses here;
we will make the listing available online.
5 Corpus Creation and Annotation
We selected 3000 sentences from the British Na-
tional Corpus (BNC) (Burnard, 2000), containing
4702 CSFs (1.56 per sentence). These sentences
with their CSFs were sense annotated. To select
the 3000 sentences, we randomly sampled sentences
from the various written and spoken sections of the
327
Present Simple
1 Things that are always true
It gets cold in the winter.
2 Regular and repeated actions and habits
My parents often eat meat.
3 General facts
Mr. Brown is a teacher.
4 A future event arranged for a definite time
The next train arrives at 11:30.
5 Plans, expectations and hopes
We hope to see you soon.
6 Ordering someone to do something
Take your hands out of your pockets!
7 Something happening now, with verbs that are
not used in the present progressive in this sense
I do not deny the allegation.
8 Events happening now (informal;
common in books, scripts, radio etc.)
She goes up to this man and looks into his eyes.
9 Past actions
I was sitting in the park reading a newspaper
when all of a sudden this dog jumps at me.
10 Newspaper headlines, for recent events
Quake hits central Iran.
11 When describing the content of a book
Thompson gives an exhaustive list in chapter six.
?be + to + infinitive?
1 Events that are likely to happen in the near future
Police officers are to visit every home in the area.
2 Official arrangements, formal instructions & or-
ders
You are not to leave without my permission.
3 In an if-clause to say that something must
happen before something else can happen
If the human race is to survive, we must look at
environmental problems now.
Table 1: The full set of senses of the ?present simple?
and ?be + to + infinitive? abstract syntactic forms (ASFs),
with an example for each.
corpus, giving each section an equal weight. To
guarantee ample representation of ASFs, we man-
ually defined auxiliary words typical of each ASF
(e.g., ?does?, ?been? etc), and sampled hundreds of
sentences for each set of these auxiliary words. To
make sure that our definition of auxiliary words does
not skew the sampling process, and to obtain ASFs
that do not have clear auxiliary words, we have also
added 1000 random sentences. The number of CSF
instances obtained for each ASF ranges from 100
(future perfect) to over 850 (present simple). All
senses are represented; the number of senses repre-
sented by at least 15 CSFs is 77 (out of 103, average
number of CSFs per sense is 45.65).
We implemented an interactive application that
displays a sentence and asks an annotator to (1) mark
words that participate in the CSFs contained in the
sentence; (2) specify the ASF(s) of these CSFs; and
(3) select the appropriate ASF sense from the set
of possible senses. Annotators could also indicate
?none of these senses?, which they did for 2.6% (122
out of 4702) of the CSFs.
Annotation was done by two annotators (univer-
sity students). To evaluate inter-annotator agree-
ment, a set of 210 sentences (7% of the corpus),
containing at least 10 examples of each ASF, was
tagged by both annotators. The CSF+ASF identifi-
cation inter-annotator agreement was 98.7%, and the
inter-annotator agreement for the senses was 84.2%.
We will make the annotated corpus and annotation
guidelines available online.
6 Learning Algorithm
In this section we describe our learning model for
the TSD task. First, note that the syntactic sense is
not easy to deduce from readily computable anno-
tations such as the sentence?s POS tagging, depen-
dency structure, or parse tree (see Section 8). Hence,
a learning algorithm is definitely needed.
As common in supervised learning, we encode the
CSFs into feature vectors and then apply a learning
algorithm to induce a classifier. We first discuss the
feature set and then the algorithm.
Features. We utilize three sets of features: basic
features, lexical features, and a set of features based
on part-of-speech (POS) tags (Table 2). The ?aux-
iliary words? referred to in the table are the manu-
ally specified words for each ASF that have assisted
us in sampling the corpus (see Section 5). ?Content
words? are the non-auxiliary words appearing in the
CSF4. Content words are usually verbs, since we fo-
cus here on tense-related ASFs. The position and
distance of a form are based on its leftmost word
(auxiliary or content).
The personal pronouns used in the position fea-
tures are: I, you, he, she, it, they, and we. For
4Usually, there is a single content word. However, there may
be more than one, e.g. for phrasal verbs.
328
simplicity, we considered every word starting with
a capital letter that is not the first word in the sen-
tence to be a name.
Each ?Conditional? CSF contains two tense CSFs.
The one that is not the CSF currently encoded by the
features is referred to as its ?mate?.
For the time lexical features we used 16 words
(e.g., recently, often, now). For the reported speech
lexical features we used 14 words (e.g., said, replied,
wrote5). The words were obtained from the gram-
mar texts and our corpus development set.
The POS tagset used by the POS-based features is
that of the WSJ PennTreebank (see Section 7). The
possible verb tags in this tagset are: VB for the base
form, VBD for past tense, VBN for past participle,
VBG for a present participle or gerund (-ing), VBP
for present tense that is not 3rd person singular, and
VBZ for present simple 3rd person singular.
Conjunctions and prepositions are addressed
through the POS tags CC and IN. Using the PRP
tag to detect pronouns or lexical lists for conjunc-
tions and prepositions yielded no significant change
in the results.
In Section 7 we explore the impact each of the
feature sets has on the performance of the algorithm.
Our results indicate that the basic features have the
strongest impact, the POS-based features enhance
the performance in specific cases and the lexical fea-
tures only marginally affect the final results.
Algorithm. Denote by xi the feature vector of a
CSF instance i, by Ci the set of possible labels for
xi, and by ci ? Ci the correct label. The training
set is {(xj , Cj , cj)}nj=1. Let (xn+1, Cn+1) be a test
CSF. As noted in Section 3, there are two versions
of the task, one in which Ci includes the totality of
sense labels, and one in which it includes only the la-
bels associated with a particular ASF. In both cases,
the task is to select which of the labels in Cn+1 is its
correct label cn+1.
Owing to the task structure, it is preferable to
use an algorithm that allows us to restrict the pos-
sible labels of each CSF. For both task versions, this
would help in computing better probabilities during
the training stage, since we know the ASF type of
training CSFs. For the task version in which the ASF
5These are all in a past form due to the semantics of the
reported speech form.
Basic Features
Form words. Auxiliary and content words of the CSF.
Form type. The type, if it is known during test time.
Other forms. The auxiliary and content words (and
type, if known) of the other CSFs present in the sen-
tence.
Position. The position of the CSF in the sentence, its
distance from the end of the sentence, whether it is in
the first (last) three words in the sentence, its distance
from the closest personal pronoun or name.
Wish. Is there a CSF of type ?wish? before the en-
coded form, the number of CSFs between that ?wish?
form and the encoded CSF (if there are several such
?wish? forms, we take the closest one to the encoded
form).
Conditional. Does the word ?if? appear before the en-
coded form, is the ?if? the first word in the sentence,
the number of CSFs between the ?if? and the encoded
form, the auxiliary and content words (and type, if
known) of the mate form, is there a comma between
the encoded form and its mate form, does the word
?then? appear between the encoded form and its mate
form.
Punctuation. The type of end of sentence marker, dis-
tance of the encoded form from the closest predeces-
sor (successor) comma.
Lexical Features
Time. Time words appearing in the sentence, if any.
Reported speech. Reported speech words appearing
in the sentence, if any.
Be. Does the encoded form contain the verb ?be?.
Features Based on POS Tags
Form.The POS of the verb in the encoded form.
Other forms. The POS of the verb in the other CSFs
in the sentence.
POS tags. The POS tags of the two words to the left
(right) of the encoded form.
Conjunction POS. Is there a Conjunction (CC) be-
tween the encoded form and its closest predecessor
(successor) form, the distance from that conjunction.
Preposition POS. Is there a Preposition (IN) between
the encoded form and its closest predecessor (succes-
sor) form, the distance from that preposition.
Table 2: Basic features (top), lexical features (middle)
and POS tags-based features (bottom) used by the TSD
classifier.
type is known at test time, this would also help dur-
ing the test stage.
For the version in which ASF type is known at test
time, we experimented in two scenarios. In the first,
329
we take the ASF type at test time from the manual
annotation and provide it to the algorithm. In the
second, instead of the manual annotation, we imple-
mented a simple rule-based classifier for selecting
ASF types. The classifier decides what is the type of
an ASF according to the POS tag of its verb and to
its auxiliary words (given in the annotation). For ex-
ample, if we see the auxiliary phrase ?had been? and
the verb POS is not VBG, then the ASF is ?past per-
fect simple?. This classifier?s accuracy on our devel-
opment (test) data is 94.1 (91.6)%. In this scenario,
when given a test CSF, Xn+1, its set of possible la-
bels Cn+1 is defined by the classifier output. In the
features in which ASF type is used (see table 2), it is
taken from the classifier output in this case.
The sequential model algorithm presented by
Even-Zohar and Roth (2001) directly supports this
label restriction requirement 6. We use the SNOW
learning architecture for multi-class classification
(Roth, 1998), which contains an implementation of
that algorithm. The SNOW system allows us not
to define restrictions if so desired. It also lets us
choose the learning algorithm used when it builds
its classifier network. The algorithm can be Percep-
tron (MacKay, 2002), Winnow (Littlestone, 1988)
or Naive Bayes (MacKay, 2002)7. In Section 7 we
analyze the effect that these decisions have on our
results.
Classifier Selection. Investigating the best config-
uration of the SNOW system with development data,
we found that Naive Bayes gave the best or close
to best result in all experimental conditions. We
therefore report our results when this algorithm is
used. Naive Bayes is particularly useful when rela-
tively small amounts of training CSF instances are
available (Zhang, 2004), and achieves good results
when compared to other classifiers for the WSD task
(Mooney, 1996), which might explain our results.
Fine tuning of Winnow parameters also leads to high
performance (sometimes the best), but most other
parameter configurations lead to disappointing re-
6Note that the name of the learning algorithm is derived
from the fact that it utilizes classifiers to sequentially restrict
the number of competing classes while maintaining with high
probability the presence of the true outcome. The classification
task it performs is not sequential in nature.
7Or a combination of these algorithms, which we did not
explore in this paper.
sults. For the Perceptron, most parameter config-
urations lead to good results (much better than the
baseline), but these were a few percent worse than
the best Winnow or Naive Bayes results.
7 Experimental Results
Experimental setup. We divided the 3000 anno-
tated sentences (containing 4702 CSFs) to three
datasets: training data (2100 sentences, 3183
forms), development data (300 sentences, 498
forms) and test data (600 sentences, 1021 forms).
We used the development data to design the features
for our learning model and to tune the parameters
of the SNOW sequential model. In addition we used
this data to design the rules of the ASF type classifier
(which is not statistical and does not have a training
phase).
For the POS features, we induced POS tags using
the MXPOST POS tagger (Ratnaparkhi, 1996). The
tagger was trained on sections 2-21 of the WSJ Pen-
nTreebank (Marcus et al, 1993) annotated with gold
standard POS tags. We used a publicly available im-
plementation of the sequential SNOW model8.
We experimented in three conditions. In the first
(TypeUnknown), the ASF type is not known at test
time. In the last two, it is known at test time.
These two conditions differ in whether the type is
taken from the gold standard annotation of the test
sentences (TypeKnown), or from the output of the
simple rule-based classifier (TypeClassifier, see Sec-
tion 6). For both conditions, the results reported be-
low are when both ASF type features and possible
labels sets are provided during training by the man-
ual annotation. This is true also for the training of
the MFS baseline (see below)9.
We report an algorithm?s quality using accuracy,
that is, the number of test CSFs that were correctly
resolved by the algorithm divided by the total num-
ber of test CSFs.
Baseline. We compared the performance of our al-
gorithm to the ?most frequent sense? (MFS) base-
8http://l2r.cs.uiuc.edu/?cogcomp/asoftware.php?
skey=SNOW
9For the TypeClassifier condition, we also experimented us-
ing an ML technique that sometimes reduces noise, where train-
ing is done using the classifier types. We obtained very similar
results to those reported.
330
TypeUnknown TypeClassifier TypeKnown
Our algorithm 49.7% 58.8% 62%
MFS baseline 13.5% 42.9% 46.7%
Table 3: Performance of our algorithm and of the MFS
baseline where at test time ASF type is known (right),
unknown (left) or given by a simple rule-based classifier
(middle). Our algorithm is superior in all three condi-
tions.
Constrained Model Unconstrained Classifier
All Base+Lexical All Base+Lexical
features features features features
Type 57.9% 57.7% 53% 50.1%
features
No type 57.2% 55.4% 48% 42.6%
features
Table 4: Impact of POS features. When the constrained
model is used (left section), POS features have no effect
on the results when ASF type information is encoded.
When an unconstrained classifier is used, POS features
affect the results both when ASF type features are used
and when they are not (see discussion in the text).
line. This baseline is common in semantic disam-
biguation tasks and is known to be quite strong. In
the condition where the ASF type is not known at
test time, MFS gives each form in the test set the
sense that was the overall most frequent in the train-
ing set. That is, in this case the baseline gives all
test set CSFs the same sense. When the ASF type
is known at test time, MFS gives each test CSF the
most frequent sense of that ASF type in the training
set. That is, in this case all CSFs having the same
ASF type get the same sense, and forms of different
types are guaranteed to get different senses.
Recall that the condition where ASF type is
known at test time is further divided to two condi-
tions. In the TypeKnown condition, MFS selects the
most frequent sense of the manually created ASF
type, while in the TypeClassifier condition it selects
the most frequent sense of the type decided by the
rule-based classifier. In this condition, if the classi-
fier makes a mistake, MFS will necessarily make a
mistake as well.
Note that a random baseline which selects a sense
for every test CSF from a uniform distribution over
the possible senses (103 in our case) would score
very poorly.
Results. Table 3 shows our results. Results are
shown where ASF type is not known at test time
(left), when it is decided at test time by a rule-based
classifier (middle) and when it is known at test time
(right). Our algorithm outperforms the MFS base-
line in all three conditions. As expected, both our al-
gorithm and the MFS baseline perform better when
ASF type information is available at test time (Type-
Classifier and TypeKnown conditions), and improve
as this data becomes more accurate (the TypeKnown
condition)10.
Analyzing the per-type performance of our algo-
rithm reveals that it outperforms the MFS baseline
for each and every ASF type. For example, in the
TypeKnown condition, the accuracy gain of our al-
gorithm over the baseline11 varies from 4% for the
?present perfect? to 30.6% and 29.1% for the ?past
perfect? and ?present simple? ASFs.
Below we analyze the roles of the different com-
ponents of our learning algorithm in performing the
TSD task. Since this is the first exploration of the
task, it is important to understand what properties
are essential for achieving good performance. The
analysis is done by experimenting with development
data, and focuses on the TypeKnown and TypeUn-
known conditions. Patterns for the TypeClassifier
condition are very similar to the patterns for the
TypeKnown condition.
The Possible Senses Constraint. We use the
learning model of Even-Zohar and Roth (2001),
which allows us to constrain the possible senses
an input vector can get to the senses of its ASF
type. We ran our model without this constraint dur-
ing both training and test time (recall that for the
above results, this constraint was always active dur-
ing training). In this case, the only difference be-
tween the TypeKnown and the TypeUnknown con-
ditions is whether ASF type features are encoded at
test time. In the TypeKnown condition, the accu-
racy of the algorithm drops from 57.9% (when us-
ing training and test time constraints and ASF type
features) to 53% (when using only ASF type fea-
tures but no constraints). In the TypeUnknown con-
dition, accuracy drops from 57.24% (when using
training time constraints) to 48.03% (when neither
constraints nor ASF type features are used). Note
10Recall that the performance of the rule-based ASF type
classifier on test data is not 100% but 91.6% (Section 6).
11accuracy(algorithm)? accuracy(MFS).
331
that the difference between the constrained model
and the unconstrained model is quite large.
The MFS baseline achieves on development data
42.9% and 13.2% in the TypeKnown and TypeUn-
known conditions respectively12. Thus, the algo-
rithm outperforms the baseline both when the con-
strained model is used and when an unconstrained
multi-class classifier is used.
Note also that when constraints on the possible
labels are available at training time, test time con-
straints and ASF type features (whose inclusion is
the difference between the TypeKnown and Type-
Unknown) have a minor effect on the results (57.9%
for TypeKnown compared to 57.24% for TypeUn-
known). However, when training time constraints
on the possible labels are not available at training
time, ASF type features alone do have a significant
effect on the result (53% for TypeKnown compared
to 48.03% for TypeUnknown).
POS Features. We next explore the impact of the
POS features on the results. These features encode
the inflection of the verbs in the CSF, as well as the
POS tags of the two words to the left and right of the
CSF.
Verb forms provide some partial information cor-
responding to the ASF type features encoded at the
TypeKnown scenario. Table 4 shows that when both
label constraints and ASF type features are used,
POS features have almost no impact on the final re-
sults. When the constrained model is used but ASF
type features are not encoded, POS features have an
effect on the results. We conclude that when using
the constrained model, POS features are important
mainly for ASF type information. When the uncon-
strained classifier is used, POS features have an ef-
fect on performance whether ASF type features are
encoded or not. In the last case the impact of POS
features is larger. In other words, when using an un-
constrained classifier, POS features give more than
ASF type information to to the model.
Lexical Features. To explore the impact of the
lexical features, we removed the following features:
time words, reported speech words and ?be? indi-
cation features. We saw no impact on model per-
formance when using the constrained model, and a
12Note that these numbers are for development data only.
0.5% decrease when using the unconstrained classi-
fier. That is, our model does not require these lexical
features, which is somewhat counter-intuitive. Lex-
ical statistics may turn out to be helpful when using
a much larger training set.
Conditional and Wish Features. The condition-
als and ?wish? features have a more substantial im-
pact on the results, as they have a role in defining the
overall syntactic structure of the sentence. Discard-
ing these features leads to 4% and 1.4% degradation
in model accuracy when using the constrained and
unconstrained models respectively.
8 Relevant Previous Work
As far as we know, this is the first paper to address
the TSD task. In this section we describe related
research directions and compare them with TSD.
A relevant task to TSD is WSD (Section 1 and
Section 3). Many algorithmic approaches and tech-
niques have been applied to supervised WSD (for
reviews see (Agirre and Edmonds, 2006; Mihalcea
and Pedersen, 2005; Navigli, 2009)). Among these
are various classifiers, ensemble methods combin-
ing several supervised classifiers, bootstrapping and
semi-supervised learning methods, using the Web
as a corpus and knowledge-based methods relying
mainly on machine readable dictionaries. Specif-
ically related to this paper are works that exploit
syntax (Martinez et al, 2002; Tanaka et al, 2007)
and ensemble methods (e.g. (Brody et al, 2006))
to WSD. The references above also describe some
unsupervised word sense induction algorithms.
Our TSD algorithm uses the SNOW algorithm,
which is a sparse network of classifiers (Section 6).
Thus, it most resembles the ensemble approach to
WSD. That approach has achieved very good results
in several WSD shared tasks (Pedersen, 2000; Flo-
rian and Yarowsky, 2002).
Since temporal reasoning is a direct applica-
tion of TSD, research on this direction is relevant.
Such research goes back to (Passonneau, 1988),
which introduced the PUNDIT temporal reasoning
system. For each tensed clause, PUNDIT first de-
cides whether it refers to an actual time (as in ?We
flew TWA to Boston?) or not (as in ?Tourists flew
TWA to Boston?, or ?John always flew his own plane
to Boston?). The temporal structure of actual time
332
clauses is then further analyzed. PUNDIT?s classi-
fication is much simpler than in the TSD task, ad-
dressing only actual vs. non-actual time. PUNDIT?s
algorithmic approach is that of a Prolog rule based
system, compared to our statistical learning corpus-
based approach. We are not aware of further re-
search that followed their sense disambiguation di-
rection.
Current temporal reasoning research focuses on
temporal ordering of events (e.g., (Lapata, 2006;
Chambers and Jurafsky, 2008)), for which an ac-
cepted atomic task is the identification of the tem-
poral relation between two expressions (see e.g., the
TempEval task in SemEval ?07 (Verhagen et al,
2007)). This direction is very different from TSD,
which deals with the semantics of individual con-
crete tense syntactic forms. In this sense, TSD is an
even more atomic task for temporal reasoning.
A potential application of TSD is machine trans-
lation where it can assist in translating tense and as-
pect. Indeed several papers have explored tense and
aspect in the MT context. Dorr (1992) explored the
integration of tense and aspect information with lex-
ical semantics for machine translation. Schiehlen
(2000) analyzed the effect tense understanding has
on MT. Ye and Zhang (2005) explored tense tagging
in a cross-lingual context. Ye et al, (2006) extracted
features for tense translation between Chinese and
English. Murata et al, (2007) compared the perfor-
mance of several MT systems in translating tense
and aspect and found that various ML techniques
perform better on the task.
Another related field is ?deep? parsing, where a
sentence is annotated with a structure containing in-
formation that might be relevant for semantic inter-
pretation (e.g. (Hajic, 1998; Baldwin et al, 2007)).
TSD senses, however, are not explicitly represented
in these grammatical structures, and we are not
aware of any work that utilized them to do some-
thing close to TSD. This is a good subject for future
research.
9 Conclusion and Future Work
In this paper we introduced the Tense Sense Disam-
biguation (TSD) task, defined as selecting the cor-
rect sense of a concrete tense syntactic form in a sen-
tence among the senses of abstract syntactic forms
in a syntactic sense dictionary. Unlike in other se-
mantic disambiguation tasks, the sense to be disam-
biguated is not lexical but of a syntactic structure.
We prepared a syntactic sense dictionary, annotated
a corpus by it, and developed a supervised classifier
for sense disambiguation that outperformed a strong
baseline.
An obvious direction for future work is to expand
the annotated corpus and improve the algorithm by
experimenting with additional features. For exam-
ple, we saw that seeing the full paragraph containing
a sentence helps human annotators decide on the ap-
propriate sense which implies that using larger con-
texts may improve the algorithm.
TSD can be a very useful operation for various
high-level applications, for example textual infer-
ence, question answering, and information retrieval,
in the same way that textual entailment (Dagan et
al., 2006) was designed to be. In fact, TSD can assist
textual entailment as well, since the sense of a tense
form may provide substantial information about the
relations entailed from the sentence. Using TSD
in such applications is a major direction for future
work.
References
Eneko Agirre and Philip Edmonds (Eds). 2006. Word
Sense Disambiguation: Algorithms and Applications.
Springer Verlag.
Timothy Baldwin, Mark Dras, Julia Hockenmaier, Tracy
Holloway King, and Gertjan van Noord. 2007. The
Impact of Deep Linguistic Processing on Parsing
Technology. IWPT ?07.
Douglas Biber, Stig Johansson, Geoffrey Leech, Susan
Conard, Edward Finegan. 1999. Longman Grammar
of Spoken and Written English. Longman.
Samuel Brody, Roberto Navigli and Mirella Lapata.
2006. Ensemble Methods for Unsupervised WSD.
ACL-COLING ?06.
Lou Burnard. 2000. The British National Corpus User
Reference Guide. Technical Report, Oxford Univer-
sity.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
Combining Implicit Constraints Improves Temporal
Ordering. EMNLP ?08.
Michelle Cutrer. 1994. Time and Tense in Narratives and
in Everyday Language. PhD dissertation, University
of California at San Diego.
333
Ido Dagan, Oren Glickman and Bernardo Magnini. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge. Lecture Notes in Computer Science 2006,
3944:177-190.
John Dinsmore. 1991. Partitioned representations. Dor-
drecht, Netherlands: Kluwer.
Bonnie Dorr. 1992. A Two-Level Knowledge Repre-
sentation for Machine Translation: Lexical Semantics
and Tense/Aspect. In James Pustejovsky and Sabine
Bergler, editors, Lexical Semantics and Knowledge
Representation.
Yair Even-Zohar and Dan Roth. 2001. A Sequential
Model for Multi-Class Classification. EMNLP ?01.
Gilles Fauconnier. 2007. Mental Spaces. in Dirk Geer-
aerts and Hubert Cuyckens, editors, The Oxford Hand-
book of Cognitive Linguistics.
Radu Florian and David Yarowsky. 2002. Modeling
Consensus: Classifier Combination for Word Sense
Disambiguation. EMNLP ?02.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. Uni-
versity of Chicago Press.
Jan Hajic. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. Issues of
Valency and Meaning, 106?132.
Martin Hewings. 2005. Advanced Grammar in Use, Sec-
ond Edition. Cambridge University University.
Mirella Lapata and Alex Lascarides. 2006. Learning
Sentence-internal Temporal Relations. Journal of Ar-
tificial Intelligence Research, 27:85?117.
Nick Littlestone. 1988. Learning Quickly When Irrele-
vant Attributes Abound: A New Linear-threshold Al-
gorithm. Machine Learning, 285?318.
David MacKay. 2002. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Mitchell P. Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Martinez, Eneko Agirre, Lluis Marquez. 2002.
Syntactic Features for High Precision Word Sense Dis-
ambiguation. COLING ?02.
Rada Mihalcea and Ted Pedersen. 2005. Advances in
Word Sense Disambiguation. Tutorial in ACL ?05.
Raymond J. Mooney. 1996. Comparative Experiments
on Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. EMNLP ?96.
Masaki Murata, Qing Ma, Kiyotaka Uchimoto, Toshiyuki
Kanamaru and Hitoshi Isahara. 2007. Japanese-to-
English translations of Tense, Aspect, and Modality
Using Machine-Learning Methods and Comparison
with Cachine-Translation Systems on Market. LREC
?07.
Raymond Murphy. 1994. English Grammar In Use, Sec-
ond Edition. Cambridge University Press.
Raymond Murphy. 2007. Essential Grammar In Use,
Third Edition. Cambridge University Press.
Roberto Navigli. 2009. Word Sense Disambiguation: a
Survey. ACM Computing Surveys, 41(2) 1?69.
Rebecca J. Passonneau. 1988. A Computational Model
of Semantics of Tenses and Aspect. Computational
Linguistics, 14(2):44?60.
Ted Pedersen. 2000. A Simple Approach to Building En-
sembles of Naive Bayesian Classifiers for Word Sense
Disambiguation. NAACL ?00.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?06.
Dan Roth. 1998. Learning to Resolve Natural Language
Ambiguities: A Unified Approach. AAAI ?98.
Michael Schiehlen. 2000. Granularity Effects in Tense
Translation. COLING ?00.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 Task 15: TempEval Temporal
Relation Identification. ACL ?07.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae
Fujita and Chikara Hashimoto. 2007. Word Sense
Disambiguation Incorporating Lexical and Structural
Semantic Information. EMNLP-CoNLL ?07.
Dave Willis and Jon Wright. 2003. Collins Cobuild El-
ementary English Grammar, Second Edition. Harper-
Collins Publishers.
Dave Willis. 2004. Collins Cobuild Intermediate English
Grammar, Second Edition. HarperCollins Publishers.
Yang Ye, Victoria Li Fossum and Steven Abney. 2006.
Latent Features in Automatic Tense Translation be-
tween Chinese and English. SIGHAN ?06.
Yang Ye and Zhu Zhang. 2005. Tense Tagging for Verbs
in Cross-Lingual Context: A Case Study. IJCNLP ?05.
Harry Zhang. 2004. The Optimality of Naive Bayes.
FLAIRS ?04.
334
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 684?693,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improved Fully Unsupervised Parsing with Zoomed Learning
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
We introduce a novel training algorithm
for unsupervised grammar induction, called
Zoomed Learning. Given a training set T and
a test set S, the goal of our algorithm is to
identify subset pairs Ti, Si of T and S such
that when the unsupervised parser is trained
on a training subset Ti its results on its paired
test subset Si are better than when it is trained
on the entire training set T . A successful ap-
plication of zoomed learning improves overall
performance on the full test set S.
We study our algorithm?s effect on the leading
algorithm for the task of fully unsupervised
parsing (Seginer, 2007) in three different En-
glish domains, WSJ, BROWN and GENIA, and
show that it improves the parser F-score by up
to 4.47%.
1 Introduction
Grammar induction is the task of learning grammati-
cal structure from plain text without human supervi-
sion. The task is of great importance both for the
understanding of human language acquisition and
since its output can be used by NLP applications,
avoiding the costly and error prone creation of man-
ually annotated corpora. Many recent works have
addressed the task (e.g. (Klein and Manning, 2004;
Seginer, 2007; Cohen and Smith, 2009; Headden et
al., 2009)) and its importance has increased due to
the recent availability of huge corpora.
A basic challenge to this research direction is
how to utilize training data in the best possible
way. Klein and Manning (2004) report results for
their dependency model with valence (DMV) for
unsupervised dependency parsing when it is trained
and tested on the same corpus (both when sentence
length restriction is imposed, such as for WSJ10,
and when it is not, such as for the entire WSJ). To-
day?s best unsupervised dependency parsers, which
are rooted in this model, train on short sentences
only: both Headen et al, (2009) and Cohen and
Smith (2009) train on WSJ10 even when the test set
includes longer sentences.
Recently, Spitkovsky et al, (2010) demonstrated
that training the DMV model on sentences of up to
15 words length yields better results on the entire
section 23 of WSJ (with no sentence length restric-
tion) than training with the entire WSJ corpus.
In contrast to these dependency models, the
Seginer constituency parser achieves its best perfor-
mance when trained on the entire WSJ corpus ei-
ther if sentence length restriction is imposed on the
test corpus or not. The sentence length restriction
training protocol of (Spitkovsky et al, 2010), harms
this parser. When the parser is trained with the
entire WSJ corpus its F-score performance on the
WSJ10, WSJ20 and the entire WSJ corpora are 76,
64.8 and 56.7 respectively. When training is done
with WSJ10 (WSJ20) performance degrades to 60
(72.2), 37.4 (61.9) and 29.7 (48) respectively.
In this paper we introduce the Zoomed Learn-
ing (ZL) technique for unsupervised parser training:
given a training set T and a test set S, it identifies
subset pairs Ti, Si of T and S such that when the
unsupervised parser is trained on a training subset
Ti its results on its paired test subset Si are better
than when it is trained on the entire training set T . A
684
successful application of zoomed learning improves
performance on the full test set S.
We describe ZL algorithms of increasing sophis-
tication. In the simplest algorithm the subsets are
randomly selected while in the more sophisticated
versions subset selection is done using a fully unsu-
pervised measure of constituency parse tree quality.
We apply ZL to the Seginer parser, the best al-
gorithm for fully unsupervised constituency parsing.
The input is a plain text corpus without any annota-
tion, not even POS tagging1, and the output is an
unlabeled bracketing for each sentence.
We experiment in three different English do-
mains: WSJ (economic newspaper), GENIA (biolog-
ical articles) and BROWN (heterogeneous domains),
and show that ZL improves the parser F-score by as
much as 4.47%.
2 Related Work
Unsupervised parsing has attracted researchers for
over a quarter of a century (see (Clark, 2001; Klein,
2005) for reviews). In recent years efforts have been
made to evaluate the algorithms on manually anno-
tated corpora such as the WSJ PennTreebank. Re-
cent works on unlabeled bracketing or dependencies
induction include (Klein and Manning, 2002; Klein
and Manning, 2004; Dennis, 2005; Bod, 2006a;
Bod, 2006b; Bod, 2007; Smith and Eisner, 2006;
Seginer, 2007; Cohen et al, 2008; Cohen and Smith,
2009; Headden et al, 2009). Most of the works
above use POS tag sequences, created either manu-
ally or by a supervised algorithm, as input. The only
exception is Seginer?s parser, which induces brack-
eting from plain text.
Our confidence-based ZL algorithms use the
PUPA unsupervised parsing quality score (Reichart
and Rappoport, 2009b). As far as we know, PUPA is
the only unsupervised quality assessment algorithm
for syntactic parsers that has been proposed. Com-
bining PUPA with Seginer?s parser thus preserves the
fully unsupervised nature of the task.
Quality assessment of a learning algorithm?s out-
put has been addressed for supervised algorithms
1For clarity of exposition, we still refer to this corpus as our
training corpus. In the algorithms presented in this paper, the
test set is included in the training set which is a common prac-
tice in unsupervised parsing.
(see (Caruana and Niculescu-Mizil, 2006) for a sur-
vey) and specifically for supervised syntactic pars-
ing (Yates et al, 2006; Reichart and Rappoport,
2007; Ravi et al, 2008; Kawahara and Uchimoto,
2008). All these algorithms are based on manually
annotated data and thus do not preserve the unsuper-
vised nature of the task addressed in this paper.
We experiment with the Seginer parser for two
reasons. First, this is the best algorithm for the task
of fully unsupervised parsing which motivates us to
improve its performance. Second, this is the only
publicly available unsupervised parser that induces
constituency trees. The PUPA score we use in our
confidence-based algorithms is applicable for con-
stituency trees only. When additional constituency
parsers will be made available, we will test ZL with
them as well. Interestingly, the results reported for
other constituency models (the CCM model (Klein
and Manning, 2002) and the U-DOP model (Bod,
2006a; Bod, 2006b)) are reported when the parser is
trained on its test corpus even if the sentences is that
corpus are of bounded length (e.g. WSJ10). This
raises the question if using more training data (e.g.
the entire WSJ) wisely can enhance these models.
Recently, Spitkovsky et al, (2010) proposed three
approaches for improvement of unsupervised gram-
mar induction by considering the complexity of the
training data. The approaches have been applied
to the DMV unsupervised dependency parser (Klein
and Manning, 2004) and improved its performance.
One of these approaches is to train the model with
sentences whose length is up to 15 words. As noted
above, such a training protocol fails to improve the
performance of the Seginer parser.
The other approaches in that paper, bootstrapping
via iterated learning of increasingly longer sentences
and a combination of the bootstrapping and the short
sentences approaches, are not directly applicable to
the Seginer parser since its training method cannot
be trivially bootstrapped with parses created in for-
mer steps (Seginer, 2007).
Related machine learning methods. ZL is re-
lated to ensemble methods. Both ZL and such meth-
ods produce multiple learners, each of them trained
on a different subset of the training data, and decide
which learner to use for a particular test instance.
Bagging (Breiman, 1996) and boosting (Freund and
Schapire, 1996), where the experts utilize the same
685
learning algorithm and differ in the sample of the
training data they use for its training, were applied
to supervised parsing (Henderson and Brill, 2000;
Becker and Osborne, 2005). In Section 3 we discuss
the connection of ZL to boosting.
Owing to the fact that ZL produces different
learners, it is natural to use it in conjunction with
an ensemble method, which is what we do in this
paper with our EZL model (Section 3).
ZL is also related to active learning (AL) (Cohn
and Ladner, 1994). AL also uses training subset se-
lection, with the goal of obtaining a faster learning
curve for an algorithm. AL is done in supervised
settings, usually in order to minimize human anno-
tation costs. AL algorithms providing faster learning
than random subset selection for parsing have been
proposed (Reichart and Rappoport, 2009a; Hwa,
2004). However, we are not aware of AL applica-
tions in which the overall performance on the test
set has been improved. In addition, our application
here is to an unsupervised problem.
Algorithms that utilize unsupervised clustering
for class decomposition in order to improve classi-
fiers? performance (e.g. (Vilalta and Rish, 2003)) are
related to ZL. In such methods, examples that be-
long to the same class are clustered, and the induced
clusters are considered as separate classes. These
methods, however, have been applied only to super-
vised classification in contrast to our work that ad-
dresses unsupervised structured learning. Moreover,
after class decomposition a classifier is trained with
the entire training data while the subsets identified
by a ZL algorithm are parsed by a parser trained only
with the sentences they contain.
3 Zoomed Learning Algorithms
Zoomed Learning proposes that performance on a
particular test instance might improve if training is
done on a proper subset of the training set. The
ZL view is clearly applicable when the training data
is comprised of subsets originating from different
sources having different natures. If the test data is
also similarly composed, performance on any partic-
ular test instance might improve if training is done
on a training subset coming from the same source.
However, even when the training and test data are
from the same source, a ZL algorithm may capture
fine differences between subsets.
The ZL idea is therefore related to the notions of
in-domain and out-of-domain (domain adaptation).
In the former, the training and test data are assumed
to originate from the same domain. In the latter, the
test data comes from a different domain, and there-
fore has different statistics from the training data.
Indeed, the performance of NLP algorithms in do-
main adaptation scenarios is markedly lower than in
in-domain ones (McClosky et al, 2006).
ZL takes this observation to the extreme, assum-
ing that a similar situation might exist even in in-
domain scenarios. After all, a ?domain? is only a
coarse qualification of the nature of a data set. In
NLP, a domain is usually specified as the genre of
the text involved (e.g., ?newspapers?). However,
there are additional axes that might influence the
statistics obtained from training data, e.g., the syn-
tactic nature of sentences.
This section presents our ZL algorithms. We start
with the simplest possible ZL algorithm where the
subsets are randomly selected. We then describe ZL
algorithms based on quality-based parse selection.
We first detail a basic version and then an extended
version consisting of another level of parse selec-
tion. Finally, we briefly discuss the PUPA quality
measure that we use to evaluate the quality of a parse
tree.
In all versions of the algorithm the input consists
of a set T of N training sentences, a set S ? T of
test sentences, and an integer number NH ? N .
Zoomed Learning with Random Selection
(RZL). The simplest ZL algorithm randomly assigns
each of the training sentences to one of n sets (n = 2
in this paper). More explicitly, the set number is
drawn from a uniform distribution on {1, 2, . . . n}.
Each set is then parsed by a parser that is trained
only with the sentences contained in that set.
The intuition behind this algorithm is that differ-
ent sets of sentences are likely to manifest differ-
ent syntactic patterns. Consequently, the best way to
learn the syntactic patterns of any given set of sen-
tences might be to train the parser on the sentences
contained in the set.
While simple, in Section 5 it is shown to improve
the performance of the Seginer parser.
The Basic Quality-Based Algorithm (BZL). The
idea of the basic ZL algorithm is that sentences for
686
which the parser provides low quality parses man-
ifest different syntactic patterns than the sentences
for which the parser provides high quality parses.
The main challenge is therefore to estimate the qual-
ity of the produced parses without supervision.
The algorithm has three stages. In the first, we
create the fully-trained model by training the parser
using all of the N sentences of T . We then parse
these N sentences using the fully-trained model.
In the second, we compute a parse confidence
score for each of the N sentences, based on the N
parses produced in the first stage. We divide the
training sentences to two subsets: a high quality sub-
set H consisting of the top scored NH sentences,
and a lower quality subset L consisting of the other
NL = N ? NH sentences.
As is common practice for this problem (Klein
and Manning, 2004; Seginer, 2007), the test set is
contained in the training set. This methodology is
a valid one because the training set is unannotated.
Our test set is thus naturally divided into two sub-
sets, a high quality subset HT consisting of the test
set sentences contained in H and a lower quality
subset LT consisting of the test set sentences con-
tained in L.
In the third stage, each of the test subsets is parsed
by a model trained only on its corresponding train-
ing subset. This stage is motivated by our assump-
tion that the high and low quality subsets manifest
dissimilar syntactic patterns, and consequently the
statistics of the parser?s parameters suitable for one
subset differ from those suitable for another.
We compute the confidence score in the second
stage using the unsupervised PUPA algorithm (Re-
ichart and Rappoport, 2009b). POS tags for it are
induced using the fully unsupervised algorithm of
Clark (2003). The parser we experiment with is the
incremental parser of Seginer (2007), whose input
consists of raw sentences and does not include any
kind of supervised POS tags (created either manu-
ally or by a supervised algorithm). Consequently,
our algorithm is fully unsupervised. The only pa-
rameter it has is NH but ZL improves parser perfor-
mance for most NH values.
BZL is related to boosting. In boosting after train-
ing one member of the ensemble, examples are re-
weighted such that examples that are classified cor-
rectly are down-weighted. BZL does something sim-
ilar: it uses PUPA to estimate which sentences are
given high quality parse trees, and down-weights ex-
amples with high (low) PUPA score to 0 when train-
ing the L-trained (H-trained) model. However, in
boosting the entire test set is annotated by the same
learning model, while ZL parses each test subset
with a model trained on its corresponding training
subset.
The Extended Quality-Based Algorithm (EZL).
The basic algorithm produces an ensemble of two
parsing experts: the one trained on H and the one
trained on L. It uses the ensemble to parse the test
set by applying the H-trained expert to HT and the
L-trained expert to LT . Naturally, there are other
ways to utilize the ensemble to parse the test set. In
addition, even if parse trees generated by the experts
are better with high probability than those of the
fully trained parser, they are not guaranteed to be so.
The fully trained parser is therefore also a valuable
member in the ensemble. Consequently, we intro-
duce an extended zoomed learning algorithm (EZL).
The extended version is implemented as a final
fourth stage of the previously described basic algo-
rithm. In this stage, the two test subsets are parsed
by the fully trained parsing model, in addition to be-
ing parsed by the zooming parsing models. We now
have two parses for each test sentence s: PZ(s), the
parse created by a parser trained with the sentences
contained in its corresponding training subset, and
PF (s), created by the fully trained parser.
For each of the two parses of each test sentence,
a confidence score is computed by PUPA. As will
be reviewed below, PUPA uses a set of parsed sen-
tences to compute the statistics on which its scores
are based. Therefore, there are two sources for a dif-
ference between the scores of the two parse trees of a
given test sentence: the difference between the trees
themselves, and the difference between the parses of
the other sentences in the set.
The PUPA score for PZ(s) is computed using the
parses created for the sentences contained in the test
subset of s by a parser trained with the correspond-
ing training subset. The PUPA score for PF (s) is
computed using the parses created for the entire test
set by the fully trained parser.
The algorithm now outputs a final parse by select-
ing for each sentence the parse tree having the higher
PUPA score.
687
The PUPA Confidence Score. In the second and
fourth stages of the confidence-based algorithms, an
unsupervised confidence score is computed for each
of the induced parse trees. The confidence score
algorithm we use is the POS-based Unsupervised
Parse Assessment (PUPA) algorithm (Reichart and
Rappoport, 2009b). We provide here a brief descrip-
tion of this algorithm.
The input to PUPA is a set I of parsed sentences,
and its output consists of a confidence score in [0, 1]
assigned to each sentence in I .
The PUPA algorithm collects statistics of the syn-
tactic structures (parse tree constituents) contained
in the set I of parsed sentences. The constituent rep-
resentation is based on the POS tags of the words in
the yield of the constituent and of the words in the
yields of neighboring constituents. We follow Re-
ichart and Rappoport (2009b) and induce the POS
tags using the fully unsupervised POS induction al-
gorithm of Clark (2003).
The algorithm then goes over each individual tree
in the set I and scores it according to the collected
statistics The PUPA algorithm is guided by the idea
that syntactic structures that are frequently created
by the parser are more likely to be correct than struc-
tures the parser produces less frequently. Therefore,
constituents that are more frequent in the set I re-
ceive higher scores after proper regularization is ap-
plied to prevent potential biases. The tree score is a
combination of the scores of its constituents.
Full details of the PUPA algorithm are given in
(Reichart and Rappoport, 2009b). The resulting
score was shown to be strongly correlated with the
extrinsic quality of the parse tree, defined to be its F-
score similarity to the manually created (gold stan-
dard) parse tree of the sentence.
4 Experimental Setup
We experimented with three English corpora: the
WSJ Penn Treebank (Marcus et al, 1993) consist-
ing of economic newspaper texts, the BROWN cor-
pus (Francis and Kucera, 1979) consisting of texts
of various English genres (e.g. fiction, humor, ro-
mance, mystery and adventure) and the GENIA cor-
pus (Kim et al, 2003) consisting of abstracts of sci-
entific articles from the biological domain. All cor-
pora were stripped of all annotation (bracketing and
POS tags).
For all corpora we report the parser perfor-
mance on the entire corpus (WSJ: 49206 sentences,
BROWN: 24243 sentences, GENIA: 4661 sentences).
For WSJ we also provide an analysis of the per-
formance of the parser when applied to sentences
of bounded length. These sub-corpora are WSJ10
(7422 sentences), WSJ20 (25522 sentences) and
WSJ40 (47513 sentences) where WSJY denotes
the subset of WSJ containing sentences of length at
most Y (excluding punctuation).
Seginer?s parser achieves its best reported results
when trained on the full WSJ corpus. Consequently,
for all corpora, we compare the performance of the
parser when trained with the ZL algorithms to its
performance when trained with the full corpus.
The POS tags required as input by the PUPA al-
gorithm are induced by the fully unsupervised POS
induction algorithm of Clark (2003)2. Reichart and
Rappoport (2009b) demonstrated an unsupervised
technique for the estimation of the number of in-
duced POS tags with which the correlation between
PUPA?s score and the parse F-score is maximized.
When exploring an experimental setup identical to
our WSJ setup, they set the number of induced tags
to be 5. We therefore induced 5 POS tags for each
corpus, using all its sentences as input for Clark?s al-
gorithm. Our implementation of the PUPA algorithm
will be made available on line.
For each corpus we performed K experiments
with each of the three ZL algorithms, where K
equals to the number of sentences in the corpus di-
vided by 1000 (rounded upwards). In each experi-
ment the size of the high quality H and lower quality
L training subsets is different. H consists of the NH
top ranked sentences according to PUPA (or NH ran-
domly selected sentences for RZL), with NH chang-
ing from 1000 upwards in steps of 1000. L consists
of the rest of the sentences in the training corpus
(WSJ). The results reported for RZL are averaged
over 10 runs.
We report the parser performance on the test cor-
pus for each training protocol. Following the un-
supervised parsing literature multiple brackets and
brackets covering a single word are not counted, but
the sentence level bracket is. We exclude punctua-
2www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
688
WSJ10, F(Full) = 76 WSJ20, F(Full) = 64.82 WSJ40, F(Full) = 57.54 WSJ, F(Full) = 56.7
NH 25% 50% 75% 25% 50% 75% 25% 50% 75% 25% 50% 75%
EZL 76.38 76.80 76.14 65.75 66.14 65.66 58.32 58.75 58.56 57.47 57.90 57.73
+0.38 +0.80 +0.14 +0.93 +1.30 +0.82 +0.78 +1.21 +1.02 +0.77 +1.20 + 1.13
BZL 75.07 75.78 75.02 65.08 65.74 64.79 58.13 58.70 58.21 57.30 57.88 57.66
-0.93 -0.22 -0.98 +0.26 +0.92 -0.03 +0.59 +1.16 +0.67 +0.60 +1.18 +1.06
RZL 75.41 75.00 75.32 64.43 64.66 65.32 57.27 57.63 58.39 56.44 56.84 57.59
-0.59 -1.00 -0.68 -0.39 -0.16 +0.50 -0.27 +0.09 +0.85 -0.26 +0.14 +0.89
WSJ10 WSJ20 WSJ40 WSJ
|LT | 10% 20% 30% 10% 20% 30% 10% 20% 30% 10% 20% 30%
EZL 1.32 0.95 0.61 2.98 3.13 1.76 2.60 2.80 2.62 2.44 2.40 2.50
BZL 0.37 0.80 0.53 2.38 3.12 1.23 2.34 3.20 3.35 2.28 2.50 3.23
RZL -2.10 -1.88 -1.20 -0.91 -0.50 0.72 0.30 0.35 1.50 0.34 0.50 1.60
Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA
are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top
table: Results for various values of NH (the number of sentences in the high quality training subset). Evaluation
is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and
the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL
algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from
the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific NH
value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the
improvements for the LT ?s of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports
results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
tion and null elements as in (Klein, 2005). To evalu-
ate the quality of a parse tree with respect to its gold
standard, the unlabeled parsing F-score is used.
5 Results
Entire Corpus Results. We start by discussing
the effect of ZL on the performance of the Seginer
parser when no length restriction is imposed on the
test corpus sentences (WSJ, BROWN and GENIA).
Table 1 (top, right section, for WSJ), Figure 1 (top
line, right graph, for WSJ), and Table 2 (the left sec-
tion of each table, top table for BROWN and bot-
tom table for GENIA) present the difference between
the F-score performance of the Seginer parser when
trained with the ZL algorithms and the parser?s per-
formance when trained with the entire corpus.
For all test corpora and sizes of the high quality
training subset (NH ), zoomed learning improves the
parser performance. ZL improves the parser perfor-
mance by 1.13% (WSJ), 1.46% (BROWN, the number
does not appear in the table) and 4.47% (GENIA).
For WSJ, the most substantial improvement is pro-
vided by EZL, while for BROWN and GENIA the best
results for some NH values are achieved by BZL and
for others by EZL (and for GENIA with small NH
values even by RZL).
Note, that for all three corpora zoomed learning
with random selection (RZL) improves the parser
performance on the entire test corpus, although to a
lesser extent than confidence-based ZL. This is true
for almost all NH values, including those that do not
appear in the tables. See Figure 1 (top line, right-
most graph) for WSJ.
We follow the unsupervised parsing literature and
provide performance analysis for WSJ sentences of
bounded length (WSJ10, WSJ20 and WSJ40). To
prevent clutter, for BROWN and GENIA we report
only entire corpus results.
Table 1 (top, left three sections) and Figure 1
(top line, three leftmost graphs) present results for
WSJ10, WSJ20 and WSJ40.
The result patterns for the sub-corpora are similar
to those reported for the entire WSJ corpus. EZL and
BZL both improve over the fully-trained parser, and
689
BROWN ENTIRE CORPUS (F = 57.19) LT HT
NH 10% 30% 50% 70% 10% 30% 50% 70% 10% 30% 50% 70%
EZL 0.55 0.69 0.64 0.66 0.65 0.82 1.15 1.31 -1.44 -0.03 0.04 0.30
BZL 1.11 0.80 0.02 -0.10 1.42 1.20 0.76 0.51 -4.80 -1.30 -0.79 -0.37
RZL 0.257 0.755 0.49 0.24 0.23 0.75 0.60 0.53 0.44 0.76 0.42 0.12
GENIA ENTIRE CORPUS (F = 42.71) LT HT
NH 10% 30% 50% 70% 10% 30% 50% 70% 10% 30% 50% 70%
EZL 0.01 0.83 1.10 1.66 -0.01 0.76 0.80 3.37 0.34 1.00 1.40 1.55
BZL -0.46 1.40 2.74 4.47 -0.54 0.40 0.96 4.09 0.42 4.29 4.60 5.49
RZL 0.61 1.70 2.09 1.99 0.28 2.08 3.30 3.86 3.04 3.22 2.80 1.85
Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire
corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset
(right column section, HT ) of each corpus, as a function of the high quality training set size (NH). Since the tables
present entire corpus results, the training and test subsets are identical.
the improvement of the former is more substantial.
Baselines. A key principle of ZL is the selection
of subsets that are better parsed by a parser trained
only with the sentences they contain than with a
parser trained with the entire training corpus. To
verify the importance of this principle we consid-
ered two alternative training protocols.
In the first, the entire test corpus is parsed with
a parser that was trained with a subset of randomly
selected sentences from the training set. We run this
protocol for all three corpora (and for the WSJ sub-
corpora) with various training set sizes and obtained
substantial degradation in the parser performance.
The performance monotonically increases with the
training set size and reached its maximum when the
entire corpus is used. We conclude that using less
training material harms the parser performance if a
test subset is not carefully selected.
The second protocol is the ?less is more proto-
col of Spitkovsky et al, (2010) in which we parsed
each test corpus using a parser that was trained with
all training sentences of a bounded length. Unlike
in their paper, in which this protocol improves the
perofrmance of the DMV unsupervised dependency
parser (Klein and Manning, 2004), for the Seginer
parser the protocol harms the results. When pars-
ing the entire WSJ with a WSJ10-trained parser or
with a WSJ20-trained parser, the F-score results are
59.99% and 72.22% compared to 76.00% of the
fully-trained parser. For GENIA the numbers are
15.61 and 35.87 compared to 42.71 and for BROWN
they are 36.05 and 50.02 compared to 57.19 3.
It is also interesting that sentence length is gen-
erally not a good subset selection criterion for ZL.
When parsing WSJ10 with a WSJ10-trained parser,
F-score results are 59.29 while the F-score of the
fully-trained parser on this corpus is 76.00. The
same phenomenon is observed with WSJ20 (F-score
of 61.90 with WSJ20 training and of 64.82 with
the entire WSJ training), and for the BROWN corpus
(65.01 vs. 69.43 for BROWN10 and 61.90 vs 62.92
for BROWN20). For GENIA, however, while parsing
GENIA10 with a GENIA10-trained parser harms the
performance (45.28 vs. 60.23), parsing GENIA20
with a GENIA20-trained parser enhances the perfor-
mance (53.23 vs. 50.00).
These results emphasize the power of random se-
lection for ZL as random selection does provide a
good selection criterion.
LT vs. HT. In what follows we analyze the ZL
algorithms aiming to characterize their strengths and
weaknesses.
Table 1 (bottom), the middle and right sections
of Table 2 (both tables) and Figure 1 (second and
third lines) present the performance of the ZL algo-
rithms on the lower quality and higher quality test
subsets (LT and HT ). The results patterns for WSJ
and BROWN are different than those of GENIA.
For WSJ (and its sub-corpora) and BROWN,
3We repeated this protocol multiple times for each corpus,
training the parser with sentences of length 5 to 45 in steps of 5.
In all cases we observed performance degradation compared to
the fully-trained parser.
690
1 2 3 4
x 104
?2
?1
0
1
WSJ10, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?0.5
0
0.5
1
1.5
WSJ20, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?0.5
0
0.5
1
1.5
WSJ40, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?0.5
0
0.5
1
1.5
WSJ, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?3
?2
?1
0
1
WSJ10, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?4
?2
0
2
4
WSJ20, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?4
?2
0
2
4
WSJ40, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?4
?2
0
2
4
WSJ, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?10
?8
?6
?4
?2
0
WSJ10, High Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?6
?4
?2
0 
1
WSJ20, High Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?6
?4
?2
0 
1
WSJ40, High Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?6
?4
?2
0 
1
X
F 
Sc
or
e 
D
iff
er
en
ce
WSJ, High Quality set
1 2 3 4
x 104
?5
0
5
10
WSJ, Low Quality set
X
F 
Sc
or
e D
iffe
re
nc
e
0 1 2 3 4 5
x 104
?30
?20
?10
0
WSJ, High Quality set
X
F 
Sc
or
e D
iffe
re
nc
e
0 1 2 3 4 5
x 104
?1
0
1
2
3
WSJ, Whole Corpus
X
F 
Sc
or
e D
iffe
re
nc
e
Figure 1: WSJ results. Top Three Lines: Difference in F-score performance of the Seginer parser between training
with ZL and training with the entire WSJ corpus. Results are presented for the entire corpus (top line), the lower
quality test subset (LT , middle line) and the higher quality test subset (HT , bottom line) as a function of the size of
the high quality training subset X = NH , measured in sentences. The curve with triangles is for the extended zoomed
learning algorithm (EZL), the solid curve is for the basic zoomed learning algorithm (BZL) and the dashed curve is
for zoomed learning with random selection (RZL). Bottom line: Comparison between the performance of the Seginer
parser with the EZL algorithm (curves with triangles) and when subset selection is performed using the oracle F-score
of the trees (solid curves). F-score differences from the performance of the fully trained parser are presented for the
WSJ test corpus as a function of NH , the high quality training subset size. Oracle selection is superior for the lower
quality subset but inferior for the high quality subset.
confidence-based ZL (BZL and EZL) provides a sub-
stantial improvement for LT . For WSJ, F-score im-
provement is up to 1.32% (WSJ10), 3.13% (WSJ20),
3.35% (WSJ40) and 3.23% (the entire WSJ). For
BROWN the improvement is up to 1.42%.
For HT , confidence-based ZL is less effective
when these corpora are considered. As indicated
in the third line of Figure 1, for WSJ and its sub-
corpora, EZL leads to a small improvement on HT ,
while BZL generally leads to a performance degra-
dation on this test subset. For BROWN (the right sec-
tion of Table 2 (top)), confidence-based ZL gener-
ally leads to a performance degradation on HT .
For GENIA, EZL and BZL improve the parser per-
formance on both LT and HT for most NH values.
Understanding this difference is a subject for future
research. Our initial hypothesis is that due to the
relative small size of the GENIA corpus (4661 sen-
tences compared 24243 and 49206 sentences of WSJ
and BROWN respectively), there is more room for
improvement in the parser performance on this cor-
pus, and consequently ZL improves on both sets.
Oracle Analysis. Confidence-based ZL is based
on the idea that sentences for which the fully-trained
691
parser provides parses of similar quality manifest
similar syntactic patterns. Consequently, the parser
performance on a set of such sentences can be im-
proved if it is trained only with the sentences con-
tained in the set. An oracle experiment, where se-
lection is based on the F-score computed using the
gold standard tree instead of on the PUPA score, can
shed light on the validity of this idea.
Figure 1 (bottom line) compares the performance
of EZL with that of the oracle-based zoomed learn-
ing algorithm when the test corpus is the entire WSJ.
For the low quality test subset, oracle selection is
dramatically better than confidence-based selection.
For the high quality test subset the opposite pattern
holds, that is, EZL is superior. These differences lead
to the entire corpus pattern where EZL is superior for
most NH values.
Oracle-based and confidence-based zoomed
learning demonstrate the same trend: they improve
over the baseline for LT much more than for HT .
For HT , oracle-based ZL even harms results and
so does BZL, which does not benefit from the
averaging effect of EZL. The magnitude of the
effect of oracle-based zoomed learning is much
stronger. These results support our idea that training
the parser on a set selected by a well-designed
confidence test leads to improvement of the parser
performance for the selected sentences when the
fully-trained parser produces parses of mediocre
quality for them.
Integration of the experimental results for zoomed
learning with the three selection methods: random,
confidence-based and oracle-based leads to an im-
portant conclusion that should guide future research.
The more accurate the confidence score used by the
zoomed learning algorithm, the more substantial is
the performance improvement for the low quality
test subset, at the cost of more substantial degrada-
tion in the performance on the high quality subset
(but recall the different GENIA pattern which should
be further explored).
EZL Variants. For confidence-based ZL we ex-
plored two methods for utilizing the ensemble mem-
bers for generating a final parse tree for each of the
test sentences. In BZL, the L-trained parser and the
H-trained parser generate parse trees for LT and
HT sentences respectively. In EZL, for each sen-
tence the final parse is selected between the parse
created by a parser trained with the sentences con-
tained in its corresponding training subset, and the
parse created by the fully trained parser.
There are other ways to use the ensemble mem-
bers. While for all corpora it is beneficial to use
the L-trained parser for the low quality test subset
(LT ), the results for WSJ and BROWN imply that it
might be better to use the fully-trained parser or the
EZL algorithm to parse the high quality test subset
(HT ). We have experimented with these methods
and got only a minor improvement over the results
reported here (improvement is more substantial for
BROWN than for WSJ but does not exceed 0.5% for
both). This can also be inferred from the relative
minor performance degradation of BZL and EZL on
HT .
We also explored a ZL scenario in which the en-
tire test set is parsed either by the H-trained parser
or by the L-trained parser. These protocols result in
substantial degradation in parser performance (com-
pared to the fully-trained parser) since the perfor-
mance of the H-trained parser on LT and the per-
formance of the L-trained parser on HT are poor.
6 Conclusions
We introduced zoomed learning ? a training algo-
rithm for unsupervised parsers. We applied three
variants of ZL to the best fully unsupervised pars-
ing algorithm (Seginer, 2007) and show an improve-
ment of up to 4.47% in three English domains: WSJ,
BROWN and GENIA.
Future research should focus on the development
of more accurate estimators of parser output qual-
ity, and experimentation with different corpora, lan-
guages and parsers.
Developing a quality assessment algorithm for de-
pendency trees will allow us to apply confidence-
based ZL to unsupervised dependency parsing. Par-
ticularly, it will enable us to explore the combina-
tion of the methods proposed in (Spitkovsky et al,
2010) with ZL for the DMV model and to integrate
the PUPA score into their bootstrapping algorithm.
Another direction is to apply ZL to other NLP
tasks and ML areas, supervised and unsupervised.
692
References
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ?05.
Rens Bod, 2006a. An all-subtrees approach to unsuper-
vised parsing. ACL-COLING ?06.
Rens Bod, 2006b. Unsupervised parsing with U-DOP.
CoNLL ?06.
Rens Bod, 2007. Is the end of supervised parsing in
sight? ACL ?07.
Leo Breiman, 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Rich Caruana and Alexandru Niculescu-Mizil, 2006.
An empirical comparison of supervised learning algo-
rithms. ICML ?06.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Alexander Clark, 2003. Combining distributional and
morphological information for part of speech induc-
tion. EACL ?03.
Shay Cohen, Kevin Gimpel and Noah Smith, 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. NIPS ?08.
Shay Cohen and Noah Smith, 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. NAACL ?09.
David Cohn, Les Atlas and Richard Ladner. 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201?221.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. CogSci ?05.
W. N. Francis and H. Kucera 1979. Manual of infor-
mation to accompany a standard corpus of present-day
edited American English, for use with digital com-
puters. Department of Linguistics, Brown University
Press, Providence, RI.
Yoav Freund and Robert E. Schapire, 1996. Experiments
with a new boosting algorithm. ICML ?96.
William Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. NAACL
?09.
John Henderson and Eric Brill, 2000. Bagging and
boosting a treebank parser. NAACL ?00.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Daisuke Kawahara and Kiyotaka Uchimoto 2008.
Learning reliability of parses for domain adaptation of
dependency parsing. IJCNLP ?08.
Dan Klein and Christopher Manning, 2002. A genera-
tive constituent-context model for improved grammar
induction. ACL ?02.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. ACL ?04.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and Jun?ichi
Tsujii, 2003. GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics, (sup-
plement: 11th ISMB) 19:i180?i182, Oxford Univer-
sity Press, 2003.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson,
2006. Reranking and self-training for parser adapta-
tion. ACL-COLING ?06.
Sujith Ravi, Kevin Knight and Radu Soricut, 2008. Au-
tomatic prediction of parser accuracy. EMNLP ?08.
Roi Reichart and Ari Rappoport, 2007. An ensemble
method for selection of high quality parses. ACL ?07.
Roi Reichart and Ari Rappoport, 2009a. Sample se-
lection for statistical parsers: cognitively driven algo-
rithms and evaluation measures. CoNLL ?09.
Roi Reichart and Ari Rappoport, 2009b. Automatic se-
lection of high quality parses created by a fully unsu-
pervised parser. CoNLL ?09.
Yoav Seginer, 2007. Fast unsupervised incremental pars-
ing. ACL ?07.
Noah Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
ACL-COLING ?06.
Valentin Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky, 2010. From baby steps to leapfrog: how ?less is
more? in unsupervised dependency parsing. NAACL
?10.
Ricardo Vilalta and Irina Rish, 2003. A decomposition
of classes via clustering to explain and improve naive
bayes. ECML ?03.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting parser errors using web-based
semantic filters . EMNLP ?06.
693
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1368?1378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning to Map into a Universal POS Tagset
Yuan Zhang, Roi Reichart, Regina Barzilay
Massachusetts Institute of Technology
{yuanzh, roiri, regina}@csail.mit.edu
Amir Globerson
The Hebrew University
gamir@cs.huji.ac.il
Abstract
We present an automatic method for mapping
language-specific part-of-speech tags to a set
of universal tags. This unified representation
plays a crucial role in cross-lingual syntactic
transfer of multilingual dependency parsers.
Until now, however, such conversion schemes
have been created manually. Our central hy-
pothesis is that a valid mapping yields POS
annotations with coherent linguistic proper-
ties which are consistent across source and
target languages. We encode this intuition
in an objective function that captures a range
of distributional and typological characteris-
tics of the derived mapping. Given the ex-
ponential size of the mapping space, we pro-
pose a novel method for optimizing over soft
mappings, and use entropy regularization to
drive those towards hard mappings. Our re-
sults demonstrate that automatically induced
mappings rival the quality of their manually
designed counterparts when evaluated in the
context of multilingual parsing.1
1 Introduction
In this paper, we explore an automatic method for
mapping language-specific part-of-speech tags to a
universal tagset. In multilingual parsing, this uni-
fied input representation is required for cross-lingual
syntactic transfer. Specifically, the universal tagset
annotations enable an unlexicalized parser to capi-
talize on annotations from one language when learn-
ing a model for another.
1The source code and data for the work presented in this
paper is available at http://groups.csail.mit.edu/
rbg/code/unitag/emnlp2012
While the notion of a universal POS tagset is
widely accepted, in practice it is hardly ever used
for annotation of monolingual resources. In fact,
available POS annotations are designed to capture
language-specific idiosyncrasies and therefore are
substantially more detailed than a coarse universal
tagset. To reconcile these cross-lingual annotation
differences, a number of mapping schemes have
been proposed in the parsing community (Zeman
and Resnik, 2008; Petrov et al 2011; Naseem et
al., 2010). In all of these cases, the conversion is
performed manually and has to be repeated for each
language and annotation scheme anew.
Despite the apparent simplicity, deriving a map-
ping is by no means easy, even for humans. In fact,
the universal tagsets manually induced by Petrov
et al(2011) and by Naseem et al(2010) disagree
on 10% of the tags. An example of such discrep-
ancy is the mapping of the Japanese tag ?PVfin? to
the universal tag ?particle? according to one scheme,
and to ?verb? according to another. Moreover, the
quality of this conversion has a direct implication on
the parsing performance. In the Japanese example
above, this difference in mapping yields a 6.7% dif-
ference in parsing accuracy.
The goal of our work is to induce the mapping
for a new language, utilizing existing manually-
constructed mappings as training data. The exist-
ing mappings developed in the parsing community
rely on gold POS tags for the target language. A
more realistic scenario is to employ the mapping
technique to resource-poor languages where gold
POS annotations are lacking. In such cases, a map-
ping algorithm has to operate over automatically in-
1368
duced clusters on the target language (e.g., using
the Brown algorithm) and convert them to universal
tags. We are interested in a mapping approach that
can effectively handle both gold tags and induced
clusters.
Our central hypothesis is that a valid mapping
yields POS annotations with coherent linguistic
properties which are consistent across languages.
Since universal tags play the same linguistic role
in source and target languages, we expect similar-
ity in their global distributional statistics. Figure 1a
shows statistics for two close languages, English and
German. We can see that their unigram frequencies
on the five most common tags are very close. Other
properties concern POS tag per sentence statistics ?
e.g., every sentence has to have at least one verb. Fi-
nally, the mappings can be further constrained by ty-
pological properties of the target language that spec-
ify likely tag sequences. This information is readily
available even for resource poor language (Haspel-
math et al 2005). For instance, since English and
German are prepositional languages, we expect to
observe adposition-noun sequences but not the re-
verse (see Figure 1b for sample sentences). We en-
code these heterogeneous properties into an objec-
tive function that guides the search for the optimal
mapping.
Having defined a quality measure for mappings,
our goal is to find the optimal mapping. However,
such partition optimization problems2 are NP hard
(Garey and Johnson, 1979). A naive approach to
the problem is to greedily improve the map, but it
turns out that this approach yields poor quality map-
pings. We therefore develop a method for optimiz-
ing over soft mappings, and use entropy regulariza-
tion to drive those towards hard mappings. We con-
struct the objective in a way that facilitates simple
monotonically improving updates corresponding to
solving convex optimization problems.
We evaluate our mapping approach on 19
languages that include representatives of Indo-
European, Semitic, Basque, Japonic and Turkic fam-
ilies. We measure mapping quality based on the
target language parsing accuracy. In addition to
considering gold POS tags for the target language,
2Instances of related hard problems are 3-partition and
subset-sum.
we also evaluate the mapping algorithm on auto-
matically induced POS tags. In all evaluation sce-
narios, our model consistently rivals the quality
of manually induced mappings. We also demon-
strate that the proposed inference procedure outper-
forms greedy methods by a large margin, highlight-
ing the importance of good optimization techniques.
We further show that while all characteristics of
the mapping contribute to the objective, our largest
gain comes from distributional features that capture
global statistics. Finally, we establish that the map-
ping quality has a significant impact on the accuracy
of syntactic transfer, which motivates further study
of this topic.
2 Related Work
Multilingual Parsing Early approaches for multi-
lingual parsing used parallel data to bridge the gap
between languages when modeling syntactic trans-
fer. In this setup, finding the mapping between var-
ious POS annotation schemes was not essential; in-
stead, the transfer algorithm could induce it directly
from the parallel data (Hwa et al 2005; Xi and
Hwa, 2005; Burkett and Klein, 2008). However,
more recent transfer approaches relinquish this data
requirement, learning to transfer from non-parallel
data (Zeman and Resnik, 2008; McDonald et al
2011; Cohen et al 2011; Naseem et al 2010).
These approaches assume access to a common input
representation in the form of universal tags, which
enables the model to connect patterns observed in
the source language to their counterparts in the tar-
get language.
Despite ongoing efforts to standardize POS tags
across languages (e.g., EAGLES initiative (Eynde,
2004)), many corpora are still annotated with
language-specific tags. In previous work, their map-
ping to universal tags was performed manually. Yet,
even though some of these mappings have been de-
veloped for the same CoNLL dataset (Buchholz and
Marsi, 2006; Nivre et al 2007), they are not identi-
cal and yield different parsing performance (Zeman
and Resnik, 2008; Petrov et al 2011; Naseem et al
2010). The goal of our work is to automate this pro-
cess and construct mappings that are optimized for
performance on downstream tasks (here we focus on
parsing). As our results show, we achieve this goal
1369
Noun Verb Det. Prep. Adj.0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Unigr
amFr
equen
cy
EnglishGerman -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases]and sales [by corporate insiders]
-Einer der sich [f?r den Milliard?r] [ausspricht] [ist] Steve Jobs dem Perot [f?r den aufbau]der Computerfirma Next 20 Millionen Dollar [bereitstellte]
(a) (b)
Figure 1: Illustration of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five
tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in
blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions.
on a broad range of languages and evaluation sce-
narios.
Syntactic Category Refinement Our work also
relates to work in syntactic category refinement in
which POS categories and parse tree non-terminals
are refined in order to improve parsing perfor-
mance (Finkel et al 2007; Klein and Manning,
2003; Matsuzaki et al 2005; Petrov et al 2006;
Petrov and Klein, 2007; Liang et al 2007). Our
work differs from these approaches in two ways.
First, these methods have been developed in the
monolingual setting, while our mapping algorithm is
designed for multilingual parsing. Second, these ap-
proaches are trained on the syntactic trees of the tar-
get language, which enables them to directly link the
quality of newly induced categories with the quality
of syntactic parsing. In contrast, we are not given
trees in the target language. Instead, our model is
informed by mappings derived for other languages.
3 Task Formulation
The input to our task consists of a target corpus writ-
ten in a language T , and a set of non-parallel source
corpora written in languages {S1, . . . , Sn}. In the
source corpora, each word is annotated with both
a language-specific POS tag and a universal POS
tag (Petrov et al 2011). In the target corpus each
word is annotated only with a language-specific POS
tag, either gold or automatically induced.
Our goal is to find a map from the set of LT target
language tags to the set of K universal tags. We as-
sume that each language-specific tag is only mapped
to one universal tag, which means we never split a
language-specific tag and LT ? K holds for every
language. We represent the map by a matrix A of
size K ? LT where A(c|f) = 1 if the target lan-
guage tag f is mapped to the universal tag c, and
A(c|f) = 0 otherwise.3 Note that each column of
A should contain a single value of 1. We will later
relax the requirement thatA(c|f) ? {0, 1}. A candi-
date mappingA can be applied to the target language
to produce sentences labeled with universal tags.
4 Model
In this section we describe an objective that reflects
the quality of an automatic mapping.
Our key insight is that for a good mapping, the
statistics over the universal tags should be similar for
source and target languages because these tags play
the same role cross-linguistically. For example, we
should expect the frequency of a particular universal
tag to be similar in the source and target languages.
One choice to make when constructing an objec-
tive is the source languages to which we want to be
similar. It is clear that choosing all languages is not a
good idea, since they are not all expected to have dis-
tributional properties similar to the target language.
There is strong evidence that projecting from sin-
gle languages can lead to good parsing performance
3We use c and f to reflect the fact that universal tags are
a coarse version (hence c) of the language specific fine tags
(hence f ).
1370
(McDonald et al 2011). Therefore, our strategy is
to choose a single source language for comparison.
The choice of the source language is based on sim-
ilarity between typological properties; we describe
this in detail in Section 5.
We must also determine which statistical proper-
ties we expect to be preserved across languages. Our
model utilizes three linguistic phenomena which are
consistent across languages: POS tag global distri-
butional statistics, POS tag per sentence statistics,
and typology-based ordering statistics. We define
each of these below.
4.1 Mapping Characterization
We focus on three categories of mapping properties.
For each of the relevant statistics we define a func-
tion Fi(A) that has low values if the source and tar-
get statistics are similar.
Global distributional statistics: The unigram and
bigram statistics of the universal tags are expected
to be similar across languages with close typological
profiles. We use pS(c1, c2) to denote the bigram dis-
tribution over universal tags in the source language,
and pT (f1, f2) to denote the bigram distribution over
language specific tags in the target language. The
bigram distribution over universal tags in the target
language depends on A and pT (f1, f2) and is given
by:
pT (c1, c2;A) =
?
f1,f2
A(c1|f1)A(c2|f2)pT (f1, f2)
(1)
To enforce similarity between source and target dis-
tributions, we wish to minimize the KL divergence
between the two: 4
Fbi(A) = DKL[pS(c1, c2)|pT (c1, c2;A)] (2)
We similarly define Funi(A) as the distance be-
tween unigram distributions.
Per sentence statistics: Another defining property
of POS tags is their average count per sentence.
Specifically, we focus on the verb count per sen-
tence, which we expect be similar across languages.
4We use the KL divergence because it assigns low weights
to infrequent universal tags. Furthermore, this choice results in
a simple, EM-like parameter estimation algorithm as discussed
in Section 5.
To express this constraint, we use nv(s,A) to
denote the number of verbs (i.e., the universal
tags corresponding to verbs according to A) in
sentence s. This is a linear function of A. We also
use E[nv(s,A)] to denote the average number of
verbs per sentence, and V [nv(s,A)] to denote the
variance. We estimate these two statistics from
the source language and denote them by ESv, VSv.
Good mappings are expected to follow these
patterns by having a variance upper bounded by
VSv and an average lower bounded by ESv.5 This
corresponds to minimizing the following objectives:
FEv(A) = max [0, ESv ? E[nv(s,A)]]
FV v(A) = max [0, V [nv(s,A)]? VSv]
Note that the above objectives are convex in A,
which will make optimization simpler. We refer to
the two terms jointly as Fverb(A).
Typology-based ordering statistics: Typolog-
ical features can be useful for determining the
relative order of different tags. If we know that
the target language has a particular typological
feature, we expect its universal tags to obey the
given relative ordering. Specifically, we expect it to
agree with ordering statistics for source languages
with a similar typology. We consider two such
features here. First, in pre-position languages the
preposition is followed by the noun phrase. Thus, if
T is such a language, we expect the probability of
a noun phrase following the adposition to be high,
i.e., cross some threshold. Formally, we define C1 =
{noun, adj, num, pron, det} and consider the set of
bigram distributions Spre that satisfy the following
constraint:
?
c?C1
pT (adp,c) ? apre (3)
where apre =
?
c?C1 pS(adp,c) is calculated from
the source language. This constraint set is non-
convex in A due to the bilinearity of the bi-
gram term. To simplify optimization6 we take an
5The rationale is that we want to put a lower bound on the
number of verbs per sentence, and induce it from the source
language. Furthermore, we expect the number of verbs to be
well concentrated, and we induce its maximal variance from
the source language.
6In Section 5 we shall see that this makes optimization eas-
ier.
1371
approach inspired by the posterior regularization
method (Ganchev et al 2010) and use the objective:
Fc(A) = min
r(c1,c2)?Spre
DKL[r(c1, c2)|pT (c1, c2;A)]
(4)
The above objective will attain lower values for A
such that pT (c1, c2;A) is close to the constraint set.
Specifically, it will have a value of zero when the
bigram distribution induced by A has the property
specified in Spre. We similarly define a set Spost
for post-positional languages.
As a second typological feature, we consider the
Demonstrative-Noun ordering. In DN languages we
want the probability of a determiner to come be-
fore C2 = {noun, adj, num}, (i.e., frequent universal
noun-phrase tags), to cross a threshold. This con-
straint translates to:
?
c?C2
pT (det, c) ? adet (5)
where adet =
?
c?C2 pS(det, c) is a threshold de-
termined from the source language. We denote the
set of distributions that have this property by SDN,
and add them to the constraint in (4). The overall
constraint set is denoted by S.
4.2 The Overall Objective
We have defined a set of functions Fi(A) that are
expected to have low values for good mappings. To
combine those, we use a weighted sum: F?(A) =?
i ?i ? Fi(A). (The weights in this equation are
learned; we discussed the procedure in Section 5)
Optimizing over the set of mappings is difficult
since each mapping is a discrete set whose size is
exponential size in LT . Technically, the difficulty
comes from the requirement that elements of A are
integral and its columns sum to one. To relax this
restriction, we will allow A(c|f) ? [0, 1] and en-
courage A to correspond to a mapping by adding an
entropy regularization term:
H[A] = ?
?
f
?
c
A(c|f) logA(c|f) (6)
This term receives its minimal value when the con-
ditional probability of the universal tags given a
language-specific tag is 1 for one universal tag and
zero for the others.
The overall objective is then: F (A) = F?(A) +
? ?H[A], where ? is the weight of the entropy term.7
The resulting optimization problem is:
min
A??
F (A) (7)
where ? is the set of non-negative matrices whose
columns sum to one:
? =
{
A :
A(c|f) ? 0 ?c, f
?K
c=1A(c|f) = 1 ?f
}
(8)
5 Parameter Estimation
In this section we describe the parameter estimation
process for our model. We start by describing how
to optimize A. Next, we discuss the weight selec-
tion algorithm, and finally the method for choosing
source languages.
5.1 Optimizing the Mapping A
Recall that our goal is to solve the optimization
problem in Eq. (7). This objective is non convex
since the function H[A] is concave, and the objec-
tive F (A) involves bilinear terms in A and loga-
rithms of their sums (see Equations (1) and (2)).
While we do not attempt to solve the problem
globally, we do have a simple update scheme that
monotonically decreases the objective. The update
can be derived in a similar manner to expectation
maximization (EM) (Neal and Hinton, 1999) and
convex concave procedures (Yuille and Rangarajan,
2003). Figure 2 describes our optimization algo-
rithm. The key ideas in deriving it are using pos-
terior distributions as in EM, and using a variational
formulation of entropy. The term Fc(A) is handled
in a similar way to the posterior regularization algo-
rithm derivation. A detailed derivation is provided
in the supplementary file. 8
The kth iteration of the algorithm involves several
steps:
? In step 1, we calculate the current esti-
mate of the bigram distribution over tags,
pT (c1, c2;Ak).
7Note that as ? ? ?, only valid maps will be selected by
the objective.
8The supplementary file is available at http://groups.
csail.mit.edu/rbg/code/unitag/emnlp2012.
1372
? In step 2, we find the bigram distribution in
the constraint set S that is closest in KL di-
vergence to pT (c1, c2;Ak), and denote it by
rk(c1, c2). This optimization problem is con-
vex in r(c1, c2).
? In step 3, we calculate the bigram posterior
over language specific tags given a pair of uni-
versal tags. This is analogous to the standard
E-step in EM.
? In step 4, we use the posterior in step 3 and the
bigram distributions pS(c1, c2) and rk(c1, c2)
to obtain joint counts over language specific
and universal bigrams.
? In step 5, we use the joint counts from step 4
to obtain counts over pairs of language specific
and universal tags.
? In step 6, analogous to the M-step in EM, we
optimize over the mapping matrix A. The ob-
jective is similar to the Q function in EM, and
also includes the Fverb(A) term, and a linear
upper bound on the entropy term. The objec-
tive can be seen to be convex in A.
As mentioned above, each of the optimization prob-
lems in steps 2 and 6 is convex, and can therefore be
solved using standard convex optimization solvers.
Here, we use the CVX package (Grant and Boyd,
2008; Grant and Boyd, 2011). It can be shown that
the algorithm improves F (A) at every iteration and
converges to a local optimum.
The above algorithm generates a mapping A that
may contain fractional entries. To turn it into a hard
mapping we round A by mapping each f to the c
that maximizes A(c|f) and then perform greedy im-
provement steps (one f at a time) to further improve
the objective. The regularization constant ? is tuned
to minimize the F?(A) value of the rounded A.
5.2 Learning the Objective Weights
Our F?(A) objective is a weighted sum of the in-
dividual Fi(A) functions. In the following, we de-
scribe how to learn the ?i weights for every target
language. We would like F?(A) to have low values
when A is a good map. Since our performance goal
is parsing accuracy, we consider a map to be good
Initialize A0.
Repeat
Step 1 (calculate current bigram estimate):
pT (c1, c2;A
k) =
?
f1,f2
Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 2 (incorporate constraints):
rk(c1, c2) = arg min
r?S
DKL[r(c1, c2)|pT (c1, c2;A
k)]
Step 3 (calculate model posterior):
p(f1, f2|c1, c2;Ak) ? Ak(c1|f1)Ak(c2|f2)pT (f1, f2)
Step 4: (complete joint counts):
Nk(c1, c2, f1, f2) = p(f1, f2|c1, c2;A
k)
(
rk(c1, c2) + pS(c1, c2)
)
Step 5 (obtain pairwise):
Mk(c, f) = Nk1 (c, f) +N
k
2 (c, f)
where Nk1 (c, f) =
?
c2,f2
Nk(c, c2, f, f2) and similarly for
Nk2 (c, f).
Step 6 (M step with entropy linearization): Set Ak+1 to be the
solution of
min
A??
?
?
c,f
[
Mk(c, f) logA(c|f) + A(c|f) logAk(c|f)
]
+ Fverb(A)
Until Convergence of Ak
Figure 2: An iterative algorithm for minimizing our ob-
jective in Eq. (7). For simplicity we assume that all the
weights ?i and ? are equal to one. It can be shown that
the objective monotonically decreases in every iteration.
if it results in high parsing accuracy, as measured
when projecting a parser from to S to T .
Since we do not have annotated parses in T , we
use the other source languages S = {S1, . . . , Sn}
to learn the weight. For each Si as the target, we
first train a parser for each language in S \ {Si} as
if it was the source, using the map of Petrov et al
(2011), and choose S?i ? S \ {Si} which gives the
highest parsing accuracy on Si. Next we generate
7000 candidate mappings for Si by randomly per-
turbing the map of (Petrov et al 2011). We evalu-
ate the quality of each candidate A by projecting the
parser of S?i to Si, and recording the parsing accu-
racy. Among all the candidates we choose the high-
est accuracy one and denote it by A?(Si). We now
want the score F (A?(Si)) to be lower than that of all
other candidates. To achieve this, we train a ranking
SVM whose inputs are pairs of mapsA?(Si) and an-
1373
other worse A(Si). These map pairs are taken from
many different traget languages, i.e. many different
Si. The features given to the SVM are the terms of
the score Fi(A). The goal of the SVM is to weight
these terms such that the better map A?(Si) has a
lower score. The weights assigned by the SVM are
taken as ?i.
5.3 Source Language Selection
As noted in Section 4 we construct F (A) by choos-
ing a single source language S. Here we describe the
method for choosing S. Our goal is to choose S that
is closest to T in terms of typology. Assume that
languages are described by binary typological vec-
tors vL. We would like to learn a diagonal matrix
D such that d(S, T ;D) = (vS ? vT )TD(vS ? vT )
reflects the similarity between the languages. In our
context, a good measure of similarity is the perfor-
mance of a parser trained on S and projected on T
(using the optimal map A). We thus seek a matrix
D such that d(S, T ;D) is ranked according to the
parsing accuracy. The matrix D is trained using an
SVM ranking algorithm that tries to follow the rank-
ing of parsing accuracy. Similar to the technique for
learning the objective weights, we train across many
pairs of source languages.9
The typological features we use are a subset
of the features described in ?The World Atlas of
Languages Structure? (WALS, (Haspelmath et al
2005)), and are shown in Table 1.
6 Evaluation Set-Up
Datasets We test our model on 19 languages: Ara-
bic, Basque, Bulgarian, Catalan, Chinese, Czech,
Danish, Dutch, English, German, Greek, Hungar-
ian, Italian, Japanese, Portuguese, Slovene, Span-
ish, Swedish, and Turkish. Our data is taken from
the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al 2007). The
CoNLL datasets consist of manually created depen-
dency trees and language-specific POS tags. Fol-
lowing Petrov et al(2011), our model maps these
language-specific tags to a set of 12 universal tags:
noun, verb, adjective, adverb, pronoun, determiner,
adposition, numeral, conjunction, particle, punctua-
tion mark and X (a general tag).
9Ties are broken using the F (A) objective.
Evaluation Procedure We perform a separate ex-
periment for each of the 19 languages as the tar-
get and a source language chosen from the rest (us-
ing the method from Section 5.3). For the selected
source language, we assume access to the mapping
of Petrov et al(2011).
Evaluation Measures We evaluate the quality of
the derived mapping in the context of the target lan-
guage parsing accuracy. In both the training and
test data, the language-specific tags are replaced
with universal tags: Petrov?s tags for the source lan-
guages and learned tags for the target language. We
train two non-lexicalized parsers using source anno-
tations and apply them to the target language. The
first parser is a non-lexicalized version of the MST
parser (McDonald et al 2005) successfully used in
the multilingual context (McDonald et al 2011). In
the second parser, parameters of the target language
are estimated as a weighted mixture of parameters
learned from supervised source languages (Cohen et
al., 2011). For the parser of Cohen et al(2011), we
trained the model on the four languages used in the
original paper ? English, German, Czech and Ital-
ian. When measuring the performance on each of
these four languages, we selected another set of four
languages with a similar level of diversity.10
Following the standard evaluation practice in
parsing, we use directed dependency accuracy as our
measure of performance.
Baselines We compare mappings induced by our
model against three baselines: the manually con-
structed mapping of Petrov et al(2011), a randomly
constructed mapping and a greedy mapping. The
greedy mapping uses the same objective as our full
model, but optimizes it using a greedy method. In
each iteration, this method makes |LT | passes over
the language-specific tags, selecting a substitution
that contributes the most to the objective.
Initialization To reduce the dimension of our al-
gorithm?s search space and speed up our method, we
start by clustering the language-specific POS tags of
the target into |K| = 12 clusters using an unsuper-
10We also experimented with a version of the Cohen et al
(2011) model trained on all the source languages. This set-up
resulted in decreased performance. For this reason, we chose to
train the model on the four languages.
1374
ID Feature Description Values
81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV
85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions
86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive
87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective
88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative, before and after
Table 1: The set of typological features that we use for source language selection. The first column gives the ID of
the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable
values for each feature; besides these values each feature can also have a value of ?No dominant order?.
vised POS induction algorithm (Lee et al 2010).11
Our mapping algorithm then learns the connection
between these clusters and universal tags.
For initialization, we perform multiple random
restarts and select the one with the lowest final ob-
jective score.
7 Results
We first present the results of our model using the
gold POS tags for the target language. Table 2 sum-
marizes the performance of our model and the base-
lines.
Comparison against Baselines On average, the
mapping produced by our model yields parsers with
higher accuracy than all of the baselines. These re-
sults are consistent for both parsers (McDonald et
al., 2011; Cohen et al 2011). As expected, random
mappings yield abysmal results ? 20.2% and 12.7%
for the two parsers. The low accuracy of parsers that
rely on the Greedy mapping ? 29.9% and 25.4% ?
show that a greedy approach is a poor strategy for
mapping optimization.
Surprisingly, our model slightly outperforms the
mapping of (Petrov et al 2011), yielding an aver-
age accuracy of 56.7% as compared to the 55.4%
achieved by its manually constructed counterpart for
the direct transfer method (McDonald et al 2011).
Similar results are observed for the mixture weights
parser (Cohen et al 2011). The main reason for
these differences comes from mistakes introduced in
the manual mapping. For example, in Czech tag ?R?
is labeled as ?pronoun?, while actually it should be
mapped to ?adposition?. By correcting this mistake,
we gain 5% in parsing accuracy for the direct trans-
fer parser.
11This pre-clustering results in about 3% improvement, pre-
sumably since it uses contextual information beyond what our
algorithm does.
Overall, the manually constructed mapping and
our model?s output disagree on 21% of the assign-
ments (measured on the token level). However,
the extent of disagreement is not necessarily predic-
tive of the difference in parsing performance. For
instance, the manual and automatic mappings for
Catalan disagree on 8% of the tags and their pars-
ing accuracy differs by 5%. For Greek on the other
hand, the disagreement between mappings is much
higher ? 17%, yet the parsing accuracy is very
close. This phenomenon shows that not all mistakes
have equal weight. For instance, a confusion be-
tween ?pronoun? and ?noun? is less severe in the
parsing context than a confusion between ?pronoun?
and ?adverb?.
Impact of Language Selection To assess the
quality of our language selection method, we com-
pare the model against an oracle that selects the best
source for a given target language. As Table 2 shows
our method is very close to the oracle performance,
with only 0.7% gap between the two. In fact, for
10 languages our method correctly predicts the best
pairing. This result is encouraging in other contexts
as well. Specifically, McDonald et al(2011) have
demonstrated that projecting from a single oracle-
chosen language can lead to good parsing perfor-
mance, and our technique may allow such projection
without an oracle.
Relations between Objective Values and Opti-
mization Performance The suboptimal perfor-
mance of the Greedy method shows that choosing
a good optimization strategy plays a critical role in
finding the desired mapping. A natural question to
ask is whether the objective value is predictive of the
end goal parsing performance. Figure 3 shows the
objective values for the mappings computed by our
method and the baselines for four languages. Over-
1375
Direct Transfer Parser (Accuracy) Mixture Weight Parser (Accuracy)
Tag Diff.
Random Greedy Petrov Model Best Pair Random Greedy Petrov Model.
Catalan 15.9 32.5 74.8 79.3 79.3 12.6 24.6 65.6 73.9 8.8
Italian 16.4 41.0 68.7 68.3 71.4 11.7 33.5 64.2 61.9 6.7
Portuguese 15.8 24.6 72.0 75.1 75.1 10.7 14.1 70.4 72.6 12.2
Spanish 11.5 27.4 72.1 68.9 68.9 6.4 26.5 58.8 62.8 7.5
Danish 35.5 23.7 46.6 46.5 49.2 4.2 23.7 51.4 51.7 5.0
Dutch 18.0 22.1 58.2 56.8 57.3 7.1 15.3 54.9 53.2 4.9
English 14.7 19.0 51.6 49.0 49.0 13.3 15.1 47.5 41.8 17.7
German 15.8 24.3 55.7 50.4 51.6 20.9 18.7 52.4 51.8 15.0
Swedish 15.1 26.3 63.1 63.1 63.1 9.1 36.5 55.7 55.9 8.2
Bulgarian 17.4 28.0 51.6 63.4 63.4 22.6 39.9 64.6 60.4 35.7
Czech 19.0 34.4 47.7 57.3 57.3 12.7 26.2 48.3 55.7 28.5
Slovene 15.6 21.8 43.5 51.4 52.8 11.3 20.7 42.2 53.0 38.8
Greek 17.3 19.5 62.3 59.7 59.8 22.0 15.2 56.2 57.0 17.0
Hungarian 28.4 44.1 53.8 52.3 52.3 4.0 43.8 46.4 51.7 18.1
Arabic 22.1 45.4 51.5 51.2 52.9 3.9 40.9 48.3 51.1 15.7
Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8
Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1
Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8
Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9
Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3
Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). The second section
is for the weighted mixture parsing model (Cohen et al 2011). The first two columns (Random and Greedy) of each
section present the parsing performance with a random or a greedy mapping. The third column (Petrov) shows the
results when the mapping of Petrov et al(2011) is used. The fourth column (Model) shows the results when our
mapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the best
source language is selected for every target language. The last column (Tag Diff.) presents the difference between our
mapping and the mapping of Petrov et al(2011) by showing the percentage of target language tokens for which the
two mappings select a different universal tag.
all, our method and the manual mappings reach sim-
ilar values, both considerably better than other base-
lines. While the parsing performance correlates with
the objective, the correlation is not perfect. For in-
stance, on Greek our mapping has a better objective
value, but lower parsing performance.
Ablation Analysis We next analyze the contribu-
tion of each component of our objective to the result-
ing performance.12 The strongest factor in our ob-
jective is the distributional features capturing global
statistics. Using these features alone achieves an
average accuracy of 51.1%, only 5.6% less than
the full model score. Adding just the verb-related
constraints to the distributional similarity objectives
improves the average model performance by 2.1%.
12The results are consistent for both parsers, here we report
the accuracy for the direct transfer method (McDonald et al
2011).
Adding just the typological constraints yields a very
modest performance gain of 0.5%. This is not sur-
prising ? the source language is selected to be typo-
logically similar to the target language, and thus its
distributional properties are consistent with typolog-
ical features. However, adding both the verb-related
constraints and the typological constraints results in
a synergistic performance gain of 5.6% over the dis-
tributional similarity objective, a gain which is much
better than the sum of the two individual gains.
Application to Automatically Induced POS Tags
A potential benefit of the proposed method is to re-
late automatically induced clusters in the target lan-
guage to universal tags. In our experiments, we in-
duce such clusters using Brown clustering,13 which
13In our experiments, we employ Liang?s implementation
http://cs.stanford.edu/?pliang/software/. The number of clus-
ters is set to 30.
1376
Catalan German Greek Arabic0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
 
 ModelPetrovGreedyRandom
Figure 3: Objective values for the different mappings
used in our experiments for four languages. Note that
the goal of the optimization procedure is to minimize the
objective value.
has been successfully used for similar purposes in
parsing research (Koo et al 2008). We then map
these clusters to the universal tags using our algo-
rithm.
The average parsing accuracy on the 19 languages
is 45.5%. Not surprisingly, automatically induced
tags negatively impact parsing performance, yield-
ing a decrease of 11% when compared to mappings
obtained using manual POS annotations (see Ta-
ble 2). To further investigate the impact of inaccu-
rate tags on the mapping performance, we compare
our model against the oracle mapping model that
maps each cluster to the most common universal tag
of its members. Parsing accuracy obtained using this
method is 45.1%, closely matching the performance
of our mapping algorithm.
An alternative approach to mapping words into
universal tags is to directly partition words into K
clusters (without passing through language specific
tags). In order for these clusters to be meaningful
as universal tags, we can provide several prototypes
for each cluster (e.g., ?walk? is a verb etc.). To test
this approach we used the prototype driven tagger of
Haghighi and Klein (2006) with 15 prototypes per
universal tag.14 The resulting universal tags yield
an average parsing accuracy of 40.5%. Our method
(using Brown clustering as above) outperforms this
14Oracle prototypes were obtained by taking the 15 most
frequent words for each universal tag. This yields almost the
same total number of prototypes as those in the experiment of
(Haghighi and Klein, 2006).
baseline by about 5%.
8 Conclusions
We present an automatic method for mapping
language-specific part-of-speech tags to a set of uni-
versal tags. Our work capitalizes on manually de-
signed conversion schemes to automatically create
mappings for new languages. Our experimental re-
sults demonstrate that automatically induced map-
pings rival the quality of their hand-crafted coun-
terparts. We also establish that the mapping quality
has a significant impact on the accuracy of syntactic
transfer, which motivates further study of this topic.
Finally, our experiments show that the choice of
mapping optimization scheme plays a crucial role in
the quality of the derived mapping, highlighting the
importance of optimization for the mapping task.
Acknowledgments
The authors acknowledge the support of the NSF
(IIS-0835445), the MURI program (W911NF-10-1-
0533) and the DARPA BOLT program. We thank
Tommi Jaakkola, the members of the MIT NLP
group and the ACL reviewers for their suggestions
and comments. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper
are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP,
pages 50?61.
Frank Van Eynde. 2004. Part of speech tagging en lem-
matisering van het corpus gesproken nederlands. In
Technical report.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272?279.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. JMLR, 11:2001?2049.
1377
Michael Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman & Co.
Michael C. Grant and Stephen P. Boyd. 2008. Graph im-
plementations for nonsmooth convex programs. In Re-
cent Advances in Learning and Control, Lecture Notes
in Control and Information Sciences, pages 95?110.
Springer-Verlag Limited.
Michael C. Grant and Stephen P. Boyd. 2011. CVX:
Matlab software for disciplined convex programming,
version 1.21.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
NAACL, pages 320?327.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Journal of Natural Language Engineering, 11:311?
325.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL, pages 595?603.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging. In
Proceedings of EMNLP, pages 853?861.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchi-
cal dirichlet processes. In Proceedings of EMNLP-
CoNLL, pages 688?697.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of ACL, pages 75?82.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of EMNLP, pages 523?530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP, pages 62?72.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP, pages 1234?1244.
Radford M. Neal and Geoffrey E. Hinton. 1999. A view
of the em algorithm that justifies incremental, sparse,
and other variants. In Michael I. Jordan, editor, Learn-
ing in Graphical Models, pages 355?368. MIT Press.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL-
COLING, pages 433?440.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851?858.
Alan Yuille and Anand Rangarajan. 2003. The concave-
convex procedure (cccp). In Proceedings of Neural
Computation, volume 15, pages 915?936.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 35?42.
1378
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1434?1444, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improved Parsing and POS Tagging Using Inter-Sentence
Consistency Constraints
Alexander M. Rush1? Roi Reichart1? Michael Collins2 Amir Globerson3
1MIT CSAIL, Cambridge, MA, 02139, USA
{srush|roiri}@csail.mit.edu
2Department of Computer Science, Columbia University, New-York, NY 10027, USA
mcollins@cs.columbia.edu
3School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel
gamir@cs.huji.ac.il
Abstract
State-of-the-art statistical parsers and POS
taggers perform very well when trained with
large amounts of in-domain data. When train-
ing data is out-of-domain or limited, accuracy
degrades. In this paper, we aim to compen-
sate for the lack of available training data by
exploiting similarities between test set sen-
tences. We show how to augment sentence-
level models for parsing and POS tagging with
inter-sentence consistency constraints. To deal
with the resulting global objective, we present
an efficient and exact dual decomposition de-
coding algorithm. In experiments, we add
consistency constraints to the MST parser
and the Stanford part-of-speech tagger and
demonstrate significant error reduction in the
domain adaptation and the lightly supervised
settings across five languages.
1 Introduction
State-of-the-art statistical parsers and POS taggers
perform very well when trained with large amounts
of data from their test domain. When training data is
out-of-domain or limited, the performance of the re-
sulting model often degrades. In this paper, we aim
to compensate for the lack of available training data
by exploiting similarities between test set sentences.
Most parsing and tagging models are defined at the
sentence-level, which makes such inter-sentence in-
formation sharing difficult. We show how to aug-
ment sentence-level models with inter-sentence con-
straints to encourage consistent descisions in similar
? Both authors contributed equally to this work.
contexts, and we give an efficient algorithm with for-
mal guarantees for decoding such models.
In POS tagging, most taggers perform very well
on word types that they have observed in training
data, but they perform poorly on unknown words.
With a global objective, we can include constraints
that encourage a consistent tag across all occur-
rences of an unknown word type to improve accu-
racy. In dependency parsing, the parser can benefit
from surface-level features of the sentence, but with
sparse or out-of-domain training data these features
are very noisy. Using a global objective, we can add
constraints that encourage similar surface-level con-
texts to exhibit similar syntactic behaviour.
The first contribution of this work is the use of
Markov random fields (MRFs) to model global con-
straints between sentences in dependency parsing
and POS tagging. We represent each word as a node,
the tagging or parse decision as its label, and add
constraints through edges. MRFs allow us to include
global constraints tailored to these problems, and to
reason about inference in the corresponding global
models.
The second contribution is an efficient dual de-
composition algorithm for decoding a global ob-
jective with inter-sentence constraints. These con-
straints generally make direct inference challenging
since they tie together the entire test corpus. To alle-
viate this issue, our algorithm splits the global infer-
ence problem into subproblems - decoding of indi-
vidual sentences, and decoding of the global MRF.
These subproblems can be solved efficiently through
known methods. We show empirically that by iter-
atively solving these subproblems, we can find the
1434
exact solution to the global model.
We experiment with domain adaptation and
lightly supervised training. We demonstrate that
global models with consistency constraints can im-
prove upon sentence-level models for dependency
parsing and part-of-speech tagging. For domain
adaptation, we show an error reduction of up to 7.7%
when adapting the second-order projective MST
parser (McDonald et al 2005) from newswire to
the QuestionBank domain. For lightly supervised
learning, we show an error reduction of up to 12.8%
over the same parser for five languages and an error
reduction of up to 10.3% over the Stanford trigram
tagger (Toutanova et al 2003) for English POS tag-
ging. The algorithm requires, on average, only 1.7
times the costs of sentence-level inference and finds
the exact solution on the vast majority of sentences.
2 Related Work
Methods that combine inter-sentence information
with sentence-level algorithms have been applied to
a number of NLP tasks. The most similar models to
our work are skip-chain CRFs (Sutton and Mccal-
lum, 2004), relational markov networks (Taskar et
al., 2002), and collective inference with symmetric
clique potentials (Gupta et al 2010). These mod-
els use a linear-chain CRF or MRF objective mod-
ified by potentials defined over pairs of nodes or
clique templates. The latter model makes use of La-
grangian relaxation. Skip-chain CRFs and collective
inference have been applied to problems in IE, and
RMNs to named entity recognition (NER) (Bunescu
and Mooney, 2004). Finkel et al(2005) also inte-
grated non-local information into entity annotation
algorithms using Gibbs sampling.
Our model can be applied to a variety of off-the-
shelf structured prediction models. In particular, we
focus on dependency parsing which is characterized
by a more complicated structure compared to the IE
tasks addressed by previous work.
Another line of work that integrates corpus-level
declarative information into sentence-level models
includes the posterior regularization (Ganchev et al
2010; Gillenwater et al 2010), generalized expec-
tation (Mann and McCallum, 2007; Mann and Mc-
Callum, ), and Bayesian measurements (Liang et al
2009) frameworks. The power of these methods has
been demonstrated for a variety of NLP tasks, such
as unsupervised and semi-supervised POS tagging
and parsing. The constraints used by these works
differ from ours in that they encourage the posterior
label distribution to have desired properties such as
sparsity (e.g. a given word can take a small number
of labels with a high probability). In addition, these
methods use global information during training as
opposed to our approach which applies test-time in-
ference global constraints.
The application of dual decomposition for infer-
ence in MRFs has been explored by Wainwright et
al. (2005), Komodakis et al(2007), and Globerson
and Jaakkola (2007). In NLP, Rush et al(2010)
and Koo et al(2010) applied dual decomposition to
enforce agreement between different sentence-level
algorithms for parsing and POS tagging. Work on
dual decomposition for NLP is related to the work
of Smith and Eisner (2008) who apply belief prop-
agation to inference in dependency parsing, and to
constrained conditional models (CCM) (Roth and
Yih, 2005) that impose inference-time constraints
through an ILP formulation.
Several works have addressed semi-supervised
learning for structured prediction, suggesting objec-
tives based on the max-margin principles (Altun and
Mcallester, 2005), manifold regularization (Belkin
et al 2005), a structured version of co-training
(Brefeld and Scheffer, 2006) and an entropy-based
regularizer for CRFs (Wang et al 2009). The com-
plete literature on domain adaptation is beyond the
scope of this paper, but we refer the reader to Blitzer
and Daume (2010) for a recent survey.
Specifically for parsing and POS tagging, self-
training (Reichart and Rappoport, 2007), co-training
(Steedman et al 2003) and active learning (Hwa,
2004) have been shown useful in the lightly su-
pervised setup. For parser adaptation, self-training
(McClosky et al 2006; McClosky and Charniak,
2008), using weakly annotated data from the tar-
get domain (Lease and Charniak, 2005; Rimell and
Clark, 2008), ensemble learning (McClosky et al
2010), hierarchical bayesian models (Finkel and
Manning, 2009) and co-training (Sagae and Tsujii,
2007) achieve substantial performance gains. For a
recent survey see Plank (2011). Constraints simi-
lar to those we use for POS tagging were used by
Subramanya et al(2010) for POS tagger adaptation.
1435
Their work, however, does not show how to decode
a global, corpus-level, objective that enforces these
constraints, which is a major contribution of this pa-
per.
Inter-sentence syntactic consistency has been ex-
plored in the psycholinguistics and NLP literature.
Phenomena such as parallelism and syntactic prim-
ing ? the tendency to repeat recently used syntactic
structures ? have been demonstrated in human lan-
guage corpora (e.g. WSJ and Brown) (Dubey et al
2009) and were shown useful in generative and dis-
criminative parsers (e.g. (Cheung and Penn, 2010)).
We complement these works, which focus on con-
sistency between consecutive sentences, and explore
corpus level consistency.
3 Structured Models
We begin by introducing notation for sentence-
level dependency parsing as a structured prediction
problem. The goal of dependency parsing is to
find the best parse y for a tagged sentence x =
(w1/t1, . . . , wn/tn) with words w and POS tags t.
Define the index set for dependency parsing as
I(x) = {(m,h) : m ? {1 . . . n},
h ? {0 . . . n},m 6= h}
where h = 0 represents the root word. A depen-
dency parse is a vector y = {y(m,h) : (m,h) ?
I(x)} where y(m,h) = 1 if m is a modifier of the
head word h. We define the set Y(x) ? {0, 1}|I(x)|
to be the set of all valid dependency parses for a sen-
tence x. In this work, we use projective dependency
parses, but the method also applies to the set of non-
projective parse trees.
Additionally, we have a scoring function f :
Y(x)? R. The optimal parse y? for a sentence x is
given by, y? = argmaxy?Y(x) f(y). This sentence-
level decoding problem can often be solved effi-
ciently. For example in commonly used projec-
tive dependency parsing models (McDonald et al
2005), we can compute y? efficiently using variants
of the Viterbi algorithm.
For this work, we make the assumption that we
have an efficient algorithm to find the argmax of
f(y) +
?
(m,h)?I(x)
u(m,h)y(m,h) = f(y) + u ? y
where u is a vector in R|I(x)|. In practice, u will be
a vector of Lagrange multipliers associated with the
dependencies of y in our dual decomposition algo-
rithm given in Section 6.
We can construct a very similar setting for POS
tagging where the goal is to find the best tagging
y for a sentence x = (w1, . . . , wn). We skip the
formal details here.
We next introduce notation for Markov random
fields (MRFs) (Koller and Friedman, 2009). An
MRF consists of an undirected graph G = (V,E),
a set of possible labels for each node Li for i ?
{1, . . . , |V |}, and a scoring function g. The index
set for MRFs is
IMRF = {(i, l) : i ? {1 . . . |V |}, l ? Li}
? {((i, j), li, lj) : (i, j) ? E, li ? Li, lj ? Lj}
A label assignment in the MRF is a binary vector
z with z(i, l) = 1 if the label l is selected at node i
and z((i, j), li, lj) = 1 if the labels li, lj are selected
for the nodes i, j.
In applications such as parsing and POS tagging,
some of the label assignments are not allowed. For
example, in dependency parsing the resulting struc-
ture must be a tree. Consequently, if every node
in the MRF corresponds to a word in a document
and its label corresponds to the index of its head
word, the resulting dependency structure for each
sentence must be acyclic. The set of all valid la-
bel assignments (one label per node) is given by
Z ? {0, 1}|IMRF|.
We score label assignments in the MRF with a
scoring function g : Z ? R. The best assignment
z? in an MRF is given by, z? = argmaxz?Z g(z).
We focus on pairwise MRFs where this function g is
a linear function of z whose parameters are denoted
by ?
g(z) = z ? ? =
?
(i,l)?IMRF
z(i, l)?(i, l) +
?
((i,j),li,lj)?IMRF
z((i, j), li, lj)?((i, j), li, lj)
As in parsing, we make the assumption that we
have an efficient algorithm to find the argmax of
g(z) +
?
(i,l)?IMRF(x)
u(i, l)z(i, l)
1436
He/PRP saw/VBD an/DT American/JJ man/NN
The/DT smart/JJ girls/NNS stood/VBD outside/RB
Danny/DT walks/VBZ a/DT long/JJ distance/NN
NN
Figure 1: An example constraint from dependency pars-
ing. The black nodes are modifiers observed in the train-
ing data. Each gray node corresponds to a possible mod-
ifier in the test corpus. The constraint applies to all mod-
ifiers in the context DT JJ. The white node corresponds
to the consensus POS tag of the head word of these mod-
ifiers.
4 A Parsing Example
In this section we give a detailed example of global
constraints for dependency parsing. The aim is to
construct a global objective that encourages similar
contexts across the corpus to exhibit similar syntac-
tic behaviour. We implement this objective using an
MRF with a node for each word in the test set. The
label of each node is the index of the word it mod-
ifies. We add edges to this MRF to reward consis-
tency among similar contexts. Furthermore, we add
nodes with a fixed label to incorporate contexts seen
in the training data.
Specifically, we say that the context of a word is
its POS tag and the POS tags of some set of the
words around it. We expand on this notion of con-
text in Section 8; for simplicity we assume here that
the context includes only the previous word?s POS
tag. Our constraints are designed to bias words in
the same context to modify words with similar POS
tags.
Figure 1 shows a global MRF over a small parsing
example with one training sentence and two test sen-
tences. The MRF contains a node associated with
each word instance, where the label of the node is
the index of the word it modifies. In this corpus, the
context DT JJ appears once in training and twice in
testing. We hope to choose head words with similar
POS tags for these two test contexts biased by the
observed training context.
More concretely, for each context c ?
{1, . . . , C}, we have a set Sc of associated
word indices (s,m) that appear in the context,
where s is a sentence index and m is a position
in that sentence. For instance, in our example
S1 = {(1, 2), (2, 4)} consists of all positions in
the test set where we see JJ preceded by DT.
Futhermore, we have a set Oc of indices (s,m,TR)
of observed instances of the context in the training
data where TR denotes a training index. In our
example O1 = {(1, 4,TR)} consists of the one
training instance. We associate each word instance
with a single context c.
We then define our MRF to include one consensus
node for each set Sc as well as a word node for each
instance in the set Sc ?Oc. Thus the set of variables
corresponds to V = {1, . . . , C} ? (
?C
c=1 Sc ? Oc).
Additionally, we include an edge from each node
i ? Sc?Oc to its consensus node c,E = {(i, c) : c ?
{1, . . . , C}, i ? Sc ?Oc}. The word nodes from Sc
have the label set of possible head indices L(s,m) =
{0, . . . , ns} where ns is the length of the sentence s.
The observed nodes from Oc have a singleton label
set L(s,m,TR) with the observed index. The consen-
sus nodes have the label set Lc = T ? {NULL}
where T is the set of POS tags and the NULL sym-
bol represents the constraint being turned off.
We can now define the scoring function g for this
MRF. The scoring function aims to reward consis-
tency among the head POS tag at each word and the
consensus node
?((i, c), li, lc) =
?
???
???
?1 if pos(li) = lc
?2 if pos(li) is close to lc
?3 lc = NULL
0 otherwise
where posmaps a word index to its POS tag. The pa-
rameters ?1 ? ?2 ? ?3 ? 0 determine the bonus for
identical POS tags, similar POS tags, and for turning
off the constraint .
We construct a similar model for POS tagging.
We choose sets Tc corresponding to the c?th un-
known word type in the corpus. The MRF graph
is identical to the parsing case with Tc replacing Sc
and we no longer have Oc. The label sets for the
word nodes are now L(s,m) = T where the label is
1437
the POS tag chosen at that word, and the label set for
the consensus node is Lc = T ? {NULL}. We use
the same scoring function as in parsing to enforce
consistency between word nodes and the consensus
node.
5 Global Objective
Recall the definition of sentence-level parsing,
where the optimal parse y? for a single sentence
x under a scoring function f is given by: y? =
argmaxy?Y(x) f(y). We apply this objective to
a set of sentences, specified by the tuple X =
(x1, ..., xr), and the product of possible parses
Y(X) = Y(x1) ? . . . ? Y(xr). The sentence-level
decoding problem is to find the optimal dependency
parses Y ? = (Y ?1 , ..., Y ?r ) ? Y(X) under a global
objective
Y ? = argmax
Y ?Y(X)
F (Y ) = argmax
Y ?Y(X)
r?
s=1
f(Ys)
where F : Y(X) ? R is the global scoring func-
tion.
We now consider scoring functions where the
global objective includes inter-sentence constraints.
Objectives of this form will not factor directly
into individual parsing problems; however, we can
choose to write them as the sum of two convenient
terms: (1) A simple sum of sentence-level objec-
tives; and (2) A global MRF that connects the local
structures.
For convenience, we define the following index
set.
J (X) = {(s,m, h) : s ? {1, . . . , r},
(m,h) ? I(xs)}
This set enumerates all possible dependencies at
each sentence in the corpus. We say the parses Ys
are consistent with a label assignment z if for all
(s,m, h) ? J (X) we have that z((s,m), h) =
Ys(m,h). In other words, the labels in z match the
head words chosen in parse Ys.
With this notation we can write the full global de-
coding objective as
(Y ?, z?) = argmax
Y ?Y(X), z?Z
F (Y ) + g(z) (1)
s.t. ?(s,m, h) ? J (X), z((s,m), h) = Ys(m,h)
Set u(1)(s,m, h)? 0 for all (s,m, h) ? J (X)
for k = 1 to K do
z(k) ? argmax
z?Z
(g(z) +
?
(s,m,h)?J (X)
u(k)(s,m, h)z((s,m), h))
Y (k) ? argmax
Y ?Y(X)
(F (Y ) ?
?
(s,m,h)?J (X)
u(k)(s,m, h)Ys(m,h))
if Y (k)s (m,h) = z(k)((s,m), h)
for all (s,m, h) ? J (X) then
return (Y (k), z(k))
for all (s,m, h) ? J (X),
u(k+1)(s,m, h)? u(k)(s,m, h) +
?k(z(k)((s,m), h)? Y (k)s (m,h))
return (Y (K), z(K))
Figure 2: The global decoding algorithm for dependency
parsing models.
The solution to this objective maximizes the local
models as well as the global MRF, while maintain-
ing consistency among the models. Specifically, the
MRF we use in the experiments has a simple naive
Bayes structure with the consensus node connected
to all relevant word nodes.
The global objective for POS tagging has a similar
form. As before we add a node to the MRF for each
word in the corpus. We use the POS tag set as our
labels for each of these nodes. The index set con-
tains an element for each possible tag at each word
instance in the corpus.
6 A Global Decoding Algorithm
We now consider the decoding question: how to
find the structure Y ? that maximizes the global ob-
jective. We aim for an efficient solution that makes
use of the individual solvers at the sentence-level.
For this work, we make the assumption that the
graph chosen for the MRF has small tree-width, e.g.
our naive Bayes constraints, and can be solved effi-
ciently using dynamic programming.
Before we describe our dual decomposition al-
gorithm, we consider the difficulty of solving the
global objective directly. We have an efficient dy-
namic programming algorithm for solving depen-
dency parsing at the sentence-level, and efficient al-
gorithms for solving the MRF. It follows that we
1438
could construct an intersected dynamic program-
ming algorithm that maintains the product of states
over both models. This algorithm is exact, but it
is very inefficient. Solving the intersected dynamic
program requires decoding simultaneously over the
entire corpus, with an additional multiplicative fac-
tor for solving the MRF. On top of this cost, we need
to alter the internal structure of the sentence-level
models.
In contrast, we can construct a dual decomposi-
tion algorithm which is efficient, produces a certifi-
cate when it finds an exact solution, and directly
uses the sentence-level parsing models. Considering
again the global objective of equation 1, we note that
the difficulty in decoding this objective comes en-
tirely from the constraints z((s,m), h) = Ys(m,h).
If these were not there, the problem would factor
into two parts, an optimization of F over the test
corpus Y(X) and an optimization of g over possible
MRF assignments Z . The first problem factors nat-
urally into sentence-level parsing problems and the
second can be solved efficiently given our assump-
tions on the MRF topology G.
Recent work has shown that a relaxation based
on dual decomposition often produces an exact so-
lution for such problems (Koo et al 2010). To
apply dual decomposition, we introduce Lagrange
multipliers u(s,m, h) for the agreement constraints
between the sentence-level models and the global
MRF. The Lagrangian dual is the function L(u) =
maxz g(z, u) + maxy F (y, u) where
g(z, u) = g(z) +
?
(s,m,h)?J (X)
u(s,m, h)z((s,m), h)),
F (y, u) = F (Y ) ?
?
(s,m,h)?J (X)
u(s,m, h)Ys(m,h)
In order to find minu L(u), we use subgradient de-
scent. This requires computing g(z, u) and F (y, u)
for fixed values of u, which by our assumptions from
Section 3 are efficient to calculate.
The full algorithm is given in Figure 2. We start
with the values of u initialized to 0. At each itera-
tion k, we find the best set of parses Y (k) over the
entire corpus and the best MRF assignment z(k). We
then update the value of u based on the difference
between Y (k) and z(k) and a rate parameter ?. On
the next iteration, we solve the same decoding prob-
? 0.7 ?0.8 ? 0.9 1.0
All Contexts 66.8 57.9 46.8 33.3
Head in Context 76.0 67.9 57.2 42.3
Table 1: Exploratory statistics for constraint selection.
The table shows the percentage of context types for which
the probability of the most frequent head tag is at least p.
Head in Context refers to the subset of contexts where the
most frequent head is within the context itself. Numbers
are based on Section 22 of the Wall Street Journal and are
given for contexts that appear at least 10 times.
lems modified by the new value of u. If at any point
the current solutions Y (k) and z(k) satisfy the con-
sistency constraint, we return their current values.
Otherwise, we stop at a max iteration K and return
the values from the last iteration.
We now give a theorem for the formal guarantees
of this algorithm.
Theorem 1 If for some k ? {1 . . .K} in the algo-
rithm in Figure 2, Y (k)s (m,h) = z(k)(s,m, h) for
all (s,m, h) ? J , then (Y (k), z(k)) is a solution to
the maximization problem in equation 1.
We omit the proof for brevity. It is a slight variation
of the proof given by Rush et al(2010).
7 Consistency Constraints
In this section we describe the consistency con-
straints used for the global models of parsing and
tagging.
Parsing Constraints. Recall from Section 4 that
we choose parsing constraints based on the word
context. We encourage words in similar contexts to
choose head words with similar POS tags.
We use a simple procedure to select which con-
straints to add. First define a context template to
be a set of offsets {r, . . . , s} with r ? 0 ? s that
specify the neighboring words to include in a con-
text. In the example of Figure 1, the context tem-
plate {?1, 0, 1, 2} applied to the word girls/NNS
would produce the context JJ NNS VBD RB. For
each word in the corpus, we consider all possible
templates with s? r < 4. We use only contexts that
predict the head POS of the context in the training
data with probability 1 and prefer long over short
contexts. Once we select the context of each word,
we add a consensus node for each context type in
1439
the corpus. We connect each word node to its corre-
sponding consensus node.
Local context does not fully determine the POS
tag of the head word, but for certain contexts it pro-
vides a strong signal. Table 1 shows context statis-
tics for English. For 46.8% of the contexts, the most
frequent head tag is chosen ? 90% of the time. The
pattern is even stronger for contexts where the most
frequent head tag is within the context itself. In
this case, for 57.2% of the contexts the most fre-
quent head tag is chosen ? 90% of the time. Con-
sequently, if more than one context can be selected
for a word, we favor the contexts where the most
frequent head POS is inside the context.
POS Tagging Constraints. For POS tagging, our
constraints focus on words not observed in the train-
ing data. It is well-known that each word type ap-
pears only with a small number of POS tags. In Sec-
tion 22 of the WSJ corpus, 96.35% of word types
appear with a single POS tag.
In most test sets we are unlikely to see an un-
known word more than once or twice. To fix this
sparsity issue, we import additional unannotated
sentences for each unknown word from the New
York Times Section of the NANC corpus (Graff,
1995). These sentences give additional information
for unknown word types.
Additionally, we note that morphologically re-
lated words often have similar POS tags. We can
exploit this relationship by connecting related word
types to the same consensus node. We experimented
with various morphological variants and found that
connecting a word type with the type generated by
appending the suffix ?s? was most beneficial. For
each unknown word type, we also import sentences
for its morphologically related words.
8 Experiments and Results
We experiment in two common scenarios where
parsing performance is reduced from the fully su-
pervised, in-domain case. In domain adaptation, we
train our model completely in one source domain
and test it on a different target domain. In lightly su-
pervised training, we simulate the case where only
a limited amount of annotated data is available for a
language.
Base ST Model ER
WSJ? QTB 89.63 89.99 90.43 7.7
QTB?WSJ 74.89 74.97 75.76 3.5
Table 2: Dependency parsing UAS for domain adapta-
tion. WSJ is the Penn TreeBank. QTB is the Question-
Bank. ER is error reduction. Results are significant using
the sign test with p ? 0.05.
Data for Domain Adaptation We perform do-
main adaptation experiments in English using the
WSJ PennTreebank (Marcus et al 1993) and the
QuestionBank (QTB) (Judge et al 2006). In the
WSJ ? QTB scenario, we train on sections 2-21
of the WSJ and test on the entire QTB (4000 ques-
tions). In the QTB?WSJ scenario, we train on the
entire QTB and test on section 23 of the WSJ.
Data for Lightly Supervised Training For all
English experiments, our data was taken from the
WSJ PennTreebank: training sentences from Sec-
tion 0, development sentences from Section 22, and
test sentences from Section 23. For experiments
in Bulgarian, German, Japanese, and Spanish, we
use the CONLL-X data set (Buchholz and Marsi,
2006) with training data taken from the official train-
ing files. We trained the sentence-level models with
50-500 sentences. To verify the robustness of our
results, our test sets consist of the official test sets
augmented with additional sentences from the offi-
cial training files such that each test file consists of
25,000 words. Our results on the official test sets are
very similar to the results we report and are omitted
for brevity.
Parameters The model parameters, ?1, ?2, and ?3
of the scoring function (Section 4) and ? of the
Lagrange multipliers update rule (Section 6), were
tuned on the English development data. In our dual
decomposition inference algorithm, we use K =
200 maximum iterations and tune the decay rate fol-
lowing the protocol described by Koo et al(2010).
Sentence-Level Models For dependency parsing
we utilize the second-order projective MST parser
(McDonald et al 2005)1 with the gold-standard
POS tags of the corpus. For POS tagging we use
the Stanford POS tagger (Toutanova et al 2003)2.
1http://sourceforge.net/projects/mstparser/
2http://nlp.stanford.edu/software/tagger.shtml
1440
50 100 200 500
Base ST Model (ER) Base ST Model (ER) Base ST Model (ER) Base ST Model (ER)
Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33)
Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93)
Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35)
Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 (1.27)
Ger 68.21 68.28 68.83 (1.95) 72.19 72.29 72.76 (2.05) 74.34 74.45 74.95 (2.4) 77.20 77.09 77.51 (1.4)
Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,
German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency
parser of McDonald et al(2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the
same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p ? 0.05, all
50, 100, and 200 results are significant, as are Eng and Ger 500.
50 100 200 500
Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER)
Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)
Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)
Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et
al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All
results are significant using the sign test with p ? 0.05.
Evaluation and Baselines To measure parsing
performance, we use unlabeled attachment score
(UAS) given by the CONLL-X dependency parsing
shared task evaluation script (Buchholz and Marsi,
2006). We compare the accuracy of dependency
parsing with global constraints to the sentence-level
dependency parser of McDonald et al(2005) and to
a self-training baseline (Steedman et al 2003; Re-
ichart and Rappoport, 2007). The parsing baseline is
equivalent to a single round of dual decomposition.
For the self-training baseline, we parse the test cor-
pus, append the labeled test sentences to the training
corpus, train a new parser, and then re-parse the test
set. We run this procedure for a single iteration.
For POS tagging we measure token level POS ac-
curacy for all the words in the corpus and also for
unknown words (words not observed in the train-
ing data). We compare the accuracy of POS tagging
with global constraints to the accuracy of the Stan-
ford POS tagger 3.
Domain Adaptation Accuracy Results are pre-
sented in Table 2. The constrained model reduces
the error of the baseline on both cases. Note that
when the base parser is trained on the WSJ corpus its
UAS performance on the QTB is 89.63%. Yet, the
constrained model is still able to reduce the baseline
error by 7.7%.
3We do not run self-training for POS tagging as it has been
shown unuseful for this application (Clark et al 2003).
Lightly Supervised Accuracy The parsing results
are given in Table 3. Our model improves over
the baseline parser and self-training across all lan-
guages and training set sizes. The best results are
for Japanese and English with error reductions of
2.33 ? 12.82% and 1.93 ? 6.64% respectively. The
self-training baseline achieves small gains on some
languages, but generally performs similarly to the
standard parser.
The POS tagging results are given in Table 4. Our
model improves over the baseline tagger for the en-
tire training size range. For 50 training sentences
we reduce 10.33% of the overall error, and 11.53%
of the error on unknown words. Although the tagger
performance substantially improves when the train-
ing set grows to 500 sentences, our model still pro-
vides an overall error reduction of 4.64% and of
8.33% for unknown words.
9 Discussion
Efficiency Since dual decomposition often re-
quires hundreds of iterations to converge, a naive im-
plementation would be orders of magnitude slower
than the underlying sentence-level model. We use
two techniques to speed-up the algorithm.
First, we follow Koo et al(2010) and use lazy
decoding as part of dual decomposition. At each it-
eration k, we cache the result of the MRF z(k) and
set of parse tree Y (k). In the next iteration, we only
1441
50 100 150 200iteration0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
percen
tage of
 parsin
g time
englishgermanjapanese
Figure 3: Efficiency of dependency parsing decoding for
three languages. The plot shows the speed of each iter-
ation of the subgradient algorithm relative to a round of
unconstrained parsing.
Most Effective Contexts
WSJ? QTB QTB?WSJ
WRB VBP VBD NN NN ,
DT JJS NN IN IN PRP VBZ
VBP PRP VB JJ JJ NN ,
DT NN NN VB IN JJ JJ NN
RBS JJ NN IN NN POS NN NN
Table 5: The five most effective constraint contexts from
the domain adaptation experiments. The bold POS tag
indicates the modifier word of the context.
Where/
WRB
VBN
are/
VBP
diamonds/
NNS
mined/
VBN
?
How/
WRB
VBP
do/
VBP
you/
PRP
measure/
VB
earthquakes/
NNS
?
Why/
WRB
VBP
do/
VBP
people/
NNS
get/
VB
calluses/
NNS
?
VBP
Figure 4: Subset of sentences with the context WRB VBP
from WSJ? QTB domain adaptation. In the first round,
the parser chooses VBN for the first sentence, which is in-
consistent with similar contexts. The constraints correct
this choice in later rounds.
recompute the solution Y ?s for a sentence s if the
weight u(s,m, h) for some m,h was updated. A
similar technique is applied to the MRF.
Second, during the first iteration of the algorithm
we apply max-marginal based pruning using the
threshold defined by Weiss and Taskar (2010). This
produces a pruned hypergraph for each sentence,
which allows us to avoid recomputing parse features
and to solve a simplified search problem.
To measure efficiency, we compare the time spent
in dual decomposition to the speed of unconstrained
inference. Across experiments, the mean dual de-
composition time is 1.71 times the cost of uncon-
strained inference. Figure 3 shows how this time is
spent after the first iteration. The early iterations are
around 1% of the total cost, and because of lazy de-
coding this quickly drops to almost nothing.
Exactness To measure exactness, we count the
number of sentences for which we should remove
the constraints in order for the model to reach con-
vergence. For dependency parsing, across languages
removing constraints on 0.6% of sentences yields
exact convergence. Removing these constraints has
very little effect on the final outcome of the model.
For POS tagging, the algorithm finds an exact so-
lution after removing constraints from 0.2% of the
sentences.
Constraint Analysis We can also look at the num-
ber, size, and outcome of the constraints chosen in
the experiments. In the lightly supervised experi-
ments, the average number of constraints is 3298 for
25000 tokens, where the median constraint connects
19 different tokens. Of these constraints around 70%
are active (non-NULL). The domain adaptation ex-
periments have a similar number of constraints with
around 75% of constraints active. In both experi-
ments many of the constraints are found to be con-
sistent after the first iteration, but as Figure 3 im-
plies, other constraints take multiple iterations to
converge.
Qualitative Analysis In order to understand why
these simple consistency constraints are effective,
we take a qualitative look at the the domain adap-
tation experiments on the QuestionBank. Table 5
ranks the five most effective contextual constraints
from both experiments. For the WSJ? QTB exper-
iment, the most effective constraint relates the inital
question word with an adjacent verb. Figure 4 shows
1442
sentences where this constraint applies in the Ques-
tionBank. For the QTB?WSJ experiment, the ef-
fective contexts are mostly long base noun phrases.
These occur often in the WSJ but are rare in the sim-
pler QuestionBank sentences.
10 Conclusion
In this work we experiment with inter-sentence
consistency constraints for dependency parsing and
POS tagging. We have proposed a corpus-level ob-
jective that augments sentence-level models with
such constraints and described an exact and effi-
cient dual decomposition algorithm for its decod-
ing. In future work, we intend to explore efficient
techniques for joint parameter learning for both the
global MRF and the local models.
Acknowledgments Columbia University gratefully
acknowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or the
US government. Alexander Rush was supported by a
National Science Foundation Graduate Research Fellow-
ship.
References
Y. Altun and D. Mcallester. 2005. Maximum margin
semi-supervised learning for structured variables. In
NIPS.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In AISTATS.
John Blitzer and Hal Daume. 2010. Icml 2010 tutorial
on domain adaptation. In ICML.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
R.C. Bunescu and R.J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL.
J.C.K Cheung and G. Penn. 2010. Utilizing extra-
sentential context for parsing. In EMNLP.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping pos taggers using unlabelled data. In
CoNLL.
A. Dubey, F. Keller, and P. Sturt. 2009. A proba-
bilistic corpus-based model of parallelism. Cognition,
109(2):193?210.
Jenny Rose Finkel and Christopher Manning. 2009. Hi-
erarchical bayesian domain adaptation. In NAACL.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured Latent
Variable Models. Journal of Machine Learning Re-
search, 11:2001?2049.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar in-
duction. In Proceedings of the ACL Conference Short
Papers.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
map lp-relaxations. In NIPS.
D. Graff. 1995. North american news text corpus. Lin-
guistic Data Consortium, LDC95T21.
Rahul Gupta, Sunita Sarawagi, and Ajit A. Diwan. 2010.
Collective inference for extraction mrfs coupled with
symmetric clique potentials. JMLR.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In ACL-COLING.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In IJCNLP.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In ICML.
G.S. Mann and A. McCallum. Generalized expectation
criteria for semi-supervised learning with weakly la-
beled data. Journal of Machine Learning Research,
11:955?984.
G.S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In ICML.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
1443
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In ACL, sort papers.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adapatation for parsing. In
NAACL.
R.T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In HLT/EMNLP.
Barbara Plank. 2011. Domain Adaptation for Parsing.
Ph.d. thesis, University of Groningen.
R. Reichart and A. Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains. In
EMNLP.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
Kenji Sagae and Junichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In EMNLP-CoNLL.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In EMNLP.
C. Sutton and A. Mccallum. 2004. Collective segmen-
tation and labeling of distant entities in information
extraction. In In ICML Workshop on Statistical Re-
lational Learning and Its Connections.
B. Taskar, P. Abbeel, and d. Koller. 2002. Discriminative
probabilistic models for relational data. In UAI.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields. In NIPS.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS, volume 1284.
1444
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 278?289,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
An Unsupervised Model for Instance Level Subcategorization Acquisition
Simon Baker
Computer Laboratory
University of Cambridge
sb895@cam.ac.uk
Roi Reichart
Technion, IIT
Haifa, Israel
roiri@ie.technion.ac.il
Anna Korhonen
Computer Laboratory
University of Cambridge
alk23@cam.ac.uk
Abstract
Most existing systems for subcategoriza-
tion frame (SCF) acquisition rely on su-
pervised parsing and infer SCF distribu-
tions at type, rather than instance level.
These systems suffer from poor portability
across domains and their benefit for NLP
tasks that involve sentence-level process-
ing is limited. We propose a new unsuper-
vised, Markov Random Field-based model
for SCF acquisition which is designed
to address these problems. The system
relies on supervised POS tagging rather
than parsing, and is capable of learning
SCFs at instance level. We perform eval-
uation against gold standard data which
shows that our system outperforms several
supervised and type-level SCF baselines.
We also conduct task-based evaluation in
the context of verb similarity prediction,
demonstrating that a vector space model
based on our SCFs substantially outper-
forms a lexical model and a model based
on a supervised parser
1
.
1 Introduction
Subcategorization frame (SCF) acquisition in-
volves identifying the arguments of a predicate
and generalizing about its syntactic frames,
where each frame specifies the syntactic type and
number of arguments permitted by the predicate.
For example, in sentences (1)-(3) the verb distin-
guish takes three different frames, the difference
between which is not evident when considering
the phrase structure categorization:
(1) Direct Transitive: [They]NP [distin-
guished]VP [the mast]NP [of [ships on the
horizon ]NP ]PP .
1
The verb similarity dataset used for the evaluation of our
model is publicly available at ie.technion.ac.il/?roiri/.
(2) Indirect Transitive: [They]NP [distin-
guished]VP [between [me and you]ADVP ]PP .
(3) Ditransitive: [They]NP [distinguished]VP
[him]NP [from [the other boys]NP ]PP.
As SCFs describe the syntactic realization of
the verbal predicate-argument structure, they are
highly valuable for a variety of NLP tasks. For
example, verb subcategorization information has
proven useful for tasks such as parsing (Carroll
and Fang, 2004; Arun and Keller, 2005; Cholakov
and van Noord, 2010), semantic role labeling
(Bharati et al., 2005; Moschitti and Basili, 2005),
verb clustering, (Schulte im Walde, 2006; Sun
and Korhonen, 2011) and machine translation (hye
Han et al., 2000; Haji?c et al., 2002; Weller et al.,
2013).
SCF induction is challenging. The argument-
adjunct distinction is difficult even for humans,
and is further complicated by the fact that both ar-
guments and adjuncts can appear frequently in po-
tential argument head positions (Korhonen et al.,
2000). SCFs are also highly sensitive to domain
variation so that both the frames themselves and
their probabilities vary depending on the meaning
and behavior of predicates in the domain in ques-
tion (e.g. (Roland and Jurafsky, 1998; Lippincott
et al., 2010; Rimell et al., 2013), Section 4).
Because of the strong impact of domain vari-
ation, SCF information is best acquired automat-
ically. Existing data-driven SCF induction sys-
tems, however, do not port well between do-
mains. Most existing systems rely on hand-
written rules (Briscoe and Carroll, 1997; Korho-
nen, 2002; Preiss et al., 2007) or simple co-
occurrence statistics (O?Donovan et al., 2005;
Chesley and Salmon-Alt, 2006; Ienco et al., 2008;
Messiant et al., 2008; Lenci et al., 2008; Al-
tamirano and Alonso i Alemany, 2010; Kawa-
hara and Kurohashi, 2010) applied to the gram-
matical dependency output of supervised statisti-
cal parsers. Even the handful of recent systems
278
that use modern machine learning techniques (De-
bowski, 2009; Lippincott et al., 2012; Van de
Cruys et al., 2012; Reichart and Korhonen, 2013)
use supervised parsers to pre-process the data
2
.
Supervised parsers are notoriously sensitive to
domain variation (Lease and Charniak, 2005). As
annotation of data for each new domain is un-
realistic, current SCF systems suffer from poor
portability. This problem is compounded for
the many systems that employ manually devel-
oped SCF rules because rules are inherently ig-
norant to domain-specific preferences. The few
SCF studies that focused on specific domains (e.g.
biomedicine) have reported poor performance due
to these reasons (Rimell et al., 2013).
Another limitation of most current SCF systems
is that they produce a type-level SCF lexicon (i.e.
a lexicon which lists, for a given predicate, dif-
ferent SCF types with their relative frequencies).
Such a lexicon provides a useful high-level pro-
file of the syntactic behavior of the predicate in
question, but is less useful for downstream NLP
tasks (e.g. information extraction, parsing, ma-
chine translation) that involve sentence processing
and can therefore benefit from SCF information
at instance level. Sentences (1)-(3) demonstrate
this limitation - a prior distribution over the pos-
sible syntactic frames of distinguish provides only
a weak signal to a sentence level NLP application
that needs to infer the verbal argument structure of
its input sentences.
We propose a new unsupervised model for SCF
induction which addresses these problems with
existing systems. Our model does not use a parser
or hand-written rules, only a part-of-speech (POS)
tagger is utilizes in order to produce features for
machine learning. While POS taggers are also
sensitive to domain variation, they can be adapted
to domains more easily than parsers because they
require much smaller amounts of annotated data
(Lease and Charniak, 2005; Ringger et al., 2007).
However, as we demonstrate in our experiments,
domain adaptation of POS tagging may not even
be necessary to obtain good results on the SCF ac-
quisition task.
Our model, based on the Markov Random Field
(MRF) framework, performs instance-based SCF
learning. It encodes syntactic similarities among
verb instances across different verb types (derived
2
(Lippincott et al., 2012) does not use a parser, but the
syntactic frames induced by the system do not capture sets of
arguments for verbs, so are not SCFs in a traditional sense.
from a lexical and POS-based feature representa-
tion of verb instances) as well as prior beliefs on
the tendencies of specific instances of the same
verb type to take the same SCF.
We evaluate our model against corpora anno-
tated with verb instance SCFs (Quochi et al.,
2012). In addition, following the Levin verb
clustering tradition (Levin, 1993) which ties verb
meanings with their syntactic properties, we eval-
uate the semantic predictive power of our clusters.
In the former evaluation, our model outperforms a
number of strong baselines, including supervised
and type-level ones, achieving an accuracy of up
to 69.2%. In the latter evaluation a vector space
model that utilized our induced SCFs substantially
outperforms the output of a type-level SCF system
that uses the fully trained Stanford parser.
2 Previous Work
Several SCF acquisition systems are available for
English (O?Donovan et al., 2005; Preiss et al.,
2007; Lippincott et al., 2012; Van de Cruys et
al., 2012; Reichart and Korhonen, 2013) and other
languages, including French (Messiant, 2008),
Italian (Lenci et al., 2008), Turkish (Uzun et al.,
2008), Japanese (Kawahara and Kurohashi, 2010)
and Chinese (Han et al., 2008). The promi-
nent input to these systems are grammatical re-
lations (GRs) which express binary dependen-
cies between words (e.g. direct and indirect ob-
jects, various types of complements and conjunc-
tions). These are generated by some parsers (e.g.
(Briscoe et al., 2006)) and can be extracted from
the output of others (De-Marneffe et al., 2006).
Two representative systems for English are the
Cambridge system (Preiss et al., 2007) and the
BioLexicon system which was used to acquire a
substantial lexicon for biomedicine (Venturi et al.,
2009). These systems extract GRs at the verb in-
stance level from the output of a parser: the RASP
general-language unlexicalized parser
3
(Briscoe et
al., 2006) and the lexicalized Enju parser tuned to
the biomedical domain (Miyao and Tsujii, 2005),
respectively. They generate potential SCFs by
mapping GRs to a predefined SCF inventory us-
ing a set of manually developed rules (the Cam-
bridge system) or by simply considering the sets
of GRs including verbs in question as potential
SCFs (BioLexicon). Finally, a type level lexicon
3
A so-called unlexicalized parser is a parser trained with-
out explicit SCF annotations.
279
is built through noisy frame filtering (based on
frequencies or on external resources and annota-
tions), which aims to remove errors from parsing
and argument-adjunct distinction. Clearly, these
systems require extensive manual work: a-priori
definition of an SCF inventory and rules, manu-
ally annotated sentences for training a supervised
parser, SCF annotations for parser lexicalization,
and manually developed resources for optimal fil-
tering.
A number of recent works have applied mod-
ern machine learning techniques to SCF induc-
tion, including point-wise co-occurrence of ar-
guments (Debowski, 2009), a Bayesian network
model (Lippincott et al., 2012), multi-way tensor
factorization (Van de Cruys et al., 2012) and De-
terminantal Point Processes (DPPs) -based clus-
tering (Reichart and Korhonen, 2013). However,
all of these systems induce type-level SCF lexi-
cons and, except from the system of (Lippincott et
al., 2012) that is not capable of learning traditional
SCFs, they all rely on supervised parsers.
Our new system differs from previous ones in
a number of respects. First, in contrast to most
previous systems, our system provides SCF anal-
ysis for each verb instance in its sentential con-
text, yielding more precise SCF information for
systems benefiting from instance-based analysis.
Secondly, it addresses SCF induction as an unsu-
pervised clustering problem, avoiding the use of
supervised parsing or any of the sources of man-
ual supervision used in previous works. Our sys-
tem relies on POS tags - however, we show that it
is not necessary to train a tagger with in-domain
data to obtain good performance on this task, and
therefore our approach provides a more domain-
independent solution to SCF acquisition.
We employ POS-tagging instead of unsuper-
vised parsing for two main reasons. First, while
a major progress has been made on unsupervised
parsing (e.g. (Cohen and Smith, 2009; Berg-
Kirkpatrick et al., 2010)), the performance is still
considerably behind that of supervised parsing.
For example, the state-of-the-art discriminative
model of (Berg-Kirkpatrick et al., 2010) achieves
only 63% directed arc accuracy for WSJ sentences
of up to 10 words, compared to more than 95%
obtained with supervised parsers. Second, current
unsupervised parsers produce unlabeled structures
which are substantially less useful for SCF acqui-
sition than labeled structures produced by super-
vised parsers (e.g. grammatical relations).
Finally, a number of recent works addressed re-
lated tasks such as argument role clustering for
SRL (Lang and Lapata, 2011a; Lang and Lapata,
2011b; Titvo and Klementiev, 2012) in an unsu-
pervised manner. While these works differ from
ours in the task (clustering arguments rather than
verbs) and the level of supervision (applying a su-
pervised parser), like us they analyze the verb ar-
gument structure at the instance level.
3 Model
We address SCF induction as an unsupervised
verb instance clustering problem. Given a set of
plain sentences, our algorithm aims to cluster the
verb instances in its input into syntactic clusters
that strongly correlate with SCFs. In this sec-
tion we introduce a Markov Random Field (MRF)
model for this task: Section 3.1 describes our
model?s structure, components and objective; Sec-
tion 3.2 describes the model potentials and the
knowledge they encode; and Section 3.3 describes
how clusters are induced from the model.
3.1 Model Structure
We implement our model in the MRF framework
(Koller and Friedman, 2009). This enables us to
encode the two main sources of information that
govern SCF selection in verb instances: (1) At
the sentential context, the verbal syntactic frame
is encoded through syntactic features. Verb in-
stances with similar feature representations should
therefore take the same syntactic frame; and (2)
At the global context, per verb type SCF distribu-
tions tend to be Zipfian (Korhonen et al., 2000).
Instances of the same verb type should therefore
be biased to take the same syntactic frame.
Given a collection of plain input sentences, we
denote the number of verb instances in the col-
lection with n, and the number of data-dependent
equivalence classes (ECs) with K (see below for
their definition), and define an undirected graphi-
cal model (MRF), G = (V,E, L). We define the
vertex set as V = X ?C, with X = {x
1
, . . . , x
n
}
consisting of one vertex for every verb instance in
the input collection, and C = {c
1
. . . c
K
} consist-
ing of one vertex for each data-dependent EC. The
set of labels used by the model, L, corresponds to
the syntactic frames taken by the verbs in the in-
put data. The edge set E is defined through the
model?s potentials that are described below.
280
We encode information in the model through
three main sets of potentials: one set of single-
ton potentials - defined over individual model ver-
texes, and two sets of pairwise potentials - defined
between pairs of vertexes. The first set consists of
a singleton potential for each vertex in the model.
Reflecting the Zipfian distribution of SCFs across
the instances of the same verb type, these poten-
tials encourage the model to assign such verb in-
stances to the same frame (cluster). The infor-
mation encoded in these potentials is induced via
a pre-processing clustering step. The second set
consists of a pairwise potential for each pair of ver-
texes x
i
, x
j
? X - that is, for each verb instance
pair in the input, across verb types. These poten-
tials encode the belief, computed as feature-based
similarity (see below), that their verb instance ar-
guments implement the same SCF.
Finally, potentials from the last set bias the
model to assign the same SCF to high cardinal-
ity sets of cross-type verb instances based on their
syntactic context. While these are pairwise poten-
tials defined between verb instance vertexes (X)
and EC vertexes (C), they are designed so that
they bias the assignment of all verb instance ver-
texes that are connected to the same EC vertex to-
wards the same frame assignment (l ? L). The
two types of pairwise potentials complement each
other by modeling syntactic similarities among
verb instance pairs, as well as among higher cardi-
nality verb instance sets.
The resulted maximum aposteriori problem
(MAP) takes the following form:
MAP (V ) = argmax
x,c?V
n?
i=1
?
i
(x
i
) +
n?
i=1
n?
j=1
?
i,j
(x
i
, x
j
)+
n?
i=1
K?
j=1
?
i,j
(x
i
, c
j
) ? I(x
i
? EC
j
) +
K?
i=1
K?
j=1
?
i,j
(c
i
, c
j
)
where the predicate I(x
i
? EC
j
) returns 1 if
the i-th verb instance belongs the j-th equivalence
class and 0 otherwise. The ? pairwise potentials
defined between EC vertexes are very simple po-
tentials designed to promise different assignments
for each pair of EC vertexes. They do so by assign-
ing a ?? score to assignments where their argu-
ment vertexes take the same frame and a 0 other-
wise. In the rest of this section we do not get back
to this simple set of potentials.
A graphical illustration of the model is given
in Figure 1. Note that we could have selected a
richer model structure, for example, by defining
a similarity potential over all verb instance ver-
texes that share an equivalence class. However, as
the figure demonstrates, even the structure of the
pruned version of our model (see Section 3.3) usu-
ally contains cycles, which makes inference NP-
hard (Shimony, 1994). Our design choices aim to
balance between the expressivity of the model and
the complexity of inference. In Section 3.3 we de-
scribe the LP relaxation algorithm we use for in-
ference.
C1 C2
Figure 1: A graphical illustration of our model
(after pruning, see Sec. 3.3) for twenty verb in-
stances (|X| = 20), each represented with a black
vertex, and two equivalence classes (ECs), each
represented with a gray vertex (|C| = 2). Solid
lines represent edges (and ?
i,j
pairwise potentials)
between verb instance vertexes. Dashed lines rep-
resent edges between verb instance vertexes and
EC vertexes (?
i,j
pairwise potentials) or between
EC vertexes (?
i,j
pairwise potentials) .
3.2 Potentials and Encoded Knowledge
Pairwise Syntactic Similarity Potentials. The
pairwise syntactic similarity potentials are defined
for each pair of verb instance vertexes, x
i
, x
j
? X .
They are designed to encourage the model to as-
sign verb instances with similar fine-grained fea-
ture representations to the same frame (l ? L)
and verb instances with dissimilar representations
to different frames. For this aim, for every verb
pair i, j with feature representation vectors v
i
, v
j
and verb instance vertexes x
i
, x
j
? X , we define
the following potential function:
?
i,j
(x
i
= l
1
, x
j
= l
2
) =
{
?(v
i
, v
j
) if l
1
= l
2
0 otherwise
}
Where l
1
, l
2
? L are label pairs and ? is a verb
instance similarity function. Below we describe
the feature representation and the ? function.
The verb instance feature representation is de-
fined through the following process. For each
281
word instance in the input sentences we first build
a basic feature representation (see below). Then,
for each verb instance we construct a final fea-
ture representation defined to be the concatena-
tion of that verb?s basic feature representation with
the basic representations of the words in a size
2 window around the represented verb. The fi-
nal feature representation for the i-th verb in-
stance in our dataset is therefore defined to be
v
i
= [w
?2
, w
?1
, vb
i
, w
+1
, w
+2
], where w
?k
and
w
+k
are the basic feature representations of the
words in distance ?k or +k from the i-th verb in-
stance in its sentence, and vb
i
is the basic feature
representation of that verb instance.
Our basic feature representation is inspired
from the feature representation of the MST parser
(McDonald et al., 2005) except that in the parser
the features represent a directed edge in the com-
plete directed graph defined over the words in a
sentence that is to be parsed, while our features are
generated for word n-grams. Particularly, our fea-
ture set is a concatenation of two sets derived from
the MST set described in Table 1 of (McDonald et
al., 2005) in the following way: (1) In both sets the
parent word in the parser?s set is replaced with the
represented word; (2) In one set every child word
in the parser?s set is replaced by the word to the
left of the represented word and in the other set it
is replaced by the word to its right. This choice of
features allows us to take advantage of a provably
useful syntactic feature representation without the
application of any parse tree annotation or parser.
We compute the similarity between the syntac-
tic environments of two verb instances, i, j, using
the following equation:
?(v
i
, v
j
) = W ? cos(v
i
, v
j
)? S
Where W is a hyperparameter designed to bias
verb instances of the same verb type towards the
same frame. Practically, W was tuned to be 3 for
instances of the same type, and 1 otherwise
4
.
While the cosine function is the standard mea-
sure of similarity between two vectors, its val-
ues are in the [0, 1] range. In the MRF modeling
framework, however, we must encode a negative
pairwise potential value between two vertexes in
order to encourage the model to assign different
labels (frames) to them. We therefore added the
positive hyperparameter S which was tuned, with-
4
All hyperparameters that require gold-standard annota-
tion for tuning, were tuned using held-out data (Section 4).
out access to gold standard manual annotations, so
that there is an even number of negative and pos-
itive pairwise syntactic similarity potentials after
the model is pruned (see Section 3.3)
5
.
Type Level Singleton Potentials. The goal of
these potentials is to bias verb instances of the
same type to be assigned to the same syntactic
frame while still keeping the instance based nature
of our algorithm. For this aim, we applied Algo-
rithm 1 for pre-clustering of the verb instances and
encoded the induced clusters into the local poten-
tials of the corresponding x ? X vertexes. For
every x ? X the singleton potential is therefore
defined to be:
?
i
(x
i
= l) =
{
F ? max? if l is induced by Algorithm 1
0 otherwise
}
where max? is the maximum ? score across all
verb instance pairs in the model and F = 0.2 is a
hyperparamter.
Algorithm 1 has two hyperparameters: T and
M , the first is a similarity cut-off value used to de-
termine the initial set of clusters, while the second
is used to determine whether two clusters are simi-
lar enough to be merged. We tuned these hyperpa-
rameters, without manually annotated data, so that
the number of clusters induced by this algorithm
will be equal to the number of gold standard SCFs.
T was tuned so that the first part of the algorithm
generates an excessive number of clusters, and M
was then tuned so that these clusters are merged to
the desired number of clusters.
The ? function, used to measure the similar-
ity between two verbs, is designed to bias the in-
stances of the same verb type to have a higher sim-
ilarity score. Algorithm 1 therefore tends to assign
such instances to the same cluster. In our experi-
ments that was always the case for this algorithm.
High Cardinality Verb Sets Potentials. This
set of potentials aims to bias larger sets of verb
instances to share the same SCF. It is inspired by
(Rush et al., 2012) who demonstrated, that syn-
tactic structures that appear at the same syntac-
tic context, in terms of the surrounding POS tags,
tend to manifest similar syntactic behavior. While
they demonstrated the usefulness of their method
for dependency parsing and POS tagging, we im-
plement it for higher level SCFs.
We identified syntactic contexts that imply simi-
lar SCFs for verb instances appearing inside them.
5
The values in practice are S = 0.43 for labour legislation
and S = 0.38 for environment.
282
Algorithm 1 Verb instance pre-clustering algo-
rithm.
?
? is the average ? score between the mem-
bers of its cluster arguments. T and M are hyper-
parametes tuned without access to gold standard
data.
Require: K = ?
for all x ? X do
for all k ? K do
for all u ? k do
if ?(v
x
, v
u
) > T then
k = k ? {x}
Go to next x
end if
end for
end for
k1 = {x}
K = K ? k1
end for
for all k
1
, k
2
? K: k
1
6= k
2
do
if
?
?(k
1
, k
2
) > M then
Merge (k
1
, k
2
)
end if
end for
Contexts are characterized by the coarse POS tag
to the left and to the right of the verb instance.
While the number of context sets is bounded only
by the number of frames our model is designed
to induce, in practice we found that defining two
equivalence sets led to the best performance gain,
and the sets we used are presented in Table 1.
In order to encode this information into our
MRF, each set of syntactic contexts is associated
with an equivalence class (EC) vertex c ? C and
the verb instance vertexes of all verbs that appear
in a context from that set are connected with an
edge to c. The pairwise potential between a vertex
x ? X and its equivalence class is defined to be:
?
i,j
(x
i
= l
1
, c
j
= l
2
) =
{
U if l
1
= l
2
0 otherwise
}
U = 10 is a hyperparameter that strongly biases x
vertexes to get the same SCF as their EC vertex.
3.3 Verb Cluster Induction
In this section we describe how we induce verb
instance clusters from our model. This process
is based on the following three steps: (1) Graph
pruning; (2) Induction of an Ensemble of approx-
imate MAP inference solutions in the resulted
graphical model; and, (3) Induction of a final clus-
tering solution based on the ensemble created at
step 2. Below we explain the necessity of each of
these steps and provide the algorithmic details.
EC-1 EC-2
Left Right Left Right
, D V T
N D R T
V . N D
R D R N
Table 1: POS contexts indicative for the syntactic
frame of the verb instance they surround. D: de-
terminer, N: noun, V: verb, T: the preposition ?to?
(which has its own POS tag in the WSJ POS tag set
which we use), R: adverb. EC-1 and EC-2 stand
for the first and second equivalence class respec-
tively. In addition, the following contexts where
associated with both ECs: (T,D), (T,N), (N,N)
and (V, I) where I stands for a preposition.
Graph Pruning. The edge set of our model
consists of an edge for every pair of verb in-
stance vertexes and of the edges that connect verb
instance vertexes and equivalence class vertexes.
This results in a large tree-width graph which sub-
stantially complicates MRF inference. To alleviate
this we prune all edges with a positive score lower
than p
+
and all edges with a negative score higher
than p
?
, where p
+
and p
?
are manually tuned hy-
perparametes
6
.
MAP Inference. For most reasonable values of
p
+
and p
?
our graph still contains cycles even af-
ter it is pruned, which makes inference NP-hard
(Shimony, 1994). Yet, thanks to our choice of an
edge-factorized model, there are various approxi-
mate inference algorithms suitable for our case.
We applied the message passing algorithm for
linear-programming (LP) relaxation of the MAP
assignment (MPLP, (Sontag et al., 2008)). LP re-
laxation algorithms for the MAP problem define
an upper bound on the original objective which
takes the form of a linear program. Consequently,
a minimum of this upper bound can be found us-
ing standard LP solvers or, more efficiently, using
specialized message passing algorithms (Yanover
et al., 2006). The MPLP algorithm described in
(Sontag et al., 2008) is appealing in that it itera-
tively computes tighter upper bounds on the MAP
objective (for details see their paper).
Cluster Ensemble Generation and a Final
Solution. As our MAP objective is non-convex,
6
The values used in practice are p
+
= 0.28, p
?
= ?0.17
for the labour legislation dataset, and p
+
= 0.25, p
?
=
?0.20 for the environment set.
283
the convergent point of an optimization algorithm
applied to it is highly sensitive to its initializa-
tion. To avoid convergence to arbitrary local max-
ima which may be of poor quality, we turn to a
perturbation protocol where we repeatedly intro-
duce random noise to the MRF?s potential func-
tions and then compute the approximate MAP so-
lution of the resulted model using the MPLP algo-
rithm. Noising was done by adding an  term to
the lambda values described in section 3.2
7
. This
protocol results in a set of cluster (label) assign-
ments for the involved verb instances, which we
treat as an ensemble of experts from which a final,
high quality, solution is to be induced.
The basic idea in ensemble learning is that if
several experts independently cluster together two
verb instances, our belief that these verbs belong
in the same cluster should increase. (Reichart et
al., 2012) implemented this idea through the k-
way normalized cut clustering algorithm (Yu and
Shi, 2003). Its input is an undirected graph
?
G =
(
?
V ,
?
E,
?
W ) where
?
V is the set of vertexes,
?
E is
the set of edges and
?
W is a non-negative and sym-
metric edge weight matrix. To apply this model
to our task, we construct the input graph
?
G from
the labelings (frame assignments) contained in the
ensemble. The graph vertexes
?
V correspond to the
verb instances and the (i, j)-th entry of the matrix
?
W is the number of ensemble members that assign
the same label to the i-th and j-th verb instances.
For A,B ?
?
V define:
links(A,B) =
?
i?A,j?B
?
W (i, j)
Using this definition, the normalized link ratio
of A and B is defined to be:
NormLinkRatio(A,B) =
links(A,B)
links(A,
?
V )
The k-way normalized cut problem is to mini-
mize the links that leave a cluster relative to the
total weight of the cluster. Denote the set of clus-
terings of
?
V that consist of k clusters by
?
C =
{c?
1
, . . . c?
t
} and the j-th cluster of the i-th cluster-
7
 was accepted by first sampling a number in the [0, 1]
range using the Java psuodorandom generator and then scal-
ing it to 1% of cos(v
i
, v
j
). This value was tuned, without
access to gold standard manual annotations, so that there is
an even number of negative and positive pairwise syntactic
similarity potentials after the model is pruned (Section 3.3).
ing by c?
ij
. Then
c
?
= argmin
c?
i
?
?
C
k
?
j=1
NormLinkRatio(c?
ij
,
?
V ? c?
ij
)
The algorithm of (Yu and Shi, 2003) solves this
problem very efficiently as it avoids the heavy
eigenvalues and eigenvectors computations re-
quired by traditional approaches.
4 Experiments and Results
Our model is unique compared to existing systems
in two respects. First, it does not utilize supervi-
sion in the form of either a supervised syntactic
parser and/or manually crafted SCF rules. Conse-
quently, it induces unnamed frames (clusters) that
are not directly comparable to the named frames
induced by previous systems. Second, it induces
syntactic frames at the verb instance, rather than
type, level. Evaluation, and especially comparison
to previous work, is therefore challenging.
We therefore evaluate our system in two ways.
First, we compare its output, as well as the output
of a number of clustering baselines, to the gold
standard annotation of corpora from two differ-
ent domains (the only publicly available ones with
instance level SCF annotation, to the best of our
knowledge). Second, in order to compare the out-
put of our system to a rule-based SCF system that
utilizes a supervised syntactic parser, we turn to
a task-based evaluation. We aim to predict the
degree of similarity between verb pairs and, fol-
lowing (Pado and Lapata, 2007) , we do so using
a syntactic-based vector space model (VSM). We
construct three VSMs - (a) one that derives fea-
tures from our clusters; (b) one whose features
come from the output of a state-of-the-art verb
type level, rule based, SCF system (Reichart and
Korhonen, 2013) that uses a modern parser (Klein
and Manning, 2003); and (c) a standard lexical
VSM. Below we show that our system compares
favorably in both evaluations.
Data. We experimented with two datasets taken
from different domains: labor legislation and en-
vironment (Quochi et al., 2012). These datasets
were created through web crawling followed by
domain filtering. Each sentence in both datasets
may contain multiple verbs but only one target
verb has been manually annotated with a SCF.
The labour legislation domain dataset contains
4415 annotated verb instances (and hence also
284
sentences) of 117 types, and the environmental
domain dataset contains 4503 annotated verb in-
stances of 116 types. In both datasets no verb type
accounts for more than 4% of the instances and
only up to 35 verb types account for 1% of the
instances or more. The lexical difference between
the corpora is substantial: they share only 42 anno-
tated verb types in total, of which only 2 verb types
(responsible for 4.1% and 5.2% of the instances in
the environment and labor legislation domains re-
spectively) belong to the 20 most frequent types
(responsible for 37.9% and 46.85% of the verb in-
stances in the respective domains) of each corpus.
The 29 members of the SCF inventory are de-
tailed in (Quochi et al., 2012). Table 2, presenting
the distribution of the 5 highest frequency frames
in each corpus, demonstrates that, in addition to
the significant lexical difference, the corpora differ
to some extent in their syntactic properties. This is
reflected by the substantially different frequencies
of the ?dobj:iobj-prep:su? and ?dobj:su? frames.
As a pre-processing step we first POS tagged
the datasets with the Stanford tagger (Toutanova
et al., 2003) trained on the standard POS training
sections of the WSJ PennTreebank corpus.
4.1 Evaluation Against SCF Gold Standard
Experimental Protocol The computational com-
plexity of our algorithm does not allow us to run it
on thousands of verb instances in a feasible time.
We therefore repeatedly sampled 5% of the sen-
tences from each dataset, ran our algorithm as well
as the baselines (see below) and report the average
performance of each method. The number of rep-
etitions was 40 and samples were drawn from a
uniform distribution while still promising that the
distribution of gold standard SCFs in each sam-
ple is identical to their distribution in the entire
dataset. Before running this protocol, 5% of each
corpus was kept as held-out data on which hyper-
parameter tuning was performed.
EvaluationMeasures and Baselines. We com-
pare our system?s output to instance-level gold
standard annotation. We use standard measures
for clustering evaluation, one measure from each
of the two leading measure types: the V measure
(Rosenberg and Hirschberg, 2007), which is an in-
formation theoretic measure, and greedy many-to-
one accuracy, which is a mapping-based measure.
For the latter, each induced cluster is first mapped
to the gold SCF frame that annotates the highest
number of verb instances this induced cluster also
annotates and then a standard instance-level accu-
racy score is computed (see, e.g., (Reichart and
Rappoport, 2009)). Both measures scale from 100
(perfect match with gold standard) to 0 (no match).
As mentioned above, comparing the perfor-
mance of our system with respect to a gold stan-
dard to the performance of previous type-level
systems that used hand-crafted rules and/or su-
pervised syntactic parsers would be challenging.
We therefore compare our model to the follow-
ing baselines: (a) The most frequent class (MFC)
baseline which assigns all verb instances with the
SCF that is the most frequent one in the gold stan-
dard annotation of the data; (b) The Random base-
line which simply assigns every verb instance with
a randomly selected SCF; (c) Algorithm 1 of sec-
tion 3.2 which generates unsupervised verb in-
stance clustering such that verb instances of the
same type are assigned to the same cluster; and
(d) Finally, we also compare our model against
versions where everything is kept fixed, except a
subset of potentials which is omitted. This enables
us to study the intricacies of our model and the rel-
ative importance of its components. For all mod-
els, the number of induced clusters is equal to the
number of SCFs in the gold standard.
Results Table 3 presents the results, demon-
strating that our full model substantially outper-
forms all baselines. For the first two simple heuris-
tic baselines (MFC and Random) the margin is
higher than 20% for both the greedy M-1 mapping
measure and the V measure. Note tat the V score
of the MFC baseline is 0 by definition, as it as-
signs all items to the same cluster. The poor per-
formance of these simple baselines is an indication
of the difficulty of our task.
Recall that the type level clustering induced by
Algorithm 1 is the main source of type level in-
formation our model utilizes (through its single-
ton potentials). The comparison to the output of
this algorithm (the Type Pre-clustering baseline)
therefore shows the quality of the instance level
refinement our model provides. As seen in table 3,
our model outperforms this baseline by 6.9% for
the M-1 measure and 5.2% for the V measure.
In order to compare our model to its compo-
nents we exclude either the EC potentials (? and
?) only (Model - EC), or the EC and the singleton
potentials (?
i
, Model - EC - Type pre-clustering).
The results show that our model gains much more
285
Environment Labour Legislation
SCF Frequency SCF Frequency
dobj:su 46% dobj:su 39%
su 9% dobj:iobj-prep:su 15%
iobj-prep:su 8% su 10%
dobj:iobj-prep:su 6% su:xcompto-vbare 8%
su:xcompto-vbare 6% iobj-prep:su 7%
Table 2: Top 5 most frequent SCFs for the Environment and Labour Legislation datasets used in our
experiments.
Environment Labour Legislation
M-1 V M-1 V
Full Model 66.4 57.3 69.2 55.6
Baselines
MFC 46.2 0 39.4 0
Random 34.6 28.1 36.5 27.8
Type Pre-clustering 60.1 52.1 62.3 51.4
Model Components
Model - EC 64.9 56.2 67.4 54.6
Model - EC - Type pre-clustering 48.3 48.9 45.7 44.7
Table 3: Results for our full model, the baselines (Type Pre-clustering: the pre-clustering algorithm
(Algorithm 1 of section 3.2), MFC: the most frequent class (SCF) in the gold standard annotation and
Random: random SCF assignment) and the model components. The full model outperforms all other
models across measures and datasets.
from the type level information encoded through
the singleton potentials than from the EC poten-
tials. Yet, EC potentials do lead to an improvement
of up to 1.5% in M-1 and up to 1.1% in V and are
therefore responsible for up to 26.1% and 21.2%
of the improvement over the type pre-clustering
baseline in terms of M-1 and V, respectively.
4.2 Task Based Evaluation
We next evaluate our model in the context of vec-
tor space modeling for verb similarity prediction
(Turney and Pantel, 2010). Since most previous
word similarity works used noun datasets, we con-
structed a new verb pair dataset, following the pro-
tocol used in the collection of the wordSimilarity-
353 dataset (Finkelstein et al., 2002).
Our dataset consists of 143 verb pairs, con-
structed from 122 unique verb lemma types. The
participating verbs appear ? 10 times in the con-
catenation of the labour legislation and the envi-
ronment datasets. Only pairs of verbs that were
considered at least remotely similar by human
judges (independent of those that provided the
similarity scores) were included. A similarity
score between 1 and 10 was assigned to each pair
by 10 native English speaking annotators and were
then averaged in order to get a unique pair score.
Our first baseline is a standard VSM based on
lexical collocations. In this model features corre-
spond to the number of collocations inside a size
2 window of the represented verb with each of the
5000 most frequent nouns in the Google n-gram
corpus (Goldberg and Orwant, 2013). Since our
corpora are limited in size, we use the collocation
counts from the Google corpus.
We used our model to generate a vector repre-
sentation of each verb in the following way. We
run the model 5000 times, each time over a set of
verbs consisting of one instance of each of the 122
verb types participating in the verb similarity set.
The output of each such run is transformed to a
binary vector for each participating verb, where
all coordinates are assigned the value of 0, ex-
cept from the one that corresponds to the cluster to
which the verb was assigned which has the value
of 1. The final vector representation is a concate-
nation of the 5000 binary vectors. Note that for
this task we did not use the graph cut algorithm to
generate a final clustering from the multiple MRF
286
runs. Instead we concatenated the output of all
these runs into one feature representation that fa-
cilitates similarity prediction. For our model we
estimated the verb pair similarity using the Tani-
mato similarity score for binary vectors:
T (X,Y ) =
?
i
X
i
? Y
i
?
i
x
i
? Y
i
For the baseline model, where the features are
collocation counts, we used the standard cosine
similarity.
Our second baseline is identical to our model,
except that: (a) the data is parsed with the Stan-
ford parser (version 3.3.0, (Klein and Manning,
2003)) which was trained with sections 2-21 of the
WSJ corpus; (b) the phrase structure output of the
parser is transformed to the CoNLL dependency
format using the official CoNLL 2007 conversion
script (Johansson and Nugues, 2007); and then (c)
the SCF of each verb instance is inferred using the
rule-based system used by (Reichart and Korho-
nen, 2013). The vector space representation for
each verb is then created using the process we de-
scribed for our model and the same holds for vec-
tor comparison. This baseline allows direct com-
parison of frames induced by our SCF model with
those derived from a supervised parser?s output.
We computed the Pearson correlation between
the scores of each of the models and the human
scores. The results demonstrate the superiority
of our model in predicting verb similarity: the
correlation of our model with the human scores
is 0.642 while the correlation of the lexical col-
location baseline is 0.522 and that of the super-
vised parser baseline is only 0.266. The results
indicate that in addition to their good alignment
with SCFs, our clusters are also highly useful for
verb meaning representation. This is in line with
the verb clustering theory of the Levin tradition
(Levin, 1993) which ties verb meaning with their
syntactic properties. We consider this an intrigu-
ing direction of future work.
5 Conclusions
We presented an MRF-based unsupervised model
for SCF acquisition which produces verb instance
level SCFs as output. As opposed to previous sys-
tems for the task, our model uses only a POS tag-
ger, avoiding the need for a statistical parser or
manually crafted rules. The model is particularly
valuable for NLP tasks benefiting from SCFs that
are applied across text domains, and for the many
tasks that involve sentence-level processing.
Our results show that the accuracy of the model
is promising, both when compared against gold
standard annotations and when evaluated in the
context of a task. In the future we intend to im-
prove our model by encoding additional informa-
tion in it. We will also adapt it to a multilingual
setup, aiming to model a wide range of languages.
Acknowledgments
The first author is supported by the Common-
wealth Scholarship Commission (CSC) and the
Cambridge Trust.
References
Ivana Romina Altamirano and Laura Alonso i Ale-
many. 2010. IRASubcat, a highly customizable,
language independent tool for the acquisition of ver-
bal subcategorization information from corpus. In
Proceedings of the NAACL 2010 Workshop on Com-
putational Approaches to Languages of the Ameri-
cas.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Proceedings of ACL-05.
Taylor Berg-Kirkpatrick, Alexander Bouchard-Cote,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL-HLT-10.
Akshar Bharati, Sriram Venkatapathy, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In Proceedings of CoNLL-05.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of ANLP-97.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceed-
ings of ACL-COLING-06.
John Carroll and Alex Fang. 2004. The automatic ac-
quisition of verb subcategorisations and their impact
on the performance of an HPSG parser. In Proceed-
ings of IJCNLP-04.
Paula Chesley and Susanne Salmon-Alt. 2006. Au-
tomatic extraction of subcategorization frames for
french. In Proceedings of LREC-06.
Kostadin Cholakov and Gertjan van Noord. 2010. Us-
ing unknown word techniques to learn known words.
In Proceedings of EMNLP-10.
287
Shay Cohen and Noah Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of
NAACL-HLT-09.
Marie-Catherine De-Marneffe, Bill Maccartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06.
Lukasz Debowski. 2009. Valence extraction using EM
selection and co-occurrence matrices. Proceedins of
LREC-09.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ei-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116?131.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of english books. In Proceedings of (*SEM)-13. As-
sociation for Computational Linguistics.
Jan Haji?c, Martin mejrek, Bonnie Dorr, Yuan Ding, Ja-
son Eisner, Daniel Gildea, Terry Koo, Kristen Par-
ton, Gerald Penn, Dragomir Radev, and Owen Ram-
bow. 2002. Natural language generation in the con-
text of machine translation. Technical report, Cen-
ter for Language and Speech Processing, Johns Hop-
kins University, Baltimore. Summer Workshop Final
Report.
Chung hye Han, Benoit Lavoie, Martha Palmer, Owen
Rambow, Richard Kittredge, Tanya Korelsky, and
Myunghee Kim. 2000. Handling structural diver-
gences and recovering dropped arguments in a ko-
rean/english machine translation system. In Pro-
ceedings of the AMTA-00.
Dino Ienco, Serena Villata, and Cristina Bosco. 2008.
Automatic extraction of subcategorization frames
for italian. In Proceedings of LREC-08.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of NODALIDA-07.
Daisuke Kawahara and Sadao Kurohashi. 2010. Ac-
quiring reliable predicate-argument structures from
raw corpora for case frame compilation. In Proceed-
ings of LREC-10.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-03.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. The
MIT Press.
Anna Korhonen, Genevieve Gorrell, and Diana Mc-
Carthy. 2000. Statistical filtering and subcate-
gorization frame acquisition. In Proceedings of
EMNLP-00.
Anna Korhonen. 2002. Semantically motivated sub-
categorization acquisition. In Proceedings of the
ACL-02 workshop on Unsupervised lexical acquisi-
tion.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In Proceedings of ACL-11.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
Proceedings of EMNLP-11.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Proceedings of IJCNLP-
05.
Alessandro Lenci, Barbara Mcgillivray, Simonetta
Montemagni, and Vito Pirrelli. 2008. Unsupervised
acquisition of verb subcategorization frames from
shallow-parsed corpora. In Proceedings of LREC-
08.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. Chicago, IL.
Tom Lippincott, Anna Korhonen, and Diarmuid Os-
eaghdha. 2010. Exploring subdomain variation in
biomedical language. BMC Bioinformatics.
Tom Lippincott, Aanna Korhonen, and Diarmuid Os-
eaghdha. 2012. Learning syntactic verb frames us-
ing graphical models. In Proceedings of ACL-12.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-05.
Cedric Messiant, Anna Korhonen, and Thierry
Poibeau. 2008. LexSchem: A large subcategoriza-
tion lexicon for French verbs. In Proceedings of
LREC-08.
Cedric Messiant. 2008. A subcategorization acquis-
tion system for french verbs. In Proceedings of
ACL08-SRW.
Yusuke Miyao and Junichi Tsujii. 2005. Probabilistic
disambiguaton models for wide-coverage hpsg pars-
ing. In Proceedings of ACL-05.
Alessandro Moschitti and Roberto Basili. 2005. Verb
subcategorization kernels for automatic semantic la-
beling. In Proceedings of the ACL-SIGLEX Work-
shop on Deep Lexical Acquisition.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the penn-ii and penn-iii treebanks. Computational
Linguistics, 31:328?365.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33:161?199.
288
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of ACL-07.
Valeria Quochi, Francesca Frontini, Roberto Bartolini,
Olivier Hamon, Marc Poch, Muntsa Padr, Nuria Bel,
Gregor Thurmair, Antonio Toral, and Amir Kam-
ram. 2012. Third evaluation report. evaluation of
panacea v3 and produced resources. Technical re-
port.
Roi Reichart and Anna Korhonen. 2013. Improved
lexical acquisition through dpp-based verb cluster-
ing. In Proceedings of ACL-13.
Roi Reichart and Ari Rappoport. 2009. The nvi
clustering evaluation measure. In Proceedings of
CoNLL-09.
Roi Reichart, Gal Elidan, and Ari Rappoport. 2012. A
diverse dirichlet process ensemble for unsupervised
induction of syntactic categories. In Proceedings of
COLING-12.
Laura Rimell, Thomas Lippincott, Karin Verspoor, He-
len Johnson, and Anna Korhonen. 2013. Acqui-
sition and evaluation of verb subcategorization re-
sources for biomedicine. Journal of Biomedical In-
formatics, 46:228?237.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proceedings of the ACL-07 Linguistic
Annotation Workshop.
Douglas Roland and Daniel Jurafsky. 1998. subcate-
gorization frequencies are affected by corpus choice.
In Proceedings of ACL-98.
Andrew Rosenberg and Julia Hirschberg. 2007. V
measure: a conditional entropybased external cluster
evaluation measure. In Proceedings of EMNLP-07.
Alexander Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and pos
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP-12.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of german semantic verb
classes. Computational Linguistics, 32(2):159?194.
Solomon Shimony. 1994. Finding the maps for belief
networks is np-hard. Artificial Intelligence, 68:399?
310.
David Sontag, Talya Meltzer, Amir Globerson, Tommi
Jaakkola, and Yair Weiss. 2008. Tightening lp re-
laxations for map using message passing. In Pro-
ceedings of UAI-08.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP-11.
Ivan Titvo and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of EMNLP-12.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of NAACL-03.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37:141?188.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and
Anna Korhonen. 2012. Multi-way tensor factor-
ization for unsupervised lexical acquisition. In Pro-
ceedings of COLING-12.
Giulia Venturi, Simonetta Montemagni, Simone
Marchi, Yutaka Sasaki, Paul Thompson, John Mc-
Naught, and Sophia Ananiadou. 2009. Bootstrap-
ping a verb lexicon for biomedical information ex-
traction. Computational Linguistics and Intelligent
Text Processing, 5449:137?148.
Marion Weller, Alexander Fraser, and Sabine Schulte
im Walde. 2013. Using subcategorization knowl-
edge to improve case prediction for translation to
german. In Proceedings of ACL-13.
Chen Yanover, Talya Meltzer, and Yair Weiss. 2006.
Linear programming relazations and belief pro-
pogataion an empitical study. JMLR Special Issue
on Machine Learning and Large Scale Optimization.
Stella Yu and Jianbo Shi. 2003. Multiclass spectral
clustering. In Proceedings of ICCV-13.
289
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 70?79,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Multi Event Extraction Guided by Global Constraints
Roi Reichart Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{roiri, regina}@csail.mit.edu
Abstract
This paper addresses the extraction of event
records from documents that describe multi-
ple events. Specifically, we aim to identify
the fields of information contained in a docu-
ment and aggregate together those fields that
describe the same event. To exploit the in-
herent connections between field extraction
and event identification, we propose to model
them jointly. Our model is novel in that it
integrates information from separate sequen-
tial models, using global potentials that en-
courage the extracted event records to have
desired properties. While the model con-
tains high-order potentials, efficient approxi-
mate inference can be performed with dual-
decomposition. We experiment with two data
sets that consist of newspaper articles de-
scribing multiple terrorism events, and show
that our model substantially outperforms tra-
ditional pipeline models.
1 Introduction
Today, most efforts in information extraction have
focused on the field extraction task, commonly for-
mulated as a sequence tagging problem. When a
document describes a single event, the list of ex-
tracted fields provides a useful abstraction of the in-
put document. In practice, however, a typical news-
paper document describes multiple events, and a flat
list of field values may not contain the sufficient
structure required for many NLP applications. Our
goal is therefore to extract event templates which ag-
gregate field values for individual events.
Consider, for instance, the New York Times arti-
cle excerpt in Figure 1 that describes three related
terrorist events. As this example illustrates, in order
to populate the corresponding event templates, the
model needs to identify segments that describe indi-
vidual events. Such segmentation is challenging, as
event boundaries are not explicitly demarcated in the
text. Moreover, descriptions of different events are
often intermingled, as in the above example, further
complicating boundary recovery.
In this paper, we consider a model that jointly
performs event segmentation and field extraction.
This model capitalizes on the inherent connection
between the two tasks in order to reduce the ambi-
guity of template-based extraction. For example, the
distribution of field values in the text provides strong
clues about event segmentation, such as the presence
of multiple new fields strongly signaling a segment
boundary. Likewise, knowledge of the boundaries
enables the model to rule out mutually inconsistent
predictions, such as extracting two distinct locations
for the same event.
We formulate our approach as a joint model that
marks each word with field and event labels si-
multaneously. At the sentence level, segmentation
and field extraction taggers are implemented using
separate sequence models operating over local fea-
tures. At the document level, the model encourages
global consistency via potentials that link the ex-
tracted event records and their fields. Some of these
potentials are limited to fields of an individual event
such as the ?single city per event? constraint. Others
encode discourse-level properties of the whole doc-
ument and thus involve records of multiple events,
70
A powerful car bomb exploded today in Baghdad inside the holiest Shiite shrine . As many as 95 people were killed
in the event, according to sources in Washington. The blast came only two days after another car bomb exploded in a crowded
street in Mosul in the northern part of Iraq, killing 13 pedestrians, in an attack carried out by Al Qaeda. Together with the
previous attack by Al Qaeda, the shooting in Najaf three weeks ago that killed 15 American soldiers, violence seemed
to spike to its highest level. The bombing today, happened around 9am, when the roads are crowded with people. ...
Organization Tactic Target Weapon Fatalities City Country
Event 1 ? bombing Shiite shrine car bomb 95 people Baghdad ?
Event 2 Al Qaeda bombing ? car bomb 13 pedestrians Mosul Iraq
Event 3 Al Qaeda shooting ?- ? 15 American Soldiers Najaf ?
Figure 1: A New York times article describing three terrorist events and a table demonstrating the corresponding event records.
such as the tendency in newspaper reporting to fea-
ture the main event at the beginning and repeatedly
throughout the document.
While these high-order potentials encode impor-
tant linguistic properties of valid assignments, they
greatly complicate learning and inference. There-
fore, our method estimates the parameters of the lo-
cal sequence models and the global potentials sep-
arately. Then, at inference time, it finds variable
assignments that are most consistent with both the
local models and the global potentials. Inference
is implemented via dual-decomposition, an efficient
algorithm shown to be effective for complex joint
inference problems.
We evaluate our approach for event extraction on
two data sets, one is a new collection of long news-
paper articles and the other is a subset of the MUC-
4 documents. Both data sets consist of articles that
describe multiple terrorist events (40.3 and 12.4 sen-
tences and 4.4 and 3.1 events per article for each data
set on average). We demonstrate the benefits of the
joint model for event extraction; it outperforms a tra-
ditional pipeline model by a significant margin. For
instance, it yields an absolute gain of 8.5% for our
new corpus when measured using document-level F-
score. Our results show the effectiveness of global
constraints in the context of template extraction and
motivate their exploration in other IE tasks.
2 Previous Work
Event-Template Extraction Event template extrac-
tion has been previously explored in the MUC-4
scenario template task. Work on this task has fo-
cused on pipeline models which decouple the task
into the sub-tasks of field extraction and event-based
text segmentation. For example, rule-based meth-
ods (Rau et al, 1992; Chinchor et al, 1993) identify
generalizations both for single field fillers and for re-
lations between fields and use them to fill event tem-
plates. Likewise, classifier-based algorithms (Chieu
et al, 2003; Xiao et al, 2004; Maslennikov and
Chua, 2007; Patwardhan and Riloff, 2009) gener-
ally train individual classifiers for each type of field
and aggregate candidate fillers based on a senten-
tial event classifier. Finally, unsupervised techniques
(Chambers and Jurafsky, 2011) have combined clus-
tering, semantic roles, and syntactic relations in or-
der to both construct and fill event templates.
In our work, we also address the sub-tasks of
field extraction and event segmentation individu-
ally; however, we link them through soft global con-
straints and encourage consistency through joint in-
ference. To facilitate the joint inference, we use a
linear-chain CRF for each sub-task.
Global Constraints Previous work demonstrated
the benefits of applying declarative constraints in in-
formation extraction (Finkel et al, 2005; Roth and
tau Yih, 2004; Chang et al, 2007; Druck and Mc-
Callum, 2010). Constraints have been explored both
at sentence and document level. For example, Finkel
et al (2005) employ document-level constraints to
encourage global consistency of named entity as-
signments. Likewise, Chang et al (2007) use con-
straints at multiple levels, such as sentence-level
constraints to specify field boundaries and global
constraints to ensure relation-level consistency. In
our work we focus on document-level constraints.
We utilize both discourse and record-coherence con-
straints to encourage consistency between local se-
quence models.
There has also been unsupervised work that
demonstrates the benefit of domain-specific con-
straints (Chen et al, 2011). In our work we show
that domain-specific constraints based on the com-
mon structure of newspaper articles are also useful
to guide a supervised model.
71
3 Model
Problem Formulation Given a document, our goal
is to extract field values and aggregate them into
event records. The training data consists of event an-
notations where each word in the document is tagged
with a field and with an event id. If a word is not a
filler for a field, it is annotated with a default NULL
field value. At test time, the number of events is not
given and has to be inferred from the data.
Model Structure Our model is built around the
connection between local extraction decisions and
global constraints on event structure. Based on
local cues, the model can identify candidate field
fillers. However, connecting them to events requires
a broader document context. To effectively capture
this context, the model needs to group together por-
tions of the document that describe the same event.
Global constraints are instrumental in this process,
as they drive the aggregation of contiguous segments
computed by a local segmentation model. In ad-
dition, global constraints coordinate local decisions
and thereby enable us to express important discourse
dependencies between various assignments.
To implement these ideas in a computational
framework, we define an undirected graphical model
with a vertex set V = X ? Y ? Z . X is a set of ob-
served nodes; xi represents the ithe word in a docu-
ment. Y and Z are sets of unobserved nodes corre-
sponding to the field and event assignments respec-
tively of the ith word. The number of input words in
a document is denoted by n.
We define three types of potentials:
? Field-labeling Potentials associate words in a
document with field labels based on their local
sentential context.
? Event-labeling Potentials associate words in a
document with event boundaries based on the
local surroundings of a candidate boundary.
? Global Consistency Potentials link the ex-
tracted event records and their fields to encour-
age global consistency. These potentials are de-
fined over the entire set of variables related to a
document.
The resulting maximum aposteriori problem is:
MAP (?) =
?
f?F
?f (rf )
where ?f are the potential functions and {rf |f ?
{1, . . . , n}, f ? F} is the set of their variables.
3.1 Modeling Local Dependencies
Field Labeling The first step of the model is tagging
the words in the input document with fields. Fol-
lowing traditional approaches, we employ a linear-
chain CRF (Lafferty et al, 2001) that operates
over standard lexical, POS-based and syntactic fea-
tures (Finkel et al, 2005; Finkel and Manning, 2009;
Bellare and McCallum, 2009; Yao et al, 2010).
Event Segmentation At the local level, event analy-
sis involves identification of event boundaries which
we model as linear segmentation. To this end, we
employ a binary CRF that predicts whether a given
word starts a description of a new event or continues
the description of the current event, based on lex-
ical and POS-based features. In addition, we add
features obtained from the output of the field extrac-
tion CRF. These features capture the intuition that
boundary sentences often contain multiple fields.
The potential functions of these components are
given by the likelihoods of the corresponding CRFs.
3.2 Modeling Global Dependencies
The main function of the global constraints is to
link extracted fields to the corresponding events.
In addition, the model can use global constraints
to resolve potentially inconsistent decisions of the
local models by encouraging them to agree with
global, document-level properties. We consider two
types of global consistency potentials: discourse po-
tentials that involve interactions between multiple
records, and record coherence potentials that cap-
ture patterns at the level of individual records.
The general form of a global potential p is:
?f (xf?p, yf?p, zf?p) =
{
?p if potential-property holds
0 otherwise
Where f ? p is the index set of variables over
which the potential is defined. Table 1 gives a formal
description of all the potentials. Below we describe
the linguistic intuition behind these potentials.
Discourse Potentials To populate event records
with extracted information, the model needs to
72
Discourse
MAIN EVENT Two consecutive sentences without fields indicate a transition
to the main event:
(?Si, Si+1 s.t. (?k ? Si, yk = NULL) ? (?k ? Si+1, yk = NULL)) ?
(?l ? i s.t. (?u, u ? i, u < l, 1fME(Su)=1), ?p ? Sl, zp = CENTRAL)
SEGMENT BOUNDARY Event changes should take place in multi-field sentences:
?i, j ? I, ((i = j + 1) ? (zi! = zj)) ?
(?i1 . . . it ? I s.t. 1[fs?SB(i,i1,...it)=1] ? 1[ff?SB(i1,...it)=1])
EVENT REDUNDANCY Events should not significantly overlap:
?i, j ? {1, . . . , |Z|}, ?k, l ? I s.t.
((yk = yl) ? (yk! = NULL) ? (zk = i) ? (zl = j) ? (xk! = xl))
Record Coherence
FIELD SPARSITY Some fields take a single unique value per record:
?K,L ? I, C ? ?, ((YK = C) ? (YL = C) ? (ZK = ZL)) ? (XK = XL)
RECORD DENSITY Words associated with a field should fill the field if it is otherwise empty:
?i ? ?, C ? ?, (?k ? I s.t. (1[Cind(xk)=1]) ? (zk = i)) ? (?l ? I s.t. (yl = C) ? (zl = i))
Table 1: Logical formulations of the properties encouraged by the global potentials. Si is the set of indexes corre-
sponding the the ith sentence. fME(Su) = 1 iff there is no event change in sentence Su. fs?SB(i1, . . . , it) = 1 iff the
corresponding words appear in the same sentence. ff?SB(i1, . . . , it) = 1 iff the corresponding words have different,
non-NULL, field values. Cind(xk) = 1 iff xk is assigned to C in a training event record. CENTRAL is the central
event of the document, defined to be its first event. I = {1, . . . , n}, ? = {1, . . . |Y |}, ? = {1, . . . , |Z|}.
group together sentences that describe the same
event. The local boundary model can only predict
contiguous blocks of event descriptions, but it can-
not link together blocks that appear in different parts
of the document. Our approach towards this task
is informed by regularity in the discourse organiza-
tion of news articles. A typical news story is de-
voted to a single event, mixed with short descrip-
tions of other events. Therefore, we prefer event as-
signments where long segments with no field values
? e.g., background descriptions ? are associated with
the main event. This intuition is formalized in the
Main Event Potential shown in Table 1.
The second discourse constraint concerns detec-
tion of event boundaries. We prefer assignments in
which the boundary sentence contains a large num-
ber of fields. This preference is expressed in the Seg-
ment Boundary Potential shown in Table 1.
The final discourse constraint favors assignments
that reduce redundancy in generated records. It is
unlikely that a document describes several events
with significant factual overlap. This constraint
is implemented in the Event Redundancy Potential
shown in Table 1.
Record Coherence Potentials These potentials
capture properties of valid field assignments in the
context of a given event record. The first potential
in this group ? Field Sparsity Potential ? is ap-
plied to fields, such as City, that tend to take a single
unique value per event record.1 This potential dis-
courages assignments that link this field with multi-
ple values within the same event. Similar constraints
have been effectively used in information extraction
in the past (Finkel et al, 2005). In our work, we ap-
ply this constraint at the event level, rather than at
the document level, thereby enabling multiple vari-
able values for multi-event documents.
The second record coherence potential ? Record
Density Potential ? aims to reduce empty fields in
the event record. This potential turns on when a lo-
cal extractor fails to identify a filler for a field when
processing a given event segment. If this segment
contains words that are labeled as potential fillers in
the context of other events in the training data, we
prefer assignments that associate them with the field
that otherwise would have been empty. This poten-
tial is inspired by the one sense per discourse con-
straint (Gale et al, 1992) that associates all the oc-
currences of the word in a document with the same
semantic meaning.
1The potential is defined for the following fields: Terrorist
Organization, Weapon, City, and Country.
73
4 Inference
Dual Decomposition The global potentials encode
important document level information that links to-
gether the extracted event records and their fields.
Introducing these potentials, however, greatly com-
plicates inference. Consider the MAP equation of
Section 3. If the intersection between each pair of
subsets, fi, fj ? F , had been empty, we could have
found the MAP assignment by solving each poten-
tial separately. However, since many subset pairs do
overlap, we must enforce agreement among the as-
signments which results in an NP-hard problem.
In order to avoid this computational bottleneck we
turn to dual-decomposition (Rush et al, 2010; Koo
et al, 2010), an inference technique that enables ef-
ficient computation of a tight upper bound on the
MAP objective, while preserving the original depen-
dencies of the model. Dual decomposition has been
recently applied to a joint model for biomedical en-
tity and event extraction by Riedel and McCallum
(2011). In their work, however, events are defined in
the sentence level. Here we show how this technique
can be applied to a model which involves document-
level potentials.
We first re-write the MAP equation, such that it
contains a local potential for each of the unobserved
variables, as required by the inference algorithm:
MAP (?) = max
y,z
?
j?J
?j(rj) +
?
f?F
?f (rf )
where we denote the set of indexes of all unob-
served variables with J and refer to each of them
with rj . We then define the dual problem:
min
?
L(?), L(?) =
?
j?J
max
rj
[?j(rj) +
?
f :j?f
?fj(rj)]+
?
f?F
max
rf
[?f (rf )?
?
j?f
?fj(rj)]
where for every f ? F and j ? f , ?fj is a vector of
Lagrange multipliers with an entry for each possi-
ble assignment of rj . We add the notation ?f for the
matrix of Lagrange multipliers for all the variables
in f , and for an assignment M of the variables in f
we define ?f (M) to be the corresponding vector of
Lagrange multipliers. The multipliers can be viewed
as messages transferred between the potentials to en-
courage agreement between their assignments.
The dual objective, L(?), forms an upper bound
on the MAP objective. Our inference algorithm
Set g0fj ? 0 for all j ? J, f ? F
for k = 1 to K do
for j ? J do
rlkj = argmax
rj
[?j(rj) +
?
f :j?f
?fj(rj)]
end? TRUE
for f ? F do
rpkf = argmaxrf
[?f (rf )?
?
j?f
?fj(rj)]
for j ? f do
if rlkj 6= rpkfj then
gkfj(rlkj ) + = 1
gkfj(rpkfj) ? = 1
end? FALSE
?k+1fj = ?
k
fj ? ?k ? gkfj
if end then
return Rk
?k ? 1/k
return (RK )
(a)
rlkj : Sort [?j(rj) +
?
f :j?f
?fj(rj)]. Return the minimizing rj .
rpkf :
MMAkf ?: Minimum-Message assignment
PRAkf ?: Property-Respecting assignment
if (?p ? sum(?f (PRA)) > (?1) ? sum(?f (MMA)) then
rpkf = PRAkf
else
rpkf = MMAkf
(b)
Figure 2: The inference algorithm. (a): The dual-
decomposition algorithm. (b): Algorithms for the
argmax operations of the dual-decomposition algorithm.
therefore searches for its minimum, i.e. the tightest
upper bound of the original MAP objective. L(?) is
convex and non-differentiable and can therefore be
minimized by the subgradient descent algorithm in
Figure 2 (a).
Individual Potentials Maximization The inference
algorithm requires efficient solvers for its argmax
problems. For the field labeling and event segmen-
tation potentials, the messages are encoded into the
feature space of the CRF, and exact maximization is
achieved through standard CRF decoding. For the
local potentials, (rlkj ), the maximizing assignments
are computed by sorting the messages for each un-
observed variable (Figure 2 (b)).
The global potentials are more challenging. Ide-
ally, we could find the optimal assignment, rp?f , that
agrees with the assignments of the other potentials
( rp?f = argmin
?
j?f ?fj(rpj)) and at the same
time respects the property encouraged by its own po-
74
tential (?p(rp?f ) > 0). In practice, however, there
may be no such assignment, in which case the as-
signment conflict needs to be resolved.
We first compute the minimum-message assign-
ment (MMA), the assignment that minimizes the
message sum. If this assignment respects the poten-
tial property then it is the optimal assignment. Oth-
erwise, we compute the property-respecting assign-
ment (PRA), the assignment with the (approximate)
lowest message sum under the condition that the po-
tential property holds. From these two assignments
we select the one with the higher score.
Finding the MMA is simple, as it is the minimum-
message assignment of each unobserved variable
separately. However, finding the global optimal
PRA is computationally demanding, as it requires
searching over a very large assignment space. We
therefore trade accuracy for efficiency and restrict
each potential to modify the MMA assignment for
only one type of variables: Y (fields) or Z (events).
The discourse potentials and the FIELD SPARSITY
potential are restricted to changes of the event vari-
ables, while the RECORD DENSITY potential is re-
stricted to changes of the field variables.
For the MAIN EVENT potential, consecutive sen-
tences with no fields trigger a return to the main
event. For the SEGMENT BOUNDARY potential,
event changes that take place in sentences with a
small number of fields are removed. For our work,
this threshold is set to three. For the EVENT RE-
DUNDANCY potential, redundant events are inte-
grated with the largest event in which they are con-
tained. For the RECORD DENSITY potential, words
seen in both training records and event text are used
to fill empty fields. For each empty field in each
event, words labeled with event are scanned for can-
didate fillers, and those with the minimal impact on
the message sum are assigned to that field.
Finally, for the FIELD SPARSITY potential, if a
field contains more than one word or phrase per
event, the event assignments of these words or
phrases are recomputed. This computation is imple-
mented as a minimum matching problem in a bipar-
tite graph. One side of the graph consists of a vertex
for every word or phrase assigned to the addressed
field, and the other side consists of one vertex for
each event in the document. If the number of phrases
assigned to the field is larger than the number of
events in the document, some of the event vertices
will be assigned to new events. The edge weights
are the sum of message changes corresponding to
relabeling the word or phrase with the new event.
We solve this problem efficiently (O(n3)) using the
Kuhn-Munkres algorithm (Kuhn, 1955).
5 Experiments
Data This work focuses on multi-event extraction.
While some of the articles in the MUC test corpus
do have multiple events, the majority contain only
one (77.5%) or two (12%). We therefore created two
corpora for our experiments. The first is a new cor-
pus of 70 articles from New York Times (NYT) LDC
corpus, each describing one or more terrorist events
from various parts of the world. The second, also of
70 articles, consists of a subset of the MUC articles
that describe more than one event. We stripped this
corpus from the MUC annotation and annotated it
according to our scheme.
Annotations were provided by two annotators
with graduate school educations. Every word was
tagged with a field and an event id. The 8 fields
we use are: Terrorist Organization, Target, Tactic,
Weapon, Fatalities, Injuries, Country and City.
We compared the agreement between annotators
on 10 articles by computing the percentage of words
for which the annotators gave the same labeling.
The inter-annotator agreement was 90.9% (kappa =
0.9) when fields and events are evaluated together
(i.e., the annotators are considered to agree only
when they assign the same field and event id to the
word), 97.8% (kappa = 0.97) for events only, and
92% (kappa = 0.91) for fields only.
The two corpora differ from each other with re-
spect to several important properties. The New-York
Times articles are longer (40.3 compared to 12.4
sentences per article) and describe a larger number
of events (4.4 compared to 3.1 events per article on
average). In addition, while our hypothesis about
the predominance of the main (first) event cover-
age holds for both corpora, it better characterizes the
New-York Times corpus, as is demonstrated by the
following two statistics.
First, in the NYT corpus the average number of
sentences containing field fillers for the main event
is 14.7, while for any other event the average number
75
is 3.2. In the MUC corpus the corresponding num-
bers are 5.3 and 2.0. Second, in the NYT corpus
the number of times an article goes back to a pre-
viously described event is 182 (average of 2.6 times
per article), of which 154 (84.6%) are transitions to
the main event. In the MUC corpus the number of
times an article goes back to a previously described
event is only 38 (average of 0.54 times per article),
but, similarly to the NYT, in as much as 32 (84.2%)
of these cases the transitions are to the main event.
Experimental Setup For both corpora, we used 30
articles for training (1218 sentences in NYT, 423 in
MUC), 7 articles for development (358 sentences in
NYT, 79 in MUC) and 33 articles for test (1244 sen-
tences in NYT, 367 in MUC). The sentences were
POS tagged with the MXPOST tagger (Ratnaparkhi,
1996) and parsed with the Charniak parser (Char-
niak and Johnson, 2005).
We trained our model with a two steps procedure.
First, the local CRFs were separately trained on the
training articles. Then, we trained the parameters of
the global potentials using the structured perceptron
algorithm (Collins, 2002) on the development data.
We perform joint inference over the local CRFs
as well as the global potentials with dual decompo-
sition. This algorithm is guaranteed to give the MAP
assignment if it converges to a solution in which all
the potentials agree on the label assignment for the
variables in their scope. To deal with disagreements,
we ran the algorithm for 200 iterations past the point
of fluctuations around the dual minimum. The final
label assignment is determined by a majority vote
between the potentials in the 10 iterations with the
highest total inter potential agreement (Sontag et al,
2010).
Baselines We compare our algorithm to two base-
line models. The first baseline is related to previous
techniques that decompose the task into field extrac-
tion and event segmentation sub-tasks (Jean-Louis
et al, 2011; Patwardhan and Riloff, 2007; Patward-
han and Riloff, 2009). For this PIPELINE baseline,
we run the CRF models described in Section 3.1,
first the field CRF and then the event CRF. The field-
based features of the event CRF are extracted from
the output of the field CRF.
Our model incorporates global dependencies into
a document level model. An alternative approach is
to encode this information as local features that re-
flect global dependencies (Liang et al, 2008). We
therefore constructed a second baseline, the bidirec-
tional pipeline model (BI-PIPELINE), that considers
global features which encode similar properties to
those encouraged by our global potentials. We im-
plement this by incorporating event-based features
into the feature set of the field labeling CRF, while
kipping the event segmentation CRF fixed. 2 As in
the pipeline model, each CRF is trained separately
on the training data. The BI-PIPELINE model, how-
ever, emulates our joint inference procedure by it-
eratively running a field labeling and an event seg-
mentation CRFs. The number of iterations for this
model was estimated on development data.
Evaluation Measures We follow the MUC-4
scoring guidelines (Chinchor, 1992). To compare
between a learned and a gold standard event, we
compute the word-level F-score between each of
their fields and average the results. If a field is empty
in both event records, it is not counted in the mutual
event score, while if it is empty in only one of the
event records, its F-score is 0.
Ideally, the measure should be able to capture
paraphrases. For example, if the Tactic field in
a gold event record contains the words ?bombing?
and ?blast?, the measure is expected to give a per-
fect score to a learned record that contains one of
these words. Therefore, as in the MUC-4 guidelines,
we count pre-specified synonyms and morphologi-
cal derivations of the same word only once.
For every document, we then map the learned
events to the gold events in a greedy 1-1 manner
using the Kuhn-Munkres algorithm (Kuhn, 1955).
Once we have an event mapping, we can report
an average recall, precision and F-score across the
test set for all fields, events and documents (where
the document F-score is the average F-score of its
events). We use the sign test to measure the statis-
tical significance for our results. Since the number
of events described in a document is not given to the
models as input, we also report the average ratio be-
tween the number of induced and gold events.
2Example additional features are: (1) whether a word with
the same most frequent field (MFF) as the encoded word previ-
ously appeared in its event; (2) whether a new event is started
in the sentence of the encoded word; and (3) whether the event
of the encoded word contains at least one word annotated with
the MFF of the encoded word.
76
NYT Documents Events Fields Event Number
R P F R P F R P F Ratio
Joint Model 38.7 42.4 38.5 36.2 40.8 36.4 43.6 49.1 43.8 0.95
Bi-pipeline Model 33.3 30.8 30.2 31.9 30.1 29.4 38.8 36.6 35.7 1.14
Pipeline Model 28.3 27.0 26.2 27.1 26.8 25.5 35.4 34.8 33.2 1.5
MUC Documents Events Fields Event Number
R P F R P F R P F Ratio
Joint Model 49.8 43.2 43.5 48.7 43.0 42.7 53.6 45.9 46.2 0.88
Bi-pipeline Model 38.1 38.6 36.3 34.3 33.9 32.2 41.5 40.5 38.6 0.92
Pipeline Model 30.8 32.8 29.7 29.9 32.0 28.9 37.9 40.1 36.6 0.89
Table 2: Performance of the joint model and the pipeline models on the event record extraction task. Top table is for
the New-York Times data. Bottom table is for the MUC data. All results are statistically significant with p < 0.05.
NYT TO TAR TAC WEAP INJ FAT CO CITY
Joint Model 21.9 23.4 49.0 39.6 40.8 49.1 43.1 46.6
Bi-pipeline Model 8.4 19.7 47.5 20.9 25.9 18.3 38.8 38.1
Pipeline Model 7.1 18.1 41.9 36.9 19.1 16.5 38.0 46.1
MUC TO TAR TAC WEAP INJ FAT CO CITY
Joint Model 49.0 25.2 63.6 62.0 43.3 21.1 19.7 38.3
Bi-pipeline Model 28.0 24.7 38.2 55.8 42.7 25.6 37.5 37.2
Pipeline Model 34.9 23.4 50.3 56.5 10.4 12.4 30.0 32.0
Table 3: Comparison between the joint model and the pipeline models for the different fields. When the joint model is
superior results are statistically significance with p < 0.05.
(a)
NYT Fields Events
R P F GF LF
Joint model 47.3 51.3 49.2 54.8 61.3
Bi-Pipeline 31.0 43.8 36.3 48.8 56.2
Pipeline Model 39.2 55.4 45.9 51.3 52.9
(b)
MUC Fields Events
R P F GF LF
Joint model 47.3 51.3 49.2 62.8 70.0
Bi-Pipeline 49.5 36.1 41.8 62.2 62.0
Pipeline Model 31.0 43.8 36.3 65.5 70.3
Table 4: Performance of the joint and the pipeline models on the labeling tasks of assigning words to fields (left) and
to events (right). Field values are computed for words tagged with the non-NULL field. Events values are computed
for words that are assigned to a non-NULL field by the gold standard (GF) or by the model (LF). When the joint model
is superior, results for fields are statistically significant with p < 0.01 and for events with p < 0.05.
6 Results
Event-Records Results for event record extraction,
the main task addressed in this paper, are presented
in Table 2. For all measures, the model outperforms
the pipeline baselines, with an F-score difference of
up to 13.8%.
The rightmost column of the table demonstrates
the tendency of our model to under-segment. For
both corpora our model extracts a smaller number
of events than the gold standard on average (5% for
NYT, 12% for MUC). The pipeline baselines extract
more events than our model on average. For NYT
they over-segment (14% for bi-pipeline, 53% for the
pipeline) while for MUC they under-segment (8%
and 11% respectively). These differences are ex-
pected as the baselines cannot combine different text
segments that describe the same event.
Table 3 presents per-field F-score performance.
The joint model outperforms the pipeline baselines
for 7 out of the 8 fields in the NYT experiments, and
for 6 out of 8 fields in the MUC experiments.
Model Components Table 6 presents the perfor-
mance of variants of the joint model created by ex-
cluding each potential type. The results demonstrate
the significance of both discourse and record co-
herence potentials for the performance of the full
model.
Sub-tasks Performance A model for our task
77
(a)
Gold Fields Gold Events
NYT Doc. Events Fields Ratio Doc. Events Fields
Joint
Model
69.1 62.5 64.4 1.05 45.7 46.5 50.0
Bi-
Pipeline
? ? ? ? 41.7 40.8 46.1
Pipeline 47.9 43.9 51.3 1.56 40.8 40.4 43.9
(b)
Gold Fields Gold Events
MUC Doc. Events Fields Ratio Doc. Events Fields
Joint
model
78.5 75.0 74.5 0.76 50.8 47.9 51.4
Bi-
Pipeline
? ? ? ?- 37.0 34.3 39.9
Pipeline 76.1 71.1 72.0 0.78 32.6 31.2 36.0
Table 5: Performance of the joint model and the pipeline models when the gold standard for one of the labeling tasks
is given at test time. Results are statistically significant with p < 0.05.
NYT
Excluded Component Documents Events Fields Event
Rat.
Record Coherence 32.1 31.0 37.7 1.04
Discourse 26.7 26.3 34.3 1.5
MUC
Record Coherence 37.4 33.6 39.6 0.88
Discourse 37.7 36.6 42.7 0.89
Table 6: The effect of the record coherence potentials and
of the discourse potentials on the performance of the joint
model. Results are presented for F-scores, each line is for
the full model when potentials of one type are excluded.
should determine both when a word is a good field
filler and to which event the field belongs. Since
our main evaluation collapses the effect of these de-
cisions together, we performed two additional sets
of experiments to analyze the model?s accuracy on
each sub-task separately.
Figure 4 presents the performance of the different
models on the labeling tasks of assigning words to
fields and to events. The number of words associated
with a field differs between the gold standard and
the models? output. For fields, we therefore report
word level recall, precision and F-score between the
set of words assigned a non-NULL field by a model
and the corresponding gold standard set. For events,
we compute the fraction of words assigned the cor-
rect event among the words assigned to a non-NULL
field in either the gold standard or the output of the
model.
Figure 5 presents the document F-score when the
gold-standard fields (left) or events (right) of the test
set are known at test time. Note that when the gold
standard fields are known, the BI-PIPELINE model
is not applicable anymore since it is designed to
improve field assignment using event-informed fea-
tures. The results demonstrate that encoding field
information to the models is more valuable than en-
coding information about events. This provides us
with an important direction for future improvement
of our model.
Accuracy and Efficiency When we ran our algo-
rithm on the joint task of the NYT data-set it con-
verged after 89 iterations. For the MUC joint task
and the ablation analysis experiments we ran the al-
gorithm for 200 iterations past the point of fluctua-
tions around the dual minimum.
On a 2GHz CPU, 2GB RAM machine, it took
our dual-decomposition algorithm 15 minutes and
10 seconds to complete its run on the entire NYT test
set. For the MUC joint task experiment, in the 10
iterations considered for the majority vote, there is
full agreement between the potentials for 97.77% of
the unobserved variables. That is, the voting scheme
affects the assignment of only 2.23% of the unob-
served variables.
7 Conclusions
In this paper we presented a joint model for identify-
ing fields of information and aggregating them into
event records. We experimented with two data sets
of newspaper articles containing multiple event de-
scriptions. Our results demonstrate the importance
and effectiveness of global constraints for event
record extraction.
Acknowledgements
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C0172. Any opin-
ions, findings and conclusions expressed in the ma-
terial are those of the author(s) and do not neces-
sarily reflect the views of DARPA, AFRL or the US
government. Thanks also to the members of the MIT
NLP group and to Amir Globerson for their sugges-
tions and comments.
78
References
Kedar Bellare and Andrew McCallum. 2009. General-
ized expectation criteria for bootstrapping extractors
using record-text alignment. In EMNLP.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
ACL.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint driven learn-
ing. In ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In ACL.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In ACL.
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19(3):409?449.
Nancy Chinchor. 1992. Muc-4 evaluation metrics. In
Fourth Message Understanding Conference (MUC-4).
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In EMNLP.
Gregory Druck and Andrew McCallum. 2010. High-
performance semi-supervised learning using discrimi-
natively constrained generative models. In ICML.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In
NAACL.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings of the
4th DARPA Speech and Natural Language Workshop.
Ludovic Jean-Louis, Romaric Besancon, and Olivier Fer-
ret. 2011. Text segmentation and graph-based meth-
ods for template filling in information extraction. In
IJCNLP.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
EMNLP.
Harold W. Kuhn. 1955. The hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83?97.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Percy Liang, Hal Daume, and Dan Klein. 2008. Struc-
ture compilation: trading structure for features. In
ICML.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extraction
from free text. In ACL.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In EMNLP.
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In EMNLP.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In WVLC.
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Muc-4 test results and analysis.
In Fourth Message Understanding Conference (MUC-
4).
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
EMNLP.
Dan Roth and Wen tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In CoNLL.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In EMNLP.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2010. Introduction to dual decomposition for infer-
ence. In Optimization for Machine Learning, editors
S. Sra, S. Nowozin, and S. J. Wright: MIT Press.
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules for
weakly supervised information extraction. In COL-
ING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction-
without labelled data. In EMNLP.
79
Proceedings of NAACL-HLT 2013, pages 928?937,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Information Structure Analysis of Scientific Documents Through
Discourse and Lexical Constraints
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Roi Reichart
University of Cambridge, UK
rr439@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Abstract
Inferring the information structure of scien-
tific documents is useful for many down-
stream applications. Existing feature-based
machine learning approaches to this task re-
quire substantial training data and suffer from
limited performance. Our idea is to guide
feature-based models with declarative domain
knowledge encoded as posterior distribution
constraints. We explore a rich set of discourse
and lexical constraints which we incorporate
through the Generalized Expectation (GE) cri-
terion. Our constrained model improves the
performance of existing fully and lightly su-
pervised models. Even a fully unsupervised
version of this model outperforms lightly su-
pervised feature-based models, showing that
our approach can be useful even when no la-
beled data is available.
1 Introduction
Techniques that enable automatic analysis of the in-
formation structure of scientific articles can help sci-
entists identify information of interest in the grow-
ing volume of scientific literature. For example,
classification of sentences according to argumenta-
tive zones (AZ) ? an information structure scheme
that is applicable across scientific domains (Teufel
et al, 2009) ? can support information retrieval, in-
formation extraction and summarization (Teufel and
Moens, 2002; Tbahriti et al, 2006; Ruch et al,
2007; Liakata et al, 2012; Contractor et al, 2012).
Previous work on sentence-based classification of
scientific literature according to categories of infor-
mation structure has mostly used feature-based ma-
chine learning, such as Support Vector Machines
(SVM) and Conditional Random Fields (CRF) (e.g.
(Teufel and Moens, 2002; Lin et al, 2006; Hiro-
hata et al, 2008; Shatkay et al, 2008; Guo et al,
2010; Liakata et al, 2012)). Unfortunately, the per-
formance of these methods is rather limited, as indi-
cated e.g. by the relatively low numbers reported by
Liakata et al (2012) in biochemistry and chemistry
with per-class F-scores ranging from .18 to .76.
We propose a novel approach to this task in which
traditional feature-based models are augmented with
explicit declarative expert and domain knowledge,
and apply it to sentence-based AZ. We explore two
sources of declarative knowledge for our task - dis-
course and lexical. One way to utilize discourse
knowledge is to guide the model predictions by en-
coding a desired predicted class (i.e. information
category) distribution in a given position in the doc-
ument. Consider, for example, sentence (1) from the
first paragraph of the Discussion section in a paper:
(1) In time, this will prove to be most suitable for
detailed analysis of the role of these hormones in
mammary cancer development.
Although the future tense and cue phrases such as
?in time? can indicate that authors are discussing fu-
ture work (i.e. the ?Future work? class in the AZ
scheme), in this case they refer to their own contri-
bution (i.e. the ?Conclusion? class in AZ). As most
authors discuss their own contribution in the begin-
ning of the Discussion section and future directions
in the end, encoding the desired class distribution as
a function of the position in this section can guide
the model to the right decision.
Likewise, lexical knowledge can guide the model
928
through predicted class distributions for sentences
that contain specific vocabulary. Consider, for ex-
ample, sentence (2):
(2) The values calculated for lungs include the
presumed DNA adduct of BA and might thus be
slightly overestimated.
The verb ?calculated? usually indicates the
?Method? class, but, when accompanied by the
modal verb ?might?, it is more likely to imply that
authors are interpreting their own results (i.e. the
?Conclusion? class in AZ). This can be explicitly
encoded in the model through a target distribution
for sentences containing certain modal verbs.
Recent work has shown that explicit declaration
of domain and expert knowledge can be highly use-
ful for structured NLP tasks such as parsing, POS
tagging and information extraction (Chang et al,
2007; Mann and McCallum, 2008; Ganchev et al,
2010). These works have encoded expert knowledge
through constraints, with different frameworks dif-
fering in the type of constraints and the inference
and learning algorithms used. We build on the Gen-
eralized Expectation (GE) framework (Mann and
McCallum, 2007) which encodes expert knowledge
through a preference (i.e. soft) constraints for pa-
rameter settings for which the predicted label distri-
bution matches a target distribution.
In order to integrate domain knowledge with a
features-based model, we develop a simple taxon-
omy of constraints (i.e. desired class distributions)
and employ a top-down classification algorithm on
top of a Maximum Entropy Model augmented with
GE constraints. This algorithm enables us to break
the multi-class prediction into a pipeline of consecu-
tive, simpler predictions which can be better assisted
by the encoded knowledge.
We experiment in the biological domain with the
eight-category AZ scheme (Table 1) adapted from
(Mizuta et al, 2006) and described in (Contractor
et al, 2012). The results show that our constrained
model substantially outperforms a baseline uncon-
strained Maximum Entropy Model. While this type
of constrained models have previously improved
the feature-based model performance mostly in the
weakly supervised and domain adaptation scenarios
(e.g. (Mann and McCallum, 2007; Mann and Mc-
Callum, 2008; Ganchev et al, 2010)), we demon-
strate substantial gains both when the Maximum En-
Table 1: The AZ categories included in the categorization
scheme of this paper.
Zone Definition
Background (BKG) the background of the study
Problem (PROB) the research problem
Method (METH) the methods used
Result (RES) the results achieved
Conclusion (CON) the authors? conclusions
Connection (CN) work consistent with the current work
Difference (DIFF) work inconsistent with the current work
Future work (FUT) the potential future direction of the research
tropy Model is fully trained and when its training
data is sparse. This demonstrates the importance of
expert knowledge for our task and supports our mod-
eling decision that combines feature-based methods
with domain knowledge encoded via constraints.
2 Previous work
Information structure analysis The information
structure of scientific documents (e.g. journal ar-
ticles, abstracts, essays) can be analyzed in terms
of patterns of topics, functions or relations observed
in multi-sentence scientific text. Computational ap-
proaches have mainly focused on analysis based
on argumentative zones (Teufel and Moens, 2002;
Mizuta et al, 2006; Hachey and Grover, 2006;
Teufel et al, 2009), discourse structure (Burstein et
al., 2003; Webber et al, 2011), qualitative dimen-
sions (Shatkay et al, 2008), scientific claims (Blake,
2009), scientific concepts (Liakata et al, 2010) and
information status (Markert et al, 2012).
Most existing methods for analyzing scientific
text according to information structure use full
supervision in the form of thousands of manu-
ally annotated sentences (Teufel and Moens, 2002;
Burstein et al, 2003; Mizuta et al, 2006; Shatkay
et al, 2008; Guo et al, 2010; Liakata et al, 2012;
Markert et al, 2012). Because manual annotation is
prohibitively expensive, approaches based on light
supervision are now emerging for the task, including
those based on active learning and self-training (Guo
et al, 2011) and unsupervised methods (Varga et al,
2012; Reichart and Korhonen, 2012). Unfortunately,
these approaches do not reach the performance level
of fully supervised models, let alne exceed it. Our
novel method addresses this problem.
Declarative knowledge and constraints Previ-
ous work has shown that incorporating declara-
tive constraints into feature-based machine learning
929
models works well in many NLP tasks (Chang et
al., 2007; Mann and McCallum, 2008; Druck et al,
2008; Bellare et al, 2009; Ganchev et al, 2010).
Such constraints can be used in a semi-supervised or
unsupervised fashion. For example, (Mann and Mc-
Callum, 2008) shows that using CRF in conjunction
with auxiliary constraints on unlabeled data signif-
icantly outperforms traditional CRF in information
extraction, and (Druck et al, 2008) shows that using
declarative constraints alone for unsupervised learn-
ing achieves good results in text classification. We
show that declarative constraints can be highly use-
ful for the identification of information structure of
scientific documents. In contrast with most previous
works, we show that such constraints can improve
the performance of a fully supervised model. The
constraints are particularly helpful for identifying
low-frequency information categories, but still yield
high performance on high-frequency categories.
3 Maximum-Entropy Estimation and
Generalized Expectation (GE)
In this section we describe the Generalized Expecta-
tion method for declarative knowledge encoding.
Maximum Entropy (ME) The idea of General-
ized Expectation (Dud??k, 2007; Mann and McCal-
lum, 2008; Druck et al, 2008) stems from the prin-
ciple of maximum entropy (Jaynes, 1957; Pietra and
Pietra, 1993) which raises the following constrained
optimization problem:
max
p
H(?)
subject to Ep[f(?)] = Ep?[f(?)]
p(?) ? 0
?
p(?) = 1, (1)
where p?(?) is the empirical distribution, p(?) is a
probability distribution in the model and H(?) is the
corresponding information entropy, f(?) is a collec-
tion of feature functions, and Ep[f(?)] and Ep?[f(?)]
are the expectations of f with respect to p(?) and
p?(?). An example of p(?) could be a conditional
probability distribution p(y|x), and H(?) could be
a conditional entropy H(y|x). The optimal p(y|x)
will take on an exponential form:
p?(y|x) =
1
Z?
exp(? ? f(x, y)), (2)
where ? is the Lagrange multipliers in the corre-
sponding unconstrained objective function, and Z?
is the partition function. The dual problem be-
comes maximizing the conditional log-likelihood of
labeled data L (Berger et al, 1996):
max
?
?
(xi,yi)?L
log(p?(yi|xi)), (3)
which is usually known as a Log-linear or Maximum
Entropy Model (MaxEnt).
ME with Generalized Expectation The objec-
tive function and the constraints on expectations in
(1) can be generalized to:
max
?
?
?
x
p?(x)D(p?(y|x)||p0(y|x))
? g(Ep?(x)[Ep?(y|x)[f(x, y)|x]]), (4)
where D(p?||p0) is the divergence from p? to a base
distribution p0, and g(?) is a constraint/penalty func-
tion that takes empirical evidence Ep?(x,y)[f(x, y)] as
a reference point (Pietra and Pietra, 1993; Chen et
al., 2000; Dud??k, 2007). Note that a special case of
this is MaxEnt where p0 is set to be a uniform distri-
bution, D(?) to be the KL divergence, and g(?) to be
an equality constraint.
The constraint g(?) can be set in a relaxed manner:
?
k
1
2?2k
(Ep?(x)[Ep?(y|x)[fk(x, y)|x]]? Ep?(x,y)[fk(x, y)])
2,
which is the logarithm of a Gaussian distribution
centered at the reference values with a diagonal co-
variance matrix (Pietra and Pietra, 1993), and the
dual problem will become a regularized MaxEnt
with a Gaussian prior (?k = 0, ?2k =
1
?2k
) over the
parameters:
max
?
?
(xi,yi)?L
log(p?(yi|xi))?
?
k
?2k
2?2k
(5)
Such a model can be further extended to include
expert knowledge or auxiliary constraints on unla-
beled data U (Mann and McCallum, 2008; Druck et
al., 2008; Bellare et al, 2009):
max
?
?
(xi,yi)?L
log(p?(yi|xi))?
?
k
?2k
2?2k
? ?g?(Ep?(y|x)[f
?(x, y)]) (6)
where f?(?) is a collection of auxiliary feature func-
tions on U , g?(?) is a constraint function that takes
expert/declarative knowledge Ep?(y|x)[f
?(x, y)] as a
reference point, and ? is the weight of the auxiliary
GE term.
930
The auxiliary constraint g?(?) can take on many
forms and the one we used in this work is an L2
penalty function (Dud??k, 2007). We trained the
model with L-BFGS (Nocedal, 1980) in supervised,
semi-supervised and unsupervised fashions on la-
beled and/or unlabeled data, using the Mallet soft-
ware (McCallum, 2002).
4 Incorporating Expert Knowledge into
GE constraints
We defined the auxiliary feature functions ? the ex-
pert knowledge on unlabeled data as1:
f?k (x, y) = 1(xk,yk)(x, y),
such that Ep?(y|x)[fk(x, y)] = p
?(yk|xk), (7)
where 1(xk,yk)(x, y) is an indicator function, and
p?(yk|xk) is a conditional probability specified in
the form of
p?(yk|xk) ? [ak, bk] (8)
by experts. In particular, we took
p?(yk|xk) =
?
?
?
ak if p?(yk|xk) < a
bk if p?(yk|xk) > b
p?(yk|xk) if a ? p?(yk|xk) ? b
(9)
as the reference point when calculating g?(?).
We defined two types of constraints: those based
on discourse properties such as the location of a sen-
tence in a particular section or paragraph, and those
based on lexical properties such as citations, refer-
ences to tables and figures, word lists, tenses, and
so on. Note that the word lists actually contain both
lexical and semantic information.
To make an efficient use of the declarative knowl-
edge we build a taxonomy of information structure
categories centered around the distinction between
categories that describe the authors? OWN work and
those that describe OTHER work (see Section 5). In
practice, our model labels every sentence with an
AZ category augmented by one of the two cate-
gories, OWN or OTHER. In evaluation we consider
only the standard AZ categories which are part of
the annotation scheme of (Contractor et al, 2012).
1Accordingly, Ep?(y|x)[fk(x, y)] = p?(yk|xk)
Table 2: Discourse and lexical constraints for identifying infor-
mation categories at different levels of the information structure
taxonomy.
(a) OWN / OTHER
OWN Discourse
(1) Target(last part of paragraph) = 1
(2) Target(last part of section) = 1
Lexical
(3) Target(tables/figures) ? 1
(4) ?x ? {w|w?we} Target(x) = 1
? ?y ? {w|w?previous} Target(y) = 0
(5) ?x ? {w|w?thus} Target(x) = 1
OTHER Lexical
(6) Target(cite) = 1
(7) Target(cite) > 1
(8) Backward(cite) = 1
? ?x ? {w|w?in addition} Target(x) = 1
(b) PROB / METH / RES / CON / FUT
PROB Discourse
(1) Target(last part in section) = 1
Lexical
(2) ?x ? {w|w?aim} Target(x) = 1
(3) ?x ? {w|w?question} Target(x) = 1
(4) ?x ? {w|w?investigate} Target(x) = 1
METH Lexical
(5) ?x ? {w|w?{use,method}} Target(x) = 1
RES Lexical
(6) Target(tables/figures) ? 1
(7) ?x ? {w|w?observe} Target(x) = 1
CON Lexical
(8) Target(cite) ? 1
(9) ?x ? {w|w?conclude} Target(x) = 1
(10) ?x ? {w|w?{suggest, thus, because, likely}}
Target(x) = 1
FUT Discourse
(11) Target(first part in section) = 1
(12) Target(last part in section) = 1
? ?x ? {w|w?{will,need,future}} Target(x) = 1
Lexical
(13) ?x {w|w?will,future} Target(x) = 1
(14) Target(present continuous tense) = 1
(c) BKG / CN / DIFF
BKG Discourse
(1) Target(first part in paragraph) = 1
(2) Target(first part in section) = 1
Lexical
(3) ?x ? {w|w?we} Target(x) = 1
? ?y ? {w|w?previous} Target(y) = 0
(4) Forward(cite) = 1
? ?x ? {w|w?{consistent,inconsistent,than}}
(Target(x) = 0 ? Forward(x) = 0)
CN Lexical
(5) ?x ? {w|w?consistent} Target(x) = 1
(6) ?x ? {w|w?consistent} Forward(x) = 1
DIFF Lexical
(7) ?x ? {w|w?inconsistent} Target(x) = 1
(8) ?x ? {w|w?inconsistent} Forward(x) = 1
(9) ?x ? {w|w?{inconsistent,than,however}}
Forward(x) = 1 ? ?y ? {w|w?we} Forward(y) = 1
? ?z ? {w|w?previous}} Forward(z) = 0
931
Table 3: The lexical sets used as properties in the constraints.
Cue Synonyms
we our, present study
previous previously, recent, recently
thus therefore
aim objective, goal, purpose
question hypothesis, ?
investigate explore, study, test, examine, evaluate, assess, deter-
mine, characterize, analyze, report, present
use employ
method algorithm, assay
observe see, find, show
conclude conclusion, summarize, summary
suggest illustrate, demonstrate, imply, indicate, confirm, re-
flect, support, prove, reveal
because result from, attribute to
likely probable, probably, possible, possibly, may, could
need remain
future further
consistent match, agree, support, in line, in agreement, similar,
same, analogous
inconsistent conflicting, conflict, contrast, contrary, differ, differ-
ent, difference
than compare
however other hand, although, though, but
The constraints in Table 2(a) refer to the top level
of this taxonomy: distinction between the authors?
own work and the work of others, and the constraints
in Tables 2(b)-(c) refer to the bottom level of the tax-
onomy: distinction between AZ categories related to
the authors? own work (Table 2(b)) and other?s work
(Table 2(c)).
The first and second columns in each table refer
to the y and x variables in Equation (8), respectively.
The functions Target(?), Forward(?) and Backward(?)
refer to the property value for the target, next and
preceding sentence, respectively. If their value is 1
then the property holds for the respective sentence,
if it is 0, the property does not hold. In some cases
the value of such functions can be greater than 1,
meaning that the property appears multiple times in
the sentence. Terms of the form {w|w?{wi}} refer
to any word/bi-grams that have the same sense aswi,
where the actual word set we use with every example
word in Table 2 is described in Table 3.
For example, take constraints (1) and (4) in Table
2(a). The former is a standard discourse constraint
that refers to the probability that the target sentence
describes the authors? own work given that it appears
in the last of the ten parts in the paragraph. The lat-
ter is a standard lexical constraint that refers to the
probability that a sentence presents other people?s
work given that it contains any words in {we, our,
present study} and that it doesn?t contain any words
Figure 1: The constraint taxonomy for top-down modeling.
INFO [Table 2(a)]
OWN [Table 2(b)]
PROB METH RES CON FUT
OTHER [Table 2(c)]
BKG CN DIFF
in {previous, previously, recent, recently}. Our con-
straint set further includes constraints that combine
both types of information. For example, constraint
(12) in Table 2(b) refers to the probability that a sen-
tence discusses future work given that it appears in
the last of the ten parts of the section (discourse) and
that it contains at least one word in {will, future, fur-
ther, need, remain} (lexical).
5 Top-Down Model
An interesting property of our task and domain is
that the available expert knowledge does not directly
support the distinctions between AZ categories, but
it does provide valuable indirect guidance. For ex-
ample, the number of citations in a sentence is only
useful for separating the authors? work from other
people?s work, but not for further fine grained dis-
tinctions between zone categories. Moreover, those
constraints that are useful for making fine grained
distinctions between AZ categories are usually use-
ful only for a particular subset of the categories only.
For example, all the constraints in Table 2(b) are
conditioned on the assumption that the sentence de-
scribes the authors? own work.
To make the best use of the domain knowledge,
we developed a simple constraint taxonomy, and ap-
ply a top-down classification approach which uti-
lizes it. The taxonomy is presented in Figure 1. For
classification we trained three MaxEnt models aug-
mented with GE constraints: one for distinguishing
between OWN and OTHER2, one for distinguishing
between the AZ categories under the OWN auxiliary
category and one for distinguishing between the AZ
categories under the OTHER auxiliary category. At
test time we first apply the first classifier and based
on its prediction we apply either the classifier that
distinguishes between OWN categories or the one
that distinguishes between OTHER categories.
2For the training of this model, each training data AZ cate-
gory is mapped to its respective auxiliary class.
932
6 Experiments
Data We used the full paper corpus used by Contrac-
tor et al (2012) which contains 8171 sentences from
50 biomedical journal articles. The corpus is anno-
tated according to the AZ scheme described in Table
1. AZ describes the logical structure, scientific argu-
mentation and intellectual attribution of a scientific
paper. It was originally introduced by Teufel and
Moens (2002) and applied to computational linguis-
tics papers, and later adapted to other domains such
as biology (Mizuta et al, 2006) ? which we used in
this work ? and chemistry (Teufel et al, 2009).
Table 4 shows the AZ class distribution in full ar-
ticles as well as in individual sections. Since section
names vary across scientific articles, we grouped
similar sections before calculating the statistics (e.g.
Discussion and Conclusions sections were grouped
under Discussion). We can see that although there is
a major category in each section (e.g. CON in Dis-
cussion), up to 36.5% of the sentences in each sec-
tion still belong to other categories.
Features We extracted the following features
from each sentence and used them in the feature-
based classifiers: (1) Discourse features: location in
the article/section/paragraph. For this feature each
text batch was divided to ten equal size parts and the
corresponding feature value identifies the relevant
part; (2) Lexical features: number of citations and
references to tables and figures (0, 1, or more), word,
bi-gram, verb, and verb class (obtained by spectral
clustering (Sun and Korhonen, 2009)); (3) Syntac-
tic features: tense and voice (POS tags of main and
auxiliary verbs), grammatical relation, subject and
object. The lexical and the syntactic features were
extracted for the represented sentence as well as for
its surrounding sentences. We used the C&C POS
tagger and parser (Curran et al, 2007) for extract-
ing the lexical and the syntactic features. Note that
all the information encoded into our constraints is
also encoded in the features and is thus available to
the feature-based model. This enables us to properly
evaluate the impact of our modeling decision which
augments a feature-based model with constraints.
Baselines We compared our model against four
baselines, two with full supervision: Support Vec-
tor Machines (SVM) and Maximum Entropy Mod-
els (MaxEnt), and two with light supervision: Trans-
Table 4: Class distribution (shown in percentages) in articles
and their individual sections in the AZ-annotated corpus.
BKG PROB METH RES CON CN DIFF FUT
Article 16.9 2.8 34.8 17.9 22.3 4.3 0.8 0.2
Introduction 74.8 13.2 5.4 0.6 5.9 0.1 - -
Methods 0.5 0.2 97.5 1.4 0.2 0.2 0.1 -
Results 4.0 2.1 11.7 68.9 12.1 1.1 0.1 -
Discussion 16.9 1.1 0.7 1.5 63.5 13.3 2.4 0.7
Table 5: Performance of baselines on the Discussion section.
BKG PROB METH RES CON CN DIFF FUT
Full supervision
SVM .56 0 0 0 .84 .35 0 0
MaxEnt .55 .08 0 0 .84 .38 0 0
Light supervision with 150 labeled sentence
SVM .26 0 0 0 .80 .05 0 0
TSVM .25 .04 .04 .03 .33 14 .06 .02
MaxEnt .25 0 0 0 .80 .10 0 0
MaxEnt+ER .23 0 0 0 .80 .07 0 0
ductive SVM (TSVM) and semi-supervised Max-
Ent based on Entropy Regularization (ER) (Vapnik,
1998; Jiao et al, 2006). SVM and MaxEnt have
proved successful in information structure analysis
(e.g. (Merity et al, 2009; Guo et al, 2011)) but,
to the best of our knowledge, their semi-supervised
versions have not been used for AZ of full articles.
Parameter tuning The boundaries of the ref-
erence probabilities (ak and bk in Equation (8))
were defined and optimized on the development data
which consists of one third of the corpus. We con-
sidered six types of boundaries: Fairly High for
1, High for [0.9,1), Medium High for [0.5,0.9),
Medium Low for [0.1,0.5), Low for [0,0.1), and
Fairly Low for 0.
Evaluation We evaluated the precision, recall and
F-score for each category, using a standard ten-fold
cross-validation scheme. The models were tested on
each of the ten folds and trained on the rest of them,
and the results were averaged across the ten folds.
7 Results
We report results at two levels of granularity. We
first provide detailed results for the Discussion sec-
tion which should be, as is clearly evident from Ta-
ble 4, the most difficult section for AZ prediction as
only 63.5% of its sentences take its most dominant
class (CON). As we show below, this is also where
our constrained model is most effective. We then
show the advantages of our model for other sections.
Results for the Discussion section To get a bet-
933
Table 6: Discussion section performance of MaxEnt, MaxEnt+GE and a MaxEnt+GE model that does not include our top-down
classification scheme. Results are presented for different amounts of labeled training data. The MaxEnt+GE (Top-down) model
outperforms the MaxEnt in 44 out of 48 cases, and MaxEnt+GE (Flat) in 39 out of 48 cases.
MaxEnt MaxEnt + GE (Top-down) MaxEnt + GE (Flat)
50 100 150 500 1000 Full 50 100 150 500 1000 Full 50 100 150 500 1000 Full
BKG .10 .26 .25 .44 .48 .55 .49 .49 .48 .52 .55 .57 .35 .37 .37 .46 .51 .53
PROB 0 0 0 0 0 0 .38 .16 .29 .13 .30 .41 .38 .23 .19 .39 .38 .33
METH 0 0 0 0 0 0 .17 .22 .37 .35 .50 .39 .16 .17 .21 .24 .32 .29
RES 0 0 0 0 0 0 .18 .24 .58 0 0 .46 .13 .05 .21 .31 .25 .34
CON .79 .80 .80 .83 .83 .84 .77 .78 .82 .83 .84 .84 .63 .66 .68 .74 .78 .78
CN .02 .04 .10 .24 .34 .38 .29 .31 .33 .35 .40 .39 .21 .21 .24 .26 .30 .32
DIFF 0 0 0 0 0 0 .26 .25 .25 .19 .24 .21 .14 .16 .15 .14 .18 .17
FUT 0 0 0 0 0 0 .35 .38 .31 .25 .35 .31 .36 .36 .39 .33 .25 .37
Figure 2: Performance of the MaxEnt and MaxEnt+GE models on the Introduction (left), Methods (middle) and Results (right)
sections. The MaxEnt+GE model is superior.
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
 
 
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
 
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 
0
0.2
0.4
0.6
0.8
1
BKG PROB METH RES CON CN DIFF FUT
F-s
co
re
MaxEnt MaxEnt+GE
 Table 7: Discussion section performance of the MaxEnt, Max-
Ent+GE and unsupervised GE models when the former two are
trained with 150 labeled sentences. Unsupervised GE outper-
forms the standard MaxEnt model for all categories except for
CON ? the major c tegory of the section. The result pattern for
the other sections are very similar.
MaxEnt MaxEnt + GE Unsup GE
P R F P R F P R F
BKG .38 .19 .25 .49 .48 .48 .49 .44 .46
PROB 0 0 0 .38 .23 .29 .28 .38 .32
METH 0 0 0 .29 .50 .37 .08 .56 .14
RES 0 0 0 .68 .51 .58 .08 .51 .14
CON .69 .96 .80 .81 .84 .82 .74 .69 .71
CN .35 .06 .10 .39 .29 .33 .40 .13 .20
DIFF 0 0 0 .21 .30 .25 .12 .13 .12
FUT 0 0 0 .24 .44 .31 .26 .61 .36
ter understanding of the nature of the challenge we
face, Table 5 shows the F-scores of fully- and semi-
supervised SVM and MaxEnt on the Discussion sec-
tion. The dominant zone category CON, which ac-
counts for 63.5% of the section sentences, has the
highest F-scores for all methods and scenarios. Most
of the methods also identify the second and the third
most frequent zones BKG and CN, but with relatively
lower F-scores. Other low-frequency categories can
hardly be identified by any of the methods regardless
of the amount of labeled data available for training.
Note that the compared models perform quite sim-
ilarly. We therefore use the MaxEnt model, which
Table 8: Analysis of the impact of the different constraint types
for the lightly supervised and the fully supervised cases. Results
are presented for the Discussion section. Using only the lexical
constraints is generally preferable in the fully supervised case.
Combining the different constraint types is preferable for the
lightly supervised case.
Discourse Lexical Discourse+Lexical
150 Full 150 Full 150 Full
BKG .29 .55 .46 .58 .48 .57
PROB 0 0 .37 .40 .29 .41
METH 0 .11 .29 .35 .37 .39
RES 0 .06 .32 .47 .58 .46
CON .81 .84 .80 .84 .82 .84
CN .12 .34 .35 .42 .33 .39
DIFF 0 0 .21 .21 .25 .21
FUT 0 0 0 .29 .31 .31
is most naturally augmented with GE constraints, as
the baseline unconstrained model.
When adding the GE constraints we observe a
substantial performance gain, in both the fully and
the lightly supervised cases, especially for the low-
frequency categories. Table 6 presents the F-scores
of MaxEnt with and without GE constraints (?Max-
Ent+GE (Top-down)? and ?MaxEnt?) in the light
and full supervision scenarios. Incorporating GE
into MaxEnt results in a substantial F-score im-
provement for all AZ categories except for the ma-
jor category CON for which the performance is kept
very similar. In total, MaxEnt+GE (Top-down) is
934
better in 44 out of the 48 cases presented in the table.
Importantly, the constrained model provides sub-
stantial improvements for both the relatively high-
frequency classes (BKG and CN which together label
30.2% of the sentences) and for the low-frequency
classes (which together label 6.4% of the sentences).
The table also clearly demonstrates the impact of
our tree-based top-down classification scheme, by
comparing the Top-down version of MaxEnt+GE
to the standard ?Flat? version. In 39 out of 48
cases, the Top-down model performs better. In some
cases, especially for high-frequency categories and
when the amount of training data increases, un-
constrained MaxEnt even outperforms the flat Max-
Ent+GE model. The results presented in the rest of
the paper for the MaxEnt+GE model therefore refer
to its Top-down version.
All sections We next turn to the performance of
our model on the three other sections. Our exper-
iments show that augmenting the MaxEnt model
with domain knowledge constraints improves per-
formance for all the categories (either low or high
frequency), except the major section category, and
keep the performance for the latter on the same level.
Figure 2 demonstrates this pattern for the lightly su-
pervised case with 150 training sentences but the
same pattern applies to all other amounts of training
data, including the fully supervised case. Naturally,
we cannot demonstrate all these cases due to space
limitations. The result patterns are very similar to
those presented above for the Discussion section.
Unsupervised GE We next explore the quality of
the domain knowledge constraints when used in iso-
lation from a feature-based model. The objective
function of this model is identical to Equation (6)
except that the first (likelihood) term is omitted. Our
experiments reveal that this unsupervised GE model
outperforms standard MaxEnt for all the categories
except the major category of the section, when up
to 150 training sentences are used. Table 7 demon-
strates this for the Discussion section. This pattern
holds for the other scientific article sections. Even
when more than 150 labeled sentences are used, the
unsupervised model better detects the low frequency
categories (i.e. those that label less than 10% of
the sentences) for all sections. These results provide
strong evidence for the usefulness of our constraints
even when they are used with no labeled data.
Model component analysis We finally analyze
the impact of the different types of constraints on
the performance of our model. Table 8 presents the
Discussion section performance of the constrained
model with only one or the full set of constraints.
Interestingly, when the feature-based model is fully
trained the application of the lexical constraints
alone results in a very similar performance to the
application of the full set of lexical and discourse
constraints. It is only in the lightly supervised case
where the full set of constraints is required and re-
sults in the best performing model.
8 Conclusions and Future Work
We have explored the application of posterior dis-
course and lexical constraints for the analysis of the
information structure of scientific documents. Our
results are strong. Our constrained model outper-
forms standard feature-based models by a large mar-
gin in both the fully and the lightly supervised cases.
Even an unsupervised model based on these con-
straints provides substantial gains over feature-based
models for most AZ categories.
We provide a detailed analysis of the results
which reveals a number of interesting properties of
our model which may be useful for future research.
First, the constrained model significantly outper-
forms its unconstrained counterpart for low-medium
frequency categories while keeping the performance
on the major section category very similar to that of
the baseline model. Improved modeling of the major
category is one direction for future research. Sec-
ond, our full constraint set is most beneficial in the
lightly supervised case while the lexical constraints
alone yield equally good performance in the fully
supervised case. This calls for better understand-
ing of the role of discourse constraints for our task
as well as for the design of additional constraints
that can enhance the model performance either in
combination with the existing constraints or when
separately applied to the task. Finally, we demon-
strated that our top-down tree classification scheme
provides a substantial portion of our model?s impact.
A clear direction for future research is the design of
more fine-grained constraint taxonomies which can
enable efficient usage of other constraint types and
can result in further improvements in performance.
935
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
UAI ?09, pages 43?50, Arlington, Virginia, United
States. AUAI Press.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.,
22(1):39?71.
Catherine Blake. 2009. Beyond genes, proteins, and
abstracts: Identifying scientific claims from full-text
biomedical articles. J Biomed Inform, 43(2):173?89.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the write stuff: Automatic identification of
discourse structure in student essays. IEEE Intelligent
Systems, 18(1):32?39.
M.W. Chang, L. Ratinovc, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
Stanley F. Chen, Ronald Rosenfeld, and Associate Mem-
ber. 2000. A survey of smoothing techniques for me
models. IEEE Transactions on Speech and Audio Pro-
cessing, 8:37?50.
Danish Contractor, Yufan Guo, and Anna Korhonen.
2012. Using argumentative zones for extractive sum-
marization of scientific articles. In COLING.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguisti-
cally motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Session,
pages 33?36.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602.
Miroslav Dud??k. 2007. Maximum entropy density
estimation and modeling geographic distributions of
species. Ph.D. thesis.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins
Karolinska, Lin Sun, and Ulla Stenius. 2010. Identi-
fying the information structure of scientific abstracts:
an investigation of three different schemes. In Pro-
ceedings of BioNLP, pages 99?107.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 273?283.
Ben Hachey and Claire Grover. 2006. Extractive sum-
marisation of legal texts. Artif. Intell. Law, 14:305?
345.
K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka.
2008. Identifying sections in scientific abstracts us-
ing conditional random fields. In Proceedings of 3rd
International Joint Conference on Natural Language
Processing, pages 381?388.
E. T. Jaynes. 1957. Information Theory and Statistical
Mechanics. Physical Review Online Archive (Prola),
106(4):620?630.
F. Jiao, S. Wang, C. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL, pages 209?216.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of LREC?10.
Maria Liakata, Shyamasree Saha, Simon Dobnik, Colin
Batchelor, and Dietrich Rebholz-Schuhmann. 2012.
Automatic recognition of conceptualisation zones in
scientific articles and two life science applications.
Bioinformatics, 28:991?1000.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings of
BioNLP-06, pages 65?72.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regular-
ization. In ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In ACL.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of ACL 2012, pages 795?804.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate
argumentative zoning with maximum entropy models.
In Proceedings of the 2009 Workshop on Text and Ci-
tation Analysis for Scholarly Digital Libraries, pages
19?26.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2006.
Zone analysis in biology articles as a basis for in-
formation extraction. International Journal of Med-
ical Informatics on Natural Language Processing in
Biomedicine and Its Applications, 75(6):468?487.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices
with Limited Storage. Mathematics of Computation,
35(151):773?782.
936
S. Della Pietra and V. Della Pietra. 1993. Statistical mod-
eling by me. Technical report, IBM.
Roi Reichart and Anna Korhonen. 2012. Document and
corpus level inference for unsupervised and transduc-
tive learning of information structure of scientic docu-
ments. In Proceedings of COLING 2012.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. Int J Med Inform, 76(2-3):195?200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008.
Multi-dimensional classification of biomedical text:
Toward automated, practical provision of high-utility
text to diverse users. Bioinformatics, 24(18):2086?
2093.
L. Sun and A. Korhonen. 2009. Improving verb cluster-
ing with automatically acquired selectional preference.
In Proceedings of EMNLP, pages 638?647.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75(6):488?495.
S. Teufel and M. Moens. 2002. Summarizing scien-
tific articles: Experiments with relevance and rhetor-
ical status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards discipline-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In EMNLP.
V. N. Vapnik. 1998. Statistical learning theory. Wiley,
New York.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone iden-
tification using probabilistic graphical models. In
Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12).
B. Webber, M. Egg, and V. Kordoni. 2011. Discourse
structure and language technology. Natural Language
Engineering, 18:437?490.
937
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improved Unsupervised POS Induction through Prototype Discovery
Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science, 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
We present a novel fully unsupervised al-
gorithm for POS induction from plain text,
motivated by the cognitive notion of proto-
types. The algorithm first identifies land-
mark clusters of words, serving as the
cores of the induced POS categories. The
rest of the words are subsequently mapped
to these clusters. We utilize morpho-
logical and distributional representations
computed in a fully unsupervised manner.
We evaluate our algorithm on English and
German, achieving the best reported re-
sults for this task.
1 Introduction
Part-of-speech (POS) tagging is a fundamental
NLP task, used by a wide variety of applications.
However, there is no single standard POS tag-
ging scheme, even for English. Schemes vary
significantly across corpora and even more so
across languages, creating difficulties in using
POS tags across domains and for multi-lingual
systems (Jiang et al, 2009). Automatic induction
of POS tags from plain text can greatly alleviate
this problem, as well as eliminate the efforts in-
curred by manual annotations. It is also a problem
of great theoretical interest. Consequently, POS
induction is a vibrant research area (see Section 2).
In this paper we present an algorithm based
on the theory of prototypes (Taylor, 2003), which
posits that some members in cognitive categories
are more central than others. These practically de-
fine the category, while the membership of other
elements is based on their association with the
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
central members. Our algorithm first clusters
words based on a fine morphological representa-
tion. It then clusters the most frequent words,
defining landmark clusters which constitute the
cores of the categories. Finally, it maps the rest
of the words to these categories. The last two
stages utilize a distributional representation that
has been shown to be effective for unsupervised
parsing (Seginer, 2007).
We evaluated the algorithm in both English and
German, using four different mapping-based and
information theoretic clustering evaluation mea-
sures. The results obtained are generally better
than all existing POS induction algorithms.
Section 2 reviews related work. Sections 3 and
4 detail the algorithm. Sections 5, 6 and 7 describe
the evaluation, experimental setup and results.
2 Related Work
Unsupervised and semi-supervised POS tagging
have been tackled using a variety of methods.
Schu?tze (1995) applied latent semantic analysis.
The best reported results (when taking into ac-
count all evaluation measures, see Section 5) are
given by (Clark, 2003), which combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al, 1992). Clark?s tagger is very sen-
sitive to its initialization. Reichart et al (2010b)
propose a method to identify the high quality runs
of this algorithm. In this paper, we show that
our algorithm outperforms not only Clark?s mean
performance, but often its best among 100 runs.
Most research views the task as a sequential la-
beling problem, using HMMs (Merialdo, 1994;
Banko and Moore, 2004; Wang and Schuurmans,
2005) and discriminative models (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Several
1298
techniques were proposed to improve the HMM
model. A Bayesian approach was employed by
(Goldwater and Griffiths, 2007; Johnson, 2007;
Gao and Johnson, 2008). Van Gael et al (2009)
used the infinite HMM with non-parametric pri-
ors. Grac?a et al (2009) biased the model to induce
a small number of possible tags for each word.
The idea of utilizing seeds and expanding them
to less reliable data has been used in several pa-
pers. Haghighi and Klein (2006) use POS ?pro-
totypes? that are manually provided and tailored
to a particular POS tag set of a corpus. Fre-
itag (2004) and Biemann (2006) induce an ini-
tial clustering and use it to train an HMM model.
Dasgupta and Ng (2007) generate morphological
clusters and use them to bootstrap a distributional
model. Goldberg et al (2008) use linguistic con-
siderations for choosing a good starting point for
the EM algorithm. Zhao and Marcus (2009) ex-
pand a partial dictionary and use it to learn dis-
ambiguation rules. Their evaluation is only at the
type level and only for half of the words. Ravi
and Knight (2009) use a dictionary and an MDL-
inspired modification to the EM algorithm.
Many of these works use a dictionary provid-
ing allowable tags for each or some of the words.
While this scenario might reduce human annota-
tion efforts, it does not induce a tagging scheme
but remains tied to an existing one. It is further
criticized in (Goldwater and Griffiths, 2007).
Morphological representation. Many POS in-
duction models utilize morphology to some ex-
tent. Some use simplistic representations of termi-
nal letter sequences (e.g., (Smith and Eisner, 2005;
Haghighi and Klein, 2006)). Clark (2003) models
the entire letter sequence as an HMM and uses it
to define a morphological prior. Dasgupta and Ng
(2007) use the output of the Morfessor segmenta-
tion algorithm for their morphological representa-
tion. Morfessor (Creutz and Lagus, 2005), which
we use here as well, is an unsupervised algorithm
that segments words and classifies each segment
as being a stem or an affix. It has been tested on
several languages with strong results.
Our work has several unique aspects. First,
our clustering method discovers prototypes in a
fully unsupervised manner, mapping the rest of
the words according to their association with the
prototypes. Second, we use a distributional repre-
sentation which has been shown to be effective for
unsupervised parsing (Seginer, 2007). Third, we
use a morphological representation based on sig-
natures, which are sets of affixes that represent a
family of words sharing an inflectional or deriva-
tional morphology (Goldsmith, 2001).
3 Distributional Algorithm
Our algorithm is given a plain text corpus and op-
tionally a desired number of clusters k. Its output
is a partitioning of words into clusters. The al-
gorithm utilizes two representations, distributional
and morphological. Although eventually the latter
is used before the former, for clarity of presenta-
tion we begin by detailing the base distributional
algorithm. In the next section we describe the mor-
phological representation and its integration into
the base algorithm.
Overview. The algorithm consists of two main
stages: landmark clusters discovery, and word
mapping. For the former, we first compute a dis-
tributional representation for each word. We then
cluster the coordinates corresponding to high fre-
quency words. Finally, we define landmark clus-
ters. In the word mapping stage we map each word
to the most similar landmark cluster.
The rationale behind using only the high fre-
quency words in the first stage is twofold. First,
prototypical members of a category are frequent
(Taylor, 2003), and therefore we can expect the
salient POS tags to be represented in this small
subset. Second, higher frequency implies more re-
liable statistics. Since this stage determines the
cores of all resulting clusters, it should be as accu-
rate as possible.
Distributional representation. We use a sim-
plified form of the elegant representation of lexi-
cal entries used by the Seginer unsupervised parser
(Seginer, 2007). Since a POS tag reflects the
grammatical role of the word and since this rep-
resentation is effective to parsing, we were moti-
vated to apply it to the present task.
Let W be the set of word types in the corpus.
The right context entry of a word x ? W is a pair
of mappings r intx : W ? [0, 1] and r adjx :
W ? [0, 1]. For each w ? W , r adjx(w) is an
adjacency score of w to x, reflecting w?s tendency
to appear on the right hand side of x.
For each w ? W , r intx(w) is an interchange-
ability score of x with w, reflecting the tendency
of w to appear to the left of words that tend to ap-
pear to the right of x. This can be viewed as a
1299
similarity measure between words with respect to
their right context. The higher the scores the more
the words tend to be adjacent/interchangeable.
Left context parameters l intx and l adjx are
defined analogously.
There are important subtleties in these defini-
tions. First, for two words x,w ? W , r adjx(w)
is generally different from l adjw(x). For exam-
ple, if w is a high frequency word and x is a low
frequency word, it is likely that w appears many
times to the right of x, yielding a high r adjx(w),
but that x appears only a few times to the left of w
yielding a low l adjw(x). Second, from the defi-
nition of r intx(w) and r intw(x), it is clear that
they need not be equal.
These functions are computed incrementally by
a bootstrapping process. We initialize all map-
pings to be identically 0. We iterate over the words
in the training corpus. For every word instance x,
we take the word immediately to its right y and
update x?s right context using y?s left context:
?w ? W : r intx(w) +=
l adjy(w)
N(y)
?w ? W : r adjx(w) +=
{
1 w = y
l inty(w)
N(y) w 6= y
The division by N(y) (the number of times y
appears in the corpus before the update) is done in
order not to give a disproportional weight to high
frequency words. Also, r intx(w) and r adjx(w)
might become larger than 1. We therefore nor-
malize them after all updates are performed by the
number of occurrences of x in the corpus.
We update l intx and l adjx analogously using
the word z immediately to the left of x. The up-
dates of the left and right functions are done in
parallel.
We define the distributional representation of a
word type x to be a 4|W | + 2 dimensional vector
vx. Each word w yields four coordinates, one for
each direction (left/right) and one for each map-
ping type (int/adj). Two additional coordinates
represent the frequency in which the word appears
to the left and to the right of a stopping punc-
tuation. Of the 4|W | coordinates corresponding
to words, we allow only 2n to be non-zero: the
n top scoring among the right side coordinates
(those of r intx and r adjx), and the n top scoring
among the left side coordinates (those of l intx
and l adjx). We used n = 50.
The distance between two words is defined to
be one minus the cosine of the angle between their
representation vectors.
Coordinate clustering. Each of our landmark
clusters will correspond to a set of high frequency
words (HFWs). The number of HFWs is much
larger than the number of expected POS tags.
Hence we should cluster HFWs. Our algorithm
does that by unifying some of the non-zero coordi-
nates corresponding to HFWs in the distributional
representation defined above.
We extract the words that appear more than N
times per million1 and apply the following proce-
dure I times (5 in our experiments).
We run average link clustering with a threshold
? (AVGLINK?, (Jain et al, 1999)) on these words,
in each iteration initializing every HFW to have
its own cluster. AVGLINK? means running the av-
erage link algorithm until the two closest clusters
have a distance larger than ?. We then use the in-
duced clustering to update the distributional rep-
resentation, by collapsing all coordinates corre-
sponding to words appearing in the same cluster
into a single coordinate whose value is the sum
of the collapsed coordinates? values. In order to
produce a conservative (fine) clustering, we used a
relatively low ? value of 0.25.
Note that the AVGLINK? initialization in each
of the I iterations assigns each HFW to a sepa-
rate cluster. The iterations differ in the distribu-
tional representation of the HFWs, resulting from
the previous iterations.
In our English experiments, this process re-
duced the dimension of the HFWs set (the num-
ber of coordinates that are non-zero in at least one
of the HFWs) from 14365 to 10722. The aver-
age number of non-zero coordinates per word de-
creased from 102 to 55.
Since all eventual POS categories correspond to
clusters produced at this stage, to reduce noise we
delete clusters of less than five elements.
Landmark detection. We define landmark clus-
ters using the clustering obtained in the final iter-
ation of the coordinate clustering stage. However,
the number of clusters might be greater than the
desired number k, which is an optional parame-
ter of the algorithm. In this case we select a sub-
set of k clusters that best covers the HFW space.
We use the following heuristic. We start from the
most frequent cluster, and greedily select the clus-
1We used N = 100, yielding 1242 words for English and
613 words for German.
1300
ter farthest from the clusters already selected. The
distance between two clusters is defined to be the
average distance between their members. A clus-
ter?s distance from a set of clusters is defined to
be its minimal distance from the clusters in the
set. The final set of clusters {L1, ..., Lk} and their
members are referred to as landmark clusters and
prototypes, respectively.
Mapping all words. Each word w ? W is as-
signed the cluster Li that contains its nearest pro-
totype:
d(w,Li) = minx?Li{1 ? cos(vw, vx)}
Map(w) = argminLi{d(w,Li)}
Words that appear less than 5 times are consid-
ered as unknown words. We consider two schemes
for handling unknown words. One randomly maps
each such word to a cluster, using a probabil-
ity proportional to the number of unique known
words already assigned to that cluster. However,
when the number k of landmark clusters is rela-
tively large, it is beneficial to assign all unknown
words to a separate new cluster (after running the
algorithm with k? 1). In our experiments, we use
the first option when k is below some threshold
(we used 15), otherwise we use the second.
4 Morphological Model
The morphological model generates another word
clustering, based on the notion of a signature.
This clustering is integrated with the distributional
model as described below.
4.1 Morphological Representation
We use the Morfessor (Creutz and Lagus, 2005)
word segmentation algorithm. First, all words in
the corpus are segmented. Then, for each stem,
the set of all affixes with which it appears (its sig-
nature, (Goldsmith, 2001)) is collected. The mor-
phological representation of a word type is then
defined to be its stem?s signature in conjunction
with its specific affixes2 (See Figure 1).
We now collect all words having the same rep-
resentation. For instance, if the words joined and
painted are found to have the same signature, they
would share the same cluster since both have the
affix ? ed?. The word joins does not share the same
cluster with them since it has a different affix, ? s?.
This results in coarse-grained clusters exclusively
defined according to morphology.
2A word may contain more than a single affix.
Types join joins joined joining
Stem join join join join
Affixes ? s ed ing
Signature {?, ed, s, ing}
Figure 1: An example for a morphological representation,
defined to be the conjunction of its affix(es) with the stem?s
signature.
In addition, we incorporate capitalization infor-
mation into the model, by constraining all words
that appear capitalized in more than half of their
instances to belong to a separate cluster, regard-
less of their morphological representation. The
motivation for doing so is practical: capitalization
is used in many languages to mark grammatical
categories. For instance, in English capitalization
marks the category of proper names and in Ger-
man it marks the noun category . We report En-
glish results both with and without this modifica-
tion.
Words that contain non-alphanumeric charac-
ters are represented as the sequence of the non-
alphanumeric characters they include, e.g., ?vis-a`-
vis? is represented as (?-?, ?-?). We do not as-
sign a morphological representation to words in-
cluding more than one stem (like weatherman), to
words that have a null affix (i.e., where the word
is identical to its stem) and to words whose stem
is not shared by any other word (signature of size
1). Words that were not assigned a morphologi-
cal representation are included as singletons in the
morphological clustering.
4.2 Distributional-Morphological Algorithm
We detail the modifications made to our base
distributional algorithm given the morphological
clustering defined above.
Coordinate clustering and landmarks. We
constrain AVGLINK? to begin by forming links be-
tween words appearing in the same morphologi-
cal cluster. Only when the distance between the
two closest clusters gets above ? we remove this
constraint and proceed as before. This is equiv-
alent to performing AVGLINK? separately within
each morphological cluster and then using the re-
sult as an initial condition for an AVGLINK? coor-
dinate clustering. The modified algorithm in this
stage is otherwise identical to the distributional al-
gorithm.
Word mapping. In this stage words that are not
prototypes are mapped to one of the landmark
1301
clusters. A reasonable strategy would be to map
all words sharing a morphological cluster as a sin-
gle unit. However, these clusters are too coarse-
grained. We therefore begin by partitioning the
morphological clusters into sub-clusters according
to their distributional behavior. We do so by apply-
ing AVGLINK? (the same as AVGLINK? but with a
different parameter) to each morphological clus-
ter. Since our goal is cluster refinement, we use a
? that is considerably higher than ? (0.9).
We then find the closest prototype to each such
sub-cluster (averaging the distance across all of
the latter?s members) and map it as a single unit
to the cluster containing that prototype.
5 Clustering Evaluation
We evaluate the clustering produced by our algo-
rithm using an external quality measure: we take
a corpus tagged by gold standard tags, tag it using
the induced tags, and compare the two taggings.
There is no single accepted measure quantifying
the similarity between two taggings. In order to
be as thorough as possible, we report results using
four known measures, two mapping-based mea-
sures and two information theoretic ones.
Mapping-based measures. The induced clus-
ters have arbitrary names. We define two map-
ping schemes between them and the gold clus-
ters. After the induced clusters are mapped, we
can compute a derived accuracy. The Many-to-1
measure finds the mapping between the gold stan-
dard clusters and the induced clusters which max-
imizes accuracy, allowing several induced clusters
to be mapped to the same gold standard cluster.
The 1-to-1 measure finds the mapping between
the induced and gold standard clusters which max-
imizes accuracy such that no two induced clus-
ters are mapped to the same gold cluster. Com-
puting this mapping is equivalent to finding the
maximal weighted matching in a bipartite graph,
whose weights are given by the intersection sizes
between matched classes/clusters. As in (Reichart
and Rappoport, 2008), we use the Kuhn-Munkres
algorithm (Kuhn, 1955; Munkres, 1957) to solve
this problem.
Information theoretic measures. These are
based on the observation that a good clustering re-
duces the uncertainty of the gold tag given the in-
duced cluster, and vice-versa. Several such mea-
sures exist; we use V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009),
VI?s (Meila, 2007) normalized version.
6 Experimental Setup
Since a goal of unsupervised POS tagging is in-
ducing an annotation scheme, comparison to an
existing scheme is problematic. To address this
problem we compare to three different schemes
in two languages. In addition, the two English
schemes we compare with were designed to tag
corpora contained in our training set, and have
been widely and successfully used with these cor-
pora by a large number of applications.
Our algorithm was run with the exact same pa-
rameters on both languages: N = 100 (high fre-
quency threshold), n = 50 (the parameter that
determines the effective number of coordinates),
? = 0.25 (cluster separation during landmark
cluster generation), ? = 0.9 (cluster separation
during refinement of morphological clusters).
The algorithm we compare with in most detail
is (Clark, 2003), which reports the best current
results for this problem (see Section 7). Since
Clark?s algorithm is sensitive to its initialization,
we ran it a 100 times and report its average and
standard deviation in each of the four measures.
In addition, we report the percentile in which our
result falls with respect to these 100 runs.
Punctuation marks are very frequent in corpora
and are easy to cluster. As a result, including them
in the evaluation greatly inflates the scores. For
this reason we do not assign a cluster to punctua-
tion marks and we report results using this policy,
which we recommend for future work. However,
to be able to directly compare with previous work,
we also report results for the full POS tag set.
We do so by assigning a singleton cluster to each
punctuation mark (in addition to the k required
clusters). This simple heuristic yields very high
performance on punctuation, scoring (when all
other words are assumed perfect tagging) 99.6%
(99.1%) 1-to-1 accuracy when evaluated against
the English fine (coarse) POS tag sets, and 97.2%
when evaluated against the German POS tag set.
For English, we trained our model on the
39832 sentences which constitute sections 2-21 of
the PTB-WSJ and on the 500K sentences from
the NYT section of the NANC newswire corpus
(Graff, 1995). We report results on the WSJ part
of our data, which includes 950028 words tokens
in 44389 types. Of the tokens, 832629 (87.6%)
1302
English Fine k=13 Coarse k=13 Fine k=34
Prototype Clark Prototype Clark Prototype Clark
Tagger ? ? % Tagger ? ? % Tagger ? ? %
Many?to?1 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 90
55.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 90
1?to?1 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 100
54.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100
NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 100
0.795 0.943 0.033 100 1.052 1.221 0.046 100 0.809 0.885 0.022 100
V 0.636 0.581 0.015 100 0.590 0.543 0.018 100 0.677 0.659 0.008 100
0.542 0.478 0.019 100 0.484 0.429 0.023 100 0.608 0.588 0.010 98
German k=17 k=26
Prototype Clark Prototype Clark
Tagger ? ? % Tagger ? ? %
Many?to-1 64.6 64.7 1.2 41 68.2 67.8 1.0 60
58.9 59.1 1.4 40 63.2 62.8 1.2 60
1?to?1 53.7 52.0 1.8 77 56.0 52.0 2.1 99
48.0 46.0 2.3 78 50.7 45.9 2.6 99
NVI 0.667 0.675 0.019 66 0.640 0.682 0.019 100
0.819 0.829 0.025 66 0.785 0.839 0.025 100
V 0.646 0.645 0.010 50 0.675 0.657 0.008 100
0.552 0.553 0.013 48 0.596 0.574 0.010 100
Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark?s average score (?),
Clark?s standard deviation (?) and the fraction of Clark?s results that scored worse than our model (%). For the mapping based
measures, results are accuracy percentage. For V ? [0, 1], higher is better. For high quality output, NV I ? [0, 1] as well, and
lower is better. In each entry, the top number indicates the score when including punctuation and the bottom number the score
when excluding it. In English, our results are always better than Clark?s. In German, they are almost always better.
are not punctuation. The percentage of unknown
words (those appearing less than five times) is
1.6%. There are 45 clusters in this annotation
scheme, 34 of which are not punctuation.
We ran each algorithm both with k=13 and
k=34 (the number of desired clusters). We com-
pare the output to two annotation schemes: the fine
grained PTB WSJ scheme, and the coarse grained
tags defined in (Smith and Eisner, 2005). The
output of the k=13 run is evaluated both against
the coarse POS tag annotation (the ?Coarse k=13?
scenario) and against the full PTB-WSJ annotation
scheme (the ?Fine k=13? scenario). The k=34 run
is evaluated against the full PTB-WSJ annotation
scheme (the ?Fine k=34? scenario).
The POS cluster frequency distribution tends to
be skewed: each of the 13 most frequent clusters
in the PTB-WSJ cover more than 2.5% of the to-
kens (excluding punctuation) and together 86.3%
of them. We therefore chose k=13, since it is both
the number of coarse POS tags (excluding punctu-
ation) as well as the number of frequent POS tags
in the PTB-WSJ annotation scheme. We chose
k=34 in order to evaluate against the full 34 tags
PTB-WSJ annotation scheme (excluding punctua-
tion) using the same number of clusters.
For German, we trained our model on the 20296
sentences of the NEGRA corpus (Brants, 1997)
and on the first 450K sentences of the DeWAC
corpus (Baroni et al, 2009). DeWAC is a cor-
pus extracted by web crawling and is therefore
out of domain. We report results on the NEGRA
part, which includes 346320 word tokens of 49402
types. Of the tokens, 289268 (83.5%) are not
punctuation. The percentage of unknown words
(those appearing less than five times) is 8.1%.
There are 62 clusters in this annotation scheme,
51 of which are not punctuation.
We ran the algorithms with k=17 and k=26.
k=26 was chosen since it is the number of clus-
ters that cover each more than 0.5% of the NE-
GRA tokens, and in total cover 96% of the (non-
punctuation) tokens. In order to test our algo-
rithm in another scenario, we conducted experi-
ments with k=17 as well, which covers 89.9% of
the tokens. All outputs are compared against NE-
GRA?s gold standard scheme.
We do not report results for k=51 (where the
number of gold clusters is the same as the number
of induced clusters), since our algorithm produced
only 42 clusters in the landmark detection stage.
We could of course have modified the parame-
ters to allow our algorithm to produce 51 clusters.
However, we wanted to use the exact same param-
eters as those used for the English experiments to
minimize the issue of parameter tuning.
In addition to the comparisons described above,
we present results of experiments (in the ?Fine
1303
B B+M B+C F(I=1) F
M-to-1 53.3 54.8 58.2 57.3 61.0
1-to-1 50.2 51.7 55.1 54.8 60.0
NVI 0.782 0.720 0.710 0.742 0.652
V 0.569 0.598 0.615 0.597 0.636
Table 2: A comparison of partial versions of the model in
the ?Fine k=13? WSJ scenario. M-to-1 and 1-to-1 results are
reported in accuracy percentage. Lower NVI is better. B is the
strictly distributional algorithm, B+M adds the morphologi-
cal model, B+C adds capitalization to B, F(I=1) consists of
all components, where only one iteration of coordinate clus-
tering is performed, and F is the full model.
M-to-1 1-to-1 V VI
Prototype 71.6 63.5 0.677 2.00
Clark 69.8 54.5 0.659 2.18
HK ? 41.3 ? ?
J 43?62 37?47 ? 4.23?5.74
GG ? ? ? 2.8
GJ ? 40?49.9 ? 4.03?4.47
VG ? ? 0.54-0.59 2.5?2.9
GGTP-45 65.4 44.5 ? ?
GGTP-17 70.2 49.5 ? ?
Table 4: Comparison of our algorithms with the recent fully
unsupervised POS taggers for which results are reported. The
models differ in the annotation scheme, the corpus size and
the number of induced clusters (k) that they used. HK:
(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.
GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse
tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags,
k=25?50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine
tags, k=50. VG: (Van Gael et al, 2009), 1.17M tokens, fine
tags, k=47?192. GGTP-45: (Grac?a et al, 2009), 1.17M to-
kens, fine tags, k=45. GGTP-17: (Grac?a et al, 2009), 1.17M
tokens, coarse tags, k=17. Lower VI values indicate better
clustering. VI is computed using e as the base of the loga-
rithm. Our algorithm gives the best results.
k=13? scenario) that quantify the contribution of
each component of the algorithm. We ran the base
distributional algorithm, a variant which uses only
capitalization information (i.e., has only one non-
singleton morphological class, that of words ap-
pearing capitalized in most of their instances) and
a variant which uses no capitalization information,
defining the morphological clusters according to
the morphological representation alone.
7 Results
Table 1 presents results for the English and Ger-
man experiments. For English, our algorithm ob-
tains better results than Clark?s in all measures and
scenarios. It is without exception better than the
average score of Clark?s and in most cases better
than the maximal Clark score obtained in 100 runs.
A significant difference between our algorithm
and Clark?s is that the latter, like most algorithms
which addressed the task, induces the clustering
0 5 10 15 20 25 30 35 40 45
0
0.2
0.4
0.6
0.8
1
 
 
Gold Standard
Induced
Figure 2: POS class frequency distribution for our model
and the gold standard, in the ?Fine k=34? scenario. The dis-
tributions are similar.
by maximizing a non-convex function. These
functions have many local maxima and the specific
solution to which algorithms that maximize them
converge strongly depends on their (random) ini-
tialization. Therefore, their output?s quality often
significantly diverges from the average. This issue
is discussed in depth in (Reichart et al, 2010b).
Our algorithm is deterministic3.
For German, in the k=26 scenario our algorithm
outperforms Clark?s, often outperforming even its
maximum in 100 runs. In the k=17 scenario, our
algorithm obtains a higher score than Clark with
probability 0.4 to 0.78, depending on the measure
and scenario. Clark?s average score is slightly bet-
ter in the Many-to-1 measure, while our algorithm
performs somewhat better than Clark?s average in
the 1-to-1 and NVI measures.
The DeWAC corpus from which we extracted
statistics for the German experiments is out of do-
main with respect to NEGRA. The correspond-
ing corpus in English, NANC, is a newswire cor-
pus and therefore clearly in-domain with respect
to WSJ. This is reflected by the percentage of un-
known words, which was much higher in German
than in English (8.1% and 1.6%), lowering results.
Table 2 shows the effect of each of our algo-
rithm?s components. Each component provides
an improvement over the base distributional algo-
rithm. The full coordinate clustering stage (sev-
eral iterations, F) considerably improves the score
over a single iteration (F(I=1)). Capitalization in-
formation increases the score more than the mor-
phological information, which might stem from
the granularity of the POS tag set with respect to
names. This analysis is supported by similar ex-
periments we made in the ?Coarse k=13? scenario
(not shown in tables here). There, the decrease in
performance was only of 1%?2% in the mapping
3The fluctuations inflicted on our algorithm by the random
mapping of unknown words are of less than 0.1% .
1304
Excluding Punctuation Including Punctuation Perfect Punctuation
M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V
Van Gael 59.1 48.4 0.999 0.530 62.3 51.3 0.861 0.591 64.0 54.6 0.820 0.610
Prototype 67.5 58.8 0.809 0.608 71.6 63.5 0.663 0.677 71.6 63.9 0.659 0.679
Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al, 2009) and ours with various punctuation assign-
ment schemes. Left section: punctuation tokens are excluded. Middle section: punctuation tokens are included. Right section:
perfect assignment of punctuation is assumed.
based measures and 3.5% in the V measure.
Finally, Table 4 presents reported results for all
recent algorithms we are aware of that tackled the
task of unsupervised POS induction from plain
text. Results for our algorithm?s and Clark?s are
reported for the ?Fine, k=34? scenario. The set-
tings of the various experiments vary in terms of
the exact annotation scheme used (coarse or fine
grained) and the size of the test set. However, the
score differences are sufficiently large to justify
the claim that our algorithm is currently the best
performing algorithm on the PTB-WSJ corpus for
POS induction from plain text4.
Since previous works provided results only for
the scenario in which punctuation is included, the
reported results are not directly comparable. In
order to quantify the effect various punctuation
schemes have on the results, we evaluated the
?iHMM: PY-fixed? model (Van Gael et al, 2009)
and ours when punctuation is excluded, included
or perfectly tagged5. The results (Table 3) indi-
cate that most probably even after an appropriate
correction for punctuation, our model remains the
best performing one.
8 Discussion
In this work we presented a novel unsupervised al-
gorithm for POS induction from plain text. The al-
gorithm first generates relatively accurate clusters
of high frequency words, which are subsequently
used to bootstrap the entire clustering. The dis-
tributional and morphological representations that
we use are novel for this task.
We experimented on two languages with map-
ping and information theoretic clustering evalua-
tion measures. Our algorithm obtains the best re-
ported results on the English PTB-WSJ corpus. In
addition, our results are almost always better than
Clark?s on the German NEGRA corpus.
4Grac?a et al (2009) report very good results for 17 tags in
the M-1 measure. However, their 1-1 results are quite poor,
and results for the common IT measures were not reported.
Their results for 45 tags are considerably lower.
5We thank the authors for sending us their data.
We have also performed a manual error anal-
ysis, which showed that our algorithm performs
much better on closed classes than on open
classes. In order to asses this quantitatively, let
us define a random variable for each of the gold
clusters, which receives a value corresponding to
each induced cluster with probability proportional
to their intersection size. For each gold cluster,
we compute the entropy of this variable. In ad-
dition, we greedily map each induced cluster to a
gold cluster and compute the ratio between their
intersection size and the size of the gold cluster
(mapping accuracy).
We experimented in the ?Fine k=34? scenario.
The clusters that obtained the best scores were
(brackets indicate mapping accuracy and entropy
for each of these clusters) coordinating conjunc-
tions (95%, 0.32), prepositions (94%, 0.32), de-
terminers (94%, 0.44) and modals (93%, 0.45).
These are all closed classes.
The classes on which our algorithm performed
worst consist of open classes, mostly verb types:
past tense verbs (47%, 2.2), past participle verbs
(44%, 2.32) and the morphologically unmarked
non-3rd person singular present verbs (32%, 2.86).
Another class with low performance is the proper
nouns (37%, 2.9). The errors there are mostly
of three types: confusions between common and
proper nouns (sometimes due to ambiguity), un-
known words which were put in the unknown
words cluster, and abbreviations which were given
a separate class by our algorithm. Finally, the al-
gorithm?s performance on the heterogeneous ad-
verbs class (19%, 3.73) is the lowest.
Clark?s algorithm exhibits6 a similar pattern
with respect to open and closed classes. While
his algorithm performs considerably better on ad-
verbs (15% mapping accuracy difference and 0.71
entropy difference), our algorithm scores consid-
erably better on prepositions (17%, 0.77), su-
perlative adjectives (38%, 1.37) and plural proper
names (45%, 1.26).
6Using average mapping accuracy and entropy over the
100 runs.
1305
Naturally, this analysis might reflect the arbi-
trary nature of a manually design POS tag set
rather than deficiencies in automatic POS induc-
tion algorithms. In future work we intend to ana-
lyze the output of such algorithms in order to im-
prove POS tag sets.
Our algorithm and Clark?s are monosemous
(i.e., they assign each word exactly one tag), while
most other algorithms are polysemous. In order to
assess the performance loss caused by the monose-
mous nature of our algorithm, we took the M-1
greedy mapping computed for the entire dataset
and used it to compute accuracy over the monose-
mous and polysemous words separately. Results
are reported for the English ?Fine k=34? scenario
(without punctuation). We define a word to be
monosemous if more than 95% of its tokens are
assigned the same gold standard tag. For English,
there are approximately 255K polysemous tokens
and 578K monosemous ones. As expected, our
algorithm is much more accurate on the monose-
mous tokens, achieving 76.6% accuracy, com-
pared to 47.1% on the polysemous tokens.
The evaluation in this paper is done at the token
level. Type level evaluation, reflecting the algo-
rithm?s ability to detect the set of possible POS
tags for each word type, is important as well. It
could be expected that a monosemous algorithm
such as ours would perform poorly in a type level
evaluation. In (Reichart et al, 2010a) we discuss
type level evaluation at depth and propose type
level evaluation measures applicable to the POS
induction problem. In that paper we compare the
performance of our Prototype Tagger with lead-
ing unsupervised POS tagging algorithms (Clark,
2003; Goldwater and Griffiths, 2007; Gao and
Johnson, 2008; Van Gael et al, 2009). Our al-
gorithm obtained the best results in 4 of the 6
measures in a margin of 4?6%, and was second
best in the other two measures. Our results were
better than Clark?s (the only other monosemous
algorithm evaluated there) on all measures in a
margin of 5?21%. The fact that our monose-
mous algorithm was better than good polysemous
algorithms in a type level evaluation can be ex-
plained by the prototypical nature of the POS phe-
nomenon (a longer discussion is given in (Reichart
et al, 2010a)). However, the quality upper bound
for monosemous algorithms is obviously much
lower than that for polysemous algorithms, and
we expect polysemous algorithms to outperform
monosemous algorithms in the future in both type
level and token level evaluations.
The skewed (Zipfian) distribution of POS class
frequencies in corpora is a problem for many POS
induction algorithms, which by default tend to in-
duce a clustering having a balanced distribution.
Explicit modifications to these algorithms were in-
troduced in order to bias their model to produce
such a distribution (see (Clark, 2003; Johnson,
2007; Reichart et al, 2010b)). An appealing prop-
erty of our model is its ability to induce a skewed
distribution without being explicitly tuned to do
so, as seen in Figure 2.
Acknowledgements. We would like to thank
Yoav Seginer for his help with his parser.
References
Michele Banko and Robert C. Moore, 2004. Part of
Speech Tagging in Context. COLING ?04.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi and
Eros Zanchetta, 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ?06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18(4):467?479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Mathias Creutz and Krista Lagus, 2005. Inducing the
Morphological Lexicon of a Natural Language from
Unannotated Text. AKRR ?05.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ?07.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ?04.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ?08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ?08.
1306
John Goldsmith, 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ?07.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learning for Sequence Labeling. HLT?NAACL ?06.
Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,
1999. Data Clustering: A Review. ACM Computing
Surveys 31(3):264?323.
Wenbin Jiang, Liang Huang and Qun Liu, 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. ACL ?09.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMM POS-Taggers? EMNLP-CoNLL ?07.
Harold W. Kuhn, 1955. The Hungarian method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Marina Meila, 2007. Comparing Clustering ? an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873?895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155?172.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32?38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
?09.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Roi Reichart, Omri Abend and Ari Rappoport, 2010a.
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study. CoNLL ?10.
Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.
Improved Unsupervised POS Induction Using In-
trinsic Clustering Quality and a Zipfian Constraint.
CoNLL ?10.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ?07.
Hinrich Schu?tze, 1995. Distributional part-of-speech
tagging. EACL ?95.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ?05.
John R. Taylor, 2003. Linguistic Categorization: Pro-
totypes in Linguistic Theory, Third Edition. Oxford
University Press.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP?KE ?05.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ?09.
1307
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 663?672,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Neutralizing Linguistically Problematic Annotations
in Unsupervised Dependency Parsing Evaluation
Roy Schwartz1 Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem
{roys02|omria01|arir}@cs.huji.ac.il
2Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
roiri@csail.mit.edu
Abstract
Dependency parsing is a central NLP task. In
this paper we show that the common eval-
uation for unsupervised dependency parsing
is highly sensitive to problematic annotations.
We show that for three leading unsupervised
parsers (Klein and Manning, 2004; Cohen and
Smith, 2009; Spitkovsky et al, 2010a), a small
set of parameters can be found whose mod-
ification yields a significant improvement in
standard evaluation measures. These param-
eters correspond to local cases where no lin-
guistic consensus exists as to the proper gold
annotation. Therefore, the standard evaluation
does not provide a true indication of algorithm
quality. We present a new measure, Neutral
Edge Direction (NED), and show that it greatly
reduces this undesired phenomenon.
1 Introduction
Unsupervised induction of dependency parsers is a
major NLP task that attracts a substantial amount
of research (Klein and Manning, 2004; Cohen et
al., 2008; Headden et al, 2009; Spitkovsky et al,
2010a; Gillenwater et al, 2010; Berg-Kirkpatrick
et al, 2010; Blunsom and Cohn, 2010, inter alia).
Parser quality is usually evaluated by comparing its
output to a gold standard whose annotations are lin-
guistically motivated. However, there are cases in
which there is no linguistic consensus as to what the
correct annotation is (Ku?bler et al, 2009). Examples
include which verb is the head in a verb group struc-
ture (e.g., ?can? or ?eat? in ?can eat?), and which
? Omri Abend is grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship.
noun is the head in a sequence of proper nouns (e.g.,
?John? or ?Doe? in ?John Doe?). We refer to such
annotations as (linguistically) problematic. For such
cases, evaluation measures should not punish the al-
gorithm for deviating from the gold standard.
In this paper we show that the evaluation mea-
sures reported in current works are highly sensitive
to the annotation in problematic cases, and propose
a simple new measure that greatly neutralizes the
problem.
We start from the following observation: for three
leading algorithms (Klein and Manning, 2004; Co-
hen and Smith, 2009; Spitkovsky et al, 2010a), a
small set (at most 18 out of a few thousands) of pa-
rameters can be found whose modification dramati-
cally improves the standard evaluation measures (the
attachment score measure by 9.3-15.1%, and the
undirected measure by a smaller but still significant
1.3-7.7%). The phenomenon is implementation in-
dependent, occurring with several algorithms based
on a fundamental probabilistic dependency model1.
We show that these parameter changes can be
mapped to edge direction changes in local structures
in the dependency graph, and that these correspond
to problematic annotations. Thus, the standard eval-
uation measures do not reflect the true quality of the
evaluated algorithm.
We explain why the standard undirected evalua-
tion measure is in fact sensitive to such edge direc-
1It is also language-independent; we have produced it in five
different languages: English, Czech, Japanese, Portuguese, and
Turkish. Due to space considerations, in this paper we focus
on English, because it is the most studied language for this task
and the most practically useful one at present.
663
tion changes, and present a new evaluation measure,
Neutral Edge Direction (NED), which greatly allevi-
ates the problem by ignoring the edge direction in lo-
cal structures. Using NED, manual modifications of
model parameters always yields small performance
differences. Moreover, NED sometimes punishes
such manual parameter tweaking by yielding worse
results. We explain this behavior using an exper-
iment revealing that NED always prefers the struc-
tures that are more consistent with the modeling as-
sumptions lying in the basis of the algorithm. When
manual parameter modification is done against this
preference, the NED results decrease.
The contributions of this paper are as follows.
First, we show the impact of a small number of an-
notation decisions on the performance of unsuper-
vised dependency parsers. Second, we observe that
often these decisions are linguistically controversial
and therefore this impact is misleading. This reveals
a problem in the common evaluation of unsuper-
vised dependency parsing. This is further demon-
strated by noting that recent papers evaluate the task
using three gold standards which differ in such deci-
sions and which yield substantially different results.
Third, we present the NED measure, which is agnos-
tic to errors arising from choosing the non-gold di-
rection in such cases.
Section 2 reviews related work. Section 3 de-
scribes the performed parameter modifications. Sec-
tion 4 discusses the linguistic controversies in anno-
tating problematic dependency structures. Section 5
presents NED. Section 6 describes experiments with
it. A discussion is given in Section 7.
2 Related Work
Grammar induction received considerable attention
over the years (see (Clark, 2001; Klein, 2005) for
reviews). For unsupervised dependency parsing, the
Dependency Model with Valence (DMV) (Klein and
Manning, 2004) was the first to beat the simple
right-branching baseline. A technical description of
DMV is given at the end of this section.
The great majority of recent works, including
those experimented with in this paper, are elabora-
tions of DMV. Smith and Eisner (2005) improved
the DMV results by generalizing the function maxi-
mized by DMV?s EM training algorithm. Smith and
Eisner (2006) used a structural locality bias, experi-
menting on five languages. Cohen et al (2008) ex-
tended DMV by using a variational EM training al-
gorithm and adding logistic normal priors. Cohen
and Smith (2009, 2010) further extended it by us-
ing a shared logistic normal prior which provided a
new way to encode the knowledge that some POS
tags are more similar than others. A bilingual joint
learning further improved their performance.
Headden et al (2009) obtained the best reported
results on WSJ10 by using a lexical extension of
DMV. Gillenwater et al (2010) used posterior reg-
ularization to bias the training towards a small num-
ber of parent-child combinations. Berg-Kirkpatrick
et al (2010) added new features to the M step of the
DMV EM procedure. Berg-Kirkpatrick and Klein
(2010) used a phylogenetic tree to model parame-
ter drift between different languages. Spitkovsky
et al (2010a) explored several training protocols
for DMV. Spitkovsky et al (2010c) showed the
benefits of Viterbi (?hard?) EM to DMV training.
Spitkovsky et al (2010b) presented a novel lightly-
supervised approach that used hyper-text mark-up
annotation of web-pages to train DMV.
A few non-DMV-based works were recently pre-
sented. Daume? III (2009) used shift-reduce tech-
niques. Blunsom and Cohn (2010) used tree sub-
stitution grammar to achieve best results on WSJ?.
Druck et al (2009) took a semi-supervised ap-
proach, using a set of rules such as ?A noun is usu-
ally the parent of a determiner which is to its left?,
experimenting on several languages. Naseem et al
(2010) further extended this idea by using a single
set of rules which globally applies to six different
languages. The latter used a model similar to DMV.
The controversial nature of some dependency
structures was discussed in (Nivre, 2006; Ku?bler
et al, 2009). Klein (2005) discussed controversial
constituency structures and the evaluation problems
stemming from them, stressing the importance of a
consistent standard of evaluation.
A few works explored the effects of annotation
conventions on parsing performance. Nilsson et
al. (2006) transformed the dependency annotations
of coordinations and verb groups in the Prague
TreeBank. They trained the supervised MaltParser
(Nivre et al, 2006) on the transformed data, parsed
the test data and re-transformed the resulting parse,
664
w3 w2 w1
(a)
w3 w2 w1
(b)
Figure 1: A dependency structure on the words
w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b))
an edge-flip of w2?w1.
thus improving performance. Klein and Manning
(2004) observed that a large portion of their errors is
caused by predicting the wrong direction of the edge
between a noun and its determiner. Ku?bler (2005)
compared two different conversion schemes in Ger-
man supervised constituency parsing and found one
to have positive influence on parsing quality.
Dependency Model with Valence (DMV). DMV
(Klein and Manning, 2004) defines a probabilistic
grammar for unlabeled dependency structures. It is
defined as follows: the root of the sentence is first
generated, and then each head recursively generates
its right and left dependents. The parameters of the
model are of two types: PSTOP and PATTACH .
PSTOP (dir, h, adj) determines the probability to
stop generating arguments, and is conditioned on 3
arguments: the head h, the direction dir ((L)eft
or (R)ight) and adjacency adj (whether the head
already has dependents ((Y )es) in direction dir or
not ((N)o)). PATTACH(arg|h, dir) determines the
probability to generate arg as head h?s dependent in
direction dir.
3 Significant Effects of Edge Flipping
In this section we present recurring error patterns
in some of the leading unsupervised dependency
parsers. These patterns are all local, confined to a
sequence of up to three words (but mainly of just
two consecutive words). They can often be mended
by changing the directions of a few types of edges.
The modified parameters described in this section
were handpicked to improve performance: we ex-
amined the local parser errors occurring the largest
number of times, and found the corresponding pa-
rameters. Note that this is a valid methodology,
since our goal is not to design a new algorithm but
to demonstrate that modifying a small set of param-
eters can yield a major performance boost and even-
tually discover problems with evaluation methods or
algorithms.
I
PRP
want
VBP
to
TO
eat
VB
.
ROOT
Figure 2: A parse of the sentence ?I want to eat?, before
(straight line) and after (dashed line) an edge-flip of the
edge ?to???eat?.
We start with a few definitions. Consider Fig-
ure 1(a) that shows a dependency structure on the
words w1, w2, w3. Edge flipping (henceforth, edge-
flip) the edge w2?w1 is the following modification
of a parse tree: (1) setting w2?s parent as w1 (instead
of the other way around), and (2) setting w1?s par-
ent as w3 (instead of the edge w3?w2). Figure 1(b)
shows the dependency structure after the edge-flip.
Note that (1) imposes setting a new parent to w2,
as otherwise it would have had no parent. Setting
this parent to be w3 is the minimal modification of
the original parse, since it does not change the at-
tachment of the structure [w2, w1] to the rest of the
sentence, but only the direction of the internal edge.
Figure 2 presents a parse of the sentence ?I want
to eat?, before and after an edge-flip of the edge
?to???eat?.
Since unsupervised dependency parsers are gen-
erally structure prediction models, the predictions
of the parse edges are not independent. Therefore,
there is no single parameter which completely con-
trols the edge direction, and hence there is no direct
way to perform an edge-flip by parameter modifica-
tion. However, setting extreme values for the param-
eters controlling the direction of a certain edge type
creates a strong preference towards one of the direc-
tions, and effectively determines the edge direction.
This procedure is henceforth termed parameter-flip.
We show that by performing a few parameter-
flips, a substantial improvement in the attachment
score can be obtained. Results are reported for three
algorithms.
Parameter Changes. All the works experimented
with in this paper are not lexical and use sequences
of POS tags as their input. In addition, they all use
the DMV parameter set (PSTOP and PATTACH) for
parsing. We will henceforth refer to this set, condi-
tioned on POS tags, as the model parameter set.
We show how an edge in the dependency graph
is encoded using the DMV parameters. Say the
665
model prefers setting ?to? (POS tag: TO) as a de-
pendent of the infinitive verb (POS tag: V B) to its
right (e.g., ?to eat?). This is reflected by a high
value of PATTACH(TO|V B,L), a low value of
PATTACH(V B|TO,R), since ?to? tends to be a left
dependent of the verb and not the other way around,
and a low value of PSTOP (V B,L,N), as the verb
usually has at least one left argument (i.e., ?to?).
A parameter-flip of w1?w2 is hence performed
by setting PATTACH(w2|w1, R) to a very low
value and PATTACH(w1|w2, L) to a very high
value. When the modifications to PATTACH
are insufficient to modify the edge direction,
PSTOP (w2, L,N) is set to a very low value and
PSTOP (w1, R,N) to a very high value2.
Table 1 describes the changes made for the three
algorithms. The ?+? signs in the table correspond to
edges in which the algorithm disagreed with the gold
standard, and were thus modified. Similarly, the ???
signs in the table correspond to edges in which the
algorithm agreed with the gold standard, and were
thus not modified. The number of modified param-
eters does not exceed 18 (out of a few thousands).
The Freq. column in the table shows the percent-
age of the tokens in sections 2-21 of PTB WSJ that
participate in each structure. Equivalently, the per-
centage of edges in the corpus which are of either
of the types appearing in the Orig. Edge column.
As the table shows, the modified structures cover a
significant portion of the tokens. Indeed, 42.9% of
the tokens in the corpus participate in at least one of
them3.
Experimenting with Edge Flipping. We experi-
mented with three DMV-based algorithms: a repli-
cation of (Klein and Manning, 2004), as appears in
(Cohen et al, 2008) (henceforth, km04), Cohen and
Smith (2009) (henceforth, cs09), and Spitkovsky et
al. (2010a) (henceforth, saj10a). Decoding is done
using the Viterbi algorithm4. For each of these algo-
rithms we present the performance gain when com-
pared to the original parameters.
The training set is sections 2-21 of the Wall Street
2Note that this yields unnormalized models. Again, this is
justified since the resulting model is only used as a basis for
discussion and is not a fully fledged algorithm.
3Some tokens participate in more than one structure.
4http://www.cs.cmu.edu/?scohen/parser.html.
Structure Freq. Orig. Edge km04 cs09 saj10a
Coordination
(?John & Mary?) 2.9% CC?NNP ? + ?
Prepositional
Phrase (?in
the house?)
32.7%
DT?NN + + +
DT?NNP ? + +
DT?NNS ? ? +
IN?DT + + ?
IN?NN + + ?
IN?NNP + ? ?
IN?NNS ? + ?
PRP$?NN ? ? +
Modal Verb
(?can eat?) 2.4% MD?V B ? + ?
Infinitive Verb
(?to eat?) 4.5% TO?V B ? + +
Proper Name
Sequence
(?John Doe?)
18.5% NNP?NNP + ? ?
Table 1: Parameter changes for the three algorithms. The
Freq. column shows what percentage of the tokens in sec-
tions 2-21 of PTB WSJ participate in each structure. The
Orig. column indicates the original edge. The modified
edge is of the opposite direction. The other columns show
the different algorithms: km04: basic DMV model (repli-
cation of (Klein and Manning, 2004)); cs09; (Cohen and
Smith, 2009); saj10a: (Spitkovsky et al, 2010a).
Journal Penn TreeBank (Marcus et al, 1993). Test-
ing is done on section 23. The constituency annota-
tion was converted to dependencies using the rules
of (Yamada and Matsumoto, 2003)5.
Following standard practice, we present the at-
tachment score (i.e., percentage of words that have a
correct head) of each algorithm, with both the origi-
nal parameters and the modified ones. We present
results both on all sentences and on sentences of
length ? 10, excluding punctuation.
Table 2 shows results for all algorithms6. The
performance difference between the original and the
modified parameter set is considerable for all data
sets, where differences exceed 9.3%, and go up to
15.1%. These are enormous differences from the
perspective of current algorithm evaluation results.
4 Linguistically Problematic Annotations
In this section, we discuss the controversial nature
of the annotation in the modified structures (Ku?bler
5http://www.jaist.ac.jp/?h-yamada/
6Results are slightly worse than the ones published in the
original papers due to the different decoding algorithms (cs09
use MBR while we used Viterbi) and a different conversion pro-
cedure (saj10a used (Collins, 1999) and not (Yamada and Mat-
sumoto, 2003)) ; see Section 5.
666
Algo. ? 10 ? ?
Orig. Mod. ? Orig. Mod. ?
km04 45.8 59.8 14 34.6 43.9 9.3
cs09 60.9 72.9 12 39.9 54.6 14.7
saj10a 54.7 69.8 15.1 41.6 54.3 12.7
Table 2: Results of the original (Orig. columns), the
modified (Mod. columns) parameter sets and their dif-
ference (? columns) for the three algorithms.
et al, 2009). We remind the reader that structures
for which no linguistic consensus exists as to their
correct annotation are referred to as (linguistically)
problematic.
We begin by showing that all the structures mod-
ified are indeed linguistically problematic. We then
note that these controversies are reflected in the eval-
uation of this task, resulting in three, significantly
different, gold standards currently in use.
Coordination Structures are composed of two
proper nouns, separated by a conjunctor (e.g., ?John
and Mary?). It is not clear which token should be the
head of this structure, if any (Nilsson et al, 2006).
Prepositional Phrases (e.g., ?in the house? or ?in
Rome?), where every word is a reasonable candidate
to head this structure. For example, in the annotation
scheme used by (Collins, 1999) the preposition is the
head, in the scheme used by (Johansson and Nugues,
2007) the noun is the head, while TUT annotation,
presented in (Bosco and Lombardo, 2004), takes the
determiner to be the noun?s head.
Verb Groups are composed of a verb and an aux-
iliary or a modal verb (e.g., ?can eat?). Some
schemes choose the modal as the head (Collins,
1999), others choose the verb (Rambow et al, 2002).
Infinitive Verbs (e.g., ?to eat?) are also in contro-
versy, as in (Yamada and Matsumoto, 2003) the verb
is the head while in (Collins, 1999; Bosco and Lom-
bardo, 2004) the ?to? token is the head.
Sequences of Proper Nouns (e.g., ?John Doe?)
are also subject to debate, as PTB?s scheme takes the
last proper noun as the head, and BIO?s scheme de-
fines a more complex scheme (Dredze et al, 2007).
Evaluation Inconsistency Across Papers. A fact
that may not be recognized by some readers is that
comparing the results of unsupervised dependency
parsers across different papers is not directly pos-
sible, since different papers use different gold stan-
dard annotations even when they are all derived from
the Penn Treebank constituency annotation. This
happens because they use different rules for con-
verting constituency annotation to dependency an-
notation. A probable explanation for this fact is that
people have tried to correct linguistically problem-
atic annotations in different ways, which is why we
note this issue here7.
There are three different annotation schemes
in current use: (1) Collins head rules (Collins,
1999), used in e.g., (Berg-Kirkpatrick et al, 2010;
Spitkovsky et al, 2010a); (2) Conversion rules of
(Yamada and Matsumoto, 2003), used in e.g., (Co-
hen and Smith, 2009; Gillenwater et al, 2010); (3)
Conversion rules of (Johansson and Nugues, 2007)
used, e.g., in the CoNLL shared task 2007 (Nivre et
al., 2007) and in (Blunsom and Cohn, 2010).
The differences between the schemes are substan-
tial. For instance, 14.4% of section 23 is tagged dif-
ferently by (1) and (2)8.
5 The Neutral Edge Direction (NED)
Measure
As shown in the previous sections, the annotation
of problematic edges can substantially affect perfor-
mance. This was briefly discussed in (Klein and
Manning, 2004), which used undirected evaluation
as a measure which is less sensitive to alternative
annotations. Undirected accuracy was commonly
used since to assess the performance of unsuper-
vised parsers (e.g., (Smith and Eisner, 2006; Head-
den et al, 2008; Spitkovsky et al, 2010a)) but also
of supervised ones (Wang et al, 2005; Wang et al,
2006). In this section we discuss why this measure
is in fact not indifferent to edge-flips and propose a
new measure, Neutral Edge Direction (NED).
7Indeed, half a dozen flags in the LTH Constituent-to-
Dependency Conversion Tool (Johansson and Nugues, 2007)
are used to control the conversion in problematic cases.
8In our experiments we used the scheme of (Yamada and
Matsumoto, 2003), see Section 3. The significant effects of
edge flipping were observed with the other two schemes as well.
667
w1
w2
w3
(a)
w1
w3
w2
(b)
w4
w3
w2
(c)
Figure 3: A dependency structure on the words
w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an
edge-flip of w2?w3, and when the direction of the edge
between w2 and w3 is switched and the new parent of w3
is set to be some other word, w4 (Figure 3(c)).
Undirected Evaluation. The measure is defined
as follows: traverse over the tokens and mark a cor-
rect attachment if the token?s induced parent is either
(1) its gold parent or (2) its gold child. The score is
the ratio of correct attachments and the number of
tokens.
We show that this measure does not ignore edge-
flips. Consider Figure 3 that shows a depen-
dency structure on the words w1, w2, w3 before (Fig-
ure 3(a)) and after (Figure 3(b)) an edge-flip of
w2?w3. Assume that 3(a) is the gold standard and
that 3(b) is the induced parse. Consider w2. Its
induced parent (w3) is its gold child, and thus undi-
rected evaluation does not consider it an error. On
the other hand, w3 is assigned w2?s gold parent, w1.
This is considered an error, since w1 is neither w3?s
gold parent (as it is w2), nor its gold child9. There-
fore, one of the two tokens involved in the edge-flip
is penalized by the measure.
Recall the example ?I want to eat? and the edge-
flip of the edge ?to???eat? (Figure 2). As ?to??s
parent in the induced graph (?want?) is neither its
gold parent nor its gold child, the undirected evalu-
ation measure marks it as an error. This is an exam-
ple where an edge-flip in a problematic edge, which
should not be considered an error, was in fact con-
sidered an error by undirected evaluation.
Neutral Edge Direction (NED). The NED measure
is a simple extension of the undirected evaluation
measure10. Unlike undirected evaluation, NED ig-
nores all errors directly resulting from an edge-flip.
9Otherwise, the gold parse would have contained a
w1?w2?w3?w1 cycle.
10An implementation of NED is available at
http://www.cs.huji.ac.il/?roys02/software/ned.html
NED is defined as follows: traverse over the to-
kens and mark a correct attachment if the token?s in-
duced parent is either (1) its gold parent (2) its gold
child or (3) its gold grandparent. The score is the ra-
tio of correct attachments and the number of tokens.
NED, by its definition, ignores edge-flips. Con-
sider again Figure 3, where we assume that 3(a) is
the gold standard and that 3(b) is the induced parse.
Much like undirected evaluation, NED will mark the
attachment of w2 as correct, since its induced parent
is its gold child. However, unlike undirected evalua-
tion, w3?s induced attachment will also be marked as
correct, as its induced parent is its gold grandparent.
Now consider another induced parse in which the
direction of the edge between w2 and w3 is switched
and the w3?s parent is set to be some other word,
w4 (Figure 3(c)). This should be marked as an er-
ror, even if the direction of the edge between w2 and
w3 is controversial, since the structure [w2, w3] is no
longer a dependent of w1. It is indeed a NED error.
Note that undirected evaluation gives the parses in
Figure 3(b) and Figure 3(c) the same score, while if
the structure [w2, w3] is problematic, there is a major
difference in their correctness.
Discussion. Problematic structures are ubiquitous,
with more than 40% of the tokens in PTB WSJ
appearing in at least one of them (see Section 3).
Therefore, even a substantial difference in the at-
tachment between two parsers is not necessarily in-
dicative of a true quality difference. However, an at-
tachment score difference that persists under NED is
an indication of a true quality difference, since gen-
erally problematic structures are local (i.e., obtained
by an edge-flip) and NED ignores such errors.
Reporting NED alone is insufficient, as obviously
the edge direction does matter in some cases. For
example, in adjective?noun structures (e.g., ?big
house?), the correct edge direction is widely agreed
upon (?big???house?) (Ku?bler et al, 2009), and
thus choosing the wrong direction should be con-
sidered an error. Therefore, we suggest evaluating
using both NED and attachment score in order to get
a full picture of the parser?s performance.
A possible criticism on NED is that it is only in-
different to alternative annotations in structures of
size 2 (e.g., ?to eat?) and does not necessarily handle
larger problematic structures, such as coordinations
668
ROOT
John
and Mary
(a)
ROOT
John
and
Mary
(b)
ROOT
in
house
the
(c)
ROOT
in
the
house
(d)
ROOT
house
in
the
(e)
Figure 4: Alternative parses of ?John and Mary? and ?in
the house?. Figure 4(a) follows (Collins, 1999), Fig-
ure 4(b) follows (Johansson and Nugues, 2007). Fig-
ure 4(c) follows (Collins, 1999; Yamada and Matsumoto,
2003). Figure 4(d) and Figure 4(e) show induced parses
made by (km04,saj10a) and cs09, respectively.
(see Section 4). For example, Figure 4(a) and Fig-
ure 4(b) present two alternative annotations of the
sentence ?John and Mary?. Assume the parse in Fig-
ure 4(a) is the gold parse and that in Figure 4(b) is
the induced parse. The word ?Mary? is a NED error,
since its induced parent (?and?) is neither its gold
child nor its gold grandparent. Thus, NED does not
accept all possible annotations of structures of size
3. On the other hand, using a method which accepts
all possible annotations of structures of size 3 seems
too permissive. A better solution may be to modify
the gold standard annotation, so to explicitly anno-
tate problematic structures as such. We defer this
line of research to future work.
NED is therefore an evaluation measure which is
indifferent to edge-flips, and is consequently less
sensitive to alternative annotations. We now show
that NED is indifferent to the differences between the
structures originally learned by the algorithms men-
tioned in Section 3 and the gold standard annotation
in all the problematic cases we consider.
Most of the modifications made are edge-flips,
and are therefore ignored by NED. The exceptions
are coordinations and prepositional phrases which
are structures of size 3. In the former, the alter-
native annotations differ only in a single edge-flip
(i.e., CC?NNP ), and are thus not NED errors. Re-
garding prepositional phrases, Figure 4(c) presents
the gold standard of ?in the house?, Figure 4(d) the
parse induced by km04 and saj10a and Figure 4(e)
the parse induced by cs09. As the reader can verify,
both induced parses receive a perfect NED score.
In order to further demonstrate NED?s insensitiv-
ity to alternative annotations, we took two of the
three common gold standard annotations (see Sec-
tion 4) and evaluated them one against the other. We
considered section 23 of WSJ following the scheme
of (Yamada and Matsumoto, 2003) as the gold stan-
dard and of (Collins, 1999) as the evaluated set. Re-
sults show that the attachment score is only 85.6%,
the undirected accuracy is improved to 90.3%, while
the NED score is 95.3%. This shows that NED is sig-
nificantly less sensitive to the differences between
the different annotation schemes, compared to the
other evaluation measures.
6 Experimenting with NED
In this section we show that NED indeed reduces
the performance difference between the original and
the modified parameter sets, thus providing empiri-
cal evidence for its validity. For brevity, we present
results only for the entire WSJ corpus. Results on
WSJ10 are similar. The datasets and decoding algo-
rithms are the same as those used in Section 3.
Table 3 shows the score differences between the
parameter sets using attachment score, undirected
evaluation and NED. A substantial difference per-
sists under undirected evaluation: a gap of 7.7% in
cs09, of 3.5% in saj10a and of 1.3% in km04.
The differences are further reduced using NED.
This is consistent with our discussion in Section 5,
and shows that undirected evaluation only ignores
some of the errors inflicted by edge-flips.
For cs09, the difference is substantially reduced,
but a 4.2% performance gap remains. For km04 and
saj10a, the original parameters outperform the new
ones by 3.6% and 1% respectively.
We can see that even when ignoring edge-flips,
some difference remains, albeit not necessarily in
the favor of the modified models. This is because
we did not directly perform edge-flips, but rather
parameter-flips. The difference is thus a result of
second-order effects stemming from the parameter-
flips. In the next section, we explain why the remain-
ing difference is positive for some algorithms (cs09)
and negative for others (km04, saj10a).
For completeness, Table 4 shows a comparison of
some of the current state-of-the-art algorithms, using
attachment score, undirected evaluation and NED.
The training and test sets are those used in Section 3.
The table shows that the relative orderings of the al-
gorithms under NED is different than under the other
669
Algo. Mod. ? Orig.Attach. Undir. NED
km04 9.3 (43.9?34.6) 1.3 (54.2?52.9) ?3.6 (63?66.6)
cs09 14.7 (54.6?39.9) 7.7 (56.9?49.2) 4.2 (66.8?62.6)
saj10a 12.7 (54.3?41.6) 3.5 (59.4?55.9) ?1 (66.8?67.8)
Table 3: Differences between the modified and original
parameter sets when evaluated using attachment score
(Attach.), undirected evaluation (Undir.), and NED.
measures. This is an indication that NED provides a
different perspective on algorithm quality11 .
Algo. Att10 Att? Un10 Un? NED10 NED?
bbdk10 66.1 49.6 70.1 56.0 75.5 61.8
bc10 67.2 53.6 73 61.7 81.6 70.2
cs09 61.5 42 66.9 50.4 81.5 62.9
gggtp10 57.1 45 62.5 53.2 80.4 65.1
km04 45.8 34.6 60.3 52.9 78.4 66.6
saj10a 54.7 41.6 66.5 55.9 78.9 67.8
saj10c 63.8 46.1 72.6 58.8 84.2 70.8
saj10b? 67.9 48.2 74.0 57.7 86.0 70.7
Table 4: A comparison of recent works, using Att (at-
tachment score) Un (undirected evaluation) and NED, on
sentences of length ? 10 (excluding punctuation) and
on all sentences. The gold standard is obtained using
the rules of (Yamada and Matsumoto, 2003). bbdk10:
(Berg-Kirkpatrick et al, 2010), bc10: (Blunsom and
Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10:
(Gillenwater et al, 2010), km04: A replication of (Klein
and Manning, 2004), saj10a: (Spitkovsky et al, 2010a),
saj10c: (Spitkovsky et al, 2010c), saj10b?: A lightly-
supervised algorithm (Spitkovsky et al, 2010b).
7 Discussion
In this paper we explored two ways of dealing with
cases in which there is no clear theoretical justifi-
cation to prefer one dependency structure over an-
other. Our experiments suggest that it is crucial to
deal with such structures if we would like to have
a proper evaluation of unsupervised parsing algo-
rithms against a gold standard.
The first way was to modify the parameters of the
parsing algorithms so that in cases where such prob-
lematic decisions are to be made they follow the gold
standard annotation. Indeed, this modification leads
to a substantial improvement in the attachment score
of the algorithms.
11Results may be different than the ones published in the
original papers due to the different conversion procedures used
in each work. See Section 4 for discussion.
The second way was to change the evaluation.
The NED measure we proposed does not punish for
differences between gold and induced structures in
the problematic cases. Indeed, in Section 6 (Table 3)
we show that the differences between the original
and modified models are much smaller when eval-
uating with NED compared to when evaluating with
the traditional attachment score.
As Table 3 reveals, however, even when evaluat-
ing with NED, there is still some difference between
the original and the modified model, for each of the
algorithms we consider. Moreover, for two of the al-
gorithms (km04 and saj10a) NED prefers the original
model while for one (cs09) it prefers the modified
version. In this section we explain these patterns and
show that they are both consistent and predictable.
Our hypothesis, for which we provide empirical
justification, is that in cases where there is no theo-
retically preferred annotation, NED prefers the struc-
tures that are more learnable by DMV. That is, NED
gives higher scores to the annotations that better fit
the assumptions and modeling decisions of DMV,
the model that lies in the basis of the parsing algo-
rithms.
To support our hypothesis we perform an experi-
ment requiring two preparatory steps for each algo-
rithm. First, we construct a supervised version of
the algorithm. This supervised version consists of
the same statistical model as the original unsuper-
vised algorithm, but the parameters are estimated to
maximize the likelihood of a syntactically annotated
training corpus, rather than of a plain text corpus.
Second, we construct two corpora for the algo-
rithm, both consist of the same text and differ only
in their syntactic annotation. The first is annotated
with the gold standard annotation. The second is
similarly annotated except in the linguistically prob-
lematic structures. We replace these structures with
the ones that would have been created with the un-
supervised version of the algorithm (see Table 1 for
the relevant structures for each algorithm)12. Each
12In cases the structures are comprised of a single edge, the
second corpus is obtained from the gold standard by an edge-
flip. The only exceptions are the cases of the prepositional
phrases. Their gold standard and the learned structures for each
of the algorithms are shown in Figure 4. In this case, the sec-
ond corpus is obtained from the gold standard by replacing each
prepositional phrase in the gold standard with the corresponding
670
corpus is divided into a training and a test set.
We then train the supervised version of the algo-
rithms on each of the training sets. We parse the test
data twice, once with each of the resulting models.
We evaluate both parsed corpora against the corpus
annotation from which they originated.
The training set of each corpus consists of sec-
tions 2?21 of WSJ20 (i.e., WSJ sentences of length
?20, excluding punctuation)13 and the test set is sec-
tion 23 of WSJ?. Evaluation is performed using
both NED and attachment score. The patterns we
observed are very similar for both. For brevity, we
report only attachment score results.
km04 cs09 saj10a
Orig. Gold Orig. Gold Orig. Gold
NED,
Unsup. 66.6 63 62.6 66.8 67.8 66.8
Sup. 71.3 69.9 63.3 69.9 71.8 69.9
Table 5: The first line shows the NED results from
Section 6, when using the original parameters (Orig.
columns) and the modified parameters (Gold columns).
The second line shows the results of the supervised ver-
sions of the algorithms using the corpus which agrees
with the unsupervised model in the problematic cases
(Orig.) and the gold standard (Gold).
The results of our experiment are presented in Ta-
ble 5 along with a comparison to the NED scores
from Section 6. The table clearly demonstrates that a
set of parameters (original or modified) is preferred
by NED in the unsupervised experiments reported in
Section 6 (top line) if and only if the structures pro-
duced by this set are better learned by the supervised
version of the algorithm (bottom line).
This observation supports our hypothesis that in
cases where there is no theoretical preference for
one structure over the other, NED (unlike the other
measures) prefers the structures that are more con-
sistent with the modeling assumptions lying in the
basis of the algorithm. We consider this to be a de-
sired property of a measure since a more consistent
model should be preferred where no theoretical pref-
erence exists.
learned structure.
13In using WSJ20, we follow (Spitkovsky et al, 2010a),
which showed that training the DMV on sentences of bounded
length yields a higher score than using the entire corpus. We
use it as we aim to use an optimal setting.
8 Conclusion
In this paper we showed that the standard evalua-
tion of unsupervised dependency parsers is highly
sensitive to problematic annotations. We modified a
small set of parameters that controls the annotation
in such problematic cases in three leading parsers.
This resulted in a major performance boost, which
is unindicative of a true difference in quality.
We presented Neutral Edge Direction (NED), a
measure that is less sensitive to the annotation of
local structures. As the problematic structures are
generally local, NED is less sensitive to their alterna-
tive annotations. In the future, we suggest reporting
NED along with the current measures.
Acknowledgements. We would like to thank Shay
Cohen for his assistance with his implementation of
the DMV parser and Taylor Berg-Kirkpatrick, Phil
Blunsom and Jennifer Gillenwater for providing us
with their data sets. We would also like to thank
Valentin I. Spitkovsky for his comments and for pro-
viding us with his data sets.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero and Dan Klein, 2010. Painless unsu-
pervised learning with features. In Proc. of NAACL.
Taylor Berg-Kirkpatrick and Dan Klein, 2010. Phyloge-
netic Grammar Induction. In Proc. of ACL.
Cristina Bosco and Vincenzo Lombardo, 2004. Depen-
dency and relational structure in treebank annotation.
In Proc. of the Workshop on Recent Advances in De-
pendency Grammar at COLING?04.
Phil Blunsom and Trevor Cohn, 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP.
Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008.
Logistic Normal Priors for Unsupervised Probabilistic
Grammar Induction. In Proc. of NIPS.
Shay B. Cohen and Noah A. Smith, 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying. In
Proc. of HLT-NAACL.
Michael J. Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Hal Daume? III, 2009. Unsupervised search-based struc-
tured prediction. In Proc. of ICML.
671
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o V. Grac?a and Fernando Pereira,
2007. Frustratingly Hard Domain Adaptation for De-
pendency Parsing. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
Gregory Druck, Gideon Mann and Andrew McCal-
lum, 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In
Proc. of ACL.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o V. Grac?a,
Ben Taskar and Fernando Preira, 2010. Sparsity in
dependency grammar induction. In Proc. of ACL.
William P. Headden III, David McClosky, and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. In Proc. of
COLING.
William P. Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Proc.
of HLT-NAACL.
Richard Johansson and Pierre Nugues, 2007. Ex-
tended Constituent-to-Dependency Conversion for En-
glish. In Proc. of NODALIDA.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL.
Sandra Ku?bler, 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proc. of RANLP.
Sandra Ku?bler, R. McDonald and Joakim Nivre, 2009.
Dependency Parsing. Morgan And Claypool Publish-
ers.
Mitchell Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics 19:313-330.
Tahira Naseem, Harr Chen, Regina Barzilay and Mark
Johnson, 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. of EMNLP.
Joakim Nivre, 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre, Johan Hall and Jens Nilsson, 2006. Malt-
Parser: A data-driven parser-generator for depen-
dency parsing. In Proc. of LREC-2006.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel and Deniz Yuret,
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of the CoNLL Shared Task, EMNLP-
CoNLL, 2007.
Jens Nilsson, Joakim Nivre and Johan Hall, 2006. Graph
transformations in data-driven dependency parsing.
In Proc. of ACL.
Owen Rambow, Cassandre Creswell, Rachel Szekely,
Harriet Tauber and Marilyn Walker, 2002. A depen-
dency treebank for English. In Proc. of LREC.
Noah A. Smith and Jason Eisner, 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proc. of IJCAI.
Noah A. Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induc-
tion. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010a. From Baby Steps to Leapfrog: How ?Less
is More? in Unsupervised Dependency Parsing. In
Proc. of NAACL-HLT.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010b. Profiting from Mark-Up: Hyper-Text An-
notations for Guided Parsing. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky
and Christopher D. Manning, 2010c. Viterbi training
improves unsupervised dependency parsing. In Proc.
of CoNLL.
Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005.
Strictly Lexical Dependency Parsing. In IWPT.
Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schu-
urmans, 2006. Improved Large Margin Dependency
Parsing via Local Constraints and Laplacian Regular-
ization. In Proc. of CoNLL.
Hiroyasu Yamada and Yuji Matsumoto, 2003. Statistical
dependency analysis with support vector machines. In
Proc. of the International Workshop on Parsing Tech-
nologies.
672
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1486?1495,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Confidence Driven Unsupervised Semantic Parsing
Dan Goldwasser ? Roi Reichart ? James Clarke ? Dan Roth ?
?Department of Computer Science, University of Illinois at Urbana-Champaign
{goldwas1,clarkeje,danr}@illinois.edu
?Computer Science and Artificial Intelligence Laboratory, MIT
roiri@csail.mit.edu
Abstract
Current approaches for semantic parsing take
a supervised approach requiring a consider-
able amount of training data which is expen-
sive and difficult to obtain. This supervision
bottleneck is one of the major difficulties in
scaling up semantic parsing.
We argue that a semantic parser can be trained
effectively without annotated data, and in-
troduce an unsupervised learning algorithm.
The algorithm takes a self training approach
driven by confidence estimation. Evaluated
over Geoquery, a standard dataset for this
task, our system achieved 66% accuracy, com-
pared to 80% of its fully supervised counter-
part, demonstrating the promise of unsuper-
vised approaches for this task.
1 Introduction
Semantic parsing, the ability to transform Natural
Language (NL) input into a formal Meaning Repre-
sentation (MR), is one of the longest standing goals
of natural language processing. The importance of
the problem stems from both theoretical and practi-
cal reasons, as the ability to convert NL into a formal
MR has countless applications.
The term semantic parsing has been used ambigu-
ously to refer to several semantic tasks (e.g., se-
mantic role labeling). We follow the most common
definition of this task: finding a mapping between
NL input and its interpretation expressed in a well-
defined formal MR language. Unlike shallow se-
mantic analysis tasks, the output of a semantic parser
is complete and unambiguous to the extent it can be
understood or even executed by a computer system.
Current approaches for this task take a data driven
approach (Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007), in which the learning algorithm is
given a set of NL sentences as input and their cor-
responding MR, and learns a statistical semantic
parser ? a set of parameterized rules mapping lex-
ical items and syntactic patterns to their MR. Given
a sentence, these rules are applied recursively to de-
rive the most probable interpretation.
Since semantic interpretation is limited to the syn-
tactic patterns observed in the training data, in or-
der to work well these approaches require consider-
able amounts of annotated data. Unfortunately an-
notating sentences with their MR is a time consum-
ing task which requires specialized domain knowl-
edge and therefore minimizing the supervision ef-
fort is one of the key challenges in scaling semantic
parsers.
In this work we present the first unsupervised
approach for this task. Our model compensates
for the lack of training data by employing a self
training protocol based on identifying high confi-
dence self labeled examples and using them to re-
train the model. We base our approach on a sim-
ple observation: semantic parsing is a difficult struc-
tured prediction task, which requires learning a com-
plex model, however identifying good predictions
can be done with a far simpler model capturing re-
peating patterns in the predicted data. We present
several simple, yet highly effective confidence mea-
sures capturing such patterns, and show how to use
them to train a semantic parser without manually an-
notated sentences.
Our basic premise, that predictions with high con-
fidence score are of high quality, is further used to
improve the performance of the unsupervised train-
1486
ing procedure. Our learning algorithm takes an EM-
like iterative approach, in which the predictions of
the previous stage are used to bias the model. While
this basic scheme was successfully applied to many
unsupervised tasks, it is known to converge to a
sub optimal point. We show that by using confi-
dence estimation as a proxy for the model?s pre-
diction quality, the learning algorithm can identify
a better model compared to the default convergence
criterion.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), consist-
ing of natural language questions and their prolog
interpretations used to query a database consisting
of U.S. geographical information. Our experimental
results show that using our approach we are able to
train a good semantic parser without annotated data,
and that using a confidence score to identify good
models results in a significant performance improve-
ment.
2 Semantic Parsing
We formulate semantic parsing as a structured pre-
diction problem, mapping a NL input sentence (de-
noted x), to its highest ranking MR (denoted z). In
order to correctly parametrize and weight the pos-
sible outputs, the decision relies on an intermediate
representation: an alignment between textual frag-
ments and their meaning representation (denoted y).
Fig. 1 describes a concrete example of this termi-
nology. In our experiments the input sentences x
are natural language queries about U.S. geography
taken from the Geoquery dataset. The meaning rep-
resentation z is a formal language database query,
this output representation language is described in
Sec. 2.1.
The prediction function, mapping a sentence to its
corresponding MR, is formalized as follows:
z? = Fw(x) = arg max
y?Y,z?Z
wT?(x,y, z) (1)
Where ? is a feature function defined over an input
sentence x, alignment y and output z. The weight
vector w contains the model?s parameters, whose
values are determined by the learning process.
We refer to the arg max above as the inference
problem. Given an input sentence, solving this in-
How many states does the Colorado river run through? 
count( state( traverse( river( const(colorado))))
x 
z 
y 
Figure 1: Example of an input sentence (x), meaning rep-
resentation (z) and the alignment between the two (y) for
the Geoquery domain
ference problem based on ? and w is what com-
promises our semantic parser. In practice the pars-
ing decision is decomposed into smaller decisions
(Sec. 2.2). Sec. 4 provides more details about the
feature representation and inference procedure used.
Current approaches obtain w using annotated
data, typically consisting of (x, z) pairs. In Sec. 3 we
describe our unsupervised learning procedure, that is
how to obtain w without annotated data.
2.1 Target Meaning Representation
The output of the semantic parser is a logical for-
mula, grounding the semantics of the input sen-
tence in the domain language (i.e., the Geoquery
domain). We use a subset of first order logic con-
sisting of typed constants (corresponding to specific
states, etc.) and functions, which capture relations
between domains entities and properties of entities
(e.g., population : E ? N ). The seman-
tics of the input sentence is constructed via func-
tional composition, done by the substitution oper-
ator. For example, given the function next to(x)
and the expression const(texas), substitution
replaces the occurrence of the free variable x
with the expression, resulting in a new formula:
next to(const(texas)). For further details
we refer the reader to (Zelle and Mooney, 1996).
2.2 Semantic Parsing Decisions
The inference problem described in Eq. 1 selects the
top ranking output formula. In practice this decision
is decomposed into smaller decisions, capturing lo-
cal mapping of input tokens to logical fragments and
their composition into larger fragments. These deci-
sions are further decomposed into a feature repre-
sentation, described in Sec. 4.
The first type of decisions are encoded directly by
the alignment (y) between the input tokens and their
corresponding predicates. We refer to these as first
1487
order decisions. The pairs connected by the align-
ment (y) in Fig. 1 are examples of such decisions.
The final output structure z is constructed by
composing individual predicates into a complete
formula. For example, consider the formula pre-
sented in Fig. 1: river( const(colorado))
is a composition of two predicates river and
const(colorado). We refer to the composition
of two predicates, associated with their respective
input tokens, as second order decisions.
In order to formulate these decisions, we intro-
duce the following notation. c is a constituent in the
input sentence x and D is the set of all function and
constant symbols in the domain. The alignment y is
a set of mappings between constituents and symbols
in the domain y = {(c, s)} where s ? D.
We denote by si the i-th output predicate compo-
sition in z, by si?1(si) the composition of the (i?1)-
th predicate on the i-th predicate and by y(si) the in-
put word corresponding to that predicate according
to the alignment y.
3 Unsupervised Semantic Parsing
Our learning framework takes a self training ap-
proach in which the learner is iteratively trained over
its own predictions. Successful application of this
approach depends heavily on two important factors
- how to select high quality examples to train the
model on, and how to define the learning objective
so that learning can halt once a good model is found.
Both of these questions are trivially answered
when working in a supervised setting: by using the
labeled data for training the model, and defining the
learning objective with respect to the annotated data
(for example, loss-minimization in the supervised
version of our system).
In this work we suggest to address both of the
above concerns by approximating the quality of
the model?s predictions using a confidence measure
computed over the statistics of the self generated
predictions. Output structures which fall close to the
center of mass of these statistics will receive a high
confidence score.
The first issue is addressed by using examples as-
signed a high confidence score to train the model,
acting as labeled examples.
We also note that since the confidence score pro-
vides a good indication for the model?s prediction
performance, it can be used to approximate the over-
all model performance, by observing the model?s to-
tal confidence score over all its predictions. This
allows us to set a performance driven goal for our
learning process - return the model maximizing the
confidence score over all predictions. We describe
the details of integrating the confidence score into
the learning framework in Sec. 3.1.
Although using the model?s prediction score (i.e.,
wT?(x,y, z)) as an indication of correctness is a
natural choice, we argue and show empirically, that
unsupervised learning driven by confidence estima-
tion results in a better performing model. This
empirical behavior also has theoretical justification:
training the model using examples selected accord-
ing to the model?s parameters (i.e., the top rank-
ing structures) may not generalize much further be-
yond the existing model, as the training examples
will simply reinforce the existing model. The statis-
tics used for confidence estimation are different than
those used by the model to create the output struc-
tures, and can therefore capture additional informa-
tion unobserved by the prediction model. This as-
sumption is based on the well established idea of
multi-view learning, applied successfully to many
NL applications (Blum and Mitchell, 1998; Collins
and Singer, 1999). According to this idea if two
models use different views of the data, each of them
can enhance the learning process of the other.
The success of our learning procedure hinges
on finding good confidence measures, whose confi-
dence prediction correlates well with the true quality
of the prediction. The ability of unsupervised confi-
dence estimation to provide high quality confidence
predictions can be explained by the observation that
prominent prediction patterns are more likely to be
correct. If a non-random model produces a predic-
tion pattern multiple times it is likely to be an in-
dication of an underlying phenomenon in the data,
and therefore more likely to be correct. Our specific
choice of confidence measures is guided by the intu-
ition that unlike structure prediction (i.e., solving the
inference problem) which requires taking statistics
over complex and intricate patterns, identifying high
quality predictions can be done using much simpler
patterns that are significantly easier to capture.
In the reminder of this section we describe our
1488
Algorithm 1 Unsupervised Confidence driven
Learning
Input: Sentences {xl}Nl=1,
initial weight vector w
1: define Confidence : X ? Y ? Z ? R,
i = 0, Si = ?
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = arg maxy,zw
T?(xl,y, z)
5: Si = Si ? {xl, y?, z?}
6: end for
7: Confidence = compute confidence statistics
8: Sconfi = select from Si using Confidence
9: wi ? Learn(?iS
conf
i )
10: i = i+ 1
11: until Sconfi has no new unique examples
12: best = arg maxi(
?
s?Si
Confidence(s))/|S|
13: return wbest
learning approach. We begin by introducing the
overall learning framework (Sec. 3.1), we then ex-
plain the rational behind confidence estimation over
self-generated data and introduce the confidence
measures used in our experiments (Sec. 3.2). We
conclude with a description of the specific learning
algorithms used for updating the model (Sec. 3.3).
3.1 Unsupervised Confidence-Driven Learning
Our learning framework works in an EM-like
manner, iterating between two stages: making pre-
dictions based on its current set of parameters and
then retraining the model using a subset of the pre-
dictions, assigned high confidence. The learning
process ?discovers? new high confidence training
examples to add to its training set over multiple it-
erations, and converges when the model no longer
adds new training examples.
While this is a natural convergence criterion, it
provides no performance guarantees, and in practice
it is very likely that the quality of the model (i.e., its
performance) fluctuates during the learning process.
We follow the observation that confidence estima-
tion can be used to approximate the performance of
the entire model and return the model with the high-
est overall prediction confidence.
We describe this algorithmic framework in detail
in Alg. 1. Our algorithm takes as input a set of
natural language sentences and a set of parameters
used for making the initial predictions1. The algo-
rithm then iterates between the two stages - predict-
ing the output structure for each sentence (line 4),
and updating the set of parameters (line 9). The
specific learning algorithms used are discussed in
Sec. 3.3. The training examples required for learn-
ing are obtained by selecting high confidence exam-
ples - the algorithm first takes statistics over the cur-
rent predicted set of output structures (line 7), and
then based on these statistics computes a confidence
score for each structure, selecting the top ranked
ones as positive training examples, and if needed,
the bottom ones as negative examples (line 8). The
set of top confidence examples (for either correct or
incorrect prediction), at iteration i of the algorithm,
is denoted Sconfi . The exact nature of the confidence
computation is discussed in Sec. 3.2.
The algorithm iterates between these two stages,
at each iteration it adds more self-annotated exam-
ples to its training set, learning therefore converges
when no new examples are added (line 11). The al-
gorithm keeps track of the models it trained at each
stage throughout this process, and returns the one
with the highest averaged overall confidence score
(lines 12-13). At each stage, the overall confidence
score is computed by averaging over all the confi-
dence scores of the predictions made at that stage.
3.2 Unsupervised Confidence Estimation
Confidence estimation is calculated over a batch of
input (x) - output (z) pairs. Each pair decomposes
into smaller first order and second order decisions
(defined Sec. 2.2). Confidence estimation is done by
computing the statistics of these decisions, over the
entire set of predicted structures. In the rest of this
section we introduce the confidence measures used
by our system.
Translation Model The first approach essentially
constructs a simplified translation model, capturing
word-to-predicate mapping patterns. This can be
considered as an abstraction of the prediction model:
we collapse the intricate feature representation into
1Since we commit to the max-score output prediction, rather
than summing over all possibilities, we require a reasonable ini-
tialization point. We initialized the weight vector using simple,
straight-forward heuristics described in Sec. 5.
1489
high level decisions and take statistics over these de-
cisions. Since it takes statistics over considerably
less variables than the actual prediction model, we
expect this model to make reliable confidence pre-
dictions. We consider two variations of this ap-
proach, the first constructs a unigram model over the
first order decisions and the second a bigram model
over the second order decisions. Formally, given a
set of predicted structures we define the following
confidence scores:
Unigram Score:
p(z|x) =
|z|?
i=1
p(si|y(si))
Bigram Score:
p(z|x) =
|z|?
i=1
p(si?1(si)|y(si?1), y(si))
Structural Proportion Unlike the first approach
which decomposes the predicted structure into in-
dividual decisions, this approach approximates the
model?s performance by observing global properties
of the structure. We take statistics over the propor-
tion between the number of predicates in z and the
number of words in x.
Given a set of structure predictions S, we com-
pute this proportion for each structure (denoted as
Prop(x, z)) and calculate the average proportion
over the entire set (denoted as AvProp(S)). The
confidence score assigned to a given structure (x,y)
is simply the difference between its proportion and
the averaged proportion, or formally
PropScore(S, (x, z)) = AvProp(S)?Prop(x, z)
This measure captures the global complexity of the
predicted structure and penalizes structures which
are too complex (high negative values) or too sim-
plistic (high positive values).
Combined The two approaches defined above
capture different views of the data, a natural question
is then - can these two measures be combined to pro-
vide a more powerful estimation? We suggest a third
approach which combines the first two approaches.
It first uses the score produced by the latter approach
to filter out unlikely candidates, and then ranks the
remaining ones with the former approach and selects
those with the highest rank.
3.3 Learning Algorithms
Given a set of self generated structures, the param-
eter vector can be updated (line 9 in Alg. 1). We
consider two learning algorithm for this purpose.
The first is a binary learning algorithm, which
considers learning as a classification problem, that
is finding a set of weights w that can best sepa-
rate correct from incorrect structures. The algo-
rithm decomposes each predicted formula and its
corresponding input sentence into a feature vector
?(x,y, z) normalized by the size of the input sen-
tence |x|, and assigns a binary label to this vector2.
The learning process is defined over both positive
and negative training examples. To accommodate
that we modify line 8 in Alg. 1, and use the con-
fidence score to select the top ranking examples as
positive examples, and the bottom ranking examples
as negative examples. We use a linear kernel SVM
with squared-hinge loss as the underlying learning
algorithm.
The second is a structured learning algorithm
which considers learning as a ranking problem, i.e.,
finding a set of weights w such that the ?gold struc-
ture? will be ranked on top, preferably by a large
margin to allow generalization.The structured learn-
ing algorithm can directly use the top ranking pre-
dictions of the model (line 8 in Alg. 1) as training
data. In this case the underlying algorithm is a struc-
tural SVM with squared-hinge loss, using hamming
distance as the distance function. We use the cutting-
plane method to efficiently optimize the learning
process? objective function.
4 Model
Semantic parsing as formulated in Eq. 1 is an in-
ference procedure selecting the top ranked output
logical formula. We follow the inference approach
in (Roth and Yih, 2007; Clarke et al, 2010) and
formalize this process as an Integer Linear Program
(ILP). Due to space consideration we provide a brief
description, and refer the reader to that paper for
more details.
2Without normalization longer sentences would have more
influence on binary learning problem. Normalization is there-
fore required to ensure that each sentence contributes equally to
the binary learning problem regardless of its length.
1490
4.1 Inference
The inference decision (Eq. 1) is decomposed into
smaller decisions, capturing mapping of input to-
kens to logical fragments (first order) and their com-
position into larger fragments (second order). We
encode a first-order decision as ?cs, a binary vari-
able indicating that constituent c is aligned with the
logical symbol s. A second-order decision ?cs,dt, is
encoded as a binary variable indicating that the sym-
bol t (associated with constituent d) is an argument
of a function s (associated with constituent c). We
frame the inference problem over these decisions:
Fw(x) = arg max
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?w
T?2(x, c, s, d, t) (2)
We restrict the possible assignments to the deci-
sion variables, forcing the resulting output formula
to be syntactically legal, for example by restricting
active ?-variables to be type consistent, and force
the resulting functional composition to be acyclic.
We take advantage of the flexible ILP framework,
and encode these restrictions as global constraints
over Eq. 2. We refer the reader to (Clarke et al,
2010) for a full description of the constraints used.
4.2 Features
The inference problem defined in Eq. (2) uses two
feature functions: ?1 and ?2.
First-order decision features ?1 Determining if
a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.3 Existing ap-
proaches rely on annotated data to extend the lexi-
con. Instead we rely on external knowledge (Miller
et al, 1990) and add features which measure the lex-
ical similarity between a constituent and a logical
symbol?s surface forms (as defined by the lexicon).
3The lexicon contains on average 1.42 words per function
and 1.07 words per constant.
Model Description
INITIAL MODEL Manually set weights (Sec. 5.1)
PRED. SCORE normalized prediction (Sec. 5.1)
ALL EXAMPLES All top structures (Sec. 5.1)
UNIGRAM Unigram score (Sec. 3.2)
BIGRAM Bigram score (Sec. 3.2)
PROPORTION Words-predicate prop (Sec. 3.2)
COMBINED Combined estimators (Sec. 3.2)
RESPONSE BASED Supervised (binary) (Sec. 5.1)
SUPERVISED Fully Supervised (Sec. 5.1)
Table 1: Compared systems and naming conventions.
Second-order decision features ?2 Second order
decisions rely on syntactic information. We use
the dependency tree of the input sentence. Given
a second-order decision ?cs,dt, the dependency fea-
ture takes the normalized distance between the head
words in the constituents c and d. In addition, a set
of features indicate which logical symbols are usu-
ally composed together, without considering their
alignment to the text.
5 Experiments
In this section we describe our experimental evalua-
tion. We compare several confidence measures and
analyze their properties. Tab. 1 defines the naming
conventions used throughout this section to refer to
the different models we evaluated. We begin by de-
scribing our experimental setup and then proceed to
describe the experiments and their results. For the
sake of clarity we focus on the best performing mod-
els (COMBINED using BIGRAM and PROPORTION)
first and discuss other models later in the section.
5.1 Experimental Settings
In all our experiments we used the Geoquery
dataset (Zelle and Mooney, 1996), consisting of U.S.
geography NL questions and their corresponding
Prolog logical MR. We used the data split described
in (Clarke et al, 2010), consisting of 250 queries for
evaluation purposes. We compared our system to
several supervised models, which were trained us-
ing a disjoint set of queries. Our learning system
had access only to the NL questions, and the log-
ical forms were only used to evaluate the system?s
performance. We report the proportion of correct
structures (accuracy). Note that this evaluation cor-
1491
responds to the 0/1 loss over the predicted structures.
Initialization Our learning framework requires an
initial weight vector as input. We use a straight for-
ward heuristic and provide uniform positive weights
to three features. This approach is similar in spirit
to previous works (Clarke et al, 2010; Zettlemoyer
and Collins, 2007). We refer to this system as INI-
TIAL MODEL throughout this section.
Competing Systems We compared our system to
several other systems:
(1) PRED. SCORE: An unsupervised frame-
work using the model?s internal prediction score
(wT?(x,y, z)) for confidence estimation.
(2) ALL EXAMPLES: Treating all predicted struc-
tures as correct, i.e., at each iteration the model is
trained over all the predictions it made. The re-
ported score was obtained by selecting the model at
the training iteration with the highest overall confi-
dence score (see line 12 in Alg. 1).
(3) RESPONSE BASED: A natural upper bound to
our framework is the approach used in (Clarke et al,
2010). While our approach is based on assessing
the correctness os the model?s predictions according
to unsupervised confidence estimation, their frame-
work is provided with external supervision for these
decisions, indicating if the predicted structures are
correct.
(4) SUPERVISED: A fully supervised framework
trained over 250 (x, z) pairs using structured SVM.
5.2 Results
Our experiments aim to clarify three key points:
(1) Can a semantic parser indeed be trained with-
out any form of external supervision? this is our
key question, as this is the first attempt to approach
this task with an unsupervised learning protocol.4 In
order to answer it, we report the overall performance
of our system in Tab. 2.
The manually constructed model INITIALMODEL
achieves a performance of 0.22. We can expect
learning to improve on this baseline. We com-
pare three self-trained systems, ALL EXAMPLES,
PREDICTIONSCORE and COMBINED, which differ
4While unsupervised learning for various semantic tasks has
been widely discussed, this is the first attempt to tackle this task.
We refer the reader to Sec. 6 for further discussion of this point.
in their sample selection strategy, but all use con-
fidence estimation for selecting the final seman-
tic parsing model. The ALL EXAMPLES approach
achieves an accuracy score of 0.656. PREDICTION-
SCORE only achieves a performance of 0.164 us-
ing the binary learning algorithm and 0.348 us-
ing the structured learning algorithm. Finally, our
confidence-driven technique COMBINED achieved a
score of 0.536 for the binary case and 0.664 for the
structured case, the best performing models in both
cases. As expected, the supervised systems RE-
SPONSE BASED and SUPERVISED achieve the best
performance.
These results show that training the model with
training examples selected carefully will improve
learning - as the best performance is achieved with
perfect knowledge of the predictions correctness
(RESPONSE BASED). Interestingly the difference
between the structured version of our system and
that of RESPONSE BASED is only 0.07, suggesting
that we can recover the binary feedback signal with
high precision. The low performance of the PRE-
DICTIONSCORE model is also not surprising, and it
demonstrates one of the key principles in confidence
estimation - the score should be comparable across
predictions done over different inputs, and not the
same input, as done in PREDICTIONSCORE model.
(2) How does confidence driven sample selection
contribute to the learning process? Comparing
the systems driven by confidence sample-selection
to the ALL EXAMPLES approach uncovers an inter-
esting tradeoff between training with more (noisy)
data and selectively training the system with higher
quality examples. We argue that carefully select-
ing high quality training examples will result in bet-
ter performance. The empirical results indeed sup-
port our argument, as the best performing model
(RESPONSE BASED) is achieved by sample selec-
tion with perfect knowledge of prediction correct-
ness. The confidence-based sample selection system
(COMBINED) is the best performing system out of
all the self-trained systems. Nonetheless, the ALL
EXAMPLES strategy performs well when compared
to COMBINED, justifying a closer look at that aspect
of our system.
We argue that different confidence measures cap-
ture different properties of the data, and hypothe-
1492
size that combining their scores will improve the re-
sulting model. In Tab. 3 we compare the results of
the COMBINED measure to the results of its individ-
ual components - PROPORTION and BIGRAM. We
compare these results both when using the binary
and structured learning algorithms. Results show
that using the COMBINED measure leads to an im-
proved performance, better than any of the individ-
ual measures, suggesting that it can effectively ex-
ploit the properties of each confidence measure. Fur-
thermore, COMBINED is the only sample selection
strategy that outperforms ALL EXAMPLES.
(3) Can confidence measures serve as a good
proxy for the model?s performance? In the unsu-
pervised settings we study the learning process may
not converge to an optimal model. We argue that
by selecting the model that maximizes the averaged
confidence score, a better model can be found. We
validate this claim empirically in Tab. 4. We com-
pare the performance of the model selected using
the confidence score to the performance of the fi-
nal model considered by the learning algorithm (see
Sec. 3.1 for details). We also compare it to the best
model achieved in any of the learning iterations.
Since these experiments required running the
learning algorithm many times, we focused on the
binary learning algorithm as it converges consider-
ably faster. In order to focus the evaluation on the
effects of learning, we ignore the initial model gen-
erated manually (INITIAL MODEL) in these exper-
iments. In order to compare models performance
across the different iterations fairly, a uniform scale,
such as UNIGRAM and BIGRAM, is required. In the
case of the COMBINED measure we used the BI-
GRAM measure for performance estimation, since it
is one of its underlying components. In the PRED.
SCORE and PROPORTION models we used both their
confidence prediction, and the simple UNIGRAM
confidence score to evaluate model performance (the
latter appear in parentheses in Tab. 4).
Results show that the over overall confidence
score serves as a reliable proxy for the model perfor-
mance - using UNIGRAM and BIGRAM the frame-
work can select the best performing model, far better
than the performance of the default model to which
the system converged.
Algorithm Supervision Acc.
INITIAL MODEL ? 0.222
SELF-TRAIN: (Structured)
PRED. SCORE ? 0.348
ALL EXAMPLES ? 0.656
COMBINED ? 0.664
SELF-TRAIN: (Binary)
PRED. SCORE ? 0.164
COMBINED ? 0.536
RESPONSE BASED
BINARY 250 (binary) 0.692
STRUCTURED 250 (binary) 0.732
SUPERVISED
STRUCTURED 250 (struct.) 0.804
Table 2: Comparing our Self-trained systems with
Response-based and supervised models. Results show
that our COMBINED approach outperforms all other un-
supervised models.
Algorithm Accuracy
SELF-TRAIN: (Structured)
PROPORTION 0.6
BIGRAM 0.644
COMBINED 0.664
SELF-TRAIN: (Binary)
BIGRAM 0.532
PROPORTION 0.504
COMBINED 0.536
Table 3: Comparing COMBINED to its components BI-
GRAM and PROPORTION. COMBINED results in a better
score than any of its components, suggesting that it can
exploit the properties of each measure effectively.
Algorithm Best Conf. estim. Default
PRED. SCORE 0.164 0.128 (0.164) 0.134
UNIGRAM 0.52 0.52 0.4
BIGRAM 0.532 0.532 0.472
PROPORTION 0.504 0.27 (0.504) 0.44
COMBINED 0.536 0.536 0.328
Table 4: Using confidence to approximate model perfor-
mance. We compare the best result obtained in any of the
learning algorithm iterations (Best), the result obtained
by approximating the best result using the averaged pre-
diction confidence (Conf. estim.) and the result of us-
ing the default convergence criterion (Default). Results
in parentheses are the result of using the UNIGRAM con-
fidence to approximate the model?s performance.
1493
6 Related Work
Semantic parsing has attracted considerable interest
in recent years. Current approaches employ various
machine learning techniques for this task, such as In-
ductive Logic Programming in earlier systems (Zelle
and Mooney, 1996; Tang and Mooney, 2000) and
statistical learning methods in modern ones (Ge and
Mooney, 2005; Nguyen et al, 2006; Wong and
Mooney, 2006; Kate and Mooney, 2006; Zettle-
moyer and Collins, 2005; Zettlemoyer and Collins,
2007; Zettlemoyer and Collins, 2009).
The difficulty of providing the required supervi-
sion motivated learning approaches using weaker
forms of supervision. (Chen and Mooney, 2008;
Liang et al, 2009; Branavan et al, 2009; Titov and
Kozhevnikov, 2010) ground NL in an external world
state directly referenced by the text. The NL input in
our setting is not restricted to such grounded settings
and therefore we cannot exploit this form of supervi-
sion. Recent work (Clarke et al, 2010; Liang et al,
2011) suggest using response-based learning proto-
cols, which alleviate some of the supervision effort.
This work takes an additional step in this direction
and suggest an unsupervised protocol.
Other approaches to unsupervised semantic anal-
ysis (Poon and Domingos, 2009; Titov and Kle-
mentiev, 2011) take a different approach to seman-
tic representation, by clustering semantically equiv-
alent dependency tree fragments, and identifying
their predicate-argument structure. While these ap-
proaches have been applied successfully to semantic
tasks such as question answering, they do not ground
the input in a well defined output language, an essen-
tial component in our task.
Our unsupervised approach follows a self training
protocol (Yarowsky, 1995; McClosky et al, 2006;
Reichart and Rappoport, 2007b) enhanced with con-
straints restricting the output space (Chang et al,
2007; Chang et al, 2009). A Self training proto-
col uses its own predictions for training. We esti-
mate the quality of the predictions and use only high
confidence examples for training. This selection cri-
terion provides an additional view, different than the
one used by the prediction model. Multi-view learn-
ing is a well established idea, implemented in meth-
ods such as co-training (Blum and Mitchell, 1998).
Quality assessment of a learned model output was
explored by many previous works (see (Caruana and
Niculescu-Mizil, 2006) for a survey), and applied
to several NL processing tasks such as syntactic
parsing (Reichart and Rappoport, 2007a; Yates et
al., 2006), machine translation (Ueffing and Ney,
2007), speech (Koo et al, 2001), relation extrac-
tion (Rosenfeld and Feldman, 2007), IE (Culotta and
McCallum, 2004), QA (Chu-Carroll et al, 2003)
and dialog systems (Lin and Weng, 2008).
In addition to sample selection we use confidence
estimation as a way to approximate the overall qual-
ity of the model and use it for model selection. This
use of confidence estimation was explored in (Re-
ichart et al, 2010), to select between models trained
with different random starting points. In this work
we integrate this estimation deeper into the learning
process, thus allowing our training procedure to re-
turn the best performing model.
7 Conclusions
We introduced an unsupervised learning algorithm
for semantic parsing, the first for this task to the best
of our knowledge. To compensate for the lack of
training data we use a self-training protocol, driven
by unsupervised confidence estimation. We demon-
strate empirically that our approach results in a high
preforming semantic parser and show that confi-
dence estimation plays a vital role in this success,
both by identifying good training examples as well
as identifying good over all performance, used to
improve the final model selection.
In future work we hope to further improve un-
supervised semantic parsing performance. Particu-
larly, we intend to explore new approaches for confi-
dence estimation and their usage in the unsupervised
and semi-supervised versions of the task.
Acknowledgments We thank the anonymous re-
viewers for their helpful feedback. This material
is based upon work supported by DARPA under
the Bootstrap Learning Program and Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, AFRL, or the US government.
1494
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In ACL.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised l earning algorithms. In
ICML.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the ACL.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
D. Chen and R. Mooney. 2008. Learning to sportscast: a
test of grounded language acquisition. In ICML.
J. Chu-Carroll, J. Prager K. Czuba, and A. Ittycheriah.
2003. In question answering, two heads are better than
on. In HLT-NAACL.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL, 7.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP?VLC.
A. Culotta and A. McCallum. 2004. Confidence estima-
tion for information extraction. In HLT-NAACL.
R. Ge and R. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In CoNLL.
R. Kate and R. Mooney. 2006. Using string-kernels for
learning semantic parsers. In ACL.
Y. Koo, C. Lee, and B. Juang. 2001. Speech recogni-
tion and utterance verification based on a generalized
confidence score. IEEE Transactions on Speech and
Audio Processing, 9(8):821?832.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
ACL.
P. Liang, M.I. Jordan, and D. Klein. 2011. Deep compo-
sitional semantics from shallow supervision. In ACL.
F. Lin and F. Weng. 2008. Computing confidence scores
for all sub parse trees. In ACL.
D. McClosky, E. Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In HLT-NAACL.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J.
Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Seman-
tic parsing with structured svm ensemble classification
models. In ACL.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In EMNLP.
R. Reichart and A. Rappoport. 2007a. An ensemble
method for selection of high quality parses. In ACL.
R. Reichart and A. Rappoport. 2007b. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
R. Reichart, R. Fattal, and A. Rappoport. 2010. Im-
proved unsupervised pos induction using intrinsic
clustering quality and a zipfian constraint. In CoNLL.
B. Rosenfeld and R. Feldman. 2007. Using corpus statis-
tics on entities to improve semi?supervised relation
extraction from the web. In ACL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construction
of database interfaces: integrating statistical and rela-
tional learning for semantic parsing. In EMNLP.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In ECML.
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In ACL.
I. Titov and M. Kozhevnikov. 2010. Bootstrapping
semantic analyzers from non-contradictory texts. In
ACL.
N. Ueffing and H. Ney. 2007. Word-level confidence es-
timation for machine translation. Computational Lin-
guistics, 33(1):9?40.
Y.W. Wong and R. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In
NAACL.
Y.W. Wong and R. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised method. In ACL.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In EMNLP.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
AAAI.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI.
L. Zettlemoyer and M. Collins. 2007. Online learning of
relaxed CCG grammars for parsing to logical form. In
CoNLL.
L. Zettlemoyer and M. Collins. 2009. Learning context-
dependent mappings from sentences to logical form.
In ACL.
1495
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862?872,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improved Lexical Acquisition through DPP-based Verb Clustering
Roi Reichart
University of Cambridge, UK
rr439@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Abstract
Subcategorization frames (SCFs), selec-
tional preferences (SPs) and verb classes
capture related aspects of the predicate-
argument structure. We present the first
unified framework for unsupervised learn-
ing of these three types of information.
We show how to utilize Determinantal
Point Processes (DPPs), elegant proba-
bilistic models that are defined over the
possible subsets of a given dataset and
give higher probability mass to high qual-
ity and diverse subsets, for clustering. Our
novel clustering algorithm constructs a
joint SCF-DPP DPP kernel matrix and uti-
lizes the efficient sampling algorithms of
DPPs to cluster together verbs with sim-
ilar SCFs and SPs. We evaluate the in-
duced clusters in the context of the three
tasks and show results that are superior to
strong baselines for each 1.
1 Introduction
Verb classes (VCs), subcategorization frames
(SCFs) and selectional preferences (SPs) capture
different aspects of predicate-argument structure.
SCFs describe the syntactic realization of verbal
predicate-argument structure, SPs capture the se-
mantic preferences verbs have for their arguments
and VCs in the Levin (1993) tradition provide a
shared level of abstraction for verbs that share
many aspects of their syntactic and semantic be-
havior.
These three of types of information have proved
useful for Natural Language Processing (NLP)
1The source code of the clustering algorithms and evalu-
ation is submitted with this paper and will be made publicly
available upon acceptance of the paper.
tasks which require information about predicate-
argument structure, including parsing (Shi and Mi-
halcea, 2005; Cholakov and van Noord, 2010;
Zhou et al, 2011), semantic role labeling (Swier
and Stevenson, 2004; Dang, 2004; Bharati et al,
2005; Moschitti and Basili, 2005; zap, 2008; Zapi-
rain et al, 2009), and word sense disambiguation
(Dang, 2004; Thater et al, 2010; O? Se?aghdha and
Korhonen, 2011), among many others.
Because lexical information is highly sensitive
to domain variation, approaches that can identify
VCs, SCFs and SPs in corpora have become in-
creasingly popular, e.g. (O?Donovan et al, 2005;
Schulte im Walde, 2006; Erk, 2007; Preiss et al,
2007; Van de Cruys, 2009; Reisinger and Mooney,
2011; Sun and Korhonen, 2011; Lippincott et al,
2012).
The task of SCF induction involves identifying
the arguments of a verb lemma and generalizing
about the frames (i.e. SCFs) taken by the verb,
where each frame includes a number of arguments
and their syntactic types. For example, in (1),
the verb ?show? takes the frame SUBJ-DOBJ-
CCOMP (subject, direct object, and clausal
complement).
(1) [A number of SCF acquisition papers]SUBJ
[show]VERB [their readers]DOBJ [which fea-
tures are most valuable for the acquisition
process]CCOMP.
SP induction involves identifying and classify-
ing the lexical items in a given argument slot. In
sentence (2), for example, the verb ?show? takes
the frame SUBJ-DOBJ. The direct object in this
frame is likely to be inanimate.
(2) [Most SCF and SP acquisition papers]SUBJ,
862
[show]VERB [no evidence to the usefulness of
joint learning leaning for these tasks]DOBJ.
Finally, VC induction involves clustering to-
gether verbs with similar meaning, reflected in
similar SCFs and SPs. For example, ?show? in the
above examples could get clustered together with
?demonstrate? and ?indicate?.
Because these challenging tasks capture com-
plementary information about predicate argument
structure, they should be able to inform and sup-
port each other. Recently, researchers have be-
gun to investigate the benefits of their joint learn-
ing. Schulte im Walde et al (2008) integrated SCF
and VC acquisition and used it for WordNet-based
SP classification. O? Se?aghdha (2010) presented a
?dual-topic? model for SPs that induces also verb
clusters. Both works reported SP evaluation with
promising results. Lippincott et al (2012) pre-
sented a joint model for inducing simple syntac-
tic frames and VCs. They reported high accuracy
results on VCs. de Cruys et al (2012) introduced
a joint model for SCF and SP acquisition. They
evaluated both the SCFs and SPs, obtaining rea-
sonable result on both tasks.
In this paper, we present the first unified frame-
work for unsupervised learning of the three types
of information - SCFs, SPs and VCs. Our frame-
work is based on Determinantal Point Processes
(DPPs, (Kulesza, 2012; Kulesza and Taskar,
2012c)), elegant probabilistic models that are de-
fined over the possible subsets of a given dataset
and give higher probability mass to high quality
and diverse subsets.
We first show how individual-task DPP kernel
matrices can be naturally combined to construct a
joint kernel. We use this to construct a joint SCF-
SP kernel. We then introduce a novel clustering
algorithm based on iterative DPP sampling which
can (contrary to other probabilistic frameworks
such as Markov random fields) be performed both
accurately and efficiently. When defined over the
joint SCF and SP kernel, this new algorithm can
be used to induce VCs that are valuable for both
tasks.
We also contribute by evaluating the value of
the clusters induced by our model for the acquisi-
tion of the three information types. Our evaluation
against a well-known VC gold standard shows that
our clustering model outperforms the state-of-the-
art verb clustering algorithm of Sun and Korhonen
(2009), in our setup where no manually created
SCF or SP data is available. Our evaluation against
a well-known SCF gold standard and in the con-
text of SP disambiguation tasks shows results that
are superior to strong baselines, demonstrating the
benefit our approach.
2 Previous Work
SCF acquisition Most current works induce SCFs
from the output of an unlexicalized parser (i.e.
a parser trained without SCF annotations) using
hand-written rules (Briscoe and Carroll, 1997; Ko-
rhonen, 2002; Preiss et al, 2007) or grammatical
relation (GR) co-occurrence statistics (O?Donovan
et al, 2005; Chesley and Salmon-Alt, 2006; Ienco
et al, 2008; Messiant et al, 2008; Lenci et al,
2008; Altamirano and Alonso i Alemany, 2010;
Kawahara and Kurohashi, 2010).
Only a handful of SCF induction works are
unsupervised. Carroll and Rooth (1996) applied
an EM-based approach to a context-free grammar
based model, Dkebowski (2009) used point-wise
co-occurrence of arguments in parsed Polish data
and Lippincott et al (2012) presented a Bayesian
network model for syntactic frame induction that
identifies SPs on argument types. However, the
frames induced by Lippincott et al (2012) do not
capture sets of arguments for verbs so are far sim-
pler than traditional SCFs.
Current approaches to SCF acquisition suffer
from lack of semantic information which is needed
to guide the purely syntax-driven acquisition pro-
cess. Previous works have showed the benefit of
hand-coded semantic information in SCF acquisi-
tion (Korhonen, 2002). We will address this prob-
lem in an unsupervised way: our approach is to
consider SCFs together with semantic SPs through
VCs which generalize over syntactically and se-
mantically similar verbs.
SP acquisition Considerable research has been
conducted on SP acquisition, with a variety of
unsupervised models proposed for this task that
use no hand-crafted information during training.
The latter approaches include latent variable mod-
els (O? Se?aghdha, 2010; Ritter and Etzioni, 2010;
Reisinger and Mooney, 2011), distributional sim-
ilarity methods (Bhagat et al, 2007; Basili et
al., 2007; Erk, 2007) and methods based on
non-negative tensor factorization (Van de Cruys,
2009). These works use a variety of linguistic fea-
tures in the acquisition process but none of them
863
integrates the three information types covered in
our work.
Verb clustering A variety of VC approaches
have been proposed in the literature. These in-
clude syntactic, semantic and mixed syntactic-
semantic classifications (Grishman et al, 1994;
Miller, 1995; Baker et al, 1998; Palmer et al,
2005; Schuler, 2006; Hovy et al, 2006). We fo-
cus on Levin style classes (Levin, 1993) which
are defined in terms of diathesis alternations and
capture generalizations over a range of syntactic
and semantic properties. Previous unsupervised
VC acquisition approaches clustered a variety of
linguistic features using different (e.g. K-means
and spectral) algorithms (Schulte im Walde, 2006;
Joanis et al, 2008; Sun et al, 2008; Li and Brew,
2008; Korhonen et al, 2008; Sun and Korhonen,
2009; Vlachos et al, 2009; Sun and Korhonen,
2011). The linguistic features included SCFs and
SPs, but these were induced separately and then
feeded as features to the clustering algorithm. Our
framework combines together SCF-motivated and
SP-motivated kernel matrices , and uses the joint
kernel to induce verb clusters which are likely to
be highly relevant for both tasks. Importantly, no
manual or automatic system for SCF or SP acqui-
sition has been utilized when constructing the ker-
nel matrices, we only consider features extracted
from the output of an unlexicalized parser. Our ap-
proach hence provides a framework for acquiring
valuable information for the three tasks together.
Joint Modeling A small number of works have
recently investigated joint approaches to SCFs,
SPs and VCs. Each of them has addressed only
a subset of the tasks and all but one have eval-
uated the performance in the context of one task
only. O? Se?aghdha (2010) presented a ?dual-topic?
model for SPs that induces VCs, reporting evalua-
tion of SPs only. Lippincott et al (2012) presented
a Bayesian network model for syntactic frame
(rather than full SCF) induction that induces VCs.
Only VCs are evaluated. de Cruys et al (2012)
presented a joint unsupervised model of SCF and
SP acquisition based on non-negative tensor fac-
torization. Both SCFs and SPs were evaluated. Fi-
nally, the model of Schulte im Walde et al (2008)
addresses the three types of information but SP
parameters are estimated with a WordNet based
method and only the SPs are evaluated. Although
evaluation of these recent joint models has been
partial, the results have been encouraging and fur-
ther motivate the development of a framework that
acquires the three types of information together.
3 The Unified Framework
In this section we present our unified framework.
Our idea is to utilize DPPs for verb clustering that
informs both SCF and SP acquisition. DPPs define
a probability distribution over the possible subsets
of a given set. These models assign higher prob-
ability mass to subsets that are both high quality
and diverse.
Our novel clustering algorithm makes use of
three DPP properties that are appealing for our
purpose: (1) The existence of efficient sam-
pling algorithms for these models, which enable
tractable sampling of high quality and diverse verb
subsets; (2) Such verb subsets form natural high
quality seeds for hierarchical clustering; and (3)
Given individual-task DPP kernel matrices there
are various simple and natural ways to combine
them into a new DPP kernel matrix.
Individual task DPP kernels represent (i) the
quality of a data point (verb) as its average feature-
based similarity with the other points in the data
set and (ii) the divergence between a pair of points
as the inverse similarity between them. For dif-
ferent tasks, different feature sets are used for the
kernel construction. The high quality and diverse
subsets sampled from the DPP model are consid-
ered good cluster seeds as they are likely to be rel-
atively uniformly spread and to provide good cov-
erage of the data set. The algorithm induces an
hierarchical clustering, which is particularly suit-
able for semantic tasks, where a set of clusters that
share a parent consists of pure members (i.e. most
of the points in each cluster member belong to the
same gold cluster) and together provide good cov-
erage of the verb space.
After a brief description of the Determinantal
Point Processes (DPP) framework (Section 3.1),
we discuss the construction of the joint DPP ker-
nel, given a kernel for each individual task, In sec-
tion 3.3 we present the DPP-Cluster clustering al-
gorithm.
3.1 Determinantal Point Processes
Determinantal point processes (DPPs) are elegant
probabilistic models of repulsion that offer effi-
cient and exact algorithms for sampling, marginal-
ization, conditioning, and other inference tasks.
Recently (Kulesza, 2012; Kulesza and Taskar,
864
2012c) introduced them to the machine learning
community and demonstrated their usefulness for
a variety of tasks including document summariza-
tion, image search, modeling non-overlapping hu-
man poses in images and video and automati-
cally building timelines of important news stories
(Kulesza and Taskar, 2010; Kulesza and Taskar,
2012a; Gillenwater et al, 2012; Kulesza and
Taskar, 2012b). Below we provide a brief descrip-
tion of the framework, a comprehensive survey
can be found in (Kulesza and Taskar, 2012c).
Given a set of items Y = {y1, . . . , yN}, a DPP
P defines a probability measure on the set of all
subsets of Y , 2Y . Kulesza and Taskar (2012c) re-
stricted their discussion of DDPs to L-ensembles,
where the probability of a subsetY ? Y is defined
through a positive semi-definite matrix L indexed
by the elements of Y:
PL(Y = Y ) =
det(LY )?
Y?Y det(LY )
= det(LY )det(L+ I)
(1)
Where I is the N ? N identity matrix and
det(L?) = 1. Since L is positive semi-definite, it
can be decomposed to L = BTB. This allows the
construction of an intuitively interpretable model
where each column Bi is the product of a quality
term qi ? R+ and a vector of (normalized) diver-
sity features ?i ? RD, ||?i|| = 1. In this model,
qi measures an inherent quality of the i ? th item
in Y while ?Ti ?j ? [?1, 1] is a similarity measure
between items i and j. With this representation we
can write:
Lij = qi?Ti ?jqj (2)
Sij = ?Ti ?j =
Lij?
LiiLjj
(3)
PL(Y = Y ) ? (
?
i?Y
q2i )det(SY ) (4)
It can be shown that the first term in equation 4 in-
creases with the quality of the selected items, and
the second term increases with their diversity. As
a consequence, this distribution places most of its
weight on sets that are both high quality and di-
verse.
Although the number of possible realizations of
Y is exponential in N , many inference procedures
can be performed accurately and efficiently (i.e.
in polynomial time which is very short in prac-
tice). In particular, sampling, which NP-hard for
alternative models such as Markov Random Fields
(MRFs), is efficient, theoretically and practically,
for DPPs.
3.2 Constructing a Joint Kernel Matrix
DPPs are particularly suitable for joint modeling
as they come with various simple and intuitive
ways to combine individual model kernel matrices
into a joint kernel. This stems from the fact that
every positive-semidefinite matrix forms a legal
DPP kernel (equation 1). Given individual model
DPP kernels, we would therefore like to combine
them into a positive-semidefinite matrix.
While there are various ways to construct a
positive-semidefinite matrix from two positive-
semidefinite matrices ? for example, by taking
their sum ? in this work we are motivated by the
product of experts approach (Hinton, 2002), rea-
soning that high quality assignments according to
a product of models have to be of high quality ac-
cording to each individual model, and sick for a
product combination. 2
In practice we construct the joint kernel in the
following way. We build on the aforementioned
property that a matrix L is positive semi-definite
iff L = BTB. Given two DPPs, PL1 defined by
L1 = AT1A1 and PL2 defined by L2 = AT2A2, we
construct the joint kernel L12:
L12 = L1L2L2L1 = CTC (5)
Where C = AT2A2AT1A1 and CT =
AT1A1AT2A2.
3.3 Clustering Algorithm
Algorithm (1) and Figure (1) provide a pseudo-
code of the algorithm and an example output. Be-
low is a detailed description.
Features Our algorithm builds two DPP ker-
nel matrices (the GenKernelMatrix function),
in which the rows and columns correspond to the
verbs in the data set, such that the (i, j)-th entry
corresponds to verbs number i and j. Following
equations 2 and 3 one matrix is built for SCF and
one for SP, and they are then combined into the
2Note that we do not take a product of the individual mod-
els but only of their kernel matrices. Yet, if we construct the
joint matrix by a multiplication then it follows from a simple
generalization of the Cauchy-Binet formula that its principle
minors, which define the subset probabilities (equation 1), are
a sum of multiplications of the principle minors of the indi-
vidual model kernels. Still, we do not have guarantees that
our choice of kernel combination is the right one. We leave
this for future research.
865
joint kernel matrix (the GenJointMat function)
following equation 5. Each kernel matrix requires
a proper feature representation ? and quality score
q.
In both kernels we represent a verb by the
counts of the grammatical relations (GRs) it par-
ticipates in. In the SCF kernel a GR is represented
by the GR type and the POS tags of the verb and
its arguments. In the SP kernels the GRs are rep-
resented by the POS tags of the verb and its ar-
guments as well as by the argument head word.
Based on this feature representation, the similarity
(opposite divergence) is encoded to the model by
equation 3 as the dot product between the normal-
ized feature vectors. The quality score qi of the
i-th verb is the average similarity of this verb with
the other verbs in the dataset.
Cluster set construction In its while loop, the
algorithm iteratively generates fixed-size cluster
sets such that each data point belongs to exactly
one cluster in one set. These cluster sets form
the leaf level of the tree in Figure (1). It does
so by extracting the T highest probability K-point
samples from a set of M subsets, each of which
sampled from the joint DPP model, and cluster-
ing them by the cluster procedure. The sampling
is done by the K-DPP sampling process ((Kulesza
and Taskar, 2012c), page 62) 3.
The cluster procedure first seeds a K-cluster
set with the highest probability sample. Then, it
gradually extends the clusters by iteratively map-
ping the samples, in decreasing order of probabil-
ity, to the existing clusters (them1Mapping func-
tion). Mapping is done by attaching every point
in the mapped subset to its closet cluster, where
the distance between a point and the cluster is the
maximum over the distances between the point
and each of the points in the cluster. The map-
ping is many-to-one, that is, multiple points in the
subset can be assigned to the same cluster.
Based on the DPP properties, the higher the
probability of a sampled subset, the more likely it
is to consist of distinct points that provide a good
coverage of the verb set. By iteratively extending
the clusters with high probability subsets, we thus
expect each cluster set to consist of clusters that
demonstrate these properties.
3K-DPP is a DPP conditioned on the sample size. As
shown in ((Kulesza and Taskar, 2012c), Section 2.4.3) this
conditional distribution is also a DPP. We could have obtained
samples of size K by sampling the DPP and rejecting sam-
ples of other sizes but this would have been slower.
SET 1-2-3-4 (45,K)
SET 1-2 (23,K)
SET1 (12,K) SET2 (11,K)
SET3-4(22,K)
SET 3 (12,K) SET4 (10,K)
Figure 1: An example output hierarchy of DPP-
Cluster for a set of 45 data points. Each set is
augmented with the number of points (left num-
ber) and clusters (right number) it includes. The
iterative DPP-samples clustering (the While loop)
generates the lowest level of the tree, by dividing
the data set into cluster sets, each of which con-
sists of K clusters. Each point in the data set be-
longs to exactly one cluster in exactly one set. The
agglomerative clustering then iteratively combines
cluster sets such that in each iteration two sets are
combined to one set with K clusters.
Agglomerative Clustering Finally, the
AgglomerativeClustering function builds a
hierarchy of cluster sets, by iteratively combining
cluster set pairs. In each iteration it computes the
similarity between any such pair, defined to be the
lowest similarity between their cluster members,
which is in turn defined to be the lowest cosine
similarity between their point members. The most
similar cluster sets are combined such that each
of the clusters in one set is mapped to its most
similar cluster in the other set. In this step the
algorithm generates data partitions at different
granularity levels from finest (from the iterative
sampling step) to the coarsest set (generated by
the last agglomerative clustering iteration and
consisting of exactly K clusters). This property is
useful as the optimal level of generalization may
be task dependent.
4 Evaluation
Data sets and gold standards We evaluated the
SCFs and verb clusters on gold standard datasets.
We based our set of the largest available joint set
for SCFs and VCs - that of (de Cruys et al, 2012).
It provides SCF annotations for 183 verbs (an av-
erage of 12.3 SCF types per verb) obtained by
annotating 250 corpus occurrences per verb with
the SCF types of (de Cruys et al, 2012). The
verbs represent a range of Levin classes at the top
level of the hierarchy in VerbNet (Kipper-Schuler,
2005). Where a verb has more than one Verb-
Net class, we assign it to the one supported by the
highest number of member verbs. To ensure suf-
866
|C| = 20, 21.6 |C| = 40, 41 |C| = 60, 58.6 |C| = 69, 77.6 |C| = 89, 97.4
Model R P F R P F R P F R P F R P F
DPP-cluster 93.1 17.3 29.3 77.9 25.4 38.3 63 31.9 42.3 43.8 33.6 38.1 34.4 40.6 37.2
AC 67 17.8 28.2 46.6 24 31.7 40.5 29.4 34 33 34.9 33.9 24.7 41.1 30.9
SC 32.1 27.5 29.6 26.6 35.9 30.6 23.7 41.5 30.2 22.8 43.6 29.9 21.6 48.7 29.9
Table 1: Verb clustering evaluation for the last five iterations of our DPP-cluster model and the baseline
agglomerative clustering algorithm (AC, see text for its description), and for the spectral clustering (SC)
algorithm of (Sun and Korhonen, 2009) with the same number of clusters induced by DPP-cluster. |C| is
the number of clusters for DPP-cluster and SC (first number) and for AC (second number). The F-score
performance of DPP-cluster is superior in 4 out of 5 cases.
Arg. per verb P (DPP) P(AC) P (B) P (NF) R (DPP) R (AC) R (B) R(NF) ERR DPP ERR AC ERR B
? 200 (133 verbs) 27.3 23.7 27.3 23.1 9.9 7.6 8 11.3 3.4 0.16 1.55
? 600 (205 verbs) 26.5 25 27.3 22.6 14.8 11.5 11.9 16.6 2.3 0.50 1.1
? 1000 (238 verbs) 24.6 23.6 25.6 21.1 17.5 13.8 14.7 19.8 1.6 0.42 0.95
Table 2: Performance of the Corpus Statistics SP baseline (non-filtered, NF) as well as for three filtering
methods: frequency based (filter-baseline, B), DPP-cluster based (DPP) and AC cluster based (AC). P
(method) and R (method) present the precision and recall of the method respectively. The error reduc-
tion ratio (ERR) is the ratio between the reduction in precision error achieved by each method and the
increase in recall error (each method is compared to the NF baseline). Ratio greater than 1 means that
the reduction in precision error is larger than the increase in recall error (see text for exact definition).
DPP based filtering provides substantially better ratio.
ficient representation of each class, we collected
from VerbNet the verbs for which at least one of
the possible classes is represented in the 183 verbs
set by at least one and at most seven verbs. This
yielded 101 additional verbs which we added to
the gold standard with the initial 183 verbs.
We parsed the BNC corpus with the RASP
parser (Briscoe et al, 2006) and used it for feature
extraction. Since 176 out of the 183 initial verbs
are represented in this corpus, our final gold stan-
dard consists of 34 classes containing 277 verbs,
of which 176 have SCF gold standard and has been
evaluated for this task. We set the parameters of
our algorithm on an held-out data, consisting of
different verbs than those used in our experiments,
to be M = 10000, K = 20 and T = 10.
Clustering Evaluation We first evaluate the
quality of the clusters induced by our algorithm
(DPP-cluster) compared to the gold standard VCs
(table 1). To evaluate the importance of the DPP
component, we compare to the performance of a
version of our algorithm where everything is kept
fixed except from the sampling which is done from
a uniform distribution rather than from the DPP
joint kernel (this model is denoted in the table
with AC for agglomerative clustering) 4. We also
compare to the state-of-the-art spectral clustering
method of Sun and Korhonen (2009) where our
4Importantly, the kernel matrix L used in the agglomera-
tive clustering process is also used by AC.
kernel matrix is used for the distance between data
points (SC) 5.
We evaluated the unified cluster set induced in
each iteration of our algorithm and of the AC base-
line and induced the same number of clusters as in
each iteration of our algorithm using the SC base-
line. Since the number of clusters in each iteration
is not an argument for our algorithm or for the AC
baseline, the number of clusters slightly differ be-
tween the two. The AC and SC baseline results
were averaged over 5 and 100 runs respectively.
DPP-cluster has produced identical output across
runs.
The table demonstrates the superiority of the
DPP-cluster model. For four out of five conditions
its F-score performance outperforms the baselines
by 4.2-8.3%. Moreover, in all conditions its recall
performances are substantially higher than those
of the baselines (by 9.7-26.1%). Note that DPP-
cluster runs for 17 iterations while the AC baseline
performs only 6. We therefore evaluated only the
last 5 iterations of each model 6.
SCF evaluation For this evaluation, we first
built a baseline SCF lexicon based on the parsed
5Sun and Korhonen (2009) report better results than those
we report for their algorithm (on a different data set). Note,
however, that they used the output of a rule-based SCF sys-
tem as a source of features, as opposed to our unsupervised
approach.
6For the additional comparable iteration the result pattern
is very similar to the (C = 89, 97.4) case in the table, and is
not presented due to space limitations.
867
Algorithm 1 The DPP-cluster clustering algo-
rithm. K is the size of the sampled subsets, M is
the number of subsets sampled at each iteration, Y
is the verb set, T is the number of most probable
samples to be used in each iteration
Algorithm DPP-cluster :
Arguments: K,M,Y ,T
Return: cluster sets S = {S1, . . . Sn}
i? 1
S ? ?
while Y 6= ? do
(L1, S1)? GenKernelMatrix(Y, SCF )
(L2, S2)? GenKernelMatrix(Y, SP )
(L12, S12)? GenJointMat(L1, L2)
samples? sampleDpp(L,K,M)
topSamples? exTop(samples, T )
Si ? cluster(topSamples, L)
Y ? Y ? elements(Si)
S ? S ? Si
i? i+ 1
end while
AgglomerativeClustering(S)
???????????????????
???
Function cluster :
Arguments: topSamples,L
Return: S
S ? ?, topSample? ?
i? 1
while (topSample ? elements(S) = ?) do
topSample? topSamples(i)
S ? m1Mapping(topSample, S)
i? i+ 1
if (i > size(topSamples)) then
return S
end if
end while
BNC corpus. We do this by gathering the GR com-
binations for each of the verbs in our gold stan-
dard, assuming they are frames and gathering their
frequencies. Note that this corpus statistics base-
line is a very strong baseline that performs very
similarly to (de Cruys et al, 2012), the best unsu-
pervised SCF model we are aware of, when run on
their dataset 7.
As shown in table 3 the corpus statistics base-
line achieves high recall (84%) at the cost of
low precision (52.5%) (similar pattern has been
7personal communication with the authors.
demonstrated for the system of de Cruys et al
(2012)). On the other extreme, two other com-
monly used baselines strongly prefer precision.
These are the Most Frequent SCF (O?Donovan et
al., 2005) which uniformly assigns to all verbs the
two most frequent SCFs in general language, tran-
sitive (SUBJ-DOBJ) and intransitive (SUBJ) (and
results in poor F-score), and a filtering that re-
moves frames with low corpus frequencies (which
results in low recall even when trying to provide
the maximum recall for a given precision level).
The task we address is therefore to improve the
precision of the corpus statistics baseline in a way
that does not substantially harm the F-score.
To remedy this imbalance, we apply a cluster
based filtering method on top of the maximum-
recall frequency filter. This filter excludes a candi-
date frame from a verb?s lexicon only if it meets
the frequency filter criterion and appears in no
more than N other members of the cluster of the
verb in question. The filter utilizes the clustering
produced by the seventh to last iteration of DPP-
cluster that contains seven clusters with approxi-
mately 30 members each. Such clustering should
provide a good generalization level for the task.
We report results for moderate as well as ag-
gressive filtering (N = 3 andN = 7 respectively).
Table 3 clearly demonstrates that cluster based fil-
tering (DPP-cluster and AC) is the only method
that provides a good balance between the recall
and the precision of the SCF lexicon. Moreover,
the lexicon induced by this method includes a sub-
stantially higher number of frames per verb com-
pared to the other filtering methods. While both
AC and DPP-cluster still prefer recall to precision,
DPP-cluster does so to a smaller extent 8. This
clearly demonstrates that the clustering serves to
provide SCF acquisition with semantic informa-
tion needed for improved performance.
SP evaluation We explore a variant of the
pseudo-disambiguation task of Rooth et al (1999)
which has been applied to SP acquisition by a
number of recent papers (e.g. (de Cruys et al,
2012)). Rooth et al (1999) proposed to judge
which of two verbs v and v? is more likely to take a
given noun n as its argument. In their experiments
the model has to choose between a pair (v, n) that
8We show results for the maximum recall frequency fil-
tering with precision equals to 80 or 90. When the frequency
threshold is further reduced from 0.03, the same result pat-
tern hold. We do not give a detailed description due to space
limitations.
868
Corpus Statistics: [P = 52.5, R = 84, F = 64.6, AF = 12.3]
Most Frequent SCF: [P = 86.7, R = 22.5, F = 35.8, AF = 2]
Clustering Moderate Clustering Aggressive
Maximum Recall Frequency Threshold Model P R F AF P R F AF
threshold = 0.03, Prec. > 80 DPP-cluster 60.8 68.3 64.3 8.7 64.1 64.2 64.2 7.7
[P=88.7,R=52.4,F=65.9,AF=4.5] AC 58 73.2 64.6 9.7 61.3 68.9 64.7 8.6
threshold = 0.05, Prec. > 90 DPP-cluster 60.1 64.6 62.3 8.7 63.3 59.3 61.3 7.2
[P=92.3,R=44.4,F=59.9,AF=3.7] AC 57.5 70.6 63.2 9.4 60.7 65.4 62.7 8.3
Table 3: SCF Results for the DPP-cluster model compared to the Corpus Statistics baseline, Most Fre-
quent SCF baseline, maximum-recall frequency thresholding with the maximum threshold values that
keep precision above 80 (threshold = 0.03) and above 90 (threshold = 0.05), and the AC clustering base-
line. AF is the average number of frames per verb. All methods except from cluster based filtering
(DPP-cluster and AC) induce lexicons with strong recall/precision imbalance. Cluster based fil-
tering keeps a larger number of frames in the lexicon compared to the frequency thresholding
baseline, while keeping similar F-score levels. DPP-cluster provides better recall/precision balance
than AC.
appears only in the test corpus and a pair (v?, n)
that appears neither in the test nor in the training
corpus. Note, however, that this test only evaluates
the capability of a model to distinguish a correct
unseen verb-argument pair from an incorrect one,
but not its capability to identify erroneous pairs
when no alternative pair is presented. This last
property can strongly affect the precision of the
model.
We therefore propose to measure both aspects
of the SP task by computing both the recall and the
precision between the list of possible arguments a
verb can take according to the model and the cor-
responding test corpus list 9.
We evaluate the value of our clustering for SP
acquisition in the particularly challenging scenario
of domain adaptation. For each of the verbs in
our set we induce a list of possible noun direct ob-
jects from the BNC corpus and an equivalent list
from the North American News Text (NANT) cor-
pus. Following previous work (e.g. (de Cruys et
al., 2012)) arguments are identified using a parser
(RASP in our case). Using the verb clusters we
create a filtered version of the BNC argument lex-
icon which includes in the noun argument list of
a verb only those nouns that appear in the BNC
as arguments of that verb and of one of its cluster
members. For each verb we then compare the fil-
tered as well as the non-filtered BNC induced lex-
icon to the NANT lexicon by computing the aver-
age recall and precision between the argument lists
9In principle these measures can take into account the
probability assigned by the model to each argument and the
corresponding test corpus frequency. In this work we com-
pute probability-ignorant scores and keep more sophisticated
evaluations for future research.
and then report the average scores across all verbs.
We compare to a baseline which maintains only
noun arguments that appear at least twice in BNC
10. As a final measure of performance we compute
the ratio between the reduction in precision error
(i.e. pmodel?pbaseline100?pbaseline ) and the increase in recall er-
ror ( rbaseline?rmodel100?rmodel ).Table 2 presents the results for verbs with up to
200, 600 and 1000 noun arguments in the training
data. In all cases, the relative error reduction of the
DPP cluster filter is substantially higher than that
of the frequency baseline. Note that for this task
the baseline AC clusters are of low quality which
is reflects by an error reduction ratio of up to 0.5.
5 Conclusions and Future Work
In this paper we have presented the first unified
framework for the induction of verb clusters, sub-
categorization frames and selectional preferences
from corpus data. Our key idea is to cluster to-
gether verbs with similar SCFs and SPs and to use
the resulting clusters for SCF and SP induction. To
implement our idea we presented a novel method
which involves constructing a product DPP model
for SCFs and SPs and introduced a new algorithm
that utilizes the efficient DPP sampling algorithms
to cluster together verbs with similar SCFs and
SPs. The induced clusters performed well in eval-
uation against a VerbNet -based gold standard and
proved useful in improving the quality of SCFs
and SPs over strong baselines.
Our results demonstrate the benefits of a uni-
fied framework for acquiring lexical informa-
10we experimented with other threshold values for this
baseline but the recall in those case becomes very low.
869
tion about different aspects of verbal predicate-
argument structure. Not only the acquisition of
different types information (syntactic and seman-
tic) can support and inform each other, but also a
unified framework can be useful for NLP tasks and
applications which require rich information about
predicate-argument structure. In future work we
plan to apply our approach on larger scale data
sets and gold standards and to evaluate it in differ-
ent domains, languages and in the context of NLP
tasks such as syntactic parsing and SRL.
In addition, in our current framework SCF and
SP information is used for clustering which is in
turn used to improve SCF and SP quality. At this
stage no further information flows from the SCF
and SP models to the clustering model. A natural
extension of our unified framework is to construct
a joint model in which the predictions for all three
tasks inform each other at all stages of the predic-
tion process.
Acknowledgements
The work in this paper was funded by the Royal
Society University Research Fellowship (UK).
References
Ivana Romina Altamirano and Laura Alonso i Ale-
many. 2010. IRASubcat, a highly customizable,
language independent tool for the acquisition of ver-
bal subcategorization information from corpus. In
Proceedings of the NAACL HLT 2010 Young Inves-
tigators Workshop on Computational Approaches to
Languages of the Americas.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING-
ACL-98.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
RANLP 2007, Borovets, Bulgaria.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In EMNLP-07, page
161170, Prague, Czech Republic.
Akshar Bharati, Sriram Venkatapathy, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In CoNLL-05.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In ANLP-
97.
E.J. Briscoe, J. Carroll, and R. Watson. 2006. The
second relsease of the rasp system. In COLING/ACL
interactive presentation session.
Glenn Carroll and Mats Rooth. 1996. Valence induc-
tion with a head-lexicalized pcfg. In EMNLP-96.
Paula Chesley and Susanne Salmon-Alt. 2006. Au-
tomatic extraction of subcategorization frames for
french. In LREC-06.
Kostadin Cholakov and Gertjan van Noord. 2010. Us-
ing unknown word techniques to learn known words.
In EMNLP-10.
Hoa Trang Dang. 2004. Investigations into the Role of
Lexical Semantics in Word Sense Disambiguation.
Ph.D. thesis, CIS, University of Pennsylvania.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau,
and Anna Korhonen. 2012. Multi-way tensor fac-
torization for unsupervised lexical acquisition. In
COLING-12.
Lukasz Dkebowski. 2009. Valence extraction us-
ing EM selection and co-occurrence matrices. Lan-
guage resources and evaluation, 43(4):301?327.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL 2007, Prague,
Czech Republic.
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Dis-
covering diverse and salient threads in document
collections. In EMNLP-12.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. 1994. Comlex syntax: Building a computa-
tional lexicon. In COLNIG-94.
G.E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Compu-
tation, 14:1771?1800.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Porceedings 0f NAACL-HLT-06
short papers.
Dino Ienco, Serena Villata, and Cristina Bosco. 2008.
Automatic extraction of subcategorization frames
for italian. In LREC-08.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic verb
classification. Natural Language Engineering.
Daisuke Kawahara and Sadao Kurohashi. 2010. Ac-
quiring reliable predicate-argument structures from
raw corpora for case frame compilation. In LREC-
10.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, June.
870
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceddings
of COLING-08.
Anna Korhonen. 2002. Semantically motivated
subcategorization acquisition. In Proceedings of
the ACL-02 workshop on Unsupervised lexical
acquisition-Volume 9.
A. Kulesza and B. Taskar. 2010. Structured determi-
nantal point processes. In NIPS-10.
A. Kulesza and B. Taskar. 2012a. k-dpps: fixed-size
determinantal point processes. In ICML-11.
A. Kulesza and B. Taskar. 2012b. Learning determi-
nantal point processes. In UAI-12.
Alex Kulesza and Ben Taskar. 2012c. Determi-
nantal point processes for machine learning. In
arXiv:1207.6083.
A. Kulesza. 2012. Learning with determinantal point
processes. Ph.D. thesis, CIS, University of Pennsyl-
vania.
Alessandro Lenci, Barbara McGillivray, Simonetta
Montemagni, and Vito Pirrelli. 2008. Unsuper-
vised acquisition of verb subcategorization frames
from shallow-parsed corpora. In LREC-08.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. Chicago, IL.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In ACL-
08.
Tom Lippincott, Anna Korhonen, and Diarmuid O?
Se?aghdha. 2012. Learning syntactic verb frames
using graphical models. In ACL-12, Jeju, Korea.
Ce?dric Messiant, Anna Korhonen, and Thierry
Poibeau. 2008. LexSchem: A large subcategoriza-
tion lexicon for French verbs. In LREC-08.
George A. Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Alessandro Moschitti and Roberto Basili. 2005. Verb
subcategorization kernels for automatic semantic la-
beling. In Proceedings of the ACL-SIGLEX Work-
shop on Deep Lexical Acquisition.
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the penn-ii and penn-iii treebanks. Computational
Linguistics, 31:328?365.
Diarmuid O? Se?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In EMNLP-11, Edinburgh, UK.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In ACL-10, Uppsala, Swe-
den.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In ACL-07.
Joseph Reisinger and Raymond Mooney. 2011. Cross-
cutting models of lexical semantics. In EMNLP-11,
Edinburgh, UK.
Alan Ritter and Oren Etzioni. 2010. A latent dirich-
let alocation method for selectional preferences. In
ACL-10.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
ACL-99.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
S. Schulte im Walde, C. Hying, C. Scheible, and
H. Schmid. 2008. Combining EM training and the
MDL principle for an automatic verb classification
incorporating selectional preferences. In ACL-08,
pages 496?504.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of german semantic verb
classes. Computational Linguistics, 32(2):159?194.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In CICLING-05.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In EMNLP-09, Singapore.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In EMNLP-11.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
Lecture Notes in Computer Science, 4919(16).
Robert Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP-04.
Stefan Thater, Hagen Furstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In ACL-
10, Uppsala, Sweden.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the workshop on Geometric Mod-
els for Natural Language Semantics (GEMS).
871
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet.
Benat Zapirain, Eneko Agirre, and Lluis Marquex.
2009. Generalizing over lexical features: Selec-
tional preferences for semantic role classification. In
ACL-IJCNLP-09, Singapore.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
ACL-11, Portland, OR.
872
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 57?66,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Unsupervised POS Induction Using Intrinsic Clustering Quality
and a Zipfian Constraint
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Raanan Fattal
Institute of computer science
The Hebrew University
raananf@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
Modern unsupervised POS taggers usually
apply an optimization procedure to a non-
convex function, and tend to converge to
local maxima that are sensitive to start-
ing conditions. The quality of the tag-
ging induced by such algorithms is thus
highly variable, and researchers report av-
erage results over several random initial-
izations. Consequently, applications are
not guaranteed to use an induced tagging
of the quality reported for the algorithm.
In this paper we address this issue using
an unsupervised test for intrinsic cluster-
ing quality. We run a base tagger with
different random initializations, and select
the best tagging using the quality test. As
a base tagger, we modify a leading un-
supervised POS tagger (Clark, 2003) to
constrain the distributions of word types
across clusters to be Zipfian, allowing us
to utilize a perplexity-based quality test.
We show that the correlation between our
quality test and gold standard-based tag-
ging quality measures is high. Our re-
sults are better in most evaluation mea-
sures than all results reported in the liter-
ature for this task, and are always better
than the Clark average results.
1 Introduction
Unsupervised part-of-speech (POS) induction is
of major theoretical and practical importance. It
counters the arbitrary nature of manually designed
tag sets, and avoids manual corpus annotation
costs. The task enjoys considerable current inter-
est in the research community (see Section 3).
Most unsupervised POS tagging algorithms ap-
ply an optimization procedure to a non-convex
function, and tend to converge to local maxima
that strongly depend on the algorithm?s (usually
random) initialization. The quality of the tag-
gings produced by different initializations varies
substantially. Figure 1 demonstrates this phe-
nomenon for a leading POS induction algorithm
(Clark, 2003). The absolute variability of the in-
duced tagging quality is 10-15%, which is around
20% of the mean. Strong variability has also been
reported by other authors (Section 3).
The common practice in the literature is to re-
port mean results over several random initializa-
tions of the algorithm (e.g. (Clark, 2003; Smith
and Eisner, 2005; Goldwater and Griffiths, 2007;
Johnson, 2007)). This means that applications us-
ing the induced tagging are not guaranteed to use
a tagging of the reported quality.
In this paper we address this issue using an
unsupervised test for intrinsic clustering quality.
We present a quality-based algorithmic family Q.
Each of its concrete member algorithms Q(B) runs
a base tagger B with different random initializa-
tions, and selects the best tagging according the
quality test. If the test is highly positively corre-
lated with external tagging quality measures (e.g.,
those based on gold standard tagging), Q(B) will
produce better results than B with high probability.
We experiment with two base taggers, Clark?s
original tagger (CT) and Zipf Constrained Clark
(ZCC). ZCC is a novel algorithm of interest in its
own right, which is especially suitable as a base
tagger in the family Q. ZCC is a modification of
Clark?s algorithm in which the distribution of the
number of word types in a cluster (cluster type
size) is constrained to be Zipfian. This property
holds for natural languages, hence we can expect
a higher correlation between ZCC and an accepted
unsupervised quality measure, perplexity.
We show that for both base taggers, the corre-
lation between our unsupervised quality test and
gold standard based tagging quality measures is
high. For the English WSJ corpus, the Q(ZCC)
57
0.45 0.5 0.550
20
40
V
0.7 0.8 0.90
10
20
NVI
0.4 0.5 0.60
20
40
Many to 1
0.4 0.50
20
40
1 to 1
Figure 1: Distribution of the quality of the tag-
gings produced in 100 runs of the Clark POS in-
duction algorithm (with different random initial-
izations) for sections 2-21 of the WSJ corpus. All
graphs are 10-bin histograms presenting the num-
ber of runs (y-axis) with the corresponding qual-
ity (x-axis). Quality is evaluated with 4 clustering
evaluation measures: V, NVI, greedy m-1 map-
ping and greedy 1-1 mapping. The quality of the
induced tagging varies considerably.
algorithm gives better results than CT with proba-
bility 82-100% (depending on the external quality
measure used). Q(CT) is shown to be better than
the original CT algorithm as well. Our results are
better in most evaluation measures than all previ-
ous results reported in the literature for this task,
and are always better than Clark?s average results.
Section 2 describes the ZCC algorithm and our
quality measure. Section 3 discusses previous
work. Section 4 presents the experimental setup
and Section 5 reports our results.
2 The Q(ZCC) Algorithm
Given an N word corpus M consisting of plain
text, with word types W = {w1, . . . , wm}, the
unsupervised POS induction task is to find a class
membership function g from W into a set of class
labels {c1, . . . , cn}. In the version tackled in this
paper, the number of classes n is an input of the al-
gorithm. The membership function g can be used
to tag a corpus if it is deterministic (as the func-
tion learned in this work) or if a rule for selecting
a single tag for every word is provided.
Most modern unsupervised POS taggers pro-
duce taggings of variable quality that strongly de-
pend on their initialization. Our approach towards
generating a single high quality tagging is to use a
family of algorithms Q. Each member Q(B) of Q
utilizes a base tagger B, which is run using several
random initializations. The final output is selected
according to an unsupervised quality test. We fo-
cus here on Clark?s tagger (Clark, 2003) (CT),
probably the leading POS induction algorithm (see
Table 3).
We start with a description of the original CT.
We then detail ZCC, a modification of CT that
constrains the clustering space by adding a Zipf-
based constraint. Our perplexity-based unsuper-
vised tagging quality test is discussed next. Fi-
nally, we provide an unsupervised technique for
selecting the parameter of the Zipfian constraint.
2.1 The Original Clark Tagger (CT)
The tagger?s statistical model combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al, 1992; Ney et al, 1994; Martin et
al., 1998). In the Brown algorithm a class assign-
ment function g is selected such that the class bi-
gram likelihood of the corpus, p(M |g), is max-
imized. Morphological and distributional infor-
mation is introduced to the Clark model through
a prior p(g). The prior prefers morphologically
uniform clusters and skewed cluster sizes.
The probability function the algorithm tries to
maximize is:
(1) p(M, g) = p(M |g) ? p(g)
(2) p(M |g) = ?i=Ni=2 p(g(wi)|g(wi?1))
(3) p(g) = ?nj=1 ?j
?
g(w)=j qj(w)
Where qj(wi) is the probability of assigning
wi ? W by cluster cj according to the morpho-
logical model and ?j is the coefficient of cluster
j, which equals to the number of word types as-
signed to that cluster divided by the total number
of word types in the vocabulary W . The objective
of the algorithm is formally specified by:
g? = argmaxgp(M, g)
To find the cluster assignment g? an iterative
algorithm is applied. As initialization, the words
in W are randomly assigned to clusters (clusters
are thus of similar sizes). Then, for each word
(words are ordered by their frequency in the cor-
pus) the algorithm computes the effect that mov-
ing it from its current cluster to each of the other
clusters would have on the probability function.
The word is moved to the cluster having the high-
est positive effect (if there is no such cluster, the
word is not moved). The last step is performed it-
eratively until no improvement to the probability
function is possible through a single operation.
58
The probability function has many local max-
ima and the one to which the algorithm conver-
gences strongly depends on the initial assignment
of words to clusters. The quality of the clusters in-
duced in different runs of the algorithm is highly
variable (Figure 1).
2.2 The Cluster Type Size Zipf Constraint
The motivation behind using a Zipfian constraint is
the following observation: when a certain statistic
is known to affect the quality of the induced clus-
tering and it is not explicitly manipulated by the al-
gorithm, strong fluctuations in its values are likely
to imply that there are uncontrolled fluctuations in
the quality of the induced clusterings. Thus, in-
troducing a constraint that we believe holds in real
data increases the correlation between clustering
quality and a well accepted unsupervised quality
measure (perplexity).
Our ZCC algorithm searches for a class assign-
ment function g that maximizes the probability
function (1) under a constraint on the clustering
space, namely constraining the cluster type size
distribution induced by g to be Zipfian. This con-
straint holds in many languages (Mitzenmacher,
2004) and is demonstrated in Figure 3 for the En-
glish corpus with which we experiment in this pa-
per.
Zipf?s law predicts that the fraction of elements
in class k is given by:
f(k; s;n) = 1/k
s
?n
i=1(1/is)
where s is a parameter of the distribution and n the
number of clusters.
Denote the cluster type size distribution derived
from the algorithm?s cluster assignment function g
by T (g). The objective of the algorithm is
g?? = argmaxgp(M, g) s.t. T (g) ? Zipf(s)
To impose the Zipfian distribution on the in-
duced clusters size, we make two modifications to
the original CT algorithm. First, at initialization,
words are randomly assigned to clusters in a way
that cluster sizes are distributed according to the
Zipfian distribution (with a parameter s). Specifi-
cally, we randomly select words to be assigned to
the first cluster until the fraction of word types in
the cluster equals to the prediction given by Zipf?s
law. We then randomly assign words to the second
cluster and so on.
Second, we change the basic operation of the al-
gorithm from moving a word to a cluster to swap-
ping two words between two different clusters.
For each word wi (again, words are ordered by
their frequency in the corpus as in CT), the algo-
rithm computes the effect on the probability func-
tion of moving it from its current cluster ccurr to
each of the other clusters. We denote the cluster
showing the best effect by cbest. Then, we search
the words of cbest for the word wj whose transition
to ccurr has the best effect on the probability func-
tion. If the sum of the effects of moving wi from
ccurr to cbest and moving wj from cbest to ccurr
is positive, the swapping is performed. If swap-
ping is not performed, we repeat the process for
wi, this time searching for cbest among all other
clusters except of former cbest candidates1.
2.3 Unsupervised Identification of High
Quality Runs
Perplexity is a standard measure for language
model evaluation. A language model defines the
transition probabilities for every word wi given the
words that precede it. The perplexity of a language
model for a given corpus having N words is de-
fined to be
N
?
?
?
?
N
?
i=1
1
p(wi|w1 . . . wi?1)
An important property of perplexity that makes
it attractive as a measure for language model per-
formance is that in some sense the best model for
any corpus has the lowest perplexity for that cor-
pus (Goodman, 2001). Thus, the lower the per-
plexity of the language model, the better it is.
Clark (2003) proposed a perplexity based test
for the quality of his POS induction algorithm. In
that test, a bigram class-based language model is
trained on a training corpus (using the tagging of
the unsupervised tagger) and applied to another
test corpus. In such a model the transition prob-
ability from a word wj to a word wi is given
by p(C(wi)|C(wj)) where C(wk) is the class as-
signed by the POS induction algorithm to wk. In
the training phase the bigram transition probabili-
ties are computed using the training corpus, and in
1To make the algorithm more time efficient, for each word
wi we perform only three iterations of the searching for cbest,
and for each cbest candidate we compute for at most 500
words the effect on the probability function of the removal
to ccurr .
59
2 4 6 8 10880
882
884
886
888
890
K
A
ve
ra
g
e
 P
e
rp
le
xi
ty
2 4 6 8 100.4
0.5
0.6
0.7
0.8
0.9
K
R
a
n
k 
C
o
rr
e
la
tio
n
Figure 2: Left: average perplexity vs. the param-
eter K (tightness of the entropy outliers filter; see
text for a full explanation). Right: Spearman?s
rank correlation between perplexity and an exter-
nal (many-to-one) quality of the clustering as a
function of K. The three curves are for ZCC,
using different exponents (triangles: 0.9, circles:
1.3, solid: 1.1). A model whose quality improves
(decreased perplexity) with K (left) demonstrates
better correlation between perplexity and external
quality (right). In all three graphs the x axis is in
units of 5K (e.g., a graph x value of 2 means that
10 clusterings were removed from the top of the
list and 10 from its bottom).
the test phase the perplexity of the learned model
is evaluated on the test corpus. Better POS induc-
tion algorithms yield lower perplexity language
models. However, Clark did not study the correla-
tion between the perplexity measure and the gold
standard tagging.
In this paper, we use Clark?s perplexity based
test as the unsupervised quality test used by the
family Q. To provide a high quality prediction, this
test should highly correlate with external cluster-
ing quality. To the best of our knowledge, such a
correlation has not been explored so far.
2.4 Unsupervised Parameter Selection
The base ZCC algorithm has one input parame-
ter, the exponent s of the Zipfian distribution. Vir-
tually all unsupervised algorithms utilize param-
eters whose values affect their results. While it
is methodologically valid to simply determine a
value based on reasonable considerations or a de-
velopment set, to keep the fully unsupervised na-
ture of our work we now present a method for
identifying the best parameter assignment. The
method also casts some additional interesting light
on the nature of the problem.
Like cluster type size, the distribution of cluster
instance size in natural languages is also Zipfian
(see Figure 3). A naive application of this con-
straint into the ZCC algorithm would be to allow
swapping words between clusters only if they an-
notate the same number of word instances in the
corpus. However, this constraint, either by itself
or in combination with the cluster type size con-
straint, is too restrictive.
We utilize it for parameter selection as follows.
Recall that our family of algorithms Q(B) runs a
base tagger B several times. Each specific run
yields a clustering Ci. The final result is selected
from the set of clusterings C = {Ci}. We do
not explicitly address the number of instances con-
tained in a cluster, but we can prune from C those
clusterings for which this distribution is very dif-
ferent. Again, imposing a constraint that is known
to hold reduces quality fluctuations between dif-
ferent runs.
To measure the similarity between the cluster
instance size distribution of two clusterings in-
duced by two runs of the algorithm, we treat the
clusters induced by a given run as samples from
a random variable. The events of this variable are
the induced clusters and the probability assigned
to each event is equal to the number of word in-
stances contained in the corresponding cluster, di-
vided by the total number of word instances in the
tagged corpus. The entropy of this random vari-
able is used as a statistic for the word instance
distribution. Clusterings having similar cluster in-
stance size distributions also have similar values
of this statistic.
We apply an entropy outliers filter to the set of
clusterings C. In this filter, we sort the members
of C (these are clusterings obtained in different
runs of the base tagger) according to their clus-
ter instance size entropy, and prune K runs from
the beginning and K runs from the end of the list.
The perplexity-based quality test described above
is applied only to members of C that were not
pruned in this step.
Figure 2 (left) shows the average perplexity of
a set of clusterings as a function of the parame-
ter K of the entropy-based filter. Results are pre-
sented for 100 runs of ZCC2 with three different
exponent values (0.9, 1.1, 1.3). These assignments
yield considerably different Zipfian distributions.
While all three models have similar average per-
plexity over all 100 runs, only the solid line (cor-
responding to an exponent value of 1.1) consis-
2See Section 4 for the experimental setup.
60
tently decreases (improves) with K. The circled
line (corresponding to an exponent value of 1.3)
monotonically decreases with K until a certain K
value, while the line with triangles (correspond-
ing to an exponent value of 0.9) remains relatively
constant.
Figure 2 (right) shows that models for which
the entropy-based filter improves perplexity more
drastically, exhibit better correlation between per-
plexity and external clustering quality3.
Our unsupervised parameter selection method is
thus based on finding a value which exhibits a con-
sistent decrease in perplexity as a function of K,
the number of clusterings pruned from the begin-
ning and end of the entropy-sorted list. In the rest
of this paper we show results where the exponent
value is 1.1.
3 Previous Work
Unsupervised POS induction/tagging is a fruitful
area of research. A major direction is Hidden
Markov Models (HMM) (Merialdo, 1994; Banko
and Moore, 2004; Wang and Schuurmans, 2005).
Several recent works have tried to improve this
model using Bayesian estimation (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008), sophisticated initialization (Goldberg et al,
2008), induction of an initial clustering used to
train an HMM (Freitag, 2004; Biemann, 2006),
infinite HMM models (Van Gael et al, 2009), in-
tegration of integer linear programming into the
parameter estimation process (Ravi and Knight,
2009), and biasing the model such that the num-
ber of possible tags that each word can get is small
(Grac?a et al, 2009).
The Bayesian works integrated into the model
information about the distribution of words to POS
tags. For example, Johnson (2007) integrated to
the EM-HMM model a prior that prefers cluster-
ings where the distributions of hidden states to
words is skewed.
Other approaches include transformation based
learning (Brill, 1995), contrastive estimation for
conditional random fields (Smith and Eisner,
2005), Markov random fields (Haghighi and
Klein, 2006), a multilingual approach (Snyder et
al., 2008; Snyder et al, 2008) and expanding a
3The figure is for greedy many-to-one mapping and
Spearman?s rank correlation coefficient, explained in further
Sections. Other external measures and rank correlation scores
demonstrate the same pattern.
partial dictionary and use it to learn disambigua-
tion rules (Zhao and Marcus, 2009).
These works, except (Haghighi and Klein,
2006; Johnson, 2007; Gao and Johnson, 2008)
and one experiment in (Goldwater and Griffiths,
2007), used a dictionary listing the allowable tags
for each word in the text. This dictionary is usu-
ally extracted from the manual tagging of the text,
contradicting the unsupervised nature of the task.
Clearly, the availability of such a dictionary is not
always a reasonable assumption (see e.g. (Gold-
water and Griffiths, 2007)).
In a different algorithmic direction, (Schuetze,
1995) applied latent semantic analysis with SVD
based dimensionality reduction, and (Schuetze,
1995; Clark, 2003; Dasgupta and NG, 2007) used
distributional and morphological statistics to find
meaningful word types clusters. Clark (2003) is
the only such work to have evaluated its algorithm
as a POS tagger for large corpora, like we do in
this paper.
A Zipfian constraint was utilized in (Goldwater
and et al, 2006) for language modeling and mor-
phological disambiguation.
The problem of convergence to local maxima
has been discussed in (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Goldwater and Grif-
fiths, 2007; Johnson, 2007; Gao and Johnson,
2008) with a detailed demonstration in (Johnson,
2007). All these authors (except Smith and Eisner
(2005), see below), however, reported average re-
sults over several runs and did not try to identify
the runs that produce high quality tagging.
Smith and Eisner (2005) initialized with all
weights equal to zero (uninformed, deterministic
initialization) and performed unsupervised model
selection across smoothing parameters by evaluat-
ing the training criterion on unseen, unlabeled de-
velopment data. In this paper we show that for the
tagger of (Clark, 2003) such a method provides
mediocre results (Table 2) even when the train-
ing criterion (likelihood or data probability for this
tagger) is evaluated on the test set. Moreover, we
show that our algorithm outperforms existing POS
taggers for most evaluation measures (Table 3).
Identifying good solutions among many runs of
a randomly-initialized algorithm is a well known
problem. We discuss here the work of (Smith and
Eisner, 2004) that addressed the problem in the un-
supervised POS tagging context. In this work, de-
terministic annealing (Rose et al, 1990) was ap-
61
plied to an HMM model for unsupervised POS
tagging with a dictionary. This method is not sen-
sitive to its initialization, and while it is not the-
oretically guaranteed to converge to a better so-
lution than the traditional EM-HMM, it was ex-
perimentally shown to achieve better results. The
problem has, of course, been addressed in other
contexts as well (see, e.g., (Wang et al, 2002)).
4 Experimental Setup and Evaluation
Setup. We used the English WSJ PennTreebank
corpus in our experiments. We induced POS tags
for sections 2-21 (43K word types, 950K word in-
stances of which 832K (87.6%) are not punctua-
tion marks), using Q(ZCC), Q(CT), and CT. For
the unsupervised quality test, we trained the bi-
gram class-based language model on sections 2-21
with the induced clusters, and computed its per-
plexity on section 23.
In Q(ZCC) and Q(CT), the base taggers were
run a 100 times each, using different random ini-
tializations. In each run we induce 13 clusters,
since this is the number of unique POS tags re-
quired to cover 98% of the word types in WSJ
(Figure 3)4. Some previous work (e.g., (Smith and
Eisner, 2005)) also induced 13 non-punctuation
tags.
We compare the results of our algorithm to
those of the original Clark algorithm5. The in-
duced clusters are evaluated against two POS tag
sets: one is the full set of WSJ POS tags, and the
other consists of the non-punctuation tags of the
first set.
Punctuation marks constitute a sizeable volume
of corpus tokens and are easy to cluster correctly.
Hence, evaluting against the full tag set that in-
cludes punctuation artificially increases the qual-
ity of the reported results, which is why we report
results for the non-punctuation tag set. However,
to be able to directly compare with previous work,
we also report results for the full WSJ POS tag
set. We do so by assigning a singleton cluster to
each punctuation mark (in addition to the 13 clus-
ters). This simple heuristic yields very high per-
formance on punctuation, scoring (when all other
terminals are assumed perfect tagging) 99.6% in
1-to-1 accuracy.
4Some words can get more than one POS tag. In the fig-
ure, for these words we increased the counters of all their
possible tags.
5Downloaded from www.cs.rhul.ac.uk/home/alexc/
RHUL/Downloads.html.
In addition to comparing the different algo-
rithms, we compare the correlation between our
tagging quality test and external clustering quality
for both the original CT algorithm and our ZCC
algorithm.
Clustering Quality Evaluation. The induced
POS tags have arbitrary names. To evaluate them
against a manually annotated corpus, a proper
correspondence with the gold standard POS tags
should be established. Many evaluation measures
for unsupervised clustering against gold standard
exist. Here we use measures from two well ac-
cepted families: mapping based and information
theoretic (IT) based. For a recent discussion on
this subject see (Reichart and Rappoport, 2009).
The mapping based measures are accuracy with
greedy many-to-1 (M-1) and with greedy 1-to-1
(1-1) mappings of the induced to the gold labels.
In the former mapping, two induced clusters can
be mapped to the same gold standard cluster, while
in the latter mapping each and every induced clus-
ter is assigned a unique gold cluster.
After each induced label is mapped to a gold
label, tagging accuracy is computed. Accuracy is
defined to be the number of correctly tagged words
in the corpus divided by the total number of words
in the corpus.
The IT based measures we use are V (Rosen-
berg and Hirschberg, 2007) and NVI (Reichart and
Rappoport, 2009). The latter is a normalization of
the VI measure (Meila, 2007). VI and NVI induce
the same order over clusterings but NVI values for
good clusterings lie in [0, 1]. For V, the higher
the score, the better the clustering. For NVI lower
scores imply improved clustering quality. We use
e as the base of the logarithm.
Evaluation of the Quality Test. To mea-
sure the correlation between the score produced
by the tagging quality test and the external qual-
ity of a tagging, we use two well accepted mea-
sures: Spearman?s rank correlation coefficient
and Kendall Tau (Kendall and Dickinson, 1990).
These measure the correlation between two sorted
lists. For the computation of these measures, we
rank the clusterings once according to the identifi-
cation criterion and once according to the external
quality measure.
The measures are given by the equations:
(6) kendall ? tau = 2(nc?nd)r(r?1)
(7) Spearsman = 1 ? 6
?r
i=1 d
2
i
r(r2?1)
62
0 10 20 30 400
0.2
0.4
0.6
0.8
1
Number of POS Tags
F
ra
ct
io
n
 o
f 
It
e
m
s
Figure 3: The fraction of word types (solid curve)
and word instances (dashed curve) labeled with
the k (X axis) most frequent POS tags (in types
and tokens respectively) in sections 2-21 of the
WSJ corpus.
where r is the number of runs (100 in our case),
nc and nd are the numbers of concordant and dis-
cordant pairs respectively6 and di is the absolute
value of the difference between the ranks of item
i.
The two measures have the properties that a
perfect agreement between rankings results in a
score of 1, a perfect disagreement results in a score
of ?1, completely independent rankings have the
value of 0 on the average, the range of values is
between ?1 and 1, and increasing values imply
increasing agreement between the rankings. For a
discussion see (Lapata, 2006).
5 Results
Table 1 presents the results of the Q(ZCC) and
Q(CT) algorithms, which are both better than
those of the original Clark tagger CT. The Q al-
gorithms provide a tagging that is better than that
produced by CT in 82-100% (Q(ZCC)) and 75-
100% (Q(CT)) of the cases.
The Q(ZCC) algorithm is superior when eval-
uated with the mapping based measures. The
Q(CT) algorithm is superior when evaluated with
the IT measures.
Table 3 presents reported results for all recent
algorithms we are aware of that tackled the task
of unsupervised POS induction from plain text 7.
The settings of the various experiments vary in
terms of the exact gold annotation scheme used
for evaluation (the full WSJ set was used by all
authors except Goldwater and Griffiths (2007) and
6A pair r, t in two lists X and Y is concordant if
sign(Xt ? Xr) = sign(Yt ? Yr), where Xr is the index
of r in the list X .
7VG and GG used 2 as the base of the logarithm in IT
measures, which affects VI. We converted the VI numbers
reported in their papers to base e.
the GGTP-17 model which used the set of 17
coarse grained tags proposed by (Smith and Eis-
ner, 2005)) and the size of the test set. The num-
bers reported for the algorithms of other works are
the average performance over multiple runs, since
no method for identification of high quality tag-
gings was used.
The results of our algorithms are superior, ex-
cept for the M-1 performance of some of the mod-
els of (Johnson, 2007) and of the GGTP-17 and
GGTP-45 models of (Grac?a et al, 2009). Note
that the models of (Johnson, 2007) and the GGTP-
45 model induce 40-50 clusters compared to our
34 (13 non-punctuation plus the additional 21 sin-
gleton punctuation tags). Increasing the number
of clusters is known to improve the M-1 mea-
sure (Reichart and Rappoport, 2009). GGTP-17
gives the best M-1 results, but its 1-1 results are
much worse than those of Q(ZCC), Q(CT), and
CT, and the information theoretic measures V and
NVI were not reported for it.
Recall that the Q algorithms tag punctuation
marks according to the scheme which assigns each
of them a unique cluster (Section 4), while previ-
ous work does not distinguish punctuation marks
from other tokens. To quantify the effect vari-
ous punctuation schemes have on the results re-
ported in Table 3, we evaluated the ?iHMM: PY-
fixed? model (Van Gael et al, 2009) and the Q al-
gorithms when punctuation is excluded and when
both PY-fixed and Q algorithms use the punctua-
tion scheme described in Section 4.
For the PY-fixed, which induces 91 clusters,
results are (punctuation is excluded, heuristic is
used): V(0.530, 0.608), NVI (0.999, 0.823), 1-1
(0.484, 0.543), M-1 (0.591, 0.639). The results
for the Q algorithms are given in Table 1 (top
line: excluding punctuation, bottom line: using
the heuristic). The Q algorithms are better for the
V, NVI and 1-1 measures. For M-1 evaluation,
PY-fixed, which induces substantially more clus-
ters (91 compared to our 34) is better.
In what follows, we provide an analysis of the
components of our algorithms. To explore the
quality of our tagging component, ZCC, table 4
compares the mean, mode and standard deviation
of a 100 runs of ZCC with 100 runs of the original
CT algorithm8. The performance of the tagging
8In mode calculation we treat the 100 runs as samples of
a continuous random variable. We divide the results range
to 10 bins of the same size. The mode is the center of the
bin having the largest number of runs. If there is more than
63
Alg. V NVI 1-1 M-1
Q(ZCC)
no punct. 0.538 (85, 2.6) 0.849 (82, 3.2) 0.521 (100, 4.3) 0.533 (84, 1.7)
with punct. 0.637 (85, 1.8) 0.678 (82, 2.6) 0.58 (100, 3) 0.591 (84, 1.18)
Q(CT)
no punct. 0.545 (92, 3.3) 0.837 (88, 4.4) 0.492 (99,1.4) 0.526 (75, 1)
with punct. 0.644 (92, 2.5) 0.662 (88, 4.2) 0.555 (99, 0.5) 0.585 (75, 0.58)
Table 1: Quality of the tagging produced by Q(ZCC) and Q(CT). The top (bottom) line for each algorithm
presents the results when punctuation is not included (is included) in the evaluation (Section 4). The left
number in the parentheses is the fraction of Clark?s (CT) results that scored worse than our models (%
from 100 runs). The right number in the parentheses is 100 times the difference between the score of our
model and the mean score of 100 runs of Clark?s (CT). Q(ZCC) is better than Q(CT) in the mappings
measures, while Q(CT) is better in the IT measures. Both are better than the original Clark tagger CT.
Data Probability Likelihood Perplexity
V m-to-1 V m-to-1 V m-to-1
Alg. SRC KT SRC KT SRC KT SRC KT SRC KT SRC KT
CT 0.2 0.143 0.071 0.045 0.338 0.23 0.22 0.148 0.568 0.397 0.476 0.33
ZCC 0.134 0.094 0.118 0.078 0.517 0.352 0.453 0.321 0.82 0.62 0.659 0.484
Table 2: Correlation of unsupervised quality measures (columns) with clustering quality of two base
taggers (CT and ZCC, rows). Correlation is measured by Spearman (SRC) and Kendall Tau (KT) rank
correlation coefficients. The quality measures are data probability (left part), likelihood (middle side)
and perplexity (right part), and correlation is between these and two of the external evaluation measures,
m-to-1 mapping and V (results for the other two clustering evaluation measures, 1-1 mapping and NVI,
are very similar). Results for the perplexity quality test used by family Q are superior; data probability
and likelihood provide only a mediocre indication for the quality of induced clustering. Note that the
correlation values are much higher for ZCC than for CT.
components are quite similar, with a small advan-
tage to CT in mean and to ZCC in mode.
Our quality test is based on the perplexity of a
class bigram language model trained with the in-
duced tagging. To emphasize its strength we com-
pare it to two natural quality tests: the likelihood
and value of the probability function to which the
tagging algorithm converges (equations (2) and (1)
in Section 2.1). The results are shown in Table
2 First, we see that our perplexity quality test is
much better correlated with the quality of the tag-
ging induced by both ZCC and CT. Second, the
correlation is indeed much higher for ZCC than
for CT.
The power of Q(ZCC) lies in the combination
between the perplexity-based quality test and the
tagging component ZCC. The performance of the
tagging component ZCC does not provide a def-
inite improvement over the original Clark tagger.
ZCC compromises mean tagging results for an im-
proved correlation between Q?s quality measure
one such bin, we average their centers. We use this technique
since it is rare to see two different runs of either algorithm
with the exact same quality.
and gold standard-based tagging evaluation.
6 Conclusion
In this paper we addressed unsupervised POS tag-
ging as a task where the quality of a single tag-
ging is to be reported, rather than the average per-
formance of a tagging algorithm over many runs.
We introduced a family of algorithms Q(B) based
on an unsupervised test for tagging quality that is
used to select a high quality tagging from the out-
put of multiple runs of a POS tagger B.
We introduced the ZCC tagger which modifies
the original Clark tagger by constraining the clus-
tering space using a cluster type size Zipfian con-
straint, conforming with a known property of nat-
ural languages.
We showed that the tagging produced by our
Q(ZCC) algorithm is better than that of the Clark
algorithm with a probability of 82-100%, depend-
ing on the measure used. Moreover, our tagging
outperforms in most evaluation measures the re-
sults reported in all recent works that addressed
the task.
In future work, we intend to try to improve
64
Alg. V VI M-1 1-1
Q(ZCC) 0.637 2.06 0.591 0.58
Q(CT) 0.644 2.01 0.585 0.555
CT 0.619 2.14 0.576 0.543
HK ? ? ? 0.413
J ? 4.23 -
5.74
0.43 -
0.62
0.37 ?
0.47
GG ? 2.8 ? ?
G-J ? 4.03 ?
4.47
? 0.4 ?
0.499
VG 0.54 -
0.59
2.49 ?
2.91
? ?
GGTP-45 ? ? 0.654 0.445
GGTP-17 ? ? 0.702 0.495
Table 3: Comparison of our algorithms with the
recent fully unsupervised POS taggers for which
results are reported. HK: (Haghighi and Klein,
2006), trained and evaluated with a corpus of
193K tokens and 45 induced tags. GG: (Goldwa-
ter and Griffiths, 2007), trained and evaluated with
a corpus of 24K tokens and 17 induced tags. J :
(Johnson, 2007) inducing 25-50 tags (the results
that are higher than Q in the M-1 measure are for
40-50 tags). GJ: (Gao and Johnson, 2008), induc-
ing 50 tags. VG: (Van Gael et al, 2009), inducing
47-192 tags. GGTP-45: (Grac?a et al, 2009), in-
ducing 45 tags. GGTP-17: (Grac?a et al, 2009),
inducing 17 tags. All five were trained and evalu-
ated with the full WSJ PTB (1.17M words). Lower
VI values indicates better clustering.
Statistic V NVI M-1 1-1
CT
Mean 0.512 0.881 0.516 0.478
Mode 0.502 0.886 0.514 0.465
Std 0.022 0.035 0.018 0.028
ZCC
Mean 0.503 0.908 0.512 0.478
Mode 0.509 0.907 0.518 0.47
Std 0.021 0.036 0.018 0.0295
Table 4: Average performance of ZCC compared
with CT (results presented without punctuation).
Presented are mean, mode (see text for its calcu-
lation), and standard deviation (std). CT mean re-
sults are slightly better, and both algorithms have
about the same standard deviation. ZCC sacrifices
a small amount of mean quality for a good corre-
lation with our quality test, which allows Q(ZCC)
to be much better than the mean of CT and most
of its runs.
our quality measure, experiment with additional
languages, and apply the ?family of algorithms?
paradigm to additional relevant NLP tasks.
References
Michele Banko and Robert C. Moore, 2003. Part of
Speech Tagging in Context. COLING ?04.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ?06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Eric Brill, 1995. Unsupervised Learning if Disam-
biguation Rules for Part of Speech Tagging. 3rd
Workshop on Very Large Corpora.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18:467-479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP ?07.
Steven Finch, Nick Chater and Martin Redington,
1995. Acquiring syntactic information from distri-
butional statistics. Connectionist models of memory
and language. UCL Press, London.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ?04.
Jianfeng Gao and Mark Johnson, 2008. A compari-
son of Bayesian estimators for unsupervised Hidden
Markov Model POS taggers. EMNLP ?08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ?08
Sharon Goldwater, Tom Griffiths, and Mark Johnson,
2006. Interpolating between types and tokens by es-
timating power-law generators. NIPS ?06.
Sharon Goldwater and Tom Griffiths, 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. ACL ?07.
Joshua Goodman, 2001. A Bit of Progress in Lan-
guage Modeling, Extended Version. Microsoft Re-
search Technical Report MSR-TR-2001-72.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
65
Maurice Kendall and Jean Dickinson, 1990. Rank
Correlation methods. Oxford University Press, New
York.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learning for Sequence Labeling. HLT-NAACL ?06.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMM POS-Taggers? EMNLP-CoNLL ?07.
Harold W. Kuhn, 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83-97.
Mirella Lapata, 2006. Automatic Evaluation of In-
formation Ordering: Kendall?s Tau. Computational
Linguistics, 4:471-484.
Sven Martin, Jorg Liermann, and Hermann Ney, 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19-37.
Marina Meila, 2007. Comparing Clustering - an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155-172.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions. Internet Mathematics, 1(2):226-251.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Hermann Ney, Ute Essen, and Reinhard Kneser,
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 8:1-38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
?09.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox,
1990. Statistical Mechanics and Phase Transitions
in Clustering. Physical Review Letters, 65(8):945-
948.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ?07.
Hinrich Schuetze, 1995. Distributional part-of-speech
tagging. EACL ?95.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ?04.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ?05.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay, 2009. Adding More Lan-
guages Improves Unsupervised Multilingual Part-
of-Speech Tagging: A Bayesian Non-Parametric
Approach. NAACL ?09.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay, 2008. Unsupervised Multi-
lingual Learning for POS Tagging. EMNLP ?08.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP-KE ?05.
Shaojun Wang, Dale Schuurmans and Yunxin Zhao,
2002. The Latent Maximum Entropy Principle.
ISIT ?02.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ?09.
66
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77?87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study
Roi Reichart1? Omri Abend2?? Ari Rappoport2
1ICNC 2Institute of Computer Science
Hebrew University of Jerusalem
{roiri|omria01|arir}@cs.huji.ac.il
Abstract
Clustering is a central technique in NLP.
Consequently, clustering evaluation is of
great importance. Many clustering algo-
rithms are evaluated by their success in
tagging corpus tokens. In this paper we
discuss type level evaluation, which re-
flects class membership only and is inde-
pendent of the token statistics of a partic-
ular reference corpus. Type level evalua-
tion casts light on the merits of algorithms,
and for some applications is a more natural
measure of the algorithm?s quality.
We propose new type level evaluation
measures that, contrary to existing mea-
sures, are applicable when items are pol-
ysemous, the common case in NLP. We
demonstrate the benefits of our measures
using a detailed case study, POS induc-
tion. We experiment with seven leading
algorithms, obtaining useful insights and
showing that token and type level mea-
sures can weakly or even negatively corre-
late, which underscores the fact that these
two approaches reveal different aspects of
clustering quality.
1 Introduction
Clustering is a central machine learning technique.
In NLP, clustering has been used for virtually ev-
ery semi- and unsupervised task, including POS
tagging (Clark, 2003), labeled parse tree induction
(Reichart and Rappoport, 2008), verb-type clas-
sification (Schulte im Walde, 2006), lexical ac-
quisition (Davidov and Rappoport, 2006; Davi-
dov and Rappoport, 2008), multilingual document
?
* Both authors equally contributed to this paper.
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
clustering (Montavlo et al, 2006), coreference res-
olution (Nicolae and Nicolae, 2006) and named
entity recognition (Elsner et al, 2009). Conse-
quently, the methodology of clustering evaluation
is of great importance. In this paper we focus
on external clustering evaluation, i.e., evaluation
against manually annotated gold standards, which
exist for almost all such NLP tasks. External eval-
uation is the dominant form of clustering evalu-
ation in NLP, although other methods have been
proposed (see e.g. (Frank et al, 2009)).
In this paper we discuss type level evaluation,
which evaluates the set membership structure cre-
ated by the clustering, independently of the token
statistics of the gold standard corpus. Many clus-
tering algorithms are evaluated by their success
in tagging corpus tokens (Clark, 2003; Nicolae
and Nicolae, 2006; Goldwater and Griffiths, 2007;
Gao and Johnson, 2008; Elsner et al, 2009). How-
ever, in many cases a type level evaluation is the
natural one. This is the case, for example, when
a POS induction algorithm is used to compute a
tag dictionary (the set of tags that each word can
take), or when a lexical acquisition algorithm is
used for constructing a lexicon containing the set
of frames that a verb can participate in, or when a
sense induction algorithm computes the set of pos-
sible senses of each word. In addition, even when
the goal is corpus tagging, a type level evaluation
is highly valuable, since it may cast light on the
relative or absolute merits of different algorithms
(as we show in this paper).
Clustering evaluation has been extensively in-
vestigated (Section 3). However, the discussion
centers around the monosemous case, where each
item belongs to exactly one cluster, although pol-
ysemy is the common case in NLP.
The contribution of the present paper is as fol-
lows. First, we discuss the issue of type level eval-
uation and explain why even in the monosemous
case a token level evaluation presents a skewed
77
picture (Section 2). Second, we show for the
common polysemous case why adapting existing
information-theoretic measures to type level eval-
uation is not natural (Section 3). Third, we pro-
pose new mapping-based measures and algorithms
to compute them (Section 4). Finally, we perform
a detailed case study with part-of-speech (POS)
induction (Section 5). We compare seven lead-
ing algorithms, showing that token and type level
measures can weakly or even negatively correlate.
This shows that type level evaluation indeed re-
veals aspects of a clustering solution that are not
revealed by the common tagging-based evaluation.
Clustering is a vast research area. As far as we
know, this is the first NLP paper to propose type
level measures for the polysemous case.
2 Type Level Clustering Evaluation
This section motivates why both type and token
level external evaluations should be done, even in
the monosemous case.
Clustering algorithms compute a set of induced
clusters (a clustering). Some algorithms directly
compute a clustering, while some others produce
a tagging of corpus tokens from which a clustering
can be easily derived. A clustering is monosemous
if each item is allowed to belong to a single cluster
only, and polysemous otherwise. An external eval-
uation is one which is based on a comparison of an
algorithm?s result to a gold standard. In this paper
we focus solely on external evaluation, which is
the most common evaluation approach in NLP.
Token and type level evaluations reflect differ-
ent aspects of a clustering. External token level
evaluation assesses clustering quality according to
the clustering?s accuracy on a given manually an-
notated corpus. This is certainly a useful evalua-
tion measure, e.g. when the purpose of the cluster-
ing algorithm is to annotate a corpus to serve as
input to another application.
External type level evaluation views the com-
puted clustering as a set membership structure and
evalutes it independently of the token statistics in
the gold standard corpus. There are two main
cases in which this is useful. First, a type level
evaluation can be the natural one in light of the
problem itself. For example, if the purpose of
the clustering algorithm is to automatically build
a lexicon (e.g., VerbNet (Kipper et al, 2000)),
then the lexicon structure itself should be evalu-
ated. Second, it may be valuable to decouple cor-
pus statistics from the induced clustering when the
latter is to be used for annotating corpora that ex-
hibit different statistics. In other words, if we eval-
uate an algorithm that will be invoked on a diverse
set of corpora having different token statistics, a
type level evaluation might provide a better picture
(or at least a complementary one) on the quality of
the clustering algorithm.
To motivate type level evaluation, consider POS
induction, which exemplifies both cases above.
Clearly, a word form may belong to several parts
of speech (e.g., ?contrast? is both a noun and a
verb, ?fast? is both an adjective and an adverb,
?that? can be a determiner, conjunction and adverb,
etc.). As an evaluation of a POS induction algo-
rithm, it is natural to evaluate the lexicon it gener-
ates, even if the main goal is to annotate a corpus.
The lexicon lists the possible POS tags for each
word, and thus its evaluation is a polysemous type
level one.
Even if we ignore polysemy, type level evalua-
tion is useful for a POS induction algorithm used
to tag a corpus. There are POS classes whose
members are very frequent, e.g., determiners and
prepositions. Here, a very small number of word
types usually accounts for a large portion of corpus
tokens. For example, in the WSJ Penn Treebank
(Marcus et al, 1993), there are 43,740 word types
and over 1M word tokens. Of the types, 88 are
tagged as prepositions. These types account for
only 0.2% of the types, but for as many as 11.9%
of the tokens. An algorithm which is accurate only
on prepositions would do much better in a token
level evaluation than in a type level one.
This phenomenon is not restricted to preposi-
tions or English. In the WSJ corpus, determiners
account for 0.05% of the types but for 9.8% of the
tokens. In the German NEGRA corpus (Brants,
1997), the article class (both definite and indefi-
nite) accounts for 0.04% of the word types and for
12.5% of the word tokens, and the coordinating
conjunctions class accounts for 0.05% of the word
types but for 3% of the tokens.
The type and token behavior differences result
from the Zipfian distribution of word tokens to
word types (Mitzenmacher, 2004). Since the word
frequency distribution is Zipfian, any clustering al-
gorithm that is accurate only on a small number of
frequent words (not necessarily members of a par-
ticular class) would perform well in a token level
evaluation but not in a type one. For example,
78
the most frequent 100 word types (regardless of
POS class) in WSJ (NEGRA) account for 43.9%
(41.3%) of the tokens in the corpus. These words
appear in 32 out of the 34 non-punctuation POS
classes in WSJ and in 38 out of the 51 classes in
NEGRA.
Other natural language entities also demonstrate
Zipfian distribution of tokens to types. For exam-
ple, the distribution of syntactic categories in parse
tree constituents is Zipfian, as shown in (Reichart
and Rappoport, 2008) for English, German and
Chinese corpora. Thus, the distinction between to-
ken and type level evaluation is important also for
grammar induction algorithms.
It may be argued that a token level evaluation
is sufficient since it already reflects type informa-
tion. In this paper we demonstrate that this is not
the case, by showing that they correlate weakly or
even negatively in an important NLP task.
3 Existing Clustering Evaluation
Measures
Clustering evaluation is challenging. Many mea-
sures have been proposed in the past decades
(Pfitzner et al, 2008). In this section, we briefly
survey the three main types: mapping based,
counting pairs, and information theoretic mea-
sures, and motivate our decision to focus on the
first in this paper.
Mapping based measures are based on a post-
processing step in which each induced cluster is
mapped to a gold class (or vice versa). The stan-
dard mappings are greedy many-to-one (M-1) and
greedy one-to-one (1-1). Several measures which
rely on these mappings were proposed. The most
common and perhaps the simplest one is accu-
racy, which computes the fraction of items cor-
rectly clustered under the mapping. Other mea-
sures include: L (Larsen, 1999), D (Van Dongen,
2000), misclassification index (MI) (Zeng et al,
2002), H (Meila and Heckerman, 2001), clustering
F-measure (Fung et al, 2003) and micro-averaged
precision and recall (Dhillon et al, 2003). In Sec-
tion 4 we show why existing mapping-based mea-
sures cannot be applied to the polysemous type
case and present new mapping-based measures for
this case.
Counting pairs measures are based on a com-
binatorial approach which examines the number
of data element pairs that are clustered similarly
in the reference and proposed clustering. Among
these are Rand Index (Rand, 1971), Adjusted Rand
Index (Hubert and Arabie, 1985), ? statistic (Hu-
bert and Schultz, 1976), Jaccard (Milligan et al,
1983), Fowlkes-Mallows (Fowlkes and Mallows,
1983) and Mirkin (Mirkin, 1996). Schulte im
Walde (2006) used such a measure for type level
evaluation of monosemous verb type clustering.
Meila (2007) described a few problems with
such measures. A serious one is that their values
are unbounded, making it hard to interpret their
results. This can be solved by adjusting their val-
ues to lie in [0, 1], but even adjusted measures suf-
fer from severe distributional problems, limiting
their usability in practice. We thus do not address
counting pairs measures in this paper.
Information-theoretic (IT) measures. IT
measures assume that the items in the dataset are
taken from a known distribution (usually the uni-
form distribution), and thus the gold and induced
clusters can be treated as random variables. These
measures utilize a co-occurrence matrix I between
the gold and induced clusters. We denote the in-
duced clustering by K and the gold clustering by
C. Iij contains the number of items in the in-
tersection of the i-th gold class and the j-th in-
duced cluster. When assuming the uniform dis-
tribution, the probability of an event (a gold class
c or an induced cluster k) is its relative size, so
p(c) = ?|K|k=1
Ick
N and p(k) =
?|C|
c=1
Ick
N (N is the
total number of clustered items).
Under this assumption we define the entropies
and the conditional entropies:
H(C) = ? P|C|c=1
P|K|
k=1 Ick
N log
P|K|
k=1 Ick
N
H(C|K) = ? P|K|k=1
P|C|
c=1
Ick
N log
Ick
P|C|
c=1 Ick
H(K) and H(K|C) are defined similarly.
In Section 5 we use two IT measures for token
level evaluation, V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009)
(a normalized version of VI (Meila, 2007)). The
appealing properties of these measures have been
extensively discussed in these references; see also
(Pfitzner et al, 2008). V and NVI are defined as
follows:
h =
(
1 H(C) = 0
1 ? H(C|K)H(C) H(C) 6= 0
c =
(
1 H(K) = 0
1 ? H(K|C)H(K) H(K) 6= 0
V = 2hc
h + c
79
NV I(C, K) =
(
H(C|K)+H(K|C)
H(C) H(C) 6= 0
H(K) H(C) = 0
In the monosemous case (type or token), the ap-
plication of the measures described in this section
to type level evaluation is straightforward. In the
polysemous case, however, they suffer from seri-
ous shortcomings.
Consider a case in which each item is assigned
exactly r gold clusters and each gold cluster has
the exact same number of items (i.e., each has a
size of l?r|C| , where l is the number of items). Now,
consider an induced clustering where there are |C|
induced clusters (|K| = |C|) and each item is as-
signed to all induced clusters. The co-occurrence
matrix in this case should have identical values in
all its entries. Even if we allow the weight each
item contributes to the matrix to depend on its gold
and induced entry sizes, the situation will remain
the same. This is because all items have the exact
same entry size and both gold and induced cluster-
ings have uniform cluster sizes.
In this case, the random variables defined by the
induced and gold clustering assignments are in-
dependent (this easily follows from the definition
of independent events, since the joint probability
is the multiplication of the marginals). Hence,
H(K|C) = H(K) and H(C|K) = H(C), and
both V and NVI obtain their worst possible val-
ues1. However, the score should surely depend on
r (the size of each word?s gold entry). Specifi-
cally, when r = |C| we get that the induced and
gold clusterings are identical. This case should not
get the worst score, and it should definitely score
higher than the case in which r = 1, where K is
dramatically different from C.
The problem can in theory be solved by pro-
viding the number of clusters per item as an input
to the algorithm. However, in NLP this is unre-
alistic (even if the total number of clusters can be
provided) and the number should be determined
by the algorithm. We therefore do not consider
IT-based measures in this paper, deferring them to
future work.
4 Mapping Based Measures for
Polysemous Type Evaluation
In this section we present new type level evalu-
ation measures for the polysemous case. As we
1V values are in [0, 1], 0 being the worst. NVI obtains its
highest and worst possible value, 1 + log(|K|)H(C) .
show below, these measures do not suffer from the
problems discussed for IT measures in Section 3.
All measures are mapping-based: first, a map-
ping between the induced and gold clusters is per-
formed, and then a measure E is computed. As
is common in the clustering evaluation literature
(Section 3), we use M-1 and 1-1 greedy mappings,
defined to be those that maximize the correspond-
ing measure E.
Let C = {c1, ..., cn} be the set of gold classes
and K = {k1, ..., km} be the set of induced clus-
ters. Denote the number of words types by l. Let
Ai ? C, Bi ? K, i = 1...l be the set of gold
classes and set of induced clusters for each word.
The polysemous nature of task is reflected by the
fact that Ai and Bi are subsets, rather than mem-
bers, of C and K respectively.
Our measures address quality from two persec-
tives, that of the individual items clustered (Sec-
tion 4.1) and that of the clusters (Section 4.2).
Item-based measures especially suit evaluation of
clustering quality for the purpose of lexicon induc-
tion, and have no counterpart in the monosemous
case. Cluster-based measures are a direct general-
ization of existing mapping based measures to the
polysemous case.
The difficulty in designing item-based and
cluster-based measures is that the number of clus-
ters assigned to each item is determined by the
clustering algorithm. Below we show how to over-
come this.
4.1 Item-Based Evaluation
For a given mapping h : K ? C, denote
h(Bi) = {h(x) : x ? Bi}. A fundamental quan-
tity for item-based evaluation is the number of cor-
rect clusters for each item (word type) under this
mapping, denoted by IMi (IM stands for ?item
match?):
IMi = |Ai ? h(Bi)|
The total item match IM is defined to be:
IM = ?li=1 IMi =
?l
i=1 |Ai ? h(Bi)|
In the monosemous case, IM is normalized by
the number of items, yielding an accuracy score.
Applying a similar definition in the polysemous
case, normalizing instead by the total number of
gold clusters assigned to the items, can be easily
manipulated. Even a clustering which has the cor-
rect number of induced clusters (equal to the num-
ber of gold classes) but which assigns each item to
80
all induced clusters, receives a perfect score under
both greedy M-1 and 1-1 mappings. This holds for
any induced clustering for which ?i, Ai ? h(Bi).
Note that using a mapping from C to K (or a
combination of both directions) would exhibit the
same problem.
To overcome the problem, we use the harmonic
average of two normalized terms (F-score). We
use two average variants, micro and macro. Macro
average computes the total number of matches
over all words and normalizes in the end. Recall
(R), Precision (P) and their harmonic average (F-
score) are accordingly defined:
R = IMPl
i=1 |Ai|
P = IMPl
i=1 |h(Bi)|
MacroI = 2RP
R + P =
= 2IM
Pl
i=1 |Ai| +
Pl
i=1 |h(Bi)|
= F (h) ?
l
X
i=1
IMi
F (h) is a constant depending on h. As all items
are equally weighted, those with larger gold and
induced entries have more impact on the measure.
The micro average, aiming to give all items an
equal status, first computes an F-score for each
item and then averages over them. Hence, each
item contributes at most 1 to the measure. This
MicroI measure is given by:
Ri = IMi|Ai| Pi =
IMi
|h(Bi)| Fi =
2RiPi
Ri+Pi =
2IMi
|Ai|+|h(Bi)|
MicroI = 1
l
l
X
i=1
Fi =
1
l
l
X
i=1
2IMi
|Ai| + |h(Bi)|
=
= 1
l
l
X
i=1
wi(h) ? IMi
Where wi(h) is a weight depending on h but
also on i.
For both measures, the maximum score is 1. It
is obtained if and only if Ai = h(Bi) for every i.
In 1-1 mapping, when the number of induced
clusters is larger than the number of gold clus-
ters, some of the induced clusters are not mapped.
To preserve the nature of 1-1 mapping that pun-
ishes for excessive clusters2, we define |h(Bi)| to
be equal to |Bi| even for these unmapped clusters.
Recall that any induced clustering in which
?i, Ai ? h(Bi) gets the best score under a greedy
mapping with the accuracy measure. In MacroI
and MicroI the obtained recalls are perfect, but the
precision terms reflect deviation from the correct
solution.
2And to allow us to compute it accurately, see below.
In the example in Section 3 showing an unrea-
sonable behavior of IT-based measures, the score
depends on r for both MacroI and MicroI. With
our new measures, recall is always 1, but precision
is rn . This is true both for 1-1 and M-1 mappings.
Hence, the new measures show reasonable behav-
ior in this example for all r values.
MicroI was used in (Dasgupta and Ng, 2007)
with a manually compiled mapping. Their map-
ping was not based on a well-defined scheme but
on a heuristic. Moreover, providing a manual
mapping might be impractical when the number of
clusters is large, and can be inaccurate, especially
when the clustering is not of very high quality.
In the following we discuss how to compute the
1-1 and M-1 greedy mappings for each measure.
1-1 Mapping. We compute h by finding the
maximal weighted matching in a bipartite graph.
In this graph one side represents the induced clus-
ters, the other represents the gold classes and
the matchings correspond to 1-1 mappings. The
problem can be efficiently solved by the Kuhn-
Munkres algorithm (Kuhn, 1955; Munkres, 1957).
To be able to use this technique, edge weights
must not depend upon h. In 1-1 mapping,
|h(Bi)| = |Bi|, and therefore F (h) = F and
wi(h) = wi. That is, both quantities are inde-
pendent of h3. For MacroI, the weight on the edge
between the s-th gold class and the j-th induced
cluster is: W (esj) =
?l
i=1 F ? Is?AiIj?Bi . For
MicroI it is: W (esj) =
?l
i=1 wi ? Is?AiIj?Bi .
Is?Ai is 1 if s ? Ai and 0 otherwise.
M-1 Mapping. There are two problems in ap-
plying the bipartite graph technique to finding an
M-1 mapping. First, under such mapping wi(h)
and F (h) do depend on h. The problem may
be solved by selecting some constant weighting
scheme. However, a more serious problem also
arises.
Consider a case in which an item x has a gold
entry {C1} and an induced entry {K1, K2}. Say
the chosen mapping mapped both K1 and K2 to
C1. By summing over the graph?s edges selected
by the mapping, we add weight (F (h) for MacroI
and wi(h) for MicroI) both to the edge between
K1 and C1 and to the edge between K2 and C1.
However, the item?s IMi is only 1. This prohibits
3Consequently, the increase in MacroI and MicroI follow-
ing an increase of 1 in an item?s gold/induced intersection size
(IMi) is independent of h.
81
the use of the bipartite graph method for the M-1
case.
Since we are not aware of any exact method for
solving this problem, we use a hill-climbing al-
gorithm. We start with a random mapping and a
random order on the induced clusters. Then we
iterate over the induced clusters and map each of
them to the gold class which maximizes the mea-
sure given that the rest of the mapping remains
constant. We repeat the process until no improve-
ment to the measure can be obtained by changing
the assignment of a single induced cluster. Since
the score depends on the initial random mapping
and random order, we repeat this process several
times and choose the maximum between the ob-
tained scores.
4.2 Cluster-Based Evaluation
The cluster-based evaluation measures we propose
are a direct generalization of existing monose-
mous mapping based measures to the polysemous
type case.
For a given mapping h : K ? C, we define h? :
Kh ? C. Kh is defined to be a clustering which
is obtained by performing set union between every
two clusters in K that are mapped to the same gold
cluster. The resulting h? is always 1-1. We denote
|Kh| = mh.
Our motivation for using h? in the definition of
the measures instead of h is to stay as close as
possible to accuracy, the most common mapping-
based measure in the monosemous case. M-1
(monosemous) accuracy does not punish for split-
ing classes. For instance, in a case where there is
a gold cluster ci and two induced clusters k1 and
k2 such that ci = k1 ? k2, the M-1 accuracy is the
same as in the case where there is one cluster k1
such that ci = k1. M-1 accuracy, despite its in-
difference to splitting, was shown to reflect better
than 1-1 accuracy the clustering?s applicability for
subsequent applications (at least in some contexts)
(Headden III et al, 2008).
Recall that in item-based evaluation, IMi mea-
sures the intersection between the induced and
gold entries of each item. Therefore, the set union
operation is not needed for that case, since when
an item appears in two induced clusters that are
mapped to the same gold cluster, its IMi is in-
creased only by 1.
A fundamental quantity for cluster-based eval-
uation is the intersection between each induced
cluster and the gold class to which it is mapped.
We denote this value by CMj (CM stands for
?cluster match?):
CMj = |kj ? h?(kj)|
The total intersection (CM ) is accordingly de-
fined to be:
CM = ?mhj=1 CMj =
?mh
j=1 |kj ? h?(kj)|
As with the item-based evaluation (Section 4.1),
using CM or a derived accuracy as a measure is
problematic. A clustering that assigns n induced
classes to each word (n is the number of gold
classes) will get the highest possible score under
every greedy mapping (1-1 or M-1), as will any
clustering in which ?i, Ai ? h(Bi).
As in the item-based evaluation, a possible so-
lution is based on defining recall, precision and F-
score measures, computed either in the micro or in
the macro level. The macro cluster-based measure
turns out to be identical to the macro item-based
measure MacroI4.
The following derivation shows the equivalence
for the 1-1 case. The M-1 case is similar. We note
that h = h? in the 1-1 case and we therefore ex-
change them in the definition of CM . It is enough
to show that CM = IM , since the denominator is
the same in both cases:
CM = Pmj=1 |kj ? h(kj)| =
= Pmj=1
Pl
i=1 Ii?kj Ii?h(kj) =
= Pli=1
Pm
j=1 Ii?kj Ii?h(kj) =
= Pli=1 |Ai ? h(Bi)| = IM
The micro cluster-based measures are defined:
Rj = CMj|h?(kj)| Pj =
CMj
|kj | Fj =
2RjPj
Rj+Pj
The micro cluster measure MicroC is obtained
by taking a weighted average over the Fj?s:
MicroC = ?k?Kh
|k|
N? Fk
Where N? = ?z?Kh |z| is the number of clus-
tered items after performing the set union and
including repetitions. If, in the 1-1 case where
m > n, an induced cluster is not mapped, we de-
fine Fk = 0. A definition of the measure using
a reverse mapping (i.e., from C to K) would have
used a weighted average with weights proportional
to the gold classes? sizes.
4Hence, we have six type level measures: MacroI (which
is equal to MacroC), MicroI, and MicroC, each of which in
two versions, M-1 and 1-1.
82
The definition of h? causes a similar computa-
tional difficulty as in the M-1 item-based mea-
sures. Consequently, we apply a hill climbing
algorithm similar to the one described in Sec-
tion 4.1.
The 1-1 mapping is computed using the same
bipartite graph method described in Section 4.1.
The graph?s vertices correspond to gold and in-
duced clusters and an edge?s weight is the F-score
between the class and cluster corresponding to its
vertices times the cluster?s weight (|k|/N?).
5 Evaluation of POS Induction Models
As a detailed case study for the ideas presented
in this paper, we apply the various measures for
the POS induction task, using seven leading POS
induction algorithms.
5.1 Experimental Setup
POS Induction Algorithms. We experimented
with the following models: ARR10 (Abend et al,
2010), Clark03 (Clark, 2003), GG07 (Goldwa-
ter and Griffiths, 2007), GJ08 (Gao and Johnson,
2008), and GVG09 (Van Gael et al, 2009) (three
models). Additional recent good results for vari-
ous variants of the POS induction problem are de-
scribed in e.g., (Smith and Eisner, 2004; Grac?a et
al., 2009).
Clark03 and ARR10 are monosemous algo-
rithms, allowing a single cluster for each word
type. The other algorithms are polysemous. They
perform sequence labeling where each token is
tagged in its context, and different tokens (in-
stances) of the same type (word form) may receive
different tags.
Data Set. All models were tested on sections
2-21 of the PTB-WSJ, which consists of 39832
sentences, 950028 tokens and 39546 unique types.
Of the tokens, 832629 (87.6%) are not punctuation
marks.
Evaluation Measures. Type level evaluation
used the measures MacroI (which is equal to
MacroC), MicroI and MicroC both with greedy
1-1 and M-1 mappings as described in Section 4.
The type level gold (induced) entry is defined to
be the set of all gold (induced) clusters with which
it appears.
For the token level evaluation, six measures are
used (see Section 3): accuracy with M-1 and 1-1
mappings, NVI, V, H(C|K) and H(K|C), using e
as the logarithm?s base. We use the full WSJ POS
tags set excluding punctuation5.
Punctuation. Punctuation marks occupy a
large volume of the corpus tokens (12.4% in our
experimental corpus), and are easy to cluster.
Clustering punctuation marks thus greatly inflates
token level results. To study the relationship be-
tween type and token level evaluations in a fo-
cused manner, we excluded punctuation from the
evaluation (they are still used during training, so
algorithms that rely on them are not harmed).
Number of Induced Clusters. The number
of gold POS tags in WSJ is 45, of which 11 are
punctuation marks. Therefore, for the ARR10 and
Clark03 models, 34 clusters were induced. For
GJ08 we received the output with 45 clusters. The
iHMM models of GVG09 determine the number
of clusters automatically (resulting in 47, 91 and
192 clusters, see below). For GG07, our com-
puting resources did not enable us to induce 45
clusters and we therefore used 176. Our focus in
this paper is to study the type vs. token distinction
rather than to provide a full scope comparison be-
tween algorithms, for which more clustering sizes
would need to be examined.
Configurations. We ran the ARR10 tagger
with the configuration detailed in (Abend et al,
2010). For Clark03, we ran his neyessenmorph
model7 10 times (using an unknown words thresh-
old of 5) and report the average score for each
measure. The models of GVG09 were run in the
three configurations reported in their paper: one
with a Dirichlet process prior and fixed parame-
ters, another with a Pittman-Yore prior with fixed
parameters, and a third with a Dirichlet process
prior with parameters learnt from the data. All five
models were run in an optimal configuration.
We obtained the code of Goldwater and Grif-
fiths? BHMM model and ran it for 10K iterations
with an annealing technique for parameter estima-
tion. That was the best parameter estimation tech-
nique available to us. This is the first time that this
model is evaluated on such a large experimental
corpus, and it performed well under these condi-
tions.
The output of the model of GJ08 was sent to
us by the authors. The model was run on sec-
5We use all WSJ tokens in the training stage, but omit
punctuation marks during evaluation.
6The 17 most frequent tags cover 94% of the word in-
stances and more than 99% of the word types in the WSJ
gold standard tagging.
7www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
83
tions 2-21 of the WSJ-PTB using significantly
inferior computing resources compared to those
used for producing the results reported in their
paper. While this model cannot be compared to
the aforementioned six models due to the subopti-
mal configuration, we evaluate its output using our
measures to get a broader variety of experimental
results8.
5.2 Results and Discussion
Table 1 presents the scores of the compared mod-
els under all evaluation measures (six token level,
six type level). What is important here to note
are the differences between type and token level
evaluations for the algorithms. We are mainly
interested in two things: (1) seeing how relative
rankings change in the two evaluation types, thus
showing that the two types are not highly corre-
lated and are both useful; and (2) insights gained
by using a type level evaluation in addition to the
usual token level one.
Note that the table should not be used to deduce
which algorithm is the ?best? for the task, even ac-
cording to a single evaluation type. This is be-
cause, as explained above, the algorithms do not
induce the same number of clusters and this affects
their results.
Results indicate that type level evaluation re-
veals aspects of the clustering quality that are not
expressed in the token level. For the Clark03
model the disparity is most apparent. While in
the token level it performs very well (better than
the polysemous algorithms for the 1-1, V and NVI
token level measures), in the type level it is the
second worst in the item-based 1-1 scores and the
worst in the M-1 scores.
Here we have a clear demonstration of the value
of type level evaluation. The Clark03 algorithm
is assessed as excellent using token level evalua-
tion (second only to ARR10 in M-1, 1-1, V and
NVI), and only a type level one shows its rela-
tively poor type performance. Although readers
may think that this is natural due to the algorithm?s
monosemous nature, this is not the case, since the
monosemous ARR10 generally ranked first in the
type level measures (more on this below).
The disparity is also observed for polysemous
algorithms. The GG07 model?s token level scores
are mediocre, while in the type level MicroC 1-1
8We would like to thank all authors for sending us the
data.
measure this model is the best and in the type level
MicroI and MacroI 1-1 measures it is the second
best.
Monosemous vs. polysemous algorithms. The
table shows that the ARR10 model achieves the
best results in most type and token level evalua-
tion measures. The fact that this monosemous al-
gorithm outperforms the polysemous ones, even
in a type level evaluation, may seem strange at
first sight but can be explained as follows. Pol-
ysemous tokens account for almost 60% of the
corpus (565K out of 950K), so we could expect
that a monosemous algorithm should do badly in
a token-level evaluation. However, for most of the
polysemous tokens the polysemy is only weakly
present in the corpus9, so it is hard to detect even
for polysemous algorithms. Regarding types, pol-
ysemous types constitute only 16.6% of the cor-
pus types, so a monosemous algorithm which is
quite good in assigning types to clusters has a good
chance of beating polysemous algorithms in a type
level evaluation.
Hence, monosemous POS induction algorithms
are not at such a great disadvantage relative to pol-
ysemous ones. This observation, which was fully
motivated by our type level case study, might be
used to guide future work on POS induction, and
it thus serves as another demonstration for the util-
ity of type level evaluation.
Hill climbing algorithm. For the type level
measures with greedy M-1 mapping, we used the
hill-climbing algorithm described in Section 4.
Recall that the mapping to which our algorithm
converges depends on its random initialization.
We therefore ran the algorithm with 10 differ-
ent random initializations and report the obtained
maximum for MacroI, MicroI and MicroC in Ta-
ble 1. The different initializations caused very lit-
tle fluctuation: not more than 1% in the 9 (7) best
runs for the item-based (MicroC) measures. We
take this result as an indication that the obtained
maximum is a good approximation of the global
maximum.
We tried to improve the algorithm by selecting
an intelligent initialization heuristic. We used the
M-1 mapping obtained by mapping each induced
cluster to the gold class with which it has the high-
9Only about 27% of the tokens are instances of words that
are polysemous but not weakly polysemous (we call a word
weakly polysemous if more than 95% of its instances (tokens)
are tagged by the same tag).
84
Token Level Evaluation Type Level Evaluation
MacroI MicroI MicroC
M-1 1-1 NVI V H(C|K) H(K|C) M-1 1-1 M-1 1-1 M-1 1-1
ARR10 0.675 0.588 0.809 0.608 1.041 1.22 0.579 0.444 0.596 0.455 0.624 0.403
Clark03 0.65 0.484 0.887 0.586 1.04 1.441 0.396 0.301 0.384 0.288 0.463 0.347
GG07 0.5 0.415 0.989 0.479 1.523 1.241 0.497 0.405 0.461 0.398 0.563 0.445
GVG09(1) 0.51 0.444 1.033 0.477 1.471 1.409 0.513 0.354 0.436 0.352 0.486 0.33
GVG09(2) 0.591 0.484 0.998 0.529 1.221 1.564 0.637 0.369 0.52 0.373 0.548 0.32
GVG09(3) 0.668 0.368 1.132 0.534 0.978 2.18 0.736 0.280 0.558 0.276 0.565 0.199
GJ08* 0.605 0.383 1.09 0.506 1.231 1.818 0.467 0.298 0.446 0.311 0.561 0.291
Table 1: Token level (left columns) and type level (right columns) results for seven POS induction
algorithms (rows) (see text for details). Token and type level performance are weakly correlated and
complement each other as evaluation measures. ARR10, a monosemous algorithm, yields the best results
in most measures. (GJ08* results are different from those reported in the original paper because it was
run with weaker computing resources than those used there.)
est weight edge in the bipartite graph. Recall from
Section 4.1 that this is a reasonable approximation
of the greedy M-1 mapping. Again, we ran it for
the three type level measures for 10 times with a
random update order on the induced clusters. This
had only a minor effect on the final scores.
Number of clusters. Previous work (Reichart
and Rappoport, 2009) demonstrated that in data
sets where a relatively small fraction of the gold
classes covers most of the items, it is reasonable
to choose this number to be the number of induced
clusters. In our experimental data set, this number
(the ?prominent cluster number?) is around 17 (see
Section 5.1). Up to this number, increasing the
number of clusters is likely to have a positive ef-
fect on token level M-1, 1-1, H(C|K), and H(K|C)
scores. Inducing a larger number of clusters, how-
ever, is likely to positively affect M-1 and H(C|K)
but to have a negative effect on 1-1 and H(K|C).
This tendency is reflected in Table 1. For the
GG07 model the number of induced clusters, 17,
approximates the number of prominent clusters
and is lower than the number of induced clus-
ters of the other models. This is reflected by
its low token level M-1 and H(C|K) performance
and its high quality H(K|C) and NVI token level
scores. The GVG (1)-(3) models induced 47, 91
and 192 clusters respectively. This might explain
the high token level M-1 and H(C|K) performance
of GVG(3), as well as its high M-1 type level
performance, compared to its mediocre scores in
other measures.
The item based measures. The table indicates
that there is no substantial difference between the
two item based type level scores with 1-1 map-
ping. The definitions of MacroI and MicroI imply
that if |Ai|+ |h(Bi)| (which equals |Ai|+ |Bi| un-
der a 1-1 mapping) is constant for all word types,
then a clustering will score equally on both 1-1
type measures. Indeed, in our experimental cor-
pus 83.4% of the word types have one POS tag,
12.5% have 2, 3.1% have 3 and only 1% of the
words have more. Therefore, |Ai| is roughly con-
stant. The ARR10 and Clark03 models assign a
word type to a single cluster. For the other models,
the number of clusters per word type is generally
similar to that of the gold standard. Consequently,
|Bi| is roughly constant as well, which explains
the similar behavior of the two measures.
Note that for other clustering tasks |Ai| may not
necessarily be constant, so the MacroI and MicroI
scores are not likely to be as similar under the 1-1
mapping.
6 Summary
We discussed type level evaluation for polysemous
clustering, presented new mapping-based evalu-
ation measures, and applied them to the evalua-
tion of POS induction algorithms, demonstrating
that type level measures provide value beyond the
common token level ones.
We hope that type level evaluation in general
and the proposed measures in particular will be
used in the future for evaluating clustering perfor-
mance in NLP tasks.
References
Omri Abend, Roi Reichart and Ari Rappoport, 2010.
Improved Unsupervised POS Induction through Pro-
totype Discovery. ACL ?10.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
85
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ?07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport. 2008. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08
I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. In-
formation Theoretic Co-clustering. KDD ?03
Micha Elsner, Eugene Charniak, and Mark Johnson,
2009. Structured Generative Models for Unsuper-
vised Named-Entity Clustering. NAACL ?09.
Stella Frank, Sharon Goldwater, and Frank Keller,
2009. Evaluating Models of Syntactic Category
Acquisition without Using a Gold Standard. Proc.
31st Annual Conf. of the Cognitive Science Society,
2576?2581.
E.B Fowlkes and C.L. Mallows, 1983. A Method for
Comparing Two Hierarchical Clusterings. Journal
of American statistical Association,78:553-569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester,
2003. Hierarchical Document Clustering using Fre-
quent Itemsets. SIAM International Conference on
Data Mining ?03.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ?08.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ?07.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. COLING
?08.
L. Hubert and J. Schultz, 1976. Quadratic Assignment
as a General Data Analysis Strategy. British Journal
of Mathematical and Statistical Psychology, 29:190-
241.
L. Hubert and P. Arabie, 1985. Comparing Partitions.
Journal of Classification, 2:193-218.
Maurice Kandall and Jean Dickinson, 1990. Rank
Correlation Methods. Oxford University Press, New
York.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Harold W. Kuhn, 1955. The Hungarian Method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Bjornar Larsen and Chinatsu Aone, 1999. Fast and ef-
fective text mining using linear-time document clus-
tering. KDD ?99.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313-330.
Marina Meila and David Heckerman, 2001. An Ex-
perimental Comparison of Model-based Clustering
Methods. Machine Learning, 42(1/2):9-29.
Marina Meila, 2007. Comparing Clustering ? an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
C.W Milligan, S.C Soon and L.M Sokol, 1983. The
Effect of Cluster Size, Dimensionality and the Num-
ber of Clusters on Recovery of True Cluster Struc-
ture. IEEE transactions on Pattern Analysis and
Machine Intelligence, 5:40-47.
Boris G. Mirkin, 1996. Mathematical Classification
and Clustering. Kluwer Academic Press.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions . Internet Mathematics, 1(2):226-251.
Soto Montalvo, Raquel Martnez, Arantza Casillas, and
Vctor Fresno, 2006. Multilingual Document Clus-
tering: an Heuristic Approach Based on Cognate
Named Entities. ACL ?06.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Cristina Nicolae and Gabriel Nicolae, 2006. BEST-
CUT: A Graph Algorithm for Coreference Resolu-
tion. EMNLP ?06.
Darius M. Pfitzner, Richard E. Leibbrandt and David
M.W Powers, 2008. Characterization and Evalua-
tion of Similarity Measures for Pairs of Clusterings.
Knowledge and Information Systems: An Interna-
tional Journal, DOI 10.1007/s10115-008-0150-6.
William Rand, 1971. Objective Criteria for the Evalu-
ation of Clustering Methods. Journal of the Ameri-
can Statstical Association, 66(336):846-850.
86
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-based External
Cluster Evaluation Measure. EMNLP ?07.
Sabine Schulte im Walde, 2006. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Computational Linguistics, 32(2):159-194.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ?04.
Stijn Van Dongen, 2000. Performance Criteria for
Graph Clustering and Markov Cluster Experiments.
Technical report CWI, Amsterdam
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and
Guang R. Gao, 2002. An Adaptive Meta-clustering
Approach: Combining the Information from Differ-
ent Clustering Results . CSB 00:276
87
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 21?29,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Reconstructing Native Language Typology from Foreign Language Usage
Yevgeni Berzak
CSAIL MIT
berzak@mit.edu
Roi Reichart
Technion IIT
roiri@ie.technion.ac.il
Boris Katz
CSAIL MIT
boris@mit.edu
Abstract
Linguists and psychologists have long
been studying cross-linguistic transfer, the
influence of native language properties on
linguistic performance in a foreign lan-
guage. In this work we provide empirical
evidence for this process in the form of a
strong correlation between language simi-
larities derived from structural features in
English as Second Language (ESL) texts
and equivalent similarities obtained from
the typological features of the native lan-
guages. We leverage this finding to re-
cover native language typological similar-
ity structure directly from ESL text, and
perform prediction of typological features
in an unsupervised fashion with respect to
the target languages. Our method achieves
72.2% accuracy on the typology predic-
tion task, a result that is highly competi-
tive with equivalent methods that rely on
typological resources.
1 Introduction
Cross-linguistic transfer can be broadly described
as the application of linguistic structure of a
speaker?s native language in the context of a
new, foreign language. Transfer effects may be
expressed on various levels of linguistic perfor-
mance, including pronunciation, word order, lex-
ical borrowing and others (Jarvis and Pavlenko,
2007). Such traces are prevalent in non-native
English, and in some cases are even cele-
brated in anecdotal hybrid dialect names such as
?Frenglish? and ?Denglish?.
Although cross-linguistic transfer was exten-
sively studied in Psychology, Second Language
Acquisition (SLA) and Linguistics, the conditions
under which it occurs, its linguistic characteristics
as well as its scope remain largely under debate
(Jarvis and Pavlenko, 2007; Gass and Selinker,
1992; Odlin, 1989).
In NLP, the topic of linguistic transfer was
mainly addressed in relation to the Native Lan-
guage Identification (NLI) task, which requires to
predict the native language of an ESL text?s au-
thor. The overall high performance on this classi-
fication task is considered to be a central piece of
evidence for the existence of cross-linguistic trans-
fer (Jarvis and Crossley, 2012). While the success
on the NLI task confirms the ability to extract na-
tive language signal from second language text, it
offers little insight into the linguistic mechanisms
that play a role in this process.
In this work, we examine the hypothesis that
cross-linguistic structure transfer is governed by
the typological properties of the native language.
We provide empirical evidence for this hypothe-
sis by showing that language similarities derived
from structural patterns of ESL usage are strongly
correlated with similarities obtained directly from
the typological features of the native languages.
This correlation has broad implications on the
ability to perform inference from native language
structure to second language performance and vice
versa. In particular, it paves the way for a novel
and powerful framework for comparing native
languages through second language performance.
This framework overcomes many of the inher-
ent difficulties of direct comparison between lan-
guages, and the lack of sufficient typological doc-
umentation for the vast majority of the world?s lan-
guages.
Further on, we utilize this transfer enabled
framework for the task of reconstructing typolog-
ical features. Automated prediction of language
typology is extremely valuable for both linguistic
studies and NLP applications which rely on such
information (Naseem et al., 2012; T?ackstr?om et
al., 2013). Furthermore, this task provides an ob-
jective external testbed for the quality of our native
21
language similarity estimates derived from ESL
texts.
Treating native language similarities obtained
from ESL as an approximation for typological
similarities, we use them to predict typological
features without relying on typological annotation
for the target languages. Our ESL based method
yields 71.4% ? 72.2% accuracy on the typology re-
construction task, as compared to 69.1% ? 74.2%
achieved by typology based methods which de-
pend on pre-existing typological resources for the
target languages.
To summarize, this paper offers two main con-
tributions. First, we provide an empirical result
that validates the systematic existence of linguistic
transfer, tying the typological characteristics of the
native language with the structural patterns of for-
eign language usage. Secondly, we show that ESL
based similarities can be directly used for predic-
tion of native language typology. As opposed to
previous approaches, our method achieves strong
results without access to any a-priori knowledge
about the target language typology.
The remainder of the paper is structured as fol-
lows. Section 2 surveys the literature and positions
our study in relation to previous research on cross-
linguistic transfer and language typology. Section
3 describes the ESL corpus and the database of
typological features. In section 4, we delineate
our method for deriving native language similar-
ities and hierarchical similarity trees from struc-
tural features in ESL. In section 5 we use typolog-
ical features to construct another set of language
similarity estimates and trees, which serve as a
benchmark for the typological validity of the ESL
based similarities. Section 6 provides a correla-
tion analysis between the ESL based and typology
based similarities. Finally, in section 7 we report
our results on typology reconstruction, a task that
also provides an evaluation framework for the sim-
ilarity structures derived in sections 4 and 5.
2 Related Work
Our work integrates two areas of research, cross-
linguistic transfer and linguistic typology.
2.1 Cross-linguistic Transfer
The study of cross-linguistic transfer has thus far
evolved in two complementary strands, the lin-
guistic comparative approach, and the computa-
tional detection based approach. While the com-
parative approach focuses on case study based
qualitative analysis of native language influence
on second language performance, the detection
based approach revolves mainly around the NLI
task.
Following the work of Koppel et al. (2005), NLI
has been gaining increasing interest in NLP, cul-
minating in a recent shared task with 29 partici-
pating systems (Tetreault et al., 2013). Much of
the NLI efforts thus far have been focused on ex-
ploring various feature sets for optimizing classifi-
cation performance. While many of these features
are linguistically motivated, some of the discrimi-
native power of these approaches stems from cul-
tural and domain artifacts. For example, our pre-
liminary experiments with a typical NLI feature
set, show that the strongest features for predicting
Chinese are strings such as China and in China.
Similar features dominate the weights of other lan-
guages as well. Such content features boost clas-
sification performance, but are hardly relevant for
modeling linguistic phenomena, thus weakening
the argument that NLI classification performance
is indicative of cross-linguistic transfer.
Our work incorporates an NLI component, but
departs from the performance optimization orien-
tation towards leveraging computational analysis
for better understanding of the relations between
native language typology and ESL usage. In par-
ticular, our choice of NLI features is driven by
their relevance to linguistic typology rather than
their contribution to classification performance. In
this sense, our work aims to take a first step to-
wards closing the gap between the detection and
comparative approaches to cross-linguistic trans-
fer.
2.2 Language Typology
The second area of research, language typology,
deals with the documentation and comparative
study of language structures (Song, 2011). Much
of the descriptive work in the field is summa-
rized in the World Atlas of Language Structures
(WALS)
1
(Dryer and Haspelmath, 2013) in the
form of structural features. We use the WALS fea-
tures as our source of typological information.
Several previous studies have used WALS fea-
tures for hierarchical clustering of languages and
typological feature prediction. Most notably, Teh
et al. (2007) and subsequently Daum?e III (2009)
1
http://wals.info/
22
predicted typological features from language trees
constructed with a Bayesian hierarchical cluster-
ing model. In Georgi et al. (2010) additional clus-
tering approaches were compared using the same
features and evaluation method. In addition to the
feature prediction task, these studies also evalu-
ated their clustering results by comparing them to
genetic language clusters.
Our approach differs from this line of work
in several aspects. First, similarly to our WALS
based baselines, the clustering methods presented
in these studies are affected by the sparsity of
available typological data. Furthermore, these
methods rely on existing typological documen-
tation for the target languages. Both issues are
obviated in our English based framework which
does not depend on any typological information
to construct the native language similarity struc-
tures, and does not require any knowledge about
the target languages except from the ESL essays of
a sample of their speakers. Finally, we do not com-
pare our clustering results to genetic groupings,
as to our knowledge, there is no firm theoretical
ground for expecting typologically based cluster-
ing to reproduce language phylogenies. The em-
pirical results in Georgi et al. (2010), which show
that typology based clustering differs substantially
from genetic groupings, support this assumption.
3 Datasets
3.1 Cambridge FCE
We use the Cambridge First Certificate in English
(FCE) dataset (Yannakoudakis et al., 2011) as our
source of ESL data. This corpus is a subset of
the Cambridge Learner Corpus (CLC)
2
. It con-
tains English essays written by upper-intermediate
level learners of English for the FCE examination.
The essay authors represent 16 native lan-
guages. We discarded Dutch and Swedish speak-
ers due to the small number of documents avail-
able for these languages (16 documents in total).
The remaining documents are associated with the
following 14 native languages: Catalan, Chinese,
French, German, Greek, Italian, Japanese, Korean,
Polish, Portuguese, Russian, Spanish, Thai and
Turkish. Overall, our corpus comprises 1228 doc-
uments, corresponding to an average of 87.7 doc-
uments per native language.
2
http://www.cambridge.org/gb/elt/
catalogue/subject/custom/item3646603
3.2 World Atlas of Language Structures
We collect typological information for the FCE
native languages from WALS. Currently, the
database contains information about 2,679 of
the world?s 7,105 documented living languages
(Lewis, 2014). The typological feature list has 188
features, 175 of which are present in our dataset.
The features are associated with 9 linguistic cat-
egories: Phonology, Morphology, Nominal Cate-
gories, Nominal Syntax, Verbal Categories, Word
Order, Simple Clauses, Complex Sentences and
Lexicon. Table 1 presents several examples for
WALS features and their range of values.
One of the challenging characteristics of WALS
is its low coverage, stemming from lack of avail-
able linguistic documentation. It was previously
estimated that about 84% of the language-feature
pairs in WALS are unknown (Daum?e III, 2009;
Georgi et al., 2010). Even well studied languages,
like the ones used in our work, are lacking values
for many features. For example, only 32 of the
WALS features have known values for all the 14
languages of the FCE corpus. Despite the preva-
lence of this issue, it is important to bear in mind
that some features do not apply to all languages by
definition. For instance, feature 81B Languages
with two Dominant Orders of Subject, Object, and
Verb is relevant only to 189 languages (and has
documented values for 67 of them).
We perform basic preprocessing, discarding 5
features that have values for only one language.
Further on, we omit 19 features belonging to the
category Phonology as comparable phonological
features are challenging to extract from the ESL
textual data. After this filtering, we remain with
151 features, 114.1 features with a known value
per language, 10.6 languages with a known value
per feature and 2.5 distinct values per feature.
Following previous work, we binarize all the
WALS features, expressing each feature in terms
of k binary features, where k is the number of
values the original feature can take. Note that
beyond the well known issues with feature bi-
narization, this strategy is not optimal for some
of the features. For example, the feature 111A
Non-periphrastic Causative Constructions whose
possible values are presented in table 1 would
have been better encoded with two binary features
rather than four. The question of optimal encoding
for the WALS feature set requires expert analysis
and will be addressed in future research.
23
ID Type Feature Name Values
26A Morphology Prefixing vs. Suffixing in Little affixation, Strongly suffixing, Weakly
Inflectional Morphology suffixing, Equal prefixing and suffixing,
Weakly prefixing, Strong prefixing.
30A Nominal Number of Genders None, Two, Three, Four, Five or more.
Categories
83A Word Order Order of Object and Verb OV, VO, No dominant order.
111A Simple Clauses Non-periphrastic Causative Neither, Morphological but no compound,
Constructions Compound but no morphological, Both.
Table 1: Examples of WALS features. As illustrated in the table examples, WALS features can take
different types of values and may be challenging to encode.
4 Inferring Language Similarities from
ESL
Our first goal is to derive a notion of similarity be-
tween languages with respect to their native speak-
ers? distinctive structural usage patterns of ESL. A
simple way to obtain such similarities is to train
a probabilistic NLI model on ESL texts, and in-
terpret the uncertainty of this classifier in distin-
guishing between a pair of native languages as a
measure of their similarity.
4.1 NLI Model
The log-linear NLI model is defined as follows:
p(y|x; ?) =
exp(? ? f(x, y))
?
y
?
?Y
exp(? ? f(x, y
?
))
(1)
where y is the native language, x is the observed
English document and ? are the model parame-
ters. The parameters are learned by maximizing
the L2 regularized log-likelihood of the training
data D = {(x
1
, y
1
), ..., (x
n
, y
n
)}.
L(?) =
n
?
i=1
log p(y
i
|x
i
; ?)? ????
2
(2)
The model is trained using gradient ascent with L-
BFGS-B (Byrd et al., 1995). We use 70% of the
FCE data for training and the remaining 30% for
development and testing.
As our objective is to relate native language and
target language structures, we seek to control for
biases related to the content of the essays. As pre-
viously mentioned, such biases may arise from the
essay prompts as well as from various cultural fac-
tors. We therefore define the model using only un-
lexicalized morpho-syntactic features, which cap-
ture structural properties of English usage.
Our feature set, summarized in table 2, contains
features which are strongly related to many of the
structural features in WALS. In particular, we use
features derived from labeled dependency parses.
These features encode properties such as the types
of dependency relations, ordering and distance be-
tween the head and the dependent. Additional
syntactic information is obtained using POS n-
grams. Finally, we consider derivational and in-
flectional morphological affixation. The annota-
tions required for our syntactic features are ob-
tained from the Stanford POS tagger (Toutanova
et al., 2003) and the Stanford parser (de Marneffe
et al., 2006). The morphological features are ex-
tracted heuristically.
4.2 ESL Based Native Language Similarity
Estimates
Given a document x and its author?s native lan-
guage y, the conditional probability p(y
?
|x; ?) can
be viewed as a measure of confusion between lan-
guages y and y
?
, arising from their similarity with
respect to the document features. Under this in-
terpretation, we derive a language similarity ma-
trix S
?
ESL
whose entries are obtained by averaging
these conditional probabilities on the training set
documents with the true label y, which we denote
as D
y
= {(x
i
, y) ? D}.
S
?
ESL
y,y
?
=
?
?
?
1
|D
y
|
?
(x,y)?D
y
p(y
?
|x; ?) if y
?
6= y
1 otherwise
(3)
For each pair of languages y and y
?
, the matrix
S
?
ESL
contains an entry S
?
ESL
y,y
?
which captures
the average probability of mistaking y for y
?
, and
an entry S
?
ESL
y
?
,y
, which represents the opposite
24
Feature Type Examples
Unlexicalized labeled dependencies Relation = prep Head = VBN Dependent = IN
Ordering of head and dependent Ordering = right Head = NNS Dependent = JJ
Distance between head and dependent Distance = 2 Head = VBG Dependent = PRP
POS sequence between head and dependent Relation = det POS-between = JJ
POS n-grams (up to 4-grams) POS bigram = NN VBZ
Inflectional morphology Suffix = ing
Derivational morphology Suffix = ity
Table 2: Examples of syntactic and morphological features of the NLI model. The feature values are set
to the number of occurrences of the feature in the document. The syntactic features are derived from the
output of the Stanford parser. A comprehensive description of the Stanford parser dependency annotation
scheme can be found in the Stanford dependencies manual (de Marneffe and Manning, 2008).
confusion. We average the two confusion scores to
receive the matrix of pairwise language similarity
estimates S
ESL
.
S
ESL
y,y
?
= S
ESL
y
?
,y
=
1
2
(S
?
ESL
y,y
?
+ S
?
ESL
y
?
,y
)
(4)
Note that comparable similarity estimates can
be obtained from the confusion matrix of the clas-
sifier, which records the number of misclassifica-
tions corresponding to each pair of class labels.
The advantage of our probabilistic setup over this
method is its robustness with respect to the actual
classification performance of the model.
4.3 Language Similarity Tree
A particularly informative way of representing
language similarities is in the form of hierarchi-
cal trees. This representation is easier to inspect
than a similarity matrix, and as such, it can be
more instrumental in supporting linguistic inquiry
on language relatedness. Additionally, as we show
in section 7, hierarchical similarity trees can out-
perform raw similarities when used for typology
reconstruction.
We perform hierarchical clustering using the
Ward algorithm (Ward Jr, 1963). Ward is a
bottom-up clustering algorithm. Starting with a
separate cluster for each language, it successively
merges clusters and returns the tree of cluster
merges. The objective of the Ward algorithm is
to minimize the total within-cluster variance. To
this end, at each step it merges the cluster pair
that yields the minimum increase in the overall
within-cluster variance. The initial distance ma-
trix required for the clustering algorithm is de-
fined as 1 ? S
ESL
. We use the Scipy implemen-
tation
3
of Ward, in which the distance between a
newly formed cluster a ? b and another cluster c
is computed with the Lance-Williams distance up-
date formula (Lance and Williams, 1967).
5 WALS Based Language Similarities
In order to determine the extent to which ESL
based language similarities reflect the typological
similarity between the native languages, we com-
pare them to similarities obtained directly from the
typological features in WALS.
The WALS based similarity estimates between
languages y and y
?
are computed by measuring the
cosine similarity between the binarized typologi-
cal feature vectors.
S
WALS
y,y
?
=
v
y
? v
y
?
?v
y
??v
y
?
?
(5)
As mentioned in section 3.2, many of the WALS
features do not have values for all the FCE lan-
guages. To address this issue, we experiment with
two different strategies for choosing the WALS
features to be used for language similarity compu-
tations. The first approach, called shared-all, takes
into account only the 32 features that have known
values in all the 14 languages of our dataset. In
the second approach, called shared-pairwise, the
similarity estimate for a pair of languages is deter-
mined based on the features shared between these
two languages.
As in the ESL setup, we use the two matrices
of similarity estimates to construct WALS based
hierarchical similarity trees. Analogously to the
ESL case, a WALS based tree is generated by the
3
http://docs.scipy.org/.../scipy.
cluster.hierarchy.linkage.html
25
Figure 1: shared-pairwise WALS based versus
ESL based language similarity scores. Each point
represents a language pair, with the vertical axis
corresponding to the ESL based similarity and
the horizontal axis standing for the typological
shared-pairwise WALS based similarity. The
scores correlate strongly with a Pearson?s coeffi-
cient of 0.59 for the shared-pairwise construction
and 0.50 for the shared-all feature-set.
Ward algorithm with the input distance matrix 1?
S
WALS
.
6 Comparison Results
After independently deriving native language sim-
ilarity matrices from ESL texts and from typo-
logical features in WALS, we compare them to
one another. Figure 1 presents a scatter plot
of the language similarities obtained using ESL
data, against the equivalent WALS based similar-
ities. The scores are strongly correlated, with a
Pearson Correlation Coefficient of 0.59 using the
shared-pairwise WALS distances and 0.50 using
the shared-all WALS distances.
This correlation provides appealing evidence
for the hypothesis that distinctive structural pat-
terns of English usage arise via cross-linguistic
transfer, and to a large extent reflect the typologi-
cal similarities between the respective native lan-
guages. The practical consequence of this result is
the ability to use one of these similarity structures
to approximate the other. Here, we use the ESL
based similarities as a proxy for the typological
similarities between languages, allowing us to re-
construct typological information without relying
on a-priori knowledge about the target language
typology.
In figure 2 we present, for illustration purposes,
the hierarchical similarity trees obtained with the
Ward algorithm based on WALS and ESL similar-
ities. The trees bear strong resemblances to one
other. For example, at the top level of the hier-
archy, the Indo-European languages are discerned
from the non Indo-European languages. Further
down, within the Indo-European cluster, the Ro-
mance languages are separated from other Indo-
European subgroups. Further points of similarity
can be observed at the bottom of the hierarchy,
where the pairs Russian and Polish, Japanese and
Korean, and Chinese and Thai merge in both trees.
In the next section we evaluate the quality of
these trees, as well as the similarity matrices used
for constructing them with respect to their ability
to support accurate nearest neighbors based recon-
struction of native language typology.
7 Typology Prediction
Although pairwise language similarities derived
from structural features in ESL texts are highly
correlated with similarities obtained directly from
native language typology, evaluating the absolute
quality of such similarity matrices and trees is
challenging.
We therefore turn to typology prediction based
evaluation, in which we assess the quality of
the induced language similarity estimates by their
ability to support accurate prediction of unseen ty-
pological features. In this evaluation mode we
project unknown WALS features to a target lan-
guage from the languages that are closest to it in
the similarity structure. The underlying assump-
tion of this setup is that better similarity structures
will lead to better accuracies in the feature predic-
tion task.
Typological feature prediction not only pro-
vides an objective measure for the quality of the
similarity structures, but also has an intrinsic value
as a stand-alone task. The ability to infer typolog-
ical structure automatically can be used to create
linguistic databases for low-resource languages,
and is valuable to NLP applications that exploit
such resources, most notably multilingual parsing
(Naseem et al., 2012; T?ackstr?om et al., 2013).
Prediction of typological features for a target
language using the language similarity matrix is
performed by taking a majority vote for the value
of each feature among the K nearest languages of
the target language. In case none of the K nearest
languages have a value for a feature, or given a tie
26
(a) Hierarchical clustering using WALS based shared-
pairwise distances.
(b) Hierarchical clustering using ESL based distances.
Figure 2: Language Similarity Trees. Both trees
are constructed with the Ward agglomerative hi-
erarchical clustering algorithm. Tree (a) uses the
WALS based shared-pairwise language distances.
Tree (b) uses the ESL derived distances.
between several values, we iteratively expand the
group of nearest languages until neither of these
cases applies.
To predict features using a hierarchical cluster
tree, we set the value of each target language fea-
ture to its majority value among the members of
the parent cluster of the target language, excluding
the target language itself. For example, using the
tree in figure 2(a), the feature values for the target
language French will be obtained by taking ma-
jority votes between Portuguese, Italian and Span-
ish. Similarly to the matrix based prediction, miss-
ing values and ties are handled by backing-off to a
larger set of languages, in this case by proceeding
to subsequent levels of the cluster hierarchy. For
the French example in figure 2(a), the first fall-
back option will be the Romance cluster.
Following the evaluation setups in Daum?e III
(2009) and Georgi et al. (2010), we evaluate the
WALS based similarity estimates and trees by con-
structing them using 90% of the WALS features.
We report the average accuracy over 100 random
folds of the data. In the shared-all regime, we pro-
vide predictions not only for the remaining 10%
of features shared by all languages, but also for all
the other features that have values in the target lan-
guage and are not used for the tree construction.
Importantly, as opposed to the WALS based
prediction, our ESL based method does not re-
quire any typological features for inferring lan-
guage similarities and constructing the similarity
tree. In particular, no typological information is
required for the target languages. Typological fea-
tures are needed only for the neighbors of the tar-
get language, from which the features are pro-
jected. This difference is a key advantage of our
approach over the WALS based methods, which
presuppose substantial typological documentation
for all the languages involved.
Table 3 summarizes the feature reconstruction
results. The ESL approach is highly competitive
with the WALS based results, yielding comparable
accuracies for the shared-all prediction, and lag-
ging only 1.7% ? 3.4% behind the shared-pairwise
construction. Also note that for both WALS based
and ESL based predictions, the highest results are
achieved using the hierarchical tree predictions,
confirming the suitability of this representation for
accurately capturing language similarity structure.
Figure 3 presents the performance of the
strongest WALS based typological feature com-
pletion method, WALS shared-pairwise tree, as a
function of the percentage of features used for ob-
taining the language similarity estimates. The fig-
ure also presents the strongest result of the ESL
method, using the ESL tree, which does not re-
quire any such typological training data for ob-
taining the language similarities. As can be seen,
the WALS based approach would require access to
almost 40% of the currently documented WALS
features to match the performance of the ESL
method.
The competitive performance of our ESL
method on the typology prediction task underlines
27
Method NN 3NN Tree
WALS shared-all 71.6 71.4 69.1
WALS shared-pairwise 73.1 74.1 74.2
ESL 71.4 70.7 72.2
Table 3: Typology reconstruction results. Three
types of predictions are compared, nearest neigh-
bor (NN), 3 nearest neighbors (3NN) and near-
est tree neighbors (Tree). WALS shared-all are
WALS based predictions, where only the 32 fea-
tures that have known values in all 14 languages
are used for computing language similarities. In
the WALS shared-pairwise predictions the lan-
guage similarities are computed using the WALS
features shared by each language pair. ESL re-
sults are obtained by projection of WALS features
from the closest languages according to the ESL
language similarities.
its ability to extract strong typologically driven
signal, while being robust to the partial nature of
existing typological annotation which hinders the
performance of the baselines. Given the small
amount of ESL data at hand, these results are
highly encouraging with regard to the prospects
of our approach to support typological inference,
even in the absence of any typological documen-
tation for the target languages.
8 Conclusion and Outlook
We present a novel framework for utilizing cross-
linguistic transfer to infer language similarities
from morpho-syntactic features of ESL text. Trad-
ing laborious expert annotation of typological fea-
tures for a modest amount of ESL texts, we
are able to reproduce language similarities that
strongly correlate with the equivalent typology
based similarities, and perform competitively on
a typology reconstruction task.
Our study leaves multiple questions for future
research. For example, while the current work ex-
amines structure transfer, additional investigation
is required to better understand lexical and phono-
logical transfer effects.
Furthermore, we currently focuse on native lan-
guage typology, and assume English as the foreign
language. This limits our ability to study the con-
straints imposed on cross-linguistic transfer by the
foreign language. An intriguing research direction
would be to explore other foreign languages and
compare the outcomes to our results on English.
Figure 3: Comparison of the typological fea-
ture completion performance obtained using the
WALS tree with shared-pairwise similarities and
the ESL tree based typological feature comple-
tion performance. The dotted line represents the
WALS based prediction accuracy, while the hor-
izontal line is the ESL based accuracy. The
horizontal axis corresponds to the percentage of
WALS features used for constructing the WALS
based language similarity estimates.
Finally, we plan to formulate explicit models
for the relations between specific typological fea-
tures and ESL usage patterns, and extend our ty-
pology induction mechanisms to support NLP ap-
plications in the domain of multilingual process-
ing.
Acknowledgments
We would like to thank Yoong Keok Lee, Jesse
Harris and the anonymous reviewers for valuable
comments on this paper. This material is based
upon work supported by the Center for Brains,
Minds, and Machines (CBMM), funded by NSF
STC award CCF-1231216, and by Google Faculty
Research Award.
References
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A limited memory algorithm for
bound constrained optimization. SIAM Journal on
Scientific Computing, 16(5):1190?1208.
Hal Daum?e III. 2009. Non-parametric Bayesian areal
linguistics. In Proceedings of human language tech-
nologies: The 2009 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 593?601. Association for
Computational Linguistics.
28
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
URL http://nlp. stanford. edu/software/dependencies
manual. pdf.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Susan M Gass and Larry Selinker. 1992. Language
Transfer in Language Learning: Revised edition,
volume 5. John Benjamins Publishing.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, pages 385?393. Association for
Computational Linguistics.
Scott Jarvis and Scott A Crossley. 2012. Approaching
language transfer through text classification: Explo-
rations in the detection-based approach, volume 64.
Multilingual Matters.
Scott Jarvis and Aneta Pavlenko. 2007. Crosslinguis-
tic influence in language and cognition. Routledge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 624?
628. ACM.
Godfrey N Lance and William Thomas Williams.
1967. A general theory of classificatory sorting
strategies ii. clustering systems. The computer jour-
nal, 10(3):271?277.
M. Paul Lewis. 2014. Ethnologue: Languages of the
world. www.ethnologue.com.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629?637. Asso-
ciation for Computational Linguistics.
Terence Odlin. 1989. Language transfer: Cross-
linguistic influence in language learning. Cam-
bridge University Press.
J.J. Song. 2011. The Oxford Handbook of Linguistic
Typology. Oxford Handbooks in Linguistics. OUP
Oxford.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. Proceedings of NAACL-HLT.
Yee Whye Teh, Hal Daum?e III, and Daniel M Roy.
2007. Bayesian agglomerative clustering with co-
alescents. In NIPS.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. NAACL/HLT 2013, page 48.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Joe H Ward Jr. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
statistical association, 58(301):236?244.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In ACL, pages 180?189.
29
