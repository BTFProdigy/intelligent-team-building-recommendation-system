Generation of Referring Expression Using Prefix Tree Structure 
                       Sibabrata Paladhi                                   Sivaji Bandyopadhyay 
      Department of Computer Sc. & Engg.             Department of Computer Sc. & Engg.          
               Jadavpur University, India                              Jadavpur University, India 
             sibabrata_paladhi@yahoo.com                        sivaji_cse_ju@yahoo.com 
 
Abstract 
This paper presents a Prefix Tree (Trie) 
based model for Generation of Referring 
Expression (GRE). The existing algorithms 
in GRE lie in two extremities. Incremental 
algorithm is simple and speedy but less ex-
pressive in nature whereas others are com-
plex and exhaustive but more expressive in 
nature. Our prefix tree based model not 
only incorporates all relevant features of 
GRE (like describing set, generating Boo-
lean and context sensitive description etc.) 
but also try to attain simplicity and speed 
properties of Incremental algorithm. Thus 
this model provides a simple and linguisti-
cally rich approach to GRE. 
1 Introduction 
Generation of referring expression (GRE) is an 
important task in the field of Natural Language 
Generation (NLG) systems (Reiter and Dale, 
1995). The task of any GRE algorithm is to find a 
combination of properties that allow the audience 
to identify an object (target object) from a set of 
objects (domain or environment). The properties 
should satisfy the target object and dissatisfy all 
other objects in the domain. We sometimes call it 
distinguishing description because it helps us to 
distinguish the target from potential distractors, 
called contrast set. When we generate any natural 
language text in a particular domain, it has been 
observed that the text is centered on certain objects 
for that domain. When we give introductory de-
scription of any object, we generally give full 
length description (e.g. ?The large black hairy 
dog?). But the later references to that object tend to 
be shorter and only support referential communica-
tion goal of distinguishing the target from other 
objects. For example the expression ?The black 
dog? suffices if the other dogs in the environment 
are all non black. Grice, an eminent philosopher of 
language, has stressed on brevity of referential 
communication to avoid conversational implica-
ture. Dale (1992) developed Full Brevity algorithm 
based on this observation. It always generates 
shortest possible referring description to identify 
an object. But Reiter and Dale (1995) later proved 
that Full Brevity requirement is an NP-Hard task, 
thus computationally intractable and offered an 
alternative polynomial time Incremental Algo-
rithm. This algorithm adds properties in a prede-
termined order, based on the observation that hu-
man speakers and audiences prefer certain kinds of 
properties when describing an object in a domain 
(Krahmer et al 2003). The Incremental Algorithm 
is accepted as state of the art algorithm in NLG 
domain. Later many refinements (like Boolean de-
scription and set representation (Deemter 2002), 
context sensitivity (Krahmer et al2002) etc) have 
been incorporated into this algorithm. Several ap-
proaches have also been made to propose an alter-
native algorithmic framework to this problem like 
graph-based (Krahmer et al 2003), conceptual 
graph based (Croitoru and Deemter 2007) etc that 
also handle the above refinements. In this paper we 
propose a new Prefix Tree (Trie) based framework 
for modeling GRE problems. Trie is an ordered 
tree data structure which allows the organization of 
prefixes in such a way that the branching at each 
level is guided by the parts of prefixes. There are 
several advantages of this approach: 1) Trie data 
structure has already been extensively used in 
many domains where search is the key operation. 
2) The structure is scalable and various optimized 
algorithms are there for time, space optimizations.  
  In this paper it is shown how scenes can be 
represented using a Trie (section 2) and how de-
scription generation can be formalized as a search 
problem (section 3). In section 4 the algorithm is 
explained using an example scene. In section 5, the 
basic algorithm is extended to take care of different 
scenarios. The algorithm is analyzed for time com-
697
plexity in section 6 and conclusion is drawn in sec-
tion 7. 
2 Modeling GRE Using Trie Structure 
In this section, it is shown how a scene can be rep-
resented using a trie data structure. The scheme is 
based on Incremental algorithm (Reiter and Dale 
1995) and incorporates the attractive properties 
(e.g. speed, simplicity etc) of that algorithm. Later 
it is extended to take care of different refinements 
(like relational, boolean description etc) that could 
not be handled by Incremental algorithm. Reiter 
and Dale (1995) pointed out the notion of 
?PreferredAttributes? (e.g. Type, Size, Color etc) 
which is a sequence of attributes of an object that 
human speakers generally use to identify that ob-
ject from the contrast set. We assume that the ini-
tial description of an entity is following this se-
quence (e.g. ?The large black dog?) then the later 
references will be some subset of initial description 
(like ?The dog? or ?The large dog?) which is de-
fined as the prefix of the initial description. So, we 
have to search for a prefix of the initial full length 
description so that it is adequate to distinguish the 
target object. Following the Incremental version 
we will add properties one by one from the 
?PreferredAttributes? list. In our model, the root 
consists of all entities in the domain and has empty 
description. Then at each level, branching is made 
based on different values of corresponding pre-
ferred attribute. The outgoing edge is labeled with 
that value. For example, at the first level, branch-
ing is made based on different values of ?Type? 
attribute like ?Dog?, ?Cat?, ?Poodle? etc. A node in 
Trie will contain only those objects which have the 
property(s) expressed by the edges, constituting the 
path from root to that node. After construction of 
the Trie structure for a given domain in this way, 
referring expression generation problem for an ob-
ject r is reduced to search the tree for a node which 
consists of r and no other object. Description for r 
can be found from the search path itself as we have 
said earlier. Now we will introduce some notations 
that we will use to describe the actual algorithm. 
Let D be the Domain, r be the target object and P 
be the ?PreferredAttributes? List.  Ni   = {d | 
d?D and d is stored at node Ni} where Ni is an i-th 
level node. Obviously  No   = D since No is root 
node. E(Ni, Nki+1) is an edge between parent node 
Ni and Nki+1, k-th child of that node (considering an 
enumeration among children nodes). Since every 
edges in Trie are labeled, thus {E} ? {N} x L x 
{N}, where {E} and {N} are set of all edges and 
nodes respectively in the tree and L is the set of 
attribute values. Let Val(E(Ni, Nki+1)) denotes the 
label or value of the edge and  Val(E(Ni, Nki+1))    
= {d | d?D and d is satisfied by the edge value} 
i.e. the set contains those objects who have this 
property. We define  Nki+1  =  Ni   
?  Val(E(Ni, Nki+1)) where Ni and Nki+1 are par-
ent and child node respectively. Similarly  Nki   = 
 Ni-1   ?   Val(E(Ni-1, Nki))  . Ultimately, we 
can say that ? i  Ni   =  No ?  Val(E(No,N1)) 
 ? ?? ?  Val(E(Ni-1,Ni))  . Since our con-
struction is basically a tree, each node is reachable 
from root and there exists a unique path from root 
to that node. So, for each node in the tree we will 
get some description. We will formulate referring 
expression construction as search in the con-
structed tree for the node min(k){Nk} such that  Nk   
= {r}. If Nk is leaf node then description of r will 
be same with the full description but if it is an in-
termediate node then description is some proper 
prefix of initial description. But the point is that, in 
both cases the later reference is prefix of initial one 
(as both ?ab? and ?abc? are prefixes of ?abc?).  
3 Basic Algorithm  
Based on above discussions, algorithms are devel-
oped for construction of Trie from the domain and 
generation of reference description for any object 
in that domain. The Trie construction algorithm 
ConstructTrie(D,P,T) is shown in figure 1, Refer-
ring expression generation algorithm MakeRe-
fExpr(r,p,T,L) is shown in figure 2, where T is a 
node pointer and p is pointer to parent of that node. 
Our algorithm MakeRefExpr returns set of attrib-
ute-values L to identify r in the domain. As dis-
cussed earlier, it is basically a node searching algo-
rithm. In course of searching, if it is found that an 
intermediate node N doesn?t have r i.e. r?   N   
then our search will not move forward through the 
subtree rooted at N. Our search will proceed 
through next level iff r?  N  . For a node Nk, if 
we get  Nk   = {r} then we have succeeded and 
our algorithm will return L, set of descriptions for 
that node. If there is no distinguishing description 
exists for r, then ? (null) will be returned.  We 
698
would like to point out that our algorithm will find 
out only one description that exists at the minimum 
level of the tree. Moreover, a description is added 
to L only if it is distinguishing i.e. the connecting 
edge must remove some contrasting object(s). 
Thus, the child node should contain less number of 
objects than that of parent node. In this case, cardi-
nality of parent Ni (Card(Ni)) will be greater than 
that of child (Card(Ni+1)). This condition is in-
cluded in our algorithm and if (Card (P?N)) > 
Card (T?N) holds then only the value is added 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
P->N and T->N respectively represents parent and 
child node. After finding a distinguishing descrip-
tion for r, search will neither move further down 
the tree nor explore the remaining branches of the 
current node. Search will explore the next branch 
only if the search in current branch returned NULL 
description i.e. when L? = ?  in the algorithm. If 
we reach a leaf node and that contains r along with 
other objects then it is not possible to distinguish 
r?. In that case, the algorithm returns NULL indi-
cating that no description exists at all. It has been 
later shown that some distinguishing description 
may still exist and the algorithm will be modified 
to find that. It should be mentioned that once the 
prefix tree is constructed offline, it can be used 
repetitively to find description for any object in the 
domain throughout the text generation phase. Our 
MakeRefExpr() algorithm is very simple and it 
doesn?t employ any set theoretic operation, which 
is a non trivial task, to find current contrast set at 
every steps of algorithm. In existing algorithms, 
computing referential description for every object 
require computing similar things (like finding cur-
rent contrast set, ruled out objects) again and again. 
And it has to be repeated every time the object is 
referred. It is not possible to generate description 
once, store it and use it later because of the fact 
that domain may also change in course of time 
(Krahmer, 2002). That?s why every time we want 
to refer to ?r?, such rigorous set operations need to 
be computed. But in our prefix tree structure, once 
the tree is constructed, it is very easy to find de-
scription for that object using simple tree search 
function. It is also very easy to add/delete objects 
to/from domain. We have to follow just the initial 
properties of that object to find the proper branch-
ing at each level, followed by addition /deletion of 
that object to /from relevant nodes, which is essen-
tially a search operation. The disadvantage of our 
algorithm is that space complexity is high but it 
can be tackled using bit Vector representation of 
individual nodes of the prefix tree. Besides, several 
methods are there for compressing Trie structure. 
But these optimization techniques are beyond the 
scope of our current discussion. 
4 Formalizing A Scene using Prefix Tree  
Consider an example scene in figure 3, from 
[Krahmer 2002]. In this scene, there is a finite do-
main of entities D. Let D = {d1, d2, d3, d4}, P = 
{Type, Size, Color} and values are Type = {dog, 
cat}; Size = {small, large}; Color = {black, white}. 
A scene is usually represented as a database (or 
ConstructTrie(D, P, T) { 
  If (D = ?  ?  P = ? )  
  Then Stop 
  Else 
     Create a node N at T 
     Set  N   = D 
     Extract front attribute Ai from list P 
     P?  =   P ? { Ai } 
     For each value Vj  of  attribute  Ai  do 
          Create Edge Ej with label Vj as T?Nextj 
      
      Dj?  = D ?    Val(Ej)     
          ConstructTrie(Dj?  , P?, T?Nextj) 
        
End For 
   End If 
} 
 
 
  
Figure 2. Expression Generation Algorithm 
Figure 1. Prefix Tree Generation Algorithm 
MakeRefExpr(r, P, T, L) { 
    If ( r ?  T?N  ) 
            Then  L ? ?  
             Return L 
     Else If ({r} =  T?N  ) 
            L = L ? Val(P?Ej )     
            Return L 
    Else If (isLeaf (T) ? {r} ?  N  ) 
             Then L ? ?  
             Return L 
    Else { 
         If (Card(P?N) > Card (T?N))  
             Then L = L ? Val(P?Ej ) 
         P = T 
         For each outgoing edge T? Nextj (Ej)  do 
            L? = MakeRefExpr(r, P,T? Childj, L) 
                If (L? ?? ) 
                Then Return L? 
        } } 
                                                                                
699
knowledge base) listing the properties of each ele-
ment in D. Thus: 
d1 : ? Type : dog ? , ? Size : small ? , ? Color: white ? 
d2 : ? Type : dog ? , ? Size : large ? , ? Color: white ? 
d3 : ? Type : dog ? , ? Size : large ? , ? Color: black ? 
d4:  ? Type : cat ? ,  ? Size: small ? ,  ? Color: white ? 
Now it will be shown how our MakeRefExpr() 
algorithm will find a description for a target object 
r. Let r = {d1}. In the first phase, starting from 
root, edge labeled D is traversed. Since d1 exists in 
the node and D discards some objects (d4), D is 
distinguishing description and it is added to L. In 
the next phase the node connected by the edge la-
beled L does not contain d1 so search will not pro-
ceed further. Rather the node connected by the 
edge labeled S contains d1. Since, d1 is the only 
object, then we are done and the referring expres-
sion is ?The small dog?. But for d2, we have to 
search upto the leaf node which generates the de-
scription ?The large white dog?. 
 
 
  
            Figure 3.  Scene Representation  
5 Extension of Basic Algorithm  
5.1 Specifying Overlapping Values  
Deemter (2002) has shown incompleteness of In-
cremental algorithm in case of overlapping values. 
Due to vagueness of properties, sometimes it is 
hard to classify an object in a particular class. Con-
sider the example scene D = {a,b,c,d} Color: 
{Red(a,b); Orange(a,c,d)} Size: {Large(a,b); 
Small(c,d)}. In this case a can not be properly clas-
sified by Color type. Incremental algorithm always 
select Red(a,b) at first phase, since it rules out 
maximum distractors and returns failure because it 
can?t distinguish a from b at second phase. Deem-
ter(2002) suggested inclusion of all overlapping 
values that are true of target while also removing 
some distractors. So, referring expression for a is 
?The red orange desk?. But it fails to obey Gricean 
maxims of conversational implicature. We con-
sider the failure as ?Early Decision? problem and 
defer the decision making in our model. We keep 
in our mind the fact that human beings seldom take 
instantaneous decision. Rather they consider all 
opportunities in parallel and take decision in the 
favor of the best one at later point of time. Since, 
our algorithm searches in parallel through all 
promising branches until some description is 
found; it mimics the capabilities of human mind to 
consider in parallel. Our algorithm will generate 
?The large orange desk? which will help audiences 
to better identify the desk. The execution sequence 
is shown in figure 4.  
 
       
 
         Figure 4.  Dealing with overlapping values 
 
5.2 Describing Set of Objects 
Generation of referring description for a set of ob-
jects is very important in NLG. Deemter?s (2002) 
suggestion can be easily incorporated into our 
framework. We will represent target r as set of ob-
jects. Now our algorithm will try to find a node in 
the tree which only consists of all objects in the set 
r. In this way, we can find a distinguishing de-
scription for any set, for which description exists. 
In figure 3, the description for the set {d2,d3} is 
?The large dogs?. Thus, our basic algorithm is able 
to describe set of objects. In case of set like {d2, d3, 
d4} where there is no separate node consisting all 
the object, we need to partition the set and find 
description for individual set. In our case the pos-
sible partitions are {d2, d3} and {d4} for which 
separate nodes exist.  
700
5.3 Boolean Descriptions     
Deemter (2002) shown that Incremental algorithm 
is only intersectively complete. But he argues that 
other Boolean combination of properties can be 
used to generate description for an object. Consider 
the example from (Deemter, 2002).  Let D = {a, b, 
c, d, e} Type: {Dog(a,b,c,d,e); Poodle(a,b)} Color: 
{Black(a,b,c); White(d,e)} and r = {c}. In this sce-
nario Incremental algorithm is not able to indi-
viduate any of the animals. However a description 
for c exists, ?The black dog that is not a poodle?. 
Since {c} = [[Black]] ? [[ ? Poodle]]. Deemter 
(2002) has modified the Incremental algorithm by 
adding negative values for each attribute. Now we 
will show that our basic algorithm can be modified 
to take care of this situation. In our basic algorithm 
ConstructTrie(), we add branches at each level for 
negative values also. In this case our simple rou-
tine MakeRefExpr() is able to find boolean de-
scription while remaining as close as to Incre-
mental algorithm. In figure 5, we show part of the 
trie structure, which is generated for the above 
scene. The dashed arrows show the alternative 
search paths for node containing {c}. 
   
 
Figure 5.  Trie structure (Partial) incorporating  
negation of  properties  
 
For referring objects using disjunction of proper-
ties we have do same thing as negations. We have 
to extend our prefix tree structure by adding extra 
edges at different levels for making implicit infor-
mation explicit as described in [Krahmer 2002]. 
5.4 Incorporating Context Sensitivity     
Krahmer and Theune [2002] have added the notion 
of context sensitivity into GRE. Earlier algorithms 
assumed that all objects in environment are equally 
salient. Krahmer and Theune refined the idea by 
assigning some degree of salience to each object. 
They proposed that during referring any object, the 
object needs to be distinguished only from those 
objects which are more salient (having higher sali-
ence weight). An object that has been mentioned 
recently, is linguistically more salient than other 
objects and can be described using fewer proper-
ties (?The dog? instead of ?The large black hairy 
dog?). They introduced the concept of centering 
theory, hierarchical focus constraints in the field of 
NLG and devised a constant function mapping sw: 
D ?? , where sw is salience weight function, D is 
domain and ?  is set of natural numbers. We can 
incorporate this idea into our model easily. In each 
node of the prefix tree we keep a field ?salience 
weight? (sw) for each of the object stored in that 
node in the form (di, swi). During describing an 
object if we find a node that is containing r where 
it is the most salient then we need not traverse 
higher depth of the tree. So, we have to modify 
MakeRefExpr() algorithm by adding more condi-
tions. If the current node is N and both 1) r?  N   
and 2) ? d?  N   (d ? r ? sw(d) < sw(r)) hold 
then r is the most salient and the edges constituting 
the path from root to N represents distinguishing 
description for r. In figure 6, a is most salient dog 
and referred to as ?The dog? whereas b is referred 
to as ?The small dog?. 
 
 
 
Figure 6:  Trie structure (Partial) representing Con-
text Sensitivity 
5.5 Relational Descriptions 
Relational descriptions are used to single out an 
object with reference to other one. For example 
?The cup on the table? is used to distinguish a cup 
from other cups which are not on the table. Dale 
and Haddock (1991) first offer the idea of rela-
701
tional description and extend Full Brevity algo-
rithm to incorporate this idea. Later Krahmer et al 
(2003) Graph based framework for generating rela-
tional description. We follow Krahmer (2002) and 
denote relations as Spatial: {In(a,b); Left_of(c,d)} 
etc. Then we treat ?Spatial? as another attribute and 
consider ?In?, ?Left_of? as different values for that 
attribute. In this way, our basic algorithm itself is 
capable of handling relational descriptions. The 
only modification that we add that when a relation 
R is included, the MakeRefExpr() should be 
called again for the relatum. Thus, if Val(E(Ni, 
Nki+1)) expresses a relation of r with r? then we 
have to call MakeRefExpr (r?,p,T,L) again to find 
description for  r?. 
5.6 Modeling Full Brevity 
In this section, we will show that our prefix tree 
structure can be so modified that it can generate 
shortest possible description which is requirement 
of Full Brevity (Dale, 1992). Consider a scene 
where a domain is identified by set of n attributes 
{A1, A2?An}. We can generate n! number of dif-
ferent permutations of Ai?s ? i? [1,n]. We con-
sider each permutation as different PreferredAt-
tributes list Pk and generate all possible prefix 
trees Tk for each Pk ? k? [1,n!] for same domain 
D. Now, we connect roots of all trees with a com-
mon dummy root node with edges having empty 
description (?). Now, if we search the branches of 
new combined tree in parallel, it?s obvious that we 
can always find the target node at lowest possible 
level. Thus we can generate shortest length de-
scription using our algorithm. 
6 Complexity of The Algorithm  
Let the domain entities are identified by a number 
of attributes and each attribute has on the average 
k number of different values. So, our Con-
structTrie() algorithm takes ?(ka) time. Now we 
will consider different cases for analyzing the time 
complexity of our MakeRefExpr() algorithm.          
 1) In case of non overlapping properties, our 
search tree will be pruned at each level by a factor 
of k. Thus the time complexity will be ?(logk(ka)) 
= ?(a) which is linear. 
2) In case of overlapping properties, we have to 
search whole tree in worst case (although in aver-
age cases also there will be large pruning, as found 
from test cases) which will take ?(ka) time.                    
3) In case of achieving full brevity requirement, 
both time and space complexity will be exponen-
tial as in the original algorithm by Dale (1992).  
7 Conclusions 
In this paper, we present a new Prefix tree (Trie) 
based approach for modeling GRE problems. We 
construct the trie in such a way that a node at a par-
ticular level consists of only those objects which 
are satisfied by values of the edges, constituting 
the path from root to that node. We formulate de-
scription generation as a search problem. So, when 
we reach the target node, the attribute values corre-
sponding to the edges in the path automatically 
form the distinguishing description. Different sce-
narios of GRE problems like representation of set, 
boolean descriptions etc. is taken care of in this 
paper. We have shown that in simple non overlap-
ping scenarios, our algorithm will find distinguish-
ing description in linear time. 
8 References 
E. Krahmer and M. Theune. 2002. Efficient Context 
Sensitive Generation of Referring Expressions. CSLI 
Publ, Stanford : 223 ? 264 
E. Krahmer, S. van Erk and A. Verlag. 2003. Graph 
based Generation of Referring Expressions Computa-
tional Linguistics, 29(1): 53-72 
H. Horacek. 2004. On Referring to Set of Objects Natu-
rally.  Proceedings of Third INLG, Brokenhurst, U.K: 
70-79 
M. Croitoru  and van Deemter. 2007. A conceptual 
Graph Approach to the Generation of Referring Ex-
pressions. Proceedings of IJCAI 2007 : 2456-2461  
R. Dale and N. Haddock. 1991.  Generating Referring 
Expressions containing Relations. Proceedings of 
Fifth ACL- EACL conference, 161-166 
R. Dale. 1992. Generating Referring Expressions: 
Building Descriptions in a Domain of Objects and 
Processes. MIT Press 
R. Dale  and E. Reiter. 1995. Computational Interpreta-
tions of the Gricean Maxims in the generation of Re-
ferring Expressions. Cognitive Science (18): 233 ? 
263 
van Deemter. 2002. Generating Referring Expressions: 
Boolean Extensions of Incremental Algorithm. Com-
putational Linguistics 28(1): 37-52 
702
A Document Graph Based Query Focused Multi-Document Summarizer 
                  Sibabrata Paladhi                                       Sivaji Bandyopadhyay 
      Department of Computer Sc. & Engg.             Department of Computer Sc. & Engg.             
               Jadavpur University, India                              Jadavpur University, India                                  
            sibabrata_paladhi@yahoo.com                        sivaji_cse_ju@yahoo.com 
 
 
Abstract 
This paper explores the research issue and 
methodology of a query focused multi-
document summarizer. Considering its pos-
sible application area is Web, the computa-
tion is clearly divided into offline and 
online tasks. At initial preprocessing stage 
an offline document graph is constructed, 
where the nodes are basically paragraphs of 
the documents and edge scores are defined 
as the correlation measure between the 
nodes. At query time, given a set of key-
words, each node is assigned a query de-
pendent score, the initial graph is expanded 
and keyword search is performed over the 
graph to find a spanning tree identifying 
relevant nodes satisfying the keywords. 
Paragraph ordering of the output summary 
is taken care of so that the output looks co-
herent. Although all the examples, shown 
in this paper are based on English language, 
we show that our system is useful in gener-
ating query dependent summarization for 
non- English languages also. We also pre-
sent the evaluation of the system. 
 
1 Introduction 
With the proliferation of information in the Inter-
net, it is becoming very difficult for users to iden-
tify the exact information. So many sites are pro-
viding same piece of information and a typical 
query based search in Google results in thousands 
of links if not million. Web Search engines gener-
ally produce query dependent snippets for each 
result which help users to explore further. An 
automated query focused multi-document summar-
izer, which will generate a query based short  
 
 
summary of web pages will be very useful to get a 
glimpse over the complete story. Automated multi-
document summarization has drawn much atten-
tion in recent years. Most multi-document sum-
marizers are query independent, which produce 
majority of information content from multiple 
documents using much less lengthy text. Each of 
the systems fall into two different categories: either 
they are sentence extraction based where they just 
extract relevant sentences and concatenate them to 
produce summary or they fuse information from 
multiple sources to produce a coherent summary.  
In this paper, we propose a query focused multi-
document summarizer, based on paragraph extrac-
tion scheme. Unlike traditional extraction based 
summarizers which do not take into consideration 
the inherent structure of the document, our system 
will add structure to documents in the form of 
graph. During initial preprocessing, text fragments 
are identified from the documents which constitute 
the nodes of the graph. Edges are defined as the 
correlation measure between nodes of the graph.              
We define our text fragments as paragraph rather 
than sentence with the view that generally a para-
graph contains more correlated information 
whereas sentence level extraction might lead to 
loss of some coherent information.  
 Since the system produces multi-document 
summary based on user?s query, the response time 
of the system should be minimal for practical pur-
pose. With this goal, our system takes following 
steps: First, during preprocessing stage (offline) it 
performs some query independent tasks like identi-
fying seed summary nodes and constructing graph 
over them. Then at query time (online), given a set 
of keywords, it expands the initial graph and per-
forms keyword search over the graph to find a 
spanning tree identifying relevant nodes (para-
graphs) satisfying the keywords. The performance 
of the system depends much on the identification 
of the initial query independent nodes (seed nodes). 
Although, we have presented all the examples in 
the current discussion for English language only,    
we argue that our system can be adapted to work in 
multilingual environment (i.e. Hindi, Bengali, 
Japanese etc.) with some minor changes in imple-
mentation of the system like incorporating lan-
guage dependent stop word list, stemmer, WodrNet 
like lexicon etc. 
 In section 2, related works in this field is pre-
sented. In section 3 the overall approach is de-
scribed. In section 4 query independent preprocess-
ing steps are explained. In section 5 query depend-
ent summary generation and paragraph ordering 
scheme is presented. Section 6 presents the evalua-
tion scheme of the system. In section 7 we discuss 
how our system can be modified to work in multi-
lingual scenario. In section 8 we have drawn con-
clusion and discussed about future work in this 
field. 
2 Related Work 
A lot of research work has been done in the do-
main of multi-document summarization (both 
query dependent/independent). MEAD (Radev et 
al., 2004) is centroid based multi-document sum-
marizer which generates summaries using cluster 
centroids produced by topic detection and tracking 
system. NeATS (Lin and Hovy, 2002) selects im-
portant content using sentence position, term fre-
quency, topic signature and term clustering. XDoX 
(Hardy et al, 2002) identifies the most salient 
themes within the document set by passage cluster-
ing and then composes an extraction summary, 
which reflects these main themes.  
Graph based methods have been proposed for 
generating query independent summaries. Web-
summ (Mani and Bloedorn, 2000) uses a graph-
connectivity model to identify salient information. 
Zhang et al(2004) proposed the methodology of 
correlated summarization for multiple news arti-
cles. In the domain of single document summariza-
tion a system for query-specific document summa-
rization has been proposed (Varadarajan and Hris-
tidis, 2006) based on the concept of document 
graph. 
In this paper, the graph based approach has been 
extended to formulate a framework for generating 
query dependent summary from related  multiple 
document set describing same event.  
3 Graph Based Modeling 
The proposed graph based multi-document sum-
marization method consists of following steps: (1) 
The document set D = {d1,d2, ?  dn} is processed 
to extract text fragments, which are paragraphs in 
our case as it has been discussed earlier. Here, we 
assume that the entire document in a particular set 
are related i.e. they describe the same event. Some 
document clustering techniques may be adopted to 
find related documents from a large collection. 
Document clustering is out of the scope of our cur-
rent discussion and is itself a research interest. Let 
for a document di, the paragraphs are 
{pi1,pi2,?pim}. But the system can be easily modi-
fied to work with sentence level extraction.  Each 
text fragment becomes a node of the graph. (2) 
Next, edges are created between nodes across the 
document where edge score represents the degree 
of correlation between inter documents nodes. (3) 
Seed nodes are extracted which identify the rele-
vant paragraphs within D and a search graph is 
built offline to reflect the semantic relationship 
between the nodes. (4) At query time, each node is 
assigned a query dependent score and the search 
graph is expanded. (5) A query dependent multi-
document summary is generated from the search 
graph which is nothing but constructing a total 
minimal spanning tree T (Varadarajan and Hristi-
dis, 2006). For a set of keywords Q = {q1,q2, .. qn} , 
T is total if ?q?Q, T consists of at least one node 
satisfying q and T is  minimal if no node can be 
removed from T while getting the total T. 
4 Building Query Independent Compo-
nents  
Mainly there are two criteria for the performance 
evaluation of such systems: First it?s accuracy i.e. 
the quality of output with respect to specific que-
ries and next of course the turn around time i.e., 
how fast it can produce the result. Both are very 
important aspects of such system, and we will 
show how these aspects are taken care of in our 
system.  Runtime of such system greatly depends 
on how well the query independent graph is con-
structed. At one extreme, offline graph can be built 
connecting all the nodes from each of the docu-
ments, constituting a total document graph. But 
keyword search over such large graph is time con-
suming and practically not plausible. On the other 
hand, it is possible to select query specific nodes at 
runtime and to create a graph over those nodes. But 
if the number of such nodes is high, then calculat-
ing similarity scores between all the nodes will                                                                          
take large computing time, thus resulting in slower  
performance.  
We will take an intermediate approach to attack 
the problem. It can be safely assumed that signifi-
cant information for a group of keywords can be 
found in ?relevant/topic paragraphs? of the docu-
ments. So, if relevant/topic nodes can be selected 
from document set D during offline processing, 
then the significant part of the search graph can be 
constructed offline which greatly reduce the online 
processing time. For example, if a user wants to 
find the information about the IBM Hindi speech 
recognition system, then the keywords are likely to 
be {IBM, speech recognition, accuracy}. For a set 
of news articles about this system, the topic para-
graphs, identified offline, naturally satisfy first two 
keywords and theoretically, they are the most in-
formative paragraphs for those keywords. The last 
term ?accuracy? (relevant for accuracy of the sys-
tem) may not be satisfied by seed nodes. So, at run 
time, the graph needs to be expanded purposefully 
by including nodes so that the paragraphs, relevant 
to ?accuracy of the system? are included. 
4.1 Identification of Seed/ Topic Nodes 
At the preprocessing stage, text is tokenized, stop 
words are eliminated, and words are stemmed 
(Porter, 1980). The text in each document is split 
into paragraphs and each paragraph is represented 
with a vector of constituent words. If we consider 
pair of related document, then the inter document 
graph can be represented as a set of nodes in the 
form of bipartite graph. The edges connect two 
nodes corresponding to paragraphs from different 
documents. The similarity between two nodes is 
expressed as the edge weight of the bipartite graph. 
Two nodes are related if they share common words 
(except stop words) and the degree of relationship 
can be measured by adapting some traditional IR 
formula (Varadarajan and Hristidis, 2006). 
 
( ( ( ( ) , ) ( ( ) , ) ) . ( ) )( ) ( ( ) ) ( ( ) )
t f t u w t f t v w id f w
S c o r e e
s iz e t u s i z e t v
+
=
+
?  
Where ( , )tf d w  is number of occurrence of w in 
d, ( )id f w is the inverse of the number of docu-
ments containing w, and ( )size d is the size of the 
documents in words. The score can be accurately 
set if stemmer and lexicon are used to match the 
equivalent words. With the idea of page ranking 
algorithms, it can be easily observed that a para-
graph in a document is relevant if it is highly re-
lated to many relevant paragraphs of other docu-
ment. If some less stringent rules are adopted, then 
a node from a document is selected as seed/topic 
node if it has high edge scores with nodes of other 
document. Actually for a particular node, total 
edge score is defined as the sum of scores of all out 
going edges from that node. The nodes with higher 
total edge scores than some predefined threshold 
are included as seed nodes. In Figure 1. correlation 
between two news articles is shown as a bipartite 
graph.   
But the challenge for multi-document summari-
zation is that the information stored in different 
documents inevitably overlap with each other.  So, 
before inclusion of a particular node (paragraph), it 
has to be checked whether it is being repeated or 
not. Two paragraphs are said to be similar if they 
share for example, 70% words (non stop words) in 
common.   
 
 
Figure 1.  A bipartite graph representing correlation 
among two news articles on same event.  
 
4.2 Offline Construction of Search Graph 
After detection of seed/topic nodes a search graph 
is constructed. For nodes, pertaining to different 
documents, edge scores are already calculated, but 
for intra document nodes, edge scores are calcu-
lated in the similar fashion as said earlier. Since, 
highly dense graph leads to higher 
search/execution time, only the edges having edge 
scores well above the threshold value might be 
considered. The construction of query independent 
part of the search graph completes the offline proc-
essing phase of the system. 
5 Building Query Dependent Compo-
nents  
At query time, first, the nodes of the already con-
structed search graph are given a query dependent 
score. The score signifies the relevance of the 
paragraph with respect to given queries. During 
evaluation if it is found that any keyword is not 
satisfied by the seed nodes, then system goes back 
to individual document structure and collects rele-
vant nodes. Finally, it expands the offline graph by 
adding those nodes, fetched from individual docu-
ments. Next, the expanded search graph is proc-
essed to find the total minimum spanning tree T 
over the graph. 
5.1 Expanding Search Graph 
When query arrives, system evaluates nodes of the 
offline search graph and computes query depend-
ent score. This computation is based on ranking 
principals from IR community. The most popular 
IR ranking is okapi equation (Varadarajan and 
Hristidis, 2006) which is based on tf-idf principle.  
          
1 1 3
3
, 1
0 .5 ( ). ( 1).ln . .
0 .5 ( (1 ) )t Q d
N df k tf k q tf
d ld f k q tfk b b tf
avd l
+
?
? + +
+ +
? + +
?
                                                                                                                                                        
 
tf is the term?s frequency in document, qtf is term?s 
frequency in the query, N is the total no. of docu-
ments in collection, df is the number of documents 
that contain the term, dl is the document length 
(number of words), avdl is the average document 
length and k1 (1.0 ? 2.0), b (0.75), k3 (0 -1000) are 
constants. 
During node score computation, the system in-
telligently partitions the query set Q into two parts. 
One part consists of qi?s which are satisfied by at 
least one node from offline search graph. The other 
part consists of qi?s which are not satisfied by any 
node from offline search graph. The system then 
computes query dependent scores for the nodes of 
all the individual documents for the unsatisfied 
keyword set and relevant nodes (having score 
above threshold) are added to the search graph. 
Edge scores are computed only for edges connect-
ing newly added nodes with the existing ones and 
between the new nodes. In this way, the offline 
graph is expanded by adding some query depend-
ent nodes at runtime. Query dependent scoring can 
be made faster using a full text indexing which is a 
mapping Ki ? (Di , Ni); where Ki?s are content 
words (i.e., not stop words)
 
and Di?s and Ni?s are 
respectively the document ids and the node ids 
within the document set. Since, the node score is 
calculated at runtime, it needs to be accelerated. 
Thus a full text index developed offline will be of 
great help. 
5.2 Summary Generation 
Summary generation is basically a keyword search 
technique in the expanded search graph. This is to 
mention that the search technique discussed here is 
basically based on AND semantic, i.e. it requires 
all the keywords to be present in the summary, but 
the algorithm can be modified to take care of OR 
semantic also. Keyword search in graph structure 
is itself a research topic and several efficient algo-
rithms are there to solve the problem. DBXplorer 
(Agrawal et al, 2002), BANKS (Bhalotia et al, 
2002), are popular algorithms in this field which 
consider relational database as graph and devise 
algorithms for keyword based search in the graph. 
Finally, Varadarajan and Hristidis (2006) has pro-
posed Top-k Enumeration and MultiResultExpand-
ing search for constructing total minimum span-
ning tree over a document graph. Any of the above 
popular algorithms can be adapted to use within 
our framework. 
In our system we have used a search algorithm 
which finds different combinations of nodes that 
represent total spanning tree. For each of the com-
bination we compute score of the summary based 
on some IR principle (Varadarajan and Hristidis, 
2006). Then we take the one having best score 
(minimal in our case). If the graph is not too dense, 
then the response time will be small enough. The 
equation given below is used to compute the score 
of individual spanning tree T. 
 
1 1
s c o r e
s c o r e s c o r e
e T
n T
T a b
ne?
?
= +? ?
 
Where scoreT the score of the spanning tree, e and 
n is are edge and node of T respectively, scoree  
and scoren  are edge score and individual node 
score respectively. a and b are non zero positive 
constants in the range of [0 ? 1]. For a particular 
search graph, it is possible to find many total span-
ning trees, having different summary scores. In our 
system, the summary with the best score is consid-
ered. 
In Figure 2 two sample news stories are shown 
along with system identified seed nodes, shown in 
bold. A query based summary from that related 
document set is shown in Figure 3. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.3 Paragraph Ordering Scheme 
In the previous sections, the techniques for genera-
tion of summary nodes have been discussed. Here,  
we will investigate the method for ordering them 
into a coherent text.  In case of single document 
summarization, sentence/paragraph ordering is 
done based on the position of extracted paragraphs/ 
sentences in the original document. But in multi-
document scenario, the problem is non trivial since 
information is extracted from different documents 
and no single document can provide ordering. Be-
sides, the ordering of information in two different 
documents may be significantly varying because  
 
       
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Paragraphs of two news articles with five extracted seed/ topic paragraphs (in bold). Un-
derlined paragraphs are added later during graph expansion phase. 
 
Software major IBM has developed a speech recognition technology in Hindi which would help physically challenged and 
less literate Hindi speakers access information through a variety of systems.  [Doc-2, Para - 0 ] 
Besides, the technology could also enable C-DAC to ensure a high level of accuracy in Hindi translation in a number of do-
mains like administration, finance, agriculture and the small-scale industry.   [Doc-1, Para-5] 
A spellchecker to correct spoken-word errors also enhances the accuracy of the system.  [Doc-2, Para - 4 ] 
 
Figure 3. Automatic summary based on {speech recognition, accuracy, spellchecker} query 
P0: Software giant IBM has developed a speech recognition 
software in Hindi. 
P1 : The company hopes that this development will help 
physically challenged and less literate Hindi speakers to 
access information using a variety of applications. 
P2 : The Desktop Hindi Speech Recognition Technology 
developed by the IBM India Software Lab in collabora-
tion with Centre for Development of Advanced Com-
puting would provide a natural interface for human-
computer interaction. 
P3 : The new IBM technology could help to provide a natu-
ral interface for human-computer interaction. 
P4: According to Dr. Daniel Dias, Director, IBM Indian 
Research Laboratory, the technology which helps tran-
scribe continuous Hindi speech instantly into text form, 
could find use in a variety of appli In Figure 1. corre-
lation between two news articles is shown 
as a bipartite graph. cations like voice-enabled 
ATMs, car navigation systems, banking, telecom, railways, 
and airlines. 
P5: Besides, the technology could also enable C-DAC to 
ensure a high level of accuracy in Hindi translation in a 
number of domains like administration, finance, agri-
culture and the small-scale industry. 
P6: The IBM Desktop Hindi Speech Recognition software 
is capable of recognizing over 75,000 Hindi words with 
dialectical variations, providing an accuracy of 90 to 95%. 
P7: What?s more; this software also has an integrated spell-
checker that corrects spoken-word errors, enhancing the 
accuracy to a great extent. 
P8:  The Desktop Hindi Speech Recognition Technology 
also integrates a number of user-friendly features such as 
the facility to convert text to digits and decimals, date and 
currency format, and into fonts which could be imported to 
any Windows-based application. 
P9: ?IBM believes in taking high-end research to the benefit 
of the masses and bridging the digital divide through a 
faster diffusion process,? concluded Dias. 
P0: Software major IBM has developed a speech recog-
nition technology in Hindi which would help physically 
challenged and less literate Hindi speakers access in-
formation through a variety of systems. 
P1 : Called the Desktop Hindi Speech Recognition technol-
ogy, this software was developed by the IBM India Soft-
ware Lab jointly with the Centre for Development of Ad-
vanced Computing. 
P2 : The technology, which helps transcribe continuous 
Hindi speech instantly into text form, could find use in a 
variety of applications like voice-enabled ATMs, car 
navigation systems, banking, telecom, railways and 
airlines, said Dr Daniel Dias, Director, IBM India Re-
search Laboratory. 
P3 : The system can recognize more than 75,000 Hindi 
words with dialectical variations, providing an accuracy 
level of 90-95 per cent, he said. 
P4:  A spellchecker to correct spoken-word errors also 
enhances the accuracy of the system. 
P5:  The technology also has integrated many user-
friendly features such as facility to convert text to digits 
and decimals, date and currency format, and into fonts 
which could be imported to any windows-based applica-
tion. 
P6: "IBM believes in taking high-end research to the benefit 
of the masses and bridging the digital divide through a 
faster diffusion process", Dias said. 
P7: The technology also would enable C-DAC to ensure 
high-level accuracy in Hindi translation in a host of do-
mains, including administration, finance, agriculture and 
small scale industry. 
the writing styles of different authors are different. 
In case of news event summarization, chronologi-
cal ordering is a popular choice which considers 
the temporal sequence of information pieces, when 
deciding the ordering process. 
In this paper, we will propose a scheme of or-
dering which is different from the above two ap-
proaches in that, it only takes into consideration 
the semantic closeness of information pieces 
(paragraphs) in deciding the ordering among them. 
First, the starting paragraph is identified which is 
the paragraph with lowest positional ranking 
among selected ones over the document set. Next 
for any source node (paragraph) we find the sum-
mary node that is not already selected and have 
(correlation value) with the source node. This node 
will be selected as next source node in ordering. 
This ordering process will continue until the nodes 
are totally ordered. The above ordering scheme 
will order the nodes independent of the actual or-
dering of nodes in the original document, thus 
eliminating the source bias due to individual writ-
ing style of human authors. Moreover, the scheme 
is logical because we select a paragraph for posi-
tion p at output summary, based on how coherent it 
is with the (p-1)th paragraph. 
6 Evaluation 
Evaluation of summarization methods is generally 
performed in two ways. Evaluation measure based 
on information retrieval task is termed as the ex-
trinsic method, while the evaluation based on user 
judgments is called the intrinsic measure. We 
adopted the latter, since we concentrated more on 
user?s satisfaction. We measure the quality of out-
put based on the percentage of overlap of system 
generated output with the manual extract. Salton et 
al (1997) observed that an extract generated by one 
person is likely to cover 46% of the information 
that is regarded as most important by another per-
son. Mitra et. al. (1998) proposed an interesting 
method for evaluation of paragraph based auto-
matic summarization and identified the following 
four quality-measures ? Optimistic (O), Pessimistic 
(P), Intersection (I) and Union (U) based evalua-
tion. For evaluation purpose, we identify different 
related document set (D) from different domains 
like technical, business etc and keyword (query) 
list for each domain. Users are asked to manually 
prepare the multi-document summarization based 
on the given queries. They prepared it by marking 
relevant paragraphs over D. Based on the excerpts 
prepared by the users; the above scores are calcu-
lated as O: Percentage overlap with that manual 
extract for which the number of common para-
graphs is highest, P: Percentage overlap with that 
manual extract for which the number of common 
paragraphs is lowest; I: Percentage overlap with 
the intersection of manual extracts; U: Percentage 
overlap with the union of manual extracts. The re-
sults are shown in Table 1. A comparative survey 
of quality measures for the set of articles is shown 
in Figure 3. 
 
              Table 1.  Evaluation score 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  D Omeasure Pmeasure Imeasure Umeasure 
article1 
&    
article2 
44.4 27 33.3 66.6 
article3 
&    
article4 
75 60 50 100 
article5 
&    
article6 
50 35.5 25 66 
article7 
&    
article8 
45.5 28.7 33.3 56.4 
O -  M easure
0
10
20
30
40
50
60
70
80
1 2 3 4
Dat a Set
Sc
or
e
Sc
or
e
Sc
or
e
Sc
or
e
P - Measure
0
20
40
60
80
1 2 3 4
Data Set
Sc
o
re
  
 
 
 
 
 
 
 
 
 
 
     
 
 
 
 
 
 
 
 
 
 
 
Figure 3.  Comparative measure scores for set 
                 of articles 
7 Baseline Approach to Multilingual 
Summarization 
Our baseline approach to multilingual multidocu-
ment summarization is to apply our English based 
multi-document summarization system to other 
non-English languages like Hindi, Bengali, Japa-
nese etc. We have initially implemented the system 
for English language only, but it can be modified 
to work in multilingual scenario also. To work 
with other languages, the system requires some 
language dependent tools for that particular lan-
guage: 
1) A stop word list of that language is required be-
cause they have no significance in finding similar-
ity between the paragraphs and need to be removed 
during initial preprocessing stage. 
2) A language dependent stemmer is required. In 
most of the languages, stemmer is yet to be devel-
oped. Another problem is that suffix stripping is 
not the only solution for all languages because 
some languages have affix, circumfix etc. in their 
inflected form. A morphological analyzer to find 
the root word may be used for those languages. 
3) A lexicon for that language is required to match 
the similar words. For English, WordNet is widely 
available. For other languages also, similar type of 
lexicons are required. 
If these tools are available then our system can 
be tuned to generate query dependent multilingual 
multi-document summary. 
8 Conclusion and Future Work 
In this work we present a graph based approach for 
query dependent multi-document summarization 
system. Considering its possible application in the 
web document, we clearly divided the computation 
into two segments. Extraction of seed/topic sum-
mary nodes and construction of offline graph is a 
part of query independent computation. At query 
time, the precomputed graph is processed to extract 
the best multi-document summary. We have tested 
our algorithm with news articles from different 
domains. The experimental results suggest that our 
algorithm is effective. Although we experimented 
with pair of articles, the proposed algorithm can be 
improved to handle more than two articles simul-
taneously. 
The important aspect of our system is that it can 
be modified to compute query independent sum-
mary which consists of topic nodes, generated dur-
ing preprocessing stage. The paragraph ordering 
module can be used to define ordering among 
those topic paragraphs. Another important aspect is 
that our system can be tuned to generate summary 
with custom size specified by users. The spanning 
tree generation algorithm can be so modified that it 
produces not only total spanning tree but also takes 
care of the size requirement of user. Lastly, it is 
shown that our system can generate summary for 
other non-English documents also if some lan-
guage dependent tools are available. 
The performance of our algorithm greatly de-
pends on quality of selection of topic nodes. So if 
we can improve the identification of topic para-
graphs and shared topics among multiple docu-
ments it would surely enhance the quality of our 
system. 
9 References 
A. Singhal , M. Mitra, and C. Buckley. 1997. Automatic 
Text Summarization by Paragraph Extraction. Pro-
ceedings of ACL/EACL Workshop.  
C.-Y. Lin and E.H. Hovy. 2002. From Single to Multi-
document Summarization: A Prototype System and 
its Evaluation. Proceedings of ACL: 457?464. 
I -  M easure
0
10
20
30
40
50
60
1 2 3 4
Dat a Set
Sc
or
e
U  -  M easure
0
20
40
60
80
100
120
1 2 3 4
Dat a Set
Sc
or
e
D.R. Radev, H. Jing, M. Sty? and D. Tam. 2004. Cen-
troid-based summarization of multiple documents. 
Information Processing and Management, 
Vol.40:919?938. 
G. Salton , A. Singhal , M. Mitra, and C. Buckley. 1997. 
Automatic text structuring and summarization. In-
formation Processing and Management: Vol. 33, No. 
2: 193-207. 
G. Bhalotia, C. Nakhe, A. Hulgeri, S. Chakrabarti and 
S.Sudarshan. 2002. Keyword Searching and Brows-
ing in Databases using BANKS. Proceedings of 
ICDE : 431-440. 
 
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. 
Wise and X. Zhang. 2002. Cross-document summari-
zation by concept classification. Proceedings of 
SIGIR.02: 65-69 . 
I. Mani and E. Bloedorn. 2000. Summarizing Similari-
ties and Differences Among Related Documents. In-
formation Retrieval, Vol. 1(1): 35-67. 
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3):130?137. 
R. Varadarajan,. V. Hristidis. 2006. A system for query-
specific document summarization. Proceedings of 
CIKM 2006: 622-631. 
S. Agrawal, S. Chaudhuri, and G. Das.2002. DBXplorer: 
A System for Keyword-Based Search over Relational 
Databases. Proceedings of ICDE: 5-16. 
Y. Zhang, X. Ji, C. H. Chu, and H. Zha. 2004. Correlat-
ing Summarization of Multisource News with K-
Way Graph Biclustering. SIGKDD Explorations 
6(2): 34-42. 
JU-PTBSGRE:  GRE Using Prefix Tree Based Structure  
                            Abstract 
This paper presents a Prefix Tree based model 
of Generation of Referring Expression (RE). 
Our algorithm PTBSGRE works in two phas-
es. First, an encoded prefix tree is constructed 
describing the domain structure. Subsequent-
ly, RE is generated using that structure. We 
evaluated our system using Dice, MASI, Ac-
curacy, Minimality and Uniqueness scoring 
method using standard TEVAl tool and the re-
sult is encouraging. 
1 Introduction 
Generation of referring expression (GRE) is an 
important task in the field of Natural Language 
Generation (NLG) systems. The existing algo-
rithms in GRE lie in two extremities. Incremental 
Algorithm is simple and speedy but less expressive 
in nature whereas others are complex and exhaus-
tive but more expressive in nature. We propose a 
new Prefix Tree (Trie) based framework for mod-
eling GRE problems. It incorporates intricate fea-
tures of GRE (like set and boolean descriptions, 
context sensitivity, relational description etc.) 
while achieving attractive properties of Incremen-
tal algorithm (simplicity, speed etc.). The prefix 
tree based algorithm works in two phases. First, it 
encodes the description, stored in the knowledge 
base, in the form of prefix tree structure. Secondly, 
it generates the referring expression identifying the 
target object, which is basically a node search 
problem in the tree. The edges in our encoded trie 
structure are labeled and the path from root to that 
node forms the distinguishing description for the 
target object.  
Let D be the Domain, r be the target object and 
P be the ?PreferredAttributes? List.The Trie con-
structionn algorithm  ConstructTrie(D,P,T) is 
shown in figure 1, Referring expression generation 
algorithm MakeRefExpr(r,p,T,L) is shown in 
figure 2, where T is a node pointer and p is pointer 
to parent of that node. Our algorithm MakeRe-
fExpr returns set of attribute-values L to identify r  
in the domain. [[Ni]]= {d |d?D and d is stored at 
node Ni where Ni is an i-th level node}. Card(N) is 
cardinality of set of objects in node N. 
 
Figure 1. Prefix Tree Generation Algorithm 
 
Figure 2. Expression Generation Algorithm 
The significant achievement is that incompleteness 
of previous algorithms can be tackled in this model 
in a straightforward way. For example, in case of 
vague descriptions (overlapping properties), In-
cremental and other algorithms are unable to find 
unambiguous description even if it exists but our 
prefix tree model takes into account hearer model      
                         
                     Sibabrata Paladhi 
 
Sivaji Bandyopadhyay 
            Department of Computer Sc. & Engg. Department of Computer Sc. & Engg. 
Jadavpur University, India Jadavpur University, India 
            sibabrata_paladhi@yahoo.com            sivaji_cse_ju@yahoo.com 
 
 
 
230
and generate description for identifying the target 
object. Besides, in case of Boolean, plural, context 
sensitive and relational description generation our 
model provides a simple and linguistically rich 
approach to GRE. 
2 Evaluation Results  
In Table 1 and 2 the evaluation results for Furni-
ture and People data has been shown. 
 
 
Table1: Evaluation Result of Furniture data 
 
Table2: Evaluation Result of People data 
References  
R. Dale and E. Reiter. 1995. Computational Interpretations of 
the Gricean Maxims in the generation of Referring Expres-
sions. Cognitive Science (18): 233 ?263 
S. Paladhi and S. Bandyopadhyay. 2008. Generation of Refer-
ring Expression Using Prefix Tree Structure. Proceedings 
of  IJCNLP: 697-702 
van Deemter. 2002. Boolean Extensions of Incremental Algo-
rithm. Computational Linguistics 28(1): 37-52 
231
