Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 115?119, Dublin, Ireland, August 23-29 2014.
Method51 for Mining Insight from Social Media Datasets
Simon Wibberley
University of Sussex
simon.wibberley
@sussex.ac.uk
Jeremy Reffin
University of Sussex
j.p.reffin
@sussex.ac.uk
David Weir
University of Sussex
d.j.weir
@sussex.ac.uk
Abstract
We present Method51, a social media analysis software platform with a set of accompanying
methodologies. We discuss a series of case studies illustrating the platform?s application, and
motivating our methodological proposals.
1 Introduction
Social scientists wish to apply language processing technology on social media datasets to answer soci-
ological questions. To that end, the technology should support methodologies that allow analysts to gain
valuable insight from the datasets under examination. In previous work we have argued for the impor-
tance of agility when dealing with social media datasets (Wibberley et al., 2013). In this paper we present
a series of case studies, carried out on Twitter, that illustrate the importance of that agile paradigm, and
how they have motivated the development of several additional methodologies, including ?Twitcident?,
?Patterns of Use?, and ?Russian Doll? analysis. First, we present Method51
1
, the technological counter-
part to our methodological paradigm.
2 Method51
Method51 uses active learning, coupled with a Na??ve Bayes model, to allow social scientists to construct
chains of linked, bespoke classifiers. The framework, initially an extension of DUALIST (Settles, 2011),
utilises an EM step to harness information from large amounts of unlabelled data, and allows the analyst
to expedite learning by specifying features that are highly indicative of a class. Using this approach,
a classifier can be trained within minutes (Settles, 2011). This enables analysts to evolve the way that
the data is being analysed without significant loss of effort. Method51 provides significant additional
functionality including collaborative gold standard and classifier construction, processing pipeline con-
struction, data collection and storage, data visualisation, various filtering and processing modules, and
time-based data selection. Figure 1a show the pipeline construction interface, which allows analysts to
knit together chains of bespoke classifiers.
Figure 1b shows the ?Coding? interface which is the primary point of contact between the analyst and
the data, where documents and features are labelled. Classifier evaluation statistics are also displayed, so
analysts can rapidly assess whether the data and technology are amenable to their analytical approach.
Method51 aims to put social science researchers at the centre of the data exploration process. Insight is
generated through the iterative interaction of the subject matter expert and the data itself.
Method51 combines two strands of existing work. The first body of work employs tailored automatic
data analysis, using supervised machine-learning approaches (Carvalho et al., 2011; Papacharissi and de
Fatima Oliveira, 2012; Meraz and Papacharissi, 2013; Hopkins and King, 2010; Burnap et al., 2013b).
A second body of work focusses on providing user interfaces that enable researchers to customise their
processing pipeline, based on the requirements of their investigation (Blessing et al., 2013; Black et al.,
2012; Burnap et al., 2013a).
1
Method51 has been released under an open source license, and is available at https://github.com/simonwibberley/method51
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
115
(a) Pipeline Construction Interface
(b) Coding interface
3 Case Studies
Over the past 18 months we have conducted a wide range of studies using Method51. Three illustrative
case studies are presented here in chronological order; each investigation highlighted new analytical
challenges and therefore motivated methodological and technological development.
3.1 EU Sentiment
In the first study, we examined the attitudes of EU citizens towards the Eurozone crisis of 2013 (Bartlett et
al., textitforthcoming). We set out with a preconceived structure and methodology. We tracked 6 topics,
referencing EU institutions or prominent people, and collected Tweets in 3 languages (English, French,
and German) to create 18 distinct streams of messages. For each stream, we constructed a bespoke
classification pipeline with a common analytical architecture of three successive layers: a classifier for
relevancy, a classifier to determine whether Tweets were attitudinal, and a classifier to determine the
polarity of the sentiment being expressed.
We found that, broadly, the data did not fit the pre-conceived analytical architecture neatly and that
classifier performance was a poor fit with human judgements, particularly for the attitudinal and senti-
ment layers. Table 1 illustrates the range of performance of classifiers measured against human-annotated
gold standard. Although relevancy classifiers performed adequately, the reliability of attitudinal and sen-
timent classifiers varies widely across streams.
Investigation revealed a number of underlying issues and prompted a series of methodological re-
sponses:
Need for flexible architecture We observed that each stream presented different challenges that could
only be appropriately addressed by a bespoke architecture tailored to the anatomy of the stream. For
example, relevancy classification was only sometimes required. The ?Euro? stream was targeted towards
conversation about the Euro currency, but required relevancy filtering as the query ?#euro? matched con-
versations regarding a wide variety of other topics such as sport competitions. Conversely, the ?Barroso?
stream, tracking messages regarding the president of the European Commission, Jos?e Manuel Barroso,
was of sufficiently high precision not to warrant relevancy filtering.
?Twitcident? analysis We observed that attitudes were typically only exposed when some event in the
world ?provoked? a burst of reactions that was related to the topic of interest. These response bursts,
which usually occur over a matter of hours to days, have been labelled ?Twitcidents?. We found that the
nature of the response ? and how this should be mapped onto the broader topic ? was unique to the
event itself. The ?Twitcident? analysis principle states that each event needs to be studied separately in
order to be correctly interpreted. Using a common classification architecture, or otherwise aggregating
116
results across Twitcidents, is likely to create a misleading picture of how opinions are being shaped by
events over time. As an example, a speech by the UK Prime Minister expressing a sceptical view of
the EU prompted many enthusiastic responses. Messages that commented positively on his speech were
contributing evidence of negative sentiment towards the EU. Clearly the reaction to that speech had to be
analysed separately in order to allow for this reversal of sentiment polarity.
Exploratory ?Patterns of Use? analysis The poor fit between the data and the imposed classification
architecture prompted us to adopt a new approach to constructing pipelines. The framework enables
analysts to ?fail fast?: engage actively with the data and explore how it is structured, before committing
to the pipeline framework. This is feasible because Method51 enables classifiers to be built quickly.
This ?Patterns of Use? analysis is inspired by Grounded Theory (Glaser et al., 1968), and mandates
that classification categories should arise from an unbiased examination of the available data. Categories
arise naturally from the analyst?s engagement with the data.
3.2 Father?s Day
Our initial ideas about ?Patterns of Use? analysis were explored further in the Father?s Day study. Our
aim here was to identify users likely to respond positively to a targeted advertisement for Father?s Day
gift ideas, that assessment being driven by the content of a Tweet sent by the user. We collected and
analysed Tweets mentioning Father?s Day in the days leading up to the event, and our first attempts at
analysing underlying patterns prompted a revision to our methodology.
?Russian Doll? approach We observed that at any stage the data tends to be dominated by one pattern of
usage, obscuring other underlying patterns. We developed a ?Russian Doll? approach, which mandates
that at each layer a classifier is built to unpack from the data Tweets that match this most prominent
pattern of usage. With this usage pattern stripped out, new structure is typically revealed in the remaining
data, which can in turn be unpacked using simple bespoke classifiers. Chained together, this pipeline of
classifiers removes successive different patterns of usage to expose underlying structure.
In this case, our a-priori expectation was that the stream would contain marketing Tweets, general
conversation about Father?s Day, and our target subset: people expressing uncertainty about what gift to
get. Our analysis revealed, however, a significant critical class of Tweets (and Tweeters) the presence of
which was unexpected ? a category for which targeted marketing Tweets would be wholly inappropriate.
The content of the Tweets matching this category (dubbed ?Sad/Distressed?) was negative, with Tweeters
venting sad or angry feelings towards absent fathers, generally though family breakdown or bereavement.
It was only through this careful patterns of use analysis that we identified this critical subgroup.
The final architecture consisted of two layers that dealt with existing marketing Tweets, a layer that
assessed the suitability of the Tweet as a potential target for a marketing campaign, and a final layer
that identified explicit requests for gift ideas. This pipeline showed good performance as measured by
F-scores against gold-standard annotated data (see Table 2). The unexpected ?Sad/Distressed? category
constituted a high proportion (10%) of all Tweets in the data set. Of the remaining tweets, 50% were
classified ?Marketing?, 36% were classified ?Miscellaneous? comments about Father?s Day, and 4% as
?Gift Idea Request?, our target group.
3.3 Mark Duggan
Our final case study illustrates an analysis conducted using the ?Twitcident? and ?Patterns of Use? tech-
niques. The aim was to dissect the online reaction to developments in the Mark Duggan inquest, an
enquiry into the shooting by police in London of a young black man. Over the course of about a month,
Tweets regarding Mark Duggan were collected with a view to analysing reactions as the case progressed.
The response showed the familiar ?Twitcident? pattern (see Figure 2). Twitcidents were examined indi-
vidually, culminating in an analysis of the large response on Twitter to the final verdict. Four specific
categories of response were identified and analysed. These were: (i) ?No Justice? ? where a Tweet
included accusations of institutional racism and/or claims that Duggan was unarmed; (ii) ?Justice? ?
that Duggan ?had it coming?, and/or that Duggan was armed; (iii) ?Riot? ? warning of possible rioting,
117
Range Split
Relevance 0.5 - 0.9 2-way
Attitudinal 0.3 - 0.9 2-way
Sentiment 0.1 - 0.8 3-way
Table 1: Range of F1-scores of EU Sentiment
Individual Marketing
F1 0.873 0.844
(a) Marketing level 1
Suitable Sad Marketing
F1 0.785 0.564 0.227
(c) Suitability
Individual Marketing
F1 0.834 0.490
(b) Marketing level 2
Request Other
F1 0.583 0.959
(d) Gift request
Table 2: F-scores of Russion Dolls Analysis
Justice No Justice Riot Watching
F1 0.636 0.842 0.737 0.451
Split 58% 17% 7% 18%
Table 3: Verdict classifier performance
making calls for calm; and (iv) ?Watching? ? people neutrally expressing interest in a case. Pipeline
performance was good as measured by F-scores against gold-standard (see Table 3)
This case further illustrates how patterns of response are specific to a particular situation, greatly
limiting the usefulness of pre-defined classifiers (e.g. for sentiment) in real-world investigations.
Figure 2: Mark Duggan ?Twitcidents?
4 Discussion
We will end with a discussion of how we have interpreted the developments in methodology described
above, and how that interpretation leads to an intuitive framework for further work.
We have presented the application of three distinct methodologies for mining insight from social me-
dia data. The insights gained from each case study are the result of three interdependent factors; (i) the
question that is being posed, (ii) the extend to which the answers to the question reside in the data, and
(iii) the extent to which the technology is capable of addressing the question given the data. We encapsu-
late this line of interpretation as ?Question, Data, and Technology?. By considering these factors analysts
can remain plastic about how the ?Data? and ?Technology? can mutually constrain and inform the ?Ques-
tion?. For example, if answers to the question are not represented in the data in a form the technology
can recognise, then the question should be revised. Conversely, the technology may reveal unexpected
characteristics in the data that contribute towards the analysts understanding of what the question should
be. Careful alignment between all three tend to result in valuable insights being discovered.
Crucial to this alignment is assessing the performance of the technology on the data, given the question
being posed. Method51 provides some functionality towards supporting this, such as accuracy and F-
scores. However, these measures indirectly indicate whether the classifier is behaving sensibly by virtue
118
of the gold standard evaluation data being an i.i.d sample of the unlabelled data.
The behaviour of classifiers on unlabelled data forms a crucial role in how the technology supports the
analyst in their investigation. Directly exposing that behaviour and developing an informed understand-
ing of the relationship between ?Question, Data, and Technology? would only expedite and contribute
towards reliable insights being discovered. Incorporating technology into Method51 that exposes how
unlabelled data are effected by analytical processes is an area for further work.
In general, we have found that considering the interaction between ?Question, Data, and Technology?
provides an intuitive framework for refining the focus of methodological and technological development
towards demonstrably useful innovations.
Acknowledgments
This work was supported by the ESRC National Centre for Research Methods grant DU/512589110.
We are grateful to our collaborators at the Centre for the Analysis of social media, Jamie Bartlett and
Carl Miller for valuable contributions to this work. We thank the anonymous reviewers for their helpful
comments. This work was partially supported by the Open Society Foundation.
References
J. Bartlett, Miller C, J. Reffin, D. Weir, and Wibberley S. forthcoming. Vox digitas.
http://www.demos.co.uk/publications.
W. Black, R. Procter, S. Gray, and S. Ananiadou. 2012. A data and analysis resource for an experiment in text
mining a collection of micro-blogs on a political topic. In LREC. ELRA.
A. Blessing, J. Sonntag, F. Kliche, U. Heid, J. Kuhn, and M. Stede. 2013. Towards a tool for interactive concept
building for large scale analysis in the humanities. In LaTeCH, Social Sciences, and Humanities. ACL.
P. Burnap, N. Avis, and O. Rana. 2013a. Making sense of self-reported socially significant data using computa-
tional methods. International Journal of Social Research Methodology.
P. Burnap, O. Rana, N. Avis, M. Williams, W. Housley, A. Edwards, J. Morgan, and L. Sloan. 2013b. Detect-
ing tension in online communities with computational twitter analysis. Technological Forecasting and Social
Change.
P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva. 2011. Liars and saviors in a sentiment annotated corpus of
comments to political debates. In ACL: Human Language Technologies.
Barney G Glaser, Anselm L Strauss, and Elizabeth Strutzel. 1968. The discovery of grounded theory; strategies
for qualitative research. Nursing Research.
D. J. Hopkins and G. King. 2010. A method of automated nonparametric content analysis for social science.
American Journal of Political Science.
Sharon Meraz and Zizi Papacharissi. 2013. Networked gatekeeping and networked framing on #egypt. The
International Journal of Press/Politics, 18(2):138?166.
Zizi Papacharissi and Maria de Fatima Oliveira. 2012. Affective news and networked publics: the rhythms of
news storytelling on #egypt. Journal of Communication, 62(2):266?282.
B. Settles. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and
instances. In Empirical Methods in Natural Language Processing.
Simon Wibberley, David Weir, and Jeremy Reffin. 2013. Language technology for agile social media science. In
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Human-
ities, pages 36?42, Sofia, Bulgaria, August. Association for Computational Linguistics.
119
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 36?42,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Language Technology for Agile Social Media Science
Simon Wibberley
Department of Informatics
University of Sussex
sw206@susx.ac.uk
Jeremy Reffin
Department of Informatics
University of Sussex
j.p.reffin@susx.ac.uk
David Weir
Department of Informatics
University of Sussex
davidw@susx.ac.uk
Abstract
We present an extension of the DUALIST
tool that enables social scientists to engage
directly with large Twitter datasets. Our
approach supports collaborative construc-
tion of classifiers and associated gold stan-
dard data sets. The tool can be used to
build classifier cascades that decomposes
tweet streams, and provide analysis of tar-
geted conversations. A central concern is
to provide an environment in which social
science researchers can rapidly develop an
informed sense of what the datasets look
like. The intent is that they develop, not
only an informed view as to how the data
could be fruitfully analysed, but also how
feasible it is to analyse it in that way.
1 Introduction
In recent years, automatic social media analysis
(SMA) has emerged, not only as a major focus
of attention within the academic NLP community,
but as an area that is of increasing interest to a va-
riety of business and public sectors organisations.
Among the many social media platforms in use to-
day, the one that has received the most attention is
Twitter, the second most popular social media net-
work in the world with over 400 million tweets
sent each day. The popularity of Twitter as a tar-
get of SMA derives from both the public nature
of tweets, and the availability of the Twitter API
which provides a variety of flexible methods for
scraping tweets from the live Twitter stream.
A plethora of social media monitoring plat-
forms now exist, that are mostly concerned with
providing product marketing oriented services1.
For example, brand monitoring services seek to
provide companies with an understanding of what
1http://wiki.kenburbary.com/social-media-monitoring-
wiki lists 230 Social Media Monitoring Solutions
is being said about their brands and products,
with language processing technology being used
to capture relevant comments or conversations and
apply some form of sentiment analysis (SA), in or-
der to derive insights into what is being said. This
paper forms part of a growing body of work that
is attempting to broaden the scope of SMA be-
yond the realm of product marketing, and into ar-
eas of concern to social scientists (Carvalho et al,
2011; Diakopoulos and Shamma, 2010; Gonzalez-
Bailon et al, 2010; Marchetti-Bowick and Cham-
bers, 2012; O?Connor et al, 2010; Tumasjan et al,
2011; Tumasjan et al, 2010).
Social media presents an enormous opportunity
for the social science research community, consti-
tuting a window into what large numbers of people
are talking. There are, however, significant obsta-
cles facing social scientists interested in making
use of big social media datasets, and it is important
for the NLP research community to gain a better
understanding as to how language technology can
support such explorations.
A key requirement, and the focus of this paper,
is agility: the social scientist needs to be able to
engage with the data in a way that supports an it-
erative process, homing in on a way of analysing
the data that is likely to produce valuable insight.
Given what is typically a rather broad topic as a
starting point, there is a need to see what issues re-
lated to that topic are being discussed and to what
extent. It can be important to get a feeling for the
kind of language being used in these discussions,
and there is a need to rapidly assess the accuracy
of the automated decision making. There is little
value in developing an analysis of the data on an
approach that relies on the technology making de-
cisions that are so nuanced that the method being
used is highly unreliable. As the answers to these
questions are being exposed, insights emerge from
the data, and it becomes possible for the social sci-
entist to progressively refine the topics that are be-
36
ing targetted, and ultimately create a way of au-
tomatically analysing the data that is likely to be
insightful.
Supporting this agile methodology presents se-
vere challenges from an NLP perspective, where
the predominant approaches use classifiers that
involve supervised machine learning. The need
for substantial quantities of training data, and
the detrimental impact on performance that re-
sults when applying them to ?out-of-domain? data
mean that exisiting approaches cannot support the
agility that is so important when social scientists
engage with big social media datasets.
We describe a tool being developed in collab-
oration with a team of social scientists to support
this agile methodology. We have built a frame-
work based on DUALIST, an active learning tool
for building classifiers (Settles, 2011; Settles and
Zhu, 2012). This framework provides a way for
a group of social scientists to collaboratively en-
gage with a stream of tweets, with a goal of con-
structing a chain (or cascade) of automatic docu-
ment classification layers that isolate and analyse
targeted conversions on Twitter. Section 4 dis-
cusses ways in which the design of our frame-
work is intended to support the agile methodol-
ogy mentioned above, with particular emphasis on
the value of DUALIST?s active learning approach,
and the crucial role of the collaborative gold stan-
dard and model building activities. Section 4.3
discusses additional data processing step that have
been introduced to increase the frameworks use-
fulness, and section 5 introduces some projects to
which the framework is being applied.
2 Related Work
Work that focuses on addressing sociological
questions with SMA broadly fall into one of three
categories.
? Approaches that employ automatic data analy-
sis without tailoring the analysis to the specifics of
the situation e.g. (Tumasjan et al, 2010; Tumas-
jan et al, 2011; O?Connor et al, 2010; Gonzalez-
Bailon et al, 2010; Sang and Bos, 2012; Bollen
et al, 2011). This body of research involves lit-
tle or no manual inspection of the data. An an-
alytical technique is selected a-priori, applied to
the SM stream, and the results from that analy-
sis are then aligned with a real-world phenomenon
in order to draw predictive or correlative conclu-
sions about social media. A typical approach is
to predict election outcomes by counting mentions
of political parties and/or politicians as ?votes? in
various ways. Further content analysis is then
overlaid, such as sentiment or mood anlysis, in
an attempt to improve performance. However the
generic language-analysis techniques that are ap-
plied lead to little or no gain, often causing ad-
justments to target question to something with less
strict assessment criteria, such as poll trend instead
of election outcome (Tumasjan et al, 2010; Tu-
masjan et al, 2011; O?Connor et al, 2010; Sang
and Bos, 2012). This research has been criticised
for applying out-of-domain techniques in a ?black
box? fashion, and questions have been raised as
to how sensitive the results are to parameters cho-
sen (Gayo-Avello, 2012; Jungherr et al, 2012).
? Approaches that employ manual analysis of
the data by researchers with a tailored analyti-
cal approach (Bermingham and Smeaton, 2011;
Castillo et al, 2011).This approach reflects tra-
ditional research methods in the social sciences.
Through manual annotation effort, researchers en-
gage closely with the data in a manual but in-
teractive fashion, and this effort enables them to
uncover patterns in the data and make inferences
as to how SM was being used in the context of
the sociocultural phenomena under investigation.
This research suffers form either being restricted
to fairly small datasets.
? Approaches that employ tailored automatic
data analysis, using a supervised machine-learning
approach(Carvalho et al, 2011; Papacharissi and
de Fatima Oliveira, 2012; Meraz and Papacharissi,
2013; Hopkins and King, 2010). This research in-
fers properties of the SM data using statistics from
their bespoke machine learning analysis. Mannual
annotation effort is required to train the classifiers
and is typically applied in a batch process at the
commencement of the investigation.
Our work aims to expand this last category, im-
proving the quality of research by capturing more
of the insight-provoking engagement with the data
seen in more traditional research.
3 DUALIST
Our approach is built around DUALIST (Settles,
2011; Settles and Zhu, 2012), an open-source
project designed to enable non-technical analysts
to build machine-learning classifiers by annotat-
ing documents with just a few minutes of effort.
37
In Section 4, we discuss various ways in which
we have extended DUALIST, including function-
ality allowing multiple annotators to work in par-
allel; incorporating functionality to create ?gold-
standard? test sets and measure inter-annotator
agreement; and supporting on-going performance
evaluation against the gold standard during the
process of building a classifier. DUALIST pro-
vides a graphical interface with which an annota-
tor is able to build a Na??ve Bayes? classifier given
a collection of unlabelled documents. During the
process of building a classifier, the annotator is
presented with a selection of documents (in our
case tweets) that he/she has an opportunity to la-
bel (with one of the class labels), and, for each
class, a selection of features (tokens) that the an-
notator has an opportunity to mark as being strong
features for that class.
Active learning is used to select both the docu-
ments and the features being presented for annota-
tion. Documents are selected on the basis of those
that the current model is most uncertain about
(as measured by posterior class entropy), and fea-
tures are selected for a given class on the basis
of those with highest information gain occurring
frequently with that class. After a batch of docu-
ments and features have been annotated, a revised
model is built using both the labelled data and the
current model?s predictions for the remaining un-
labelled data, through the use of the Expectation-
Maximization algorithm. This new model is then
used as the basis for selecting the set of documents
and features that will be presented to the annotator
for the next iteration of the model building pro-
cess. Full details can be found in Settles (2011).
The upshot of this is two-fold: not only can a
reasonable model be rapidly created, but the re-
searcher is exposed to an interesting non-uniform
sample of the training data. Examples that are rel-
atively easy for the model to classify, i.e. those
with low entropy, are ranked lower in the list of
unlabelled data awaiting annotation. The effect of
this is that the training process facilitates a form of
data exploration that exposes the user to the hard-
est border cases.
4 Extending DUALIST for Social Media
Science Research
This section describes ways in which we have ex-
tended DUALIST to provide an integrated data ex-
ploration tool for social scientists. As outlined in
the introduction, our vision is that a team of social
scientists will be able to use this tool to collabora-
tively work towards the construction of a cascade
of automatic document classification layers that
carve up an incoming Twitter data stream in order
to pick out one or more targeted ?conversations?,
and provide an analysis of what is being discussed
in each of these ?conversations?. In what follows,
we refer to the social scientists as the researchers
and the activity during which the researchers are
working towards delivering a useful classifier cas-
cade as data engagement.
4.1 Facilitating data engagement
When embarking on the process of building one
of the classifiers in the cascade, researchers bring
preconceptions as to the basis for the classifica-
tion. It is only when engaging with the data that
it becomes possible to develop an adequate clas-
sification policy. For example, when looking for
tweets that express some attitude about a targeted
issue, one needs a policy as to how a tweet that
shares a link to an opinion piece on that topic
without any further comment should be classified.
There are a number of ways in which we support
the classification policy development process.
? One of the impacts of the active learning ap-
proach adopted in DUALIST is that by presenting
tweets that the current model is most unsure of,
DUALIST will very rapidly expose issues around
how to make decisions on boundary cases.
? We have extended DUALIST to allow multi-
ple researchers to build a classifier concurrently.
In addition to reducing the time it takes to build
classifiers, this fosters a collaborative approach to
classification policy development.
? We have added functionality that allows for the
collaborative construction of gold standard data
sets. Not only does this provide feedback dur-
ing the model building process as to when perfor-
mance begins to plateau, but, as a gold standard
is being built, researchers are shown the current
inter-annotator agreement score, and are shown
examples of tweets where there is disagreement
among annotators. This constitutes yet another
way in which researchers are confronted with the
most problematic examples.
4.2 Building classifier cascades
Having considered issues that relate to the con-
struction of an individual classifier, we end this
38
section by briefly considering issues relating to
the classifier cascade. The Twitter API provides
basic boolean search functionality that is used to
scrape the Twitter stream, producing the input to
the cascade. A typical strategy is to select query
terms for the boolean search with a view to achiev-
ing a reasonably high recall of relevant tweets2.
An effective choice of query terms that actually
achieves this is one of the things that is not well
understood in advance, but which we expect to
emerge during the data engagement phase. Cap-
turing an input stream that contains a sufficiently
large proportion of interesting (relevant) tweets is
usually achieved at the expense of precision (the
proportion of tweets in the stream being scraped
that are relevant). As a result, the first task that is
typically undertaken during the data engagement
phase involves building a relevancy classifier, to
be deployed at the top of the classifier cascade,
that is designed to filter out irrelevant tweets from
the stream of tweets being scraped.
When building the relevancy classifier, the re-
searchers begin to see how well their preconcep-
tions match the reality of the data stream. It is only
through the process of building this classifier that
the researchers begin to get a feel for the compo-
sition of the relevant data stream. This drives the
researcher?s conception as to how best to divide
up the stream into useful sub-streams, and, as a
result, provides the first insights into an appropri-
ate cascade architecture. Our experience is that in
many cases, classifiers at upper levels of the cas-
cade are involved in decomposing data streams in
useful ways, and classifiers that are lower down
in the cascade are designed to measure some facet
(e.g. sentiment polarity) of the material on some
particular sub-stream.
4.3 Tools for Data Analysis
As social scientists are starting to engage with
real-world data using this framework, it has
emerged that certain patterns of downstream data
analysis are of particular use.
Time series analysis. For many social phenom-
ena, the timing and sequence of social media mes-
sages are of critical importance, particularly for a
platform such as Twitter. Our framework supports
tweet volume analysis across any time frame, al-
2In many cases it is very hard to estimate recall since there
is no way to estimate accurately the volume of relevant tweets
in the full Twitter stream.
lowing researchers to review changes over time
in any classifier?s input or output tweet flows
(classes). This extends the common approach of
sentiment tracking over time to tracking over time
any attitudinal (or other) response whose essen-
tial features can be captured by a classifier of this
kind. These class-volume-by-time-interval plots
can provide insight into how and when the stream
changes in response to external events.
Link analysis. It is becoming apparent that link
sharing (attaching a URL to a tweet, typically
pointing to a media story) is an important aspect of
how information propagates through social media,
particularly on Twitter. For example, the mean-
ing of a tweet can sometimes only be discerned by
inspecting the link to which it points. We are in-
troducing to the framework automatic expansion
of shortened URLs and the ability to inspect link
URL contents, allowing researchers to interpret
tweets more rapidly and accurately. A combina-
tion of link analysis with time series analysis is
also providing researchers with insights into how
mainstream media stories propagate through soci-
ety and shape opinion in the social media age.
Language use analysis. Once a classifier has
been initially established, the framework analyses
the language employed in the input tweets using
an information gain (IG) measure. High IG fea-
tures are those that have occurrence distributions
that closely align the document classification dis-
tributions; essentially they are highly indicative of
the class. This information is proving useful to so-
cial science researchers for three purposes. First,
it helps identify the words and phrases people em-
ploy to convey a particular attitude or opinion in
the domain of interest. Second, it can provide in-
formation on how the language employed shifts
over time, for example as new topics are intro-
duced or external events occur. Third, it can be
used to select candidate keywords with which to
augment the stream?s boolean scraper query. In
this last case, however, we need to augment the
analysis; many high IG terms make poor scraper
terms because they are poorly selective in the more
general case (i.e. outside of the context of the ex-
isting query-selected sample). We take a sample
using the candidate term alone with the search API
and estimate the relevancy precision of the scraped
tweet sample by passing the tweets through the
first-level relevancy classifier. The precision of the
39
new candidate term can be compared to the preci-
sion of existing terms and a decision made.
5 Applications and Extensions
The framework?s flexibility enables it to be applied
to any task that can be broken down into a series of
classification decisions, or indeed where this ap-
proach materially assists the social scientist in ad-
dressing the issue at hand. In order to explore its
application, our framework is being applied to a
variety of tasks:
Identifying patterns of usage. People use the
same language for different purposes; the frame-
work is proving to be a valuable tool for eluci-
dating these usage patterns and for isolating data
sets that illustrate these patterns. As an example,
the authors (in collaboration with a team of so-
cial scientists) are studying the differing ways in
which people employ ethnically and racially sensi-
tive language in conversations on-line. The frame-
work has helped to reveal and isolate a number of
distinct patterns of usage.
Tracking changes in opinion over time. Sen-
timent classifiers trained in one domain perform
poorly when applied to another domain, even
when the domains are apparently closely related
(Pang and Lee, 2008). Traditionally, this has
forced a choice between building bespoke clas-
sifiers (at significant cost), or using generic sen-
timent classifiers (which sacrifice performance).
The ability to rapidly construct sentiment classi-
fiers that are specifically tuned to the precise do-
main can significantly increase classifier perfor-
mance without imposing major additional costs.
Moving beyond sentiment, with these bespoke
classifiers it is in principle possible to track over
time any form of opinion that is reflected in lan-
guage. In a second study, the authors are (in col-
laboration with a team of social scientists) build-
ing cascades of bespoke classifiers to investigate
shifts in citizens? attitudes over time (as expressed
in social media) to a range of political and social
issues arising across the European Union.
Entity disambiguation. References to individ-
uals are often ambiguous. In the general case,
word sense disambiguation is most success-
fully performed by supervised-learning classifiers
(Ma`rquez et al, 2006), and the low cost of pro-
ducing classifiers using this framework makes this
approach practical for situations where we require
repeated high recall, high precision searches of
large data sets for a specific entity. As an example,
this approach is being employed in the EU attitu-
dinal survey study.
Repeated complex search. In situations where
a fixed but complex search needs to be performed
repeatedly over a relatively long period of time,
then a supervised-learning classifier can be ex-
pected both to produce the best results and to be
cost-effective in terms of the effort required to
train it. The authors have employed this approach
in a commercial environment (Lyra et al, 2012),
and the ability to train classifiers more quickly
with this framework reduces the cost still further
and makes this a practical approach in a wider
range of circumstances.
With regard to extension of the framework, we
have identified a number of avenues for expansion
and improvement that will significantly increase
its usefulness and applicability to real-world sce-
narios, and we have recently commenced an 18-
month research programme to formalise and ex-
tend the framework and its associated methodol-
ogy for use in social science research3.
Conclusions and Future Work
We describe an agile analysis framework built
around the DUALIST tool designed to support ef-
fective exploration of large twitter data sets by
social scientists. The functionality of DUAL-
IST has been extended to allow the scraping of
tweets through access to the Twitter API, collab-
orative construction of both gold standard data
sets and Na??ve Bayes? classifiers, an Information
Gain-based method for automatic discovery of
new search terms, and support for the construction
of classifier cascades. Further extensions currently
under development include grouping tweets into
threads conversations, and automatic clustering of
relevant tweets in order to discover subtopics un-
der discussion.
Acknowledgments
We are grateful to our collaborators at the Cen-
tre for the Analysis of social media, Jamie Bartlett
and Carl Miller for valuable contributions to this
work. We thank the anonymous reviewers for their
helpful comments. This work was partially sup-
ported by the Open Society Foundation.
3Towards a Social Media Science, funded by the UK
ESRC National Centre for Research Methods.
40
References
[Bermingham and Smeaton2011] Adam Bermingham
and Alan F Smeaton. 2011. On using Twitter to
monitor political sentiment and predict election
results. In Proceedings of the Workshop on Senti-
ment Analysis where AI meets Psychology (SAAIP),
IJCNLP 2011, pages 2?10.
[Bollen et al2011] Johan Bollen, Alberto Pepe, and
Huina Mao. 2011. Modeling public mood and emo-
tion: Twitter sentiment and socio-economic phe-
nomena. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media,
pages 450?453.
[Carvalho et al2011] Paula Carvalho, Lu??s Sarmento,
Jorge Teixeira, and Ma?rio J. Silva. 2011. Liars and
saviors in a sentiment annotated corpus of comments
to political debates. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, pages 564?568, Stroudsburg, PA,
USA.
[Castillo et al2011] Carlos Castillo, Marcelo Mendoza,
and Barbara Poblete. 2011. Information credibility
on Twitter. In Proceedings of the 20th International
Conference on World wide web, pages 675?684.
[Diakopoulos and Shamma2010] Nicholas A Di-
akopoulos and David A Shamma. 2010. Charac-
terizing debate performance via aggregated Twitter
sentiment. In Proceedings of the 28th international
conference on Human factors in computing systems,
pages 1195?1198.
[Gayo-Avello2012] Daniel Gayo-Avello. 2012. I
wanted to predict elections with twitter and all i
got was this lousy paper a balanced survey on elec-
tion prediction using Twitter data. arXiv preprint
arXiv:1204.6441.
[Gonzalez-Bailon et al2010] Sandra Gonzalez-Bailon,
Rafael E Banchs, and Andreas Kaltenbrunner. 2010.
Emotional reactions and the pulse of public opin-
ion: Measuring the impact of political events on
the sentiment of online discussions. arXiv preprint
arXiv:1009.4019.
[Hopkins and King2010] Daniel J. Hopkins and Gary
King. 2010. A method of automated nonparametric
content analysis for social science. American Jour-
nal of Political Science, 54(1):229?247.
[Jungherr et al2012] Andreas Jungherr, Pascal Ju?rgens,
and Harald Schoen. 2012. Why the Pirate Party
won the German election of 2009 or the trouble
with predictions: A response to Tumasjan, Sprenger,
Sander, & Welpe. Social Science Computer Review,
30(2):229?234.
[Lyra et al2012] Matti Lyra, Daoud Clarke, Hamish
Morgan, Jeremy Reffin, and David Weir. 2012.
Challenges in applying machine learning to media
monitoring. In Proceedings of Thirty-second SGAI
International Conference on Artificial Intelligence
(AI-2012).
[Marchetti-Bowick and Chambers2012] Micol
Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant su-
pervision: political forecasting with Twitter. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 603?612.
[Ma`rquez et al2006] Llu??s Ma`rquez, Gerard Escudero,
David Mart??nez, and German Rigau. 2006. Su-
pervised corpus-based methods for wsd. In Eneko
Agirre and Philip Edmonds, editors, Word Sense
Disambiguation, volume 33 of Text, Speech and
Language Technology, pages 167?216. Springer
Netherlands.
[Meraz and Papacharissi2013] Sharon Meraz and Zizi
Papacharissi. 2013. Networked gatekeeping and
networked framing on #egypt. The International
Journal of Press/Politics, 18(2):138?166.
[O?Connor et al2010] Brendan O?Connor, Ramnath
Balasubramanyan, Bryan R Routledge, and Noah A
Smith. 2010. From tweets to polls: Linking text
sentiment to public opinion time series. In Proceed-
ings of the International AAAI Conference on We-
blogs and Social Media, pages 122?129.
[Pang and Lee2008] Bo Pang and Lillian Lee. 2008.
Opinion mining and sentiment analysis. Founda-
tions and trends in Information Retrieval, 2(1-2):1?
135.
[Papacharissi and de Fatima Oliveira2012] Zizi Pa-
pacharissi and Maria de Fatima Oliveira. 2012.
Affective news and networked publics: the rhythms
of news storytelling on #egypt. Journal of Commu-
nication, 62(2):266?282.
[Sang and Bos2012] Erik Tjong Kim Sang and Johan
Bos. 2012. Predicting the 2011 dutch senate elec-
tion results with Twitter. Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics 2012, page 53.
[Settles and Zhu2012] Burr Settles and Xiaojin Zhu.
2012. Behavioral factors in interactive training of
text classifiers. In Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 563?567.
[Settles2011] Burr Settles. 2011. Closing the loop:
Fast, interactive semi-supervised annotation with
queries on features and instances. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1467?1478.
[Tumasjan et al2010] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2010. Predicting elections with Twitter: What 140
characters reveal about political sentiment. In Pro-
ceedings of the fourth international AAAI confer-
ence on weblogs and social media, pages 178?185.
41
[Tumasjan et al2011] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2011. Election forecasts with Twitter how 140 char-
acters reflect the political landscape. Social Science
Computer Review, 29(4):402?418.
42
