Coling 2010: Poster Volume, pages 108?116,
Beijing, August 2010
Benchmarking of Statistical Dependency Parsers for French
Marie Candito!, Joakim Nivre!, Pascal Denis! and Enrique Henestroza Anguiano!
! Alpage (Universit? Paris 7/INRIA)
! Uppsala University, Department of Linguistics and Philology
marie.candito@linguist.jussieu.fr {pascal.denis, henestro}@inria.fr joakim.nivre@ling?l.uu.se
Abstract
We compare the performance of three
statistical parsing architectures on the
problem of deriving typed dependency
structures for French. The architectures
are based on PCFGs with latent vari-
ables, graph-based dependency parsing
and transition-based dependency parsing,
respectively. We also study the in?u-
ence of three types of lexical informa-
tion: lemmas, morphological features,
and word clusters. The results show that
all three systems achieve competitive per-
formance, with a best labeled attachment
score over 88%. All three parsers bene?t
from the use of automatically derived lem-
mas, while morphological features seem
to be less important. Word clusters have a
positive effect primarily on the latent vari-
able parser.
1 Introduction
In this paper, we compare three statistical parsers
that produce typed dependencies for French. A
syntactic analysis in terms of typed grammatical
relations, whether encoded as functional annota-
tions in syntagmatic trees or in labeled depen-
dency trees, appears to be useful for many NLP
tasks including question answering, information
extraction, and lexical acquisition tasks like collo-
cation extraction.
This usefulness holds particularly for French,
a language for which bare syntagmatic trees
are often syntactically underspeci?ed because
of a rather free order of post-verbal comple-
ments/adjuncts and the possibility of subject in-
version. Thus, the annotation scheme of the
French Treebank (Abeill? and Barrier, 2004)
makes use of ?at syntagmatic trees without VP
nodes, with no structural distinction between
complements, adjuncts or post-verbal subjects,
but with additional functional annotations on de-
pendents of verbs.
Parsing is commonly enhanced by using more
abstract lexical information, in the form of mor-
phological features (Tsarfaty, 2006), lemmas
(Seddah et al, 2010), or various forms of clusters
(see (Candito and Seddah, 2010) for references).
In this paper, we explore the integration of mor-
phological features, lemmas, and linear context
clusters.
Typed dependencies can be derived using many
different parsing architectures. As far as statistical
approaches are concerned, the dominant paradigm
for English has been to use constituency-based
parsers, the output of which can be converted
to typed dependencies using well-proven conver-
sion procedures, as in the Stanford parser (Klein
and Manning, 2003). In recent years, it has
also become popular to use statistical dependency
parsers, which are trained directly on labeled de-
pendency trees and output such trees directly, such
as MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006). Dependency parsing has been
applied to a fairly broad range of languages, espe-
cially in the CoNLL shared tasks in 2006 and 2007
(Buchholz and Marsi, 2006; Nivre et al, 2007).
We present a comparison of three statistical
parsing architectures that output typed dependen-
cies for French: one constituency-based architec-
ture featuring the Berkeley parser (Petrov et al,
2006), and two dependency-based systems using
radically different parsing methods, MSTParser
(McDonald et al, 2006) and MaltParser (Nivre et
al., 2006). These three systems are compared both
in terms of parsing accuracy and parsing times, in
realistic settings that only use predicted informa-
tion. By using freely available software packages
that implement language-independent approaches
108
and applying them to a language different from
English, we also hope to shed some light on the
capacity of different methods to cope with the
challenges posed by different languages.
Comparative evaluation of constituency-based
and dependency-based parsers with respect to la-
beled accuracy is rare, despite the fact that parser
evaluation on typed dependencies has been ad-
vocated for a long time (Lin, 1995; Carroll et
al., 1998). Early work on statistical dependency
parsing often compared constituency-based and
dependency-based methods with respect to their
unlabeled accuracy (Yamada and Matsumoto,
2003), but comparison of different approaches
with respect to labeled accuracy is more recent.
Cer et al (2010) present a thorough analysis of
the best trade-off between speed and accuracy in
deriving Stanford typed dependencies for English
(de Marneffe et al, 2006), comparing a number of
constituency-based and dependency-based parsers
on data from the Wall Street Journal. They con-
clude that the highest accuracy is obtained using
constituency-based parsers, although some of the
dependency-based parsers are more ef?cient.
For German, the 2008 ACL workshop on pars-
ing German (K?bler, 2008) featured a shared task
with two different tracks, one for constituency-
based parsing and one for dependency-based pars-
ing. Both tracks had their own evaluation metrics,
but the accuracy with which parsers identi?ed
subjects, direct objects and indirect objects was
compared across the two tracks, and the results
in this case showed an advantage for dependency-
based parsing.
In this paper, we contribute results for a
third language, French, by benchmarking both
constituency-based and dependency-based meth-
ods for deriving typed dependencies. In addi-
tion, we investigate the usefulness of morphologi-
cal features, lemmas and word clusters for each of
the different parsing architectures. The rest of the
paper is structured as follows. Section 2 describes
the French Treebank, and Section 3 describes the
three parsing systems. Section 4 presents the ex-
perimental evaluation, and Section 5 contains a
comparative error analysis of the three systems.
Section 6 concludes with suggestions for future
research.
2 Treebanks
For training and testing the statistical parsers, we
use treebanks that are automatically converted
from the French Treebank (Abeill? and Barrier,
2004) (hereafter FTB), a constituency-based tree-
bank made up of 12, 531 sentences from the Le
Monde newspaper. Each sentence is annotated
with a constituent structure and words bear the
following features: gender, number, mood, tense,
person, de?niteness, wh-feature, and clitic case.
Nodes representing dependents of a verb are la-
beled with one of 8 grammatical functions.1
We use two treebanks automatically obtained
from FTB, both described in Candito et al
(2010). FTB-UC is a modi?ed version of the
original constituency-based treebank, where the
rich morphological annotation has been mapped
to a simple tagset of 28 part-of-speech tags, and
where compounds with regular syntax are bro-
ken down into phrases containing several simple
words while remaining sequences annotated as
compounds in FTB are merged into a single token.
Function labels are appended to syntactic category
symbols and are either used or ignored, depending
on the task.
FTB-UC-DEP is a dependency treebank de-
rived from FTB-UC using the classic technique of
head propagation rules, ?rst proposed for English
by Magerman (1995). Function labels that are
present in the original treebank serve to label the
corresponding dependencies. The remaining un-
labeled dependencies are labeled using heuristics
(for dependents of non-verbal heads). With this
conversion technique, output dependency trees are
necessarily projective, and extracted dependen-
cies are necessarily local to a phrase, which means
that the automatically converted trees can be re-
garded as pseudo-projective approximations to the
correct dependency trees (Kahane et al, 1998).
Candito et al (2010) evaluated the converted trees
for 120 sentences, and report a 98% labeled at-
tachment score when comparing the automatically
converted dependency trees to the manually cor-
rected ones.
1These are SUJ (subject), OBJ (object), A-OBJ/DE-OBJ
(indirect object with preposition ? / de), P-OBJ (indirect
object with another preposition / locatives), MOD (modi?er),
ATS/ATO (subject/object predicative complement).
109
SNP-SUJ
DET
une
NC
lettre
VN
V
avait
VPP
?t?
VPP
envoy?e
NP-MOD
DET
la
NC
semaine
ADJ
derni?re
PP-A_OBJ
P+D
aux
NP
NC
salari?s
une lettre avait ?t? envoy?e la semaine derni?re aux salari?s
de
t
suj
au
x-t
ps
au
x-
pa
ss mod
de
t mod
a_obj
obj
Figure 1: An example of constituency tree of the FTB-UC (left), and the corresponding dependency tree
(right) for A letter had been sent the week before to the employees.
Figure 1 shows two parallel trees from FTB-UC
and FTB-UC-DEP. In all reported experiments in
this paper, we use the usual split of FTB-UC: ?rst
10% as test set, next 10% as dev set, and the re-
maining sentences as training set.
3 Parsers
Although all three parsers compared are statis-
tical, they are based on fairly different parsing
methodologies. The Berkeley parser (Petrov et
al., 2006) is a latent-variable PCFG parser, MST-
Parser (McDonald et al, 2006) is a graph-based
dependency parser, and MaltParser (Nivre et al,
2006) is a transition-based dependency parser.
The choice to include two different dependency
parsers but only one constituency-based parser is
motivated by the study of Seddah et al (2009),
where a number of constituency-based statisti-
cal parsers were evaluated on French, including
Dan Bikel?s implementation of the Collins parser
(Bikel, 2002) and the Charniak parser (Charniak,
2000). The evaluation showed that the Berke-
ley parser had signi?cantly better performance for
French than the other parsers, whether measured
using a parseval-style labeled bracketing F-score
or a CoNLL-style unlabeled attachment score.
Contrary to most of the other parsers in that study,
the Berkeley parser has the advantage of a strict
separation of parsing model and linguistic con-
straints: linguistic information is encoded in the
treebank only, except for a language-dependent
suf?x list used for handling unknown words.
In this study, we compare the Berkeley parser
to MSTParser and MaltParser, which have the
same separation of parsing model and linguistic
representation, but which are trained directly on
labeled dependency trees. The two dependency
parsers use radically different parsing approaches
but have achieved very similar performance for a
wide range of languages (McDonald and Nivre,
2007). We describe below the three architectures
in more detail.2
3.1 The Berkeley Parser
The Berkeley parser is a freely available imple-
mentation of the statistical training and parsing
algorithms described in (Petrov et al, 2006) and
(Petrov and Klein, 2007). It exploits the fact that
PCFG learning can be improved by splitting sym-
bols according to structural and/or lexical proper-
ties (Klein and Manning, 2003). Following Mat-
suzaki et al (2005), the Berkeley learning algo-
rithm uses EM to estimate probabilities on sym-
bols that are automatically augmented with la-
tent annotations, a process that can be viewed
as symbol splitting. Petrov et al (2006) pro-
posed to score the splits in order to retain only the
most bene?cial ones, and keep the grammar size
manageable: the splits that induce the smallest
losses in the likelihood of the treebank are merged
back. The algorithm starts with a very general
treebank-induced binarized PCFG, with order h
horizontal markovisation. created, where at each
level a symbol appears without track of its orig-
inal siblings. Then the Berkeley algorithm per-
forms split/merge/smooth cycles that iteratively
re?ne the binarized grammar: it adds two latent
annotations on each symbol, learns probabilities
for the re?ned grammar, merges back 50% of the
splits, and smoothes the ?nal probabilities to pre-
vent over?tting. All our experiments are run us-
ing BerkeleyParser 1.0,3 modi?ed for handling
2For replicability, models, preprocessing tools and ex-
perimental settings are available at http://alpage.
inria.fr/statgram/frdep.html.
3http://www.eecs.berkeley.edu/
\~petrov/berkeleyParser
110
French unknown words by Crabb? and Candito
(2008), with otherwise default settings (order 0
horizontal markovisation, order 1 vertical marko-
visation, 5 split/merge cycles).
The Berkeley parser could in principle be
trained on functionally annotated phrase-structure
trees (as shown in the left half of ?gure 1), but
Crabb? and Candito (2008) have shown that this
leads to very low performance, because the split-
ting of symbols according to grammatical func-
tions renders the data too sparse. Therefore, the
Berkeley parser was trained on FTB-UC without
functional annotation. Labeled dependency trees
were then derived from the phrase-structure trees
output by the parser in two steps: (1) function la-
bels are assigned to phrase structure nodes that
have functional annotation in the FTB scheme;
and (2) dependency trees are produced using the
same procedure used to produce the pseudo-gold
dependency treebank from the FTB (cf. Section 2).
The functional labeling relies on the Maximum
Entropy labeler described in Candito et al (2010),
which encodes the problem of functional label-
ing as a multiclass classi?cation problem. Specif-
ically, each class is of the eight grammatical func-
tions used in FTB, and each head-dependent pair
is treated as an independent event. The feature
set used in the labeler attempt to capture bilexi-
cal dependencies between the head and the depen-
dent (using stemmed word forms, parts of speech,
etc.) as well as more global sentence properties
like mood, voice and inversion.
3.2 MSTParser
MSTParser is a freely available implementation
of the parsing models described in McDonald
(2006). These models are often described as
graph-based because they reduce the problem
of parsing a sentence to the problem of ?nding
a directed maximum spanning tree in a dense
graph representation of the sentence. Graph-based
parsers typically use global training algorithms,
where the goal is to learn to score correct trees
higher than incorrect trees. At parsing time a
global search is run to ?nd the highest scoring
dependency tree. However, unrestricted global
inference for graph-based dependency parsing
is NP-hard, and graph-based parsers like MST-
Parser therefore limit the scope of their features
to a small number of adjacent arcs (usually two)
and/or resort to approximate inference (McDon-
ald and Pereira, 2006). For our experiments, we
use MSTParser 0.4.3b4 with 1-best projective de-
coding, using the algorithm of Eisner (1996), and
second order features. The labeling of dependen-
cies is performed as a separate sequence classi?-
cation step, following McDonald et al (2006).
To provide part-of-speech tags to MSTParser,
we use the MElt tagger (Denis and Sagot, 2009),
a Maximum Entropy Markov Model tagger en-
riched with information from a large-scale dictio-
nary.5 The tagger was trained on the training set
to provide POS tags for the dev and test sets, and
we used 10-way jackkni?ng to generate tags for
the training set.
3.3 MaltParser
MaltParser6 is a freely available implementation
of the parsing models described in (Nivre, 2006)
and (Nivre, 2008). These models are often char-
acterized as transition-based, because they reduce
the problem of parsing a sentence to the prob-
lem of ?nding an optimal path through an abstract
transition system, or state machine. This is some-
times equated with shift-reduce parsing, but in
fact includes a much broader range of transition
systems (Nivre, 2008). Transition-based parsers
learn models that predict the next state given the
current state of the system, including features over
the history of parsing decisions and the input sen-
tence. At parsing time, the parser starts in an ini-
tial state and greedily moves to subsequent states
? based on the predictions of the model ? until a
terminal state is reached. The greedy, determinis-
tic parsing strategy results in highly ef?cient pars-
ing, with run-times often linear in sentence length,
and also facilitates the use of arbitrary non-local
features, since the partially built dependency tree
is ?xed in any given state. However, greedy in-
ference can also lead to error propagation if early
predictions place the parser in incorrect states. For
the experiments in this paper, we use MaltParser
4http://mstparser.sourceforge.net
5Denis and Sagot (2009) report a tagging accuracy of
97.7% (90.1% on unknown words) on the FTB-UC test set.
6http://www.maltparser.org
111
1.3.1 with the arc-eager algorithm (Nivre, 2008)
and use linear classi?ers from the LIBLINEAR
package (Fan et al, 2008) to predict the next state
transitions. As for MST, we used the MElt tagger
to provide input part-of-speech tags to the parser.
4 Experiments
This section presents the parsing experiments that
were carried out in order to assess the state of the
art in labeled dependency parsing for French and
at the same time investigate the impact of different
types of lexical information on parsing accuracy.
We present the features given to the parsers, dis-
cuss how they were extracted/computed and inte-
grated within each parsing architecture, and then
summarize the performance scores for the differ-
ent parsers and feature con?gurations.
4.1 Experimental Space
Our experiments focus on three types of lexical
features that are used either in addition to or as
substitutes for word forms: morphological fea-
tures, lemmas, and word clusters. In the case
of MaltParser and MSTParser, these features are
used in conjunction with POS tags. Motivations
for these features are rooted in the fact that French
has a rather rich in?ectional morphology.
The intuition behind using morphological fea-
tures like tense, mood, gender, number, and per-
son is that some of these are likely to provide ad-
ditional cues for syntactic attachment or function
type. This is especially true given that the 29 tags
used by the MElt tagger are rather coarse-grained.
The use of lemmas and word clusters, on the
other hand, is motivated by data sparseness con-
siderations: these provide various degrees of gen-
eralization over word forms. As suggested by Koo
et al (2008), the use of word clusters may also re-
duce the need for annotated data.
All our features are automatically produced:
no features except word forms originate from the
treebank. Our aim was to assess the performance
currently available for French in a realistic setting.
Lemmas Lemmatized forms are extracted us-
ing Lefff (Sagot, 2010), a large-coverage morpho-
syntactic lexicon for French, and a set of heuristics
for unknown words. More speci?cally, Lefff is
queried for each (word, pos), where pos is the
tag predicted by the MElt tagger. If the pair is
found, we use the longest lemma associated with
it in Lefff. Otherwise, we rely on a set of simple
stemming heuristics using the form and the pre-
dicted tag to produce the lemma. We use the form
itself for all other remaining cases.7
Morphological Features Morphological fea-
tures were extracted in a way similar to lemmas,
again by querying Lefff and relying on heuristics
for out-of-dictionary words. Here are the main
morphological attributes that were extracted from
the lexicon: mood and tense for verbs; person
for verbs and pronouns; number and gender for
nouns, past participles, adjectives and pronouns;
whether an adverb is negative; whether an adjec-
tive, pronoun or determiner is cardinal, ordinal,
de?nite, possessive or relative. Our goal was to
predict all attributes found in FTB that are recov-
erable from the word form alone.
Word Form Clusters Koo et al (2008) have
proposed to use unsupervised word clusters as
features in MSTParser, for parsing English and
Czech. Candito and Crabb? (2009) showed that,
for parsing French with the Berkeley parser, us-
ing the same kind of clusters as substitutes for
word forms improves performance. We now ex-
tend their work by comparing the impact of such
clusters on two additional parsers.
We use the word clusters computed by Can-
dito and Crabb? (2009) using Percy Liang?s im-
plementation8 of the Brown unsupervised cluster-
ing algorithm (Brown et al, 1992). It is a bottom-
up hierarchical clustering algorithm that uses a bi-
gram language model over clusters. The result-
ing cluster ids are bit-strings, and various lev-
els of granularity can be obtained by retaining
only the ?rst x bits. Candito and Crabb? (2009)
used the L?Est R?publicain corpus, a 125 mil-
lion word journalistic corpus.9 To reduce lexi-
7Candito and Seddah (2010) report the following cover-
age for the Lefff : around 96% of the tokens, and 80.1% of
the token types are present in the Lefff (leaving out punctua-
tion and numeric tokens, and ignoring case differences).
8http://www.eecs.berkeley.edu/~pliang/
software
9http://www.cnrtl.fr/corpus/
estrepublicain
112
cal data sparseness caused by in?ection, they ran
a lexicon-based stemming process on the corpus
that removes in?ection marks without adding or
removing lexical ambiguity. The Brown algo-
rithm was then used to compute 1000 clusters of
stemmed forms, limited to forms that appeared at
least 20 times.
We tested the use of clusters with different val-
ues for two parameters: nbbits = the cluster pre-
?x length in bits, to test varying granularities, and
minocc = the minimum number of occurrences in
the L?Est R?publicain corpus for a form to be re-
placed by a cluster or for a cluster feature to be
used for that form.
4.2 Parser-Specific Configurations
Since the three parsers are based on different ma-
chine learning algorithms and parsing algorithms
(with different memory requirements and parsing
times), we cannot integrate the different features
described above in exactly the same way. For the
Berkeley parser we use the setup of Candito and
Seddah (2010), where additional information is
encoded within symbols that are used as substi-
tutes for word forms. For MaltParser and MST-
Parser, which are based on discriminative models
that permit the inclusion of interdependent fea-
tures, additional information may be used either
in addition to or as substitutes for word forms.
Below we summarize the con?gurations that have
been explored for each parser:
? Berkeley:
1. Morphological features: N/A.
2. Lemmas: Concatenated with POS tags
and substituted for word forms.
3. Clusters: Concatenated with morpho-
logical suf?xes and substituted for word
forms; grid search for optimal values of
nbbits and minocc.
? MaltParser and MSTParser:
1. Morphological features: Added as
features.
2. Lemmas: Substituted for word forms
or added as features.
3. Clusters: Substituted for word forms or
added as features; grid search for opti-
mal values of nbbits and minocc.
4.3 Results
Table 1 summarizes the experimental results. For
each parser we give results on the development
set for the baseline (no additional features), the
best con?guration for each individual feature type,
and the best con?guration for any allowed combi-
nation of the three features types. For the ?nal
test set, we only evaluate the baseline and the best
combination of features. Scores on the test set
were compared using a ?2-test to assess statisti-
cal signi?cance: unless speci?ed, all differences
therein were signi?cant at p ? 0.01.
The MSTParser system achieves the best la-
beled accuracy on both the development set and
the test set. When adding lemmas, the best con-
?guration is to use them as substitutes for word
forms, which slightly improves the UAS results.
For the clusters, their use as substitutes for word
forms tends to degrade results, whereas using
them as features alone has almost no impact. This
means that we could not replicate the positive ef-
fect10 reported by Koo et al (2008) for English
and Czech. However, the best combined con-
?guration is obtained using lemmas instead of
words, a reduced set of morphological features,11
and clusters as features, with minocc=50, 000 and
nbbits=10.
MaltParser has the second best labeled accu-
racy on both the development set and the test set,
although the difference with Berkeley is not sig-
ni?cant on the latter. MaltParser has the lowest
unlabeled accuracy of all three parsers on both
datasets. As opposed to MSTParser, all three fea-
ture types work best for MaltParser when used in
addition to word forms, although the improvement
is statistically signi?cant only for lemmas and
clusters. Again, the best model uses all three types
of features, with cluster features minocc=600 and
nbbits=7. MaltParser shows the smallest discrep-
ancy from unlabeled to labeled scores. This might
be because it is the only architecture where label-
ing is directly done as part of parsing.
10Note that the two experiments cannot be directly com-
pared. Koo et al (2008) use their own implementation of an
MST parser, which includes extra second-order features (e.g.
grand-parent features on top of sibling features).
11As MSTParser training is memory-intensive, we re-
moved the features containing information already encoded
part-of-speech tags.
113
Development Set Test Set
Baseline Morpho Lemma Cluster Best Baseline Best
Parser LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
Berkeley 85.1 89.3 ? ? 85.9 90.0 86.5 90.8 86.5 90.8 85.6 89.6 86.8 91.0
MSTParser 87.2 90.0 87.2 90.2 87.2 90.1 87.2 90.1 87.5 90.3 87.6 90.3 88.2 90.9
MaltParser 86.2 89.0 86.3 89.0 86.6 89.2 86.5 89.2 86.9 89.4 86.7 89.3 87.3 89.7
Table 1: Experimental results for the three parsing systems. LAS=labeled accuracy, UAS=unlabeled accuracy, for sentences
of any length, ignoring punctuation tokens. Morpho/Lemma/Cluster=best con?guration when using morphological features
only (resp. lemmas only, clusters only), Best=best con?guration using any combination of these.
For Berkeley, the lemmas improve the results
over the baseline, and its performance reaches that
of MSTParser for unlabeled accuracy (although
the difference between the two parsers is not sig-
ni?cant on the test set). The best setting is ob-
tained with clusters instead of word forms, using
the full bit strings. It also gives the best unlabeled
accuracy of all three systems on both the devel-
opment set and the test set. For the more impor-
tant labeled accuracy, the point-wise labeler used
is not effective enough.
Overall, MSTParser has the highest labeled ac-
curacy and Berkeley the highest unlabeled ac-
curacy. However, results for all three systems
on the test set are roughly within one percent-
age point for both labeled and unlabeled ac-
curacy, which means that we do not ?nd the
same discrepancy between constituency-based
and dependency-based parser that was reported
for English by Cer et al (2010).
Table 2 gives parsing times for the best con?g-
uration of each parsing architecture. MaltParser
runs approximately 9 times faster than the Berke-
ley system, and 10 times faster than MSTParser.
The difference in ef?ciency is mainly due to the
fact that MaltParser uses a linear-time parsing al-
gorithm, while the other two parsers have cubic
time complexity. Given the rather small differ-
ence in labeled accuracy, MaltParser seems to be
a good choice for processing very large corpora.
5 Error Analysis
We provide a brief analysis of the errors made by
the best performing models for Berkeley, MST-
Parser and MaltParser on the development set, fo-
cusing on labeled and unlabeled attachment for
nouns, prepositions and verbs. For nouns, Berke-
Bky Malt MST
Tagging _ 0:27 0:27
Parsing 12:19 0:58 (0:18) 14:12 (12:44)
Func. Lab. 0:23 _ _
Dep. Conv. 0:4 _ _
Total 12:46 1:25 14:39
Table 2: Parsing times (min:sec) for the dev set, for the
three architectures, on an imac 2.66GHz. The ?gures within
brackets show the pure parsing time without the model load-
ing time, when available.
ley has the best unlabeled attachment, followed by
MSTParser and then MaltParser, while for labeled
attachment Berkeley and MSTParser are on a par
with MaltParser a bit behind. For prepositions,
MSTParser is by far the best for both labeled and
unlabeled attachment, with Berkeley and Malt-
Parser performing equally well on unlabeled at-
tachment and MaltParser performing better than
Berkeley on labeled attachment.12 For verbs,
Berkeley has the best performance on both labeled
and unlabeled attachment, with MSTParser and
MaltParser performing about equally well. Al-
though Berkeley has the best unlabeled attach-
ment overall, it also has the worst labeled attach-
ment, and we found that this is largely due to the
functional role labeler having trouble assigning
the correct label when the dependent is a prepo-
sition or a clitic.
For errors in attachment as a function of word
distance, we ?nd that precision and recall on de-
pendencies of length > 2 tend to degrade faster
for MaltParser than for MSTParser and Berkeley,
12In the dev set, for MSTParser, 29% of the tokens that
do not receive the correct governor are prepositions (883 out
of 3051 errors), while these represent 34% for Berkeley (992
out of 2914), and 30% for MaltParser (1016 out of 3340).
114
with Berkeley being the most robust for depen-
dencies of length > 6. In addition, Berkeley is
best at ?nding the correct root of sentences, while
MaltParser often predicts more than one root for a
given sentence. The behavior of MSTParser and
MaltParser in this respect is consistent with the re-
sults of McDonald and Nivre (2007).
6 Conclusion
We have evaluated three statistical parsing ar-
chitectures for deriving typed dependencies for
French. The best result obtained is a labeled at-
tachment score of 88.2%, which is roughly on a
par with the best performance reported by Cer et
al. (2010) for parsing English to Stanford depen-
dencies. Note two important differences between
their results and ours: First, the Stanford depen-
dencies are in a way deeper than the surface de-
pendencies tested in our work. Secondly, we ?nd
that for French there is no consistent trend fa-
voring either constituency-based or dependency-
based methods, since they achieve comparable re-
sults both for labeled and unlabeled dependencies.
Indeed, the differences between parsing archi-
tectures are generally small. The best perfor-
mance is achieved using MSTParser, enhanced
with predicted part-of-speech tags, lemmas, mor-
phological features, and unsupervised clusters of
word forms. MaltParser achieves slightly lower
labeled accuracy, but is probably the best option
if speed is crucial. The Berkeley parser has high
accuracy for unlabeled dependencies, but the cur-
rent labeling method does not achieve a compara-
bly high labeled accuracy.
Examining the use of lexical features, we ?nd
that predicted lemmas are useful in all three ar-
chitectures, while morphological features have a
marginal effect on the two dependency parsers
(they are not used by the Berkeley parser). Unsu-
pervised word clusters, ?nally, give a signi?cant
improvement for the Berkeley parser, but have a
rather small effect for the dependency parsers.
Other results for statistical dependency pars-
ing of French include the pilot study of Candito
et al (2010), and the work ofSchluter and van
Genabith (2009), which resulted in an LFG sta-
tistical French parser. However, the latter?s re-
sults are obtained on a modi?ed subset of the FTB,
and are expressed in terms of F-score on LFG f-
structure features, which are not comparable to
our attachment scores. There also exist a num-
ber of grammar-based parsers, evaluated on gold
test sets annotated with chunks and dependen-
cies (Paroubek et al, 2005; de la Clergerie et al,
2008). Their annotation scheme is different from
that of the FTB, but we plan to evaluate the statis-
tical parsers on the same data in order to compare
the performance of grammar-based and statistical
approaches.
Acknowledgments
The ?rst, third and fourth authors? work was sup-
ported by ANR Sequoia (ANR-08-EMER-013).
We are grateful to our anonymous reviewers for
their comments.
References
Abeill?, A. and N. Barrier. 2004. Enriching a french
treebank. In LREC?04.
Bikel, D. M. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In HLT-02.
Brown, P., V. Della Pietra, P. Desouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4).
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In CoNLL
2006.
Candito, M. and B. Crabb?. 2009. Improving gener-
ative statistical parsing with semi-supervised word
clustering. In IWPT?09.
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In NAACL/HLT Workshop SPMRL 2010.
Candito, M., B. Crabb?, and P. Denis. 2010. Statis-
tical french dependency parsing : treebank conver-
sion and ?rst results. In LREC 2010.
Carroll, J., E. Briscoe, and A. San?lippo. 1998. Parser
evaluation: A survey and a new proposal. In LREC
1998.
Cer, D., M.-C. de Marneffe, D. Jurafsky, and C. Man-
ning. 2010. Parsing to stanford dependencies:
Trade-offs between speed and accuracy. In LREC
2010.
Charniak, E. 2000. A maximum entropy inspired
parser. In NAACL 2000.
115
Crabb?, B. and M. Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
TALN 2008.
de la Clergerie, E. V., C. Ayache, G. de Chalendar,
G. Francopoulo, C. Gardent, and P. Paroubek. 2008.
Large scale production of syntactic annotations for
french. In First International Workshop on Auto-
mated Syntactic Annotations for Interoperable Lan-
guage Resources.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC 2006.
Denis, P. and B. Sagot. 2009. Coupling an an-
notated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC 2009.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In COLING
1996.
Fan, R.-E., K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classi?cation. Journal of Machine Learning
Research, 9.
Kahane, S., A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In ACL/COLING
1998.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL 2003.
Koo, T., X. Carreras, and M. Collins. 2008. Sim-
ple semi-supervised dependency parsing. In ACL-
08:HLT.
K?bler, S. 2008. The PaGe 2008 shared task on pars-
ing german. In ACL-08 Workshop on Parsing Ger-
man.
Lin, D. 1995. A dependency-based method for evalu-
ating broad-coverage parsers. In IJCAI-95.
Magerman, D. M. 1995. Statistical decision-tree mod-
els for parsing. In ACL 1995.
Matsuzaki, T., Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL 2005.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL 2007.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL 2006.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In CoNLL 2006.
McDonald, R. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, J., Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In LREC 2006.
Nivre, J., J. Hall, S. K?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In CoNLL
Shared Task of EMNLP-CoNLL 2007.
Nivre, J. 2006. Inductive Dependency Parsing.
Springer.
Nivre, J. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Lin-
guistics, 34.
Paroubek, P., L.-G. Pouillot, I. Robba, and A. Vilnat.
2005. Easy : Campagne d??valuation des analy-
seurs syntaxiques. In TALN 2005, EASy workshop :
campagne d??valuation des analyseurs syntaxiques.
Petrov, S. and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL-07: HLT.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL 2006.
Sagot, B. 2010. The Lefff, a freely available and large-
coverage morphological and syntactic lexicon for
french. In LREC 2010.
Schluter, N. and J. van Genabith. 2009. Dependency
parsing resources for french: Converting acquired
lfg f-structure. In NODALIDA 2009.
Seddah, D., M. Candito, and B. Crabb?. 2009. Cross
parser evaluation and tagset variation: a french tree-
bank study. In IWPT 2009.
Seddah, D., G. Chrupa?a, O. Cetinoglu, J. van Gen-
abith, and M. Candito. 2010. Lemmatization and
statistical lexicalized parsing of morphologically-
rich languages. In NAACL/HLT Workshop SPMRL
2010.
Tsarfaty, R. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In COL-
ING/ACL 2006 Student Research Workshop.
Yamada, H. and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT 2003.
116
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1222?1233,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Parse Correction with Specialized Models for Difficult Attachment Types
Enrique Henestroza Anguiano and Marie Candito
Alpage (Universite? Paris Diderot / INRIA)
Paris, France
henestro@inria.fr, marie.candito@linguist.jussieu.fr
Abstract
This paper develops a framework for syntac-
tic dependency parse correction. Dependen-
cies in an input parse tree are revised by se-
lecting, for a given dependent, the best gov-
ernor from within a small set of candidates.
We use a discriminative linear ranking model
to select the best governor from a group of
candidates for a dependent, and our model in-
cludes a rich feature set that encodes syntac-
tic structure in the input parse tree. The parse
correction framework is parser-agnostic, and
can correct attachments using either a generic
model or specialized models tailored to dif-
ficult attachment types like coordination and
pp-attachment. Our experiments show that
parse correction, combining a generic model
with specialized models for difficult attach-
ment types, can successfully improve the qual-
ity of predicted parse trees output by sev-
eral representative state-of-the-art dependency
parsers for French.
1 Introduction
In syntactic dependency parse correction, attach-
ments in an input parse tree are revised by selecting,
for a given dependent, the best governor from within
a small set of candidates. The motivation behind
parse correction is that attachment decisions, espe-
cially traditionally difficult ones like pp-attachment
and coordination, may require substantial contextual
information in order to be made accurately. Because
syntactic dependency parsers predict the parse tree
for an entire sentence, they may not be able to take
into account sufficient context when making attach-
ment decisions, due to computational complexity.
Assuming nonetheless that a predicted parse tree is
mostly accurate, parse correction can revise difficult
attachments by using the predicted tree?s syntactic
structure to restrict the set of candidate governors
and extract a rich set of features to help select among
them. Parse correction is also appealing because it
is parser-agnostic: it can be trained to correct the
output of any dependency parser.
In Section 2 we discuss work related to parse
correction, pp-attachment and coordination resolu-
tion. In Section 3 we discuss dependency struc-
ture and various statistical dependency parsing ap-
proaches. In Section 4 we introduce the parse cor-
rection framework, and Section 5 describes the fea-
tures and learning model used in our implementa-
tion. In Section 6 we present experiments in which
parse correction revises the predicted parse trees of
four state-of-the-art dependency parsers for French.
We provide concluding remarks in Section 7.
2 Related Work
Previous research directly concerning parse correc-
tion includes that of Attardi and Ciaramita (2007),
working on English and Swedish, who use an ap-
proach that considers a fixed set of revision rules:
each rule describes movements in the parse tree
leading from a dependent?s original governor to a
new governor, and a classifier is trained to select
the correct revision rule for a given dependent. One
drawback of this approach is that the classes lack
semantic coherence: a sequence of movements does
not necessarily have the same meaning across differ-
1222
ent syntactic trees. Hall and Nova?k (2005), working
on Czech, define a neighborhood of candidate gov-
ernors centered around the original governor of a de-
pendent, and a Maximum Entropy model determines
the probability of each candidate-dependent attach-
ment. We follow primarily from their work in our
use of neighborhoods to delimit the set of candidate
governors. Our main contributions are: specialized
corrective models for difficult attachment types (co-
ordination and pp-attachment) in addition to a gen-
eral corrective model; more sophisticated features,
feature combinations, and feature selection; and a
ranking model trained directly to select the true gov-
ernor from among a set of candidates.
There has also been other work on techniques
similar to parse correction. Attardi and Dell?Orletta
(2009) investigate reverse revision: a left-to-right
transition-based model is first used to parse a sen-
tence, then a right-to-left transition-based model is
run with additional features taken from the left-to-
right model?s predicted parse. This approach leads
to improved parsing results on a number of lan-
guages. While their approach is similar to parse cor-
rection in that it uses a predicted parse to inform a
subsequent processing step, this information is used
to improve a second parser rather than a model for
correcting errors. McDonald and Pereira (2006)
consider a method for recovering non-projective at-
tachments from a graph representation of a sentence,
in which an optimal projective parse tree has been
identified. The parse tree?s edges are allowed to be
rearranged in ways that introduce non-projectivity
in order to increase its overall score. This rearrange-
ment approach resembles parse correction because
it is a second step that can revise attachments made
in the first step, but it differs in a number of ways: it
is dependent on a graph-based parsing approach, it
does not model errors made by the parser, and it can
only output non-projective variants of the predicted
parse tree.
As a process that revises the output of a syntac-
tic parser, parse reranking is also similar to parse
correction. A well-studied subject (e.g. the work
of Charniak and Johnson (2005) and of Collins and
Koo (2005)), parse reranking is concerned with the
reordering of n-best ranked parse trees output by
a syntactic parser. Parse correction has a num-
ber of advantages compared to reranking: it can be
used with parsers that do not output n-best ranked
parses, it can be easily restricted to specific attach-
ment types, and its output space of parse trees is not
limited to those appearing in an n-best list. How-
ever, parse reranking has the advantage of selecting
the globally optimal parse for a sentence from an n-
best list, while parse correction makes only locally
optimal revisions in the predicted parse for a sen-
tence.
2.1 Difficult Attachment Types
Research on pp-attachment traditionally formulates
the problem in isolation, as in the work of Pantel and
Lin (2000) and of Olteanu and Moldovan (2005).
Examples consist of tuples of the form (v, n1, p, n2),
where either v or n1 is the true governor of the
pp comprising p and n2, and the task is to choose
between v and n1. Recently, Atterer and Schu?tze
(2007) have criticized this formulation as unrealistic
because it uses an oracle to select candidate gover-
nors, and they find that successful approaches for
the isolated problem perform no better than state-
of-the-art parsers on pp-attachment when evaluated
on full sentences. With parse correction, candi-
date governors are identified automatically with no
(v, n1, p, n2) restriction, and for several representa-
tive parsers we find that parse correction improves
pp-attachment performance.
Research on coordination resolution has also of-
ten formulated the problem in isolation. Resnik
(1999) uses semantic similarity to resolve noun-
phrase coordination of the form (n1, cc, n2, n3),
where the coordinating conjunction cc coordinates
either the heads n1 and n2 or the heads n1 and
n3. The same criticism as the one made by At-
terer and Schu?tze (2007) for pp-attachment might
be applied to this approach to coordination reso-
lution. In another formulation, the input consists
of a raw sentence, and coordination structure is
then detected and disambiguated using discrimina-
tive learning models (Shimbo and Hara, 2007) or
coordination-specific parsers (Hara et al, 2009). Fi-
nally, other work has focused on introducing spe-
cialized features for coordination into existing syn-
tactic parsing models (Hogan, 2007). Our approach
is novel with respect to previous work by directly
modeling the correction of coordination errors made
by general-purpose dependency parsers.
1223
ouvrit
Elle porte
la
avec
cle?
la
Figure 1: An unlabeled dependency tree for: Elle ouvrit
la porte avec la cle?. (She opened the door with the key).
3 Dependency Parsing
Dependency syntax involves the representation of
syntactic information for a sentence in the form a
directed graph, whose edges encode word-to-word
relationships. An edge from a governor to a de-
pendent indicates, roughly, that the presence of the
dependent is syntactically legitimated by the gover-
nor. An important property of dependency syntax is
that each word, except for the root of the sentence,
has exactly one governor; dependency syntax is thus
represented by trees. Figure 1 shows an example
of an unlabeled dependency tree.1 For languages
like English or French, most sentences can be rep-
resented with a projective dependency tree: for any
edge from word g to word d, g dominates any inter-
vening word between g and d.
Dependency trees are appealing syntactic repre-
sentations, closer than constituency trees to the se-
mantic representations useful for NLP applications.
This is true even with the projectivity requirement,
which occasionally creates syntax-semantics mis-
matches. Dependency trees have recently seen a
surge of interest, particularly with the introduction
of supervised models for dependency parsing us-
ing linear classifiers. Such parsers fall into two
main categories: transition-based parsing and graph-
based parsing. Additionally, an alternative method
for obtaining the dependency parse for a sentence
is to parse the sentence with a constituency-based
parser and then use an automatic process to convert
the output into dependency structure.
1Edges are generally labeled with the surface grammatical
function that the dependent bears with respect to its governor.
In this paper we focus on unlabeled dependency parsing, setting
aside labeling as a separate task.
3.1 Transition-Based Parsing
In transition-based dependency parsing, whose sem-
inal works are that of Yamada and Matsumoto
(2003) and Nivre (2003), the parsing process ap-
plies a sequence of incremental actions, which typ-
ically manipulate a buffer position in the sentence
and a stack for built sub-structures. Actions are of
the type ?read word from buffer?, ?build a depen-
dency from node on top of the stack to node that
begins the buffer?, etc. In a greedy version of this
process, the action to apply at each step is determin-
istically chosen to be the best-scoring action accord-
ing to a classifier, which is trained on a dependency
treebank converted into sequences of actions. The
strengths of this framework are O(n) time complex-
ity and a lack of restrictions on the locality of fea-
tures. A major drawback is its greedy behavior: it
can potentially make difficult attachment decisions
early in the processing of a sentence, without being
able to reconsider them when more information be-
comes available. Beamed versions of the algorithm
(Johansson and Nugues, 2006) partially address this
problem, but still do not provide a global optimiza-
tion for selecting the output parse tree.
3.2 Graph-Based Parsing
In graph-based dependency parsing, whose seminal
work is that of McDonald et al (2005), the parsing
process selects the globally optimal parse tree from
a graph containing attachments (directed edges) be-
tween each pair of words (nodes) in a sentence.
It finds the k-best scoring parse trees, both during
training and at parse time, where the score of a tree
is the sum of the scores of its factors (consisting of
one or more linked edges). While large factors are
desirable for capturing sophisticated linguistic con-
straints, they come at the cost of time complexity:
for the projective case, adaptations of Eisner?s algo-
rithm (Eisner, 1996) are O(n3) for 1-edge factors
(McDonald et al, 2005) or sibling 2-edge factors
(McDonald and Pereira, 2006), and O(n4) for gen-
eral 2-edge factors (Carreras, 2007) or 3-edge fac-
tors (Koo and Collins, 2010).
3.3 Constituency-Based Parsing
Beyond the two main approaches to dependency
parsing, there is also the approach of constituency-
1224
based parsing followed by a conversion step to de-
pendency structure. We use the three-step parsing
architecture previously tested for French by Candito
et al (2010a): (i) A constituency parse tree is out-
put by the BerkeleyParser, which has been trained to
learn a probabilistic context-free grammar with la-
tent annotations (Petrov et al, 2006) that has parsing
time complexity O(n3) (Matsuzaki et al, 2005); (ii)
A functional role labeler using a Maximum Entropy
model adds functional annotations to links between
a verb and its dependents; (iii) Constituency trees
are automatically converted into projective depen-
dency trees, with remaining unlabeled dependencies
assigned labels using a rule-based approach.
3.4 Baseline Parsers
In this paper, we use the following baseline parsers:
MaltParser (Nivre et al, 2007) for transition-based
parsing; MSTParser (McDonald et al, 2005) (with
sibling 2-edge factors) and BohnetParser (Bohnet,
2010) (with general 2-edge factors) for graph-based
parsing; and BerkeleyParser (Petrov et al, 2006) for
constituency-based parsing.
For MaltParser and MSTParser, we use the best
settings from a benchmarking of parsers for French
(Candito et al, 2010b), except that we remove un-
supervised word clusters as features. The parsing
models are thus trained using features including pre-
dicted part-of-speech tags, lemmas and morpholog-
ical features. For BohnetParser, we trained a new
model using these same predicted features. For
BerkelyParser, which was included in the bench-
marking experiments, we trained a model using the
so-called ?desinflection? process that addresses data
sparseness due to morphological variation: both
at training and parsing time, terminal symbols are
word forms in which redundant morphological suf-
fixes are removed, provided the original part-of-
speech ambiguities are kept (Candito et al, 2010b).
All models are trained on the French Treebank
(FTB) (Abeille? and Barrier, 2004), consisting of
12,351 sentences from the Le Monde newspaper, ei-
ther ?desinflected? for the BerkeleyParser, or con-
verted to projective dependency trees (Candito et al,
2010a) for the three dependency-native parsers.2 For
2The projectivity constraint is linguistically valid for most
French parses: the authors report < 2% non-projective edges in
a hand-corrected subset of the converted FTB.
INPUT: Predicted parse tree T
LOOP: For each chosen dependent d ? D
? Identify candidates Cd from T
? Predict c? = argmax
c ? Cd
S(c, d, T )
? Update T{gov(d) ? c?}
OUTPUT: Corrected version of parse tree T
Figure 2: The parse correction algorithm.
the dependency-native models, features include pre-
dicted part-of-speech (POS) tags from the MElt tag-
ger (Denis and Sagot, 2009), as well as predicted
lemmas and morphological features from the Lefff
lexicon (Sagot, 2010). These models constitute the
state-of-the-art for French dependency parsing: un-
labeled attachment scores (UAS) on the FTB test set
are 89.78% for MaltParser, 91.04% for MSTParser,
91.78% for BohnetParser, and 90.73% for Berkeley-
Parser.
4 Parse Correction
The parse correction algorithm is a post-processing
step to dependency parsing, where attachments from
the predicted parse tree of a sentence are corrected
by considering alternative candidate governors for
each dependent. This process can be useful for at-
tachments made too early in transition-based pars-
ing, or with features that are too local in MST-based
parsing.
The input is the predicted parse T of a sentence.
From T a set D of dependent nodes are chosen for
attachment correction. For each d ? D in left-to-
right sentence order, a set Cd of candidate governors
from T is identified, and then the highest scoring
c ? Cd, using a function S(c, d, T ), is assigned as
the new governor of d in T . Pseudo-code for parse
correction is shown in Figure 2.3
3Contrary to Hall and Nova?k (2005), our iterative algorithm
(along with the fact that Cd never includes nodes that are domi-
nated by d) ensures that corrected structures are trees, so it does
not require additional processing to eliminate cycles and pre-
serve connectivity.
1225
4.1 Choosing Dependents
Various criteria may be used to choose the set D
of dependents to correct. In the work of Hall and
Nova?k (2005) and of Attardi and Ciaramita (2007),
D contains all nodes in the input parse tree. How-
ever, one advantage of parse correction is its ability
to focus on specific attachment types, so an addi-
tional criterion for choosing dependents is to look
separately at those dependents that correspond to
difficult attachment types.
Analyzing errors made by the dependency parsers
introduced in Section 3 on the development set of
the FTB, we observe that two major sources of er-
ror across different parsers are coordination and pp-
attachment. Coordination accounts for around 10%
of incorrect attachments and has an error rate rang-
ing from 30 ? 40%, while pp-attachment accounts
for around 30% of incorrect attachments and has an
error rate of around 15%.
In this paper, we pay special attention to coordina-
tion and pp-attachment. Given the FTB annotation
scheme, coordination can be corrected by changing
the governor (first conjunct) of the coordinating con-
junction that governs the second conjunct, and pp-
attachment can be corrected by changing the gover-
nor of the preposition that heads the pp.4 We thus
train specialized corrective models for when the de-
pendents are coordinating conjunctions and preposi-
tions, in addition to a generic corrective model that
can be applied to any dependent.5
4.2 Identifying Candidate Governors
The set of candidate governors Cd for a dependent
d can be chosen in different ways. One method is
to let every other node in T be a candidate gover-
nor for d. However, parser error analysis has shown
that errors often occur in local contexts. Hall and
Nova?k (2005) define a neighborhood as a set of
nodes Nm(d) around the original predicted gover-
nor co of d, where Nm(d) includes all nodes in the
4The FTB handles pp-attachment in a typical fashion, but
coordination may be handled differently by other schemes (e.g.
the coordinating conjunction governs both conjuncts).
5In our experiments, we never revise punctuation and clitic
dependents. Since punctuation attachments mostly carry little
meaning, they are often annotated inconsistently and ignored
in parsing evaluations (including ours). Clitics are not revised
because they have a very low attachment error rate (2%).
parse tree T within graph distance m of d that pass
through co. They find that around 2/3 of the incor-
rect attachments in the output of Czech parses can be
corrected by selecting the best governor from within
N3(d). Similarly, in oracle experiments reported in
section 6, we find that around 1/2 of coordination
and pp-attachments in the output of French parses
can be corrected by selecting the best governor from
within N3(d). We thus use neighborhoods to delimit
the set of candidate governors.
While one can simply assign Cd ? Nm(d), we
add additional restrictions. First, in order to preserve
projectivity within T , we keep in Cd only those c
such that the update T{gov(d) ? c} would result
in a projective tree.6 Additionally, we discard candi-
dates with certain POS categories that are very un-
likely to be governors: clitics and punctuation are
always discarded, while determiners are discarded if
the dependent is a preposition.
4.3 Scoring Candidate Governors
A new governor c? for a dependent d is predicted by
selecting the highest scoring candidate c ? Cd ac-
cording to a function S(c, d, T ), which takes into
account features over c, d, and the parse tree T . We
use a linear model for our scoring function, which
allows for relatively fast training and prediction. Our
scoring function uses a weight vector ~w ? F, where
F is the feature space for dependents we wish to cor-
rect (either generic, or specialized for prepositions
or for coordinating conjunction), as well as the map-
ping ? : C?D?T ? F from combinations of candi-
date c ? C, dependent d ? D, and parse tree T ? T,
to vectors in the feature space F. The scoring func-
tion returns the inner product of ~w and ?(c, d, T ):
S(c, d, T ) = ~w ? ?(c, d, T ) (1)
4.4 Algorithm Complexity
The time complexity of our algorithm is O(n) in
the length n of the input sentence, which is consis-
tent with past work on parse correction by Hall and
Nova?k (2005) and by Attardi and Ciaramita (2007).
6We also keep candidates that would lead to a non-projective
tree, as long as it would be projective if we ignored punctuation.
This relaxation of the projectivity constraint leads to better or-
acle scores while retaining the key linguistic properties of pro-
jectivity.
1226
Attachments for up to n dependents in a sentence
are deterministically corrected in one pass. For each
such dependent d, the algorithm uses a linear model
to select a new governor after extracting features for
a local set of candidate governors Cd, whose size
does not dependent on n in the average case.7 Lo-
cality in candidate governor identification and fea-
ture extraction preserves linear time complexity in
the overall algorithm.
5 Model Learning
We now discuss our training setup, features, and
learning approach for obtaining the weight vector ~w.
5.1 Training Setup
The parse correction training set pairs gold parse
trees with corresponding predicted parse trees out-
put by a syntactic parser, and it is obtained us-
ing a jackknifing procedure to automatically parse
the gold-annotated training section of a dependency
treebank with a syntactic dependency parser.
We extract separate training sets for each type of
dependent we wish to correct (generic, prepositions,
coordinating conjunctions). Given p, then for each
token d we wish to correct in a sentence in the train-
ing section, we note its true governor gd in the gold
parse tree of the sentence, identify a set of candidate
governors Cd in the predicted parse T , and get fea-
ture vectors {?(c, d, T ) : c ? Cd}.
5.2 Feature Space
In order to learn an effective scoring function, we
use a rich feature space F that encodes syntactic con-
text surrounding a candidate-dependent pair (c, d)
within a parse tree T . Our primary features are indi-
cator functions for realizations of linguistic or tree-
based feature classes.8 From these primary features
we generate more complex feature combinations of
length up to P , which are then added to F. Each
combo represents a set of one or more primary fea-
tures, and is an indicator function that fires if and
only if all of its members do.
7Degenerate parse trees (e.g. flat trees) could lead to cases
where |Cd|=n, but for linguistically coherent parse trees |Cd| is
rather O(km), where k is the average -arity of syntactic parse
trees and m is the neighborhood distance used.
8For instance, there is a binary feature that is 1 if feature
class ?POS of c? takes on the value ?verb?, and 0 otherwise.
5.2.1 Primary Feature Classes
The primary feature classes we use are listed be-
low, grouped into categories corresponding to their
use in different corrective models (dobj is the object
of the dependent, cgov is the governor of the candi-
date, and cd?1 and cd+1 are the closest dependents
of c linearly to the left and right, respectively, of d).
Generic features (always included)
? POS, lemma, and number of dependents of c
? POS and dependency label of cd?1
? POS and dependency label of cd+1
? POS of cgov
? POS and lemma of d
? POS of dobj and whether dobj has a determiner
? Whether c is the predicted governor of d
? Binned linear distance between c and d
? Linear direction of c with respect to d
? POS sequence for nodes on path from c to d
? Graph distance between c and d
? Whether there is punctuation between c and d
Features exclusive to coordination
Whether d would coordinate two conjuncts that:
? Have the same POS
? Have the same word form
? Have number agreement
? Are both nouns with the same cardinality
? Are both proper nouns or both common nouns
? Are both prepositions with the same word form
? Are both prepositions with object of same POS
Features exclusive to pp-attachment
? Whether d immediately follows a punctuation
? Whether d heads a pp likely to be the agent of
a passive verb
? If c is a coordinating conjunction, then whether
c would coordinate two prepositions with the
same word form, and whether there is at least
one open-category word linearly between c and
d (in which case c is an unlikely governor)
1227
? If c is linearly after d, then whether there exists
a plausible rival candidate to the left of d (im-
plemented as whether there is a noun or adjec-
tive linearly before d, without any intervening
finite verb)
5.2.2 Feature Selection
Feature combos allow our models to effectively
sidestep linearity constraints, at the cost of an expo-
nential increase in the size of the feature space F. In
order to accommodate combos, we use feature se-
lection to help reduce the resulting space.
Our first feature selection technique is to apply a
frequency threshold: if a feature or a combo appears
less than K times among instances in our training
set, we remove it from F. In addition to making the
feature space more tractable, frequency thresholding
makes our scoring function less reliant on rare fea-
tures and combos.
Following frequency thresholding, we employ an
additional technique using conditional entropy (CE)
that we term CE-reduction. Let Y be a random vari-
able for whether or not an attachment is true, and
let A be a random variable for different combos that
can appear in an attachment. We calculate the CE of
a combo a with respect to Y as follows,
H(Y |A=a) = ?
?
y?Y
p(y|a) log p(y|a) (2)
where the probability p(y|a) is approximated from
the training set as freq(a, y)/freq(a), with exam-
ple balancing used here to account for more false at-
tachments (Y = 0) than true ones (Y = 1) in our train-
ing set. Having calculated the CE of each combo,
we remove from F those combos for which a subset
combo (or feature) exists with equal or lesser CE.
This eliminates any overly specific combo a when
the extra features encoded in a, compared to some
subset b, do not help a explain Y any better than b.
5.3 Ranking Model
The ranking setting for learning is used when a
model needs to discriminate between mutually ex-
clusive candidates that vary from instance to in-
stance. This is typically used in parse reranking
(Charniak and Johnson, 2005), where for each sen-
tence the model must select the correct parse from
within an n-best list. Denis and Baldridge (2007)
INPUT: Aggressiveness C, rounds R.
INITIALIZE: ~w0?(0, ..., 0), ~wavg?(0, ..., 0)
REPEAT: R times
LOOP: For t = 1, 2, . . . , |X|
? Get feature vectors {~xt,c : c ? Cdt}
? Get true governor gt ? Cdt
? Let ht = argmax
c?Cdt?{gt}
(~wt?1 ? ~xt,c)
? Let mt = (~wt?1 ? ~xt,gt)? (~wt?1 ? ~xt,ht)
IF: mt < 1
? Let ?t = min
{
C , 1?mt?~xt,gt?~xt,ht?2
}
? Set ~wt ? ~wt?1 + ?t(~xt,gt ? ~xt,ht)
ELSE:
? Set ~wt ? ~wt?1
? Set ~wavg ? ~wavg + ~wt
? Set ~w0 ? ~w|X|
OUTPUT: ~wavg/(R ? |X|)
Figure 3: Averaged PA-Ranking training algorithm.
also show that ranking outperforms a binary classifi-
cation approach to pronoun resolution (using a Max-
imum Entropy model), where for each pronominal
anaphor the model must select the correct antecedent
among candidates in a text.9
In our ranking approach to parse correction (PA-
Ranking), the weight vector is trained to select the
true governor from a set of candidates Cd for a de-
pendent d. The training set X is defined such that
the tth instance is a collection of feature vectors
{~xt,c = ?(c, dt, Tt) : c ? Cdt}, where Cdt is the
candidate set for the dependent dt within the pre-
dicted parse Tt, and the class is the true governor gt.
Instances in which gt 6? Cdt are discarded.
PA-Ranking training is carried out using a varia-
tion of the Passive-Aggressive algorithm (Crammer
et al, 2006), which has been adapted to the rank-
ing setting, implemented using the Polka library.10
For each training iteration t, the margin is defined as
9We considered a binary training approach to parse correc-
tion in which the model is trained to independently classify can-
didates as true or false governors, as used by Hall and Nova?k
(2005). However, we found that this approach performs no bet-
ter (and often worse) than the ranking approach, and is less ap-
propriate from a modeling standpoint.
10http://polka.gforge.inria.fr/
1228
mt = (~wt?1 ? ~xt,gt)? (~wt?1 ? ~xt,ht), where ht is the
highest scoring incorrect candidate. The algorithm
is passive because an update to the weight vector is
made if and only if mt < 1, either for incorrect pre-
dictions (mt < 0) or for correct predictions with in-
sufficient margin (0?mt<1). The new weight vec-
tor ~wt is as close as possible to ~wt?1, subject to the
aggressive constraint that the new margin be greater
than 1. We use weight averaging, so the final out-
put ~wavg is the average over the weight vectors after
each training step. Pseudo-code for the training al-
gorithm is shown in Figure 3. The rounds parameter
R determines the number of times to run through the
training set, and the aggressiveness parameter C sets
an upper limit on the update magnitude.
6 Experiments
We present experiments where we applied parse cor-
rection to the output of four state-of-the-art depen-
dency parsers for French. We conducted our eval-
uation on the FTB using the standard training, de-
velopment (dev), and test splits (containing 9,881,
1,235 and 1,235 sentences, respectively). To train
our parse correction models, we generated special-
ized training sets corresponding to each parser by
doing 10-fold jackknifing on the FTB training set
(cf. Section 5.1). Each parser was run on the FTB
dev and test sets, providing baseline unlabeled at-
tachment score (UAS) results and output parse trees
to be corrected.
6.1 Oracles and Neighborhood Size
To determine candidate neighborhood size, we con-
sidered an oracle scoring function that always se-
lects the true governor of a dependent if it appears
in the set of candidate governors, and otherwise se-
lects the predicted governor. Results for this oracle
on the dev set are shown in Table 1. The baseline
corresponds to m=1, where the oracle just selects
the predicted governor. Incrementing m to 2 and
to 3 resulted in substantial gains in oracle UAS, but
further incrementing m to 4 resulted in a relatively
small additional gain. We found that average can-
didate set size increases about linearly in m, so we
decided to use m=3 in order to have a high UAS up-
per bound without adding candidates that are very
unlikely to be true governors.
Neighborhood Size (m)
Base 2 3 4
Berkeley
Coords 67.2 76.5 82.8 84.8
Preps 82.9 88.5 92.2 93.2
Overall 90.1 94.0 96.0 96.5
Bohnet
Coords 70.1 80.6 85.6 87.7
Preps 85.4 89.4 93.4 94.5
Overall 91.2 94.4 96.1 96.6
Malt
Coords 60.9 72.2 78.2 80.5
Preps 82.6 88.1 92.6 93.7
Overall 89.3 93.2 95.1 95.8
MST
Coords 63.6 73.7 80.7 84.4
Preps 84.7 89.4 93.4 94.4
Overall 90.2 93.7 95.6 96.2
MST Overall Reranking top-100 parses: 95.4
Table 1: Parse correction oracle UAS (%) for differ-
ent neighborhood sizes, by dependent type (coordinating
conjunctions, prepositions, or all dependents). Also, a
reranking oracle for MSTParser using the top-100 parses.
We also compared the oracle for parse correc-
tion with an oracle for parse reranking, in which the
parse with the highest UAS for a sentence is selected
from the top-100 parses output by MSTParser. We
found that for MSTParser, the oracle for parse cor-
rection using neighborhood size m=3 (95.6% UAS)
is comparable to the oracle for parse reranking using
the top-100 parses (95.4% UAS). This is an encour-
aging result, showing that parse correction is capable
of the same improvement as parse reranking without
needing to process an n-best list of parses.
6.2 Feature Space Parameters
For the feature space F, we performed a grid search
to find good values for the parameters K (frequency
threshold), P (combo length), and CE-reduction.
We found that P=3 with CE-reduction allowed for
the most compactness without sacrificing correction
performance, for all of our corrective models. Ad-
ditionally, K=2 worked well for the coordinating
conjunction models, while K=10 worked well for
the preposition and generic models. CE-reduction
proved useful in greatly reducing the feature space
without lowering correction performance: it reduced
the size of the coordinating conjunction models from
400k to 65k features each, the preposition models
from 400k to 75k features each, and the generic
models from 800k to 200k features each.
1229
Corrective UAS (%)
Configuration Coords Preps Overall
Berkeley
Baseline 68.3 83.8 90.73
Generic 69.4 84.9* 91.13*
Specialized 71.5* 85.1* 91.23*
Bohnet
Baseline 70.5 86.1 91.78
Generic 71.2 86.4 91.88
Specialized 72.7* 86.2 91.88
Malt
Baseline 59.8 83.2 89.78
Generic 63.2* 84.5* 90.39*
Specialized 64.0* 85.0* 90.47*
MST
Baseline 60.5 85.9 91.04
Generic 64.2* 86.2 91.25*
Specialized 68.0* 86.2 91.36*
Table 2: Coordinating conjunction, preposition, and over-
all UAS (%) by corrective configuration on the test set.
Significant improvements over the baseline starred.
6.3 Corrective Configurations
For our evaluation of parse correction, we compared
two different configurations: generic (corrects all
dependents using the generic model) and specialized
(corrects coordinating conjunctions and prepositions
using their respective specialized models, and cor-
rects other dependents using the generic model).
The PA-Ranking aggressiveness parameter C was
set to 1 for our experiments, while the rounds pa-
rameter R was tuned separately for each corrective
model using the dev set. For our final tests, we ap-
plied each combination of parser + corrective con-
figuration by sequentially revising all dependents in
the output parse that had a relevant POS tag given
the corrective configuration. In the FTB test set,
this amounted to an evaluation over 5,706 prepo-
sition tokens, 801 coordinating conjunction tokens,
and 31,404 overall (non-punctuation) tokens.11
6.4 Results
Final results for the test set are shown in Table 2.
The overall UAS of each parser (except Bohnet-
Parser) was significantly improved under both cor-
rective configurations.12 The specialized configura-
11Since the MElt tagger and BerkeleyParser POS tagging ac-
curacies were around 97%, the sets of tokens considered for re-
vision differed slightly from the sets of tokens (with gold POS
tags) used to calculate UAS scores.
12We used McNemar?s Chi-squared test with p = 0.05 for all
significance tests.
tion performed as well as, and in most cases bet-
ter than, the generic configuration, indicating the
usefulness of specialized models and features for
difficult attachment types. Interestingly, the lower
the baseline parser?s UAS, the larger the overall
improvement from parse correction under the spe-
cialized configuration: MaltParser had the lowest
baseline and the highest error reduction (6.8%),
BerkeleyParser had the second-lowest baseline and
the second-highest error reduction (5.4%), MST-
Parser had the third-lowest baseline and the third-
highest error reduction (3.6%), and BohnetParser
had the highest baseline and the lowest error re-
duction (1.2%). It may be that the additional er-
rors made by a low-baseline parser, compared to a
high-baseline parser, involve relatively simpler at-
tachments that parse correction can better model.
Parse correction achieved significant improve-
ments for coordination resolution under the spe-
cialized configuration for each parser. MaltParser
and MSTParser had very low baseline coordinat-
ing conjunction UAS (around 60%), while Berke-
leyParser and BohnetParser had higher baselines
(around 70%). The highest error reduction was
achieved by MSTParser (19.0%), followed by Malt-
Parser (10.4%), BerkeleyParser (10.1%), and finally
BohnetParser (7.5%). The result for MSTParser was
surprising: although it had the second-highest base-
line overall UAS, it shared the lowest baseline coor-
dinating conjunction UAS and had the highest er-
ror reduction with parse correction. An explana-
tion for this result is that the annotation scheme for
coordination structure in the dependency FTB has
the first conjunct governing the coordinating con-
junction, which governs the second conjunct. Since
MSTParser is limited to sibling 2-edge factors (cf.
section 3), it is unable to jointly consider a full coor-
dination structure. BohnetParser, which uses general
2-edge factors, can consider full coordination struc-
tures and consequently has a much higher baseline
coordinating conjunction UAS than MSTParser.
Parse correction achieved significant but mod-
est improvements in pp-attachment performance un-
der the specialized configuration for MaltParser and
BerkeleyParser. However, parse correction did not
significantly improve pp-attachment performance
for MSTParser or BohnetParser, the two parsers that
had the highest baseline preposition UAS (around
1230
Modification Type
w?c c?w w?w Mods
Berkeley
Coords 40 14 33 10.9 %
Preps 118 39 41 3.5 %
Overall 228 67 104 1.3 %
Bohnet
Coords 32 15 33 10.0 %
Preps 52 46 32 2.3 %
Overall 150 121 130 1.1 %
Malt
Coords 55 21 56 16.5 %
Preps 149 50 76 4.8 %
Overall 390 172 293 2.4 %
MST
Coords 80 20 51 18.9 %
Preps 64 45 26 2.4 %
Overall 183 88 117 1.1 %
Table 3: Breakdown of modifications made under the
specialized configuration for each parser, by dependent
type. w?c is wrong-to-correct, c?w is correct-to-
wrong, w?w is wrong-to-wrong, and Mods is the per-
centage of tokens modified.
86%). These results are a bit disappointing, but they
suggest that there may be a performance ceiling for
pp-attachment beyond which rich lexical informa-
tion (syntactic and semantic) or full sentence con-
texts are needed. For English, the average human
performance on pp-attachment for the (v, n1, p, n2)
problem formulation is just 88.2% when given only
the four head-words, but increases to 93.2% when
given the full sentence (Ratnaparkhi et al, 1994).
If similar levels of human performance exist for
French, additional sources of information may be
needed to improve pp-attachment performance.
In addition to evaluating UAS improvements for
parse correction, we took a closer look at the best
corrective configuration (specialized) and analyzed
the types of attachment modifications made (Ta-
ble 3). In most cases there were around 2?3 times
as many error-correcting modifications (w?c) as
error-creating modifications (c?w), and the overall
% of tokens modified was very low overall (around
1-2%). Parse correction is thus conservative in the
number of modifications made, and rather accurate
when it does decide to modify an attachment.
Finally, we compared the running times of the
four parsers, as well as that of parse correction, on
the test set using a 2.66 GHz Intel Core 2 Duo ma-
chine. BerkeleyParser took 600s, BohnetParser took
450s using both cores (800s using a single core),
MaltParser took 45s, and MSTParser took 1000s. A
rough version of parse correction in the specialized
configuration took around 200s (for each parser). An
interesting result is that parse correction improves
MaltParser the most while retaining an overall time
complexity of O(n), compared to O(n3) or higher
for the other parsers. This suggests that linear-time
transition-based parsing and parse correction could
combine to form an attractive system that improves
parsing performance while retaining high speed.
7 Conclusion
We have developed a parse correction framework for
syntactic dependency parsing that uses specialized
models for difficult attachment types. Candidate
governors for a given dependent are identified in a
neighborhood around the predicted governor, and a
scoring function selects the best governor. We used
discriminative linear ranking models with features
encoding syntactic context, and we tested parse cor-
rection on coordination, pp-attachment, and generic
dependencies in the outputs of four representative
statistical dependency parsers for French. Parse cor-
rection achieved improvements in unlabeled attach-
ment score for three out of the four parsers, with
MaltParser seeing the greatest improvement. Since
both MaltParser and parse correction run in O(n)
time, a combined system could prove useful in situ-
ations where high parsing speed is required.
Future work on parse correction might focus on
developing specialized models for other difficult
attachment types, such as verb-phrase attachment
(verb dependents account for around 15% of incor-
rect attachments across all four parsers). Also, se-
lectional preferences and subcategorization frames
(from hand-built resources or extracted using distri-
butional methods) could make for useful features in
the pp-attachment corrective model; we suspect that
richer lexical information is needed in order to in-
crease the currently modest improvements achieved
by parse correction on pp-attachment.
Acknowledgments
We would like to thank Pascal Denis for his help us-
ing the Polka library, and Alexis Nasr for his advice
and comments. This work was partially funded by
the ANR project Sequoia ANR-08-EMER-013.
1231
References
A. Abeille? and N. Barrier. 2004. Enriching a French
treebank. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
Lisbon, Portugal, May.
G. Attardi and M. Ciaramita. 2007. Tree revision learn-
ing for dependency parsing. In Proceedings of the
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 388?
395, Rochester, New York, April.
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proceedings of the 2009 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 261?264, Boulder, Colorado,
June.
M. Atterer and H. Schu?tze. 2007. Prepositional phrase
attachment without oracles. Computational Linguis-
tics, 33(4):469?476.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 89?97, Beijing, China, August.
M. Candito, B. Crabbe?, and P. Denis. 2010a. Statistical
French dependency parsing: Treebank conversion and
first results. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation, Valetta, Malta, May.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010b. Benchmarking of statistical depen-
dency parsers for French. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 108?116, Beijing, China, August.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, pages
957?961, Prague, Czech Republic, June.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 173?180,
Ann Arbor, Michigan, June.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551?585.
P. Denis and J. Baldridge. 2007. A ranking approach
to pronoun resolution. In Proceedings of the 20th In-
ternational Joint Conference on Artifical intelligence,
pages 1588?1593, Hyderabad, India, January.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, Hong Kong, China, De-
cember.
J.M. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the 16th conference on Computational linguistics-
Volume 1, pages 340?345, Santa Cruz, California, Au-
gust.
K. Hall and V. Nova?k. 2005. Corrective modeling for
non-projective dependency parsing. In Proceedings
of the Ninth International Workshop on Parsing Tech-
nologies, pages 42?52, Vancouver, British Columbia,
October.
K. Hara, M. Shimbo, H. Okuma, and Y. Matsumoto.
2009. Coordinate structure analysis with global struc-
tural constraints and alignment-based local features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 967?975, Suntec, Singapore, Au-
gust.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, number 1, page 680,
Prague, Czech Republic, June.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 206?210, New York City, New
York, June.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1?11, Uppsala, Sweden, July.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 75?82, Ann Arbor, Michigan,
June.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 81?88, Trento, Italy, April.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 91?98, Ann
Arbor, Michigan, June.
1232
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02):95?135.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, pages 149?
160, Nancy, France, April.
M. Olteanu and D. Moldovan. 2005. PP-attachment dis-
ambiguation using large context. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 273?280, Vancouver, British Columbia, Octo-
ber.
P. Pantel and D. Lin. 2000. An unsupervised approach
to prepositional phrase attachment using contextually
similar words. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics, volume 38, pages 101?108, Hong Kong, October.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A max-
imum entropy model for prepositional phrase attach-
ment. In Proceedings of the Workshop on Human Lan-
guage Technology, pages 250?255, Plainsboro, New
Jersey, March.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to prob-
lems of ambiguity in natural language. Journal of Ar-
tificial Intelligence Research, 11(95):130.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceedings
of the Seventh International Conference on Language
Resources and Evaluation, Valetta, Malta, May.
M. Shimbo and K. Hara. 2007. A discriminative learn-
ing model for coordinate conjunctions. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 610?619, Prague,
Czech Republic, June.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy, France, April.
1233
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 743?753,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Strategies for Contiguous Multiword Expression Analysis and
Dependency Parsing
Marie Candito
Alpage
Paris Diderot Univ
INRIA
marie.candito@
linguist.univ-paris-diderot.fr
Matthieu Constant
Universite? Paris-Est
LIGM
CNRS
Matthieu.Constant@
u-pem.fr
Abstract
In this paper, we investigate various strate-
gies to predict both syntactic dependency
parsing and contiguous multiword expres-
sion (MWE) recognition, testing them on
the dependency version of French Tree-
bank (Abeille? and Barrier, 2004), as in-
stantiated in the SPMRL Shared Task
(Seddah et al, 2013). Our work focuses
on using an alternative representation of
syntactically regular MWEs, which cap-
tures their syntactic internal structure. We
obtain a system with comparable perfor-
mance to that of previous works on this
dataset, but which predicts both syntactic
dependencies and the internal structure of
MWEs. This can be useful for capturing
the various degrees of semantic composi-
tionality of MWEs.
1 Introduction
A real-life parsing system should comprise the
recognition of multi-word expressions (MWEs1),
first because downstream semantic-oriented ap-
plications need some marking in order to dis-
tinguish between regular semantic composition
and the typical semantic non-compositionality of
MWEs. Second, MWE information, is intuitively
supposed to help parsing.
That intuition is confirmed in a classical but
non-realistic setting in which gold MWEs are pre-
grouped (Arun and Keller, 2005; Nivre and Nils-
son, 2004; Eryig?it et al, 2011). But the situation
is much less clear when switching to automatic
MWE prediction. While Cafferkey et al (2007)
report a small improvement on the pure parsing
1Multiword expressions can be roughly defined as con-
tinuous or discontinuous sets of tokens, which either do not
exhibit full freedom in lexical selection or whose meaning is
not fully compositional. We focus in this paper on contiguous
multiword expressions, also known as ?words with spaces?.
task when using external MWE lexicons to help
English parsing, Constant et al (2012) report re-
sults on the joint MWE recognition and parsing
task, in which errors in MWE recognition allevi-
ate their positive effect on parsing performance.
While the realistic scenario of syntactic pars-
ing with automatic MWE recognition (either done
jointly or in a pipeline) has already been investi-
gated in constituency parsing (Green et al, 2011;
Constant et al, 2012; Green et al, 2013), the
French dataset of the SPMRL 2013 Shared Task
(Seddah et al, 2013) only recently provided the
opportunity to evaluate this scenario within the
framework of dependency syntax.2 In such a sce-
nario, a system predicts dependency trees with
marked groupings of tokens into MWEs. The
trees show syntactic dependencies between se-
mantically sound units (made of one or several
tokens), and are thus particularly appealing for
downstream semantic-oriented applications, as de-
pendency trees are considered to be closer to
predicate-argument structures.
In this paper, we investigate various strate-
gies for predicting from a tokenized sentence
both MWEs and syntactic dependencies, using the
French dataset of the SPMRL 13 Shared Task. We
focus on the use of an alternative representation
for those MWEs that exhibit regular internal syn-
tax. The idea is to represent these using regular
syntactic internal structure, while keeping the se-
mantic information that they are MWEs.
We devote section 2 to related work. In sec-
tion 3, we describe the French dataset, how MWEs
are originally represented in it, and we present
and motivate an alternative representation. Sec-
tion 4 describes the different architectures we test
2The main focus of the Shared Task was on pre-
dicting both morphological and syntactic analysis for
morphologically-rich languages. The French dataset is the
only one containing MWEs: the French treebank has the
particularity to contain a high ratio of tokens belonging to
a MWE (12.7% of non numerical tokens).
743
for predicting both syntax and MWEs. Section 5
presents the external resources targeted to improve
MWE recognition. We describe experiments and
discuss their results in section 6 and conclude in
section 7.
2 Related work
We gave in introduction references to previous
work on predicting MWEs and constituency pars-
ing. To our knowledge, the first works3 on predict-
ing both MWEs and dependency trees are those
presented to the SPMRL 2013 Shared Task that
provided scores for French (which is the only
dataset containing MWEs). Constant et al (2013)
proposed to combine pipeline and joint systems in
a reparser (Sagae and Lavie, 2006), and ranked
first at the Shared Task. Our contribution with
respect to that work is the representation of the
internal syntactic structure of MWEs, and use of
MWE-specific features for the joint system. The
system of Bjo?rkelund et al (2013) ranked second
on French, though with close UAS/LAS scores. It
is a less language-specific system that reranks n-
best dependency parses from 3 parsers, informed
with features from predicted constituency trees. It
uses no feature nor treatment specific to MWEs as
it focuses on the general aim of the Shared Task,
namely coping with prediction of morphological
and syntactic analysis.
Concerning related work on the representa-
tion of MWE internal structure, we can cite the
Prague Dependency Bank, which captures both
regular syntax of non-compositional MWEs and
their MWE status, in two distinct annotation lay-
ers (Bejc?ek and Stranak, 2010). Our represen-
tation also resembles that of light-verb construc-
tions (LVC) in the hungarian dependency treebank
(Vincze et al, 2010): the construction has regular
syntax, and a suffix is used on labels to express it
is a LVC (Vincze et al, 2013).
3 Data: MWEs in Dependency Trees
The data we use is the SPMRL 13 dataset for
French, in dependency format. It contains pro-
jective dependency trees that were automatically
derived from the latest status of the French Tree-
bank (Abeille? and Barrier, 2004), which con-
sists of constituency trees for sentences from the
3Concerning non contiguous MWEs, we can cite the work
of Vincze et al (2013), who experimented joint dependency
parsing and light verb construction identification.
newspaper Le Monde, manually annotated with
phrase structures, morphological information, and
grammatical functional tags for dependents of
verbs. The Shared Task used an enhanced version
of the constituency-to-dependency conversion of
Candito et al (2010), with different handling of
MWEs. The dataset consists of 18535 sentences,
split into 14759, 1235 and 2541 sentences for
training, development, and final evaluation respec-
tively.
We describe below the flat representation of
MWEs in this dataset, and the modified represen-
tation for regular MWEs that we propose.
a. Flat representation:
L? abus de biens sociaux fut de?nonce? en vain
suj
de
t
dep
cpd
dep cpd
dep cpd
au
x
tps
mod
dep
cpd
b. Structured representation:
L? abus de biens sociaux fut de?nonce? en vain
suj
det
dep
obj
.p
mod
au
x
tps
mod
dep
cpd
Figure 1: French dependency tree for L?abus de
biens sociaux fut de?nonce? en vain (literally the
misuse of assets social was denounced in vain,
meaning The misuse of corporate assets was de-
nounced in vain), containing two MWEs (in red).
Top: original flat representation. Bottom: Tree af-
ter regular MWEs structuring.
3.1 MWEs in Gold Data: Flat representation
In gold data, the MWEs appear in an expanded
flat format: each MWE bears a part-of-speech
and consists of a sequence of tokens (hereafter
the ?components? of the MWE), each having their
proper POS, lemma and morphological features.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node per MWE com-
ponent (more generally one node per token). The
first component of a MWE is taken as the head
of the MWE. All subsequent components of the
MWE depend on the first one, with the special
label dep_cpd (hence the name flat represen-
744
tation). Furthermore, the first MWE component
bears a feature mwehead equal to the POS of the
MWE. An example is shown in Figure 1. The
MWE en vain (pointlessly) is an adverb, contain-
ing a preposition and an adjective. The latter de-
pends on former, which bears mwehead=ADV+.
The algorithm to recover MWEs is: any node
having dependents with the dep_cpd label forms
a MWE with such dependents.
3.2 Alternative representation for regular
MWEs
In the alternative representation we propose, ir-
regular MWEs are unchanged and appear as flat
MWEs (e.g. en vain in Figure 1 has pattern prepo-
sition+adjective, which is not considered regular
for an adverb, and is thus unchanged). Regular
MWEs appear with ?structured? syntax: we mod-
ify the tree structure to recover the regular syn-
tactic dependencies. For instance, in the bottom
tree of the figure, biens is attached to the prepo-
sition, and the adjective sociaux is attached to bi-
ens, with regular labels. Structured MWEs can-
not be spotted using the tree topology and la-
bels only. Features are added for that purpose:
the syntactic head of the structured MWE bears
a regmwehead for the POS of the MWE (abus
in Figure 1), and the other components of the
MWE bear a regcomponent feature (the orange
tokens in Figure 1).4 With this representation,
the algorithm to recover regular MWEs is: any
node bearing regmwehead forms a MWE with
the set of direct or indirect dependents bearing a
regcomponent feature.
3.2.1 Motivations
Our first motivation is to increase the quantity of
information conveyed by the dependency trees,
by distinguishing syntactic regularity and seman-
tic regularity. Syntactically regular MWEs (here-
after regular MWEs) show various degrees of se-
mantic non-compositionality. For instance, in the
French Treebank, population active (lit. active
population, meaning ?working population?) is a
partially compositional MWE. Furthermore, some
sequences are both syntactically and semantically
regular, but encoded as MWE due to frozen lexi-
cal selection. This is the case for de?ficit budge?taire
(lit. budgetary deficit, meaning ?budget deficit?),
4The syntactic head of a structured MWE may not be the
first token, whereas the head token of a flat MWE is always
the first one.
because it is not possible to use de?ficit du bud-
get (budget deficit). Our alternative representa-
tion distinguishes between syntactic internal reg-
ularity and semantic regularity. This renders the
syntactic description more uniform and it provides
an internal structure for regular MWEs, which is
meaningful if the MWE is fully or partially com-
positional. For instance, it is meaningful to have
the adjective sociaux attach to biens instead of on
the first component abus. Moreover, such a dis-
tinction opens the way to a non-binary classifica-
tion of MWE status: the various criteria leading to
classify a sequence as MWE could be annotated
separately and using nominal or scaled categories
for each criteria. For instance, de?ficit budge?taire
could be marked as fully compositional, but with
frozen lexical selection. Further, annotation is of-
ten incoherent for the MWEs with both regular
syntax and a certain amount of semantic compo-
sitionality, the same token sequence (with same
meaning) being sometimes annotated as MWE
and sometimes not.
More generally, keeping a regular representa-
tion would allow to better deal with the interac-
tion between idiomatic status and regular syntax,
such as the insertion of modifiers on MWE sub-
parts (e.g. make a quick decision).
Finally, using regular syntax for MWEs pro-
vides a more uniform training set. For instance for
a sequence N1 preposition N2, though some exter-
nal attachments might vary depending on whether
the sequence forms a MWE or not, some may
not, and the internal dependency structure (N1 ?
(preposition ? N2)) is quite regular. One objec-
tive of the current work is to investigate whether
this increased uniformity eases parsing or whether
it is mitigated by the additional difficulty of find-
ing the internal structure of a MWE.
Total Nb of regular MWEs
nb of (% of nouns, adverbs,
MWEs prepositions, verbs)
train 23658 12569 (64.7, 19.2, 14.6, 1.5)
dev 2120 1194 (66.7, 17.7, 14.7, 0.8)
test 4049 2051 (64.5, 19.9, 13.6, 2.0)
Table 1: Total number of MWEs and number of
regular MWEs in training, development and test
set (and broken down by POS of MWE).
745
3.2.2 Implementation
We developed an ad hoc program for structur-
ing the regular MWEs in gold data. MWEs are
first classified as regular or irregular, using reg-
ular expressions over the sequence of parts-of-
speech within the MWE. To define the regular
expressions, we grouped gold MWEs according
to the pair [global POS of the MWE + sequence
of POS of the MWE components], and designed
regular expressions to match the most frequent
patterns that looked regular according to our lin-
guistic knowledge. The internal structure for the
matching MWEs was built deterministically, us-
ing heuristics favoring local attachments.5 Table 1
shows the proportions of MWEs classified as regu-
lar, and thus further structured. About half MWEs
are structured, and about two thirds of structured
MWEs are nouns.
For predicted parses with structured MWEs, we
use an inverse transformation of structured MWEs
into flat MWEs, for evaluation against the gold
data. When a predicted structured MWE is flat-
tened, all the dependents of any token of the MWE
that are not themselves belonging to the MWE are
attached to the head component of the MWE.
3.3 Integration of MWE features into labels
In some experiments, we make use of alterna-
tive representations, which we refer later as ?la-
beled representation?, in which the MWE features
are incorporated in the dependency labels, so that
MWE composition and/or the POS of the MWE be
totally contained in the tree topology and labels,
and thus predictable via dependency parsing. Fig-
ure shows the labeled representation for the sen-
tence of Figure 1.
For flat MWEs, the only missing information is
the MWE part-of-speech: we concatenate it to the
dep_cpd labels. For instance, the arc from en
to vain is relabeled dep_cpd_ADV. For struc-
tured MWEs, in order to get full MWE account
within the tree structure and labels, we need to in-
corporate both the MWE POS, and to mark it as
5The six regular expressions that we obtained cover nomi-
nal, prepositional, adverbial and verbal compounds. We man-
ually evaluated both the regular versus irregular classification
and the structuring of regular MWEs on the first 200 MWEs
of the development set. 113 of these were classified as regu-
lar, and we judged that all of them were actually regular, and
were correctly structured. Among the 87 classified as irregu-
lar, 7 should have been tagged as regular and structured. For
4 of them, the classification error is due to errors on the (gold)
POS of the MWE components.
c. Labeled representation:
L? abus de biens sociaux fut de?nonce? en vain
suj
det
dep
r N
obj
.p
r N
m
od
r N
au
x
tps
mod
dep
cpd
ADV
Figure 2: Integration of all MWE information into
labels for the example of Figure 1.
belonging to a MWE. The suffixed label has the
form FCT_r_POS. For instance, in bottom tree
of Figure 1, arcs pointing to the non-head compo-
nents (de, biens, sociaux) are suffixed with _r to
mark them as belonging to a structured MWE, and
with _N since the MWE is a noun.
In both cases, this label suffixing is translated
back into features for evaluation against gold data.
4 Architectures for MWE Analysis and
Parsing
The architectures we investigated vary depending
on whether the MWE status of sequences of to-
kens is predicted via dependency parsing or via an
external tool (described in section 5), and this di-
chotomy applies both to structured MWEs and flat
MWEs. More precisely, we consider the following
alternative for irregular MWEs:
? IRREG-MERGED: gold irregular MWEs are
merged for training; for parsing, irregular
MWEs are predicted externally, merged into
one token at parsing time, and re-expanded
into several tokens for evaluation;
? IRREG-BY-PARSER: the MWE status, flat
topology and POS are all predicted via de-
pendency parsing, using representations for
training and parsing, with all information for
irregular MWEs encoded in topology and la-
bels (as for in vain in Figure 2).
For regular MWEs, their internal structure is al-
ways predicted by the parser. For instance the un-
labeled dependencies for abus de biens sociaux are
the same, independently of predicting whether it
746
forms a MWE or not. But we use two kinds of
predictions for their MWE status and POS:
? REG-POST-ANNOTATION: the regular
MWEs are encoded/predicted as shown for
abus de biens sociaux in bottom tree of
Figure 1, and their MWE status and POS is
predicted after parsing, by an external tool.
? REG-BY-PARSER: all regular MWE infor-
mation (topology, status, POS) is predicted
via dependency parsing, using representa-
tions with all information for regular MWEs
encoded in topology and labels (Figure 2).
Name prediction of prediction of
reg MWEs irreg MWEs
JOINT irreg-by-parser reg-by-parser
JOINT-REG irreg-merged reg-by-parser
JOINT-IRREG irreg-by-parser reg-post-annot
PIPELINE irreg-merged reg-post-annot
Table 2: The four architectures, depending on how
regular and irregular MWEs are predicted.
We obtain four architectures, schematized in ta-
ble 2. We describe more precisely two of them,
the other two being easily inferable:
JOINT-REG architecture:
? training set: irregular MWEs merged into
one token, regular MWEs are structured, and
integration of regular MWE information into
the labels (FCT_r_POS).
? parsing: (i) MWE analysis with classifica-
tion of MWEs into regular or irregular, (ii)
merge of predicted irregular MWEs, (iii) tag-
ging and morphological prediction, (iv) pars-
ing
JOINT-IRREG architecture:
? training set: flat representation of irregu-
lar MWEs, with label suffixing (dep_cpd_
POS), structured representation of regular
MWEs without label suffixing.
? parsing: (i) MWE analysis and classifica-
tion into regular or irregular, used for MWE-
specific features, (ii) tagging and morpholog-
ical prediction, (iii) parsing,
We compare these four architectures between
them and also with two simpler architectures used
by (Constant et al, 2013) within the SPMRL 13
Shared Task, in which regular and irregular MWEs
are not distinguished:
Uniform joint architecture: The joint systems
perform syntactic parsing and MWE analysis via
a single dependency parser, using representations
as in 3.3.
Uniform pipeline architecture:
? training set: MWEs merged into one token
? parsing: (i) MWE analysis, (ii) merge of pre-
dicted MWEs, (iii) tagging and morphologi-
cal prediction, (iv) parsing
For each architecture, we apply the appropriate
normalization procedures on the predicted parses,
in order to evaluate against (i) the pseudo-gold
data in structured representation, and (ii) the gold
data in flat representation.
5 Use of external MWE resources
In order to better deal with MWE prediction, we
use external MWE resources, namely MWE lexi-
cons and an MWE analyzer. Both resources help
to predict MWE-specific features (section 5.3) to
guide the MWE-aware dependency parser. More-
over, in some of the architectures, the external
MWE analyzer is used either to pre-group irreg-
ular MWEs (for the architectures using IRREG-
MERGED), or to post-annotate regular MWEs.
5.1 MWE lexicons
MWE lexicons are exploited as sources of fea-
tures for both the dependency parser and the ex-
ternal MWE analyzer. In particular, two large-
coverage general-language lexicons are used: the
Lefff6 lexicon (Sagot, 2010), which contains ap-
proximately half a million inflected word forms,
among which approx. 25, 000 are MWEs; and
the DELA7 (Courtois, 2009; Courtois et al, 1997)
lexicon, which contains approx. one million in-
flected forms, among which about 110, 000 are
MWEs. These resources are completed with spe-
cific lexicons freely available in the platform Uni-
tex8: the toponym dictionary Prolex (Piton et al,
1999) and a dictionary of first names. Note that the
lexicons do not include any information on the ir-
regular or the regular status of the MWEs. In order
to compare the MWEs present in the lexicons and
those encoded in the French treebank, we applied
the following procedure (hereafter called lexicon
6We use the version available in the POS tagger MElt (De-
nis and Sagot, 2009).
7We use the version in the platform Unitex
(http://igm.univ-mlv.fr/?unitex). We had to convert the
DELA POS tagset to that of the French Treebank.
8http://igm.univ-mlv.fr/?unitex
747
lookup): in a given sentence, the maximum num-
ber of non overlapping MWEs according to the
lexicons are systematically marked as such. We
obtain about 70% recall and 50% precision with
respect to MWE spanning.
5.2 MWE Analyzer
The MWE analyzer is a CRF-based sequential la-
beler, which, given a tokenized text, jointly per-
forms MWE segmentation and POS tagging (of
simple tokens and of MWEs), both tasks mutu-
ally helping each other9. The MWE analyzer inte-
grates, among others, features computed from the
external lexicons described in section 5.1, which
greatly improve POS tagging (Denis and Sagot,
2009) and MWE segmentation (Constant and Tel-
lier, 2012). The MWE analyzer also jointly classi-
fies its predicted MWEs as regular or irregular (the
distinction being learnt on gold training set, with
structured MWEs cf. section 3.2).
5.3 MWE-specific features
We introduce information from the external MWE
resources in different ways:
Flat MWE features: MWE information can
be integrated as features to be used by the de-
pendency parser. We tested to incorporate the
MWE-specific features as defined in the gold flat
representation (section 3.1): the mwehead=POS
feature for the MWE head token, POS being the
part-of-speech of the MWE; the component=y
feature for the non-first MWE component.
Switch: instead or on top of using the mwehead
feature, we use the POS of the MWE instead of the
POS of the first component of a flat MWE. For in-
stance in Figure 1, the token en gets pos=ADV in-
stead of pos=P. The intuition behind this feature
is that for an irregular MWE, the POS of the lin-
early first component, which serves as head, is not
always representative of the external distribution
of the MWE. For regular MWEs, the usefulness of
such a trick is less obvious. The first component
of a regular MWE is not necessarily its head (for
instance for a nominal MWE with internal pattern
adjective+noun), so the switch trick could be detri-
mental in such cases.10
9Note that in our experiments, we use this analyzer for
MWE analysis only, and discard the POS tagging predic-
tion. Tagging is performed along with lemmatization with
the Morfette tool (section 6.1).
10We also experimented to use POS of MWE plus suffixes
to force disjoint tagsets for single words, irregular MWEs and
6 Experiments
6.1 Settings and evaluation metrics
MWE Analysis and Tagging: For the MWE
analyzer, we used the tool lgtagger11 (version
1.1) with its default set of feature templates, and a
10-fold jackknifing on the training corpus.
Parser: We used the second-order graph-based
parser available in Mate-tools12 (Bohnet, 2010).
We used the Anna3.3 version, in projective
mode, with default feature sets and parameters
proposed in the documentation, augmented or not
with MWE-specific features, depending on the
experiments.
Morphological prediction: Predicted lemmas,
POS and morphology features are computed
with Morfette version 0.3.5 (Chrupa?a et al,
2008; Seddah et al, 2010)13, using 10 iterations
for the tagging perceptron, 3 iterations for the
lemmatization perceptron, default beam size for
the decoding of the joint prediction, and the
Lefff (Sagot, 2010) as external lexicon used for
out-of-vocabulary words. We performed a 10-fold
jackknifing on the training corpus.
Evaluation metrics: we evaluate our parsing sys-
tems by using the standard metrics for depen-
dency parsing: Labeled Attachment Score (LAS)
and Unlabeled Attachment Score (UAS), com-
puted using all tokens including punctuation. To
evaluate statistical significance of parsing perfor-
mance differences, we use eval07.pl14 with -b op-
tion, and then Dan Bikel?s comparator.15 For
MWEs, we use the Fmeasure for recognition of
untagged MWEs (hereafter FUM) and for recog-
nition of tagged MWEs (hereafter FTM).
6.2 MWE-specific feature prediction
In all our experiments, for the switch trick (section
5.3), the POS of MWE is always predicted using
the MWE analyzer. For the flat MWE features, we
experimented both with features predicted by the
MWE analyzer, and with features predicted using
the external lexicons mentioned in section 5.1 (us-
ing the lexicon lookup procedure). Both kinds of
regular MWEs, but this showed comparable results.
11http://igm.univ-mlv.fr/?mconstan
12http://code.google.com/p/mate-tools/
13https://sites.google.com/site/morfetteweb/
14http://nextens.uvt.nl/depparse-wiki/SoftwarePage
15The compare.pl script, formerly available at
www.cis.upenn.edu/ dbikel/
748
LABELED STRUCTURED FLAT
REPRES. REPRESENTATION REPRESENTATION
MWE swi. swi. LAS UAS LAS FUM FTM LAS UAS FUM FTM
ARCHI feats irreg reg irreg irreg
bsline - - - 84.5 89.3 87.0 83.6 80.6 84.2 88.1 73.5 70.7
JOINT best + + + 85.3 89.7 87.5 85.4 82.6 85.2 88.8 77.6 74.5
JOINT- bsline - - - 84.7 89.4 87.0 83.5 80.3 84.5 88.0 78.3 75.9
IRREG best + + + 85.1 89.8 87.4 85.0 81.6 84.9 88.3 79.0 76.5
JOINT- bsline - NA - 84.2 89.1 86.7 84.2 80.8 84.0 88.0 73.3 70.3
REG best + NA + 84.7 89.3 86.9 84.1 80.7 84.6 88.3 76.3 73.2
PIPE bsline - NA - 84.6 89.2 86.9 84.1 80.7 84.5 87.9 78.8 76.3
LINE best - NA + 84.7 89.4 87.0 84.2 80.8 84.6 88.1 78.8 76.3
Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for
statistical significance evaluation). The UAS for the structured representation is the same as the one for
the labeled representation, and is not repeated.
prediction lead to fairly comparable results, so in
all the following, the MWE features, when used,
are predicted using the external lexicons.
6.3 Tuning features for each architecture
We ran experiments for all value combinations
of the following parameters: (i) the architecture,
(ii) whether MWE features are used, whether the
switch trick is applied or not (iii) for irregular
MWEs and (iv) for regular MWEs.
We performed evaluation of the predicted parses
using the three representations described in sec-
tion 3, namely flat, structured and labeled repre-
sentations. In the last two cases, the evaluation
is performed against an instance of the gold data
automatically transformed to match the represen-
tation type. Moreover, for the ?labeled representa-
tion? evaluation, though the MWE information in
the predicted parses is obtained in various ways,
depending on the architecture, we always map all
this information in the dependency labels, to ob-
tain predicted parses matching the ?labeled repre-
sentation?. While the evaluation in flat represen-
tation is the only one comparable to other works
on this dataset, the other two evaluations provide
useful information. In the ?labeled representation?
evaluation, the UAS provides a measure of syn-
tactic attachments for sequences of words, inde-
pendently of the (regular) MWE status of subse-
quences. For the sequence abus de biens sociaux,
suppose that the correct internal structure is pre-
dicted, but not the MWE status. The UAS for
labeled representation will be maximal, whereas
for the flat representation, the last two tokens will
count as incorrect for UAS. For LAS, in both cases
the three last tokens will count as incorrect if the
wrong MWE status is predicted. So to sum up on
the ?labeled evaluation?, we obtain a LAS eval-
uation for the whole task of parsing plus MWE
recognition, but an UAS evaluation that penalizes
less errors on MWE status, while keeping a rep-
resentation that is richer: predicted parses contain
not only the syntactic dependencies and MWE in-
formation, but also a classification of MWEs into
regular and irregular, and the internal syntactic
structure of regular MWEs.
The evaluation on ?structured representation?
can be interpreted as an evaluation of the parsing
task plus the recognition of irregular MWEs only:
both LAS and UAS are measured independently
of errors on regular MWE status (note the UAS is
exactly the same than in the ?labeled? case).
For each architecture, Table 3 shows the results
for two systems: first the baseline system without
any MWE features nor switches and immediately
below the best settings for the architecture. The
JOINT baseline corresponds to a ?pure? joint sys-
tem without external MWE resources (hence the
minus sign for the first three columns). For each
architecture except the PIPELINE one, differences
between the baseline and the best setting are sta-
tistically significant (p < 0.01). Differences be-
tween best PIPELINE and best JOINT-REG are
not. Best JOINT has statistically significant dif-
ference (p < 0.01) over both best JOINT-REG
and best PIPELINE. The situation for best JOINT-
IRREG with respect to the other three is borderline
(with various p-values depending on the metrics).
Concerning the tuning of parameters, it appears
that the best setting is to use MWE-features, and
switch for both regular and irregular MWEs, ex-
cept for the pipeline architecture for which results
without MWE features are slightly better. So over-
all, informing the parser with independently pre-
749
LABELED STRUCTURED FLAT
REPRESENTATION REPRESENTATION REPRESENTATION
LAS UAS LAS UAS FUM FTM LAS UAS FUM FTM
SYSTEM irreg irreg
baseline JOINT 84.13 88.93 86.62 88.93 83.6 79.2 83.97 87.80 73.9 70.5
best JOINT 84.59 89.21 86.92 89.21 85.7 81.4 84.48 88.13 77.0 73.5
best JOINT-IRREG 84.50 89.21 86.97 89.24 86.3 82.1 84.36 87.75 78.6 75.4
best JOINT-REG 84.31 89.0 86.63 89.00 84.5 80.4 84.18 87.95 76.4 73.3
best PIPELINE 84.02 88.83 86.49 88.83 84.4 80.4 83.88 87.33 77.6 74.4
Table 4: Final results on test set for baseline and the best system for each architecture.
dicted POS of MWE has positive impact. The
best architectures are JOINT and JOINT-IRREG,
with the former slightly better than the latter for
parsing metrics, though only some of the differ-
ences are significant between the two. It can be
noted though, that JOINT-IRREG performs over-
all better on MWEs (last two columns of table
3), whereas JOINT performs better on irregular
MWEs: the latter seems to be beneficial for pars-
ing, but is less efficient to correctly spot the regular
MWEs.
Concerning the three distinct representations,
evaluating on structured representation (hence
without looking at regular MWE status) leads to
a rough 2 point performance increase for the LAS
and a one point increase for the UAS, with respect
to the evaluation against flat representation. This
quantifies the additional difficulty of deciding for
a regular sequence of tokens whether it forms a
MWE or not. The evaluation on the labeled rep-
resentation provides an evaluation of the full task
(parsing, regular/irregular MWE recognition and
regular MWEs structuring), with a UAS that is less
impacted by errors on regular MWE status, while
LAS reflects the full difficulty of the task.16
6.4 Results on test set and comparison
We provide the final results on the test set in
table 4. We compare the baseline JOINT sys-
tem with the best system for all four reg/irreg
architectures (cf. section 6.3). We observe the
same general trend as in the development corpus,
but with tinier differences. JOINT and JOINT-
IRREG significantly outperform the baseline and
the PIPELINE, on labeled representation and flat
representation. We can see that there is no sig-
nificant difference between JOINT and JOINT-
16The slight differences in LAS between the labeled and
the flat representations are due to side effects of errors on
MWE status: some wrong reattachments performed to obtain
flat representation decrease the UAS, but also in some cases
the LAS.
DEV TEST
System UAS LAS UAS LAS
reg/irreg joint 88.79 85.15 88.13 84.48
Bjork13 88.30 84.84 87.87 84.37
Const13 pipeline 88.73 85.28 88.35 84.91
Const13 joint 88.21 84.60 87.76 84.14
uniform joint 88.81 85.42 87.96 84.59
Table 5: Comparison on dev set of our best archi-
tecture with reg/irregular MWE distinction (first
row), with the single-parser architectures of (Con-
stant et al, 2013) (Const13) and (Bjo?rkelund et
al., 2013) (Bjork13). Uniform joint is our reimple-
mentation of Const13 joint, enhanced with mwe-
features and switch.
IRREG and between JOINT-REG and JOINT-
IRREG. JOINT slightly outperforms JOINT-REG
(p < 0.05). On the structured representation, the
two best systems (JOINT and JOINT-IRREG) sig-
nificantly outperform the other systems (p < 0.01
for all; p < 0.05 for JOINT-REG).
Moreover, we provide in table 5 a comparison
of our best architecture with reg/irregular MWE
distinction with other architectures that do not
make this distinction, namely the two best com-
parable systems designed for the SPMRL Shared
Task (Seddah et al, 2013): the pipeline sim-
ple parser based on Mate-tools of Constant et
al. (2013) (Const13) and the Mate-tools system
(without reranker) of Bjo?rkelund et al (2013)
(Bjork13). We also reimplemented and improved
the uniform joint architecture of Constant et al
(2013), by adding MWE features and switch. Re-
sults can only be compared on the flat representa-
tion, because the other systems output poorer lin-
guistic information. We computed statistical sig-
nificance of differences between our systems and
Const13. On dev, the best system is the enhanced
uniform joint, but differences are not significant
between that and the best reg/irreg joint (1st row)
and the Const13 pipeline. But on the test corpus
(which is twice bigger), the best system is Const13
750
Tasks LAS UAS ALL MWE REG MWE IRREG MWE
System Parsing MWE FUM FTM FUM FTM FUM FTM
Our best system (best JOINT) + all 85.15 88.78 77.6 74.5 70.8 67.8 85.4 82.6
Uniform pipeline/gold MWEs + - 88.73 90.60 - - - - - -
CRF-based MWE analyzer - all - - 78.8 76.3 73.5 71.9 84.2 80.8
JOINT-REG + all 84.58 88.34 76.3 73.2 69.3 66.5 84.1 80.7
JOINT-REG/gold irreg. MWE + reg. 85.86 89.19 82.9 78.8 70.0 67.2 - -
Table 6: Comparison with simpler tasks on the flat representation of the development set.
pipeline, with statistically significant differences
over our joint systems. So the first observation
is that our architectures that distinguish between
reg/irreg MWEs do not outperform uniform ar-
chitectures. But we note that the differences are
slight, and the output we obtain is enhanced with
regular MWE internal structure. It can thus be
noted that the increased syntactic uniformity ob-
tained by our MWE representation is mitigated so
far by the additional complexity of the task. The
second observation is that currently the best sys-
tem on this dataset is a pipeline system, as results
on test set show (and somehow contrary to results
on dev set). The joint systems that integrate MWE
information in the labels seem to suffer from in-
creased data sparseness.
6.5 Evaluating the double task with respect
to simpler tasks
In this section, we propose to better evaluate the
difficulty of combining the tasks of MWE analy-
sis and dependency parsing by comparing our sys-
tems with systems performing simpler tasks: i.e.
MWE recognition without parsing, and parsing
with no or limited MWE recognition, simulated by
using gold MWEs. We also provide a finer eval-
uation of the MWE recognition task, in particular
with respect to their regular/irregular status.
We first compare our best system with a parser
where all MWEs have been perfectly pre-grouped,
in order to quantify the difficulty that MWEs add
to the parsing task. We also compare the per-
formance on MWEs of our best system with that
achieved by the CRF-based analyzer described in
section 5.2. Next, we compare the best JOINT-
REG system with the one based on the same ar-
chitecture but where the irregular MWEs are per-
fectly pre-identified, in order to quantify the dif-
ficulty added by the irregular MWEs. Results are
given in table 6. Without any surprise, the task
is much easier without considering MWE recog-
nition. We can see that without considering MWE
analysis the parsing accuracy is about 2.5 points
better in terms of LAS. In the JOINT-REG ar-
chitecture, assuming gold irregular MWE identi-
fication, increases LAS by 1.3 point. In terms
of MWE recognition, as compared with the CRF-
based analyzer, our best system is around 2 points
below. But the situation is quite different when
breaking the evaluation by MWE type. Our sys-
tem is 1 point better than the CRF-based analyzer
for irregular MWEs. This shows that considering
a larger syntactic context helps recognition of ir-
regular MWEs. The ?weak point? of our system is
therefore the identification of regular MWEs.
7 Conclusion
We experimented strategies to predict both MWE
analysis and dependency structure, and tested
them on the dependency version of French Tree-
bank (Abeille? and Barrier, 2004), as instantiated
in the SPMRL Shared Task (Seddah et al, 2013).
Our work focused on using an alternative repre-
sentation of syntactically regular MWEs, which
captures their syntactic internal structure. We ob-
tain a system with comparable performance to that
of previous works on this dataset, but which pre-
dicts both syntactic dependencies and the internal
structure of MWEs. This can be useful for captur-
ing the various degrees of semantic composition-
ality of MWEs. The main weakness of our system
comes from the identification of regular MWEs, a
property which is highly lexical. Our current use
of external lexicons does not seem to suffice, and
the use of data-driven external information to bet-
ter cope with this identification can be envisaged.
References
Anne Abeille? and Nicolas Barrier. 2004. Enriching
a french treebank. In Proceedings of LREC 2004,
Lisbon, Portugal.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
751
french. In Proceedings of ACL 2005, Ann Arbor,
USA.
Eduard Bejc?ek and Pavel Stranak. 2010. Annota-
tion of multiword expressions in the prague depen-
dency treebank. Language Resources and Evalua-
tion, 44:7?21.
Anders Bjo?rkelund, ?Ozlem C?etinog?lu, Thomas Farkas,
Richa?rdand Mu?ller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the spmrl 2013 shared task. In Pro-
ceedings of the 4th Workshop on Statistical Parsing
of Morphologically Rich Languages: Shared Task,
Seattle, WA.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of COLING 2010, Beijing, China.
Conor Cafferkey, Deirdre Hogan, and Josef van Gen-
abith. 2007. Multi-word units in treebank-based
probabilistic parsing and generation. In Proceed-
ings of the 10th International Conference on Re-
cent Advances in Natural Language Processing
(RANLP?07), Borovets, Bulgaria.
Marie Candito, Benoit Crabbe?, and Pascal Denis.
2010. Statistical french dependency parsing : Tree-
bank conversion and first results. In Proceedings of
LREC 2010, Valletta, Malta.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of LREC 2008, Marrakech,
Morocco. ELDA/ELRA.
Matthieu Constant and Isabelle Tellier. 2012. Eval-
uating the impact of external lexical resources into
a crf-based multiword segmenter and part-of-speech
tagger. In Proceedings of LREC 2012, Istanbul,
Turkey.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate
multiword expression recognition and parsing. In
Proceedings of ACL 2012, Stroudsburg, PA, USA.
Matthieu Constant, Marie Candito, and Djame? Sed-
dah. 2013. The ligm-alpage architecture for the
spmrl 2013 shared task: Multiword expression anal-
ysis and dependency parsing. In Proceedings of the
4th Workshop on Statistical Parsing of Morphologi-
cally Rich Languages: Shared Task, Seattle, WA.
Blandine Courtois, Myle`ne Garrigues, Gaston Gross,
Maurice Gross, Rene? Jung, Mathieu-Colas Michel,
Anne Monceaux, Anne Poncet-Montange, Max Sil-
berztein, and Robert Vive?s. 1997. Dictionnaire
e?lectronique DELAC : les mots compose?s binaires.
Technical Report 56, University Paris 7, LADL.
Blandine Courtois. 2009. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise, 87:11?22.
Pascal Denis and Beno??t Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art POS tagging with less human ef-
fort. In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC?09), Hong Kong.
Gu?ls?en Eryig?it, Tugay Ilbay, and Ozan Arkan Can.
2011. Multiword expressions in statistical depen-
dency parsing. In Proceedings of the IWPT Work-
shop on Statistical Parsing of Morphologically-Rich
Languages (SPMRL?11), Dublin, Ireland.
Spence Green, Marie-Catherine de Marneffe, John
Bauer, and Christofer D. Manning. 2011. Multi-
word expression identification with tree substitution
grammars: A parsing tour de force with french. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Proceedings of the LREC
Workshop : Methodologies and Evaluation of Multi-
word Units in Real-World Applications (MEMURA),
Lisbon, Portugal.
Odile Piton, Denis Maurel, and Claude Belleil. 1999.
The prolex data base : Toponyms and gentiles for
nlp. In Proceedings of the Third International Work-
shop on Applications of Natural Language to Data
Bases (NLDB?99), Klagenfurt, Austria.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of NAACL/HLT
2006, Companion Volume: Short Papers, Strouds-
burg, PA, USA.
Beno??t Sagot. 2010. The lefff, a freely available, accu-
rate and large-coverage lexicon for french. In Pro-
ceedings of LREC 2010, Valletta, Malta.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Djame? Seddah, Reut Tsarfaty, Sandra K??ubler, Marie
Candito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,
Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Wolin?ski, Alina
Wro?blewska, and Eric Villemonte de la Cle?rgerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morpholog-
ically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing of Morphologically
Rich Languages: Shared Task, Seattle, WA.
752
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In Proceedings of
LREC 2010, Valletta, Malta.
Veronika Vincze, Ja?nos Zsibrita, and Istva`n Nagy T.
2013. Dependency parsing for identifying hungar-
ian light verb constructions. In Proceedings of In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan.
753
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 76?84,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Parsing word clusters
Marie Candito? and Djam? Seddah??
? Alpage (Universit? Paris 7/INRIA), 30 rue du ch?teau des rentiers 75013 Paris, France
? Universit? Paris-Sorbonne, 28, rue Serpente, 75006 Paris, France
Abstract
We present and discuss experiments in sta-
tistical parsing of French, where terminal
forms used during training and parsing are
replaced by more general symbols, particu-
larly clusters of words obtained through un-
supervised linear clustering. We build on the
work of Candito and Crabb? (2009) who pro-
posed to use clusters built over slightly coars-
ened French inflected forms. We investigate
the alternative method of building clusters
over lemma/part-of-speech pairs, using a raw
corpus automatically tagged and lemmatized.
We find that both methods lead to compara-
ble improvement over the baseline (we ob-
tain F1=86.20% and F1=86.21% respectively,
compared to a baseline of F1=84.10%). Yet,
when we replace gold lemma/POS pairs with
their corresponding cluster, we obtain an up-
per bound (F1=87.80) that suggests room for
improvement for this technique, should tag-
ging/lemmatisation performance increase for
French.
We also analyze the improvement in perfor-
mance for both techniques with respect to
word frequency. We find that replacing word
forms with clusters improves attachment per-
formance for words that are originally either
unknown or low-frequency, since these words
are replaced by cluster symbols that tend to
have higher frequencies. Furthermore, clus-
tering also helps significantly for medium to
high frequency words, suggesting that training
on word clusters leads to better probability es-
timates for these words.
1 Introduction
Statistical parsing techniques have dramatically im-
proved over the last 15 years, yet lexical data sparse-
ness remains a critical problem. And the richer the
morphology of a language, the sparser the treebank-
driven lexicons will be for that language.
Koo et al (2008) have proposed to use word clus-
ters as features to improve graph-based statistical
dependency parsing for English and Czech. Their
clusters are obtained using unsupervised clustering,
which makes it possible to use a raw corpus con-
taining several million words. Candito and Crabb?
(2009) applied clustering to generative constituency
parsing for French. They use a desinflection step that
removes some inflection marks from word forms and
then replaces them with word clusters, resulting in
a significant improvement in parsing performance.
Clustering words seems useful as a way of address-
ing the lexical data sparseness problem, since counts
on clusters are more reliable and lead to better prob-
ability estimates. Clustering also appears to address
the mismatch of vocabularies between the original
treebank and any external, potentially out-of-domain
corpus: clusters operate as an intermediary between
words from the treebank and words from the exter-
nal corpus used to compute clusters. Furthermore,
parsing word clusters instead of word forms aug-
ments the known vocabulary.
However, depending on the clustering method,
clusters are either not very reliable or are available
only for very frequent words. In order to parse word
clusters one needs to determine which word clusters
are reliable enough to be beneficial, so the tuning
of parameters such as cluster granularity and cluster
reliability becomes very important.
The aim of this paper is to give an in-depth study
of the "parsing word clusters" technique. In particu-
lar, starting from the Candito and Crabb? (2009) ex-
periments, we investigate the use of clustering lem-
76
mas instead of desinflected forms. We also pro-
vide an analysis of the performance gains obtained
with respect to word frequency (frequent words, rare
words, unknown words).
In the next section, we describe the French tree-
bank used as the basis for all of our experiments.
We describe in section 3 the statistical parser used
for training and testing. We then describe the desin-
flection process used prior to unsupervised cluster-
ing (section 4), and the Brown algorithm we use for
unsupervised clustering (section ). We describe our
experiments and results in section 6, and provide a
discussion in section 7. We then point out some re-
lated work and conclude in section 9.
2 French Treebank
For our experiments, we used the French Tree-
bank (Abeill? et al, 2003), which contains 12531
sentences, 350931 tokens, from the newspaper
Le Monde. We used the treebank instantiation
(hereafter FTB-UC) as first described in (Candito
and Crabb?, 2009), where :
(i) the rich original annotation containing morpho-
logical and functional information is mapped to a
simpler phrase-structure treebank with a tagset of
28 part-of-speech tags, and no functional annotation
(ii) some compounds with regular syntax are broken
down into phrases containing several simple words
(iii) the remaining sequences annotated as com-
pound words in the FTB are merged into a single
token, whose components are separated with an
underscore
For all experiments in this paper (tagging and
parsing) we used the same partition of the treebank
as these authors : first 10% for test, next 10% for
dev and the rest for training1.
3 Berkeley Parser
We report here experiments using the Berkeley
PCFG parser with latent annotations (Petrov et al,
2006), hereafter BKY, which is a constituent parser
that has been proven to perform well for French
(Crabb? and Candito, 2008; Seddah et al, 2009),
1More precisely the partition is : first 1235 sentences for
test, next 1235 sentences for development, and remaining 9881
sentences for training.
though a little lower than a combination of a tagger
plus the dependency-based MST parser (Candito et
al., 2010). Though PCFG-style parsers operate on
too narrow a domain of locality, splitting symbols
according to structural and/or lexical properties is
known to help parsing (Klein and Manning., 2003).
Following (Matsuzaki et al, 2005), the BKY algo-
rithm uses EM to estimate probabilities on symbols
that are automatically augmented with latent anno-
tations, a process which can be viewed as symbol
splitting. It iteratively evaluates each such split and
merges back the less beneficial ones. Crabb? and
Candito (2008) show that some of the information
carried by the latent annotations is lexical, since re-
placing words by their gold part-of-speech tag leads
to worse results than the corresponding perfect tag-
ging test, with words unchanged. This is a clear in-
dication that lexical distinctions are used, and perco-
late up the parse tree via the latent annotations.
We now describe how the BKY software handles
rare and unknown words, as this is pertinent to our
discussion in section 6. P (w|tag) is calculated us-
ing Bayes? rule, as P (tag|w)P (w)/P (tag). Rel-
ative frequency estimates are used for words that
are sufficiently frequent. For rare words (appear-
ing less than 10 times in our settings), P (tag|w)
is smoothed using the proportion of tokens in the
second half of the training set that were not seen in
the first half, and that have this tag. For unknown
words, words signatures are used: these are word
classes determined by information such as the word
suffix, whether the word is capitalized, whether it
contains digits, etc. P (w|tag) is estimated with
P (signature(w)|tag), and is also smoothed in the
same way rare words are.
4 Morphological clustering
A first approach to word clustering is to cluster
forms on a morphological basis. In the case of
a relatively morphologically rich language such as
French, this is an obvious way to reduce lexical
sparseness caused by inflection.
(Candito and Crabb?, 2009) proposed the use of
a desinflection method, without resorting to part-of-
speech tagging. We propose an alternate method
here, which uses lemmas and part-of-speech tags
that are output by a tagger/lemmatizer. Because
77
counts on lemmas are more reliable, clustering
over lemmas presumably produces clusters that are
more reliable than those produced by clustering over
desinflected forms. However, this approach does
create a constraint in which automatically tagged
and lemmatized text is required as input to the
parser, leading to the introduction of tagging errors.
Both morphological clustering methods make use
of the Lefff lexicon (Sagot, 2010). Before we de-
scribe these two methods, we briefly give basic in-
formation on French inflectional morphology and on
the Lefff.
4.1 French inflection and the Lefff lexicon
French nouns appear in singular and plural forms,
and have an intrinsic gender. The number and gen-
der of a noun determines the number and gender of
determiners, adjectives, past participles that depend
on it. Hence in the general case, past participles and
adjectives have four different forms. The major in-
flectional variation appears for finite verbs that vary
for tense, mood, person and number. A regular verb
may correspond to more than 60 inflected forms if
all tenses and mood are included. In practice, some
forms occur very rarely, because some tense/mood
pairs are rare, and further, in the case of newspa-
per text for instance, the first and second persons are
also rare. So for instance in the FTB-UC, there are
33 different forms for the highly frequent verb and
auxiliary avoir (to have), that appears 4557 times.
The medium frequency verb donner (to give) occurs
155 times, under 15 different forms. In the whole
treebank, there are 27130 unique word forms, corre-
sponding to 17570 lemmas.
The Lefff is a freely available rich morphologi-
cal and syntactic French lexicon (Sagot, 2010). It
contains 110, 477 lemmas (simple and compounds)
and 536, 375 inflected forms. The coverage on the
FTB-UC is high : around 96% of the tokens, and
80, 1% of the types are present in the Lefff (leaving
out punctuation and numeric tokens, and ignoring
case differences).
4.2 Desinflection
The aim of the desinflection step is to reduce lex-
ical data sparseness caused by inflection, without
hurting parsability and without committing oneself
as far as lexical ambiguity is concerned. The idea
is to leave unchanged the parser?s task in disam-
biguating part-of-speech tags. In that case, mor-
phological clustering using lemmas is not an option,
since lemma assignment presupposes POS disam-
biguation. Furthermore, useful information such as
verb mood (which is needed to capture, for instance,
that infinitive verbs have no overt subject or that par-
ticipial clauses are sentence modifiers) is discarded
during lemmatization, though it is encoded in the
FTB with different projections for finite verbs (pro-
jecting sentences) versus non finite verbs (projecting
VPpart or VPinf).
The intuition of Candito and Crabb? (2009) is
that other inflectional markers in French (gender and
number for determiners, adjectives, pronouns and
nouns, or tense and person for verbs) are not crucial
for inferring the correct phrase-structure projection
for a given word. Consequently, they proposed to
achieve morphological clustering by desinflection,
namely by removing unneeded inflectional markers,
identified using the Lefff. This lexicon-based tech-
nique can be viewed as an intermediate method be-
tween stemming and lemmatization.
The desinflection process is as follows: for a to-
ken t to desinflect, if it is known in the lexicon,
then for each inflected lexical entry le of t, try to
get a corresponding singular entry. If correspond-
ing singular entries exist for all such le and all have
the same form, then replace t by the correspond-
ing form. For instance for wt=entr?es (ambigu-
ous between entrances and entered, fem, plural), the
two lexical entries are [entr?es/N/fem/plu] and [en-
tr?es/V/fem/plu/part/past]2 , each have a correspond-
ing singular lexical entry, with form entr?e.
The same process is used to map feminine forms
to corresponding masculine forms. This allows one
to change mang?e (eaten, fem, sing) into mang?
(eaten, masc, sing). But for the form entr?e, am-
biguous between N and Vpastpart entries, only the
participle has a corresponding masculine entry (with
form entr?). In that case, in order to preserve the
original part-of-speech ambiguity, entr?e is not re-
placed by entr?. Finite verb forms, when unambigu-
ous with other parts-of-speech, are mapped to sec-
ond person plural present indicative corresponding
forms. This choice was made in order to avoid cre-
2This is just an example and not the real Lefff format.
78
Dev set Overall Overall (-punct) Unseen (4.8)
POS acc 97.38 96.99 91.95
Lemma acc 98.20 97.93 92.52
Joint acc 96.35 95.81 87.16
Test set Overall Overall (-punct) Unseen (4.62)
POS acc 97.68 97.34 90.52
Lemma acc 98.36 98.12 91.54
Joint acc 96.74 96.26 85.28
Table 1: MORFETTE performance on the FTB-UC dev
and test sets (with and without punctuation)
ating ambiguity: the second person plural forms end
with a very typical -ez suffix, and the resulting form
is very unlikely ambiguous. For the first token of a
sentence, if it is unknown in the lexicon, the algo-
rithm tries to desinflect the corresponding lowercase
form.
This desinflection process reduces the number
of distinct tokens in the FTB-UC training set from
24110 to 18052.
4.3 Part-of-speech tagging and lemmatization
In order to assign morphological tags and lemmas to
words we use a variation of the MORFETTE model
described in (Chrupa?a et al, 2008). It is a se-
quence labeling model which combines the predic-
tions of two classification models (one for morpho-
logical tagging and one for lemmatization) at decod-
ing time, using a beam search.
While (Chrupa?a et al, 2008) use Maximum Entropy
training to learn PM and PL, we use the MORFETTE
models described in (Seddah et al, 2010), that are
trained using the Averaged Sequence Perceptron al-
gorithm (Freund and Schapire, 1999). The two clas-
sification models incorporate additional features cal-
culated using the Lefff lexicon.
Table 1 shows detailed results on dev set and
test set of the FTB-UC, when MORFETTE is trained
on the FTB-UC training set. To the best of our
knowledge the parts-of-speech tagging performance
is state-of-the-art for French3 and the lemmatization
performance has no comparable results.
5 Unsupervised clustering
3A pure MAXENT based tagger is described in (Denis and
Sagot, 2009), that also uses the Lefff, under the form of features
for the known categories of a word in the lexicon. The authors
report 97.70% of accuracy and 90.01% for unseen data.
We use the Brown et al (1992) hard clustering al-
gorithm, which has proven useful for various NLP
tasks such as dependency parsing (Koo et al, 2008)
and named entity recognition (Liang, 2005). The al-
gorithm to obtain C clusters is as follows: each of
the C most frequent tokens of the corpus is assigned
its own distinct cluster. For the (C + 1)th most fre-
quent token, create a (C + 1)th cluster. Then for
each pair among the C + 1 resulting clusters, merge
the pair that minimizes the loss in the likelihood of
the corpus, according to a bigram language model
defined on the clusters. Repeat this operation for the
(C + 2)th most frequent token, etc. The result is a
hard clustering of words in the corpus into C distinct
clusters, though the process can be continued to fur-
ther merge pairs of clusters among the C clusters,
ending with a single cluster for the entire vocabu-
lary. A binary tree hierarchy of merges for the C
clusters can be obtained by tracing the merging pro-
cess, with each cluster identified by its path within
this binary tree. Clusters can thus be used at various
levels of granularity.
6 Experiments and results
6.1 Clustering
For the Brown clustering algorithm, we used Percy
Liang?s code4, run on the L?Est R?publicain corpus,
a 125 million word journalistic corpus, freely avail-
able at CNRTL5. The corpus was first tokenized and
segmented into sentences. For compound words,
the 240 most frequent compounds of the FTB-UC
were systematically recognized as one token. We
tried out the two alternate morphological clustering
processes described in section 4 as a preprocessing
step before the unsupervised clustering algorithm
was run on the L?Est R?publicain corpus :
(i) word forms were replaced by corresponding
desinflected form
(ii) word forms were replaced by a concatenation
of the part-of-speech tag and lemma obtained with
MORFETTE6 .
4http://www.eecs.berkeley.edu/ pliang/software
5http://www.cnrtl.fr/corpus/estrepublicain
6Because these experiments were first run with a version
of Morfette that was not yet optimized for lemmatization, we
chose to overide the MORFETTE lemma when the Lefff lemma
is available for a given form and part-of-speech tag pair sup-
plied by Morfette. Morfette?s current results (version 0.3.1) in
79
Name Terminal symbols Vocabulary size Terminal symbols
in training set in training set in dev/test sets
BASELINE wf 24110 wf
DFL Desinflected wf 18052 Desinflected wf
DFL+CLUST>X Cluster1(desinflected wf) 1773 (X = 200) Cluster1(desinflected wf)
GOLDCATLEMMA Gold POS+lemma 18654 Gold POS+lemma
AUTOCATLEMMA Gold POS+lemma 18654 Automatic POS+lemma
GOLDCATLEMMA+CLUST>X Cluster2(gold POS+lemma) 1298 (X = 200) Cluster2(gold POS+lemma)
AUTOCATLEMMA+CLUST>X Cluster2(gold POS+lemma) 1298 (X = 200) Cluster2(automatic POS+lemma)
Table 2: Types of terminal symbols used for training and parsing
In the first case we obtain clusters of desinflected
forms, whereas in the second case we obtain clusters
of tag+lemma pairs. Note that lemmas alone could
be used, but as noted earlier, important syntactic
information would be lost, particularly for verb
mood. We did try using clusters of lemmas, coupled
with a few suffixes to record the verb mood, but this
resulted in more or less the same performance as
clusters of tag+lemma pairs.
6.2 Berkeley parser settings
For BKY we used Slav Petrov?s code, adapted for
French by Crabb? and Candito (2008) by modify-
ing the suffixes used to classify unknown words. We
use the partition between training, development and
test sets introduced in section 2. Note though that
the BKY algorithm itself uses two sets of sentences
at training: a learning set and a smaller validation
set for tuning model hyperparameters. In all experi-
ments in this paper, we used 2% of the training set as
as a validation set, and 98% as a learning set. This
differs from (Candito and Crabb?, 2009), where the
dev set was used as a validation set.
6.3 Experiments
We then tested several settings differing only in the
terminal symbols used in the training set, and in the
dev and test sets. We list these settings in table 2. For
the settings involving unsupervised linear clustering:
DFL+CLUST>X: Each desinflected form df is re-
placed by Cluster1(df) : if df occurred more than
X times in the L?Est R?publicain corpus, it is re-
placed by its cluster id, otherwise, a special clus-
ter UNKC is used. Further, a _c suffix is added if
lemmatization renders this step obsolete.
the desinflected form starts with a capital letter, and
additional features are appended, capturing whether
the form is all digits, ends with ant, or r, or ez (cf.
this is the ending of the desinflected forms of unam-
biguous finite verbs). (Candito and Crabb?, 2009)
showed that these additional features are needed be-
cause clusters are noisy: linear context clustering
sometimes groups together items that belong to dif-
ferent parts-of-speech.
GOLDCATLEMMA+CLUST>X: The terminal
form used is the gold part-of-speech concatenated
to the cluster id of the gold POS+lemma, or UNKC
if that pair did not occur more than X times in the
L?Est R?publicain corpus.
AUTOCATLEMMA+CLUST>X: For the
training set, the same setting as GOLD-
CATLEMMA+CLUST>X is used. But for the
dev and test sets, predicted parts-of-speech and
lemmas are used, as output by the MORFETTE tag-
ger/lemmatizer: the terminal form is the predicted
part-of-speech concatenated with the cluster id of
the predicted POS+lemma, or UNKC if that pair
was not frequent enough.
For the CLUST>X experiments, we report results
with X = 200. We have found empirically that
varying X between 20 and 700 has very little effect
on performance gains, both for clustering of desin-
flected forms and clustering of tag+lemma pairs.
Also, all results are with a maximum number of
clusters set to 1000, and we found that limiting the
number of clusters (by taking only a prefix of the
cluster bit string) degrades results.
6.4 Evaluation metrics
We evaluate parsing performance using labeled F-
Measure (combining labeled precision and labeled
80
DEV SET
TERMINAL SYMBOLS F1<40 F1 UAS Tagging Acc.
BASELINE 86.06 83.81 89.23 96.44
DFL 86.65 84.67 (+0.86) 89.86 96.52
DFL+CLUST>200 87.57 85.53 (+1.72) 90.68 96.47
AUTOCATLEMMA 86.77 84.52 (+0.71) 89.97 96.25
AUTOCATLEMMA+CLUST>200 87.53 85.19 (+1.38) 90.39 96.78
GOLDCATLEMMA 87.74 85.53 (+1.72) 91.42 98.49
GOLDCATLEMMA+CLUST>200 88.83 86.52 (+2.71) 92.11 99.46
TEST SET
TERMINAL SYMBOLS F1<40 F1 UAS Tagging Acc.
BASELINE 86.16 84.10 89.57 96.97
DFL 87.13 85.07 (+0.93) 90.45 97.08
DFL+CLUST>200 88.22 86.21 (+2.11) 90.96 96.98
AUTOCATLEMMA 86.85 84.83 (+0.73) 90.30 96.58
AUTOCATLEMMA+CLUST>200 87.99 86.20 (+2.10) 91.22 97.11
GOLDCATLEMMA 88.16 85.90 (+1.80) 91.52 98.54
GOLDCATLEMMA+CLUST>200 89.93 87.80 (+3.70) 92.83 99.41
Table 3: Parsing performance on the dev set/test set when training and parsing make use of clustered terminal symbols.
F1<40 is the F-Measure combining labeled precision and labeled recall for sentences of less than 40 words. All other
metrics are for all sentences of the dev set/test set. UAS = Unlabeled attachement score of converted constituency
trees into surface dependency trees. All metrics ignore punctuation tokens.
recall) both for sentences of less than 40 words, and
for all sentences7 . We also use the unlabeled attach-
ment score (UAS), obtained when converting the
constituency trees output by the BKY parsers into
surface dependency trees, using the conversion pro-
cedure and software of (Candito et al, 2010)8. Punc-
tuation tokens are ignored in all metrics.
7 Discussion
Results are shown in table 3. Our hope was that us-
ing lemmatization would improve overall accuracy
of unsupervised clustering, hence leading to better
parsing performance. However, results using both
methods are comparable.
7Note that often for statistical constituent parsing results are
given for sentences of less than 40 words, whereas for depen-
dency parsing, there is no such limitation. The experiment DFL
and DFL+CLUST>200 are reproduced from the previous work
(Candito and Crabb?, 2009). More precisely, this previous work
reports F1 = 88.29 on the test set, but for sentences ? 40
words, for a DFL+CLUST>20 experiment, and as previously
mentioned, the dev set was used as validation set for the BKY
algorithm. We report now F1 = 88.22 for the same less-than-
40-words sentences, leaving dev set unused at training time.
8The conversion uses head propagation rules to find the head
on the right-hand side of the CFG rules, first proposed for En-
glish in (Magerman, 1995). Hence the process is highly sensi-
tive to part-of-speech tags.
Table 3 shows that both morphological clustering
techniques (DFL and AUTOCATLEMMA) slightly
improve performance (+0.97 and +0.73 F1 over the
baseline for the test set)9. In the case of AUTO-
CATLEMMA, morphological ambiguity is totally ab-
sent in training set: each terminal symbol is the gold
POS+lemma pair, and hence appears with a unique
part-of-speech in the whole training set. But at pars-
ing time, the terminal symbols are the POS+lemma
pairs predicted by MORFETTE, which are wrong for
approximately 3% of the tokens. So when com-
paring the impact on parsing of the two morpho-
logical clustering techniques, it seems that the ad-
vantage of lemmatization (a sounder morphological
clustering compared to the desinflection process) is
counterbalanced by tagging errors that lead to wrong
POS+lemma pairs. Indeed, it can be verified that
9We have computed p-values for pairs of results, us-
ing Dan Bikel?s statistical significance tester for evalb out-
put (http://www.cis.upenn.edu/ dbikel/software.html). All ex-
periments have a p-value < 0.05 both for recall and preci-
sion when compared to the baseline. The differences between
DFL and AUTOCATLEMMA, and between DFL+CLUST>200
and AUTOCATLEMMA+CLUST>200 are not statistically sig-
nificant (p ? value > 0.2). The gain obtained by adding
the unsupervised clustering is clearly significant (p ? value >
0.005), both when comparing AUTOCATLEMMA and AUTO-
CATLEMMA+CLUST>200, and DFL and DFL+CLUST>200.
81
BASELINE DFL+CLUST>200 AUTOCATLEMMA+CLUST>200
FREQUENCY RANGE #tokens in dev set UAS Tagging UAS Tagging UAS Tagging
in original training set
any 31733 (100%) 89.23 96.43 90.68 96.45 90.39 96.78
0 (original unseen) 1892 (5.96%) 84.78 82.56 88.79 89.22 88.64 91.17
0 < x ? 5 3376 (10.64%) 86.49 94.52 88.68 93.13 88.33 95.41
5 < x ? 10 1906 (6.01%) 90.35 96.59 91.50 95.02 91.55 96.12
10 < x ? 20 2248 (7.08%) 89.55 96.71 91.37 95.42 90.57 95.91
20 < x ? 50 3395 (10.70%) 91.87 96.35 92.40 95.96 91.72 95.94
x ? 50 18916 (59.61%) 89.53 98.12 90.75 (+1.22) 98.12 90.56 (+1.03) 97.91
Table 4: Tagging accuracy and UAS scores for words in the dev set, grouped by ranges of frequencies in the original
training set.
when parsing the gold POS+lemma pairs (the non
realistic GOLDCATLEMMA setting10), performance
is greatly improved (+1.80 F1 over the baseline).
Replacing morphological clusters by their corre-
sponding unsupervised clusters leads to a further im-
provement, both for F1 score and for UAS. But here
again, using desinflection or tagging+lemmatisation
leads to more or less the same improvement. But
while the former method is unlikely improvable, the
latter method might reveal more effective if the per-
formance of the tagging/lemmatisation phase im-
proves. The GOLDCATLEMMA+CLUST>200 ex-
periment gives the upper bound performance : it
leads to a +3.70F1 increase over the baseline, for
the test set. In that case, the terminal symbols are
made of the perfect POS plus the cluster of the per-
fect POS+lemma pair. Very few such terminal sym-
bols are unseen in training set, and all are unam-
biguous with respect to part-of-speech (hence the
99.46% tagging accuracy).
In order to better understand the causes of im-
provement, we have broken down the tagging accu-
racy scores and the UAS scores according to various
ranges of word frequencies. For word forms in the
dev set that occur x times in the original training
set, for x in a certain range, we look at how many
are correctly tagged by the parsers, and how many
receive the correct head when constituents are con-
verted into surface dependencies.
The results are shown in table 4. Unseen words
and rare words are much better handled (about +4
points for UAS for unseen words, and +2 points
10The GOLDCATLEMMA experiment leads to high tagging
accuracy, though not perfect, because of POS+lemma pairs
present in dev/test sets but missing in the training set.
for forms appearing less than 5 times). This is
simply obtained because the majority of original
rare or unknowns are replaced by terminal symbols
(either a cluster id or the UNKC token, plus suf-
fixes) that are shared by many forms in the tree-
bank, leading to higher counts. This can be veri-
fied by analyzing tagging accuracies and UAS scores
for various frequency ranges for the modified termi-
nal symbols : the symbols that replace the word
forms in the training set for DFL+CLUST>X and
AUTOCATLEMMA+CLUST>X experiments. This
is shown in table 5. It can be seen that the major-
ity of the tokens have now high-frequency. For in-
stance for the DFL+CLUST>200 experiment, there
are only 0.09% terminal symbols in the dev set that
are unseen in training set, and 92.62% appear more
than 50 times. The parsers do not perform very
well on low-frequency modified terminal symbols,
but they are so few that it has little impact on the
overall performance.
Hence, in our parsing word clusters experiments,
there are almost no real unseen anymore, there are
only terminal symbols made of a cluster id or UNKC
(plus suffixes). More precisely, for instance about
30% of the original unseen in the dev set, are re-
placed by a UNKC* symbol, which means that 70%
are replaced by a cluster-based symbol and are thus
?connected? to the known vocabulary.
Interestingly, the improvement in performance is
also evident for words with high frequency in the
original treebank: for the forms appearing more
than 50 times in the original training set, the UAS
increases by +1.22 with DFL+CLUST>200 and
+1.03 with AUTOCATLEMMA+CLUST>200 (table
4). This means that despite any imperfections in the
82
DFL+CLUST>200 AUTOCATLEMMA+CLUST>200
FREQUENCY RANGE percentage UAS Tagging percentage UAS Tagging
in modified training set of dev set of dev set
any 100 90.68 96.45 100 90.39 96.78
0 (effective unseen) 0.09 86.21 58.62 0.08 84.00 40.00
0 < x ? 5 0.45 88.19 86.81 0.32 70.30 70.30
5 < x ? 10 0.64 90.69 91.18 0.37 92.24 79.31
10 < x ? 20 1.31 90.12 94.22 0.87 89.53 88.09
20 < x ? 50 4.88 88.64 92.19 3.30 86.44 92.65
x ? 50 92.62 90.81 96.83 95.07 90.60 97.21
replaced by UNKC* 8.58 90.64 88.10 8.73 89.67 90.07
Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of fre-
quencies in the modified training sets. The ?replaced by UNKC*? line corresponds to the case where the desinflected
form or the POS+lemma pair does not appear more than 200 times in the L?est R?publicain corpus.
unsupervised Brown clustering, which uses very lo-
cal information, the higher counts lead to better es-
timates even for high-frequency words.
8 Related work
We have already cited the previous work of Koo et
al. (2008) which has directly inspired ours. Sagae
and Gordon (2009) explores the use of syntactic
clustering to improve transition-based dependency
parsing for English : using an available 30 mil-
lion word corpus parsed with a constituency parser,
words are represented as vectors of paths within the
obtained constituency parses. Words are then clus-
tered using a similarity metric between vectors of
syntactic paths. The clusters are used as features to
help a transition-based dependency parser. Note that
the word representation for clustering is more com-
plex (paths in parse trees), thus these authors have
to cluster a smaller vocabulary : the top 5000 most
frequent words are clustered.
Agirre et al (2008) use the same approach of re-
placing words by more general symbols, but these
symbols are semantic classes. They test various
methods to assign semantic classes (gold seman-
tic class, most-frequent sense in sense-tagged data,
or a fully unsupervised sense tagger). Though the
method is very appealing, the reported improvement
in parsing is rather small, especially for the fully un-
supervised method.
Versley and Rehbein (2009) cluster words accord-
ing to linear context features, and use the clusters
as features to boost discriminative German parsing
for unknown words. Another approach to augment
the known vocabulary for a generative probabilistic
parser is the one pursued in (Goldberg et al, 2009).
Within a plain PCFG, the lexical probabilities for
words that are rare or absent in the treebank are
taken from an external lexical probability distribu-
tion, estimated using a lexicon and the Baulm-Welch
training of an HMM tagger. This is proven useful to
better parse Hebrew.
9 Conclusion and future work
We have provided a thorough study of the results
of parsing word clusters for French. We showed
that the clustering improves performance both for
unseen and rare words and for medium- to high-
frequency words. For French, preprocessing words
with desinflection or with tagging+lemmatisation
lead to comparable results. However, the method
using POS tagging is expected to yield higher per-
formance should a better tagger become available in
the future.
One avenue for further improvement is to use a
clustering technique that makes explicit use of syn-
tactic or semantic similarity, instead of simple linear
context sharing. While the Brown clustering algo-
rithm can be run on large raw corpus, it uses ex-
tremely local information (bigrams). The resulting
clusters are thus necessarily noisy, and semantic or
syntactic clustering would certainly be more appro-
priate. Since resource-based semantic clustering is
difficult for French due to a lack of resources, clus-
tering based on distributional syntactic similarity is
a worthwhile technique to investigate in the future.
83
Acknowledgments
This work was supported by the ANR Sequoia
(ANR-08-EMER-013). We are grateful to our
anonymous reviewers for their comments and to
Grzegorz Chrupala for helping us with Morfette.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer, Dor-
drecht.
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment perfor-
mance with sense information. In Proceedings of
ACL-08: HLT, pages 317?325, Columbus, Ohio, June.
Association for Computational Linguistics.
Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational
linguistics, 18(4):467?479.
Marie Candito and Beno?t Crabb?. 2009. Im-
proving generative statistical parsing with semi-
supervised word clustering. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 138?141, Paris, France, Octo-
ber. Association for Computational Linguistics.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical french dependency parsing : Treebank
conversion and first results. In Proceedings of
LREC?2010, Valletta, Malta.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Proc.
of PACLIC, Hong Kong, China.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine learning, 37(3):277?296.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In Proc. of EACL-09, pages 327?335, Athens, Greece.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08, pages 595?603, Columbus, USA.
Percy Liang. 2005. Semi-supervised learning for natural
language. In MIT Master?s thesis, Cambridge, USA.
D.M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proc. of ACL?95, pages 276?283,
Morristown, NJ, USA.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages 75?
82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL-06, Sydney,
Australia.
Kenji Sagae and Andrew S. Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Proceed-
ings of the 11th International Conference on Pars-
ing Technologies (IWPT?09), pages 192?201, Paris,
France, October. Association for Computational Lin-
guistics.
Beno?t Sagot. 2010. The Lefff, a freely available
and large-coverage morphological and syntactic lexi-
con for french. In Proceedings of LREC?10, Valetta,
Malta.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
84
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 85?93,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Lemmatization and Lexicalized Statistical Parsing of Morphologically Rich
Languages: the Case of French
Djam? Seddah
Alpage Inria & Univ. Paris-Sorbonne
Paris, France
Grzegorz Chrupa?a
Spoken Language System, Saarland Univ.
Saarbr?cken, Germany
?zlem ?etinog?lu and Josef van Genabith
NCLT & CNGL, Dublin City Univ.
Dublin, Ireland
Marie Candito
Alpage Inria & Univ. Paris 7
Paris, France
Abstract
This paper shows that training a lexicalized
parser on a lemmatized morphologically-rich
treebank such as the French Treebank slightly
improves parsing results. We also show that
lemmatizing a similar in size subset of the En-
glish Penn Treebank has almost no effect on
parsing performance with gold lemmas and
leads to a small drop of performance when au-
tomatically assigned lemmas and POS tags are
used. This highlights two facts: (i) lemmati-
zation helps to reduce lexicon data-sparseness
issues for French, (ii) it also makes the pars-
ing process sensitive to correct assignment of
POS tags to unknown words.
1 Introduction
Large parse-annotated corpora have led to an explo-
sion of interest in statistical parsing methods, includ-
ing the development of highly successful models for
parsing English using the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1994)). Over the
last 10 years, parsing performance on the PTB has
hit a performance plateau of 90-92% f-score using
the PARSEVAL evaluation metric. When adapted to
other language/treebank pairs (such as German, He-
brew, Arabic, Italian or French), to date these mod-
els have performed much worse.
A number of arguments have been advanced
to explain this performance gap, including limited
amounts of training data, differences in treebank an-
notation schemes, inadequacies of evaluation met-
rics, linguistic factors such as the degree of word or-
der variation, the amount of morphological informa-
tion available to the parser as well as the effects of
syncretism prevalent in many morphologically rich
languages.
Even though none of these arguments in isola-
tion can account for the systematic performance gap,
a pattern is beginning to emerge: morphologically
rich languages tend to be susceptible to parsing per-
formance degradation.
Except for a residual clitic case system, French
does not have explicit case marking, yet its mor-
phology is considerably richer than that of English,
and French is therefore a candidate to serve as an
instance of a morphologically rich language (MRL)
that requires specific treatment to achieve reasonable
parsing performance.
Interestingly, French also exhibits a limited
amount of word order variation occurring at dif-
ferent syntactic levels including (i) the word level
(e.g. pre or post nominal adjective, pre or post ver-
bal adverbs); (ii) phrase level (e.g. possible alterna-
tions between post verbal NPs and PPs). In order
to avoid discontinuous constituents as well as traces
and coindexations, treebanks for this language, such
as the French Treebank (FTB, (Abeill? et al, 2003))
or the Modified French Treebank (MFT, (Schluter
and van Genabith, 2007)), propose a flat annota-
tion scheme with a non-configurational distinction
between adjunct and arguments.
Finally, the extraction of treebank grammars from
the French treebanks, which contain less than a third
of the annotated data as compared to PTB, is subject
to many data sparseness issues that contribute to a
performance ceiling, preventing the statistical pars-
ing of French to reach the same level of performance
as for PTB-trained parsers (Candito et al, 2009).
This data sparseness bottleneck can be summa-
rized as a problem of optimizing a parsing model
along two axes: the grammar and the lexicon. In
both cases, the goal is either to get a more compact
grammar at the rule level or to obtain a consider-
85
ably less sparse lexicon. So far, both approaches
have been tested for French using different means
and with different degrees of success.
To obtain better grammars, Schluter and van Gen-
abith (2007) extracted a subset of an early release
of the FTB and carried out extensive restructuring,
extensions and corrections (referred to as the Modi-
fied French Treebank MFT) to support grammar ac-
quisition for PCFG-based LFG Parsing (Cahill et
al., 2004) while Crabb? and Candito (2008) slightly
modified the original FTB POS tagset to optimize
the grammar with latent annotations extracted by the
Berkeley parser (BKY, (Petrov et al, 2006)).
Moreover, research oriented towards adapting
more complex parsing models to French showed
that lexicalized models such as Collins? model 2
(Collins, 1999) can be tuned to cope effectively with
the flatness of the annotation scheme in the FTB,
with the Charniak model (Charniak, 2000) perform-
ing particularly well, but outperformed by the BKY
parser on French data (Seddah et al, 2009).
Focusing on the lexicon, experiments have been
carried out to study the impact of different forms of
word clustering on the BKY parser trained on the
FTB. Candito et al (2009) showed that using gold
lemmatization provides a significant increase in per-
formance. Obviously, less sparse lexical data which
retains critical pieces of information can only help a
model to perform better. This was shown in (Candito
and Crabb?, 2009) where distributional word clus-
ters were acquired from a 125 million words corpus
and combined with inflectional suffixes extracted
from the training data. Training the BKY parser
with 1000 clusters boosts its performance to the cur-
rent state-of-the-art with a PARSEVAL F1 score of
88.28% (baseline was 86.29 %).
We performed the same experiment using the
CHARNIAK parser and recorded only a small im-
provement (from 84.96% to 85.51%). Given the
fact that lexical information is crucial for lexicalized
parsers in the form of bilexical dependencies, this
result raises the question whether this kind of clus-
tering is in fact too drastic for lexicalized parsers as
it may give rise to head-to-head dependencies which
are too coarse. To answer this question, in this paper
we explore the impact of lemmatization, as a (rather
limited) constrained form of clustering, on a state-
of-the-art lexicalized parser (CHARNIAK). In order
to evaluate the influence of lemmatization on this
parser (which is known to be highly tuned for En-
glish) we carry out experiments on both the FTB and
on a lemmatized version of the PTB. We used gold
lemmatization when available and an automatic sta-
tistical morphological analyzer (Chrupa?a, 2010) to
provide more realistic parsing results.
The idea is to verify whether lemmatization will help
to reduce data sparseness issues due to the French
rich morphology and to see if this process, when
applied to English will harm the performance of a
parser optimized for the limited morphology of En-
glish.
Our results show that the key issue is the way un-
seen tokens (lemmas or words) are handled by the
CHARNIAK parser. Indeed, using pure lemma is
equally suboptimal for both languages. On the other
hand, feeding the parser with both lemma and part-
of-speech slightly enhances parsing performance for
French.
We first describe our data sets in Section 2, intro-
duce our data driven morphology process in Section
3, then present experiments in Section 4. We dis-
cuss our results in Section 5 and compare them with
related research in Section 6 before concluding and
outlining further research.
2 Corpus
THE FRENCH TREEBANK is the first annotated and
manually corrected treebank for French. The data is
annotated with labeled constituent trees augmented
with morphological annotations and functional an-
notations of verbal dependents. Its key properties,
compared with the PTB, are the following :
Size: The FTB consists of 350,931 tokens and
12,351 sentences, that is less than a third of the size
of PTB. The average length of a sentence is 28.41
tokens. By contrast, the average sentence length in
the Wall Street Journal section of the PTB is 25.4
tokens.
A Flat Annotation Scheme: Both the FTB and the
PTB are annotated with constituent trees. However,
the annotation scheme is flatter in the FTB. For in-
stance, there are no VPs for finite verbs and only one
sentential level for clauses or sentences whether or
not they are introduced by a complementizer. Only
the verbal nucleus (VN) is annotated and comprises
86
the verb, its clitics, auxiliaries, adverbs and nega-
tion.
Inflection: French morphology is richer than En-
glish and leads to increased data sparseness for sta-
tistical parsing. There are 24,098 lexical types in
the FTB, with an average of 16 tokens occurring for
each type.
Compounds: Compounds are explicitly annotated
and very frequent in the treebank: 14.52% of to-
kens are part of a compound. Following Candito
and Crabb? (2009), we use a variation of the tree-
bank where compounds with regular syntactic pat-
terns have been expanded. We refer to this instance
as FTB-UC.
Lemmatization: Lemmas are included in the tree-
bank?s morphological annotations and denote an ab-
straction over a group of inflected forms. As there
is no distinction between semantically ambiguous
lexemes at the word form level, polysemic homo-
graphs with common inflections are associated with
the same lemma (Abeill? et al, 2003). Thus, except
for some very rare cases, a pair consisting of a word
form and its part-of-speech unambiguously maps to
the same lemma.
2.1 Lemmatizing the Penn Treebank
Unlike the FTB, the PTB does not have gold lem-
mas provided within the treebank. We use the finite
state morphological analyzer which comes within
the English ParGram Grammar (Butt et al, 1999) for
lemmatization. For open class words (nouns, verbs,
adjectives, adverbs) the word form is sent to the mor-
phological analyzer. The English ParGram morpho-
logical analyzer outputs all possible analyses of the
word form. The associated gold POS from the PTB
is used to disambiguate the result. The same process
is applied to closed class words where the word form
is different from the lemma (e.g. ?ll for will). For the
remaining parts of speech the word form is assigned
to the lemma.
Since gold lemmas are not available for the PTB,
a large-scale automatic evaluation of the lemmatizer
is not possible. Instead, we conducted two manual
evaluations. First, we randomly extracted 5 sam-
ples of 200 <POS,word> pairs from Section 23 of
the PTB. Each data set is fed into the lemmatiza-
tion script, and the output is manually checked. For
the 5x200 <POS,word> sets the number of incorrect
lemmas is 1, 3, 2, 0, and 2. The variance is small
indicating that the results are fairly stable. For the
second evaluation, we extracted each unseen word
from Section 23 and manually checked the accuracy
of the lemmatization. Of the total of 1802 unseen
words, 394 words are associated with an incorrect
lemma (331 unique) and only 8 with an incorrect
<POS,lemma> pair (5 unique). For an overall un-
seen word percentage of 3.22%, the lemma accu-
racy is 77.70%. If we assume that all seen words
are correctly lemmatized, overall accuracy would be
99.28%.
2.2 Treebank properties
In order to evaluate the influence of lemmatization
on comparable corpora, we extracted a random sub-
set of the PTB with properties comparable to the
FTB-UC (mainly with respect to CFG size and num-
ber of tokens). We call this PTB subset S.PTB. Ta-
ble 1 presents a summary of some relevant features
of those treebanks.
FTBUC S.PTB PTB
# of tokens 350,931 350,992 1,152,305
# of sentences 12,351 13,811 45,293
average length 28,41 25.41 25.44
CFG size 607,162 638,955 2,097,757
# unique CFG rules 43,413 46,783 91,027
# unique word forms 27,130 26,536 47,678
# unique lemmas 17,570 20,226 36,316
ratio words/lemma 1.544 1.311 1.312
Table 1: French and Penn Treebanks properties
Table 1 shows that the average number of word
forms associated with a lemma (i.e. the lemma ratio)
is higher in the FTB-UC (1.54 words/lemma) than in
the PTB (1.31). Even though the PTB ratio is lower,
it is still large enough to suggest that even the limited
English morphology should be taken into account
when aiming at reducing lexicon sparseness.
Trying to learn French and English morphology
in a data driven fashion in order to predict lemma
from word forms is the subject of the next section.
3 Morphology learning
In order to assign morphological tags and lemmas
to words we use the MORFETTE model (Chrupa?a,
2010), which is a variation of the approach described
in (Chrupa?a et al, 2008).
87
MORFETTE is a sequence labeling model which
combines the predictions of two classification mod-
els (one for morphological tagging and one for
lemmatization) at decoding time, using beam search.
3.1 Overview of the Morfette model
The morphological classes correspond simply to the
(fine-grained) POS tags. Lemma classes are edit
scripts computed from training data: they specify
which string manipulations (such as character dele-
tions and insertions) need to be performed in order
to transform the input string (word form) into the
corresponding output string (lemma).
The best sequence of lemmas and morphological
tags for input sentence x is defined as:
(?l, m?) = arg max
(l,m)
P (l,m|x)
The joint probability is decomposed as follows:
P (l0...li,m0...mi|x) =PL(li|mi,x)PM (mi|x)
? P (m0...mi?1, l0...li?1|x)
where PL(li|mi,x) is the probability of lemma class
l at position i according to the lemma classifier,
PM (mi|x) is the probability of the tag m at posi-
tion i according to the morphological tag classifier,
and x is the sequence of words to label.
While Chrupa?a et al (2008) use Maximum En-
tropy training to learn PM and PL, here we learn
them using Averaged Perceptron algorithm due to
Freund and Schapire (1999). It is a much simpler
algorithm which in many scenarios (including ours)
performs as well as or better than MaxEnt.
We also use the general Edit Tree instantiation of
the edit script as developed in (Chrupa?a, 2008). We
find the longest common substring (LCS) between
the form w and the lemma w?. The portions of the
string in the word form before (prefix) and after (suf-
fix) the LCS need to be modified in some way, while
the LCS (stem) stays the same. If there is no LCS,
then we simply record that we need to replace w
with w? . As for the modifications to the prefix and
the suffix, we apply the same procedure recursively:
we try to find the LCS between the prefix of w and
the prefix of w?. If we find one, we recurse; if we do
not, we record the replacement; we do the same for
the suffix.
3.2 Data Set
We trained MORFETTE on the standard splits of the
FTB with the first 10% as test set, the next 10% for
the development set and the remaining for training
(i.e. 1235/1235/9881 sentences). Lemmas and part-
of-speech tags are given by the treebank annotation
scheme.
As pointed out in section 2.1, PTB?s lemmas have
been automatically generated by a deterministic pro-
cess, and only a random subset of them have been
manually checked. For the remainder of this paper,
we treat them as gold, regardless of the errors in-
duced by our PTB lemmatizer.
The S.PTB follows the same split as the FTB-UC,
first 10% for test, next 10% for dev and the last 80%
for training (i.e. 1380/1381/11050 sentences).
MORFETTE can optionally use a morphological
lexicon to extract features. For French, we used the
extended version of Lefff (Sagot et al, 2006) and for
English, the lexicon used in the Penn XTAG project
(Doran et al, 1994). We reduced the granularity of
the XTAG tag set, keeping only the bare categories.
Both lexicons contain around 225 thousands word
form entries.
3.3 Performance on French and English
Table 2 presents results of MORFETTE applied to the
development and test sets of our treebanks. Part-of-
speech tagging performance for French is state-of-
the-art on the FTB-UC, with an accuracy of 97.68%,
on the FTB-UC test set, only 0.02 points (absolute)
below the MaxEnt POS tagger of Denis and Sagot
(2009). Comparing MORFETTE?s tagging perfor-
mance for English is a bit more challenging as we
only trained on one third of the full PTB and evalu-
ated on approximately one section, whereas results
reported in the literature are usually based on train-
ing on sections 02-18 and evaluating on either sec-
tions 19-21 or 22-24. For this setting, state-of-the-
art POS accuracy for PTB tagging is around 97.33%.
On our PTB sample, MORFETTE achieves 96.36%
for all words and 89.64 for unseen words.
Comparing the lemmatization performance for both
languages on the same kind of data is even more dif-
ficult as we are not aware of any data driven lem-
matizer on the same data. However, with an overall
accuracy above 98% for the FTB-UC (91.5% for un-
88
seen words) and above 99% for the S.PTB (95% for
unseen words), lemmatization performs well enough
to properly evaluate parsing on lemmatized data.
FTBUC S.PTB
DEV All Unk. (4.8) All Unk. (4.67)
POS acc 97.38 91.95 96.36 88.90
Lemma acc 98.20 92.52 99.11 95.51
Joint acc 96.35 87.16 96.26 87.05
TEST All Unk. (4.62) All Unk. (5.04)
POS acc 97.68 90.52 96.53 89.64
Lemma acc 98.36 91.54 99.13 95.72
Joint acc 96.74 85.28 96.45 88.49
Table 2: POS tagging and lemmatization performance on
the FTB and on the S.PTB
4 Parsing Experiments
In this section, we present the results of two sets
of experiments to evaluate the impact of lemmatiza-
tion on the lexicalized statistical parsing of two lan-
guages, one morphologically rich (French), but with
none of its morphological features exploited by the
CHARNIAK parser, the other (English) being quite
the opposite, with the parser developed mainly for
this language and PTB annotated data. We show that
lemmatization results in increased performance for
French, while doing the same for English penalizes
parser performance.
4.1 Experimental Protocol
Data The data sets described in section 3.2 are used
throughout. The version of the CHARNIAK parser
(Charniak, 2000) was released in August 2005 and
recently adapted to French (Seddah et al, 2009).
Metrics We report results on sentences of length
less than 40 words, with three evaluation met-
rics: the classical PARSEVAL Labeled brackets F1
score, POS tagging accuracy (excluding punctua-
tion tags) and the Leaf Ancestor metric (Sampson
and Babarczy, 2003) which is believed to be some-
what more neutral with respect to the treebank an-
notation scheme than PARSEVAL (Rehbein and van
Genabith, 2007).
Treebank tag sets Our experiments involve the in-
clusion of POS tags directly in tokens. We briefly
describe our treebank tag sets below.
? FTB-UC TAG SET: ?CC? This is the tag set de-
veloped by (Crabb? and Candito, 2008) (Table
4), known to provide the best parsing perfor-
mance for French (Seddah et al, 2009). Like in
the FTB, preterminals are the main categories,
but they are also augmented with a WH flag
for A, ADV, PRO and with the mood for verbs
(there are 6 moods). No information is propa-
gated to non-terminal symbols.
ADJ ADJWH ADV ADVWH CC CLO CLR CLS CS DET
DETWH ET I NC NPP P P+D P+PRO PONCT PREF PRO
PROREL PROWH V VIMP VINF VPP VPR VS
Table 4: CC tag set
? THE PTB TAG SET This tag set is described
at length in (Marcus et al, 1994) and contains
supplementary morphological information (e.g.
number) over and above what is represented in
the CC tag set for French. Note that some infor-
mation is marked at the morphological level in
English (superlative, ?the greatest (JJS)?) and
not in French (? le plus (ADV) grand (ADJ)?).
CC CD DT EX FW IN JJ JJR JJS LS MD NN NNP NNPS
NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH
VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB
Table 5: PTB tag set
4.2 Cross token variation and parsing impact
From the source treebanks, we produce 5 versions
of tokens: tokens are generated as either simple
POS tag, gold lemma, gold lemma+gold POS, word
form, and word form+gold POS. The token versions
successively add more morphological information.
Parsing results are presented in Table 3.
Varying the token form The results show that
having no lexical information at all (POS-only) re-
sults in a small drop of PARSEVAL performance for
French compared to parsing lemmas, while the cor-
responding Leaf Ancestor score is actually higher.
For English having no lexical information at all
leads to a drop of 2 points in PARSEVAL. The so-
called impoverished morphology of English appears
to bring enough morphological information to raise
tagging performance to 95.92% (from POS-only to
word-only).
For French the corresponding gain is only 2 points
of POS tagging accuracy. Moreover, between these
89
Tokens
POS-only
lemma-only
word-only
(1)lemma-POS
(1)word-POS
French Treebank UC
F1 score Pos acc. leaf-Anc.
84.48 100 93.97
84.77 94.23 93.76
84.96 96.26 94.08
86.83(1) 98.79 94.65
86.13(2) 98.4 94.46
Sampled Penn Treebank
F1 score Pos acc. leaf-Anc.
85.62 100 94.02
87.69 89.22 94.92
88.64 95.92 95.10
89.59(3) 99.97 95.41
89.53(4) 99.96 95.38
Table 3: Parsing performance on the FTB-UC and the S.PTB with tokens variations using gold lemmas and gold POS.
( p-value (1) & (2) = 0.007; p-value (3) & (4) = 0.146. All other configurations are statistically significant.)
two tokens variations, POS-only and word-only,
parsing results gain only half a point in PARSEVAL
and almost nothing in leaf Ancestor.
Thus, it seems that encoding more morphology
(i.e. including word forms) in the tokens does not
lead to much improvement for parsing French as op-
posed to English. The reduction in data sparseness
due to the use of lemmas alone is thus not sufficient
to counterbalance the lack of morphological infor-
mation.
However, the large gap between POS tagging
accuracy seen between lemma-only and word-only
for English indicates that the parser makes use of
this information to provide at least reasonable POS
guesses.
For French, only 0.2 points are gained for PAR-
SEVAL results between lemma-only to word-only,
while POS accuracy benefits a bit more from includ-
ing richer morphological information.
This raises the question whether the FTB-UC pro-
vides enough data to make its richer morphology in-
formative enough for a parsing model.
Suffixing tokens with POS tags It is only when
gold POS are added to the lemmas that one can see
the advantage of a reduced lexicon for French. In-
deed, performance peaks for this setting (lemma-
POS). The situation is not as clear for English, where
performance is almost identical when gold POS are
added to lemmas or words. POS Tagging is nearly
perfect, thus a performance ceiling is reached. The
very small differences between those two configura-
tions (most noticeable with the Leaf Ancestor score
of 95.41 vs. 95.38) indicates that the reduced lemma
lexicon is actually of some limited use but its impact
is negligible compared to perfect tagging.
While the lemma+POS setting clearly boosts per-
formance for parsing the FTB, the situation is less
clear for English. Indeed, the lemma+POS and the
word+POS gold variations give almost the same re-
sults. The fact that the POS tagging accuracy is close
to 100% in this mode shows that the key parameter
for optimum parsing performance in this experiment
is the ability to guess POS for unknown words well.
In fact, the CHARNIAK parser uses a two letter
suffix context for its tagging model, and when gold
POS are suffixed to any type of token (being lemma
or word form), the PTB POS tagset is used as a sub-
stitute for lack of morphology.
It should also be noted that the FTB-UC tag set
does include some discriminative features (such as
PART, INF and so on) but those are expressed by
more than two letters, and therefore a two letter
suffix tag cannot really be useful to discriminate
a richer morphology. For example, in the PTB,
the suffix BZ, as in VBZ, always refers to a verb,
whereas the FTB pos tag suffix PP, as in NPP
(Proper Noun) is also found in POS labels such as
VPP (past participle verb).
4.3 Realistic Setup: Using Morfette to help
parsing
Having shown that parsing French benefits from a
reduced lexicon is not enough as results imply that a
key factor is POS tag guessing. We therefore test our
hypothesis in a more realistic set up. We use MOR-
FETTE to lemmatize and tag raw words (instead of
the ?gold? lemma-based approach described above),
and the resulting corpus is then parsed using the cor-
responding training set.
In order to be consistent with PARSEVAL POS eval-
uation, which does not take punctuation POS into
account, we provide a summary of MORFETTE?s
performance for such a configuration in (Table 6).
Results shown in Table 7 confirm our initial hy-
90
POS acc Lemma acc Joint acc
FTB-UC 97.34 98.12 96.26
S.PTB 96.15 99.04 96.07
Table 6: PARSEVAL Pos tagging accuracy of treebanks
test set
pothesis for French. Indeed, parsing performance
peaks with a setup involving automatically gener-
ated lemma and POS pairs, even though the differ-
ence with raw words+auto POS is not statistically
significant for the PARSEVAL F1 metric1. Note that
parser POS accuracy does not follow this pattern. It
is unclear exactly why this is the case. We specu-
late that the parser is helped by the reduced lexicon
but that performance suffers when a <lemma,POS>
pair has been incorrectly assigned by MORFETTE,
leading to an increase in unseen tokens. This is con-
firmed by parsing the same lemma but with gold
POS. In that case, parsing performance does not suf-
fer too much from CHARNIAK?s POS guessing on
unseen data.
For the S.PTB, results clearly show that both the
automatic <lemma,POS> and <word,POS> config-
urations lead to very similar results (yet statistically
significant with a F1 p-value = 0.027); having the
same POS accuracy indicates that most of the work
is done at the level of POS guessing for unseen
tokens, and in this respect the CHARNIAK parser
clearly takes advantage of the information included
in the PTB tag set.
F1 score Pos acc. leaf-Anc.
S.PTB
auto lemma only 87.11 89.82 94.71
auto lemma+auto pos (a) 88.15 96.21 94.85
word +auto pos (b) 88.28 96.21 94.88
F1 p-value: (a) and (b) 0.027
auto lemma+gold pos 89.51 99.96 95,36
FTB-UC
auto lemma only 83.92 92.98 93.53
auto lemma+auto pos (c) 85.06 96.04 94.14
word +auto pos (d) 84.99 96.47 94.09
F1 p-value: (c) and (d) 0.247
auto lemma+gold pos 86.39 97.35 94.68
Table 7: Realistic evaluation of parsing performance
1Statistical significance is computed using Dan Bikel?s
stratified shuffling implementation: www.cis.upenn.edu/
~dbikel/software.html.
5 Discussion
When we started this work, we wanted to explore
the benefit of lemmatization as a means to reduce
data sparseness issues underlying statistical lexical-
ized parsing of small treebanks for morphologically
rich languages, such as the FTB. We showed that
the expected benefit of lemmatization, a less sparse
lexicon, was in fact hidden by the absence of inflec-
tional information, as required by e.g. the CHAR-
NIAK parser to provide good POS guesses for un-
seen words. Even the inclusion of POS tags gen-
erated by a state-of-the-art tagger (MORFETTE) did
not lead to much improvement compared to a parser
run in a regular bare word set up.
An unexpected effect is that the POS accuracy
of the parser trained on the French data does not
reach the same level of performance as our tag-
ger (96.47% for <word, auto POS> vs. 97.34% for
MORFETTE). Of course, extending the CHARNIAK
tagging model to cope with lemmatized input should
be enough, because its POS guessing model builds
on features such as capitalization, hyphenation and
a two-letter suffix (Charniak, 2000). Those features
are not present in our current lemmatized input and
thus cannot be properly estimated.
CHARNIAK also uses the probability that a given
POS is realized by a previously unobserved word.
If any part of a <lemma,POS> pair is incorrect, the
number of unseen words in the test set would be
higher than the one estimated from the training set,
which only contained correct lemmas and POS tags
in our setting. This would lead to unsatisfying POS
accuracy. This inadequate behavior of the unknown
word tagging model may be responsible for the POS
accuracy result for <auto lemma> (cf. Table 7, lines
<auto lemma only> for both treebanks).
We believe that this performance degradation (or
in this case the somewhat less than expected im-
provement in parsing results) calls for the inclusion
of all available lexical information in the parsing
model. For example, nothing prevents a parsing
model to condition the generation of a head upon
a lemma, while the probability to generate a POS
would depend on both morphological features and
(potentially) the supplied POS.
91
6 Related Work
A fair amount of recent research in parsing morpho-
logically rich languages has focused on coping with
unknowns words and more generally with the small
and limited lexicons acquired from treebanks. For
instance, Goldberg et al (2009) augment the lex-
icon for a generative parsing model by including
lexical probabilities coming from an external lexi-
con. These are estimated using an HMM tagger with
Baum-Welch training. This method leads to a sig-
nificant increase of parsing performance over pre-
viously reported results for Modern Hebrew. Our
method is more stratified: external lexical resources
are included as features for MORFETTE and there-
fore are not directly seen by the parser besides gen-
erated lemma and POS.
For parsing German, Versley and Rehbein (2009)
cluster words according to linear context features.
The clusters are then integrated as features to boost a
discriminative parsing model to cope with unknown
words. Interestingly, they also include all possible
information: valence information, extracted from a
lexicon, is added to verbs and preterminal nodes are
annotated with case/number. This leads their dis-
criminative model to state-of-the-art results for pars-
ing German.
Concerning French, Candito and Crabb? (2009)
present the results of different clustering methods
applied to the parsing of FTB with the BKY parser.
They applied an unsupervised clustering algorithm
on the 125 millions words ?Est Republicain? corpus
to get a reduced lexicon of 1000 clusters which they
then augmented with various features such as capi-
talization and suffixes. Their method is the best cur-
rent approach for the probabilistic parsing of French
with a F1 score (<=40) of 88.29% on the standard
test set. We run the CHARNIAK parser on their clus-
terized corpus. Table 8 summarizes the current state-
of-the-art for lexicalized parsing on the FTB-UC.2
Clearly, the approach consisting in extending clus-
ters with features and suffixes seems to improve
CHARNIAK?s performance more than our method.
2For this comparison, we also trained the CHARNIAK parser
on a disinflected variation of the FTB-UC. Disinflection is a de-
terministic, lexicon based process, standing between stemming
and lemmatization, which preserves POS assignment ambigui-
ties (Candito and Crabb?, 2009).
In that case, the lexicon is drastically reduced, as
well as the amount of out of vocabulary words
(OOVs). Nevertheless, the relatively low POS ac-
curacy, with only 36 OOVs, for this configuration
confirms that POS guessing is the current bottleneck
if a process of reducing the lexicon increases POS
assignment ambiguities.
tokens F1 Pos acc % of OOVs
raw word (a) 84.96 96.26 4.89
auto <lemma,pos> (b) 85.06 96.04 6.47
disinflected (c) 85.45 96.51 3.59
cluster+caps+suffixes (d) 85.51 96.89 0.10
Table 8: CHARNIAK parser performance summary on the
FTB-UC test set (36340 tokens). Compared to (a), all F1 re-
sults, but (b), are statistically significant (p-values < 0.05), dif-
ferences between (c) & (d), (b) & (c) and (b) & (d) are not
(p-values are resp. 0.12, 0.41 and 0.11). Note that the (b) &
(d) p-value for all sentences is of 0.034, correlating thus the
observed gap in parsing performance between these two con-
figuration.
7 Conclusion
We showed that while lemmatization can be of
some benefit to reduce lexicon size and remedy data
sparseness for a MRL such as French, the key factor
that drives parsing performance for the CHARNIAK
parser is the amount of unseen words resulting from
the generation of <lemma,POS> pairs for the FTB-
UC. For a sample of the English PTB, morphologi-
cal analysis did not produce any significant improve-
ment.
Finally, even if this architecture has the potential to
help out-of-domain parsing, adding morphological
analysis on top of an existing highly tuned statisti-
cal parsing system can result in suboptimal perfor-
mance. Thus, in future we will investigate tighter
integration of the morphological features with the
parsing model.
Acknowledgments
D. Seddah and M. Candito were supported by the ANR
Sequoia (ANR-08-EMER-013); ?. ?etinog?lu and J.
van Genabith by the Science Foundation Ireland (Grant
07/CE/I1142) as part of the Centre for Next Generation
Localisation at Dublin City University; G. Chrupa?a by
BMBF project NL-Search (contract 01IS08020B).
92
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer, Dor-
drecht.
Miriam Butt, Mar?a-Eugenia Ni?o, and Fr?d?rique
Segond. 1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 320?
327, Barcelona, Spain.
Marie Candito and Beno?t Crabb?. 2009. Im-
proving generative statistical parsing with semi-
supervised word clustering. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 138?141, Paris, France, Octo-
ber. Association for Computational Linguistics.
Marie Candito, Benoit Crabb?, and Djam? Seddah. 2009.
On statistical parsing of french with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL 2000), pages 132?
139, Seattle, WA.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupa?a. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
Grzegorz Chrupa?a. 2010. Morfette: A tool for su-
pervised learning of morphology. http://sites.
google.com/site/morfetteweb/. Version
0.3.1.
Michael Collins. 1999. Head Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Proc.
of PACLIC, Hong Kong, China.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: A wide
coverage grammar for english. In Proceedings of the
15th conference on Computational linguistics, pages
922?928, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine learning, 37(3):277?296.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In Proc. of EACL-09, pages 327?335, Athens, Greece.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for german.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague.
Benoit Sagot, Lionel Cl?ment, Eric V. de La Clergerie,
and Pierre Boullier. 2006. The lefff 2 syntactic lexi-
con for french: Architecture, acquisition, use. Proc. of
LREC 06, Genoa, Italy.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
93
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1?11,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Probabilistic Lexical Generalization for French Dependency Parsing
Enrique Henestroza Anguiano and Marie Candito
Alpage (Universite? Paris Diderot / INRIA)
Paris, France
enrique.henestroza anguiano@inria.fr, marie.candito@linguist.jussieu.fr
Abstract
This paper investigates the impact on French
dependency parsing of lexical generalization
methods beyond lemmatization and morpho-
logical analysis. A distributional thesaurus
is created from a large text corpus and used
for distributional clustering and WordNet au-
tomatic sense ranking. The standard approach
for lexical generalization in parsing is to map
a word to a single generalized class, either re-
placing the word with the class or adding a
new feature for the class. We use a richer
framework that allows for probabilistic gener-
alization, with a word represented as a prob-
ability distribution over a space of general-
ized classes: lemmas, clusters, or synsets.
Probabilistic lexical information is introduced
into parser feature vectors by modifying the
weights of lexical features. We obtain im-
provements in parsing accuracy with some
lexical generalization configurations in exper-
iments run on the French Treebank and two
out-of-domain treebanks, with slightly better
performance for the probabilistic lexical gen-
eralization approach compared to the standard
single-mapping approach.
1 Introduction
In statistical, data-driven approaches to natural lan-
guage syntactic parsing, a central problem is that of
accurately modeling lexical relationships from po-
tentially sparse counts within a training corpus. Our
particular interests are centered on reducing lexical
data sparseness for linear classification approaches
for dependency parsing. In these approaches, linear
models operate over feature vectors that generally
represent syntactic structure within a sentence, and
feature templates are defined in part over the word
forms of one or more tokens in a sentence. Because
treebanks used for training are often small, lexical
features may appear relatively infrequently during
training, especially for languages with richer mor-
phology than English. This may, in turn, impede the
parsing model?s ability to generalize well outside of
its training set with respect to lexical features.
Past approaches for achieving lexical generaliza-
tion in dependency parsing have used WordNet se-
mantic senses in parsing experiments for English
(Agirre et al, 2011), and word clustering over large
corpora in parsing experiments for English (Koo
et al, 2008) as well as for French (Candito et al,
2010b). These approaches map each word to a sin-
gle corresponding generalized class (synset or clus-
ter), and integrate generalized classes into parsing
models in one of two ways: (i) the replacement
strategy, where each word form is simply replaced
with a corresponding generalized class; (ii) a strat-
egy where an additional feature is created for the
corresponding generalized class.
Our contribution in this paper is applying prob-
abilistic lexical generalization, a richer framework
for lexical generalization, to dependency parsing.
Each word form is represented as a categorical dis-
tribution over a lexical target space of generalized
classes, for which we consider the spaces of lemmas,
synsets, and clusters. The standard single-mapping
approach from previous work can be seen as a sub-
case: each categorical distribution assigns a proba-
bility of 1 to a single generalized class. The method
1
we use for introducing probabilistic information into
a feature vector is based on that used by Bunescu
(2008), who tested the use of probabilistic part-of-
speech (POS) tags through an NLP pipeline.
In this paper, we perform experiments for French
that use the replacement strategy for integrating
generalized classes into parsing models, comparing
the single-mapping approach for lexical generaliza-
tion with our probabilistic lexical generalization ap-
proach. In doing so, we provide first results on the
application to French parsing of WordNet automatic
sense ranking (ASR), using the method of McCarthy
et al (2004). For clustering we deviate from most
previous work, which has integrated Brown clusters
(Brown et al, 1992) into parsing models, and instead
use distributional lexical semantics to create both a
distributional thesaurus - for probabilistic general-
ization in the lemma space and ASR calculation -
and to perform hierarchical agglomerative clustering
(HAC). Though unlexicalized syntactic HAC clus-
tering has been used to improve English dependency
parsing (Sagae and Gordon, 2009), we provide first
results on using distributional lexical semantics for
French parsing. We also include an out-of-domain
evaluation on medical and parliamentary text in ad-
dition to an in-domain evaluation.
In Section 2 we describe the lexical target spaces
used in this paper, as well as the method of integrat-
ing probabilistic lexical information into a feature
vector for classification. In Section 3 we discuss de-
pendency structure and transition-based parsing. In
Section 4 we present the experimental setup, which
includes our parser implementation, the construction
of our probabilistic lexical resources, and evaluation
settings. We report parsing results both in-domain
and out-of-domain in Section 5, we provide a sum-
mary of related work in Section 6, and we conclude
in Section 7.
2 Probabilistic Lexical Target Spaces
Using terms from probability theory, we define a lex-
ical target space as a sample space ? over which
a categorical distribution is defined for each lexi-
cal item in a given source vocabulary. Because we
are working with French, a language with relatively
rich morphology, we use lemmas as the base lexi-
cal items in our source vocabulary. The outcomes
contained in a sample space represent generalized
classes in a target vocabulary. In this paper we con-
sider three possible target vocabularies, with cor-
responding sample spaces: ?l for lemmas, ?s for
synsets, and ?c for clusters.
2.1 ?l Lemma Space
In the case of the lemma space, the source and tar-
get vocabularies are the same. To define an ap-
propriate categorical distribution for each lemma,
one where the possible outcomes also correspond to
lemmas, we use a distributional thesaurus that pro-
vides similarity scores for pairs of lemmas. Such
a thesaurus can be viewed as a similarity function
D(x, y), where x, y ? V and V is the vocabulary
for both the source and target spaces.
The simplest way to define a categorical distribu-
tion over ?l, for a lemma x ? V , would be to use
the following probability mass function px:
px(y) =
D(x, y)
?
y??V
D(x, y?)
(1)
One complication is the identity similarity D(x, x):
although it can be set equal to 1 (or the similar-
ity given by the thesaurus, if one is provided), we
choose to assign a pre-specified probability mass m
to the identity lemma, with the remaining mass used
for generalization across other lemmas. Addition-
ally, in order to account for noise in the thesaurus,
we restrict each categorical distribution to a lemma?s
k-nearest neighbors. The probability mass function
px over the space ?l that we use in this paper is fi-
nally as follows:
px(y) =
?
?
?
?
?
?
?
?
?
?
?
m, if y = x
(1?m)D(x, y)
?
y??Nx(k)
D(x, y?)
, if y ? Nx(k)
0, otherwise
(2)
2.2 ?s Synset Space
In the case of the synset space, the target vacabulary
contains synsets from the Princeton WordNet sense
hierarchy (Fellbaum, 1998). To define an appro-
priate categorical distribution over synsets for each
2
lemma x in our source vocabulary, we first use the
WordNet resource to identify the set Sx of different
senses of x. We then use a distributional thesaurus to
perform ASR, which determines the prevalence with
respect to x of each sense s ? Sx, following the
approach of McCarthy et al (2004). Representing
the thesaurus as a similarity function D(x, y), let-
ting Nx(k) be the set of k-nearest neighbors for x,
and letting W (s1, s2) be a similarity function over
synsets in WordNet, we define a prevalence function
Rx(s) as follows:
Rx(s) =
?
y?Nx(k)
D(x, y)
max
s? ? Sy
W (s, s?)
?
t?Sx
max
s? ? Sy
W (t, s?)
(3)
This function essentially weights the semantic con-
tribution that each distributionally-similar neighbor
adds to a given sense for x. With the prevalence
scores of each sense for x having been calculated,
we use the following probability mass function px
over the space ?s, where Sx(k) is the set of k-most
prevalent senses for x:
px(s) =
?
?
?
?
?
?
?
Rx(s)
?
s??Sx(k)
Rx(s?)
, if s ? Sx(k)
0, otherwise
(4)
Note that the first-sense ASR approach to using
WordNet synsets for parsing, which has been previ-
ously explored in the literature (Agirre et al, 2011),
corresponds to setting k=1 in Equation 4.
2.3 ?c Cluster Space
In the case of the cluster space, any approach for
word clustering may be used to create a reduced tar-
get vocabulary of clusters. Defining a categorical
distribution over clusters would be interesting in the
case of soft clustering of lemmas, in which a lemma
can participate in more than one cluster, but we have
not yet explored this clustering approach.
In this paper we limit ourselves to the simpler
hard clustering HAC method, which uses a distri-
butional thesaurus and iteratively joins two clusters
together based on the similarities between lemmas
in each cluster. We end up with a simple probability
mass function px over the space ?c for a lemma x
with corresponding cluster cx:
px(c) =
{
1, if c = cx
0, otherwise (5)
2.4 Probabilistic Feature Generalization
In a typical classifier-based machine learning setting
in NLP, feature vectors are constructed using indi-
cator functions that encode categorical information,
such as POS tags, word forms or lemmas.
In this section we will use a running example
where a and b are token positions of interest to a
classifier, and for which feature vectors are created.
If we let t stand for POS tag and l stand for lemma,
a feature template for this pair of tokens might then
be [talb]. Feature templates are instantiated as ac-
tual features in a vector space depending on the cat-
egorical values they can take on. One possible in-
stantiation of the template [talb] would then be the
feature [ta=verb?lb=avocat], which indicates that a
is a verb and b is the lemma avocat (?avocado? or
?lawyer?), with the following indicator function:
f =
{
1, if ta=verb ? lb=avocat
0, otherwise (6)
To perform probabilistic feature generalization, we
replace the indicator function, which represents a
single original feature, with a collection of weighted
functions representing a set of derived features. Sup-
pose the French lemma avocat is in our source vo-
cabulary and has multiple senses in ?s (s1 for the
?avocado? sense, s2 for the ?lawyer? sense, etc.),
as well as a probability mass function pav. We
discard the old feature [ta=verb?lb=avocat] and
add, for each si, a derived feature of the form
[ta=verb?xb=si], where x represents a target space
generalized class, with the following weighted indi-
cator function:
f(i) =
{
pav(si), if ta=verb ? lb=avocat
0, otherwise (7)
This process extends easily to generalizing multiple
categorical variables. Consider the bilexical feature
[la=manger?lb=avocat], which indicates that a
is the lemma manger (?eat?) and b is the lemma
avocat. If both lemmas manger and avocat appear
3
ouvrit
Elle porte
la
avec
cle?
la
Figure 1: An unlabeled dependency tree for ?Elle ouvrit
la porte avec la cle?? (?She opened the door with the key?).
in our source vocabulary and have multiple senses
in ?s, with probability mass functions pma and pav,
then for each pair i, j we derive a feature of the
form [xa=si?xb=sj], with the following weighted
indicator function:
f(i,j)=
{
pma(si)pav(sj), if la=manger?lb=avocat
0, otherwise (8)
3 Dependency Parsing
Dependency syntax involves the representation of
syntactic information for a sentence in the form of
a directed graph, whose edges encode word-to-word
relationships. An edge from a governor to a de-
pendent indicates, roughly, that the presence of the
dependent is syntactically legitimated by the gover-
nor. An important property of dependency syntax is
that each word, except for the root of the sentence,
has exactly one governor; dependency syntax is thus
represented by trees. Figure 1 shows an example
of an unlabeled dependency tree.1 For languages
like English or French, most sentences can be rep-
resented with a projective dependency tree: for any
edge from word g to word d, g dominates any inter-
vening word between g and d.
Dependency trees are appealing syntactic repre-
sentations, closer than constituency trees to the se-
mantic representations useful for NLP applications.
This is true even with the projectivity requirement,
which occasionally creates syntax-semantics mis-
matches. Dependency trees have recently seen a
surge of interest, particularly with the introduction
of supervised models for dependency parsing using
linear classifiers.
1Our experiments involve labeled parsing, with edges addi-
tionally labeled with the surface grammatical function that the
dependent bears with respect to its governor.
3.1 Transition-Based Parsing
In this paper we focus on transition-based pars-
ing, whose seminal works are that of Yamada and
Matsumoto (2003) and Nivre (2003). The parsing
process applies a sequence of incremental actions,
which typically manipulate a buffer position in the
sentence and a stack for built sub-structures. In the
arc-eager approach introduced by Nivre et al (2006)
the possible actions are as follows, with s0 being the
token on top of the stack and n0 being the next token
in the buffer:
? SHIFT: Push n0 onto the stack.
? REDUCE: Pop s0 from the stack.
? RIGHT-ARC(r): Add an arc labeled r from s0
to n0; push n0 onto the stack.
? LEFT-ARC(r): Add an arc labeled r from n0
to s0; pop s0 from the stack.
The parser uses a greedy approach, where the ac-
tion selected at each step is the best-scoring action
according to a classifier, which is trained on a de-
pendency treebank converted into sequences of ac-
tions. The major strength of this framework is its
O(n) time complexity, which allows for very fast
parsing when compared to more complex global op-
timization approaches.
4 Experimental Setup
We now discuss the treebanks used for training and
evaluation, the parser implementation and baseline
settings, the construction of the probabilistic lexical
resources, and the parameter tuning and evaluation
settings.
4.1 Treebanks
The treebank we use for training and in-domain
evaluation is the French Treebank (FTB) (Abeille?
and Barrier, 2004), consisting of 12,351 sentences
from the Le Monde newspaper, converted to projec-
tive2 dependency trees (Candito et al, 2010a). For
our experiments we use the usual split of 9,881 train-
ing, 1,235 development, and 1,235 test sentences.
2The projectivity constraint is linguistically valid for most
French parses: the authors report < 2% non-projective edges in
a hand-corrected subset of the converted FTB.
4
Moving beyond the journalistic domain, we use
two additional treebank resources for out-of-domain
parsing evaluations. These treebanks are part of
the Sequoia corpus (Candito and Seddah, 2012),
and consist of text from two non-journalistic do-
mains annotated using the FTB annotation scheme:
a medical domain treebank containing 574 develop-
ment and 544 test sentences of public assessment
reports of medicine from the European Medicines
Agency (EMEA) originally collected in the OPUS
project (Tiedemann, 2009), and a parliamentary do-
main treebank containing 561 test sentences from
the Europarl3 corpus.
4.2 Parser and Baseline Settings
We use our own Python implementation of the arc-
eager algorithm for transition-based parsing, based
on the arc-eager setting of MaltParser (Nivre et al,
2007), and we train using the standard FTB training
set. Our baseline feature templates and general set-
tings correspond to those obtained in a benchmark-
ing of parsers for French (Candito et al, 2010b),
under the setting which combined lemmas and mor-
phological features.4 Automatic POS-tagging is per-
formed using MElt (Denis and Sagot, 2009), and
lemmatization and morphological analysis are per-
formed using the Lefff lexicon (Sagot, 2010). Ta-
ble 1 lists our baseline parser?s feature templates.
4.3 Lexical Resource Construction
We now describe the construction of our probabilis-
tic lexical target space resources, whose prerequi-
sites include the automatic parsing of a large corpus,
the construction of a distributional thesaurus, the use
of ASR on WordNet synsets, and the use of HAC
clustering.
4.3.1 Automatically-Parsed Corpus
The text corpus we use consists of 125 mil-
lion words from the L?Est Republicain newspa-
per5, 125 million words of dispatches from the
Agence France-Presse, and 225 million words from
a French Wikipedia backup dump6. The corpus is
3http://www.statmt.org/europarl/
4That work tested the use of Brown clusters, but obtained no
improvement compared to a setting without clusters. Thus, we
do not evaluate Brown clustering in this paper.
5http://www.cnrtl.fr/corpus/estrepublicain/
6http://dumps.wikimedia.org/
Feature Templates
Unigram tn0 ; ln0 ; cn0 ; wn0 ; ts0 ; ls0 ; cs0 ; ws0 ; ds0 ;
tn1 ; ln1 ; tn2 ; tn3 ; ts1 ; ts2 ; tn0l ; ln0l ; dn0l ;
ds0l ; ds0r ; ls0h ; {min0 : i ? |M |};
{mis0 : i ? |M |}
Bigram ts0tn0 ; ts0 ln0 ; ls0 ln0 ; ln0tn1 ; tn0 tn0l ;
tn0dn0l ; {mis0mjn0 : i; j ? |M |};
{tn0min0 : i ? |M |}; {ts0mis0 : i ? |M |}
Trigram ts2ts1 ts0 ; ts1ts0 tn0 ; ts0 tn0tn1 ; tn0 tn1tn2 ;
tn1tn2 tn3 ; ts0ds0lds0r
Table 1: Arc-eager parser feature templates. c = coarse
POS tag, t = fine POS tag, w = inflected word form, l =
lemma, d = dependency label, mi = morphological fea-
ture from set M . For tokens, ni = ith token in the buffer,
si = ith token on the stack. The token subscripts l, r, and
h denote partially-constructed syntactic left-most depen-
dent, right-most dependent, and head, respectively.
preprocessed using the Bonsai tool7, and parsed us-
ing our baseline parser.
4.3.2 Distributional Thesaurus
We build separate distributional thesauri for
nouns and for verbs,8 using straightforward meth-
ods in distributional lexical semantics based primar-
ily on work by Lin (1998) and Curran (2004). We
use the FreDist tool (Henestroza Anguiano and De-
nis, 2011) for thesaurus creation.
First, syntactic contexts for each lemma are ex-
tracted from the corpus. We use all syntactic de-
pendencies in which the secondary token has an
open-class POS tag, with labels included in the con-
texts and two-edge dependencies used in the case of
prepositional-phrase attachment and coordination.
Example contexts are shown in Figure 2. For verb
lemmas we limit contexts to dependencies in which
the verb is governor, and we add unlexicalized ver-
sions of contexts to account for subcategorization.
For noun lemmas, we use all dependencies in which
the noun participates, and all contexts are lexical-
ized. The vocabulary is limited to lemmas with at
least 1,000 context occurrences, resulting in 8,171
nouns and 2,865 verbs.
Each pair of lemma x and context c is sub-
sequently weighted by mutual informativeness us-
ing the point-wise mutual information metric, with
7http://alpage.inria.fr/statgram/frdep/fr_
stat_dep_parsing.html
8We additionally considered adjectives and adverbs, but our
initial tests yielded no parsing improvements.
5
? One-Edge Context: ?obj? N |avocat
? One-Edge Context: ?obj? N
(unlexicalized)
? Two-Edge Context: ?mod? P |avec ?obj? N |avocat
? Two-Edge Context: ?mod? P |avec ?obj? N
(unlexicalized)
Figure 2: Example dependency contexts for the verb
lemma manger. The one-edge contexts corresponds to
the phrase ?manger un avocat? (?eat an avocado?), and
the two-edge contexts corresponds to the phrase ?manger
avec un avocat? (?eat with a lawyer?).
probabilities estimated using frequency counts:
I(x, c) = log
(
p(x, c)
p(x)p(c)
)
(9)
Finally, we use the cosine metric to calculate the dis-
tributional similarity between pairs of lemmas x, y:
D(x, y) =
?
c
I(x, c)I(y, c)
?
(
?
c
I(x, c)2
)
?
(
?
c
I(y, c)2
)
(10)
4.3.3 WordNet ASR
For WordNet synset experiments we use the
French EuroWordNet9 (FREWN). A WordNet
synset mapping10 allows us to convert synsets in the
FREWN to Princeton WordNet version 3.0, and af-
ter discarding a small number of synsets that are
not covered by the mapping we retain entries for
9,833 nouns and 2,220 verbs. We use NLTK, the
Natural Language Toolkit (Bird et al, 2009), to cal-
culate similarity between synsets. As explained in
Section 2.2, ASR is performed using the method of
McCarthy et al (2004). We use k=8 for the distri-
butional nearest-neighbors to consider when ranking
the senses for a lemma, and we use the synset sim-
ilarity function of Jiang and Conrath (1997), with
default information content counts from NLTK cal-
culated over the British National Corpus11.
9http://www.illc.uva.nl/EuroWordNet/
10http://nlp.lsi.upc.edu/tools/download-map.
php
11http://www.natcorp.ox.ac.uk/
Source Evaluation Set
Vocabulary FTB Eval EMEA Eval Europarl
Nouns
FTB train 95.35 62.87 94.69
Thesaurus 96.25 79.00 97.83
FREWN 80.51 73.09 87.06
Verbs
FTB train 96.54 94.56 97.76
Thesaurus 98.33 97.82 99.54
FREWN 88.32 91.48 91.98
Table 2: Lexical occurrence coverage (%) of source
vocabularies over evaluation sets. FTB Eval contains
both the FTB development and test sets, while EMEA
Eval contains both the EMEA development and test sets.
Proper nouns are excluded from the analysis.
4.3.4 HAC Clustering
For the HAC clustering experiments in this paper,
we use the CLUTO package12. The distributional
thesauri described above are taken as input, and the
UPGMA setting is used for cluster agglomeration.
We test varying levels of clustering, with a parame-
ter z which determines the proportion of cluster vo-
cabulary size with respect to the original vocabulary
size (8,171 for nouns and 2,865 for verbs).
4.3.5 Resource Coverage
The coverage of our lexical resources over the
FTB and two out-of-domain evaluation sets, at the
level of token occurrences of verbs and common
nouns, is described in Table 2. We can see that
the FTB training set vocabulary provides better cov-
erage than the FREWN for both nouns and verbs,
while the coverage of the thesauri (and derived clus-
ters) is the highest overall.
4.4 Tuning and Evaluation
We evaluate four lexical target space configurations
against the baseline of lemmatization, tuning pa-
rameters using ten-fold cross-validation on the FTB
training set. The feature templates are the same as
those in Table 1, with the difference that features
involving lemmas are modified by the probabilistic
feature generalization technique described in Sec-
tion 2.4, using the appropriate categorical distribu-
tions. In all configurations, we exclude the French
auxiliary verbs e?tre and avoir from participation in
lexical generalization, and we replace proper nouns
12http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/download
6
with a special lemma13. Below we describe the
tuned parameters for each configuration.
? RC: Replacement with cluster in ?c
For clusters and the parameter z (cf. Section
4.3.4), we settled on relative cluster vocabulary
size z=0.6 for nouns and z=0.7 for verbs. We
also generalized lemmas not appearing in the
distributional thesaurus into a single unknown
class.
? PKNL: Probabilistic k-nearest lemmas in ?l
For the parameters k and m (cf. Section 2.1),
we settled on k=4 and m=0.5 for both nouns
and verbs. We also use the unknown class for
low-frequency lemmas, as in the RC configura-
tion.
? RS: Replacement with first-sense (k=1) in ?s
Since the FREWN has a lower-coverage vo-
cabulary, we did not use an unknown class for
out-of-vocabulary lemmas; instead, we mapped
them to unique senses. In addition, we did not
perform lexical generalization for verbs, due to
low cross-validation performance.
? PKPS: Probabilistic k-prevalent senses in ?s
For this setting we decided to not place any
limit on k, due to the large variation in the
number of senses for different lemmas. As
in the RS configuration, we mapped out-of-
vocabulary lemmas to unique senses and did
not perform lexical generalization for verbs.
5 Results
Table 3 shows labeled attachment score (LAS) re-
sults for our baseline parser (Lemmas) and four lex-
ical generalization configurations. For comparison,
we also include results for a setting that only uses
word forms (Forms), which was the baseline for pre-
vious work on French dependency parsing (Candito
et al, 2010b). Punctuation tokens are not scored,
and significance is calculated using Dan Bikel?s ran-
domized parsing evaluation comparator14, at signif-
icance level p=0.05.
13Proper nouns tend to have sparse counts, but for computa-
tional reasons we did not include them in our distributional the-
saurus construction. We thus chose to simply generalize them
Parse Evaluation Set LAS
Configuration FTB Test EMEA Dev EMEA Test Europarl
Forms 86.85 84.08 85.41 86.01
Lemmas 87.30 84.34 85.41 86.26
RC 87.32 84.28 85.71* 86.28
PKNL 87.46 84.63* 85.82* 86.26
RS 87.34 84.48 85.54 86.34
PKPS 87.41 84.63* 85.68* 86.22
Table 3: Labeled attachment score (LAS) on in-domain
(FTB) and out-of-domain (EMEA, Europarl) evaluation
sets for the baseline (Lemmas) and four lexical general-
ization configurations (RC, PKNL, RS, PKPS). Signif-
icant improvements over the baseline are starred. For
comparison, we also include a simpler setting (Forms),
which does not use lemmas or morphological features.
5.1 In-Domain Results
Our in-domain evaluation yields slight improve-
ments in LAS for some lexical generalization con-
figurations, with PKNL performing the best. How-
ever, the improvements are not statistically signifi-
cant. A potential explanation for this disappointing
result is that the FTB training set vocabulary cov-
ers the FTB test set at high rates for both nouns
(95.25%) and verbs (96.54%), meaning that lexi-
cal data sparseness is perhaps not a big problem
for in-domain dependency parsing. While WordNet
synsets could be expected to provide the added ben-
efit of taking word sense into account, sense ambi-
guity is not really treated due to ASR not providing
word sense disambiguation in context.
5.2 Out-Of-Domain Results
Our evaluation on the medical domain yields statisti-
cally significant improvements in LAS, particularly
for the two probabilistic target space approaches.
PKNL and PKPS improve parsing for both the
EMEA dev and test sets, while RC improves pars-
ing for only the EMEA test set and RS does not sig-
nificantly improve parsing for either set. As in our
in-domain evaluation, PKNL performs the best over-
all, though not significantly better than other lexi-
cal generalization settings. One explanation for the
improvement in the medical domain is the substan-
tial increase in coverage of nouns in EMEA afforded
into a single class.
14http://www.cis.upenn.edu/
?
dbikel/software.
html
7
by the distributional thesaurus (+26%) and FREWN
(+16%) over the base coverage afforded by the FTB
training set.
Our evaluation on the parliamentary domain
yields no improvement in LAS across the different
lexical generalization configurations. Interestingly,
Candito and Seddah (2012) note that while Europarl
is rather different from FTB in its syntax, its vocabu-
lary is surprisingly similar. From Table 2 we can see
that the FTB training set vocabulary has about the
same high level of coverage over Europarl (94.69%
for nouns and 97.76% for verbs) as it does over the
FTB evaluation sets (95.35% for nouns and 96.54%
for verbs). Thus, we can use the same reasoning as
in our in-domain evaluation to explain the lack of
improvement for lexical generalization methods in
the parliamentary domain.
5.3 Lexical Feature Use During Parsing
Since lexical generalization modifies the lexical fea-
ture space in different ways, we also provide an anal-
ysis of the extent to which each parsing model?s lex-
ical features are used during in-domain and out-of-
domain parsing. Table 4 describes, for each config-
uration, the number of lexical features stored in the
parsing model along with the average lexical fea-
ture use (ALFU) of classification instances (each in-
stance represents a parse transition) during training
and parsing.15
Lexical feature use naturally decreases when
moving from the training set to the evaluation sets,
due to holes in lexical coverage outside of a parsing
model?s training set. The single-mapping configura-
tions (RC, RS) do not increase the number of lexical
features in a classification instance, which explains
the fact that their ALFU on the FTB training set (6.0)
is the same as that of the baseline. However, the de-
crease in ALFU when parsing the evaluation sets is
less severe for these configurations than for the base-
line: when parsing EMEA Dev with the RC configu-
ration, where we obtain a significant LAS improve-
ment over the baseline, the reduction in ALFU is
only 13% compared to 22% for the baseline parser.
For the probabilistic generalization configurations,
we also see decreases in ALFU when parsing the
15We define the lexical feature use of a classification instance
to be the number of lexical features in the parsing model that
receive non-zero values in the instance?s feature vector.
Parse Lexical Feats Average Lexical Feature Use
Configuration In Model FTB Train FTB Dev EMEA Dev
Lemmas 294k 6.0 5.5 4.7
RC 150k 6.0 5.8 5.2
PKNL 853k 15.7 14.8 12.0
RS 253k 6.0 5.6 4.9
PKPS 500k 9.2 8.6 7.0
Table 4: Parsing model lexical features (rounded to near-
est thousand) and average lexical feature use in classifi-
cation instances across different training and evaluation
sets, for the baseline (Lemmas) and four lexical general-
ization configurations (PKNL, RC, PKPS, and RS).
evaluation sets, though their higher absolute ALFU
may help explain the strong medical domain parsing
performance for these configurations.
5.4 Impact on Running Time
Another factor to note when evaluating lexical gen-
eralization is the effect that it has on running time.
Compared to the baseline, the single-mapping con-
figurations (RC, RS) speed up feature extraction and
prediction time, due to reduced dimensionality of
the feature space. On the other hand, the proba-
bilistic generalization configurations (PKNL, PKPS)
slow down feature extraction and prediction time,
due to an increased dimensionality of the feature
space and a higher ALFU. Running time is there-
fore a factor that favors the single-mapping approach
over our proposed probabilistic approach.
Taking a larger view on our findings, we hy-
pothesize that in order for lexical generalization
to improve parsing, an approach needs to achieve
two objectives: (i) generalize sufficiently to ensure
that lemmas not appearing in the training set are
nonetheless associated with lexical features in the
learned parsing model; (ii) substantially increase
lexical coverage over what the training set can pro-
vide. The first of these objectives seems to be ful-
filled through our lexical generalization methods, as
indicated in Table 4. The second objective, how-
ever, seems difficult to attain when parsing text in-
domain, or even out-of-domain if the domains have
a high lexical overlap (as is the case for Europarl).
Only for our parsing experiments in the medical do-
main do both objectives appear to be fulfilled, as
evidenced by our LAS improvements when parsing
EMEA with lexical generalization.
8
6 Related Work
We now discuss previous work concerning the use of
lexical generalization for parsing, both in the classic
in-domain setting and in the more recently popular
out-of-domain setting.
6.1 Results in Constituency-Based Parsing
The use of word classes for parsing dates back to the
first works on generative constituency-based pars-
ing, whether using semantic classes obtained from
hand-built resources or less-informed classes cre-
ated automatically. Bikel (2000) tried incorporat-
ing WordNet-based word sense disambiguation into
a parser, but failed to obtain an improvement. Xiong
et al (2005) generalized bilexical dependencies in
a generative parsing model using Chinese semantic
resources (CiLin and HowNet), obtaining improve-
ments for Chinese parsing. More recently, Agirre
et al (2008) show that replacing words with Word-
Net semantic classes improves English generative
parsing. Lin et al (2009) use the HowNet resource
within the split-merge PCFG framework (Petrov et
al., 2006) for Chinese parsing: they use the first-
sense heuristic to append the most general hyper-
nym to the POS of a token, obtaining a semantically-
informed symbol refinement, and then guide further
symbol splits using the HowNet hierarchy. Other
work has used less-informed classes, notably unsu-
pervised word clusters. Candito and Crabbe? (2009)
use Brown clusters to replace words in a generative
PCFG-LA framework, obtaining substantial parsing
improvements for French.
6.2 Results in Dependency Parsing
In dependency parsing, word classes are integrated
as features in underlying linear models. In a seminal
work, Koo et al (2008) use Brown clusters as fea-
tures in a graph-based parser, improving parsing for
both English and Czech. However, attempts to use
this technique for French have lead to no improve-
ment when compared to the use of lemmatization
and morphological analysis (Candito et al, 2010b).
Sagae and Gordon (2009) augment a transition-
based English parser with clusters using unlexical-
ized syntactic distributional similarity: each word is
represented as a vector of counts of emanating un-
lexicalized syntactic paths, with counts taken from
a corpus of auto-parsed phrase-structure trees, and
HAC clustering is performed using cosine similarity.
For semantic word classes, (Agirre et al, 2011) inte-
grate WordNet senses into a transition-based parser
for English, reporting small but significant improve-
ments in LAS (+0.26% with synsets and +0.36%
with semantic files) on the full Penn Treebank with
first-sense information from Semcor.
We build on previous work by attempting to
reproduce, for French, past improvements for in-
domain English dependency parsing with general-
ized lexical classes. Unfortunately, our results for
French do not replicate the improvements for En-
glish using semantic sense information (Agirre et al,
2011) or word clustering (Sagae and Gordon, 2009).
The primary difference between our paper and previ-
ous work, though, is our evaluation of a novel prob-
abilistic approach for lexical generalization.
6.3 Out-Of-Domain Parsing
Concerning techniques for improving out-of-
domain parsing, a related approach has been to use
self-training with auto-parsed out-of-domain data,
as McClosky and Charniak (2008) do for English
constituency parsing, though in that approach
lexical generalization is not explicitly performed.
Candito et al (2011) use word clustering for do-
main adaptation of a PCFG-LA parser for French,
deriving clusters from a corpus containing text
from both the source and target domains, and they
obtain parsing improvements in both domains.
We are not aware of previous work on the use of
lexical generalization for improving out-of-domain
dependency parsing.
7 Conclusion
We have investigated the use of probabilistic lexi-
cal target spaces for reducing lexical data sparse-
ness in a transition-based dependency parser for
French. We built a distributional thesaurus from an
automatically-parsed large text corpus, using it to
generate word clusters and perform WordNet ASR.
We tested a standard approach to lexical gener-
alization for parsing that has been previously ex-
plored, where a word is mapped to a single cluster
or synset. We also introduced a novel probabilis-
tic lexical generalization approach, where a lemma
9
is represented by a categorical distribution over the
space of lemmas, clusters, or synsets. Probabilities
for the lemma space were calculated using the dis-
tributional thesaurus, and probabilities for the Word-
Net synset space were calculated using ASR sense
prevalence scores, with probabilistic clusters left for
future work.
Our experiments with an arc-eager transition-
based dependency parser resulted in modest but sig-
nificant improvements in LAS over the baseline
when parsing out-of-domain medical text. However,
we did not see statistically significant improvements
over the baseline when parsing in-domain text or
out-of-domain parliamentary text. An explanation
for this result is that the French Treebank training set
vocabulary has a very high lexical coverage over the
evaluation sets in these domains, suggesting that lex-
ical generalization does not provide much additional
benefit. Comparing the standard single-mapping ap-
proach to the probabilistic generalization approach,
we found a slightly (though not significantly) better
performance for probabilistic generalization across
different parsing configurations and evaluation sets.
However, the probabilistic approach also has the
downside of a slower running time.
Based on the findings in this paper, our focus
for future work on lexical generalization for de-
pendency parsing is to continue improving parsing
performance on out-of-domain text, specifically for
those domains where lexical variation is high with
respect to the training set. One possibility is to
experiment with building a distributional thesaurus
that uses text from both the source and target do-
mains, similar to what Candito et al (2011) did
with Brown clustering, which may lead to a stronger
bridging effect across domains for probabilistic lex-
ical generalization methods.
Acknowledgments
This work was funded in part by the ANR project
Sequoia ANR-08-EMER-013.
References
A. Abeille? and N. Barrier. 2004. Enriching a French tree-
bank. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation, Lisbon,
Portugal, May.
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics,
pages 317?325, Columbus, Ohio, June.
E. Agirre, K. Bengoetxea, K. Gojenola, and J. Nivre.
2011. Improving dependency parsing with semantic
classes. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics, pages
699?703, Portland, Oregon, June.
D.M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proceedings of the
EMNLP/VLC-2000, pages 155?163, Hong Kong, Oc-
tober.
S. Bird, E. Loper, and E. Klein. 2009. Natural Language
Processing with Python. O?Reilly Media Inc.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and
J.C. Lai. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18(4):467?479.
R.C. Bunescu. 2008. Learning with probabilistic fea-
tures for improved pipeline models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 670?679, Honolulu, Hawaii,
October.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 138?141, Paris,
France, October.
M. Candito and D. Seddah. 2012. Le corpus Sequoia :
annotation syntaxique et exploitation pour l?adaptation
d?analyseur par pont lexical. In Actes de la 19e`me
confe?rence sur le traitement automatique des langues
naturelles, Grenoble, France, June. To Appear.
M. Candito, B. Crabbe?, and P. Denis. 2010a. Statistical
French dependency parsing: Treebank conversion and
first results. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
Valetta, Malta, May.
M. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010b. Benchmarking of statistical depen-
dency parsers for French. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 108?116, Beijing, China, August.
M. Candito, E. Henestroza Anguiano, D. Seddah, et al
2011. A Word Clustering Approach to Domain Adap-
tation: Effective Parsing of Biomedical Texts. In Pro-
ceedings of the 12th International Conference on Pars-
ing Technologies, Dublin, Ireland, October.
J.R. Curran. 2004. From distributional to semantic simi-
larity. Ph.D. thesis, University of Edinburgh.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
10
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation, Hong Kong, China, De-
cember.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
E. Henestroza Anguiano and P. Denis. 2011. FreDist:
Automatic construction of distributional thesauri for
French. In Actes de la 18e`me confe?rence sur le traite-
ment automatique des langues naturelles, pages 119?
124, Montpellier, France, June.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics, pages 595?603, Columbus, Ohio,
June.
X. Lin, Y. Fan, M. Zhang, X. Wu, and H. Chi. 2009. Re-
fining grammars for parsing with hierarchical semantic
knowledge. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1298?1307, Singapore, August.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, Volume 2, pages 768?774, Montreal, Quebec,
August.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics, pages 279?286,
Barcelona, Spain, July.
D. McClosky and E. Charniak. 2008. Self-training for
biomedical parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics, pages 101?104, Columbus, Ohio, June.
J. Nivre, J. Hall, J. Nilsson, G. Eryi it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning, pages 221?225, New York City, NY,
June.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02):95?135.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, pages 149?
160, Nancy, France, April.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
K. Sagae and A. Gordon. 2009. Clustering words by
syntactic similarity improves dependency parsing of
predicate-argument structures. In Proceedings of the
11th International Conference on Parsing Technolo-
gies, pages 192?201, Paris, France, October.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation, Valetta, Malta, May.
J. Tiedemann. 2009. News from OPUS - A collection of
multilingual parallel corpora with tools and interfaces.
In Recent Advances in Natural Language Processing,
volume 5, pages 237?248. John Benjamins, Amster-
dam.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005. Pars-
ing the penn chinese treebank with semantic knowl-
edge. In Proceedings of the International Joint Con-
ference on Natural Language Processing, pages 70?
81, Jeju Island, Korea, October.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies, pages 195?206, Nancy, France, April.
11
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 46?52,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
The LIGM-Alpage Architecture for the SPMRL 2013 Shared Task:
Multiword Expression Analysis and Dependency Parsing
Matthieu Constant
Universite? Paris-Est
LIGM
CNRS
Marie Candito
Alpage
Paris Diderot Univ
INRIA
Djame? Seddah
Alpage
Paris Sorbonne Univ
INRIA
Abstract
This paper describes the LIGM-Alpage sys-
tem for the SPMRL 2013 Shared Task. We
only participated to the French part of the de-
pendency parsing track, focusing on the real-
istic setting where the system is informed nei-
ther with gold tagging and morphology nor
(more importantly) with gold grouping of to-
kens into multi-word expressions (MWEs).
While the realistic scenario of predicting both
MWEs and syntax has already been investi-
gated for constituency parsing, the SPMRL
2013 shared task datasets offer the possibil-
ity to investigate it in the dependency frame-
work. We obtain the best results for French,
both for overall parsing and for MWE recog-
nition, using a reparsing architecture that com-
bines several parsers, with both pipeline archi-
tecture (MWE recognition followed by pars-
ing), and joint architecture (MWE recognition
performed by the parser).
1 Introduction
As shown by the remarkable permanence over the
years of specialized workshops, multiword expres-
sions (MWEs) identification is still receiving consid-
erable attention. For some languages, such as Ara-
bic, French, English, or German, a large quantity of
MWE resources have been generated (Baldwin and
Nam, 2010). Yet, while special treatment of com-
plex lexical units, such as MWEs, has been shown to
boost performance in tasks such as machine transla-
tion (Pal et al, 2011), there has been relatively little
work exploiting MWE recognition to improve pars-
ing performance.
Indeed, a classical parsing scenario is to pre-
group MWEs using gold MWE annotation (Arun
and Keller, 2005). This non-realistic scenario has
been shown to help parsing (Nivre and Nilsson,
2004; Eryigit et al, 2011), but the situation is quite
different when switching to automatic MWE predic-
tion. In that case, errors in MWE recognition al-
leviate their positive effect on parsing performance
(Constant et al, 2012). While the realistic scenario
of syntactic parsing with automatic MWE recogni-
tion (either done jointly or in a pipeline) has already
been investigated in constituency parsing (Caffer-
key et al, 2007; Green et al, 2011; Constant et al,
2012; Green et al, 2013), the French dataset of the
SPMRL 2013 Shared Task (Seddah et al, 2013) of-
fers one of the first opportunities to evaluate this sce-
nario within the framework of dependency syntax.
In this paper, we discuss the systems we submit-
ted to the SPMRL 2013 shared task. We focused
our participation on the French dependency parsing
track using the predicted morphology scenario, be-
cause it is the only data set that massively contains
MWEs. Our best system ranked first on that track
(for all training set sizes). It is a reparsing system
that makes use of predicted parses obtained both
with pipeline and joint architectures. We applied it
to the French data set only, as we focused on MWE
analysis for dependency parsing. Section 2 gives its
general description, section 3 describes the handling
of MWEs. We detail the underlying parsers in sec-
tion 4 and their combination in section 5. Experi-
ments are described and discussed in sections 6 and
7.
2 System Overview
Our whole system is made of several single statisti-
cal dependency parsing systems whose outputs are
combined into a reparser. We use two types of sin-
46
gle parsing architecture: (a) pipeline systems; (b)
?joint? systems.
The pipeline systems first perform MWE analy-
sis before parsing. The MWE analyzer (section 3)
merges recognized MWEs into single tokens and
the parser is then applied on the sentences with this
new tokenization. The parsing model is learned on
a gold training set where all marked MWEs have
been merged into single tokens. For evaluation, the
merged MWEs appearing in the resulting parses are
expanded, so that the tokens are exactly the same in
gold and predicted parses.
The ?joint? systems directly output dependency
trees whose structure comply with the French
dataset annotation scheme. As shown in Figure 1,
such trees contain not only syntactic dependencies,
but also the grouping of tokens into MWEs, since the
first component of an MWE bears dependencies to
the subsequent components of the MWE with a spe-
cific label dep_cpd. At that stage, the only missing
information is the POS of the MWEs, which we pre-
dict by applying a MWE tagger in a post-processing
step.
la caisse d? e?pargne avait ferme? la veille
suj
de
t
dep
cpd
dep cpd
au
x
tps
mod
dep
cpd
Figure 1: French dependency tree for La caisse
d?e?pargne avait ferme? la veille (The savings bank had
closed the day before), containing two MWEs (in red).
3 MWE Analyzer and MWE Tagger
The MWE analyzer we used in the pipeline sys-
tems is based on Conditional Random Fields (CRF)
(Lafferty et al, 2001) and on external lexicons fol-
lowing (Constant and Tellier, 2012). Given a tok-
enized text, it jointly performs MWE segmentation
and POS tagging (of simple tokens and of MWEs),
both tasks mutually helping each other1. CRF is a
prominent statistical model for sequence segmenta-
1Note though that we keep only the MWE segmentation, and
use rather the Morfette tagger-lemmatizer, cf. section 4.
tion and labelling. External lexicons used as sources
of features greatly improve POS tagging (Denis
and Sagot, 2009) and MWE segmentation (Constant
and Tellier, 2012). Our lexical resources are com-
posed of two large-coverage general-language lexi-
cons: the Lefff2 lexicon (Sagot, 2010), which con-
tains approx. half a million inflected word forms,
among which approx. 25, 000 are MWEs; and the
DELA3 (Courtois, 2009; Courtois et al, 1997) lex-
icon, which contains approx. one million inflected
forms, among which about 110, 000 are MWEs.
These resources are completed with specific lexi-
cons freely available in the platform Unitex4: the
toponym dictionary Prolex (Piton et al, 1999) and a
dictionary of first names.
The MWE tagger we used in the joint systems
takes as input a MWE within a dependency tree, and
outputs its POS. It is a pointwise classifier, based
on a MaxEnt model that integrates different features
capturing the MWE local syntactic context, and in
particular the POS at the token level (and not at
the MWE level). The features comprise: the MWE
form, its lemma, the sequence of POS of its compo-
nents, the POS of its first component, its governor?s
POS in the syntactic parse, the POS following the
MWE, the POS preceding the MWE, the bigram of
the POS following and preceding the MWE.
4 Dependency Parsers
For our development, we trained 3 types of parsers,
both for the pipeline and the joint architecture:
? MALT, a pure linear-complexity transition-
based parser (Nivre et al, 2006)
? Mate-tools 1, the graph-based parser available
in Mate-tools5 (Bohnet, 2010)
? Mate-tools 2, the joint POS tagger and
transition-based parser with graph-based com-
pletion available in Mate-tools (Bohnet and
Nivre, 2012).
2We use the version available in the POS tagger MElt (Denis
and Sagot, 2009).
3We use the version in the platform Unitex (http://igm.univ-
mlv.fr/?unitex). We had to convert the DELA POS tagset to the
FTB one.
4http://igm.univ-mlv.fr/?unitex
5Available at http://code.google.com/p/mate-tools/. We
used the Anna3.3 version.
47
Such parsers require some preprocessing of the
input text: lemmatization, POS tagging, morphol-
ogy analyzer (except the joint POS tagger and
transition-based parser that does not require prepro-
cessed POS tagging). We competed for the scenario
in which this information is not gold but predicted.
Instead of using the predicted POS, lemma and mor-
phological features provided by the shared task orga-
nizers, we decided to retrain the tagger-lemmatizer
Morfette (Chrupa?a et al, 2008; Seddah et al, 2010),
in order to apply a jackknifing on the training set, so
that parsers are made less sensitive to tagging errors.
Note that no feature pertaining to MWEs are used at
this stage.
5 Reparser
The reparser is an adaptation to labeled dependency
parsing of the simplest6 system proposed in (Sagae
and Lavie, 2006). The principle is to build an arc-
factored merge of the parses produced by n input
parsers, and then to find the maximum spanning
tree among the resulting merged graph7. We im-
plemented the maximum spanning tree algorithm8
of (Eisner, 1996) devoted to projective dependency
parsing. During the parse merging, each arc is unla-
beled, and is given a weight, which is the frequency
it appears in the n input parses. Once the maxi-
mum spanning tree is found, each arc is labeled by
its most voted label among the m input parses con-
taining such an arc (with arbitrary choice in case of
ties).
6 Experiments
6.1 Settings
MWE Analysis and Tagging
For the MWE analyzer, we used the tool lgtag-
ger9 (version 1.1) with its default set of feature tem-
6The other more complex systems were producing equiva-
lent scores.
7In order to account for labeled MWE recognition, we in-
tegrated in the ?dep cpd? arcs the POS of the corresponding
MWE. For instance, if the label ?dep cpd? corresponds to an arc
in a multiword preposition (P), the arc is relabeled ?dep cpd P?.
At evaluation time, the output parse labels are remapped to the
official annotation scheme.
8More precisely, we based our implementation on the
pseudo-code given in (McDonald, 2006).
9http://igm.univ-mlv.fr/?mconstan
plates. The MWE tagger model was trained using
the Wapiti software(Lavergne et al, 2010). We used
the default parameters and we forced the MaxEnt
mode.
Parsers
For MALT (version 1.7.2), we used the arceager
algorithm, and the liblinear library for training. As
far as the features are concerned, we started with
the feature templates given in Bonsai10 (Candito et
al., 2010), and we added some templates (essentially
lemma bigrams) during the development tests, that
slightly improved performance. For the two Mate-
tools parsers, we used the default feature sets and
parameters proposed in the documentation.
Morphological prediction
Predicted lemmas, POS and morphology features
are computed with Morfette version 0.3.5 (Chrupa?a
et al, 2008; Seddah et al, 2010)11, using 10 iter-
ations for the tagging perceptron, 3 iterations for
the lemmatization perceptron, default beam size for
the decoding of the joint prediction, and the Lefff
(Sagot, 2010) as external lexicon used for out-of-
vocabulary words. We performed a jackknifing on
the training corpus, with 10 folds for the full corpus,
and 20 folds for the 5k track12.
6.2 Results
We first provide the results on the development cor-
pus. Table 1 shows the general parsing accuracy of
our different systems. Results are displayed in three
different groups corresponding to each kind of sys-
tems: the two single parser architectures ones (joint
and pipeline) and the reparsing one. Each system
was tested both when learned on the full training
data set and on the 5k one. The joint and pipeline
systems were evaluated with the three parsers de-
scribed in section 4. For the reparser, we tested dif-
ferent combinations of parsers in the full training
data set mode. We found that the best combination
includes all parsers but MALT in joint mode. We did
not tune our reparsing system in the 5k training data
set mode. We assumed that the best combination in
this mode was the same as with full training.
10http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html
11Available at https://sites.google.com/site/morfetteweb/
12Note that for the 5k track, we retrained Morfette using the
5k training corpus only, whereas the official 5k training set con-
tains predicted morphology trained on the full training set.
48
full 5k
type parser LAS UAS LaS LAS UAS LaS
Joint
MALT 80.91 84.74 89.18 78.61 83.16 87.51
Mate-tools 1 84.60 88.21 91.43 82.02 86.23 90.02
Mate-tools 2 84.40 88.08 91.02 81.66 85.97 89.38
Pipeline
MALT 82.56 86.22 90.22 80.79 84.71 89.19
Mate-tools 1 85.28 88.73 91.85 83.23 86.97 90.67
Mate-tools 2 84.82 88.31 91.45 82.79 86.56 90.26
Reparser
joint only 85.28 88.77 91.70 - - -
pipeline only 85.79 89.17 91.94 - - -
all 86.12 89.36 92.22 - - -
best ensemble 86.23 89.55 92.21 84.25 87.88 91.17
Table 1: Parsing results on development corpus (38820 tokens)
COMP MWE MWE+POS
R P F R P F R P F
joint Mate-tools 1 76.3 82.4 79.2 74.3 80.6 77.3 70.7 76.7 73.6
pipeline Mate-tools 1 80.8 82.7 81.7 79.0 83.6 81.2 75.6 80.1 77.8
best reparser 81.1 82.5 81.8 79.2 83.0 81.0 76.1 79.8 77.9
Table 2: MWE Results on the development corpus (2119 MWEs) with full training.
Table 2 contains the MWE results on the devel-
opment data set with full training, for three systems:
the best single-parser joint and pipeline systems (i.e.
with Mate-tools 1) and the best reparser. We do not
provide results for the 5k training because they show
similar trends. We provide the 9 MWE-related mea-
sures defined in the shared task. The symbols R, P
and F respectively correspond to recall, precision
and F-measure. COMP corresponds to evaluation
of the non-head MWE components (i.e. the non-first
MWE components, cf. Figure 1). MWE corresponds
to the recognition of a complete MWE. MWE+POS
stands for the recognition of a complete MWE asso-
ciated with its correct POS.
We submitted to the shared task our best
(reparser) system according to the tuning described
above. We also sent the two best pipeline systems
(Mate-tools 1 and Mate-tools 2) and the best joint
system (Mate-tools 1), in order to compare our sin-
gle systems to the other competitors. The official re-
sults of our systems are provided in table 3 for gen-
eral parsing and in table 4 for MWE recognition. We
also show the ranking of each of these systems in the
competition.
7 Discussion
In table 3, we can note that for the 5k training set
scenario, there is a general drop of parsing perfor-
mance (approximately 2 points), but the trends are
exactly the same as for the full training set sce-
nario. Concerning the performance on MWE analy-
sis (table 4), the pipeline Mate-tools-1 system very
slightly outperforms the best reparser system in the
5k scenario, contrary to the full training set scenario,
but the difference is not significant. In the following,
we focus on the full training set scenario.
Let us first discuss the overall parsing perfor-
mance, by looking at the results on the develop-
ment corpus (table 1). As far as the single-parser
systems are concerned, we can note that for both
the joint and pipeline systems, MALT achieves
lower performance than the graph-based (Mate-
tools-1) and the joint tagger-parser (Mate-tools-2),
which have comparable performance. Moreover,
the pipeline systems achieve overall better than their
joint counterpart, though the increase between joint
and pipeline architecture is much bigger for MALT
than for the Mate parsers (for MALT, compare
49
training type parser LAS UAS LaS Rank
Full
Reparser best 85.86 89.19 92.20 1
Pipeline Mate-tools 1 84.91 88.35 91.73 3
Pipeline Mate-tools 2 84.87 88.40 91.51 4
Joint Mate-tools 1 84.14 87.67 91.24 7
5k
Reparser best 83.60 87.40 90.76 1
Pipeline Mate-tools 1 82.53 86.51 90.14 4
Pipeline Mate-tools 2 82.15 86.18 89.79 6
Joint Mate-tools 1 81.63 85.76 89.56 7
Table 3: Official parsing results on the evaluation corpus (75216 tokens)
training type parser COMP MWE MWE+POS Rank
Full
Reparser best ensemble 81.3 80.7 77.5 1
Pipeline Mate-tools 1 81.2 80.8 77.4 2
Pipeline Mate-tools 2 81.2 80.8 76.6 3
Joint Mate-tools 1 79.6 77.4 74.1 6
5k
Pipeline Mate-tools 1 78.7 77.7 74.0 1
Reparser best ensemble 78.9 77.2 73.8 2
Pipeline Mate-tools 2 78.7 77.7 73.3 5
Joint Mate-tools 1 75.9 72.2 75.9 10
Table 4: Official MWE results on the evaluation corpus (4043 MWEs). The scores correspond to the F-measure.
LAS=80.91 for the joint system, and LAS=82.56
for the pipeline architecture, while for Mate-tools-
1, compare LAS=84.60 with LAS=85.28). The best
reparser system provides a performance increase of
approximately one point over the best single-parser
system (Mate-tools-1), both for LAS and UAS,
which suggests that the parsers have complementary
strengths.
When looking at performance on MWE recog-
nition and tagging (2), we can note greater varia-
tion between the F-measures obtained by the single-
parser systems, but this is due to the much lower
number of MWEs with respect to the number of
tokens (there are 38820 tokens and 2119 MWEs
in the dev set). The MWE analyzer used in the
pipeline systems leads to better MWE recognition
(F ? measure = 81.2 on dev set) than when the
analysis is left to the bare ?joint? parsers (joint Mate-
tools 1 achieves F-measure= 77.3).
Contrary to the situation for overall parsing per-
formance, the reparser system does not lead to better
MWE recognition with respect to the MWE analyzer
of the pipeline systems. Indeed the performance on
MWEs are quite similar between the reparser sys-
tem and the MWE analyzer (for the MWE metric,
on the dev set we get F=81.0 versus 81.2 for best
reparser and pipeline systems respectively, whereas
we get 80.7 and 80.8 on the test set. These differ-
ences are not significant). This is because the MWEs
predicted by the MWE analyzer are present in three
of the single-parser systems taken into account in the
reparsing process, and are thus much favored in the
voting.
In order to understand better our parsing systems?
performance on MWE recognition, we provide in ta-
ble 5 the MWE+POS results broken down by MWE
part-of-speech, for the dev set. Not surprisingly,
we can note that performance varies greatly de-
pending on the POS, with better performance on
closed classes (conjunctions, determiners, preposi-
tions, pronouns) than on open classes. The lowest
performance is on adjectives and verbs, but given the
raw numbers of gold MWEs, the major impact on
overall performance is given by the results on nom-
inal MWEs (either common or proper nouns). A lit-
tle less than one third of the nominal gold MWEs
50
R P F Nb gold Nb predicted Nb correct
adjectives 46.9 75.0 57.7 32 20 15
adverbs 74.7 83.0 78.7 360 324 269
conjunctions 90.1 83.7 86.8 91 98 82
clitics - 0.00 - 0 1 0
determiners 96.0 96.8 96.4 252 250 242
nouns 72.7 76.2 74.4 973 928 707
prepositions 84.6 84.9 84.8 345 344 292
pronouns 75.0 87.5 80.8 28 24 21
verbs 66.7 66.7 66.67 33 33 22
unknown 0 0 0 5 0 0
ALL 77.9 81.6 79.7 2119 2022 1650
Table 5: MWE+POS results on the development corpus, broken down by POS (recall, precision, F-measure, number
of gold MWEs, predicted MWEs, correct MWEs with such POS.
is not recognized (R = 72.7), and about one quar-
ter of the predicted nominal MWEs are wrong (P =
76.2). Though these results can be partly explained
by some inconsistencies in MWE annotation in the
French Treebank (Constant et al, 2012), there re-
mains room for improvement for open class MWE
recognition.
8 Conclusion
We have described the LIGM-Alpage system for the
SPMRL 2013 shared task, restricted to the French
track. We provide the best results for the realistic
scenario of predicting both MWEs and dependency
syntax, using a reparsing architecture that combines
several parsers, both pipeline (MWE recognition
followed by parsing) and joint (MWE recognition
performed by the parser). In the future, we plan to
integrate features specific to MWEs into the joint
system, so that the reparser outperforms both the
joint and pipeline systems, not only on parsing (as
it is currently the case) but also on MWE recogni-
tion.
References
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of french. In
Proceedings of the Annual Meeting of the Association
For Computational Linguistics (ACL?05), pages 306?
313.
T. Baldwin and K.S. Nam. 2010. Multiword expressions.
In Handbook of Natural Language Processing, Second
Edition. CRC Press, Taylor and Francis Group.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1455?1465,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING?10), Beijing, China.
C. Cafferkey, D. Hogan, and J. van Genabith. 2007.
Multi-word units in treebank-based probabilistic pars-
ing and generation. In Proceedings of the 10th Inter-
national Conference on Recent Advances in Natural
Language Processing (RANLP?07).
M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010. Benchmarking of statistical depen-
dency parsers for french. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING?10), Beijing, China.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette.
In In Proc. of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Matthieu Constant and Isabelle Tellier. 2012. Evaluat-
ing the impact of external lexical resources into a crf-
based multiword segmenter and part-of-speech tagger.
In Proceedings of the 8th conference on Language Re-
sources and Evaluation (LREC?12).
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
51
tiword expression recognition and parsing. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume
1, ACL ?12, pages 204?212, Stroudsburg, PA, USA.
Association for Computational Linguistics.
B. Courtois, M. Garrigues, G. Gross, M. Gross,
R. Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein, and R. Vive?s. 1997. Dic-
tionnaire e?lectronique DELAC : les mots compose?s
binaires. Technical Report 56, University Paris 7,
LADL.
B. Courtois. 2009. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise, 87:11?22.
P. Denis and B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation (PACLIC?09), pages 110?
119.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational lin-
guistics - Volume 1, COLING ?96, pages 340?345,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
G. Eryigit, T. Ilbay, and O. Arkan Can. 2011. Multiword
expressions in statistical dependency parsing. In Pro-
ceedings of the IWPT Workshop on Statistical Pars-
ing of Morphologically-Rich Languages (SPRML?11),
pages 45?55.
S. Green, M.-C. de Marneffe, J. Bauer, and C. D. Man-
ning. 2011. Multiword expression identification with
tree substitution grammars: A parsing tour de force
with french. In Proceedings of the conference on
Empirical Method for Natural Language Processing
(EMNLP?11), pages 725?735.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML?01), pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL?10), pages 504?513.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and J. Nilsson. 2004. Multiword units in syn-
tactic parsing. In Proceedings of Methodologies and
Evaluation of Multiword Units in Real-World Applica-
tions (MEMURA).
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proceedings of the fifth international conference
on Language Resources and Evaluation (LREC?06),
pages 2216?2219, Genoa, Italy.
Santanu Pal, Tanmoy Chkraborty, and Sivaji Bandy-
opadhyay. 2011. Handling multiword expressions
in phrase-based statistical machine translation. In
Proceedings of the Machine Translation Summit XIII,
pages 215?224.
O. Piton, D. Maurel, and C. Belleil. 1999. The prolex
data base : Toponyms and gentiles for nlp. In Proceed-
ings of the Third International Workshop on Applica-
tions of Natural Language to Data Bases (NLDB?99),
pages 233?237.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion
Volume: Short Papers, NAACL-Short ?06, pages 129?
132, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
B. Sagot. 2010. The lefff, a freely available, accurate
and large-coverage lexicon for french. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC?10).
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proc. of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Djame? Seddah, Reut Tsarfaty, Sandra K??ubler, Marie
Candito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Gold-
berg, Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Wolin?ski, Alina
Wro?blewska, and Eric Villemonte de la Cle?rgerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morphologi-
cally rich languages. In Proceedings of the 4th Work-
shop on Statistical Parsing of Morphologically Rich
Languages: Shared Task, Seattle, WA.
52
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
