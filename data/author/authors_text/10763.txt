Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689?696
Manchester, August 2008
Scientific Paper Summarization Using Citation Summary Networks
Vahed Qazvinian
School of Information
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
Quickly moving to a new area of research
is painful for researchers due to the vast
amount of scientific literature in each field
of study. One possible way to overcome
this problem is to summarize a scientific
topic. In this paper, we propose a model of
summarizing a single article, which can be
further used to summarize an entire topic.
Our model is based on analyzing others?
viewpoint of the target article?s contribu-
tions and the study of its citation summary
network using a clustering approach.
1 Introduction
It is quite common for researchers to have to
quickly move into a new area of research. For
instance, someone trained in text generation may
want to learn about parsing and someone who
knows summarization well, may need to learn
about question answering. In our work, we try to
make this transition as painless as possible by au-
tomatically generating summaries of an entire re-
search topic. This enables a researcher to find the
chronological order and the progress in that par-
ticular field of study. An ideal such system will re-
ceive a topic of research, as the user query, and will
return a summary of related work on that topic. In
this paper, we take the first step toward building
such a system.
Studies have shown that different citations to the
same article often focus on different aspects of that
article, while none alone may cover a full descrip-
tion of its entire contributions. Hence, the set of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
citation summaries, can be a good resource to un-
derstand the main contributions of a paper and how
that paper affects others. The citation summary of
an article (A), as defined in (Elkiss et al, 2008),
is a the set of citing sentences pointing to that ar-
ticle. Thus, this source contains information about
A from others? point of view. Part of a sample ci-
tation summary is as follows:
In the context of DPs, this edge based factorization method
was proposed by (Eisner, 1996).
Eisner (1996) gave a generative model with a cubic parsing
algorithm based on an edge factorization of trees.
Eisner (Eisner, 1996) proposed an
O(n
3
) parsing algorithm for PDG.
If the parse has to be projective, Eisner?s
bottom-up-span algorithm (Eisner, 1996) can be
used for the search.
The problem of summarizing a whole scientific
topic, in its simpler form, may reduce to summa-
rizing one particular article. A citation summary
can be a good resource to make a summary of a
target paper. Then using each paper?s summary
and some knowledge of the citation network, we?ll
be able to generate a summary of an entire topic.
Analyzing citation networks is an important com-
ponent of this goal, and has been widely studied
before (Newman, 2001).
Our main contribution in this paper is to use ci-
tation summaries and network analysis techniques
to produce a summary of a single scientific article
as a framework for future research on topic sum-
marization. Given that the citation summary of any
article usually has more than a few sentences, the
main challenge of this task is to find a subset of
these sentences that will lead to a better and shorter
summary.
689
Cluster Nodes Edges
DP 167 323
PBMT 186 516
Summ 839 1425
QA 238 202
TE 56 44
Table 1: Clusters and their citation network size
1.1 Related Work
Although there has been work on analyzing ci-
tation and collaboration networks (Teufel et al,
2006; Newman, 2001) and scientific article sum-
marization (Teufel and Moens, 2002), to the
knowledge of the author there is no previous work
that study the text of the citation summaries to
produce a summary. (Bradshaw, 2003; Bradshaw,
2002) get benefit from citations to determine the
content of articles and introduce ?Reference Di-
rected Indexing? to improve the results of a search
engine.
In other work, (Nanba et al, 2004b; Nanba et
al., 2004a) analyze citation sentences and automat-
ically categorize citations into three groups using
160 pre-defined phrase-based rules. This catego-
rization is then used to build a tool for survey gen-
eration. (Nanba and Okumura, 1999) also discuss
the same citation categorization to support a sys-
tem for writing a survey. (Nanba and Okumura,
1999; Nanba et al, 2004b) report that co-citation
implies similarity by showing that the textual simi-
larity of co-cited papers is proportional to the prox-
imity of their citations in the citing article.
Previous work has shown the importance of
the citation summaries in understanding what a
paper says. The citation summary of an article
A is the set of sentences in other articles which
cite A. (Elkiss et al, 2008) performed a large-
scale study on citation summaries and their impor-
tance. They conducted several experiments on a
set of 2, 497 articles from the free PubMed Cen-
tral (PMC) repository
1
. Results from this exper-
iment confirmed that the ?Self Cohesion? (Elkiss
et al, 2008) of a citation summary of an arti-
cle is consistently higher than the that of its ab-
stract. (Elkiss et al, 2008) also conclude that ci-
tation summaries are more focused than abstracts,
and that they contain additional information that
does not appear in abstracts. (Kupiec et al, 1995)
use the abstracts of scientific articles as a target
summary, where they use 188 Engineering Infor-
mation summaries that are mostly indicative in na-
1
http://www.pubmedcentral.gov
ture. Abstracts tend to summarize the documents
topics well, however, they don?t include much use
of metadata. (Kan et al, 2002) use annotated bib-
liographies to cover certain aspects of summariza-
tion and suggest guidelines that summaries should
also include metadata and critical document fea-
tures as well as the prominent content-based fea-
tures.
Siddharthan and Teufel describe a new task to
decide the scientific attribution of an article (Sid-
dharthan and Teufel, 2007) and show high human
agreement as well as an improvement in the per-
formance of Argumentative Zoning (Teufel, 2005).
Argumentative Zoning is a rhetorical classification
task, in which sentences are labeled as one of Own,
Other, Background, Textual, Aim, Basis, Contrast
according to their role in the author?s argument.
These all show the importance of citation sum-
maries and the vast area for new work to analyze
them to produce a summary for a given topic.
2 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and pro-
ceedings from ACL conferences and workshops
and includes almost 11, 000 papers. To produce
the ACL Anthology Network (AAN), (Joseph and
Radev, 2007) manually performed some prepro-
cessing tasks including parsing references and
building the network metadata, the citation, and
the author collaboration networks.
The full AAN includes all citation and collabo-
ration data within the ACL papers, with the citation
network consisting of 8, 898 nodes and 38, 765 di-
rected edges.
2.1 Clusters
We built our corpus by extracting small clusters
from the AAN data. Each cluster includes pa-
pers with a specific phrase in the title or con-
tent. We used a very simple approach to col-
lect papers of a cluster, which are likely to be
talking about the same topic. Each cluster con-
sists of a set of articles, in which the topic
phrase is matched within the title or the content
of papers in AAN. In particular, the five clus-
ters that we collected this way, are: Dependency
Parsing (DP), Phrased Based Machine Translation
(PBMT), Text Summarization (Summ), Question
Answering (QA), and Textual Entailment (TE).
Table 1 shows the number of articles and citations
in each cluster. For the evaluation purpose we
690
ACL-ID Title Year CS Size
D
P
C96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 1996 66
P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 1997 55
P99-1065 A Statistical Parser For Czech 1999 54
P05-1013 Pseudo-Projective Dependency Parsing 2005 40
P05-1012 On-line Large-Margin Training Of Dependency Parsers 2005 71
P
B
M
T
N03-1017 Statistical Phrase-Based Translation 2003 180
W03-0301 An Evaluation Exercise For Word Alignment 2003 14
J04-4002 The Alignment Template Approach To Statistical Machine Translation 2004 50
N04-1033 Improvements In Phrase-Based Statistical Machine Translation 2004 24
P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 2005 65
S
u
m
m
A00-1043 Sentence Reduction For Automatic Text Summarization 2000 19
A00-2024 Cut And Paste Based Text Summarization 2000 20
C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 2000 19
W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 2000 31
W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 2003 15
Q
A
A00-1023 A Question Answering System Supported By Information Extraction 2000 13
W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 2002 19
P02-1006 Learning Surface Text Patterns For A Question Answering System 2002 74
D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 2003 42
P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 2003 27
T
E
D04-9907 Scaling Web-Based Acquisition Of Entailment Relations 2004 12
H05-1047 A Semantic Approach To Recognizing Textual Entailment 2005 8
H05-1079 Recognising Textual Entailment With Logical Inference 2005 9
W05-1203 Measuring The Semantic Similarity Of Texts 2005 17
P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 2005 10
Table 2: Papers chosen from clusters for evaluation, with their publication year, and citation summary
size
chose five articles from each cluster. Table 2 shows
the title, year, and citation summary size for the 5
papers chosen from each cluster. The citation sum-
mary size of a paper is the size of the set of citation
sentences that cite that paper.
3 Analysis
3.1 Fact Distribution
We started with an annotation task on 25 papers,
listed in Table 2, and asked a number of annota-
tors to read the citation summary of each paper
and extract a list of the main contributions of that
paper. Each item on the list is a non-overlapping
contribution (fact) perceived by reading the cita-
tion summary. We asked the annotators to focus
on the citation summary to do the task and not on
their background on this topic.
As our next step we manually created the union
of the shared and similar facts by different anno-
tators to make a list of facts for each paper. This
fact list made it possible to review all sentences in
the citation summary to see which facts each sen-
tence contained. There were also some unshared
facts, facts that only appear in one annotator?s re-
sult, which we ignored for this paper.
Table 3 shows the shared and unshared facts ex-
tracted by four annotators for P99-1065.
The manual annotation of P99-1065 shows that
the fact ?Czech DP? appears in 10 sentences out
of all 54 sentences in the citation summary. This
shows the importance of this fact, and that ?Depen-
Fact Occurrences
S
h
a
r
e
d
f
4
: ?Czech DP? 10
f
1
: ?lexical rules? 6
f
3
: ?POS/ tag classification? 6
f
2
: ?constituency parsing? 5
f
5
: ?Punctuation? 2
f
6
: ?Reordering Technique? 2
f
7
: ?Flat Rules? 2
U
n
s
h
a
r
e
d
?Dependency conversion?
?80% UAS?
?97.0% F-measure?
?Generative model?
?Relabel coordinated phrases?
?Projective trees?
?Markovization?
Table 3: Facts of P99-1065
dency Parsing of Czech? is one of the main contri-
butions of this paper. Table 3 also shows the num-
ber of times each shared fact appears in P99-1065?s
citation summary sorted by occurrence.
After scanning through all sentences in the ci-
tation summary, we can come up with a fact dis-
tribution matrix for an article. The rows of this
matrix represent sentences in the citation summary
and the columns show facts. A 1 value in this ma-
trix means that the sentence covers the fact. The
matrix D shows the fact distribution of P99-1065.
IDs in each row show the citing article?s ACL ID,
and the sentence number in the citation summary.
These matrices, created using annotations, are par-
ticularly useful in the evaluation process.
691
D =
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
f
1
f
2
f
3
f
4
f
5
f
6
f
7
W06-2935:1 1 0 0 0 0 0 0
W06-2935:2 0 0 0 0 0 0 0
W06-2935:3 0 0 1 1 0 0 0
W06-2935:4 0 0 0 0 0 0 1
W06-2935:5 0 0 0 0 0 0 0
W06-2935:6 0 0 0 0 1 0 0
W05-1505:7 0 1 0 1 0 0 0
W05-1505:8 0 0 0 0 0 1 0
.
.
.
.
.
.
.
.
.
.
.
.
W05-1518:54 0 0 0 0 0 0 0
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
3.2 Similarity Measures
We want to build a network with citing sentences
as nodes and similarities of two sentences as edge
weights. We?d like this network to have a nice
community structure, whereby each cluster corre-
sponds to a fact. So, a similarity measure in which
we are interested is the one which results in high
values for pairs of sentences that cover the same
facts. On the other hand, it should return a low
value for pairs that do not share a common contri-
bution of the target article.
The following shows two sample sentences from
P99-1065 that cover the same fact and we want the
chosen similarity measure to return a high value
for them:
So, Collins et al(1999) proposed a tag classification for
parsing the Czech treebank.
The Czech parser of Collins et al(1999) was run on a dif-
ferent data set... .
Conversely, we?d like the similarity of the two fol-
lowing sentences that cover no shared facts, to be
quite low:
Collins (1999) explicitly added features to his parser to im-
prove punctuation dependency parsing accuracy.
The trees are then transformed into Penn Treebank
style constituencies- using the technique described in
(Collins et al 1999).
We used P99-1065 as the training sample, on
which similarity metrics were trained, and left the
others for the test. To evaluate a similarity mea-
sure for our purpose we use a simple approach. For
each measure, we sorted the similarity values of all
pairs of sentences in P99-1065?s citation summary
in a descending order. Then we simply counted the
number of pairs that cover the same fact (out of 172
such fact sharing pairs) in the top 100, 200 and 300
highly similar ones out of total 2, 862 pairs. Table
4 shows the number of fact sharing pairs among
the top highest similar pairs. Table 4 shows how
cosine similarity that uses a tf-idf measure outper-
forms the others. We tried three different poli-
cies for computing IDF values to compute cosine
Measure Top 100 Top 200 Top 300
tf-idf (General) 34 66 74
tf-idf (AAN) 34 56 74
LCSS 26 37 54
tf 24 34 46
tf2gen 13 26 35
tf-idf (DP) 16 26 28
Levenshtein 2 9 16
Table 4: Different similarity measures and their
performances in favoring fact-sharing sentences.
Each column shows the number of fact-sharing
pairs among top highly similar pairs.
similarity: a general IDF, an AAN-specific IDF
where IDF values are calculated only using the
documents of AAN, and finally DP-specific IDF
in which we only used all-DP data set. Table 4
also shows the results for an asymmetric similarity
measure, generation probability (Erkan, 2006) as
well as two string edit distances: the longest com-
mon substring and the Levenshtein distance (Lev-
enshtein, 1966).
4 Methodology
In this section we discuss our graph clustering
method for article summarization, as well as other
baseline methods used for comparisons.
4.1 Network-Based Clustering
The Citation Summary Network of an article A is
a network in which each sentence in the citation
summary of A is a node. This network is a com-
plete undirected weighted graph where the weight
of an edge between two nodes shows the similarity
of the two corresponding sentences of those nodes.
The similarity that we use, as described in sec-
tion 3.2, is such that sentences with the same facts
have high similarity values. In other words, strong
edges in the citation summary network are likely
to indicate a shared fact between two sentences.
A graph clustering method tries to cluster the
nodes of a graph in a way that the average intra-
cluster similarity is maximum and the average
inter-cluster similarity is minimum. To find the
communities in the citation summary network we
use (Clauset et al, 2004), a hierarchical agglom-
eration algorithm which works by greedily opti-
mizing the modularity in a linear running time for
sparse graphs.
To evaluate how well the clustering method works,
we calculated the purity for the clusters found of
each paper. Purity (Manning et al, 2008) is a
method in which each cluster is assigned to the
class with the majority vote in the cluster, and then
692
ACL-ID #Facts |C| #Clusters |?| Purity(?,C)
D
P
C96-1058 4 4 0.636
P97-1003 5 5 0.750
P99-1065 7 7 0.724
P05-1013 5 3 0.689
P05-1012 7 5 0.500
P
B
M
T
N03-1017 8 4 0.464
W03-0301 3 3 0.777
J04-4002 5 5 0.807
N04-1033 5 4 0.615
P05-1033 6 5 0.650
S
u
m
m
A00-1043 5 4 0.812
A00-2024 5 2 0.333
C00-1072 3 4 0.857
W00-0403 6 4 0.682
W03-0510 4 3 0.727
Q
A
A00-1023 3 2 0.833
W00-0603 7 4 0.692
P02-1006 7 5 0.590
D03-1017 7 4 0.500
P03-1001 6 4 0.500
T
E
D04-9907 7 3 0.545
H05-1047 4 3 0.833
H05-1079 5 3 0.625
W05-1203 3 3 0.583
P05-1014 4 2 0.667
Table 5: Number of real facts, clusters and purity
for each evaluated article
the accuracy of this assignment is measured by di-
viding the number of correctly assigned documents
by N . More formally:
purity(?,C) =
1
N
?
k
max
j
|?
k
? c
j
|
where ? = {?
1
, ?
2
, . . . , ?
K
} is the set of clus-
ters and C = {c
1
, c
2
, . . . , c
J
} is the set of classes.
?
k
is interpreted as the set of documents in ?
k
and
c
j
as the set of documents in c
j
. For each evalu-
ated article, Table 5 shows the number of real facts
(|C| = J), the number of clusters (|?| = K) and
purity(?,C) for each evaluated article. Figure 1
shows the clustering result for J04-4002, in which
each color (number) shows a real fact, while the
boundaries and capital labels show the clustering
result.
4.1.1 Sentence Extraction
Once the graph is clustered and communities are
formed, to build a summary we extract sentences
from the clusters. We tried these two different sim-
ple methods:
? Cluster Round-Robin (C-RR)
We start with the largest cluster, and extract
sentences in the order they appear in each
cluster. So we extract first, the first sentences
from each cluster, then the second ones, and
so on, until we reach the summary length
limit |S|. Previously, we mentioned that facts
with higher weights appear in greater num-
ber of sentences, and clustering aims to clus-
ter such fact-sharing sentences in the same
"
#
$
%
&
Figure 1: Each node is a sentence in the citation
summary for paper J04-4002. Colors (numbers)
represent facts and boundaries show the clustering
result
communities. Thus, starting with the largest
community is important to ensure that the
system summary first covers the facts that
have higher frequencies and therefore higher
weights.
? Cluster Lexrank (C-lexrank)
The second method we used was Lexrank
(Erkan and Radev, 2004) inside each cluster.
In other words, for each cluster ?
i
we made a
lexical network of the sentences in that clus-
ter (N
i
) . Using Lexrank we can find the
most central sentences in N
i
as salient sen-
tences of ?
i
to include in the main summary.
We simply choose, for each cluster ?
i
, the
most salient sentence of ?
i
, and if we have
not reached the summary length limit, we do
that for the second most salient sentences of
each cluster, and so on. The way of ordering
clusters is again by decreasing size.
Table 6 shows the two system summaries gen-
erated with C-RR and C-lexrank methods for P99-
1065. The sentences in the table appear as they
were extracted automatically from the text files of
papers, containing sentence fragments and malfor-
mations occurring while doing the automatic seg-
mentation.
4.2 Baseline Methods
We also conducted experiments with two baseline
approaches. To produce the citation summary we
used Mead?s (Radev et al, 2004) Random Sum-
mary and Lexrank (Erkan and Radev, 2004) on
the entire citation summary network as baseline
techniques. Lexrank is proved to work well in
multi-document summarization (Erkan and Radev,
2004). It first builds a lexical network, in which
693
ID Sentence
C-RR
W05-1505:9 3 Constituency Parsing for Dependency Trees A pragmatic justification for using constituency- based parser in order
to predict dependency struc- tures is that currently the best Czech dependency- tree parser is a constituency-based parser (Collins et al 1999; Zeman, 2004).
W04-2407:27 However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al 1999), we will give this measure as well.
J03-4003:33 3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999).
H05-1066:51 Furthermore, we can also see that the MST parsers perform favorably compared to the more powerful
lexicalized phrase-structure parsers, such as those presented by Collins et al(1999) and Zeman (2004) that use expensive O(n5) parsing al- gorithms.
E06-1011:21 5.2 Czech Results For the Czech data, we used the predefined train- ing, development and testing split
of the Prague Dependency Treebank (Hajic et al 2001), and the automatically generated POS tags supplied with the data,
which we reduce to the POS tag set from Collins et al(1999).
C-Lexrank
P05-1012:16 The Czech parser of Collins et al(1999) was run on a different data set and most other dependency parsers are evaluated using English.
W04-2407:26 More precisely, parsing accuracy is measured by the attachment score, which is
a standard measure used in studies of dependency parsing (Eisner, 1996; Collins et al 1999).
W05-1505:14 In an attempt to extend a constituency-based pars- ing model to train on dependency trees,
Collins transforms the PDT dependency trees into con- stituency trees (Collins et al 1999).
P06-1033:31 More specifi- cally for PDT, Collins et al(1999) relabel coordi- nated phrases after converting dependency struc- tures to phrase
structures, and Zeman (2004) uses a kind of pattern matching, based on frequencies of the parts-of-speech of conjuncts and conjunc- tions.
P05-1012:17 In par- ticular, we used the method of Collins et al(1999) to simplify part-of-speech tags since
the rich tags used by Czech would have led to a large but rarely seen set of POS features.
Table 6: System Summaries for P99-1065. (a) Using C-RR, (b) using C-Lexrank with length of 5
sentences
nodes are sentences and a weighted edge between
two nodes shows the lexical similarity. Once this
network is built, Lexrank performs a random walk
to find the most central nodes in the graph and re-
ports them as summary sentences.
5 Experimental Setup
5.1 Evaluation Method
Fact-based evaluation systems have been used in
several past projects (Lin and Demner-Fushman,
2006; Marton and Radul, 2006), especially in
the TREC question answering track. (Lin and
Demner-Fushman, 2006) use stemmed unigram
similarity of responses with nugget descriptions to
produce the evaluation results, whereas (Marton
and Radul, 2006) uses both human judgments and
human descriptions to evaluate a response.
An ideal summary in our model is one that cov-
ers more facts and more important facts. Our def-
inition for the properties of a ?good? summary of
a paper is one that is relatively short and consists
of the main contributions of that paper. From this
viewpoint, there are two criteria for our evaluation
metric. First, summaries that contain more impor-
tant facts are preferred over summaries that cover
fewer relevant facts. Second, facts should not be
equally weighted in this model, as some of them
may show more important contributions of a pa-
per, while others may not.
To evaluate our system, we use the pyra-
mid evaluation method (Nenkova and Passonneau,
2004) at sentence level. Each fact in the citation
summary of a paper is a summarization content
unit (SCU) (Nenkova and Passonneau, 2004), and
the fact distribution matrix, created by annotation,
provides the information about the importance of
each fact in the citation summary.
The score given by the pyramid method for a
summary is a ratio of the sum of the weights of
its facts to the sum of the weights of an optimal
summary. This score ranges from 0 to 1, and high
scores show the summary content contain more
heavily weighted facts. We believe that if a fact
appears in more sentences of the citation summary
than another fact, it is more important, and thus
should be assigned a higher weight. To weight the
facts we build a pyramid, and each fact falls in a
tier. Each tier shows the number of sentences a fact
appears in. Thus, the number of tiers in the pyra-
mid is equal to the citation summary size. If a fact
appears in more sentences, it falls in a higher tier.
So, if the fact f
i
appears |f
i
| times in the citation
summary it is assigned to the tier T
|f
i
|
.
The pyramid score formula that we use is com-
puted as follows. Suppose the pyramid has n tiers,
T
i
, where tier T
n
on top and T
1
on the bottom. The
weight of the facts in tier T
i
will be i (i.e. they ap-
peared in i sentences). If |T
i
| denotes the number
of facts in tier T
i
, and D
i
is the number of facts in
the summary that appear in T
i
, then the total fact
weight for the summary is D =
?
n
i=1
i?D
i
. Ad-
ditionally, the optimal pyramid score for a sum-
mary with X facts, is
Max =
?
n
i=j+1
i?|T
i
|+j?(X?
?
n
i=j+1
|T
i
|)
where j = max
i
(
?
n
t=i
|T
t
| ? X). Subsequently,
the pyramid score for a summary is calculated as
P =
D
Max
.
694
5.2 Results and Discussion
Based on the described evaluation method we con-
ducted a number of experiments to evaluate dif-
ferent summaries of a given length. In particular,
we use a gold standard and a random summary to
determine how good a system summary is. The
gold standard is a summary of a given length that
covers as many highly weighted facts as possible.
To make a gold summary we start picking sen-
tences that cover new and highly weighted facts,
until the summary length limit is reached. On the
other hand, in the random summary sentences are
extracted from the citation summary in a random
manner. We expect a good system summary to be
closer to the gold than it is to the random one.
Table 7 shows the value of pyramid score P , for
the experiments on the set of 25 papers. A P score
of less than 1 for a gold shows that there are more
facts than can be covered with a set of |S| sen-
tences.
This table suggests that C-lexrank has a higher
average score, P , for the set of evaluated articles
comparing C-RR and Lexrank.
As mentioned earlier in section 4.1.1, once the
citation summary network is clustered in the C-RR
method, the sentences from each cluster are chosen
in a round robin fashion, which will not guarantee
that a fact-bearing sentence is chosen.
This is because all sentences, whether they
cover any facts or not, are assigned to some clus-
ter anyway and such sentences might appear as the
first sentence in a cluster. This will sometimes re-
sult in a low P score, for which P05-1012 is a good
example.
6 Conclusion and Future Work
In this work we use the citation summaries to un-
derstand the main contributions of articles. The
citation summary size, in our experiments, ranges
from a few sentences to a few hundred, of which
we pick merely a few (5 in our experiments) most
important ones.
As a method of summarizing a scientific paper,
we propose a clustering approach where commu-
nities in the citation summary?s lexical network
are formed and sentences are extracted from sep-
arate clusters. Our experiments show how our
clustering method outperforms one of the cur-
rent state-of-art multi-document summarizing al-
gorithms, Lexrank, on this particular problem.
A future improvement will be to use a reorder-
ing approach like Maximal Marginal Relevance
A
r
t
i
c
l
e
G
o
l
d
M
e
a
d
?
s
R
a
n
d
o
m
L
e
x
r
a
n
k
C
-
R
R
C
-
l
e
x
r
a
n
k
D
P
C96-1058 1.00 0.27 0.73 0.73 0.73
P97-1003 1.00 0.08 0.40 0.60 0.40
P99-1065 0.94 0.30 0.54 0.82 0.67
P05-1013 1.00 0.15 0.69 0.97 0.67
P05-1012 0.95 0.14 0.57 0.26 0.62
P
B
M
T
N03-1017 0.96 0.26 0.36 0.61 0.64
W03-0301 1.00 0.60 1.00 1.00 1.00
J04-4002 1.00 0.33 0.70 0.48 0.48
N04-1033 1.00 0.38 0.38 0.31 0.85
P05-1033 1.00 0.37 0.77 0.77 0.85
S
u
m
m
A00-1043 1.00 0.66 0.95 0.71 0.95
A00-2024 1.00 0.26 0.86 0.73 0.60
C00-1072 1.00 0.85 0.85 0.93 0.93
W00-0403 1.00 0.55 0.81 0.41 0.70
W03-0510 1.00 0.58 1.00 0.83 0.83
Q
A
A00-1023 1.00 0.57 0.86 0.86 0.86
W00-0603 1.00 0.33 0.53 0.53 0.60
P02-1006 1.00 0.49 0.92 0.49 0.87
D03-1017 1.00 0.00 0.53 0.26 0.85
P03-1001 1.00 0.12 0.29 0.59 0.59
T
E
D04-9907 1.00 0.53 0.88 0.65 0.94
H05-1047 1.00 0.83 0.66 0.83 1.00
H05-1079 1.00 0.67 0.78 0.89 0.56
W05-1203 1.00 0.50 0.71 1.00 0.71
P05-1014 1.00 0.44 1.00 0.89 0.78
Mean 0.99 0.41 0.71 0.69 0.75
Table 7: Evaluation Results (|S| = 5)
(MMR) (Carbonell and Goldstein, 1998) to re-rank
clustered documents within each cluster in order
to reduce the redundancy in a final summary. An-
other possible approach is to assume the set of sen-
tences in the citation summary as sentences talk-
ing about the same event, yet generated in differ-
ent sources. Then one can apply the method in-
spired by (Barzilay et al, 1999) to identify com-
mon phrases across sentences and use language
generation to form a more coherent summary. The
ultimate goal, however, is to produce a topic sum-
marizer system in which the query is a scientific
topic and the output is a summary of all previous
works in that topic, preferably sorted to preserve
chronology and topicality.
7 Acknowledgments
The authors would like to thank Bonnie Dorr,
Jimmy Lin, Saif Mohammad, Judith L. Klavans,
Ben Shneiderman, and Aleks Aris from UMD,
Bryan Gibson, Joshua Gerrish, Pradeep Muthukr-
ishnan, Arzucan
?
Ozg?ur, Ahmed Hassan, and Thuy
Vu from University of Michigan for annotations.
This paper is based upon work supported by the
National Science Foundation grant ?iOPENER: A
Flexible Framework to Support Rapid Learning in
Unfamiliar Research Domains?, jointly awarded
to U. of Michigan and U. of Maryland as IIS
0705832. Any opinions, findings, and conclusions
or recommendations expressed in this paper are
695
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Barzilay, Regina, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context of
multi-document summarization. In ACL?99, pages
550?557.
Bradshaw, Shannon. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Bradshaw, Shannon. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
Conference on Research and Advanced Technology
for Digital Libraries.
Carbonell, Jaime G. and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In SIGIR?98,
pages 335?336.
Clauset, Aaron, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70(6):066111, Dec.
Elkiss, Aaron, Siwei Shen, Anthony Fader, G?unes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of the
American Society for Information Science and Tech-
nology, 59(1):51?62.
Erkan, G?unes? and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Erkan, G?unes?. 2006. Language model-based docu-
ment clustering using random walks. In Proceed-
ings of the HLT-NAACL conference, pages 479?486,
New York City, USA, June. Association for Compu-
tational Linguistics.
Joseph, Mark T. and Dragomir R. Radev. 2007. Ci-
tation analysis, centrality, and the ACL Anthol-
ogy. Technical Report CSE-TR-535-07, University
of Michigan. Department of Electrical Engineering
and Computer Science.
Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliogra-
phy as a Resource for Indicative Summarization. In
Proceedings of LREC 2002, Las Palmas, Spain.
Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In SIGIR
?95, pages 68?73, New York, NY, USA. ACM.
Levenshtein, Vladimir I. 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions and Rever-
sals. Soviet Physics Doklady, 10:707.
Lin, Jimmy J. and Dina Demner-Fushman. 2006.
Methods for automatically evaluating answers
to complex questions. Information Retrieval,
9(5):565?587.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Marton, Gregory and Alexey Radul. 2006. Nugge-
teer: Automatic nugget-based evaluation using de-
scriptions and judgements. In Proceedings of
NAACL/HLT.
Nanba, Hidetsugu and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI1999, pages 926?931.
Nanba, Hidetsugu, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004a. Bilingual presri:
Integration of multiple research paper databases. In
Proceedings of RIAO 2004, pages 195?211, Avi-
gnon, France.
Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-
mura. 2004b. Classification of research papers using
citation links and citation types: Towards automatic
review article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. Proceedings of the HLT-NAACL con-
ference.
Newman, Mark E. J. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Radev, Dragomir, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko Dim-
itrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion,
Simone Teufel, Michael Topper, Adam Winkel, and
Zhu Zhang. 2004. MEAD - a platform for multi-
document multilingual text summarization. In LREC
2004, Lisbon, Portugal, May.
Siddharthan, Advaith and Simone Teufel. 2007.
Whose idea was this, and why does it matter? at-
tributing scientific work to citations. In Proceedings
of NAACL/HLT-07.
Teufel, Simone and Marc Moens. 2002. Summarizing
scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Teufel, Simone, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the EMNLP, Sydney, Australia,
July.
Teufel, Simone. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
696
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584?592,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Citations to Generate Surveys of Scientific Paradigms
Saif Mohammad??, Bonnie Dorr???, Melissa Egan??, Ahmed Hassan?,
Pradeep Muthukrishan?, Vahed Qazvinian?, Dragomir Radev??, David Zajic?
Institute for Advanced Computer Studies? and Computer Science?, University of Maryland.
Human Language Technology Center of Excellence?. Center for Advanced Study of Language.
{saif,bonnie,mkegan,dmzajic}@umiacs.umd.edu
Department of Electrical Engineering and Computer Science?
School of Information?, University of Michigan.
{hassanam,mpradeep,vahed,radev}@umich.edu
Abstract
The number of research publications in var-
ious disciplines is growing exponentially.
Researchers and scientists are increasingly
finding themselves in the position of having
to quickly understand large amounts of tech-
nical material. In this paper we present the
first steps in producing an automatically gen-
erated, readily consumable, technical survey.
Specifically we explore the combination of
citation information and summarization tech-
niques. Even though prior work (Teufel et
al., 2006) argues that citation text is unsuitable
for summarization, we show that in the frame-
work of multi-document survey creation, cita-
tion texts can play a crucial role.
1 Introduction
In today?s rapidly expanding disciplines, scientists
and scholars are constantly faced with the daunting
task of keeping up with knowledge in their field. In
addition, the increasingly interconnected nature of
real-world tasks often requires experts in one dis-
cipline to rapidly learn about other areas in a short
amount of time.
Cross-disciplinary research requires scientists in
areas such as linguistics, biology, and sociology
to learn about computational approaches and appli-
cations, e.g., computational linguistics, biological
modeling, social networks. Authors of journal ar-
ticles and books must write accurate surveys of pre-
vious work, ranging from short summaries of related
research to in-depth historical notes.
Interdisciplinary review panels are often called
upon to review proposals in a wide range of areas,
some of which may be unfamiliar to panelists. Thus,
they must learn about a new discipline ?on the fly?
in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by
combining two currently available technologies: (1)
bibliometric lexical link mining that exploits the
structure of citations and relations among citations;
and (2) summarization techniques that exploit the
content of the material in both the citing and cited
papers.
It is generally agreed upon that manually written
abstracts are good summaries of individual papers.
More recently, Qazvinian and Radev (2008) argue
that citation texts are useful in creating a summary
of the important contributions of a research paper.
The citation text of a target paper is the set of sen-
tences in other technical papers that explicitly refer
to it (Elkiss et al, 2008a). However, Teufel (2005)
argues that using citation text directly is not suitable
for document summarization.
In this paper, we compare and contrast the use-
fulness of abstracts and of citation text in automati-
cally generating a technical survey on a given topic
from multiple research papers. The next section pro-
vides the background for this work, including the
primary features of a technical survey and also the
types of input that are used in our study (full pa-
pers, abstracts, and citation texts). Following this,
we describe related work and point out the advances
of our work over previous work. We then describe
how citation texts are used as a new input for multi-
document summarization to produce surveys of a
given technical area. We apply four different sum-
marization techniques to data in the ACL Anthol-
584
ogy and evaluate our results using both automatic
(ROUGE) and human-mediated (nugget-based pyra-
mid) measures. We observe that, as expected, ab-
stracts are useful in survey creation, but, notably, we
also conclude that citation texts have crucial survey-
worthy information not present in (or at least, not
easily extractable from) abstracts. We further dis-
cover that abstracts are author-biased and thus com-
plementary to the broader perspective inherent in ci-
tation texts; these differences enable the use of a
range of different levels and types of information in
the survey?the extent of which is subject to survey
length restrictions (if any).
2 Background
Automatically creating technical surveys is sig-
nificantly distinct from that of traditional multi-
document summarization. Below we describe pri-
mary characteristics of a technical survey and we
present three types of input texts that we used for
the production of surveys.
2.1 Technical Survey
In the case of multi-document summarization, the
goal is to produce a readable presentation of mul-
tiple documents, whereas in the case of technical
survey creation, the goal is to convey the key fea-
tures of a particular field, basic underpinnings of the
field, early and late developments, important con-
tributions and findings, contradicting positions that
may reverse trends or start new sub-fields, and ba-
sic definitions and examples that enable rapid un-
derstanding of a field by non-experts.
A prototypical example of a technical survey is
that of ?chapter notes,? i.e., short (50?500 word)
descriptions of sub-areas found at the end of chap-
ters of textbook, such as Jurafsky and Martin (2008).
One might imagine producing such descriptions au-
tomatically, then hand-editing them and refining
them for use in an actual textbook.
We conducted a human analysis of these chapter
notes that revealed a set of conventions, an outline
of which is provided here (with example sentences
in italics):
1. Introductory/opening statement: The earliest
computational use of X was in Y, considered by
many to be the foundational work in this area.
2. Definitional follow up: X is def ined as Y.
3. Elaboration of definition (e.g., with an exam-
ple): Most early algorithms were based on Z.
4. Deeper elaboration, e.g., pointing out issues
with initial approaches: Unfortunately, this
model seems to be wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction of additional specific instances /
historical background with citations: Two clas-
sic approaches are described in Q.
7. References to other summaries: R provides a
comprehensive guide to the details behind X.
The notion of text level categories or zoning
of technical papers?related to the survey compo-
nents enumerated above?has been investigated pre-
viously in the work of Nanba and Kan (2004b) and
Teufel (2002). These earlier works focused on the
analysis of scientific papers based on their rhetori-
cal structure and on determining the portions of pa-
pers that contain new results, comparisons to ear-
lier work, etc. The work described in this paper fo-
cuses on the synthesis of technical surveys based on
knowledge gleaned from rhetorical structure not un-
like that of the work of these earlier researchers, but
perhaps guided by structural patterns along the lines
of the conventions listed above.
Although our current approach to survey creation
does not yet incorporate a fully pattern-based com-
ponent, our ultimate objective is to apply these pat-
terns to guide the creation and refinement of the final
output. As a first step toward this goal, we use cita-
tion texts (closest in structure to the patterns iden-
tified by convention 7 above) to pick out the most
important content for survey creation.
2.2 Full papers, abstracts, and citation texts
Published research on a particular topic can be sum-
marized from two different kinds of sources: (1)
where an author describes her own work and (2)
where others describe an author?s work (usually in
relation to their own work). The author?s descrip-
tion of her own work can be found in her paper. How
others perceive her work is spread across other pa-
pers that cite her work. We will refer to the set of
sentences that explicitly mention a target paper Y as
the citation text of Y.
585
Traditionally, technical survey generation has
been tackled by summarizing a set of research pa-
pers pertaining to the topic. However, individual re-
search papers usually come with manually-created
?summaries??their abstracts. The abstract of a pa-
per may have sentences that set the context, state the
problem statement, mention how the problem is ap-
proached, and the bottom-line results?all in 200 to
500 words. Thus, using only the abstracts (instead
of full papers) as input to a summarization system is
worth exploring.
Whereas the abstract of a paper presents what the
authors think to be the important contributions of a
paper, the citation text of a paper captures what oth-
ers in the field perceive as the contributions of the
paper. The two perspectives are expected to have
some overlap in their content, but the citation text
also contains additional information not found in ab-
stracts (Elkiss et al, 2008a). For example, how a
particular methodology (described in one paper) was
combined with another (described in a different pa-
per) to overcome some of the drawbacks of each.
A citation text is also an indicator of what contri-
butions described in a paper were more influential
over time. Another distinguishing feature of citation
texts in contrast to abstracts is that a citation text
tends to have a certain amount of redundant informa-
tion. This is because multiple papers may describe
the same contributions of a target paper. This redun-
dancy can be exploited to determine the important
contributions of the target paper.
Our goal is to test the hypothesis that an ef-
fective technical survey will reflect information on
research not only from the perspective of its au-
thors but also from the perspective of others who
use/commend/discredit/add to it. Before describ-
ing our experiments with technical papers, abstracts,
and citation texts, we first summarize relevant prior
work that used these sources of information as input.
3 Related work
Previous work has focused on the analysis of cita-
tion and collaboration networks (Teufel et al, 2006;
Newman, 2001) and scientific article summarization
(Teufel and Moens, 2002). Bradshaw (2003) used
citation texts to determine the content of articles and
improve the results of a search engine. Citation
texts have also been used to create summaries of sin-
gle scientific articles in Qazvinian and Radev (2008)
and Mei and Zhai (2008). However, there is no pre-
vious work that uses the text of the citations to pro-
duce a multi-document survey of scientific articles.
Furthermore, there is no study contrasting the qual-
ity of surveys generated from citation summaries?
both automatically and manually produced?to sur-
veys generated from other forms of input such as the
abstracts or full texts of the source articles.
Nanba and Okumura (1999) discuss citation cate-
gorization to support a system for writing a survey.
Nanba et al (2004a) automatically categorize cita-
tion sentences into three groups using pre-defined
phrase-based rules. Based on this categorization a
survey generation tool is introduced in Nanba et al
(2004b). They report that co-citation (where both
papers are cited by many other papers) implies sim-
ilarity by showing that the textual similarity of co-
cited papers is proportional to the proximity of their
citations in the citing article.
Elkiss et al (2008b) conducted several exper-
iments on a set of 2,497 articles from the free
PubMed Central (PMC) repository.1 Results from
this experiment confirmed that the cohesion of a ci-
tation text of an article is consistently higher than
the that of its abstract. They also concluded that ci-
tation texts contain additional information are more
focused than abstracts.
Nakov et al (2004) use sentences surrounding ci-
tations to create training and testing data for seman-
tic analysis, synonym set creation, database cura-
tion, document summarization, and information re-
trieval. Kan et al (2002) use annotated bibliogra-
phies to cover certain aspects of summarization and
suggest using metadata and critical document fea-
tures as well as the prominent content-based features
to summarize documents. Kupiec et al (1995) use a
statistical method and show how extracts can be used
to create summaries but use no annotated metadata
in summarization.
Siddharthan and Teufel (2007) describe a new
reference task and show high human agreement as
well as an improvement in the performance of ar-
gumentative zoning (Teufel, 2005). In argumenta-
tive zoning?a rhetorical classification task?seven
1http://www.pubmedcentral.gov
586
classes (Own, Other, Background, Textual, Aim,
Basis, and Contrast) are used to label sentences ac-
cording to their role in the author?s argument.
Our aim is not only to determine the utility of cita-
tion texts for survey creation, but also to examine the
quality distinctions between this form of input and
others such as abstracts and full texts?comparing
the results to human-generated surveys using both
automatic and nugget-based pyramid evaluation
(Lin and Demner-Fushman, 2006; Nenkova and Pas-
sonneau, 2004; Lin, 2004).
4 Summarization systems
We used four summarization systems for our
survey-creation approach: Trimmer, LexRank, C-
LexRank, and C-RR. Trimmer is a syntactically-
motivated parse-and-trim approach. LexRank is a
graph-based similarity approach. C-LexRank and C-
RR use graph clustering (?C? stands for clustering).
We describe each of these, in turn, below.
4.1 Trimmer
Trimmer is a sentence-compression tool that extends
the scope of an extractive summarization system by
generating multiple alternative sentence compres-
sions of the most important sentences in target doc-
uments (Zajic et al, 2007). Trimmer compressions
are generated by applying linguistically-motivated
rules to mask syntactic components of a parse of a
source sentence. The rules can be applied iteratively
to compress sentences below a configurable length
threshold, or can be applied in all combinations to
generate the full space of compressions.
Trimmer can leverage the output of any con-
stituency parser that uses the Penn Treebank con-
ventions. At present, the Stanford Parser (Klein and
Manning, 2003) is used. The set of compressions
is ranked according to a set of features that may in-
clude metadata about the source sentences, details of
the compression process that generated the compres-
sion, and externally calculated features of the com-
pression.
Summaries are constructed from the highest scor-
ing compressions, using the metadata and maximal
marginal relevance (Carbonell and Goldstein, 1998)
to avoid redundancy and over-representation of a
single source.
4.2 LexRank
We also used LexRank (Erkan and Radev, 2004), a
state-of-the-art multidocument summarization sys-
tem, to generate summaries. LexRank first builds a
graph of all the candidate sentences. Two candidate
sentences are connected with an edge if the similar-
ity between them is above a threshold. We used co-
sine as the similarity metric with a threshold of 0.15.
Once the network is built, the system finds the most
central sentences by performing a random walk on
the graph.
The salience of a node is recursively defined on
the salience of adjacent nodes. This is similar to
the concept of prestige in social networks, where the
prestige of a person is dependent on the prestige of
the people he/she knows. However, since random
walk may get caught in cycles or in disconnected
components, we reserve a low probability to jump
to random nodes instead of neighbors (a technique
suggested by Langville and Meyer (2006)).
Note also that unlike the original PageRank
method, the graph of sentences is undirected. This
updated measure of sentence salience is called as
LexRank. The sentences with the highest LexRank
scores form the summary.
4.3 Cluster Summarizers: C-LexRank, C-RR
Two clustering methods proposed by Qazvinian and
Radev (2008)?C-RR and C-LexRank?were used
to create summaries. Both create a fully connected
network in which nodes are sentences and edges are
cosine similarities. A cutoff value of 0.1 is applied
to prune the graph and make a binary network. The
largest connected component of the network is then
extracted and clustered.
Both of the mentioned summarizers cluster the
network similarly but use different approaches to se-
lect sentences from different communities. In C-
RR sentences are picked from different clusters in
a round robin (RR) fashion. C-LexRank first calcu-
lates LexRank within each cluster to find the most
salient sentences of each community. Then it picks
the most salient sentence of each cluster, and then
the second most salient and so forth until the sum-
mary length limit is reached.
587
Most of work in QA and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answer
locator Rinaldi et al 2003; Tomuro, 2003. In addition, number of researchers have built systems to take reading
comprehension examinations designed to evaluate children?s reading levels Charniak et al 2000; Hirschman et al
1999; Ng et al 2000; Riloff and Thelen, 2000; Wang et al 2000. so-called ? definition ? or ? other ?
questions at recent TREC evalua - tions Voorhees, 2005 serve as good examples. To better facilitate user
information needs, recent trends in QA research have shifted towards complex, context-based, and interactive
question answering Voorhees, 2001; Small et al 2003; Harabagiu et al 2005. [And so on.]
Table 1: First few sentences of the QA citation texts survey generated by Trimmer.
5 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and proceed-
ings of ACL conferences and workshops. It has
almost 11,000 papers. To produce the ACL An-
thology Network (AAN), Joseph and Radev (2007)
manually parsed the references before automatically
compiling the network metadata, and generating ci-
tation and author collaboration networks. The AAN
includes all citation and collaboration data within
the ACL papers, with the citation network consist-
ing of 11,773 nodes and 38,765 directed edges.
Our evaluation experiments are on a set of papers
in the research area of Question Answering (QA)
and another set of papers on Dependency parsing
(DP). The two sets of papers were compiled by se-
lecting all the papers in AAN that had the words
Question Answering and Dependency Parsing, re-
spectively, in the title and the content. There were
10 papers in the QA set and 16 papers in the DP set.
We also compiled the citation texts for the 10 QA
papers and the citation texts for the 16 DP papers.
6 Experiments
We automatically generated surveys for both QA
and DP from three different types of documents: (1)
full papers from the QA and DP sets?QA and DP
full papers (PA), (2) only the abstracts of the QA
and DP papers?QA and DP abstracts (AB), and
(3) the citation texts corresponding to the QA and
DP papers?QA and DP citations texts (CT).
We generated twenty four (4x3x2) surveys,
each of length 250 words, by applying Trimmer,
LexRank, C-LexRank and C-RR on the three data
types (citation texts, abstracts, and full papers) for
both QA and DP. (Table 1 shows a fragment of one
of the surveys automatically generated from QA ci-
tation texts.) We created six (3x2) additional 250-
word surveys by randomly choosing sentences from
the citation texts, abstracts, and full papers of QA
and DP. We will refer to them as random surveys.
6.1 Evaluation
Our goal was to determine if citation texts do in-
deed have useful information that one will want to
put in a survey and if so, how much of this infor-
mation is not available in the original papers and
their abstracts. For this we evaluated each of the
automatically generated surveys using two separate
approaches: nugget-based pyramid evaluation and
ROUGE (described in the two subsections below).
Two sets of gold standard data were manually cre-
ated from the QA and DP citation texts and abstracts,
respectively:2 (1) We asked two impartial judges to
identify important nuggets of information worth in-
cluding in a survey. (2) We asked four fluent speak-
ers of English to create 250-word surveys of the
datasets. Then we determined how well the differ-
ent automatically generated surveys perform against
these gold standards. If the citation texts have only
redundant information with respect to the abstracts
and original papers, then the surveys of citation texts
will not perform better than others.
6.1.1 Nugget-Based Pyramid Evaluation
For our first approach we used a nugget-based
evaluation methodology (Lin and Demner-Fushman,
2006; Nenkova and Passonneau, 2004; Hildebrandt
et al, 2004; Voorhees, 2003). We asked three impar-
tial annotators (knowledgeable in NLP but not affil-
iated with the project) to review the citation texts
and/or abstract sets for each of the papers in the QA
and DP sets and manually extract prioritized lists
2Creating gold standard data from complete papers is fairly
arduous, and was not pursued.
588
of 2?8 ?nuggets,? or main contributions, supplied
by each paper. Each nugget was assigned a weight
based on the frequency with which it was listed by
annotators as well as the priority it was assigned
in each case. Our automatically generated surveys
were then scored based on the number and weight
of the nuggets that they covered. This evaluation ap-
proach is similar to the one adopted by Qazvinian
and Radev (2008), but adapted here for use in the
multi-document case.
The annotators had two distinct tasks for the QA
set, and one for the DP set: (1) extract nuggets for
each of the 10 QA papers, based only on the citation
texts for those papers; (2) extract nuggets for each
of the 10 QA papers, based only on the abstracts of
those papers; and (3) extract nuggets for each of the
16 DP papers, based only on the citation texts for
those papers.3
We obtained a weight for each nugget by revers-
ing its priority out of 8 (e.g., a nugget listed with
priority 1 was assigned a weight of 8) and summing
the weights over each listing of that nugget.4
To evaluate a given survey, we counted the num-
ber and weight of nuggets that it covered. Nuggets
were detected via the combined use of annotator-
provided regular expressions and careful human re-
view. Recall was calculated by dividing the com-
bined weight of covered nuggets by the combined
weight of all nuggets in the nugget set. Precision
was calculated by dividing the number of distinct
nuggets covered in a survey by the number of sen-
tences constituting that survey, with a cap of 1. F-
measure, the weighted harmonic mean of precision
and recall, was calculated with a beta value of 3 in
order to assign the greatest weight to recall. Recall
is favored because it rewards surveys that include
highly weighted (important) facts, rather than just a
3We first experimented using only the QA set. Then to show
that the results apply to other datasets, we asked human anno-
tators for gold standard data on the DP citation texts. Addi-
tional experiments on DP abstracts were not pursued because
this would have required additional human annotation effort to
establish a point we had already made with the QA set, i.e., that
abstracts are useful for survey creation.
4Results obtained with other weighting schemes that ig-
nored priority ratings and multiple mentions of a nugget by a
single annotator showed the same trends as the ones shown by
the selected weighting scheme, but the latter was a stronger dis-
tinguisher among the four systems.
Human Performance: Pyramid F-measureHuman1 Human2 Human3 Human4 Average
Input: QA citation surveysQA?CT nuggets 0.524 0.711 0.468 0.695 0.599QA?AB nuggets 0.495 0.606 0.423 0.608 0.533Input: QA abstract surveysQA?CT nuggets 0.542 0.675 0.581 0.669 0.617QA?AB nuggets 0.646 0.841 0.673 0.790 0.738Input: DP citation surveysDP?CT nuggets 0.245 0.475 0.378 0.555 0.413
Table 2: Pyramid F-measure scores of human-created
surveys of QA and DP data. The surveys are evaluated
using nuggets drawn from QA citation texts (QA?CT),
QA abstracts (QA?AB), and DP citation texts (DP?CT).
great number of facts.
Table 2 gives the F-measure values of the 250-
word surveys manually generated by humans. The
surveys were evaluated using the nuggets drawn
from the QA citation texts, QA abstracts, and DP ci-
tation texts. The average of their scores (listed in the
rightmost column) may be considered a good score
to aim for by the automatic summarization methods.
Table 3 gives the F-measure values of the surveys
generated by the four automatic summarizers, evalu-
ated using nuggets drawn from the QA citation texts,
QA abstracts, and DP citation texts. The table also
includes results for the baseline random summaries.
When we used the nuggets from the abstracts
set for evaluation, the surveys created from ab-
stracts scored higher than the corresponding surveys
created from citation texts and papers. Further, the
best surveys generated from citation texts outscored
the best surveys generated from papers. When we
used the nuggets from citation sets for evaluation,
the best automatic surveys generated from citation
texts outperform those generated from abstracts and
full papers. All these pyramid results demonstrate
that citation texts can contain useful information that
is not available in the abstracts or the original papers,
and that abstracts can contain useful information that
is not available in the citation texts or full papers.
Among the various automatic summarizers, Trim-
mer performed best at this task, in two cases ex-
ceeding the average human performance. Note also
that the random summarizer outscored the automatic
summarizers in cases where the nuggets were taken
from a source different from that used to generate
the survey. However, one or two summarizers still
tended to do well. This indicates a difficulty in ex-
589
System Performance: Pyramid F-measure
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT nuggets 0.321 0.434 0.268 0.295 0.616
QA?AB nuggets 0.305 0.388 0.349 0.320 0.543
Input: QA abstract surveys
QA?CT nuggets 0.452 0.383 0.480 0.441 0.404
QA?AB nuggets 0.623 0.484 0.574 0.606 0.622
Input: QA full paper surveys
QA?CT nuggets 0.239 0.446 0.299 0.190 0.199
QA?AB nuggets 0.294 0.520 0.387 0.301 0.290
Input: DP citation surveys
DP?CT nuggets 0.219 0.231 0.170 0.372 0.136
Input: DP abstract surveys
DP?CT nuggets 0.321 0.301 0.263 0.311 0.312
Input: DP full paper surveys
DP?CT nuggets 0.032 0.000 0.144 * 0.280
Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data. The surveys are evaluated using nuggets
drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).
Human Performance: ROUGE-2human1 human2 human3 human4 average
Input: QA citation surveysQA?CT refs. 0.1807 0.1956 0.0756 0.2019 0.1635QA?AB refs. 0.1116 0.1399 0.0711 0.1576 0.1201Input: QA abstract surveysQA?CT refs. 0.1315 0.1104 0.1216 0.1151 0.1197QA-AB refs. 0.2648 0.1977 0.1802 0.2544 0.2243Input: DP citation surveysDP?CT refs. 0.1550 0.1259 0.1200 0.1654 0.1416
Table 4: ROUGE-2 scores obtained for each of the manu-
ally created surveys by using the other three as reference.
ROUGE-1 and ROUGE-L followed similar patterns.
tracting the overlapping survey-worthy information
across the two sources.
6.1.2 ROUGE evaluation
Table 4 presents ROUGE scores (Lin, 2004) of
each of human-generated 250-word surveys against
each other. The average (last column) is what the au-
tomatic surveys can aim for. We then evaluated each
of the random surveys and those generated by the
four summarization systems against the references.
Table 5 lists ROUGE scores of surveys when the
manually created 250-word survey of the QA cita-
tion texts, survey of the QA abstracts, and the survey
of the DP citation texts, were used as gold standard.
When we use manually created citation text
surveys as reference, then the surveys gener-
ated from citation texts obtained significantly bet-
ter ROUGE scores than the surveys generated from
abstracts and full papers (p < 0.05) [RESULT 1].
This shows that crucial survey-worthy information
present in citation texts is not available, or hard to
extract, from abstracts and papers alone. Further,
the surveys generated from abstracts performed sig-
nificantly better than those generated from the full
papers (p < 0.05) [RESULT 2]. This shows that ab-
stracts and citation texts are generally denser in sur-
vey worthy information than full papers.
When we use manually created abstract sur-
veys as reference, then the surveys generated
from abstracts obtained significantly better ROUGE
scores than the surveys generated from citation texts
and full papers (p < 0.05) [RESULT 3]. Further, and
more importantly, the surveys generated from cita-
tion texts performed significantly better than those
generated from the full papers (p < 0.05) [RESULT
4]. Again, this shows that abstracts and citation texts
are richer in survey-worthy information. These re-
sults also show that abstracts of papers and citation
texts have some overlapping information (RESULT
2 and RESULT 4), but they also have a signifi-
cant amount of unique survey-worthy information
(RESULT 1 and RESULT 3).
Among the automatic summarizers, C-LexRank
and LexRank perform best. This is unlike the results
found through the nugget-evaluation method, where
Trimmer performed best. This suggests that Trim-
590
System Performance: ROUGE-2
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT refs. 0.11561 0.17013 0.09522 0.13501 0.16984
QA?AB refs. 0.08264 0.11653 0.07600 0.07013 0.10336
Input: QA abstract surveys
QA?CT refs. 0.04516 0.05892 0.06149 0.05369 0.04114
QA?AB refs. 0.12085 0.13634 0.12190 0.20311 0.13357
Input: QA full paper surveys
QA?CT refs. 0.03042 0.03606 0.03599 0.28244 0.03986
QA?AB refs. 0.04621 0.05901 0.04976 0.10540 0.07505
Input: DP citation surveys
DP?CT refs. 0.10690 0.13164 0.08748 0.04901 0.10052
Input: DP abstract surveys
DP?CT refs. 0.07027 0.07321 0.05318 0.20311 0.07176
Input: DP full paper surveys
DP?CT refs. 0.03770 0.02511 0.03433 * 0.04554
Table 5: ROUGE-2 scores of automatic surveys of QA and DP data. The surveys are evaluated by using human
references created from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT). These
results are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4.
* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).
mer is better at identifying more useful nuggets of
information, but C-LexRank and LexRank are bet-
ter at producing unigrams and bigrams expected in
a survey. To some extent this may be due to the fact
that Trimmer uses smaller (trimmed) fragments of
source sentences in its summaries.
7 Conclusion
In this paper, we investigated the usefulness of di-
rectly summarizing citation texts (sentences that cite
other papers) in the automatic creation of technical
surveys. We generated surveys of a set of Ques-
tion Answering (QA) and Dependency Parsing (DP)
papers, their abstracts, and their citation texts us-
ing four state-of-the-art summarization systems (C-
LexRank, C-RR, LexRank, and Trimmer). We then
used two separate approaches, nugget-based pyra-
mid and ROUGE, to evaluate the surveys. The re-
sults from both approaches and all four summa-
rization systems show that both citation texts and
abstracts have unique survey-worthy information.
These results also demonstrate that, unlike single
document summarization (where citing sentences
have been suggested to be inappropriate (Teufel
et al, 2006)), multidocument summarization?
especially technical survey creation?benefits con-
siderably from citation texts.
We next plan to generate surveys using both cita-
tion texts and abstracts together as input. Given the
overlapping content of abstracts and citation texts,
discovered in the current study, it is clear that re-
dundancy detection will be an integral component of
this future work. Creating readily consumable sur-
veys is a hard task, especially when using only raw
text and simple summarization techniques. There-
fore we intend to combine these summarization and
bibliometric techniques with suitable visualization
methods towards the creation of iterative technical
survey tools?systems that present surveys and bib-
liometric links in a visually convenient manner and
which incorporate user feedback to produce even
better surveys.
Acknowledgments
This work was supported, in part, by the National
Science Foundation under Grant No. IIS-0705832
(iOPENER: Information Organization for PENning
Expositions on Research) and Grant No. 0534323
(Collaborative Research: BlogoCenter - Infrastruc-
ture for Collecting, Mining and Accessing Blogs),
in part, by the Human Language Technology Cen-
ter of Excellence, and in part, by the Center for Ad-
vanced Study of Language (CASL). Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsors.
591
References
Shannon Bradshaw. 2003. Reference directed indexing:
Redeeming relevance for subject search in citation in-
dexes. In Proceedings of the 7th European Conference
on Research and Advanced Technology for Digital Li-
braries.
Jaime G. Carbonell and Jade Goldstein. 1998. The use
of mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 335?336, Melbourne, Australia.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008a. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008b. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Overview of the trec 2003 question-answering track.
In Proceedings of the 2004 Human Language Tech-
nology Conference and the North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT/NAACL 2004).
Mark Joseph and Dragomir Radev. 2007. Citation analy-
sis, centrality, and the ACL Anthology. Technical Re-
port CSE-TR-535-07, University of Michigan. Dept.
of Electrical Engineering and Computer Science.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics (2nd edition). Prentice-Hall.
Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 2002. Using the Annotated Bibliography as a
Resource for Indicative Summarization. In Proceed-
ings of LREC 2002, Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95,
pages 68?73, New York, NY, USA. ACM.
Amy Langville and Carl Meyer. 2006. Google?s PageR-
ank and Beyond: The Science of Search Engine Rank-
ings. Princeton University Press.
Jimmy J. Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to complex
questions. Information Retrieval, 9(5):565?587.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
workshop on Text Summarization Branches Out.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Preslav I. Nakov, Schwartz S. Ariel, and Hearst A. Marti.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In Workshop on Search and
Discovery in Bioinformatics.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference informa-
tion. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura,
and Suguru Saito. 2004a. Bilingual presri: Integration
of multiple research paper databases. In Proceedings
of RIAO 2004, pages 195?211, Avignon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Okumura.
2004b. Classification of research papers using cita-
tion links and citation types: Towards automatic re-
view article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
Proceedings of EMNLP, pages 103?110, Australia.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and Af-
fect in Text: Theory and Applications, pages 159?170.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of the
Twelfth Text Retrieval Conference (TREC 2003).
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing and Management (Spe-
cial Issue on Summarization).
592
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 54?61,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
The ACL Anthology Network Corpus 
 
Dragomir R. Radev1,2, Pradeep Muthukrishnan1, Vahed Qazvinian1 
1Department of Electrical Engineering and Computer Science 
2School of Information 
University of Michigan 
{radev,mpradeep,vahed}@umich.edu 
 
Abstract 
We introduce the ACL Anthology Net-
work (AAN), a manually curated net-
worked database of citations, 
collaborations, and summaries in the field 
of Computational Linguistics. We also 
present a number of statistics about the 
network including the most cited authors, 
the most central collaborators, as well as 
network statistics about the paper citation, 
author citation, and author collaboration 
networks. 
1 Introduction 
The ACL Anthology is one of the most 
successful initiatives of the ACL. It was in-
itiated by Steven Bird and is now maintained 
by Min Yen Kan. It includes all papers pub-
lished by ACL and related organizations as 
well as the Computational Linguistics journal 
over a period of four decades. It is available 
at http://www.aclweb.org/anthology-new/ .  
One fundamental problem with the ACL 
Anthology, however, is the fact that it is just 
a collection of papers. It doesn?t include any 
citation information or any statistics about the 
productivity of the various researchers who 
contributed papers to it. We embarked on an 
ambitious initiative to manually annotate the 
entire Anthology in order to make it possible 
to compute such statistics.  
In addition, we were able to use the anno-
tated data for extracting citation summaries of 
all papers in the collection and we also anno-
tated each paper by the gender of the authors 
(and are currently in the process of doing si-
milarly for their institutions) in the goal of 
creating multiple gold standard data sets for 
training automated systems for performing 
such tasks.  
2 Curation 
The ACL Anthology includes 13,739 pa-
pers (excluding book reviews and posters). 
Each of the papers was converted from pdf to 
text using an OCR tool (www.pdfbox.org). 
After this conversion, we extracted the refer-
ences semi-automatically using string match-
ing. The above process outputs all the 
references as a single block so we then ma-
nually inserted line breaks between refer-
ences. These references were then manually 
matched to other papers in the ACL Antholo-
gy using a ?k-best? (with k = 5) string match-
ing algorithm built into a CGI interface. A 
snapshot of this interface is shown in Figure 
1. The matched references were stored to-
gether to produce the citation network. Refer-
ences to publications outside of the AAN 
were recorded but not included in the net-
work. 
 In order to fix the issue of wrong author 
names and multiple author identities we had 
to perform a lot of manual post-processing. 
The first names and the last names were 
swapped for a lot of authors. For example, the 
author name "Caroline Brun" was present as 
"Brun Caroline" in some of her papers. 
Another big source of error was the exclusion 
of middle names or initials in a number of 
papers. For example, Julia Hirschberg had 
two identities as "Julia Hirschberg" and "Julia 
B. Hirschberg". There were a few spelling 
mistakes, like "Madeleine Bates" was miss-
pelled as "Medeleine Bates". 
Finally, many papers included incorrect 
titles in their citation sections. Some used the 
wrong years and/or venues as well. 
 
54
 Figure 1: CGI interface used for matching new references to existing papers 
 
 
Figure 2: Snapshot of the different statistics computed for an author 
55
                  Figure 3: Snapshot of the different statistics for a paper 
 
3 Statistics 
 
Using the metadata and the citations ex-
tracted after curation, we have built three dif-
ferent networks.   
The paper citation network is a directed 
network with each node representing a paper 
labeled with an ACL ID number and the 
edges representing a citation within that paper 
to another paper represented by an ACL ID. 
The paper citation network consists of 13,739 
papers and 54,538 citations.   
The author citation network and the author 
collaboration network are additional networks 
derived from the paper citation network. In 
both of these networks a node is created for 
each unique author. In the author citation 
network an edge is an occurrence of an author 
citing another author. For example, if a paper 
written by Franz Josef Och cites a paper writ-
ten by Joshua Goodman, then an edge is 
created between Franz Josef Och and Joshua 
Goodman. Self citations cause self loops in 
the author citation network. The author cita-
tion network consists of 11,180 unique au-
thors and 332,815 edges (196,905 edges if 
duplicates are removed). 
In the author collaboration network, an 
edge is created for each collaboration. For 
example, if a paper is written by Franz Josef 
Och and Hermann Ney, then an edge is 
created between the two authors.  
Table 1 shows some brief statistics about 
the first two releases of the data set (2006 and 
2007). Table 2 describes the most current re-
lease of the data set (from 2008).  
 
2006 
 Paper 
citation 
network 
Author 
citation 
network 
Author 
collaboration 
network 
n 8898 7849 7849 
m 8765 137,007 41,362 
2007 
 Paper 
citation 
network 
Author 
citation 
network 
Author 
collaboration 
network 
n 9767 9421 9421 
m 44,142 158,479 45,878 
       Table 1: Growth of citation volume 
 
 
 
 
Paper 
Citation 
Network  
Author 
Citation 
Network  
Author 
Collaboration 
Network 
Nodes  13,739  10,409 10,409 
Edges  54,538  195,505 57,614 
Diameter  22  10  20 
Average 9.34  43.11  11.07 
56
Degree  
Largest 
Connected 
Component  
11,409  9061  7910 
Watts Stro-
gatz cluster-
ing 
coefficient 
0.18 0.46 0.65 
Newman 
clustering 
coefficient 
0.07 0.14 0.36 
clairlib avg. 
directed 
shortest 
path  
5.91 3.32 5.87 
Ferrer avg. 
directed 
shortest 
path  
5.35 3.29 4.66 
harmonic 
mean geo-
desic dis-
tance  
63.93 5.47 9.40 
harmonic 
mean geo-
desic dis-
tance with 
self-loops 
counted 
63.94 5.47 9.40 
     Table 2: Network Statistics of the cita-
tion and collaboration network. The re-
maining authors (11,180-10,409) are not 
cited and are therefore removed from the 
network analysis
 
 Paper 
Citation 
Network 
Author 
Citation 
Network 
Author 
Collaboratio
n Network 
In-degree Stats 
Power Law 
Exponent 
2.50 2.20 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
2.00 1.55 2.18 
Out-degree stats 
Power Law 
Exponent 
3.70 2.56 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
2.12 1.54 2.18 
Total Degree Stats 
Power Law 
Exponent 
2.72 2.27 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
1.81 1.46 2.18 
Table 3: Degree Statistics of the citation 
and collaboration networks 
 
A lot of different statistics have been 
computed based on the data set release in 
2007 by Radev et al The statistics include 
PageRank scores which eliminate PageRank's 
inherent bias towards older papers, Impact 
factor, correlations between different meas-
ures of impact like H-Index, total number of 
incoming citations, PageRank. They also re-
port results from a regression analysis using 
H-Index scores from different sources (AAN, 
Google Scholar) in an attempt to identify 
multi-disciplinary authors.  
4 Sample rankings 
This section shows some of the rankings 
that were computed using AAN. 
57
 Rank Icit Title 
1 590 Building A Large Annotated 
Corpus Of English: The Penn 
Treebank 
2 444 The Mathematics Of Statistical 
Machine Translation: Parameter 
Estimation 
3 324 Attention Intentions And The 
Structure Of Discourse 
4 271 A Maximum Entropy Approach 
To Natural Language Processing 
5 270  Bleu: A Method For 
Automatic Evaluation Of 
6 246  A Maximum-Entropy-Inspired 
Parser 
7 230 A Stochastic Parts Program And 
Noun Phrase Parser For 
Unrestricted Text 
8 221 A Systematic Comparison Of 
Various  Statistical Alignment 
9 211 A Maximum Entropy Model For 
Part-Of-Speech Tagging 
10 211 Three Generative Lexicalized 
Models For Statistical Parsing 
Table 4: Papers with the most incoming 
citations (icit) 
Rank PR Title 
1 1099.1 A Stochastic Parts Program 
And Noun Phrase Parser For 
Unrestricted Text 
2 943.8 Finding Clauses In Unrestricted 
Text By Finitary And 
Stochastic Methods 
3 568.8 A Stochastic Approach To 
Parsing 4 543.1 A Statistical Approach To 
Machine Translation 
5 414.1 Building A Large Annotated 
Corpus Of English: The Penn 
Treebank 
6 364.9 The Mathematics Of Statistical 
Machine Translation: Parameter  
Estimation 
7 362.2 The Contribution Of Parsing To 
Prosodic Phrasing In An 
Experimental  
Text-To-Speech System 
8 301.6 Attention Intentions And The 
Structure Of Discourse 
9 250.5 Bleu: A Method For Automatic 
Evaluation Of Machine 
Translation 
10 242.5 A Maximum Entropy Approach 
To Natural Language 
Processing 
 Table 5: Papers with highest PageRank 
(PR) scores 
It must be noted that the PageRank scores 
are not accurate because of the lack of cita-
tions outside AAN. Specifically, out of the 
155,858 total number of citations, only 
54,538 are within AAN. 
 
  Rank Icit Author Name 
1 (1) 3886 (3815) Och, Franz Josef 
2 (2) 3297 (3119) Ney, Hermann 
3 (3) 3067 (3049) Della Pietra, Vincent J. 
4 (5) 2746 (2720) Mercer, Robert L. 
5 (4) 2741 (2724) Della Pietra, Stephen 
A. 6 (6) 2605 (2589) Marcus, Mitchell P. 
7 (8) 2454 (2407) Collins, Michael John 
8 (7) 2451 (2433) Brown, Peter F. 
9 (9) 2428 (2390)  Church, Kenneth Ward 
10 (10) 2047 (1991) Marcu, Daniel 
      Table 6: Authors with most incoming 
citations (the values in parentheses are us-
ing non-self- citations) 
 
Rank h Author Name 
1 18 Knight, Kevin 
2 16 Church, Kenneth Ward 
3 15 Manning, Christopher D. 
3 15 Grishman, Ralph 
3 15 Pereira, Fernando C. N. 
6 14 Marcu, Daniel 
6 14 Och, Franz Josef 
6 14 Ney, Hermann 
6 14 Joshi, Aravind K. 
6 14 Collins, Michael John 
      Table 7: Authors with the highest h-
index 
 
Rank ASP Author Name 
1  2.977 Hovy, Eduard H. 
2  2.989 Palmer, Martha Stone 
3  3.011 Rambow, Owen 
4  3.033 Marcus, Mitchell P. 
5  3.041 Levin, Lori S. 
6  3.052 Isahara, Hitoshi 
7  3.055 Flickinger, Daniel P. 
8  3.071 Klavans, Judith L. 
9  3.073 Radev, Dragomir R. 
10 3.077 Grishman, Ralph 
Table 8: Authors with the least average  
shortest path (ASP) length in the author 
collaboration network 
 
 
58
5 Related phrases 
We have also computed the related phras-
es for every author using the text from the 
papers they have authored, using the simple 
TF-IDF scoring scheme (see Figure 4).  
 
 
Figure 4: Snapshot of the related phrases 
for Franz Josef Och 
6 Citation summaries 
The citation summary of an article, P, is 
the set of sentences that appear in the litera-
ture and cite P. These sentences usually men-
tion at least one of the cited paper?s contribu-
tions. We use AAN to extract the citation 
summaries of all articles, and thus the citation 
summary of P is a self-contained set and only 
includes the citing sentences that appear in 
AAN papers. Extraction is performed auto-
matically using string-based heuristics by 
matching the citation pattern, author names 
and publication year, within the sentences. 
The following example shows the citation 
summary extracted for ?Koo, Terry, Carreras, 
Xavier, Collins, Michael John, Simple Semi-
supervised Dependency Parsing". The cita-
tion summary of (Koo et al, 2008) mentions 
KCC08, dependency parsing, and the use of 
word clustering in semi-supervised NLP. 
 
 
 
 
 
 
 
 
 
 
 
 
C08-1051 1 7:191 Furthermore, recent studies revealed that word clustering is useful for semi-supervised learn-
ing in NLP (Miller et al, 2004; Li and McCallum, 2005; Kazama and Torisawa, 2008; Koo et al, 2008). 
 
D08-1042 2 78:214 There has been a lot of progress in learning dependency tree parsers (McDonald et al, 2005; 
Koo et al, 2008; Wang et al, 2008). 
 
W08-2102 3 194:209 The method shows improvements over the method described in (Koo et al, 2008), which 
is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, 2006), suggesting 
that the incorporation of constituent structure can improve dependency accuracy. 
 
W08-2102 4 32:209 The model also recovers dependencies with significantly higher accuracy than state-of-the-
art dependency parsers such as (Koo et al, 2008; McDonald and Pereira, 2006). 
 
W08-2102 5 163:209 KCC08 unlabeled is from (Koo et al, 2008), a model that has previously been shown to 
have higher accuracy than (McDonald and Pereira, 2006). 
 
W08-2102 6 164:209 KCC08 labeled is the labeled dependency parser from (Koo et al, 2008); here we only 
evaluate the unlabeled accuracy. 
 
Figure 5: Sample citation summary 
 
59
 Figure 6: Snapshot of the citation summary for a paper 
 
 
The citation text that we have extracted for 
each paper is a good resource to generate 
summaries of the contributions of that paper. 
We have previously developed systems using 
clustering the similarity networks to generate 
short, and yet informative, summaries of in-
dividual papers (Qazvinian and Radev 2008), 
and more general scientific topics, such as 
Dependency Parsing, and Machine Transla-
tion (Radev et al 2009) . 
 
 
7 Gender annotation 
We have manually annotated the gender of 
most authors in AAN using the name of the 
author. If the gender cannot be identified 
without any ambiguity using the name of the 
author, we resorted to finding the homepage 
of the author. We have been able to annotate 
8,578 authors this way: 6,396 male and 2,182 
female.  
 
8 Downloads 
The following files can be downloaded: 
Text files of the paper: The raw text files 
of the papers after converting them from pdf 
to text is available for all papers. The files are 
named by the corresponding ACL ID. 
Metadata: This file contains all the meta-
data associated with each paper. The metada-
ta associated with every paper consists of the 
paper id, title, year, venue. 
Citations: The paper citation network indi-
cating which paper cites which other paper. 
Figure 7 includes some examples.  
 
 
id = {C98-1096} 
author = {Jing, Hongyan; McKeown, Kathleen R.} 
title = {Combining Multiple, Large-Scale Resources in a Reusable Lexicon for Natural Language Genera-
tion} 
venue = {International Conference On Computational Linguistics} 
year = {1998} 
 
id = {J82-3004} 
author = {Church, Kenneth Ward; Patil, Ramesh} 
title = {Coping With Syntactic Ambiguity Or How To Put The Block In  The Box On The Table} 
venue = {American Journal Of Computational Linguistics} 
year = {1982} 
60
 A00-1001 ==> J82-3002 
A00-1002 ==> C90-3057 
C08-1001 ==> N06-1007 
     C08-1001 ==> N06-1008 
 
Figure 7: Sample contents of the downloadable corpus 
 
We also include a large set of scripts 
which use the paper citation network and the 
metadata file to output the auxiliary networks 
and the different statistics. 
The scripts are documented here:  
http://clair.si.umich.edu/ .The data set has 
already been downloaded from 2,775 unique 
IPs since June 2007. Also, the website has 
been very popular based on access statistics. 
There have been more than 2M accesses in 
2009.  
References  
Vahed Qazvinian and Dragomir R. Radev. Scien-
tific paper summarization using citation sum-
mary networks. In COLING 2008, Manchester, 
UK, 2008. 
Dragomir R. Radev, Mark Joseph, Bryan Gibson, 
and Pradeep Muthukrishnan. A Bibliometric 
and Network Analysis of the Field of Computa-
tional Linguistics. JASIST, 2009 to appear. 
61
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895?903,
Beijing, August 2010
Citation Summarization Through Keyphrase Extraction
Vahed Qazvinian
Department of EECS
University of Michigan
vahed@umich.edu
Dragomir R. Radev
School of Information and
Department of EECS
University of Michigan
radev@umich.edu
Arzucan ?Ozgu?r
Department of EECS
University of Michigan
ozgur@umich.edu
Abstract
This paper presents an approach to sum-
marize single scientific papers, by extract-
ing its contributions from the set of cita-
tion sentences written in other papers. Our
methodology is based on extracting sig-
nificant keyphrases from the set of cita-
tion sentences and using these keyphrases
to build the summary. Comparisons show
how this methodology excels at the task
of single paper summarization, and how it
out-performs other multi-document sum-
marization methods.
1 Introduction
In recent years statistical physicists and computer
scientists have shown great interest in analyzing
complex adaptive systems. The study of such sys-
tems can provide valuable insight on the behav-
ioral aspects of the involved agents with potential
applications in economics and science. One such
aspect is to understand what motivates people to
provide the n+1st review of an artifact given that
they are unlikely to add something significant that
has not already been said or emphasized. Cita-
tions are part of such complex systems where ar-
ticles use citations as a way to mention different
contributions of other papers, resulting in a col-
lective system.
The focus of this work is on the corpora cre-
ated based on citation sentences. A citation sen-
tence is a sentence in an article containing a ci-
tation and can contain zero or more nuggets (i.e.,
non-overlapping contributions) about the cited ar-
ticle. For example the following sentences are a
few citation sentences that appeared in the NLP
literature in past that talk about Resnik?s work.
The STRAND system (Resnik, 1999), for example, uses
structural markup information from the pages, without
looking at their content, to attempt to align them.
Resnik (1999) addressed the issue of
language identification for finding Web pages in
the languages of interest.
Mining the Web for bilingual text (Resnik, 1999) is not
likely to provide sufficient quantities of high quality
data..
The set of citations is important to analyze be-
cause human summarizers have put their effort
collectively but independently to read the target
article and cite its important contributions. This
has been shown in other work too (Elkiss et al,
2008; Nanba et al, 2004; Qazvinian and Radev,
2008; Mei and Zhai, 2008; Mohammad et al,
2009). In this work, we introduce a technique
to summarize the set of citation sentences and
cover the major contributions of the target paper.
Our methodology first finds the set of keyphrases
that represent important information units (i.e.,
nuggets), and then finds the best set of k sentences
to cover more, and more important nuggets.
Our results confirm the effectiveness of the
method and show that it outperforms other state
of the art summarization techniques. Moreover,
as shown in the paper, this method does not need
to calculate the full cosine similarity matrix for a
document cluster, which is the most time consum-
ing part of the mentioned baseline methods.
1.1 Related Work
Previous work has used citations to produce sum-
maries of scientific work (Qazvinian and Radev,
895
2008; Mei and Zhai, 2008; Elkiss et al, 2008).
Other work (Bradshaw, 2003; Bradshaw, 2002)
benefits from citations to determine the content of
articles and introduce ?Reference Directed Index-
ing? to improve the results of a search engine.
In other work, (Nanba and Okumura, 1999) an-
alyze citation sentences and automatically cate-
gorize citations into three groups using 160 pre-
defined phrase-based rules to support a system for
writing a survey. Previous research has shown
the importance of the citation summaries in un-
derstanding what a paper contributes. In partic-
ular, (Elkiss et al, 2008) performed a large-scale
study on citation summaries and their importance.
Results from this experiment confirmed that the
?Self Cohesion? (Elkiss et al, 2008) of a citation
summary of an article is consistently higher than
the that of its abstract and that citations contain
additional information that does not appear in ab-
stracts.
Kan et al (2002) use annotated bibliographies
to cover certain aspects of summarization and sug-
gest using metadata and critical document features
as well as the prominent content-based features to
summarize documents. Kupiec et al (1995) use
a statistical method and show how extracts can
be used to create summaries but use no annotated
metadata in summarization.
Siddharthan and Teufel describe a new task to
decide the scientific attribution of an article (Sid-
dharthan and Teufel, 2007) and show high hu-
man agreement as well as an improvement in the
performance of Argumentative Zoning (Teufel,
2005). Argumentative Zoning is a rhetorical clas-
sification task, in which sentences are labeled as
one of Own, Other, Background, Textual, Aim,
Basis, Contrast according to their role in the au-
thor?s argument. These all show the importance
of citation summaries and the vast area for new
work to analyze them to produce a summary for a
given topic.
The Maximal Marginal Relevance (MMR)
summarization method, which is based on a
greedy algorithm, is described in (Carbonell and
Goldstein, 1998). MMR uses the full similarity
matrix to choose the sentences that are the least
similar to the sentences already selected for the
summary. We selected this method as one of our
Fact Occurrences
f1: ? Supervised Learning? 5
f2: ? instance/concept relations? 3
f3: ?Part-of-Speech tagging? 3
f4: ?filtering QA results? 2
f5: ?lexico-semantic information? 2
f6: ?hyponym relations? 2
Table 2: Nuggets of P03-1001 extracted by anno-
tators.
baseline methods, which we have explained in
more details in Section 4.
2 Data
In order to evaluate our method, we use the ACL
Anthology Network (AAN), which is a collec-
tion of papers from the Computational Linguistics
journal and proceedings from ACL conferences
and workshops and includes more than 13, 000 pa-
pers (Radev et al, 2009). We use 25 manually an-
notated papers from (Qazvinian and Radev, 2008),
which are highly cited articles in AAN. Table 1
shows the ACL ID, title, and the number of cita-
tion sentences for these papers.
The annotation guidelines asked a number of
annotators to read the citation summary of each
paper and extract a list of the main contribu-
tions of that paper. Each item on the list is a
non-overlapping contribution (nugget) perceived
by reading the citation summary. The annota-
tion strictly instructed the annotators to focus on
the citing sentences to do the task and not their
own background on the topic. Then, extracted
nuggets are reviewed and those nuggets that have
only been mentioned by 1 annotator are removed.
Finally, the union of the rest is used as a set of
nuggets representing each paper.
Table 2 lists the nuggets extracted by annotators
for P03-1001.
3 Methodology
Our methodology assumes that each citation sen-
tence covers 0 or more nuggets about the cited
papers, and tries to pick sentences that maximize
nugget coverage with respect to summary length.
These nuggets are essentially represented using
keyphrases. Therefore, we try to extract signifi-
cant keyphrases in order to represent nuggets each
sentence contains. Here, the keyphrases are ex-
896
ACL-ID Title # citations
N03-1017 Statistical Phrase-Based Translation 180
P02-1006 Learning Surface Text Patterns For A Question Answering System 74
P05-1012 On-line Large-Margin Training Of Dependency Parsers 71
C96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 66
P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 65
P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 55
P99-1065 A Statistical Parser For Czech 54
J04-4002 The Alignment Template Approach To Statistical Machine Translation 50
D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 42
P05-1013 Pseudo-Projective Dependency Parsing 40
W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 31
P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 27
N04-1033 Improvements In Phrase-Based Statistical Machine Translation 24
A00-2024 Cut And Paste Based Text Summarization 20
W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 19
A00-1043 Sentence Reduction For Automatic Text Summarization 19
C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 19
W05-1203 Measuring The Semantic Similarity Of Texts 17
W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 15
W03-0301 An Evaluation Exercise For Word Alignment 14
A00-1023 A Question Answering System Supported By Information Extraction 13
D04-9907 Scaling Web-Based Acquisition Of Entailment Relations 12
P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 10
H05-1047 A Semantic Approach To Recognizing Textual Entailment 8
H05-1079 Recognising Textual Entailment With Logical Inference 9
Table 1: List of papers chosen from AAN for evaluation together with the number of sentences citing
each.
unique all max freq
unigrams 229,631 7,746,792 437,308
bigrams 2,256,385 7,746,791 73,957
3-grams 5,125,249 7,746,790 3,600
4-grams 6,713,568 7,746,789 2,408
Table 3: Statistics on the abstract corpus in AAN
used as the background data
pressed using N -grams, and thus these building
units are the key elements to our summarization.
For each citation sentence di, our method first ex-
tracts a set of important keyphrases, Di, and then
tries to find sentences that have a larger number of
important and non-redundant keyphrases. In order
to take the first step, we extract statistically sig-
nificantly frequent N -grams (up to N = 4) from
each citing sentence and use them as the set of
representative keyphrases for that citing sentence.
3.1 Automatic Keyphrase Extraction
A list of keyphrases for each citation sentence can
be generated by extracting N -grams that occur
significantly frequently in that sentence compared
to a large corpus of such N -grams. Our method
for such an extraction is inspired by the previ-
ous work by Tomokiyo and Hurst (Tomokiyo and
Hurst, 2003).
A language model, M, is a statistical model
that assigns probabilities to a sequence of N -
grams. Every language model is a probability dis-
tribution over all N -grams and thus the probabili-
ties of all N -grams of the same length sum up to
1. In order to extract keyphrases from a text us-
ing statistical significance we need two language
models. The first model is referred to as the Back-
ground Model (BM) and is built using a large
text corpus. Here we build the BM using the text
of all the paper abstracts provided in AAN1. The
second language model is called the Foreground
Model (FM) and is the model built on the text
from which keyphrases are being extracted. In
this work, the set of all citation sentences that cite
a particular target paper are used to build a fore-
ground language model.
Let gi be an N -gram of size i and CM(gi) de-
note the count of gi in the modelM. First, we ex-
tract the counts of each N -grams in both the back-
ground (BM) and the foreground corpora (FM).
1http://chernobog.si.umich.edu/clair/anthology/index.cgi
897
MBM =
X
gi?{BM?FM}
1
NBM =
X
gi?{BM?FM}
CBM(gi)
NFM =
X
gi?FM
CFM(gi)
p?FM(gi) = CFM(gi)/NFM
p?BM(gi) = (CBM(gi) + 1)/(MBM +NBM)
The last equation is also known as Laplace
smoothing (Manning and Schutze, 2002) and han-
dles the N -grams in the foreground corpus that
have a 0 occurrence frequency in the background
corpus. Next, we extract N -grams from the fore-
ground corpus that have significant frequencies
compared to the frequency of the same N -grams
in the background model and its individual terms
in the foreground model.
To measure how randomly a set of consecu-
tive terms are forming an N -gram, Tomokiyo and
Hurst (Tomokiyo and Hurst, 2003) use pointwise
divergence. In particular, for an N -gram of size i,
gi = (w1w2 ? ? ?wi),
?gi(FMi?FM1) = p?FM(gi) log(
p?FM(gi)
Qi
j=1 p?FM(wj)
)
This equation shows the extent to which the
terms forming gi have occurred together ran-
domly. In other words, it indicates the extent of in-
formation that we lose by assuming independence
of each word by applying the unigram model, in-
stead of the N -gram model.
In addition, to measure how randomly a se-
quence of words appear in the foreground model
with respect to the background model, we use
pointwise divergence as well. Here, pointwise di-
vergence defines how much information we lose
by assuming that gi is drawn from the background
model instead of the foreground model:
?gi(FMi?BMi) = p?FM(gi) log(
p?FM(gi)
p?BM(gi)
)
(Corley and Mihalcea, 2005) applied or uti-
lized lexical based word overlap measures.
{overlap measures, word overlap, lexical
based, utilized lexical}
Table 4: Example: citation sentence for W05-
1203 written by D06-1621, and its extracted bi-
grams.
We set the criteria of choosing a sequence of
words as significant to be whether it has posi-
tive pointwise divergence with respect to both the
background model, and individual terms of the
foreground model. In other words we extract all gi
from FM for which the both properties are posi-
tive:
?gi(FMi?BMi) > 0
?gi(FMi?FM1) ? 0
The equality condition in the second equation
is specifically set to handle unigrams, in which
p?FM(gi) =
?i
j=1 p?FM(wj).
In order to handle the text corpora and build-
ing the language models, we have used the CMU-
Cambridge Language Model toolkit (Clarkson
and Rosenfeld, 1997). We use the set of cita-
tion sentences for each paper to build foreground
language models. Furthermore, we employ this
tool and make the background model using nearly
11,000 abstracts from AAN. Table 3 summarizes
some of the statistics about the background data.
Once keyphrases (significant N -grams) of each
sentence are extracted, we remove all N -grams in
which more than half of the terms are stopwords.
For instance, we remove all stopword unigrams,
if any, and all bigrams with at least one stop-
word in them. For 3-grams and 4-grams we use
a threshold of 2 and 3 stopwords respectively. Af-
ter that, the set of remaining N -grams is used to
represent each sentence and to build summaries.
Table 4 shows an example of a citation sentence
from D06-1621 citing W05-1203 (Corley and Mi-
halcea, 2005), and its extracted bigrams.
3.2 Sentence Selection
After extracting the set of keyphrases for each sen-
tence, di, the sentence is represented using its set
898
of N -grams, denoted by Di. Then, the goal is
to pick sentences (sets) for each paper that cover
more important and non-redundant keyphrases.
Essentially, keyphrases that have been repeated in
more sentences are more important and could rep-
resent more important nuggets. Therefore, sen-
tences that contain more frequent keyphrases are
more important. Based on this intuition we define
the reward of building a summary comprising a
set of keyphrases S as
f(S) = |S ?A|
where A is the set of all keyphrases from sen-
tences not in the summary.
The set function f has three main properties.
First, it is non-negative. Second, it is mono-
tone (i.e., For every set v we have f(S + v) ?
f(S)). Third, f is sub-modular. The submodular-
ity means that for a set v and two sets S ? T we
have
f(S + v)? f(S) ? f(T + v)? f(T )
Intuitively, this property implies that adding a set
v to S will increase the reward at least as much
as it would to a larger set T . In the summariza-
tion setting, this means that adding a sentence to
a smaller summary will increase the reward of the
summary at least as much as adding it to a larger
summary that subsumes it. The following theorem
formalizes this and is followed by a proof.
Theorem 1 The reward function f is submodular.
Proof
We start by defining a gain function G of adding
sentence (set) Di to Sk?1 where Sk?1 is the set
of keyphrases in a summary built using k? 1 sen-
tences, and Di is a candidate sentence to be added:
G(Di,Sk?1) = f(Sk?1 ?Di)? f(Sk?1)
Simple investigation through a Venn diagram
proof shows that G can be re-written as
G(Di,Sk?1) = |Di ? (?j 6=iDj)? Sk?1|
Let?s denote Di? (?j 6=iDj) by ?i. The follow-
ing equations prove the theorem.
Sk?1 ? Sk
S ?k?1 ? S ?k
?i ? S ?k?1 ? ?i ? S ?k
?i ? Sk?1 ? ?i ? Sk
| ?i ?Sk?1| ? | ?i ?Sk|
G(Di,Sk?1) ? G(Di,Sk)
f(Sk?1 ?Di)? f(Sk?1) ? f(Sk ?Di)? f(Sk)
Here, S ?k is the set of all N -grams in the vo-
cabulary that are not present in Sk. The gain of
adding a sentence, Di, to an empty summary is a
non-negative value.
G(Di,S0) = C ? 0
By induction, we will get
G(Di,S0) ? G(Di,S1) ? ? ? ? ? G(Di,Sk) ? 0
2
Theorem 1 implies the general case of submodu-
larity:
?m,n, 0 ? m ? n ? |D| ? G(Di,Sm) ? G(Di,Sn)
Maximizing this submodular function is an NP-
hard problem (Khuller et al, 1999). A common
way to solve this maximization problem is to start
with an empty set, and in each iteration pick a set
that maximizes the gain. It has been shown be-
fore in (Kulik et al, 2009) that if f is a submod-
ular, nondecreasing set function and f(?) = 0,
then such a greedy algorithm finds a set S , whose
gain is at least as high as (1 ? 1/e) of the best
possible solution. Therefore, we can optimize the
keyphrase coverage as described in Algorithm 1.
4 Experimental Setup
We use the annotated data described in Section 2.
In summary, the annotation consisted of two parts:
nugget extraction and nugget distribution analy-
sis. Five annotators were employed to annotate
the sentences in each of the 25 citation summaries
and write down the nuggets (non-overlapping con-
tributions) of the target paper. Then using these
899
Summary generated using bigram-based keyphrases
ID Sentence
P06-1048:1 Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al2003; Nguyen et al2004a; Turner and Charniak 2005;
McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes.
J05-4004:18 Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore
1996; Mahesh 1997; Carroll et al1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daume III and Marcu
2002; Daume III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process.
A00-2024:9 The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.
N03-1026:17 To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000)
and Jing (2000).
P06-2019:5 Jing (2000) was perhaps the first to tackle the sentence compression problem.
Table 5: Bigram-based summary generated for A00-1043.
Algorithm 1 The greedy algorithm for summary
generation
k ? the number of sentences in the summary
Di ? keyphrases in di
S ? ?
for l = 1 to k do
sl ? argmaxDi?D |Di ? (?j 6=iDj)|
S ? S ? sl
for j = 1 to |D| do
Dj ? Dj ? sl
end for
end for
return S
nugget sets, each sentence was annotated with the
nuggets it contains. This results in a sentence-
fact matrix that helps with the evaluation of the
summary. The summarization goal and the intu-
ition behind the summarizing system is to select a
few (5 in our experiments) sentences and cover as
many nuggets as possible. Each sentence in a cita-
tion summary may contain 0 or more nuggets and
not all nuggets are mentioned an equal number of
times. Covering some nuggets (contributions) is
therefore more important than others and should
be weighted highly.
To capture this property, the pyramid score
seems the best evaluation metric to use. We use
the pyramid evaluation method (Nenkova and Pas-
sonneau, 2004) at the sentence level to evaluate
the summary created for each set. We benefit
from the list of annotated nuggets provided by the
annotators as the ground truth of the summariza-
tion evaluation. These annotations give the list of
nuggets covered by each sentence in each citation
summary, which are equivalent to the summariza-
tion content unit (SCU) as described in (Nenkova
and Passonneau, 2004).
The pyramid score for a summary is calculated
as follows. Assume a pyramid that has n tiers, Ti,
where tier Ti > Tj if i > j (i.e., Ti is not below
Tj , and that if a nugget appears in more sentences,
it falls in a higher tier.). Tier Ti contains nuggets
that appeared in i sentences, and thus has weight
i. Suppose |Ti| shows the number of nuggets in
tier Ti, and Qi is the size of a subset of Ti whose
members appear in the summary. Further suppose
Q shows the sum of the weights of the facts that
are covered by the summary. Q =
?n
i=1 i?Qi.
In addition, the optimal pyramid score for a sum-
mary with X facts, is
Max =
n
X
i=j+1
i? |Ti|+ j ? (X ?
n
X
i=j+1
|Ti|)
where j = maxi(
?n
t=i |Tt| ? X). The pyra-
mid score for a summary is then calculated as fol-
lows.
P = QMax
This score ranges from 0 to 1, and a high
score shows the summary contains more heavily
weighted facts.
4.1 Baselines and Gold Standards
To evaluate the quality of the summaries gen-
erated by the greedy algorithm, we compare its
pyramid score in each of the 25 citation sum-
maries with those of a gold standard, a random
summary, and four other methods. The gold stan-
dards are summaries created manually using 5
sentences. The 5 sentences are manually selected
in a way to cover as many nuggets as possible with
higher priority for the nuggets with higher fre-
quencies. We also created random summaries us-
ing Mead (Radev et al, 2004). These summaries
900
are basically a random selection of 5 sentences
from the pool of sentences in the citation sum-
mary. Generally we expect the summaries cre-
ated by the greedy method to be significantly bet-
ter than random ones.
In addition to the gold and random summaries,
we also used 4 baseline state of the art sum-
marizers: LexRank, the clustering C-RR and
C-LexRank, and Maximal Marginal Relevance
(MMR). LexRank (Erkan and Radev, 2004) works
based on a random walk on the cosine similar-
ity of sentences and prints out the most frequently
visited sentences. Said differently, LexRank first
builds a network in which nodes are sentences and
edges are cosine similarity values. It then uses the
eigenvalue centralities to find the most central sen-
tences. For each set, the top 5 sentences on the list
are chosen for the summary.
The clustering methods, C-RR and C-LexRank,
work by clustering the cosine similarity network
of sentences. In such a network, nodes are sen-
tences and edges are cosine similarity of node
pairs. Clustering would intuitively put nodes with
similar nuggets in the same clusters as they are
more similar to each other. The C-RR method as
described in (Qazvinian and Radev, 2008) uses a
round-robin fashion to pick sentences from each
cluster, assuming that the clustering will put the
sentences with similar facts into the same clus-
ters. Unlike C-RR, C-LexRank uses LexRank to
find the most salient sentences in each cluster, and
prints out the most central nodes of each cluster as
summary sentences.
Finally, MMR uses the full cosine similarity
matrix and greedily chooses sentences that are the
least similar to those already selected for the sum-
mary (Carbonell and Goldstein, 1998). In partic-
ular,
MMR = arg min
di?D?A
[
max
dj?A
Sim(di, dj)
]
where A is the set of sentences in the summary,
initially set to A = ?. This method is different
from ours in that it chooses the least similar sen-
tence to the summary in each iteration.
4.2 Results and Discussion
As mentioned before, we use the text of the ab-
stracts of all the papers in AAN as the back-
ground, and each citation set as a separate fore-
ground corpus. For each citation set, we use the
method described in Section 3.1 to extract signif-
icant N -grams of each sentence. We then use the
keyphrase set representation of each sentence to
build the summaries using Algorithm 1. For each
of the 25 citation summaries, we build 4 differ-
ent summaries using unigrams, bigrams, 3-grams,
and 4-grams respectively. Table 5 shows a 5-
sentence summary created using algorithm 1 for
the paper A00-1043 (Jing, 2000).
The pyramid scores for different methods are
reported in Figure 1 together with the scores
of gold standards, manually created to cover as
many nuggets as possible in 5 sentences, as
well as summary evaluations of the 4 baseline
methods described above. This Figure shows
how the keyphrase based summarization method
when employing N -grams of size 3 or smaller,
outperforms other baseline systems significantly.
More importantly, Figure 1 also indicates that this
method shows more stable results and low varia-
tion in summary quality when keyphrases of size 3
or smaller are employed. In contrast, MMR shows
high variation in summary qualities making sum-
maries that obtain pyramid scores as low as 0.15.
Another important advantage of this method is
that we do not need to calculate the cosine simi-
larity of the pairs of sentences, which would add a
running time of O(|D|2|V |) in the number of doc-
uments, |D|, and the size of the vocabulary |V | to
the algorithm.
5 Conclusion and Future Work
This paper presents a summarization methodol-
ogy that employs keyphrase extraction to find im-
portant contributions of scientific articles. The
summarization is based on citation sentences and
picks sentences to cover nuggets (represented by
keyphrases) or contributions of the target papers.
In this setting the best summary would have as few
sentences and at the same time as many nuggets
as possible. In this work, we use pointwise KL-
divergence to extract statistically significant N -
grams and use them to represent nuggets. We
then apply a new set function for the task of sum-
marizing scientific articles. We have proved that
this function is submodular and concluded that a
901
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Gold Mead LexRank C?RR C?LexRank MMR 1?gram 2?gram 3?gram 4?gram
Py
ra
m
id
 S
co
re
Figure 1: Evaluation Results (summaries with 5 sentences): The median pyramid score over 25 datasets
using different methods.
greedy algorithm will result in a near-optimum set
of covered nuggets using only 5 sentences. Our
experiments in this paper confirm that the sum-
maries created based on the presented algorithm
are better than randomly generated summary, and
also outperform other state of the art summariza-
tion methods in most cases. Moreover, we show
how this method generates more stable summaries
with lower variation in summary quality when N -
grams of size 3 or smaller are employed.
A future direction for this work is to perform
post-processing on the summaries and re-generate
sentences that cover the extracted nuggets. How-
ever, the ultimate goal is to eventually develop
systems that can produce summaries of entire
research areas, summaries that will enable re-
searchers to easily and quickly switch between
fields of research.
One future study that will help us generate
better summaries is to understand how nuggets
are generated by authors. In fact, modeling the
nugget coverage behavior of paper authors will
help us identify more important nuggets and dis-
cover some aspects of the paper that would oth-
erwise be too difficult by just reading the paper
itself.
6 Acknowledgements
This work is in part supported by the National
Science Foundation grant ?iOPENER: A Flexi-
ble Framework to Support Rapid Learning in Un-
familiar Research Domains?, jointly awarded to
University of Michigan and University of Mary-
land as IIS 0705832, and in part by the NIH Grant
U54 DA021519 to the National Center for Inte-
grative Biomedical Informatics.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of the supporters.
References
Bradshaw, Shannon. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Bradshaw, Shannon. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
902
Conference on Research and Advanced Technology
for Digital Libraries.
Carbonell, Jaime G. and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries. In SI-
GIR?98, pages 335?336.
Clarkson, PR and R Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45?148.
Elkiss, Aaron, Siwei Shen, Anthony Fader, Gu?nes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51?62.
Erkan, Gu?nes? and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research.
Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliogra-
phy as a Resource for Indicative Summarization. In
Proceedings of LREC 2002, Las Palmas, Spain.
Khuller, Samir, Anna Moss, and Joseph (Seffi) Naor.
1999. The budgeted maximum coverage problem.
Inf. Process. Lett., 70(1):39?45.
Kulik, Ariel, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In SODA ?09, pages
545?554.
Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In SIGIR
?95, pages 68?73, New York, NY, USA. ACM.
Manning, Christopher D. and Hirich Schutze. 2002.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts, London, England.
Mei, Qiaozhu and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientific literature.
In Proceedings of ACL ?08, pages 816?824.
Mohammad, Saif, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using
citations to generate surveys of scientific paradigms.
In NAACL 2009, pages 584?592, June.
Nanba, Hidetsugu and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926?931.
Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-
mura. 2004. Classification of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation. In Proceedings
of the 11th SIG Classification Research Workshop,
pages 117?134, Chicago, USA.
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. Proceedings of the HLT-NAACL con-
ference.
Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.
Radev, Dragomir, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Radev, Dragomir R., Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In ACL workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries.
Siddharthan, Advaith and Simone Teufel. 2007.
Whose idea was this, and why does it matter? at-
tributing scientific work to citations. In Proceedings
of NAACL/HLT-07.
Teufel, Simone. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 workshop on Multi-
word expressions, pages 33?40.
903
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1245?1255,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
What?s with the Attitude? Identifying Sentences with Attitude in Online
Discussions
Ahmed Hassan Vahed Qazvinian
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
hassanam,vahed,radev@umich.edu
Dragomir Radev
Abstract
Mining sentiment from user generated content
is a very important task in Natural Language
Processing. An example of such content is
threaded discussions which act as a very im-
portant tool for communication and collabo-
ration in the Web. Threaded discussions in-
clude e-mails, e-mail lists, bulletin boards,
newsgroups, and Internet forums. Most of the
work on sentiment analysis has been centered
around finding the sentiment toward products
or topics. In this work, we present a method
to identify the attitude of participants in an
online discussion toward one another. This
would enable us to build a signed network
representation of participant interaction where
every edge has a sign that indicates whether
the interaction is positive or negative. This
is different from most of the research on so-
cial networks that has focused almost exclu-
sively on positive links. The method is exper-
imentally tested using a manually labeled set
of discussion posts. The results show that the
proposed method is capable of identifying at-
titudinal sentences, and their signs, with high
accuracy and that it outperforms several other
baselines.
1 Introduction
Mining sentiment from text has a wide range of
applications from mining product reviews on the
Web (Morinaga et al, 2002; Turney and Littman,
2003) to analyzing political speeches (Thomas et al,
2006). Automatic methods for sentiment mining are
very important because manual extraction of them is
very costly, and inefficient. A new application of
sentiment mining is to automatically identify atti-
tudes between participants in an online discussion.
An automatic tool to identify attitudes will enable
us to build a signed network representation of par-
ticipant interaction in which the interaction between
two participants is represented using a positive or
a negative edge. Even though using signed edges
in social network studies is clearly important, most
of the social networks research has focused only on
positive links between entities. Some work has re-
cently investigated signed networks (Leskovec et al,
2010; Kunegis et al, 2009), however this work was
limited to a few number of datasets in which users
were allowed to explicitly add negative, as well as
positive, relations. This work will pave the way for
research efforts to examine signed social networks
in more detail. It will also allow us to study the re-
lation between explicit relations and the text under-
lying those relation.
Although similar, identifying sentences that dis-
play an attitude in discussions is different from iden-
tifying opinionated sentences. A sentence in a dis-
cussion may bear opinions about a definite target
(e.g., price of a camera) and yet have no attitude to-
ward the other participants in the discussion. For in-
stance, in the following discussion Alice?s sentence
has her opinion against something, yet no attitude
toward the recipient of the sentence, Bob.
Alice: ?You know what, he turned out to
be a great disappointment?
Bob: ?You are completely unqualified to
judge this great person?
However, Bob shows strong attitude toward Alice.
In this work, we look at ways to predict whether a
sentence displays an attitude toward the text recip-
ient. An attitude is the mental position of one par-
ticipant with regard to another participant. it could
be either positive or negative. We consider features
which takes into account the entire structure of sen-
tences at different levels or generalization. Those
1245
features include lexical items, part-of-speech tags,
and dependency relations. We use all those patterns
to build several pairs of models that represent sen-
tences with and without attitude.
The rest of the paper is organized as follows. In
Section 2 we review some of the related prior work
on identifying polarized words and subjectivity anal-
ysis. We explain the problem definition and discuss
our approach in Sections 3 & 4. Finally, in Sec-
tions 5 & 6 we introduce our dataset and discuss the
experimental setup. Finally, we conclude in Section
7.
2 Related Work
Identifying the polarity of individual words is a well
studied problem. In previous work, Hatzivassiloglou
and McKeown (1997) propose a method to iden-
tify the polarity of adjectives. They use a manu-
ally labeled corpus to classify each conjunction of
an adjective as ?the same orientation? as the adjec-
tive or ?different orientation?. Their method can
label simple in ?simple and well-received? as the
same orientation and simplistic in ?simplistic but
well-received? as the opposite orientation of well-
received. Although the results look promising, the
method would only be applicable to adjectives since
noun conjunctions may collocate regardless of their
semantic orientations (e.g., ?rise and fall?).
In other work, Turney and Littman (2003) use sta-
tistical measures to find the association between a
given word and a set of positive/negative seed words.
In order to get word co-occurrence statistics they use
the ?near? operator from a commercial search en-
gine on a given word and a seed word.
In more recent work, Takamura et al (2005) used
the spin model to extract word semantic orientation.
First, they construct a network of words using def-
initions, thesaurus, and co-occurrence statistics. In
this network, each word is regarded as an electron,
which has a spin and each spin has a direction tak-
ing one of two values: up or down. Then, they use
the energy point of view to propose that neighboring
electrons tend to have the same spin direction, and
therefore neighboring words tend to have the same
polarity orientations. Finally, they use the mean field
method to find the optimal solution for electron spin
directions.
Previous work has also used WordNet, a lexi-
cal database of English, to identify word polarity.
Specifically, Hu and Liu (2004) use WordNet syn-
onyms and antonyms to predict the polarity of any
given word with unknown polarity. They label each
word with the polarity of its synonyms and the op-
posite polarity of its antonyms. They continue in
a bootstrapping manner to label all unlabeled in-
stances. This work is very similar to (Kamps et al,
2004) in which a network of WordNet synonyms
is used to find the shortest path between any given
word, and the words ?good? and ?bad?. Kim and
Hovy (Kim and Hovy, 2004) used WordNet syn-
onyms and antonyms to expand two lists of positive
and negative seed words. Similarly, Andreevskaia
and Bergler (2006) used WordNet to expand seed
lists with fuzzy sentiment categories, in which words
could be more central to one category than the other.
Finally, Kanayama and Nasukawa (2006) used syn-
tactic features and context coherency, defined as the
tendency for same polarities to appear successively,
to acquire polar atoms.
All the work mentioned above focus on the task
of identifying the polarity of individual words. Our
proposed work is identifying attitudes in sentences
that appear in online discussions. Perhaps the most
similar work to ours is the prior work on subjectivity
analysis, which is to identify text that present opin-
ions as opposed to objective text that present fac-
tual information (Wiebe, 2000). Prior work on sub-
jectivity analysis mainly consists of two main cate-
gories: The first category is concerned with identify-
ing the subjectivity of individual phrases and words
regardless of the sentence and context they appear
in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;
Banea et al, 2008). In the second category, sub-
jectivity of a phrase or word is analyzed within its
context (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, ). A good study of the applications
of subjectivity analysis from review mining to email
classification is given in (Wiebe, 2000). Somasun-
daran et al (2007) develop genre-speci.c lexicons
using interesting function word combinations for de-
tecting opinions in meetings. Despite similarities,
our work is different from subjectivity analysis be-
cause the later only discriminates between opinions
and facts. A discussion sentence may display an
1246
opinion about some topic yet no attitude. The lan-
guage constituents considered in opinion detection
may be different from those used to detect attitude.
Moreover, extracting attitudes from online discus-
sions is different from targeting subjective expres-
sions (Josef Ruppenhofer and Wiebe, 2008; Kim
and Hovy, 2004). The later usually has a limited
set of targets that compete for the subjective expres-
sions (for example in movie review, targets could be:
director, actors, plot, and so forth). We cannot use
similar methods because we are working on an open
domain where anything could be a target. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
There is also some related work on mining on-
line discussions. Lin et al(2009) proposes a sparse
coding-based model simultaneously model seman-
tics and structure of threaded discussions. Shen
et al(2006) proposes three clustering methods for
exploiting the temporal information in the streams,
as well as an algorithm based on linguistic fea-
tures to analyze the discourse structure information.
Huang et al(2007) used an SVM classifier to extract
(thread-title, reply) pairs as chat knowledge from on-
line discussion forums to support the construction
of a chatbot for a certain domain. Other work has
focused on the structure of questions and question-
answer pairs in online forums and discussions (Ding
et al, 2008; Cong et al, 2008).
3 Problem Definition
Assume we have a set of sentences exchanged be-
tween participants in an online discussion. Our ob-
jective is to identify sentences that display an atti-
tude from the text writer to the text recepient from
those that do not. An attitude is the mental posi-
tion of one particpant with regard to another partic-
ipant. An attitude may not be directly observable,
but rather inferred from what particpants say to one
another. The attitude could be either positive or neg-
ative. Strategies for showing a positive attitude may
include agreement, and praise, while strategies for
showing a negative attitude may include disagree-
ment, insults, and negative slang. After identifying
sentences that display an attitude, we also predict the
sign (positive or negative) of that attitude.
4 Approach
In this section, we describe a model which, given a
sentence, predicts whether it carries an attitude from
the text writer toward the text recipient or not. Any
given piece of text exchanged between two partici-
pants in a discussion could carry an attitude toward
the text recipient, an attitude towards the topic, or
no attitude at all. As we are only interested in at-
titudes between participants, we limit our study to
sentences that use second person pronouns. Second
person pronouns are usually used in conversational
genre to indicate that the text writer is addressing the
text recipient. After identifying those sentences, we
do some pre-processing to extract the most relevant
fragments. We examine these fragments to to iden-
tify the polarity of every word in the sentence. Every
word could be assigned a semantic orientation. The
semantic orientation could be either positive, nega-
tive, or neutral. The existence of polarized words in
any sentence is an important indicator of whether it
carries an attitude or not.
The next step is to extract several patterns at
different levels of generalization representing any
given sentence. We use those patterns to build two
Markov models for every kind of patterns. The first
model characterizes the relation between different
tokens for all patterns that correspond to sentences
that have an attitude. The second model is similar to
the first one, but it uses all patterns that correspond
to sentences that do not have an attitude. Given a
new sentence, we extract the corresponding patterns
and estimate the likelihood of every pattern being
generated from the two corresponding models. We
then compare the likelihood of the sentence under
the two models and use this as a feature to predict
the existence of an attitude. A pair of models will
be built for every kind of patterns. If we have n dif-
ferent patterns, we will have n different likelihood
ratios that come from n pairs of models.
4.1 Word Polarity Identification
Identifying the polarity of words is an important step
for our method. Our word identification module is
similar to the work in (Annon, 2010). We construct
a graph where each node represent a word/part-of-
speech pair. Two nodes are linked if the words are
related. We use WordNet (Miller, 1995) to link re-
1247
lated words based on synonyms, hypernyms, and
similar to relations. For words that do not appear
in Wordnet, we used Wiktionary, a collaboratively
constructed dictionary. We also add some links
based on co-occurrence statistics between words as
from a large corpus. The resulting graph is a graph
G(W,E) where W is a set of word/part-of-speech
pairs, and E is the set of edges connecting related
words.
We define a random walk model on the graph,
where the set of nodes correspond to the state space
of the random walk. Transition probabilities are cal-
culated by normalizing the weights of the edges out
of every node. Let S+ and S? be two sets of ver-
tices representing seed words that are already la-
beled as either positive or negative respectively. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). For
any given word w, we calculate the mean hitting
time betweenw, and the two seed sets h(w|S+), and
h(w|S?). The mean hitting time h(i|k) is defined as
the average number of steps a random walker, start-
ing in state i 6= k, will take to enter state k for the
first time (Norris, 1997). If h(w|S+) is greater than
h(w|S?), the word is classified as negative, oth-
erwise it is classified as positive. We also use the
method described in (Wilson et al, 2005) to deter-
mine the contextual polarity of the identified words.
The set of features used to predict contextual polar-
ity include word, sentence, polarity, structure, and
other features.
4.2 Identifying Relevant Parts of Sentences
The writing style in online discussion forums is very
informal. Some of the sentence are very long, and
punctuation marks are not always properly used. To
solve this problem, we decided to use the grammat-
ical structure of sentences to identify the most rele-
vant part of sentences that would be the subject of
further analysis. Figure 1 shows a parse tree repre-
senting the grammatical structure of a particular sen-
tence. If we closely examine the sentence, we will
notice that we are only interested in a part of the
sentence that includes the second person pronoun
?you?. We extract this part, by starting at the word
of interest , in this case ?you?, and go up in the hi-
erarchy till we hit the first sentence clause. Once,
we reach a sentence clause, we extract the corre-
sponding text if it is grammatical, otherwise we go
up one more level to the closest sentence clause. We
used the Stanford parser to generate the grammatical
structure of sentences (Klein and Manning, 2003).
Figure 1: An example showing how to identify the rele-
vant part of a sentence.
4.3 Sentences as Patterns
The fragments we extracted earlier are more rele-
vant to our task and are more suitable for further
analysis. However, these fragments are completely
lexicalized and consequently the performance of any
analysis based on them will be limited by data spar-
sity. We can alleviate this by using more general
representations of words. Those general representa-
tions can be used a long with words to generate a set
of patterns that represent each fragment. Each pat-
tern consists of a sequence of tokens. Examples of
such patterns could use lexical items, part-of-speech
(POS) tags, word polarity tags, and dependency re-
lations.
We use three different patterns to represent each
fragments:
? Lexical patterns: All polarized words are re-
places with the corresponding polarity tag, and
all other words are left as is.
? Part-of-speech patterns: All words are replaced
with their POS tags. Second person pronouns
are left as is. Polarized words are replaced with
their polarity tags and their POS tags.
? Dependency grammar patterns: the shortest
path connecting every second person pronoun
1248
to the closed polarized word is extracted. The
second person pronoun, the polarized word tag,
and the types of the dependency relations along
the path connecting them are used as a pat-
tern. It has been shown in previous work on
relation extraction that the shortest path be-
tween any two entities captures the the in-
formation required to assert a relationship be-
tween them (Bunescu and Mooney, 2005). Ev-
ery polarized word is assigned to the closest
second person pronoun in the dependency tree.
This is only useful for sentences that have po-
larized words.
Table 1 shows the different kinds of representa-
tions for a particular sentence. We use text, part-
of-speech tags, polarity tags, and dependency rela-
tions. The corresponding patterns for this sentence
are shown in Table 2.
4.4 Building the Models
Given a set of patterns representing a set of sen-
tences, we can build a graph G = V,E,w where
V is the set of all possible token that may appear
in the patterns. E = V ? V is the set of possible
transitions between any two tokens. w : E ? [0..1]
is a weighting function that assigns to every pair of
states (i, j) a weight w(i, j) representing the proba-
bility that we have a transition from state i to state
j.
This graph corresponds to a Markovian model.
The set of states are the vocabulary, and the the tran-
sition probabilities between states are estimated us-
ing Maximum Likelihood estimation as follows:
Pij =
Nij
Ni
whereNij is the number of times we saw a transition
from i to state j, and Ni is the total number of times
we saw state i in the training data. This is similar to
building a language model over the language of the
patterns.
We build two such models for every kind of pat-
terns. The first model is built using all sentences that
appeared in the training dataset and was labeled as
having an attitude, and the second model is built us-
ing all sentences in the training dataset that do not
have an attitude. If we have n kinds of patterns, we
will build one such pair for every kind of patterns.
Hence, we will end up with 2n models.
4.5 Identifying Sentences with Attitude
We split our training data into two splits; the first
containing all sentences that have an attitude and the
second containing all sentences that do not have an
attitude. Given the methodology described in the
previous section, we build n pairs of Markov mod-
els. Given any sentence, we extract the correspond-
ing patterns and estimate the log likelihood that this
sequence of tokens was generated from every model.
Given a model M , and sequence of tokens T =
(T1, T2, . . . TSn), the probability of this token se-
quence being generated from M is:
PM (T ) =
n?
i=2
P (Ti|T1, . . . , Ti?1) =
n?
i=2
W (Ti?1, Ti)
where n is the number of tokens in the pattern, and
W is the probability transition function.
The log likelihood is then defined as:
LLM (T ) =
n?
i=2
logW (Ti?1, Ti)
For every pair of models, we may use the ratio be-
tween the two likelihoods as a feature:
f =
LLMatt(T )
LLMnoatt(T )
where T is the token sequence, LLMatt(T ) is the log
likelihood of the sequence given the attitude model,
and LLMnoatt(T ) is the log likelihood of the pattern
given the no-attitude model.
Given the n kinds of patterns, we can calculate
three different features. A standard machine learn-
ing classifier is then trained using those features to
predict whether a given sentence has an attitude or
not.
4.6 Identifying the Sign of an Attitude
To determine the orientation of an attitude sentence,
we tried two different methods. The first method as-
sumes that the orientation of an attitude sentence is
directly related to the polarity of the words it con-
tains. If the sentence has only positive and neutral
1249
Table 1: Tags used for building patterns
Text That makes your claims so ignorant
POS That/DT makes/VBZ your/PRP$ claims/NNS so/RB ignorant/JJ
Polarity That/O makes/O your/O claims/O so/O ignorant/NEG
Dependency your
poss
? claims
nsubj
? ignorant
Table 2: Sample patterns
Lexical pattern That makes your claims so NEG
POS pattern DT VBZ your PRP$ NNS RB NEG JJ
Dependency pattern your poss nsubj NEG
words, it is classified as positive. If the sentence
has only negative and neutral words, it is classified
as negative. If the sentence has both positive and
negative words, we calculate the summation of the
polarity scores of all positive words and that of all
negative words. The polarity score of a word is an
indicator of how strong of a polarized word it is. If
the former is greater, we classify the sentence as pos-
itive,otherwise we classify the sentence as negative.
The problem with this method is that it assumes
that all polarized words in a sentence with an atti-
tude target the text recipient. Unfortunately, that is
not always correct. For example, the sentence ?You
are completely unqualified to judge this great per-
son? has a positive word ?great? and a negative word
?unqualified?. The first method will not be able to
predict whether the sentence is positive or negative.
To solve this problem, we use another method that
is based on the paths that connect polarized words to
second person pronouns in a dependency parse tree.
For every positive word w , we identify the shortest
path connecting it to every second person pronoun
in the sentence then we compute the average length
of the shortest path connecting every positive word
to the closest second person pronoun. We repeat for
negative words and compare the two values. The
sentence is classified as positive if the average length
of the shortest path connecting positive words to the
closest second person pronoun is smaller than the
corresponding value for negative words. Otherwise,
we classify the sentence as negative.
5 Data
Our data was randomly collected from a set of dis-
cussion groups. We collected a large number of
threads from the first quarter of 2009 from a set of
Usenet discussion groups. All threads were in En-
glish, and had 5 posts or more. We parsed the down-
loaded threads to identify the posts and senders. We
kept posts that have quoted text and discarded all
other posts. The reason behind that is that partici-
pants usually quote other participants text when they
reply to them. This restriction allows us to iden-
tify the target of every post, and raises the proba-
bility that the post will display an attitude from its
writer to its target. We plan to use more sophsticated
methods for reconstructing the reply structure like
the one in (Lin et al, 2009). From those posts, we
randomly selected approximately 10,000 sentences
that use second person pronouns. We explained ear-
lier how second person pronouns are used in discus-
sions genres to indicate the writer is targeting the
text recipient. Given a random sentence selected
from some random discussion thread, the probabil-
ity that the sentence does not have an attitude is sig-
nificantly larger than the probability that it will have
an attitude. Hence, restricting our dataset to posts
with quoted text and sentences with second person
pronouns is very important to make sure that we
will have a considerable amount of attitudinal sen-
tences. The data was tokenized, sentence-split, part-
of-speech tagged with the OpenNLP toolkit. It was
parsed with the Stanford dependency parser (Klein
and Manning, 2003).
5.1 Annotation Scheme
The goals of the annotation scheme are to distin-
guish sentences that display an attitude from those
that do not. Sentences could display either a neg-
ative or a positive attitude. Disagreement, insults,
and negative slang are indicators of negative attitude.
1250
A B C D
A - 82.7 80.6 82.1
B 81.0 - 81.9 82.9
C 77.8 78.2 - 83.8
D 78.3 77.7 78.6 -
Table 3: Inter-annotator agreement
Agreement, and praise are indicators of positive at-
titude. Our annotators were instructed to read every
sentence and assign two labels to it. The first speci-
fies whether the sentence displays an attitude or not.
The existence of an attitude was judged on a three
point scale: attitude, unsure, and no-attitude. The
second is the sign of the attitude. If an attitude ex-
ists, annotators were asked to specify whether the
attitude is positive or negative. To evaluate inter-
annotator agreement, we use the agr operator pre-
sented in (Wiebe et al, 2005). This metric measures
the precision and recall of one annotator using the
annotations of another annotator as a gold standard.
The process is repeated for all pairs of annotators,
and then the harmonic mean of all values is reported.
Formally:
agr(A|B) =
|A ?B|
|A|
(1)
where A, and B are the annotation sets produced by
the two reviewers. Table 3 shows the value of the
agr operator for all pairs of annotators. The har-
monic mean of the agr operator is 80%. The agr
operator was used over the Kappa Statistic because
the distribution of the data was fairly skewed.
6 Experiments
6.1 Experimental Setup
We performed experiments on the data described in
the previous section. The number of sentences with
an attitude was around 20% of the entire dataset.
The class imbalance caused by the small number of
attitude sentences may hurt the performance of the
learning algorithm (Provost, 2000). A common way
of addressing this problem is to artificially rebal-
ance the training data. To do this we down-sample
the majority class by randomly selecting, without
replacement, a number of sentences without an at-
titude that equals the number of sentences with an
attitude. That resulted in a balanced subset, approx-
imately 4000 sentences, that we used in our experi-
ments.
We used Support Vector Machines (SVM) as a
classifier. We optimized SVM separately for every
experiment. We used 10-fold cross validation for all
tests. We evaluate our results in terms of precision,
recall, accuracy, and F1. Statistical significance was
tested using a 2-tailed paired t-test. All reported re-
sults are statistically significant at the 0.05 level. We
compare the proposed method to several other base-
lines that will be described in the next subsection.
We also perform experiments to measure the perfor-
mance if we mix features from the baselines and the
proposed method.
6.2 Baselines
The first baseline is based on the hypothesis that the
existence of polarized words is a strong indicator
that the sentence has an attitude. As a result, we
use the number of polarized word in the sentence,
the percentage of polarized words to all other words,
and whether the sentences has polarized words with
mixed or same sign as features to train an SVM clas-
sifier to detect attitude.
The second baseline is based on the proximity be-
tween the polarized words and the second person
pronouns. We assume that every polarized word is
associated with the closest second person pronoun.
Let w be a polarized word and p(w) be the closes
second person pronoun, and surf dist(w, p(w)) be
the surface distance betweenw and p(w). This base-
line uses the minimum, maximum, and average of
surf dist(w, p(w)) for all polarized words as fea-
tures to train an SVM classifier to identify sentences
with attitude.
The next baseline uses the dependency tree dis-
tance instead of the surface distance. We assume that
every polarized word is associated to the second per-
son pronoun that is connected to it using the smallest
shortest path. The dep dist(w, p(w)) is calculated
similar to the previous baselines but using the de-
pendency tree distance. The minimum, maximum,
and average of this distance for all polarized words
are used as features to train an SVM classifier.
1251
Figure 2: Accuracy, Precision, and Recall for the Pro-
posed Approach and the Baselines.
0 10 20 30 40 50 60 70 80 90 1000
10
20
30
40
50
60
70
80
90
100
Recall
Prec
ision
 
 MMSurfDistDepDistPol
Figure 3: Precision Recall Graph.
6.3 Results and Discussion
Figure 2 compares the accuracy, precision, and re-
call of the proposed method (ML), the polarity based
classifier (POL), the surface distance based classi-
fier (Surf Dist), and the dependency distance based
classifier (Dep Dist). The values are selected to opti-
mize F1. The figure shows that the surface distance
based classifier behaves poorly with low accuracy,
precision, and recall. The two other baselines be-
have poorly as well in terms of precision and accu-
racy, but they do very well in terms of recall. We
looked at some of the examples to understand why
those two baselines achieve very high recall. It turns
out that they tend to predict most sentences that have
polarized words as sentences with attitude. This re-
sults in many false positives and low true negative
rate. Achieving high recall at the expense of losing
precision is trivial. On the other hand, we notice that
0 10 20 30 40 50 60 70 80 90 10075
76
77
78
79
80
81
82
Training Set Size (%)
Accu
racy
Figure 4: Accuracy Learning Curve for the Proposed
Method.
the proposed method results in very close values of
precision and recall at the optimum F1 point.
To better compare the performance of the pro-
posed method and the baseline, we study the the
precision-recall curves for all methods in Figure 3.
We notice that the proposed method outperforms all
baselines at all operating points. We also notice that
the proposed method provides a nice trade-off be-
tween precision and recall. This allows us some flex-
ibility in choosing the operating point. For example,
in some applications we might be interested in very
high precision even if we lose recall, while in other
applications we might sacrifice precision in order to
get high recall. On the other hand, we notice that
the baselines always have low precision regardless
of recall.
Table 4 shows the accuracy, precision, recall, and
F1 for the proposed method and all baselines. It also
shows the performance when we add features from
the baselines to the proposed method, or merge some
of the baselines. We see that we did not get any im-
provement when we added the baseline features to
the proposed method. We believe that the proposed
method captures all the information captured by the
baselines and more.
Our proposed method uses three different features
that correspond to the three types of patterns we use
to represent every sentence. To understand the con-
tributions of every feature, we measure the perfor-
mance of every feature by itself and also all possible
combinations of pairs of features. We compare that
1252
to the performance we get when using all features in
Table 5. We see that the part-of-speech patterns per-
forms better than the text patterns. This makes sense
because the former suffers from data sparsity. De-
pendency patterns performs best in terms of recall,
while part-of-speech patterns outperform all others
in terms of precision, and accuracy. All pairs of
features outperform any single feature that belong
to the corresponding pair in terms of F1. We also
notice that using the three features results in better
performance when compared to all other combina-
tions. This shows that every kind of pattern captures
slightly different information when compared to the
others. It also shows that merging the three features
improves performance.
One important question is how much data is re-
quired to the proposed model. We constructed a
learning curve, shown in Figure 4, by fixing the
test set size at one tenth of the data, and varying
the training set size. We carried out ten-fold cross
validation as with our previous experiments. We see
that adding more data continues to increase the accu-
racy, and that accuracy is quite sensitive to the train-
ing data. This suggests that adding more data to this
model could lead to even better results.
We also measured the accuracy of the two meth-
ods we proposed for predicting the sign of attitudes.
The accuracy of the first model that only uses the
count and scores of polarized words was 95%. The
accuracy of the second method that used depen-
dency distance was 97%.
6.4 Error Analysis
We had a closer look at the results to find out what
are the reasons behind incorrect predictions. We
found two main reasons. First, errors in predicting
word polarity usually propagates and results in er-
rors in attitude prediction. The reasons behind incor-
rect word polarity predictions is ambiguity in word
senses and infrequent words that have very few con-
nection in thesaurus. A possible solution to this type
of errors is to improve the word polarity identifica-
tion module by including word sense disambigua-
tion and adding more links to the words graph using
glosses or co-occurrence statistics. The second rea-
son is that some sentences are sarcastic in nature. It
is so difficult to identify such sentences. Identify-
ing sarcasm should be addressed as a separate prob-
Method Accuracy Precision Recall F1
ML 80.3 81.0 79.4 80.2
POL 73.1 66.4 93.9 77.7
ML+POL 79.9 77.9 83.4 80.5
SurfDist 70.2 67.1 79.2 72.7
DepDist 73.1 66.4 93.8 77.8
SurfDist+ 73.1 66.4 93.8 77.7
DepDist
ML+SurfDist 73.9 67.2 93.6 78.2
ML+DepDist 72.8 66.1 93.8 77.6
ML+SurfDist+ 74.0 67.2 93.4 78.2
DepDist
SurfDist+ 73.1 66.3 93.8 77.7
DepDist+POL
ML+SurfDist+ 73.0 66.2 93.8 77.6
DepDist+POL
Table 4: Precision, Recall, F1, and Accuracy for the pro-
posed method, the baselines, and different combinations
of proposed method and the baselines features
Method Accuracy Precision Recall F1
txt 75.5 74.1 78.6 76.2
pos 77.7 78.2 76.9 77.5
dep 74.7 70.4 85.1 77.0
txt+pos 77.8 77.0 79.4 78.1
txt+dep 79.4 79.6 79.2 79.4
pos+dep 80.4 79.1 82.5 80.7
txt+pos+dep 80.3 81.0 79.4 80.2
Table 5: Precision, Recall, F1, and Accuracy for different
combinations of the proposed method?s features.
lem. A method that utilizes holistic approaches that
takes context and previous interactions between dis-
cussion participants into consideration could be used
to address it.
7 Conclusions
We have shown that training a supervised Markov
model of text, part-of-speech, and dependecy pat-
terns allows us to identify sentences with attitudes
from sentences without attitude. This model is more
accurate than several other baselines that use fea-
tures based on the existence of polarized word, and
proximity between polarized words and second per-
son pronouns both in text and dependecy trees. This
method allows to extract signed social networks
from multi-party online discussions. This opens the
door to research efforts that go beyond standard so-
cial network analysis that is based on positve links
1253
only. It also allows us to study dynamics behind in-
teractions in online discussions, the relation between
text and social interactions, and how groups form
and break in online discussions.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL?06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT ?05, pages 724?731, Morristown, NJ,
USA. Association for Computational Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08, pages 467?
474.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
ACL?08, pages 710?718.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion fo-
rums. In IJCAI?07, pages 423?428.
Swapna Somasundaran Josef Ruppenhofer and Janyce
Wiebe. 2008. Finding the sources and targets
of subjective expressions. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08).
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03, pages 423?430.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a so-
cial network with negative edges. In WWW?09, pages
741?750, New York, NY, USA.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010. Predicting positive and negative links in online
social networks. In WWW ?10, pages 641?650, New
York, NY, USA. ACM.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP?05.
Foster Provost. 2000. Machine learning from imbal-
anced data sets 101. In Proceedings of the AAAI Work-
shop on Imbalanced Data Sets.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
1254
meetings. In Proceedings of the SIGdial Workshop on
Discourse and Dialogue.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
1(2):0.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP?05, Vancouver,
Canada.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
1255
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589?1599,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Rumor has it: Identifying Misinformation in Microblogs
Vahed Qazvinian Emily Rosengren Dragomir R. Radev Qiaozhu Mei
University of Michigan
Ann Arbor, MI
{vahed,emirose,radev,qmei}@umich.edu
Abstract
A rumor is commonly defined as a state-
ment whose true value is unverifiable. Ru-
mors may spread misinformation (false infor-
mation) or disinformation (deliberately false
information) on a network of people. Identi-
fying rumors is crucial in online social media
where large amounts of information are easily
spread across a large network by sources with
unverified authority. In this paper, we address
the problem of rumor detection in microblogs
and explore the effectiveness of 3 categories of
features: content-based, network-based, and
microblog-specific memes for correctly iden-
tifying rumors. Moreover, we show how these
features are also effective in identifying disin-
formers, users who endorse a rumor and fur-
ther help it to spread. We perform our exper-
iments on more than 10,000 manually anno-
tated tweets collected from Twitter and show
how our retrieval model achieves more than
0.95 in Mean Average Precision (MAP). Fi-
nally, we believe that our dataset is the first
large-scale dataset on rumor detection. It can
open new dimensions in analyzing online mis-
information and other aspects of microblog
conversations.
1 Introduction
A rumor is an unverified and instrumentally relevant
statement of information spread among people (Di-
Fonzo and Bordia, 2007). Social psychologists ar-
gue that rumors arise in contexts of ambiguity, when
the meaning of a situation is not readily apparent,
or potential threat, when people feel an acute need
for security. For instance rumors about ?office ren-
ovation in a company? is an example of an ambigu-
ous context, and the rumor that ?underarm deodor-
ants cause breast cancer? is an example of a context
in which one?s well-being is at risk (DiFonzo et al,
1994).
The rapid growth of online social media has made
it possible for rumors to spread more quickly. On-
line social media enable unreliable sources to spread
large amounts of unverified information among peo-
ple (Herman and Chomsky, 2002). Therefore, it is
crucial to design systems that automatically detect
misinformation and disinformation (the former of-
ten seen as simply false and the latter as deliberately
false information).
Our definition of a rumor is established based on
social psychology, where a rumor is defined as a
statement whose truth-value is unverifiable or delib-
erately false. In-depth rumor analysis such as deter-
mining the intent and impact behind the spread of
a rumor is a very challenging task and is not possi-
ble without first retrieving the complete set of social
conversations (e.g., tweets) that are actually about
the rumor. In our work, we take this first step to
retrieve a complete set of tweets that discuss a spe-
cific rumor. In our approach, we address two basic
problems. The first problem concerns retrieving on-
line microblogs that are rumor-related. In the second
problem, we try to identify tweets in which the ru-
mor is endorsed (the posters show that they believe
the rumor).
2 Related Work
We review related work on 3 main areas: Analyzing
rumors, mining microblogs, and sentiment analysis
and subjectivity detection.
2.1 Rumor Identification and Analysis
Though understanding rumors has been the sub-
ject of research in psychology for some time (All-
port and Lepkin, 1945), (Allport and Postman,
1947), (DiFonzo and Bordia, 2007), research has
1589
only recently begun to investigate how rumors are
manifested and spread differently online. Mi-
croblogging services, like Twitter, allow small
pieces of information to spread quickly to large au-
diences, allowing rumors to be created and spread in
new ways (Ratkiewicz et al, 2010).
Related research has used different methods to
study the spread of memes and false information
on the web. Leskovec et al use the evolution
of quotes reproduced online to identify memes and
track their spread overtime (Leskovec et al, 2009).
Ratkiewicz et al (Ratkiewicz et al, 2010) created
the ?Truthy? system, identifying misleading politi-
cal memes on Twitter using tweet features, includ-
ing hashtags, links, and mentions. Other projects
focus on highlighting disputed claims on the Inter-
net using pattern matching techniques (Ennals et al,
2010). Though our project builds on previous work,
our work differs in its general focus on identifying
rumors from a corpus of relevant phrases and our at-
tempts to further discriminate between phrases that
confirm, refute, question, and simply talk about ru-
mors of interest.
Mendoza et al explore Twitter data to analyze the
behavior of Twitter users under the emergency situ-
ation of 2010 earthquake in Chile (Mendoza et al,
). They analyze the re-tweet network topology and
find that the patterns of propagation in rumors dif-
fer from news because rumors tend to be questioned
more than news by the Twitter community.
2.2 Sentiment Analysis
The automated detection of rumors is similar to tra-
ditional NLP sentiment analysis tasks. Previous
work has used machine learning techniques to iden-
tify positive and negative movie reviews (Pang et
al., 2002). Hassan et al use a supervised Markov
model, part of speech, and dependency patterns to
identify attitudinal polarities in threads posted to
Usenet discussion posts (Hassan et al, 2010). Oth-
ers have designated sentiment scores for news sto-
ries and blog posts based on algorithmically gener-
ated lexicons of positive and negative words (God-
bole et al, 2007). Pang and Lee provide a detailed
overview of current techniques and practices in sen-
timent analysis and opinion mining (Pang and Lee,
2008; Pang and Lee, 2004).
Though rumor classification is closely related to
opinion mining and sentiment analysis, it presents
a different class of problem because we are con-
cerned not just with the opinion of the person post-
ing a tweet, but with whether the statements they
post appear controversial. The automatic identifica-
tion of rumors from a corpus is most closely related
to the identification of memes done in (Leskovec et
al., 2009), but presents new challenges since we seek
to highlight a certain type of recurring phrases. Our
work presents one of the first attempts at automatic
rumor analysis.
2.3 Mining Twitter Data
With its nearly constant update of new posts and
public API, Twitter can be a useful source for
collecting data to be used in exploring a num-
ber of problems related to natural language pro-
cessing and information diffusion (Bifet and Frank,
2010). Pak and Paroubek demonstrated experimen-
tally that despite frequent occurrences of irregular
speech patterns in tweets, Twitter can provide a use-
ful corpus for sentiment analysis (Pak and Paroubek,
2010). The diversity of Twitter users make this
corpus especially valuable. Ratkiewicz et alalso
use Twitter to detect and track misleading political
memes (Ratkiewicz et al, 2010).
Along with many advantages, using Twitter as a
corpus for sentiment analysis does present unusual
challenges. Because posts are limited to 140 charac-
ters, tweets often contain information in an unusu-
ally compressed form and, as a result, grammar used
may be unconventional. Instances of sarcasm and
humor are also prevalent (Bifet and Frank, 2010).
The procedures we used for the collection and anal-
ysis of tweets are similar to those described in previ-
ous work. However, our goal of developing compu-
tational methods to identify rumors being transmit-
ted through tweets differentiates our project.
3 Problem Definition
Assume we have a set of tweets that are about the
same topic that has some controversial aspects. Our
objective in this work is two-fold: (1) Extract tweets
that are about the controversial aspects of the story
and spread misinformation (Rumor retrieval). (2)
Identify users who believe that misinformation ver-
sus users who refute or question the rumor (Belief
1590
Name Rumor Regular Expression Query Status #tweets
obama Is Barack Obama muslim? Obama & (muslim|islam) false 4975
airfrance Air France mid-air crash photos? (air.france|air france) & (photo|pic|pix) false 505
cellphone Cell phone numbers going public? (cell|cellphone|cell phone) mostly false 215
michelle Michelle Obama hired too many staff? staff & (michelle obama|first lady|1st lady) partly true 299
palin Sarah Palin getting divorced? palin & divorce false 4423
Table 1: List of rumor examples and their corresponding queries used to collect data from Twitter
classification).
The following two tweets are two instances of the
tweets written about president Obama and the Mus-
lim world. The first tweet below is about president
Obama and Muslim world, where the second tweet
spread misinformation that president Obama is Mus-
lim.
(non-rumor) ?As Obama bows to Muslim leaders
Americans are less safe not only at home but also
overseas. Note: The terror alert in Europe... ?
(rumor) ?RT @johnnyA99 Ann Coulter Tells Larry
King Why People Think Obama Is A Muslim
http://bit.ly/9rs6pa #Hussein via @NewsBusters
#tcot ..?
The goal of the retrieval task is to discriminate
between such tweets. In the second task, we use
the tweets that are flagged as rumorous, and identify
users that endorse (believe) the rumor versus users
who deny or question it. The following three tweets
are about the same story. The first user is a believer
and the second and third are not.
(confirm) ?RT @moronwatch: Obama?s a Muslim. Or
if he?s not, he sure looks like one #whyimvotingre-
publican.?
(deny) ?Barack Obama is a Christian man who had
a Christian wedding with 2 kids baptised in Jesus
name. Tea Party clowns call that muslim #p2 #gop?
(doubtful) ?President Barack Obama?s Religion:
Christian, Muslim, or Agnostic? - The News
of Today (Google): Share With Friend...
http://bit.ly/bk42ZQ?
The first task is substantially more challenging
than a standard IR task because of the requirement of
both high precision (every result should be actually
discussing the rumor) and high recall (the set should
be complete). To do this, we submit a handcrafted
regexp (extracted from about.com) to Twitter and re-
trieve a large primitive set of tweets that is supposed
to have a high recall. This set however, contains a lot
of false positives, tweets that match the regexp but
are not about the rumor (e.g., ?Obama meets muslim
leaders?). Moreover, a rumor is usually stated using
various instances (e.g., ?Barack HUSSEIN Obama?
versus ?Obama is muslim?). Our goal is then to de-
sign a learning framework that filters all such false
positives and retrieves various instances of the same
rumor
Although our second task, belief classification,
can be viewed as an opinion mining task, it is sub-
stantially different from opinion mining in nature.
The difference from a standard opinion mining task
is that here we are looking for attitudes about a sub-
tle statement (e.g., ?Palin is getting divorce?) instead
of the overall sentiment of the text or the opinion
towards an explicit object or person (e.g., ?Sarah
Palin?).
4 Data
As September 2010, Twitter reports that its users
publish nearly 95 million tweets per day1. This
makes Twitter an excellent case to analyze misin-
formation in social media.
Our goal in this work was to collect and annotate
a large dataset that includes all the tweets that are
written about a rumor in a certain period of time. To
collect such a complete and self-contained dataset
about a rumor, we used the Twitter search API, and
retrieved all the tweets that matched a given regular
expression. This API is the only API that returns re-
sults from the entire public Twitter stream and not
a small randomly selected sample. To overcome the
rate limit enforced by Twitter, we collected match-
ing tweets once per hour, and remove any duplicates.
To use the search API, we carefully designed reg-
ular expression queries to be broad enough to match
1http://twitter.com/about
1591
all the tweets that are about a rumor. Each query
represents a popular rumor that is listed as ?false?
or only ?partly true? on About.com?s Urban Leg-
ends reference site2 between 2009 and 2010. Table 1
lists the rumor examples that we used to collect our
dataset alng with their corresponding regular ex-
pression queries and the number of tweets collected.
4.1 Annotation
We asked two annotators to go over all the tweets
in the dataset and mark each tweet with a ?1? if it
is about any of the rumors from Table 1, and with
a ?0? otherwise. This annotation scheme will be
used in our first task to detect false positives, tweets
that match the broad regular expressions and are re-
trieved, but are not about the rumor. For instance,
both of the following tweets match the regular ex-
pression for the palin example, but only the sec-
ond one is rumorous.
(0) ?McCain Divorces Palin over her ?untruths and out
right lies? in the book written for her. McCain?s
team says Palin is a petty liar and phony?
(1) ?Sarah and Todd Palin to divorce, according to local
Alaska paper. http://ow.ly/iNxF?
We also asked the annotators to mark each pre-
viously annotated rumorous tweet with ?11? if the
tweet poster endorses the rumor and with ?12? if the
user refutes the rumor, questions its credibility, or is
neutral.
(12) ?Sarah Palin Divorce Rumor Debunked on Face-
book http://ff.im/62Evd?
(11) ?Todd and Sarah Palin to divorce
http://bit.ly/15StNc?
Our annotation of more than 10,400 tweets shows
that %35 of all the instances that matched the regu-
lar expressions are false positives, tweets that are not
rumor-related but match the initial queries. More-
over, among tweets that are about particular ru-
mors, nearly %43 show the poster believe the rumor,
demonstrating the importance of identifying misin-
formation and those who are misinformed. Table 2
shows the basic statistics extracted from the annota-
tions for each story.
2http://urbanlegends.about.com
Rumor non-rumor (0) believe (11) deny/ (12) total
doubtful/neutral
obama 3,036 926 1,013 4975
airfrance 306 71 128 505
cellphone 132 74 9 215
michelle 83 191 25 299
palin 86 1,709 2,628 4,423
total 3,643 2,971 3,803 10,417
Table 2: Number of instances in each class from the an-
notated data
task ?
rumor retrieval 0.954
belief classification 0.853
Table 3: Inter-judge agreement in two annotation tasks in
terms of ?-statistic
4.2 Inter-Judge Agreement
To calculate the annotation accuracy, we annotated
500 instances twice. These annotations were com-
pared with each other, and the Kappa coefficient (?)
was calculated. The ? statistic is formulated as
? = Pr(a)? Pr(e)1? Pr(e)
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that anno-
tators agree by chance if each annotator is randomly
assigning categories (Krippendorff, 1980; Carletta,
1996). Table 3 shows that annotators can reach
a high agreement in both extracting rumors (? =
0.95) and identifying believers (? = 0.85).
5 Approach
In this section, we describe a general framework,
which given a tweet, predicts (1) whether it is a
rumor-related statement, and if so (2) whether the
user believes the rumor or not. We describe 3 sets of
features, and explain why these are intuitive to use
for identification of rumors.
We process the tweets as they appear in the user
timeline, and do not perform any pre-processing.
Specially, we think that capitalization might be an
important property. So, we do not lower-case the
tweet texts either.
Our approach is based on building different Bayes
classifiers as high level features and then learning
a linear function of these classifiers for retrieval in
the first task and classification in the second. Each
1592
Bayes classifier, which corresponds to a feature fi,
calculates the likelihood ratio for a given tweet t, as
shown in Equation 1.
P (?+i |t)
P (??i |t)
= P (?
+
i )
P (??i )
P (t|?+i )
P (t|??i )
(1)
Here ?+i and ??i are two probabilistic models built
based on feature fi using a set of positive (+) and
negative (?) training data. The likelihood ratio ex-
presses how many times more likely the tweet t is
under the positive model than the negative model
with respect to fi.
For computational reasons and to avoid dealing
with very small numbers we use the log of the like-
lihood ratio to build each classifier.
LLi = log
P (?+i |t)
P (??i |t)
= log P (?
+
i )
P (??i )
+ log P (t|?
+
i )
P (t|??i )
(2)
The first term P (?+i )P (??i ) can be easily calculated us-ing the maximum likelihood estimates of the prob-
abilities (i.e., the estimate of each probability is the
corresponding relative frequency). The second term
is calculated using various features that we explain
below.
5.1 Content-based Features
The first set of features are extracted from the text of
the tweets. We propose 4 content based features. We
follow (Hassan et al, 2010) and present the tweet
with 2 different patterns:
? Lexical patterns: All the words and segments
in the tweet are represented as they appear and
are tokenized using the space character.
? Part-of-speech patterns: All words are replaced
with their part-of-speech tags. To find the part-
of-speech of a hashtag we treat it as a word
(since they could have semantic roles in the
sentence), by omitting the tag sign, and then
precede the tag with the label TAG/. We also
introduce a new tag, URL, for URLs that appear
in a tweet.
From each tweet we extract 4 (2 ? 2) features,
corresponding to unigrams and bigrams of each rep-
resentation. Each feature is the log-likelihood ra-
tio calculated using Equation 2. More formally,
we represent each tweet t, of length n, lexically
as (w1w2 ? ? ?wn) and with part-of-speech tags as
(p1p2 ? ? ? pn). After building the positive and nega-
tive models (?+, ??) for each feature using the train-
ing data, we calculate the likelihood ratio as defined
in Equation 2 where
P (t|?+)
P (t|??) =
n?
j=1
log P (wj |?
+)
P (wj |??)
(3)
for unigram-lexical features (TXT1) and
P (t|?+)
P (t|??) =
n?1?
j=1
log P (wjwj+1|?
+)
P (wjwj+1|??)
(4)
for bigram-based lexical features (TXT2). Simi-
larly, we define the unigram and bigram-based part-
of-speech features (POS1 and POS2) as the log-
likelihood ratio with respect to the positive and neg-
ative part-of-speech models.
5.2 Network-based Features
The features that we have proposed so far are all
based on the content of individual tweets. In the
second set of features we focus on user behavior on
Twitter. We observe 4 types of network-based prop-
erties, and build 2 features that capture them.
Twitter enables users to re-tweet messages from
other people. This interaction is usually easy to de-
tect because the re-tweeted messages generally start
with the specific pattern: ?RT @user?. We use this
property to infer about the re-tweeted message.
Let?s suppose a user ui re-tweets a message t from
the user uj (ui: ?RT @uj t?). Intuitively, t is more
likely to be a rumor if (1) uj has a history of posting
or re-tweeting rumors, or (2) ui has posted or re-
tweeted rumors in the past.
Given a set of training instances, we build a pos-
itive (?+) and a negative (??) user models. The
first model is a probability distribution over all users
that have posted a positive instance or have been re-
tweeted in a positive instance. Similarly, the sec-
ond model is a probability distribution over users
1593
that have posted (or been re-tweeted in) a negative
instance. After building the models, for a given
tweet we calculate two log-likelihood ratios as two
network-based features.
The first feature is the log-likelihood ratio that ui
is under a positive user model (USR1) and the sec-
ond feature is the log-likelihood ratio that the tweet
is re-tweeted from a user (uj) who is under a positive
user model than a negative user model (USR2).
The distinction between the posting user and the
re-tweeted user is important, since some times the
users modify the re-tweeted message in a way that
changes its meaning and intent. In the following ex-
ample, the original user is quoting president Obama.
The second user is re-tweeting the first user, but has
added more content to the tweet and made it sound
rumorous.
original message (non-rumor) ?Obama says he?s do-
ing ?Christ?s work?.?
re-tweeted (rumor) ?Obama says he?s doing ?Christ?s
work.? Oh my God, CHRIST IS A MUSLIM.?
5.3 Twitter Specific Memes
Our final set of features are extracted from memes
that are specific to Twitter: hashtags and URLs.
Previous work has shown the usefulness of these
memes (Ratkiewicz et al, 2010).
5.3.1 Hashtags
One emergent phenomenon in the Twitter ecosys-
tem is the use of hashtags: words or phrases prefixed
with a hash symbol (#). These hashtags are created
by users, and are widely used for a few days, then
disappear when the topic is outdated (Huang et al,
2010).
In our approach, we investigate whether hashtags
used in rumor-related tweets are different from other
tweets. Moreover, we examine whether people who
believe and spread rumors use hashtags that are dif-
ferent from those seen in tweets that deny or ques-
tion a rumor.
Given a set of training tweets of positive and neg-
ative examples, we build two statistical models (?+,
??), each showing the usage probability distribution
of various hashtags. For a given tweet, t, with a set
of m hashtags (#h1 ? ? ?#hm), we calculate the log-
likelihood ratio using Equation 2 where
Feature LL-ratio model
Content
TXT1 content unigram content unigram
TXT2 content bigram content unigram
POS1 content pos content pos unigram
POS2 content pos content pos bigram
Twitter
URL1 content unigram target URL unigram
URL2 content bigram target URL bigram
TAG hashtag hashtag
Network USR1 tweeting user all users in the dataUSR2 re-tweeted user all users in the data
Table 4: List of features used in our optimization frame-
work. Each feature is a log-likelihood ratio calculated
against a a positive (+) and negative (?) training models.
P (t|?+)
P (t|??) =
m?
j=1
log P (#hj |?
+)
P (#hj |??)
(5)
5.3.2 URLs
Previous work has discussed the role of URLs
in information diffusion on Twitter (Honeycutt and
Herring, 2009). Twitter users share URLs in their
tweets to refer to external sources or overcome the
length limit forced by Twitter. Intuitively, if a tweet
is a positive instance, then it is likely to be similar to
the content of URLs shared by other positive tweets.
Using the same reasoning, if a tweet is a negative
instance, then it should be more similar to the web
pages shared by other negative instances.
Given a set of training tweets, we fetch all the
URLs in these tweets and build ?+ and ?? once for
unigrams and once for bigrams. These models are
merely built on the content of the URLs and ignore
the tweet content. Similar to previous features, we
calculate the log-likelihood ratio of the content of
each tweet with respect to ?+ and ?? for unigrams
(URL1) and bigrams URL2).
Table 4 summarizes the set of features used in our
proposed framework, where each feature is a log-
likelihood ratio calculated against a positive (+) and
negative (?) training models. To build these lan-
guage models, we use the CMU Language Modeling
toolkit (Clarkson and Rosenfeld, 1997).
5.4 Optimization
We build an L1-regularized log-linear model (An-
drew and Gao, 2007) on various features discussed
before to predict each tweet. Suppose, a procedure
generates a set of candidates for an input x. Also,
1594
let?s suppose ? : X ? Y ? RD is a function that
maps each (x, y) to a vector of feature values. Here,
the feature vector is the vector of coefficients corre-
sponding to different network, content, and twitter-
based properties, and the parameter vector ? ? RD
(D ? 9 in our experiments) assigns a real-valued
weight to each feature. This estimator chooses ? to
minimize the sum of least squares and a regulariza-
tion term R.
?? = argmin
?
{12
?
i
||??, xi? ? yi||22 +R(?)} (6)
where the regularizer term R(?) is the weighted L1
norm of the parameters.
R(?) = ?
?
j
|?j | (7)
Here, ? is a parameter that controls the amount of
regularization (set to 0.1 in our experiments).
Gao et. al (Gao et al, 2007) argue that op-
timizing L1-regularized objective function is chal-
lenging since its gradient is discontinuous whenever
some parameters equal zero. In this work, we use
the orthant-wise limited-memory quasi-Newton al-
gorithm (OWL-QN), which is a modification of L-
BFGS that allows it to effectively handle the dis-
continuity of the gradient (Andrew and Gao, 2007).
OWL-QN is based on the fact that when restricted
to a single orthant, the L1 regularizer is differen-
tiable, and is in fact a linear function of ?. Thus,
as long as each coordinate of any two consecutive
search points does not pass through zero R(?) does
not contribute at all to the curvature of the function
on the segment joining them. Therefore, we can use
L-BFGS to approximate the Hessian of L(?) alone
and use it to build an approximation to the full reg-
ularized objective that is valid on a given orthant.
This algorithm works quite well in practice, and typ-
ically reaches convergence in even fewer iterations
than standard L-BFGS (Gao et al, 2007).
6 Experiments
We design 2 sets of experiments to evaluate our ap-
proach. In the first experiment we assess the effec-
tiveness of the proposed method when employed in
an Information Retrieval (IR) framework for rumor
retrieval and in the second experiment we employ
various features to detect users? beliefs in rumors.
6.1 Rumor Retrieval
In this experiment, we view different stories as
queries, and build a relevance set for each query.
Each relevance set is an annotation of the entire
10,417 tweets, where each tweet is marked as rel-
evant if it matches the regular expression query and
is marked as a rumor-related tweet by the annotators.
For instance, according to Table 2 the cellphone
dataset has only 83 relevant documents out of the
entire 10,417 documents.
For each query we use 5-fold cross-validation,
and predict the relevance of tweets as a function of
their features. We use these predictions and rank
all the tweets with respect to the query. To evalu-
ate the performance of our ranking model for a sin-
gle query (Q) with the set of relevant documents
{d1, ? ? ? , dm}, we calculate Average Precision as
AP (Q) = 1m
m?
k=1
Precision(Rk) (8)
where Rk is the set of ranked retrieval results from
the top result to the kth relevant document, dk (Man-
ning et al, 2008).
6.1.1 Baselines
We compare our proposed ranking model with a
number of other retrieval models. The first two sim-
ple baselines that indicate a difficulty lower-bound
for the problem are Random and Uniform meth-
ods. In the Random baseline, documents are ranked
based on a random number assignment to them. In
the Uniform model, we use a 5-fold cross validation,
and in each fold the label of the test documents is de-
termined by the majority vote from the training set.
The main baseline that we use in this work, is the
regular expression that was submitted to Twitter to
collect data (regexp). Using the same regular ex-
pression to mark the relevance of the documents will
cause a recall value of 1.00 (since it will retrieve all
the relevant documents), but will also retrieve false
positives, tweets that match the regular expression
but are not rumor-related. We would like to inves-
tigate whether using training data will help us de-
crease the rate of false positives in retrieval.
Finally, using the Lemur Toolkit software3, we
employ a KL divergence retrieval model with
3http://www.lemurproject.org/
1595
Dirichlet smoothing (KL). In this model, documents
are ranked according to the negation of the diver-
gence of query and document language models.
More formally, given the query language model ?Q,
and the document language model ?D, the docu-
ments are ranked by ?D(?Q||?D), where D is the
KL-divergence between the two models.
D(?Q||?D) =
?
w
p(w|?Q) log
p(w|?Q)
p(w|?D)
(9)
To estimate p(w|?D), we use Bayesian smoothing
with Dirichlet priors (Berger, 1985).
ps(w|?D) =
C(w,D) + ?.p(w|?S)
?+
?
w C(w,D)
(10)
where, ? is a parameter, C is the count function, and
thetaS is the collection language model. Higher val-
ues of ? put more emphasis on the collection model.
Here, we try two variants of the model, one using
the default parameter value in Lemur (? = 2000),
and one in which ? is tuned based on the the data
(? = 10). Using the test data to tune the parameter
value, ?, will help us find an upper-bound estimate
of the effectiveness of this method.
Table 5 shows the Mean Average Precision
(MAP) and F?=1 for each method in the rumor re-
trieval task. This table shows that a method that
employs training data to re-rank documents with
respect to rumors makes significant improvements
over the baselines and outperforms other strong re-
trieval systems.
6.1.2 Feature Analysis
To investigate the effectiveness of using indi-
vidual features in retrieving rumors, we perform
5-fold cross validations for each query, using
different feature sets each time. Figure 1 shows
the average precision and recall for our pro-
posed optimization system when content-based
(TXT1+TXT2+POS1+POS2), network-based
(USR1+USR2), and twitter specific memes
(TAG+URL1+URL2) are employed individually.
Figure 1 shows that features that are calculated us-
ing the content language models are very effective in
achieving high precision and recall. Twitter specific
features, especially hashtags, can result in high pre-
cisions but lead to a low recall value because many
Figure 1: Average precision and recall of the proposed
method employing each set of features: content-based,
network-based, and twitter specific.
tweets do not share hashtags or are not written based
on the contents of external URLs.
Finally, we find that user history can be a good
indicator of rumors. However, we believe that this
feature could be more helpful with a complete user
set and a more comprehensive history of their activ-
ities.
6.1.3 Domain Training Data
As our last experiment with rumor retrieval we in-
vestigate how much new labeled data from an emer-
gent rumor is required to effectively retrieve in-
stances of that particular rumor. This experiment
helps us understand how our proposed framework
could be generalized to other stories.
To do this experiment, we use the obama story,
which is a large dataset with a significant number of
false positive instances. We extract 400 randomly
selected tweets from this dataset and keep them for
testing. We also build an initial training dataset of
the other 4 rumors, and label them as not relevant.
We assess the performance of the retrieval model as
we gradually add the rest of the obama tweets. Fig-
ure 2 shows both Average Precision and labeling ac-
curacy versus the size of the labeled data used from
the obama dataset. This plot shows that both mea-
sures exhibit a fast growth and reach 80% when the
number of labeled data reaches 2000.
6.2 Belief Classification
In previous experiments we showed that maximiz-
ing a linear function of log-likelihood ratios is an
effective method in retrieving rumors. Here, we in-
1596
Method MAP 95% C.I. F?=1 95% C.I.
Random 0.129 [-0.065, 0.323] 0.164 [-0.051, 0.379]
Uniform 0.129 [-0.066, 0.324] 0.198 [-0.080, 0.476]
regexp 0.587 [0.305, 0.869] 0.702 [0.479, 0.925]
KL (? = 2000) 0.678 [0.458, 0.898] 0.538 [0.248, 0.828]
KL (? = 10) 0.803 [0.641, 0.965] 0.681 [0.614, 0.748]
LL (all 9 features) 0.965 [0.936, 0.994] 0.897 [0.828, 0.966]
Table 5: Mean Average Precision (MAP) and F?=1 of each method in the rumor retrieval task. (C.I.: Confidence
Interval)
Method Accuracy Precision Recall F?=1 Win/Loss Ratio
random 0.501 0.441 0.513 0.474 1.004
uniform 0.439 0.439 1.000 0.610 0.781
TXT 0.934 0.925 0.924 0.924 14.087
POS 0.742 0.706 0.706 0.706 2.873
content (TXT+POS) 0.941 0.934 0.930 0.932 15.892
network (USR) 0.848 0.873 0.765 0.815 5.583
TAG 0.589 0.734 0.099 0.175 1.434
URL 0.664 0.630 0.570 0.598 1.978
twitter (TAG+URL) 0.683 0.658 0.579 0.616 2.155
all 0.935 0.944 0.906 0.925 14.395
Table 6: Accuracy, precision, recall, F?=1, and win/loss ratio of belief classification using different features.
Figure 2: Average Precision and Accuracy learning curve
for the proposed method employing all 9 features.
vestigate whether this method, and in particular, the
proposed features are useful in detecting users? be-
liefs in a rumor that they post about. Unlike re-
trieval, detecting whether a user endorses a rumor or
refutes it may be possible using similar methods re-
gardless of the rumor. Intuitively, linguistic features
such as negation (e.g., ?obama is not a muslim?), or
capitalization (e.g., ?barack HUSSEIN obama ...?),
user history (e.g., liberal tweeter vs. conservative
tweeter), hashtags (e.g., #tcot vs. #tdot), and URLs
(e.g., links to fake airfrance crash photos) should
help to identify endorsements.
We perform this experiment by making a pool
of all the tweets that are marked as ?rumorous? in
the annotation task. Table 2 shows that there are
6,774 such tweets, from which 2,971 show belief
and 3,803 tweets show that the user is doubtful, de-
nies, or questions it.
Using various feature settings, we perform 5-fold
cross-validation on these 6,774 rumorous tweets.
Table 6 shows the results of this experiment in terms
of F-score, classification accuracy, and win/loss ra-
tio, the ratio of correct classification to an incorrect
1597
classification.
7 Conclusion
In this paper we tackle the fairly unaddressed prob-
lem of identifying misinformation and disinform-
ers in Microblogs. Our contributions in this pa-
per are two-fold: (1) We propose a general frame-
work that employs statistical models and maximizes
a linear function of log-likelihood ratios to retrieve
rumorous tweets that match a more general query.
(2) We show the effectiveness of the proposed fea-
ture in capturing tweets that show user endorsement.
This will help us identify disinformers or users that
spread false information in online social media.
Our work has resulted in a manually annotated
dataset of 10,000 tweets from 5 different controver-
sial topics. To the knowledge of authors this is the
first large-scale publicly available rumor dataset, and
can open many new dimensions in studying the ef-
fects of misinformation or other aspects of informa-
tion diffusion in online social media.
In this paper we effectively retrieve instances of
rumors that are already identified and evaluated by
an external source such as About.com?s Urban Leg-
ends reference. Identifying new emergent rumors
directly from the Twitter data is a more challenging
task. As our future work, we aim to build a sys-
tem that employs our findings in this paper and the
emergent patterns in the re-tweet network topology
to identify whether a new trending topic is a rumor
or not.
8 Acknowledgments
The authors would like to thank Paul Resnick,
Rahul Sami, and Brendan Nyhan for helpful discus-
sions. This work is supported by the National Sci-
ence Foundation grant ?SoCS: Assessing Informa-
tion Credibility Without Authoritative Sources? as
IIS-0968489. Any opinions, findings, and conclu-
sions or recommendations expressed in this paper
are those of the authors and do not necessarily re-
flect the views of the supporters.
References
Floyd H. Allport and Milton Lepkin. 1945. Wartime ru-
mors of waste and special privilege: why some people
believe them. Journal of Abnormal and Social Psy-
chology, 40(1):3 ? 36.
Gordon Allport and Leo Postman. 1947. The psychology
of rumor. Holt, Rinehart, and Winston, New York.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In ICML ?07,
pages 33?40.
James Berger. 1985. Statistical decision theory and
Bayesian Analysis (2nd ed.). New York: Springer-
Verlag.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Bernhard
Pfahringer, Geoff Holmes, and Achim Hoffmann, edi-
tors, Discovery Science, volume 6332 of Lecture Notes
in Computer Science, pages 1?15. Springer Berlin /
Heidelberg.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical
language modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45?148.
Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip, and urban legend. Diogenes, 54:19?35, Febru-
ary.
Nicholas DiFonzo, P. Prashant Bordia, and Ralph L. Ros-
now. 1994. Reining in rumors. Organizational Dy-
namics, 23(1):47?62.
Rob Ennals, Dan Byler, John Mark Agosta, and Barbara
Rosario. 2010. What is disputed on the web? In Pro-
ceedings of the 4th workshop on Information Credibil-
ity, WICOW ?10, pages 67?74.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In ACL ?07.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM),
Boulder, CO, USA.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Edward S Herman and Noam Chomsky. 2002. Manu-
facturing Consent: The Political Economy of the Mass
Media. Pantheon.
Courtenay Honeycutt and Susan C. Herring. 2009. Be-
yond microblogging: Conversation and collaboration
1598
via twitter. Hawaii International Conference on Sys-
tem Sciences, 0:1?10.
Jeff Huang, Katherine M. Thornton, and Efthimis N.
Efthimiadis. 2010. Conversational tagging in twitter.
In Proceedings of the 21st ACM conference on Hyper-
text and hypermedia, HT ?10, pages 173?178.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ?09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497?506.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.
Twitter under crisis: Can we trust what we rt?
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL?04, Morristown,
NJ, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of confer-
ence on Empirical methods in natural language pro-
cessing, EMNLP?02, pages 79?86.
Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno
Gonc?alves, Snehal Patil, Alessandro Flammini, and
Filippo Menczer. 2010. Detecting and tracking
the spread of astroturf memes in microblog streams.
CoRR, abs/1011.3768.
1599
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 616?620,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Measuring Word Relatedness Using Heterogeneous Vector Space Models
Wen-tau Yih
Microsoft Research
One Microsoft Way
Redmond, WA
scottyih@microsoft.com
Vahed Qazvinian?
Department of EECS
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Abstract
Noticing that different information sources of-
ten provide complementary coverage of word
sense and meaning, we propose a simple and
yet effective strategy for measuring lexical se-
mantics. Our model consists of a committee
of vector space models built on a text cor-
pus, Web search results and thesauruses, and
measures the semantic word relatedness us-
ing the averaged cosine similarity scores. De-
spite its simplicity, our system correlates with
human judgements better or similarly com-
pared to existing methods on several bench-
mark datasets, including WordSim353.
1 Introduction
Measuring the semantic relatedness of words is a
fundamental problem in natural language process-
ing and has many useful applications, including
textual entailment, word sense disambiguation, in-
formation retrieval and automatic thesaurus discov-
ery. Existing approaches can be roughly catego-
rized into two kinds: knowledge-based and corpus-
based, where the former includes graph-based algo-
rithms and similarity measures operating on a lexical
database such as WordNet (Budanitsky and Hirst,
2006; Agirre et al, 2009) and the latter consists
of various kinds of vector space models (VSMs)
constructed with the help of a large collection of
text (Reisinger and Mooney, 2010; Radinsky et al,
2011). In this paper, we present a conceptually
simple model for solving this problem. Observing
that various kinds of information sources, such as
?Work conducted while interning at Microsoft Research.
general text corpora, Web search results and the-
sauruses, have different word and sense coverage,
we first build individual vector space models from
each of them separately. Given two words, each
VSM measures the semantic relatedness by the co-
sine similarity of the corresponding vectors in its
space. The final prediction is simply the averaged
cosine scores derived from these VSMs. Despite
its simplicity, our system surprisingly yields very
strong empirical performance. When comparing the
predictions with the human annotations on four dif-
ferent datasets, our system achieves higher correla-
tion than existing methods on two datasets and pro-
vides very competitive results on the others.
The rest of this paper is organized as follows. Sec-
tion 2 briefly reviews the related work. Section 3 de-
tails how we construct each individual vector space
model, followed by the experimental evaluation in
Section 4. Finally, Section 5 concludes the paper.
2 Background
Prior work on measuring lexical semantics can be
categorized as knowledge-based or corpus-based.
Knowledge-based methods leverage word relations
encoded in lexical databases such as WordNet and
provide graph-based similarity measures. Detailed
comparisons of these methods can be found in (Bu-
danitsky and Hirst, 2006). Corpus-based methods
assume related words tend to co-occur or to ap-
pear in similar context. For example, Gabrilovich
and Markovitch (2007) measure word relatedness by
whether they tend to occur in the same Wikipedia
topic. In contrast, Reisinger and Mooney (2010)
use the conventional ?context vector? ? neighboring
616
terms of the occurrences of a target word ? as the
word representation. In addition, they argue that it
is difficult to capture different senses of a word with
a single vector, and introduce a multi-prototype rep-
resentation. More recently, Radinsky et al (2011)
analyze the temporal aspects of words and argue that
non-identical terms in two term vectors should also
be compared based on their temporal usage when
computing the similarity score. They construct the
vectors using Wikipedia titles, Flickr image tags,
and Del.icio.us bookmarks, and extract the temporal
frequency of each concept from 130 years of New
York Times archive. Methods that combine models
from different sources do exist. For instance, Agirre
et al (2009) derive a WordNet-based measure us-
ing PageRank and combined it with several corpus-
based vector space models using SVMs.
3 Vector Space Models from
Heterogeneous Sources
In this section, we describe how we construct vari-
ous vector space models (VSMs) to represent words,
including corpus-based, Web-based and thesaurus-
based methods.
Corpus-based VSMs follow the standard ?distri-
butional hypothesis,? which states that words ap-
pearing in the same contexts tend to have simi-
lar meaning (Harris, 1954). Each target word is
thus represented by a high-dimensional sparse term-
vector that consists of words occurring in its con-
text. Given a corpus, we first collect terms within
a window of [?10,+10] centered at each occur-
rence of a target word. This bag-of-words repre-
sentation is then mapped to the TF-IDF term vector:
each term is weighted by log(freq) ? log(N/df),
where freq is the number of times the term appears
in the collection, df the document frequency of the
term in the whole corpus and N the number of total
documents. We further employed two simple tech-
niques to improve the quality of these term-vectors:
vocabulary and term trimming. Top 1,500 terms
with high document frequency values are treated
as stopwords and removed from the vocabulary.
Moreover, we adopted a document-specific feature
selection method (Kolcz and Yih, 2007) designed
originally for text classification and retain only the
top 200 high-weighted terms for each term-vector1.
The corpus-based VSMs are created using English
Wikipedia (Snapshot of Nov. 2010), consisting of
917M words after preprocessing (markup tags re-
moval and sentence splitting).
Web-based VSMs leverage Web search results to
form a vector of each query (Sahami and Heilman,
2006). For each word to compare, we issue it as a
query and retrieve the set of relevant snippets (top
30 in our experiments) using a popular commercial
search engine, Bing. All these snippets together are
viewed as a pseudo-document and mapped to a TF-
IDF vector as in the corpus-based method. We do
not allow for automatic query expansion in our ex-
periments to ensure that the retrieved snippets are di-
rectly relevant to the target word and not expansions
based on synonyms, hypernyms or hyponyms. We
apply vocabulary trimming (top 1,000 terms with
high DF values), but not term-trimming as the vec-
tors have much fewer terms due to the small number
of snippets collected.
Both the corpus-based and Web-based VSMs rely
on the distributional hypothesis, which is often criti-
cized for two weaknesses. The first is that word pairs
that appear in the same context or co-occur are not
necessarily highly semantically related. For exam-
ple, ?bread? and ?butter? often have cosine scores
higher than synonyms using corpus-based vectors
because of the phrase ?bread and butter?. The sec-
ond is that general corpora often have skewed cov-
erage of words due to the Zipf?s law. Regardless of
the size of the corpus, the number of occurrences
of a rarely used word is typically very low, which
makes the quality of the corresponding vector unre-
liable. To address these two issues, we include the
thesaurus-based VSMs in this work as well. For
each group of similar words (synset) defined in the
thesaurus, we treat it as a ?document? and create a
document?word matrix, where each word is again
weighted using its TF-IDF value. Each column vec-
tor in this matrix is thus the thesaurus-based vec-
tor of the corresponding word. Notice that given
two words and their corresponding vectors, the co-
sine score is more general than simply checking
1In preliminary experiments, we found that active terms
with low TF-IDF values tend to be noise. By aggressively
removing them, the quality of the term-vectors can be signifi-
cantly improved.
617
whether these two words belong to a group of sim-
ilar words, as it judges how often they overlap in
various documents (i.e., sets of similar words). We
explored using two different thesauri in our exper-
iments: WordNet and the Encarta thesaurus devel-
oped by Bloomsbury Publishing, where the former
consists of 227,446 synsets and 190,052 words and
the latter contains 46,945 synsets and 50,184 words.
Compared to existing knowledge-based approaches,
our VSM transformation is very simple and straight-
forward. It is also easy to extend our method to other
languages as only a thesaurus is required rather than
a complete lexical database such as WordNet.
4 Experimental Evaluation
In this section, we evaluate the quality of the VSMs
constructed using methods described in Section 3 on
different benchmark datasets, as well as the perfor-
mance when combining them.
4.1 Benchmark datasets
We follow the standard evaluation method, which di-
rectly tests the correlation of the word relatedness
measures with human judgements on a set of word
pairs, using the Spearman?s rank correlation coeffi-
cient. Our study was conducted using four differ-
ent datasets, including WS-353, RG-65, MC-30 and
MTurk-287.
The WordSim353 dataset (WS-353) is the largest
among them and has been used extensively in re-
cent work. Originally collected by Finkelstein et
al. (2001), the dataset consists of 353 word pairs.
The degree of relatedness of each pair is assessed
on a 0-10 scale by 13-16 human judges, where the
mean is used as the final score. Examining the
relations between the words in each pair, Agirre
et al (2009) further split this dataset into similar
pairs (WS-sim) and related pairs (WS-rel), where
the former contains synonyms, antonyms, identical
words and hyponyms/hypernyms and the latter cap-
ture other word relations. Collected by Rubenstein
and Goodenough (1965), RG-65 contains 65 pairs
of words that are either synonyms or unrelated, as-
sessed on a 0-4 scale by 51 human subjects. Taking
30 pairs from them, Miller and Charles (1991) cre-
ated the (MC-30) dataset by reassessing these word
pairs using 38 subjects. These 30 pairs of words
are also a subset of WS-353. Although these three
datasets contain overlapping word pairs, their scores
are different because of the degree of relatedness
were given by different human subjects. In addition
to these datasets, we also evaluate our VSMs on the
Mturk-287 dataset that consists of 287 word pairs
collected by (Radinsky et al, 2011) using Amazon
MTurk.
4.2 Results and Analysis
Table 1 summarizes the results of various methods,
where the top part lists the performance of state-of-
the-art systems and the bottom shows the results of
individual vector space models, as well as combin-
ing these models using the averaged cosine scores.
We make several observations here. First, while
none of the four VSMs we tested outperforms the
best existing systems on the benchmark datasets,
surprisingly, using the averaged cosine scores of
these models, the performance is improved substan-
tially. It achieves higher Spearman?s rank coeffi-
cient on WS-353 and MTurk-287 than any other sys-
tems2 and are close to the state-of-the-art on MC-
30 and RG-65. Unlike some approach like (Hughes
and Ramage, 2007), which performs well on some
datasets but poorly on others, combing the VSMs
from heterogeneous sources is more robust. Individ-
ually, we notice that Wikipedia context VSM pro-
vides consistently strong results, while thesaurus-
based models work only reasonable on MC-30 and
RG-65, potentially because other datasets contain
more out-of-vocabulary words or proper nouns. Due
to the inherent ambiguity of the task, there is a high
variance among judgements from different annota-
tors. Therefore, it is unrealistic to assume any of the
methods can correlate perfectly to the mean human
judgement scores. In fact, the inter-agreement study
done on the WS-353 dataset indicates that the result
of our approach of combining heterogeneous VSMs
is close to the averaged human performance.
It is intriguing to see that by using the averaged
cosine scores, the performance can be improved
over the best individual model (i.e., Wikipedia). Ex-
amining the scores of some word pairs carefully sug-
2This may not be statistically significant. Without having
the exact output of existing systems, it is difficult to conduct a
robust statistical significance test given the small sizes of these
datasets.
618
Spearman?s ?
Method WS-353 WS-sim WS-rel MC-30 RG-65 MTurk-287
(Radinsky et al, 2011) 0.80 - - - - 0.63
(Reisinger and Mooney, 2010) 0.77 - - - - -
(Agirre et al, 2009) 0.78 0.83 0.72 0.92 0.96 -
(Gabrilovich and Markovitch, 2007) 0.75 - - - - 0.59
(Hughes and Ramage, 2007) 0.55 - - 0.90 0.84 -
Web Search 0.56 0.56 0.54 0.48 0.44 0.44
Wikipedia 0.73 0.80 0.73 0.87 0.83 0.62
Bloomsbury 0.45 0.60 0.60 0.71 0.78 0.29
WordNet 0.37 0.49 0.49 0.79 0.78 0.25
Combining VSMs 0.81 0.87 0.77 0.89 0.89 0.68
Table 1: The performance of the state-of-the-art methods and different vector space models on measuring semantic
word relatedness using the cosine similarity.
gests the broader coverage of different words and
senses could be the reason. For example, some
of the words in the datasets have multiple senses,
such as ?jaguar vs. car? and ?jaguar vs. cat?. Al-
though in previous work, researchers try to capture
word senses using different vectors (Reisinger and
Mooney, 2010) from the same text corpus, this is in
fact difficult in practice. The usage of words in a big
text corpus, which contains diversified topics, may
still be biased to one word sense. For example, in
the Wikipeida term vector that represents ?jaguar?,
we found that most of the terms there are related to
?cat?. Although some terms are associated with the
?car? meaning, the signals are rather weak. Simi-
larly, WordNet does not indicate ?jaguar? could be
related to ?car? at all. In contrast, the ?car? sense
of ?jaguar? dominates the vector created using the
search engine. As a result, incorporating models
from different sources could be more effective than
relying on word sense discovering algorithms op-
erating solely on one corpus. Another similar but
different example is the pair of ?bread? and ?but-
ter?, which are treated as synonyms by corpus-based
VSMs, but is demoted after adding the thesaurus-
based models.
5 Conclusion
In this paper we investigated the usefulness of het-
erogeneous information sources in improving mea-
sures of semantic word relatedness. Particularly, we
created vector space models using 4 data sources
from 3 categories (corpus-based, Web-based and
thesaurus-based) and found that simply averaging
the cosine similarity derived from these models
yields a very robust measure. Other than directly ap-
plying it to measuring semantic relatedness, our ap-
proach is complementary to more sophisticated sim-
ilarity measures such as developing kernel functions
for different structured data (Croce et al, 2011),
where the similarity between words serves as a basic
component.
While this result is interesting and encouraging, it
also raises several research questions, such as how
to enhance the quality of each vector space model
and whether the models can be combined more ef-
fectively3. We also would like to study whether sim-
ilar techniques can be useful when comparing longer
text segments like phrases or sentences, with poten-
tial applications in paraphrase detection and recog-
nizing textual entailment.
Acknowledgments
We thank Joseph Reisinger for providing his pro-
totype vectors for our initial study, Silviu-Petru
Cucerzan for helping process the Wikipedia files and
Geoffrey Zweig for preparing the Bloomsbury the-
saurus data. We are also grateful to Chris Meek
for valuable discussions and to anonymous review-
ers for their comments.
3We conducted some preliminary experiments (not reported
here) on tuning the weights of combining different models
based on cross-validation, but did not find consistent improve-
ments, perhaps due to the limited size of the data.
619
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and wordnet-based ap-
proaches. In NAACL ?09, pages 19?27.
A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32:13?47, March.
D. Croce, A. Moschitti, and R. Basili. 2011. Structured
lexical similarity via convolution kernels on depen-
dency trees. In Proceedings of EMNLP 2011, pages
1034?1046, July.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing
search in context: The concept revisited. In WWW,
pages 406?414. ACM.
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using wikipedia-based explicit se-
mantic analysis. In IJCAI ?07, pages 1606?1611.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
A. Kolcz and W. Yih. 2007. Raising the baseline for
high-precision text classifiers. In KDD ?07, pages
400?409.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. Language and cognitive pro-
cesses, 6(1):1?28.
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis. In
WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In NAACL ?10.
H. Rubenstein and J. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8:627?633, October.
M. Sahami and T. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international con-
ference on World Wide Web, pages 377?386. ACM.
620
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555?564,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Identifying Non-explicit Citing Sentences for Citation-based
Summarization
Vahed Qazvinian
Department of EECS
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
Identifying background (context) informa-
tion in scientific articles can help schol-
ars understand major contributions in their
research area more easily. In this paper,
we propose a general framework based
on probabilistic inference to extract such
context information from scientific papers.
We model the sentences in an article and
their lexical similarities as a Markov Ran-
dom Field tuned to detect the patterns that
context data create, and employ a Belief
Propagation mechanism to detect likely
context sentences. We also address the
problem of generating surveys of scien-
tific papers. Our experiments show greater
pyramid scores for surveys generated us-
ing such context information rather than
citation sentences alone.
1 Introduction
In scientific literature, scholars use citations to re-
fer to external sources. These secondary sources
are essential in comprehending the new research.
Previous work has shown the importance of cita-
tions in scientific domains and indicated that ci-
tations include survey-worthy information (Sid-
dharthan and Teufel, 2007; Elkiss et al, 2008;
Qazvinian and Radev, 2008; Mohammad et al,
2009; Mei and Zhai, 2008).
A citation to a paper in a scientific article may
contain explicit information about the cited re-
search. The following example is an excerpt from
a CoNLL paper1 that contains information about
Eisner?s work on bottom-up parsers and the notion
of span in parsing:
?Another use of bottom-up is due to Eisner
(1996), who introduced the notion of a span.?
1Buchholz and Marsi ?CoNLL-X Shared Task On Multi-
lingual Dependency Parsing?, CoNLL 2006
However, the citation to a paper may not always
include explicit information about the cited paper:
?This approach is one of those described in Eis-
ner (1996)?
Although this sentence alone does not provide any
information about the cited paper, it suggests that
its surrounding sentences describe the proposed
approach in Eisner?s paper:
?... In an all pairs approach, every possible
pair of two tokens in a sentence is considered
and some score is assigned to the possibility of
this pair having a (directed) dependency rela-
tion. Using that information as building blocks,
the parser then searches for the best parse for
the sentence. This approach is one of those de-
scribed in Eisner (1996).?
We refer to such implicit citations that contain
information about a specific secondary source but
do not explicitly cite it, as sentences with con-
text information or context sentences for short.
We look at the patterns that such sentences cre-
ate and observe that context sentences occur with-
ing a small neighborhood of explicit citations. We
also discuss the problem of extracting context sen-
tences for a source-reference article pair. We pro-
pose a general framework that looks at each sen-
tence as a random variable whose value deter-
mines its state about the target paper. In summary,
our proposed model is based on the probabilistic
inference of these random variables using graphi-
cal models. Finally we give evidence on how such
sentences can help us produce better surveys of re-
search areas. The rest of this paper is organized as
follows. Preceded by a review of prior work in
Section 2, we explain the data collection and our
annotation process in Section 3. Section 4 explains
our methodology and is followed by experimental
setup in Section 5.
555
#Refs
ACL-ID Author Title Year all AAN # Sents
P08-2026 McClosky & Charniak Self-Training for Biomedical Parsing 2008 12 8 102
N07-1025? Mihalcea Using Wikipedia for Automatic ... 2007 21 12 153
N07-3002 Wang Learning Structured Classifiers ... 2007 22 14 74
P06-1101 Snow et, al. Semantic Taxonomy Induction ... 2006 19 9 138
P06-1116 Abdalla & Teufel A Bootstrapping Approach To ... 2006 24 10 231
W06-2933 Nivre et, al. Labeled Pseudo-Projective Dependency ... 2006 27 5 84
P05-1044 Smith & Eisner Contrastive Estimation: Training Log-Linear ... 2005 30 13 262
P05-1073 Toutanova et, al. Joint Learning Improves Semantic Role Labeling 2005 14 10 185
N03-1003 Barzilay & Lee Learning To Paraphrase: An Unsupervised ... 2003 26 13 203
N03-2016? Kondrak et, al. Cognates Can Improve Statistical Translation ... 2003 8 5 92
Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publi-
cation year, number of references (in AAN) and number of sentences. Papers marked with ? are used to
calculate inter-judge agreement.
2 Prior Work
Analyzing the structure of scientific articles and
their relations has received a lot of attention re-
cently. The structure of citation and collaboration
networks has been studied in (Teufel et al, 2006;
Newman, 2001), and summarization of scientific
documents is discussed in (Teufel and Moens,
2002). In addition, there is some previous work
on the importance of citation sentences. Elkiss et
al, (Elkiss et al, 2008) perform a large-scale study
on citations in the free PubMed Central (PMC)
and show that they contain information that may
not be present in abstracts. In other work, Nanba
et al (Nanba and Okumura, 1999; Nanba et al,
2004b; Nanba et al, 2004a) analyze citation sen-
tences and automatically categorize them in order
to build a tool for survey generation.
The text of scientific citations has been used in
previous research. Bradshaw (Bradshaw, 2002;
Bradshaw, 2003) uses citations to determine the
content of articles. Similarly, the text of cita-
tion sentences has been directly used to produce
summaries of scientific papers in (Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Mohammad
et al, 2009). Determining the scientific attribu-
tion of an article has also been studied before.
Siddharthan and Teufel (Siddharthan and Teufel,
2007; Teufel, 2005) categorize sentences accord-
ing to their role in the author?s argument into pre-
defined classes: Own, Other, Background, Tex-
tual, Aim, Basis, Contrast.
Little work has been done on automatic cita-
tion extraction from research papers. Kaplan et
al, (Kaplan et al, 2009) introduces ?citation-site?
as a block of text in which the cited text is dis-
cussed. The mentioned work uses a machine
learning method for extracting citations from re-
search papers and evaluates the result using 4 an-
notated articles.
In our work we use graphical models to ex-
tract context sentences. Graphical models have
a number of properties and corresponding tech-
niques and have been used before on Information
Retrieval tasks. Romanello et al (Romanello et
al., 2009) use Conditional Random Fields (CRF)
to extract references from unstructured text in dig-
ital libraries of classic texts. Similar work include
term dependency extraction (Metzler and Croft,
2005), query expansion (Metzler and Croft, 2007),
and automatic feature selection (Metzler, 2007).
3 Data
The ACL Anthology Network (AAN)2 is a col-
lection of papers from the ACL Anthology3 pub-
lished in the Computational Linguistics journal
and proceedings from ACL conferences and work-
shops and includes more than 14, 000 papers over
a period of four decades (Radev et al, 2009).
AAN includes the citation network of the papers
in the ACL Anthology. The papers in AAN are
publicly available in text format retrieved by an
OCR process from the original pdf files, and are
segmented into sentences.
To build a corpus for our experiments we picked
10 recently published papers from various areas
in NLP4, each of which had references for a to-
tal of 203 candidate paper-reference pairs. Table 1
lists these papers together with their authors, titles,
publication year, number of references, number of
references within AAN, and the number of sen-
2http://clair.si.umich.edu/clair/anthology/
3http://www.aclweb.org/anthology-new/
4Regardless of data selection, the methodology in this
work is applicable to any of the papers in AAN.
556
L&PS&al Sentence
? ? ?
C C Jacquemin (1999) and Barzilay and McKeown (2001) identify
phrase level paraphrases, while Lin and Pantel (2001) and
Shinyama et al (2002) acquire structural paraphrases encoded
as templates.
1 1 These latter are the most closely related to the sentence-level para-
phrases we desire, and so we focus in this section on template-
induction approaches.
C 0 Lin and Pantel (2001) extract inference rules, which are related
to paraphrases (for example, X wrote Y implies X is the author of
Y), to improve question answering.
1 0 They assume that paths in dependency trees that take similar argu-
ments (leaves) are close in meaning.
1 0 However, only two-argument templates are considered.
0 C Shinyama et al (2002) also use dependency-tree information to
extract templates of a limited form (in their case, determined by
the underlying information extraction application).
1 1 Like us (and unlike Lin and Pantel, who employ a single large
corpus), they use articles written about the same event in different
newspapers as data.
1 1 Our approach shares two characteristics with the two methods just
described: pattern comparison by analysis of the patterns respec-
tive arguments, and use of nonparallel corpora as a data source.
0 0 However, extraction methods are not easily extended to generation
methods.
1 1 One problem is that their templates often only match small frag-
ments of a sentence.
1 1 While this is appropriate for other applications, deciding whether
to use a given template to generate a paraphrase requires informa-
tion about the surrounding context provided by the entire sentence.
? ? ?
Table 2: Part of the annotation for N03-1003 with
respect to two of its references ?Lin and Pan-
tel (2001)? (the first column) ?Shinyama et al
(2002)? (the second column). Cs indicate explicit
citations, 1s indicate implicit citations and 0s are
none.
tences.
3.1 Annotation Process
We annotated the sentences in each paper from Ta-
ble 1. Each annotation instance in our setting cor-
responds to a paper-reference pair, and is a vec-
tor in which each dimension corresponds to a sen-
tence and is marked with a C if it explicitly cites
the reference, and with a 1 if it implicitly talks
about it. All other sentences are marked with 0s.
Table 2 shows a portion of two separate annota-
tion instances of N03-1003 corresponding to two
of its references. Our annotation has resulted in
203 annotation instances each corresponding to
one paper-reference pair. The goal of this work
is to automatically identify all context sentences,
which are marked as ?1?.
3.1.1 Inter-judge Agreement
We also asked a neutral annotator5 to annotate
two of our datasets that are marked with ? in Ta-
ble 1. For each paper-reference pair, the annotator
was provided with a vector in which explicit cita-
5Someone not involved with the paper but an expert in
NLP.
ACL-ID vector size # Annotations ?
N07-1025? 153 21 0.889 ? 0.30
N03-2016? 92 8 0.853 ? 0.35
Table 3: Average ? coefficient as inter-judge
agreement for annotations of two sets
tions were already marked with Cs. The annota-
tion guidelines instructed the annotator to look at
each explicit citation sentence, and read up to 15
sentences before and after, then mark context sen-
tences around that sentence with 1s. Next, the 29
annotation instances done by the external annota-
tor were compared with the corresponding anno-
tations that we did, and the Kappa coefficient (?)
was calculated. The ? statistic is formulated as
? =
Pr(a)? Pr(e)
1? Pr(e)
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that an-
notators agree by chance if each annotator is ran-
domly assigning categories. To calculate ?, we ig-
nored all explicit citations (since they were pro-
vided to the external annotator) and used the bi-
nary categories (i.e., 1 for context sentences, and
0 otherwise) for all other sentences. Table 3 shows
the annotation vector size (i.e., number of sen-
tences), number of annotation instances (i.e., num-
ber of references), and average ? for each set. The
average ? is above 0.85 in both cases, suggest-
ing that the annotation process has a low degree
of subjectivity and can be considered reliable.
3.2 Analysis
In this section we describe our analysis. First,
we look at the number of explicit citations each
reference has received in a paper. Figure 1 (a)
shows the histogram corresponding to this distri-
bution. It indicates that the majority of references
get cited in only 1 sentence in a scientific arti-
cle, while the maximum being 9 in our collected
dataset with only 1 instance (i.e., there is only 1
reference that gets cited 9 times in a paper). More-
over, the data exhibits a highly positive-skewed
distribution. This is illustrated on a log-log scale
in Figure 1 (b). This highly skewed distribution
indicates that the majority of references get cited
only once in a citing paper. The very small number
of citing sentences can not make a full inventory of
the contributions of the cited paper, and therefore,
extracting explicit citations alone without context
557
gap size 0 1 2 4 9 10 15 16
instance 273 14 2 1 2 1 1 1
Table 4: The distribution of gaps in the annotated
data
sentences may result in information loss about the
contributions of the cited paper.
1 2 3 4 5 6 7 8 9
0
20
40
60
80
100
120
140
cit
100 101
10?3
10?2
10?1
100
cit
p(cit)
 
 
alpha = 3.13; D=0.02
a b
Figure 1: (a) Histogram of the number of differ-
ent citations to each reference in a paper. (b) The
distribution observed for the number of different
citations on a log-log scale.
Next, we investigate the distance between con-
text sentences and the closest citations. For each
context sentence, we find its distance to the clos-
ets context sentence or explicit citation. Formally,
we define the gap to be the number of sentences
between a context sentence (marked with 1) and
the closest context sentence or explicit citation
(marked with either C or 1) to it. For example,
the second column of Table 2 shows that there is a
gap of size 1 in the 9th sentence in the set of con-
text and citation sentences about Shinyama et al
(2002). Table 4 shows the distribution of gap sizes
in the annotated data. This observation suggests
that the majority of context sentences directly oc-
cur after or before a citation or another context
sentence. However, it shows that gaps between
sentences describing a cited paper actually exist,
and a proposed method should have the capability
to capture them.
4 Proposed Method
In this section we propose our methodology that
enables us to identify the context information of a
cited paper. Particularly, the task is to assign a bi-
nary label XC to each sentence Si from a paper S,
where XC = 1 shows a context sentence related
to a given cited paper, C. To solve this problem
we propose a systematic way to model the net-
work level relationship between consecutive sen-
tences. In summary, each sentence is represented
with a node and is given two scores (context, non-
context), and we update these scores to be in har-
mony with the neighbors? scores.
A particular class of graphical models known
as Markov Random Fields (MRFs) are suited for
solving inference problems with uncertainty in ob-
served data. The data is modeled as an undirected
graph with two types of nodes: hidden and ob-
served. Observed nodes represent values that are
known from the data. Each hidden node xu, cor-
responding to an observed node yu, represents the
true state underlying the observed value. The state
of a hidden node is related to the value of its cor-
responding observed node as well as the states of
its neighboring hidden nodes.
The local Markov property of an MRF indi-
cates that a variable is conditionally independent
on all other variables given its neighbors: xv ?
? xV \cl(v)|xne(v), where ne(v) is the set of neigh-
bors of v, and cl(v) = {v} ? ne(v) is the closed
neighborhood of v. Thus, the state of a node is as-
sumed to statistically depend only upon its hidden
node and each of its neighbors, and independent
of any other node in the graph given its neighbors.
Dependencies in an MRF are represented using
two functions: Compatibility function (?) and Po-
tential function (?). ?uv(xc, xd) shows the edge
potential of an edge between two nodes u, v of
classes xc and xd. Large values of ?uv would
indicate a strong association between xc and xd
at nodes u, v. The Potential function, ?i(xc, yc),
shows the statistical dependency between xc and
yc at each node i assumed by the MRF model.
In order to find the marginal probabilities of
xis in a MRF we can use Belief Propagation
(BP) (Yedidia et al, 2003). If we assume the yis
are fixed and show ?i(xi, yi) by ?i(xi), we can
find the joint probability distribution for unknown
variables xi as
p({x}) = 1
Z
?
ij
?ij(xi, xj)
?
i
?i(xi)
In the BP algorithm a set of new variables m is
introduced where mij(xj) is the message passed
from i to j about what state xj should be in. Each
message, mij(xj), is a vector with the same di-
mensionality of xj in which each dimension shows
i?s opinion about j being in the corresponding
class. Therefore each message could be consid-
ered as a probability distribution and its compo-
nents should sum up to 1. The final belief at a
558
Figure 2: The illustration of the message updating
rule. Elements that make up the message from a
node i to another node j: messages from i?s neigh-
bors, local evidence at i, and propagation function
between i, j summed over all possible states of
node i.
node i, in the BP algorithm, is also a vector with
the same dimensionality of messages, and is pro-
portional to the local evidence as well as all mes-
sages from the node?s neighbors:
bi(xi)? k?i(xi)
?
j?ne(i)
mji(xi) (1)
where k is the normalization factor of the be-
liefs about different classes. The message passed
from i to j is proportional to the propagation func-
tion between i, j, the local evidence at i, and all
messages sent to i from its neighbors except j:
mij(xj)?
?
xi
?i(xi)?ij(xi, xj)
?
k?ne(i)\j
mki(xi) (2)
Figure 2 illustrates the message update rule.
Convergence can be determined based on a va-
riety of criteria. It can occur when the maximum
change of any message between iteration steps is
less than some threshold. Convergence is guaran-
teed for trees but not for general graphs. However,
it typically occurs in practice (McGlohon et al,
2009). Upon convergence, belief scores are deter-
mined by Equation 1.
4.1 MRF construction
To find the sentences from a paper that form the
context information of a given cited paper, we
build an MRF in which a hidden node xi and
an observed node yi correspond to each sentence.
The structure of the graph associated with the
MRF is dependent upon the validity of a basic as-
sumption. This assumption indicates that the gen-
eration of a sentence (in form of its words) only
(a) (b)
Figure 3: The structure of the MRF constructed
based on the independence of non-adjacent sen-
tences; (a) left, each sentence is independent on
all other sentences given its immediate neighbors.
(b) right, sentences have dependency relationship
with each other regardless of their position.
depends on its surrounding sentences. Said dif-
ferently, each sentence is written independently of
all other sentences given a number of its neigh-
bors. This local dependence assumption can result
in a number of different MRFs, each built assum-
ing a dependency between a sentence and all sen-
tences within a particular distance. Figure 3 shows
the structure of the two MRFs at either extreme of
the local dependence assumption. In Figure 3 a,
each sentence only depends on one following and
one preceding sentence, while Figure 3 b shows
an MRF in which sentences are dependent on each
other regardless of their position. We refer to the
former by BP1, and to the latter by BPn. Gen-
erally, we use BPi to denote an MRF in which
each sentence is connected to i sentences before
and after.
?ij(xc, xd) xd = 0 xd = 1
xc = 0 0.5 0.5
xc = 1 1? Sij Sij
Table 5: The compatibility function ? between
any two nodes in the MRFs from the sentences in
scientific papers
4.2 Compatibility Function
The compatibility function of an MRF represents
the association between the hidden node classes.
A node?s belief to be in class 1 is its probability to
be included in the context. The belief of a node i,
about its neighbor j to be in either classes is as-
sumed to be 0.5 if i is in class 0. In other words, if
a node is not part of the context itself, we assume
559
it has no effect on its neighbors? classes. In con-
trast, if i is in class 1 its belief about its neighbor
j is determined by their mutual lexical similarity.
If this similarity is close to 1 it indicates a stronger
tie between i, j. However, if i, j are not similar,
i?s probability of being in class 1, should not af-
fect that of j?s. To formalize this assumption we
use the sigmoid of the cosine similarity of two sen-
tences to build ?. More formally, we define S to
be
Sij =
1
1 + e?cosine(i,j)
The sigmoid function obtains a value of 0.5 for
a cosine of 0 indicating that there is no bias in the
association of the two sentences. The matrix in Ta-
ble 5 shows the compatibility function built based
on the above arguments.
4.3 Potential Function
The node potential function of an MRF can incor-
porate some other features observable from data.
Here, the goal is to find all sentences that are about
a specific cited paper, without having explicit cita-
tions. To build the node potential function of the
observed nodes, we use some sentence level fea-
tures. First, we use the explicit citation as an im-
portant feature of a sentence. This feature can af-
fect the belief of the corresponding hidden node,
which can in turn affect its neighbors? beliefs. For
a given paper-reference pair, we flag (with a 1)
each sentence that has an explicit citation to the
reference.
The second set of features that we are inter-
ested in are discourse-based features. In particu-
lar we match each sentence with specific patterns
and flag those that match. The first pattern is a bi-
gram in which the first term matches any of ?this;
that; those; these; his; her; their; such; previ-
ous?, and the second term matches any of ?work;
approach; system; method; technique; result; ex-
ample?. The second pattern includes all sentences
that start with ?this; such?.
Finally, the similarity of each sentence to the
reference is observable from the data and can be
used as a sentence-level feature. Intuitively, if a
sentence has higher similarity with the reference
paper, it should have a higher potential of being
in class 1 or C. The flag of each sentence here is
a value between 0 and 1 and is determined by its
cosine similarity to the reference. Once the flags
for each sentence, Si are determined, we calculate
normalized fi as the unweighted linear combina-
tion of individual features. Based on fis, we com-
pute the potential function, ?, as shown in Table 6.
?i(xc, yc) xc = 0 xc = 1
1? fi fi
Table 6: The node potential function ? for each
node in the MRFs from the sentences in scientific
papers is built using the sentences? flags computed
using sentence level features.
5 Experiments
The intrinsic evaluation of our methodology
means to directly compare the output of our
method with the gold standards obtained from the
annotated data. Our methodology finds the sen-
tences that cite a reference implicitly. Therefore
the output of the inference method is a vector, ?,
of 1?s and 0?s, whereby a 1 at element i means
that sentence i in the source document is a con-
text sentence about the reference while a 0 means
an explicit citation or neither. The gold standard
for each paper-reference pair, ? (obtained from the
annotated vectors in Section 3.1 by changing all
Cs to 0s), is also a vector of the same format and
dimensionality.
Precision, recall, and F? for this task can be de-
fined as
p = ? ? ?
? ? 1 ; r =
? ? ?
? ? 1 ; F? =
(1 + ?2)p ? r
?2p + r (3)
where 1 is a vector of 1?s with the same dimen-
sionality and ? is a non-negative real number.
5.1 Baseline Methods
The first baseline that we use is an IR-based
method. This baseline, B1, takes explicit citations
as an input but use them to find context sentences.
Given a paper-reference pair, for each explicit ci-
tation sentence, marked with C, B1 picks its pre-
ceding and following sentences if their similarities
to that sentence is greater than a cutoff (the median
of all such similarities), and repeats this for neigh-
boring sentences of newly marked sentences. In-
tuitively, B1 tries to find the best chain (window)
around citing sentences.
As the second baseline, we use the hand-crafted
discourse based features used in MRF?s potential
function. Particularly, this baseline, B2, marks
560
paper B1 B2 SVM BP1 BP4 BPn
P08-2026 0.441 0.237 0.249 0.470 0.613 0.285
N07-1025 0.388 0.102 0.124 0.313 0.466 0.138
N07-3002 0.521 0.339 0.232 0.742 0.627 0.315
P06-1101 0.125 0.388 0.127 0.649 0.889 0.193
P06-1116 0.283 0.104 0.100 0.307 0.341 0.130
W06-2933 0.313 0.100 0.176 0.338 0.413 0.160
P05-1044 0.225 0.100 0.060 0.172 0.586 0.094
P05-1073 0.144 0.100 0.144 0.433 0.518 0.171
N03-1003 0.245 0.249 0.126 0.523 0.466 0.125
N03-2016 0.100 0.181 0.224 0.439 0.482 0.185
Table 7: Average F?=3 for similarity based baseline (B1), discourse-based baseline (B2), a supervised
method (SVM) and three MRF-based methods.
each sentence that is within a particular distance
(4 in our experiments) of an explicit citation and
matches one of the two patterns mentioned in Sec-
tion 4.3. After marking all such sentences, B2
also marks all sentences between them and the
closest explicit citation, which is no farther than
4 sentences away. This baseline helps us under-
stand how effectively this sentence level feature
can work in the absence of other features and the
network structure.
Finally, we use a supervised method, SVM,
to classify sentences as context/non-context. We
use 4 features to train the SVM model. These
4 features comprise the 3 sentence level features
used in MRF?s potential function (i.e., similar-
ity to reference, explicit citation, matching certain
regular-expressions) and a network level feature:
distance to the closes explicit citation. For each
source paper, P , we use all other source papers
and their source-reference annotation instances to
train a model. We then use this model to clas-
sify all instances in P . Although the number of
references and thus source-reference pairs are dif-
ferent for different papers, this can be considered
similar to a 10-fold cross validation scheme, since
for each source paper the model is built using all
source-reference pairs of all other 9 papers.
We compare these baselines with 3 MRF-based
systems each with a different assumption about in-
dependence of sentences. BP1 denotes an MRF
in which each sentence is only connected to 1 sen-
tence before and after. In BP4 locality is more
relaxed and each sentence is connected to 4 sen-
tences on each sides. BPn denotes an MRF in
which all sentences are connected to each other
regardless of their position in the paper.
Table 7 shows F?=3 for our experiments and
shows how BP4 outperforms the other methods
on average. The value 4 may suggest the fact that
although sentences might be independent of dis-
tant sentences, they depend on more than one sen-
tence on each side.
The final experiment we do to intrinsically eval-
uate the MRF-base method is to compare differ-
ent sentence-level features. The first feature used
to build the potential function is explicit citations.
This feature does not directly affect context sen-
tences (i.e., it affects the marginal probability of
context sentences through the MRF network con-
nections). Therefore, we do not alter this fea-
ture in comparing different features. However, we
look at the effect of the second and the third fea-
tures: hand-crafted regular expression-based fea-
tures and similarity to the reference. For each pa-
per, we use BP4 to perform 3 experiments: two in
absence of each feature and one including all fea-
tures. Figure 4 shows the average F?=3 for each
experiment. This plot shows that the features lead
to better results when used together.
6 Impact on Survey Generation
We also performed an extrinsic evaluation of
our context extraction methodology. Here we
show how context sentences add important survey-
worthy information to explicit citations. Previous
work that generate surveys of scientific topics use
the text of citation sentences alone (Mohammad
et al, 2009; Qazvinian and Radev, 2008). Here,
we show how the surveys generated using citations
and their context sentences are better than those
generated using citation sentences alone.
We use the data from (Mohammad et al, 2009)
561
... Naturally, our current work on question answering for the reading comprehension task is most related to those of
(Hirschman et al , 1999; Charniak et al , 2000; Riloffand Thelen, 2000 ; Wang et al , 2000). In fact, all of this
body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the
same development set of stories. The work of (Hirschman et al , 1999) initiated this series of work, and it reported
an accuracy of 36.3% on answering the questions in the test stories. Subsequently, the work of (Riloffand Thelen ,
2000) and (Chaxniak et al , 2000) improved the accuracy further to 39.7% and 41%, respectively. However, all
of these three systems used handcrafted, deterministic rules and algorithms...
...The cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM
> S-SVM > Retrieval-M. Compared with retrieval-based [Yang et al 2003], pattern-based [Ravichandran et al 2002
and Soubbotin et al 2002], and deep NLP-based [Moldovan et al 2002, Hovy et al 2001; and Pasca et al 2001]
answer selection, machine learning techniques are more effective in constructing QA components from scratch. These
techniques suffer, however, from the problem of requiring an adequate number of handtagged question-answer
training pairs. It is too expensive and labor intensive to collect such training pairs for supervised machine
learning techniques ...
... As expected, the definition and person-bio answer types are covered well by these resources. The web has
been employed for pattern acquisition (Ravichandran et al , 2003), document retrieval (Dumais et al , 2002), query
expansion (Yang et al , 2003), structured information extraction, and answer validation (Magnini et al , 2002). Some
of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a
less complex approach to find correct answers ...
Table 8: A portion of the QA survey generated by LexRank using the context information.
Figure 4: Average F?=3 for BP4 employing dif-
ferent features.
that contains two sets of cited papers and corre-
sponding citing sentences, one on Question An-
swering (QA) with 10 papers and the other on De-
pendency Parsing (DP) with 16 papers. The QA
set contains two different sets of nuggets extracted
by experts respectively from paper abstracts and
citation sentences. The DP set includes nuggets
extracted only from citation sentences. We use
these nugget sets, which are provided in form of
regular expressions, to evaluate automatically gen-
erated summaries. To perform this experiment we
needed to build a new corpus that includes con-
text sentences. For each citation sentence, BP4 is
used on the citing paper to extract the proper con-
text. Here, we limit the context size to be 4 on
each side. That is, we attach to a citing sentence
any of its 4 preceding and following sentences if
citation survey context survey
QA
CT nuggets 0.416 0.634
AB nuggets 0.397 0.594
DP
CT nuggets 0.324 0.379
Table 9: Pyramid F?=3 scores of automatic
surveys of QA and DP data. The QA surveys
are evaluated using nuggets drawn from citation
texts (CT), or abstracts (AB), and DP surveys are
evaluated using nuggets from citation texts (CT).
BP4 marks them as context sentences. Therefore,
we build a new corpus in which each explicit ci-
tation sentence is replaced with the same sentence
attached to at most 4 sentence on each side.
After building the context corpus, we use
LexRank (Erkan and Radev, 2004) to generate 2
QA and 2 DP surveys using the citation sentences
only, and the new context corpus explained above.
LexRank is a multidocument summarization sys-
tem, which first builds a cosine similarity graph of
all the candidate sentences. Once the network is
built, the system finds the most central sentences
by performing a random walk on the graph. We
limit these surveys to be of a maximum length of
1000 words. Table 8 shows a portion of the sur-
vey generated from the QA context corpus. This
example shows how context sentences add mean-
ingful and survey-worthy information along with
citation sentences. Table 9 shows the Pyramid
F?=3 score of automatic surveys of QA and DP
562
data. The QA surveys are evaluated using nuggets
drawn from citation texts (CT), or abstracts (AB),
and DP surveys are evaluated using nuggets from
citation texts (CT). In all evaluation instances the
surveys generated with the context corpora excel
at covering nuggets drawn from abstracts or cita-
tion sentences.
7 Conclusion
In this paper we proposed a framework based on
probabilistic inference to extract sentences that
appear in the scientific literature, and which are
about a secondary source, but which do not con-
tain explicit citations to that secondary source.
Our methodology is based on inference in an MRF
built using the similarity of sentences and their
lexical features. We show, by numerical exper-
iments, that an MRF in which each sentence is
connected to only a few adjacent sentences prop-
erly fits this problem. We also investigate the use-
fulness of such sentences in generating surveys of
scientific literature. Our experiments on generat-
ing surveys for Question Answering and Depen-
dency Parsing show how surveys generated using
such context information along with citation sen-
tences have higher quality than those built using
citations alone.
Generating fluent scientific surveys is difficult
in absence of sufficient background information.
Our future goal is to combine summarization
and bibliometric techniques towards building au-
tomatic surveys that employ context information
as an important part of the generated surveys.
8 Acknowledgments
The authors would like to thank Arzucan ?Ozgu?r
from University of Michigan for annotations.
This paper is based upon work supported by the
National Science Foundation grant ?iOPENER: A
Flexible Framework to Support Rapid Learning in
Unfamiliar Research Domains?, jointly awarded
to University of Michigan and University of Mary-
land as IIS 0705832. Any opinions, findings, and
conclusions or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the National Science Foun-
dation.
References
Shannon Bradshaw. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Shannon Bradshaw. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
Conference on Research and Advanced Technology
for Digital Libraries.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009.
Automatic extraction of citation contexts for re-
search paper summarization: A coreference-chain
based approach. In Proceedings of the 2009 Work-
shop on Text and Citation Analysis for Scholarly
Digital Libraries, pages 88?95, Suntec City, Sin-
gapore, August. Association for Computational Lin-
guistics.
Mary McGlohon, Stephen Bay, Markus G. Anderle,
David M. Steier, and Christos Faloutsos. 2009.
Snare: a link analytic system for graph labeling and
risk detection. In KDD ?09: Proceedings of the 15th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1265?1274.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In SI-
GIR ?05: Proceedings of the 28th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 472?479.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In SI-
GIR ?07: Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 311?318.
Donald A. Metzler. 2007. Automatic feature selection
in the markov random field model for information
retrieval. In CIKM ?07: Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 253?262.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
563
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 584?592, Boulder, Colorado, June.
Association for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004a. Bilingual presri:
Integration of multiple research paper databases. In
Proceedings of RIAO 2004, pages 195?211, Avi-
gnon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Oku-
mura. 2004b. Classification of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation. In Proceedings
of the 11th SIG Classification Research Workshop,
pages 117?134, Chicago, USA.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In ACL workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries.
Matteo Romanello, Federico Boschetti, and Gregory
Crane. 2009. Citations in the digital library of clas-
sics: Extracting canonical references by using con-
ditional random fields. In Proceedings of the 2009
Workshop on Text and Citation Analysis for Schol-
arly Digital Libraries, pages 80?87, Suntec City,
Singapore, August. Association for Computational
Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summarizing
scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the EMNLP, Sydney, Australia,
July.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss. 2003. Understanding belief propagation and
its generalizations. pages 239?269.
564
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098?1108,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning From Collective Human Behavior to
Introduce Diversity in Lexical Choice
Vahed Qazvinian
Department of EECS
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Dragomir R. Radev
School of Information
Department of EECS
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We analyze collective discourse, a collective
human behavior in content generation, and
show that it exhibits diversity, a property of
general collective systems. Using extensive
analysis, we propose a novel paradigm for de-
signing summary generation systems that re-
flect the diversity of perspectives seen in real-
life collective summarization. We analyze 50
sets of summaries written by human about the
same story or artifact and investigate the diver-
sity of perspectives across these summaries.
We show how different summaries use vari-
ous phrasal information units (i.e., nuggets) to
express the same atomic semantic units, called
factoids. Finally, we present a ranker that em-
ploys distributional similarities to build a net-
work of words, and captures the diversity of
perspectives by detecting communities in this
network. Our experiments show how our sys-
tem outperforms a wide range of other docu-
ment ranking systems that leverage diversity.
1 Introduction
In sociology, the term collective behavior is used to
denote mass activities that are not centrally coordi-
nated (Blumer, 1951). Collective behavior is dif-
ferent from group behavior in the following ways:
(a) it involves limited social interaction, (b) mem-
bership is fluid, and (c) it generates weak and un-
conventional norms (Smelser, 1963). In this paper,
we focus on the computational analysis of collective
discourse, a collective behavior seen in interactive
content contribution and text summarization in on-
line social media. In collective discourse each in-
dividual?s behavior is largely independent of that of
other individuals.
In social media, discourse (Grosz and Sidner,
1986) is often a collective reaction to an event. One
scenario leading to collective reaction to a well-
defined subject is when an event occurs (a movie is
released, a story occurs, a paper is published) and
people independently write about it (movie reviews,
news headlines, citation sentences). This process of
content generation happens over time, and each per-
son chooses the aspects to cover. Each event has
an onset and a time of death after which nothing is
written about it. Tracing the generation of content
over many instances will reveal temporal patterns
that will allow us to make sense of the text gener-
ated around a particular event.
To understand collective discourse, we are inter-
ested in behavior that happens over a short period
of time. We focus on topics that are relatively well-
defined in scope such as a particular event or a single
news event that does not evolve over time. This can
eventually be extended to events and issues that are
evolving either in time or scope such as elections,
wars, or the economy.
In social sciences and the study of complex sys-
tems a lot of work has been done to study such col-
lective systems, and their properties such as self-
organization (Page, 2007) and diversity (Hong and
Page, 2009; Fisher, 2009). However, there is little
work that studies a collective system in which mem-
bers individually write summaries.
In most of this paper, we will be concerned with
developing a complex systems view of the set of col-
lectively written summaries, and give evidence of
1098
the diversity of perspectives and its cause. We be-
lieve that out experiments will give insight into new
models of text generation, which is aimed at model-
ing the process of producing natural language texts,
and is best characterized as the process of mak-
ing choices between alternate linguistic realizations,
also known as lexical choice (Elhadad, 1995; Barzi-
lay and Lee, 2002; Stede, 1995).
2 Prior Work
In summarization, a number of previous methods
have focused on diversity. (Mei et al, 2010) in-
troduce a diversity-focused ranking methodology
based on reinforced random walks in information
networks. Their random walk model introduces the
rich-gets-richer mechanism to PageRank with rein-
forcements on transition probabilities between ver-
tices. A similar ranking model is the Grasshopper
ranking model (Zhu et al, 2007), which leverages
an absorbing random walk. This model starts with
a regular time-homogeneous random walk, and in
each step the node with the highest weight is set
as an absorbing state. The multi-view point sum-
marization of opinionated text is discussed in (Paul
et al, 2010). Paul et al introduce Compar-
ative LexRank, based on the LexRank ranking
model (Erkan and Radev, 2004). Their random walk
formulation is to score sentences and pairs of sen-
tences from opposite viewpoints (clusters) based on
both their representativeness of the collection as well
as their contrastiveness with each other. Once a lex-
ical similarity graph is built, they modify the graph
based on cluster information and perform LexRank
on the modified cosine similarity graph.
The most well-known paper that address diver-
sity in summarization is (Carbonell and Goldstein,
1998), which introduces Maximal Marginal Rele-
vance (MMR). This method is based on a greedy
algorithm that picks sentences in each step that are
the least similar to the summary so far. There are
a few other diversity-focused summarization sys-
tems like C-LexRank (Qazvinian and Radev, 2008),
which employs document clustering. These papers
try to increase diversity in summarizing documents,
but do not explain the type of the diversity in their in-
puts. In this paper, we give an insightful discussion
on the nature of the diversity seen in collective dis-
course, and will explain why some of the mentioned
methods may not work under such environments.
In prior work on evaluating independent contri-
butions in content generation, Voorhees (Voorhees,
1998) studied IR systems and showed that rele-
vance judgments differ significantly between hu-
mans but relative rankings show high degrees of sta-
bility across annotators. However, perhaps the clos-
est work to this paper is (van Halteren and Teufel,
2004) in which 40 Dutch students and 10 NLP re-
searchers were asked to summarize a BBC news re-
port, resulting in 50 different summaries. Teufel
and van Halteren also used 6 DUC1-provided sum-
maries, and annotations from 10 student participants
and 4 additional researchers, to create 20 summaries
for another news article in the DUC datasets. They
calculated the Kappa statistic (Carletta, 1996; Krip-
pendorff, 1980) and observed high agreement, indi-
cating that the task of atomic semantic unit (factoid)
extraction can be robustly performed in naturally oc-
curring text, without any copy-editing.
The diversity of perspectives and the unprece-
dented growth of the factoid inventory also affects
evaluation in text summarization. Evaluation meth-
ods are either extrinsic, in which the summaries are
evaluated based on their quality in performing a spe-
cific task (Spa?rck-Jones, 1999) or intrinsic where the
quality of the summary itself is evaluated, regardless
of any applied task (van Halteren and Teufel, 2003;
Nenkova and Passonneau, 2004). These evaluation
methods assess the information content in the sum-
maries that are generated automatically.
Finally, recent research on analyzing online so-
cial media shown a growing interest in mining news
stories and headlines because of its broad appli-
cations ranging from ?meme? tracking and spike
detection (Leskovec et al, 2009) to text summa-
rization (Barzilay and McKeown, 2005). In sim-
ilar work on blogs, it is shown that detecting top-
ics (Kumar et al, 2003; Adar et al, 2007) and sen-
timent (Pang and Lee, 2004) in the blogosphere can
help identify influential bloggers (Adar et al, 2004;
Java et al, 2006) and mine opinions about prod-
ucts (Mishne and Glance, 2006).
1Document Understanding Conference
1099
3 Data Annotation
The datasets used in our experiments represent two
completely different categories: news headlines, and
scientific citation sentences. The headlines datasets
consist of 25 clusters of news headlines collected
from Google News2, and the citations datasets have
25 clusters of citations to specific scientific papers
from the ACL Anthology Network (AAN)3. Each
cluster consists of a number of unique summaries
(headlines or citations) about the same artifact (non-
evolving news story or scientific paper) written by
different people. Table 1 lists some of the clusters
with the number of summaries in them.
ID type Name Story/Title #
1 hdl miss Miss Venezuela wins miss universe?09 125
2 hdl typhoon Second typhoon hit philippines 100
3 hdl russian Accident at Russian hydro-plant 101
4 hdl redsox Boston Red Sox win world series 99
5 hdl gervais ?Invention of Lying? movie reviewed 97
? ? ? ? ? ? ? ? ?
25 hdl yale Yale lab tech in court 10
26 cit N03-1017 Statistical Phrase-Based Translation 172
27 cit P02-1006 Learning Surface Text Patterns ... 72
28 cit P05-1012 On-line Large-Margin Training ... 71
29 cit C96-1058 Three New Probabilistic Models ... 66
30 cit P05-1033 A Hierarchical Phrase-Based Model ... 65
? ? ? ? ? ? ? ? ?
50 cit H05-1047 A Semantic Approach to Recognizing ... 7
Table 1: Some of the annotated datasets and the number
of summaries in each of them (hdl = headlines; cit = cita-
tions)
3.1 Nuggets vs. Factoids
We define an annotation task that requires explicit
definitions that distinguish between phrases that rep-
resent the same or different information units. Un-
fortunately, there is little consensus in the literature
on such definitions. Therefore, we follow (van Hal-
teren and Teufel, 2003) and make the following dis-
tinction. We define a nugget to be a phrasal infor-
mation unit. Different nuggets may all represent
the same atomic semantic unit, which we call as a
factoid. In the following headlines, which are ran-
domly extracted from the redsox dataset, nuggets
are manually underlined.
red sox win 2007 world series
boston red sox blank rockies to clinch world series
2news.google.com
3http://clair.si.umich.edu/clair/anthology/
boston fans celebrate world series win; 37 arrests re-
ported
These 3 headlines contain 9 nuggets, which rep-
resent 5 factoids or classes of equivalent nuggets.
f1 : {red sox, boston, boston red sox}
f2 : {2007 world series, world series win, world series}
f3 : {rockies}
f4 : {37 arrests}
f5 : {fans celebrate}
This example suggests that different headlines on
the same story written independently of one an-
other use different phrases (nuggets) to refer to the
same semantic unit (e.g., ?red sox? vs. ?boston? vs.
?boston red sox?) or to semantic units corresponding
to different aspects of the story (e.g., ?37 arrests? vs.
?rockies?). In the former case different nuggets are
used to represent the same factoid, while in the latter
case different nuggets are used to express different
factoids. This analogy is similar to the definition of
factoids in (van Halteren and Teufel, 2004).
The following citation sentences to Koehn?s work
suggest that a similar phenomenon also happens in
citations.
We also compared our model with pharaoh (Koehn et al
2003).
Koehn et al(2003) find that
phrases longer than three words improve per-
formance little.
Koehn et al(2003) suggest limiting phrase length
to three words or less.
For further information on these parameter settings,
confer (koehn et al 2003).
where the first author mentions ?pharaoh? as a
contribution of Koehn et al but the second and third
use different nuggets to represent the same contribu-
tion: use of trigrams. However, as the last citation
shows, a citation sentence, unlike news headlines,
may cover no information about the target paper.
The use of phrasal information as nuggets is an es-
sential element to our experiments, since some head-
line writers often try to use uncommon terms to re-
fer to a factoid. For instance, two headlines from the
redsox cluster are:
Short wait for bossox this time
Soxcess started upstairs
1100
Following these examples, we asked two anno-
tators to annotate all 1, 390 headlines, and 926 ci-
tations. The annotators were asked to follow pre-
cise guidelines in nugget extraction. Our guidelines
instructed annotators to extract non-overlapping
phrases from each headline as nuggets. Therefore,
each nugget should be a substring of the headline
that represents a semantic unit4.
Previously (Lin and Hovy, 2002) had shown that
information overlap judgment is a difficult task for
human annotators. To avoid such a difficulty, we
enforced our annotators to extract non-overlapping
nuggets from a summary to make sure that they are
mutually independent and that information overlap
between them is minimized.
Finding agreement between annotated well-
defined nuggets is straightforward and can be cal-
culated in terms of Kappa. However, when nuggets
themselves are to be extracted by annotators, the
task becomes less obvious. To calculate the agree-
ment, we annotated 10 randomly selected head-
line clusters twice and designed a simple evalua-
tion scheme based on Kappa5. For each n-gram,
w, in a given headline, we look if w is part of any
nugget in either human annotations. If w occurs
in both or neither, then the two annotators agree
on it, and otherwise they do not. Based on this
agreement setup, we can formalize the ? statistic
as ? = Pr(a)?Pr(e)1?Pr(e) where Pr(a) is the relative ob-
served agreement among annotators, and Pr(e) is
the probability that annotators agree by chance if
each annotator is randomly assigning categories.
Table 2 shows the unigram, bigram, and trigram-
based average ? between the two human annotators
(Human1, Human2). These results suggest that
human annotators can reach substantial agreement
when bigram and trigram nuggets are examined, and
has reasonable agreement for unigram nuggets.
4 Diversity
We study the diversity of ways with which human
summarizers talk about the same story or event and
explain why such a diversity exists.
4Before the annotations, we lower-cased all summaries and
removed duplicates
5Previously (Qazvinian and Radev, 2010) have shown high
agreement in human judgments in a similar task on citation an-
notation
Average ?
unigram bigram trigram
Human1 vs. Human2
0.76? 0.4 0.80? 0.4 0.89? 0.3
Table 2: Agreement between different annotators in terms
of average Kappa in 25 headline clusters.
100 101 102
10?2
10?1
100
Pr(X ?
 
c)
c
headlines
 
 Pr(X ? c)
100 101 102
10?2
10?1
100
Pr(X ?
 
c)
c
citations
 
 Pr(X ? c)
Figure 1: The cumulative probability distribution for the
frequency of factoids (i.e., the probability that a factoid
will be mentioned in c different summaries) across in
each category.
4.1 Skewed Distributions
Our first experiment is to analyze the popularity of
different factoids. For each factoid in the annotated
clusters, we extract its count, X , which is equal to
the number of summaries it has been mentioned in,
and then we look at the distribution of X . Fig-
ure 1 shows the cumulative probability distribution
for these counts (i.e., the probability that a factoid
will be mentioned in at least c different summaries)
in both categories.
These highly skewed distributions indicate that a
large number of factoids (more than 28%) are only
mentioned once across different clusters (e.g., ?poor
pitching of colorado? in the redsox cluster), and
that a few factoids are mentioned in a large number
of headlines (likely using different nuggets). The
large number of factoids that are only mentioned in
one headline indicates that different summarizers in-
crease diversity by focusing on different aspects of
a story or a paper. The set of nuggets also exhibit
similar skewed distributions. If we look at individ-
ual nuggets, the redsox set shows that about 63
(or 80%) of the nuggets get mentioned in only one
headline, resulting in a right-skewed distribution.
The factoid analysis of the datasets reveals two
main causes for the content diversity seen in head-
lines: (1) writers focus on different aspects of the
story and therefore write about different factoids
1101
(e.g., ?celebrations? vs. ?poor pitching of col-
orado?). (2) writer use different nuggets to represent
the same factoid (e.g., ?redsox? vs. ?bosox?). In the
following sections we analyze the extent at which
each scenario happens.
100 101 102 1030
200
400
600
800
1000
number of summaries
Inve
ntor
y si
ze
headlines
 
 NuggetsFactoids
100 101 102 1030
50
100
150
200
250
300
350
number of summaries
Inve
ntor
y si
ze
citations
 
 NuggetsFactoids
Figure 2: The number of unique factoids and nuggets ob-
served by reading n random summaries in all the clusters
of each category
4.2 Factoid Inventory
The emergence of diversity in covering different fac-
toids suggests that looking at more summaries will
capture a larger number of factoids. In order to ana-
lyze the growth of the factoid inventory, we perform
a simple experiment. We shuffle the set of sum-
maries from all 25 clusters in each category, and then
look at the number of unique factoids and nuggets
seen after reading nth summary. This number shows
the amount of information that a randomly selected
subset of n writers represent. This is important to
study in order to find out whether we need a large
number of summaries to capture all aspects of a
story and build a complete factoid inventory. The
plot in Figure 4.1 shows, at each n, the number of
unique factoids and nuggets observed by reading n
random summaries from the 25 clusters in each cat-
egory. These curves are plotted on a semi-log scale
to emphasize the difference between the growth pat-
terns of the nugget inventories and the factoid inven-
tories6.
This finding numerically confirms a similar ob-
servation on human summary annotations discussed
in (van Halteren and Teufel, 2003; van Halteren
and Teufel, 2004). In their work, van Halteren and
Teufel indicated that more than 10-20 human sum-
maries are needed for a full factoid inventory. How-
ever, our experiments with nuggets of nearly 2, 400
independent human summaries suggest that neither
the nugget inventory nor the number of factoids will
be likely to show asymptotic behavior. However,
these plots show that the nugget inventory grows at
a much faster rate than factoids. This means that a
lot of the diversity seen in human summarization is
a result of the so called different lexical choices that
represent the same semantic units or factoids.
4.3 Summary Quality
In previous sections we gave evidence for the diver-
sity seen in human summaries. However, a more
important question to answer is whether these sum-
maries all cover important aspects of the story. Here,
we examine the quality of these summaries, study
the distribution of information coverage in them,
and investigate the number of summaries required
to build a complete factoid inventory.
The information covered in each summary can be
determined by the set of factoids (and not nuggets)
and their frequencies across the datasets. For exam-
ple, in the redsox dataset, ?red sox?, ?boston?, and
?boston red sox? are nuggets that all represent the
same piece of information: the red sox team. There-
fore, different summaries that use these nuggets to
refer to the red sox team should not be seen as very
different.
We use the Pyramid model (Nenkova and Pas-
sonneau, 2004) to value different summary factoids.
Intuitively, factoids that are mentioned more fre-
quently are more salient aspects of the story. There-
fore, our pyramid model uses the normalized fre-
quency at which a factoid is mentioned across a
dataset as its weight. In the pyramid model, the in-
dividual factoids fall in tiers. If a factoid appears in
more summaries, it falls in a higher tier. In princi-
ple, if the term wi appears |wi| times in the set of
6Similar experiment using individual clusters exhibit similar
behavior
1102
headlines it is assigned to the tier T|wi|. The pyra-
mid score that we use is computed as follows. Sup-
pose the pyramid has n tiers, Ti, where tier Tn is
the top tier and T1 is the bottom. The weight of
the factoids in tier Ti will be i (i.e. they appeared
in i summaries). If |Ti| denotes the number of fac-
toids in tier Ti, and Di is the number of factoids in
the summary that appear in Ti, then the total factoid
weight for the summary is D =
?n
i=1 i ? Di. Ad-
ditionally, the optimal pyramid score for a summary
is Max =
?n
i=1 i? |Ti|. Finally, the pyramid score
for a summary can be calculated as
P =
D
Max
Based on this scoring scheme, we can use the an-
notated datasets to determine the quality of individ-
ual headlines. First, for each set we look at the vari-
ation in pyramid scores that individual summaries
obtain in their set. Figure 3 shows, for each clus-
ter, the variation in the pyramid scores (25th to 75th
percentile range) of individual summaries evaluated
against the factoids of that cluster. This figure in-
dicates that the pyramid score of almost all sum-
maries obtain values with high variations in most of
the clusters For instance, individual headlines from
redsox obtain pyramid scores as low as 0.00 and
as high as 0.93. This high variation confirms the pre-
vious observations on diversity of information cov-
erage in different summaries.
Additionally, this figure shows that headlines gen-
erally obtain higher values than citations when con-
sidered as summaries. One reason, as explained be-
fore, is that a citation may not cover any important
contribution of the paper it is citing, when headlines
generally tend to cover some aspects of the story.
High variation in quality means that in order to
capture a larger information content we need to read
a greater number of summaries. But how many
headlines should one read to capture a desired level
of information content? To answer this question,
we perform an experiment based on drawing random
summaries from the pool of all the clusters in each
category. We perform a Monte Carlo simulation, in
which for each n, we draw n random summaries,
and look at the pyramid score achieved by reading
these headlines. The pyramid score is calculated us-
ing the factoids from all 25 clusters in each cate-
gory7. Each experiment is repeated 1, 000 times to
find the statistical significance of the experiment and
the variation from the average pyramid scores.
Figure 4.3 shows the average pyramid scores over
different n values in each category on a log-log
scale. This figure shows how pyramid score grows
and approaches 1.00 rapidly as more randomly se-
lected summaries are seen.
100 101 102 103
10?2
10?1
100
number of summaries
Py
ram
id S
cor
e
 
 
headlines
citations
Figure 4: Average pyramid score obtained by reading n
random summaries shows rapid asymptotic behavior.
5 Diversity-based Ranking
In previous sections we showed that the diversity
seen in human summaries could be according to dif-
ferent nuggets or phrases that represent the same fac-
toid. Ideally, a summarizer that seeks to increase di-
versity should capture this phenomenon and avoid
covering redundant nuggets. In this section, we use
different state of the art summarization systems to
rank the set of summaries in each cluster with re-
spect to information content and diversity. To evalu-
ate each system, we cut the ranked list at a constant
length (in terms of the number of words) and calcu-
late the pyramid score of the remaining text.
5.1 Distributional Similarity
We have designed a summary ranker that will pro-
duce a ranked list of documents with respect to the
diversity of their contents. Our model works based
on ranking individual words and using the ranked
list of words to rank documents that contain them.
In order to capture the nuggets of equivalent se-
mantic classes, we use a distributional similarity of
7Similar experiment using individual clusters exhibit similar
results
1103
00.2
0.4
0.6
0.8
1
abo
rtio
n
am
az
on
bab
ies
bur
ger
co
lom
bia
en
gla
nd
ger
vai
s
goo
gle
irel
and
ma
ine
me
rcu
ry
mis
s
mo
nke
y
mo
za
rt
no
bel prie
st
ps3
slim
ra
dia
tion
re
dso
x
ru
ss
ian
sc
ien
tist
so
upy
sw
ede
n
typ
hoo
n
yal
e
A0
0_1
023
A0
0_1
043
A0
0_2
024
C0
0_1
072
C9
6_1
058
D0
3_1
017
D0
4_9
907
H0
5_1
047
H0
5_1
079
J04
_40
02
N0
3_1
017
N0
4_1
033
P0
2_1
006
P0
3_1
001
P0
5_1
012
P0
5_1
013
P0
5_1
014
P0
5_1
033
P9
7_1
003
P9
9_1
065
W0
0_0
403
W0
0_0
603
W0
3_0
301
W0
3_0
510
W0
5_1
203
Py
ram
id S
cor
e
 
 
headlines
citations
Figure 3: The 25th to 75th percentile pyramid score range in individual clusters
words that is inspired by (Lee, 1999). We represent
each word by its context in the cluster and find the
similarity of such contexts. Particularly, each word
wi is represented by a bag of words, `i, that have a
surface distance of 3 or smaller to wi anywhere in
the cluster. In other words, `i contains any word that
co-occurs with wi in a 4-gram in the cluster. This
bag of words representation of words enables us to
find the word-pair similarities.
sim(wi, wj) =
~`
i ? ~`j
?
|~`i|| ~`j |
(1)
We use the pair-wise similarities of words in each
cluster, and build a network of words and their simi-
larities. Intuitively, words that appear in similar con-
texts are more similar to each other and will have a
stronger edge between them in the network. There-
fore, similar words, or words that appear in similar
contexts, will form communities in this graph. Ide-
ally, each community in the word similarity network
would represent a factoid. To find the communities
in the word network we use (Clauset et al, 2004), a
hierarchical agglomeration algorithm which works
by greedily optimizing the modularity in a linear
running time for sparse graphs.
The community detection algorithm will assign
to each word wi, a community label Ci. For each
community, we use LexRank to rank the words us-
ing the similarities in Equation 1, and assign a score
to each word wi as S(wi) =
Ri
|Ci|
, where Ri is the
rank of wi in its community, and |Ci| is the number
of words that belong to Ci. Figure 5.1 shows part
police
second
soxcelebrations red jumpbaseball
unhappy
sweeps
pitching
hittingarrest
victorytitle dynasty
fan poorer
2nd
poor
glory
Pajek
Figure 5: Part of the word similarity graph in the redsox
cluster
of the word similarity graph in the redsox cluster,
in which each node is color-coded with its commu-
nity. This figure illustrates how words that are se-
mantically related to the same aspects of the story
fall in the same communities (e.g., ?police? and ?ar-
rest?). Finally, to rank sentences, we define the score
of each document Dj as the sum of the scores of its
words.
pds(Dj) =
?
wi?Dj
S(wi)
Intuitively, sentences that contain higher ranked
words in highly populated communities will have a
smaller score. To rank the sentences, we sort them
in an ascending order, and cut the list when its size
is greater than the length limit.
5.2 Other Methods
5.2.1 Random
For each cluster in each category (citations and
headlines), this method simply gets a random per-
1104
mutations of the summaries. In the headlines
datasets, where most of the headlines cover some
factoids about the story, we expect this method to
perform reasonably well since randomization will
increase the chances of covering headlines that fo-
cus on different factoids. However, in the citations
dataset, where a citing sentence may cover no infor-
mation about the cited paper, randomization has the
drawback of selecting citations that have no valuable
information in them.
5.2.2 LexRank
LexRank (Erkan and Radev, 2004) works by first
building a graph of all the documents (Di) in a
cluster. The edges between corresponding nodes
(di) represent the cosine similarity between them is
above a threshold (0.10 following (Erkan and Radev,
2004)). Once the network is built, the system finds
the most central sentences by performing a random
walk on the graph.
p(dj) = (1? ?)
1
|D|
+ ?
?
di
p(di)P (di ? dj) (2)
5.2.3 MMR
Maximal Marginal Relevance (MMR) (Carbonell
and Goldstein, 1998) uses the pairwise cosine simi-
larity matrix and greedily chooses sentences that are
the least similar to those already in the summary. In
particular,
MMR = argminDi?D?A
[
maxDj?A Sim(Di, Dj)
]
where A is the set of documents in the summary,
initialized to A = ?.
5.2.4 DivRank
Unlike other time-homogeneous random walks
(e.g., PageRank), DivRank does not assume that
the transition probabilities remain constant over
time. DivRank uses a vertex-reinforced random
walk model to rank graph nodes based on a diversity
based centrality. The basic assumption in DivRank
is that the transition probability from a node to other
is reinforced by the number of previous visits to the
target node (Mei et al, 2010). Particularly, let?s as-
sume pT (u, v) is the transition probability from any
node u to node v at time T . Then,
pT (di, dj) = (1? ?).p
?(dj) + ?.
p0(di, dj).NT (dj)
DT (di)
(3)
whereNT (dj) is the number of times the walk has
visited dj up to time T and
DT (di) =
?
dj?V
p0(di, dj)NT (dj) (4)
Here, p?(dj) is the prior distribution that deter-
mines the preference of visiting vertex dj . We try
two variants of this algorithm: DivRank, in which
p?(dj) is uniform, and DivRank with priors in
which p?(dj) ? l(Dj)?? , where l(Dj) is the num-
ber of the words in the document Dj and ? is a pa-
rameter (? = 0.8).
5.2.5 C-LexRank
C-LexRank is a clustering-based model in which
the cosine similarities of document pairs are used to
build a network of documents. Then the the network
is split into communities, and the most salient doc-
uments in each community are selected (Qazvinian
and Radev, 2008). C-LexRank focuses on finding
communities of documents using their cosine simi-
larity. The intuition is that documents that are more
similar to each other contain similar factoids. We ex-
pect C-LexRank to be a strong ranker, but incapable
of capturing the diversity caused by using different
phrases to express the same meaning. The reason is
that different nuggets that represent the same factoid
often have no words in common (e.g., ?victory? and
?glory?) and won?t be captured by a lexical measure
like cosine similarity.
5.3 Experiments
We use each of the systems explained above to rank
the summaries in each cluster. Each ranked list is
then cut at a certain length (50 words for headlines,
and 150 for citations) and the information content
in the remaining text is examined using the pyramid
score.
Table 3 shows the average pyramid score achieved
by different methods in each category. The method
based on the distributional similarities of words out-
performs other methods in the citations category. All
methods show similar results in the headlines cate-
gory, where most headlines cover at least 1 factoid
about the story and a random ranker performs rea-
sonably well. Table 4 shows top 3 headlines from
3 rankers: word distributional similarity (WDS), C-
LexRank, and MMR. In this example, the first 3
1105
Method
headlines citations Mean
pyramid 95% C.I. pyramid 95% C.I.
R 0.928 [0.896, 0.959] 0.716 [0.625, 0.807] 0.822
MMR 0.930 [0.902, 0.960] 0.766 [0.684, 0.847] 0.848
LR 0.918 [0.891, 0.945] 0.728 [0.635, 0.822] 0.823
DR 0.927 [0.900, 0.955] 0.736 [0.667, 0.804] 0.832
DR(p) 0.916 [0.884, 0.949] 0.764 [0.697, 0.831] 0.840
C-LR 0.942 [0.919, 0.965] 0.781 [0.710, 0.852] 0.862
WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872
R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; C-
LR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence In-
terval
Table 3: Comparison of different ranking systems
Method Top 3 headlines
WDS
1: how sweep it is
2: fans celebrate red sox win
3: red sox take title
C-LR
1: world series: red sox sweep rockies
2: red sox take world series
3: red sox win world series
MMR
1:red sox scale the rockies
2: boston sweep colorado to win world series
3: rookies respond in first crack at the big time
C-LR=C-LexRank; WDS=Word Distributional Similarity
Table 4: Top 3 ranked summaries of the redsox cluster
using different methods
headlines produced by WDS cover two important
factoids: ?red sox winning the title? and ?fans cel-
ebrating?. However, the second factoid is absent in
the other two.
6 Conclusion and Future Work
Our experiments on two different categories of
human-written summaries (headlines and citations)
showed that a lot of the diversity seen in human
summarization comes from different nuggets that
may actually represent the same semantic informa-
tion (i.e., factoids). We showed that the factoids ex-
hibit a skewed distribution model, and that the size
of the nugget inventory asymptotic behavior even
with a large number of summaries. We also showed
high variation in summary quality across different
summaries in terms of pyramid score, and that the
information covered by reading n summaries has a
rapidly growing asymptotic behavior as n increases.
Finally, we proposed a ranking system that employs
word distributional similarities to identify semanti-
cally equivalent words, and compared it with a wide
range of summarization systems that leverage diver-
sity.
In the future, we plan to move to content from
other collective systems on Web. In order to gen-
eralize our findings, we plan to examine blog com-
ments, online reviews, and tweets (that discuss the
same URL). We also plan to build a generation sys-
tem that employs the Yule model (Yule, 1925) to de-
termine the importance of each aspect (e.g. who,
when, where, etc.) in order to produce summaries
that include diverse aspects of a story.
Our work has resulted in a publicly available
dataset 8 of 25 annotated news clusters with nearly
1, 400 headlines, and 25 clusters of citation sen-
tences with more than 900 citations. We believe that
this dataset can open new dimensions in studying di-
versity and other aspects of automatic text genera-
tion.
7 Acknowledgments
This work is supported by the National Science
Foundation grant number IIS-0705832 and grant
number IIS-0968489. Any opinions, findings, and
conclusions or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect the views of the supporters.
References
Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M.
Lukose. 2004. Implicit structure and the dynamics of
8http://www-personal.umich.edu/?vahed/
data.html
1106
Blogspace. In WWW?04, Workshop on the Weblogging
Ecosystem.
Eytan Adar, Daniel S. Weld, Brian N. Bershad, and
Steven S. Gribble. 2007. Why we search: visualiz-
ing and predicting user behavior. In WWW?07, pages
161?170, New York, NY, USA.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing - Volume 10,
EMNLP ?02, pages 164?171.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Comput. Linguist., 31(3):297?328.
Herbert Blumer. 1951. Collective behavior. In Lee, Al-
fred McClung, Ed., Principles of Sociology.
Jaime G. Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR?98, pages
335?336.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70(6).
Michael Elhadad. 1995. Using argumentation in text
generation. Journal of Pragmatics, 24:189?220.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Len Fisher. 2009. The Perfect Swarm: The Science of
Complexity in Everyday Life. Basic Books.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12:175?204, July.
Lu Hong and Scott Page. 2009. Interpreted and
generated signals. Journal of Economic Theory,
144(5):2174?2196.
Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates.
2006. Modeling the spread of influence on the blogo-
sphere. In WWW?06.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and
Andrew Tomkins. 2003. On the bursty evolution of
blogspace. In WWW?03, pages 568?576, New York,
NY, USA.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, pages 25?32.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ?09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497?506.
Chin-Yew Lin and Eduard Hovy. 2002. Manual and au-
tomatic evaluation of summaries. In ACL-Workshop
on Automatic Summarization.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 1009?1018.
Gilad Mishne and Natalie Glance. 2006. Predicting
movie sales from blogger sentiment. In AAAI 2006
Spring Symposium on Computational Approaches to
Analysing Weblogs (AAAI-CAAW 2006).
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Scott E. Page. 2007. The Difference: How the Power of
Diversity Creates Better Groups, Firms, Schools, and
Societies. Princeton University Press.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL?04, Morristown,
NJ, USA.
Michael Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 66?76.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Neil J. Smelser. 1963. Theory of Collective Behavior.
Free Press.
Karen Spa?rck-Jones. 1999. Automatic summarizing:
factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in automatic text summa-
rization, chapter 1, pages 1 ? 12. The MIT Press.
Manfred Stede. 1995. Lexicalization in natural language
generation: a survey. Artificial Intelligence Review,
(8):309?336.
Hans van Halteren and Simone Teufel. 2003. Examin-
ing the consensus between human summaries: initial
experiments with factoid analysis. In Proceedings of
1107
the HLT-NAACL 03 on Text summarization workshop,
pages 57?64, Morristown, NJ, USA. Association for
Computational Linguistics.
Hans van Halteren and Simone Teufel. 2004. Evaluating
information content by factoid analysis: human anno-
tation and stability. In EMNLP?04, Barcelona.
Ellen M. Voorhees. 1998. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
In SIGIR ?98: Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 315?323.
G. Udny Yule. 1925. A mathematical theory of evo-
lution, based on the conclusions of dr. j. c. willis,
f.r.s. Philosophical Transactions of the Royal Society
of London. Series B, Containing Papers of a Biological
Character, 213:21?87.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity in
ranking using absorbing random walks. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 97?104.
1108
