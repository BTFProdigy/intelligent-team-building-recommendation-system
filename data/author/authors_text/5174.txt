The Automat ic  Translation of Discourse Structures 
Danie l  Marcu  
Information Sciences Institute and 
Department of Computer Science 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina del Rey, CA 90292 
marcu@isi.edu 
Lynn Car l son  
U.S. Department of Defense 
Ft. Meade, MD 20755 
Imcarls@afterlife. ncsc. mil 
Maki  Watanabe 
Department of Linguistics 
University of Southern California 
Los Angeles, CA 90089 
mwatanab@usc.edu 
Abst rac t  
We empirically show that there are significant differ- 
ences between the discourse structure of Japanese 
texts and the discourse structure of their corre- 
sponding English translations. To improve trans- 
lation quality, we propose a computational model 
for rewriting discourse structures. When we train 
our model on a parallel corpus of manually built 
Japanese and English discourse structure trees, we 
learn to rewrite Japanese trees as trees that are 
closer to the natural English rendering than the orig- 
inal ones. 
1 Mot ivat ion  
Almost all current MT systems process text one sen- 
tence at a time. Because of this limited focus, MT 
systems cannot re-group and re-order the clauses 
and sentences of an input text to achieve the most 
natural rendering in a target language. Yet, even 
between languages as close as English and French, 
there is a 10% mismatch in number of sentences 
- -  what is said in two sentences in one language 
is said in only one, or in three, in the other (Gale 
and Church, 1993). For distant language pairs, such 
as Japanese and English, the differences are more 
significant. 
Consider, for example, Japanese sentence (1), a 
word-by-word "gloss" of it (2), and a two-sentence 
translation of it that was produced by a professional 
translator (3). 
(1) 
\[The Ministry of Health and Welfare last year 
revealed I \] \[population of future estimate ac- 
cording to 2\] \[in future 1.499 persons as the 
lowest s\] \[that after *SAB* rising to turn that 4\] 
\[*they* estimated but s \] \[already the estimate 
misses a point ~\] \[prediction became. 7\] 
(2) 
\[In its future population estimates'\] \[made (3) 
public last year, 2\] \[the Ministry of Health and 
Welfare predicted that the SAB would drop to 
a new low of 1.499 in the future, s) \[but would 
make a comeback after that, 4\] \[increasing once 
again, s\] \[However, it looks as if that prediction 
will be quickly shattered. 6\] 
The labeled spans of text represent elementary 
discourse units (edus), i.e., minimal text spans that 
have an unambiguous discourse function (Mann and 
Thompson, 1988). If we analyze the text frag- 
ments closely, we will notice that in translating sen- 
tence (1), a professional translator chose to realize 
the information in Japanese unit 2 first (unit 2 in 
text (1) corresponds roughly to unit 1 in text (3)); 
to realize then some of the information in Japanese 
unit 1 (part of unit 1 in text (1) corresponds to unit 
2 in text (3)); to fuse then information given in units 
1, 3, and 5 in text (1) and realize it in English as 
unit 3; and so on. Also, the translator chose to re- 
package the information in the original Japanese sen- 
tence into two English sentences. 
At the elementary unit level, the correspondence 
between Japanese sentence (1) and its English trans- 
lation (3) can be represented as in (4}, where j C e 
denotes the fact that the semantic ontent of unit 
j is realized fully in unit e; j D e denotes the fact 
that the semantic ontent of unit e is realized fully 
in unit j; j = e denotes the fact that units j and e 
are semantically equivalent; and j ~ e denotes the 
fact that there is a semantic overlap between units j 
and e, but neither proper inclusion nor proper equiv- 
alence. 
.!1 D e2;jt  -~ e3; 
.12 ---- el; 
33 C e3; 
.14 ~ e4;j4 ~ es; 
.15 ~ e3; 
.16 C e6; 
.17 C e6 
(4) 
9
C :.(~(.(:S'.~0r I t 
J 
e 3bota'lc.r- o31.~ c; - 3ttlio~te- e 
.~- - ' - -~ I  
:~) (~) 
l~ - IP .~ra~2,~ ,~,Ooll l t , \] iLtt ~: 
I..,? (The Ministry dL '(popu ~t ,m - 
ofHgMth .~nd ~f Ntu le  esti~n.~te 
reve~ie,:~) 
............... -,,.,~__~ ._~--- ~ ...-.<_-7_"" ___~__--,~ 
(5) (6) (7) 
~.. .~ ---....~ hev\] estin'at~d ~ ~alr~acy the redicti0n 
6?,) (~) but) estlnate miss - a becar.e) 
e 1 499 persons \[SAI~ rising - \[o 
a: the lowest) turn th~1) 
' i :  .................................................................................................................... ==:~::::?2' ~2~?:::-~.:.,~a ............... 
Is) 
. . . . . . .  _~_ .b~ ~, ,~d i Ho,e~v~, 
looks as if 
, ~ , !  r .~  ,~. - pNxil ~i~',~ will 
(1) 12) O) . ,,- quickly 
in its future made public the MiniMry ~i~.~ , .~-  dn iota| ihatN~ CNI. 
popukltion kin yem', o~ I'~slth and ~ '  "~ 
?-~imm?~ Welfare (4) (5) 
the SAB make II onto again. 
wotdd drop to 
afte~ that, 
a r, ew I~ ol 
1.499 In the 
fulute. 
Figure 1: The discourse structures of texts (1) and (3). 
Hence. the mappings in (4) provide all explicit 
representation of the way information is re-ordered 
and re-packaged when translated from Japanese into 
English. However, when translat ing text, it is also 
the case that t he rhetorical rendering changes. What  
is realized ill Japanese using an CONTRAST relation 
can be realized in English using, for example, a COXl- 
PARISON or  a CONCESSION relation. 
Figure I presents in the style of Mann and Thomp-  
son (1988) the discourse structures of text frag- 
ments (1) and (3), Each discourse structure is a 
tree whose leaves correspond to the edus and whose 
internal nodes correspond to contiguous text spans. 
Each node is characterized by a s tatus  (NUCLEUS or 
SATELLITE) and a rhetor ica l  re lat ion,  which is a re- 
lation that holds between two non-overlapping text 
spans. The distinction between nuclei and satellites 
comes from the empirical observation that the nu- 
cleus expresses what is more essential to the writer's 
intention than the satellite: and that the nucleus of 
a rhetorical relation is comprehensible independent 
of tile satellite, but not vice versa. When spans are 
equally important ,  the relation is nmltinuclear: for 
example,  the CONTRAST relation that holds between 
unit \[3\] and span \[4.5\] in the rhetorical structure of 
the English text in figure 1 is nmhinuclear.  Rhetor- 
ical relations that end in the suffix "'-e'" denote re- 
lations that correspond to embedded syntactic con- 
stituents. For example, the ELABORATION-OBJECT- 
ATTRIBUTE-E relation that holds between units 2 
and 1 in the English discourse structure corresponds 
to a restrictive relative. 
If one knows the mappings at the edu level, 
one can determine the mappings at the span (dis- 
course constituent) level as well. For example, us- 
ing the elementary mappings in (4), one call deter- 
mine that Japanese span \[1,2\] corresponds to English 
span \[I,2\], Japanese unit \[4\] to English span \[4,5\], 
Japanese span \[6.7\] to English unit \[6\], Japanese 
span \[1.5\] to English span \[1.5\], and so on. As Fig- 
ure 1 shows, the CONCESSION relation that holds be- 
tween spans \[1,5\] and \[6,7\] in the Japanese tree corre- 
sponds to a similar relation that. holds between span 
\[1,5\] and unit \[6\] in the English tree (modulo the fact 
that,  in Japanese, the relation holds between sen- 
t ence fragments, while in English it holds between 
full sentences). However, the TEMPORAL-AFTER re- 
lation that holds between units \[:3\] and \[4\] ill the 
Japanese tree is realized as a CONTRAST relation 
between unit \[3\] and span \[4.5\] in the English tree. 
And because Japanese units \[6\] and \[7\] are fused 
into unit \[6\] in English, the relation ELABORATION- 
OBJECT-ATTRIBUTE-E is 11o longer made explicit in 
the English text. 
Some of the differences between the two discourse 
trees in Figure 1 have been tradit ionally addressed 
10
Corpus k~ (#) k, (#) k,~ (#) k~ (#) 
Japanese 0.856 (80) 0.785 (3377) 0.724 (3377) 0.650 (3377) 
English 0.925 (60) 0.866 (1826) 0.839 (1826) 0.748 (1826) 
Table 1: Tagging reliability 
in MT systems at the syntactic level. For exam- 
ple, the re-ordering of units 1 and 2, can be dealt 
with using only syntactic models. However, as we 
will see in Section 2, there are significant differences 
between Japanese and English with respect to the 
way information is packaged and organized rhetori- 
cally not only at the sentence level, but also, at the 
paragraph and text levels. More specifically, as hu- 
mans translate Japanese into English, they re-order 
the clauses, sentences, and paragraphs of Japanese 
texts, they re-package the information into clauses, 
sentences, and paragraphs that are not a one-to-one 
mapping of the original Japanese units, and they 
rhetorically re-organize the structure of the trans- 
lated text so as to reflect rhetorical constraints pe- 
cific to English. If a translation system is to produce 
text that is not only grammatical but also coherent, 
it will have to ensure that the discourse structure of 
the target text reflects the natural renderings of the 
target language, and not that of the source language. 
In Section 2, we empirically show that there are 
significant differences between the rhetorical struc- 
ture of Japanese texts and their corresponding En- 
glish translations. These differences justify our in- 
vestigation into developing computational models 
for discourse structure rewriting. In Section 3, we 
present such a rewriting model, which re-orders the 
edus of the original text, determines English-specific 
clause, sentence, and paragraph boundaries, and re- 
builds the Japanese discourse structure of a text us- 
ing English-specific rhetorical renderings. In Sec- 
tion 4, we evaluate the performance of an imple- 
mentation of this model. We end with a discussion. 
2 Experiment 
In order to assess the role of discourse structure in 
MT, we built manually a corpus of discourse trees 
for 40 Japanese texts and their corresponding trans- 
lations. The texts were selected randomly from the 
ARPA corpus (White and O'Connell, 1994). On av- 
erage, each text had about 460 words. The Japanese 
texts had a total of 335 paragraphs and 773 sen- 
tences. The English texts had a total of 337 para- 
graphs and 827 sentences. 
We developed a discourse annotation protocol for 
Japanese and English along the lines followed by 
Marcu et al (1999). We used Marcu's discourse an- 
notation tool (1999) in order to manually construct 
the discourse structure of all Japanese and English 
texts in the corpus. 10% of the Japanese and En- 
glish texts were rhetorically labeled by two of us. 
The tool and the annotation protocol are available 
at http://www.isi.edu/~marcu/software/. The an- 
notation procedure yielded over the entire corpus 
2641 Japanese edus and 2363 English edus. 
We computed the reliability of the annotation us- 
ing Marcu et al (1999)'s method for computing 
kappa statistics (Siegel and Castellan, 1988) over hi- 
erarchical structures. Table 1 displays average kappa 
statistics that reflect the reliability of the annota- 
tion of elementary discourse units, k~,, hierarchical 
discourse spans, ks, hierarchical nuclearity assign- 
ments, k,~, and hierarchical rhetorical relation as- 
signments, k~. Kappa figures higher than 0.8 corre- 
spond to good agreement; kappa figures higher than 
0.6 correspond to acceptable agreement. All kappa 
statistics were statistically significant at levels higher 
than a = 0.01. In addition to the kappa statis- 
tics, table 1 also displays in parentheses the average 
number of data points per document, over which the 
kappa statistics were computed. 
For each pair of Japanese-English discourse struc- 
tures, we also built manually an alignment file, 
which specified in the notation discussed on page 1 
the correspondence b tween the edus of the Japanese 
text and the edus of its English translation. 
We computed the similarity between English and 
Japanese discourse trees using labeled recall and pre- 
cision figures that reflected the resemblance of tile 
Japanese and English discourse structures with re- 
spect to their assignment of edu boundaries, hierar- 
chical spans, nuclearity, and rhetorical relations. 
Because the trees we compared differ from one 
language to the other in the number of elementary 
units, the order of these units, and the way the units 
are grouped recursively into discourse spans, we 
computed two types of recall and precision figures. 
In computing Position-Dependent (P-D) recall and 
precision figures, a Japanese span was considered to 
match an English span when the Japanese span con- 
tained all the Japanese dus that corresponded to tile 
edus in the English span, and when the Japanese and 
English spans appeared in tile same position with 
respect to the overall structure. For example, the 
English tree in figure 1 is characterized by 10 sub- 
sentential spans: \[1\], \[2\], \[3\], \[4\], \[5\], \[6\], \[1,2\], \[4,5\], 
\[3,5\], and \[1,5\]. (Span \[1,6\] subsumes 2 sentences, 
so it is not sub-sentential.) The Japanese discourse 
tree has only 4 spans that could be matched in the 
same positions with English spans, namely spans 
\[1,2\], \[4\], \[5\], and \[1,5\]. Hence the similarity between 
the Japanese tree and the English tree with respect 
11 11
Level Units Spans Status/Nuclearity Relations 
P-D P P-D R P-D P P -DR P -DP  P -DR P-I) P P -DR 
Sentence 29.1 25.0 27.2 22.7 21.3 17.7 14.9 12.4 
Paragraph 53.9 53.4 46.8 47.3 38.6 39.0 31.9 32.3 
Text 41.3 42.6 31.5 32.6 28.8 29.9 26.1 27.1 
Weighted Average 36.0 32.5 31.8 28.4 26.0 23.1 20.1 17.9 
All 8.2 7.4 5.9 5.3 4.4 3.9 3.3 3.0 
P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I P 
Sentence 71.0 61.0 56.0 46.6 44.3 36.9 30.5 25.4 
Paragraph 62.1 61.6 53.2 53.8 43.3 43.8 35.1 35.5 
Text 74.1 76.5 54.4 56.5 48.5 50.4 41.1 42.7 
Weighted Average 55.2 49.2 44.8 39.9 33.1 29.5 
26.8 24.3 All 
69.6 63.0 
74.5 66.8 50.6 45.8 39.4 35.7 
Table 2: Similarity of the Japanese 
to their discourse structure below the sentence level 
has a recall of 4/10 and a precision of 4/11 (in Fig- 
ure 1, there are 11 sub-sentential Japanese spans). 
In computing Position-Independent (P-I) recall 
and precision figures, even when a Japanese span 
"floated" during the translation to a position in the 
English tree that was different from the position in 
the initial tree, the P-I recall and precision figures 
were not affected. The Position-Independent figures 
reflect the intuition that if two trees tl and t2 both 
have a subtree t, tl and t2 are more similar than 
if they were if they didn't share any tree. At the 
sentence level, we hence assume that if, for exam- 
ple, the syntactic structure of a relative clause is 
translated appropriately (even though it is not ap- 
propriately attached), this is better than translating 
wrongly that clause. The Position-Independent fig- 
ures offer a more optimistic metric for comparing 
discourse trees. They span a wider range of values 
than the Position-Dependent figures, which enable 
a finer grained comparison, which in turn enables 
a better characterization of the differences between 
Japanese and English discourse structures. When 
one takes an optimistic stance, for the spans at the 
sub-sentential level in the trees in Table 1 the recall 
is 6/10 and the precision is 6/11 because in addition 
to spans \[1,2\], \[4\], \[5\], and \[1,5\], one can also match 
Japanese span \[1\] to English span \[2\] and Japanese 
span \[2\] to Japanese span \[1\]. 
In order to provide a better estimate of how close 
two discourse trees were, we computed Position- 
Dependent and -Independent recall and precision fig- 
ures for the sentential level (where units are given by 
edus and spans are given by sets of edus or single sen- 
tences); paragraph level (where units are given by 
sentences and spans are given by sets of sentences 
or single paragraphs); and text level (where units 
are given by paragraphs and spans are given by sets 
of paragraphs). These figures offer a detailed pic- 
ture of how discourse structures and relations are 
mapped from one language to the other across all 
and English discourse structures 
discourse levels, from sentence to text. The differ- 
ences at the sentence level can be explained by differ- 
ences between the syntactic structures of Japanese 
and English. The differences at the paragraph and 
text levels have a purely rhetorical explanation. 
As expected, when we computed the recall and 
precision figures with respect to the nuclearity and 
relation assignments, we also factored in the statuses 
and the rhetorical relations that labeled each pair of 
spans. 
Table 2 smnmarizes the results (P-D and P- 
I (R)ecall and (P)recision figures) for each level 
(Sentence, Paragraph, and Text). The numbers 
in the "Weighted Average" line report averages of 
the Sentence-, Paragraph-, and Text-specific figures, 
weighted according to the number of units at each 
level. The numbers in the "All" line reflect recall and 
precision figures computed across the entire trees, 
with no attention paid to sentence and paragraph 
boundaries. 
Given the significantly different syntactic struc- 
tures of Japanese and English, we were not surprised 
by the low recall and precision results that reflect 
the similarity between discourse trees built below 
the sentence level. However, as Table 2 shows, there 
are significant differences between discourse trees at 
the paragraph and text levels as well. For exam- 
pie, the Position-Independent figures show that only 
about 62% of the sentences and only about 53% of 
the hierarchical spans built across sentences could 
be matched between the two corpora. When one 
looks at the status and rhetorical relations associ- 
ated with the spans built across sentences at the 
paragraph level, the P-I recall and precision figures 
drop to about 43% and 35% respectively. 
The differences in recall and precision are ex- 
plained both by differences in the way information is 
packaged into paragraphs in the two languages and 
the way it is structured rhetorically both within and 
above the paragraph level. 
These results strongly suggest hat if one attempts 
12
to translate Japanese into English on a sentence-by- 
sentence basis, it is likely that the resulting text will 
be unnatural from a discourse perspective. For ex- 
ample, if some information rendered using a CON- 
TRAST relation in Japanese is rendered using an 
ELABORATION relation in English, it would be in- 
appropriate to use a discourse marker like "but" in 
the English translation, although that would be con- 
sistent with the Japanese discourse structure. 
An inspection of the rhetorical mappings between 
Japanese and English revealed that some Japanese 
rhetorical renderings are consistently mapped into 
one or a few preferred renderings in English. For ex- 
ample, 34 of 115 CONTRAST relations in the Japanese 
texts are mapped into CONTRAST relations in En- 
glish; 27 become nuclei of relations uch as ANTITHE- 
SIS and CONCESSION, 14 are translated as COMPAR- 
ISON relations, 6 as satellites of CONCESSION rela- 
tions, 5 as LIST relations, etc. Our goal is to learn 
these systematic discourse mapping rules and exploit 
them in a machine translation context. 
3 Towards  a d i scourse -based  
mach ine  t rans la t ion  sys tem 
3.1 Overa l l  a rch i tec ture  
We are currently working towards building the mod- 
ules of a Discourse-Based Machine Translation sys- 
tem that works along the following lines. 
1. A discourse parser, such as those described by 
Sumita et al (1992), Kurohashi (1994), and 
MarcH (1999), initially derives the discourse 
structure of the text given as input. 
2. A discourse-structure transfer module 
rewrites the discourse structure of the input 
text so as to reflect a discourse rendering 
that is natural to the target language. 
3. A statistical module maps the input text 
into the target language using translation and 
language models that incorporate discourse- 
specific features, which are extracted from the 
outputs of the discourse parser and discourse 
transfer modules. 
In this paper, we focus only on the discourse- 
structure transfer module. That is, we investigate 
the feasibility of building such a module. 
3.2 The  d i scourse -based  t rans fer  mode l  
In order to learn to rewrite discourse structure trees, 
we first address a related problem, which we define 
below: 
Def in i t ion  3.1 Given two trees Ts and Tt and a 
correspondence Table C defined between Ts and Tt 
at the leaf level in terms of-----, C, D, and ~ relations, 
find a sequence of actions that rewrites the tree T~ 
into Tt. 
If for any tuple (Ts, Tt, C> such a sequence of actions 
can be derived, it is then possible to use a corpus 
of (Ts, Tt, C) tuples in order to automatically learn 
to derive from an unseen tree Ts,, which has the 
same structural properties as the trees Ts, a tree 
Ttj, which has structural properties imilar to those 
of the trees Tt. 
In order to solve the problem in definition 3.1, we 
extend the shift-reduce parsing paradigm applied by 
Magerman (1995), Hermjakob and Mooney (1997), 
and MarcH (1999). In this extended paradigm, the 
transfer process starts with an empty Stack and an 
Input List that contains a sequence of elementary 
discourse trees edts, one edt for each edu in the tree 
Ts given as input. The status and rhetorical rela- 
tion associated with each edt is undefined. At each 
step, the transfer module applies an operation that is 
aimed at building from the units in T, the discourse 
tree Tt. In the context of our discourse-transfer mod- 
ule, we need 7 types of operations: 
? SHIFT operations transfer the first edt from 
the input list into the stack; 
? REDUCE operations pop the two discourse 
trees located at the top of the stack; combine 
them into a new tree updating the statuses 
and rhetorical relation names of the trees in- 
volved in the operation; and push the new 
tree on the top of the stack. These opera- 
tions are used to build the structure of the 
discourse tree in the target language. 
? BREAK operations are used in order to break 
the edt at the beginning of the input list into 
a predetermined number of units. These op- 
erations are used to ensure that the result- 
ing tree has the same number of edts as Tt. 
A BREAK operation is necessary whenever a 
Japanese edu is mapped into nmltiple English 
units. 
? CREATE-NEXT operations are used in order 
to create English discourse constituents that 
have no correspondent in the Japanese tree. 
? FUSE operations are used in order to fuse the 
edt at the top of the stack into the tree that 
immediately precedes it. These operations 
are used whenever multiple Japanese edus are 
mapped into one English edu. 
? SWAP operations wap the edt at the begin- 
ning of the input list with an edt found one 
or more positions to the right. These oper- 
ations are necessary for re-ordering discourse 
constituents. 
? ASSIGNTYPE operations assign one or more of 
the following types to the tree at the top of 
the stack: Unit, MultiUnit, Sentence, Para- 
graph, MultiParagraph, and Text. These op- 
13
erations are necessary in order to ensure sen- 
tence and paragraph boundaries that are spe- 
cific to the target language. 
For example, the first sentence of the English tree in 
Figure 1 can be obtained from the original Japanese 
sequence by following the sequence of actions (5), 
whose effects are shown in Figure 2. For the purpose 
of compactness, the figure does not illustrate the ef- 
fect of ASSIGNTYPE actions. For the same purpose, 
some lines correspond to more than one action, 
BREAK 2; SWAP 2; SHIFT; ASSIGNTYPE 
UNIT; SHIFT; REDUCE-NS-ELABORATION- 
OBJECT-ATTRIBUTE-E; ASSIGNTYPE 
MULTIUNIT; SHIFT; ASSIGNTYPE UNIT; 
SHIFT; ASSIGNTYPE UNIT; FUSE; 
ASSIGNTYPE UNIT; SWAP 2; SHIFT; 
ASSIGNTYPE UNIT; FUSE; BREAK 2; (5) 
SHIFT; ASSIGNTYPE UNIT; SHIFT; 
ASSIGNTYPE UNIT; REDUCE-NS- 
ELABORATION-ADDITIONAL; ASSIGNTYPE 
MULTIUNIT; REDUCE-NS-CONTRAST; 
ASSIGNTYPE MULTIUNIT; REDUCE-SN- 
BACKGROUND; ASSIGNTYPE SENTENCE. 
For our corpus, in order to enable a discourse- 
based transfer module to derive any English dis- 
course tree starting from any Japanese discourse 
tree, it is sufficient o implement: 
* one SHIFT operation; 
? 3 x 2 ? 85 REDUCE operations; (For each 
of the three possible pairs of nuclear- 
ity assignments NUCLEUS-SATELLITE (NS), 
SATELLITE-NUCLEUS (SN), AND NUCLEUS- 
NUCLEUS (NN), there are two possible ways 
to reduce two adjacent rees (one results 
in a binary tree, the other in a non-binary 
tree (Marcu, 1999)), and 85 relation names.) 
? three types of BREAK operations; (In our cor- 
pus, a Japanese unit is broken into two, three, 
or at most four units.) 
? one type of CREATE-NEXT operation; 
? one type of FUSE operation; 
? eleven types of SWAP operations; (In our 
corpus, Japanese units are at most l l posi- 
tions away from their location in an English- 
specific rendering.) 
? seven types of ASSIGN~\]~YPE operations: Unit, 
MultiUnit, Sentence, MultiSentence, Para- 
graph, MultiParagraph, and Text. 
These actions are sufficient for rewriting any tree 
Ts into any tree Tt, where Tt may have a different 
number of edus, where the edus of Tt may have a 
different ordering than the edus of Ts, and where 
the hierarchical structures of the two trees may be 
different as well. 
3.3 Learn ing  the  parameters  o f  the  
d i scourse - t rans fer  mode l  
We associate with each configuration of our trans- 
fer model a learning case. The cases were gener- 
ated by a program that automatically derived the 
sequence of actions that mapped the Japanese trees 
in our corpus into the sibling English trees, using the 
correspondences at the elementary unit level that 
were constructed manually. Overall, the 40 pairs of 
Japanese and English discourse trees yielded 14108 
cases. 
To each learning example, we associated a set of 
features from the following classes: 
Operat iona l  and  d iscourse  features  reflect the 
number of trees in the stack, the input list, 
and the types of the last five operations. 
They encode information pertaining to the 
types of the partial trees built up to a certain 
t ime and the rhetorical relations that hold be- 
tween these trees. 
Cor respondence-based  features  reflect the nu- 
clearity, rhetorical relations, and types of 
the Japanese trees that correspond to the 
English-like partial trees derived up to a given 
time. 
Lex ica l features  specify whether the Japanese 
spans that correspond to the structures de- 
rived up to a given time use potential dis- 
course markers, such as dakara (because) and 
no ni (although). 
The discourse transfer module uses the C4.5 pro- 
gram (Quinlan, 1993) in order to learn decision trees 
and rules that specify how Japanese discourse trees 
should be mapped into English-like trees. A ten-fold 
cross-validation evaluation of the classifier yielded an 
accuracy of 70.2% (+ 0.21). 
In order to better understand the strengths and 
weaknesses of the classifier, we also attempted to 
break the problem into smaller components. Hence, 
instead of learning all actions at once, we attempted 
to learn first whether the rewriting procedure should 
choose a SHIFT, REDUCE, BREAK, FUSE, SWAP, or 
ASSIGNTYPE operation (the "Main Action Type" 
classifier in table 3), and only then to refine this 
decision by determining what type of reduce opera- 
tion to perform, how many units to break a Japanese 
units into, how big the distance to the SWAP-ed unit 
should be, and what type of ASSIGNTYPE operation 
one should perform. Table 3 shows the sizes of each 
14
STACK 
2 
2 1" 
\[~A BORATION_(IB~TI- 
2 l "  
}~1 ABORATION_O~E_E - -  
2 I "  1' 3 
. . . . . . . . . . . . . . .  
2 1'* 1",3 
EI.ABOP, AT ION~(~E_E  
2 1" I', 3,5 
IiLA B( )RA T ION_(~T 'E_ I~ 
2 1" I', 3, 5 4' 4'" 
HI ABOl,b%TIONIOBIE~:'r A'rrRIBUTE\],3,~ .LAB(~T ON-ADDIT 
2 1"" 4" 4" 
2 1" 1 ", 3, 5 ~ A ~  \[: 
BACKGROUND 
2 1"" . - FJ.ABOIIATION-ADD\[1 r. 3 ,y -~ 
4" 4" 
INPUT LIST 
1 2 3 4 5 6 7 
1" 1'" 2 3 4 5 6 7 
2 I"  1' 3 4 5 
I"  I' 3 4 5 6 
1" 3 4 5 6 7 
1' 3 4 5 6 7 
4 5 6 7 
4 5 6 7 
4 6 7 
6 7 
c)N~ 
6 
"IONAL 
6 
)N~J. 
6 
7 
7 
BREAK 2 
SWAP 2 
SHIFT 
SHIFT 
REDUCE-NS-ELABORATION-OBJECT-ATTRIBUTE-E 
SHIFT; SHIFT 
FUSE 
SWAP 2; SHIFT; FUSE 
BREAK 2; SHIFT; SHIFT 
REDUCE-NS-ELABORATION-ADDITIONAL 
REDUCE-NN-CONTRAST 
REDUCE-SN-BACKGROUND 
ASSIGNTYPE SENTENCE 
Figure 2: Example of incremental tree reconstruction. 
data set and the performance of each of these classi- 
tiers, as determined using a ten-fold cross-validation 
procedure. For the purpose of comparison, each clas- 
sifier is paired with a majority baseline. 
The results in Table 3 show that the most diffi- 
cult subtasks to learn are that of determining the 
number of units a Japanese unit should be broken 
into and that of determining the distance to the unit 
that is to be swapped. The features we used are 
not able to refine the baseline classifiers for these 
action types. The confusion matrix for the "Main 
Action Type" classifier (see Table 5) shows that the 
system has trouble mostly identifying BREAK and 
CREATE-NEXT actions. The system has difficulty 
learning what type of nuclearity ordering to pre- 
fer (the "Nuclearity-Reduce" classifier) and what re- 
lation to choose for the English-like structure (the 
"Relation-Reduce" classifier). 
Figure 3 shows a typical learning curve, the one 
that corresponds to the "Reduce Relation" classifier. 
Our learning curves suggest hat more training data 
may improve performance. However, they also sug- 
gest that better features may be needed in order to 
improve performance significantly. 
Table 4 displays ome learned rules. The first rule 
accounts for rhetorical mappings in which the or- 
der of the nucleus and satellite of an ATTRIBUTION 
relation is changed when translated from Japanese 
into English. The second rule was learned in order 
to map EXAMPLE Japanese satellites into EVIDENCE 
English satellites. 
1R 15
Classifier @ cases 
General 
(Learns all classes at once) 
Main Action Type 
AssignType 
Break 
Nuclearity-Reduce 
Relation-Reduce 
Swap 
14108 
14108 
6416 
394 
2388 
2388 
842 
Accuracy (10-fold cross validation) 
70.20% (+0.21) 
82.53% (?0.25) 
90.46% (?0.39) 
82.91% (?1.40) 
67.43% (?1.03) 
48.20% (?i.01) 
62.98% (?1.62) 
Majority baseline accuracy 
22.05% (on ASSIGNTYPE UNIT) 
45.47% (on ASSIGNTYPE) 
57.30% (on ASSIGNTYPE Unit) 
82.91% (on BREAK 2) 
50.92% (on KS) 
17.18% (on ELABORATION- 
OBJECT-ATTRIBUTE-E)  
62.98% (on SWAP 1) 
Table 3: Performance of the classifiers 
~oo 
440o 
~oo 
38 oo 
~oa 
I I I I 
RtlauoaRcaa? e 
tC~ xlO 3 
Figure 3: Learning curve for the Relation-Reduce 
classifier. 
if rhetRelOfStack-llnJapTree = ATTRIBUTION 
then rhetRelOffFopStacklnEngTree ~ ATTRIBUTION 
if rhetRelOffFopStacklnJapTree ---- EXAMPLE A 
isSentenceTheLastUnitlnJapTreeOfropStack = f lse
then rhetRelOfI'opStackInEngTree ~ EVIDENCE 
Table 4: Rule examples for the Relation-Reduce 
classifier. 
4 Eva luat ion  o f  the  d i scourse -based  
t rans fer  modu le  
By applying the General classifier or the other six 
classifiers successively, one can map any Japanese 
discourse tree into a tree whose structure comes 
closer to the natural rendering of English. To evalu- 
ate the discourse-based transfer module, we carried 
out a ten-fold cross-validation experiment. That is, 
we trained the classifiers on 36 pairs of manually 
built and aligned discourse structures, and we then 
used the learned classifiers in order to map 4 un- 
seen Japanese discourse trees into English-like trees. 
We measured the similarity of the derived trees with 
the English trees built manually, using the metrics 
discussed in Section 2. We repeated the procedure 
ten times, each time training and testing on different 
subsets of tree pairs. 
Act ion  (a) (b) (c) (d) (e) (f) (g) 
ASSIGNTYPE (a) 660 
BREAK (b) 1 2 28 1 
CREATE-NEXT (C) I S 
FUSE (d) 69 8 3 
REDUCE (e) 4 18 193 30 3 
SHIFT (f) 1 4 15 44 243 25 
.SWAP (g) 3 4 14 43 25 
Table 5: Confusion matrix for the Main Action Type 
classifier. 
We take the results reported in Table 2 as a base- 
line for our model. The baseline corresponds to ap- 
plying no knowledge of discourse. Table 6 displays 
the absolute improvement (in percentage points) in 
recall and precision figures obtained when the Gen- 
eral classifier was used to map Japanese trees into 
English-looking trees. The General classifier yielded 
the best results. The results in Table 6 are averaged 
over a ten-fold cross-validation experiment. 
The results in Table 6 show that our model 
outperforms the baseline with respect to building 
English-like discourse structures for sentences, but 
it under-performs the baseline with respect o build- 
ing English-like structures at the paragraph and text 
levels. The main shortcoming of our model seems to 
come from its low performance in assigning para- 
graph boundaries. Because our classifier does not 
learn correctly which spans to consider paragraphs 
and which spans not, the recall and precision results 
at the paragraph and text levels are negatively af- 
fected. The poorer esults at the paragraph and text 
levels can be also explained by errors whose effect cu- 
mulates during the step-by-step tree-reconstruction 
procedure; and by the fact that, for these levels, 
there is less data to learn from. 
However, if one ignores the sentence and para- 
graph boundaries and evaluates the discourse struc- 
tures overall, one can see that our model outper- 
forms the baseline on all accounts according to 
the Position-Dependent evaluation; outperforms the 
baseline with respect to the assignment of elemen- 
tary units, hierarchical spans, and nuclearity sta- 
tuses according to the Position-Independent evalu- 
ation and under-performs the baseline only slightly 
16 16
Level Units 
P-D R P-D P 
Spans 
P-D R P-D P 
Status/Nuclearity 
P-D R P-D P 
Relations 
P-D R P-D P 
Sentence +9.1 +25.5 +2.0 +19.9 +0.4 +13.4 -0.01 +8.4 
Paragraph -14.7 +1.4 -12.5 -1.7 -11.0 -2.4 -9.9 -3.3 
Text -9.6 -13.5 -7.1 -11.1 -6.3 -10.0 -5.2 -8.8 
Weighted Average +1.5 +14.1 -2.1 +9.9 -3.1 +6.4 -3.0 +3.9 
All -1.2 +2.5 -0.1 +2.9 +0.6 +3.5 +0.7 +2.6 
P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I P 
Sentence +13.4 +30.4 +3.1 +36.1 -6.3 +18.6 -10.1 +3.9 
Paragraph -15.6 +0.6 -13.5 -0.8 -11.7 -1.8 -10.3 -2.8 
Text -15.4 -23.3 -13.0 -20.4 -13.2 -19.5 -11.5 -17.0 
Weighted Average +3.6 +15.5 -2.7 +17.1 -8.5 +7.3 -10.5 -0.4 
All +12.7 +29.6 +2.0 +28.8 -5.1 +13.0 -7.9 +2.2 
Table 6: Relative evaluation of the discourse-based transfer module with respect o the figures in Table 2. 
with respect o the rhetorical relation assignment 
according to the Position-Independent evaluation. 
More sophisticated discourse features, such as those 
discussed by Maynard (1998), for example, and a 
tighter integration with the lexicogrammar of the 
two languages may yield better cues for learning 
discourse-based translation models. 
5 Conc lus ion  
We presented a systematic empirical study of the 
role of discourse structure in MT. Our study strongly 
supports the need for enriching MT systems with 
a discourse module, capable of re-ordering and re- 
packaging the information i  a source text in a way 
that is consistent with the discourse rendering of a 
target language. We presented an extended shift- 
reduce parsing model that can be used to map dis- 
course trees specific to a source language into dis- 
course trees specific to a target language. Our model 
outperforms a baseline with respect o its ability to 
predict the discourse structure of sentences. Our 
model also outperforms the baseline with respect 
to its ability to derive discourse structures that are 
closer to the natural, rhetorical rendering in a tar- 
get language than the original discourse structures 
in the source language. Our model is still unable to 
determine correctly how to re-package sentences into 
paragraphs; a better understanding of the notion of 
"paragraph" is required in order to improve this. 
Re ferences  
William A. Gale and Kenneth W. Church. 1993. A 
program for aligning sentences in bilingual cor- 
pora. Computational Linguistics, 19(1):75-102. 
Ulf Hermjakob and Raymond J. Mooney. 1997. 
Learning parse and translation decisions from ex- 
amples with rich context. In Proc. of ACL'97, 
pages 482-489, Madrid, Spain.. 
Sadao Kurohashi and Makoto Nagao. 1994. Auto- 
matic detection of discourse structure by check- 
ing surface information in sentences. In Proc. of 
COLING'94, volume 2, pages 1123-1127, Kyoto, 
Japan. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proc. of A CL '95, pages 
276-283, Cambridge, Massachusetts. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243-281. 
Daniel Marcu. 1999. A decision-based approach to 
rhetorical parsing. In Proc. of A CL'99, pages 365- 
372, Maryland. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments inconstructing a cor- 
pus of discourse trees. In Proc. of the A CL'99 
Workshop on Standards and Tools for Discourse 
Tagging, pages 48-57, Maryland. 
Senko K. Maynard. 1998. Principles of Japanese 
Discourse: A Handbook. Cambridge Univ. Press. 
J. Ross Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann Publishers. 
Sidney Siegel and N.J. Castellan. 1988. Non- 
parametric Statistics for the Behavioral Sciences. 
McGraw-Hill, Second edition. 
Kazuo Sumita, Kenji Ono, T. Chino, Teruhiko 
Ukita, and Shin'ya Amano. 1992. A discourse 
structure analyzer for Japanese text. In Proceed- 
ings of the International Conference on Fifth Gen- 
eration Computer Systems, v 2, pages 1133-1140. 
J. White and T. O'Connell. 1994. Evaluation in 
the ARPA machine-translation program: 1993 
methodology. In Proceedings ofthe ARPA Human 
Language Technology Workshop, pages 135-140, 
Washington, D.C. 
17 17
An Empirical Investigation of the Relation Between Discourse Structure and 
Co-Reference 
Dan Cristea 
Department of Computer Science 
University "A.I. Cuza" 
Ia~i, Romania 
dcristea @ infoiasi, m 
Daniel Marcu 
In fo rmat ion  Sc iences  Inst itute and 
Depar tment  of  Computer  Sc ience 
Univers i ty  of  Southern Cal i forn ia  
Los  Angeles ,  CA, USA 
ma twig @ isi. edu 
Nancy Ide 
Department ofComputer Science 
Vassar College 
Poughkeepsie, NY, USA 
ide @ cs. vassal: edu 
Valentin Tablan* 
Department of Computer Science 
University of Sheffield 
United Kingdom 
v. tablan @ sheJ.'field, ac. uk 
Abstract 
We compare the potential of two classes el' linear and hi- 
erarchical models of discourse to determine co-reference 
links and resolve anaphors. The comparison uses a co l  
pus of thirty texts, which were manually annotated for 
co-reference and discourse structure. 
1 Introduction 
Most current anaphora resolution systems implelnent a
pipeline architecture with three modules (Lappin and Le- 
ass, 1994; Mitkov, 1997; Kameyama, 1997). 
1. A COLLF.CT module determines a list of potential 
antecedents (LPA) for each anaphor (l~ronourl, deli- 
nile noun, proper name, etc.) that have the potential 
to resolve it. 
2. A FILTI~,P, module eliminates referees incompatible 
with the anaphor fi'om the LPA. 
3. A PP, EFERENCE module determines the most likely 
antecedent on the basis of an ordering policy. 
In most cases, the COLLECT module determines an LPA 
by enumerating all antecedents in a window o1' text that 
precedes the anaphor under scrutiny (Hobbs, 1978; Lap- 
pin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; 
Ge et al, 1998). This window can be as small as two 
or three sentences or as large as the entire preceding 
text. The FILTEP, module usually imposes emantic on- 
straints by requiring that the anaphor and potential an- 
tecedents have the same number and gendm; that selec- 
tional restrictions are obeyed, etc. The PREFERENCE 
module imposes preferences on potential antecedents 
on the basis of their grammatical roles, parallelism, 
fi'equency, proximity, etc. In some cases, anaphora 
resolution systems implement hese modules explic- 
itly (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 
* On leave fi'om lhe Faculty of Computer Science, University "AI. I. 
Cuza" of lasi. 
1997; Kameyama, 1997). In other cases, these modules 
are integrated by means of statistical (Ge et al, 1998) or 
uncertainty reasoning teclmiques (Mitkov, 1997). 
The fact that current anaphora resolution systems rely 
exclusively on the linear nature of texts in order to de- 
termine the LPA of an anaphor seems odd, given flint 
several studies have claimed that there is a strong rela- 
tion between discourse structure and reference (Sidner, 
1981 ; Grosz and Sidner, 1986; Grosz et al, 1995; Fox, 
1987; Vonk ct al., 1992; Azzam el al., 1998; Hitzcman 
and Pocsio, 1998). These studies claim, on the one hand, 
that the use of referents in naturally occurring texts im- 
poses constraints on the interpretation f discourse; and, 
on the other, that the structure of discourse constrains the 
LPAs to which anaphors can be resolved. The oddness 
of the situation can be explained by lho fac! that both 
groups seem primafilcie m be right. Empirical exper- 
iments studies that employ linear techniques for deter- 
mining the LPAs o1' almphol's report recall and precision 
anaphora resolution results in the range of 80% (Lappin 
and Leass, 1994; Ge ct al., 1998). Empirical experiments 
that investigated the relation between discourse structure 
and reference also claim that by exploiting the structure 
of discourse one has the potential of determining correct 
co-referential links for more than 80% of the referential 
expressions (Fox, 1987; Cristca et al, 1998) although to 
date, no discourse-based anaphora resolution system has 
been implemented. Since no direct comparison of these 
two classes of approaches has been made, it is difficult to 
determine which group is right, and what method is the 
best. 
In this paper, we attempt o Iill this gap by empiri- 
cally comparing the potential of linear and hierarchical 
models el' discourse to correctly establish co-referential 
links in texts, and hence, their potential to correctly re- 
solve anaphors. Since it is likely that both linear- and 
discourse-based anaphora resolution systems can imple- 
ment similar FILTER and PI~,I~FERENCE strategies, we fo- 
cus here only on the strategies that can be used to COL- 
208 
LECT lists of potential antecedents. Specilically, we fo- 
ct, s on deterlnining whether discourse llteories can help 
an anaphora resolution system detemfine LPAs that are 
"better" than the LPAs that can be contputed from a lin- 
ear interpretation f texts. Section 2 outlines the theoreti- 
cal assumptions of otu" empirical investigation. Section 3 
describes our experiment. We conclude with a discussion 
of tile results. 
2 Background 
2.1 Assumptions 
Our approach is based on the following assmnptions: 
I. For each anaphor in a text, an anaphora resolution 
system must produce an LPA that contains a refer- 
ent to which 111e anaphor can be resolvcd. The size 
of this LPA wuies fronl system to system, depend- 
ing on tile theory a system implements. 
2. The smaller the I,PA (while retaining a correct an- 
tecedent), the less likely that errors ill the \]7tI,TH{ 
:(lid PI',V,I;F, RI;,NCI ~, modules wil l  affect the ability of 
a system to select the appropriate referent. 
3. Theory A is better than lheory B for the task of rel- 
erence resolution if theory A produces I J 'As that 
contain more antecedents o which amtphors can be 
corrcclly resolved than theory B, and if the l,l~As 
produced by theory A arc smaller than those pro- 
duccd by theory B. l;or cxaml)lc, if for a given 
anaphor, theory A produces an I,PA thai contains a 
referee to which the anaphor can be resolved, while 
theory B produces an IJ~A that does not contain 
such a re\[eree, theory A is better than theory B. 
Moreover, if for a given anaphor, theory A produces 
an Lt)A wilh two referees and theory B produces an 
LPA with seven rel'crees (each LPA containing a ref- 
eree to which tile anal)her can be resolved), lheory 
A is considered better than theory 11 because it has a 
higher probability of solving that anaphor correctly. 
We consider two classes of models for determining the 
LPAs of anaphors ill a text: 
Linear-k models. This is at class of linear models in 
which the LPAs include all the references foulad in the 
discourse unit under scrutiny and the k discourse units 
that immediately precede it. Linear-0 models an ap- 
proach that assumes that :tll anaphors can be resolved 
intra-unit; Linear- 1 models an approach that cor,'esponds 
roughly to centering (Grosz et al, 1995). Linear-k is con- 
sistent with the asslunl)tions that underlie most current 
anaphora resohltion systems, which look back h units in 
order to resolve an anaphor. 
l) iscourse-V1:k models. In |his class ()1'models, LPAs 
include all lhe refcrentM expressions fotmd in the dis- 
course unit under scrutiny and the k discourse units that 
hierarchically precede it. The units that hierarchically 
precede a given unit are determined according to Veins 
Theory (VT) (Cristea et al, 1998), which is described 
brielly below. 
2.2 Veins Theory 
VT extends and formalizes the relation between dis- 
course  s t ruc ture  and reference proposed by Fox (1987). 
It identilies "veins", i.e., chains of elementary discourse 
units, over discourse structure trees that are built accord- 
ing to the requirements put forth in Rhetorical Structure 
Theo,y (RST) (Mann and Thompson, 1988). 
One of the conjectures ()1' VT is that the vein expres- 
sion of an elementary discourse unit provides a coher- 
ent "abstract" of the discourse fi'agmcnt hat contains 
that unit. As an internally coherent discottrse fragment, 
most ()1' the anaphors and referentM expressions (REst 
in a unit must be resolved to referees that oceul" in the 
text subs:used by the units in tile vein. This conjec- 
ture is consistent with Fox's view (1987) that the units 
that contain referees to which anaphors can be resolved 
are determined by the nuclearity of the discourse units 
thal precede the anaphors and the overall structure of dis- 
course. According to V'I; REs of both satellites and nu- 
clei can access referees of hierarchically preceding nt,- 
cleus nodes. REs of nuclei can mainly access referees of 
preceding nuclei nodes and of directly subordinated, pre- 
ceding satellite nodes. And the interposition ()1' a nucleus 
after a satellite blocks tim accessibility of the satellite for 
all nodes that att'e lovcer in the corresponding discourse 
structure (see (Cristea et al, 1998) for a full delinition). 
Hence, the fundamental intuition unde,lying VT is 
that the RST-spceilie distinction between nuclei and 
satellites constrains the range of referents to which 
anaphors can 19e resolved; in other words, the nucleus- 
satellite distinction induces for each anaphor (and each 
referential expression) a Do,naita of Refcrenlial Acces- 
sibility (DRA). For each anaphor a in a discourse unit 
~z, VT hypothesizes that a can be resolved by examin- 
ing referential expressions that were used in a subset o1' 
the discourse units that precede it; this subset is called 
the DRA of u. For any elcntentary unit u in a text, the 
corresponding DRA is computed autonmtically from the 
rhetorical representation f that text in two steps: 
1. lteads for each node are computed bottom-up over 
the rhetorical representation tree. Heads ()1" elemen- 
tary discottrse traits are the traits themselves. Heads 
of internal nodes, i.e., discourse spans, are con> 
pt, ted by taking the union of the heads of the im- 
mediate child nodes that :ire nuclei. For example, 
for the text in Figure I, whose rhetorical structure is 
shown in Figure 2, the head ()1' span 115,711 is unit 5 
because the head ()t' the inmmdiate nucleus, the ele- 
mentary unit 5, is 5. However, the head of span 116,7\] 
is the list (6,7) because both immediate children are 
nuclei of a multinuclear relation. 
2. Using the results of step 1, Vein expressions are 
eOmlmted top-down lbr each node in the tree. The 
vein of the root is its head. Veins of child nodes 
209 
i. \[Michael D. Casey,\[a top Johnson&Johnson 
...... get, moved teCGe~ Therapy In~,  
a small biotechnology concern here, 
2. to become_~_t>president and ch ieC  
operating officer. \[ 
3. \[Mr. Casey, 46 years old,\] was\[ president of 
J&J's McNeil Pharmaceutical subsidiary,\] 
*t. which was merged with another J&J unit, 
Ortho Pharmaceutical Corp., this year in 
a cost-cutting move. 
5. I,Ir. Casev\[ succeeds M. James Barrett, 50, 
as\[president of ~,~netic Ther-ap~. 
6. Mr. Barrett remains chief executive officer 
7. and becomes chainaan. 
8. \[Mr-\] r. Casey\] said 
9. ~made the move te {\]{e smaller compan~ 
i0. because~saw health care moving toward 
technologies like 
products. 
ll. Ube l ieve  that the field is emerging and i~ 
prepared to break loose, 
12.\[he\[said. 
Figure 1: An example of text and its elementary units. 
The |'eferential expressions surrounded by boxes and el- 
lipses correspond to two distinct co-referential equiv- 
alence classes. Referential expressions urrounded by 
boxes refer to Mr: Casey; those surrounded by ellipses 
refer to Genetic Thercq~y Inc. 
are computed recursively according to tile rules de- 
scribed by Cristea et al(1998). The DRA ot" a unit u 
is given by the units that precede u in 1t~e vein. 
For example, for the text and RST tree in Figures 1 
and 2, the vein expression of unit 3, which contains 
units 1 and 3, suggests that anaphors from unit 3 
should be resolved only to referential expressions 
in units 1 and 3. Because unit 2 is a satellite to 
unit 1, it is considered to be "blocked" to referen- 
tial links fi'om trait 3. In contrast, tile DRA of unit 
9, consisting o1' units 1, 8, and 9, reflects the intu- 
ition that anaphors l?om unit 9 can be resolved only 
to referential ext)ressions fi'om unit 1, which is the 
most important trait in span \[1,7\], and to unit 8, a 
satellite that immediately precedes unit 9. Figure 2 
shows the heads and veins of all internal nodes in 
the rhetorical representation. 
2.3 Comparing models 
The premise underlying out" experiment is that there are 
potentially significant differences in the size of the search 
space rcquired to resolve referential cxpressions when 
using Linear models vs. Discou|'se-VT models. For ex- 
ample, for text and tile RST tree in Figures 1 and 2, the 
Discourse-VT model narrows tlle search space required 
to resolve the a|mphor the smaller company in unit 9. 
According to VT, we look lbr potential antecede|Us for 
the smaller company in the DRA of unit 9, which lists 
units I, 8, and 9. The antecedent Genetic Therapy Inc. 
appears in unit 1 ; therefore, using VT we search back 2 
units (units 8 and 1) to lind a correct antecedent. In con- 
trast, to resolve the same reference using a linear model, 
four units (units 8, 7, 6, and 5) must he examined be- 
fore Genetic The;zq?y is found. Assuming that referen- 
tial links are established as tile text is processed, Genetic 
Therapy would be linked back to pronottn its in unit 2, 
which would in turn be linked to the first occurrence of 
the antecedent,Genetic Therapy Inc., in unit 1, tile an- 
tecedent determined irectly by using V'E 
In general, when hierarchical adjacency is considered, 
an anaphor may be resolved to a referent hat is not the 
closest in a linear interpretation of a text. Simihu'ly, aref- 
erential expression can be linked to a referee that is not 
the closest in a linear interpretation of a text. However, 
this does not create problems because we are focusing 
here only on co-referential relations of identity (see sec- 
tion 3). Since these relations induce equivalence classes 
over tile set of referential expressions in a text, it is suf\[i- 
cient that an anaphor or referential expression is resolved 
to any of the members of the relevant equiw|lence class. 
For example, according to V'I, the referential expression 
MI: Casey in unit 5 in Figm'e 1 can be linked directly 
only to the referee Mr Casey in unit 1, because the DRA 
o1' unit 5 is { 1,5}. By considering the co-referential links 
of the REs in the other units, tile full equivalence class 
can be determined. This is consistent with tile distinction 
between "direct" and "indirect" references discussed by 
Cristea, et al(1998). 
3 The Experiment 
3.1 Materials 
We used thirty newspaper texts whose lengths varied 
widely; the mean o- is 408 words and tile standard e- 
viation/t is 376. Tile texts were annotated manually for 
co-reference r lations of identity (Hirschman and Chin- 
chef, 1997). Tile co-reference relations define equiv- 
alence classes oil the set of all marked referents in a 
text. Tile texts were also manually annotated by Marcu 
et al (1999) with disconrse structures built in the style 
of Mann and Thompson (1988). Each discourse analy- 
sis yielded an average of 52 elementary discourse units. 
See (Hirschman and Chinchor, 1997) and (Marcu et al, 
1999) for details of tile annotation processes. 
210 
H = 1 9 * 
V=lg*  
t I= l  
"?r ... = 1 9 * . . . . . . .  
H=I  L -  - - -  V=lg*  _ . . . . . . . . . .  
H= i |  ~{:-.,, - 
V 1 9 :+:~f- . . . . .  ~'}'-~%l. 3 5 9 * 
1 2 3 4 
H=3 
V=1359 
DF~,= 1 3 
___ - - - - -  - - ~  
_ _ - - -  - - - ___  
I - _  I 
H=5 
- - - - - __  _ 
';,~ = 1 59* 
q _}L___= 6 7 
._~-"-"- \; =xt,,5 67 9 * 
5 //%,,\ 
6 7 
H=9 
"?" = 1 9 * 
I H=9 i . . . .  i la---- . . . . . . . . . . . .  
m 
V= 1 9"  ~ ~---- . . . .  -- i 21-25 
\ [ -  
13-213 
9 191011*  
\" = 1 (g)9  
DRA = 1 11 12 
Figure 2: The I),ST analysis of the text in ligure I. The trcc is rcprescnted using the conventions proposed by Mann 
and Thompson (1988). 
3.2 Compar ing  potent ia l  to es tab l i sh  co - re ferent ia l  
l inks  
3.2.1 Method  
The annotations for co-reference rchttions and rhetorical 
struclure trues for the thirty texts were fused, yielding 
representations that rcllect not  only tile discourse strut- 
lure, but also the co-reference quivalencc lasses spe- 
citic to each tcxl. Based on this information, we cval- 
ualed the potential of each of the two classes (51" mod- 
els discussed in secdon 2 (Linear-k and Discourse-VT-k) 
to correctly establish co-referential links as follows: For 
each model, each k, and each marked referential expres- 
sion o., we determined whether or not tlle corresponding 
LPA (delined over k elementary units) contained a ref- 
eree from the same equiwdence class. For example, for 
the Linear-2 model and referential expression lhe .vmaller 
company in t, nit 9, we estimated whether a co-refercntial 
link could be established between the smaller company 
and another referential expression in units 7, 8, or 9. 
For the Discourse-VT-2 model and the same referential 
expression, we estimated whether a co-referential link 
could bE established between the smaller company and 
another eferential expression in units 1, 8, or 9, which 
correspond to the DRA of unit 9. 
qb enable a fair comparison of the two models, when k 
is la,'ger than the size of the DRA of a given unit, WE ex- 
tend thatDRA using the closest units that precede the unit 
under scrutiny and are not ah'eady in the DRA. Hence, 
for the Linear-3 model and the referential expression the 
smaller conq~any in trait 9, we estimate whether a co- 
referential link can be established between the xmaller 
company and another eferential expression in units 9, 8, 
7, or 6. For tile Discourse-VT-3 model and tile same rcf- 
ermltial expression, we estimate whclher a co-referential 
link can be established between the smaller company and 
another eferential expression in units 9, 8, 1, or 7, which 
correspond I(5 the DRA of mill 9 (unfls 9, 8, and 1) and to 
unit 7, the closest unit preceding unit 9 that is not  ill ils 
I)RA. 
For the l)iscottrse-VT-k models, we assume Ihat the 
Fxtended DRA (EDRA) of size \]c of a unit ~t. (EDRAz: ('~)) 
is given by the lh'st 1 _< k units of a sequence that 
lists, in reverse order, the units of the DRA of '~z plus 
the /c - l units that precede tt but arc not in its DRA. 
For example, \['or the text in Figure 1, the follow- 
ing relations hold: EDRAc,(!)) = 9; EDRAI(C.)) = 
9, 8; EDRA.,(9) = .q,8, I; EI)RA3(9) := 9 ,8 ,1 ,7 ;  
I'~DRA.,I(!)) = !),8, 1,7,6. For Linear-k inodels, the 
EDRAz:(u) is given by u and the k units that immedi- 
ately precede ~t. 
The potential p(M,  a, EDRA,~) (5t' a model M to de- 
termine correct co-referential links with respect o a ref- 
Erential expression a in unit u, given a corresponding 
EDRA of size k (EDRAt.(u)), is assigned the value 1 if 
the EDRA contains a co-referent from the same equiwt- 
lence class as a. Otherwise, p(M, ,, EDRAt~) is assigned 
the value O. The potential p(k4, 6', k) of a model M 
to determine correct co-rEferential links for all referen- 
tial expressions in a corpus of texts C, using EDRAs 
of size k, is computed as the SUlll oF the potentials 
p(M, a.,EI)RA#) of  all referential expressions ct in C'. 
This potential is normalized to a wdue bEtweEn 0 and 
1 by dividing p(k/l, C, k) by the number ot' referential 
211 
expressions in the corpus that have an antecedent. 
By examining the potential of each model to correctly 
determine co-referential expressions for each k, it is pos- 
sible to determine the degree to which an implementa- 
tion of a given approach can contribute to the overall 
efficiency of anaphora resolution systems. That is, if a 
given model has the potential to correctly determine a
significant percentage of co-referential expressions with 
small DRAs, an anaphora resolution system implement- 
ing that model will have to consider fewer options over- 
all. Hence, the probability of error is reduced. 
3.2.2 Results 
The graph in Figure 3 shows the potentials of the Linear- 
k and Discourse-VT-k models to correctly determine co- 
referential links for each k from 1 to 20. The graph in 
Figure 4 represents he same potefftials but focuses only 
on ks in the interval \[2,9\]. As these two graphs how, the 
potentials increase monotonically with k, the VT-k mod- 
els always doing better than the Linear-k models. Even- 
tually, for large ks, the potential performance of the two 
models converges to 100%. 
The graphs in Figures 3 and 4 also suggest resolution 
strategies for implemented systems. For example, the 
graphs suggests that by choosing to work with EDRAs 
of size 7, a discourse-based system has the potential of 
resolving more than 90% of the co-referential links in 
a text correctly. To achieve the same potential, a linear- 
based system needs to look back 8 units. I fa system does 
not look back at all and attempts to resolve co-referential 
links only within the unit under scrutiny (k = 0), it has 
the potential to correctly resolve about 40% of the co- 
referential links. 
To provide a clearer idea of how the two models differ, 
Figure 5 shows, for each k, the value of the Discourse- 
VT-k potentials divided by the value of the Linear-k po- 
tentials. For k = 0, the potentials of both models are 
equal because both use only the unit in focus in order to 
determine co-referential links. For k = 1, the Discourse- 
VT-1 model is about 7% better than the Linear-! model. 
As the value of k increases, the value Discourse-VT- 
k/Linear-k converges to 1. 
In Figures 6 and 7, we display the number of excep- 
tions, i.e., co-referential links that Discourse-VT-k and 
Linear-k models cannot determine correctly. As one 
can see, over the whole corpus, for each k _< 3, the 
Discourse-VT-k models have the potential to determine 
correctly about 100 more co-referential links than the 
Linear-k models. As k increases, the performance of the 
two models converges. 
3,2,3 Statistical significance 
In order to assess the statistical significance of the differ- 
ence between the potentials of the two models to estab- 
lish correct co-referential links, we carried out a Paired- 
Samples T Test for each k. In general, a Paired-Samples 
T Test checks whether the mean of casewise differences 
between two variables differs from 0. For each text in 
I O0 00% 
__~____x.~-.,:..-~'.. . . . . . .  
a0~? oo~??'~ ?- .~:..- ...... 
70.00% 
60 00% ~' 
5000% . 
40O0% ? 
o 
EDRA s i ze  
- - - -  VT-k  . . . . . . .  tmeaf .k  
Figure 3: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (0 < 
k < 20). 
9"5.00% 
90.00"/,, 
85.00% 
800O% 
75.(X~% 
70.007~ 
~ ~ . - ' " ' "  -"'"" --"" .... .. 
J 
1 2 3 4 5 6 7 9 
EDI1A Bt=~ 
? ---13---  VT-k - - . I f , , .  Linear-k 
Figure 4: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (2 < 
k _< 9). 
the corpus and each k, we determined the potentials of 
both VT-k and Linear-k models to establish correct co- 
referential links in that text. For ks smaller than 4, the 
difference in potentials was statistically significant. For 
example, for k = 3, t = 3.345, df = 29, P = 0.002. For 
values of k larger than or equal to 4, the difference was no 
longer significant. These results are consistent with the 
graphs shown in Figure 3 to 7, which all show that the 
potentials of Discourse-VT-k and Linear-k models con- 
verges to the same value as the value of k increases. 
3.3 Comparing the effort required to establish 
co-referential links 
3.3.1 Method 
The method escribed in section 3.2.1 estimates the po- 
tential of Linear-k and Discourse-VT-k models to deter- 
mine correct co-referential links by treating EDRAs as 
sets. However, from a computational perspective (and 
212 
L'07 / Q 
t .a6  '" 
. . . .  \[ / ", 
t l ' . ,  u 
0.9~ 
o.97 I 
~,oo I "' 
700 c , " ? 
"', o 
'500 " - . -  "~-~ 
400 \ ' - - ,  "'*x ? "--- 12~:~: ...... 
2OO 
~00 
2 3 4 5 6 7 t3 10 
L 
Figure 5: A direct comparison of Discourse-VT-k 
and Linear-VT-k potentials to correctly determine co- 
referential links (0 < k < 20). 
Figure 7: The number of co-referential links that caunot 
be correctly determined by Discourse-VT-k and Linear-k 
models (1 < k < 10). 
14130 
1200 
0 lOOO 
600 
401) 
200 
\ 
- - -~u ,~:  ?~:  :g~: \[t..:~g<_ .~, ~ g :~. . . t~_~.~_~_ , .=~r ,~ ~ 
EORA ~lz~ 
Figure 6: The number of co-referential links that cannot 
be correctly determined by Discourse-VT-k and Linear-k 
models (0 < /~' < 20). 
presumably, from a psycholinguistic perspective as well) 
it also makes sense to compare the effort required by the 
two classes of models to establish correct co-referential 
links. We estimate this effort using a very simple metric 
that assumes that the closer an antecedent is to a cor- 
responding referential expression in the EDRA, the bet- 
ter. Hence, in estimating the effort to establish a co- 
referential link, we treat EDRAs as ordered lists. For ex- 
ample, using the Linear-9 model, to determine the correct 
antecedent of the referential expression the smaller com- 
pany in unit 9 of Figure 1, it is necessary to search back 
through 4 units (to unit 5, which contains the referent Ge- 
netic Therapy). Had unit 5 been Ml: Cassey succeeds M. 
James Barrett, 50, we would have had to go back 8 units 
(to unit 1) in order to correctly resolve the RE the smaller 
company. In contrast, in the Discourse-VT-9 model, we 
go back only 2 units because unit 1 is two units away 
fi'om unit 9 (EDRA:~ (9) = 9, 8, 1,7, 6, 5,4, 3, 2). 
We consider that the effort e(AJ, a, EDRAa.) of a 
model M to determine correct co-referential links with 
respect o one referential, in unit u, given a correspond- 
ing EDRA of size L" (EDRA~.(,)) is given by the number 
of units between u and the first unit in EDRAk(u) that 
contains a co.-referential expression of a. 
The effort e(M, C, k) of a model M to determine cor- 
rect co-referential links for all referential expressions in 
a corpus of texts C using EDRAs of size k was computed 
as the sum of the efforts e(M, a, EDRAk) of all referen- 
tia ! expressions a in C. 
3.3.2 Results 
Figure 8 shows the Discourse-VT-k and Linear-k efforts 
computed over all referential expressions in the corpus 
and all ks. It is possible, for a given referent a and a 
given k, that no co-referential link exists in the units of 
the corresponding EDRAa.. In this case, we consider that 
the effort is equal to k. As a consequence, for small ks 
the effort required to establish co-referential links is sim- 
ilar for both theories, because both can establish only a 
limited number of links. However, as k increases, the 
effort computed over the entire corpus diverges dramat- 
ically: using the Discourse-VT model, the search space 
for co-referential links is reduced by about 800 units for a 
corpus containing roughly 1200 referential expressions. 
3.3.3 Statistical significance 
A Paired-Samples T Test was performed for each k. For 
each text in the corpus and each k, we determined the 
effort of both VT-k and Linear& models to establish cor- 
rect co-referential links in that text. For all ks the dif- 
ference in effort was statistically significant. For exam- 
ple, for k = T, we obtained the values t = 3.5l, df = 
29, P = 0.001. These results are intuitive: because 
EDRAs are treated as ordered lists and not as sets, the 
effect of the discourse structure on establishing correct 
co-referential links is not diminished as/,' increases. 
4 Conclusion 
We analyzed empirically the potentials of discourse and 
linear models of text to determine co-referential links. 
Our analysis suggests that by exploiting the hierarchi- 
cal structure of texts, one can increase the potential 
213 
7000 . . . . . . . . . .  i - ; ; ;~ : ' : - ' ; ;  . . . .  ~ . . . . .  
~ 0 0 -  
k . . . . . . . . . . . . . . . . . . . . . . . . . .  
I000 
tORA ~ z Q  
- - V T ~ e s s  . . . . . . .  ~ o s s  
Figure 8: The effort required by Linear-k and Discourse- 
VT-k models to determine correct co-referential links 
(0< h< 100). 
q 
of natural anguage systems to correctly determine co- 
referential links, which is a requirement for correctly re- 
solving anaphors. If one treats all discourse units in the 
preceding discourse qually, the increase is statistically 
significant only when a discourse-based coreference sys- 
tem looks back at most four discourse units in order to 
establish co-referential links. However, if one assumes 
that proximity plays an important role in establishing co- 
referential links and that referential expressions are more 
likely to be linked to referees that were used recently in 
discourse, the increase is statistically significant no mat- 
ter how many units a discourse-based co-reference sys~ 
tern looks back in order to establish co-referential links. 
Acknowledgements. We are grateful to Lynette 
Hirschman and Nancy Chinchor for making available 
their corpora of co-reference annotations. We are also 
grateful to Graeme Hirst for comments and feedback on 
a previous draft of this paper. 
References 
Saliha Azzam, Kevin Humphreys, and Robert 
Gaizauskas. 1998. Evaluating a focus-based ap- 
proach to anaphora resolution. In Proceedings of 
the 361h Ammal Meeting of the Associatiot~ for 
Computational Linguistics and of the 17th Inter- 
national Conference on Computational Linguistics 
(COLlNG/ACL'98), pages 74-78, Montreal, Canada, 
August 10-14. 
Dan Cristea, Nancy Ide, and Laurent Romary. 1998. 
Veins theory: A model of global discourse cohesion 
and coherence. In Proceedings of the 36th Ammal 
Meeting of the Association Jot" Computational Lin- 
guistics attd of the 17th lntertmtional Conference on 
Computational Linguistics (COLING/ACL'98), pages 
281-285, Montreal, Canada, August. 
Barbara Fox. 1987. Discourse Structure and Anaphora. 
Cambridge Studies in Linguistics; 48. Cambridge Uni- 
versity Press. 
Niyu Oe, John Hale, and Eugene Charniak. 1998. A sta- 
tistical approach to anaphora resolution. In Proceed- 
ings of the Sixth Worksho t) on Vet 3, Large Corpora, 
pages 161-170, Montreal, Canada, August 15-16. 
Barbara J. Grosz and Candace L. Sidner. 1986. At- 
tention, intentions, and the structure of discourse. 
Computational Linguistics, 12(3): 175-204, July- 
September. 
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 
1995. Centering: A framework tbr modeling the lo- 
cal coherence of discourse. Computational Linguis- 
tics, 21 (2):203-226, June. 
Lynette Hirschman and Nancy Chinchor, 1997. MUC-7 
Coreference Task Definition, July 13. 
Janet Hitzeman and Massimo Poesio. 1998. Long dis- 
tance pronominalization a d global focus. In Ptv- 
ceedings of the 36th Ammal Meeting of the Associ- 
ation for Computational Linguistics attd of the 17th 
hzternational Conference on Computational Linguis- 
tics (COLING/ACL'98), pages 550-556, Montreal, 
Canada, August. 
Jerry H. Hobbs. 1978. Resolving pronoun references. 
Lingua, 44:311-338. 
Megumi Kameyama. 1997. Recognizing referential 
links: An information extraction perspective. In Pro- 
ceedings of the ACL/EACL'97 Workshop on Opera- 
tional Factors in Practical, Robust Anaphora Resoht- 
tion, pages 46-53. 
Shalom Lappin and Herbert J. Leass. 1994, An algo- 
rithm for pronominal anaphora resolution. Computa- 
tional Linguistics, 20(4):535-561. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional the- 
ory of text organization. Text, 8(3):243-28 i. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments in constructing a cor- 
pus of discourse trees. In Ptvceedings of the ACL'99 
Worksho t)on Standards and 7bols for Discout:re Tag- 
ging, pages 48-57, University of Maryland, June 22. 
Ruslan Mitkov. 1997. Factors in anaphora resolution: 
They are not the only things that matter, a case study 
based on two different approaches. In Proceedings of 
the ACL/EACL'97 Workshop on Operational Factors 
in Practical, Robust Anaphora Resolution, pages 14- 
21. 
Candace L. Sidner. 1981. Focusing for interpretation f
pronouns. Computational Linguistics, 7(4):217-231, 
October-December. 
Wietske Vonk, Lettica G.M.M. Hustinx, and Wire tt.G. 
Simons. 1992. The use of referential expressions in 
structuring discourse, l~mguage and Cognitive Pro- 
cesses, 7(3,4):301-333. 
214 
Extending a Formal and Computational Model of Rhetorical Structure 
Theory with Intentional Structures h la Grosz and Sidner 
l )an ie l  Marcu  
I n fo rmat ion  Sc iences  Inst i tute and Depar tn lent  o f  Computer  Sc ience  
Un ivers i ty  o1' Southern  Ca l i fo rn ia  
4676 Admira l ty  Way,  Sui te 1001 
Mar ina  del Rey,  CA  90292-6601 
marcuOis  i .  edu  
Abstract 
In the last decade, members of the computational lingt, is- 
tics community have adopted a perspective on discourse 
based primarily on either Rhetorical Structure Theory or 
Grosz and Sidner's Theory. However, only recently, re.+ 
searchers have started to investigate the relationship be- 
tween the two perspectives. In this paper, we use Moscr 
and Moore's (1996) work as a departure point for extend- 
ing Marcu's formalization of RST (1996). The result is 
a tirst-order axiomatization of the mathematical prol+er- 
ties o1' text structures and of the rehttionship between the 
strttcture of text and intentions. The axiomatization en- 
ables one lo use intentions for reducing the ambiguity o1' 
discourse and the structure of discourse for deriving in- 
tentional inferences. 
I Motivation 
I n the last decade, members of the computational l inguis- 
lies cotnnmnity have adopted a perspective on discourse 
based prinlarily on either l{hetorical Structure Theory 
(P, ST) (Matin and Thompson, 1988) or Grosz and Sid 
her's Theory (GST) (Grosz and Sidnet. 1986). 
In GSq, the linguistic onstituents are called discom'xe 
segments (DS) and the lingt, istic discourse slructure is 
explicitly stipulated to be a tree o1' recursively embedded 
discourse segments. Each discourse segment is charac- 
tel+ized by a prinmry intention, which is called discomwe 
segment lmrpose (DSP). GST identilies only two kinds 
o1' intention-based relations that hold between the DSPs 
of two discourse segments: domittance and sati.@tction 
precedence. When a discourse segment purpose DSPI 
that characterizes discourse segment DS1 provides part 
of the satisfaction of a discourse segment purpose DSP., 
that characterizes discourse segment DS..,, with DS1 be- 
ing embedded in DS2, it is said that there exists a domi- 
nance relation between DSP~ and DSlq, i.e., DSP.e dom- 
inates DSpI. 1t' the salislhction of DSP, is a condition of 
the satisfaction oI'DSP2, it is said that DSP1 sati,@tction- 
precedes DSP.,. 
RST has a richer ontology of relations than GST: in- 
tentional and semantic rhetorical relations are considered 
to hold between non-overlaplfing textual spans. Most 
of these relations are asymmetric, i.e., they distinguish 
between their associated nuclei, which express what is 
most essential to the writer's purpose, and their satellites, 
which support ile nuclei. In RS'I, the linguisticdiscourse 
structure is modeled recursively as a tree of related seg- 
ments. Hence, unlike GSq, where relations are consid- 
ered to hold between the DSPs associated with embed- 
ded segments, relations in RST hold between adjacent, 
non-overlapping segments. 
Because RST has traditionally been applied to build 
discourse trees of liner granularity than GST, we will 
use it here as the starting point of our discussion. As- 
sume, for example, that we are given tim following text 
(in which the elementary textual units arc labelled lbr 
reference). 
(I) INo mat lc r  how much one wants to stay a non-smoker, ^~ \] 
\[the truth is that the pressure to smoke in junior high is 
greater than it will be any other time of one's life) q \] 
IWe know tim\[ 3,000 teens tart smoking each day, q \] lal- 
though it is a fact that 90% of them once thoughl thai 
smoking was something that Ihey'd never do. D~ I
Assume for the moment hat we do not analyze this text 
as a whole, but rather, we dctcrlnine what rhetorical rela- 
tions could hold between every pair of elementary units. 
When we apply, for example, the definitions proposed 
by Mann and Thompson (1988), we obtain the set given 
below, l 
'rh, c~_,rcl(JUSTIFICNI'ION, AraB1) 
rhcl.rrel(J USTI FICATI()N, I)1 ~ I~,1 )
(2) r/+ct_rcl(F, Vll)ENCF, (h ,  I~q) 
'rhcI._.rcl(CONCFSSION, I)i, C1 ) 
'rhet_rcI(RFSTATF.MENT, 1)1, A1) 
These relations hold because the tmdcrstanding of both 
A1 (teens want to stay non-smokers) and I:h (90% o1' the 
teens think that smoking is something that they wotdd 
never do) will increase the reader's readiness to accept 
the writer's right to present Ih (the pressure on teens to 
start smoking is greater than it will be any other time 
of their lives); the understanding of c1 (3000 teens start 
smoking each day) will increase the reader's belief of 
1~1; the recognition of Ih as something compatible with 
IThroughoul this paper, we use the convention lhat rhelori- 
cal relations are represented as stated, lirst-order predicales hav- 
ing lhe fornl rhct_rel(,act.me, mLzellite, ?~mlcus). Mullint|- 
clear relalions are represented as predicales having Ihe \['orlll 
rhct_rcl( ~m',~c, n~tcle'usl , ~uclcus~ ).
523 
AI B1 Cl Ol 
JUSIIF,CAI\[ON JUSTIFICATION JusnRcATJON 
EVIDENCE D1 JU8 TIFIOAIION Ol 
c$ B1 CONCESSION 
al B1 cI D1 nl Ci 
h) c) d) 
Figure 1 : The sot o1' all RS-trees that can be built for text (1). 
JUST\[FICAIION 
:,1 .... 
AI JUSTtFIOAItON 
gl CI 
e) 
the situation presented in c1 will increase the reader's 
negative regard for the situation presented in cl ; and the 
situation presented in D, is a restatement o1' the situation 
presented in & .  
Marcu (1996) has shown that on the basis of only the 
rhetorical judgments in (2) and without considering in- 
tentions, there are five valid RS-trces that one can build 
for text ( I ) (see figure l ). What happens though when we 
consider intentions as well? Moore and Pollack (1992) 
have already shown that different high-level intentions 
yield different RS-trces. But how do we formalize tile 
relationship between intentions and rhetorical structures? 
For example, how can we use the discourse trees in fig- 
ure 1 in order to determine the primary intention asso- 
ciated with each analysis? And how can we determine 
what would be the corresponding dominance relations in 
a GST account of tile same text'? 
Consider also a slightly difl'erent problem: assume that 
besides rhetorical judgments, such as those shown in (2), 
one can also make intentional judgments. For example, 
assume that one is interested in an interpretation i  which 
one knows that the DSP of seg,nent \[&, D1\], which con- 
tains all units from A1 tO 1)1, dominates the DSP of seg- 
ment \[c1, l)~\]. Then what is the primary intention of the 
text in that case'? And how many discourse trees are both 
valid and consistent with that intentional judgment? Nei- 
ther RST nor GST can answer these questions on their 
own. However, a unified theory can. Ill this paper, we 
provide such a theory. 
2 The limits of Moser and Moore's 
approach 
In a recent proposal, Moser and Moore (1996) argued 
that the primary intentions in a GST representation can 
be derived fi'om the nuclei ot'the corresponding RST rep- 
resentation. Although their proposal is consistent with 
the cases in which each textual span is characterized by 
an explicit nucleus that encodes the primary intention of 
that span (as in the case of text (I)), it seems that an ad- 
equate account of the correspondence b tween GST and 
RST is somewhat more complicated. For example, in tile 
case of text (3) below, whose RST analysis is shown in 
ligure 2, we cannot apply Moser and Moore's approach 
because we can associate tile primary intention of dis- 
course segment \[a2, B2\] neither to trait A2 nor to trait B2. 
(3) \[John wanted to play squash with Janet,Aq \[but he 
NONVOLITIONAL 
CAUSE 
C2 
A2 B2 
Figure 2: A rhetorical analysis of text (3). 
also wanted to have dinner with Suzanne. '~2\] [He went 
crazy, c2 \] 
In Grosz and Sidner's terms, we can say that the primary 
intention ot' segment \[A2, B~\] is (Intend writer (Believe 
reader "John wanted to do two things that were incom- 
patible")). But in order to recognize this relation, we 
need to recognize that the two desires given in units A~ 
and B2 are incompatible, which is captured by the CON- 
TRAST relation that holds between the two units. In other 
words, the intention associated with segment \[A2, B2\] is a 
function both el' its nuclei, A 2 and B2, and of the rhetori- 
cal relation of CONTRAST that holds between them. 
In this paper, we generalize this obserwttion by 
making use o1" the compositionality criterion proposed 
in (Marcu, 1996), which stipulates that it 'a rhetorical 
relation holds between two textual spans, a si,nilar re- 
lation also holds between two salient constructs of those 
spans. 2 Similarly, we will assume that the primary inten- 
tion of a discourse segment is not given by the nucleus 
of the corresponding relation but rather that it depends 
on the corresponding relation and the salient constructs 
associated with that segment. 
3 Melding text structures and intentions 
3.1 Formulation of the problem 
Formally, the problem that we want to solve is the 
following. Given a sequence of textual units U = 
tq, u2 , . . . ,  UN, a set 1U~ of rhetorical relations that hold 
among these units, and a set o1' intentional judgments IH 
that pertain to the same units, find all legal discourse 
structures (trees) of U, and determine the dominance, 
satisl'action-precedence relations, and primary intentions 
of each span of these trees. 
Following (Marcu, 1996), we use tile predicates 
posiHo,z(ui, j) and vl, eId'd(,za,,,e, s ,z) with the fol- 
2Seclion 3 discusses in detail how the salient construcls are deler- 
mined. 
524 
lowing semantics: tim predicate posilion(ui, j) is tree 
for a textual unit ul in sequence U if and only if 
ul is the j-th element in the sequence; the predicate 
rhei_vel(namc, ui, uj) is true for textual units ul and 
uj witb respect o rhetorical relation name, if and only 
it' the detinition provided by RST for rhetorical relation 
name applies to textual units ui, in most cases a satellite, 
and uj, a nucleus. In order to enable discourse prob- 
lems to be characterized by rhetorical judgments that 
hold between large textual spans as well, we use pred- 
icate rh.cl_rel_ext(namc, s~, s~, *~, n~). This predicate 
is trl, e for textual spans \[ss, .%\] and \[,,.,, n0\] with respect 
to rhetorical relation name if and only if the detinition of 
rhetorical relation name applies for tim textual span that 
ranges  over  units ss - - se ,  ill most  cases a satellite, alld tex- 
tual spans that ranges over units n.~-nc, a nucleus. 3
From a rhetorical perspective, text (I) is described at 
the minimal unit level by the relations given in (2) and (4) 
below. 
f l,ositio,,.(A1,1), 1 ositio,~(lh, 2), (4) 1,ositio,,.( C~ ,3), 1,ositio,,O), , 1) 
The intentional judgments 1~1 are given by the follow- 
ing functions and predicates: 
? The predicate dom(l~, lq, 1~, h-,) is true whenever 
tbe DSP of discourse segment/span \[I1, hl\] domi- 
nates ttle DSI' of discourse segment \[l~, h:~\]. A dom- 
inance relation is well-formed if segment \[/~, h~\] 
is a proper subsegment of segment \[ll, h,t\]. i.e., 
l, </~ < h., < h, A (h ? z~ v h~ # h~). 
? The predicate salpvec(ll, Ih, lu, h..,) is true when- 
ever an intentional satisfactiol>precedence relation 
holds between the DSI's of segments Ill, hi\] and 
\[/2, h2\]. A satisfaction-precedence relation is well- 
formed if tile segments do not overlap. 
? Tile oracle function .fl(r, aq , . . . ,  ;%) takes as at: 
guments a rhetorical relation r and a set of texttufl 
units, and returns tbe primary intention that pertains 
to that relation and those units. For example, in 
the case of segment \[A2, Be\] in text (3), the ora- 
cle function .l) (CONTRAST,  A2, B2) is assu l l ted  to 
returu a Iirst-order object wltose meaning can be 
glossed as "inform the reader that John wanted to 
do two things that were incompatible". And the 
oracle function .1) (EWDI ~;NcE, B1) associated with 
segntent \[A1,1)~\] in text (1) is assuntcd to return 
a \[irst-oMer object whose nteaning can be glossed 
as "increase the reader's belief that the pressure to 
smoke in junior high is greater than it will be any 
other time of one's life". 
Without restricting the generality of the problem, dis- 
course structures are assented to be binary trees. In our 
formalization, each node era discourse structure is char- 
aclerized by l()tu" features: the status (nucleus or satel- 
lite), tim O'lJe (the rhetorical relations tlmt hold between 
3'Fhe s ~llld e subscripls COlTCgpond Io .~tm'ling ~.lll(I ending posilions. 
the text spans that that node spans over), the l)romotion 
set (the set of units that constitute the most "salient" (ira- 
pertain) part of the text that is spanned by that node), 
and tile i)rima O, intelltion. By convention, for each leaf 
node, the type is LEAF, the promotion set is tile textual 
unit to which it corresponds, and tbe primary intention 
is that of inJbmting the content of that unit. For exam- 
pie, a representation f the tree in ligure 1.a that makes 
explicit the features el' all spans that play an active role 
in the final representation is given in \[igure 3. In general, 
the salient units are computed using the comlmsitionality 
criterion proposed in (Marcu, 1996), i.e, they are given 
by the union of the salient units of the immediate sub- 
ordinated nuclei. Similarly, the primary intentions are a 
function of tbe rhetorical relation (type) and salient units 
of each span. 
The status, type, promotion set, and primary intention 
that are associated with each node in a discourse trec pro- 
vide suflieient information for a full description of an in- 
stance of a tree structure. Given the linear nature of text 
and the fact that we cannot predict in advance where the 
boundaries between various segments will be d,'awn, we 
should provide a lnethodology that permits one to enu- 
merate all possible ways in which a tree could bc built 
on the lop of a linear sequence of elementary discourse 
units. The solution we use relies on tile same intuition 
that constitutes tile foundation of chart parsing: just as a 
chart parser is capable of consklering all possible ways 
in which different words in a sentence could be chlstered 
into higher-order grammatical units, so our formalization 
is capable of considering all the possible ways in which 
different segments coukl be joined into discourse trees. 
l,et spa,tLj, or simply \[i,j\], denote a text span 
thai includes all tile elementary discourse unils be- 
tween position i and j. Then, if we consider a 
sequence of discourse units .u~, I t2 : . . .  ~'lt~t, there  
are n ways in which spans o1' length one could 
be built, spa '~Zl , l ,  st)(tLt2,2, ? ? ? , 'sl)(t'/tn,n; it - \] 
ways in which spans of length two could be built, 
? spa~z l ,2 :  S l )~Ut . .&3~. . .  , spa l tn - l ,n ;  11 -- 2 ways 
in which spans of length three could be built, 
and one 6"\])(t?l.l ; h Sl)(t l l .2,4~ . . . ~ .5 \ ]}a l tn -2 ,n ;  . . . ; 
way in which a span of length n coukl be built, spa771,n. 
Since it is impossible to determine a priori the sl)ans 
that will be used to make up a discourse tree, we will 
associate with each span that could possibly become 
part of a tree a status, a type, promotion, and primary 
intention relation and let discourse and intentional 
constraints determine the valid discourse trees. In 
other words, we want to ?tetermine from the set of 
ha- (, , .-  1)-t- (n -2 )  + . . .+  1 = n(n4- 1)/2 potentM 
spans that pertain to a sequence of n discourse units, the 
subset hat adheres to some constraints of rhetorical and 
intentional well-formedness. For example, for text 1, 
there are d + 3 -t- 2 + \[ = l0 potential spans, i.e., 
S\])(17tl ,1 ~ 8 \ ] )aTt2 ,2 :  S1) f l713,3,  s l )a?). ,1,4,  8P( t? t l ,2~ $1)(/N.2,3, 
sPa~l: l ,4: 8Payt.1,3~ s' \ ] )a?12, , t ,  and 8p(I.711,. I ,  but  
525 
~l l lS  = SATELLITE 
t ) ' l ' ype  = LEAF 
l 'romotion = {all 
lntcllt ion = f(al) 
A1 -D1 
- -  Type = EVIDENCE 
~' -~-  Pl'OlllO\[ion = {B1} 
~ "  \]IIIonIIOII =~\[  E IVlDENCE,B1) 
hi ,,-uB1 ~-Slltttls == J UST;FICATION NUCLEUS C1-DI ( '~a~ ~732 "   Type ~--  - -~-  ~Q.y  Type SlaltlS := CONCESSION 8ATELLITE 
/~'~Q~l'Oiilotion = {811 ~ Promotion = {Ol I
~" / i ,~ l t ion  = f {JUSTIFICATION,all ~ "~hl~atiozl = I (CONCESSION,C1) 
\>. \ 
" /  D I  ~__  S la t / "  N . . . . .  ATELUTE S\[a\[tlS = NUCLEUS C1 /~S l i l | t lS  = NUCLEUS BI 
~_.  \ t JTy t ,o  = LEAE - - -  Q{) ' l ' ype  = LEgs ---- { 3 )Type  = LEAF 
Promol ion = Ira} Promotion = {cq Promotion = {DI1 
hltcntion = f tin) Intention = f (el) Intention = f jta) 
Figure 3: A representation of tree l.a that includes the status, type, promotion, and primary intention features that 
characterize every node that does not have a NONE status. The nunlbers associated with each node denote the limits of 
the text span that that node characterizes. 
only seven of them play an act?ve role in 
the representation given in figure l.a, i.e., 
8\])(l~.1,1, SP(llZ2,2, $1)(t1~.3,3, 8\])(t1~'4,4, St)Ctl~l ,2, spa~.3,4, 
a l ld  .5'\])a IZ 1 ,4-  
To  formalize the constraints that pertain both to RST 
and GST, we thus assume that each potential span \[1, hi 
is characterized by the following predicates: 
? S(I, h, s lalus) provides the status of span El, h\], i.e., 
the text span that contains units / to h; staZus can 
take one of the values NUCLEUS, SATELLITE, or 
NUNS. according to the role played by that span 
in the tinal discot,rse tree. For example, for the 
tree depicted in tigure 3, some of the relations that 
hold are: ,5'(1, 2, NUCLEUS),,5'(3, 4, SATELLITE), 
,5" (1 ,3 ,  NONE) .  
? T(1, h, relation_ua.rn.e) provides the name of the 
rhetorical relation that holds between the text 
spans that are immediate subordinates o1' span 
El, h\] in the discourse tree. If the text span is 
not used in the construction of the final tree, 
the type assigned is NONE. For example, for 
the tree in ligure 3, some o1' the relations that 
hold are: T( I ,  J, LEAF), 5/'(1,2, JUSTW~CATION), 
T(3, 4, CONC~SSrON), T(1, 3, NONE). 
? P(I, h.,unit_name) provides one of the set of 
units that are salient for span El, h\]. The col- 
lection of units for which the predicate is true 
provides the promotion set of a span, i.e., all 
units that are salient for that span. If span \[1, h\] 
is not used in the tilml tree, by convention, the 
set of salient units is NONE. For example, for 
the tree in figure 3, some of the relations that 
hold are: P(1, 1., &) ,  P(1, 2, lh), P(1,3,  NONE), 
1'(3, 4, D,). 
? I l l , h, intention) provides the primary intention 
of discourse span El, h\]. The term iulenlion is 
represented using the oracle ftmction J). For ex- 
ample, for the tree in figure 3, some of the rela- 
tions that tloi(t arc: I(3, 4, f/(CONCESSION, Cj )), 
l(J,/1, .fI(P:VIDENCI~:, B\])), l(J, 3, NONE). 
3.2 An integrated formalization of RST and GST 
Using the ideas that we have discussed ill the previous 
section, we present now a first-order formalization of dis- 
course structures that makes use both of RST- and GST- 
like constraints. In this lbrmalization, wc assume a uni- 
verse that consists of the set of natural numbers fi'om J 
tO N, where N represents the number of textual units in 
the text that is considered; the set of names thai were 
defined by Mann and Thompson for each rhetorical rela- 
tion; the set of unit names that are associated with each 
textual unit; and four exlra constants: NUCLEUS, SATEL- 
LITE, NONE, and LI~2AF. The formalization is assumed lo 
provide unique name axioms for all these constants. 
The only funclion symbols that operate eve," the as- 
sumed domain are the mlditional + and - functions that 
are associated with the set of natural numbers and the or- 
acle function J). The formalization uses the traditional 
predicate symbols that pertain to the set of natural num- 
bers (<, <, >, >, =, ?)  and eight other predicate sym- 
bols: ,5', T, P and I to account for the status, type, salienl 
units, and primary intention that are associated with ev- 
ery text span; vhel_vel to account for the rhetorical rela- 
lions that hold between different extual units; position 
to account for the index of the textual units in lhe text 
dmt one considers; dora to account for dominance rela- 
tions; and satprec to account for satisfaction-precedence 
relations. 
Throughout the paper, we apply the convention that 
all unbound variables are universally quantified and that 
variables are represented in lower-case italics" and con- 
stants in SMALL CAPITALS. We also make use of the 
two extra relations, vclevaul_uni~ and relevant_tel. 
For every text span span \[/, hi, relevant_unit(l, h, u) 
describes the set ot' textual units that are relevant for 
that text span, i.e., the units whose positions in the 
initial sequence are numbers in the interval \[l, hi. It 
is only these units that can be used to label the pro- 
526 
motion set associated with a tree that subsumes all 
units in the interval \[l, hi. For every text span \[1, h.\], 
vclevcm.Z_vcl(l, h, name) describes the set of rhetorical 
relations that are relevant to that text span, i.e., the set of 
rhetorical relations that span over text units in the inter- 
val \[1, h\] and the set of extended rhetorical relations that 
span over text spans that cover the whole interval \[/, h\] 
(see (Marcu, 1996) for the formal delinitions of these re- 
httions.) 
For example, fin" text (1), which is descrihed formally 
in (2) and (4), the following is the set of all rclc'~a~zl_rel 
and vclevctn~_unil, relations that hold with respect to text 
segment \[ l ,3\]:  {vclcvanLvcl(l,3, JUSTWlCaTtON), 
'rclcvanl_vcl(l, 3, EVII)ENCl0, relevcr, t_m~it(I, 3, &) ,  
, .~z~v.,,t_~,,nit( l ,  a, B~), , .d~, : . , , z_ , , , , i t ( l ,  :~, q )} .  
The constraints that pertain to the discourse trees that 
we formalize can be partitioned into constraints related to 
the domain of objects over which each predicate ranges, 
constraints related to the structure of the tree, and con- 
straints that relate the slrucltlral COlnponenl with the in- 
tentional component. The axioms that pertain to the do- 
mains over which predicates ,5, P, and 7' range and the 
constraints related to the structure of the live are the same 
as those given by Marcu (1996). For lhe sake of com- 
pleteness, in this paper we only enumerate then\] infor- 
mally. In contrast, the axioms that pertain to intentions 
and the relation between structure and intentions are dis- 
cussed in detail. 
Constraints that concern the objects over which the 
predicates that describe every segment \[1, hi of a text 
structure range (Mareu, 1996, pp. 1072-1073). 
,, For every siren \[/, h\], the set or objects over which 
predicate ,5' ranges is the set {NUC1A,~US, SNI'ELIJTI,\],  
NONE) .  
? The status of any discourse segment is unique. 
? For every segment \[l, h\], the set of objects over 
which predicate 7' ranges is the set of rhetorical  re- 
lations that are relevant to that span. 
? At most one rhetorical rdation can connect two ad- 
jacent discourse spans 
? The pr imary  intention of a discourse segment is ei- 
ther NONE or is a function of the sal ient units that  per- 
tain to that segment and of the rhetorical  relation that 
holds between the immediate subordinated segments. 
Since we want to stay within the boundaries of Iirst-order 
logic, we express this (see formula (5) below) by means 
of a disjunction of at most N sulfformulas, which corre- 
spond to the cases in which the span has I, 2 . . . .  , or N 
salient raits. 4 
4Formula (5) reflects no preference concerning lhe order in which 
rhetorical relalions and intentions should be computed (Asher and Las- 
carides, 1998). It only asserts aconsh'ailll on the two. 
\[(1 < h < N) A (l  < I < h.)\] 
{ I( I ,  h, i 'n.t~t. io~u,) --, 
i.n.leT~ionzb = NONF, V 
(~,', .,)\[7'(I, h, ,') A ,' ? NONI:.A 
PU,  h., ..;) A (V,/)(\]'(~, :,, y) -~ ,; = y)A 
i,,.te,,gio,,4h = fz( , ' ,  ,;)\]V 
(~'r, ~c,,-2)\[{1'(/, h, r) A 'r  NONF, A 
P(I, h,..,.,) A P(~, h,.,:2) A. ; ,  ? .:_~A 
(Vv)(\]'(1, h, v) ~ (v = .';, v :j = ~2))A 
i~,~,t io~, . ,  = f .(,., ....,, :,:~)\]v 
(5) 
(~'r, a:,, a:2 . . . .  , :,:N)\[S\]~(/, h, r) A r y:- NONEA 
a;1 7~ a:~ A a:l # a::~ A . . .  A :cl ? ~;NA 
? ~;2 -7 k a'3 A . . .  A :C# ~ :;';NA 
,~:N--I :~ XNA 
P(/, h, ,:, ) A e(t, h, ~)  A . . .  A PU, h,, , ; , )A 
(V~)(P(t ,  h, y) -+ ( : /=  ~, v . . .  v y = , ; , )>  
inl.c,,.lio,tu, --- fz(r, :c,, a;u,. ?. , ,;,)\]} 
? The pr imary  intention of any discourse segment is 
unique. 
(6) \[(i < 1,. < N) A (1 5_ t < 1,.)\] 
\[(1(~,/,, i, ) A J(I, h, <) )  - -  .i, = <4 
? For  every segmeut \[l, hi, the set of objects over 
which predicate P ranges is the set of units that make 
up that segment 
Constraints that concern /lie strnctmm of the dis- 
course trees 
? The status, type, and promotion set that are associ- 
ated with a discourse segment reflect the COmlmsition - 
al ity criterion. That is, whenever a rhetorical relation 
holds between two spans, either a simihu" relation holds 
between Ihe mosl salicnl units of those spans or an ex- 
tended rhetorical relation holds between those spans. 
? Discourse segments do not overlap. 
? A discourse segment with status NONE does not par- 
ticipate in the tree at all. 
? There exists a discourse segment, the root, that 
sirens over the entire text. 
~,S'(1, N, non l ' )  A ~P( \ ] ,  N, NONF,)A 
(7) ~" (1 ,  N, NONIi) A -71(1, N, NONE) 
? The dominance relations described by Grosz and 
Sidner hold Between the DSP of a discoorse seg- 
ment and the DSP of'its most immediate subordinated 
satellite. This constraint is consistent with Moser and 
Moore's (1996) discussion of RST and GST. In fact, this 
is not surprising if we examine the definitions of dom- 
inance relation given by Grosz and Sidner and satellite 
given by Mann and Thompson:  a discourse segment 
purpose D,5't? dominates a discourse segment purpose 
D,5'1"1 if I),5'P\] contributes to the satisfaction el' the 
I),5'1?. But this is exactly the role that satellites play in 
P, ST: they do not express what is most essential for the 
writer's purpose, but rather, provide supporting informa- 
lion that contributes to the understanding of the nucleus. 
527 
The relationship between Grosz and Sidner's domi- 
nance relations and Mann and Thompson's distinction 
between uclei and satellites is formalized by axioms (8) 
and (9). 
\[(1 ~ hl _< N) A(1 ~ 11 ~ I,.I)A 
(1 ~ h,9 ~ N) A (1. < 19 < h,2)\] "~> 
{\["~,5'(11, hi, NONE) A ,~'(/2, h2, SATEI,L1TF,)A 
11 <l~ <h~ <hlA  
(s) ~(~-+'+, ,',,+)(,',+ < 6 < z~ < h~ _< h~ < \],,~A 
(13 ? 12 V h,,3 ? h,2)A 
s(/+, ha, SATI+LLm,:))\] 
dom(ll, hq, 12, h2)} 
\[(+ < h, < N) ,X (+ _< h _< h,) /, (l _< h+ < N)A 
(9) (1 ~ 1.9 .~ 11.2) A do?l+(l,, lt, l ,  12, //.2)\] "--+ 
\[-~,5'(h, i, NON.:) A S(6, h_~, SATJILUTE)\] 
Axiom (8) specities that if segment \[12, h.2\] is the imme- 
diate satellite l'segment \[lt, lq\], then there exists a dom- 
inance relation between the DSP of segment \[/1,/q\] and 
the DSP of segment \[12, h2\]. Hence, axiom (8) explicates 
the relationship between the structure of discourse and 
intentional dominance. In contrast, axiom (9) explicates 
the relationship between intentional dominance and dis- 
course structure. That is, if we know that the intention 
associated with span \[lj, 1,1\] dominates the intention as- 
sociated with span \[12, h,2\], then both those spans play an 
active role in the representation a d, moreover, the seg- 
ment \[12,11,2\] plays a SATELLITE role. 
? The satisfaction-precedence rdations described by 
Grosz and Sidner are parataetie relations that hold 
between arlfitrarily large textual spans. Neverthe- 
less, as we have seen in the examples discussed in this 
paper, the fact that a paratactic relation holds between 
spans does not imply that there exists a satisfaction- 
precedence r lation at the intentional level between those 
spans. Therefore, for satisfaction-precedence relations, 
we will have only OnE axiom, that shown in (I0), below. 
\[(t 5 hJ ~ N) A (1 ~ 11 ~ hl) A (\] <" h,2 ~ N)A 
(1 o) 0 <- z~ _< ,'+2) A .,+,~,tv.,'~4.'~, h,~, z~, \],.,_,)\] -+ 
\[S(11, h,1, NUCI+EUS) A ,5'(12, h,2, NUCI,EUS)\] 
This specifiES that the spans that are arguments of a 
satisfaction-precedence relation have a NUCLEUS status 
in the linal representation. 
4 A computat iona l  v iew o f  the  
ax iomat i za t ion  
Given the formulation discussed abovE, tinding the dis- 
course trees and the primary intentions lkw a text such as 
that given in (1) amounts to finding a model for a first- 
order theory that consists of formulas (2), (4), and the 
axioms enumerated in section 3. 
There are a number of ways in which one can pro- 
ceed with an implementation: for cxalnple, a smtight- 
forward choice is one that applies constraint-satisl'action 
techniques, an approach that extends that discussed 
in (Marcu, 1996). Given a sequence U of N textual units, 
one can take advantage of the structure of the domain and 
associate with each of the N(N-F 1)/2 possible text spans 
a status and a type variable whose domains consist in the 
set of objects over which the corresponding predicates 
,5 + and T, range. For each of the N(N + 1)/2 possible 
text spans \[l, h.\], one can also associate h, - l + \] promo- 
lion variables. These are boolean variables that specify 
whether units l, 1 + \ ] , . . .  , h belong to the promotion set 
of span \[/, hi. For each of the N(N + 1)/2 possible text 
spans \[l, hi, one can also associate h - 1 + 2 intentional 
variables: one of these wtriables has as domain the set 
of rhetorical relations that are relevant for the span \[1, hi. 
The rest of the h - /+  1 wwiables are boolean and specify 
whether unit l, l-t- \] . . . .  , or h are arguments of the oracle 
function f~ that intentionally characterizes that span. 
Hence, each text of N units yields a constraint- 
satisfaction prohlem with N(N + I)(2N + \]3)/6 vari- 
ables (NCN q- \])(2N -}- 13)/(J = 2NCN q- \])/~ -}- 
V,2<=N V,h<----N I<=N W,h<=N(h_l_F2))). (h ' - - l+ l )+~l -1  Z-,h.=l Z-,I=1 Z~,h=l 
The constl+aints associated with these wtriables arc a one- 
to-onE mapping o1' the axioms in section 3. Finding the 
set of RS-trees and the intentions that are associated with 
a given discourse reduces then to/inding all the solutions 
for a traditional constraint-satisfaction problem. 
5 App l i ca t ions  
Reasoning from text structures to intentions. Con- 
sider again the example text (1), which was usEd 
throughout this paper. As we discussed in section 1, il' 
we assume that an analyst (or a program) determines that 
the rhetorical relations given in (2) hold between the el- 
ementary units of the text, there arc live valid trees that 
correspond to text (1) (see figure 1). If we consider now 
the axioms that dEscribE the relationship bEtwEen text 
structures and intentions, we can infer, for example, thai, 
for the tree I.a, the DSP of span \[A1,131\] dominates the 
DSP of span \[cj, l)j\] and that the primary intention of 
the whole text depends on unit B1 and on the rhetori- 
cal relation of EVID\]\]NCF,. Ill such a casE, the axiomati- 
zation provides the means for drawing intentional infer- 
ences on the basis of the discourse structure. Also, al- 
though there are live discourse structures that are consis- 
tent with the rhetorical judgments in (I), they yield only 
three intentional interpretations, i.e., there arc only three 
primary intentions that one can associate to the whole 
text. One intention is that discussed above, which is as- 
sociated with analysis I.a. Another intention depends on 
unit Bz and the JUSTIFICATION relation that holds be- 
tween units A1 and lh; this intention is associated with 
the analyses hown in ligure 1.c and l.e. And another in- 
tention depends on trait Bj and the JUSTIFICATION rela- 
tion that holds between units l)j and Bj ; this intention is 
associated with the analyses hown in figure 1.b and 1.d. 
Reasoning fronl text structures to intentions can be 
also beneficial hi a context such as that described by 
Lochbaum (1998) because the rhetorical constraints can 
help prune the space of shared phms that woukl charac- 
terize an intEn tional interpretati o n of a d iscou rse. 
528 
Us ing  intentions lbr nmnaging rhetorical  aml f igu i t ies .  
Assume now that besides providing.ivdgments concern- 
ing the rhetorical rehttions that hold between various 
units, an analyst (ot" a progran0 provides judglnents of 
intentions as well. If, lk+t" cxaml+le, besides the relations 
given in (2) a program determines that the DSP of span 
tAt, 1)1\] dominates 111o DSP of unit I/i, the theory that 
corresponds to these judgments and 111e axioms given 
in section 3 yields only two wdid text structures, those 
presented in \[igure l.b and I.d. In this ease, the axiom- 
atization provides the means of using intentional judg- 
ments for reducing the ambiguity that characterizes the 
discourse parsing process. 
hwestigating the relationship between semantic and 
intentional relations. In their seminal paper, Moore 
and Polhtck (1992) showed lhat a text may be charac- 
terized by intentional and rhetorical analyses that are not 
isomorphic. For example, for the text shown in (1 I) be- 
low, which is taken from (Moore and Pollack, 1992), one 
may argue from an informational perspective that A3 is 
a CONI)ITION \['or B3. However, l}'otll an intentional per- 
spective, one may argue thai 1',3 can be used to MOTI- 
VATI'; A3. Similal + judgments can be made with respect 
to units 1{3 and c3. Hence, lhe set of relations that COln- 
pletely characterizes text (11) is thal shown in (12) be- 
low. 
(11) \[Come home by 5:00. ^ a\] \[Then we can go to the hard- 
ware store before it closes)':'\] \[That way we can linish 
Ihe bookshelves tonightY:' \] 
.rhct_.rcl(CONl)lTlON, A:~ 1',.+.) 
'rhcI_.rcl(MOTIVATION, B;:, A:: ) 
(12) rh(t_rcl({;ONI)lrlON, 1~:.., C':,. )
'r/t.CI_,"cl(MOTIVATION, C::, B:;) 
When given this discourse problenl, our imple- 
mentation produces the four discourse trees shown 
iu figure 4, each el + them having a different primary 
intention (./"/(CONI)ITION, C3), f!(MOTIVATION, a3), 
.ft(MOTWATION, B3), and ./) (CONl)rrtoN, I+:~)). 
Hence, our approach enables one to derive automatically 
and enumerate all possible rhetorical interpretations of
a text and to study the rehttionshil~ between structure 
and intentions. Our approach does not provide yet the 
mechanisms for choosing between different interpreta- 
tions, but it provides the foundations for such a study. In 
contrast, Moore and Pollaek's informal approach could 
neither derive nor enumerate all possible interpretations: 
in fact, their discttssion refers only to the two trees 
shown in ligure 4.a and .b. 
Unlike Moore and Polhtck's approach, where it is sug- 
gested that a discourse representation should reflect si- 
multaneously both its informational nd intentional inter- 
pretations, the approach presented here is capable of only 
enumerating these interpretations. The formal model we 
proposed is not rich enough to accotlllllodate conctlrretH, 
non-isomorphic interpretations. 
......... j I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ,u++ _. / -  - ~,\] ??.c~ 
+ . . . . . . . . . . . . . . . . .  + + ,+ 
A3 B3 93 C3 A3 B3 83 C2 
a) b> c) a: 
F igure 4: The set o f  all RS-t rces that can be built for 
text (I 1). 
6 Conclusion 
Crucial to tile develolmlent of syntactic theories was the 
ability to provide mechanisms capable of deriving all 
valid syntactic interpretations of a given sentence. Se- 
mantic or corpus-specific nformation was then used to 
manage the usually large number of interpretations. 
The work described in this paper sets theoretical foun- 
dations that enable a similar approach to the study of dis- 
course. The way a syntactic theory enables all wtlid syn- 
tactic trees of a sentence be derived, the same way the 
axiomatization presented here enables all valid discourse 
trees of a text be derived. But the same way a sylltac- 
tic theory may produce trues that arc incorrect ftonl a 
semantic perspective for example, the same way the ax- 
iomalization described here may produce trees that are 
incorrect when, for example, focus and cohesion are fac- 
tored in. 
A ntmlber o1' researchers have ah'eady shown how in- 
dividual rhetorical and intentional judgments can be de- 
rived automatically l'mm linguistic constructs uch as 
tense and aspect, certain patterns of pronominalization 
and anaphoric usages, it-clefts, and discourse markers or 
cue phrases. But once lhese.iudgmcnts arc made, we still 
need to determine all discourse interpretations that are 
not only consistent with these judgments but also wtlid. 
This paper provides mechanisms for deriving and enu- 
merating all valid structure of a discourse and enables a 
quantitative study el' the relation between text structures 
atld intentions. 
References 
Nicholas Asher and Alex Lascarides. 1998. Questions in dia- 
logue. Linguistics' and l'hilosophy, 21 (3):237-309. 
Barbara J. Grosz and Candace L. Sktner. 1986. Attention, in- 
tentions, and lhe structure of discourse. Co,qmlational Lin- 
guLvlics, 12(3): 175-204. 
Karcn IL Lochbaum. 1998. A collabonttive planning 
model of intentional structtlre. Computational Linguistics, 
24(4):525-572. 
William C. Marm and Sandra A. Thompson. 1988. Rhetorical 
structure lheou: Toward a functional theory of text organi- 
zation. 7Eft, 8(3):243-281. 
l)aniel Marcu. 1996. Bt, ilding up flmtorical structure trees. 111 
Proceedings of AAA 1-96, rages 1069-1074. 
Johanna 1). Moore and Mart ut E. Polhtck. 1992. A problem 
for RST: The need for multi-level discourse analysis. Com- 
pulalional LinguLvlics, 18(4):537-544. 
Megan Moser and Johanna 1). Moore. 1996. 'lbward a synthe- 
sis of two accotttlls of discot, rse structure. Computational 
Lingttistics, 22(3):409-419. 
529 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 51?60, Prague, June 2007. c?2007 Association for Computational Linguistics
Getting the structure right for word alignment: LEAF
Alexander Fraser
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
fraser@isi.edu
Daniel Marcu
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
Word alignment is the problem of annotating
parallel text with translational correspon-
dence. Previous generative word alignment
models have made structural assumptions
such as the 1-to-1, 1-to-N, or phrase-based
consecutive word assumptions, while previ-
ous discriminative models have either made
such an assumption directly or used features
derived from a generative model making one
of these assumptions. We present a new gen-
erative alignment model which avoids these
structural limitations, and show that it is
effective when trained using both unsuper-
vised and semi-supervised training methods.
1 Introduction
Several generative models and a large number of
discriminatively trained models have been proposed
in the literature to solve the problem of automatic
word alignment of bitexts. The generative propos-
als have required unrealistic assumptions about the
structure of the word alignments. Two assumptions
are particularly common. The first is the 1-to-N as-
sumption, meaning that each source word generates
zero or more target words, which requires heuristic
techniques in order to obtain alignments suitable for
training a SMT system. The second is the consec-
utive word-based ?phrasal SMT? assumption. This
does not allow gaps, which can be used to particular
advantage by SMT models which model hierarchi-
cal structure. Previous discriminative models have
either made such assumptions directly or used fea-
tures from a generative model making such an as-
sumption. Our objective is to automatically produce
alignments which can be used to build high quality
machine translation systems. These are presumably
close to the alignments that trained bilingual speak-
ers produce. Human annotated alignments often
contain M-to-N alignments, where several source
words are aligned to several target words and the re-
sulting unit can not be further decomposed. Source
or target words in a single unit are sometimes non-
consecutive.
In this paper, we describe a new generative model
which directly models M-to-N non-consecutive
word alignments. The rest of the paper is organized
as follows. The generative story is presented, fol-
lowed by the mathematical formulation. Details of
the unsupervised training procedure are described.
The generative model is then decomposed into fea-
ture functions used in a log-linear model which is
trained using a semi-supervised algorithm. Experi-
ments show improvements in word alignment accu-
racy and usage of the generated alignments in hier-
archical and phrasal SMT systems results in an in-
creased BLEU score. Previous work is discussed
and this is followed by the conclusion.
2 LEAF: a generative word alignment
model
2.1 Generative story
We introduce a new generative story which enables
the capture of non-consecutive M-to-N alignment
structure. We have attempted to use the same la-
bels as the generative story for Model 4 (Brown et
51
al., 1993), which we are extending.
Our generative story describes the stochastic gen-
eration of a target string f (sometimes referred to
as the French string, or foreign string) from a source
string e (sometimes referred to as the English string),
consisting of l words. The variable m is the length
of f . We generally use the index i to refer to source
words (ei is the English word at position i), and j to
refer to target words.
Our generative story makes the distinction be-
tween different types of source words. There are
head words, non-head words, and deleted words.
Similarly, for target words, there are head words,
non-head words, and spurious words. A head word
is linked to zero or more non-head words; each non-
head word is linked to from exactly one head word.
The purpose of head words is to try to provide a ro-
bust representation of the semantic features neces-
sary to determine translational correspondence. This
is similar to the use of syntactic head words in sta-
tistical parsers to provide a robust representation of
the syntactic features of a parse sub-tree.
A minimal translational correspondence consists
of a linkage between a source head word and a target
head word (and by implication, the non-head words
linked to them). Deleted source words are not in-
volved in a minimal translational correspondence, as
they were ?deleted? by the translation process. Spu-
rious target words are also not involved in a min-
imal translational correspondence, as they sponta-
neously appeared during the generation of other tar-
get words.
Figure 1 shows a simple example of the stochas-
tic generation of a French sentence from an English
sentence, annotated with the step number in the gen-
erative story.
1. Choose the source word type.
for each i = 1, 2, ..., l choose a word type
?i = ?1 (non-head word), ?i = 0 (deleted
word) or ?i = 1 (head word) according to the
distribution g(?i|ei)
let ?0 = 1
2. Choose the identity of the head word for each
non-head word.
for each i = 1, 2, ..., l if ?i = ?1 choose a
?linked from head word? value ?i (the position
of the head word which ei is linked to) accord-
ing to the distribution w?1(?i ? i|classe(ei))
for each i = 1, 2, ..., l if ?i = 1 let ?i = i
for each i = 1, 2, ..., l if ?i = 0 let ?i = 0
for each i = 1, 2, ..., l if ??i 6= 1 return ?fail-
ure?
3. Choose the identity of the generated target head
word for each source head word.
for each i = 1, 2, ..., l if ?i = 1 choose ?i1
according to the distribution t1(?i1|ei)
4. Choose the number of words in a target cept
conditioned on the identity of the source head
word and the source cept size (?i is 1 if the cept
size is 1, and 2 if the cept size is greater).
for each i = 1, 2, ..., l if ?i = 1 choose a For-
eign cept size ?i according to the distribution
s(?i|ei, ?i)
for each i = 1, 2, ..., l if ?i < 1 let ?i = 0
5. Choose the number of spurious words.
choose ?0 according to the distribution
s0(?0|
?
i ?i)
let m = ?0 +
?l
i=1 ?i
6. Choose the identity of the spurious words.
for each k = 1, 2, ..., ?0 choose ?0k according
to the distribution t0(?0k)
7. Choose the identity of the target non-head
words linked to each target head word.
for each i = 1, 2, ..., l and for each k =
2, 3, ..., ?i choose ?ik according to the distribu-
tion t>1(?ik|ei, classh(?i1))
8. Choose the position of the target head and non-
head words.
for each i = 1, 2, ..., l and for each k =
1, 2, ..., ?i choose a position piik as follows:
? if k = 1 choose pii1 accord-
ing to the distribution d1(pii1 ?
c?i |classe(e?i), classf (?i1))
? if k = 2 choose pii2 according to the dis-
tribution d2(pii2 ? pii1|classf (?i1))
52
source absolutely [comma] they do not want to spend that money
word type (1) DEL. DEL. HEAD non-head HEAD HEAD non-head HEAD HEAD HEAD
linked from (2) THEY do NOT|| WANT to SPEND{{ THAT MONEY
head(3) ILS PAS DESIRENT DEPENSER CET ARGENT
cept size(4) 1 2 1 1 1 1
num spurious(5) 1
spurious(6) aujourd?hui
non-head(7) ILS PAS "" ne DESIRENT DEPENSER CET ARGENT
placement(8) aujourd?hui ILS ne DESIRENT PASww DEPENSER CET ARGENT
spur. placement(9) ILS ne DESIRENT PASww DEPENSER CET ARGENT aujourd?hui
Figure 1: Generative story example, (number) indicates step number
? if k > 2 choose piik according to the dis-
tribution d>2(piik ? piik?1|classf (?i1))
if any position was chosen twice, return ?fail-
ure?
9. Choose the position of the spuriously generated
words.
for each k = 1, 2, ..., ?0 choose a position pi0k
from ?0 ? k + 1 remaining vacant positions in
1, 2, ...,m according to the uniform distribution
let f be the string fpiik = ?ik
We note that the steps which return ?failure? are
required because the model is deficient. Deficiency
means that a portion of the probability mass in the
model is allocated towards generative stories which
would result in infeasible alignment structures. Our
model has deficiency in the non-spurious target word
placement, just as Model 4 does. It has addi-
tional deficiency in the source word linking deci-
sions. (Och and Ney, 2003) presented results sug-
gesting that the additional parameters required to en-
sure that a model is not deficient result in inferior
performance, but we plan to study whether this is
the case for our generative model in future work.
Given e, f and a candidate alignment a, which
represents both the links between source and tar-
get head-words and the head-word connections of
the non-head words, we would like to calculate
p(f, a|e). The formula for this is:
p(f, a|e) =[
l?
i=1
g(?i|ei)]
[
l?
i=1
?(?i,?1)w?1(?i ? i|classe(ei))]
[
l?
i=1
?(?i, 1)t1(?i1|ei)]
[
l?
i=1
?(?i, 1)s(?i|ei, ?i)]
[s0(?0|
l?
i=1
?i)]
[
?0?
k=1
t0(?0k)]
[
l?
i=1
?i?
k=2
t>1(?ik|ei, classh(?i1))]
[
l?
i=1
?i?
k=1
Dik(piik)]
where:
?(i, i?) is the Kronecker delta function which is
equal to 1 if i = i? and 0 otherwise.
?i is the position of the closest English head word
to the left of the word at i or 0 if there is no such
word.
53
classe(ei) is the word class of the English word at
position i, classf (fj) is the word class of the French
word at position j, classh(fj) is the word class of
the French head word at position j.
p0 and p1 are parameters describing the proba-
bility of not generating and of generating a target
spurious word from each non-spurious target word,
p0 + p1 = 1.
m? =
l?
i=1
?i (1)
s0(?0|m?) =
(m?
?0
)
pm???00 p?01 (2)
Dik(j) =
?
???????
???????
d1(j ? c?i |classe(e?i), classf (?ik))
if k = 1
d2(j ? pii1|classf (?ik))
if k = 2
d>2(j ? piik?1|classf (?ik))
if k > 2
(3)
?i = min(2,
l?
i?=1
?(?i? , i)) (4)
ci =
{ ceiling(??ik=1 piik/?i) if ?i 6= 0
0 if ?i = 0 (5)
The alignment structure used in many other mod-
els can be modeled using special cases of this frame-
work. We can express the 1-to-N structure of mod-
els like Model 4 by disallowing ?i = ?1, while for
1-to-1 structure we both disallow ?i = ?1 and de-
terministically set ?i = ?i. We can also specialize
our generative story to the consecutive word M-to-N
alignments used in ?phrase-based? models, though
in this case the conditioning of the generation deci-
sions would be quite different. This involves adding
checks on source and target connection geometry to
the generative story which, if violated, would return
?failure?; naturally this is at the cost of additional
deficiency.
2.2 Unsupervised Parameter Estimation
We can perform maximum likelihood estimation of
the parameters of this model in a similar fashion
to that of Model 4 (Brown et al, 1993), described
thoroughly in (Och and Ney, 2003). We use Viterbi
training (Brown et al, 1993) but neighborhood es-
timation (Al-Onaizan et al, 1999; Och and Ney,
2003) or ?pegging? (Brown et al, 1993) could also
be used.
To initialize the parameters of the generative
model for the first iteration, we use bootstrapping
from a 1-to-N and a M-to-1 alignment. We use the
intersection of the 1-to-N and M-to-1 alignments
to establish the head word relationship, the 1-to-N
alignment to delineate the target word cepts, and the
M-to-1 alignment to delineate the source word cepts.
In bootstrapping, a problem arises when we en-
counter infeasible alignment structure where, for in-
stance, a source word generates target words but no
link between any of the target words and the source
word appears in the intersection, so it is not clear
which target word is the target head word. To ad-
dress this, we consider each of the N generated tar-
get words as the target head word in turn and assign
this configuration 1/N of the counts.
For each iteration of training we search for the
Viterbi solution for millions of sentences. Evidence
that inference over the space of all possible align-
ments is intractable has been presented, for a sim-
ilar problem, in (Knight, 1999). Unlike phrase-
based SMT, left-to-right hypothesis extension using
a beam decoder is unlikely to be effective because in
word alignment reordering is not limited to a small
local window and so the necessary beam would be
very large. We are not aware of admissible or inad-
missible search heuristics which have been shown to
be effective when used in conjunction with a search
algorithm similar to A* search for a model predict-
ing over a structure like ours. Therefore we use
a simple local search algorithm which operates on
complete hypotheses.
(Brown et al, 1993) defined two local search op-
erations for their 1-to-N alignment models 3, 4 and
5. All alignments which are reachable via these
operations from the starting alignment are consid-
ered. One operation is to change the generation de-
cision for a French word to a different English word
(move), and the other is to swap the generation de-
cision for two French words (swap). All possible
operations are tried and the best is chosen. This is
repeated. The search is terminated when no opera-
54
tion results in an improvement. (Och and Ney, 2003)
discussed efficient implementation.
In our model, because the alignment structure is
richer, we define the following operations: move
French non-head word to new head, move English
non-head word to new head, swap heads of two
French non-head words, swap heads of two English
non-head words, swap English head word links of
two French head words, link English word to French
word making new head words, unlink English and
French head words. We use multiple restarts to try to
reduce search errors. (Germann et al, 2004; Marcu
and Wong, 2002) have some similar operations with-
out the head word distinction.
3 Semi-supervised parameter estimation
Equation 6 defines a log-linear model. Each feature
function hm has an associated weight ?m. Given
a vector of these weights ?, the alignment search
problem, i.e. the search to return the best alignment
a? of the sentences e and f according to the model, is
specified by Equation 7.
p?(f, a|e) = exp(
?
m ?mhm(a, e, f))?
a?,f ? exp(
?
m ?mhm(a?, e, f ?))
(6)
a? = argmax
a
?
m
?mhm(f, a, e) (7)
We decompose the new generative model pre-
sented in Section 2 in both translation directions
to provide the initial feature functions for our log-
linear model, features 1 to 10 and 16 to 25 in Table
1.
We use backoffs for the translation decisions (fea-
tures 11 and 26 and the HMM translation tables
which are features 12 and 27) and the target cept size
distributions (features 13, 14, 28 and 29 in Table 1),
as well as heuristics which directly control the num-
ber of unaligned words we generate (features 15 and
30 in Table 1).
We use the semi-supervised EMD algorithm
(Fraser and Marcu, 2006b) to train the model. The
initial M-step bootstraps parameters as described in
Section 2.2 from a M-to-1 and a 1-to-N alignment.
We then perform the D-step following (Fraser and
A B C
D
nnnnnnnnnnnnnn E
@@@@@@@
~~~~~~~
A B C
D
nnnnnnnnnnnnnn E
@@@@@@@
~~~~~~~
Figure 2: Two alignments with the same transla-
tional correspondence
Marcu, 2006b). Given the feature function param-
eters estimated in the M-step and the feature func-
tion weights ? determined in the D-step, the E-step
searches for the Viterbi alignment for the full train-
ing corpus.
We use 1 ? F-Measure as our error criterion.
(Fraser and Marcu, 2006a) established that it is im-
portant to tune ? (the trade-off between Precision
and Recall) to maximize performance. In working
with LEAF, we discovered a methodological prob-
lem with our baseline systems, which is that two
alignments which have the same translational cor-
respondence can have different F-Measures. An ex-
ample is shown in Figure 2.
To overcome this problem we fully interlinked the
transitive closure of the undirected bigraph formed
by each alignment hypothesized by our baseline
alignment systems1. This operation maps the align-
ment shown to the left in Figure 2 to the alignment
shown to the right. This operation does not change
the collection of phrases or rules extracted from a
hypothesized alignment, see, for instance, (Koehn et
al., 2003). Working with this fully interlinked rep-
resentation we found that the best settings of ? were
? = 0.1 for the Arabic/English task and ? = 0.4 for
the French/English task.
4 Experiments
4.1 Data Sets
We perform experiments on two large alignments
tasks, for Arabic/English and French/English data
sets. Statistics for these sets are shown in Table 2.
All of the data used is available from the Linguis-
tic Data Consortium except for the French/English
1All of the gold standard alignments were fully interlinked
as distributed. We did not modify the gold standard alignments.
55
1 chi(?i|ei) source word type 9 d2(4j|classf (fj)) movement for left-most target
non-head word
2 ?(4i|classe(ei)) choosing a head word 10 d>2(4j|classf (fj)) movement for subsequent target
non-head words
3 t1(fj |ei) head word translation 11 t(fj |ei) translation without dependency on word-type
4 s(?i|ei, ?i) ?i is number of words in target cept 12 t(fj |ei) translation table from final HMM iteration
5 s0(?0|
P
i ?i) number of unaligned target words 13 s(?i|?i) target cept size without dependency onsource head word e
6 t0(fj) identity of unaligned target words 14 s(?i|ei) target cept size without dependency on ?i
7 t>1(fj |ei, classh(?i1)) non-head word translation 15 target spurious word penalty
8 d1(4j|classe(e?), classf (fj)) movement for target
head words
16-30 (same features, other direction)
Table 1: Feature functions
gold standard alignments which are available from
the authors.
4.2 Experiments
To build all alignment systems, we start with 5 iter-
ations of Model 1 followed by 4 iterations of HMM
(Vogel et al, 1996), as implemented in GIZA++
(Och and Ney, 2003).
For all non-LEAF systems, we take the best per-
forming of the ?union?, ?refined? and ?intersection?
symmetrization heuristics (Och and Ney, 2003) to
combine the 1-to-N and M-to-1 directions resulting
in a M-to-N alignment. Because these systems do
not output fully linked alignments, we fully link the
resulting alignments as described at the end of Sec-
tion 3. The reader should recall that this does not
change the set of rules or phrases that can be ex-
tracted using the alignment.
We perform one main comparison, which is of
semi-supervised systems, which is what we will use
to produce alignments for SMT. We compare semi-
supervised LEAF with a previous state of the art
semi-supervised system (Fraser and Marcu, 2006b).
We performed translation experiments on the align-
ments generated using semi-supervised training to
verify that the improvements in F-Measure result in
increases in BLEU.
We also compare the unsupervised LEAF sys-
tem with GIZA++ Model 4 to give some idea of
the performance of the unsupervised model. We
made an effort to optimize the free parameters of
GIZA++, while for unsupervised LEAF there are
no free parameters to optimize. A single iteration
of unsupervised LEAF2 is compared with heuristic
2Unsupervised LEAF is equivalent to using the log-linear
model and setting ?m = 1 for m = 1 to 10 and m = 16 to 25,
symmetrization of GIZA++?s extension of Model 4
(which was run for four iterations). LEAF was boot-
strapped as described in Section 2.2 from the HMM
Viterbi alignments.
Results for the experiments on the French/English
data set are shown in Table 3. We ran GIZA++
for four iterations of Model 4 and used the ?re-
fined? heuristic (line 1). We ran the baseline semi-
supervised system for two iterations (line 2), and in
contrast with (Fraser and Marcu, 2006b) we found
that the best symmetrization heuristic for this sys-
tem was ?union?, which is most likely due to our
use of fully linked alignments which was discussed
at the end of Section 3. We observe that LEAF
unsupervised (line 3) is competitive with GIZA++
(line 1), and is in fact competitive with the baseline
semi-supervised result (line 2). We ran the LEAF
semi-supervised system for two iterations (line 4).
The best result is the LEAF semi-supervised system,
with a gain of 1.8 F-Measure over the LEAF unsu-
pervised system.
For French/English translation we use a state of
the art phrase-based MT system similar to (Och and
Ney, 2004; Koehn et al, 2003). The translation test
data is described in Table 2. We use two trigram lan-
guage models, one built using the English portion of
the training data and the other built using additional
English news data. The BLEU scores reported in
this work are calculated using lowercased and tok-
enized data. For semi-supervised LEAF the gain of
0.46 BLEU over the semi-supervised baseline is not
statistically significant (a gain of 0.78 BLEU would
be required), but LEAF semi-supervised compared
with GIZA++ is significant, with a gain of 1.23
BLEU. We note that this shows a large gain in trans-
while setting ?m = 0 for other values of m.
56
ARABIC/ENGLISH FRENCH/ENGLISH
A E F E
TRAINING
SENTS 6,609,162 2,842,184
WORDS 147,165,003 168,301,299 75,794,254 67,366,819
VOCAB 642,518 352,357 149,568 114,907
SINGLETONS 256,778 158,544 60,651 47,765
ALIGN DISCR.
SENTS 1,000 110
WORDS 26,882 37,635 1,888 1,726
LINKS 39,931 2,292
ALIGN TEST
SENTS 83 110
WORDS 1,510 2,030 1,899 1,716
LINKS 2,131 2,176
TRANS. DEV SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)WORDS 18,255 22.0K TO 24.6K 20,562 17,454
TRANS. TEST SENTS 1,056 (4 REFERENCES) 2,380 (1 REFERENCE)WORDS 28,505 35.8K TO 38.1K 58,990 49,182
Table 2: Data sets
lation quality over that obtained using GIZA++ be-
cause BLEU is calculated using only a single refer-
ence for the French/English task.
Results for the Arabic/English data set are also
shown in Table 3. We used a large gold standard
word alignment set available from the LDC. We ran
GIZA++ for four iterations of Model 4 and used the
?union? heuristic. We compare GIZA++ (line 1)
with one iteration of the unsupervised LEAF model
(line 2). The unsupervised LEAF system is worse
than four iterations of GIZA++ Model 4. We be-
lieve that the features in LEAF are too high dimen-
sional to use for the Arabic/English task without the
backoffs available in the semi-supervised models.
The baseline semi-supervised system (line 3) was
run for three iterations and the resulting alignments
were combined with the ?union? heuristic. We ran
the LEAF semi-supervised system for two iterations.
The best result is the LEAF semi-supervised system
(line 4), with a gain of 5.4 F-Measure over the base-
line semi-supervised system.
For Arabic/English translation we train a state of
the art hierarchical model similar to (Chiang, 2005)
using our Viterbi alignments. The translation test
data used is described in Table 2. We use two tri-
gram language models, one built using the English
portion of the training data and the other built using
additional English news data. The test set is from the
NIST 2005 translation task. LEAF had the best per-
formance scoring 1.43 BLEU better than the base-
line semi-supervised system, which is statistically
significant.
5 Previous Work
The LEAF model is inspired by the literature on gen-
erative modeling for statistical word alignment and
particularly by Model 4 (Brown et al, 1993). Much
of the additional work on generative modeling of 1-
to-N word alignments is based on the HMM model
(Vogel et al, 1996). (Toutanova et al, 2002) and
(Lopez and Resnik, 2005) presented a variety of re-
finements of the HMM model particularly effective
for low data conditions. (Deng and Byrne, 2005)
described work on extending the HMM model us-
ing a bigram formulation to generate 1-to-N align-
ment structure. The common thread connecting
these works is their reliance on the 1-to-N approx-
imation, while we have defined a generative model
which does not require use of this approximation, at
the cost of having to rely on local search.
There has also been work on generative models
for other alignment structures. (Wang and Waibel,
1998) introduced a generative story based on ex-
tension of the generative story of Model 4. The
alignment structure modeled was ?consecutive M
to non-consecutive N?. (Marcu and Wong, 2002)
defined the Joint model, which modeled consec-
utive word M-to-N alignments. (Matusov et al,
2004) presented a model capable of modeling 1-to-
N and M-to-1 alignments (but not arbitrary M-to-
N alignments) which was bootstrapped from Model
4. LEAF directly models non-consecutive M-to-N
alignments.
One important aspect of LEAF is its symmetry.
(Och and Ney, 2003) invented heuristic symmetriza-
57
FRENCH/ENGLISH ARABIC/ENGLISH
SYSTEM F-MEASURE (? = 0.4) BLEU F-MEASURE (? = 0.1) BLEU
GIZA++ 73.5 30.63 75.8 51.55
(FRASER AND MARCU, 2006B) 74.1 31.40 79.1 52.89
LEAF UNSUPERVISED 74.5 72.3
LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34
Table 3: Experimental Results
tion of the output of a 1-to-N model and a M-to-1
model resulting in a M-to-N alignment, this was ex-
tended in (Koehn et al, 2003). We have used in-
sights from these works to help determine the struc-
ture of our generative model. (Zens et al, 2004)
introduced a model featuring a symmetrized lexi-
con. (Liang et al, 2006) showed how to train two
HMM models, a 1-to-N model and a M-to-1 model,
to agree in predicting all of the links generated, re-
sulting in a 1-to-1 alignment with occasional rare 1-
to-N or M-to-1 links. We improve on these works by
choosing a new structure for our generative model,
the head word link structure, which is both sym-
metric and a robust structure for modeling of non-
consecutive M-to-N alignments.
In designing LEAF, we were also inspired by
dependency-based alignment models (Wu, 1997;
Alshawi et al, 2000; Yamada and Knight, 2001;
Cherry and Lin, 2003; Zhang and Gildea, 2004). In
contrast with their approaches, we have a very flat,
one-level notion of dependency, which is bilingually
motivated and learned automatically from the paral-
lel corpus. This idea of dependency has some sim-
ilarity with hierarchical SMT models such as (Chi-
ang, 2005).
The discriminative component of our work is
based on a plethora of recent literature. This lit-
erature generally views the discriminative modeling
problem as a supervised problem involving the com-
bination of heuristically derived feature functions.
These feature functions generally include the predic-
tion of some type of generative model, such as the
HMM model or Model 4. A discriminatively trained
1-to-N model with feature functions specifically de-
signed for Arabic was presented in (Ittycheriah and
Roukos, 2005). (Lacoste-Julien et al, 2006) created
a discriminative model able to model 1-to-1, 1-to-
2 and 2-to-1 alignments for which the best results
were obtained using features based on symmetric
HMMs trained to agree, (Liang et al, 2006), and
intersected Model 4. (Ayan and Dorr, 2006) de-
fined a discriminative model which learns how to
combine the predictions of several alignment algo-
rithms. The experiments performed included Model
4 and the HMM extensions of (Lopez and Resnik,
2005). (Moore et al, 2006) introduced a discrimi-
native model of 1-to-N and M-to-1 alignments, and
similarly to (Lacoste-Julien et al, 2006) the best re-
sults were obtained using HMMs trained to agree
and intersected Model 4. LEAF is not bound by
the structural restrictions present either directly in
these models, or in the features derived from the
generative models used. We also iterate the gener-
ative/discriminative process, which allows the dis-
criminative predictions to influence the generative
model.
Our work is most similar to work using discrim-
inative log-linear models for alignment, which is
similar to discriminative log-linear models used for
the SMT decoding (translation) problem (Och and
Ney, 2002; Och, 2003). (Liu et al, 2005) presented
a log-linear model combining IBM Model 3 trained
in both directions with heuristic features which re-
sulted in a 1-to-1 alignment. (Fraser and Marcu,
2006b) described symmetrized training of a 1-to-
N log-linear model and a M-to-1 log-linear model.
These models took advantage of features derived
from both training directions, similar to the sym-
metrized lexicons of (Zens et al, 2004), including
features derived from the HMM model and Model
4. However, despite the symmetric lexicons, these
models were only able to optimize the performance
of the 1-to-N model and the M-to-1 model sepa-
rately, and the predictions of the two models re-
quired combination with symmetrization heuristics.
We have overcome the limitations of that work by
defining new feature functions, based on the LEAF
generative model, which score non-consecutive M-
to-N alignments so that the final performance crite-
rion can be optimized directly.
58
6 Conclusion
We have found a new structure over which we can
robustly predict which directly models translational
correspondence commensurate with how it is used
in hierarchical SMT systems. Our new generative
model, LEAF, is able to model alignments which
consist of M-to-N non-consecutive translational cor-
respondences. Unsupervised LEAF is comparable
with a strong baseline. When coupled with a dis-
criminative training procedure, the model leads to
increases between 3 and 9 F-score points in align-
ment accuracy and 1.2 and 2.8 BLEU points in trans-
lation accuracy over strong French/English and Ara-
bic/English baselines.
7 Acknowledgments
This work was partially supported under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022. We
would like to thank the USC Center for High Per-
formance Computing and Communications.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John D. Lafferty, I. Dan Melamed, David
Purdy, Franz J. Och, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation, final
report, JHU workshop.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maxi-
mum entropy approach to combining word alignments.
In Proceedings of HLT-NAACL, pages 96?103, New
York.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
ACL, pages 88?95, Sapporo, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
Yonggang Deng and William Byrne. 2005. Hmm word
and phrase alignment for statistical machine trans-
lation. In Proceedings of HLT-EMNLP, Vancouver,
Canada.
Alexander Fraser and Daniel Marcu. 2006a. Measuring
word alignment quality for statistical machine transla-
tion. In Technical Report ISI-TR-616, ISI/University
of Southern California.
Alexander Fraser and Daniel Marcu. 2006b. Semi-
supervised training for statistical word alignment. In
Proceedings of COLING-ACL, pages 769?776, Syd-
ney, Australia.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast decoding and
optimal decoding for machine translation. Artificial
Intelligence, 154(1-2):127?143.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT-EMNLP,
pages 89?96, Vancouver, Canada.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL, pages 127?133, Edmonton, Canada.
Simon Lacoste-Julien, Dan Klein, Ben Taskar, and
Michael Jordan. 2006. Word alignment via quadratic
assignment. In Proceedings of HLT-NAACL, pages
112?119, New York, NY.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
New York.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of ACL,
pages 459?466, Ann Arbor, MI.
Adam Lopez and Philip Resnik. 2005. Improved hmm
alignment models for languages with scarce resources.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 83?86, Ann Arbor, MI.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of EMNLP, pages 133?139,
Philadelphia, PA.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING,
Geneva, Switzerland.
59
Robert C. Moore, Wen-Tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word align-
ment. In Proceedings of COLING-ACL, pages 513?
520, Sydney, Australia.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL, pages
295?302, Philadelphia, PA.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417?449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proceedings of EMNLP,
Philadelphia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841,
Copenhagen, Denmark.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in statistical machine translation. In Pro-
ceedings of COLING-ACL, volume 2, pages 1357?
1363, Montreal, Canada.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL,
pages 523?530, Toulouse, France.
Richard Zens, Evgeny Matusov, and Hermann Ney.
2004. Improved word alignment using a symmetric
lexicon model. In Proceedings of COLING, Geneva,
Switzerland.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Proceed-
ings of COLING, Geneva, Switzerland.
60
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 97?104, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Large-Scale Exploration of Effective Global Features
for a Joint Entity Detection and Tracking Model
Hal Daume? III and Daniel Marcu
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
hdaume,marcu  @isi.edu
Abstract
Entity detection and tracking (EDT) is
the task of identifying textual mentions
of real-world entities in documents, ex-
tending the named entity detection and
coreference resolution task by consider-
ing mentions other than names (pronouns,
definite descriptions, etc.). Like NE tag-
ging and coreference resolution, most so-
lutions to the EDT task separate out the
mention detection aspect from the corefer-
ence aspect. By doing so, these solutions
are limited to using only local features for
learning. In contrast, by modeling both
aspects of the EDT task simultaneously,
we are able to learn using highly com-
plex, non-local features. We develop a
new joint EDT model and explore the util-
ity of many features, demonstrating their
effectiveness on this task.
1 Introduction
In many natural language applications, such as au-
tomatic document summarization, machine transla-
tion, question answering and information retrieval,
it is advantageous to pre-process text documents to
identify references to entities. An entity, loosely
defined, is a person, location, organization or geo-
political entity (GPE) that exists in the real world.
Being able to identify references to real-world enti-
ties of these types is an important and difficult natu-
ral language processing problem. It involves finding
text spans that correspond to an entity, identifying
what type of entity it is (person, location, etc.), iden-
tifying what type of mention it is (name, nominal,
pronoun, etc.) and finally identifying which other
mentions in the document it corefers with. The dif-
ficulty lies in the fact that there are often many am-
biguous ways to refer to the same entity. For exam-
ple, consider the two sentences below:
Bill ClintonNAMPER?1 gave a speech today tothe SenateNAMORG?2 . The PresidentNOMPER?1 outlinedhisPROPER?1 plan for budget reform to themPROORG?2 .
There are five entity mentions in these two sen-
tences, each of which is underlined (the correspond-
ing mention type and entity type appear as super-
scripts and subscripts, respectively, with coreference
chains marked in the subscripts), but only two enti-
ties:  Bill Clinton, The president, his  and  the
Senate, them  . The mention detection task is to
identify the entity mentions and their types, without
regard for the underlying entity sets, while corefer-
ence resolution groups a given mentions into sets.
Current state-of-the-art solutions to this problem
split it into two parts: mention detection and coref-
erence (Soon et al, 2001; Ng and Cardie, 2002; Flo-
rian et al, 2004). First, a model is run that attempts
to identify each mention in a text and assign it a type
(person, organization, etc.). Then, one holds these
mentions fixed and attempts to identify which ones
refer to the same entity. This is typically accom-
plished through some form of clustering, with clus-
tering weights often tuned through some local learn-
ing procedure. This pipelining scheme has the sig-
nificant drawback that the mention detection module
cannot take advantage of information from the coref-
erence module. Moreover, within the coreference
97
task, performing learning and clustering as separate
tasks makes learning rather ad-hoc.
In this paper, we build a model that solves the
mention detection and coreference problems in a
simultaneous, joint manner. By doing so, we are
able to obtain an empirically superior system as well
as integrate a large collection of features that one
cannot consider in the standard pipelined approach.
Our ability to perform this modeling is based on the
Learning as Search Optimization framework, which
we review in Section 2. In Section 3, we describe
our joint EDT model in terms of the search proce-
dure executed. In Section 4, we describe the features
we employ in this model; these include the stan-
dard lexical, semantic (WordNet) and string match-
ing features found in most other systems. We ad-
ditionally consider many other feature types, most
interestingly count-based features, which take into
account the distribution of entities and mentions
(and are not expressible in the binary classification
method for coreference) and knowledge-based fea-
tures, which exploit large corpora for learning name-
to-nominal references. In Section 5, we present our
experimental results. First, we compare our joint
system with a pipelined version of the system, and
show that joint inference leads to improved perfor-
mance. Next, we perform an extensive feature com-
parison experiment to determine which features are
most useful for the coreference task, showing that
our newly introduced features provide useful new in-
formation. We conclude in Section 6.
2 Learning as Search Optimization
When one attempts to apply current, standard ma-
chine learning algorithms to problems with combi-
natorial structured outputs, the resulting algorithm
implicitly assumes that it is possible to find the
best structures for a given input (and some model
parameters). Furthermore, most models require
much more, either in the form of feature expecta-
tions for conditional likelihood-based methods (Laf-
ferty et al, 2001) or local marginal distributions
for margin-based methods (Taskar et al, 2003). In
many cases?including EDT and coreference?this
is a false assumption. Often, we are not able to find
the best solution, but rather must employ an approx-
imate search to find the best possible solution, given
time and space constraints. The Learning as Search
Algo Learn(problem, initial, enqueue,  ,  ,  )
nodes  MakeQueue(MakeNode(problem,initial))
while nodes is not empty do
node  RemoveFront(nodes)
if none of nodes 
	 node  is  -good or
GoalTest(node) and node is not  -good then
sibs  siblings(node,  )
 update(  ,  , sibs, node  nodes)
nodes  MakeQueue(sibs)
else
if GoalTest(node) then return 
next  Operators(node)
nodes  enqueue(problem, nodes, next,  )
end if
end while
Figure 1: The generic search/learning algorithm.
Optimization (LaSO) framework exploits this diffi-
culty as an opportunity and seeks to find model pa-
rameters that are good within the context of search.
More formally, following the LaSO framework,
we assume that there is a set of input structures 
and a set of output structures  (in our case, ele-
ments  will be documents and elements 
will be documents marked up with mentions and
their coreference sets). Additionally, we provide the
structure of a search space  that results in elements
of  (we will discuss our choice for this component
later in Section 3). The LaSO framework relies on
a monotonicity assumption: given a structure 
and a node  in the search space, we must be able
to calculate whether it is possible for this node  to
eventually lead to  (such nodes are called  -good).
LaSO parameterizes the search process with a
weight vector The Rhetorical Parsing of Unrestricted 
Texts: A Surface-based Approach 
Daniel Marcu* 
Information Sciences Institute, USC 
Coherent texts are not just simple sequences ofclauses and sentences, but rather complex artifacts 
that have highly elaborate rhetorical structure. This paper explores the extent to which well-formed 
rhetorical structures can be automatically derived by means of surface-form-based algorithms. 
These algorithms identify discourse usages of cue phrases and break sentences into clauses, hy- 
pothesize rhetorical relations that hold among textual units, and produce valid rhetorical structure 
trees for unrestricted natural anguage texts. The algorithms are empirically grounded in a corpus 
analysis of cue phrases and rely on a first-order formalization of rhetorical structure trees. 
The algorithms are evaluated both intrinsically and extrinsically. The intrinsic evaluation 
assesses the resemblance b tween automatically and manually constructed rhetorical structure 
trees. The extrinsic evaluation shows that automatically derived rhetorical structures can be 
successfully exploited in the context of text summarization. 
1. Motivat ion 
Consider the text given in (1), which was taken from Scientific American, November 
1996. 
(1) With its distant orbit--50 percent farther from the sun than Earth--and 
slim atmospheric blanket, Mars experiences frigid weather conditions. 
Surface temperatures typically average about -60 degrees Celsius (-76 
degrees Fahrenheit) at the equator and can dip to -123 degrees C near 
the poles. Only the midday sun at tropical atitudes is warm enough to 
thaw ice on occasion, but any liquid water formed in this way would 
evaporate almost instantly because of the low atmospheric pressure. 
Although the atmosphere holds a small amount of water, and 
water-ice clouds sometimes develop, most Martian weather involves 
blowing dust or carbon dioxide. Each winter, for example, a blizzard of 
frozen carbon dioxide rages over one pole, and a few meters of this 
dry-ice snow accumulate as previously frozen carbon dioxide evaporates 
from the opposite polar cap. Yet even on the summer pole, where the 
sun remains in the sky all day long, temperatures never warm enough to 
melt frozen water. 
A rhetorical structure representation (tree) of its first paragraph is shown in Figure 1. 
In the rhetorical representation, which employs the conventions proposed by Mann 
and Thompson (1988), each leaf of the tree is associated with a contiguous textual 
* Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey, 
CA 90292-6601 
(~ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 3 
EVIDENCE 
With its distanl orbit M,u's experiences frigid 
( - 50 pcrccnt l~nhc~ weather  conditions. 
from \[hc sun Ihan F:~th -} / 
o . ,  . . . . . . . . . . . . .  . . . . . . . . . . . . .  0 . . . . . . . . .  2 . . . . . . . . . . . . . . . .   ON-VOLmON CAUSE 
blanket typically ~vcragc about  degrees C n~ the at tropical latit udcs If------------. ( ( -70 degrees  Fahrenhe i t  }) po les  is w~m enough to 
' "on at the equat~ thaw ice on ~ t  . but ~y l iquid water  bccaus~ o f  the 
fo r~d in Ihis way would low ntmosph~ic 
cvnporate a~l  inst~t ly  pressure. 
Figure 1 
A rhetorical structure representation f the first paragraph in text (1). 
span. The internal nodes are labeled with the names of the rhetorical relations that 
hold between the textual spans that are subsumed by their child nodes. Each relation 
between two nodes is represented graphically by means of a combination of straight 
lines and arcs. The material subsumed by the text span that corresponds tothe starting 
point of an arc is subsidiary to the material subsumed by the text span that corresponds 
to the end point of an arc. A relation represented only by straight lines corresponds 
to cases in which the subsumed text spans are equally important. Text spans that 
subsume subsidiary information, i.e., text spans that correspond to starting points 
of arcs, are called satellites. All other text spans are called nuclei. Text fragments 
surrounded by curly brackets denote parenthetical units: their deletion does not affect 
the understanding of the textual unit to which they belong. 
For example, the textual unit Mars experiences frigid weather conditions is at the end 
of an arc that originates from the textual unit With its distant orbit--50 percent farther from 
the sun than Earth---and slim atmospheric blanket because the former epresents something 
that is more essential to the writer's purpose than the latter and because the former can 
be understood even if the subsidiary span is deleted, but not vice versa. The satellite 
information JUSTIFIES in this case the writer's right to present he information i  the 
nucleus. The text spans Only the midday sun at tropical latitudes is warm enough to thaw ice on 
occasion, and but any liquid water formed in this way would evaporate almost instantly because 
of the low atmospheric pressure are connected by straight lines because they are equally 
important with respect o the writer's purpose; they correspond to the elements of 
a CONTRAST relation. The text fragment--50 percent farther from the sun than Earth--is 
surrounded by curly brackets because it is parenthetical. 
Traditionally, it has been assumed that rhetorical structures of the kind shown in 
Figure 1 can be built only if one understands fully the semantics of the text and the 
intentions of the writer. To understand, for example, that the information given in the 
last two sentences of the first paragraph of text (1) is EVIDENCE to the information 
given in the first sentence, one needs to understand that the last two sentences may 
increase the reader's belief of the first sentence. And to understand that it was the low 
atmospheric pressure that caused the liquid water to evaporate, one needs to understand 
that without he information presented in the satellite, the reader may not know the 
particular CAUSE of the situation presented in the nucleus. 
In spite of the large number of discourse-related theories that have been proposed 
so far, there have emerged no algorithms capable of deriving the discourse structure 
of free, unrestricted texts. On one hand, the theories developed in the traditional, 
truth-based semantic perspective (Kamp 1981; Lascarides and Asher 1993; Asher 1993; 
Hobbs et al 1993; Kamp and Reyle 1993; Asher and Lascarides 1994; Kameyama 
396 
Marcu Rhetorical Parsing of Unrestricted Texts 
1994; Polanyi and van den Berg 1996; van den Berg 1996; Gardent 1997; Schilder 
1997; Cristea and Webber 1997; Webber et al 1999) take the position that discourse 
structures can be built only in conjunction with fully specified clause and sentence 
syntactic structures. These theories have a grammar as their backbone and rely on 
sophisticated logics of belief and default logics in order to intertwine and characterize 
the sentence- and discourse-based linguistic phenomena. Despite their formal elegance, 
implementations of these theories cannot yet handle naturally occurring texts, such 
as that shown in (1). On the other hand, the theories aimed at characterizing the 
constraints that pertain to the structure of unrestricted texts and the computational 
mechanisms that would enable the derivation of these structures (van Dijk 1972; Zock 
1985; Grosz and Sidner 1986; Mann and Thompson 1988; Polanyi 1988, 1996; Hobbs 
1990) are either too informal or incompletely specified to support a fully automatic 
approach to discourse analysis. 
In this paper, I explore the ground found at the intersection of these two lines of 
research. More precisely, I explore the extent o which rhetorical structures of the kind 
shown in Figure 1 can be built automatically b  relying only on cohesion and connec- 
tives, i.e., phrases uch as for example, and, although, and however that are used "to link 
linguistic units at any level" (Crystal 1991, 74). 1 The results how that although cohe- 
sion and connectives are ambiguous indicators of rhetorical structure, when used in 
conjunction with a well-constrained mathematical model of valid rhetorical structures, 
they enable the implementation f surprisingly accurate rhetorical parsers. 
2. Foundation 
The hypothesis that underlies this work is that connectives, cohesion, shallow pro- 
cessing, and a well-constrained mathematical model of valid rhetorical structure trees 
(RS-trees) can be used to implement algorithms that determine 
? the elementary units of a text, i.e., the units that constitute the leaves of 
the RS-tree of that text; 
? the rhetorical relations that hold between elementary units and between 
spans of text; 
? the relative importance (nucleus or satellite) and the size of the spans 
subsumed by these rhetorical relations. 
In what follows, I examine ach facet of this hypothesis intuitively and explain how 
it contributes to the derivation of a rhetorical parsing algorithm, i.e., an algorithm 
that takes as input free, unrestricted text and that determines its valid RS-trees. For 
each facet, I consider first the arguments hat support he hypothesis and then discuss 
potential difficulties. 
2.1 Determining the Elementary Units Using Connectives and Shallow Processing 
2.1.1 Pro Arguments. Recent developments in the linguistics of punctuation (Nun- 
berg 1990; Briscoe 1996; Pascual and Virbel 1996; Say and Akman 1996; Shiuan and 
Ann 1996) have emphasized the role that punctuation can have in solving a variety 
of natural anguage processing tasks ranging from syntactic parsing to information 
1 In this paper, I use the terms connective and cue phrase interchangeably. AndI use the term discourse 
marker to refer to a connective that has a discourse function, i.e., a connective that signals arhetorical 
relation that holds between two text spans. 
397 
Computational Linguistics Volume 26, Number 3 
packaging. For example, if a sentence consists of three arguments eparated by semi- 
colons, it is likely that one can determine the boundaries of these arguments without 
relying on sophisticated forms of syntactic analysis. Shallow processing is sufficient 
to recognize the occurrences of the semicolons and to break the sentence into three 
elementary units. 
In a corpus study (described in Section 3), I noticed that in most of the cases in 
which a connective such as Although occurred at the beginning of a sentence, it marked 
the left boundary of an elementary unit whose right boundary was given by the first 
subsequent occurrence of a comma. Hence, it is likely that by using only shallow 
techniques and knowledge about connectives, one can determine, for example, that 
the elementary units of sentence (2) are those enclosed within square brackets. 
(2) \[Although Brooklyn College does not yet have a junior-year-abroad 
program,\] [a good number of students pend summers in Europe.\] 
2.1.2 Difficulties. Obviously, by relying only on orthography, connectives, and shallow 
processing it is unlikely that one will be capable of correctly determining all elementary 
units of an RS-tree. It may very well be the case that knowledge about how Although 
is used in texts can be exploited to determine the elementary units of texts, but not 
all connectives are used as consistently as Although is. Just consider, for instance, the 
highly ambiguous connective and. In some cases, and plays a sentential, syntactic role, 
while in others, it plays a discourse role, i.e., it signals a rhetorical relation that holds 
between two textual units. For example, in sentence (3), the first and is sentential, i.e., it 
makes a semantic ontribution to the interpretation of the complex noun phrase "John 
and Mary", while the second and has a discourse function, i.e., it signals a rhetorical 
relation of SEQUENCE that holds between the units enclosed within square brackets. 
(3) \[John and Mary went to the theatre\] [and saw a nice play.\] 
If a system is to use connectives to determine lementary unit boundaries, it would 
need to figure out that a boundary is required before the second occurrence of and (the 
occurrence that has a discourse function), but not before the first occurrence. It seems 
clear that shallow processing is insufficient o properly solve this problem. It remains 
an open question, however, to what degree shallow processing and knowledge about 
connectives can be successfully used to determine the elementary units of texts. Our 
results how (see Section 4), that using only such lean knowledge resources, elementary 
unit boundaries can be determined with approximately 80% accuracy. 
2.2 Determining Rhetorical Relations Using Connectives 
2.2.1 Pro Arguments. The intuition behind this choice relies on the following facts: 
Linguistic and psycholinguistic research as shown that connectives are 
consistently used by humans both as cohesive ties between adjacent 
clauses and sentences (Halliday and Hasan 1976) and as 
"macroconnectors" that signal relations that hold between large textual 
units. For example, in stories, connectives such as so, but, and and mark 
boundaries between story parts (Kintsch 1977). In naturally occurring 
conversations, somarks the terminal point of a main discourse unit and 
a potential transition in a participant's turn, whereas and coordinates 
idea units and continues a speaker's action (Schiffrin 1987). In narratives, 
398 
Marcu Rhetorical Parsing of Unrestricted Texts 
connectives signal structural relations between elements and are crucial 
for the understanding of the stories (Segal and Duchan 1997). In general, 
cue phrases are used consistently by both speakers and writers to 
highlight the most important shifts in their narratives, mark intermediate 
breaks, and signal areas of topical continuity (Bestgen and Costermans 
1997; Schneuwly 1997). Therefore, it is likely that connectives can be 
used to determine rhetorical relations that hold both between elementary 
units and between large spans of text. 
The number of discourse markers in a typical text--approximately one 
marker for every two clauses (Redeker 1990)--is sufficiently large to 
enable the derivation of rich rhetorical structures for texts. 2More 
importantly, the absence of markers correlates with a preference of 
readers to interpret he unmarked textual units as continuations of the 
topics of the units that precede them (Segal, Duchan, and Scott 1991). 
Hence, when there is no connective between two sentences, for example, 
it is likely that the second sentence laborates on the first. 
2.2.2 Difficulties. The above arguments tell us that connectives are used often and 
that they signal relations that hold both between elementary units and large spans 
of texts. Hence, previous research tells us only that connectives are potentially useful 
in determining the rhetorical structure of texts. Unfortunately, they cannot be used 
straightforwardly because they are ambiguous. 
? In some cases, connectives have a sentential function, while in other 
cases, they have a discourse function. Unless we can determine when a 
connective has a discourse function, we cannot use connectives to 
hypothesize rhetorical relations. 
? Connectives do not explicitly signal the size of the textual spans that 
they relate. 
? Connectives can signal more than one rhetorical relation. That is, there is 
no one-to-one mapping between the use of connectives and the 
rhetorical relations that they signal. 
I address each of these three problems in turn. 
Sentential and Discourse Uses of Connectives. Empirical studies on the disambiguation of
cue phrases (Hirschberg and Litman 1993) have shown that just by considering the 
orthographic environment in which they occur, one can distinguish between sentential 
and discourse uses in about 80% of cases and that these results can be improved 
with machine learning techniques (Litman 1996) or genetic algorithms (Siegel and 
McKeown 1994). I have taken Hirschberg and Litman's research one step further and 
designed a comprehensive corpus analysis of cue phrases that enabled me to design 
algorithms that improved their results and coverage. The corpus analysis is discussed 
in Section 3. The algorithm that determines elementary unit boundaries and identifies 
discourse uses of cue phrases is discussed in Section 4. 
2 A corpus of instructional texts that was studied by Moser and Moore (1997) and Di Eugenio, Moore, 
and Paolucci (1997) reflected approximately the same distribution of cue phrases: 181 of the 406 
discourse relations that they analyzed were cued relations. 
399 
Computational Linguistics Volume 26, Number 3 
Discourse Markers are Ambiguous with Respect to the Size of the Spans They Connect. Assume, 
for example, that a computer is supposed to determine, using only surface-form al- 
gorithms and knowledge about connectives, the rhetorical relation that is signaled by 
the marker In contrast, in text (4). 
(4) \[John likes sweets, t \] \[Most of all, John likes ice cream and chocolate. 2\] \[In 
contrast, Mary likes fruit. 3\] \[Especially bananas and strawberries. 4\]
During the corpus study that I discuss in Section 3, I noticed that in all its occurrences 
in a sample of texts, the connective In contrast signaled a CONTRAST relation. Hence, it 
is likely that In contrast signals a CONTRAST relation in text (4) as well. Unfortunately, 
although we know what relation In contrast signals, we do not know which spans 
the CONTRAST relation holds between: does the relation hold between spans \[1,2\] and 
\[3,4\]; or between unit 2 and span \[3,4\]; or between span \[1,2\] and unit 3; or between 
units 1 and 3; or between other units and spans? The best that we can do in this case 
is to make an exclusively disjunctive hypothesis, i.e., to hypothesize that one and only 
one of these possible relations holds. However, it is still unclear what the elements of 
such an exclusively disjunctive hypothesis hould be. 
In my previous work (Marcu 1996, 1997a, 1997b, 1999b, 2000), I have argued that 
rhetorical relations that hold between large textual spans can be explained in terms 
of similar relations that hold between their most important elementary units. For 
example, the rhetorical relation of EVIDENCE that holds between the first sentence 
of the first paragraph in text (1) and the last two sentences of the same paragraph 
can be explained in terms of a similar relation that holds between the corresponding 
nuclei: an EVIDENCE relation also holds between the nucleus of the first sentence, Mars 
experiences frigid weather conditions and each of the most important nuclei of the last 
two sentences Surface temperatures typically average about -60 degrees Celsius (-76 degrees 
Fahrenheit) at the equator and \[Surface temperatures\] can dip to -123 degrees C near the poles. 
Similarly, the CONTRAST relation that holds between the two spans \[1,2\] and \[3,4\] in 
text (4) can be explained in terms of a CONTRAST relation that holds between units 1 
and 3, the most important units in spans \[1,2\] and \[3,4\], respectively. 
The fact that rhetorical relations that hold between large textual spans can be 
explained/determined in terms of rhetorical relations that hold between elementary 
textual units suggests that rhetorical structure trees can be constructed in a bottom-up 
fashion, from rhetorical relations that have been determined to hold between ele- 
mentary textual units. Hence, to derive the rhetorical structure of text (4) it is suffi- 
cient to hypothesize with respect o the occurrence of the connective In contrast, the 
exclusively disjunctive hypothesis rhet_rel(CONTRAST, 1 3) (9 rhet_rel(CONTRAST, 1 4) (9 
rhet_rel(CONTRAST, 2 3) (grhet_rel(coNTRAST, 2, 4), because this hypothesis subsumes all 
the other possible rhetorical relations that may be signaled by the connective. 3 In Sec- 
tion 2.4, I will explain why exclusive-disjunctive hypotheses of this kind are sufficient 
for determining the rhetorical structure of texts. 
The fact that rhetorical relations that hold between large spans can be explained 
in terms of rhetorical relations that hold between elementary units should not lead 
one to conclude that a computational system should only make rhetorical hypotheses 
whose arguments are elementary units. For example, a text fragment may consist of 
three paragraphs, clearly marked by the connectives First, Second, and Third. For such 
3 Throughout this paper, I use the convention that rhetorical relations are represented as sorted, 
first-order predicates having the form rhet_rel(NAME, SATELLITE, NUCLEUS) in the case of hypotactic 
relations and the form rhet_rel(NAME, NUCLEUS, NUCLEUS) in the case of paratactic relations. 
400 
Marcu Rhetorical Parsing of Unrestricted Texts 
a fragment, it is likely that the three paragraphs are in a LIST or SEQUENCE relation. 
If a computer program exploits the occurrence of these markers, it may be able to 
derive the high-level rhetorical structure of the text fragment without determining the 
important units and relations that underlie the three paragraphs. The work presented 
in this paper acknowledges the utility of dealing both with simple relations, i.e., rhetor- 
ical relations that hold between elementary textual units, and with extended rhetorical 
relations, i.e., relations that hold between large segments. Depending on the circum- 
stances, a computational system will have to choose the types of relations it should 
hypothesize to determine the rhetorical structure of a text. 
The observation that rhetorical relations that hold between large textual spans 
can be explained in terms of rhetorical relations that hold between elementary textual 
units and the need for dealing with extended rhetorical relations amount o providing 
a composit ional ity criterion for valid rhetorical structures. This criterion posits that 
a rhetorical structure tree is valid only if each rhetorical relation that holds between 
two spans is either an extended rhetorical relation or can be explained in terms of a 
simple rhetorical relation. 
Discourse Markers are Ambiguous with Respect to the Rhetorical Relations They Signal. Dis- 
course markers are also ambiguous with respect o the rhetorical relations they signal 
and the importance of the textual spans they relate. For example, the occurrence of 
the discourse marker But at the beginning of a sentence most often signals either a 
mononuclear relation of ANTITHESIS or CONCESSION between a satellite, a textual span 
that precedes the occurrence of But, and a nucleus, a textual span that starts with But; or 
a multinuclear relation of CONTRAST between two nuclei: a textual span that precedes 
the occurrence of But and a textual span that starts with But. An exclusive disjunction 
is again an adequate way to formalize this hypothesis. For example, the exclusive dis- 
junction rhet_rel(ANTITHESIS, 1, 2) ? rhet_rel(cONCESSION, 1, 2) ? rhet_rel(CONTRAST, 1, 2) 
expresses the best hypothesis that one can make on the basis of the occurrence of the 
marker But in text (5). As mentioned already, in Section 2.4 it will become apparent 
why such exclusively disjunctive hypotheses are sufficient for deriving the rhetorical 
structure of texts. 
(5) \[Bill had no parents, t \] \[But he had seven brothers and sisters. 1\] 
2.2.3 Discussion. The more complex the texts one is trying to analyze and the more 
ambiguous the connectives a text employs, the more likely the rhetorical relations 
that hold between elementary units and spans cannot be hypothesized precisely by 
automatic means. Most often, connectives, tense, pronoun uses, etc. only suggest hat 
some rhetorical relations hold between some textual units; rarely can hypotheses be 
made with 100% confidence. 
When a computer program processes free texts and comes across a connective such 
as But, for example, unless it carries out a complete semantic analysis and understands 
the intentions of the writer, it won't  be able to determine unambiguously what relation 
to use; and it won't  be able to determine what units or spans are involved in the 
relation. What is certain, though, is that But, the hypothesis trigger in this example, 
can signal at most one such relation-- in my empirical work (see Section 3), I have never 
come across a case in which a simple connective signaled more than one rhetorical 
relation. In general then, if But occurs in unit i of a text, we know that it can signal a 
rhetorical relation that holds between one unit in the interval \[i - k,  i - 1\]  and one unit 
in the interval \[i, i + k\], where k is a sufficiently large constant; or a relation between 
two spans \[i - kl, i - 1\] and \[i, i + k2\]. Figure 2 provides a graphical representation f 
401 
Computational Linguistics Volume 26, Number 3 
i - k  i -  I : i i+k  
Figure 2 
A graphical representation f the disjunctive hypothesis that is triggered by the occurrence of
the marker But at the beginning of unit i of a text. 
the simple rhetorical relations that can be hypothesized on the basis of the connective 
But in unit i. 
In this paper, I will focus on dealing only with this sort of exclusively disjunctive 
hypotheses, i.e., hypotheses whose disjuncts subsume text spans that overlap. For 
example, in Figure 2, all disjuncts span over the segment \[i - 1, i\]. From a linguistic 
perspective, only such hypotheses make sense. Although one can hypothesize on the 
basis of the occurrence of a discourse marker in unit i that a rhetorical relation R holds 
either between units i - 2 and i - 1 or between units i and i + 1, for example, such 
a hypothesis will be ill-formed. In the discourse analyses I have carried out so far, I 
have never come across an example that would require one to deal with exclusively 
disjunctive hypotheses different from that shown graphically in Figure 2. 
2.3 Determining Rhetorical Relations Using Cohesion 
2.3.1 Pro Arguments. Youmans (1991), Hoey (1991), Morris and Hirst (1991), Salton 
et al (1995), Salton and Allan (1995), and Hearst (1997) have shown that word co- 
occurrences and more sophisticated forms of lexical cohesion can be used to deter- 
mine segments of topical and thematic ontinuity. And Morris and Hirst (1991) have 
also shown that there is a correlation between cohesion-defined textual segments and 
hierarchical, intentionally defined segments (Grosz and Sidner 1986). For example, 
if the first three paragraphs of a text talk about the moon and the subsequent two 
paragraphs talk about the Earth, it is possible that the rhetorical structure of the text 
is characterized by two spans that subsume these two sets of paragraphs and that 
a rhetorical relation of JOINT or LIST holds between the two spans. Also, studies by 
Harabagiu, Moldovan, and Maiorano (Harabagiu and Maiorano 1996; Harabagiu and 
Moldovan 1999) show that cohesion can be used to determine rhetorical relations that 
hold between smaller discourse constituents as well. For example, if one sentence talks 
about vegetables and another sentence talks about carrots and beets, it is possible that 
a rhetorical relation of ELABORATION holds between the two sentences because carrots 
and beets are kinds of vegetables. 
2.3.2 Difficulties. For the purpose of this paper, I use a very coarse model of the 
relation between cohesion and rhetorical relations. More specifically, I assume that a 
mononuclear rhetorical relation of ELABORATION or BACKGROUND holds between two 
textual segments that talk about the same thing, i.e., they share some words, and that 
a multinuclear relation of JOINT holds between two segments that talk about different 
things. This assumption is consistent with the approaches discussed in Section 2.3.1, 
402 
Marcu Rhetorical Parsing of Unrestricted Texts 
but does not follow from them. Section 5 empirically evaluates the impact that this 
assumption has on the problem of rhetorical structure derivation. 
2.4 Determining Rhetorical Structure Using a Well-Constrained Mathematical Model 
In my previous work (Marcu 1996, 1997b, 2000) I have formalized the constraints 
specific to valid rhetorical structures in the language of first-order logic. The axiom- 
atization of valid rhetorical structures that I use throughout this paper relies on the 
following features and constraints. 
? A valid rhetorical structure is a binary tree whose leaves denote 
elementary textual units. 
? Rhetorical relations hold between textual units and spans of various 
sizes. These relations are paratactic or hypotactic. Paratactic relations are 
those that hold between units (spans) of equal importance. Hypotactic 
relations are those that hold between a unit (span) that is essential for 
the writer's purpose, i.e., a nucleus, and a unit (span) that increases the 
understanding of the nucleus but is not essential for the writer's 
purpose, i.e., a satellite. 
? Each node of a rhetorical structure tree has associated a status (NUCLEUS 
or  SATELLITE), a type (the rhetorical relation that holds between the text 
spans that the node spans over), and a set of promotion units. The set of 
promotion units of a textual span is determined recursively: it is given 
by the union of the promotion sets of the immediate subspans when the 
relation that holds between these subspans is paratactic, or by the 
promotion set of the nucleus ubspan when the relation that holds 
between the immediate subspans is hypotactic. By convention, the type 
of a leaf is LEAF; and the promotion set of a leaf is a set that contains the 
leaf. 
? The status and type associated with each node are unique. Hence, for 
example, a span cannot have both the status of NUCLEUS and the status 
of  SATELLITE. 
? The rhetorical relations of a valid rhetorical structure hold only between 
adjacent spans. 
? There exists a span, which corresponds to the root node of the structure, 
that spans over the entire text. 
? The status, type, and promotion set associated with each node reflect he 
compositionality criterion discussed in Section 2.2: if a rhetorical relation 
holds between two textual spans of the tree structure of a text, either that 
relation is extended or it can be explained in terms of a simple relation 
that holds between the promotion units of the constituent subspans. 
Let us focus our attention again on text (4). We have seen that a computer program 
may be able to hypothesize the first exclusive disjunction in (6) using only knowledge 
about the discourse function of the connective In contrast. Similarly, a computer may 
be able to hypothesize that a rhetorical relation of ELABORATION holds between sen- 
tences 2 and 1 because both of them talk about John. A computer may also be able 
to hypothesize that a rhetorical relation of ELABORATION holds between sentence 4, 
403 
Computational Linguistics Volume 26, Number 3 
Status = {NUCLEUS, SATELLITE) 
( '~  Type = (CONTRAST} 
.~- - -~tatus  = {NUCLEUS) ~ Status = {NUCLEUS) 
~./- 2~ T~e = {ELABORATION} ~ ?~l) Type = {ELABORATION} 
(. ~ _ n v = ' A " = " ~  . . . . .  ( 4 ~ Status = {SATELLITE} 
TyDe = {LEAF} 
Promotion = J2) Promotion = \[3) Promotion = {4} 
Figure 3 
A valid rhetorical structure representation f text (4), which makes explicit he status, type, 
and promotion units that characterize each node. 
which starts with the marker Especially, and a sentence that precedes it. 
(6) 
( rhet_rel(CONTRAST, 1, 3) ? rhet_rel(coNTRAST, 1, 4) ? 
\] rhet_rel(cONTRAST, 2, 3)? rhet_rel(CONTRAST, 2, 4) 
RR = ~ rhet_rel(ELABORATION, 2, 1) 
I rhet_rel(ELABORATION, 4, 1) ~ rhet_rel(ELABORATION, 4, 2) ? 
( rhet_rel(ELABORATION, 4, 3)
When these hypotheses are evaluated against he constraints of valid rhetorical 
structure trees, they yield only one valid rhetorical structure representation, which is 
shown in Figure 3. This representation makes explicit he status, type, and promotion 
set of each of the nodes in the tree. Note, for example, that the CONTRAST relation that 
holds between spans \[1,2\] and \[3,4\] is explained/determined by the simple rhetorical 
relation rhet_rel(CONTRAST, 1, 3), which is one of the exclusive disjuncts hown in (6); 
hence, the rhetorical structure in Figure 3 is consistent with the compositionality cri- 
terion. Note also that the hypothesis rhet_rel(ELABORATION, 4, 2), for example, cannot 
be used instead of the CONTRAST relation to link spans \[1,2\] and \[3,4\], because the re- 
lation rhet_FeI(ELABORATION, 4, 3) was used to link units 3 and 4 and because relations 
rhet_reI(ELABORATION, 4, 2) and rhet_reI(ELABORATION, 4, 3) are exclusively disjunctive. 
In fact, even though one could have hypothesized a different relation R to hold, 
say, between the satellite 4 and the nucleus 2, such a hypothesis would not yield other 
valid trees because such trees would violate the compositionality criterion for two 
reasons: 
? Relation R cannot be used to link spans \[1,2\] and \[3,4\], for example, 
because units 2 and 4 are not in the promotion sets of spans \[1,2\] and 
\[3,4\], respectively. 
? There is no combination of rhetorical relations that would promote units 
2 and 4 as salient in spans \[1,2\] and \[3,4\], respectively. 
Hence, although we were not able to hypothesize precisely the spans and units 
between which the CONTRAST relation signaled by In contrast and the ELABORATION 
relation signaled by Especially hold, we were able to derive only one valid structure 
because the mathematical model that underlies our approach is well-constrained. 
404 
Marcu Rhetorical Parsing of Unrestricted Texts 
2.5 Discussion 
Throughout Section 2, I have argued that connectives, cohesion, shallow processing, 
and a well-constrained model of discourse can be used to automatically derive the 
rhetorical structure of free, unrestricted texts. In order to substantiate his claim, I 
need to solve two problems: 
. 
. 
First, I need to show how starting from free, unrestricted text, 
connectives and cohesion can be used to automatically determine the 
elementary units of text and hypothesize simple, extended, and 
exclusively disjunctive rhetorical relations that hold between these units 
and spans of units. I refer to this problem as the problem of rhetorical 
grounding. 
Second, I need to show how starting from a sequence of textual units 
U = 1, 2 . . . . .  n and a set RR of simple, extended, and exclusively 
disjunctive rhetorical relations that hold among these units and among 
contiguous textual spans that are defined over U, the valid rhetorical 
structures of U can be determined, i.e., the rhetorical structures that are 
consistent with the constraints given in Section 2.4. I refer to this as the 
problem of rhetorical structure derivation. 
The keen reader may have noted that in this formulation, the problem of determining 
the rhetorical structure of text is not modeled as an incremental process in which 
elementary units are determined and attached to an increasingly complex RS-tree. 
Rather, it is assumed that all elementary units of a text are determined first; that 
knowledge of connectives and cohesion is then used to (over-)hypothesize simple, 
extended, and exclusively disjunctive rhetorical relatio,ls that hold between units and 
spans of units; and that these hypotheses and the well-constrained model of valid RS- 
trees are used to determine the set of valid rhetorical interpretations that are consistent 
with both the mathematical model and the hypotheses. 
In the rest of the paper, I provide solutions to the rhetorical grounding (Sections 3, 
4.2, 4.3, and 4.4) and rhetorical structure derivation problems (Section 4.5). The prob- 
lems are solved in the context of presenting a rhetorical parsing algorithm (see Fig- 
ure 5), an algorithm that takes as input free text and determines the RS-tree of that 
text. 
3. A Corpus Analysis of Cue Phrases 
When I began this research, no empirical data existed which could answer the ques- 
tion of the extent o which connectives could be used to identify elementary units 
and hypothesize rhetorical relations. To better understand this problem, I carried out 
a corpus study. The corpus study was designed to investigate how cue phrases can be 
used to identify the elementary units of texts, as well as to determine what rhetorical 
relations hold between units and spans of text, the nuclearity of the units, and the 
sizes of the related spans. In this section, I describe the annotation schema that I used 
in the study. In Section 4, I explain how the annotated ata was used to derive algo- 
rithms that identify connective occurrences (Section 4.2), determine lementary units 
of discourse and determine which connectives have a discourse function (Section 4.3), 
and hypothesize rhetorical relations that hold between elementary units and spans of 
texts (Section 4.4). 
405 
Computational Linguistics Volume 26, Number 3 
3.1 Materials 
Many researchers have published lists of potential discourse markers and cue phrases 
(Halliday and Hasan 1976; Grosz and Sidner 1986; Martin 1992; Hirschberg and Litman 
1993; Knott 1995; Fraser 1996). I took the union of their lists and created an initial set 
of more than 450 potential discourse markers. For each potential discourse marker, I 
then used an automatic procedure that extracted from the Brown corpus a set of text 
fragments. Each text fragment contained a "window" of approximately 300 words 
and an emphasized occurrence of a cue phrase. My initial goal was to select for each 
cue phrase 10 texts in which the phrase was used at the beginning of a sentence and 
20 texts in which the phrase was used in the middle of a sentence. (In a prestudy, I 
had noticed that phrases occurring in the middle of sentences were more ambiguous 
and difficult o handle than those at the beginning of sentences.) However, since some 
of the phrases occurred in the corpus very seldom, I ended up with an average of 
17 text fragments per cue phrase. Overall, I randomly selected more than 7,600 texts. 
All the text fragments associated with a cue phrase were paired with a set of 
fields/slots in which I described two types of information. 
Discourse-related Information. This information concerned the cue phrase under scrutiny 
and was described in the following fields: 
Marker The field Marker encodes the orthographic environment that characterizes 
the use of the cue phrase. This included occurrences of periods, commas, 
colons, semicolons, etc. For example, when the cue phrase besides occurred 
within a sentence and was preceded by a comma, the Marker field was 
set to ", besides". When it occurred at the beginning of a paragraph and 
was immediately followed by a comma, the Marker field was set to "# 
Besides, ", where # denotes a paragraph break. 
Usage The field Usage encodes the functional role of the cue phrase. The role can 
be one or more of the following: SENTENTIAL~ DISCOURSE~ and PRAGMATIC. 
A cue phrase has a sentential role if it makes a semantic ontribution to the 
interpretation of text (Hirschberg and Litman 1993). A cue phrase has a 
discourse role if it signals a rhetorical relation that holds between two text 
spans. A cue phrase has a pragmatic role if it signals a relation between 
the unit to which the cue phrase belongs and the beliefs, plans, intentions, 
and/or communicative goals of the speaker/hearer (F aser 1996). 
Position The field Position specifies the position of the marker under scrutiny in 
the textual unit to which it belongs. The possible values for this field are: 
BEGINNING~ MEDIAL~ and END. 
Right boundary The Right boundary of the textual unit associated with the mark- 
er under scrutiny contains the last cue phrase, orthographic marker, or 
word of that textual unit. 
Where to link The field Where to link describes whether the textual unit that 
contains the discourse marker under scrutiny is related to a textual unit 
found BEFORE or AFTER it. 
Rhetorical relation The field Rhetorical relation specifies one or more names of 
rhetorical relations that are signaled by the cue phrase under scrutiny. To 
encode the information specific to this field, I used a set of 54 rhetorical 
relations (see Section 3.2). 
406 
Marcu Rhetorical Parsing of Unrestricted Texts 
j EXAMPLE 
2-3 
2 3 
Figure 4 
The discourse tree of text (7). 
Types of textual units The field Types of textual units describes the types of tex- 
tual units connected through a rhetorical relation that was signaled by 
the cue phrase under scrutiny. It takes values from CLAUSE to MULTI- 
PLE_PARAGRAPH. I distinguished between these types of spans because I 
intended to use the corpus study to implement a rhetorical parser that 
hypothesizes both simple and extended rhetorical relations. 
Statuses The field Statuses pecifies the rhetorical statuses (separated by a semi- 
colon) of the two textual units involved in the relation. The status of a 
textual unit can be NUCLEUS or SATELLITE. 
Clause distance The field Clause distance contains a count of the clause-like units 
that separate the units related by the marker. The count is 0 when the 
related units are adjacent. 
Sentence distance The field Sentence distance contains a count of the sentences 
that are found between the units that are related by the marker. The count 
is -1  when the related units belong to the same sentence. 
Distance to salient unit The field Distance to salient unit contains a count of the 
clause-like units that separate the textual unit that contains the marker 
under scrutiny and the textual unit that is the most salient unit of the 
span that is rhetorically related to a unit that is before or after that under 
scrutiny. In most cases, this distance is -1 ,  i.e., the unit that contains a 
marker is directly related to a unit that went before or to a unit that 
comes after. However, in some cases, this is not so. Consider, for example, 
the text given in (7) below, with respect o the cue phrase for example. 
(7) \[There are many things I do not like about fast food. 1\] \[Let's 
assume, for example, that you want to go out with someone2.\] 
\[There is no way you can take them to a fast food 
restaurant! 3 \]
A rhetorical analysis of text (7) is shown in Figure 4. It is easy to see that 
although for example signals a rhetorical relation of EXAMPLE, the relation 
does not hold between units 2 and 1, but rather, between span 2-3 and 
unit 1. More precisely, the relation holds between unit 3, which is the 
most salient unit of span 2-3, and unit 1. The field Distance to salient unit 
reflects this state of affairs. For text (7) and marker for example, its value is 0. 
When a discourse marker had more than one function or signaled more than one 
discourse relation, I enumerated all functions and relations. 
Algorithmic Information. In contrast o the discourse-related information, which has a 
general inguistic interpretation, the algorithmic information was specifically tailored 
407 
Computational Linguistics Volume 26, Number 3 
Table 1 
A corpus analysis of the cue phrase Although 
from text (8). 
Field Content 
Marker # UAlthoughU 
Usage DISCOURSE 
Right boundary 
Where to link1 AFTER 
Types of textual units1 CLAUSE;CLAUSE 
Clause distance1 0 
Sentence distance1 -1 
Distance to salient unit1 -1 
Position~ BEGINNING 
Statuses~ SATELLITE;NUCLEUS 
Rhetorical relation1 CONCESSION 
Where to link2 BEFORE 
Types of textual units2 SENTENCE;SENTENCE 
Clause distance2 6 
Sentence distance2 4 
Distance to salient unit2 -1 
Position2 BEGINNING 
Statuses2  NUCLEUS;SATELLITE 
Rhetorical relation2 ELABORATION 
Break action COMMA 
to the surface analysis aimed at determining the elementary textual units of a text. It 
concerned only one field, Break action, which specified the action that a left-to-right 
surface-based lementary unit identifier will need to take to determine the boundaries 
of elementary textual units found in the vicinity of the cue phrase. For example, an 
action of type NORMAL associated with the occurrence of the connective but encoded 
the fact that an elementary unit boundary had to be inserted immediately before the 
connective. Since a discussion of the actions and their semantics i  meaningless in
isolation, I will provide it below in Section 4.3.3, in conjunction with the clause-like 
unit boundary and discourse marker identification algorithm. 
One can argue that encoding algorithmic information in a corpus study is not 
necessary. After all, one can use the annotated ata to derive such information auto- 
matically. However, during my prestudy of cue phrases, I noticed that there is a finite 
number of ways in which cue phrases can be used to identify the elementary units 
of text. By encoding algorithmic specific information in the corpus, I only bootstrap 
the step that can take one from annotated ata to algorithmic information. This en- 
coding does not preclude the employment of more sophisticated methods that derive 
algorithmic information automatically. 
3.2 Methods and Results 
Once the database had been created, I analyzed its records and updated the fields 
according to the requirements described above. For example, Table 1 shows the in- 
formation that I associated with the fields when I analyzed the text fragment shown 
in (8), with respect o the cue phrase Although. The square brackets in (8) enclose the 
elementary units of interest. 
(8) \[How well do faculty members govern themselves?\] \[There is little 
evidence that they are giving any systematic thought o a general theory 
408 
Marcu Rhetorical Parsing of Unrestricted Texts 
of the optimum scope and nature of their part in government.\] \[They 
sometimes pay more attention to their rights\] \[than to their own internal 
problems of government.\] \[They, too, need to learn to delegate.\] \[Letting 
the administration take details off their hands would give them more 
time to inform themselves about education as a whole,\] \[an area that 
would benefit by more faculty attention.\] 
\[Although faculties insist on governing themselves,\] \[they grant little 
prestige to a member who actively participates in college or university 
government.\] 
The information encoded in Table 1 specifies that the marker Although, which 
occurs in the beginning of a paragraph (#UAlthoughU), has a DISCOURSE role and that 
the right boundary of the elementary unit to which it belongs is a comma. Although 
signals a relation of CONCESSION between the clause to which it belongs, which has a 
rhetorical status of SATELLITE, and the clause that comes immediately AFTER it, which 
has a rhetorical status of NUCLEUS. In addition to the discourse relation signaled by a 
marker such as Although, which introduces expectations (Cristea and Webber 1997), I
also found it useful to annotate the rhetorical relation that held between the sentence to 
which an expectation-based marker belonged and the text span that went before. For 
example, with respect to the connective Although in text (8), I also represented xplicitly 
in the corpus the fact that an ELABORATION relation holds between the sentence that 
contains the connective, which has the status of SATELLITE, and a sentence found six 
clauses (four sentences) BEFORE it, which has the status of NUCLEUS. It turned out that 
in most of the cases in which a phrase such as Although was used at the beginning 
of a sentence/paragraph, it not only signaled a CONCESSION relation between two 
clauses, but its use also correlated with an ELABORATION relation that held between 
two sentences or paragraphs. 
Overall, I have manually analyzed 2,100 of the text fragments in the corpus. I
annotated only 2,100 fragments because the task was too time-consuming to complete. 
Of the 2,100 instances of cue phrases that I considered, 1,197 had a discourse function, 
773 were sentential, and 244 were pragmatic. 4 
The taxonomy of relations that I used to label the 1,197 discourse uses in the corpus 
contained 54 relations. Marcu (1997b) lists their names and the number of instances 
in which each rhetorical relation was used. The number of relations is much larger 
than 24, which is the size of the taxonomy proposed initially by Mann and Thomp- 
son (1988), because during the corpus analysis, it often happened that the relations 
proposed by Mann and Thompson seemed inadequate to capture the semantics of 
the relationship between the units under consideration. Because the study described 
here was exploratory, I considered it appropriate to introduce relations that would 
better capture the meaning of these relationships. The rhetorical relation ames were 
chosen so as to reflect he intended semantics of the relations. To manage the new 
relations, I did not provide for them definitions imilar to those proposed by Mann 
and Thompson (1988); instead, I kept a list of text examples that I considered to reflect 
the meaning of each new rhetorical relation that I introduced. 
In Section 4, I will explain how the annotated data was used in order to implement 
algorithms that solve the problem of rhetorical grounding defined in Section 2.5. 
4 The three numbers add up to more than 2,100 because ome cue phrases had multiple roles in some 
text fragments. 
409 
Computational Linguistics Volume 26, Number 3 
3.3 Discussion 
The elementary textual units that I considered, such as those enclosed within square 
brackets in examples (4) and (8) were not necessarily clauses in the traditional, gram- 
matical sense. Rather, they were contiguous pans of text that could be smaller than a 
clause and that could provide grounds for deriving rhetorical inferences. For example, 
although the text in italics in the sentence "Only the midday sun at tropical atitudes is 
warm enough to thaw ice on occasion, but any liquid water formed in this way would 
evaporate almost instantly because of the low atmospheric pressure." does not represent a 
full-fledged clause, I decided to label it as an elementary unit because it provides the 
grounds for inferring a causal relation. 
Hence, in the texts that I analyzed, I did not use an objective definition of elemen- 
tary unit. Rather, I relied on a more intuitive one: whenever I found that a rhetorical 
relation held between two spans of text of significant sizes (the relation could be sig- 
naled or not by a cue phrase, or not), I assigned those spans an elementary unit status, 
although in some cases they were not full-fledged clauses. In the rest of the paper I
refer to such elementary units with the term clause-like unit. 
The main advantage of the empirical work described here is the empirical ground- 
ing that it provides for a set of algorithms that derive the rhetorical structures of un- 
restricted texts. These algorithms are grounded partly in the empirical data derived 
from the corpus and partly in the intuitions that I developed uring the discourse 
analysis of the 2,100 fragments of text. 
Since I was the only analyst of 2,100 of the 7,600 of the text fragments in the corpus 
and since I wanted to avoid evaluating the algorithms that I developed against my 
own subjective standard, I used the corpus analysis only for algorithm development. 
The testing of the algorithms was done against data that did not occur in the corpus 
and that was analyzed independently by other judges. 
4. The Rhetorical Parsing Algorithm 
The rhetorical parsing algorithm takes as input a free, unrestricted text and determines 
its rhetorical structure. The algorithm presented in this paper assumes that the rhetor- 
ical structure of a text correlates with the orthographic layout of that text. That is, it 
assumes that sentences, paragraphs, and sections correspond to hierarchical spans in 
the rhetorical representation f the text that they subsume. 
Obviously, this assumption is controversial because there is no clear-cut evidence 
that the rhetorical structure of a text correlates with its paragraph structure, for ex- 
ample. In fact, some psycholinguistic and empirical research of Heurley (1997) and 
Hearst (1997) indicates that paragraph breaks do not always occur at the same loca- 
tions as the thematic boundaries. In contrast, experiments of Bruder and Wiebe (1990) 
and Wiebe (1994) show that paragraph breaks help readers to interpret private-state 
sentences in narratives, i.e., sentences about psychological states uch as wanting and 
perceptual states such as seeing. Hence, paragraph breaks play an important role in 
story comprehension. In my own experiments ( ee Section 5), I observed that, in nine 
out of ten cases, human judges manually built rhetorical structures that correlated 
with the underlying paragraph boundaries. 
The main reason for assuming that the orthographic layout of text correlates with 
its rhetorical structure is primarily one of efficiency. In the same way sentences are 
ambiguous and syntactic parsers can derive thousands of syntactic trees, so texts are 
ambiguous and rhetorical parsers can derive thousands of rhetorical trees. Assuming 
that the rhetorical structure of a text correlates with sentence, paragraph, and section 
410 
Marcu Rhetorical Parsing of Unrestricted Texts 
Input: A text T. 
Output: The valid rhetorical structures of T. 
1. I. Determine the set D of all cue phrase (potential discourse marker) instances in T. 
2. II. Use information derived from the corpus analysis in order to determine 
3. recursively all the sections, paragraphs, entences, and clause-like units of the 
4. text and the set Dd E D of cue phrases that have a discourse function. 
5. III. For each of the three highest levels of granularity (sentences, paragraphs, 
6. and sections) 
7. III.1 Use information derived from the corpus analysis about the 
8. discourse markers Dd in order to hypothesize rhetorical relations 
9. among the elementary units that correspond to that level. 
10. III.2 Use cohesion in order to hypothesize rhetorical relations among 
11. the units for which no hypotheses were made in step III.1. 
12. III.3 Apply the proof theory discussed in Section 4.5 in order to 
13. determine all the valid text trees that correspond to that level. 
14. III.4 Assign a weight to each of the text trees and determine the tree 
15. with maximal weight. 
16. IV. Merge the best trees that correspond to each level into a discourse tree that 
17. spans the whole text and that has clause-like units as its elementary units. 
Figure 5 
Outline of the rhetorical parsing algorithm. 
boundaries ignificantly reduces the search space of possible rhetorical interpretations 
and increases the speed of a rhetorical parser. 
4.1 A Bird's-Eye View 
The rhetorical parsing algorithm, which was implemented C++, is outlined in Figure 5. 
The rhetorical parser first determines the set of all instances of cue phrases that occur in 
the text; this set includes punctuation marks such as commas, periods, and semicolons. 
In the second step (lines 2-4 in Figure 5), the rhetorical parser retraverses the input 
and by using information derived from the corpus study discussed in Section 3, it 
determines the elementary units and the cue phrases that have a discourse function 
in structuring the text. In the third step, the rhetorical parser builds the valid text 
structures for each of the three highest levels of granularity, which are the sentence, 
paragraph, and section levels (see lines 5-15 in Figure 5). Tree construction is carried 
out in four substeps. 
III.1 
III.2 
III.3 
First, the rhetorical parser uses the cue phrases that were assigned a 
discourse function in step II to hypothesize rhetorical relations between 
clause-like units, sentences, and paragraphs (see lines 7-9). Most of the 
discourse markers yield exclusively disjunctive hypotheses. 
When the textual units under consideration are characterized by no 
discourse markers, rhetorical relations are hypothesized on the basis of a 
simple cohesive device, which is similar to that used by Hearst (1997) 
(see lines 10-11). 
Once the set of textual units and the set of rhetorical relations that hold 
among the units have been determined, the algorithm derives discourse 
trees at each of the three levels that are assumed to be in correlation with 
the discourse structure: sentence, paragraph, and section levels (see lines 
12-13). The derivation is accomplished by a chart-based implementation 
411 
Computational Linguistics Volume 26, Number 3 
III.4 
of a proof theory that solves the rhetorical structure derivation problem 
(see Section 4.5). 
Since the rhetorical parsing process is ambiguous, more than one 
discourse tree is usually obtained at each of these levels. To deal with 
this ambiguity, a "best" tree is selected according to a metric to be 
discussed in Section 4.6 (see lines 14-15). 
In the final step, the algorithm assembles the trees built at each level of granularity, 
thus obtaining a discourse tree that spans over the whole text (lines 16-17 in Figure 5). 
In the rest of the paper, I discuss in detail the steps that the rhetorical parser 
follows when it derives the valid structures of a text and the algorithms that implement 
them. In the cases in which the algorithms rely on data derived from the corpus 
study in Section 3, I also discuss the relationship between the predominantly linguistic 
information that characterizes the corpus and the procedural information that can be 
exploited at the algorithmic level. Throughout the discussion, I will use the text in (1) 
as an example. 
4.2 Determining the Potential Discourse Markers of a Text 
4.2.1 From the Corpus Analysis to the Potential Discourse Markers of a Text. The 
corpus analysis discussed in Section 3 provides information about he orthographic en- 
vironment of cue phrases and the function they have in the text (sentential, discourse, 
or pragmatic). Different orthographic environments often correlate with different dis- 
course functions and different ways of breaking the surrounding text into elementary 
units. For example, if the cue phrase Besides occurs at the beginning of a sentence and 
is not followed by a comma, as in text (9), it usually signals a rhetorical relation that 
holds between the clause-like unit that contains it and the following clause(s). How- 
ever, if the same cue phrase occurs at the beginning of a sentence and is immediately 
followed by a comma, as in text (10), it usually signals a rhetorical relation that holds 
between the sentence to which Besides belongs and a textual unit that precedes it. 
(9) 
(10) 
\[Besides the lack of an adequate thical dimension to the Governor's 
case,\] \[one can ask seriously whether our lead over the Russians in 
quality and quantity of nuclear weapons is so slight as to make the tests 
absolutely necessary.\] 
\[For pride's sake, I will not say that the coy and leering vade mecum of 
those verses insinuated itself into my soul.\] \[Besides, that particular 
message does no more than weakly echo the roar in all fresh blood.\] 
I have taken each cue phrase in the corpus and evaluated its potential contribution 
in determining the elementary textual units and in hypothesizing the rhetorical rela- 
tions that hold among the units for each orthographic environment that characterized 
its usage. I used the cue phrases that had a discourse role in most of the text fragments 
and the orthographic environments that characterized them to manually develop a set 
of regular expressions that can be used to recognize potential discourse markers in 
naturally occurring texts. If a cue phrase had different discourse functions in differ- 
ent orthographic environments and could be used in different ways in identifying the 
elementary units of the surrounding text, as was the case with Besides, I created one 
regular expression for each function. I ignored both cue phrases that had a sentential 
role in a majority of the instances in the corpus and those that were too ambiguous to 
be exploited in the context of a surface-based approach. In general, I preferred to be 
412 
Marcu Rhetorical Parsing of Unrestricted Texts 
Table 2 
A list of regular expressions that correspond to occurrences ofsome 
of the potential discourse markers and punctuation marks. 
Marker Regular Expression 
Although 
because 
but 
for example 
where 
With 
Yet 
COMMA 
OPENf~AREN 
CLOSEX%REN 
DASH 
END_SENTENCE 
BEGIN_PARAGRAPH 
\[UXtXn\]Although(U \] \t I \n) 
\[,\]\[UXtXn\]+because(U \] \t\]\n) 
\[uXtXn\]+but(U I \t I \n) 
\[,\]\[UXtXn\]+for\[UXt \nJ+example(u \], I \t I \n) 
,\[UXtXn\]+where(U I \t J \n) 
\[uXtXn\]With(u I \t \[ \n) 
\[uNtNn\]Yet(u I \t I \n) 
,(U I \t I \n) 
\[,\]\[UXtXn\]+( 
)(U I \t I \n) 
\[,\]\[UXtXn\]+--(U I \t I \n) 
("-")l("?')l("~")l("."")l("?"")l("r"')) 
u*((XnXt\[UXt\]*)l(Xn\[UXtXn\]{2,})) 
conservative and to consider only potential cue phrases whose discourse role could 
be determined with a relatively high level of confidence. Table 2 shows a set of reg- 
ular expressions that correspond to some of the cue phrases in the corpus. Because 
orthographic markers, such as commas, periods, dashes, paragraph breaks, etc., play 
an important role in our surface-based approach to discourse processing, I included 
them in the list of potential discourse markers as well. 
By considering only cue phrases having a discourse function in most of the cases, 
I deliberately chose to focus more on precision than on recall with respect o the 
task of identifying the elementary units of text. That is, I chose to determine fewer 
units than humans do, hoping that, in this way, most of the identified units would be 
correct. 
4.2.2 An Algorithm for Determining the Potential Discourse Markers of a Text. Once 
the regular expressions that match potential discourse markers were derived, it was 
trivial to implement the first step of the rhetorical parser (line I in Figure 5). A program 
that uses the Unix tool lex traverses the text given as input and determines the locations 
at which potential discourse markers occur. For example, when the regular expressions 
are matched against ext (1), the algorithm recognizes all punctuation marks and the 
cue phrases hown in italics in text (11) below. 
(11) With its distant orbit--50 percent farther from the sun than Earth--and 
slim atmospheric blanket, Mars experiences frigid weather conditions. 
Surface temperatures typically average about -60 degrees Celsius (-76 
degrees Fahrenheit) at the equator and can dip to -123 degrees C near 
the poles. Only the midday sun at tropical atitudes is warm enough to 
thaw ice on occasion, but any liquid water formed in this way would 
evaporate almost instantly because of the low atmospheric pressure. 
Although the atmosphere holds a small amount of water, and 
water-ice clouds sometimes develop, most Martian weather involves 
blowing dust or carbon dioxide. Each winter, for example, a blizzard of 
frozen carbon dioxide rages over one pole, and a few meters of this 
dry-ice snow accumulate as previously frozen carbon dioxide evaporates 
from the opposite polar cap. Yet even on the summer pole, where the sun 
413 
Computational Linguistics Volume 26, Number 3 
remains in the sky all day long, temperatures never warm enough to 
melt frozen water. 
4.3 Determining the Elementary Units of a Text 
4.3.1 From the Corpus Analysis to the Elementary Units of a Text. As I discussed 
in Section 3, the corpus study encoded not only linguistic information but also algo- 
rithmic information, in the field Break action. During the corpus analysis, I generated 
a set of 11 actions that constitutes the foundation of an algorithm to automatically 
determine the elementary units of a text. The algorithm processes each sentence in the 
text given as input in a left-to-right fashion and "executes" the actions that are associ- 
ated with each potential discourse marker and each punctuation mark that occurs in 
that sentence. Because the algorithm does not use any traditional parsing and tagging 
techniques, I call it a shallow analyzer. 
The names and the intended semantics of the actions used by the shallow analyzer 
are: 
? Action NOTHING instructs the shallow analyzer to treat the cue phrase 
under consideration as a simple word. That is, no textual unit boundary 
is normally set when a cue phrase associated with such an action is 
processed. For example, the action associated with the cue phrase 
accordingly is NOTHING. 
? Action NORMAL instructs the analyzer to insert a textual boundary 
immediately before the occurrence of the marker. Textual boundaries 
correspond to elementary unit breaks. 
? Action COMMA instructs the analyzer to insert a textual boundary 
immediately after the occurrence of the first comma in the input stream. 
If the first comma is followed by an and or an or, the textual boundary is 
set after the occurrence of the next comma instead. If no comma is found 
before the end of the sentence, a textual boundary is created at the end 
of the sentence. 
? Action NORMAL_THEN_COMMA instructs the analyzer to insert a textual 
boundary immediately before the occurrence of the marker and to insert 
another textual boundary immediately after the occurrence of the first 
comma in the input stream. As in the case of the action COMMA, if the 
first comma is followed by an and or an or, the textual boundary is set 
after the occurrence of the next comma. If no comma is found before the 
end of the sentence, a textual boundary is created at the end of the 
sentence. 
? Action END instructs the analyzer to insert a textual boundary 
immediately after the cue phrase. 
? Action MATCH_PAREN instructs the analyzer to insert textual boundaries 
both before the occurrence of the open parenthesis that is normally 
characterized by such an action, and after the closed parenthesis that 
follows it. 
? Action COMMA_PAREN instructs the analyzer to insert textual boundaries 
both before the cue phrase and after the occurrence of the next comma in 
the input stream. 
414 
Marcu Rhetorical Parsing of Unrestricted Texts 
? Action MATCH_DASH instructs the analyzer to insert a textual boundary 
before the occurrence of the cue phrase. The cue phrase is usually a 
dash. The action also instructs the analyzer to insert a textual boundary 
after the next dash in the text. If such a dash does not exist, the textual 
boundary is inserted at the end of the sentence. 
The preceding three actions, MATCH_PAREN, COMMA_PAREN, and 
MATCH_DASH, are used for determining the boundaries of parenthetical 
units. 
? Action SET_AND/SET_OR instructs the analyzer to store the information 
that the input stream contains the lexeme and~or. 
? Action DUAL instructs the analyzer to insert a textual boundary 
immediately before the cue phrase under consideration if there is no 
other cue phrase that immediately precedes it. If there exists such a cue 
phrase, the analyzer will behave as in the case of the action COMMA. The 
action DUAL is usually associated with cue phrases that can introduce 
some expectations about the discourse (Cristea and Webber 1997). For 
example, the cue phrase although in text (12) signals a rhetorical relation 
of CONCESSION between the clause to which it belongs and the previous 
clause. However, in text (13), where although is preceded by an and, it 
signals a rhetorical relation of CONCESSION between the clause to which 
it belongs and the next clause in the text. 
(12) \[I went to the theatre\] \[although I had a terrible headache.\] 
(13) \[The trip was fun,\] \[and although we were badly bitten by 
blackflies,\] \[Ido not regret it.\] 
In addition to the algorithmic information that is explicitly encoded in the field Break 
action, the shallow analyzer uses information about he position of cue phrases in the 
elementary textual units to which they belong. The position information is extracted 
directly from the corpus, from the field Position. Hence, each regular expression that 
has a corresponding instantion in the texts in the corpus that could play a discourse 
function is assigned a structure with two features: 
the action that the shallow analyzer should perform in order to 
determine the boundaries of the textual units found in its vicinity; 
the relative position of the marker in the textual unit to which it belongs 
(beginning, middle, or end). 
Table 3 lists the actions and the positions in the elementary units of the cue phrases 
and orthographic markers hown in Table 2. 
4.3.2 The Section, Paragraph, and Sentence Identification Algorithm. As discussed 
in Section 4.1, the rhetorical parser assumes that sentences, paragraphs, and sections 
correspond to hierarchical spans in the rhetorical representation f the text that they 
subsume. 
The algorithm that determines the section, paragraph, and sentence boundaries i
a very simple one, which uses the set of regular expressions that are associated with 
the potential discourse markers END_SENTENCE and BEGIN_PARAGRPH found in 
Table 2 and a list of abbreviations, such as Mr., Mrs., and Inc., that prevent the setting of 
415 
Computational Linguistics Volume 26, Number 3 
Table 3 
The list of actions that correspond to the potential 
discourse markers and punctuation marks shown 
in Table 2; B = beginning, M --- middle, and E = 
end. 
Marker Position Action 
Although B COMMA 
because B DUAL 
but B NORMAL 
for example M NOTHING 
where B COMMA-PAREN 
Wi th  B COMMA 
Yet B NOTHING 
COMMA E NOTHING 
OPEN_PAREN B MATCH-PAREN 
CLOSEA~AREN E NOTHING 
DASH n MATCH-DASH 
END_SENTENCE E NOTHING 
BEGIN_PARAGRAPH B NOTHING 
sentence and paragraph boundaries at places that are inappropriate. This simple algo- 
rithm correctly located all of the paragraph boundaries and all but one of the sentence 
boundaries found in the texts that I used to evaluate the clause-like unit and discourse 
marker identification algorithm that I will present in Section 4.3.3. Other texts and 
semistructured HTML/SGML documents may need more sophisticated algorithms to 
solve this segmentation problem, such as those described by Palmer and Hearst (1997). 
4.3.3 The Clause-Like Unit and Discourse Marker Identification Algorithm. On the 
basis of the information derived from the corpus, I have designed an algorithm that 
identifies elementary textual unit boundaries in sentences and cue phrases that have 
a discourse function. Figure 6 shows only its skeleton and focuses on the variables 
and steps that are used to determine the elementary units. The steps that assert the 
discourse function of a marker are not shown; however, these steps are mentioned in 
the discussion of the algorithm given below. Marcu (1997b) provides a full description 
of the algorithm. 
The algorithm takes as input a sentence S and the array markers\[n\] of cue phrases 
(potential discourse markers) that occur in that sentence; the array is produced by a 
trivial algorithm that recognizes regular expressions (see Section 4.2.2). Each element 
in markers\[n\] is characterized by a feature structure with the following entries: 
? the action associated with the cue phrase; 
? the position in the elementary unit of the cue phrase; 
? a flag hasdiscourse~function that is initially set to "no." 
The clause-like unit and discourse marker identification algorithm traverses the 
array of cue phrases left-to-right (see the loop between lines 2 and 20) and identifies the 
elementary textual units in the sentence on the basis of the types of the markers that 
it processes. Crucial to the algorithm is the variable "status," which records the set of 
markers that have been processed earlier and that may still influence the identification 
of clause and parenthetical unit boundaries. 
416 
Marcu Rhetorical Parsing of Unrestricted Texts 
Input: 
Output: 
A sentence S. 
The array of n potential discourse markers markers\[n\] that occur in S. 
The clause-like units, parenthetical units, and discourse markers of S. 
1. status := NIL; ...; 
2. for i from 1to n 
3. if MATCHJPAREN E status V MATCH_DASH E status V COMMA_PAREN E status 
4. (deal with parenthetical information) 
5. if COMMA E status A markerTextEqual(i,",') A 
6. NextAdjacentMarkerIsNotAnd0 A NextAdjacentMarkerIsNotOr 0 
7. (insert extual boundary after comma) 
8. if (SET_AND E status V SET_OR E status) /~ markerAdjacent(i - 1,i) 
9. (deal with adjacent markers) 
10. switch(getActionType(i)) { 
11. case DUAL: (deal with DUAL markers) 
12. case NORMAL: (insert extual boundary before marker) 
13. case COMMA: status := status U {COMMA}; 
14. case NORMAL_THEN_COMMA: (insert textual boundary before marker) 
15 status := status U {COMMA}; 
16. case NOTHING: (assign discourse usage)* 
17. case MATCH_PAREN, GOMMA_PAREN~ MATCHJ)ASH: status := status U 
{getActionType(i)}; 
18. case SET_AND, SET_OR: status := status U {getActionType(i)}; 
19. } 
20. end for 
21. finishUpParentheticalsAndClauses0; 
Figure 6 
The skeleton of the clause-like unit and discourse marker identification algorithm. 
The clause-like unit identification algorithm has two main parts: lines 10-20 con- 
cern actions that are executed when the status variable is NIL. These actions can insert 
textual unit boundaries or modify the value of the status variable, thus influencing 
the processing of further markers. Lines 3-9 concern actions that are executed when 
the status variable is not NIL. We discuss each of these actions in turn. 
Lines 3-4 of the algorithm treat parenthetical information. Once an open paren- 
thesis, a dash, or a discourse marker whose associated action is COMMA_PAREN has 
been identified, the algorithm ignores all other potential discourse markers until the 
element hat closes the parenthetical unit is processed. Hence, the algorithm searches 
for the first closed parenthesis, dash, or comma, ignoring all other markers on the 
way. Obviously, this implementat ion does not assign a discourse usage to discourse 
markers that are used within a span that is parenthetic. However,  this choice is consis- 
tent with the decision, discussed in Section 4.3.1, to assign parenthetical information 
no elementary textual unit status. Because of this, the text shown in italics in text (14), 
for example, is treated as a single parenthetical unit, which is subordinated to "Yet, 
even on the summer  pole, temperatures never warm enough to melt frozen water." 
In dealing with parenthetical units, the algorithm avoids setting boundaries in cases 
in which the first comma that comes after a COMMA..PAREN marker is immediately 
followed by an or or an and. As example (14) shows, taking the first comma as the 
boundary of the parenthetical unit would be inappropriate. 
(14) \[Yet, even on the summer  pole, {where the sun remains in the sky all day 
long, and where winds are not as strong as at the Equator,} temperatures never 
warm enough to melt frozen water.\] 
417 
Computational Linguistics Volume 26, Number 3 
Obviously, one can easily find counterexamples to this rule (and to other rules 
that are employed by the algorithm). For example, the clause-like unit and discourse 
marker identification algorithm will produce rroneous results when it processes the 
sentence shown in (15) below. 
(15) \[I gave John a boat,\] \[which e liked, and a duck,\] \[which e didn't.\] 
Nevertheless, the evaluation results discussed in Section 4.3.4 show that the algorithm 
produces correct results in the majority of the cases. 
If the status variable contains the action COMMA, the occurrence of the first comma 
that is not adjacent to an and or an or marker determines the identification of a new 
elementary unit (see lines 5-7 in Figure 6). 
Usually, the discourse role of the cue phrases and and or is ignored because the 
surface-form algorithm that we propose is unable to distinguish accurately enough 
between their discourse and sentential usages. However, lines 8-9 of the algorithm 
concern cases in which their discourse function can be unambiguously determined. 
For example, in our corpus, whenever and and or immediately preceded the occurrence 
of other discourse markers (function markerAdjacent(i - 1, i) returns "true"), they had 
a discourse function. For example, in sentence (16), and acts as an indicator of a JOINT 
relation between the first two clauses of the text. 
(16) \[Although the weather on Mars is cold\] \[and although it is very unlikely 
that water exists,\] [scientists have not dismissed yet the possibility of life 
on the Red Planet.\] 
If a discourse marker is found that immediately follows the occurrence of an and (or 
an or) and if the left boundary of the elementary unit under consideration is found to 
the left of the and (or the or), a new elementary unit is identified whose right boundary 
is just before the and (or the or). In such a case, the and (or the or) is considered to 
have a discourse function as well, so the flag has_discourse_function s set to "yes." 
If any of the complex conditions in lines 3, 5, or 8 in Figure 6 is satisfied, the 
algorithm not only inserts textual boundaries as discussed above, but also resets the 
status variable to NIL. 
Lines 10-19 of the algorithm concern the cases in which the status variable is ML. 
If the type of the marker is DUAL, the determination f the textual unit boundaries 
depends on the marker under scrutiny being adjacent to the marker that precedes it. 
If it is, the status variable is set such that the algorithm will act as in the case of a 
marker of type COMMA. If the marker under scrutiny is not adjacent to the marker that 
immediately preceded it, a textual unit boundary is identified. This implementation 
will modify, for example, the status variable to COMMA when processing the marker 
although in example (17), but only insert a textual unit boundary when processing the 
same marker in example (18). The final textual unit boundaries that are assigned by 
the algorithm are shown using square brackets. 
(17) \[John is a nice guy,\] \[but although his colleagues do not pick on him,\] 
\[they do not invite him to go camping with them.\] 
(18) \[John is a nice guy,\] \[although e made a couple of nasty remarks last 
night.\] 
Line 12 of the algorithm concerns the most frequent marker type. The type NORMAL 
determines the identification ofa new clause-like unit boundary just before the marker 
418 
Marcu Rhetorical Parsing of Unrestricted Texts 
under scrutiny. Line 13 concerns the case in which the type of the marker is COMMA. 
If the marker under scrutiny is adjacent to the previous one, the previous marker is 
considered to have a discourse function as well. In either case, the status variable is 
updated such that a textual unit boundary will be identified at the first occurrence of 
a comma. When a marker of type NORMAL_THEN_COMMA is processed, the algorithm 
identifies a new clause-like unit as in the case of a marker of type NORMAL, and then 
updates the status variable such that a textual unit boundary will be identified at 
the first occurrence of a comma. In the case in which a marker of type NOTHING is 
processed, the only action that might be executed is that of assigning that marker a 
discourse usage. 
Lines 17-18 of the algorithm concern the treatment of markers that introduce x- 
pectations with respect to the occurrence of parenthetical units: the effect of processing 
such markers is that of updating the status variable according to the type of the action 
associated with the marker under scrutiny. The same effect is observed in the cases in 
which the marker under scrutiny is an and or an or. 
After processing all the markers, it is possible that some text will remain un- 
accounted for: this text usually occurs between the last marker and the end of the 
sentence. The procedure finishUpParentheticalsAndClauses0 in line 21 of Figure 6 
puts this text into the last clause-like unit that is under consideration. 
The clause-like unit boundary and discourse marker identification algorithm has 
been implemented in C++. When it processes text (11), it determines that the text has 
10 elementary units and that six cue phrases have a discourse function. Text (19) shows 
the elementary units within square brackets. The instances of parenthetical information 
are shown within curly brackets. The cue phrases that are assigned by the algorithm 
as having a discourse function are shown in italics. 
(19) \[With its distant orbit {-- 50 percent farther from the sun than Earth --} 
and slim atmospheric blanket, i \] \[Mars experiences frigid weather 
conditions. 2\] \[Surface temperatures typically average about -60 degrees 
Celsius {(-76 degrees Fahrenheit)} at the equator and can dip to -123 
degrees C near the poles. B\] \[Only the midday sun at tropical atitudes is 
warm enough to thaw ice on occasion, 4\] \[but any liquid water formed in 
this way would evaporate almost instantly 5\] \[because of the low 
atmospheric pressure. 6 \] 
\[Although the atmosphere holds a small amount of water, and 
water-ice clouds sometimes develop, 7\] \[most Martian weather involves 
blowing dust or carbon dioxide. 8\] \[Each winter, for example, a blizzard of 
frozen carbon dioxide rages over one pole, and a few meters of this 
dry-ice snow accumulate as previously frozen carbon dioxide evaporates 
from the opposite polar cap. 9\] \[Yet even on the summer pole, {where the 
sun remains in the sky all day long,} temperatures never warm enough 
to melt frozen water) ?\] 
4.3.4 Evaluation of the Clause-Like Unit and Discourse Marker Identification Algo- 
rithm. The algorithm shown in Figure 6 determines clause-like unit boundaries and 
identifies discourse uses of cue phrases using methods based on surface form. The 
algorithm relies heavily on the corpus study discussed in Section 3. 
The most important criterion for using a cue phrase in the clause-like unit and 
discourse marker identification algorithm is that the cue phrase (together with its 
orthographic neighborhood) functions as a discourse marker in the majority of the ex- 
amples in the corpus. On the one hand, the enforcement of this criterion reduces the 
419 
Computational Linguistics Volume 26, Number 3 
recall of the discourse markers that can be detected, but on the other hand, it signif- 
icantly increases the precision. I chose to ignore the ambiguous markers deliberately 
because, during the corpus analysis, I noticed that many of the markers that connect 
large textual units can be identified by a shallow analyzer. In fact, the discourse marker 
responsible for most of the algorithm recall failures is and. Since a shallow analyzer 
cannot identify with sufficient precision whether an occurrence of and has a discourse 
or a sentential usage, most of its occurrences are therefore ignored. It is true that, 
in this way, the discourse structures that the rhetorical parser eventually builds lose 
some potentially finer granularity, but fortunately, from a rhetorical analysis perspec- 
tive, the loss has insignificant global repercussions: the majority of the relations that 
the algorithm misses due to recall failures of and are JOINT and SEQUENCE relations 
that hold between adjacent clause-like units. 
To evaluate the clause-like unit and discourse marker identification algorithm, I 
randomly selected three texts, each belonging to a different genre: 
1. an expository text of 5,036 words from Scientific American; 
2. a magazine article of 1,588 words from Time; 
3. a narration of 583 words from the Brown corpus (segment 
P25:1250-1710). 
No fragment of any of the three texts was used during the corpus analysis. Three 
independent judges, graduate students in computational linguistics, broke the texts 
into elementary units. The judges were given no detailed instructions about the criteria 
that they were to apply in determining the clause-like unit boundaries. Rather, they 
were supposed to rely on their intuition and preferred efinition of clause and to insert 
a boundary between two clause-like units when they believed that a rhetorical relation 
held between those units. The locations in texts that were labeled as clause-like unit 
boundaries by at least two of the three judges were considered to be valid elementary 
unit boundaries. 
I used the valid elementary unit boundaries assigned by judges as indicators of 
discourse usages of cue phrases and I manually determined the cue phrases that 
signaled a discourse relation. For example, if an and was used in a sentence and if the 
judges agreed that a textual unit boundary existed just before the and, I assigned that 
and a discourse use. Otherwise, I assigned it a sentential usage. I applied this procedure 
to instances of all 450 cue phrases in the corpus, not only to the subset of phrases that 
were used by the rhetorical parser. Hence, I manually determined all discourse usages 
of cue phrases and all discourse boundaries between elementary units. 
I then applied the clause-like unit and discourse marker identification algorithm to 
the same texts. The algorithm found 80.8% of the discourse markers with a precision of 
89.5% (see Table 4), a result that seems to outperform Hirschberg and Litman's (1993) 
algorithm. 5 The large difference in recall between the first and the third texts is due to 
the different ext genres. In the third text, which is a narration, the discourse marker 
and occurs frequently. As discussed above, the clause-like unit and discourse marker 
identification algorithm correctly labels only a small percentage of these occurrences. 
The algorithm correctly identified 81.3% of the clause-like unit boundaries, with 
a precision of 90.3% (see Table 5). 
5 Since the algorithm proposed here and Hirschberg and Litman's algorithm were evaluated on different 
corpora, it is impossible to carry out a fair comparison. Also, the discourse markers in my three texts 
were not identified using an independent definition, as Hirschberg and Litman were. 
420 
Marcu Rhetorical Parsing of Unrestricted Texts 
Table 4 
Evaluation of the marker identification procedure. 
Text Number of Number of Number of Recall Precision 
Discourse Discourse Discourse 
Markers Markers Markers 
Identified Identified Identified 
Manually by the Correctly 
Algorithm by the 
Algorithm 
1. 174 169 150 86.2% 88.8% 
2. 63 55 49 77.8% 89.1% 
3. 38 24 23 63.2% 95.6% 
Total 275 248 222 80.8% 89.5% 
Table 5 
Evaluation of the clause-like unit boundary identification procedure. 
Text Number of Number of Number of Number of Recall 
Sentence Clause-like Clause-like Clause-like 
Boundaries Unit Unit Unit 
Boundaries Boundaries Boundaries 
Identified Identified Identified 
Manually by the Correctly 
Algorithm by the 
Algorithm 
Precision 
1. 242 428 416 371 86.7% 89.2% 
2. 80 151 123 113 74.8% 91.8% 
3. 19 61 37 36 59.0% 97.3% 
Total 341 640 576 520 81.3% 90.3% 
4.4 Hypothesiz ing Rhetorical Relations between Textual Units of Various 
Granularities 
4.4.1 From Discourse Markers to Rhetorical Relations. To hypothesize rhetorical re- 
lations, I manual ly  associated with each of the regular expressions that can be used to 
recognize potential discourse markers in naturally occurring texts (see Section 4.2.1) a 
set of features for each of the discourse roles that a discourse marker can play. Each 
set had six distinct features: 
? The feature Statuses pecifies the rhetorical status of the units that are 
linked by the discourse marker. Its value is given by the content of the 
instances of the database field Statuses that were consistent with the 
discourse usage being considered. Hence, the accepted values are 
SATELLITE_NUCLEUS, NUCLEUS_SATELLITE, and NUCLEUS_NUCLEUS. 
? The feature Where to link specifies whether the rhetorical relations 
signaled by the discourse marker concern a textual unit that goes 
BEFORE or AFTER the unit that contains the marker. Its value is given by 
the content of the instances of the database field Where to l ink that were 
consistent with the discourse usage being considered. 
? The feature Types of textual units specifies the nature of the textual units 
that are involved in the rhetorical relations. Its value is given by the 
421 
Computational Linguistics Volume 26, Number 3 
content of the instances of the database field Types of textual units that 
were consistent with the discourse usage being considered. The accepted 
values are CLAUSE, SENTENCE~ and PARAGRAPH. 
? The feature Rhetorical relation specifies the names of rhetorical relations 
that may be signaled by the cue phrase under consideration. Its value is 
given by the names listed in the instances of the database field Rhetorical 
relation that were consistent with the discourse usage being considered. 
? The feature Maximal distance specifies the maximal number of units of 
the same kind found between the textual units that are involved in the 
rhetorical relation. Its value is given by the maximal value of the 
database field Clause distance of the instances that were consistent with 
the discourse usage being considered when the related units are 
clause-like units, and by the maximal value of the field Sentence 
distance when the related units are sentences. The value is 0 when the 
related units were adjacent in all the instances in the corpus. 
? The feature Distance to salient unit is given by the maximum of the 
values of the database field Distance to salient unit of the instances that 
were consistent with the discourse usage being considered. 
Table 6 lists the feature sets associated with the cue phrases that were initially listed 
in Table 2. 
For example, the cue phrase Although as two sets of features. The first set, 
{SATELLITE_NUCLEUS, AFTER, CLAUSE, CONCESSION, 1, -1},  specifies that the marker 
signals a rhetorical relation of CONCESSION that holds between two clause-like units. 
The first trait has the status SATELLITE and the second has the status NUCLEUS. The 
clause-like unit to which the textual unit that contains the cue phrase is to be linked 
comes AFTER the one that contains the marker. The maximum number of clause-like 
units that separated two clauses related by Although in the corpus was one. And there 
were no cases in the corpus in which Although signaled a CONCESSION relation between 
a clause that preceded it and one that came after (Distance to salient unit = -1). The 
second set, {NUCLEUS_SATELLITE, BEFORE, SENTENCE V PARAGRAPH~ ELABORATION, 5, 
0} specifies that the occurrence of the marker correlates with an ELABORATION relation 
holding between two sentences or two paragraphs. The first sentence or paragraph as 
the status NUCLEUS, and the second sentence or paragraph as the status SATELLITE. 
The sentence or paragraph to which the textual unit that contains the marker is to be 
linked comes BEFORE the one that contains it. The maximum number of sentences that 
separated two units related by Although in the corpus was five. And in at least one 
example in the corpus, Although marked an ELABORATION relation between some unit 
that preceded it and a sentence that came immediately after the one that contained 
the marker (Distance to salient unit = 0). 
4.4.2 A Discourse-marker-based Algorithm for Hypothesizing Rhetorical Relations. 
At the end of step II of the rhetorical parsing algorithm (see Figure 5), the text given 
as input has been broken into sections, paragraphs, sentences, and clause-like units; 
and the cue phrases that have a discourse function have been explicitly marked. In 
step III.1, a set of rhetorical relations that hold between the clause-like units of each 
sentence, the sentences of each paragraph, and the paragraphs of each section is hy- 
pothesized, on the basis of information extracted from the corpus. 
At each level of granularity (sentence, paragraph, and section levels), a discourse- 
marker-based hypothesizing algorithm iterates over all textual units of that level and 
422 
Marcu Rhetorical Parsing of Unrestricted Texts 
r...? 
z 
o 
,.- < 
o 8 
.~R 
~< 
II 
O ~m 
0 Z 
O 
~z 
~z, 
< 
r~ 
,.~ Oa 
o 
0 
o 
? 
;-.1 
t~ 
I / 
z~ 
8 
e~ 
os Z r.~ 
0 
8 
Z ~ Z 
ozO~ 
e~z 
O> ~> r.) 
m Z co ~ Z Z Z Z Z Z  
Z Z Z Z Z Z Z  
Z ZZ Zm m Z Z Z Z Z Z  
zz  
D 
423 
Computational Linguistics Volume 26, Number 3 
over all discourse markers that are relevant o them. For each discourse marker, the 
algorithm constructs an exclusively disjunctive hypothesis concerning the rhetorical 
relations that the marker under scrutiny may signal. Hence, the algorithm assumes 
that the rhetorical structure at each level can be derived by hypothesizing rhetorical 
relations that hold between the units at that level. When it hypothesizes rhetorical re- 
lations that hold between clause-like units at the sentence l vel, it hypothesizes simple 
relations. When it hypothesizes rhetorical relations that hold between sentences and 
paragraphs (at the paragraph and section levels), it hypothesizes extended rhetorical 
relations. In all cases, it overgenerates xclusively disjunctive relations and subse- 
quently uses the discourse model to determine the combinations of hypotheses that 
are consistent with the constraints pecific to well-formed RS-trees. 
Assume that the algorithm is processing the ith unit of the sequence of n units 
and assume that unit i contains a discourse marker that signals a rhetorical relation 
NAME that links the unit under scrutiny with one that went before, and whose satellite 
goes after the nucleus. An appropriate disjunctive hypothesis in this case is then the 
one that corresponds to the graphical representation i  Figure 2. Such an exclusively 
disjunctive hypothesis enumerates all possible relations that could hold over members 
of the Cartesian product {i, i+1 . . . . .  i+Dist_sal(m) +1} x {/-Max(m),/-Max(m) +1, . . . ,  
i -  1}, where Max(m) is the maximum number of units that separated the satellite and 
the nucleus of such a relation in all the examples found in the corpus, and Dist_sal(m) 
is the maximum distance to the salient unit found in the rightmost position. The 
discourse-marker-based hypothesizer iterates over all units at the sentence, paragraph, 
and section levels, and constructs exclusively disjunctive hypotheses such as those 
described here. 
Let us consider, as an example, text (1). Given the textual units and the discourse 
markers that were identified by the clause-like unit and discourse-marker identifica- 
tion algorithm (see text (19)), we now examine the relations that are hypothesized by 
the discourse-marker-based algorithm at each level of granularity. Text (19) has three 
sentences that have more than one elementary unit. For the sentence shown in (20), 
the discourse-marker-based algorithm hypothesizes the disjunction shown in (21). This 
hypothesis is consistent with the information given in Table 6, which shows that, in 
the corpus, the marker With consistently signaled BACKGROUND and JUSTIFICATION 
relations between a satellite, the unit that contained the marker, and a nucleus, the 
unit that followed it. 
(20) 
(21) 
\[With its distant orbit {-- 50 percent farther from the sun than Earth --} 
and slim atmospheric blanket, 1\] \[Mars experiences frigid weather 
conditions, a \] 
rhet_rel(BACKGROUND, 1, 2) ? rhet_rel(JuSTIFICATION, 1, 2) 
For the sentence shown in (22), the discourse-marker-based algorithm hypothe- 
sizes the two disjunctions hown in (23). 
(22) 
(23) 
\[Only the midday sun at tropical atitudes is warm enough to thaw ice 
on occasion, 4\] \[but any liquid water formed in this way would evaporate 
almost instantly s\] \[because of the low atmospheric pressure. 6\] 
rhet_rel(CONTRAST, 4, 5) @ rhet_rel(CONTRAST, 4, 6) 
rhet_rel(CAUSE, 6, 4) ? rhet_rel(EvIDENCE, 6, 4) ? 
rhet_rel(cAusE, 6, 5) @ rhet_rel(EvIDENCE, 6, 5) 
424 
Marcu Rhetorical Parsing of Unrestricted Texts 
This hypothesis consistent with the information given in Table 6 as well: but signals 
a CONTRAST between the clause-like unit that contains the marker and a unit that 
went before; however, it is also possible that this relation involves the clause-like unit 
that comes after the one that contains the marker but (the Distance to salient unit 
feature has value 0), so rhet_rel(coNTRAST, 4,6) is hypothesized as well. The second 
disjunct concerns the marker because, which can signal either a CAUSE or an EVIDENCE 
relation. 
For sentence (24), which is the first sentence in the second paragraph of text (1), 
there is only one rhetorical relation that is hypothesized, that shown in (25). 
(24) 
(25) 
\[Although t e atmosphere holds a small amount of water, and water-ice 
clouds sometimes develop, 7\] \[most Martian weather involves blowing 
dust or carbon dioxide. B\] 
rhet_rel(cONCESSION, 7, 8) 
Text (19) has two paragraphs, each of three sentences. The first paragraph con- 
tains no discourse markers that could signal relations between sentences. Hence, the 
discourse-marker-based algorithm does not make any hypotheses of rhetorical rela- 
tions that hold among the sentences of the first paragraph. In contrast, when the 
discourse-marker-based algorithm examines the markers of the second paragraph, it
hypothesizes that a rhetorical relation of type EXAMPLE holds either between sen- 
tences 9 and \[7, 8\] or between sentences 10 and \[7, 8\], because the discourse marker 
for example is used in sentence 9. This is consistent with the information presented in 
Table 6, which specifies that a rhetorical relation of EXAMPLE holds between a satellite, 
the sentence that contains the marker, and a nucleus, the sentence that went before. 
However, the satellite of the relation could also be the sentence that follows the sen- 
tence that contains the discourse marker (the value of the Distance to salient unit 
feature is 0). Given the marker Yet, the discourse-marker-based algorithm hypothe- 
sizes that an ANTITHESIS relation holds between a sentence that preceded the one that 
contains the marker, and the sentence that contains it. The set of disjuncts hown in 
(26) represents all the hypotheses that are made by the algorithm. Note that these 
hypotheses concern extended rhetorical relations. 
(26) rhet_rel(EXAMPLE, 9, \[7, 8\]) ? rhet_rel(ExAMPLE, 10, \[7, 8\]) rhet_rel(ANTITHESIS, 9, 10) ? rhet_rel(ANTITHESIS, \[78\], 10) 
During the corpus analysis, I was not able to make a connection between discourse 
markers that signal sentence-level rhetorical relations and relations that hold between 
sequences of sentences, paragraphs, and multiparagraphs. However, I noticed that a 
discourse marker signals a paragraph-level rhetorical relation when the marker under 
scrutiny is located either at the end of the first paragraph or at the beginning of the 
second paragraph. The rhetorical parser implements his observation by assuming that 
rhetorical relations between paragraphs can be signaled only by markers that occur in 
the first sentence of the paragraph, when the marker signals a relation whose other unit 
precedes the marker, or in the last sentence of the paragraph, when the marker signals 
a relation whose other unit follows the marker. According to the results derived from 
the corpus analysis, the use of the discourse marker Although at the beginning of a 
sentence or paragraph correlates with a rhetorical relation of ELABORATION that holds 
between a satellite, the sentence or paragraph that contains the marker, and a nucleus, 
the sentence or paragraph that precedes it. The discourse-marker-based algorithm 
425 
Computational Linguistics Volume 26, Number 3 
hypothesizes only one rhetorical relation that holds between the two paragraphs of 
text (19), that shown in (27), below. 
(27) rhet_rel(ELABORATION, \[7, 10\], \[1, 6\]) 
When a section has more than two paragraphs, the rhetorical parser generates 
exclusively disjunctive hypotheses at the paragraph level as well. The current im- 
plementation of the rhetorical parser does not hypothesize any relations among the 
sections of a text. 
4.4.3 A Word-co-occurrence-based Algorithm for Hypothesizing Rhetorical Rela- 
tions. The rhetorical relations hypothesized by the discourse-marker-based algorithm 
rely entirely on occurrences of discourse markers. In building the valid rhetorical struc- 
tures of sentences, the set of rhetorical relations that are hypothesized on the basis of 
discourse marker occurrences provides ufficient information. After all, the clause-like 
units of a sentence are determined on the basis of discourse marker occurrences as 
well; so every unit of a sentence is related to at least one other unit of the same 
sentence. This might not be the case when we consider the paragraph and section 
levels, however, because discourse markers might not provide sufficient information 
for hypothesizing rhetorical relations among all sentences of a paragraph and among 
all paragraphs of a text. In fact, it is even possible that there are full paragraphs that 
use no discourse markers at all; or that use only markers that link clause-like units 
within sentences. 
In step III.2, the rhetorical parser uses cohesion (Halliday and Hasan 1976; Hearst 
1997; Hoey 1991; Salton et al 1995) to hypothesize rhetorical relations. The algorithm 
that hypothesizes such rhetorical relations assumes that if two sentences or para- 
graphs talk about the same thing, it is either the case that the sentence or paragraph 
that comes later ELABORATES on the topic of the sentence or paragraph that went be- 
fore; or that the sentence or paragraph that comes before provides the BACKGROUND 
for interpreting the sentence or paragraph that comes later. If two sentences or para- 
graphs talk about different hings, it is assumed that a multinuclear JOINT relation 
holds between the two units. The decision as to whether two sentences/paragraphs 
talk about the same thing is made by measuring the similarity between the sen- 
tences/paragraphs. If this similarity is above a certain threshold, the textual units 
are considered to be related. Otherwise, a JOINT relation is assumed to hold between 
the two units. 
Once the discourse-marker-based algorithm has hypothesized all relations it could, 
a word-co-occurrence-based algorithm examines every sentence/paragraph boundary 
for which a marker-based rhetorical relation has not been hypothesized and uses co- 
hesion to produce such a hypothesis. As in the case of the discourse-marker-based 
algorithm, each hypothesis i  an exclusive disjunction over the members of the Carte- 
sian product {i - LD . . . . .  i} x {i + 1 . . . .  , i + RD},  which contains the units found to the 
left and to the right of the boundary between units i and i + 1. Variables LD and RD 
represent arbitrarily set sizes of the spans that are considered to be relevant from a 
cohesion-based perspective. The current implementation of the rhetorical parser sets 
LD to 3 and RD to 2. 
To assess the similarity between two units l c {i - LD . . . . .  i} and r c {i + 1 . . . . .  i + 
RD},  stopwords such as the, a, and and are initially eliminated from the texts that 
correspond to these units. The suffixes of the remaining words are removed as well, so 
that words that have the same root can be accounted for by the similarity measurement 
even if they are used in different cases, moods, tenses, etc. If the similarity is above 
426 
Marcu Rhetorical Parsing of Unrestricted Texts 
a certain threshold, an ELABORATION or a BACKGROUND relation is hypothesized to 
hold between two units; otherwise, a JOINT relation is hypothesized. The value of 
the threshold is computed for each type of textual unit on the basis of the average 
similarity of all textual units at that level. 
As we have already discussed, the first paragraph in text (19) contains no discourse 
markers that could signal relations between sentences. When the word-co-occurrence- 
based algorithm examines the boundary between the first two sentences, no stemmed 
words are found to co-occur in the first two sentences, but the stem sun is found to 
co-occur in the first and third sentences. Therefore, the algorithm hypothesizes the first 
disjunct in (28). When the boundary between the last two sentences i examined, a
disjunct having the same form is hypothesized (the last two sentences of the first para- 
graph have no words in common). To distinguish between the two different sources 
that generated the disjuncts, I assign different subscripts to the rhetorical relations 
shown in (28). 
(28) 
rhet_rel(JOINT1, \[1, 2\], 3) ? rhet_rel(ELABORATION1, \[4, 6\], \[1, 2\]) 
rhet_rel(BACKGROUND1, \[1, 2\], \[4, 6\]) 
rhet_rel(ELABORATION2, \[4, 6\], \[1, 2\]) ? rhet_reI(BACKGROUND2, \[1, 2\], \[4, 6\]) ? 
rhet_rel(JOINT2, 3  \[4, 6\]) 
During my corpus study, I noticed that in most of the cases in which the number 
of sentences in a paragraph or the number of paragraphs in a section was small and no 
discourse markers were used, the relation that held between the sentences/paragraphs 
was ELABORATION. The rhetorical parser implements this empirical observation as 
well. Since the first paragraph in text (1) has only three sentences and no discourse 
marker can be used to hypothesize rhetorical relations that hold between these sen- 
tences, the word-co-occurrence-based algorithm hypothesizes the relations shown 
in (29). 
(29) rhet_reI(ELABORATION, 3, \[1, 2\]) rhet_rel(ELABORATION, \[4, 6\], 3) 
4.5 A Proof-Theoretic Account of the Problem of Rhetorical Structure Derivation 
Once the elementary units of a text have been determined and the rhetorical relations 
between them have been hypothesized at sentence, paragraph, and section levels, we 
need to determine the rhetorical structures that are consistent with these hypotheses 
and with the constraints specific to valid RS-trees. That is, we need to solve the problem 
of rhetorical structure derivation. 
One way to formalize the problem of rhetorical structure derivation is to assume 
that given as input a set of units U = 1, 2 , . . . ,  n and a set RR of simple, extended, and 
exclusively disjunctive hypotheses that hold between these units, we are interested in 
deriving objects of the form tree(status, type, promotion, left, right), where status can be 
either NUCLEUS or SATELLITE; type can be a name of a rhetorical relation; promotion 
can be a set of natural numbers from 1 to N; and left and right can be either NULL or 
recursively defined objects of type tree. 
The objects having the form tree(status, type, promotion, left, right) provide a func- 
tional representation of valid rhetorical structures. For example, with respect o the 
elementary units of text (4) and the rhetorical relations that hold between the units of 
this text (see (6)), the subtree in Figure 3 that subsumes units I and 2 can be represented 
427 
Computational Linguistics Volume 26, Number 3 
functionally using an object of type tree as shown in (30). 
(30) 
tree(NUCLEUS, ELABORATION, {1}, 
tree(NUCLEUS, LEAF, {1}, NULL, NULL), 
tree(SATELLITE, LEAF, {2}, NULL, NULL)) 
Using objects of type tree, I devised a proof theory that can be used to determine 
all valid rhetorical structures of a text. The theory consists of a set of axioms and 
rewriting rules that encode all possible ways in which one can derive the valid RS- 
trees of a text. In this paper, I present the proof theory only at the intuitive level. The 
interested reader can find further detail in Marcu (2000). 
The proof theory that I outline here assumes that the problem of rhetorical struc- 
ture derivation can be encoded as a rewriting problem in which valid RS-trees are 
constructed bottom-up. Initially, each elementary unit i in the input is associated with 
an elementary tree that has either status NUCLEUS or SATELLITE, type LEAF, and pro- 
motion set {i}. In the beginning, any of the hypothesized relations RR can be used to 
join these elementary trees into more complex trees. Once the elementary trees have 
been built, the rhetorical structure is constructed by joining adjacent trees into larger 
trees and by making sure that at every step, the resulting structure is valid. The set 
of rhetorical relations associated with each tree keeps track of the rhetorical relations 
that can still be used to extend that tree. In the beginning, an elementary tree can be 
extended using any of the hypothesized relations RR, but as soon as a relation is used, 
it becomes unavailable for subsequent extensions. 
We encode the derivation of the elementary trees using axioms (31) and (32). 
Axiom (31), for example, specifies that if i is an elementary unit in U and if relations 
RR have been hypothesized to hold between the units in U, then one can build an 
elementary tree across text span \[i, i\], having the status NUCLEUS, the type LEAF, and 
promotion set {i}; and that this tree can be rewritten into a larger tree by using relations 
from the set RR. Hence, the last argument RR enumerates the hypotheses that can be 
used to expand the tree that characterizes the text span under consideration. 
(31) 
(32) 
\[unit(i) A hold(RR)\] --, S(i, i, tree(NUCLEUS, LEAF, {i}, NULL, NULL), RR) 
\[unit(i) A hold(RR)\] --~ S(i, i, tree(SATELLITE, LEAF, {i}, NULL, NULL), RR) 
A set of 12 axioms (rewriting rules) explains how trees can be assembled into larger 
trees in a bottom-up fashion. Let us focus for the moment on the pair of axioms (33) 
and (34), which are given below. 
(33) 
(34) 
\[S( I, b, treel ( NUCLEUS, type1, pl, left1, right1), rr l ) A 
S(b q- 1, h, tree2(SATELLITE, type2, p2, left2, right2), rr2) A 
rhet_rel(name, s n) Ee rrl A rhet_rel(name, s n) E~ rr2 A 
s ff P2 A n E pl A hypotactic(name)\] --, 
S (1, h, tree(NUCLEUS, name, Pl, tree1(...), tree2 (. . . ) ), 
rrl N rr2 \~ {rhet_rel(name, s,n) } ) 
\[S(I, b, treel ( NUCLEUS, typo, pl, left1, righh ), rrl ) A 
S(b -}- 1, h, tree2(SATELLITE, type2, p2, left2, right2), rr2) A 
rhet_rel(name, s n) E~ rrl A rhet_rel(name, s n) Ca rr2 A 
s C P2 A n E pl A hypotactic(name)\] --+
S(I, h, tree(SATELLITE, name, p1, tree1(...), tree2(...)), 
rrl A rr2 \e  {rhet_rel(name, s, n)}) 
428 
Marcu Rhetorical Parsing of Unrestricted Texts 
Assume that there exist two spans: one from unit 1 to unit b that is characterized by 
valid rhetorical structure tree1 (. . .)  and rhetorical relations rrl, and the other from unit 
b + 1 to unit h that is characterized by valid rhetorical structure tree2(...) and rhetorical 
relations rr2. Assume also that rhetorical relation rhet_rel(name, s n) holds between a
unit s that is in the promotion set of span \[b + 1, h\] and a unit n that is in the promotion 
set of span \[l, b\], that rhet_rel(name, s n) can be used to extend both spans \[l, b\] and 
\[b + 1, h\] (rhet_rel(name, s, n) E? rrl and rhet_rel(name, s n) Ee rr2), and that the relation 
is hypotactic. In such a case, one can combine spans \[l, b\] and \[b + 1, h\] into a larger 
span \[l, h\] that has a valid structure whose status is either NUCLEUS (see axiom (33)) or 
SATELLITE (see axiom (34)), type name, promotion set pl, and whose children are given 
by the valid structures of the immediate subspans. The set of rhetorical relations that 
can be used to further extend this structure is given by rrl n rr2 \ {rhet_rel(name, s, n)}. 
We use operators E~ and \~ instead of E and \ because we treat each exclusive 
disjunction as a whole because each exclusive disjunction was hypothesized using 
one and only one hypothesis trigger (cue phrase). That is, we say that a rhetorical 
relation r E~ ri ? ri+l G ? .. ? ri+j if r matches any of the rhetorical relations ri . . ? ri+j. 
We consider that the result of the difference RRi \? r is a subset of RRi that contains 
all the members of RRi except he exclusive disjunction that uses relation r. Because 
axioms (33) and (34) treat each exclusive disjunction as a whole, they ensure that no 
rhetorical relation occurs more than once in a discourse structure. 
Similarly, we can define rules of inference for the cases in which an extended 
rhetorical relation holds across spans \[1,b\] and \[b + 1,hi; for the cases in which the 
satellite precedes the nucleus; and for the cases in which the relation under scrutiny 
is paratactic. (See Marcu \[2000\] for a complete list of these axioms.) Rule (35), for 
examples, corresponds to the case in which the relation under scrutiny is a simple, 
paratactic relation. 
(35) \[S (I, b, treel ( NUCLEUS, type1, pl, left1, right1), rrl ) A 
S(b ? 1, h, tree2(NUCLEUS, type2, p2, left2, right2), rr2) A 
rhet_rel(name, nl, n2) G? rrl A rhet_rel(name, nl, n2) E? rr2 A 
nl E pl A n2 E p2 A paratactic(name)\] --+
S ( I, h, tree(NUCLEUS, name, pl U p2, treel ( . . .) ,  tree2 (. . . ) ) , 
rrl n rr2 \e  {rhet_rel(name, nl, n2)}) 
Example of a Derivation of a Valid Rhetorical Structure. If we take any text of N units that is 
characterized by a set RR of rhetorical relations, the proof-theoretic a count provides all 
the necessary support for deriving the valid rhetorical structures of that text. Assume, 
for example, that we are given text (4), among which rhetorical relations RR given 
in (6), hold. In Figure 7, we sketch the derivation of the theorem that corresponds to 
the valid rhetorical structure shown in Figure 3. The relations RR1 and RR2 that the 
derivation refers to are shown below. 
(36) 
(37) 
? rhet_rel(coNTRAST, 1, 3) ? rhet_rel(coNTRAST, 1, 4) ? 
rhet_rel(CONTRAST, 2, 3) ? rhet_rel(CONTRAST, 2, 4) 
RR1 = rhet_rel(ELABORATION, 4, 1) ? rhet_reI(ELABORATION, 4, 2) ? 
rhet_rel(ELABORATION, 4, 3) 
(rhet_reI(cONTRAST, 1, 3) ? rhet_rel(coNTRAST, 1, 4) ? 
RR2 = ~ rhet_rel(CONTRAST, 2,3) G rhet_rel(CONTRAST, 2,4) 
( rhet_reI(ELABORATION, 2, 1) 
429 
Computational Linguistics Volume 26, Number 3 
1. holcl(RR )
2. unit(l) 
3. unit(2) 
4. unit(3) 
5. unit(4) 
6. S(1,1, tree(NUCLEUS, LEAF, {1}, NULL, NULL), RR) 
7. S(2, 2, tree(SATELLITE, LEAF, {2}, NULL, NULL), RR) 
8. S(1, 2, tree(NUCLEUS, ELABORATION, {1}, 
tree(NUCLEUS, LEAF, {1}, NULL, NULL), 
tree(SATELLITE, LEAF, {2}, NULL, NULL), 
RR1)) 
9. S(3, 3, tree(NUCLEUS, LEAF, {3}, NULL, NULL), RR) 
10. S(4, 4, tree(SATELLITE, LEAF, {4}, NULL, NULL), RR) 
11. S(3, 4, tree(NUCLEUS, ELABORATION, {3}, 
tree(NUCLEUS, LEAF, {3}, NULL, NULL), 
tree(SATELLITE, LEAF, {4}, NULL, NULL), 
RR2)) 
12. S(1, 4, tree(NUCLEUS, CONTRAST, {1, 3}, 
tree(NUCLEUS, ELABORATION, {1}, 
Input 
Input 
Input 
Input 
Input 
1, 2, Axiom (31), MP 
1, 3, Axiom (32), MP 
6, 7, Axiom (33), MP 
1, 4, Axiom (31), MP 
1, 5, Axiom (32), MP 
9, 10, Axiom (33), MP 
8, 11, Axiom (35), MP 
tree(NUCLEUS, LEAF, {1}, NULL, NULL), 
tree(SATELLITE, LEAF, {2}, NULL, NULL)), 
tree(NUCLEUS, ELABORATION, {3}, 
tree(NUCLEUS, LEAF, {3}, NULL, NULL), 
tree(SATELLITE, LEAF, {4}, SC null, NULL))), 
0) 
Figure 7 
A derivation of the theorem that corresponds to the valid rhetorical structure shown in 
Figure 3. 
The derivation starts with five axioms that are straightforwardly derived from 
the input of the problem. Using the axioms in lines 1 and 2, axiom (31), and the 
modus ponens rule, we derive the theorem in line 6. Using the axioms in lines 1 
and 3, axiom (32), and modus ponens, we derive the theorem in line 7. Similarly, we 
derive the theorems in lines 9 and 10. These four theorems all correspond to valid 
rhetorical structures that can be built on top of elementary units. Using the theorems 
in lines 6 and 7, axiom (33), and modus ponens, we derive the theorem in line 8. It 
corresponds to a valid rhetorical structure that can be built across span \[1, 2\]. Since 
this structure uses rhetorical relation rhet_reI(ELABORATION, 2, 1), the set of rhetorical 
relations that can be used to further expand the rhetorical structure will be given by 
the set RR1, shown in (36). Line 11 corresponds to a valid rhetorical structure that can 
be built on top of elementary span \[3,4\]. Since this structure uses rhetorical relation 
rhet_rel(ELABORATION,4,3), the set of rhetorical relations that can be used to further 
expand the rhetorical structure will be given by the set RR2, shown in (37). Using 
the theorems derived in lines 8 and 11, axiom (35), and modus ponens gives us the 
theorem in line 12 that corresponds to a valid structure for the entire text, the structure 
shown in Figure 3. 
As I have shown in Marcu (2000), the proof-theoretic account outlined here is 
both sound and complete with respect o the constraints that characterize the valid 
rhetorical structures enumerated in Section 2.4. That is, all theorems that are derived 
using the disjunctive proof-theoretic account correspond to valid text structures; and 
430 
Marcu Rhetorical Parsing of Unrestricted Texts 
Status = {NUCLEUS.SATELLITE} 
~-L  ~} Type = (BACKGROUND} 
/ / / ~  Promotion = {2} 
Status = {SATTELITE} 1 /  \ Status = {NUCLEUS} 
Type = (LEAF} f ~ ~ Type = {LEAF} 
Promotion = {1} ~.~ ~ Promotion = {2} 
a) 
Status = (NUCLEUS,SATELLITE} 
k~ -L J Type = \[JUSTIRCAT}ON} 
/ / / ~  Promotion = {2} 
Status = {SATTELITE} , /  \ Status = (NUCLEUS} 
Type = {LEAF} f ~ ~ Type = {LEAF} 
Promotion = {I} ~ ~ Promotion = {2} 
b) 
Figure 8 
All valid rhetorical structures of sentence (20). 
Status = (NUCLEUS.SATELLITE} ~ Status = {NUCLEUS,SATELLITE} 
4-6 ) Type = {CAUSE} \[ 4-6 ) Type = {EVIDENCE} 
~tatus = {N?CLEUS} ~ Status = (SATELLITE} ~ Status = {NUCLEUS} ~ Status = (SATELLITE} 
~4-5 ) ?pc = {CONTRAST} \[ 6 ) Type = {LEAF} ~4 5 ) Type = {CONTRAST} ~ 6 ) Type = {LEAF} 
Slatus ( N ~  ~ Stalus = (NUCLEUS} ~ Status {N= UCLEUS} ~ Status = (NUCLEUS} 
~ 4 ) ~AF} \[ 5 ) Type:}LEAF} ~ a ) Type ={LEAF} {, $ ) Type=}LEAF) 
I-'romoton=\[4} ~ Prornot~on=(5} ~ PFomotlon=(4} ~ Promotion={5} 
a) b) 
Status = {NUCLEUS,SATELLITE} 
Type = {CONTRAST} 
= {4.5} 
( ~  Status = {NUCLEUS( / ~  Status = {NUCLEUS} 
Type = }LEAF} ~ 5-6 ) Type = {CAUSE} 
Promotion = ~ =  {5} 
Slatus = {NUCLEUS} ~ Status = }SATELLITE) 
~ 5 )Type=(LEAF} \[ 6 )Type={LEAF} 
P{omolton ={5) ~ Prornotion = {6} 
c) 
Status = }NUCLEUS,SATELLITE} 
~ 4-6 ) Type = {CONTRAST} ~ = 14,51 
Status = {NUCLEUS( ~ Status = {NUCLEUS} 
~ 4 ) Type = {LEAF) \[5-6 ) Type=(EVIDENCE} 
Promotion = ~ =  {5} 
Status = }NUCLEUS} ~ Status = {SATELLITE} 
~. 5 ) Type = {LEAF} ~ 6 ) Type = {LEAF} 
~ Promotion={5) ~ Promotion=(6} 
d) 
Figure 9 
All valid rhetorical structures of sentence (22). 
any valid rhetorical structure can be derived through the successive application of 
modus ponens and the axioms of the disjunctive proof-theoretic account. 
Implementing the Proof-Theoretic A count. There are many ways in which one can imple- 
ment the proof theory described in this section. Since all axioms of the theory are Horn 
clauses, they can be immediately translated into a Prolog program. Equally trivial is to 
implement he proof-theoretic account using traditional parsing techniques that com- 
bine terminal and nonterminal symbols only when the constraints enumerated in the 
axioms of the proof-theoretic account are satisfied. The rhetorical parser implements 
the proof-theoretic account as a chart-parsing algorithm (see Marcu \[2000\] for details). 
When a chart-parsing implementation uses as input the rhetorical relations that were 
hypothesized by the discourse-marker- and word-co-occurrence-based algorithms at 
the sentence, paragraph, and section levels of text (19), it derives the valid rhetorical 
structures shown in Figures 8-13. 
431 
Computational Linguistics Volume 26, Number 3 
' ~  Status = {NUCLEUS,SATELLITE} 
k, " ? j J  Type = {CONCESSION} 
/ / / ~  Promotion = {8} 
Status = {SATTELITE} / ~ Status = \[NUCLEUS} 
Type = {LEAF} /./- ~-~ f ~,~ Type = {LEAF} 
Promotion = {7} ~ ~ Promotion = {8} 
Figure 10 
The valid rhetorical structure of sentence (24). 
Status = {NUCLEUS,SATELLITE} 
1-6 ) Type = {ELABORATION} ~ = {\[1-2\]} 
_Status = {NUCLEUS} ~ Status = {SATELLITE} 
1-2 )':,,T pe= {LEAF} \[ 3-6 )Type = {ELABORATION} 
Promotion = ~ =  {3} 
_Status = {NUCLEUS} ~ Status = {SATELLITE} 
\[ 3 ) Type : {LEAF} { 4-6 ) Type = {LEAF} 
Promotion = {3} ~ Promotion = {\[4-6\]} 
Figure 11 
The valid rhetorical structure of the first paragraph of text (19); see the relations in (29). 
Status : {NUCLEUS.SATELLITE} 
( 7-10 ) Type = {EXAMPLE} ~ = 117-8\]} 
Status = {NUCLEUS} ~ Status = {SATELLITE} 
{~ 7-8 ) Type = {LEAF} {~ 9-10 ) Type = {ANTITHESIS} 
Promotion = ~ =  {10} 
Status = {SATELLITE} ~ Status = {NUCLEUS} 
9 )Type={LEAF} { 10 )Type={LEAF} 
Promotion = {9} ~ Promotion = {10} 
Figure 12 
The valid rhetorical structure of the second paragraph of text (19); see the relations in (26). 
Status = {NUCLEUS} 
Type = {LEAF} 
Promotion = {\[1-6\]} 
Status = {NUCLEUS,SATELLITE} 
Type = {ELABORATION} 
Promotion = {\[1-6\]} 
Status = {SATELLITE} 
Type = {LEAF} 
Promotion = {\[7-10\]} 
Figure 13 
The valid rhetorical structure of text (19); see the relation in (27). 
432 
Marcu Rhetorical Parsing of Unrestricted Texts 
4.6 The Ambiguity of Discourse 
4.6.1 A Weight Function for Rhetorical Structures. Discourse is ambiguous the same 
way sentences are: usually, more than one discourse structure is produced for any 
given text. For example, we have seen that the rhetorical parser finds four different 
valid rhetorical structures for sentence (22) (see Figure 9). In my experiments, I no- 
ticed that the "best" discourse trees are usually those that are skewed to the right. I 
believe that the explanation for this observation is that text processing is essentially 
a left-to-right process. Usually, people write texts so that the most important ideas 
go first, both at the paragraph and at the text level. In fact, journalists are trained to 
consciously employ this "pyramid" approach to writing (Cumming and McKercher 
1994). The more text writers add, the more they elaborate on the text that went be- 
fore: as a consequence, incremental discourse building consists mostly of expansion 
of the right branches. A preference for trees that are skewed to the right is also con- 
sistent with research in psycholinguistics that shows that readers have a preference 
for interpreting unmarked textual units as continuations of the topics of the units 
that precede them (Segal, Duchan, and Scott 1991). At the structural level, this cor- 
responds to textual units that elaborate on the information that has been presented 
before. 
In order to disambiguate he discourse, the rhetorical parser computes a weight 
for each valid discourse tree and retains only the trees that are maximal. The weight 
function w, which is shown in (38), is computed recursively by summing up the 
weights of the left and right branches of a rhetorical structure and the difference 
between the depth of the right and left branches of the structure. Hence, the more 
skewed to the right a tree is, the greater its weight w is. 
(38) l 0 if isLeaf(tree), w( tree) = w( left Of ( tree) + w( right Of ( tree) + otherwise. 
depth( rightOf ( tree) - depth( leftOf ( tree) 
For example, when applied to the valid rhetorical structures of sentence (22), the 
weight function will assign the value -1 to the trees shown in Figures 9(a) and 9(b), 
and the value +1 to the trees shown in Figures 9(c) and 9(d). 
4.6.2 The Ambiguity of Discourse--An Implementation Perspective. There are two 
ways one can disambiguate discourse. One way is to consider, during the parsing 
process, all of the valid rhetorical structures of a text. When the parsing is complete, 
the structures of maximal weight can be then assigned to the text given as input. The 
other way is to consider, during the parsing process, only the partial structures that 
could lead to a structure of maximal weight. For example, if a chart parsing algorithm 
is used, we can keep in the chart only the partial structures that could lead to a final 
structure of maximal weight. 
In step III.4, the rhetorical parser shown in Figure 5 implements the second ap- 
proach. Hence, instead of keeping all the partial structures that characterize sen- 
tence (22), it will keep only the partial structures of maximal weight, i.e., the structures 
shown in Figures 9(c) and 9(d). In this way, the overall efficiency of the system is in- 
creased. 
When the rhetorical parser selects the trees of maximal weight for text (19), at each 
of the three levels of abstraction, it selects the trees shown in Figures 8(a), 9(c), 10, 
11, 12, and 13. If no weight function were used, the rhetorical parser would generate 
eight distinct valid rhetorical structures for the whole text. 
433 
Computational Linguistics Volume 26, Number 3 
? " Surta?? Icmpcramres 
? With ils d ist~t or~ill ? typically a~rag? aboul 
, . ~)  p~rcgm t~hcr  ? \[" .60  Ocgr~.. .  r . , .  , , , . ,  
' Irom the sun \[han E~th- ? M~s expcrlences trigid 
: } ~d slim atmospl~ric ' ~athc  dmons  (-76 deg~ 
a~p to - 
' , .  
' Example  : , 
, .  . . . .  - .  , . .  
. . . . . . .  gh . . . . . .  osph:rc : i mo~M . . . . . . . . .  " ~va~cr 'n di~il0e? ~?s  " Ylx'lc\[' wh '~ ; sun 
a ~m~l  amt~pl  nvo  v? b ow n~ du  : cw ~1?rs  ~ ~is  dry - i ce  ' remains  in ~h? sky all 
? (7 )  . . . . . .  : cvap?ra\[?$ Irom th~ , II I ( 10 ) I 
~ \ [ l~s  ?v? op. .  ( 8 ? t f~n c~b~ dtOxld ? : never w~m cnoug to 
Lsc ol the low atmo~,h?.c  ? 
Figure 14 
The discourse tree of maximal weight that is built by the rhetorical parsing algorithm for 
text (1). Nuclei are surrounded by solid boxes and satellites by dotted boxes; links between a
node and the subordinate nucleus or nuclei are represented by solid arrows; links between a
node and the subordinate satellites by dotted lines. Occurrences of parenthetical information 
are enclosed in the text by curly brackets; the leaves of the discourse structure are numbered 
from 1 to N, where N represents he number of elementary units in the whole text. The 
numbers associated with each node denote the units that are members of its promotion set. 
4.7 Deriving the Final Rhetorical Structure 
In the last step (lines 16-17 in Figure 5), after the trees of maximal  weight have been 
obtained at the sentence, paragraph, and section levels, the rhetorical parser merges 
the valid structures into a structure that spans the whole text of a section. In this way, 
the rhetorical parser builds one tree for each of the sections of a given document. The 
merging process is a trivial procedure that assembles the trees obtained at each level of 
granularity. That is, the trees that correspond to the sentence level are substituted for 
the leaves of the structures built at the paragraph level, and the trees that correspond to 
the paragraph levels are substituted for the leaves of the structures built at the section 
level. The promotion units associated with each span are recomputed in a bottom-up 
fashion so that they correspond to elementary units and not to sentence and paragraph 
labels. The rhetorical parser has a back-end process that uses "dot," a preprocessor 
for drawing oriented graphs, to automatical ly generate PostScript representations of 
the rhetorical structures of maximal  weight. When appl ied to text (1), the rhetorical 
parser builds the rhetorical structure shown in Figure 14. 
5. Evaluation 
There are two ways to evaluate the correctness of the discourse trees that an automatic 
process builds. One is to compare the automatical ly derived trees with trees that have 
been built manually. The other is to evaluate the impact that they have on the accuracy 
of other natural language processing tasks, such as anaphora resolution, intention 
recognition, or text summarization. The rhetorical parser presented here was evaluated 
by following both of these avenues. 
434 
Marcu Rhetorical Parsing of Unrestricted Texts 
Manually annotated tree 
1-? 
4-7 i~  
/^,,* \[EXAMPLE 
2 3 E ~ N  6-7 
4 5 
Automatically nnotated tree 
1-7 
2-3 
6 7 
a) b) 
Figure 15 
Example of discourse trees: (a) represents a manually built tree; (b) represents an 
automatically built tree. 
5.1 Evaluating the Correctness of the Trees 
5.1.1 Labeled Recall and Precision Figures. To evaluate the correctness of the trees 
built by the rhetorical parser, two analysts have manually built the rhetorical structure 
of five texts from Scientific American, which ranged in size from 161 to 725 words. The 
analysts were computational linguists who were familiar with Rhetorical Structure 
Theory (Mann and Thompson 1988). They did not agree beforehand on any annotation 
style or protocol and were not given any specific instructions besides being asked 
to build trees that were consistent with the requirements put forth by Mann and 
Thompson. The analysts were supposed to use only the set of relations proposed by 
RST and the relation TEXTUAL to link the subtrees ubsuming the title and body of a 
text. Analysts were not asked to build binary structures ( imilar to those derived by the 
rhetorical parser), although we knew that this could negatively affect he performance 
of our system. 
The performance of the rhetorical parser was estimated by applying labeled re- 
call and precision measures, which are extensively used to study the performance of
syntactic parsers. Labeled recall reflects the number of correctly labeled constituents 
identified by the rhetorical parser with respect o the number of labeled constituents 
in the corresponding manually built tree. Labeled precision reflects the number of cor- 
rectly labeled constituents identified by the rhetorical parser with respect o the total 
number of labeled constituents identified by the parser. Labeled recall and precision 
figures were computed with respect o the ability of the rhetorical parser to identify 
elementary units, hierarchical text spans, text span nuclei and satellites, and rhetorical 
relations. 
To understand how these figures were computed, assume for example that an 
analyst identified six elementary units in a text and built the discourse structure in 
Figure 15(a) and that the program identified five elementary units and built the dis- 
course structure in Figure 15(b). When we align the two structures, we obtain the labels 
in Table 7, which show that the program did not identify the breaks between units 2 
and 3, and 4 and 5 in the analyst's annotation; and that it considered the unit labeled 
6-7 in the analyst's annotation to be made of two units. Table 7 lists all constituents 
in the two structures, the associated labels at the elementary unit, span, nuclei, and 
rhetorical levels, and the corresponding recall and precision figures. As Table 7 shows, 
435 
Computational Linguistics Volume 26, Number 3 
Table 7 
Computing the performance of a rhetorical parser (P -- Program; 
A = Analyst). 
Constituent Units Spans Nuclearity 
A P A P A P A 
Relations 
1-1 * * * * N N SPAN SPAN 
2-2 * * N JOINT 
3-3 * * N JOINT 
4-4 * * N SPAN 
5-5 * * S ELABORATION 
6-6 * * N JO INT  
7-7 * * N JOINT 
2-3 * * * N S CONTRAST ANTITHESIS 
4-5 * * * N N SPAN SPAN 
6-7 * * * S S EXAMPLE EXAMPLE 
4-7 * * N N CONTRAST SPAN 
2-7 * * S S ELABORATION ELABORATION 
R=1/6 R=6/10 R=5/10 R=4/10 
P=1/5  P=6/8  P=5/8  P=4/8  
CON ,ON 
2 3 1 2 
a) b) c) 
Figure 16 
Discourse trees (b) and (c) represent alternative binary representations of the nonbinary 
discourse tree in (a). 
the program in this example identified only one of the six elementary units identified 
by the analyst (unit 1), for a recall of 1/6. Since the program identified a total of five 
units, the precision is 1/5. Similarly, recall and precision figures can be computed for 
span, nuclearity, and rhetorical relation assignments. 
This evaluation assumes that rhetorical abels are associated with the children 
nodes, and not with the father nodes, as in the formalization. For example, the EX- 
AMPLE relation that holds between spans \[4,5\] and \[6,7\] in the tree in Figure 15(a), is 
not associated with span \[4,7\], but rather, with the span \[6,7\], which is the satellite of 
the relation; and by convention, the rhetorical relation of the span \[4,5\] is set to SPAN. 
The rationale for this choice is the fact that the analysts did not construct only binary 
trees; some of the nodes in their manually built representations had multiple children. 
Representing in binary form a tree such as that shown in Figure 16(a), for example, 
would require an additional hierarchical level on the spans, as shown in Figures 16(b) 
and 16(c), that was not part of the original analysis. To avoid introducing in the anno- 
tation choices that were not part of what the analysts did, I decided for the purpose 
of evaluation to follow the procedure outlined above. 
Table 8 shows average recall and precision figures that reflect the performance 
of the rhetorical parser on the five Scientific American texts. In addition to the recall 
436 
Marcu Rhetorical Parsing of Unrestricted Texts 
Table 8 
Performance ofthe rhetorical parser. 
Analysts Program 
Recall Precision Recall Precision 
Elementary units 87.9 87.9 51.2 95.9 
Spans 89.6 89.6 63.5 87.7 
Nuclearity 79.4 88.2 50.6 85.1 
Relations 83.4 83.4 47.0 78.4 
and precision figures specific to the program, Table 8 also displays average recall and 
precision figures obtained for the trees built only by the analysts. These figures reflect 
how similar the annotations of the two analysts were and provides an upper bound 
for the performance of the rhetorical parser: if the recall and precision figures of the 
parser were the same as the figures for the analysts, the discourse trees built by the 
rhetorical parser would be indistinguishable from those built by a human. 
As the results in Table 8 show, the rhetorical parser fails to identify a fair num- 
ber of elementary units (51.2% recall); but the units it identifies tend to be correct 
(95.9% precision). As a consequence, performance atall other levels is affected. With 
respect o identifying hierarchical spans, recall is about 25% lower than the average 
human performance; with respect to labeling the nuclear status of spans, recall is about 
30% below human performance; and with respect o labeling the rhetorical relations 
that hold between spans, recall is about 40% below human performance. In general, 
the precision of the rhetorical parser comes close to the human performance l vel. 
However, since the level of granularity at which the rhetorical parser works is much 
coarser than that used by human judges, many sentences are assigned a much simpler 
structure than the structure built by humans. For example, whenever an analyst used 
a JOINT relation to connect wo clause-like units separated by an and, the rhetorical 
parser failed to identify the two units; it often treated them as a single elementary 
unit. As a consequence, the recall figures at all levels were significantly lower than 
those specific to the humans. 
5.1.2 Confus ion Matrices. Another way to evaluate the performance of the rhetorical 
parser is to build a confusion matrix over the most frequently used relations. To enable 
the reader to distinguish between rhetorical and nuclearity errors, I follow the same 
strategy as in the case of computing labeled recall and precision figures. That is, I 
consider by convention that the nuclei nodes of a rhetorical representation are labeled 
with the relation SPAN. 
Table 9 shows the distributions of rhetorical relation labels used by one of the ana- 
lysts and the program. The most frequently used relations were JOINT~ ELABORATION, 
and CONTRAST. (Label SPAN denotes the nucleus of any mononuclear relation.) Over- 
all, the 15 most frequently used relations account for more than 92% of the relations in 
the corpus. The distribution of relations inferred by the program is somewhat similar, 
with the most frequently used relations being JOINT~ ELABORATION, and CONTRAST 
as well. The program, though, shows a stronger preference for ELABORATION relations 
over JOINTS. 
Table 10 shows a confusion matrix that reflects the ability of the rhetorical parser to 
derive rhetorical structure trees. The confusion matrix compares cumulatively, over the 
entire corpus, the rhetorical relations inferred by the parser with the rhetorical relations 
437 
Computational Linguistics Volume 26, Number 3 
Table 9 
Distribution of the most frequently used 15 
relations. 
Relation Judge (%) Program (%) 
SPAN 32.62 35.65 
JOINT 14.53 14.78 
ELABORATION 12.76 20.43 
CONTRAST 7.80 7.82 
TEXTUAL 3.54 3.47 
CONDITION 3.19 1.73 
EXAMPLE 2.83 3.04 
SEQUENCE 2.83 - 
EVIDENCE 2.12 - 
OTHERWISE 2.12 0.86 
PURPOSE 1.77 0.86 
CONCESSION 1.77 1.73 
CIRCUMSTANCE 1.77 1.30 
BACKGROUND 1.77 1.73 
CAUSE 1.41 2.60 
Table 10 
Confusion matrix. 
Relation (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (1) (m) (n) (o) (p) 
SPAN (a) 51 8 9 3 
JOINT CO) 4 4 5 1 1 
ELABORATION (C) 2 5 18 1 
CONTRAST (d) 1 1 14 1 
TEXTUAL (e) 10 
CONDITION(f) 5 1 
EXAMPLE (g) 1 6 1 
SEQUENCE (h) 3 4 1 
EVIDENCE (i) 1 1 4 
OTHERWISE0) 2 
PURPOSE (k) 2 
CONCESSION (1) 2 3 
CIRCUMSTANCE (Ill) 1 1 1 
BACKGROUND (n) 2 1 
CAUSE (O) 1 1 5 
OTHER (p) 2 1 1 1 2 
NO SPAN (r) 14 8 5 2 1 1 1 2 
chosen by  one analyst .  For  any  re lat ion a, a co lumn in the confus ion  matr ix  reflects 
the number  of re lat ions of type  R that were  ( in)correct ly ident i f ied by  the rhetor ica l  
parser  w i th  respect  o the re lat ions ident i f ied by  the analyst .  For example ,  the co lumn 
labe led (d) shows  that out  of 18 textual  spans  labe led w i th  the re lat ion CONTRAST 
by  the parser,  14 were  labe led  as CONTRASTS, one as the satel l i te of an ELABORATION 
relat ion,  and  one as the satel l i te of an OTHER re lat ion by  the analyst .  In add i t ion ,  two  
of the 18 spans  had  no cor respond ing  span  in one ana lyst ' s  representat ion.  
The confus ion matr ix  shows  that the rhetor ica l  parser  does a fair ly good  job at 
recogniz ing rhetor ica l  re lat ions that are usua l ly  marked  by  cue phrases,  i.e., CONTRAST~ 
CONDITION~ EXAMPLE, OTHERWISE~ PURPOSE~ CONCESSION~ and CAUSE relat ions.  The 
confus ion matr ix  also shows  that the s imple  mode l  of cohes ion that our  parser  employs  
is not  adequate  for d i s t ingu ish ing  between rhetor ica l  re lat ions of ELABORATION and 
438 
Marcu Rhetorical Parsing of Unrestricted Texts 
I 2 k 
a) b) 
Figure 17 
A flat (a) and a binary (b) representation f the discourse structure of a text. 
JOINT; the submatrix that subsumes the labels SPAN~ JOINT, and ELABORATION shows 
the highest levels of confusion. Clearly, semantic similarity is not sufficient if one is to 
decide whether a rhetorical relation of JOINT, ELABORATION, or BACKGROUND holds 
between two textual segments. 
The rhetorical parser is unable to recognize "purely" intentional relations, such 
as EVIDENCE, which are seldomly marked. As the confusion matrix shows, the parser 
labels no relation as EVIDENCE; rather, it chooses instead ELABORATION, four times, 
JOINT, once, and a relation with a different nuclearity, once. Since the parser draws 
no temporal inferences, it labels most of the SEQUENCE relations as JOINTS, which in 
many cases is an adequate approximation. 
The relatively large number of spans that have no correspondence in one analyst's 
representations can be explained by two factors. The first factor concerns our repre- 
sentation choice. Since the discourse parser builds binary trees, it also derives text 
spans that cannot be matched against a nonbinary representation. For example, al- 
though the tree in Figure 17(b) is a binary reformulation of the tree in Figure 17(a), the 
intermediate spans \[1,k\], \[2,k\], . . . ,  \[k-l,k\] in Figure 17(b) cannot be matched against 
any span in the tree in Figure 17(a). Humans built structures uch as those shown in 
Figure 17(a); the discourse parser did not. The second factor concerns the difficulty of 
the task. Some texts have very sophisticated text structures, which are difficult to infer 
only on the basis of cue phrase occurrences and word-based similarity measures. We 
discuss some of the difficult cases in Section 5.1.3, below. 
5.1.3 Qualitative Evaluation. As the quantitative valuation results in the preceding 
section show, the rhetorical structures that can be derived by relying only on discourse 
markers and cohesion are adequate in some cases and not in others. I discuss some of 
these cases below. 
Good Discourse Structures at the Paragraph Level. In most of the cases that I inspected 
visually on a variety of texts, the partial structures built above sentences and within 
paragraphs appeared to be adequate. The explanation is simple: Paragraphs that use 
few discourse markers tend to express the most important information at their begin- 
ning, which corresponds to the first sentence being a nucleus and subsequent sentences 
elaborating on it. Such paragraphs usually have discourse structures that are similar 
to those preferred by the rhetorical parser, which favors structures that are skewed to 
the right and which hypothesizes that ELABORATION relations hold between unmarked 
sentences. If a paragraph as a more complex discourse structure, it usually employs 
discourse markers, which are detected and exploited by the rhetorical parser. 
439 
Computational Linguistics Volume 26, Number 3 
Good Discourse Structures at the Text Level, for Short Texts. The same argument applies 
for short texts as well. Texts that consist of only a few paragraphs also tend to 
have structures that are skewed to the right. When they do not, these texts usu- 
ally rely on discourse markers to signal to the reader that the content of a paragraph 
should not be interpreted as a simple ELABORATION of the material that was presented 
before. 
Good Discourse Structures for Sentences, Paragraphs, and Texts that Use Unambiguous Dis- 
course Markers. Discourse markers uch as although, in contrast, and however signal in 
most cases CONCESSION~ CONTRAST~ and ANTITHESIS relations, respectively. Since the 
use of these markers is consistent, most of the rhetorical relations that are signaled by 
such markers are correctly identified. For example, more than 75% of the CONTRAST 
relations that hold across clauses, sentences, and paragraphs in our corpus of Scientific 
American texts were correctly identified. 
Good Discourse Structures for Sentences that Use Markers Other than And. The structures 
that the discourse parser derives for sentences that use discourse markers uch as be- 
cause, if, and when closely match those built by humans. Although the discourse parser 
overhypothesizes r lations, the constrained mathematical model it relies upon consid- 
erably reduces the space of valid discourse interpretations. The nuclearity preferences 
associated with the discourse markers eliminate many of the invalid interpretations. 
As a consequence, the discourse structures built for sentences that have clause-like 
units as leaves are correct in most of these cases. 
Bad Discourse Structures for Sentences that Use the Discourse Marker And. Problems from 
this category are readily observed in the trees in Figure 1 and Figure 14. For example, 
the rhetorical parser is not able to identify that a discourse boundary should be inserted 
before the occurrence of and in the sentence "\[Surface t mperatures typically average 
about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator\] [and can dip to 
-123 degrees C near the poles.\]." As a consequence, the recall figure with respect o 
identifying the elementary units of this sentence is 0. The recall figure with respect to 
identifying the hierarchical spans of this sentence is 1/3 (the parser correctly identifies 
only the span that subsumes the entire sentence but not the two subspans that subsume 
the elementary units). The recall figures with respect o identifying the nuclearity of 
the spans and the rhetorical relations that hold between them are also negatively 
affected. Hence, it seems clear that surface-based methods are not sufficient if we are 
to approach uman performance l vels at the task of identifying elementary discourse 
units. 
By examining the failures of the elementary unit boundary identification algo- 
rithm, I have come to believe that some of the problematic cases could be solved by 
using part-of-speech tags and other syntactic information. For example, in many of the 
sentences in which and is followed by a verb, an elementary unit boundary needs to 
be inserted before its occurrence. Such a rule would be sufficient for breaking into two 
units the example sentence considered above. 6It remains to be seen whether a rhetor- 
ical parser can approach uman performance l vels without building full syntactic 
trees for the sentences under consideration. 
6 For research t at uses part-of-speech tags in order to identify elementary unit boundaries, see Marcu 
(1999a). 
440 
Marcu Rhetorical Parsing of Unrestricted Texts 
Incorrectly Labeled Intentional Relations. We can see from the trees in Figure 1 and Fig- 
ure 14 that although the rhetorical parser correctly identified the hierarchical segments 
and the nuclearity statuses in the first paragraph, it was unable to determine that a 
rhetorical relation of EVIDENCE holds between the last two sentences and the first 
sentence of the first paragraph. Instead, the parser used an ELABORATION relation. In 
general, the discourse parser is unable to correctly identify intentional relations, in 
particular, relations of EVIDENCE that hold between sentences and paragraphs. Such 
relations are usually not marked; to derive them one needs to "understand" what a 
text is about. For example, our rhetorical parser mislabeled as ELABORATION and JOINT 
all six EVIDENCE relations that hold between sentences and paragraphs in the texts in 
our Scientific American corpus. 
It seems that to build RS-trees as accurately as humans, relying only on cue phrases 
and cohesion is not sufficient. In some cases, a deeper analysis of the relation between 
connectives and rhetorical relations, such as that proposed by Grote et al (1997) in 
the context of natural language generation, may help hypothesize better elations. In 
general, though, it is unclear what forms of reasoning to use to derive, for unrestricted 
texts, relations that are as difficult to infer as the EVIDENCE relation in Figure 1. 
Bad Discourse Structures for Very Large Texts. When the discourse parser attempts to de- 
rive the structure of very large texts, the preference for structures that are skewed to 
the right and the modeling of discourse as binary trees do not always work. For exam- 
ple, some newspaper articles are written so that k facets of the most important idea are 
presented in the first paragraph. And then, each of these facets is elaborated in turn in 
subsequent paragraphs. An adequate discourse structure for such a text is one that has 
the first paragraph as nucleus and k satellites directly linked to it at the same level of 
embedding (see Figure 17(a)). The choice of modeling the discourse structure of texts 
using binary representations appears to be infelicitous in such cases because binary 
trees induce an unjustified number of additional levels of embedding (see Figure 17(b)). 
Since the rhetorical parser derives binary trees only, it cannot represent discourse struc- 
tures that would closely match the structure of newspaper articles of this kind. 
The preference for discourse trees that are skewed to the right is also problematic 
when handling texts that start by providing some background information or by mo- 
tivating the reader before presenting the main idea. For example, the text in italics in 
(39) should be the satellite of a MOTIVATION relation whose nucleus ubsumes the rest 
of the text. 
(39) Running nose. Raging fever. Aching joints. Splitting headache. Are there any poor 
souls suffering from the fiu this winter who haven't longed for a pill to make it all 
go away? Relief may be in sight. Researchers atGilead Sciences, a
pharmaceutical company in Foster City, California, reported last week in 
the Journal of the American Chemical Society that they have discovered 
a compound that can stop the influenza virus from spreading in animals. 
Tests on humans are set for later this year. 
Unfortunately, cohesion is not enough for determining this relation. Consequently, the 
discourse structure built by the rhetorical parser for this text is erroneous. 
5.2 Evaluating the Usefulness of the Trees for Text Summarization 
From a salience perspective, the elementary units in the promotion set of a node of a 
tree structure denote the most important units of the textual span that is dominated 
by that node. For example, according to the rhetorical structure in Figure 14, unit 3 
441 
Computational Linguistics Volume 26, Number 3 
is the most important unit of span \[3,6\], units 4 and 5 are the most important units 
of span \[4,6\], and unit 2 is the most important unit of the whole text. If we apply 
the concept of salience over all elementary units in a text, we can use the rhetorical 
structure to induce a partial ordering on the importance of these units. The intuition 
behind this approach is that the textual units in the promotion sets of the top nodes of 
a discourse tree are more important than the units that are salient in the nodes found 
at the bottom. When applied to the rhetorical structure in Figure 14, such an approach 
induces the partial ordering in (40), because unit 2 is the only promotion unit of the 
root; unit 8 is the only unit found in the promotion set of a node immediately below 
the root (unit 2 has been already accounted for); units 3 and 10 are the only units 
that belong to promotion sets of nodes that are two levels below the root; and so on. 
(See Marcu \[1999b\] for a mathematical formulation of this method that uses rhetorical 
structures for deriving a partial ordering of the important units in texts.) 
(40) 2>873,10>1,4 ,5 ,7 ,9>6 
If we are interested in generating a very short summary of text 19, for example, we can 
then produce an extract containing only unit 2, because this is the most important unit 
given by the partial ordering derived from the corresponding rhetorical representation. 
A longer summary will contain units 2 and 8; a longer one, units 2, 8, 3, and 10; and 
SO on .  
Using this idea, I have implemented a rhetorical-based summarization algorithm. 
The algorithm uses the rhetorical parser described in this paper to determine the 
discourse structure of a text given as input, it uses the discourse structure to induce 
a partial ordering on the elementary units in the text, and then, depending on the 
desired compression rate, it selects the p most important units in the text. 
To evaluate this summarization program, I used two corpora: the five Scientific 
American texts that I have mentioned above, and a collection of 40 short newspa- 
per articles from the TREC collection (Jing et al 1998). Both corpora were labeled 
for textual salience by a panel of independent judges: 13 judges labeled clause-like 
units as being important, somewhat important, and nonimportant in the texts of the 
Scientific American corpus; and 5 judges labeled sentences as worthy to be included 
in 10% and 20% summaries of the texts in the TREC corpus. The clauses/sentences 
which the human judges agreed were important were taken as the gold standard for 
summarization. 
The rhetorical parser derived the RS-tree of each of the 45 texts in the two corpora, 
and used the RS-tree to induce a partial ordering of the importance of the elementary 
units in the corresponding text. The rhetorical summarizer then selected the most 
important k units in a text, where k was chosen so as to match as closely as possible the 
number of units in the gold standard. The number of units selected for summarization 
was determined similarly for the other summarization programs that I used in the 
evaluation. 
To assess the performance of the rhetorical-based summarizer (and of the other 
summarizers that I discuss below), I use recall, precision, and F-value figures. The recall 
figure is given by the number of units that were correctly identified by the summarizer 
as being important, over the total number of important units in the gold standard. 
The precision figure is given by the number of units that were correctly identified by 
the summarizer as being important, over the total number of units identified by the 
summarizer. The F-value is a combined Recall-Precision value, given by the formula 
2 x Recall x Precision/(Recall + Precision). 
442 
Marcu Rhetorical Parsing of Unrestricted Texts 
Table 11 
The performance ofthe rhetorical-based summarizer. 
Corpus Method Recall Precision Dvalue 
Scient~'c American 
(Clause-level 
summarization) 
Judges 72.66 69.63 71.11 
Rhetorical-based summarizer with learning 67.57 73.53 70.42 
Rhetorical-based summarizer 51.35 63.33 56.71 
Microsoft Office97 summarizer 27.77 25.44 26.55 
Lead baseline 39.68 39.68 39.68 
Random baseline 25.70 25.70 25.70 
TREC 
Sentence-level 
summarization 
(20% compression 
rate) 
Judges 82.83 64.93 72.80 
Rhetorical-based summarizer with learning 61.79 60.83 61.31 
Rhetorical-based summarizer 46.54 49.73 48.08 
Microsoft Office97 summarizer 39.00 32.00 35.15 
Lead baseline 70.91 46.96 56.50 
Random baseline 15.80 15.80 15.80 
In order to compare the performance of the rhetorical-based summarizer with that 
of humans, I have also determined the performance of the human judges, by averaging 
the performance of each judge with respect o the gold standard. As Table 11 shows, 
the human-level F-value for the task of identifying important clauses in the Scientific 
American corpus was 71.11%; the human-level F-value for the task of identifying the 
most important 20% of the sentences in the TREC texts was 72.80%. To better assess the 
performance of the rhetorical-based summarizer, I also determined the performance of
two baseline summarizers. The lead-based summarizer assumes that the most impor- 
tant k units in a text are the first k units in that text. The random-based summarizer 
assumes that the most important k units in a text can be selected stochastically. 
As Table 11 shows, for both corpora, the rhetorical-based summarizer performs 
better than the random baseline summarizer and better than a commercial system, 
the Microsoft Office97 summarizer. The rhetorical-based summarizer outperforms the 
lead-based summarizer only for texts in the Scientific American corpus. Most of the 
newspaper articles in the TREC collection employ the pyramid journalistic style and 
have the most important sentences at the beginning of the articles. As a consequence, 
the performance of the lead-based summarizer on TREC texts is quite high. However, 
an implementation f the rhetorical parser that uses learning techniques to choose 
rhetorical interpretations that are likely to increase the performance of the rhetorical- 
based summarizer yields a program that identifies important units at levels of per- 
formance that are close to human performance for Scientific American texts and that 
are about 10% below human performance for TREC newspaper articles and about 
5% above the lead baseline. The rhetorical-based summarizer that employs learning 
techniques to improve its performance is discussed in detail in Marcu (2000). 
The data in Table 11 shows that although the rhetorical parser does not produce 
perfect rhetorical structure trees, it can be used successfully todetermine the important 
units of texts. 
6. Re lated Work 
When this research was carried out, there was no rhetorical parser for English. How- 
ever, very recently, Corston-Oliver (1998) has explored a different facet of the work 
described here and investigated the possibility of using syntactic information to hy- 
443 
Computational Linguistics Volume 26, Number 3 
pothesize relations. His system uses 13 rhetorical relations and builds discourse trees 
for articles in Microsoft's Encarta 96 Encyclopedia. I believe that the research that comes 
closest o that described in this chapter is that of Sumita et al (1992) and Kurohashi 
and Nagao (1994). 
Sumita et al (1992) report on a discourse analyzer for Japanese, which differs 
from mine in a number of ways. Particularly important is the fact that the theoretical 
foundations of Sumita et al's analyzer do not seem to be able to accommodate he 
ambiguity of discourse markers; in their system, discourse markers are considered 
unambiguous with respect to the relations that they signal. In contrast, my rhetorical 
parser uses a mathematical model in which this ambiguity is acknowledged and ap- 
propriately treated. Furthermore, the discourse trees that the rhetorical parser builds 
are more constrained structures (Marcu 2000): as a consequence, the rhetorical parser 
does not overgenerate invalid trees as Sumita et al's does. Finally, my rhetorical parser 
uses only surface-form ethods for determining the markers and textual units and 
uses clause-like units as the minimal units of the discourse trees. In contrast, Sumita et 
al. use deep syntactic and semantic processing techniques for determining the markers 
and the textual units and use sentences as minimal units in the discourse structures 
that they build. 
Kurohashi and Nagao (1994) describe a discourse structure generator that builds 
discourse trees in an incremental fashion. The algorithm proposed by Kurohashi and 
Nagao starts with an empty discourse tree and then incrementally attaches sentences to
its right frontier, in the style of Polanyi (1988). The node of attachment is determined 
on the basis of a ranking score that is computed using three different sources: cue 
phrases, chains of identical and similar words, and similarities in the syntactic structure 
of sentences. As in the case of Sumita's ystem, Kurohashi and Nagao's ystem takes 
as input a sequence of parse trees; hence, in order to work, it must be preceded by a 
full syntactic analysis of the text. The elementary units of the discourse trees built by 
Kurohashi and Nagao are sentences. 
Since the systems developed by Corston-Oliver (1998), Sumita et al (1992), and 
Kurohashi and Nagao (1994) were not evaluated intrinsically, it is difficult o compare 
the performance of their systems to ours. 
A parallel ine of research as been investigated recently by Strube and Hahn 
(1999). They have extended the centering model proposed by Grosz, Joshi, and We- 
instein (1995) by devising algorithms that build hierarchies of referential discourse 
segments. These hierarchies induce a discourse structure on text, which constrains 
the reachability of potential anaphoric antecedents. The referential segments are con- 
structed through an incremental process that compares the centers of each sentence 
with those of the structure that has been built up to that point. 
The referential structures that are built by Hahn and Strube exploit a language facet 
different from that exploited by the rhetorical parser: their algorithms rely primarily 
on cohesion and not on coherence. Because of this, the referential structures are not 
as constrained as the discourse structures that the rhetorical parser builds. In fact, 
the discourse relations between the referential segments are not even labeled. Still, I 
believe that studying the commonalities and differences between the referential and 
rhetorical segments could provide new insights into the nature of discourse. 
7. Discussion and Conclusion 
Automatically deriving the discourse structure of texts is not trivial. This paper dis- 
cusses extensively the strengths and weaknesses of an approach to discourse parsing 
that relies on cue phrases, cohesion, and a formal model of discourse. Quantitative 
444 
Marcu Rhetorical Parsing of Unrestricted Texts 
and qualitative analyses of the results show that many relations can be identified 
correctly within this framework. However, this approach is not sufficient for identi- 
fying intentional relations, such as EVIDENCE, or for choosing between ELABORATION, 
BACKGROUND~ SEQUENCE, and JOINT relations. 
The brightest side of the story is that the results in this paper show that the 
rhetorical structures derived by my parser can be used successfully in the context of 
text summarization. Hence, although the rhetorical parser does not get the RS-trees 
perfectly right, it still manages to determine the important units of text at levels of 
performance that are not far from those of humans. One possible explanation may 
be that the rhetorical-based summarizer described here exploits only the difference 
between satellites and nuclei and the hierarchical structure of text to determine text 
units that are important. The reader should not infer from this that correctly identifying 
the rhetorical relations that hold between spans cannot be useful in a summarization 
setting. It is likely, for instance, that one may want to systematically exclude from an 
abstract information that is subsumed by the satellite of an EXAMPLE relation; to do 
so, it is necessary to identify correctly the relation. 
The rhetorical summarizer is a niche application that shows how an understand- 
ing of the hierarchical organization of text can make solving difficult natural anguage 
problems easier. Recent research as shown that by exploiting the structure of dis- 
course, one can decrease storage space in information retrieval applications (Corston- 
Oliver and Dolan 1999) and address discourse-specific problems in machine translation 
(Marcu, Carlson, and Watanabe, 2000). It is possible that discourse structures of the 
kinds derived by this parser can have a positive impact on other problems as well. For 
example, Cristea et al (1999) have shown that a hierarchical model of discourse has 
a higher potential for improving the performance of a coreference resolution system 
than a linear model of discourse. And Hirschman et al (1999) have suggested that 
certain types of questions can be better answered if one has access to rhetorical struc- 
ture representations of the texts that contain the answers to the questions. How much 
of an impact the rhetorical parser presented here can have on solving these problems, 
of course, remains an empirical question. 
Acknowledgments 
I am grateful to Graeme Hirst for the help 
and advice he gave me during every stage 
of this work; and to Marilyn Walker for 
suggestions that led to significant 
improvements, especially in the Evaluation 
section of the paper. I am also grateful to 
four anonymous reviewers who provided 
thoughtful feedback on an earlier draft. 
Most of this research was conducted while I 
was at the University of Toronto and was 
supported by the Natural Sciences and 
Engineering Research Council of Canada. 
References 
Asher, Nicholas. 1993. Reference toAbstract 
Objects in Discourse. Kluwer Academic 
Publishers, Dordrecht. 
Asher, Nicholas and Alex Lascarides. 1994. 
Intentions and information i  discourse. 
In Proceedings ofthe 32nd Annual Meeting, 
pages 34-41, New Mexico State 
University, Las Cruces, NM, June. 
Association for Computational 
Linguistics. 
Bestgen, Yves and Jean Costermans. 1997. 
Temporal markers of narrative structure: 
Studies in production. In Jean Costermans 
and Michel Fayol, editors, Processing 
Interclausal Relationships. Studies in the 
Production and Comprehension f Text. 
Lawrence Erlbaum Associates, Hillsdale, 
NJ, pages 201-218. 
Briscoe, Ted. 1996. The syntax and 
semantics of punctuation and its use in 
interpretation. I  Proceedings ofthe 
Association for Computational Linguistics 
Workshop on Punctuation, pages 1-7, Santa 
Cruz, CA, June. 
Bruder, Gail A. and Janice M. Wiebe. 1990. 
Psychological test of an algorithm for 
recognizing subjectivity in narrative text. 
In Proceedings ofthe Twelfth Annual 
Conference on the Cognitive Science Society, 
pages 947-953, Cambridge, MA, July. 
445 
Computational Linguistics Volume 26, Number 3 
Corston-Oliver, Simon H. 1998. Beyond 
string matching and cue phrases: 
Improving efficiency and coverage in 
discourse analysis. In Working Notes of the 
AAAI Spring Symposium on Intelligent Text 
Summarization, pages 9-15, Stanford, 
March. 
Corston-Oliver, Simon H. and William B. 
Dolan. 1999. Less is more: Eliminating 
index terms from subordinate clauses. In 
Proceedings ofthe 37th Annual Meeting, 
pages 349-356, University of Maryland, 
June. Association for Computational 
Linguistics. 
Cristea, Dan, Nancy Ide, Daniel Marcu, and 
Valentin Tablan. 1999. Discourse structure 
and coreference: An empirical study. In 
Proceedings ofthe ACL'99 Workshop on the 
Relationship Between Discourse~Dialogue 
Structure and Reference, pages 46-53, 
University of Maryland, June. 
Cristea, Dan and Bonnie L. Webber. 1997. 
Expectations in incremental discourse 
processing. In Proceedings ofthe 35th 
Annual Meeting of the Association for 
Computational Linguistics (ACL/EACL-97), 
pages 88-95, Madrid, Spain, July. 
Crystal, David. 1991. A Dictionary of 
Linguistics and Phonetics. Third edition. 
Basil Blackwell, Oxford. 
Cumming, Carmen and Catherine 
McKercher. 1994. The Canadian Reporter: 
News Writing and Reporting. Harcourt 
Brace. 
Di Eugenio, Barbara, Johanna D. Moore, and 
Massimo Paolucci. 1997. Learning features 
that predict cue usage. In Proceedings ofthe 
35th Annual Meeting of the Association for 
Computational Linguistics (ACL/EACL-97), 
pages 80-87, Madrid, Spain, July. 
Fraser, Bruce. 1996. Pragmatic markers. 
Pragmatics, 6(2):167-190. 
Gardent, Claire. 1997. Discourse TAG. 
Technical Report CLAUS-Report Nr. 89, 
Universit/it des Saarlandes, Saarbr~icken, 
April. 
Grosz, Barbara J., Aravind K. Joshi, and 
Scott Weinstein. 1995. Centering: A
framework for modeling the local 
coherence of discourse. Computational 
Linguistics, 21(2):203-226. 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intentions, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Grote, Brigitte, Nils Lenke, and Manfred 
Stede. 1997. Ma(r)king concessions in
English and German. Discourse Processes, 
24:87-117. 
Halliday, Michael A. K. and Ruqaiya Hasan. 
1976. Cohesion in English. Longman. 
Harabagiu, Sanda and Steven Maiorano. 
1999. Knowledge-lean coreference 
resolution and its relation to textual 
cohesion and coreference. In Proceedings of
the ACL'99 Workshop on Discourse/Dialogue 
Structure and Reference, pages 29-38, 
University of Maryland, June. 
Harabagiu, Sanda M. and Dan I. Moldovan. 
1996. Textnet--A text-based intelligent 
system. In Working Notes of the AAAI Fall 
Symposium on Knowledge Representation 
Systems Based on Natural Language, pages 
32-43, Cambridge, MA. 
Hearst, Marti A. 1997. TextTiling: 
Segmenting text into multi-paragraph 
subtopic passages. Computational 
Linguistics, 23(1):33-64. 
Heurley, Laurent. 1997. Processing units in 
written texts: Paragraphs or information 
blocks. In Jean Costermans and Michel 
Fayol, editors, Processing Interclausal 
Relationships. Studies in the Production and 
Comprehension f Text. Lawrence Erlbaum 
Associates, pages 179-200. 
Hirschberg, Julia and Diane Litman. 1993. 
Empirical studies on the disambiguation 
of cue phrases. Computational Linguistics, 
19(3):501-530. 
Hirschman, Lynette, Marc Light, Eric Breck, 
and John D. Burger. 1999. Deep read: A 
reading comprehension system. In 
Proceedings ofthe 37th Annual Meeting, 
pages 325-332, University of Maryland, 
June. Association for Computational 
Linguistics. 
Hobbs, Jerry R. 1990. Literature and 
Cognition. CSLI Lecture Notes Number 21. 
Hobbs, Jerry R., Mark Stickel, Douglas 
Appelt, and Paul Martin. 1993. 
Interpretation asabduction. Artificial 
Intelligence, 63:69-142. 
Hoey, Michael. 1991. Patterns of Lexis in Text. 
Oxford University Press. 
Jing, Hongyan, Regina Barzilay, Kathleen 
McKeown, and Michael Elhadad. 1998. 
Summarization evaluation methods: 
Experiments and analysis. In Proceedings 
of the AAAI-98 Spring Symposium on 
Intelligent Text Summarization, pages 60-68, 
Stanford, March. 
Kameyama, Megumi. 1994. Indefeasible 
semantics and defeasible pragmatics. 
Technical Note 544, SRI International. A 
shorter version to appear in Kanazawa 
Makoto, Christopher Pinon, and Henriette 
de Swart, editors, Quantifiers, Deduction, 
and Context. CSLI, Stanford. 
Kamp, Hans. 1981. A theory of truth and 
semantic interpretation. I  J. A. G. 
Groenendijk, T. M. V. Janssen, and M. B. J. 
Stokhof, editors, Formal Methods in the 
446 
Marcu Rhetorical Parsing of Unrestricted Texts 
Study of Language, Mathematical Centre 
Tracts 135. Mathematisch Centrum, 
Amsterdam, pages 277-322. 
Kamp, Hans and Uwe Reyle. 1993. From 
Discourse to Logic: Introduction to 
ModelTheoretic Semantics of Natural 
Language, Formal Logic and Discourse 
Representation Theory. Kluwer Academic 
Publishers, London, Boston, Dordrecht. 
Studies in Linguistics and Philosophy, 
Volume 42. 
Kintsch, Walter. 1977. On comprehending 
stories. In Marcel Just and Patricia 
Carpenter, editors, Cognitive Processes in
Comprehension. Lawrence Erlbaum 
Associates, Hillsdale, NJ. 
Knott, Alistair. 1995. A Data-Driven 
Methodology for Motivating aSet of Coherence 
Relations. Ph.D. thesis, University of 
Edinburgh. 
Kurohashi, Sadao and Makoto Nagao. 1994. 
Automatic detection of discourse 
structure by checking surface information 
in sentences. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING-94), volume 2, pages 
1,123-1,127, Kyoto, Japan, August. 
Lascarides, Alex and Nicholas Ashen 1993. 
Temporal interpretation, discourse 
relations, and common sense entailment. 
Linguistics and Philosophy, 16(5):437-493. 
Litman, Diane J. 1996. Cue phrase 
classification using machine learning. 
Journal of Artificial Intelligence Research, 
5:53-94. 
Mann, William C. and Sandra A. 
Thompson. 1988. Rhetorical structure 
theory: Toward a functional theory of text 
organization. Text, 8(3):243-281. 
Marcu, Daniel. 1996. Building up rhetorical 
structure trees. In Proceedings ofthe 
Thirteenth National Conference on Artificial 
Intelligence (AAAI-96), volume 2, pages 
1,069-1,074, Portland, OR, August. 
Marcu, Daniel. 1997a. The rhetorical parsing 
of natural anguage texts. In Proceedings of
the 35th Annual Meeting of the Association for 
Computational Linguistics (ACL-97), pages 
96--103, Madrid, Spain, July. 
Marcu, Daniel. 1997b. The Rhetorical Parsing, 
Summarization, and Generation of Natural 
Language Texts. Ph.D. thesis, Department 
of Computer Science, University of 
Toronto, December. 
Marcu, Daniel. 1999a. A decision-based 
approach to rhetorical parsing. In 
Proceedings ofthe 37th Annual Meeting, 
pages 365-372, University of Maryland, 
June. Association for Computational 
Linguistics. 
Marcu, Daniel. 1999b. Discourse trees are 
good indicators of importance in text. In 
Inderjeet Mani and Mark Maybury, 
editors, Advances in Automatic Text 
Summarization. MIT Press, Cambridge, 
MA, pages 123-136. 
Marcu, Daniel. 2000. The Theory and Practice 
of Discourse Parsing and Summarization. 
MIT Press, Cambridge, MA. To appear. 
Marcu, Daniel, Lynn Carlson, and Maki 
Watanabe. 2000. The automatic translation 
of discourse structures. In Proceedings of
the Language Technology Joint Conference 
ANLP-NAACL2000, Seattle, WA. 
Martin, James R. 1992. English Text. System 
and Structure. John Benjamin Publishing 
Company, Philadelphia, Amsterdam. 
Morris, Jane and Graeme Hirst. 1991. 
Lexical cohesion computed by thesaural 
relations as an indicator of the structure of 
text. Computational Linguistics, 17(1):21-48. 
Moser, Megan and Johanna D. Moore. 1997. 
On the correlation of cues with discourse 
structure: Results from a corpus study. 
Forthcoming. 
Nunberg, G. 1990. The Linguistics of 
Punctuation. CSLI Lecture Notes 18, 
Stanford. University of Chicago Press. 
Palmer, David D. and Marti A. Hearst. 1997. 
Adaptive multilingual sentence boundary 
disambiguation. Computational Linguistics, 
23(2):241-269. 
Pascual, Elsa and Jacques Virbel. 1996. 
Semantic and layout properties of text 
punctuation. In Proceedings ofthe 
Association for Computational Linguistics 
Workshop on Punctuation, pages 41-48, 
Santa Cruz, CA, June. 
Polanyi, Livia. 1988. A formal model of the 
structure of discourse. Journal of 
Pragmatics, 12:601-638. 
Polanyi, Livia. 1996. The linguistic structure 
of discourse. Technical Report 
CSLI-96-200, Center for the Study of 
Language and Information. 
Polanyi, Livia and Martin H. van den Berg. 
1996. Discourse structure and discourse 
interpretation. In P. Dekker and 
M. Stokhof, editors, Proceedings ofthe Tenth 
Amsterdam Colloquium, pages 113-131. 
Department of Philosophy, University of 
Amsterdam. 
Redeker, Gisela. 1990. Ideational and 
pragmatic markers of discourse structure. 
Journal of Pragmatics, 14:367-381. 
Salton, Gerard and James Allan. 1995. 
Selective text utilization and text 
traversal. International Journal of 
Human-Computer Studies, 43:483-497. 
Salton, Gerard, Amit Singhal, Chris Buckley, 
and Mandar Mitra. 1995. Automatic text 
decomposition using text segments and 
447 
Computational Linguistics Volume 26, Number 3 
text themes. Technical Report TR-95-1555, 
Department of Computer Science, Cornell 
University. 
Say, Bilge and Varol Akman. 1996. 
Information-based aspects of punctuation. 
In Proceedings ofthe Association for 
Computational Linguistics Workshop on 
Punctuation, pages 49-56, Santa Cruz, CA, 
June. 
Schiffrin, Deborah. 1987. Discourse Markers. 
Cambridge University Press. 
Schilder, Frank. 1997. Tree discourse 
grammar, or how to get attached a
discourse. In Proceedings ofthe Second 
International Workshop on Computational 
Semantics (IWCS-II), pages 261-273, 
Tilburg, The Netherlands, January. 
Schneuwly, Bernard. 1997. Textual 
organizers and text types: Ontogenetic 
aspects in writing. In Jean Costermans 
and Michel Fayol, editors, Processing 
Interclausal Relationships. Studies in the 
Production and Comprehension f Text. 
Lawrence Erlbaum Associates, Hillsdale, 
NJ, pages 245-263. 
Segal, Erwin M. and Judith F. Duchan. 1997. 
Interclausal connectives as indicators of 
structuring in narrative. In Jean 
Costermans and Michel Fayol, editors, 
Processing Interclausal Relationships. Studies 
in the Production and Comprehension f Text. 
Lawrence Erlbaum Associates, Hillsdale, 
NJ, pages 95-119. 
Segal, Erwin M., Judith F. Duchan, and 
Paula J. Scott. 1991. The role of 
interclausal connectives in narrative 
structuring: Evidence from adults' 
interpretations of simple stories. Discourse 
Processes, 14:27-54. 
Shiuan, Peh Li and Christopher Ting Hian 
Ann. 1996. A divide-and-conquer st ategy 
for parsing. In Proceedings ofthe Association 
for Computational Linguistics Workshop on 
Punctuation, pages 57-66, Santa Cruz, CA, 
June. 
Siegel, Eric V. and Kathleen R. McKeown. 
1994. Emergent linguistic rules from 
inducing decision trees: Disambiguating 
discourse clue words. In Proceedings ofthe 
Twelfth National Conference on Artificial 
Intelligence (AAAI-94), volume 1, pages 
820-826, Seattle, WA. 
Strube, Michael and Udo Hahn. 1999. 
Functional centering---Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Sumita, K., K. Ono, T. Chino, T. Ukita, and 
S. Amano. 1992. A discourse structure 
analyzer for Japanese text. In Proceedings 
of the International Conference on Fifth 
Generation Computer Systems, volume 2, 
pages 1,133-1,140. 
van den Berg, Martin H. 1996. Discourse 
grammar and dynamic logic. In P. Dekker 
and M. Stokhof, editors, Proceedings ofthe 
Tenth Amsterdam Colloquium, pages 93-112. 
Department of Philosophy, University of 
Amsterdam. 
van Dijk, Teun A. 1972. Some Aspects of Text 
Grammars; A Study in Theoretical Linguistics 
and Poetics. Mouton, The Hague. 
Webber, Bonnie, Alistair Knott, Matthew 
Stone, and Aravind Joshi. 1999. Discourse 
relations: A structural and 
presuppositional account using 
lexicalized TAG. In Proceedings ofthe 37th 
Annual Meeting, pages 41-48, University 
of Maryland, June. Association for 
Computational Linguistics. 
Wiebe, Janice M. 1994. Tracking point of 
view in narrative. Computational 
Linguistics, 20(2):233-288. 
Youmans, Gilbert. 1991. A new tool for 
discourse analysis: The 
vocabulary-management profile. 
Language, 67(4):763-789. 
Zock, Michael. 1985. Le fil d'ariane ou les 
grammaires de texte comme guide dans 
l'organisation etl'expression de la pens~e 
en langue maternelle t/ou 6trang6re. 
Technical Report, Rapport pour l'Unesco, 
Juin. 
448 
Improving Machine Translation Performance
by Exploiting Non-Parallel Corpora
Dragos Stefan Munteanu?
Information Sciences Institute
University of Southern California
Daniel Marcu?
Information Sciences Institute
University of Southern California
We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.
We train a maximum entropy classifier that, given a pair of sentences, can reliably determine
whether or not they are translations of each other. Using this approach, we extract parallel data
from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality
of the extracted data by showing that it improves the performance of a state-of-the-art statistical
machine translation system. We also show that a good-quality MT system can be built from
scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large
non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for
which only scarce resources are available.
1. Introduction
Parallel texts?texts that are translations of each other?are an important resource in
many NLP applications. They provide indispensable training data for statistical ma-
chine translation (Brown et al 1990; Och and Ney 2002) and have been found useful in
research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), cross-
language information retrieval (Davis and Dunning 1995; Oard 1997), and annotation
projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wi-
centowski 2001).
Unfortunately, parallel texts are also scarce resources: limited in size, language
coverage, and language register. There are relatively few language pairs for which
parallel corpora of reasonable sizes are available; and even for those pairs, the corpora
come mostly from one domain, that of political discourse (proceedings of the Canadian
or European Parliament, or of the United Nations). This is especially problematic for
the field of statistical machine translation (SMT), because translation systems trained
on data from a particular domain (e.g., parliamentary proceedings) will perform poorly
when translating texts from a different domain (e.g., news articles).
One way to alleviate this lack of parallel data is to exploit a much more available
and diverse resource: comparable non-parallel corpora. Comparable corpora are texts
that, while not parallel in the strict sense, are somewhat related and convey overlap-
ping information. Good examples are the multilingual news feeds produced by news
agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc. Such
texts are widely available on the Web for many language pairs and domains. They often
? 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292. E-mail: {dragos,marcu}@isi.edu.
Submission received: 5 November 2004; Accepted for publication: 3 March 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
contain many sentence pairs that are fairly good translations of each other. The ability
to reliably identify these pairs would enable the automatic creation of large and diverse
parallel corpora.
However, identifying good translations in comparable corpora is hard. Even texts
that convey the same information will exhibit great differences at the sentence level.
Consider the two newspaper articles in Figure 1. They have been published by the
English and French editors of Agence France Presse, and report on the same event, an
epidemic of cholera in Pyongyang. The lines in the figure connect sentence pairs
that are approximate translations of each other. Discovering these links automatically
is clearly non-trivial. Traditional sentence alignment algorithms (Gale and Church
1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to
align sentences in parallel corpora and operate on the assumption that there are no
reorderings and only limited insertions and deletions between the two renderings of
a parallel document. Thus, they perform poorly on comparable, non-parallel texts.
What we need are methods able to judge sentence pairs in isolation, independent of the
(potentially misleading) context.
This article describes a method for identifying parallel sentences in comparable
corpora and builds on our earlier work on parallel sentence extraction (Munteanu,
Fraser, and Marcu 2004). We describe how to build a maximum entropy-based classifier
that can reliably judge whether two sentences are translations of each other, without
making use of any context. Using this classifier, we extract parallel sentences from very
large comparable corpora of newspaper articles. We demonstrate the quality of our
Figure 1
A pair of comparable texts.
478
Munteanu and Marcu Exploiting Non-Parallel Corpora
extracted sentences by showing that adding them to the training data of an SMT system
improves the system?s performance. We also show that language pairs for which very
little parallel data is available are likely to benefit the most from our method; by running
our extraction system on a large comparable corpus in a bootstrapping manner, we can
obtain performance improvements of more than 50% over a baseline MT system trained
only on existing parallel data.
Our main experimental framework is designed to address the commonly en-
countered situation that exists when the MT training and test data come from dif-
ferent domains. In such a situation, the test data is in-domain, and the training data
is out-of-domain. The problem is that in such conditions, translation performance
is quite poor; the out-of-domain data doesn?t really help the system to produce
good translations. What is needed is additional in-domain training data. Our goal
is to get such data from a large in-domain comparable corpus and use it to im-
prove the performance of an out-of-domain MT system. We work in the context
of Arabic-English and Chinese-English statistical machine translation systems. Our
out-of-domain data comes from translated United Nations proceedings, and our in-
domain data consists of news articles. In this experimental framework we have ac-
cess to a variety of resources, all of which are available from the Linguistic Data
Consortium:1
 large amounts of out-of-domain parallel data;
 smaller amounts of in-domain parallel data;
 in-domain MT test corpora with four reference translations; and
 in-domain comparable corpora: large collections of Arabic, Chinese, and
English news articles from various news agencies.
In summary, we call in-domain the domain of the test data that we wish to trans-
late; in this article, that in-domain data consists of news articles. Out-of-domain data
is data that belongs to any other domain; in this article, the out-of-domain data is
drawn from United Nations (UN) parliamentary proceedings. We are interested in
the situation that exists when we need to translate news data but only have UN
data available for training. The solution we propose is to get comparable news data,
automatically extract parallel sentences from it, and use these sentences as additional
training data; we will show that doing this improves translation performance on
a news test set. The Arabic-English and Chinese-English resources described in the
previous paragraph enable us to simulate our conditions of interest and perform de-
tailed measurements of the impact of our proposed solution. We can train baseline
systems on UN parallel data (using the data from the first bullet in the previous
paragraph), extract additional news data from the large comparable corpora (the fourth
bullet), accurately measure translation performance on news data against four ref-
erence translations (the third bullet), and compare the impact of the automatically
extracted news data with that of similar amounts of human-translated news data
(the second bullet).
In the next section, we give a high-level overview of our parallel sentence extrac-
tion system. In Section 3, we describe in detail the core of the system, the parallel sen-
1 http://www.ldc.upenn.edu.
479
Computational Linguistics Volume 31, Number 4
Figure 2
A Parallel Sentence Extraction System.
tence classifier. In Section 4, we discuss several data extraction experiments. In Section 5,
we evaluate the extracted data by showing that adding it to out-of-domain parallel data
improves the in-domain performance of an out-of-domain MT system, and in Section 6,
we show that in certain cases, even larger improvements can be obtained by using boot-
strapping. In Section 7, we present examples of sentence pairs extracted by our method
and discuss some of its weaknesses. Before concluding, we discuss related work.
2. A System for Extracting Parallel Sentences from Comparable Corpora
The general architecture of our extraction system is presented in Figure 2. Starting with
two large monolingual corpora (a non-parallel corpus) divided into documents, we
begin by selecting pairs of similar documents (Section 2.1). From each such pair, we
generate all possible sentence pairs and pass them through a simple word-overlap-
based filter (Section 2.2), thus obtaining candidate sentence pairs. The candidates are
presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the
sentences in each pair are mutual translations of each other.
The resources required by the system are minimal: a bilingual dictionary and a small
amount of parallel data (used for training the ME classifier). The dictionaries used in
our experiments are learned automatically from (out-of-domain) parallel corpora;2
thus, the only resource used by our system consists of parallel sentences.
2 If such a resource is unavailable, other dictionaries can be used.
480
Munteanu and Marcu Exploiting Non-Parallel Corpora
2.1 Article Selection
Our comparable corpus consists of two large, non-parallel, news corpora, one in English
and the other in the foreign language of interest (in our case, Chinese or Arabic). The
parallel sentence extraction process begins by selecting, for each foreign article, English
articles that are likely to contain sentences that are parallel to those in the foreign one.
This step of the process emphasizes recall rather than precision. For each foreign
document, we do not attempt to find the best-matching English document, but rather a
set of similar English documents. The subsequent components of the system are robust
enough to filter out the extra noise introduced by the selection of additional (possibly
bad) English documents.
We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan
2001). We first index all the English documents into a database. For each foreign docu-
ment, we take the top five translations of each of its words (according to our probabilis-
tic dictionary) and create an English language query. The translation probabilities are
only used to choose the word translations; they do not appear in the query. We use the
query to run TF-IDF retrieval against the database, take the top 20 English documents
returned by Lemur, and pair each of them with the foreign query document.
This document matching procedure is both slow (it looks at all possible document
pairs, so it is quadratic in the number of documents) and imprecise (due to noise in
the dictionary, the query will contain many wrong words). We attempt to fix these
problems by using the following heuristic: we consider it likely that articles with simi-
lar content have publication dates that are close to each other. Thus, each query is
actually run only against English documents published within a window of five days
around the publication date of the foreign query document; we retrieve the best 20
of these documents. Each query is thus run against fewer documents, so it becomes
faster and has a better chance of getting the right documents at the top.
Our experiments have shown that the final performance of the system does not
depend too much on the size of the window (for example, doubling the size to 10 days
made no difference). However, having no window at all leads to a decrease in the over-
all performance of the system.
2.2 Candidate Sentence Pair Selection
From each foreign document and set of associated English documents, we take all
possible sentence pairs and pass them through a word-overlap filter.
The filter verifies that the ratio of the lengths of the two sentences is no greater than
two. It then checks that at least half the words in each sentence have a translation in the
other sentence, according to the dictionary. Pairs that do not fulfill these two conditions
are discarded. The others are passed on to the parallel sentence selection stage.
This step removes most of the noise (i.e., pairs of non-parallel sentences) introduced
by our recall-oriented document selection procedure. It also removes good pairs that
fail to pass the filter because the dictionary does not contain the necessary entries; but
those pairs could not have been handled reliably anyway, so the overall effect of the
filter is to improve the precision and robustness of the system. However, the filter also
accepts many wrong pairs, because the word-overlap condition is weak; for instance,
stopwords almost always have a translation on the other side, so if a few of the content
3 http://www-2.cs.cmu.edu/?lemur.
481
Computational Linguistics Volume 31, Number 4
words happen to match, the overlap threshold is fulfilled and an erroneous candidate
sentence pair is selected.
2.3 Parallel Sentence Selection
For each candidate sentence pair, we need a reliable way of deciding whether the two
sentences in the pair are mutual translations. This is achieved by a Maximum Entropy
(ME) classifier (described at length in Section 3), which is the core component of our
system. Those pairs that are classified as being translations of each other constitute the
output of the system.
3. A Maximum Entropy Classifier for Parallel Sentence Identification
In the Maximum Entropy (ME) statistical modeling framework, we impose constraints
on the model of our data by defining a set of feature functions. These feature functions
emphasize properties of the data that we believe to be useful for the modeling task. For
example, for a sentence pair sp, the word overlap (the percentage of words in either
sentence that have a translation in the other) might be a useful indicator of whether the
sentences are parallel. We therefore define a feature function f (sp), whose value is the
word overlap of the sentences in sp.
According to the ME principle, the optimal parametric form of the model of our
data, taking into account the constraints imposed by the feature functions, is a log linear
combination of these functions. Thus, for our classification problem, we have:
P(ci|sp) = 1Z(sp)
k
?
j=1
?
fij(c,sp)
j
where ci is the class (c0=?parallel?, c1=?not parallel?), Z(sp) is a normalization factor, and
fij are the feature functions (indexed both by class and by feature). The resulting model
has free parameters ?j, the feature weights. The parameter values that maximize the
likelihood of a given training corpus can be computed using various optimization
algorithms (see [Malouf 2002] for a comparison of such algorithms).
3.1 Features for Parallel Sentence Identification
For our particular classification problem, we need to find feature functions that dis-
tinguish between parallel and non-parallel sentence pairs. For this purpose, we com-
pute and exploit word-level alignments between the sentences in each pair. A word
alignment between two sentences in different languages specifies which words in one
sentence are translations of which words in the other. Word alignments were first intro-
duced in the context of statistical MT, where they are used to estimate the parameters
of a translation model (Brown et al 1990). Since then, they were found useful in many
other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question
answering [Echihabi and Marcu 2003]).
Figures 3 and 4 give examples of word alignments between two English-Arabic
sentence pairs from our comparable corpus. Each figure contains two alignments. The
one on the left is a correct alignment, produced by a human, while the one on the right
482
Munteanu and Marcu Exploiting Non-Parallel Corpora
Figure 3
Alignments between two parallel sentences.
was computed automatically. As can be seen from the gloss next to the Arabic words,
the sentences in Figure 3 are parallel while the sentences in Figure 4 are not.
In a correct alignment between two non-parallel sentences, most words would
have no translation equivalents; in contrast, in an alignment between parallel sentences,
most words would be aligned. Automatically computed alignments, however, may
have incorrect connections; for example, on the right side of Figure 3, the Arabic word
issue is connected to the comma; and in Figure 4, the Arabic word at is connected to
the English phrase its case to the. Such errors are due to noisy dictionary entries and to
Figure 4
Alignments between two non-parallel sentences.
483
Computational Linguistics Volume 31, Number 4
shortcomings of the model used to generate the alignments. Thus, merely looking at
the number of unconnected words, while helpful, is not discriminative enough. Still,
automatically produced alignments have certain additional characteristics that can be
exploited.
We follow Brown et al (1993) in defining the fertility of a word in an alignment
as the number of words it is connected to. The presence, in an automatically computed
alignment between a pair of sentences, of words of high fertility (such as the Arabic
word at in Figure 4) is indicative of non-parallelism. Most likely, these connections
were produced because of a lack of better alternatives.
Another aspect of interest is the presence of long contiguous connected spans,
which we define as pairs of bilingual substrings in which the words in one substring are
connected only to words in the other substring. Such a span may contain a few words
without any connection (a small percentage of the length of the span), but no word
with a connection outside the span. Examples of such spans can be seen in Figure 3:
the English strings after saudi mediation failed or to the international court of justice together
with their Arabic counterparts. Long contiguous connected spans are indicative of
parallelism, since they suggest that the two sentences have long phrases in common.
And, in contrast, long substrings whose words are all unconnected are indicative of
non-parallelism.
To summarize, our classifier uses the following features, defined over two sen-
tences and an automatically computed alignment between them.
General features (independent of the word alignment):
 lengths of the sentences, as well as the length difference and length ratio;
 percentage of words on each side that have a translation on the other side
(according to the dictionary).
Alignment features:
 percentage and number of words that have no connection;
 the top three largest fertilities;
 length of the longest contiguous connected span; and
 length of the longest unconnected substring.
3.2 Word Alignment Model
In order to compute word alignments we need a simple and efficient model. We want to
align a large number of sentences, with many out-of-vocabulary words, in reasonable
time. We also want a model with as few parameters as possible?preferably only word-
for-word translation probabilities.
One such model is the IBM Model 1 (Brown et al 1993). According to this model,
given foreign sentence ( fj1<=j<=m), English sentence (ei1<=i<=l), and translation prob-
abilities t( fj|ei), the best alignment f ? e is obtained by linking each foreign word fj to
its most likely English translation argmaxei t( fj|ei). Thus, each foreign word is aligned to
exactly one English word (or to a special NULL token).
Due to its simplicity, this model has several shortcomings, some more structural
than others (see Moore [2004] for a discussion). Thus, we use a version that is aug-
mented with two simple heuristics that attempt to alleviate some of these shortcomings.
484
Munteanu and Marcu Exploiting Non-Parallel Corpora
One possible improvement concerns English words that appear more than once
in a sentence. According to the model, a foreign word that prefers to be aligned with
such an English word could be equally well aligned with any instance of that word. In
such situations, instead of arbitrarily choosing the first instance or a random instance,
we attempt to make a ?smarter? decision. First, we create links only for those English
words that appear exactly once; next, for words that appear more than once, we choose
which instance to link with so that we minimize the number of crossings with already
existing links.
The second heuristic attempts to improve the choice of the most likely English
translation of a foreign word. Our translation probabilities are automatically learned
from parallel data, and we learn values for both t( fj|ei) and t(ei| fj). We can therefore
decide that the most likely English translation of fj is argmaxei{t( fj|ei), t(ei| fj)}. Using
both sets of probabilities is likely to help us make a better-informed decision.
Using this alignment strategy, we follow (Och and Ney 2003) and compute one
alignment for each translation direction ( f ? e and e ? f ), and then combine them. Och
and Ney present three combination methods: intersection, union, and refined (a form of
intersection expanded with certain additional neighboring links).
Thus, for each sentence pair, we compute five alignments (two modified-IBM-
Model-1 plus three combinations) and then extract one set of general features and five
sets of alignment features (as described in the previous section).
3.3 Training and Testing
We create training instances for our classifier from a small parallel corpus. The sim-
plest way to obtain classifier training data from a parallel corpus is to generate all
possible sentence pairs from the corpus (the Cartesian product). This generates 5,0002
training instances, out of which 5,000 are positive (i.e., belong to class ?parallel?) and
the rest are negative.
One drawback of this approach is that the resulting training set is very imbalanced,
i.e., it has many more negative examples than positive ones. Classifiers trained on such
data do not achieve good performance; they generally tend to predict the majority
class, i.e., classify most sentences as non-parallel (which has indeed been the case in
our experiments). Our solution to this is to downsample, i.e., eliminate a number of
(randomly selected) negative instances.
Another problem is that the large majority of sentence pairs in the Cartesian prod-
uct have low word overlap (i.e., few words that are translations of each other). As
explained in Section 2 (and shown in Figure 2), when extracting data from a compara-
ble corpus, we only apply the classifier on the output of the word-overlap filter. Thus,
low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be
useful as training examples. We therefore use for training only those pairs from the
Cartesian product that are accepted by the word-overlap filter. This has the additional
advantage that, since all these pairs have many words in common, the classifier learns
to make distinctions that cannot be made based on word overlap alone.
To summarize, we prepare our classifier training set in the following manner: start-
ing from a parallel corpus of about 5,000 sentence pairs, we generate all the sentence
pairs in the Cartesian product; we discard the pairs that do not fulfill the conditions of
the word-overlap filter; if the resulting set is imbalanced, i.e., the ratio of non-parallel
to parallel pairs is greater than five, we balance it by removing randomly chosen non-
parallel pairs. We then compute word alignments and extract feature values.
485
Computational Linguistics Volume 31, Number 4
Using the training set, we compute values for the classifier feature weights using
the YASMET4 implementation of the GIS algorithm (Darroch and Ratcliff 1974). Since
we are dealing with few parameters and have sufficiently many training instances,
using more advanced training algorithms is unlikely to bring significant improvements.
We test the performance of the classifier by generating test instances from a differ-
ent parallel corpus (also around 5,000 sentence pairs) and checking how many of these
instances are correctly classified. We prepare the test set by creating the Cartesian
product of the sentences in the test parallel corpus and applying the word-overlap
filter (we do not perform any balancing). Although we apply the filter, we still concep-
tually classify all pairs from the Cartesian product in a two-stage classification process:
all pairs discarded by the filter are classified as ?non-parallel,? and for the rest, we obtain
predictions from the classifier. Since this is how we apply the system on truly unseen
data, this is the process in whose performance we are interested.
We measure the performance of the classification process by computing precision
and recall. Precision is the ratio of sentence pairs correctly judged as parallel to the
total number of pairs judged as parallel by the classifier. Recall is the ratio of sentence
pairs correctly identified as parallel by the classifier to the total number of truly parallel
pairs?i.e., the number of pairs in the parallel corpus used to generate the test instances.
Both numbers are expressed as percentages. More formally: let classified parallel be the
total number of sentence pairs from our test set that the classifier judged as parallel,
classified well be the number of pairs that the classifier correctly judged as parallel, and
true parallel be the total number of parallel pairs in the test set. Then:
precision = 100 ? classified well
classified parallel
recall = 100 ? classified well
true parallel
3.4 Performance Evaluation
There are two factors that influence a classifier?s performance: dictionary coverage
and similarity between the domains of the training and test instances. We performed
evaluation experiments to account for both these factors.
All our dictionaries are automatically learned from parallel data; thus, we can cre-
ate dictionaries of various coverage by learning them from parallel corpora of different
sizes. We use five dictionaries, learned from five initial out-of-domain parallel corpora,
whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English
side.
Since we want to use the classifier to extract sentence pairs from our in-domain
comparable corpus, we test it on instances generated from an in-domain parallel cor-
pus. In order to measure the effect of the domain difference, we use two training sets:
one generated from an in-domain parallel corpus and another one from an out-of-
domain parallel corpus.
In summary, for each language pair, we use the following corpora:
 five initial out-of-domain corpora of various sizes, used for learning
dictionaries;
 one out-of-domain classifier training corpus;
4 http://www.fjoch.com/YASMET.html.
486
Munteanu and Marcu Exploiting Non-Parallel Corpora
Figure 5
Precision and recall of the Arabic-English classifiers.
 one in-domain classifier training corpus; and
 one in-domain classifier test corpus.
From each initial, out-of-domain corpus, we learn a dictionary. We then take the
classifier training and test corpora and, using the method described in the previous
section, create two sets of training instances and one set of test instances. We train two
classifiers (one on each training set) and evaluate both of them on the test set.
The parallel corpora used for generating training and test instances have around
5k sentence pairs each (approximately 150k English tokens), and generate around 10k
training instances (for each training set) and 8k test instances.
Figure 6
Precision and recall of the Chinese-English classifiers.
487
Computational Linguistics Volume 31, Number 4
Figures 5 and 6 show the recall and precision of our classifiers, for both Arabic-
English and Chinese-English. The results show that the precision of our classification
process is robust with respect to dictionary coverage and training domain. Even when
starting from a very small initial parallel corpus, we can build a high-precision classifier.
Having a good dictionary and training data from the right domain does help though,
mainly with respect to recall.
The classifiers achieve high precision because their positive training examples are
clean parallel sentence pairs, with high word overlap (since the pairs with low overlap
are filtered out); thus, the classification decision frontier is pushed towards ?good-
looking? alignments. The low recall results are partly due to the word-overlap filter
(the first stage of the classification process), which discards many parallel pairs. If we
don?t apply the filter before the classifier, the recall results increase by about 20% (with
no loss in precision). However, the filter plays a very important role in keeping the
extraction pipeline robust and efficient (as shown in Figure 7, the filter discards 99% of
the candidate pairs), so this loss of recall is a price worth paying.
Classifier evaluations using different subsets of features show that most of the
classifier performance comes from the general features together with the alignment
features concerning the percentage and number of words that have no connection.
However, we expect that in real data, the differences between parallel and non-parallel
pairs are less clear than in our test data (see the discussion in Section 7) and can no
Figure 7
The amounts of data processed by our system during extraction from the Chinese-English
comparable corpus.
488
Munteanu and Marcu Exploiting Non-Parallel Corpora
Table 1
The Gigaword comparable corpora.
Foreign English
Language pair News agency and period # articles # tokens # articles # tokens
Arabic-English AFP, 1994?1997, 2002 224k 40M 650k 195M
Xinhua News, 2001
Chinese-English Xinhua News, 1995?2001 457k 162M 580k 128M
longer be accounted for only by counting the linked words; thus, the other features
should become more important.
4. Data Extraction Experiments
4.1 Controlled Experiments
The comparable corpora that we use for parallel sentence extraction are collections of
news stories published by the Agence France Presse and Xinhua News agencies. They
are parts of the Arabic, English, and Chinese Gigaword corpora which are available
from the Linguistic Data Consortium. From these collections, for each language pair,
we create an in-domain comparable corpus by putting together articles coming from
the same agency and the same time period. Table 1 presents in detail the sources and
sizes of the resulting comparable corpora. The remainder of the section presents the
various data sets that we extracted automatically from these corpora, under various
experimental conditions.
In the experiments described in Section 3.4, we started out with five out-of-domain
initial parallel corpora of various sizes and obtained five dictionaries and five out-of-
domain trained classifiers (per language pair). We now plug in each of these classifiers
(and their associated dictionaries) in our extraction system (Section 2) and apply it to
our comparable corpora. We thus obtain five Arabic-English and five Chinese-English
extracted corpora.
Note that in each of these experiments the only resource used by our system
is the initial, out-of-domain parallel corpus. Thus, the experiments fit in the frame-
work of interest described in Section 1, which assumes the availability of (limited
amounts of) out-of-domain training data and (large amounts of) in-domain comparable
data.
Table 2 shows the sizes of the extracted corpora for each initial corpus size, for
both Chinese-English and Arabic-English. As can be seen, when the initial parallel
corpus is very small, the amount of extracted data is also quite small. This is due to the
low coverage of the dictionary learned from that corpus. Our candidate pair selection
step (Section 2.2) discards pairs with too many unknown (or unrelated) words, accord-
ing to the dictionary; thus, only few sentences fulfill the word-overlap condition of
our filter.
As mentioned in Section 1, our goal is to use the extracted data as additional
MT training data and obtain better translation performance on a given in-domain MT
test set. A simple way of estimating the usefulness of the data for this purpose is to
measure its coverage of the test set, i.e., the percentage of running n-grams from the
test corpus that are also in our corpus. Tables 3 and 4 present the coverage of our
489
Computational Linguistics Volume 31, Number 4
Table 2
Size of the datasets extracted from the comparable corpora, in millions of English words.
Size of automatically extracted corpora
Size of initial parallel corpus Arabic-English Chinese-English
100k 0.09M 0.9M
1M 0.6M 5M
10M 1.9M 8.3M
50M 2.2M 10.5M
95M 2.1M 10.5M
Table 3
Coverage of the extracted corpora for Arabic-English.
Out-of-domain In-domain
Initial corpus size Initial Initial plus extracted
100k 68/16/3/0.5 82/31/8/2
1M 86/33/7/1 94/54/20/7
10M 95/51/16/3 98/67/30/12
50M 98/64/24/6 99/74/36/14
95M 98/68/28/8 99/76/38/15
Table 4
Coverage of the extracted corpora for Chinese-English.
Out-of-domain In-domain
Initial corpus size Initial Initial plus extracted
100k 75/19/2/0.2 91/41/11/3
1M 90/38/8/1 97/61/22/7
10M 97/57/18/4 99/70/29/10
50M 98/69/27/7 99/76/36/12
95M 99/73/32/9 99/78/39/14
extracted corpora. For each initial corpus size, the first column shows the coverage
of that initial corpus, and the second column shows the coverage of the initial corpus
plus the extracted corpus. Each cell contains four numbers that represent the coverage
with respect to unigrams, bigrams, trigrams, and 4-grams. The numbers show that
unigram coverage depends only on the size of the corpus (and not on the domain), but
for longer n-grams, our in-domain extracted data brings significant improvements in
coverage.
4.2 Non-Controlled Experiments Using Web-Based Non-Parallel Corpora
The extraction experiments from the previous section are controlled experiments in
which we only use limited amounts of parallel data for our extraction system. In this
490
Munteanu and Marcu Exploiting Non-Parallel Corpora
section, we describe experiments in which the goal is to assess the applicability of our
method to data that we mined from the Web.
We obtained comparable corpora from the Web by going to bilingual news web-
sites (such as Al-Jazeera) and downloading news articles in each language indepen-
dently. In order to get as many articles as possible, we used the web site?s search engine
to get lists of articles and their URLs, and then crawled those lists. We used the Agent-
Builder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling. The
tool can be programmed to automatically initiate searches with different parameters
and to identify and extract the desired article URLs (as well as other information such
as dates and titles) from the result pages. Table 5 shows the sources, time periods, and
size of the datasets that we downloaded.
For the extraction experiments, we used dictionaries of high coverage, learned from
all our available parallel training data. The sizes of these training corpora, measured in
number of English tokens, are as follows:
 Arabic-English: 100M tokens out-of-domain data and 4.5M tokens
in-domain data
 Chinese-English: 150M tokens out-of-domain data and 40M tokens
in-domain data
We applied our extraction method on both the LDC-released Gigaword corpora
and the Web-downloaded comparable corpora. For each language pair, we used the
highest precision classifier from those presented in Section 3.4. In order to obtain data
of higher quality, we didn?t use all the sentences classified as parallel, but only those for
which the probability computed by our classifier was higher than 0.70. Table 6 shows
the amounts of extracted data, measured in number of English tokens. For Arabic-
English, we were able to extract from the Gigaword corpora much more data than in
our previous experiments (see Table 2), clearly due to the better dictionary. For Chinese-
English, there was no increase in the size of extracted data (although the amount from
Table 6 is smaller than that from Table 2, it counts only sentence pairs extracted with
confidence higher than 0.70).
In the previous section, we measured, for our training corpora, their coverage of
the test set (Tables 3 and 4). We repeated the measurements for the training data from
Table 6 and obtained very similar results: using the additional extracted data improves
coverage, especially for longer n-grams.
To give the reader an idea of the amount of data that is funneled through our
system, we show in Figure 7 the sizes of the data processed by each of the system?s
Table 5
Comparable corpora downloaded from the Web.
Foreign English
Language pair News agency and period # articles # tokens # articles # tokens
Arabic-English People?s Daily, 2001?2003 70k 38M 50k 20M
Al-Jazeera, 2003
Al-Hayat, 2003
Chinese-English Voice of America, 2001?2003 25k 13M 36k 19M
491
Computational Linguistics Volume 31, Number 4
Table 6
Size of the datasets extracted for the NIST 2004 MT evaluation.
Source Arabic-English Chinese-English
Gigaword 5.3M 7.2M
Web 1.4M 2.1M
Total 6.8M 9.3M
components during extraction from the Gigaword and Web-based Chinese-English
comparable corpora. We use a dictionary learned from a parallel corpus on 190M
English tokens and a classifier trained on instances generated from a parallel corpus
of 220k English tokens. We start with a comparable corpus consisting of 500k Chinese
articles and 600k English articles. The article selection step (Section 2.1) outputs 7.5M
similar article pairs; from each article pair we generate all possible sentence pairs and
obtain 2,400M pairs. Of these, less than 1% (17M) pass the candidate selection stage
(Section 2.2) and are presented to the ME classifier. The system outputs 430k sen-
tence pairs (9.5M English tokens) that have been classified as parallel (with probability
greater than 0.7).
The figure also presents, in the lower part, the parameters that control the filtering
at each stage.
 best K results: in the article selection stage (Section 2.1), for each foreign
article we only consider the top K most similar English ones. In our
experiments, K is set to 20.
 date window: when looking for possible article pairs, we only consider
English articles whose publication dates fall within a window of 5 days
around the publication date of the foreign one.
 word overlap: the word-overlap filter (Section 2.2) will discard sentence
pairs that have less than a certain proportion of words in common
(according to the bilingual dictionary). The value we use (expressed
as a percentage of sentence length) is 50.
 length ratio: similarly, the word-overlap filter will discard pairs whose
length ratio is greater than this value, which we set to 2.
 decision threshold: The ME classifier associates a probability with each
of its predictions. Values above 0.5 indicate that the classifier considers
the particular sentence pair to be parallel; the higher the value, the
higher the classifier?s confidence. Thus, in order to obtain higher
precision, we can choose to define as parallel only those pairs for
which the classifier probability is above a certain threshold. In the
experiments from Section 4.1, we use the (default) threshold of 0.5,
while in Section 4.2 we use 0.7.
5. Machine Translation Improvements
Our main goal is to extract, from an in-domain comparable corpus, parallel training
data that improves the performance of an out-of-domain-trained SMT system. Thus,
492
Munteanu and Marcu Exploiting Non-Parallel Corpora
we evaluate our extracted corpora by showing that adding them to the out-of-domain
training data of a baseline MT system improves its performance.
5.1 Controlled Experiments
We first evaluate the extracted corpora presented in Section 4.1. The extraction system
used to obtain each of those corpora made use of a certain initial out-of-domain parallel
corpus. We train a Baseline MT system on that initial corpus. We then train another MT
system (which we call PlusExtracted) on the initial corpus plus the extracted corpus. In
order to compare the quality of our extracted data with that of human-translated data
from the same domain, we also train an UpperBound MT system, using the initial corpus
plus a corpus of in-domain, human-translated data. For each initial corpus, we use the
same amount of human-translated data as there is extracted data (see Table 2). Thus, for
each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline,
PlusExtracted, and UpperBound.
All our MT systems were trained using a variant of the alignment template model
described in (Och 2003). Each system used two language models: a very large one,
trained on 800 million English tokens, which is the same for all the systems; and a
smaller one, trained only on the English side of the parallel training data for that
particular system. This ensured that any differences in performance are caused only
by differences in the training data.
The systems were tested on the news test corpus used for the NIST 2003 MT eval-
uation.5 Translation performance was measured using the automatic BLEU evaluation
metric (Papineni et al 2002) on four reference translations.
Figures 8 and 9 show the BLEU scores obtained by our MT systems. The 95%
confidence intervals of the scores computed by bootstrap resampling (Koehn 2004)
are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for
Chinese-English.
As the results show, the automatically extracted additional training data yields
significant improvements in performance over most initial training corpora for both
language pairs. At least for Chinese-English, the improvements are quite comparable
to those produced by the human-translated data. And, as can be expected, the impact
of the extracted data decreases as the size of the initial corpus increases.
In order to check that the classifier really does something important, we per-
formed a few experiments without it. After the article selection step, we simply paired
each foreign document with the best-matching English one, assumed they are parallel,
sentence-aligned them with a generic sentence alignment method, and added the re-
sulting data to the training corpus. The resulting BLEU scores were practically the same
as the baseline; thus, our classifier does indeed help to discover higher-quality parallel
data.
5.2 Non-Controlled Experiments
We also measured the MT performance impact of the extracted corpora described
in Section 4.2. We trained a Baseline MT system on all our available (in-domain and
5 http://www.nist.gov/speech/tests/mt.
493
Computational Linguistics Volume 31, Number 4
Figure 8
MT performance improvements for Arabic-English.
out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the
extracted in-domain data. Clearly, we have access to no UpperBound system in this
case.
The results are presented in the first two rows of Table 7. Adding the extracted
corpus lowers the score for the Arabic-English system and improves the score for
the Chinese-English one; however, none of the differences are statistically significant.
Since the baseline systems are trained on such large amounts of data (see Section 4.2),
it is not surprising that our extracted corpora have no significant impact.
In an attempt to give a better indication of the value of these corpora, we used
them alone as MT training data. The BLEU scores obtained by the systems we trained
on them are presented in the third row of Table 7. For comparison purposes, the last
line of the table shows the scores of systems trained on 10M English tokens of out-
of-domain data. As can be seen, our automatically extracted corpora obtain better MT
performance than out-of-domain parallel corpora of similar size. It?s true that this is
not a fair comparison, since the extracted corpora were obtained using all our available
parallel data. The numbers do show, however, that the extracted data, although it was
obtained automatically, is of good value for machine translation.
6. Bootstrapping
As can be seen from Table 2, the amount of data we can extract from our comparable
corpora is adversely affected by poor dictionary coverage. Thus, if we start with very
little parallel data, we do not make good use of the comparable corpora. One simple
way to alleviate this problem is to bootstrap: after we?ve extracted some in-domain data,
we can use it to learn a new dictionary and go back and extract again. Bootstrapping
was also successfully applied to this problem by Fung and Cheung (2004).
We performed bootstrapping iterations starting from two very small corpora: 100k
English tokens and 1M English tokens, respectively. After each iteration, we trained
494
Munteanu and Marcu Exploiting Non-Parallel Corpora
Figure 9
MT performance improvements for Chinese-English.
(and evaluated) an MT system on the initial data plus the data extracted in that iteration.
We did not use any of the data extracted in previous iterations since it is mostly a
subset of that extracted in the current iteration. We iterated until there were no further
improvements in MT performance on our development data.
Figures 10 and 11 show the sizes of the data extracted at each iteration, for both
initial corpus sizes. Iteration 0 is the one that uses the dictionary learned from the
initial corpus. Starting with 100k words of parallel data, we eventually collect 20M
words of in-domain Arabic-English data and 90M words of in-domain Chinese-English
data.
Figures 12 and 13 show the BLEU scores of these MT systems. For comparison
purposes, we also plotted on each graph the performance of our best MT system for
that language pair, trained on all our available parallel data (Table 7).
As we can see, bootstrapping allows us to extract significantly larger amounts of
data, which leads to significantly higher BLEU scores. Starting with as little as 100k
English tokens of parallel data, we obtain MT systems that come within 7?10 BLEU
points of systems trained on parallel corpora of more than 100M English tokens. This
Table 7
BLEU scores of the systems obtained using all available parallel data.
System Arabic-English Chinese-English
Baseline 49.22 33.77
Baseline plus extracted 48.54 34.38
Extracted only 41.2 28.04
Out-of-domain data 36.81 25.11
495
Computational Linguistics Volume 31, Number 4
Figure 10
Sizes of the Arabic-English corpora extracted using bootstrapping, in millions of English tokens.
shows that using our method, a good-quality MT system can be built from very little
parallel data and a large amount of comparable, non-parallel data.
7. Examples
We conclude the description of our method by presenting a few sentence pairs ex-
tracted by our system. We chose the examples by looking for cases when a given
foreign sentence was judged parallel to several different English sentences. Figures 14
and 15 show the foreign sentence in Arabic and Chinese, respectively, followed by a
human-produced translation in bold italic font, followed by the automatically extracted
matching English sentences in normal font. The sentences are picked from the data
sets presented in Section 4.2.
The examples reveal the two main types of errors that our system makes. The
first type concerns cases when the system classifies as parallel sentence pairs that,
although they share many content words, express slightly different meanings, as in
Figure 15, example 7. The second concerns pairs in which the two sentences convey
different amounts of information. In such pairs, one of the sentences contains a trans-
Figure 11
Sizes of the Chinese-English corpora extracted using bootstrapping, in millions of English
tokens.
496
Munteanu and Marcu Exploiting Non-Parallel Corpora
Figure 12
BLEU scores of the Arabic-English MT systems using bootstrapping.
lation of the other, plus additional (often quite long) phrases (Figure 15, examples 1
and 5).
These errors are caused by the noise present in the automatically learned dictio-
naries and by the use of a weak word alignment model for extracting the classifier
Figure 13
BLEU scores of the Chinese-English MT systems using bootstrapping.
497
Computational Linguistics Volume 31, Number 4
Figure 14
Automatically extracted Arabic-English sentence pairs.
features. In an automatically learned dictionary, many words (especially the frequent,
non-content ones) will have a lot of spurious translations. The IBM-1 alignment model
takes no account of word order and allows a source word to be connected to arbitrarily
many target words. Alignments computed using this model and a noisy, automatically
learned, dictionary will contain many incorrect links. Thus, if two sentences share
several content words, these incorrect links together with the correct links between the
498
Munteanu and Marcu Exploiting Non-Parallel Corpora
Figure 15
Automatically extracted Chinese-English sentence pairs.
common content words will yield an alignment good enough to make the classifier
judge the sentence pair as parallel.
The effect of the noise in the dictionary is even more clear for sentence pairs with
few words, such as Figure 14, example 6. The sentences in that example are tables
of soccer team statistics. They are judged parallel because corresponding digits align
499
Computational Linguistics Volume 31, Number 4
to each other, and according to our dictionary, the Arabic word for ?Mexico? can be
translated as any of the country names listed in the example.
These examples also show that the problem of finding only true translation pairs
is hard. Two sentences may share many content words and yet express different mean-
ings (see Figure 14, example 1). However, our task of getting useful MT training data
does not require a perfect solution; as we have seen, even such noisy training pairs can
help improve a translation system?s performance.
8. Related Work
While there is a large body of work on bilingual comparable corpora, most of it is
focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch
2000; Koehn and Knight 2000; Gaussier et al 2004). We are aware of only three previ-
ous efforts aimed at discovering parallel sentences. Zhao and Vogel (2002) describe a
generative model for discovering parallel sentences in the Xinhua News Chinese-
English corpus. Utiyama et. al (2003) use cross-language information retrieval tech-
niques and dynamic programming to extract sentences from an English-Japanese
comparable corpus. Fung and Cheung (2004) present an extraction method similar to
ours but focus on ?very-non-parallel corpora,? aggregations of Chinese and English
news stories from different sources and time periods.
The first two systems extend algorithms designed to perform sentence alignment
of parallel texts. They start by attempting to identify similar article pairs from the two
corpora. Then they treat each of those pairs as parallel texts and align their sentences
by defining a sentence pair similarity score and use dynamic programming to find the
least-cost alignment over the whole document pair.
In the article pair selection stage, the researchers try to identify, for an article in
one language, the best matching article in the other language. Zhao and Vogel (2002)
measure article similarity by defining a generative model in which an English story
generates a Chinese story with a given probability. Utiyama et al (2003) use the
BM25 (Robertson and Walker 1994) similarity measure.
The two works also differ in the way they define the sentence similarity score.
Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type
translation model. Utiyama et al (2003) define a score based on word overlap (i.e.,
number of word pairs from the two sentences that are translations of each other),
which also includes the similarity score of the article pair from which the sentence pair
originates.
The performance of these approaches depends heavily on the ability to reliably
find similar document pairs. Moreover, comparable article pairs, even those similar
in content, may exhibit great differences at the sentence level (reorderings, additions,
etc). Therefore, they pose hard problems for the dynamic programming alignment
approach.
In contrast, our method is more robust. The document pair selection part plays
a minor role; it only acts as a filter. We do not attempt to find the best-matching
English document for each foreign one, but rather a set of similar documents. And, most
importantly, we are able to reliably judge each sentence pair in isolation, without need
for context. On the other hand, the dynamic programming approach enables discovery
of many-to-one sentence alignments, whereas our method is limited to finding one-to-
one alignments.
The approach of Fung and Cheung (2004) is a simpler version of ours. They match
each foreign document with a set of English documents, using a threshold on their
500
Munteanu and Marcu Exploiting Non-Parallel Corpora
cosine similarity. Then, from each document pair, they generate all possible sentence
pairs, compute their cosine similarity, and apply another threshold in order to select the
ones that are parallel. Using the set of extracted sentences, they learn a new dictionary,
try to extend their set of matching document pairs (by looking for other documents that
contain these sentences), and iterate.
The evaluation methodologies of these previous approaches are less direct than
ours. Utiyama et al (2003) evaluate their sentence pairs manually; they estimate that
about 90% of the sentence pairs in their final corpus are parallel. Fung and Cheung
(2004) also perform a manual evaluation of the extracted sentences and estimate their
precision to be 65.7% after bootstrapping. In addition, they also estimate the quality of
a lexicon automatically learned from those sentences. Zhao and Vogel (2002) go one
step further and show that the sentences extracted with their method improve the
accuracy of automatically computed word alignments, to an F-score of 52.56% over a
baseline of 46.46%. In a subsequent publication, Vogel (2003) evaluates these sentences
in the context of an MT system and shows that they bring improvement under special
circumstances (i.e., a language model constructed from reference translations) designed
to reduce the noise introduced by the automatically extracted corpus. We go even
further and demonstrate that our method can extract data that improves end-to-end
MT performance without any special processing. Moreover, we show that our approach
works even when only a limited amount of initial parallel data (i.e., a low-coverage
dictionary) is available.
The problem of aligning sentences in comparable corpora was also addressed for
monolingual texts. Barzilay and Elhadad (2003) present a method of aligning sentences
in two comparable English corpora for the purpose of building a training set of text-to-
text rewriting examples. Monolingual parallel sentence detection presents a particular
challenge: there are many sentence pairs that have low lexical overlap but are never-
theless parallel. Therefore pairs cannot be judged in isolation, and context becomes
an important factor. Barzilay and Elhadad (2003) make use of contextual information
by detecting the topical structure of the articles in the two corpora and aligning them
at paragraph level based on the topic assigned to each paragraph. Afterwards, they
proceed and align sentences within paragraph pairs using dynamic programming.
Their results show that both the induced topical structure and the paragraph align-
ment improve the precision of their extraction method.
A line of research that is both complementary and related to ours is that of Resnik
and Smith (2003). Their STRAND Web-mining system has a purpose that is similar
to ours: to identify translational pairs. However, STRAND focuses on extracting pairs
of parallel Web pages rather than sentences. Resnik and Smith (2003) show that their
approach is able to find large numbers of similar document pairs. Their system is
potentially a good way of acquiring comparable corpora from the Web that could then
be mined for parallel sentences using our method.
9. Discussion
The most important feature of our parallel sentence selection approach is its robust-
ness. Comparable corpora are inherently noisy environments, where even similar
content may be expressed in very different ways. Moreover, out-of-domain corpora
introduce additional difficulties related to limited dictionary coverage. Therefore, the
ability to reliably judge sentence pairs in isolation is crucial.
Comparable corpora of interest are usually of large size; thus, processing them
requires efficient algorithms. The computational processes involved in our system are
501
Computational Linguistics Volume 31, Number 4
quite modest. All the operations necessary for the classification of a sentence pair (fil-
ter, word alignment computation, and feature extraction) can be implemented efficiently
and scaled up to very large amounts of data. The task can be easily parallelized for
increased speed. For example, extracting data from 600k English documents and 500k
Chinese documents (Section 4.2) required only about 7 days of processing time on
10 processors.
The data that we extract is useful. Its impact on MT performance is comparable
to that of human-translated data of similar size and domain. Thus, although we have
focused our experiments on the particular scenario where there is little in-domain
training data available, we believe that our method can be useful for increasing the
amount of training data, regardless of the domain of interest.
As we have shown, this could be particularly effective for language pairs for which
only very small amounts of parallel data are available. By acquiring a large compara-
ble corpus and performing a few bootstrapping iterations, we can obtain a training
corpus that yields a competitive MT system.
We suspect our approach can be used on comparable corpora coming from any do-
main. The only domain-dependent element of the system is the date window parameter
of the article selection stage (Figure 7); for other domains, this can be replaced with
a more appropriate indication of where the parallel sentences are likely to be found.
For example, if the domain were that of technical manuals, one would cluster printer
manuals and aircraft manuals separately. It is important to note that our work assumes
that the comparable corpus does contain parallel sentences (which is the case for our
data). Whether this is true for comparable corpora from other domains is an empirical
question outside the scope of this article; however, both our results and those of Resnik
and Smith (2003) strongly indicate that good data is available on the Web.
Lack of parallel corpora is a major bottleneck in the development of SMT systems
for most language pairs. The method presented in this paper is a step towards the
important goal of automatic acquisition of such corpora. Comparable texts are avail-
able on the Web in large quantities for many language pairs and domains. In this
article, we have shown how they can be efficiently mined for parallel sentences.
Acknowledgments
This work was supported by DARPA-ITO
grant NN66001-00-1-9814 and NSF grant
IIS-0326276. The experiments were run on
University of Southern California?s
high-performance computer cluster HPC
(http://www.usc.edu/hpcc). We would like
to thank Hal Daume? III, Alexander Fraser,
Radu Soricut, as well as the anonymous
reviewers, for their helpful comments. Any
remaining errors are of course our own.
References
Barzilay, Regina and Noemie Elhadad. 2003.
Sentence alignment for monolingual
comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
pages 25?32, Sapporo, Japan.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Fredrick Jelinek, John D. Lafferty, Robert L.
Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Darroch, J. N. and D. Ratcliff. 1974.
Generalized iterative scaling for log-linear
models. Annals of Mathematical Statistics,
43:95?144.
Davis, Mark W. and Ted E. Dunning. 1995. A
TREC evaluation of query translation
methods for multi-lingual text retrieval. In
Fourth Text Retrieval Conference,
pages 483?498, Gaithersburg, MD.
502
Munteanu and Marcu Exploiting Non-Parallel Corpora
Diab, Mona and Steve Finch. 2000. A
statistical word-level translation model for
comparable corpora. In Proceedings of the
Conference on Content-Based Multimedia
Information Access, Paris, France.
Diab, Mona and Philip Resnik. 2002. An
unsupervised method for word sense
tagging using parallel corpora. In
Proceedings of the 40th Anniversary Meeting
of the Association for Computational
Linguistics, pages 255?262, Philadelphia.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 16?23,
Sapporo, Japan.
Fung, Pascale and Percy Cheung. 2004.
Mining very non-parallel corpora: Parallel
sentence and lexicon extraction vie
bootstrapping and EM. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004),
pages 57?63, Barcelona, Spain.
Fung, Pascale and Kenneth Ward Church.
1994. Kvec: A new approach for aligning
parallel texts. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING), pages 1096?1102,
Kyoto.
Fung, Pascale and Lo Yuen Yee. 1998. An IR
approach for translating new words from
nonparallel, comparable texts. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics,
pages 414?420, Montreal.
Gale, William A. and Kenneth Ward Church.
1991. A program for aligning sentences in
bilingual corpora. In Proceedings of the 29th
Annual Meeting of the Association for
Computational Linguistics, pages 177?184,
Berkeley, CA.
Gaussier, Eric, Jean-Michel Renders, Irina
Matveeva, Cyril Goutte, and Herve
Dejean. 2004. A geometric view on
bilingual lexicon extraction from
comparable corpora. In Proceedings of the
42nd Annual Meeting of the Association for
Computational Linguistics, pages 527?534,
Barcelona, Spain.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 388?395, Barcelona, Spain.
Koehn, Philipp and Kevin Knight. 2000.
Estimating word translation probabilities
from unrelated monolingual corpora using
the EM algorithm. In Proceedings of the
National Conference on Artificial Intelligence,
pages 711?715, Austin, TX.
Malouf, Robert. 2002. A comparison of
algorithms for maximum entropy
parameter estimation. In Sixth Conference
on Natural Language Learning, Taipei,
Taiwan.
Melamed, Dan I. 1997. A portable algorithm
for mapping bitext correspondence. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 305?312, Madrid, Spain.
Melamed, Dan I. 1999. Bitext maps and
alignment via pattern recognition.
Computational Linguistics, 25(1):107?130.
Minton, Steven N., Sorinel I. Ticrea, and
Jennifer Beach. 2003. Trainability:
Developing a responsive learning system.
In IJCAI Workshop on Information Integration
on the Web, pages 27?32, Acapulco,
Mexico.
Moore, Robert C. 2002. Fast and accurate
sentence alignment of bilingual corpora.
In Proceedings of the 5th Conference of the
Association for Machine Translation in the
Americas, pages 135?144, Tiburon, CA.
Moore, Robert C. 2004. Improving IBM
word-alignment model 1. In 42nd Annual
Meeting of the Association for Computational
Linguistics, pages 519?526, Barcelona,
Spain.
Munteanu, Dragos Stefan, Alexander Fraser,
and Daniel Marcu. 2004. Improved
machine translation performance via
parallel sentence extraction from
comparable corpora. In Proceedings of the
Human Language Technology Conference of
the North American Chapter of the Association
For Computational Linguistics,
pages 265?272, Boston, MA.
Oard, Douglas W. 1997. Cross-language text
retrieval research in the USA. In Third
DELOS Workshop on Cross-Language
Information Retrieval, pages 1?10, Zurich,
Switzerland.
Och, Franz Josef. 2003. Minimum error rate
training for statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 295?302,
Philadelphia.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
503
Computational Linguistics Volume 31, Number 4
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Ogilvie, Paul and Jamie Callan. 2001.
Experiments using the Lemur toolkit. In
Proceedings of the Tenth Text Retrieval
Conference, pages 103?108, Gaithersburg,
MD.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings
of the 40th Anniversary Meeting
of the Association for Computational
Linguistics, pages 311?318,
Philadelphia.
Rapp, Reinhard. 1999. Automatic
identification of word translations from
unrelated English and German corpora.
In Proceedings of the 27th Annual Meeting
of the Association for Computational
Linguistics, pages 519?526, College
Park, MD.
Resnik, Philip and Noah A. Smith. 2003.
The web as a parallel corpus.
Computational Linguistics, 29(3):349?380,
September.
Robertson, E. and S. Walker. 1994. Some
simple effective approximations to the
2-Poisson model for probabilistic weighted
retrieval. In Proceedings of the 17th Annual
ACM SIGIR, pages 232?241, Dublin,
Ireland.
Ticrea, Sorinel I. and Steven Minton. 2003.
Inducing web agents: Sample page
management. In Proceedings of the
International Conference on Information and
Knowledge Engineering, pages 399?403, Las
Vegas, NV, June.
Utiyama, Masao and Hitoshi Isahara. 2003.
Reliable measures for aligning
Japanese-English news articles and
sentences. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 72?79, Sapporo, Japan.
Vogel, Stephan. 2003. Using noisy bilingual
data for statistical machine translation. In
Proceedings of the 10th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 175?178,
Budapest, Hungary.
Wu, Dekai. 1994. Aligning a parallel
English-Chinese corpus statistically with
lexical criteria. In Proceedings of the 32nd
Annual Meeting of the Association for
Computational Linguistics, pages 80?87, Las
Cruces, NM.
Yarowsky, David and Grace Ngai. 2001.
Inducing multilingual POS taggers and NP
bracketers via robust projection across
aligned corpora. In Proceedings of the 2nd
Meeting of the North American Association for
Computational Linguistics, pages 200?207,
Pittsburgh, PA.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Proceedings
of the First International Conference on
Human Language Technology Research,
pages 161?168, San Diego, CA.
Zhao, Bing and Stephan Vogel. 2002.
Adaptive parallel sentences mining from
web bilingual news collection. In 2002
IEEE International Conference on Data
Mining, pages 745?748, Maebashi City,
Japan.
504
Induction of Word and Phrase Alignments
for Automatic Document Summarization
Hal Daume? III?
Information Sciences Institute
University of Southern California
Daniel Marcu?
Information Sciences Institute
University of Southern California
Current research in automatic single-document summarization is dominated by two effective,
yet na??ve approaches: summarization by sentence extraction and headline generation via bag-
of-words models. While successful in some tasks, neither of these models is able to adequately
capture the large set of linguistic devices utilized by humans when they produce summaries.
One possible explanation for the widespread use of these models is that good techniques have
been developed to extract appropriate training data for them from existing document/abstract
and document/ headline corpora. We believe that future progress in automatic summarization
will be driven both by the development of more sophisticated, linguistically informed models,
as well as a more effective leveraging of document/abstract corpora. In order to open the doors
to simultaneously achieving both of these goals, we have developed techniques for automatically
producing word-to-word and phrase-to-phrase alignments between documents and their human-
written abstracts. These alignments make explicit the correspondences that exist in such docu-
ment/abstract pairs and create a potentially rich data source from which complex summarization
algorithms may learn. This paper describes experiments we have carried out to analyze the ability
of humans to perform such alignments, and based on these analyses, we describe experiments for
creating them automatically. Our model for the alignment task is based on an extension of the
standard hidden Markov model and learns to create alignments in a completely unsupervised
fashion. We describe our model in detail and present experimental results that show that our
model is able to learn to reliably identify word- and phrase-level alignments in a corpus of
?document, abstract? pairs.
1. Introduction and Motivation
1.1 Motivation
We believe that future success in automatic document summarization will be made
possible by the combination of complex, linguistically motivated models and effective
leveraging of data. Current research in summarization makes a choice between these
two: one either develops sophisticated, domain-specific models that are subsequently
hand-tuned without the aid of data, or one develops na??ve general models that can
? 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292. Email: {hdaume,marcu}@isi.edu.
Submission received: 12 January 2005; revised submission received: 3 May 2005; accepted for
publication: 27 May 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
Figure 1
Example alignment of a single abstract sentence with two document sentences.
be trained on large amounts of data (in the form of corpora of document/extract or
document/headline pairs). One reason for this is that currently available technologies
are only able to extract very coarse and superficial information that is inadequate
for training complex models. In this article, we propose a method to overcome this
problem: automatically generating word-to-word and phrase-to-phrase alignments
between documents and their human-written abstracts.1
To facilitate discussion and to motivate the problem, we show in Figure 1 a rela-
tively simple alignment between a document fragment and its corresponding abstract
fragment from our corpus.2 In this example, a single abstract sentence (shown along
the top of the figure) corresponds to exactly two document sentences (shown along the
bottom of the figure). If we are able to automatically generate such alignments, one can
envision the development of models of summarization that take into account effects
of word choice, phrasal and sentence reordering, and content selection. Such models
could be simultaneously linguistically motivated and data-driven. Furthermore, such
alignments are potentially useful for current-day summarization techniques, including
sentence extraction, headline generation, and document compression.
A close examination of the alignment shown in Figure 1 leads us to three obser-
vations about the nature of the relationship between a document and its abstract, and
hence about the alignment itself:
 Alignments can occur at the granularity of words and of phrases.
 The ordering of phrases in an abstract can be different from the ordering of
phrases in the document.
 Some abstract words do not have direct correspondents in the document,
and many document words are never used in an abstract.
In order to develop an alignment model that could recreate such an alignment, we
need our model to be able to operate both at the word level and at the phrase level, we
need it to be able to allow arbitrary reorderings, and we need it to be able to account
for words on both the document and abstract side that have no direct correspondence. In
this paper, we develop an alignment model that is capable of learning all these aspects
of the alignment problem in a completely unsupervised fashion.
1 We will use the words abstract and summary interchangeably. When we wish to emphasize that a
particular summary is extractive, we will refer to it as an extract.
2 As part of the tokenization step, any possessive form is split off from its noun, and represented as POSS.
In most cases, this involves separating an ??s? and replacing it with POSS, as in ?John?s? ? ?John POSS.?
For consistency, we have treated ?it?s?/?its? as a special case: ?its? (the possessive) is converted to
?it POSS.?
506
Daume? and Marcu Alignments for Automatic Document Summarization
1.2 Shortcomings of Current Summarization Models
Current state-of-the-art automatic single-document summarization systems employ
one of three techniques: sentence extraction, bag-of-words headline generation, or
document compression. Sentence extraction systems take full sentences from a doc-
ument and concatenate them to form a summary. Research in sentence extraction
can be traced back to work in the mid 1950s and late 1960s by Luhn (1956) and
Edmundson (1969). Recent techniques are startlingly not terribly divergent from these
original methods; see Mani and Maybury (1999); Marcu (2000); Mani (2001) for a com-
prehensive overview. Headline generation systems, on the other hand, typically extract
individual words from a document to produce a very short headline-style summary;
see Banko, Mittal, and Witbrock (2000); Berger and Mittal (2000); Schwartz, Zajic, and
Dorr (2002) for representative examples. Between these two extremes, there has been
a relatively modest amount of work in sentence simplification (Chandrasekar, Doran,
and Bangalore 1996; Mahesh 1997; Carroll et al 1998; Grefenstette 1998; Jing 2000;
Knight and Marcu 2002) and document compression (Daume? III and Marcu 2002;
Daume? III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases,
and sentences are selected in an extraction process.
While such approaches have enjoyed some success, they all suffer from modeling
shortcomings. Sentence extraction systems and document compression models make
unrealistic assumptions about the summarization task (namely, that extraction is suffi-
cient and that sentences are the appropriate level of granularity). Headline generation
systems employ very weak models that make an incorrect bag-of-words assumption.
This assumption allows such systems to learn limited transformations to produce
headlines from arbitrary documents, but such transformations are not nearly complex
enough to adequately model anything beyond indicative summaries at a length of
around 10 words. Bag-of-words models can learn what the most important words to
keep in a headline are, but say nothing about how to structure them in a well-formed,
grammatical headline.
In our own work on document compression models (Daume? III and Marcu 2002;
Daume? III and Marcu 2004), both of which extend the sentence compression model of
Knight and Marcu (2002), we assume that sentences and documents can be summa-
rized exclusively through deletion of contiguous text segments. In Knight and Marcu?s
data, we found that from a corpus of 39,060 abstract sentences, only 1,067 were created
from corresponding document sentences via deletion of contiguous segments. In other
words, only 2.7% of the sentences in real ?document, abstract? pairs can be explained
by the model proposed by Knight and Marcu (2002). Such document compression
models do not explain the rich set of linguistic devices employed, for example, in
Figure 1.
1.3 Prior Work on Alignments
In the sentence extraction community, there exists a wide variety of techniques for
(essentially) creating alignments between document sentences and abstract sentences
(Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997; Marcu 1999); see also
Barzilay and Elhadad (2003); Quirk, Brockett, and Dolan (2004) for work describing
alignments for the monolingual paraphrasing task. These techniques typically take
into account information such as lexical overlap, synonymy, ordering, length, dis-
course structure, and so forth. The sentence alignment problem is a comparatively
simple problem to solve, and current approaches work quite well. Unfortunately, these
507
Computational Linguistics Volume 31, Number 4
alignments are the least useful, because they can only be used to train sentence ex-
traction systems.
In the context of headline generation, simple statistical models are used for aligning
documents and headlines (Banko, Mittal, and Witbrock 2000; Berger and Mittal 2000;
Schwartz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al 1993). These
models treat documents and headlines as simple bags of words and learn probabilistic
word-based mappings between the words in the documents and the words in the
headlines. Such mappings can be considered word-to-word alignments, but as our
results show (see Section 5), these models are too weak for capturing the sophisticated
operations that are employed by humans in summarizing texts.
To date, there has been very little work on the word alignment task in the context
of summarization. The most relevant work is that of Jing (2002), in which a hidden
Markov alignment model is applied to the task of identifying word and phrase-level
correspondences between documents and abstracts. Unfortunately, this model is only
able to align words that are identical up to their stems, and thus suffers from a problem
of recall. This also makes it ill-suited to the task of learning how to perform abstraction,
in which one would desire to know how words get changed. For example, Jing?s model
cannot identify any of the following alignments from Figure 1: (Connecting Point ?
Connecting Point Systems), (Mac ? Macintosh), (retailer ? seller), (Macintosh ? Apple
Macintosh systems) and (January 1989 ? last January).
Word alignment (and, to a lesser degree, phrase alignment) has been an active
topic of research in the machine translation community. Based on these efforts, one
might be initially tempted to use readily available alignment models developed in the
context of machine translation, such as GIZA++ (Och and Ney 2003), to obtain word-
level alignments in ?document, abstract? corpora. However, as we will show (Section 5),
the alignments produced by such a system are inadequate for the ?document, abstract?
alignment task.
1.4 Article Structure
In this article, we describe a novel, general model for automatically inducing word-
and phrase-level alignments between documents and their human-written abstracts.
Beginning in Section 2, we will describe the results of human annotation of such align-
ments. Based on this annotation, we will investigate the empirical linguistic properties
of such alignments, including lexical transformations and movement. In Section 3, we
will introduce the statistical model we use for deriving such alignments automatically.
The inference techniques are based on those of semi-Markov models, extensions of
hidden Markov models that allow for multiple simultaneous observations.
After our discussion of the model structure and algorithms, we discuss the various
parameterizations we employ in Section 4. In particular, we discuss three distinct mod-
els of movement, two of which are well-known in the machine translation alignment
literature, and a third one that exploits syntax in a novel, ?light? manner. We also discuss
several models of lexical rewriting, based on identities, stems, WordNet synonymy,
and automatically induced lexical replacements. In Section 5, we present experimental
results that confirm that our model is able to learn the hidden structure in our corpus
of ?document, abstract? pairs. We compare our model against well-known alignment
models designed for machine translation as well as a state-of-the-art alignment model
specifically designed for summarization (Jing 2002). Additionally, we discuss errors that
the model currently makes, supported by some relevant examples and statistics. We
conclude with some directions for future research (Section 6).
508
Daume? and Marcu Alignments for Automatic Document Summarization
Table 1
Ziff-Davis corpus statistics.
Sub-corpus Annotated
Abstracts Documents Abstracts Documents
Documents 2033 45
Sentences 13k 82k 244 2k
Words 261k 2.5M 6.4k 49k
Unique words 14k 42k 1.9k 5.9k
45k 6k
Sentences/Doc 6.28 40.83 5.42 45.3
Words/Doc 128.52 1229.71 142.33 1986.16
Words/Sent 20.47 28.36 26.25 24.20
2. Human-produced Alignments
In order to decide how to design an alignment model and to judge the quality of the
alignments produced by a system, we first need to create a set of ?gold standard?
alignments. To this end, we asked two human annotators to manually construct
such alignments between documents and their abstracts. These ?document, abstract?
pairs were drawn from the Ziff-Davis collection (Marcu 1999). Of the roughly 7,000
documents in that corpus, we randomly selected 45 pairs for annotation. We added to
this set of 45 pairs the 2,000 shorter documents from this collection, and all the work
described in the remainder of this paper focuses on this subset of 2,033 ?document,
abstract? pairs.3 Statistics for this sub-corpus and for the pairs selected for annotation
are shown in Table 1. As can be simply computed from this table, the compression rate
in this corpus is about 12%. The first five human-produced alignments were completed
separately and then discussed; the last 40 were done independently.
2.1 Annotation Guidelines
Annotators were asked to perform word-to-word and phrase-to-phrase alignments
between abstracts and documents, and to classify each alignment as either possible (P)
or sure (S), where S ? P, following the methodology used in the machine translation
community (Och and Ney 2003). The direction of containment (S ? P) is because being
a sure alignment is a stronger requirement than being a possible alignment. A full descrip-
tion of the annotation guidelines is available in a document available with the alignment
software on the first author?s web site (http://www.isi.edu/?hdaume/HandAlign).
Here, we summarize the main points.
The most important instruction that annotators were given was to align everything
in the summary to something. This was not always possible, as we will discuss shortly,
but by and large it was an appropriate heuristic. The second major instruction was to
choose alignments with maximal consecutive length: If there are two possible alignments
for a phrase, the annotators were instructed to choose the one that will result in the
longest consecutive alignment. For example, in Figure 1, this rule governs the choice of
3 The reason there are 2,033 pairs, not 2,045, is that 12 of the original 45 pairs were among the 2,000
shortest, so the 2,033 pairs are obtained by taking the 2,000 shortest and adding to them the 33 pairs that
were annotated and not already among the 2,000 shortest.
509
Computational Linguistics Volume 31, Number 4
the alignment of the word Macintosh on the summary side: lexically, it could be aligned
to the final occurrence of the word Macintosh on the document side, but by aligning it to
Apple Macintosh systems, we are able to achieve a longer consecutive sequence of aligned
words.
The remainder of the instructions have to do primarily with clarifying particular
linguistic phenomena including punctuation, anaphora (for entities, annotators are told
to feel free to align names to pronouns, for instance) and metonymy, null elements,
genitives, appositives, and ellipsis.
2.2 Annotator Agreement
To compute annotator agreement, we employed the kappa statistic. To do so, we treat
the problem as a sequence of binary decisions: given a single summary word and
document word, should the two be aligned? To account for phrase-to-phrase align-
ments, we first converted these into word-to-word alignments using the ?all pairs?
heuristic. By looking at all such pairs, we wound up with 7.2 million items over which
to compute the kappa statistic (with two annotators and two categories). Annotator
agreement was strong for sure alignments and fairly weak for possible alignments.
When considering only sure alignments, the kappa statistic for agreement was 0.63
(though it dropped drastically to 0.42 on possible alignments).
In performing the annotation, we found that punctuation and non-content words
are often very difficult to align (despite the discussion of these issues in the alignment
guidelines). The primary difficulty with function words is that when the summarizers
have chosen to reorder words to use slightly different syntactic structures, there are
lexical changes that are hard to predict.4 Fortunately, for many summarization tasks,
it is much more important to get content words right, rather than function words.
When words on a stop list of 58 function words and punctuation were ignored, the
kappa value rose to 0.68. Carletta (1995) has suggested that kappa values over 0.80
reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good
agreement.5
2.3 Results of Annotation
After the completion of these alignments, we can investigate some of their properties.
Such an investigation is interesting both from the perspective of designing a model and
from a linguistic perspective.
In the alignments, we found that roughly 16% of the abstract words are left un-
aligned. This figure includes both standard lexical words and punctuation. Of this
16%, 4% are punctuation marks (though not all punctuation is unaligned) and 7%
are function words. The remaining 5% are words that would typically be considered
content words. This rather surprising result tells us that any model we build needs
to be able to account for a reasonable portion of the abstract to not have a direct
correspondence to any portion of the document.
4 For example, the change from I gave a gift to the boy. to The boy received a gift from me. is relatively
straightforward; however, it is a matter of opinion whether to and from should be aligned ? they serve the
same role, but certainly do not mean the same thing.
5 All annotator agreement figures are calculated only on the last 40 ?document, abstract? pairs, which were
annotated independently.
510
Daume? and Marcu Alignments for Automatic Document Summarization
To get a sense of the importance of producing alignments at the phrase level, we
computed that roughly 75% of the alignments produced by humans involve only one
word on both sides. In 80% of the alignments, the summary side is a single word (thus
in 5% of the cases, a single summary word is aligned to more than one document
word). In 6.1% of the alignments, the summary side involved a phrase of length two,
and in 2.2% of the cases it involved a phrase of length three. In all these numbers, care
should be taken to note that the humans were instructed to produce phrase alignments
only when word alignments were impossible. Thus, it is entirely likely that summary
word i is aligned to document word j and summary word i + 1 is aligned to document
word j + 1, in which case we count this as two singleton alignments, rather than an
alignment of length two. These numbers suggest that looking at phrases in addition to
words is empirically important.
Lexical choice is another important aspect of the alignment process. Of all the
aligned summary words and phrases, the corresponding document word or phrase was
exactly the same as that on the summary side in 51% of the cases. When this constraint
was weakened to looking only at stems (for multi-word phrases, a match meant that
each corresponding word matched up to stem), this number rose to 67%. When broken
down into cases of singletons and non-singletons, we saw that 86% of singletons are
identical up to stem, and 48% of phrases are identical up to stem. This suggests that
looking at stems, rather than lexical items, is useful.
Finally, we investigated the issue of adjacency in the alignments. Specifically, we
consider the following question: Given that a summary phrase ending at position i
is aligned to a document phrase ending at position j, what is a likely position in the
document for the summary phrase beginning at position i + 1? It turns out that this
is overwhelmingly j + 1. In Figure 2, we have plotted the frequencies of such relative
jumps over the human-aligned data. This graph suggests that a model biased toward
stepping forward monotonically in the document is likely to be appropriate. However,
it should also be noted that backward jumps are also quite common, suggesting that a
monotonic alignment model is inappropriate for this task.
3. Statistical Alignment Model
Based on linguistic observations from the previous section, we reach several conclu-
sions regarding the development of a statistical model to produce such alignments.
First, the model should be able to produce alignments between phrases of arbitrary
length (but perhaps with a bias toward single words). Second, it should not be con-
strained by any assumptions of monotonicity or word (or stem) identity, but it might
be able to realize that monotonicity and word and stem identity are good indicators of
alignment. Third, our model must be able to account for words on the abstract side
that have no correspondence on the document side (following the terminology from the
machine translation community, we will refer to such words as null generated).
3.1 Generative Story
Based on these observations, and with an eye toward computational tractability, we
posit the following generative story for how a summary is produced, given a document:
1. Repeat until the whole summary is generated:
(a) Choose a document position j and jump there.
511
Computational Linguistics Volume 31, Number 4
Figure 2
Analysis of the motion observed in documents when considering a movement of +1 on the
summary side.
(b) Choose a document phrase length l.
(c) Generate a summary phrase based on the document phrase
spanning positions j to j + l.
2. Jump to the end of the document.
In order to account for null generated summary words, we augment the above gen-
erative story with the option to jump to a specifically designated null state from which a
summary phrase may be generated without any correspondence in the document. From
inspection of the human-aligned data, most such null generated words are function
words or punctuation; however, in some cases, there are pieces of information in the
summary that truly did not exist in the original document. The null generated words
can account for these as well (additionally, the null generated words allow the model
to ?give up? when it cannot do anything better). We require that summary phrases
produced from the null state have length 1, so that in order to generate multiple null
generated words, they must be generated independently.
In Figure 3, we have shown a portion of the generative process that would give rise
to the alignment in Figure 1.
This generative story implicitly induces an alignment between the document and
the summary: the summary phrase is considered to be aligned to the document phrase
512
Daume? and Marcu Alignments for Automatic Document Summarization
Figure 3
Beginning and end of the generative process that gave rise to the alignment in Figure 1, which is
reproduced here for convenience.
that ?generated? it. In order to make this computationally tractable, we must introduce
some conditional independence assumptions. Specifically, we assume the following:
1. Decision (a) in our generative story depends only on the position of the
end of the current document phrase (i.e., j + l).
2. Decision (b) is conditionally independent of every other decision.
3. Decision (c) depends only on the phrase at the current document position.
3.2 Statistical Model
Based on the generative story and independence assumptions described above, we can
model the entire summary generation process according to two distributions:
 jump( j? | j + l), the probability of jumping to position j? in the document
when the previous phrase ended at position j + l.
 rewrite(s | dj:j+l), the rewrite probability of generating summary phrase s
given that we are considering the sub-phrase of d beginning at position j
and ending at position j + l.
Specific parameterizations of the distributions jump and rewrite will be discussed in
Section 4 to enable the focus here to be on the more general problems of inference and
decoding in such a model. The model described by these independence assumptions
very much resembles that of a hidden Markov model (HMM), where states in the state
space are document ranges and emissions are summary words. The difference is that
instead of generating a single word in each transition between states, an entire phrase
is generated. This difference is captured by the semi-Markov model or segmental HMM
framework, described in great detail by Ostendorf, Digalakis, and Kimball (1996); see
also Ferguson (1980); Gales and Young (1993); Mitchell, Jamieson, and Harper (1995);
Smyth, Heckerman, and Jordan (1997); Ge and Smyth (2000); Aydin, Altunbasak, and
Borodovsky (2004) for more detailed descriptions of these models as well as other ap-
plications in speech processing and computational biology. In the following subsections,
we will briefly discuss the aspects of inference that are relevant to our problem, but
the interested reader is directed to Ostendorf, Digalakis, and Kimball (1996) for more
details.
3.3 Creating the State Space
Given our generative story, we can construct a semi-HMM to calculate precisely the
alignment probabilities specified by our model in an efficient manner. A semi-HMM is
513
Computational Linguistics Volume 31, Number 4
fully defined by a state space (with designated start and end states), an output alpha-
bet, transition probabilities, and observation probabilities. The semi-HMM functions
like an HMM: Beginning at the start state, stochastic transitions are made through the
state space according to the transition probabilities. At each step, one or more observa-
tions are generated. The machine stops when it reaches the end state.
In our case, the state set is large, but well structured. There is a unique initial state
?start?, a unique final state ?end?, and a state for each possible document phrase. That
is, for a document of length n, for all 1 ? i ? i? ? n, there is a state that corresponds
to the document phrase beginning at position i and ending at position i?, which we
will refer to as ri,i? . There is also a null state for each document position r?,i. Thus, S =
{?start?, ?end?} ? {ri,i? : 1 ? i ? i? ? n} ? {r?,i : 1 ? i ? n}. The output alphabet consists
of each word found in S, plus the end-of-sentence word ?. We only allow the word ?
to be emitted on a transition to the end state. The transition probabilities are managed
by the jump model, and the emission probabilities are managed by the rewrite model.
Consider the document a b (the semi-HMM for which is shown in Figure 4) in
the case when the corresponding summary is c d. Suppose the correct alignment is
that c d is aligned to a and b is left unaligned. Then, the path taken through the
semi-HMM is ?start? ? a ? ?end?. During the transition ?start? ? a, c d is emitted.
During the transition a ? ?end?, ? is emitted.
3.4 Expectation Maximization
The alignment task, as described above, is a chicken-and-egg problem: if we knew
the model components (namely, the rewrite and jump tables), we would be able to
efficiently find the best alignment. Similarly, if we knew the correct alignments, we
would be able to estimate the model components. Unfortunately, we have neither.
Expectation maximization is a general technique for learning in such chicken-and-egg
situations (Dempster, Laird, and Rubin 1977; Boyles 1983; Wu 1983). The basic idea is to
make a guess at the alignments, and then use this guess to estimate the parameters for
the relevant distributions. We can use these re-estimated distributions to make a better
guess at the alignments, and then use these (ideally better) alignments to re-estimate
the parameters.
Figure 4
Schematic drawing of the semi-HMM (with some transition probabilities) for the document a b.
514
Daume? and Marcu Alignments for Automatic Document Summarization
Formally, the EM family of algorithms tightly bound the log of an expectation of
a function by the expectation of the log of that function, through the use of Jensen?s
inequality (Jensen 1906). The tightness of the bound means that when we attempt to
estimate the model parameters, we may do so over expected alignments, rather than the
true (but unknown) alignments. EM gives formal guarantees of convergence, but is only
guaranteed to find local maxima.
3.5 Model Inference
All the inference techniques utilized in this paper are standard applications of semi-
Markov model techniques. The relevant equations are summarized in Figure 5 and
described here. In all these equations, the variables t and t? range over phrases in the
summary (specifically, the phrase st:t? ), and the variables i and j range over phrases in the
document. The interested reader is directed to Ostendorf, Digalakis, and Kimball (1996)
for more details on the generic form of these models and their inference techniques.
Unfortunately, the number of possible alignments for a given ?document,
summary? pair is exponential in the length of the summary. This would make a na??ve
implementation of the computation of p(s | d) intractable without a more clever solu-
tion. Instead, we are able to employ a variant of the forward algorithm to compute these
probabilities recursively. The basic idea is to compute the probability of generating a
prefix of the summary and ending up at a particular position in the document (this is
known as the forward probability). Since our independence assumptions tell us that
it does not matter how we got to this position, we can use this forward probability
to compute the probability of taking one more step in the summary. At the end, the
desired probability p(s | d) is simply the forward probability of reaching the end of
the summary and document simultaneously. The forward probabilities are calculated
in the ? table in Figure 5. This equation essentially says that the probability of emitting
the first t ? 1 words of the summary and ending at position j in the document can
be computed by summing over our previous position (t?) and previous state (i) and
multiplying the probability of getting there (?i(t? + 1)) with the probability of moving
from there to the current position.
Figure 5
Summary of inference equations for a semi-Markov model.
515
Computational Linguistics Volume 31, Number 4
The second standard inference problem is the calculation of the best alignment: the
Viterbi alignment. This alignment can be computed in exactly the same fashion as the
forward algorithm, with two small changes. First, the forward probabilities implicitly
include a sum over all previous states, whereas the Viterbi probabilities replace this
with a max operator. Second, in order to recover the actual Viterbi alignment, we keep
track of which previous state this max operator chose. This is computed by filling out
the ? table from Figure 5. This is almost identical to the computation of the forward
probabilities, except that instead of summing over all possible t? and i, we take the
maximum over those variables.
The final inference problem is parameter re-estimation. In the case of standard
HMMs, this is known as the Baum-Welch, Baum-Eagon or Forward-Backward algo-
rithm (Baum and Petrie 1966; Baum and Eagon 1967). By introducing backward proba-
bilities analogous to the forward probabilities, we can compute alignment probabilities
of suffixes of the summary. The backward table is the ? table in Figure 5, which is
analogous to the ? table, except that the computation proceeds from the end to the start.
By combining the forward and backward probabilities, we can compute the ex-
pected number of times a particular alignment was made (the E-step in the EM frame-
work). Based on these expectations, we can simply sum and normalize to get new
parameters (the M-step). The expected transitions are computed according to the ? table,
which makes use of the forward and backward probabilities. Finally, the re-estimated
jump probabilities are given by a? and the re-estimated rewrite probabilities are given by
b?, which are essentially relative frequencies of the fractional counts given by the ?s.
The computational complexity for the Viterbi algorithm and for the parameter re-
estimation is O
(
N2T2
)
, where N is the length of the summary and T is the number of
states (in our case, T is roughly the length of the document times the maximum phrase
length allowed). However, we will typically bound the maximum length of a phrase;
we are unlikely to otherwise encounter enough training data to get reasonable estimates
of emission probabilities. If we enforce a maximum observation sequence length of l,
then this drops to O
(
N2Tl
)
. Moreover, if the transition network is sparse, as it is in
our case, and the maximum out-degree of any node is b, then the complexity drops to
O (NTbl).
4. Model Parameterization
Beyond the conditional independence assumptions made by the semi-HMM, there are
nearly no additional constraints that are imposed on the parameterization (in terms
of the jump and rewrite distributions) of the model. There is one additional technical
requirement involving parameter re-estimation, which essentially says that the expec-
tations calculated during the forward-backward algorithm must be sufficient statistics
for the parameters of the jump and rewrite models. This constraint simply requires that
whatever information we need to re-estimate their parameters is available to us from
the forward-backward algorithm.
4.1 Parameterizing the Jump Model
Recall that the responsibility of the jump model is to compute probabilities of the form
jump(j? | j), where j? is a new position and j is an old position. We have explored several
possible parameterizations of the jump table. The first simply computes a table of
likely jump distances (i.e., jump forward 1, jump backward 3, etc.). The second models
516
Daume? and Marcu Alignments for Automatic Document Summarization
Table 2
Jump probability decomposition; the source state is either the designated start state, the
designated end state, a document phrase position spanning from i to i? (denoted ri,i? ) or a null
state corresponding to position i (denoted r?,i).
source target probability
?start? ri,i? jumprel(i)
ri,i? rj,j? jumprel( j ? i?)
ri,j? ?end? jumprel(m + 1 ? i?)
?start? r?,i jumprel(?)jumprel(i)
r?,i rj,j? jumprel( j ? i)
r?,i r?,j jumprel(?)jumprel( j ? i)
r?,i ?end? jumprel(m + 1 ? i)
ri,i? r?,j jumprel(?)jumprel( j ? i?)
this distribution as a Gaussian (though, based on Figure 2 this is perhaps not the best
model). Both of these models have been explored in the machine translation commu-
nity. Our third parameterization employs a novel syntax-aware jump model that at-
tempts to take advantage of local syntactic information in computing jumps.
4.1.1 The Relative Jump Model. In the relative jump model, we keep a table of counts
for each possible jump distance, and compute jump(j? | j) = jumprel(j? ? j). Each possi-
ble jump type and its associated probability is shown in Table 2. By these calculations,
regardless of document phrase lengths, transitioning forward between two consecutive
segments will result in jumprel(1). When transitioning from the start state p to state
ri,i? , the value we use is a jump length of i. Thus, if we begin at the first word in the
document, we incur a transition probability of j1. There are no transitions into p. We
additionally remember a specific transition jumprel(?) for the probability of transition-
ing to a null state. It is straightforward to estimate these parameters based on the
estimations from the forward-backward algorithm. In particular, jumprel(i) is simply
the relative frequency of length i jumps, and jumprel(?) is simply the count of jumps that
end in a null state to the total number of jumps. The null state remembers the position
we ended in before we jumped there, and so to jump out of a null state, we make a jump
based on this previous position.6
4.1.2 Gaussian Jump Model. The Gaussian jump model attempts to alleviate the spar-
sity of data problem in the relative jump model by assuming a parametric form to the
jumps. In particular, we assume there is a mean jump length ? and a jump variance ?2,
and then the probability of a jump of length i is given by:
i ? Nor(?,?2) ? exp
[
1
?2
(i ? ?)2
]
(1)
Some care must be taken in employing this model, since the normal distribution
is defined over a continuous space. Thus, when we discretize the calculation, the nor-
malizing constant changes slightly from that of a continuous normal distribution. In
6 In order for the null state to remember where we were, we actually introduce one null state for each
document position, and require that from a document phrase di:j, we can only jump to null state ?j.
517
Computational Linguistics Volume 31, Number 4
practice, we normalize by summing over a sufficiently large range of possible is. The pa-
rameters ? and ?2 are estimated by computing the mean jump length in the expectations
and its empirical variance. We model null states identically to the relative jump model.
4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are
extremely na??ve in that they look only at the distance jumped and completely ignore
what is being jumped over. In the syntax-aware jump model, we wish to enable the
model to take advantage of syntactic knowledge in a very weak fashion. This is quite
different from the various approaches to incorporating syntactic knowledge into ma-
chine translation systems, wherein strong assumptions about the possible syntactic
operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003).
To motivate this model, consider the first document sentence shown with its syn-
tactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance
more than one are often indicative of skipped words. From the standpoint of the rela-
tive jump models, jumping over the four words tripled it ?s sales and jumping over the
four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we
would be much more willing to jump over the latter than the former. The latter phrase
is a full syntactic constituent, while the first phrase is just a collection of nearby words.
Furthermore, the latter phrase is a prepositional phrase (and prepositional phrases
might be more likely dropped than other phrases), while the former phrase includes
a verb, a pronoun, a possessive marker, and a plain noun.
To formally capture this notion, we parameterize the syntax-aware jump model
according to the types of phrases being jumped over. That is, to jump over tripled it ?s
sales would have probability jumpsyn(VBD PRP POS NNS) while to jump over of Apple
Macintosh systems would have probability jumpsyn(PP). In order to compute the prob-
abilities for jumps over many components, we factorize so that the first probabil-
ity becomes jumpsyn(VBD)jumpsyn(PRP)jumpsyn(POS)jumpsyn(NNS). This factorization
explicitly encodes our preference for jumping over single units rather than several
syntactically unrelated units.
In order to work with this model, we must first parse the document side of the
corpus; we used Charniak?s parser (Charniak 1997). Given the document parse trees,
the re-estimation of the components of this probability distribution is done by sim-
ply counting what sorts of phrases are being jumped over. Again, we keep a single
parameter jumpsyn(?) for jumping to null states. To handle backward jumps, we simply
consider a duplication of the tag set, where jumpsyn(NP-f) denotes a forward jump over
an NP, and jumpsyn(NP-b) denotes a backward jump over an NP.
8
4.2 Parameterizing the Rewrite Model
As observed from the human-aligned summaries, a good rewrite model should be able
to account for alignments between identical word and phrases, between words that
are identical up to stem, and between different words. Intuition (as well as further
7 As can be seen from this example, we have preprocessed the data to split off possessive terms, such as the
mapping from its to it ?s.
8 In general, there are many ways to get from one position to another. For instance, to get from systems to
January, we could either jump forward over an RB and a JJ, or we could jump forward over an ADVP and
backward over an NN. In our version, we restrict all jumps to the same direction, and take the shortest
jump sequence, in terms of number of nodes jumped over.
518
Daume? and Marcu Alignments for Automatic Document Summarization
Figure 6
The syntactic tree for an example document sentence.
investigations of the data) also suggest that synonymy is an important factor to take into
consideration in a successful rewrite model. We account for each of these four factors in
four separate components of the model and then take a linear interpolation of them to
produce the final probability:
rewrite(s | d) = ?idrewriteid(s | d) + ?stemrewritestem(s | d) (2)
+ ?wnrewritewn(s | d) + ?rwrewriterw(s | d) (3)
where the ?s are constrained to sum to unity. The four rewrite distributions used are: id
is a word identity model, which favors alignment of identical words; stem is a model
designed to capture the notion that matches at the stem level are often sufficient for
alignment (i.e., walk and walked are likely to be aligned); wn is a rewrite model based
on similarity according to WordNet; and wr is the basic rewrite model, similar to a
translation table in machine translation. These four models are described in detail in
this section, followed by a description of how to compute their ?s during EM.
4.2.1 Word Identity Rewrite Model. The form of the word identity rewrite model is:
rewriteid(s | d) = ?s=d. That is, the probability is 1 exactly when s and d are identical,
and 0 when they differ. This model has no parameters.
4.2.2 Stem Identity Rewrite Model. The form of the stem identity rewrite model is
very similar to that of the word identity model:
rewritestem(s | d) = 1Zd
?|s|=|d|
|s|
?
i=1
?stem(si )=stem(di ) (4)
That is, the probability of a phrase s given d is uniform over all phrases s? that
match d up to stem (and are of the same length, i.e., |s?| = |d|), and zero otherwise. The
519
Computational Linguistics Volume 31, Number 4
normalization constant is computed offline based on a pre-computed vocabulary. This
model also has no parameters.
4.2.3 WordNet Rewrite Model. In order to account for synonymy, we allow document
phrases to be rewritten to semantically ?related? summary phrases. To compute the
value for rewritewn(s | d), we first require that both s and d can be found in WordNet. If
either cannot be found, then the probability is zero. If they both can be found, then
the graph distance between their first senses is computed (we traverse the hyper-
nymy tree up until they meet). If the two paths do not meet, then the probability is
again taken to be zero. We place an exponential model on the hypernym tree-based
distance:
rewritewn(s | d) = 1Zd
exp [??dist(s, d)] (5)
Here, dist is calculated distance, taken to be +? whenever either of the failure
conditions is met. The single parameter of this model is ?, which is computed according
to the maximum likelihood criterion from the expectations during training. The nor-
malization constant Zd is calculated by summing over the exponential distribution for
all s? that occur on the summary side of our corpus.
4.2.4 Lexical Rewrite Model. The lexical rewrite model is the ?catch all? model to
handle the cases not handled by the above models. It is analogous to a translation-table
(t-table) in statistical machine translation (we will continue to use this terminology
for the remainder of the article), and simply computes a matrix of (fractional) counts
corresponding to all possible phrase pairs. Upon normalization, this matrix gives the
rewrite distribution.
4.2.5 Estimation of the Weight Parameters. In order to weight the four models, we
need to estimate values for the ? components. This computation can be performed
inside of the EM iterations by considering for each rewritten pair its expectation of
belonging to each of the models. We use these expectations to maximize the likelihood
with respect to the ?s and then normalize them so they sum to one.
4.3 Model Priors
In the standard HMM case, the learning task is simply one of parameter estimation,
wherein the maximum likelihood criterion under which the parameters are typically
trained performs well. However, in our model, we are, in a sense, simultaneously
estimating parameters and selecting a model: The model selection is taking place at the
level of deciding how to segment the observed summary. Unfortunately, in such model
selection problems, likelihood increases monotonically with model complexity. Thus,
EM will find for us the most complex model; in our case, this will correspond to a
model in which the entire summary is produced at once, and no generalization will be
possible.
This suggests that a criterion other than maximum likelihood (ML) is more
appropriate. We advocate the maximum a posteriori (MAP) criterion in this case. While
520
Daume? and Marcu Alignments for Automatic Document Summarization
ML optimizes the probability of the data given the parameters (the likelihood), MAP
optimizes the product of the probability of the parameters with the likelihood (the
unnormalized posterior). The difficulty in our model that makes ML estimation perform
poorly is centered in the lexical rewrite model. Under ML estimation, we will simply
insert an entry in the t-table for the entire summary for some uncommon or unique
document word and are done. However, a priori we do not believe that such a parameter
is likely. The question then becomes how to express this in a way that inference remains
tractable.
From a statistical point of view, the t-table is nothing but a large multinomial model
(technically, one multinomial for each possible document phrase). Under a multinomial
distribution with parameter ? with J-many components (with all ?j positive and sum-
ming to one), the probability of an observation x is given by p (x | ?) =
?J
j=1 ?
xj
j (here, we
consider x to be a vector of length J in which all components are zero except for one,
corresponding to the actual observation).
This distribution belongs to the exponential family and therefore has a natural conju-
gate distribution. Informally, two distributions are conjugate if you can multiply them
together and get the original distribution back. In the case of the multinomial, the conju-
gate distribution is the Dirichlet distribution. A Dirichlet distribution is parameterized
by a vector ? of length J with ?j ? 0, but not necessarily summing to one. The Dirichlet
distribution can be used as a prior distribution over multinomial parameters and has
density:
p (? | ?) =
?
(
?J
j=1 ?j
)
?J
j=1 ?(?j)
?J
j=1
?
?j?1
j .
The fraction before the product is simply a normalization term that ensures that the
integral over all possible ? integrates to one.
The Dirichlet is conjugate to the multinomial because when we compute the
posterior of ? given ? and x, we arrive back at a Dirichlet distribution: p (? | x,?) ?
p (x | ?) p (? | ?) ?
?J
j=1 ?
xj+?j?1
j . This distribution has the same density as the original
model, but a ?fake count? of ?j ? 1 has been added to component j. This means that
if we are able to express our prior beliefs about the multinomial parameters found
in the t-table in the form of a Dirichlet distribution, the computation of the MAP
solution can be performed exactly as described before, but with the appropriate fake
counts added to the observed variables (in our case, the observed variables are the
alignments between a document phrase and a summary phrase). The application of
Dirichlet priors to standard HMMs has previously been considered in signal process-
ing (Gauvain and Lee 1994). These fake counts act as a smoothing parameter, sim-
ilar to Laplace smoothing (Laplace smoothing is the special case where ?j = 2 for
all j).
In our case, we believe that singleton rewrites are worth 2 fake counts, that lexical
identity rewrites are worth 4 fake counts and that stem identity rewrites are worth
3 fake counts. Indeed, since a singleton alignment between identical words satisfies
all of these criteria, it will receive a fake count of 9. The selection of these counts is
intuitive, but clearly arbitrary. However, this selection was not ?tuned? to the data to
get better performance. As we will discuss later, inference in this model over the sizes
of documents and summaries we consider is quite computationally expensive. As is
appropriate, we specified this prior according to our prior beliefs, and left the rest to the
inference mechanism.
521
Computational Linguistics Volume 31, Number 4
4.4 Parameter Initialization
We initialize all the parameters uniformly, but in the case of the rewrite parameters,
since there is a prior on them, they are effectively initialized to the maximum likelihood
solution under their prior.
5. Experimental Results
The experiments we perform are on the same Ziff-Davis corpus described in the intro-
duction. In order to judge the quality of the alignments produced, we compare them
against the gold-standard references annotated by the humans. The standard precision
and recall metrics used in information retrieval are modified slightly to deal with the
sure and possible alignments created during the annotation process. Given the set S
of sure alignments, the set S ? P of possible alignments, and a set A of hypothesized
alignments, we compute the precision as |A ? P|/|A| and the recall as |A ? S|/|S|.
One problem with these definitions is that phrase-based models are fond of making
phrases. That is, when given an abstract containing the man and a document also
containing the man, a human will align the to the and man to man. However, a phrase-
based model will almost always prefer to align the entire phrase the man to the man. This
is because it results in fewer probabilities being multiplied together.
To compensate for this, we define soft precision (SoftP in the tables) by counting
alignments where a b is aligned to a b the same as ones in which a is aligned to a and b is
aligned to b. Note, however, that this is not the same as a aligned to a b and b aligned to
b. This latter alignment will, of course, incur a precision error. The soft precision metric
induces a new, soft F-Score, labeled SoftF.
Often, even humans find it difficult to align function words and punctuation. A list
of 58 function words and punctuation marks that appeared in the corpus (henceforth
called the ignore-list) was assembled. We computed precision and recall scores both on
all words and on all words that do not appear in the ignore-list.
5.1 Systems Compared
Overall, we compare various parameter settings of our model against three other sys-
tems. First, we compare against two alignment models developed in the context of
machine translation. Second, we compare against the Cut and Paste model developed in
the context of ?summary decomposition? by Jing (2002). Each of these systems will be
discussed in more detail shortly. However, the machine translation alignment models
assume sentence pairs as input. Moreover, even though the semi-Markov model is based
on efficient dynamic programming techniques, it is still too inefficient to run on very
long ?document, abstract? pairs.
To alleviate both of these problems, we preprocess our ?document, abstract? corpus
down to an ?extract, abstract? corpus, and then subsequently apply our models to this
smaller corpus (see Figure 7). In our data, doing so does not introduce significant
noise. To generate the extracts, we paired each abstract sentence with three sentences
from the corresponding document, selected using the techniques described by Marcu
(1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated
by a human. Each pair was ranked as 0 (document sentences contain little to none
of the information in the abstract sentence), 1 (document sentences contain some of
the information in the abstract sentence) or 2 (document sentences contain all of the
522
Daume? and Marcu Alignments for Automatic Document Summarization
Figure 7
Pictorial representation of the conversion of the ?document, abstract? corpus to an ?extract,
abstract? corpus.
information). Of the 20 random examples, none were labeled as 0; 5 were labeled as 1;
and 15 were labeled as 2, giving a mean rating of 1.75. We refer to the resulting corpus
as the ?extract, abstract? corpus, statistics for which are shown in Table 3. Finally, for fair
comparison, we also run the Cut and Paste model only on the extracts.9
5.1.1 Machine Translation Models. We compare against several competing systems,
the first of which is based on the original IBM Model 4 for machine translation
(Brown et al 1993) and the HMM machine translation alignment model (Vogel, Ney,
and Tillmann 1996) as implemented in the GIZA++ package (Och and Ney 2003).
We modified the code slightly to allow for longer inputs and higher fertilities, but
otherwise made no changes. In all of these setups, 5 iterations of Model 1 were run,
followed by five iterations of the HMM model. For Model 4, 5 iterations of Model 4
were subsequently run.
In our model, the distinction between the summary and the document is clear, but
when using a model from machine translation, it is unclear which of the summary
and the document should be considered the source language and which should be
considered the target language. By making the summary the source language, we are
effectively requiring that the fertility of each summary word be very high, or that many
words are null generated (since we must generate all of the document). By making
the document the source language, we are forcing the model to make most document
words have zero fertility. We have performed experiments in both directions, but the
latter (document as source) performs better in general.
In order to seed the machine translation model so that it knows that word identity
is a good solution, we appended our corpus with sentence pairs consisting of one source
word and one target word, which were identical. This is common practice in the ma-
chine translation community when one wishes to cheaply encode knowledge from a
dictionary into the alignment model.
5.1.2 Cut and Paste Model. We also tested alignments using the Cut and Paste sum-
mary decomposition method (Jing 2002), based on a non-trainable HMM. Briefly,
9 Interestingly, the Cut and Paste method actually achieves higher performance scores when run on only
the extracts rather than the full documents.
523
Computational Linguistics Volume 31, Number 4
Table 3
Ziff-Davis extract corpus statistics.
Abstracts Extracts Documents
Documents 2033 2033
Sentences 13k 41k 82k
Words 261k 1M 2.5M
Unique words 14k 26k 42k
29k
Sentences/Doc 6.28 21.51 40.83
Words/Doc 128.52 510.99 1229.71
Words/Sent 20.47 23.77 28.36
the Cut and Paste HMM searches for long contiguous blocks of words in the
document and abstract that are identical (up to stem). The longest such sequences
are aligned. By fixing a length cutoff of n and ignoring sequences of length less
than n, one can arbitrarily increase the precision of this method. We found that
n = 2 yields the best balance between precision and recall (and the highest F-measure).
On this task, this model drastically outperforms the machine translation models.
5.1.3 The Semi-Markov Model. While the semi-HMM is based on a dynamic pro-
gramming algorithm, the effective search space in this model is enormous, even for
moderately sized ?document, abstract? pairs. The semi-HMM system was then trained
on this ?extract, abstract? corpus. We also restrict the state-space with a beam, sized at
50% of the unrestricted state-space. With this configuration, we run ten iterations of the
forward-backward algorithm. The entire computation time takes approximately 8 days
on a 128-node cluster computer.
We compare three settings of the semi-HMM. The first, semi-HMM-relative, uses
the relative movement jump table; the second, semi-HMM-Gaussian, uses the Gaussian
parameterized jump table; the third, semi-HMM-syntax, uses the syntax-based jump
model.
5.2 Evaluation Results
The results, in terms of precision, recall, and F-score, are shown in Table 4. The first three
columns are when these three statistics are computed over all words. The next three
columns are when these statistics are only computed over words that do not appear in
our ignore list of 58 stop words. Under the methodology for combining the two human
annotations by taking the union, either of the human scores would achieve a precision
and recall of 1.0. To give a sense of how well humans actually perform on this task, we
compare each human against the other.
As we can see from Table 4, none of the machine translation models is well suited
to this task, achieving, at best, an F-score of 0.298. The flipped models, in which the
document sentences are the source language and the abstract sentences are the target
language perform significantly better (comparatively). Since the MT models are not
symmetric, going the bad way requires that many document words have zero fertility,
which is difficult for these models to cope with.
524
Daume? and Marcu Alignments for Automatic Document Summarization
Table 4
Results on the Ziff-Davis corpus.
All Words Non-Stop Words
System SoftP Recall SoftF SoftP Recall SoftF
Human1 0.727 0.746 0.736 0.751 0.801 0.775
Human2 0.680 0.695 0.687 0.730 0.722 0.726
HMM (Sum=Src) 0.120 0.260 0.164 0.139 0.282 0.186
Model 4 (Sum=Src) 0.117 0.260 0.161 0.135 0.283 0.183
HMM (Doc=Src) 0.295 0.250 0.271 0.336 0.267 0.298
Model 4 (Doc=Src) 0.280 0.247 0.262 0.327 0.268 0.295
Cut and Paste 0.349 0.379 0.363 0.431 0.385 0.407
semi-HMM-relative 0.456 0.686 0.548 0.512 0.706 0.593
semi-HMM-Gaussian 0.328 0.573 0.417 0.401 0.588 0.477
semi-HMM-syntax 0.504 0.701 0.586 0.522 0.712 0.606
The Cut and Paste method performs significantly better, which is to be expected,
since it is designed specifically for summarization. As one would expect, this method
achieves higher precision than recall, though not by very much. The fact that the Cut
and Paste model performs so well, compared to the MT models, which are able to learn
non-identity correspondences, suggests that any successful model should be able to
take advantage of both, as ours does.
Our methods significantly outperform both the IBM models and the Cut and Paste
method, achieving a precision of 0.522 and a recall of 0.712, yielding an overall F-score of
0.606 when stop words are not considered. This is still below the human-against-human
F-score of 0.775 (especially considering that the true human-against-human scores are
1.0), but significantly better than any of the other models.
Among the three settings of our jump table, the syntax-based model performs
best, followed by the relative jump model, with the Gaussian model coming in worst
(though still better than any other approach). Inspecting Figure 2, the fact that the
Gaussian model does not perform well is not surprising; the data shown there is very
non-Gaussian. A double-exponential model might be a better fit, but it is unlikely that
such a model will outperform the syntax based model, so we did not perform this
experiment.
5.3 Error Analysis
The first mistake frequently made by our model is to not align summary words to null.
In effect, this means that our model of null-generated summary words is lacking. An
example of this error is shown in Example 1 in Figure 8. In this example, the model
has erroneously aligned from DOS in the abstract to from DOS in the document (the
error is shown in bold). This alignment is wrong because the context of from DOS in the
document is completely different from the context it appears in the summary. However,
the identity rewrite model has overwhelmed the locality model and forced this incor-
rect alignment. To measure the frequency of such errors, we have post-processed our
system?s alignments so that whenever a human alignment contains a null-generated
summary word, our model also predicts that this word is null-generated. Doing so will
not change our system?s recall, but it can improve the precision. Indeed, in the case
of the relative jump model, the precision jumps from 0.456 to 0.523 (F-score increases
525
Computational Linguistics Volume 31, Number 4
Figure 8
Erroneous alignments are in bold. (Top) Example of an error made by our model (from file
ZF207-585-936). From DOS should be null generated, but the model has erroneously aligned
it to an identical phrase that appeared 11 sentences earlier in the document. (Bottom) Error
(from ZF207-772-628); The DMP 300 should be aligned to the printer but is instead aligned to
a far-away occurrence of The DMP 300.
from 0.548 to 0.594) in the case of all words and from 0.512 to 0.559 (F-score increases
from 0.593 to 0.624). This corresponds to a relative improvement of roughly 8% F-score.
Increases in score for the syntax-based model are roughly the same.
The second mistake our model frequently makes is to trust the identity rewrite
model too strongly. This problem has to do either with synonyms that do not appear
frequently enough for the system to learn reliable rewrite probabilities, or with corefer-
ence issues, in which the system chooses to align, for instance, Microsoft to Microsoft,
rather than Microsoft to the company, as might be correct in context. As suggested
by this example, this problem is typically manifested in the context of coreferential
noun phrases. It is difficult to perform a similar analysis of this problem as for the
aforementioned problem (to achieve an upper bound on performance), but we can
provide some evidence. As mentioned before, in the human alignments, roughly 51% of
all aligned phrases are lexically identical. In the alignments produced by our model (on
the same documents), this number is 69%. In the case of stem identity, the hand-aligned
data suggests that stem identity should hold in 67% of the cases; in our alignments,
this number was 81%. An example of this sort of error is shown in Example 2 in
Figure 8. Here, the model has aligned The DMP 300 in the abstract to The DMP 300 in
the document, while it should have been aligned to the printer due to locality constraints
(note that the model also misses the (produces ? producing) alignment, likely as a side-
effect of it making the error depicted in bold).
In Table 5, we have shown examples of common errors made by our system (these
were randomly selected from a much longer list of errors). These examples are shown
out of their contexts, but in most cases, the error is clear even so. In the first column, we
show the summary phrase in question. In the second column, we show the document
phrase to which it should be aligned, and in the third column, we show the document
phrase that our model aligned it to (or null). In the right column, we classify the model?s
alignment as incorrect or partially correct.
The errors shown in Table 5 show several weaknesses of the model. For instance,
in the first example, it aligns to port with to port, which seems correct without con-
text, but the chosen occurrence of to port in the document is in the discussion of
a completely different porting process than that referred to in the summary (and is
526
Daume? and Marcu Alignments for Automatic Document Summarization
Table 5
Ten example phrase alignments from the hand-annotated corpus; the last column indicates
whether the semi-HMM correctly aligned this phrase.
Summary Phrase True Phrase Aligned Phrase Class
to port can port to port incorrect
OS - 2 the OS / 2 OS / 2 partial
will use will be using will using partial
word processing programs word processors word processing incorrect
consists of also includes null of partial
will test will also have to test will test partial
the potential buyer many users the buyers incorrect
The new software Crosstalk for Windows new software incorrect
are generally powered by run on null incorrect
Oracle Corp. the software publisher Oracle Corp. incorrect
several sentences away). The seventh and tenth examples (The new software and Oracle
Corp., respectively) show instances of the coreference error that occurs commonly.
6. Conclusion and Discussion
Currently, summarization systems are limited to either using hand-annotated data or
using weak alignment models at the granularity of sentences, which serve as suitable
training data only for sentence extraction systems. To train more advanced extraction
systems, such as those used in document compression models or in next-generation
abstraction models, we need to better understand the lexical correspondences between
documents and their human written abstracts. Our work is motivated by the desire to
leverage the vast number of ?document, abstract? pairs that are freely available on the
Internet and in other collections and to create word- and phrase-aligned ?document,
abstract? corpora automatically.
This article presents a statistical model for learning such alignments in a com-
pletely unsupervised manner. The model is based on an extension of a hidden Markov
model, in which multiple emissions are made in the course of one transition. We have
described efficient algorithms in this framework, all based on dynamic programming.
Using this framework, we have experimented with complex models of movement and
lexical correspondences. Unlike the approaches used in machine translation, where only
very simple models are used, we have shown how to efficiently and effectively leverage
such disparate knowledge sources and WordNet, syntax trees, and identity models.
We have empirically demonstrated that our model is able to learn the complex struc-
ture of ?document, abstract? pairs. Our system outperforms competing approaches, in-
cluding the standard machine translation alignment models (Brown et al 1993; Vogel,
Ney, and Tillmann 1996) and the state-of-the-art Cut and Paste summary alignment
technique (Jing 2002).
We have analyzed two sources of error in our model, including issues of null-
generated summary words and lexical identity. Within the model itself, we have already
suggested two major sources of error in our alignment procedure. Clearly more work
needs to be done to fix these problems. One approach that we believe will be particu-
larly fruitful would be to add a fifth model to the linearly interpolated rewrite model
based on lists of synonyms automatically extracted from large corpora. Additionally,
527
Computational Linguistics Volume 31, Number 4
investigating the possibility of including some sort of weak coreference knowledge into
the model might serve to help with the second class of errors made by the model.
One obvious aspect of our method that may reduce its general usefulness is the
computation time. In fact, we found that despite the efficient dynamic programming
algorithms available for this model, the state space and output alphabet are simply
so large and complex that we were forced to first map documents down to extracts
before we could process them (and even so, computation took roughly 1,000 processor
hours). Though we have not pursued it in this work, we do believe that there is
room for improvement computationally, as well. One obvious first approach would
be to run a simpler model for the first iteration (for example, Model 1 from machine
translation (Brown et al 1993), which tends to be very recall oriented) and use this to see
subsequent iterations of the more complex model. By doing so, one could recreate the
extracts at each iteration using the previous iteration?s parameters to make better and
shorter extracts. Similarly, one might only allow summary words to align to words found
in their corresponding extract sentences, which would serve to significantly speed up
training and, combined with the parameterized extracts, might not hurt performance.
A final option, but one that we do not advocate, would be to give up on phrases and
train the model in a word-to-word fashion. This could be coupled with heuristic phrasal
creation as is done in machine translation (Och and Ney 2000), but by doing this, one
completely loses the probabilistic interpretation that makes this model so pleasing.
Aside from computational considerations, the most obvious future effort along the
lines of this model is to incorporate it into a full document summarization system. Since
this can be done in many ways, including training extraction systems, compression
systems, headline generation systems, and even extraction systems, we left this to
future work so that we could focus specifically on the alignment task in this article.
Nevertheless, the true usefulness of this model will be borne out by its application to
true summarization tasks.
Acknowledgments
We wish to thank David Blei for helpful
theoretical discussions related to this project
and Franz Josef Och for sharing his technical
expertise on issues that made the
computations discussed in this paper
possible. We sincerely thank the anonymous
reviewers of an original conference version
of this article as well reviewers of this longer
version, all of whom gave very useful
suggestions. Some of the computations
described in this work were made possible
by the High Performance Computing Center
at the University of Southern California.
This work was partially supported by
DARPA-ITO grant N66001-00-1-9814, NSF
grant IIS-0097846, NSF grant IIS-0326276,
and a USC Dean Fellowship to Hal
Daume? III.
References
Aydin, Zafer, Yucel Altunbasak, and Mark
Borodovsky. 2004. Protein secondary
structure prediction with semi-Markov
HMMs. In Proceedings of the IEEE
International Conference on Acoustics, Speech
and Signal Processing (ICASSP), May 17?21.
Banko, Michele, Vibhu Mittal, and Michael
Witbrock. 2000. Headline generation based
on statistical translation. In Proceedings
of the Conference of the Association for
Computational Linguistics (ACL),
pages 318?325, Hong Kong, October 1?8.
Barzilay, Regina and Noemie Elhadad. 2003.
Sentence alignment for monolingual
comparable corpora. In Proceedings of
the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 25?32.
Baum, Leonard E. and J. E. Eagon. 1967. An
inequality with applications to statistical
estimation for probabilistic functions of
Markov processes and to a model of
ecology. Bulletins of the American
Mathematical Society, 73:360?363.
Baum, Leonard E. and Ted Petrie. 1966.
Statistical inference for probabilistic
functions of finite state Markov chains.
Annals of Mathematical Statistics,
37:1554?1563.
528
Daume? and Marcu Alignments for Automatic Document Summarization
Berger, Adam and Vibhu Mittal. 2000.
Query-relevant summarization using
FAQs. In Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 294?301, Hong Kong,
October 1?8.
Boyles, Russell A. 1983. On the convergence
of the EM algorithm. Journal of the Royal
Statistical Society, B(44):47?50.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Carletta, Jean. 1995. Assessing agreement
on classification tasks: The kappa
statistic. Computational Linguistics, 22(2):
249?254.
Carroll, John, Guido Minnen, Yvonne
Canning, Siobhan Devlin, and John Tait.
1998. Practical simplification of English
newspaper text to assist aphasic readers.
In Proceedings of the AAAI-98 Workshop on
Integrating Artificial Intelligence and
Assistive Technology.
Chandrasekar, Raman, Christy Doran, and
Srinivas Bangalore. 1996. Motivations and
methods for text simplification. In
Proceedings of the International Conference on
Computational Linguistics (COLING),
pages 1041?1044, Copenhagen,
Denmark.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Artificial Intelligence
(AAAI?97), pages 598?603, Providence, RI,
July 27?31.
Daume? III, Hal and Daniel Marcu. 2002. A
noisy-channel model for document
compression. In Proceedings of the
Conference of the Association for
Computational Linguistics (ACL),
pages 449?456.
Daume? III, Hal and Daniel Marcu. 2004. A
tree-position kernel for document
compression. In Proceedings of the Fourth
Document Understanding Conference (DUC
2004), Boston, MA, May 6?7.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, B39(1):1?37.
Edmundson, H. P. 1969. New methods in
automatic abstracting. Journal of the
Association for Computing Machinery,
16(2):264?285. Reprinted in Advances in
Automatic Text Summarization, I. Mani and
M. T. Maybury, (eds.).
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation. In
Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), Sapporo, Japan.
Ferguson, Jack D. 1980. Variable duration
models for speech. In Proceedings
of the Symposium on the Application of
Hidden Markov Models to Text
and Speech, pages 143?179, October,
Princeton, NJ.
Gales, Mark J. F. and Steve J. Young. 1993.
The theory of segmental hidden Markov
models. Technical report, Cambridge
University Engineering Department.
Gauvain, Jean-Luc and Chin-Hui Lee. 1994.
Maximum a-posteriori estimation for
multivariate Gaussian mixture
observations of Markov chains. IEEE
Transactions Speech and Audio Processing,
2:291?298.
Ge, Xianping and Padhraic Smyth. 2000.
Segmental semi-Markov models for
change-point detection with applications
to semiconductor manufacturing.
Technical report, University of California
at Irvine, March.
Gildea, Daniel. 2003. Loosely tree-based
alignment for machine translation. In
Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 80?87, Sapporo, Japan.
Grefenstette, Gregory. 1998. Producing
intelligent telegraphic text reduction to
provide an audio scanning service for
the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text
Summarization, pages 111?118, Stanford
University, Stanford, CA.
Jensen, J. L. W. V. 1906. Sur les fonctions
convexes et les ine?galite?s entre les valeurs
moyennes. Acta Mathematica, 30:175?193.
Jing, Hongyan. 2000. Sentence reduction for
automatic text summarization. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL),
pages 310?315, Seattle, WA.
Jing, Hongyan. 2002. Using hidden Markov
modeling to decompose human-written
summaries. Computational Linguistics,
28(4):527?544.
Knight, Kevin and Daniel Marcu. 2002.
Summarization beyond sentence
extraction: A probabilistic approach to
sentence compression. Artificial Intelligence,
139(1):91?107.
529
Computational Linguistics Volume 31, Number 4
Kupiec, Julian, Jan O. Pedersen, and Francine
Chen. 1995. A trainable document
summarizer. In Proceedings of the Annual
ACM Conference on Research and
Development in Information Retrieval,
pages 68?73.
Luhn, H. P. 1956. The automatic creation of
literature abstracts. In I. Mani and M.
Maybury, editors, Advances in Automatic
Text Summarization. MIT Press, Cambridge,
MA, pages 58?63.
Mahesh, Kavi. 1997. Hypertext summary
extraction for fast document browsing. In
Proceedings of the AAAI Spring Symposium
on Natural Language Processing for the World
Wide Web, pages 95?103, Stanford, CA.
Mani, Inderjeet. 2001. Automatic
Summarization, volume 3 of Natural
Language Processing. John Benjamins,
Amsterdam/Philadelphia.
Mani, Inderjeet and Mark Maybury, editors.
1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge,
MA.
Marcu, Daniel. 1999. The automatic
construction of large-scale corpora for
summarization research. In Proceedings of
the 22nd Conference on Research and
Development in Information Retrieval
(SIGIR?99), pages 137?144, Berkeley, CA,
August 15?19.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press, Cambridge, MA.
Mitchell, Carl D., Leah H. Jamieson, and
Mary P. Harper. 1995. On the complexity of
explicit duration HMMs. IEEE Transactions
on Speech and Audio Processing, 3(3).
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 440?447, October,
Hong Kong, China.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Ostendorf, Mari, Vassilis Digalakis, and
Owen Kimball. 1996. From HMMs to
segment models: A unified view of
stochastic modeling for speech
recognition. IEEE Transactions on Speech
and Audio Processing, 4(5):360?378,
September.
Quirk, Chris, Chris Brockett, and William
Dolan. 2004. Monolingual machine
translation for paraphrase generation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 142?149, Barcelona, Spain.
Schwartz, Richard, David Zajic, and Bonnie
Dorr. 2002. Automatic headline generation
for newspaper stories. In Proceedings of the
Document Understanding Conference (DUC),
pages 78?85, Philadelphia.
Smyth, Padhraic, David Heckerman, and
Michael I. Jordan. 1997. Probabilistic
independence networks for hidden
Markov probability models. Neural
Computation, 9(2):227?269.
Teufel, Simone and Mark Moens. 1997.
Sentence extraction as a classification task.
In In ACL/EACL-97 Workshop on Intelligent
and Scalable Text Summarization,
pages 58?65.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of the International Conference
on Computational Linguistics (COLING),
pages 836?841.
Wu, Jeff C. F. 1983. On the convergence
properties of the EM algorithm. The Annals
of Statistics, 11:95?103.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the Conference of the
Association for Computational Linguistics
(ACL), pages 523?530.
Zajic, David, Bonnie Dorr, and Richard
Schwartz. 2004. BBN/UMD at DUC-2004:
Topiary. In Proceedings of the Fourth
Document Understanding Conference (DUC
2004), pages 112?119, Boston, MA,
May 6?7.
530
Statistical Phrase-Based Translation
Philipp Koehn, Franz Josef Och, Daniel Marcu
Information Sciences Institute
Department of Computer Science
University of Southern California
koehn@isi.edu, och@isi.edu, marcu@isi.edu
Abstract
We propose a new phrase-based translation
model and decoding algorithm that enables
us to evaluate and compare several, previ-
ously proposed phrase-based translation mod-
els. Within our framework, we carry out a
large number of experiments to understand bet-
ter and explain why phrase-based models out-
perform word-based models. Our empirical re-
sults, which hold for all examined language
pairs, suggest that the highest levels of perfor-
mance can be obtained through relatively sim-
ple means: heuristic learning of phrase trans-
lations from word-based alignments and lexi-
cal weighting of phrase translations. Surpris-
ingly, learning phrases longer than three words
and learning phrases from high-accuracy word-
level alignment models does not have a strong
impact on performance. Learning only syntac-
tically motivated phrases degrades the perfor-
mance of our systems.
1 Introduction
Various researchers have improved the quality of statis-
tical machine translation system with the use of phrase
translation. Och et al [1999]?s alignment template model
can be reframed as a phrase translation system; Yamada
and Knight [2001] use phrase translation in a syntax-
based translation system; Marcu and Wong [2002] in-
troduced a joint-probability model for phrase translation;
and the CMU and IBM word-based statistical machine
translation systems1 are augmented with phrase transla-
tion capability.
Phrase translation clearly helps, as we will also show
with the experiments in this paper. But what is the best
1Presentations at DARPA IAO Machine Translation Work-
shop, July 22-23, 2002, Santa Monica, CA
method to extract phrase translation pairs? In order to
investigate this question, we created a uniform evaluation
framework that enables the comparison of different ways
to build a phrase translation table.
Our experiments show that high levels of performance
can be achieved with fairly simple means. In fact,
for most of the steps necessary to build a phrase-based
system, tools and resources are freely available for re-
searchers in the field. More sophisticated approaches that
make use of syntax do not lead to better performance. In
fact, imposing syntactic restrictions on phrases, as used in
recently proposed syntax-based translation models [Ya-
mada and Knight, 2001], proves to be harmful. Our ex-
periments also show, that small phrases of up to three
words are sufficient for obtaining high levels of accuracy.
Performance differs widely depending on the methods
used to build the phrase translation table. We found ex-
traction heuristics based on word alignments to be better
than a more principled phrase-based alignment method.
However, what constitutes the best heuristic differs from
language pair to language pair and varies with the size of
the training corpus.
2 Evaluation Framework
In order to compare different phrase extraction methods,
we designed a uniform framework. We present a phrase
translation model and decoder that works with any phrase
translation table.
2.1 Model
The phrase translation model is based on the noisy chan-
nel model. We use Bayes rule to reformulate the transla-
tion probability for translating a foreign sentence
 
into
English  as
argmax	
 

argmax 
 
 





This allows for a language model 


and a separate
translation model 
 
 


.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 48-54
                                                         Proceedings of HLT-NAACL 2003
During decoding, the foreign input sentence
 
is seg-
mented into a sequence of   phrases 


. We assume a
uniform probability distribution over all possible segmen-
tations.
Each foreign phrase 

in 

 is translated into an En-
glish phrase



. The English phrases may be reordered.
Phrase translation is modeled by a probability distribution
	
 
 



 

. Recall that due to the Bayes rule, the translation
direction is inverted from a modeling standpoint.
Reordering of the English output phrases is modeled
by a relative distortion probability distribution 
 





, where 

denotes the start position of the foreign
phrase that was translated into the  th English phrase, and

 denotes the end position of the foreign phrase trans-
lated into the 
 

th English phrase.
In all our experiments, the distortion probability distri-
bution 
 


is trained using a joint probability model (see
Section 3.3). Alternatively, we could also use a simpler
distortion model 
 
  


 ffSyntax-based Alignment of Multiple Translations: Extracting Paraphrases
and Generating New Sentences
Bo Pang
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
pabo@cs.cornell.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292 USA
{knight,marcu}@isi.edu
Abstract
We describe a syntax-based algorithm that au-
tomatically builds Finite State Automata (word
lattices) from semantically equivalent transla-
tion sets. These FSAs are good representa-
tions of paraphrases. They can be used to ex-
tract lexical and syntactic paraphrase pairs and
to generate new, unseen sentences that express
the same meaning as the sentences in the input
sets. Our FSAs can also predict the correctness
of alternative semantic renderings, which may
be used to evaluate the quality of translations.
1 Introduction
In the past, paraphrases have come under the scrutiny
of many research communities. Information retrieval re-
searchers have used paraphrasing techniques for query re-
formulation in order to increase the recall of information
retrieval engines (Sparck Jones and Tait, 1984). Natural
language generation researchers have used paraphrasing
to increase the expressive power of generation systems
(Iordanskaja et al, 1991; Lenke, 1994; Stede, 1999).
And researchers in multi-document text summarization
(Barzilay et al, 1999), information extraction (Shinyama
et al, 2002), and question answering (Lin and Pantel,
2001; Hermjakob et al, 2002) have focused on identi-
fying and exploiting paraphrases in the context of recog-
nizing redundancies, alternative formulations of the same
meaning, and improving the performance of question an-
swering systems.
In previous work (Barzilay and McKeown, 2001; Lin
and Pantel, 2001; Shinyama et al, 2002), paraphrases
are represented as sets or pairs of semantically equiva-
lent words, phrases, and patterns. Although this is ade-
quate in the context of some applications, it is clearly too
weak from a generative perspective. Assume, for exam-
ple, that we know that text pairs (stock market rose, stock
market gained) and (stock market rose, stock prices rose)
have the same meaning. If we memorized only these two
pairs, it would be impossible to infer that, in fact, con-
sistent with our intuition, any of the following sets of
phrases are also semantically equivalent: {stock market
rose, stock market gained, stock prices rose, stock prices
gained } and {stock market, stock prices } in the con-
text of rose or gained; {market rose }, {market gained
}, {prices rose } and {prices gained } in the context of
stock; and so on.
In this paper, we propose solutions for two problems:
the problem of paraphrase representation and the problem
of paraphrase induction. We propose a new, finite-state-
based representation of paraphrases that enables one to
encode compactly large numbers of paraphrases. We also
propose algorithms that automatically derive such repre-
sentations from inputs that are now routinely released in
conjunction with large scale machine translation evalu-
ations (DARPA, 2002): multiple English translations of
many foreign language texts. For instance, when given
as input the 11 semantically equivalent English transla-
tions in Figure 1, our algorithm automatically induces the
FSA in Figure 2, which represents compactly 49 distinct
renderings of the same semantic meaning. Our FSAs
capture both lexical paraphrases, such as {fighting, bat-
tle}, {died, were killed} and structural paraphrases such
as {last week?s fighting, the battle of last week}. The
contexts in which these are correct paraphrases are also
conveniently captured in the representation.
In previous work, Langkilde and Knight (1998) used
word lattices for language generation, but their method
involved hand-crafted rules. Bangalore et al (2001) and
Barzilay and Lee (2002) both applied the technique of
multi-sequence alignment (MSA) to align parallel cor-
pora and produced similar FSAs. For their purposes,
they mainly need to ensure the correctness of consensus
among different translations, so that different constituent
orderings in input sentences do not pose a serious prob-
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 102-109
                                                         Proceedings of HLT-NAACL 2003
1. At least 12 people were killed in the battle last week. 2. At least 12 people lost their lives in last week?s fighting.
3. Last week?s fight took at least 12 lives. 4. The fighting last week killed at least 12.
5. The battle of last week killed at least 12 persons. 6. At least 12 persons died in the fighting last week.
7. At least 12 died in the battle last week. 8. At least 12 people were killed in the fighting last week.
9. During last week?s fighting, at least 12 people died. 10. Last week at least twelve people died in the fighting.
11. Last week?s fighting took the lives of twelve people.
Figure 1: Sample Sentence Group from the Chinese-English DARPA Evaluation Corpus: 11 English translations of
the same Chinese sentence.
 
 
at
 
during
 last
 
the
 
least
 
last
 
week
 
battle
 
fighting
 
 were
 
died
 lost
killed
 
in
 
their
 12
persons
*e*
people
 
the
  
last
 
weekbattle
fighting
 
lives
 
in
 
last
 
week
 
fighting
?s
 
at
 
?s
 
least
 
fighting
fight
 
 
died
 
in
 
peopletwelve
the
 
 
at
 
least
 
died
 
week
 
fighting?s
 
people12
 
killed
took
 
the
 
at
of
 
last
week  
lives
 
least
 
of
 
twelve people
 
12
lives
persons
*e*
Figure 2: FSA produced by our syntax-based alignment algorithm from the input in Figure 1.
  
*e*
 the
 
during
*e*
fighting
 
battle
 last
 
*e*
week
weeks
 
fight
fighting
*e*
 
killed
of took
*e*
 
the
 
at
 
lives
 
least
of
 
twelve
 
12
 
people
persons
*e*
 
lives
died
*e*
 in
 
*e*
the  
*e*
battle
fighting
 
*e*
 
last
weeks
week
 
fighting
*e*
*e*
 
people
 
lost
 
were
their
killed
Figure 3: FSA produced by a Multi-Sequence Alignment algorithm from the input in Figure 1.
lem. In contrast, we want to ensure the correctness of
all paths represented by the FSAs, and direct application
of MSA in the presence of different constituent orderings
can be problematic. For example, when given as input the
same sentences in Figure 1, one instantiation of the MSA
algorithm produces the FSA in Figure 3, which contains
many ?bad? paths such as the battle of last week?s fight-
ing took at least 12 people lost their people died in the
fighting last week?s fighting (See Section 4.2.2 for a more
quantitative analysis.). It?s still possible to use MSA if,
for example, the input is pre-clustered to have the same
constituent ordering (Barzilay and Lee (2003)). But we
chose to approach this problem from another direction.
As a result, we propose a new syntax-based algorithm to
produce FSAs.
In this paper, we first introduce the multiple transla-
tion corpus that we use in our experiments (see Section
2). We then present the algorithms that we developed to
induce finite-state paraphrase representations from such
data (see Section 3). An important part of the paper is
dedicated to evaluating the quality of the finite-state rep-
resentations that we derive (see Section 4). Since our rep-
resentations encode thousands and sometimes millions of
equivalent verbalizations of the same meaning, we use
both manual and automatic evaluation techniques. Some
of the automatic evaluations we perform are novel as
well.
2 Data
The data we use in this work is the LDC-available
Multiple-Translation Chinese (MTC) Corpus1 developed
for machine translation evaluation, which contains 105
news stories (993 sentences) from three sources of jour-
nalistic Mandarin Chinese text. These stories were inde-
pendently translated into English by 11 translation agen-
cies. Each sentence group, which consists of 11 semanti-
cally equivalent translations, is a rich source for learning
lexical and structural paraphrases. In our experiments,
we use 899 of the sentence groups ? the sentence groups
with sentences longer than 45 words were dropped.
3 A Syntax-Based Alignment Algorithm
Our syntax-based alignment algorithm, whose pseu-
docode is shown in Figure 4, works in three steps. In the
first step (lines 1-5 in Figure 4), we parse every sentence
in a sentence group and merge all resulting parse trees
into a parse forest. In the second step (line 6), we extract
1Linguistic Data Consortium (LDC) Catalog Number
LDC2002T01, ISBN 1-58563-217-1.
1. ParseForest = 
2. foreach s ? SentenceGroup
3. t = parseTree(s);
4. ParseForest = Merge(ParseForest, t);
5. endfor
6. Extract FSA from ParseForest;
7. Squeeze FSA;
Figure 4: The Syntax-Based Alignment Algorithm.
an FSA from the parse forest and then we compact it fur-
ther using a limited form of bottom-up alignment, which
we call squeezing (line 7). In what follows, we describe
each step in turn.
Top-down merging. Given a sentence group, we pass
each of the 11 sentences to Charniak?s (2000) parser to
get 11 parse trees. The first step in the algorithm is to
merge these parse trees into one parse-forest-like struc-
ture using a top-down process.
Let?s consider a simple case in which the parse for-
est contains one single tree, Tree 1 in Figure 5, and we
are adding Tree 2 to it. Since the two trees correspond
to sentences that have the same meaning and since both
trees expand an S node into an NP and a V P , it is rea-
sonable to assume that NP1 is a paraphrase of NP2 and
V P1 is a paraphrase of V P2. We merge NP1 with NP2
and V P1 with V P2 and continue the merging process on
each of the subtrees recursively, until we either reach the
leaves of the trees or the two nodes that we examine are
expanded using different syntactic rules.
When we apply this process to the trees in Figure 5,
the NP nodes are merged all the way down to the leaves,
and we get ?12? as a paraphrase of ?twelve? and ?people?
as a paraphrase of ?persons?; in contrast, the two V P s
are expanded in different ways, so no merging is done
beyond this level, and we are left with the information
that ?were killed? is a paraphrase of ?died?.
We repeat this top-down merging procedure with each
of the 11 parse trees in a sentence group. So far, only
constituents with same syntactic type are treated as para-
phrases. However, later we shall see that we can match
word spans whose syntactic types differ.
Keyword checking. The matching process described
above appears quite strict ? the expansions must match
exactly for two nodes to be merged. But consider the fol-
lowing parse trees:
1.(S (NP1 people)(V P1 were killed in this battle))
2.(S (NP2 this battle)(V P2 killed people))
If we applied the algorithm described above, we would
mistakenly align NP1 with NP2 and V P1 with V P2 ?
the algorithm described so far makes no use of lexical
12
twelve
people
persons were killed
died
Merge
Linearization
Tree 1 Tree 2
Parse Forest
FSA / Word Lattice
BEG END
+
S
NP VP
CD12 NNpersons AUXwere VP
VBkilled
S
NP VP
CD
twelve NNpeople VBdied
NP VP
CD NN AUX VPVB
12
twelve
people
persons
...
were
...killed...died
Figure 5: Top-down merging of parse trees and FSA ex-
traction.
information.
To prevent such erroneous alignments, we also imple-
ment a simple keyword checking procedure. We note
that since the word ?battle? appears in both V P1 and
NP2, this can serve as an evidence against the merging of
(NP1, NP2) and (V P1, V P2). A similar argument can
be constructed for the word ?people?. So in this exam-
ple we actually have double evidence against merging; in
general, one such clue suffices to stop the merging.
Our keyword checking procedure acts as a filter. A list
of keywords is maintained for each node in a syntactic
tree. This list contains all the nouns, verbs, and adjectives
that are spanned by a syntactic node. Before merging two
nodes, we check to see whether the keyword lists asso-
ciated with them share words with other nodes. That is,
supposed we just merged nodes A and B, and they are ex-
panded with the same syntactic rule into A1A2...An and
B1B2...Bn respectively; before we merge each Ai with
Bi, we check for each Bi if its keyword list shares com-
mon words with any Aj (j 6= i). If they do not, we con-
tinue the top-down merging process; otherwise we stop.
  
detroit
 a
 
building
 
detroit
 
detroit
 
a
 building
building
 
in
 
?s
 building
 
building
 
reduced
 
to
 
rubble
flattened
razed
 
was
 
blasted
leveled
razed
 
razed
 
leveled
 into
 
to
detroitbuilding
 
to down  
the ground
ashes
ground
 
the ground
  
levelled
 
to
 
in detroit ground
a. Before squeezing
 
 
detroit
 a
*e*  
?s
*e*
 
building  
building
 
reduced
 
*e*
was
 
flattened
 
blasted
leveled
 levelled
 
to
razed
 
leveled
*e*
 
into
 
to  
to
rubble
 
in detroit
down ashes
the
*e*
ground
b. After squeezing
Figure 6: Squeezing effect
In our current implementation, a pair of synonyms can
not stop an otherwise legitimate merging, but it?s possi-
ble to extend our keyword checking process with the help
of lexical resources such as WordNet in future work.
Mapping Parse Forests into Finite State Automata.
The process of mapping Parse Forests into Finite State
Automata is simple. We simply traverse the parse forest
top-down and create alternative paths for every merged
node. For example, the parse forest in Figure 5 is mapped
into the FSA shown at the bottom of the same figure. In
the FSA, there is a word associated with each edge. Dif-
ferent paths between any two nodes are assumed to be
paraphrases of each other. Each path that starts from the
BEGIN node and ends at the END node corresponds
to either an original input sentence or a paraphrase sen-
tence.
Squeezing. Since we adopted a very strict matching
criterion in top-down merging, a small difference in the
syntactic structure of two trees prevents some legitimate
mergings from taking place. This behavior is also exacer-
bated by errors in syntactic parsing. Hence, for instance,
three edges labeled detroit at the leftmost of the top FSA
in Figure 6 were kept apart. To compensate for this ef-
fect, our algorithm implements an additional step, which
we call squeezing. If two different edges that go into (or
out of) the same node in an FSA are labeled with the same
word, the nodes on the other end of the edges are merged.
We apply this operation exhaustively over the FSAs pro-
duced by the top-down merging procedure. Figure 6 il-
lustrates the effect of this operation: the FSA at the top
of this figure is compressed into the more compact FSA
shown at the bottom of it. Note that in addition to reduc-
ing the redundant edges, this also gives us paraphrases
not available in the FSA before squeezing (e.g. {reduced
to rubble, blasted to ground}). Therefore, the squeezing
operation, which implements a limited form of lexically
driven alignment similar to that exploited by MSA algo-
rithms, leads to FSAs that have a larger number of paths
and paraphrases.
4 Evaluation
The evaluation for our finite state representations and al-
gorithm requires careful examination. Obviously, what
counts as a good result largely depends on the applica-
tion one has in mind. If we are extracting paraphrases for
question-reformulation, it doesn?t really matter if we out-
put a few syntactically incorrect paraphrases, as long as
we produce a large number of semantically correct ones.
If we want to use the FSA for MT evaluation (for exam-
ple, comparing a sentence to be evaluated with the pos-
sible paths in FSA), we would want all paths to be rela-
tively good (which we will focus on in this paper), while
in some other applications, we may only care about the
quality of the best path (not addressed in this paper). Sec-
tion 4.1 concentrates on evaluating the paraphrase pairs
that can be extracted from the FSAs built by our system,
while Section 4.2 is dedicated to evaluating the FSAs di-
rectly.
4.1 Evaluating paraphrase pairs
4.1.1 Human-based evaluation of paraphrases
By construction, different paths between any two
nodes in the FSA representations that we derive are para-
phrases (in the context in which the nodes occur). To
evaluate our algorithm, we extract paraphrases from our
FSAs and ask human judges to evaluate their correctness.
We compare the paraphrases we collect with paraphrases
that are derivable from the same corpus using a co-
training-based paraphrase extraction algorithm (Barzilay
and McKeown, 2001). To the best of our knowledge, this
is the most relevant work to compare against since it aims
at extracting paraphrase pairs from parallel corpus. Un-
like our syntax-based algorithm which treats a sentence
as a tree structure and uses this hierarchical structural in-
formation to guide the merging process, their algorithm
treats a sentence as a sequence of phrases with surround-
ing contexts (no hierarchical structure involved) and co-
trains classifiers to detect paraphrases and contexts for
paraphrases. It would be interesting to compare the re-
sults from two algorithms so different from each other.
For the purpose of this experiment, we randomly se-
lected 300 paraphrase pairs (Ssyn) from the FSAs pro-
duced by our system. Since the co-training-based al-
gorithm of Barzilay and McKeown (2001) takes paral-
lel corpus as input, we created out of the MTC corpus
55 ? 993 sentence pairs (Each equivalent translation set
of cardinality 11 was mapped into
(11
2
)
equivalent trans-
lation pairs.). Regina Barzilay kindly provided us the list
of paraphrases extracted by their algorithm from this par-
allel corpus, from which we randomly selected another
set of 300 paraphrases (Scotr).
Correct Partial Incorrect
Ssyn 85% 12% 3%
Judge 1 Scotr 68% 13% 19%
Ssyn 80% 13% 7%
Judge 2 Scotr 63% 13% 24%
Ssyn 81% 5% 13%
Judge 3 Scotr 68% 3% 29%
Ssyn 77% 17% 5%
Judge 4 Scotr 68% 16% 16%
Average of Ssyn 81% 12% 7%
All Judges Scotr 66% 11% 22%
Table 1: A comparison of the correctness of the para-
phrases produced by the syntax-based alignment (Ssyn)
and co-training-based (Scotr) algorithms.
The resulting 600 paraphrase pairs were mixed and
presented in random order to four human judges. Each
judge was asked to assess the correctness of 150 para-
phrase pairs (75 pairs from each system) based on the
context, i.e., the sentence group, from which the para-
phrase pair was extracted. Judges were given three
choices: ?Correct?, for perfect paraphrases, ?Partially
correct?, for paraphrases in which there is only a par-
tial overlap between the meaning of two paraphrases (e.g.
while {saving set, aid package} is a correct paraphrase
pair in the given context, {set, aide package} is consid-
ered partially correct), and ?Incorrect?. The results of the
evaluation are presented in Table 1.
Although the four evaluators were judging four differ-
ent sets, each clearly rated a higher percentage of the out-
puts produced by the syntax-based alignment algorithm
as ?Correct?. We should note that there are parameters
specific to the co-training algorithm that we did not tune
to work for this particular corpus. In addition, the co-
training algorithm recovered more paraphrase pairs: the
syntax-based algorithm extracted 8666 pairs in total with
1051 of them extracted at least twice (i.e. more or less
reliable), while the numbers for the co-training algorithm
is 2934 out of a total of 16993 pairs. This means we are
not comparing the accuracy on the same recall level.
Aside from evaluating the correctness of the para-
phrases, we are also interested in the degree of overlap
between the paraphrase pairs discovered by the two algo-
rithms so different from each other. We find that out of
the 1051 paraphrase pairs that were extracted from more
than one sentence group by the syntax-based algorithm,
62.3% were also extracted by the co-training algorithm;
and out of the 2934 paraphrase pairs from the results of
co-training algorithm, 33.4% were also extracted by the
syntax-based algorithm. This shows that in spite of the
very different cues the two different algorithms rely on,
range of ASL 1-10 10-20 20-30 30-45
recall 30.7% 16.3% 7.8% 3.8%
Table 2: Recall of WordNet-consistent synonyms.
they do discover a lot of common pairs.
4.1.2 WordNet-based analysis of paraphrases
In order to (roughly) estimate the recall (of lexical syn-
onyms) of our algorithm, we use the synonymy relation
in WordNet to extract all the synonym pairs present in
our corpus. This extraction process yields the list of all
WordNet-consistent synonym pairs that are present in our
data. (Note that some of the pairs identified as synonyms
by WordNet, like ?follow/be?, are not really synonyms in
the contexts defined in our data set, which may lead to
artificial deflation of our recall estimate.) Once we have
the list of WordNet-consistent paraphrases, we can check
how many of them are recovered by our method. Table 2
gives the percentage of pairs recovered for each range of
average sentence length (ASL) in the group.
Not surprisingly, we get higher recall with shorter sen-
tences, since long sentences tend to differ in their syn-
tactic structures fairly high up in the parse trees, which
leads to fewer mergings at the lexical level. The recall
on the task of extracting lexical synonyms, as defined
by WordNet, is not high. But after all, this is not what
our algorithm has been designed for. It?s worth notic-
ing that the syntax-based algorithm also picks up many
paraphrases that are not identified as synonyms in Word-
Net. Out of 3217 lexical paraphrases that are learned by
our system, only 493 (15.3%) are WordNet synonyms,
which suggests that paraphrasing is a much richer and
looser relation than synonymy. However, the WordNet-
based recall figures suggest that WordNet can be used as
an additional source of information to be exploited by our
algorithm.
4.2 Evaluating the FSA directly
We noted before that apart from being a natural represen-
tation of paraphrases, the FSAs that we build have their
own merit and deserve to be evaluated directly. Since our
FSAs contain large numbers of paths, we design auto-
matic evaluation metrics to assess their qualities.
4.2.1 Language Model-based evaluation
If we take our claims seriously, each path in our FSAs
that connects the start and end nodes should correspond to
a well-formed sentence. We are interested in both quan-
tity (how many sentences our automata are able to pro-
duce) and quality (how good these sentences are). To an-
swer the first question, we simply count the number of
paths produced by our FSAs.
average N (# of paths) logN
length max ave max ave
1 - 10 22749 775 10.0 5.2
10 - 20 172386 4468 12.1 6.2
20 - 30 3479544 29202 15.1 5.8
30 - 45 684589 4135 13.4 4.5
Table 3: Statistics on Number of Paths in FSAs
random variable mean std. dev
ent(FSA)? ent(SG) ?0.11586 1.25162
ent(MTS)? ent(SG) 1.74259 1.05749
Table 4: Quality judged by LM
Table 3 gives the statistics on the number of paths pro-
duced by our FSAs, reported by the average length of
sentences in the input sentence groups. For example, the
sentence groups that have between 10 and 20 words pro-
duce, on average, automata that can yield 4468 alterna-
tive, semantically equivalent formulations.
Note that if we always get the same degree of merging
per word across all sentence groups, the number of paths
would tend to increase with the sentence length. This is
not the case here. Apparently we are getting less merg-
ing with longer sentences. But still, given 11 sentences,
we are capable of generating hundreds, thousands, and in
some cases even millions of sentences.
Obviously, we should not get too happy with our abil-
ity to boost the number of equivalent meanings if they are
incorrect. To assess the quality of the FSAs generated by
our algorithm, we use a language model-based metric.
We train a 4-gram model over one year of the Wall
Street Journal using the CMU-Cambridge Statistical Lan-
guage Modeling toolkit (v2). For each sentence group
SG, we use this language model to estimate the aver-
age entropy of the 11 original sentences in that group
(ent(SG)). We also compute the average entropy of
all the sentences in the corresponding FSA built by our
syntax-based algorithm (ent(FSA)). As the statistics in
Table 4 show, there is little difference between the av-
erage entropy of the original sentences and the average
entropy of the paraphrase sentences we produce. To bet-
ter calibrate this result, we compare it with the average
entropy of 6 corresponding machine translation outputs
(ent(MTS)), which were also made available by LDC
in conjunction with the same corpus. As one can see, the
difference between the average entropy of the machine
produced output and the average entropy of the origi-
nal 11 sentences is much higher than the difference be-
tween the average entropy of the FSA-produced outputs
and the average entropy of the original 11 sentences. Ob-
viously, this does not mean that our FSAs only produce
well-formed sentences. But it does mean that our FSAs
produce sentences that look more like human produced
sentences than machine produced ones according to a lan-
guage model.
4.2.2 Word repetition analysis
Not surprisingly, the language model we used in Sec-
tion 4.2.1 is far from being a perfect judge of sentence
quality. Recall the example of ?bad? path we gave in Sec-
tion 1: the battle of last week?s fighting took at least 12
people lost their people died in the fighting last week?s
fighting. Our 4-gram based language model will not find
any fault with this sentence. Notice, however, that some
words (such as ?fighting? and ?people?) appear at least
twice in this path, although they are not repeated in any
of the source sentences. These erroneous repetitions in-
dicate mis-alignment. By measuring the frequency of
words that are mistakenly repeated, we can now examine
quantitatively whether a direct application of the MSA
algorithm suffers from different constituent orderings as
we expected.
For each sentence group, we get a list of words that
never appear more than once in any sentence in this
group. Given a word from this list and the FSA built
from this group, we count the total number of paths that
contain this word (C) and the number of paths in which
this word appears at least twice (Cr, i.e. number of er-
roneous repetitions). We define the repetition ratio to
be Cr/C, which is the proportion of ?bad? paths in this
FSA according to this word. If we compute this ra-
tio for all the words in the lists of the first 499 groups2
and the corresponding FSAs produced by an instantia-
tion of the MSA algorithm3, the average repetition ra-
tio is 0.0304992 (14.76% of the words have a non-zero
repetition ratio, and the average ratio for these words is
0.206671). In comparison, the average repetition ratio for
our algorithm is 0.0035074 (2.16% of the words have a
non-zero repetition ratio4, and the average ratio for these
words is 0.162309). The presence of different constituent
orderings does pose a more serious problem to the MSA
algorithm.
4.2.3 MT-based evaluation
Recently, Papineni et al (2002) have proposed an au-
tomatic MT system evaluation technique (the BLEU
score). Given an MT system output and a set of refer-
2MSA runs very slow for longer sentences, and we believe
using the first 499 groups should be enough to make our point.
3We thank Regina Barzilay for providing us this set of re-
sults
4Note that FSAs produced right after keyword checking will
not yield any non-zero repetition ratio. However, if there are
mis-alignment not prevented by keyword checking in an FSA,
it may contain paths with erroneous repetition of words after
squeezing.
range 0-1 1-2 2-3 3-4 4-5
count 546 256 80 15 2
Table 5: Statistics for edgain
ence translations, one can estimate the ?goodness? of the
MT output by measuring the n-gram overlap between the
output and the reference set. The higher the overlap, i.e.,
the closer an output string is to a set of reference transla-
tions, the better a translation it is.
We hypothesize that our FSAs provide a better repre-
sentation against which the outputs of MT systems can
be evaluated because they encode not just a few but thou-
sands of equivalent semantic formulations of the desired
meaning. Ideally, if the FSAs we build accept all and
only the correct renderings of a given meaning, we can
just give a test sentence to the reference FSA and see if
it is accepted by it. Since this is not a realistic expecta-
tion, we measure the edit distance between a string and
an FSA instead: the smaller this distance is, the closer it
is to the meaning represented by the FSA.
To assess whether our FSAs are more appropriate rep-
resentations for evaluating the output of MT systems, we
perform the following experiment. For each sentence
group, we hold out one sentence as test sentence, and try
to evaluate how much of it can be predicted from the other
10 sentences. We compare two different ways of estimat-
ing the predictive power. (a) we compute the edit distance
between the test sentence and the other 10 sentences in
the set. The minimum of this distance is ed(input). (b)
we use dynamic programming to efficiently compute the
minimum distance (ed(FSA)) between the test sentence
and all the paths in the FSA built from the other 10 sen-
tences. The smaller the edit distance is, the better we
are predicting a test sentence. Mathematically, the differ-
ence between these two measures ed(input)? ed(FSA)
characterizes how much is gained in predictive power by
building the FSA.
We carry out the experiment described above in a
?leave-one-out? fashion (i.e. each sentence serves as
a test sentence once). Now let edgain be the average
of ed(input) ? ed(FSA) over the 11 runs for a given
group. We compute this for all 899 groups and find the
mean for edgain to be 0.91 (std. dev = 0.78). Table 5
gives the count for groups whose edgain falls into the
specified range. We can see that the majority of edgain
falls under 2.
We are also interested in the relation between the pre-
dictive power of the FSAs and the number of reference
translations they are derived from. For a given group, we
randomly order the sentences in it, set the last one as the
test sentence, and try to predict it with the first 1, 2, 3,
... 10 sentences. We investigate whether more sentences
ed(FSAn) ed(inputn)
?ed(FSA10) ?ed(FSAn)
n mean std. dev mean std. dev
1 5.65 3.86 0 0
2 3.66 3.02 0.19 0.60
3 2.71 2.55 0.33 0.76
4 2.10 2.33 0.46 0.90
5 1.56 2.01 0.56 0.95
6 1.18 1.79 0.65 1.02
7 0.79 1.48 0.75 1.09
8 0.49 1.10 0.81 1.11
9 0.21 0.74 0.89 1.16
10 0 0 0.93 1.21
Table 6: Effect of monotonically increasing the number
of reference sentences
yield an increase in the predictive power.
Let ed(FSAn) be the edit distance from the test sen-
tence to the FSA built on the first n sentences; similarly,
let ed(inputn) be the minimum edit distance from the
test sentence to an input set that consists of only the first
n sentences. Table 6 reports the effect of using differ-
ent number of reference translations. The first column
shows that each translation is contributing to the predic-
tive power of our FSA. Even when we add the tenth trans-
lation to our FSA, we still improve its predictive power.
The second column shows that the more sentences we add
to the FSA the larger the difference between its predic-
tive power and that of a simple set. The results in Table 6
suggest that our FSA may be used in order to refine the
BLEU metric (Papineni et al, 2002).
5 Conclusion & Future Work
In this paper, we presented a new syntax-based algorithm
that learns paraphrases from a newly available dataset.
The multiple translation corpus that we use in this paper
is the first instance in a series of similar corpora that are
built and made publicly available by LDC in the context
of a series of DARPA-sponsored MT evaluations. The
algorithm we proposed constructs finite state represen-
tations of paraphrases that are useful in many contexts:
to induce large lists of lexical and structural paraphrases;
to generate semantically equivalent renderings of a given
meaning; and to estimate the quality of machine transla-
tion systems. More experiments need to be carried out
in order to assess extrinsically whether the FSAs we pro-
duce can be used to yield higher agreement scores be-
tween human and automatic assessments of translation
quality.
In our future work, we wish to experiment with more
flexible merging algorithms and to integrate better the
top-down and bottom-up processes that are used to in-
duce FSAs. We also wish to extract more abstract para-
phrase patterns from the current representation. Such pat-
terns are more likely to get reused ? which would help us
get reliable statistics for them in the extraction phase, and
also have a better chance of being applicable to unseen
data.
Acknowledgments
We thank Hal Daume? III, Ulrich Germann, and Ulf Herm-
jakob for help and discussions; Eric Breck, Hubert Chen,
Stephen Chong, Dan Kifer, and Kevin O?Neill for par-
ticipating in the human evaluation; and the Cornell NLP
group and the reviewers for their comments on this pa-
per. We especially want to thank Regina Barzilay and
Lillian Lee for many valuable suggestions and help at var-
ious stages of this work. Portions of this work were done
while the first author was visiting Information Sciences
Institute. This work was supported by the Advanced
Research and Development Activity (ARDA)?s Advance
Question Answering for Intelligence (AQUAINT) Pro-
gram under contract number MDA908-02-C-0007, the
National Science Foundation under ITR/IM grant IIS-
0081334 and a Sloan Research Fellowship to Lillian Lee.
Any opinions, findings, and conclusions or recommen-
dations expressed above are those of the authors and do
not necessarily reflect the views of the National Science
Foundation or the Sloan Foundation.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Workshop on
Automatic Speech Recognition and Understanding.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of the ACL/EACL, pages 50?57.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
ACL, pages 550?557.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the NAACL.
DARPA. 2002. In DARPA IAO Machine Translation
Workshop, Santa Monica, CA, July 22-23.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answer-
ing. In Proceedings of the Text Retrieval Conference
(TREC?2002). November.
Lidija Iordanskaja, Richard Kittredge, and Alain Polge?re.
1991. Lexical selection and paraphrase in a meaning-
text generation model. In Ce?cile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293?312. Kluwer Aca-
demic Publisher.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of of ACL/COLING.
Nils Lenke. 1994. Anticipating the reader?s problems
and the automatic generation of paraphrases. In Pro-
ceedings of the 15th International Conference on Com-
putational Linguistics, volume 1, pages 319?323, Ky-
oto, Japan, August 5?9.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Proceedings
of ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining 2001, pages 323?328.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 124?127, San Diego, CA, March 24-27.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of the Hu-
man Language Technology Conference (HLT?02), San
Diego, CA, March 24-27. Poster presentation.
Karen Sparck Jones and John I. Tait. 1984. Automatic
search term variant generation. Journal of Documen-
tation, 40(1):50?66.
Manfred Stede. 1999. Lexical Semantics and
Knowledge Representation in Multilingual Text
Generation. Kluwer Academic Publishers,
Boston/Dordrecht/London.
Sentence Level Discourse Parsing using Syntactic and Lexical Information
Radu Soricut and Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
  radu, marcu  @isi.edu
Abstract
We introduce two probabilistic models that can
be used to identify elementary discourse units
and build sentence-level discourse parse trees.
The models use syntactic and lexical features.
A discourse parsing algorithm that implements
these models derives discourse parse trees with
an error reduction of 18.8% over a state-of-
the-art decision-based discourse parser. A set
of empirical evaluations shows that our dis-
course parsing model is sophisticated enough
to yield discourse trees at an accuracy level that
matches near-human levels of performance.
1 Introduction
By exploiting information encoded in human-produced
syntactic trees (Marcus et al, 1993), research on prob-
abilistic models of syntax has driven the performance of
syntactic parsers to about 90% accuracy (Charniak, 2000;
Collins, 2000). The absence of semantic and discourse
annotated corpora prevented similar developments in se-
mantic/discourse parsing. Fortunately, recent annotation
projects have taken signicant steps towards developing
semantic (Fillmore et al, 2002; Kingsbury and Palmer,
2002) and discourse (Carlson et al, 2003) annotated cor-
pora. Some of these annotation efforts have already had
a computational impact. For example, Gildea and Juraf-
sky (2002) developed statistical models for automatically
inducing semantic roles. In this paper, we describe proba-
bilistic models and algorithms that exploit the discourse-
annotated corpus produced by Carlson et al (2003).
A discourse structure is a tree whose leaves correspond
to elementary discourse units (edu)s, and whose internal
nodes correspond to contiguous text spans (called dis-
course spans). An example of a discourse structure is
the tree given in Figure 1. Each internal node in a dis-
course tree is characterized by a rhetorical relation, such
it will use its network
The bank also says
1
2 3
[2,3]
ATTRIBUTION
to channel investments.
[ ]
[ ] [ ]
1
2 3
ENABLEMENT
Figure 1: Discourse structure of a sentence.
as ATTRIBUTION and ENABLEMENT. Within a rhetorical re-
lation a discourse span is also labeled as either NUCLEUS
or SATELLITE. The distinction between nuclei and satel-
lites comes from the empirical observation that a nucleus
expresses what is more essential to the writer?s purpose
than a satellite. Discourse trees can be represented graph-
ically in the style shown in Figure 1. The arrows link the
satellite to the nucleus of a rhetorical relation. Arrows are
labeled with the name of the rhetorical relation that holds
between the linked units. Horizontal lines correspond to
text spans, and vertical lines identify text spans which are
nuclei.
In this paper, we introduce two probabilistic models
that can be used to identify elementary discourse units
and build sentence-level discourse parse trees. We show
how syntactic and lexical information can be exploited in
the process of identifying elementary units of discourse
and building sentence-level discourse trees. Our evalu-
ation indicates that the discourse parsing model we pro-
pose is sophisticated enough to achieve near-human lev-
els of performance on the task of deriving sentence-level
discourse trees, when working with human-produced
syntactic trees and discourse segments.
2 The Corpus
For the experiments described in this paper, we use a pub-
licly available corpus (RST-DT, 2002) that contains 385
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 149-156
                                                         Proceedings of HLT-NAACL 2003
Wall Street Journal articles from the Penn Treebank. The
corpus comes conveniently partitioned into a Training set
of 347 articles (6132 sentences) and a Test set of 38 ar-
ticles (991 sentences). Each document in the corpus is
paired with a discourse structure (tree) that was manually
built in the style of Rhetorical Structure Theory (Mann
and Thompson, 1988). (See (Carlson et al, 2003) for de-
tails concerning the corpus and the annotation process.)
Out of the 385 articles in the corpus, 53 have been inde-
pendently annotated by two human annotators. We used
this doubly-annotated subset to compute human agree-
ment on the task of discourse structure derivation. In our
experiments we used as discourse structures only the dis-
course sub-trees spanning over individual sentences.
Because the discourse structures had been built on top
of sentences already associated with syntactic trees from
the Penn Treebank, we were able to create a composite
corpus which allowed us to perform an empirically driven
syntax-discourse relationship study. This composite cor-
pus was created by associating each sentence  in the dis-
course corpus with its corresponding Penn Treebank syn-
tactic parse tree 
	 and its correspond-
ing sentence-level discourse tree Cognates Can Improve Statistical Translation Models
Grzegorz Kondrak
Department of Computing Science
University of Alberta
221 Athabasca Hall
Edmonton, AB, Canada T6G 2E8
kondrak@cs.ualberta.edu
Daniel Marcu and Kevin Knight
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
marcu,knight@isi.edu
Abstract
We report results of experiments aimed at im-
proving the translation quality by incorporating
the cognate information into translation mod-
els. The results confirm that the cognate iden-
tification approach can improve the quality of
word alignment in bitexts without the need for
extra resources.
1 Introduction
In the context of machine translation, the term cognates
denotes words in different languages that are similar
in their orthographic or phonetic form and are possible
translations of each other. The similarity is usually due
either to a genetic relationship (e.g. English night and
German nacht) or borrowing from one language to an-
other (e.g. English sprint and Japanese supurinto). In
a broad sense, cognates include not only genetically re-
lated words and borrowings but also names, numbers, and
punctuation. Practically all bitexts (bilingual parallel cor-
pora) contain some kind of cognates. If the languages are
represented in different scripts, a phonetic transcription
or transliteration of one or both parts of the bitext is a
pre-requisite for identifying cognates.
Cognates have been employed for a number of bitext-
related tasks, including sentence alignment (Simard et
al., 1992), inducing translation lexicons (Mann and Ya-
rowsky, 2001), and improving statistical machine trans-
lation models (Al-Onaizan et al, 1999). Cognates are
particularly useful when machine-readable bilingual dic-
tionaries are not available. Al-Onaizan et al (1999) ex-
perimented with using bilingual dictionaries and cog-
nates in the training of Czech?English translation mod-
els. They found that appending probable cognates to the
training bitext significantly lowered the perplexity score
on the test bitext (in some cases more than when using a
bilingual dictionary), and observed improvement in word
alignments of test sentences.
In this paper, we investigate the problem of incorpo-
rating the potentially valuable cognate information into
the translation models of Brown et al (1990), which, in
their original formulation, consider lexical items in ab-
straction of their form. For training of the models, we
use the GIZA program (Al-Onaizan et al, 1999). A list
of likely cognate pairs is extracted from the training cor-
pus on the basis of orthographic similarity, and appended
to the corpus itself. The objective is to reinforce the co-
ocurrence count between cognates in addition to already
existing co-ocurrences. The results of experiments con-
ducted on a variety of bitexts show that cognate iden-
tification can improve word alignments, which leads to
better translation models, and, consequently, translations
of higher quality. The improvement is achieved without
modifying the statistical training algorithm.
2 The method
We experimented with three word similarity measu-
res: Simard?s condition, Dice?s coefficient, and LCSR.
Simard et al (1992) proposed a simple condition for de-
tecting probable cognates in French?English bitexts: two
words are considered cognates if they are at least four
characters long and their first four characters are iden-
tical. Dice?s coefficient is defined as the ratio of the
number of shared character bigrams to the total num-
ber of bigrams in both words. For example, colour and
couleur share three bigrams (co, ou, and ur), so their
Dice?s coefficient is 6
11
' 0:55. The Longest Common
Subsequence Ratio (LCSR) of two words is computed
by dividing the length of their longest common subse-
quence by the length of the longer word. For example,
LCSR(colour,couleur) = 5
7
' 0:71, as their longest com-
mon subsequence is ?c-o-l-u-r?.
In order to identify a set of likely cognates in a tok-
enized and sentence-aligned bitext, each aligned segment
is split into words, and all possible word pairings are
stored in a file. Numbers and punctuation are not con-
sidered, since we feel that they warrant a more specific
approach. After sorting and removing duplicates, the file
represents all possible one-to-one word alignments of the
bitext. Also removed are the pairs that include English
function words, and words shorter than the minimum
length (usually set at four characters). For each word pair,
a similarity measure is computed, and the file is again
sorted, this time by the computed similarity value. If the
measure returns a non-binary similarity value, true cog-
nates are very frequent near the top of the list, and be-
come less frequent towards the bottom. The set of likely
cognates is obtained by selecting all pairs with similarity
above a certain threshold. Typically, lowering the thresh-
old increases recall while decreasing precision of the set.
Finally, one or more copies of the resulting set of likely
cognates are concatenated with the training set.
3 Experiments
We induced translation models using IBM Model 4
(Brown et al, 1990) with the GIZA toolkit (Al-Onaizan
et al, 1999). The maximum sentence length in the train-
ing data was set at 30 words. The actual translations
were produced with a greedy decoder (Germann et al,
2001). For the evaluation of translation quality, we used
the BLEU metric (Papineni et al, 2002), which measures
the n-gram overlap between the translated output and one
or more reference translations. In our experiments, we
used only one reference translation.
3.1 Word alignment quality
In order to directly measure the influence of the added
cognate information on the word alignment quality, we
performed a single experiment using a set of 500 man-
ually aligned sentences from Hansards (Och and Ney,
2000). Giza was first trained on 50,000 sentences from
Hansards, and then on the same training set augmented
with a set of cognates. The set consisted of two copies of
a list produced by applying the threshold of 0:58 to LCSR
list. The duplication factor was arbitrarily selected on the
basis of earlier experiments with a different training and
test set taken from Hansards.
The incorporation of the cognate information resulted
in a 10% reduction of the word alignment error rate,
from 17.6% to 15.8%, and a corresponding improvement
in both precision and recall. An examination of ran-
domly selected alignments confirms the observation of
Al-Onaizan et al (1999) that the use of cognate informa-
tion reduces the tendency of rare words to align to many
co-occurring words.
In another experiment, we concentrated on co-oc-
curring identical words, which are extremely likely to
represent mutual translations. In the baseline model,
links were induced between 93.6% of identical words. In
the cognate-augmented model, the ratio rose to 97.2%.
3.2 Europarl
Europarl is a tokenized and sentence-aligned multilingual
corpus extracted from the Proceedings of the European
0.202
0.203
0.204
0.205
0.206
0.207
0.208
0 1 2 3 4 5 6
BL
EU
 s
co
re
Duplication factor
"Simard"
"DICE"
"LCSR"
Figure 1: BLEU scores as a function of the duplication
factor for five methods of cognates identification aver-
aged over nine language pairs.
Parliament (Koehn, 2002). The eleven official European
Union languages are represented in the corpus. We con-
sider the variety of languages as important for a valida-
tion of the cognate-based approach as general, rather than
language-specific.
As the training data, we arbitrarily selected a subset of
the corpus that consisted the proceedings from October
1998. By pairing English with the remaining languages,
we obtained nine bitexts1, each comprising about 20,000
aligned sentences (500,000 words). The test data con-
sisted of 1755 unseen sentences varying in length from 5
to 15 words from the 2000 proceedings (Koehn, 2002).
The English language model was trained separately on a
larger set of 700,000 sentences from the 1996 proceed-
ings.
Figure 1 shows the BLEU scores as a function of the
duplication factor for three methods of cognates identi-
fication averaged over nine language pairs. The results
averaged over a number of language pairs are more in-
formative than results obtained on a single language pair,
especially since the BLEU metric is only a rough approx-
imation of the translation quality, and exhibits consider-
able variance. Three different similarity measures were
compared: Simard, DICE with a threshold of 0.39, and
LCSR with a threshold of 0.58. In addition, we experi-
mented with two different methods of extending the train-
ing set with with a list of cognates: one pair as one sen-
tence (Simard), and thirty pairs as one sentence (DICE
and LCSR).2
1Greek was excluded because its non-Latin script requires a
different type of approach to cognate identification.
2In the vast majority of the sentences, the alignment links are
correctly induced between the respective cognates when multi-
Threshold Pairs Score
Baseline 0 0.2027
0.99 863 0.2016
0.71 2835 0.2030
0.58 5339 0.2058
0.51 7343 0.2073
0.49 14115 0.2059
Table 1: The number of extracted word pairs as a func-
tion of the LCSR threshold, and the corresponding BLEU
scores, averaged over nine Europarl bitexts.
The results show a statistically significant improve-
ment3 in the average BLEU score when the duplication
factor is greater than 1, but no clear trend can be discerned
for larger factors. There does not seem to be much differ-
ence between various methods of cognate identification.
Table 1 shows results of augmenting the training set
with different sets of cognates determined using LCSR.
A threshold of 0.99 implies that only identical word
pairs are admitted as cognates. The words pairs with
LCSR around 0.5 are more likely than not to be unre-
lated. In each case two copies of the cognate list were
used. The somewhat surprising result was that adding
only ?high confidence? cognates is less effective than
adding lots of dubious cognates. In that particular set
of tests, adding only identical word pairs, which almost
always are mutual translations, actually decreased the
BLEU score. Our results are consistent with the results
of Al-Onaizan et al (1999), who observed perplexity im-
provement even when ?extremely low? thresholds were
used. It seems that the robust statistical training algo-
rithm has the ability of ignoring the unrelated word pairs,
while at the same time utilizing the information provided
by the true cognates.
3.3 A manual evaluation
In order to confirm that the higher BLEU scores reflect
higher translation quality, we performed a manual evalua-
tion of a set of a hundred six-token sentences. The models
were induced on a 25,000 sentences portion of Hansards.
The training set was augmented with two copies of a cog-
nate list obtained by thresholding LCSR at 0.56. Results
ple pairs per sentence are added.
3Statistical significance was estimated in the following way.
The variance of the BLEU score was approximated by randomly
picking a sample of translated sentences from the test set. The
size of the test sample was equal to the size of the test set (1755
sentences). The score was computed in this way 200 times for
each language. The mean and the variance of the nine-language
average was computed by randomly picking one of the 200
scores for each language and computing the average. The mean
result produced was 0.2025, which is very close to the baseline
average score of 0.2027. The standard deviation of the average
was estimated to be 0.0018, which implies that averages above
0.2054 are statistically significant at the 0.95 level.
Evaluation Baseline Cognates
Completely correct 16 21
Syntactically correct 8 7
Semantically correct 14 12
Wrong 62 60
Total 100 100
Table 2: A manual evaluation of the translations gener-
ated by the baseline and the cognate-augmented models.
of a manual evaluation of the entire set of 100 sentences
are shown in Table 2. Although the overall translation
quality is low due to the small size of the training corpus
and the lack of parameter tuning, the number of com-
pletely acceptable translations is higher when cognates
are added.
4 Conclusion
Our experimental results show that the incorporation of
cognate information can improve the quality of word
alignments, which in turn result in better translations, In
our experiments, the improvement, although statistically
significant, is relatively small, which can be attributed to
the relative crudeness of the approach based on append-
ing the cognate pairs directly to the training data. In the
future, we plan to develop a method of incorporating the
cognate information directly into the training algorithm.
We foresee that the performance of such a method will
also depend on using more sophisticated word similarity
measures.
References
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,
D. Melamed, F. Och, D. Purdy, N. Smith, and D. Yarowsky.
1999. Statistical machine translation. Technical report,
Johns Hopkins University.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1990.
The mathematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics, 19(2):263?311.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada.
2001. Fast decoding and optimal decoding for machine
translation. In Proceedings of ACL-01.
P. Koehn. 2002. Europarl: A multilingual corpus for evaluation
of machine translation. In preparation.
G. Mann and D. Yarowsky. Multipath translation lexicon induc-
tion via bridge languages. In Proceedings of NAACL 2001.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of ACL-00.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings of ACL-02.
M. Simard, G. F. Foster, and P. Isabelle. 1992. Using cognates
to align sentences in bilingual corpora. In Proceedings of
TMI-92.
Evaluating Multiple Aspects of Coherence in Student Essays
Derrick Higgins
Educational
Testing Service
Jill Burstein
Educational
Testing Service
Daniel Marcu
University of Southern
California
/ Information Sciences
Institute
Claudia Gentile
Educational
Testing Service
Abstract
CriterionSM Online Essay Evaluation Service
includes a capability that labels sentences in
student writing with essay-based discourse el-
ements (e.g., thesis statements). We describe
a new system that enhances Criterion?s capa-
bility, by evaluating multiple aspects of co-
herence in essays. This system identifies fea-
tures of sentences based on semantic similarity
measures and discourse structure. A support
vector machine uses these features to capture
breakdowns in coherence due to relatedness
to the essay question and relatedness between
discourse elements. Intra-sentential quality is
evaluated with rule-based heuristics. Results
indicate that the system yields higher perfor-
mance than a baseline on all three aspects.
1 Overview
This work is motivated by a need for advanced discourse
analysis capabilities for writing instruction applications.
CriterionSM Online Essay Evaluation Service is an appli-
cation for writing instruction which includes a capability
to annotate sentences in student essays with discourse el-
ement labels. These labels include the categories Thesis
Statement, Main Idea, Supporting Idea, and Conclusion
(Burstein et al, 2003b). Though it accurately annotates
sentences with essay-based discourse labels, Criterion
does not provide an evaluation of the expressive quality
of the sentences that comprise a discourse segment. The
system might accurately label a student?s essay as hav-
ing all of the typically expected discourse elements: the-
sis statement, 3 main ideas, supporting evidence linked
to each main idea, and a conclusion. As teachers have
pointed out, however, an essay may have all of these or-
ganizational elements, but the quality of individual ele-
ments may need improvement.
In this paper, we present a capability that captures ex-
pressive quality of sentences in the discourse segments
of an essay. For this work, we have defined expressive
quality in terms of four aspects related to global and lo-
cal essay coherence. The first two dimensions capture
global coherence, and the latter two relate to local coher-
ence: a) relatedness to the essay question (topic), b) re-
latedness between discourse elements, c) intra-sentential
quality, and d) sentence-relatedness within a discourse
segment. Each dimension represents a different aspect
of coherence.
Essentially, the goal of the system is to be able to pre-
dict whether a sentence in a discourse segment has high
or low expressive quality with regard to a particular co-
herence dimension. We have deliberately developed an
approach to essay coherence that is comprised of multi-
ple dimensions, so that an instructional application may
provide appropriate feedback to student writers, based on
the system?s prediction of high or low for each dimen-
sion. For instance, sentences in the student?s thesis state-
ment may have a strong relationship to the essay topic,
but may have a number of serious grammatical errors that
make it hard to follow. For this student, we may want to
point out that on the one hand, the sentences in the thesis
address the topic, but the thesis statement as a discourse
segment might be more clearly stated if the grammar er-
rors were fixed. By contrast, the sentences that comprise
the student?s thesis statement may be grammatically cor-
rect, but only loosely related to the essay topic. For this
student, we would also want the system to provide ap-
propriate feedback to, so that the student could revise the
thesis statement text appropriately.
In earlier work, Foltz, Kintsch & Landauer (1998),
and Wiemer-Hastings & Graesser (2000) have devel-
oped systems that also examine coherence in student
writing. Their systems measure lexical relatedness be-
tween text segments by using vector-based similarity
between adjacent sentences. This linear approach to
similarity scoring is in line with the TextTiling scheme
(Hearst and Plaunt, 1993; Hearst, 1997), which may
be used to identify the subtopic structure of a text.
Miltsakaki and Kukich (2000) have also addressed the is-
sue of establishing the coherence of student essays, using
the Rough Shift element of Centering Theory. Again, this
previous work looks at the relatedness of adjacent text
segments, and does not explore global aspects of text co-
herence.
Hierarchical models of discourse have been applied to
the question of coherence (Mann and Thompson, 1986),
but so far these have been more useful in language gen-
eration than in determining how coherent a given text is,
or in identifying the specific problem, such as the break-
down of coherence in a document.
Our approach differs in fundamental ways from this
earlier work that deals with student writing. First, Foltz
et al (1998), Wiemer-Hastings and Graesser (2000),
and Miltsakaki and Kukich (2000) assume that text co-
herence is linear. They calculate the similarity between
adjacent segments of text. By contrast, our approach
considers the discourse structure in the text, following
Burstein et al (2003b). Our method considers sentences
with regard to their discourse segments, and how the sen-
tences relate to other text segments both inside (such as
the essay thesis) and outside (such as the essay topic) of a
document. This allows us to identify cases in which there
may be a breakdown in coherence due to more global as-
pects of essay-based discourse structure. Second, previ-
ous work has used Latent Semantic Analysis as a seman-
tic similarity measure (Landauer and Dumais, 1997). We
have adapted another vector-based method of semantic
representation: Random Indexing (Kanerva et al, 2000;
Sahlgren, 2001). Another difference between our sys-
tem and earlier systems is that we use essays manually
annotated on the four coherence dimensions to train our
system.
The final system employs a hybrid approach to classify
the first two of the four coherence dimensions with a high
or low quality rank. For these dimensions, a support vec-
tor machine is used to model features derived from Ran-
dom Indexing and from essay-based discourse structure
information. A third local coherence dimension compo-
nent is driven by rule-based heuristics. A fourth dimen-
sion related to coherence within a discourse segment can-
not be classified due to a lack of data characterizing low
expressive quality. This is fully explained later in the pa-
per.
2 Protocol Development and Human
Annotation
2.1 Protocol Development
The development of this system required a corpus of hu-
man annotated essay data for modeling purposes. In the
end, the goal is to have the system make judgments sim-
ilar to those made by a human with regard to ranking the
coherence of an essay on four dimensions. Therefore, we
created a detailed protocol for annotating the expressive
quality of essay-based discourse elements in essays with
regard to four aspects related to global and local essay
coherence. This protocol was designed for the following
purposes:
1. To yield annotations that are useful for the purpose
of providing students with feedback about the ex-
pressive relatedness of discourse elements in their
essays, given four relatedness dimensions;
2. To permit human annotators to achieve high levels
of consistency during the annotation process;
3. To produce annotations that have the potential of be-
ing derivable by computer programs through train-
ing on corpora annotated by humans.
2.1.1 Expressive Quality of Discourse Segments:
Protocol Description
According to writing experts who collaborated in this
work, the expressive relatedness of a sentence discourse
element may be characterized in terms of four dimen-
sions: a) relationship to prompt (essay question topic),
b) relationship to other discourse elements, c) relevance
with discourse segment, and d) errors in grammar, us-
age, and mechanics. For the sake of brevity, we refer to
these four dimensions as DimP (relatedness to prompt),
DimT (typically, relatedness to thesis), DimS (related-
ness within a discourse segment), and DimERR.
The two annotators were required to label each sen-
tence of an essay for expressive quality on the four di-
mensions (above). For the 989 essays used in this study,
each sentence had already been manually annotated with
these discourse labels: background material, thesis, main
idea, supporting idea, and conclusion (Burstein et al,
2003b).1 An assignment of high (1) or low (0) was given
to each sentence, on the dimensions relevant to the dis-
course element. Not all dimensions apply to all discourse
elements. The protocol is extremely specific as to how
annotators should label the expressive quality for each
sentence in a discourse element with regard to the four
dimensions. In this paper, we provide a brief description
of the labeling protocol, so that the purpose of each di-
mension is clear.
Figure 1 shows a sample essay and prompt. A hu-
man judge has assigned a label to each sentence in the
essay, resulting in the illustrated division into discourse
segments. In addition, the figure indicates human annota-
tors? ratings for two of our coherence dimensions (DimP
and DimT , discussed below). By and large, the essay
consistently follows up on the ideas of the essay thesis,
and so most sentences get a high relatedness score on
DimT . However, much of the essay fails to directly ad-
dress the question posed in the essay prompt, and so many
sentences are assigned low relatedness on DimP .
Dimension 1: DimP (Relatedness to Prompt)
The text of the discourse element and the prompt (text
of the essay question) must be related. Specifically, the
thesis statement, main ideas, and conclusion statement
should all contain text that is strongly related to the essay
topic. If this relationship does not exist, this is perhaps
evidence that the student has written an off-topic essay.
For this dimension, a high rank is assigned to each sen-
tence from background material, thesis, main idea and
conclusion statement that is related to the prompt text;
otherwise a low rank is assigned.
1The annotated data from the Burstein et al (2003b) study
were used to develop a commercial application that automati-
cally assigns these discourse labels to student essays.
Discourse Sentence DimP DimT
Segment
Prompt Images of beauty?both male and female?are promoted in magazines, in movies, on
billboards, and on television. Explain the extent to which you think these images can
be beneficial or harmful.
Background
A lot of people really care about how they look or how other people look. Low High
A lot of people like reading magazines or watch t.v about how you can fix your looks if
you don?t like the way your looks are. High High
Thesis
People that care about how they look is because they have problems at home, their parents
don?t pay attention to them or even that they have a high self-steem which that is not good. Low N/A
A lot of people get to the extent of killing themselfs just because they?re not happy with
there looks. Low N/A
Support Many people go thru make-overs to experiment how they will look but, some people stilldon?t like themself. N/A High
Main Point
The people that don?t like themselfs need some helps and they probably feel like that be-
cause they have told them oh! your ugly , you look like Blank! or maybe a guy never ask a
her out.
Low Low
Support
In case of a guy probably the same comments but he won?t dare to ask a girl out because
he feels that the girl is going to say no because of the way he looks. N/A High
Things like this make people don?t like each other. N/A High
Conclusion I suggest that a those people out here that are not happy with their looks get some help. Low HighTheirs alot of programs that you can get help. Low Low
Figure 1: Student essay with discourse segments and two coherence dimensions as annotated by human judge
Dimension 2: DimT (Relatedness to Thesis)
The relationship between a discourse element and
other discourse elements in the text governs the global
coherence of the essay text. For a text to hold together,
certain discourse elements must be related or the text will
appear choppy and will be difficult to follow. Specifi-
cally, a high rank is assigned to each sentence in the back-
ground material, main ideas and conclusion that is related
to the thesis, and supporting idea sentences that relate to
the relevant main idea. A conclusion sentence may also
be given a high rank if it is related to a main idea or back-
ground information. Low ranks are assigned to sentences
that do not have these relationships.
Dimension 3: DimS (Relatedness within Segment)
This dimension indicates the cohesiveness of the mul-
tiple sentences in a discourse segment of a text. This
dimension distinguishes a text segment that may go off
task within a discourse segment. For this dimension, a
high rank was assigned to each sentence in a discourse
segment that related to at least one other sentence in the
segment; otherwise the sentence received a low rank. If
the discourse segment contained only one sentence, then
the DimT label was assigned as the default.
Dimension 4: DimERR (Technical Errors)
Dimension 4 measures a sentence?s relatedness of ex-
pression with regard to grammar, mechanics and word
usage. More specifically, a sentence is considered to be
low on this dimension if it contains frequent patterns of
error, defined as follows: (a) contains 2 errors in gram-
mar, word usage or mechanics (i.e., spelling, capitaliza-
tion or punctuation), (b) is an incomplete sentence, or (c)
is a run-on sentence (i.e., 4 or more independent clauses
within a sentence).
2.2 Topics, Human Annotation, and Human
Agreement
2.2.1 Topics & Writing Genre
Essays written to two genres were used: five of the top-
ics were persuasive, and one was expository. Persuasive
writing requires the reader to state an opinion on a par-
ticular topic, support the stated opinion, and convince the
reader that the perspective is valid and well-supported.
An expository topic requires the writer only to state an
opinion on a topic. This typically elicits more personal
and descriptive writing. Four of the five sets of persua-
sive essay responses were written by college freshman,
and the fifth by 12th graders. The set of expository re-
sponses were also written by 12th graders.
2.2.2 Human Annotation
Two human judges participated in this study. The
judges were instructed to assign relevant dimension la-
bels to each sentence. Pre-training of the judges was done
using a set of approximately 50 essays across the six top-
ics in the study. During this phase, the authors and the
judges discussed and labeled the essays together. During
the next training phase, the judges labeled a total of 292
essays across six topics. They labeled the identical set of
essays, and were allowed to discuss their decisions. In the
next annotation phase, the judges did not discuss their an-
notations. In this post-training phase (annotation phase),
each judge labeled an average of about 278 unique es-
says for each of four prompts (556 essays together). Each
judge also labeled an additional set of 141 essays that was
overlapping. So, about 20 percent of the data annotated
by each judge in the annotation phase was overlapping,
Agreement ?
DimP (N=779) 99% .99
DimT (N=1890) 100% .99
DimS (N=2119) 100% .99
DimERR (N=2170) 99% .98
Table 1: Annotator agreement across coherence
dimensions?data from annotation phase
and 80 percent was unique. The 20 percent is used to ob-
tain human agreement.2 During both the training and an-
notation phases, Kappa statistics were run on their judg-
ments regularly, and if the Kappa for any particular cate-
gory fell below 0.8, then the judges were asked to review
the protocol until their agreement was acceptable. At the
end of the annotation phase, we had a total of 989 labeled
essays: 292 (training phase) + 278 ? 2 (unique essays
from annotator 1 + annotator 2, annotation phase) + 141
(overlapping set, annotation phase).
Human Judge Agreement
It is critical that the annotation process yields agree-
ment that is high enough between human judges, such
that it suggests that people can agree on how to categorize
the discourse elements. As is stated in the above section,
during the training of the judges for this study, Kappa
statistics were computed on a regular basis. Kappa be-
tween the judges for each category had to be maintained
at least 0.8, since this is believed to represent strong
agreement (Krippendorff, 1980). In Table 1 we report
human agreement for overlapping data from the four top-
ics on all four dimensions. Clearly, the level of human
agreement is quite high across all four coherence dimen-
sions. In addition, if we look at kappas of sentences based
on discourse category, no kappa falls below 0.9.
3 Method
Our final system uses a hybrid approach to label three of
the four coherence dimensions. For DimP and DimT ,
assigning coherence judgments to sentences in an essay
proceeds in three stages 1) identifying the discourse label
associated with each sentence in an essay, 2) computing
features that quantify the semantic similarity between dif-
ferent discourse segments of the essay, and 3) applying a
classifier to make a coherence judgment on a dimension.
Consistent with the human annotated data, a coherence
judgment on any dimension is either ?high? or ?low.? The
method for DimERR is rule-based, and is discussed later.
3.1 Discourse element feature identification
As noted earlier, the two human judges in this study anno-
tated the four coherence dimensions according to the hu-
2For the annotation phase, we were unable to collect data
for two essay prompts because of our annotators? availability.
This means that we only have inter-annotator agreement statis-
tics on 4 prompts, although some data from all six prompts was
available for training and testing our models (with the extra two
prompts being represented in the training phase of annotation).
man discourse label assignments. Accordingly, we also
used the human assigned discourse labels as features for
predicting coherence judgments. In a deployed system,
however, we would use discourse element labels gener-
ated from Criterion?s discourse analysis system (Burstein
et al, 2003b). Further evaluation is, of course, necessary
in order to determine the effect of using these automat-
ically assigned labels in place of the gold standard dis-
course labels.
3.2 Semantic similarity features
Given the partition of an essay into discourse segments,
we then derive a set of features from the essay in order
to predict how closely related each sentence is to various
important text segments, such as the essay topic, and dis-
course elements, such as thesis statement. As described
in Section 4, the features that are most useful for clas-
sifying sentences according to coherence are semantic
similarity features derived from Random Indexing (Kan-
erva et al, 2000; Sahlgren, 2001). Random Indexing is
a vector-based semantic representation system similar to
Latent Semantic Analysis. Our Random Indexing (RI)
semantic space is trained on about 30 million words of
newswire text.
When we extract a feature such as ?RI similarity to
prompt? for a sentence, this essentially measures to what
extent the sentence contains terms in the same semantic
domain as compared to those found in the prompt. Within
any discourse segment, any semantic information that is
word-order dependent is lost.
3.3 Support vector classification
Finally, for each sentence in the essay we use the fea-
tures derived from the essay to make a determination as
to whether it meets our criteria for coherence in these
dimensions (DimP and DimT ). To make this determi-
nation, we use a support vector machine (SVM) classi-
fier (Vapnik, 1995; Christianini and Shawe-Taylor, 2000).
Specifically, we use an SVM with a radial basis function
kernel, which exhibited good performance on a subset of
about 30 essays from the pre-training data.
4 Results
In each of the experiments below, the results are re-
ported for the entire set of 989 essays annotated for this
project. We performed ten-fold cross-validation, training
our SVM classifier on 910 of the data at a time, and testing
on the remaining 110 . We report the results on the cross-
validation set for all runs combined.
For each dimension, we also report the performance
of a simple baseline measure, which assumes that all of
our essay coherence criteria are satisfied. That is, our
baseline assigns category 1 (high relevance) to every
sentence, on every dimension.
These essays were written in response to six different
prompts, and had an average (human-assigned) score of
Score DimP DimT DimS DimERR
1?2 64.1% 71.2% 94.8% 61.1%
5?6 72.0% 70.9% 97.2% 92.9%
Table 2: Baseline performance on each coherence dimen-
sion, broken down by essay score point
4.0 on a six-point scale. Therefore, a priori, it seems pos-
sible that we could build a better baseline model by con-
ditioning its predictions on the overall score of the essay
(assigning 1?s to sentences from better-scoring essays,
and 0?s to sentences from lower-scoring essays). How-
ever, the coherence requirements of each of our dimen-
sions are usually met even in the lowest-scoring essays,
as shown in Table 2, which lists the percentage of sen-
tences in different essay score ranges which our human
annotators assigned category 1. Looking at the highest
and lowest score points on our six-point scale, it is clear
that higher-scoring essays do tend to have fewer problems
with coherence, but this effect is not overwhelming. (The
largest gap between the highest- and lowest-scoring es-
says is on DimERR, which deals with errors in grammar,
usage, and mechanics.)
4.1 DimP
According to the protocol, there are four discourse ele-
ments for which DimP , the degree of relatedness to the
essay prompt, is relevant: Background, Conclusion, Main
Point, and Thesis. The Supporting Idea category of sen-
tence is not required to be related to the prompt, because
it may express an elaboration of one of the main points of
the essay, and has a more tenuous and mediated logical
connection to the essay prompt text.
The features which we provide to the SVM for predict-
ing a sentence?s relatedness to the prompt are:
1. The RI similarity score of the target sentence with
the entire essay prompt,
2. The maximum RI similarity score of the target sen-
tence with any sentence in the essay prompt,
3. The RI similarity score of the target sentence with
the required task sentence (a designated portion of
the prompt text which contains an explicit directive
to the student to write about a specific topic),
4. The RI similarity score of the target sentence with
the entire thesis of the essay,
5. The maximum RI similarity score of the target sen-
tence with any sentence in the thesis,
6. The maximum RI similarity score of the target sen-
tence with any sentence in the preceding discourse
chunk,
7. The number of sentences in the current chunk,
8. The offset of the target sentence (sentence number)
from the beginning of the current discourse chunk,
9. The number of sentences in the current chunk whose
similarity with the prompt is greater than .2,
10. The number of sentences in the current chunk whose
similarity with the required task sentence is greater
than .2,
11. The number of sentences in the current chunk whose
similarity with the essay thesis is greater than .2,
12. The number of sentences in the current chunk whose
similarity with the prompt is greater than .4,
13. The number of sentences in the current chunk whose
similarity with the required task sentence is greater
than .4,
14. The number of sentences in the current chunk whose
similarity with the essay thesis is greater than .4,
15. The length of the target sentence in words,
16. A Boolean feature indicating whether the target sen-
tence contains a transition word, such as ?however?,
or ?although?,
17. A Boolean feature indicating whether the target sen-
tence contains an anaphoric element, and
18. The category of the current chunk. (This is encoded
as five Boolean features: one bit for each of ?Back-
ground?, ?Conclusion?, ?Main Point?, ?Supporting
Idea?, and ?Thesis?.)
In calculating features 2, 5, and 6, we use the maximum
similarity score of the sentence with any other sentence in
the relevant discourse segment, rather than simply using
the similarity score of the sentence with the entire text
chunk. We add this feature based on the intuition that for
a sentence to be relevant to another discourse segment, it
need only be connected to some part of that segment.
It is perhaps surprising that we include features which
measure the degree of similarity between the sentence
and the thesis, since we are trying to predict its related-
ness to the prompt, rather than the thesis. However, there
are two reasons we believe this is fruitful. First, since we
are dealing with a relatively small amount of text, com-
paring a single sentence to a short essay prompt, looking
at the thesis as well helps to overcome data sparsity is-
sues. Second, it may be that the relevance of the current
sentence to the prompt is mediated by the student?s thesis
statement. For example, the prompt may ask the student
to take a position on some topic. They may state this po-
sition in the thesis, and provide an example to support it
as one of their Main Points. In such a case, the example
would be more clearly linked to the Thesis, but this would
suffice for it to be related to the prompt.
Considering the similarity scores of sentences in the
current discourse segment is also, in part, an attempt to
overcome data sparsity issues, but is also motivated by
the idea that it may be an entire discourse segment which
can properly be said to be (ir)relevant to the essay prompt.
The sentence length and transition word features do
not directly reflect the relatedness of a sentence to the
prompt, but they are likely to be useful correlates.
Finally, the feature (#17) indicating the presence of
a pronoun is to help the system deal with cases in
which a sentence contains very few content words, but
is still linked to other material in the essay by means of
anaphoric elements, such as ?This is shown by my argu-
ment.? In such as case, the sentence would normally get
a low similarity score with the prompt (and other parts of
the essay), but the information that it contains a pronoun
might still allow the system to classify it correctly.
Table 3 shows results using the baseline algorithm to
classify sentences according to their relatedness to the
prompt. Table 4 presents the results using the SVM clas-
sifier. We provide precision, recall, and f-measure for the
assignment of the labels 1 and 0, and an overall accuracy
measure in the far right column. (The accuracy measure
is the value for precision and recall when 1 and 0 ranks
are collapsed. Precision and recall will be the same, since
the number of labels assigned by the model is equal to the
number of labels in the target assignment.)
The SVM model outperforms the baseline on every
subcategory, with the largest gains on Background sen-
tences, most of which are, in fact, unrelated to the prompt
according to our human judges. This low baseline result
on Background sentences could indicate that many stu-
dents have a problem with providing unnecessary and ir-
relevant prefaces to the important points in their essays.
Note that the trained SVM has around .9 recall on the
class of sentences which according to our human annota-
tors have high relevance to the prompt. This means that
our system is less likely to incorrectly assign a low rank
to a sentence that is high. So, the system will tend to err
on the side of the student, which is a preferable trade-off.
In part, this is due to the nature of the semantic similarity
measure we are using, which does not take word order
into account. While RI does allow us to capture a richer
meaning component than simply matching words which
co-occur in the target sentence and prompt, it still does
not encompass all that goes into determining whether a
sentence ?relates? to another chunk of text. Students of-
ten write something which bears a loose topical connec-
tion with the essay prompt, but does not directly address
the question. This sort of problem is hard to address with
a tool such as LSA or RI; the vocabulary of the sentence
on its own will not provide a clue to the sentence?s failure
to address the task.
4.2 DimT
The annotation protocol states that these four discourse
elements come into play for DimT : Background, Con-
clusion, Main Point, and Supporting Idea. Because this
dimension indicates the degree of relatedness to the the-
sis of the essay (and also other discourse segments in the
case of Supporting Idea and Conclusion sentences; see
Section 2.1.1 above), we do not consider thesis sentences
with regard to this aspect of coherence.
The features which we provide to the SVM for pre-
dicting whether or not a given sentence is related to the
thesis are almost the same ones used for DimP . The only
difference is that we omit features #12 and #13 in our
model of DimT . These are the features which evaluate
how many sentences in the current chunk have a simi-
larity score with the prompt and required task sentence
greater than 0.4. While DimP is to some degree sensitive
to the similarity of a sentence to the thesis, and DimT can
likewise benefit from the information about a sentence?s
similarity to the prompt, it seems that the latter link is less
important, so a single cutoff suffices for this model.
Tables 5?6 present the results for our SVM model and
for a baseline which assigns all sentences ?high? rele-
vance. The improvements on DimT are smaller than the
ones reported for DimP , but we still record an overall
gain of four percentage points in accuracy. Only on con-
clusion sentences were we unable to produce an improve-
ment over the baseline; we need to investigate this further.
Again, the system achieves high recall on sentences
with high relatedness. It outperforms the baseline by cor-
rectly identifying a modest percentage of the sentences
labeled as having low relatedness with the thesis.
4.3 DimS
DimS , which concerns whether the target sentence re-
lates to another sentence within the same discourse seg-
ment, seems another good candidate for applying our se-
mantic similarity score to the task of establishing coher-
ence. At present, however we have not made substan-
tial progress on this task. The baselines for DimS are
substantially higher than those for dimensions DimP and
DimT ? 98.1% of all sentences in our data were anno-
tated as ?highly related? with respect to this dimension.
This indicates that it is relatively rare to find a sentence
which is not related to anything in the same discourse
segment. This makes our task, to characterize those sen-
tences which are not related to the discourse segment,
much more difficult, since there are so few examples of
sentences with low-ranking coherence.
4.4 DimERR
DimERR is clearly a different kind of problem. Here, we
are looking for clarity of expression, or coherence within
a sentence. We base this solely on technical correctness.
We are able to automatically assign high and low ranks to
essay sentences using a set of rules based on the number
of grammar, usage and mechanics errors. The rules used
for DimERR are as follows: a) assign a low label if the
sentence is a fragment, if the sentence contains 2 or more
grammar, usage, and mechanics errors, or if the sentence
is a run-on, b) assign a high label if no criteria in (a) apply.
Criterion?s discourse analysis system also provides
an essay score with e-rater?, and qualitative feedback
about grammar, usage, mechanics, and style (Leacock
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1077) 0.486 1.000 0.654 0.000 0.000 0.000 0.486
Conclusion (N = 1830) 0.757 1.000 0.862 0.000 0.000 0.000 0.757
Main Point (N = 1566) 0.663 1.000 0.797 0.000 0.000 0.000 0.663
Thesis (N = 1899) 0.712 1.000 0.832 0.000 0.000 0.000 0.712
All sentence types (N = 6372) 0.675 1.000 0.806 0.000 0.000 0.000 0.675
Table 3: Baseline performance on DimP
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1077) 0.714 0.702 0.708 0.723 0.735 0.729 0.719
Conclusion (N = 1830) 0.784 0.959 0.863 0.578 0.175 0.269 0.768
Main Point (N = 1566) 0.729 0.888 0.801 0.616 0.352 0.448 0.708
Thesis (N = 1899) 0.771 0.929 0.843 0.644 0.318 0.426 0.753
All sentence types (N = 6372) 0.759 0.901 0.824 0.665 0.407 0.505 0.740
Table 4: SVM performance on DimP
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1060) 0.793 1.000 0.885 0.000 0.000 0.000 0.793
Conclusion (N = 1829) 0.834 1.000 0.909 0.000 0.000 0.000 0.834
Main Point (N = 1556) 0.742 1.000 0.852 0.000 0.000 0.000 0.742
Support (N = 10332) 0.664 1.000 0.798 0.000 0.000 0.000 0.664
All sentence types (N = 14777) 0.702 1.000 0.825 0.000 0.000 0.000 0.702
Table 5: Baseline performance on DimT
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1060) 0.856 0.980 0.914 0.827 0.368 0.509 0.853
Conclusion (N = 1829) 0.834 1.000 0.910 0.000 0.000 0.000 0.834
Main Point (N = 1556) 0.776 0.997 0.873 0.958 0.172 0.292 0.785
Support (N = 10332) 0.709 0.945 0.810 0.684 0.237 0.352 0.706
All sentence types (N = 14777) 0.744 0.962 0.839 0.709 0.221 0.337 0.741
Table 6: SVM performance on DimT
and Chodorow, 2000; Burstein et al, 2003a). We can
easily use Criterion?s outputs about grammar, usage, and
mechanics errors to assign high and low ranks to essay
sentences, using the rules described in the previous sec-
tion.
The performance of the module that does the DimERR
assignments is in Table 7. We used half of the 292 essays
from the training phase of annotation for development,
and the remaining data from the training and post-training
phases of annotation for cross-validation. Results are re-
ported for the cross-validation set. Text labeled as titles,
or opening or closing salutations, are not included in the
results. The baselines were computed by assigning all
sentences a high rank label. The baseline is high; how-
ever, the algorithm outperforms the baseline.
5 Discussion and Conclusions
There were multiple goals in this work. We wanted to in-
troduce a concept of essay coherence comprising multi-
ple aspects, and investigate what linguistic features drive
each aspect in student essay writing. Further, we wanted
Sentence N Precision Recall F-measure
Baseline
High 11789 0.83 1.00 0.91
Low 2351 0.00 0.00 0.00
Overall 14140 0.83 0.83 0.83
Algorithm
High 11789 0.88 0.96 0.92
Low 2351 0.63 0.34 0.44
Overall 14140 0.86 0.86 0.86
Table 7: Performance on DimERR
to build a system to automatically evaluate these multiple
aspects of coherence, so that appropriate feedback can be
provided through a writing instruction application.
To accomplish these goals, we have worked with writ-
ing experts to develop a comprehensive protocol that de-
tails how coherence in writing can be evaluated, either
manually or automatically. Using this protocol, human
annotators labeled a corpus of student essays, using the
coherence dimensions. These annotations built on a pre-
vious set of annotations for these data, whereby discourse
element labels were assigned. The result is a richly anno-
tated data set with information about discourse elements,
as well as their coherence in the context of the discourse
structure. Using this data set, we were able to learn what
linguistic features can be used to evaluate various aspects
of coherence in student writing. We then developed a
prototype system that ranks global and local aspects of
coherence in an essay. This capability shows promise in
ranking three aspects of coherence in essays: a) relation-
ship to essay topic, b) relationship between discourse ele-
ments, and c) intra-sentential technical quality. More low
ranking data on a fourth dimension, coherence within a
discourse segment, needs to be identified and annotated
before this dimension can be modeled.
The approach used is innovative, since it moves beyond
earlier methods of evaluating coherence in student writ-
ing that capture only local information between adjacent
sentences. Two methods are used to model the aspects
of coherence handled by the system. For the two global
coherence dimensions, DimP and DimT , a support vec-
tor machine provides a coherence ranking of sentences
based on features related to essay-based discourse infor-
mation, and semantic similarity values derived from the
RI algorithm. Using this classification method, we are
able to rank the expressive quality of sentences in essay-
based discourse segments, with regard to relatedness to
the text of the prompt, and also as they relate to the thesis
statement. With regard to the local coherence dimension,
DimERR, we use a rule-based heuristic to rank intra-
sentential quality. This addresses the issue of sentences in
essays that have serious grammatical problems that may
interfere with a reader?s comprehension. We take advan-
tage of Criterion?s identification of grammar, usage, and
mechanics errors to design the rules for ranking this local
coherence dimension.
We hope that in further investigation of this richly an-
notated data set, we will be able to build on the current
prototype and develop a full-scale writing instruction ca-
pability that provides feedback on the coherence dimen-
sions described in this paper.
Acknowledgements
We would like to thank Irma Lorenz and Shauna Cooper
for advice on protocol development and for the annota-
tion work, and Martin Chodorow for discussions about
Random Indexing. We thank the anonymous reviewers
for their helpful feedback.
Any opinions expressed here are those of the authors
and not necessarily of the Educational Testing Service.
References
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003a. CriterionSM: Online essay evaluation: An ap-
plication for automated evaluation of student essays.
In Proceedings of the Fifteenth Annual Conference on
Innovative Applications of Artificial Intelligence, Aca-
pulco, Mexico.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003b.
Finding the WRITE stuff: Automatic identification of
discourse structure in student essays. IEEE Trans-
actions on Intelligent Systems: Special Issue on Ad-
vances in Natural Language Processing, 181:32?39.
Nello Christianini and John Shawe-Taylor. 2000. Sup-
port Vector Machines and other Kernel-based Learn-
ing Methods. Cambridge University Press, Cam-
bridge, UK.
Peter Foltz, Walter Kintsch, and Thomas K. Landauer.
1998. The measurement of textual coherence with
Latent Semantic Analysis. Discourse Processes,
25(2&3):285?307.
Marti A. Hearst and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In Pro-
ceedings of ACM SIGIR, pages 59?68.
Marti A. Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33?64.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Random
indexing of text samples for Latent Semantic Analysis.
In L. R. Gleitman and A. K. Josh, editors, Proc. 22nd
Annual Conference of the Cognitive Science Society.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage Publications.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic Analy-
sis theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Claudia Leacock and Martin Chodorow. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of NAACL 2000, pages 140?147.
William Mann and Sandra Thompson. 1986. Relational
processes in discourse. Discourse Processes, 9:57?90.
Eleni Miltsakaki and Karen Kukich. 2000. Automated
evaluation of coherence in student essays. In Proceed-
ings of LREC 2000, Athens, Greece.
Magnus Sahlgren. 2001. Vector based semantic analy-
sis: Representing word meanings based on random la-
bels. In Proceedings of the ESSLLI 2001 Workshop on
Semantic Knowledge Acquisition and Categorisation.
Helsinki, Finland.
Vladimir Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York.
Peter Wiemer-Hastings and Arthur Graesser. 2000.
Select-a-Kibitzer: A computer tool that gives mean-
ingful feedback on student compositions. Interactive
Learning Environments, 8(2):149?169.
 
	 ffWhat?s in a translation rule?
Michel Galley
Dept. of Computer Science
Columbia University
New York, NY 10027
galley@cs.columbia.edu
Mark Hopkins
Dept. of Computer Science
University of California
Los Angeles, CA 90024
mhopkins@cs.ucla.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292
{knight,marcu}@isi.edu
Abstract
We propose a theory that gives formal seman-
tics to word-level alignments defined over par-
allel corpora. We use our theory to introduce a
linear algorithm that can be used to derive from
word-aligned, parallel corpora the minimal set
of syntactically motivated transformation rules
that explain human translation data.
1 Introduction
In a very interesting study of syntax in statistical machine
translation, Fox (2002) looks at how well proposed trans-
lation models fit actual translation data. One such model
embodies a restricted, linguistically-motivated notion of
word re-ordering. Given an English parse tree, children
at any node may be reordered prior to translation. Nodes
are processed independently. Previous to Fox (2002), it
had been observed that this model would prohibit certain
re-orderings in certain language pairs (such as subject-
VP(verb-object) into verb-subject-object), but Fox car-
ried out the first careful empirical study, showing that
many other common translation patterns fall outside the
scope of the child-reordering model. This is true even
for languages as similar as English and French. For
example, English adverbs tend to move outside the lo-
cal parent/children in environment. The English word
?not? translates to the discontiguous pair ?ne ... pas.?
English parsing errors also cause trouble, as a normally
well-behaved re-ordering environment can be disrupted
by wrong phrase attachment. For other language pairs,
the divergence is expected to be greater.
In the face of these problems, we may choose among
several alternatives. The first is to abandon syntax in
statistical machine translation, on the grounds that syn-
tactic models are a poor fit for the data. On this view,
adding syntax yields no improvement over robust phrase-
substitution models, and the only question is how much
does syntax hurt performance. Along this line, (Koehn
et al, 2003) present convincing evidence that restricting
phrasal translation to syntactic constituents yields poor
translation performance ? the ability to translate non-
constituent phrases (such as ?there are?, ?note that?, and
?according to?) turns out to be critical and pervasive.
Another direction is to abandon conventional English
syntax and move to more robust grammars that adapt to
the parallel training corpus. One approach here is that of
Wu (1997), in which word-movement is modeled by rota-
tions at unlabeled, binary-branching nodes. At each sen-
tence pair, the parse adapts to explain the translation pat-
tern. If the same unambiguous English sentence were to
appear twice in the corpus, with different Chinese trans-
lations, then it could have different learned parses.
A third direction is to maintain English syntax and
investigate alternate transformation models. After all,
many conventional translation systems are indeed based
on syntactic transformations far more expressive than
what has been proposed in syntax-based statistical MT.
We take this approach in our paper. Of course, the broad
statistical MT program is aimed at a wider goal than
the conventional rule-based program ? it seeks to under-
stand and explain human translation data, and automati-
cally learn from it. For this reason, we think it is impor-
tant to learn from the model/data explainability studies of
Fox (2002) and to extend her results. In addition to being
motivated by rule-based systems, we also see advantages
to English syntax within the statistical framework, such
as marrying syntax-based translation models with syntax-
based language models (Charniak et al, 2003) and other
potential benefits described by Eisner (2003).
Our basic idea is to create transformation rules that
condition on larger fragments of tree structure. It is
certainly possible to build such rules by hand, and we
have done this to formally explain a number of human-
translation examples. But our main interest is in collect-
ing a large set of such rules automatically through corpus
analysis. The search for these rules is driven exactly by
the problems raised by Fox (2002) ? cases of crossing
and divergence motivate the algorithms to come up with
better explanations of the data and better rules. Section
2 of this paper describes algorithms for the acquisition
of complex rules for a transformation model. Section 3
gives empirical results on the explanatory power of the
acquired rules versus previous models. Section 4 presents
examples of learned rules and shows the various types of
transformations (lexical and nonlexical, contiguous and
noncontiguous, simple and complex) that the algorithms
are forced (by the data) to invent. Section 5 concludes.
Due to space constraints, all proofs are omitted.
2 Rule Acquisition
Suppose that we have a French sentence, its translation
into English, and a parse tree over the English translation,
as shown in Figure 1. Generally one defines an alignment
as a relation between the words in the French sentence
and the words in the English sentence. Given such an
alignment however, what kinds of rules are we entitled
to learn from this instance? How do we know when it is
valid to extract a particular rule, especially in the pres-
ence of numerous crossings in the alignment? In this sec-
tion, we give principled answers to these questions, by
constructing a theory that gives formal semantics to word
alignments.
2.1 A Theory of Word Alignments
We are going to define a generative process through
which a string from a source alphabet is mapped to a
rooted tree whose nodes are labeled from a target alha-
bet. Henceforth we will refer to symbols from our source
alphabet as source symbols and symbols from our target
alphabet as target symbols. We define a symbol tree over
an alphabet ? as a rooted, directed tree, the nodes of
which are each labeled with a symbol of ?.
We want to capture the process by which a symbol tree
over the target language is derived from a string of source
symbols. Let us refer to the symbol tree that we want to
derive as the target tree. Any subtree of this tree will be
called a target subtree. Furthermore, we define a deriva-
tion string as an ordered sequence of elements, each of
which is either a source symbol or a target subtree.
Now we are ready to define the derivation process.
Given a derivation string S, a derivation step replaces
a substring S? of S with a target subtree T that has the
following properties:
1. Any target subtree in S ? is a subtree of T .
2. Any target subtree in S but not in S ? does not share
nodes with T .
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 1: A French sentence aligned with an English
parse tree.
il ne va pas
ne va pas
he
PRP
NP
ne pas
he
PRP
NP
S
NP VP
PRP RBAUX VB
he notdoes go
VB
go
il ne va pas
ne va pasRB
not
ne heRB
not
S
NP VP
PRP RBAUX VB
he notdoes go
il ne va pas
S
NP VP
PRP RBAUX VB
he notdoes go
NP VP
PRP RBAUX VB
he notdoes go
Figure 2: Three alternative derivations from a source sen-
tence to a target tree.
Moreover, a derivation from a string S of source sym-
bols to the target tree T is a sequence of derivation steps
that produces T from S.
Moving away from the abstract for a moment, let us
revisit the example from Figure 1. Figure 2 shows three
derivations of the target tree from the source string ?il
ne va pas?, which are all consistent with our defini-
tions. However, it is apparent that one of these deriva-
tions seems much more ?wrong? than the other. Specif-
ically, in the second derivation, ?pas? is replaced by the
English word ?he,? which makes no sense. Given the vast
space of possible derivations (according to the definition
above), how do we distinguish between good ones and
bad ones? Here is where the notion of an alignment be-
comes useful.
Let S be a string of source symbols and let T be a target
tree. First observe the following facts about derivations
from S to T (these follow directly from the definitions):
1. Each element of S is replaced at exactly one step of
the derivation.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
S
NP VP
PRP RBAUX VB
he notdoes go
il vane pas
Figure 3: The alignments induced by the derivations in
Figure 2
2. Each node of T is created at exactly one step of the
derivation.
Thus for each element s of S, we can define
replaced(s, D) to be the step of the derivation D during
which s is replaced. For instance, in the leftmost deriva-
tion of Figure 2, ?va? is replaced by the second step of the
derivation, thus replaced(va, D) = 2. Similarly, for each
node t of T , we can define created(t, D) to be the step
of derivation D during which t is created. For instance,
in the same derivation, the nodes labeled by ?AUX? and
?VP? are created during the third step of the derivation,
thus created(AUX, D) = 3 and created(VP, D) = 3.
Given a string S of source symbols and a target tree
T , an alignment A with respect to S and T is a relation
between the leaves of T and the elements of S. Choose
some derivation D from S to T . The alignment A in-
duced by D is created as follows: an element s of S is
aligned with a leaf node t of T iff replaced(s, D) =
created(t, D). In other words, a source word is aligned
with a target word if the target word is created during the
same step in which the source word is replaced. Figure 3
shows the alignments induced by the derivations of Fig-
ure 2.
Now, say that we have a source string, a target tree,
and an alignment A. A key observation is that the set
of ?good? derivations according to A is precisely the set
of derivations that induce alignments A? such that A is
a subalignment of A?. By subalignment, we mean that
A ? A? (recall that alignments are simple mathematical
relations). In other words, A is a subalignment of A? if A
aligns two elements only if A? also aligns them.
We can see this intuitively by examining Figures 2 and
3. Notice that the two derivations that seem ?right? (the
first and the third) are superalignments of the alignment
given in Figure 1, while the derivation that is clearly
wrong is not. Hence we now have a formal definition
of the derivations that we are interested in. We say that
a derivation is admitted by an alignment A if it induces a
superalignment of A. The set of derivations from source
string S to target tree T that are admitted by alignment A
can be denoted ?A(S, T ). Given this, we are ready to ob-
tain a formal characterization of the set of rules that can
ne pas
he
PRP
NP
VB
go
NP VP
PRP RBAUX VB
he notdoes go
Derivationstep: Inducedrule:
input: ne VB?pas
output: VP
RBAUX x2
notdoes
S
NP VP
PRP RBAUX VB
he notdoes go
input: NP?VP
output: S
x1 x2
Figure 4: Two derivation steps and the rules that are in-
duced from them.
be inferred from the source string, target tree, and align-
ment.
2.2 From Derivations to Rules
In essence, a derivation step can be viewed as the applica-
tion of a rule. Thus, compiling the set of derivation steps
used in any derivation of ?A(S, T ) gives us, in a mean-
ingful sense, all relevant rules that can be extracted from
the triple (S, T, A). In this section, we show in concrete
terms how to convert a derivation step into a usable rule.
Consider the second-last derivation step of the first
derivation in Figure 2. In it, we begin with a source sym-
bol ?ne?, followed by a target subtree rooted at V B, fol-
lowed by another source symbol ?pas.? These three ele-
ments of the derivation string are replaced with a target
subtree rooted at V P that discards the source symbols
and contains the target subtree rooted at V B. In general,
this replacement process can be captured by the rule de-
picted in Figure 4. The input to the rule are the roots
of the elements of the derivation string that are replaced
(where we define the root of a symbol to be simply the
symbol itself), whereas the output of the rule is a symbol
tree, except that some of the leaves are labeled with vari-
ables instead of symbols from the target alhabet. These
variables correspond to elements of the input to the rule.
For instance, the leaf labeled x2 means that when this rule
is applied, x2 is replaced by the target subtree rooted at
V B (since V B is the second element of the input). Ob-
serve that the second rule induced in Figure 4 is simply
a CFG rule expressed in the opposite direction, thus this
rule format can (and should) be viewed as a strict gener-
alization of CFG rules.
SNP VP
PRP RBAUX VB
he notdoes go
il vane pas
{ il, ne, va,pas}
{ ne, va,pas}{ il }
{ il }
{ il }
{ il }
{ne,pas} {ne,pas}
{ne,pas} {ne,pas}
{ va }
{ ne } { va }
{ va }
{pas}
Figure 5: An alignment graph. The nodes are annotated
with their spans. Nodes in the frontier set are boldfaced
and italicized.
Every derivation step can be mapped to a rule in this
way. Hence given a source string S, a target tree T , and
an alignment A, we can define the set ?A(S, T ) as the set
of rules in any derivation D ? ?A(S, T ). We can regard
this as the set of rules that we are entitled to infer from
the triple (S, T, A).
2.3 Inferring Complex Rules
Now we have a precise problem statement: learn the set
?A(S, T ). It is not immediately clear how such a set can
be learned from the triple (S, T, A). Fortunately, we can
infer these rules directly from a structure called an align-
ment graph. In fact, we have already seen numerous ex-
amples of alignment graphs. Graphically, we have been
depicting the triple (S, T, A) as a rooted, directed, acyclic
graph (where direction is top-down in the diagrams). We
refer to such a graph as an alignment graph. Formally,
the alignment graph corresponding to S, T , and A is just
T , augmented with a node for each element of S, and
edges from leaf node t ? T to element s ? S iff A aligns
s with t. Although there is a difference between a node
of the alignment graph and its label, we will not make a
distinction, to ease the notational burden.
To make the presentation easier to follow, we assume
throughout this section that the alignment graph is con-
nected, i.e. there are no unaligned elements. All of the
results that follow have generalizations to deal with un-
aligned elements, but unaligned elements incur certain
procedural complications that would cloud the exposi-
tion.
It turns out that it is possible to systematically con-
vert certain fragments of the alignment graph into rules
of ?A(S, T ). We define a fragment of a directed, acyclic
graph G to be a nontrivial (i.e. not just a single node) sub-
graph G? of G such that if a node n is in G? then either n
is a sink node of G? (i.e. it has no children) or all of its
children are in G? (and it is connected to all of them). In
VP
RBAUX VB
notdoes
ne pas
S
NP VP
input: ne VB?pas
output: VP
RBAUX x2
notdoes
input: NP?VP
output: S
x1 x2
{ ne } {pas}
{ va }
{ ne, va,pas}
{ il } { ne, va,pas}
{ il, ne, va,pas}
Figure 6: Two frontier graph fragments and the rules in-
duced from them. Observe that the spans of the sink
nodes form a partition of the span of the root.
Figure 6, we show two examples of graph fragments of
the alignment graph of Figure 5.
The span of a node n of the alignment graph is the
subset of nodes from S that are reachable from n. Note
that this definition is similar to, but not quite the same
as, the definition of a span given by Fox (2002). We
say that a span is contiguous if it contains all elements
of a contiguous substring of S. The closure of span(n)
is the shortest contiguous span which is a superset of
span(n). For instance, the closure of {s2, s3, s5, s7}
would be {s2, s3, s4, s5, s6, s7} The alignment graph in
Figure 5 is annotated with the span of each node.
Take a look at the graph fragments in Figure 6. These
fragments are special: they are examples of frontier
graph fragments. We first define the frontier set of an
alignment graph to be the set of nodes n that satisfy the
following property: for every node n? of the alignment
graph that is connected to n but is neither an ancestor nor
a descendant of n, span(n?) ? closure(span(n)) = ?.
We then define a frontier graph fragment of an align-
ment graph to be a graph fragment such that the root and
all sinks are in the frontier set. Frontier graph fragments
have the property that the spans of the sinks of the frag-
ment are each contiguous and form a partition of the span
of the root, which is also contiguous. This allows the fol-
lowing transformation process:
1. Place the sinks in the order defined by the partition
(i.e. the sink whose span is the first part of the span
of the root goes first, the sink whose span is the sec-
ond part of the span of the root goes second, etc.).
This forms the input of the rule.
2. Replace sink nodes of the fragment with a variable
corresponding to their position in the input, then
take the tree part of the fragment (i.e. project the
fragment on T ). This forms the output of the rule.
Figure 6 shows the rules derived from the given graph
fragments. We have the following result.
Theorem 1 Rules constructed according to the above
procedure are in ?A(S, T ).
Rule extraction: Algorithm 1. Thus we now have a
simple method for extracting rules of ?A(S, T ) from the
alignment graph: search the space of graph fragments for
frontier graph fragments.
Unfortunately, the search space of all fragments of a
graph is exponential in the size of the graph, thus this
procedure can also take a long time to execute. To ar-
rive at a much faster procedure, we take advantage of the
following provable facts:
1. The frontier set of an alignment graph can be identi-
fied in time linear in the size of the graph.
2. For each node n of the frontier set, there is a unique
minimal frontier graph fragment rooted at n (ob-
serve that for any node n? not in the frontier set,
there is no frontier graph fragment rooted at n?, by
definition).
By minimal, we mean that the frontier graph fragment
is a subgraph of every other frontier graph fragment with
the same root. Clearly, for an alignment graph with k
nodes, there are at most k minimal frontier graph frag-
ments. In Figure 7, we show the seven minimal frontier
graph fragments of the alignment graph of Figure 5. Fur-
thermore, all other frontier graph fragments can be cre-
ated by composing 2 or more minimal graph fragments,
as shown in Figure 8. Thus, the entire set of frontier graph
fragments (and all rules derivable from these fragments)
can be computed systematically as follows: compute the
set of minimal frontier graph fragments, compute the set
of graph fragments resulting from composing 2 minimal
frontier graph fragments, compute the set of graph frag-
ments resulting from composing 3 minimal graph frag-
ments, etc. In this way, the rules derived from the min-
imal frontier graph fragments can be regarded as a ba-
sis for all other rules derivable from frontier graph frag-
ments. Furthermore, we conjecture that the set of rules
derivable from frontier graph fragments is in fact equiva-
lent to ?A(S, T ).
Thus we have boiled down the problem of extracting
complex rules to the following simple problem: find the
set of minimal frontier graph fragments of a given align-
ment graph.
The algorithm is a two-step process, as shown below.
Rule extraction: Algorithm 2
1. Compute the frontier set of the alignment graph.
2. For each node of the frontier set, compute the mini-
mal frontier graph fragment rooted at that node.
VP
RBAUX VB
notdoes
ne pas
S
NP VP
NP
PRP
PRP
he
VB
go
go
vahe
il
Figure 7: The seven minimal frontier graph fragments of
the alignment graph in Figure 5
VP
RBAUX VB
notdoes
ne pas
VB
go
+ =
VP
RBAUX VB
notdoes
ne pas
go
S
NP VP
+ + =
NP
PRP
PRP
he
S
NP VP
PRP
he
Figure 8: Example compositions of minimal frontier
graph fragments into larger frontier graph fragments.
Step 1 can be computed in a single traversal of the
alignment graph. This traversal annotates each node with
its span and its complement span. The complement span
is computed as the union of the complement span of its
parent and the span of all its siblings (siblings are nodes
that share the same parent). A node n is in the frontier
set iff complement span(n) ? closure(span(n)) = ?.
Notice that the complement span merely summarizes the
spans of all nodes that are neither ancestors nor descen-
dents of n. Since this step requires only a single graph
traversal, it runs in linear time.
Step 2 can also be computed straightforwardly. For
each node n of the frontier set, do the following: expand
n, then as long as there is some sink node n? of the result-
ing graph fragment that is not in the frontier set, expand
n?. Note that after computing the minimal graph frag-
ment rooted at each node of the frontier set, every node
of the alignment graph has been expanded at most once.
Thus this step also runs in linear time.
For clarity of exposition and lack of space, a couple of
issues have been glossed over. Briefly:
? As previously stated, we have ignored here the is-
sue of unaligned elements, but the procedures can
be easily generalized to accommodate these. The
results of the next two sections are all based on im-
plementations that handle unaligned elements.
? This theory can be generalized quite cleanly to in-
clude derivations for which substrings are replaced
by sets of trees, rather than one single tree. This
corresponds to allowing rules that do not require the
output to be a single, rooted tree. Such a general-
ization gives some nice power to effectively explain
certain linguistic phenomena. For instance, it allows
us to immediately translate ?va? as ?does go? in-
stead of delaying the creation of the auxiliary word
?does? until later in the derivation.
3 Experiments
3.1 Language Choice
We evaluated the coverage of our model of transforma-
tion rules with two language pairs: English-French and
English-Chinese. These two pairs clearly contrast by
the underlying difficulty to understand and model syntac-
tic transformations among pairs: while there is arguably
a fair level of cohesion between English and French,
English and Chinese are syntactically more distant lan-
guages. We also chose French to compare our study with
that of Fox (2002). The additional language pair provides
a good means of evaluating how our transformation rule
extraction method scales to more problematic language
pairs for which child-reordering models are shown not to
explain the data well.
3.2 Data
We performed experiments with two corpora, the FBIS
English-Chinese Parallel Text and the Hansard French-
English corpus.We parsed the English sentences with
a state-of-the-art statistical parser (Collins, 1999). For
the FBIS corpus (representing eight million English
words), we automatically generated word-alignments us-
ing GIZA++ (Och and Ney, 2003), which we trained on
a much larger data set (150 million words). Cases other
than one-to-one sentence mappings were eliminated. For
the Hansard corpus, we took the human annotation of
word alignment described in (Och and Ney, 2000). The
corpus contains two kinds of alignments: S (sure) for
unambiguous cases and P (possible) for unclear cases,
e.g. idiomatic expressions and missing function words
(S ? P ). In order to be able to make legitimate com-
parisons between the two language pairs, we also used
GIZA++ to obtain machine-generated word alignments
for Hansard: we trained it with the 500 sentences and
additional data representing 13.7 million English words
(taken from the Hansard and European parliament cor-
pora).
3.3 Results
From a theoretical point of view, we have shown that our
model can fully explain the transformation of any parse
tree of the source language into a string of the target lan-
guage. The purpose of this section is twofold: to pro-
vide quantitative results confirming the full coverage of
our model and to analyze some properties of the trans-
formation rules that support these derivations (linguistic
analyses of these rules are presented in the next section).
Figure 9 summarizes the coverage of our model with
respect to the Hansard and FBIS corpora. For the for-
mer, we present results for the three alignments: S align-
ments, P alignments, and the alignments computed by
GIZA++. Each plotted value represents a percentage of
parse trees in a corpus that can be transformed into a tar-
get sentence using transformation rules. The x-axis rep-
resents different restrictions on the size of these rules: if
we use a model that restrict rules to a single expansion
of a non-terminal into a sequence of symbols, we are in
the scope of the child-reordering model of (Yamada and
Knight, 2001; Fox, 2002). We see that its explanatory
power is quite poor, with only 19.4%, 14.3%, 16.5%, and
12.1% (for the respective corpora). Allowing more ex-
pansions logically expands the coverage of the model,
until the point where it is total: transformation rules no
larger than 17, 18, 23, and 43 (in number of rule expan-
sions) respectively provide enough coverage to explain
the data at 100% for each of the four cases.
It appears from the plot that the quality of alignments
plays an important role. If we compare the three kinds of
alignments available for the Hansard corpus, we see that
much more complex transformation rules are extracted
from noisy GIZA++ alignments. It also appears that the
language difference produces quite contrasting results.
Rules acquired for the English-Chinese pair have, on av-
erage, many more nodes. Note that the language differ-
ence in terms of syntax might be wider than what the plot
seems to indicate, since word alignments computed for
the Hansard corpus are likely to be more errorful than the
ones for FBIS because the training data used to induce the
latter is more than ten times larger than for the former.
In Figure 10, we show the explanatory power of our
model at the node level. At each node of the frontier
set, we determine whether it is possible to extract a rule
that doesn?t exceed a given limit k on its size. The plot-
ted values represent the percentage of frontier set inter-
nal nodes that satisfy this condition. These results appear
more promising for the child-reordering model, with cov-
erage ranging from 72.3% to 85.1% of the nodes, but we
should keep in mind that many of these nodes are low in
the tree (e.g. base NPs); extraction of 1-level transfor-
mation rules generally present no difficulties when child
nodes are pre-terminals, since any crossings can be re-
solved by lexicalizing the elements involved in it. How-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
Pa
rse
 tr
ee
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 9: Percentage of parse trees covered by the model
given different constraints on the maximum size of the
transformation rules.
0.7
0.75
0.8
0.85
0.9
0.95
1
1 2 3 4 5 6 7 8 9 10 15 20 25 30 35 40 4550
No
de
 co
ve
ra
ge
Maximum number of rule expansions
"Hansard-S"
"Hansard-P"
"Hansard-GIZA"
"FBIS"
Figure 10: Same as Figure 9, except that here coverage is
evaluated at the node level.
ever, higher level syntactic constituents are more prob-
lematic for child-reordering models, and the main rea-
sons they fail to provide explanation of the parses at the
sentence level.
Table 1 shows that the extraction of rules can be per-
formed quite efficiently. Our first algorithm, which has an
exponential running time, cannot scale to process large
corpora and extract a sufficient number of rules that a
syntax-based statistical MT system would require. The
second algorithm, which runs in linear time, is on the
other hand barely affected by the size of rules it extracts.
k=1 3 5 7 10 20 50
I 4.1 10.2 57.9 304.2 - - -
II 4.3 5.4 5.9 6.4 7.33 9.6 11.8
Table 1: Running time in seconds of the two algorithms
on 1000 sentences. k represent the maximum size of rules
to extract.
NPB
DT NN RB
that Government simply tells
ADVP
VBZ
NPB
DT NNS
the people what is themgood for
WP VBZ JJ IN PRP
NPB
ADJP
VP
SG-A
SBAR-A
VHPN
VP
S
le gouvernement dit tout simplement ? les gens ce qui est bon pour eux
input:
VBZ ADVP ?NPB SBAR -S
output: S
VPx2
x1 x3 x4
Figure 11: Adverb-verb reordering.
4 Discussions
In this section, we present some syntactic transformation
rules that our system learns. Fox (2002) identified three
major causes of crossings between English and French:
the ?ne ... pas? construct, modals and adverbs, which a
child-reordering model doesn?t account for. In section 2,
we have already explained how we learn syntactic rules
involving ?ne ... pas?. Here we describe the other two
problematic cases.
Figure 11 presents a frequent cause of crossings be-
tween English and French: adverbs in French often ap-
pear after the verb, which is less common in English.
Parsers generally create nested verb phrases when ad-
verbs are present, thus no child reordering can allow a
verb and an adverb to be permuted. Multi-level reodering
as the rule in the figure can prevent crossings. Fox?s solu-
tion to the problem of crossings is to flatten verb phrases.
This is a solution for this sentence pair, since this ac-
counts for adverb-verb reorderings, but flattening the tree
structure is not a general solution. Indeed, it can only ap-
ply to a very limited number of syntactic categories, for
which the advantage of having a deep syntactic structure
is lost.
Figure 12 (dotted lines are P alignments) shows an in-
teresting example where flattening the tree structure can-
not resolve all crossings in node-reordering models. In
these models, a crossing remains between MD and AUX
no matter how VPs are flattened. Our transformation rule
model creates a lexicalized rule as shown in the figure,
where the transformation of ?will be? into ?sera? is the
only way to resolve the crossing.
In the Chinese-English domain, the rules extracted by
our algorithm often have the attractive quality that they
are the kind of common-sense constructions that are used
in Chinese language textbooks to teach students. For in-
stance, there are several that illustrate the complex re-
orderings that occur around the Chinese marker word
?de.?
NPB
DT JJ NN
the full report will
MD AUX VB
be coming in before the fall
RB IN DT NN
NPB
PP
VP-A
ADVP
VP
S
le rapport complet sera d?pos? de ici le automne prochain
input: sera  VP-A
output:
VP
VP-Awill/MD
be/AUX
VP-A
x2
Figure 12: Crossing due to a modal.
5 Conclusion
The fundamental assumption underlying much recent
work in statistical machine translation (Yamada and
Knight, 2001; Eisner, 2003; Gildea, 2003) is that lo-
cal transformations (primarily child-node re-orderings)
of one-level parent-children substructures are an adequate
model for parallel corpora. Our empirical results suggest
that this may be too strong of an assumption. To explain
the data in two parallel corpora, one English-French, and
one English-Chinese, we are often forced to learn rules
involving much larger tree fragments. The theory, algo-
rithms, and transformation rules we learn automatically
from data have several interesting aspects.
1. Our rules provide a good, realistic indicator of the
complexities inherent in translation. We believe that
these rules can inspire subsequent developments of
generative statistical models that are better at ex-
plaining parallel data than current ones.
2. Our rules put at the fingertips of linguists a very
rich source of information. They encode translation
transformations that are both syntactically and lex-
ically motivated (some of our rules are purely syn-
tactic; others are lexically grounded). A simple sort
on the counts of our rules makes explicit the trans-
formations that occur most often. A comparison of
the number of rules extracted from parallel corpora
specific to multiple language pairs provide a quanti-
tative estimator of the syntactic ?closeness? between
various language pairs.
3. The theory we proposed in this paper is independent
of the method that one uses to compute the word-
level alignments in a parallel corpus.
4. The theory and rule-extraction algorithm are also
well-suited to deal with the errors introduced by
the word-level alignment and parsing programs one
uses. Our theory makes no a priori assumptions
about the transformations that one is permitted to
learn. If a parser, for example, makes a systematic
error, we expect to learn a rule that can neverthe-
less be systematically used to produce correct trans-
lations.
In this paper, we focused on providing a well-founded
mathematical theory and efficient, linear algorithms
for learning syntactically motivated transformation rules
from parallel corpora. One can easily imagine a range
of techniques for defining probability distributions over
the rules that we learn. We suspect that such probabilis-
tic rules could be also used in conjunction with statistical
decoders, to increase the accuracy of statistical machine
translation systems.
Acknowledgements
This work was supported by DARPA contract N66001-
00-1-9814 and MURI grant N00014-00-1-0617.
References
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IX.
M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. of the 41st Meeting
of the Association for Computational Linguistics.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41th Annual Confer-
ence of the Association for Computational Linguistics.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of HLT/NAACL.
F. Och and H. Ney. 2000. Improved statistical alignment
models. Proc. of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics.
F. Och and H Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Towards Automatic Classification of Discourse Elements in Essays 
 
Jill Burstein 
ETS Technologies 
MS 18E 
Princeton, NJ 08541 
USA 
Jburstein@ 
etstechnologies.com 
Daniel Marcu 
ISI/USC 
4676 Admiralty 
Way 
Marina del Rey, 
CA, USA 
Marcu@isi.edu 
Slava Andreyev  
ETS Technologies 
MS 18E 
Princeton, NJ 08541 
USA 
sandreyev@ 
etstechnologies.com 
Martin Chodorow 
Hunter College, The 
City University of 
 New York 
New York, NY USA 
Martin.chodorow@ 
hunter.cuny.edu 
 
 
Abstract 
Educators are interested in essay 
evaluation systems that include 
feedback about writing features that 
can facilitate the essay revision 
process. For instance, if the thesis 
statement of a student?s essay could be 
automatically identified, the student 
could then use this information to 
reflect on the thesis statement with 
regard to its quality, and its relationship 
to other discourse elements in the 
essay. Using a relatively small corpus 
of manually annotated data, we use 
Bayesian classification to identify 
thesis statements.  This method yields 
results that are much closer to human 
performance than the results produced 
by two baseline systems.  
 
1 Introduction 
 
Automated essay scoring technology can 
achieve agreement with a single human judge 
that is comparable to agreement between two 
single human judges (Burstein, et al1998; Foltz, 
et al1998; Larkey, 1998; and Page and 
Peterson, 1995). Unfortunately, providing 
students with just a score (grade) is insufficient 
for instruction. To help students improve their 
writing skills, writing evaluation systems need 
to provide feedback that is specific to each 
individual?s writing and that is applicable to 
essay revision. 
The factors that contribute to improvement 
of student writing include refined sentence 
structure, variety of appropriate word usage, and 
organizational structure. The improvement of  
organizational structure is believed to be critical 
in the essay revision process toward overall 
improvement of essay quality.  Therefore, it 
would be desirable to have a system that could 
indicate as feedback to students, the discourse 
elements in their essays. Such a system could 
present to students a guided list of questions to 
consider about the quality of the discourse.  
For instance, it has been suggested by writing 
experts that if the thesis statement1 of a student?s 
essay could be automatically provided, the 
student could then use this information to reflect 
on the thesis statement and its quality. In 
addition, such an instructional application could 
utilize the thesis statement to discuss other types 
of discourse elements in the essay, such as the 
relationship between the thesis statement and the 
conclusion, and the connection between the 
thesis statement and the main points in the 
essay.  In the teaching of writing, in order to 
facilitate the revision process, students are often 
presented with ?Revision Checklists.? A revision 
checklist is a list of questions posed to the 
student to help the student reflect on the quality 
of his or her writing. Such a list might pose 
questions such as: 
a) Is the intention of my thesis statement 
clear? 
                                                           
1
 A thesis statement is generally defined as the 
sentence that explicitly identifies the purpose of the 
paper or previews its main ideas. See the Literacy 
Education On-line (LEO) site at 
http://leo.stcloudstate.edu. 
 (Annotator 1) ?In my opinion student should do what they want to do because they feel everything 
and they can't have anythig they feel because they probably feel to do just because other people do it not they 
want it. 
(Annotator 2) I think doing what students want is good for them. I sure they want to achieve in the 
highest place but most of the student give up. They they don?t get what they want. To get what they want, they 
have to be so strong and take the lesson from their parents Even take a risk, go to the library, and study hard by 
doing different thing. 
Some student they do not get what they want because of their family. Their family might be careless 
about their children so this kind of student who does not get support, loving from their family might not get 
what he wants. He just going to do what he feels right away. 
So student need a support from their family they has to learn from them and from their background. I 
learn from my background I will be the first generation who is going to gradguate from university that is what I 
want.? 
 
Figure 1: Sample student essay with human annotations of thesis statements. 
 
b) Does my thesis statement respond 
directly to the essay question?  
c) Are the main points in my essay 
clearly stated? 
d) Do the main points in my essay relate 
to my original thesis statement?  
If these questions are expressed in general 
terms, they are of little help; to be useful, they 
need to be grounded and need to refer 
explicitly to the essays students write 
(Scardamalia and Bereiter, 1985; White 1994). 
The ability to automatically identify and 
present to students the discourse elements in 
their essays can help them focus and reflect on 
the critical discourse structure of the essays.  
In addition, the ability for the application to 
indicate to the student that a discourse element 
could not be located, perhaps due to the ?lack 
of clarity? of this element, could also be 
helpful. Assuming that such a capability was 
reliable, this would force the writer to think 
about the clarity of an intended discourse 
element, such as a thesis statement. 
Using a relatively small corpus of essay 
data where thesis statements have been 
manually annotated, we built a Bayesian 
classifier using the following features:  
sentence position; words commonly used in 
thesis statements; and discourse features, 
based on Rhetorical Structure Theory (RST) 
parses (Mann and Thompson, 1988 and 
Marcu, 2000).  Our results indicate that this 
classification technique may be used toward 
automatic identification of thesis statements in 
essays.  Furthermore, we show that this 
method generalizes across essay topics. 
 
2 What Are Thesis Statements? 
 
A thesis statement is defined as the sentence that 
explicitly identifies the purpose of the paper or 
previews its main ideas (see footnote 1). This 
definition seems straightforward enough, and 
would lead one to believe that even for people to 
identify the thesis statement in an essay would be 
clear-cut.  However, the essay in Figure 1 is a 
common example of the kind of first-draft writing 
that our system has to handle. Figure 1 shows a 
student response to the essay question:  
Often in life we experience a conflict in 
choosing between something we "want" to do 
and something we feel we "should" do.  In your 
opinion, are there any circumstances in which 
it is better for people to do what they  "want" to 
do rather than what they feel they "should" do?  
Support your position with evidence from your 
own experience or your observations of other 
people.  
The writing in Figure 1 illustrates one kind of 
challenge in automatic identification of discourse 
elements, such as thesis statements.  In this case, 
the two human annotators independently chose 
different text as the thesis statement (the two texts 
highlighted in bold and italics in Figure 1).  In this 
kind of first-draft writing, it is not uncommon for 
writers to repeat ideas, or express more than one 
general opinion about the topic, resulting in text 
that seems to contain multiple thesis statements. 
Before building a system that automatically 
identifies thesis statements in essays, we wanted to 
determine whether the task was well-defined. In 
collaboration with two writing experts, a simple 
discourse-based annotation protocol was 
developed to manually annotate discourse 
elements in essays for a single essay topic.  
This was the initial attempt to annotate essay 
data using discourse elements generally 
associated with essay structure, such as thesis 
statement, concluding statement, and topic 
sentences of the essay?s main ideas. The 
writing experts defined the characteristics of 
the discourse labels.  These experts then 
annotated 100 essay responses to one English 
Proficiency Test (EPT) question, called Topic 
B, using a PC-based interface implemented in 
Java. 
We computed the agreement between the 
two human annotators using the kappa 
coefficient (Siegel and Castellan, 1988), a 
statistic used extensively in previous empirical 
studies of discourse.  The kappa statistic 
measures pairwise agreement among a set of 
coders who make categorial judgments, 
correcting for chance expected agreement. 
The kappa agreement between the two 
annotators with respect to the thesis statement 
labels was 0.733 (N=2391, where 2391 
represents the total number of sentences 
across all annotated essay responses).  This 
shows high agreement based on research in 
content analysis (Krippendorff, 1980) that 
suggests that values of kappa higher than 0.8 
reflect very high agreement and values higher 
than 0.6 reflect good agreement.  The 
corresponding z statistic was 27.1, which 
reflects a confidence level that is much higher 
than 0.01, for which the corresponding z value 
is 2.32 (Siegel and Castellan, 1988). 
 In the early stages of our project, it was 
suggested to us that thesis statements reflect 
the most important sentences in essays.  In 
terms of summarization, these sentences 
would represent indicative, generic summaries 
(Mani and Maybury, 1999; Marcu, 2000). To 
test this hypothesis (and estimate the adequacy 
of using summarization technology for 
identifying thesis statements), we carried out 
an additional experiment. The same 
annotation tool was used with two different 
human judges, who were asked this time to 
identify the most important sentence of each 
essay. The agreement between human judges 
on the task of identifying summary sentences 
was significantly lower: the kappa was 0.603 
(N=2391). Tables 1a and 1b summarize the results 
of the annotation experiments. 
Table 1a shows the degree of agreement 
between human judges on the task of identifying 
thesis statements and generic summary sentences. 
The agreement figures are given using the kappa 
statistic and the relative precision (P), recall (R), 
and F-values (F), which reflect the ability of one 
judge to identify the sentences labeled as thesis 
statements or summary sentences by the other 
judge. The results in Table 1a show that the task of 
thesis statement identification is much better 
defined than the task of identifying important 
summary sentences. In addition, Table 1b indicates 
that there is very little overlap between thesis and 
generic summary sentences: just 6% of the 
summary sentences were labeled by human judges 
as thesis statement sentences. This strongly 
suggests that there are critical differences between 
thesis statements and summary sentences, at least 
in first-draft essay writing. It is possible that thesis 
statements reflect an intentional facet (Grosz and 
Sidner, 1986) of language, while summary 
sentences reflect a semantic one (Martin, 1992). 
More detailed experiments need to be carried out 
though before proper conclusions can be derived.  
Table 1a: Agreement between human judges on 
thesis and summary sentence identification. 
Metric Thesis 
Statements 
Summary 
Sentences 
Kappa 0.733 0.603 
P (1 vs. 2) 0.73 0.44 
R (1 vs. 2) 0.69 0.60 
F (1 vs. 2) 0.71 0.51 
 
Table 1b: Percent overlap between human labeled 
thesis statements and summary sentences. 
 Thesis statements  vs. 
Summary sentences 
Percent Overlap 0.06 
 
The results in Table 1a provide an estimate for 
an upper bound of a thesis statement identification 
algorithm. If one can build an automatic classifier 
that identifies thesis statements at recall and 
precision levels as high as 70%, the performance 
of such a classifier will be indistinguishable from 
the performance of humans. 
 
3 A Bayesian Classifier for 
Identifying Thesis Statements 
 
3.1 Description of the Approach 
 
We initially built a Bayesian classifier for 
thesis statements using essay responses to one 
English Proficiency Test (EPT) test question: 
Topic B.  
McCallum and Nigam (1998) discuss two 
probabilistic models for text classification that 
can be used to train Bayesian independence 
classifiers. They describe the multinominal 
model as being the more traditional approach 
for statistical language modeling (especially in 
speech recognition applications), where a 
document is represented by a set of word 
occurrences, and where probability estimates 
reflect the number of word occurrences in a 
document. In using the alternative, 
multivariate Bernoulli model, a document is 
represented by both the absence and presence 
of features. On a text classification task, 
McCallum and Nigam (1998) show that the 
multivariate Bernoulli model performs well 
with small vocabularies, as opposed to the 
multinominal model which performs better 
when larger vocabularies are involved.  
Larkey (1998) uses the multivariate Bernoulli 
approach for an essay scoring task, and her 
results are consistent with the results of 
McCallum and Nigam (1998) (see also Larkey 
and Croft (1996) for descriptions of additional 
applications). In Larkey (1998), sets of essays 
used for training scoring models typically 
contain fewer than 300 documents.  
Furthermore, the vocabulary used across these 
documents tends to be restricted.   
Based on the success of Larkey?s 
experiments, and McCallum and Nigam?s 
findings that the multivariate Bernoulli model 
performs better on texts with small 
vocabularies, this approach would seem to be 
the likely choice when dealing with data sets 
of essay responses. Therefore, we have 
adopted this approach in order to build a thesis 
statement classifier that can select from an 
essay the sentence that is the most likely 
candidate to be labeled as thesis statement.2   
                                                           
2
 In our research, we trained classifiers using a 
classical Bayes approach too, where two classifiers 
were built: a thesis classifier and a non-thesis 
In our experiments, we used three general 
feature types to build the classifier: sentence 
position; words commonly occurring in thesis 
statements; and RST labels from outputs generated 
by an existing rhetorical structure parser (Marcu, 
2000).  
We trained the classifier to predict thesis 
statements in an essay. Using the multivariate 
Bernoulli formula, below, this gives us the log 
probability that a sentence (S) in an essay belongs 
to the class (T) of sentences that are thesis 
statements.  We found that it helped performance 
to use a Laplace estimator to deal with cases where 
the probability estimates were equal to zero. 
 
i i
i ii
log(P(T | S)) =
log(P(T)) +
log(P(A | T) /P(A)),
log(P(A | T) /P(A )),
i
i
if S contains A
if S does not contain A
?????
?
 
 
In this formula, P(T) is the prior probability that a 
sentence is in class T, P(Ai|T) is the conditional 
probability of a sentence having feature Ai , given 
that the sentence is in T, and P(Ai) is the prior 
probability that a sentence contains feature Ai, 
P( iA |T) is the conditional probability that a 
sentence does not have feature Ai, given that it is 
in T, and P( iA ) is the prior probability that a 
sentence does not contain feature Ai.  
 
3.2 Features Used to Classify Thesis 
Statements 
3.2.1 Positional Feature 
We found that the likelihood of a thesis statement 
occurring at the beginning of essays was quite high 
in the human annotated data. To account for this, 
we used one feature that reflected the position of 
each sentence in an essay. 
                                                                                           
classifier. In the classical Bayes implementation, each 
classifier was trained only on positive feature evidence, 
in contrast to the multivariate Bernoulli approach that 
trains classifiers both on the absence and presence of 
features. Since the performance of the classical Bayes 
classifiers was lower than the performance of the 
Bernoulli classifier, we report here only the 
performance of the latter. 
 3.2.2 Lexical Features 
All words from human annotated thesis 
statements were used to build the Bayesian 
classifier. We will refer to these words as the 
thesis word list.  From the training data, a 
vocabulary list was created that included one 
occurrence of each word used in all resolved 
human annotations of thesis statements.  All 
words in this list were used as independent 
lexical features. We found that the use of 
various lists of stop words decreased the 
performance of our classifier, so we did not 
use them. 
3.2.3 Rhetorical Structure Theory 
Features 
According to RST (Mann and Thompson, 
1988), one can associate a rhetorical structure 
tree to any text. The leaves of the tree 
correspond to elementary discourse units and 
the internal nodes correspond to contiguous 
text spans. Each node in a tree is characterized 
by a status (nucleus or satellite) and a 
rhetorical relation, which is a relation that 
holds between two non-overlapping text 
spans.  The distinction between nuclei and 
satellites comes from the empirical 
observation that the nucleus expresses what is 
more essential to the writer?s intention than the 
satellite; and that the nucleus of a rhetorical 
relation is comprehensible independent of the 
satellite, but not vice versa.  When spans are 
equally important, the relation is multinuclear. 
Rhetorical relations reflect semantic, 
intentional, and textual relations that hold 
between text spans as is illustrated in Figure 2. 
For example, one text span may elaborate on 
another text span; the information in two text 
spans may be in contrast; and the information 
in one text span may provide background for 
the information presented in another text span. 
Figure 2 displays in the style of Mann and 
Thompson (1988) the rhetorical structure tree 
of a text fragment. In Figure 2, nuclei are 
represented using straight lines; satellites 
using arcs. Internal nodes are labeled with 
rhetorical relation names.  
We built RST trees automatically for each 
essay using the cue-phrase-based discourse parser 
of Marcu (2000). We then associated with each 
sentence in an essay a feature that reflected the 
status of its parent node (nucleus or satellite), and 
another feature that reflected its rhetorical relation. 
For example, for the last sentence in Figure 2 we 
associated the status satellite and the relation 
elaboration because that sentence is the satellite 
of an elaboration relation.  For sentence 2, we 
associated the status nucleus and the relation 
elaboration because that sentence is the nucleus 
of an elaboration relation.  
We found that some rhetorical relations 
occurred more frequently in sentences annotated as 
thesis statements. Therefore, the conditional 
probabilities for such relations were higher and 
provided evidence that certain sentences were 
thesis statements.  The Contrast relation shown in 
Figure 2, for example, was a rhetorical relation 
that occurred more often in thesis statements.  
Arguably, there may be some overlap between 
words in thesis statements, and rhetorical relations 
used to build the classifier. The RST relations, 
however, capture long distance relations between 
text spans, which are not accounted by the words 
in our thesis word list.  
  
3.3 Evaluation of the Bayesian classifier 
 
We estimated the performance of our system using 
a six-fold cross validation procedure. We 
partitioned the 93 essays that were labeled by both 
human annotators with a thesis statement into six 
groups. (The judges agreed that 7 of the 100 essays 
they annotated had no thesis statement.) We 
trained six times on 5/6 of the labeled data and 
evaluated the performance on the other 1/6 of the 
data. 
The evaluation results in Table 2 show the average 
performance of our classifier with respect to the 
resolved annotation (Alg. wrt. Resolved), using 
traditional recall (R), precision (P), and F-value (F) 
metrics. For purposes of comparison, Table 2 also 
shows the performance of two baselines: the 
random baseline    classifies    the     thesis   
statements  
 Figure 2:  Example of RST tree.
randomly; while the position baseline assumes 
that the thesis statement is given by the first 
sentence in each essay. 
Table 2: Performance of the thesis statement 
classifier.  
System vs. system P R F 
Random baseline 
wrt. Resolved 
0.06 0.05 0.06 
Position baseline wrt. 
Resolved 
0.26 0.22 0.24  
Alg. wrt. Resolved 0.55 0.46 0.50  
1 wrt. 2 0.73 0.69 0.71  
1 wrt. Resolved 0.77 0.78 0.78  
2 wrt. Resolved 0.68 0.74 0.71  
 
4 Generality of the Thesis Statement 
Identifier 
In commercial settings, it is crucial that a 
classifier such as the one discussed in Section 3 
generalizes across different test questions. New 
test questions are introduced on a regular basis; 
so it is important that a classifier that works well 
for a given data set works well for other data 
sets as well, without requiring additional 
annotations and training.  
For the thesis statement classifier it was 
important to determine whether the positional, 
lexical, and RST-specific features are topic 
independent, and thus generalizable to new test 
questions.  If so, this would indicate that we 
could annotate thesis statements across a number 
of topics, and re-use the algorithm on additional 
topics, without further annotation. We asked a 
writing expert to manually annotate the thesis 
statement in approximately 45 essays for 4 
additional test questions: Topics A, C, D and E.  
The annotator completed this task using the 
same interface that was used by the two 
annotators in Experiment 1.  
To test generalizability for each of the five 
EPT questions, the thesis sentences selected by a 
writing expert were used for building the 
classifier.  Five combinations of 4 prompts were 
used to build the classifier in each case, and the 
resulting classifier was then cross-validated on 
the fifth topic, which was treated as test data.  
To evaluate the performance of each of the 
classifiers, agreement was calculated for each 
?cross-validation? sample (single topic) by 
comparing the algorithm selection to our writing 
expert?s thesis statement selections.  For 
example, we trained on Topics A, C, D, and E, 
using the thesis statements selected manually.  
This classifier was then used to select, 
automatically, thesis statements for Topic B.  In 
the evaluation, the algorithm?s selection was 
compared to the manually selected set of thesis 
statements for Topic B, and agreement was 
calculated. Table 3 illustrates that in all but one 
case, agreement exceeds both baselines from 
Table 2.  In this set of manual annotations, the 
human judge almost always selected one 
sentence as the thesis statement.  This is why 
Precision, Recall, and the F-value are often 
equal in Table 3. 
Table 3: Cross-topic generalizability of the thesis 
statement classifier. 
Training 
Topics 
CV Topic P R  F  
ABCD   E 0.36 0.36 0.36 
ABCE   D 0.49 0.49 0.49 
ABDE   C 0.45 0.45 0.45 
ACDE   B 0.60 0.59 0.59 
BCDE   A 0.25 0.24 0.25 
Mean  0.43 0.43 0.43 
 5 Discussion and Conclusions 
 
The results of our experimental work indicate 
that the task of identifying thesis statements in 
essays is well defined. The empirical evaluation 
of our algorithm indicates that with a relatively 
small corpus of manually annotated essay data, 
one can build a Bayes classifier that identifies 
thesis statements with good accuracy. The 
evaluations also provide evidence that this 
method for automated thesis selection in essays 
is generalizable.  That is, once trained on a few 
human annotated prompts, it can be applied to 
other prompts given a similar population of 
writers, in this case, writers at the college 
freshman level.  The larger implication is that 
we begin to see that there are underlying 
discourse elements in essays that can be 
identified, independent of the topic of the test 
question. For essay evaluation applications this 
is critical since new test questions are 
continuously being introduced into on-line essay 
evaluation applications.  
Our results compare favorably with results 
reported by Teufel and Moens (1999) who also 
use Bayes classification techniques to identify 
rhetorical arguments such as aim and 
background in scientific texts, although the texts 
we are working with are extremely noisy. 
Because EPT essays are often produced for 
high-stake exams, under severe time constraints, 
they are often ungrammatical, repetitive, and 
poorly organized at the discourse level. 
Current investigations indicate that this 
technique can be used to reliably identify other 
essay-specific discourse elements, such as, 
concluding statements, main points of 
arguments, and supporting ideas.  In addition, 
we are exploring how we can use estimated 
probabilities as confidence measures of the 
decisions made by the system. If the confidence 
level associated with the identification of a 
thesis statement is low, the system would 
instruct the student that no explicit thesis 
statement has been found in the essay. 
 
Acknowledgements 
 
We would like to thank our annotation 
experts, Marisa Farnum, Hilary Persky, Todd 
Farley, and Andrea King. 
 
References 
 
Burstein, J., Kukich, K. Wolff, S. Lu, C. 
Chodorow, M, Braden-Harder, L. and Harris 
M.D. (1998). Automated Scoring Using A 
Hybrid Feature Identification Technique. 
Proceedings of ACL, 206-210. 
Foltz, P. W., Kintsch, W., and Landauer, T.. 
(1998). The Measurement of Textual Coherence 
with Latent Semantic Analysis. Discourse 
Processes, 25(2&3), 285-307. 
Grosz B. and Sidner, C. (1986). Attention, 
Intention, and the Structure of Discourse. 
Computational Linguistics, 12 (3), 175-204. 
Krippendorff K. (1980). Content Analysis: 
An Introduction to Its Methodology. Sage Publ. 
Larkey, L. and Croft, W. B. (1996).  
Combining Classifiers in Text Categorization. 
Proceedings of  SIGIR,  289-298. 
Larkey, L. (1998). Automatic Essay Grading 
Using Text Categorization Techniques.  
Proceedings of SIGIR, pages 90-95. 
Mani, I. and Maybury, M. (1999). Advances 
in Automatic Text Summarization. The MIT 
Press. 
Mann, W.C. and Thompson, S.A.(1988). 
Rhetorical Structure Theory: Toward a 
Functional Theory of Text Organization. Text 
8(3), 243?281. 
Martin, J. (1992). English Text. System and 
Structure. John Benjamin Publishers.  
 Marcu, D. (2000). The Theory and Practice 
of Discourse Parsing and Summarization. The 
MIT Press.  
McCallum, A. and Nigam, K. (1998). A 
Comparison of Event Models for Naive Bayes 
Text Classification. The AAAI-98 Workshop on 
"Learning for Text Categorization".  
Page, E.B. and Peterson, N. (1995). The 
computer moves into essay grading: updating 
the ancient test. Phi Delta Kappa, March, 561-
565. 
Scardamalia, M. and Bereiter, C. (1985). 
Development of Dialectical Processes in 
Composition.  In Olson, D. R., Torrance, N. and 
Hildyard, A. (eds), Literacy, Language, and 
Learning: The nature of consequences of 
reading and writing.  Cambridge University 
Press. 
Siegel S. and Castellan, N.J. (1988). 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill. 
Teufel , S. and Moens, M. (1999). Discourse-
level argumentation in scientific articles. 
Proceedings of the ACL99 Workshop on 
Standards and Tools for Discourse Tagging. 
White E.M. (1994). Teaching and Assessing 
Writing. Jossey-Bass Publishers, 103-108. 
Fast Decoding and Optimal Decoding for Machine Translation
Ulrich Germann   , Michael Jahr  , Kevin Knight   , Daniel Marcu   , and Kenji Yamada  
 
Information Sciences Institute  Department of Computer Science
University of Southern California Stanford University
4676 Admiralty Way, Suite 1001 Stanford, CA 94305
Marina del Rey, CA 90292 jahr@cs.stanford.edu

germann,knight,marcu,kyamada  @isi.edu
Abstract
A good decoding algorithm is critical
to the success of any statistical machine
translation system. The decoder?s job is
to find the translation that is most likely
according to set of previously learned
parameters (and a formula for combin-
ing them). Since the space of possi-
ble translations is extremely large, typ-
ical decoding algorithms are only able
to examine a portion of it, thus risk-
ing to miss good solutions. In this pa-
per, we compare the speed and out-
put quality of a traditional stack-based
decoding algorithm with two new de-
coders: a fast greedy decoder and a
slow but optimal decoder that treats de-
coding as an integer-programming opti-
mization problem.
1 Introduction
A statistical MT system that translates (say)
French sentences into English, is divided into
three parts: (1) a language model (LM) that as-
signs a probability P(e) to any English string, (2) a
translation model (TM) that assigns a probability
P(f  e) to any pair of English and French strings,
and (3) a decoder. The decoder takes a previ-
ously unseen sentence  and tries to find the 
that maximizes P(e  f), or equivalently maximizes
P(e)  P(f  e).
Brown et al (1993) introduced a series of
TMs based on word-for-word substitution and re-
ordering, but did not include a decoding algo-
rithm. If the source and target languages are con-
strained to have the same word order (by choice
or through suitable pre-processing), then the lin-
ear Viterbi algorithm can be applied (Tillmann et
al., 1997). If re-ordering is limited to rotations
around nodes in a binary tree, then optimal decod-
ing can be carried out by a high-polynomial algo-
rithm (Wu, 1996). For arbitrary word-reordering,
the decoding problem is NP-complete (Knight,
1999).
A sensible strategy (Brown et al, 1995; Wang
and Waibel, 1997) is to examine a large subset of
likely decodings and choose just from that. Of
course, it is possible to miss a good translation
this way. If the decoder returns e  but there exists
some e for which P(e  f) 	 P(e   f), this is called
a search error. As Wang and Waibel (1997) re-
mark, it is hard to know whether a search error
has occurred?the only way to show that a decod-
ing is sub-optimal is to actually produce a higher-
scoring one.
Thus, while decoding is a clear-cut optimiza-
tion task in which every problem instance has a
right answer, it is hard to come up with good
answers quickly. This paper reports on mea-
surements of speed, search errors, and translation
quality in the context of a traditional stack de-
coder (Jelinek, 1969; Brown et al, 1995) and two
new decoders. The first is a fast greedy decoder,
and the second is a slow optimal decoder based on
generic mathematical programming techniques.
2 IBM Model 4
In this paper, we work with IBM Model 4, which
revolves around the notion of a word alignment
over a pair of sentences (see Figure 1). A word
alignment assigns a single home (English string
position) to each French word. If two French
words align to the same English word, then that
it is not clear .
| \ | \ \
| \ + \ \
| \/ \ \ \
| /\ \ \ \
CE NE EST PAS CLAIR .
Figure 1: Sample word alignment.
English word is said to have a fertility of two.
Likewise, if an English word remains unaligned-
to, then it has fertility zero. The word align-
ment in Figure 1 is shorthand for a hypothetical
stochastic process by which an English string gets
converted into a French string. There are several
sets of decisions to be made.
First, every English word is assigned a fertil-
ity. These assignments are made stochastically
according to a table n( 
 e  ). We delete from
the string any word with fertility zero, we dupli-
cate any word with fertility two, etc. If a word has
fertility greater than zero, we call it fertile. If its
fertility is greater than one, we call it very fertile.
After each English word in the new string, we
may increment the fertility of an invisible En-
glish NULL element with probability p  (typi-
cally about 0.02). The NULL element ultimately
produces ?spurious? French words.
Next, we perform a word-for-word replace-
ment of English words (including NULL) by
French words, according to the table t(f e  ).
Finally, we permute the French words. In per-
muting, Model 4 distinguishes between French
words that are heads (the leftmost French word
generated from a particular English word), non-
heads (non-leftmost, generated only by very fer-
tile English words), and NULL-generated.
Heads. The head of one English word is as-
signed a French string position based on the po-
sition assigned to the previous English word. If
an English word e  translates into something
at French position j, then the French head word
of e  is stochastically placed in French position
k with distortion probability d  (k?j  class(e ),
class(f  )), where ?class? refers to automatically
determined word classes for French and English
vocabulary items. This relative offset k?j encour-
ages adjacent English words to translate into ad-
jacent French words. If e  is infertile, then j is
taken from e  , etc. If e  is very fertile, then j
is the average of the positions of its French trans-
lations.
Non-heads. If the head of English word e 
is placed in French position j, then its first non-
head is placed in French position k ( 	 j) accord-
ing to another table d  (k?j  class(f  )). The next
non-head is placed at position q with probability
d  (q?k  class(f  )), and so forth.
NULL-generated. After heads and non-heads
are placed, NULL-generated words are permuted
into the remaining vacant slots randomly. If there
are 
 NULL-generated words, then any place-
ment scheme is chosen with probability 1/ 
 ffTowards a Unified Approach to Memory- and Statistical-Based
Machine Translation
Daniel Marcu
Information Sciences Institute and
Department of Computer Science
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
We present a set of algorithms that en-
able us to translate natural language
sentences by exploiting both a trans-
lation memory and a statistical-based
translation model. Our results show
that an automatically derived transla-
tion memory can be used within a sta-
tistical framework to often find trans-
lations of higher probability than those
found using solely a statistical model.
The translations produced using both
the translation memory and the sta-
tistical model are significantly better
than translations produced by two com-
mercial systems: our hybrid system
translated perfectly 58% of the 505
sentences in a test collection, while
the commercial systems translated per-
fectly only 40-42% of them.
1 Introduction
Over the last decade, much progress has been
made in the fields of example-based (EBMT) and
statistical machine translation (SMT). EBMT sys-
tems work by modifying existing, human pro-
duced translation instances, which are stored in
a translation memory (TMEM). Many methods
have been proposed for storing translation pairs
in a TMEM, finding translation examples that
are relevant for translating unseen sentences, and
modifying and integrating translation fragments
to produce correct outputs. Sato (1992), for ex-
ample, stores complete parse trees in the TMEM
and selects and generates new translations by
performing similarity matchings on these trees.
Veale and Way (1997) store complete sentences;
new translations are generated by modifying the
TMEM translation that is most similar to the in-
put sentence. Others store phrases; new trans-
lations are produced by optimally partitioning
the input into phrases that match examples from
the TMEM (Maruyana and Watanabe, 1992), or
by finding all partial matches and then choosing
the best possible translation using a multi-engine
translation system (Brown, 1999).
With a few exceptions (Wu and Wong, 1998),
most SMT systems are couched in the noisy chan-
nel framework (see Figure 1). In this framework,
the source language, let?s say English, is assumed
to be generated by a noisy probabilistic source.1
Most of the current statistical MT systems treat
this source as a sequence of words (Brown et al,
1993). (Alternative approaches exist, in which the
source is taken to be, for example, a sequence of
aligned templates/phrases (Wang, 1998; Och et
al., 1999) or a syntactic tree (Yamada and Knight,
2001).) In the noisy-channel framework, a mono-
lingual corpus is used to derive a statistical lan-
guage model that assigns a probability to a se-
quence of words or phrases, thus enabling one to
distinguish between sequences of words that are
grammatically correct and sequences that are not.
A sentence-aligned parallel corpus is then used
in order to build a probabilistic translation model
1For the rest of this paper, we use the terms source
and target languages according to the jargon specific to the
noisy-channel framework. In this framework, the source lan-
guage is the language into which the machine translation
system translates.
Source
P(e)
Decoder
Channel
P(f | e) f
observed 
f
best
e
argmax P(e | f) = argmax P(f | e) P(e)
e e
e
Figure 1: The noisy channel model.
that explains how the source can be turned into
the target and that assigns a probability to every
way in which a source e can be mapped into a tar-
get f. Once the parameters of the language and
translation models are estimated using traditional
maximum likelihood and EM techniques (Demp-
ster et al, 1977), one can take as input any string
in the target language f, and find the source e of
highest probability that could have generated the
target, a process called decoding (see Figure 1).
It is clear that EBMT and SMT systems have
different strengths and weaknesses. If a sen-
tence to be translated or a very similar one can be
found in the TMEM, an EBMT system has a good
chance of producing a good translation. How-
ever, if the sentence to be translated has no close
matches in the TMEM, then an EBMT system is
less likely to succeed. In contrast, an SMT sys-
tem may be able to produce perfect translations
even when the sentence given as input does not
resemble any sentence from the training corpus.
However, such a system may be unable to gener-
ate translations that use idioms and phrases that
reflect long-distance dependencies and contexts,
which are usually not captured by current transla-
tion models.
This paper advances the state-of-the-art in two
respects. First, we show how one can use an ex-
isting statistical translation model (Brown et al,
1993) in order to automatically derive a statistical
TMEM. Second, we adapt a decoding algorithm
so that it can exploit information specific both to
the statistical TMEM and the translation model.
Our experiments show that the automatically de-
rived translation memory can be used within the
statistical framework to often find translations of
higher probability than those found using solely
the statistical model. The translations produced
using both the translation memory and the statisti-
cal model are significantly better than translations
produced by two commercial systems.
2 The IBM Model 4
For the work described in this paper we used a
modified version of the statistical machine trans-
lation tool developed in the context of the 1999
Johns Hopkins? Summer Workshop (Al-Onaizan
et al, 1999), which implements IBM translation
model 4 (Brown et al, 1993).
IBM model 4 revolves around the notion of
word alignment over a pair of sentences (see Fig-
ure 2). The word alignment is a graphical repre-
sentation of an hypothetical stochastic process by
which a source string e is converted into a target
string f. The probability of a given alignment a
and target sentence f given a source sentence e is
given by
P(a, f   e) =



n 
	 
 
e 







t   
 
e 




ffAn Unsupervised Approach to Recognizing Discourse Relations
Daniel Marcu and Abdessamad Echihabi
Information Sciences Institute and
Department of Computer Science
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
 
marcu,echihabi  @isi.edu
Abstract
We present an unsupervised approach to
recognizing discourse relations of CON-
TRAST, EXPLANATION-EVIDENCE, CON-
DITION and ELABORATION that hold be-
tween arbitrary spans of texts. We show
that discourse relation classifiers trained
on examples that are automatically ex-
tracted from massive amounts of text can
be used to distinguish between some of
these relations with accuracies as high as
93%, even when the relations are not ex-
plicitly marked by cue phrases.
1 Introduction
In the field of discourse research, it is now widely
agreed that sentences/clauses are usually not un-
derstood in isolation, but in relation to other sen-
tences/clauses. Given the high level of interest in
explaining the nature of these relations and in pro-
viding definitions for them (Mann and Thompson,
1988; Hobbs, 1990; Martin, 1992; Lascarides and
Asher, 1993; Hovy and Maier, 1993; Knott and
Sanders, 1998), it is surprising that there are no ro-
bust programs capable of identifying discourse rela-
tions that hold between arbitrary spans of text. Con-
sider, for example, the sentence/clause pairs below.
a. Such standards would preclude arms sales to
states like Libya, which is also currently sub-
ject to a U.N. embargo.
b. But states like Rwanda before its present crisis
would still be able to legally buy arms.
(1)
a. South Africa can afford to forgo sales of guns
and grenades
b. because it actually makes most of its profits
from the sale of expensive, high-technology
systems like laser-designated missiles, air-
craft electronic warfare systems, tactical ra-
dios, anti-radiation bombs and battlefield mo-
bility systems.
(2)
In these examples, the discourse markers But and
because help us figure out that a CONTRAST re-
lation holds between the text spans in (1) and an
EXPLANATION-EVIDENCE relation holds between
the spans in (2). Unfortunately, cue phrases do not
signal all relations in a text. In the corpus of Rhetori-
cal Structure trees (www.isi.edu/ marcu/discourse/)
built by Carlson et al (2001), for example, we have
observed that only 61 of 238 CONTRAST relations
and 79 out of 307 EXPLANATION-EVIDENCE rela-
tions that hold between two adjacent clauses were
marked by a cue phrase.
So what shall we do when no discourse
markers are used? If we had access to ro-
bust semantic interpreters, we could, for
example, infer from sentence 1.a that ?can-
not buy arms legally(libya)?, infer from sen-
tence 1.b that ?can buy arms legally(rwanda)?, use
our background knowledge in order to infer that
?similar(libya,rwanda)?, and apply Hobbs?s (1990)
definitions of discourse relations to arrive at the
conclusion that a CONTRAST relation holds between
the sentences in (1). Unfortunately, the state of the
art in NLP does not provide us access to semantic
interpreters and general purpose knowledge bases
that would support these kinds of inferences.
The discourse relation definitions proposed by
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 368-375.
                         Proceedings of the 40th Annual Meeting of the Association for
others (Mann and Thompson, 1988; Lascarides
and Asher, 1993; Knott and Sanders, 1998) are
not easier to apply either because they assume
the ability to automatically derive, in addition to
the semantics of the text spans, the intentions and
illocutions associated with them as well.
In spite of the difficulty of determining the dis-
course relations that hold between arbitrary text
spans, it is clear that such an ability is important
in many applications. First, a discourse relation
recognizer would enable the development of im-
proved discourse parsers and, consequently, of high
performance single document summarizers (Marcu,
2000). In multidocument summarization (DUC,
2002), it would enable the development of summa-
rization programs capable of identifying contradic-
tory statements both within and across documents
and of producing summaries that reflect not only
the similarities between various documents, but also
their differences. In question-answering, it would
enable the development of systems capable of an-
swering sophisticated, non-factoid queries, such as
?what were the causes of X?? or ?what contradicts
Y??, which are beyond the state of the art of current
systems (TREC, 2001).
In this paper, we describe experiments aimed at
building robust discourse-relation classification sys-
tems. To build such systems, we train a family of
Naive Bayes classifiers on a large set of examples
that are generated automatically from two corpora:
a corpus of 41,147,805 English sentences that have
no annotations, and BLIPP, a corpus of 1,796,386
automatically parsed English sentences (Charniak,
2000), which is available from the Linguistic Data
Consortium (www.ldc.upenn.edu). We study empir-
ically the adequacy of various features for the task
of discourse relation classification and we show that
some discourse relations can be correctly recognized
with accuracies as high as 93%.
2 Discourse relation definitions and
generation of training data
2.1 Background
In order to build a discourse relation classifier, one
first needs to decide what relation definitions one
is going to use. In Section 1, we simply relied on
the reader?s intuition when we claimed that a CON-
TRAST relation holds between the sentences in (1).
In reality though, associating a discourse relation
with a text span pair is a choice that is clearly in-
fluenced by the theoretical framework one is willing
to adopt.
If we adopt, for example, Knott and
Sanders?s (1998) account, we would say that
the relation between sentences 1.a and 1.b is
ADDITIVE, because no causal connection exists
between the two sentences, PRAGMATIC, because
the relation pertains to illocutionary force and
not to the propositional content of the sentences,
and NEGATIVE, because the relation involves a
CONTRAST between the two sentences. In the
same framework, the relation between clauses 2.a
and 2.b will be labeled as CAUSAL-SEMANTIC-
POSITIVE-NONBASIC. In Lascarides and Asher?s
theory (1993), we would label the relation between
2.a and 2.b as EXPLANATION because the event in
2.b explains why the event in 2.a happened (perhaps
by CAUSING it). In Hobbs?s theory (1990), we
would also label the relation between 2.a and 2.b
as EXPLANATION because the event asserted by
2.b CAUSED or could CAUSE the event asserted in
2.a. And in Mann and Thompson theory (1988), we
would label sentence pairs 1.a, 1.b as CONTRAST
because the situations presented in them are the
same in many respects (the purchase of arms),
because the situations are different in some respects
(Libya cannot buy arms legally while Rwanda can),
and because these situations are compared with
respect to these differences. By a similar line of
reasoning, we would label the relation between 2.a
and 2.b as EVIDENCE.
The discussion above illustrates two points. First,
it is clear that although current discourse theories are
built on fundamentally different principles, they all
share some common intuitions. Sure, some theo-
ries talk about ?negative polarity? while others about
?contrast?. Some theories refer to ?causes?, some to
?potential causes?, and some to ?explanations?. But
ultimately, all these theories acknowledge that there
are such things as CONTRAST, CAUSE, and EXPLA-
NATION relations. Second, given the complexity of
the definitions these theories propose, it is clear why
it is difficult to build programs that recognize such
relations in unrestricted texts. Current NLP tech-
niques do not enable us to reliably infer from sen-
tence 1.a that ?cannot buy arms legally(libya)? and
do not give us access to general purpose knowledge
bases that assert that ?similar(libya,rwanda)?.
The approach we advocate in this paper is in some
respects less ambitious than current approaches to
discourse relations because it relies upon a much
smaller set of relations than those used by Mann and
Thompson (1988) or Martin (1992). In our work,
we decide to focus only on four types of relations,
which we call: CONTRAST, CAUSE-EXPLANATION-
EVIDENCE (CEV), CONDITION, and ELABORA-
TION. (We define these relations in Section 2.2.) In
other respects though, our approach is more ambi-
tious because it focuses on the problem of recog-
nizing such discourse relations in unrestricted texts.
In other words, given as input sentence pairs such
as those shown in (1)?(2), we develop techniques
and programs that label the relations that hold be-
tween these sentence pairs as CONTRAST, CAUSE-
EXPLANATION-EVIDENCE, CONDITION, ELABO-
RATION or NONE-OF-THE-ABOVE, even when the
discourse relations are not explicitly signalled by
discourse markers.
2.2 Discourse relation definitions
The discourse relations we focus on are defined
at a much coarser level of granularity than in
most discourse theories. For example, we con-
sider that a CONTRAST relation holds between two
text spans if one of the following relations holds:
CONTRAST, ANTITHESIS, CONCESSION, or OTH-
ERWISE, as defined by Mann and Thompson (1988),
CONTRAST or VIOLATED EXPECTATION, as defined
by Hobbs (1990), or any of the relations character-
ized by this regular expression of cognitive prim-
itives, as defined by Knott and Sanders (1998):
(CAUSAL  ADDITIVE) ? (SEMANTIC  PRAGMATIC)
? NEGATIVE. In other words, in our approach, we do
not distinguish between contrasts of semantic and
pragmatic nature, contrasts specific to violated ex-
pectations, etc. Table 1 shows the definitions of the
relations we considered.
The advantage of operating with coarsely defined
discourse relations is that it enables us to automat-
ically construct relatively low-noise datasets that
can be used for learning. For example, by extract-
ing sentence pairs that have the keyword ?But? at
the beginning of the second sentence, as the sen-
tence pair shown in (1), we can automatically col-
lect many examples of CONTRAST relations. And by
extracting sentences that contain the keyword ?be-
cause?, we can automatically collect many examples
of CAUSE-EXPLANATION-EVIDENCE relations. As
previous research in linguistics (Halliday and Hasan,
1976; Schiffrin, 1987) and computational linguis-
tics (Marcu, 2000) show, some occurrences of ?but?
and ?because? do not have a discourse function; and
others signal other relations than CONTRAST and
CAUSE-EXPLANATION. So we can expect the ex-
amples we extract to be noisy. However, empiri-
cal work of Marcu (2000) and Carlson et al (2001)
suggests that the majority of occurrences of ?but?,
for example, do signal CONTRAST relations. (In the
RST corpus built by Carlson et al (2001), 89 out of
the 106 occurrences of ?but? that occur at the begin-
ning of a sentence signal a CONTRAST relation that
holds between the sentence that contains the word
?but? and the sentence that precedes it.) Our hope
is that simple extraction methods are sufficient for
collecting low-noise training corpora.
2.3 Generation of training data
In order to collect training cases, we mined in an
unsupervised manner two corpora. The first corpus,
which we call Raw, is a corpus of 1 billion words of
unannotated English (41,147,805 sentences) that we
created by catenating various corpora made avail-
able over the years by the Linguistic Data Consor-
tium. The second, called BLIPP, is a corpus of only
1,796,386 sentences that were parsed automatically
by Charniak (2000). We extracted from both cor-
pora all adjacent sentence pairs that contained the
cue phrase ?But? at the beginning of the second sen-
tence and we automatically labeled the relation be-
tween the two sentence pairs as CONTRAST. We also
extracted all the sentences that contained the word
?but? in the middle of a sentence; we split each ex-
tracted sentence into two spans, one containing the
words from the beginning of the sentence to the oc-
currence of the keyword ?but? and one containing
the words from the occurrence of ?but? to the end
of the sentence; and we labeled the relation between
the two resulting text spans as CONTRAST as well.
Table 2 lists some of the cue phrases we
used in order to extract CONTRAST, CAUSE-
EXPLANATION-EVIDENCE, ELABORATION, and
CONTRAST CAUSE-EXPLANATION-EVIDENCE ELABORATION CONDITION
ANTITHESIS (M&T) EVIDENCE (M&T) ELABORATION (M&T) CONDITION (M&T)
CONCESSION (M&T) VOLITIONAL-CAUSE (M&T) EXPANSION (Ho)
OTHERWISE (M&T) NONVOLITIONAL-CAUSE (M&T) EXEMPLIFICATION (Ho)
CONTRAST (M&T) VOLITIONAL-RESULT (M&T) ELABORATION (A&L)
VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&T)
EXPLANATION (Ho)
( CAUSAL  ADDITIVE ) - RESULT (A&L)
( SEMANTIC  PRAGMATIC ) - EXPLANATION (A&L)
NEGATIVE (K&S)
CAUSAL -
(SEMANTIC  PRAGMATIC ) -
POSITIVE (K&S)
Table 1: Relation definitions as union of definitions proposed by other researchers (M&T ? (Mann and
Thompson, 1988); Ho ? (Hobbs, 1990); A&L ? (Lascarides and Asher, 1993); K&S ? (Knott and Sanders,
1998)).
CONTRAST ? 3,881,588 examples
[BOS  EOS] [BOS But  EOS]
[BOS  ] [but  EOS]
[BOS  ] [although  EOS]
[BOS Although  ,] [  EOS]
CAUSE-EXPLANATION-EVIDENCE ? 889,946 examples
[BOS  ] [because  EOS]
[BOS Because  ,] [  EOS]
[BOS  EOS] [BOS Thus,  EOS]
CONDITION ? 1,203,813 examples
[BOS If  ,] [  EOS]
[BOS If  ] [then  EOS]
[BOS  ] [if  EOS]
ELABORATION ? 1,836,227 examples
[BOS  EOS] [BOS  for example  EOS]
[BOS  ] [which  ,]
NO-RELATION-SAME-TEXT ? 1,000,000 examples
Randomly extract two sentences that are more
than 3 sentences apart in a given text.
NO-RELATION-DIFFERENT-TEXTS ? 1,000,000 examples
Randomly extract two sentences from two
different documents.
Table 2: Patterns used to automatically construct a
corpus of text span pairs labeled with discourse re-
lations.
CONDITION relations and the number of examples
extracted from the Raw corpus for each type of dis-
course relation. In the patterns in Table 2, the sym-
bols BOS and EOS denote BeginningOfSentence
and EndOfSentence boundaries, the ? 	
	
	 ? stand for
occurrences of any words and punctuation marks,
the square brackets stand for text span boundaries,
and the other words and punctuation marks stand for
the cue phrases that we used in order to extract dis-
course relation examples. For example, the pattern
[BOS Although 	
	
	 ,] [ 	
	
	 EOS] is used in order to
extract examples of CONTRAST relations that hold
between a span of text delimited to the left by the
cue phrase ?Although? occurring in the beginning of
a sentence and to the right by the first occurrence of
a comma, and a span of text that contains the rest of
the sentence to which ?Although? belongs.
We also extracted automatically 1,000,000 exam-
ples of what we hypothesize to be non-relations, by
randomly selecting non-adjacent sentence pairs that
are at least 3 sentences apart in a given text. We label
such examples NO-RELATION-SAME-TEXT. And
we extracted automatically 1,000,000 examples of
what we hypothesize to be cross-document non-
relations, by randomly selecting two sentences from
distinct documents. As in the case of CONTRAST
and CONDITION, the NO-RELATION examples are
also noisy because long distance relations are com-
mon in well-written texts.
3 Determining discourse relations using
Naive Bayes classifiers
We hypothesize that we can determine that a CON-
TRAST relation holds between the sentences in (3)
even if we cannot semantically interpret the two sen-
tences, simply because our background knowledge
tells us that good and fails are good indicators of
contrastive statements.
 John is good in math and sciences.
 Paul fails almost every class he takes.
(3)
Similarly, we hypothesize that we can determine that
a CONTRAST relation holds between the sentences
in (1), because our background knowledge tells us
that embargo and legally are likely to occur in con-
texts of opposite polarity. In general, we hypothe-
size that lexical item pairs can provide clues about
the discourse relations that hold between the text
spans in which the lexical items occur.
To test this hypothesis, we need to solve two
problems. First, we need a means to acquire vast
amounts of background knowledge from which we
can derive, for example, that the word pairs good
? fails and embargo ? legally are good indicators
of CONTRAST relations. The extraction patterns de-
scribed in Table 2 enable us to solve this problem.1
Second, given vast amounts of training material, we
need a means to learn which pairs of lexical items
are likely to co-occur in conjunction with each dis-
course relation and a means to apply the learned pa-
rameters to any pair of text spans in order to deter-
mine the discourse relation that holds between them.
We solve the second problem in a Bayesian proba-
bilistic framework.
We assume that a discourse relation  that holds
between two text spans,  , is determined by
the word pairs in the cartesian product defined over
the words in the two text spans ffA Noisy-Channel Model for Document Compression
Hal Daume? III and Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
hdaume,marcu  @isi.edu
Abstract
We present a document compression sys-
tem that uses a hierarchical noisy-channel
model of text production. Our compres-
sion system first automatically derives the
syntactic structure of each sentence and
the overall discourse structure of the text
given as input. The system then uses a sta-
tistical hierarchical model of text produc-
tion in order to drop non-important syn-
tactic and discourse constituents so as to
generate coherent, grammatical document
compressions of arbitrary length. The sys-
tem outperforms both a baseline and a
sentence-based compression system that
operates by simplifying sequentially all
sentences in a text. Our results support
the claim that discourse knowledge plays
an important role in document summariza-
tion.
1 Introduction
Single document summarization systems proposed
to date fall within one of the following three classes:
Extractive summarizers simply select and present
to the user the most important sentences in
a text ? see (Mani and Maybury, 1999;
Marcu, 2000; Mani, 2001) for comprehensive
overviews of the methods and algorithms used
to accomplish this.
Headline generators are noisy-channel probabilis-
tic systems that are trained on large corpora
of  Headline, Text  pairs (Banko et al, 2000;
Berger and Mittal, 2000). These systems pro-
duce short sequences of words that are indica-
tive of the content of the text given as input.
Sentence simplification systems (Chandrasekar et
al., 1996; Mahesh, 1997; Carroll et al, 1998;
Grefenstette, 1998; Jing, 2000; Knight and
Marcu, 2000) are capable of compressing long
sentences by deleting unimportant words and
phrases.
Extraction-based summarizers often produce out-
puts that contain non-important sentence fragments.
For example, the hypothetical extractive summary
of Text (1), which is shown in Table 1, can be com-
pacted further by deleting the clause ?which is al-
ready almost enough to win?. Headline-based sum-
maries, such as that shown in Table 1, are usually
indicative of a text?s content but not informative,
grammatical, or coherent. By repeatedly applying a
sentence-simplification algorithm one sentence at a
time, one can compress a text; yet, the outputs gen-
erated in this way are likely to be incoherent and
to contain unimportant information. When summa-
rizing text, some sentences should be dropped alto-
gether.
Ideally, we would like to build systems that have
the strengths of all these three classes of approaches.
The ?Document Compression? entry in Table 1
shows a grammatical, coherent summary of Text (1),
which was generated by a hypothetical document
compression system that preserves the most impor-
tant information in a text while deleting sentences,
phrases, and words that are subsidiary to the main
message of the text. Obviously, generating coher-
ent, grammatical summaries such as that produced
by the hypothetical document compression system
in Table 1 is not trivial because of many conflicting
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 449-456.
                         Proceedings of the 40th Annual Meeting of the Association for
Type of Hypothetical output Output Output is Output is
Summarizer contains only coherent grammatical
important info
Extractive John Doe has already secured the vote of most 
summarizer democrats in his constituency, which is already
almost enough to win. But without the support
of the governer, he is still on shaky ground.
Headline mayor vote constituency governer 
generator
Sentence The mayor is now looking for re-election. John Doe 
simplifier has already secured the vote of most democrats
in his constituency. He is still on shaky ground.
Document John Doe has secured the vote of most democrats.   
compressor But he is still on shaky ground.
Table 1: Hypothetical outputs generated by various types of summarizers.
goals1. The deletion of certain sentences may result
in incoherence and information loss. The deletion of
certain words and phrases may also lead to ungram-
maticality and information loss.
The mayor is now looking for re-election. John Doe
has already secured the vote of most democrats in his
constituency, which is already almost enough to win.
But without the support of the governer, he is still on
shaky grounds.
(1)
In this paper, we present a document compression
system that uses hierarchical models of discourse
and syntax in order to simultaneously manage all
these conflicting goals. Our compression system
first automatically derives the syntactic structure of
each sentence and the overall discourse structure of
the text given as input. The system then uses a sta-
tistical hierarchical model of text production in or-
der to drop non-important syntactic and discourse
units so as to generate coherent, grammatical doc-
ument compressions of arbitrary length. The system
outperforms both a baseline and a sentence-based
compression system that operates by simplifying se-
quentially all sentences in a text.
2 Document Compression
The document compression task is conceptually
simple. Given a document 
			 , our
goal is to produce a new document  by ?dropping?
words 	A Noisy-Channel Approach to Question Answering 
Abdessamad Echihabi and Daniel Marcu 
Information Sciences Institute 
Department of Computer Science 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina Del Rey, CA 90292 
{echihabi,marcu}@isi.edu 
 
Abstract 
We introduce a probabilistic noisy-
channel model for question answering and 
we show how it can be exploited in the 
context of an end-to-end QA system. Our 
noisy-channel system outperforms a state-
of-the-art rule-based QA system that uses 
similar resources. We also show that the 
model we propose is flexible enough to 
accommodate within one mathematical 
framework many QA-specific resources 
and techniques, which range from the 
exploitation of WordNet, structured, and 
semi-structured databases to reasoning, 
and paraphrasing. 
1 Introduction 
Current state-of-the-art Question Answering (QA) 
systems are extremely complex. They contain tens 
of modules that do everything from information 
retrieval, sentence parsing (Ittycheriah and 
Roukos, 2002; Hovy et al, 2001; Moldovan et al 
2002), question-type pinpointing (Ittycheriah and 
Roukos, 2002; Hovy et al, 2001; Moldovan et al 
2002), semantic analysis (Xu et al, Hovy et al, 
2001; Moldovan et al 2002), and reasoning 
(Moldovan et al 2002). They access external 
resources such as the WordNet (Hovy et al, 2001, 
Pasca and Harabagiu, 2001, Prager et al, 2001), 
the web (Brill et al, 2001), structured, and semi-
structured databases (Katz et al, 2001; Lin, 2002; 
Clarke, 2001). They contain feedback loops, 
ranking, and re-ranking modules. Given their 
complexity, it is often difficult (and sometimes 
impossible) to understand what contributes to the 
performance of a system and what doesn?t.  
In this paper, we propose a new approach to 
QA in which the contribution of various resources 
and components can be easily assessed.  The 
fundamental insight of our approach, which 
departs significantly from the current architectures, 
is that, at its core, a QA system is a pipeline of 
only two modules:  
? An IR engine that retrieves a set of M 
documents/N sentences that may contain 
answers to a given question Q. 
? And an answer identifier module that given 
a question Q and a sentence S (from the set 
of sentences retrieved by the IR engine) 
identifies a sub-string SA of S that is likely 
to be an answer to Q and assigns a score to 
it.  
Once one has these two modules, one has a QA 
system because finding the answer to a question Q 
amounts to selecting the sub-string SA of highest 
score. Although this view is not made explicit by 
QA researchers, it is implicitly present in all 
systems we are aware of. 
In its simplest form, if one accepts a whole 
sentence as an answer (SA = S), one can assess the 
likelihood that a sentence S contains the answer to 
a question Q by measuring the cosine similarity 
between Q and S. However, as research in QA 
demonstrates, word-overlap is not a good enough 
metric for determining whether a sentence contains 
the answer to a question. Consider, for example, 
the question ?Who is the leader of France?? The 
sentence ?Henri Hadjenberg, who is the leader of 
France?s Jewish community, endorsed confronting 
the specter of the Vichy past? overlaps with all 
question terms, but it does not contain the correct 
answer; while the sentence ?Bush later met with 
French President Jacques Chirac? does not overlap 
with any question term, but it does contain the 
correct answer. 
To circumvent this limitation of word-based 
similarity metrics, QA researchers have developed 
methods through which they first map questions 
and sentences that may contain answers in 
different spaces, and then compute the ?similarity? 
between them there. For example, the systems 
developed at IBM and ISI map questions and 
answer sentences into parse trees and surface-
based semantic labels and measure the similarity 
between questions and answer sentences in this 
syntactic/semantic space, using QA-motivated 
metrics. The systems developed by CYC and LCC 
map questions and answer sentences into logical 
forms and compute the ?similarity? between them 
using inference rules. And systems such as those 
developed by IBM and BBN map questions and 
answers into feature sets and compute the 
similarity between them using maximum entropy 
models that are trained on question-answer 
corpora. From this perspective then, the 
fundamental problem of question answering is that 
of finding spaces where the distance between 
questions and sentences that contain correct 
answers is small and where the distance between 
questions and sentences that contain incorrect 
answers is large.  
In this paper, we propose a new space and a 
new metric for computing this distance. Being 
inspired by the success of noisy-channel-based 
approaches in applications as diverse as speech 
recognition (Jelinek, 1997), part of speech tagging 
(Church, 1988), machine translation (Brown et al, 
1993), information retrieval (Berger and Lafferty, 
1999), and text summarization (Knight and Marcu, 
2002), we develop a noisy channel model for QA. 
This model explains how a given sentence SA that 
contains an answer sub-string A to a question Q 
can be rewritten into Q through a sequence of 
stochastic operations. Given a corpus of question-
answer pairs (Q, SA), we can train a probabilistic 
model for estimating the conditional probability 
P(Q | SA). Once the parameters of this model are 
learned, given a question Q and the set of 
sentences ? returned by an IR engine, one can find 
the sentence Si ? ? and an answer in it Ai,j by 
searching for the Si,Ai,j that maximizes the 
conditional probability P(Q | Si,Ai,j).   
In Section 2, we first present the noisy-channel 
model that we propose for this task. In Section 3, 
we describe how we generate training examples. In 
Section 4, we describe how we use the learned 
models to answer factoid questions, we evaluate 
the performance of our system using a variety of 
experimental conditions, and we compare it with a 
rule-based system that we have previously used in 
several TREC evaluations. In Section 5, we 
demonstrate that the framework we propose is 
flexible enough to accommodate a wide range of 
resources and techniques that have been employed 
in state-of-the-art QA systems.  
2 A noisy-channel for QA 
Assume that we want to explain why ?1977? in 
sentence S in Figure 1 is a good answer for the 
question ?When did Elvis Presley die?? To do this, 
we build a noisy channel model that makes explicit 
how answer sentence parse trees are mapped into 
questions. Consider, for example, the automatically 
derived answer sentence parse tree in Figure 1, 
which associates to nodes both syntactic and 
shallow semantic, named-entity-specific tags. In 
order to rewrite this tree into a question, we 
assume the following generative story: 
1. In general, answer sentences are much longer 
than typical factoid questions. To reduce the 
length gap between questions and answers and 
to increase the likelihood that our models can 
be adequately trained, we first make a ?cut? in 
the answer parse tree and select a sequence of 
words, syntactic, and semantic tags. The ?cut? 
is made so that every word in the answer 
sentence or one of its ancestors belongs to the 
?cut? and no two nodes on a path from a word 
to the root of the tree are in the ?cut?. Figure 1 
depicts graphically such a cut. 
2. Once the ?cut? has been identified, we mark 
one of its elements as the answer string. In 
Figure 1, we decide to mark DATE as the 
answer string (A_DATE). 
3. There is no guarantee that the number of words 
in the cut and the number of words in the 
question match. To account for this, we 
stochastically assign to every element si in a 
cut a fertility according to table n(? | si). We 
delete elements of fertility 0 and duplicate 
elements of fertility 2, etc. With probability p1 
we also increment the fertility of an invisible 
word NULL. NULL and fertile words, i.e. 
words with fertility strictly greater than 1 
enable us to align long questions with short 
answers. Zero fertility words enable us to align 
short questions with long answers. 
4. Next, we replace answer words (including the 
NULL word) with question words according to 
the table t(qi | sj).  
5. In the last step, we permute the question words 
according to a distortion table d, in order to 
obtain a well-formed, grammatical question. 
The probability P(Q | SA) is computed by 
multiplying the probabilities in all the steps of our 
generative story (Figure 1 lists some of the factors 
specific to this computation.) The readers familiar 
with the statistical machine translation (SMT) 
literature should recognize that steps 3 to 5 are 
nothing but a one-to-one reproduction of the 
generative story proposed in the SMT context by 
Brown et al (see Brown et al, 1993 for a detailed 
mathematical description of the model and the 
formula for computing the probability of an 
alignment and target string given a source string).1 
Figure 1: A generative model for Question 
answering  
To simplify our work and to enable us exploit 
existing off-the-shelf software, in the experiments 
we carried out in conjunction with this paper, we 
assumed a flat distribution for the two steps in our 
                                                          
1 The distortion probabilities depicted in Figure 1 are a 
simplification of the distortions used in the IBM Model 4 
model by Brown et al (1993). We chose this watered down 
representation only for illustrative purposes. Our QA system 
implements the full-blown Model 4 statistical model described 
by Brown et al 
generative story. That is, we assumed that it is 
equally likely to take any cut in the tree and 
equally likely to choose as Answer any 
syntactic/semantic element in an answer sentence. 
3 Generating training and testing 
material 
3.1 Generating training cases 
Assume that the question-answer pair in Figure 1 
appears in our training corpus. When this happens, 
we know that 1977 is the correct answer. To 
generate a training example from this pair, we 
tokenize the question, we parse the answer 
sentence, we identify the question terms and 
answer in the parse tree, and then we make a "cut" 
in the tree that satisfies the following conditions: 
a) Terms overlapping with the question are 
preserved as surface text 
b) The answer is reduced to its semantic or 
syntactic class prefixed with the symbol ?A_? 
c) Non-leaves, which don?t have any question 
term or answer offspring, are reduced to their 
semantic or syntactic class. 
d) All remaining nodes (leaves) are preserved 
as surface text. 
Condition a) ensures that the question terms 
will be identified in the sentence. Condition b) 
helps learn answer types. Condition c) brings the 
sentence closer to the question by compacting 
portions that are syntactically far from question 
terms and answer.  And finally the importance of 
lexical cues around question terms and answer 
motivates condition d).  For the question-answer 
pair in Figure 1, the algorithm above generates the 
following training example: 
Q: When did Elvis Presley die ? 
SA: Presley died PP PP in A_DATE, and 
SNT. 
Figure 2 represents graphically the conditions 
that led to this training example being generated. 
Our algorithm for generating training pairs 
implements deterministically the first two steps in 
our generative story. The algorithm is constructed 
so as to be consistent with our intuition that a 
generative process that makes the question and 
answer as similar-looking as possible is most likely 
to enable us learn a useful model. Each question-
answer pair results in one training example. It is 
the examples generated through this procedure that 
we use to estimate the parameters of our model. 
Figure 2: Generation of QA examples for training. 
3.2 Generating test cases 
Assume now that the sentence in Figure 1 is 
returned by an IR engine as a potential candidate 
for finding the answer to the question ?When did 
Elvis Presley die?? In this case, we don?t know 
what the answer is, so we assume that any 
semantic/syntactic node in the answer sentence can 
be the answer, with the exception of the nodes that 
subsume question terms and stop words. In this 
case, given a question and a potential answer 
sentence, we generate an exhaustive set of 
question-answer test cases, each test case labeling 
as answer (A_) a different syntactic/semantic node. 
Here are some of the test cases we consider for the 
question-answer pair in Figure 1: 
Q: When did Elvis Presley die ? 
SA1: Presley died A_PP PP PP , and SNT . 
Q:  When did Elvis Presley die ? 
SAi: Presley died PP PP in A_DATE, and 
SNT . 
Q:  When did Elvis Presley die ? 
SAj: Presley died PP PP PP , and NP 
return by A_NP NP . 
If we learned a good model, we would expect it to 
assign a higher probability to P(Q | Sai) than to P(Q 
| Sa1) and P(Q | Saj). 
4 Experiments 
4.1 Training Data 
For training, we use three different sets. (i) The 
TREC9-10 set consists of the questions used at 
TREC9 and 10.  We automatically generate 
answer-tagged sentences using the TREC9 and 10 
judgment sets, which are lists of answer-document 
pairs evaluated as either correct or wrong.  For 
every question, we first identify in the judgment 
sets a list of documents containing the correct 
answer.  For every document, we keep only the 
sentences that overlap with the question terms and 
contain the correct answer.  (ii) In order to have 
more variation of sentences containing the answer, 
we have automatically extended the first data set 
using the Web. For every TREC9-10 
question/answer pair, we used our Web-based IR 
to retrieve sentences that overlap with the question 
terms and contain the answer. We call this data set 
TREC9-10Web.  (iii) The third data set consists of 
2381 question/answer pairs collected from 
http://www.quiz-zone.co.uk. We use the same 
method to automatically enhance this set by 
retrieving from the web sentences containing 
answers to the questions.  We call this data set 
Quiz-Zone.  Table 1 shows the size of the three 
training corpora: 
 
Training Set # distinct questions # question-answer pairs 
TREC9-10 1091 18618 
TREC9-10Web 1091 54295 
Quiz-Zone 2381 17614 
Table 1: Size of Training Corpora 
 
To train our QA noisy-channel model, we apply 
the algorithm described in Section 3.1 to generate 
training cases for all QA pairs in the three corpora. 
To help our model learn that it is desirable to copy 
answer words into the question, we add to each 
corpus a list of identical dictionary word pairs wi-
wi.  For each corpus, we use GIZA (Al-Onaizan et 
al., 1999), a publicly available SMT package that 
implements the IBM models (Brown et al, 1993), 
to train a QA noisy-channel model that maps 
flattened answer parse trees, obtained using the 
?cut? procedure described in Section 3.1, into 
questions. 
4.2 Test Data 
We used two different data sets for the purpose of 
testing.  The first set consists of the 500 questions 
used at TREC 2002; the second set consists of 500 
questions that were randomly selected from the 
Knowledge Master (KM) repository 
(http://www.greatauk.com). The KM questions 
tend to be longer and quite different in style 
compared to the TREC questions. 
 
th e  fa i t h fu l  r e tu r n  b y  th e  
h u n d r e d s  e a c h  y e a r  to  
m a r k  th e  a n n iv e r s a r y   
 
o f  a  h e a r t  d i s e a s e  a t  G r a c e l a n d  
S N T  
N P  P P  
P r e s le y  
d i e d  P P  
in  1 9 7 7  
S N T,  .a n d  P P  
 
C o n d i t io n  a )  
 
C o n d i t io n  b )  
 
C o n d i t io n  d )  
 
C o n d i t io n  c )  
4.3 A noisy-channel-based QA system 
Our QA system is straightforward. It has only two 
modules: an IR module, and an answer-
identifier/ranker module. The IR module is the 
same we used in previous participations at TREC. 
As the learner, the answer-identifier/ranker module 
is also publicly available ? the GIZA package can 
be configured to automatically compute the 
probability of the Viterbi alignment between a 
flattened answer parse tree and a question. 
For each test question, we automatically generate a 
web query and use the top 300 answer sentences 
returned by our IR engine to look for an answer. 
For each question Q and for each answer sentence 
Si, we use the algorithm described in Section 3.2 to 
exhaustively generate all Q- Si,Ai,j pairs. Hence we 
examine all syntactic constituents in a sentence and 
use GIZA to assess their likelihood of being a 
correct answer. We select the answer Ai,j that 
maximizes P(Q | Si,Ai,j) for all answer sentences Si  
and all answers Ai,j that can be found in list 
retrieved by the IR module. Figure 3 depicts 
graphically our noisy-channel-based QA system. 
 
Figure 3: The noisy-channel-based QA system. 
4.4 Experimental Results 
We evaluate the results by generating 
automatically the mean reciprocal rank (MRR) 
using the TREC 2002 patterns and QuizZone 
original answers when testing on TREC 2002 and 
QuizZone test sets respectively.  Our baseline is a 
state of the art QA system, QA-base, which was 
ranked from second to seventh in the last 3 years at 
TREC.  To ensure a fair comparison, we use the 
same Web-based IR system in all experiments with 
no answer retrofitting. For the same reason, we use 
the QA-base system with the post-processing 
module disabled. (This module re-ranks the 
answers produced by QA-base on the basis of their 
redundancy, frequency on the web, etc.) Table 2 
summarizes results of different combinations of 
training and test sets: 
Trained on\Tested on TREC 2002 KM 
A = TREC9-10 0.325 0.108 
B = A + TREC9-10Web 0.329 0.120 
C = B + Quiz-Zone 0.354 0.132 
QA-base 0.291 0.128 
Table 2: Impact of training and test sets. 
 
For the TREC 2002 corpus, the relatively low 
MRRs are due to the small answer coverage of the 
TREC 2002 patterns.  For the KM corpus, the 
relatively low MRRs are explained by two factors: 
(i) for this corpus, each evaluation pattern consists 
of only one string ? the original answer; (ii) the 
KM questions are more complex than TREC 
questions (What piece of furniture is associated 
with Modred, Percival, Gawain, Arthur, and 
Lancelot?).  
It is interesting to see that using only the 
TREC9-10 data as training (system A in Table 2), 
we are able to beat the baseline when testing on 
TREC 2002 questions; however, this is not true 
when testing on KM questions.  This can be 
explained by the fact that the TREC9-10 training 
set is similar to the TREC 2002 test set while it is 
significantly different from the KM test set.  We 
also notice that expanding the training to TREC9-
10Web (System B) and then to Quiz-Zone (System 
C) improved the performance on both test sets, 
which confirms that both the variability across 
answer tagged sentences  (Trec9-10Web) and the 
abundance of distinct questions (Quiz-Zone) 
contribute to the diversity of a QA training corpus, 
and implicitly to the performance of our system. 
5 Framework flexibility 
Another characteristic of our framework is its 
flexibility.  We can easily extend it to span other 
question-answering resources and techniques that 
have been employed in state-of-the art QA 
systems.  In the rest of this section, we assess the 
impact of such resources and techniques in the 
context of three case studies. 
5.1 Statistical-based ?Reasoning?  
The LCC TREC-2002 QA system (Moldovan et 
al., 2002) implements a reasoning mechanism for 
justifying answers. In the LCC framework, 
 
T est 
question 
Q  
S i,A
i,j
 
Q A M odel 
trained  
using 
G IZA  
S x,A
x,y
=  argm ax (P(Q  | S i,A
i,j
)) 
A  =  A x,y 
G IZA  
S 1 
S m  
S 1,A
1,1 
S 1,A
1,v 
S m ,A
m ,1 
S m ,A
m ,w 
IR
questions and answers are first mapped into logical 
forms. A resolution-based module then proves that 
the question logically follows from the answer 
using a set of axioms that are automatically 
extracted from the WordNet glosses. For example, 
to prove the logical form of ?What is the age of our 
solar system?? from the logical form of the answer 
?The solar system is 4.6 billion years old.?, the 
LCC theorem prover shows that the atomic 
formula that corresponds to the question term 
?age? can be inferred from the atomic formula that 
corresponds to the answer term ?old? using an 
axiom that connects ?old? and ?age?, because the 
WordNet gloss for ?old? contains the word ?age?. 
Similarly, the LCC system can prove that ?Voting 
is mandatory for all Argentines aged over 18? 
provides a good justification for the question 
?What is the legal age to vote in Argentina?? 
because it can establish through logical deduction 
using axioms induced from WordNet glosses that 
?legal? is related to ?rule?, which in turn is related 
to ?mandatory?; that ?age? is related to ?aged?; 
and that ?Argentine? is related to ?Argentina?. It is 
not difficult to see by now that these logical 
relations can be represented graphically as 
alignments between question and answer terms 
(see Figure 4).  
 
 
 
 
Figure 4: Gloss-based reasoning as word-level 
alignment. 
 
The exploitation of WordNet synonyms, which is 
part of many QA systems (Hovy et al, 2001; 
Prager et al, 2001; Pasca and Harabagiu, 2001), is 
a particular case of building such alignments 
between question and answer terms. For example, 
using WordNet synonymy relations, it is possible 
to establish a connection between ?U.S.? and 
?United States? and between ?buy? and ?purchase? 
in the question-answer pair (Figure 5), thus 
increasing the confidence that the sentence 
contains a correct answer. 
  
 
 
 
Figure 5: Synonym-based alignment. 
 
The noisy channel framework we proposed in this 
paper can approximate the reasoning mechanism 
employed by LCC and accommodate the 
exploitation of gloss- and synonymy-based 
relations found in WordNet. In fact, if we had a 
very large training corpus, we would expect such 
connections to be learned automatically from the 
data. However, since we have a relatively small 
training corpus available, we rewrite the WordNet 
glosses into a dictionary by creating word-pair 
entries that establish connections between all 
Wordnet words and the content words in their 
glosses. For example, from the word ?age? and its 
gloss ?a historic period?, we create the dictionary 
entries ?age - historic? and ?age ? period?. To 
exploit synonymy relations, for every WordNet 
synset Si, we add to our training data all possible 
combinations of synonym pairs Wi,x-Wi,y.  
Our dictionary creation procedure is a crude 
version of the axiom extraction algorithm 
described by Moldovan et al (2002); and our 
exploitation of the glosses in the noisy-channel 
framework amounts to a simplified, statistical 
version of the semantic proofs implemented by 
LCC. Table 3 shows the impact of WordNet 
synonyms (WNsyn) and WordNet glosses 
(WNgloss) on our system. Adding WordNet 
synonyms and glosses improved slightly the 
performance on the KM questions.  On the other 
hand, it is surprising to see that the performance 
has dropped when testing on TREC 2002 
questions.  
Trained on\Tested on TREC 2002 KM 
C 0.354 0.132 
C+WNsyn 0.345 0.138 
C + WNgloss 0.343 0.136 
Table 3: WordNet synonyms and glosses impact. 
5.2 Question reformulation  
Hermjakob et al (2002) showed that 
reformulations (syntactic and semantic) improve 
the answer pinpointing process in a QA system.  
To make use of this technique, we extend our 
training data set by expanding every question-
answer pair Q-SA to a list (Qr-SA), Qr ? ? where ? 
is the set of question reformulations. 2   We also 
expand in a similar way the answer candidates in 
the test corpus.  Using reformulations improved the 
                                                          
2 We are grateful to Ulf Hermjakob for sharing his 
reformulations with us. 
In 1867, Secretary of State William H. Seward arranged for 
the United-States to purchase Alaska for 2 cents per acre. 
   What year did the U.S. buy Alaska? 
What  is the legal age to vote in Argentina? 
Voting  is mandatory for all Argentines aged over 18  
performance of our system on the TREC 2002 test 
set while it was not beneficial for the KM test set 
(see Table 4).  We believe this is explained by the 
fact that the reformulation engine was fine tuned 
on TREC-specific questions, which are 
significantly different from KM questions. 
Trained on\Tested on TREC 2002 KM 
C 0.354 0.132 
C+reformulations 0.365 0.128 
Table 4: Reformulations impact. 
5.3 Exploiting data in structured -and semi-
structured databases 
Structured and semi-structured databases were 
proved to be very useful for question-answering 
systems.  Lin (2002) showed through his federated 
approach that 47% of TREC-2001 questions could 
be answered using Web-based knowledge sources.  
Clarke et al (2001) obtained a 30% improvement 
by using an auxiliary database created from web 
documents as an additional resource.  We adopted 
a different approach to exploit external knowledge 
bases. 
In our work, we first generated a natural 
language collection of factoids by mining different 
structured and semi-structured databases (World 
Fact Book, Biography.com, WordNet?). The 
generation is based on manually written question-
factoid template pairs, which are applied on the 
different sources to yield simple natural language 
question-factoid pairs. Consider, for example, the 
following two factoid-question template pairs: 
Qt1: What is the capital of _c? 
St1: The capital of _c is capital(_c). 
Qt2: How did _p die? 
St2: _p died of causeDeath(_p). 
Using extraction patterns (Muslea, 1999), we 
apply these two templates on the World Fact Book 
database and on biography.com pages to instantiate 
question and answer-tagged sentence pairs such as: 
Q1: What is the capital of Greece? 
S1: The capital of Greece is Athens. 
Q2: How did Jean-Paul Sartre die? 
S2: Jean-Paul Sartre died of a lung 
ailment. 
These question-factoid pairs are useful both in 
training and testing. In training, we simply add all 
these pairs to the training data set. In testing, for 
every question Q, we select factoids that overlap 
sufficiently enough with Q as sentences that 
potentially contain the answer.  For example, given 
the question ?Where was Sartre born?? we will 
select the following factoids: 
1-Jean-Paul Sartre was born in 1905. 
2-Jean-Paul Sartre died in 1980. 
3-Jean-Paul Sartre was born in Paris. 
4-Jean-Paul Sartre died of a lung 
ailment. 
Up to now, we have collected about 100,000 
question-factoid pairs.  We found out that these 
pairs cover only 24 of the 500 TREC 2002 
questions.  And so, in order to evaluate the value of 
these factoids, we reran our system C on these 24 
questions and then, we used the question-factoid 
pairs as the only resource for both training and 
testing as described earlier (System D). Table 5 
shows the MRRs for systems C and D on the 24 
questions covered by the factoids. 
System 24 TREC 2002 questions 
C 0.472 
D 0.812 
Table 5: Factoid impact on system performance. 
 
It is very interesting to see that system D 
outperforms significantly system C. This shows 
that, in our framework, in order to benefit from 
external databases, we do not need any additional 
machinery (question classifiers, answer type 
identifiers, wrapper selectors, SQL query 
generators, etc.) All we need is a one-time 
conversion of external structured resources to 
simple natural language factoids. The results in 
Table 5 also suggest that collecting natural 
language factoids is a useful research direction: if 
we collect all the factoids in the world, we could 
probably achieve much higher MRR scores on the 
entire TREC collection. 
6 Conclusion 
In this paper, we proposed a noisy-channel model 
for QA that can accommodate within a unified 
framework the exploitation of a large number of 
resources and QA-specific techniques. We believe 
that our work will lead to a better understanding of 
the similarities and differences between the 
approaches that make up today?s QA research 
landscape. We also hope that our paper will reduce 
the high barrier to entry that is explained by the 
complexity of current QA systems and increase the 
number of researchers working in this field: 
because our QA system uses only publicly 
available software components (an IR engine; a 
parser; and a statistical MT system), it can be 
easily reproduced by other researchers.  
However, one has to recognize that the reliance of 
our system on publicly available components is not 
ideal.  The generative story that our noisy-channel 
employs is rudimentary; we have chosen it only 
because we wanted to exploit to the best extent 
possible existing software components (GIZA). 
The empirical results we obtained are extremely 
encouraging: our noisy-channel system is already 
outperforming a state-of-the-art rule-based system 
that took many person years to develop. It is 
remarkable that a statistical machine translation 
system can do so well in a totally different context, 
in question answering. However, building 
dedicated systems that employ more sophisticated, 
QA-motivated generative stories is likely to yield 
significant improvements. 
 
Acknowledgments.  This work was supported by 
the Advanced Research and Development Activity 
(ARDA)?s Advanced Question Answering for 
Intelligence (AQUAINT) Program under contract 
number MDA908-02-C-0007. 
References 
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 
Knight, John Lafferty, Dan Melamed, Franz-Josef 
Och, David Purdy, Noah A. Smith, and David 
Yarowsky. 1999. Statistical machine translation. Fi-
nal Report, JHU Summer Workshop.  
Adam L. Berger, John D. Lafferty. 1999. Information 
Retrieval as Statistical Translation. In Proceedings of 
the SIGIR 1999, Berkeley, CA. 
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais, 
Andrew Ng. 2001. Data-Intensive Question 
Answering. In Proceedings of the TREC-2001 
Conference, NIST. Gaithersburg, MD. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 
19(2):263--312. 
Kenneth W. Church. 1988. A stochastic parts program 
and noun phrase parser for unrestricted text. In 
Proceedings of the Second Conference on Applied 
Natural Language Processing, Austin, TX. 
Charles L. A. Clarke, Gordon V. Cormack, Thomas R. 
Lynam, C. M. Li, G. L. McLearn. 2001. Web 
Reinforced Question Answering (MultiText 
Experiments for TREC 2001). In Proceedings of the 
TREC-2001Conference, NIST. Gaithersburg, MD. 
Ulf Hermjakob, Abdessamad Echihabi, and Daniel 
Marcu. 2002. Natural Language Based 
Reformulation Resource and Web Exploitation for 
Question Answering. In Proceedings of the TREC-
2002 Conference, NIST. Gaithersburg, MD. 
Edward H. Hovy, Ulf Hermjakob, Chin-Yew Lin. 2001. 
The Use of External Knowledge in Factoid QA. In 
Proceedings of the TREC-2001 Conference, NIST. 
Gaithersburg, MD. 
Abraham Ittycheriah and Salim Roukos. 2002. IBM's 
Statistical Question Answering System-TREC 11. In 
Proceedings of the TREC-2002 Conference, NIST. 
Gaithersburg, MD. 
Frederick Jelinek. 1997. Statistical Methods for Speech 
Recognition. MIT Press, Cambridge, MA. 
Boris Katz, Deniz Yuret, Sue Felshin. 2001. Omnibase: 
A universal data source interface. In MIT Artificial 
Intelligence Abstracts. 
Kevin Knight, Daniel Marcu. 2002. Summarization 
beyond sentence extraction: A probabilistic approach 
to sentence compression. Artificial Intelligence 
139(1): 91-107. 
Jimmy Lin. 2002. The Web as a Resource for Question 
Answering: Perspective and Challenges. In LREC 
2002, Las Palmas, Canary Islands, Spain. 
Dan  Moldovan, Sanda Harabagiu, Roxana Girju, Paul 
Morarescu, Finley Lacatusu, Adrian Novischi, 
Adriana Badulescu, Orest Bolohan. 2002. LCC Tools 
for Question Answering. In Proceedings of the 
TREC-2002 Conference, NIST. Gaithersburg, MD. 
Ion Muslea. 1999. Extraction Patterns for Information 
Extraction Tasks: A Survey. In Proceedings of 
Workshop on Machine Learning and Information 
Extraction (AAAI-99), Orlando, FL. 
Marius Pasca, Sanda Harabagiu, 2001. The Informative 
Role of WordNet in Open-Domain Question 
Answering. In Proceedings of the NAACL 2001 
Workshop on WordNet and Other Lexical Resources, 
Carnegie Mellon University, Pittsburgh PA. 
John M. Prager, Jennifer Chu-Carroll, Krysztof Czuba. 
2001. Use of WordNet Hypernyms for Answering 
What-Is Questions. In Proceedings of the TREC-
2002 Conference, NIST. Gaithersburg, MD. 
Jinxi Xu, Ana Licuanan, Jonathan May, Scott Miller, 
Ralph Weischedel. 2002. TREC 2002 QA at BBN:  
Answer Selection and Confidence Estimation. In 
Proceedings of the TREC-2002 Conference, NIST. 
Gaithersburg, MD. 
Proceedings of the 43rd Annual Meeting of the ACL, pages 66?74,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Towards Developing Generation Algorithms for Text-to-Text Applications
Radu Soricut and Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
 
radu, marcu  @isi.edu
Abstract
We describe a new sentence realization
framework for text-to-text applications.
This framework uses IDL-expressions as
a representation formalism, and a gener-
ation mechanism based on algorithms for
intersecting IDL-expressions with proba-
bilistic language models. We present both
theoretical and empirical results concern-
ing the correctness and efficiency of these
algorithms.
1 Introduction
Many of today?s most popular natural language ap-
plications ? Machine Translation, Summarization,
Question Answering ? are text-to-text applications.
That is, they produce textual outputs from inputs that
are also textual. Because these applications need
to produce well-formed text, it would appear nat-
ural that they are the favorite testbed for generic
generation components developed within the Natu-
ral Language Generation (NLG) community. Over
the years, several proposals of generic NLG systems
have been made: Penman (Matthiessen and Bate-
man, 1991), FUF (Elhadad, 1991), Nitrogen (Knight
and Hatzivassiloglou, 1995), Fergus (Bangalore
and Rambow, 2000), HALogen (Langkilde-Geary,
2002), Amalgam (Corston-Oliver et al, 2002), etc.
Instead of relying on such generic NLG systems,
however, most of the current text-to-text applica-
tions use other means to address the generation need.
In Machine Translation, for example, sentences are
produced using application-specific ?decoders?, in-
spired by work on speech recognition (Brown et
al., 1993), whereas in Summarization, summaries
are produced as either extracts or using task-specific
strategies (Barzilay, 2003). The main reason for
which text-to-text applications do not usually in-
volve generic NLG systems is that such applica-
tions do not have access to the kind of informa-
tion that the input representation formalisms of cur-
rent NLG systems require. A machine translation or
summarization system does not usually have access
to deep subject-verb or verb-object relations (such
as ACTOR, AGENT, PATIENT, POSSESSOR, etc.)
as needed by Penman or FUF, or even shallower
syntactic relations (such as subject, object,
premod, etc.) as needed by HALogen.
In this paper, following the recent proposal
made by Nederhof and Satta (2004), we argue
for the use of IDL-expressions as an application-
independent, information-slim representation lan-
guage for text-to-text natural language generation.
IDL-expressions are created from strings using four
operators: concatenation (  ), interleave (  ), disjunc-
tion (  ), and lock (  ). We claim that the IDL
formalism is appropriate for text-to-text generation,
as it encodes meaning only via words and phrases,
combined using a set of formally defined operators.
Appropriate words and phrases can be, and usually
are, produced by the applications mentioned above.
The IDL operators have been specifically designed
to handle natural constraints such as word choice
and precedence, constructions such as phrasal com-
bination, and underspecifications such as free word
order.
66
CFGs
via intersection with
Deterministic
Non?deterministic
via intersection with
probabilistic LMs
Word/Phrase
based
Fergus, Amalgam
Nitrogen, HALogen
FUF, PENMAN
NLG System
(Nederhof&Satta 2004)
IDL
Representation
(formalism)
Semantic,
few meanings
Syntactically/
Semantically
grounded
 Syntactic
dependencies
Representation
(computational)
Linear
Exponential
Linear
Deterministic
Generation
(mechanism)
Non?deterministic
via intersection with
probabilistic LMs
Non?deterministic
via intersection with
probabilistic LMs
(this paper)
IDL Linear
Generation
(computational)
Optimal Solution
Efficient Run?time
Efficient Run?time
Optimal Solution
Efficient Run?time
All Solutions
Efficient Run?time
Optimal Solution
Linear Linear
based
Word/Phrase
Table 1: Comparison of the present proposal with
current NLG systems.
In Table 1, we present a summary of the repre-
sentation and generation characteristics of current
NLG systems. We mark by   characteristics that are
needed/desirable in a generation component for text-
to-text applications, and by  characteristics that
make the proposal inapplicable or problematic. For
instance, as already argued, the representation for-
malism of all previous proposals except for IDL is
problematic (  ) for text-to-text applications. The
IDL formalism, while applicable to text-to-text ap-
plications, has the additional desirable property that
it is a compact representation, while formalisms
such as word-lattices and non-recursive CFGs can
have exponential size in the number of words avail-
able for generation (Nederhof and Satta, 2004).
While the IDL representational properties are all
desirable, the generation mechanism proposed for
IDL by Nederhof and Satta (2004) is problematic
(  ), because it does not allow for scoring and
ranking of candidate realizations. Their genera-
tion mechanism, while computationally efficient, in-
volves intersection with context free grammars, and
therefore works by excluding all realizations that are
not accepted by a CFG and including (without rank-
ing) all realizations that are accepted.
The approach to generation taken in this paper
is presented in the last row in Table 1, and can be
summarized as a   tiling of generation character-
istics of previous proposals (see the shaded area in
Table 1). Our goal is to provide an optimal gen-
eration framework for text-to-text applications, in
which the representation formalism, the generation
mechanism, and the computational properties are all
needed and desirable (   ). Toward this goal, we
present a new generation mechanism that intersects
IDL-expressions with probabilistic language mod-
els. The generation mechanism implements new al-
gorithms, which cover a wide spectrum of run-time
behaviors (from linear to exponential), depending on
the complexity of the input. We also present theoret-
ical results concerning the correctness and the effi-
ciency input IDL-expression) of our algorithms.
We evaluate these algorithms by performing ex-
periments on a challenging word-ordering task.
These experiments are carried out under a high-
complexity generation scenario: find the most prob-
able sentence realization under an n-gram language
model for IDL-expressions encoding bags-of-words
of size up to 25 (up to 10  possible realizations!).
Our evaluation shows that the proposed algorithms
are able to cope well with such orders of complex-
ity, while maintaining high levels of accuracy.
2 The IDL Language for NLG
2.1 IDL-expressions
IDL-expressions have been proposed by Nederhof
& Satta (2004) (henceforth N&S) as a representa-
tion for finite languages, and are created from strings
using four operators: concatenation (  ), interleave
(  ), disjunction (  ), and lock (  ). The semantics of
IDL-expressions is given in terms of sets of strings.
The concatenation (  ) operator takes two argu-
ments, and uses the strings encoded by its argu-
ment expressions to obtain concatenated strings that
respect the order of the arguments; e.g.,   en-
codes the singleton set 	
 . The  nterleave (  )
operator interleaves the strings encoded by its argu-
ment expressions; e.g.,  	 encodes the set
	
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 81?88,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extracting Parallel Sub-Sentential Fragments from Non-Parallel Corpora
Dragos Stefan Munteanu
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
dragos@isi.edu
Daniel Marcu
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
marcu@isi.edu
Abstract
We present a novel method for extract-
ing parallel sub-sentential fragments from
comparable, non-parallel bilingual cor-
pora. By analyzing potentially similar
sentence pairs using a signal processing-
inspired approach, we detect which seg-
ments of the source sentence are translated
into segments in the target sentence, and
which are not. This method enables us
to extract useful machine translation train-
ing data even from very non-parallel cor-
pora, which contain no parallel sentence
pairs. We evaluate the quality of the ex-
tracted data by showing that it improves
the performance of a state-of-the-art sta-
tistical machine translation system.
1 Introduction
Recently, there has been a surge of interest in
the automatic creation of parallel corpora. Sev-
eral researchers (Zhao and Vogel, 2002; Vogel,
2003; Resnik and Smith, 2003; Fung and Cheung,
2004a; Wu and Fung, 2005; Munteanu and Marcu,
2005) have shown how fairly good-quality parallel
sentence pairs can be automatically extracted from
comparable corpora, and used to improve the per-
formance of machine translation (MT) systems.
This work addresses a major bottleneck in the de-
velopment of Statistical MT (SMT) systems: the
lack of sufficiently large parallel corpora for most
language pairs. Since comparable corpora exist in
large quantities and for many languages ? tens of
thousands of words of news describing the same
events are produced daily ? the ability to exploit
them for parallel data acquisition is highly benefi-
cial for the SMT field.
Comparable corpora exhibit various degrees of
parallelism. Fung and Cheung (2004a) describe
corpora ranging from noisy parallel, to compara-
ble, and finally to very non-parallel. Corpora from
the last category contain ?... disparate, very non-
parallel bilingual documents that could either be
on the same topic (on-topic) or not?. This is the
kind of corpora that we are interested to exploit in
the context of this paper.
Existing methods for exploiting comparable
corpora look for parallel data at the sentence level.
However, we believe that very non-parallel cor-
pora have none or few good sentence pairs; most
of their parallel data exists at the sub-sentential
level. As an example, consider Figure 1, which
presents two news articles from the English and
Romanian editions of the BBC. The articles re-
port on the same event (the one-year anniversary
of Ukraine?s Orange Revolution), have been pub-
lished within 25 minutes of each other, and express
overlapping content.
Although they are ?on-topic?, these two docu-
ments are non-parallel. In particular, they contain
no parallel sentence pairs; methods designed to ex-
tract full parallel sentences will not find any use-
ful data in them. Still, as the lines and boxes from
the figure show, some parallel fragments of data
do exist; but they are present at the sub-sentential
level.
In this paper, we present a method for extracting
such parallel fragments from comparable corpora.
Figure 2 illustrates our goals. It shows two sen-
tences belonging to the articles in Figure 1, and
highlights and connects their parallel fragments.
Although the sentences share some common
meaning, each of them has content which is not
translated on the other side. The English phrase
reports the BBC?s Helen Fawkes in Kiev, as well
81
Figure 1: A pair of comparable, non-parallel documents
Figure 2: A pair of comparable sentences.
as the Romanian one De altfel, vorbind inaintea
aniversarii have no translation correspondent, ei-
ther in the other sentence or anywhere in the whole
document. Since the sentence pair contains so
much untranslated text, it is unlikely that any par-
allel sentence detection method would consider it
useful. And, even if the sentences would be used
for MT training, considering the amount of noise
they contain, they might do more harm than good
for the system?s performance. The best way to
make use of this sentence pair is to extract and use
for training just the translated (highlighted) frag-
ments. This is the aim of our work.
Identifying parallel subsentential fragments is
a difficult task. It requires the ability to recog-
nize translational equivalence in very noisy en-
vironments, namely sentence pairs that express
different (although overlapping) content. How-
ever, a good solution to this problem would have a
strong impact on parallel data acquisition efforts.
Enabling the exploitation of corpora that do not
share parallel sentences would greatly increase the
amount of comparable data that can be used for
SMT.
2 Finding Parallel Sub-Sentential
Fragments in Comparable Corpora
2.1 Introduction
The high-level architecture of our parallel frag-
ment extraction system is presented in Figure 3.
The first step of the pipeline identifies docu-
ment pairs that are similar (and therefore more
likely to contain parallel data), using the Lemur
information retrieval toolkit1 (Ogilvie and Callan,
2001); each document in the source language is
translated word-for-word and turned into a query,
which is run against the collection of target lan-
guage documents. The top 20 results are retrieved
and paired with the query document. We then take
all sentence pairs from these document pairs and
run them through the second step in the pipeline,
the candidate selection filter. This step discards
pairs which have very few words that are trans-
lations of each other. To all remaining sentence
pairs we apply the fragment detection method (de-
scribed in Section 2.3), which produces the output
of the system.
We use two probabilistic lexicons, learned au-
1http://www-2.cs.cmu.edu/$\sim$lemur
82
Figure 3: A Parallel Fragment Extraction System
tomatically from the same initial parallel corpus.
The first one, GIZA-Lex, is obtained by running
the GIZA++2 implementation of the IBM word
alignment models (Brown et al, 1993) on the ini-
tial parallel corpus. One of the characteristics of
this lexicon is that each source word is associated
with many possible translations. Although most of
its high-probability entries are good translations,
there are a lot of entries (of non-negligible proba-
bility) where the two words are at most related. As
an example, in our GIZA-Lex lexicon, each source
word has an average of 12 possible translations.
This characteristic is useful for the first two stages
of the extraction pipeline, which are not intended
to be very precise. Their purpose is to accept most
of the existing parallel data, and not too much of
the non-parallel data; using such a lexicon helps
achieve this purpose.
For the last stage, however, precision is
paramount. We found empirically that when us-
ing GIZA-Lex, the incorrect correspondences that
it contains seriously impact the quality of our re-
sults; we therefore need a cleaner lexicon. In addi-
tion, since we want to distinguish between source
words that have a translation on the target side and
words that do not, we also need a measure of the
probability that two words are not translations of
each other. All these are part of our second lexi-
con, LLR-Lex, which we present in detail in Sec-
tion 2.2. Subsequently, in Section 2.3, we present
our algorithm for detecting parallel sub-sentential
fragments.
2.2 Using Log-Likelihood-Ratios to Estimate
Word Translation Probabilities
Our method for computing the probabilistic trans-
lation lexicon LLR-Lex is based on the the Log-
2http://www.fjoch.com/GIZA++.html
Likelihood-Ratio (LLR) statistic (Dunning, 1993),
which has also been used by Moore (2004a;
2004b) and Melamed (2000) as a measure of
word association. Generally speaking, this statis-
tic gives a measure of the likelihood that two sam-
ples are not independent (i.e. generated by the
same probability distribution). We use it to es-
timate the independence of pairs of words which
cooccur in our parallel corpus.
If source word
 
and target word  are indepen-
dent (i.e. they are not translations of each other),
we would expect that 
 	


 	



,
i.e. the distribution of  given that
 
is present
is the same as the distribution of  when
 
is not
present. The LLR statistic gives a measure of the
likelihood of this hypothesis. The LLR score of a
word pair is low when these two distributions are
very similar (i.e. the words are independent), and
high otherwise (i.e. the words are strongly associ-
ated). However, high LLR scores can indicate ei-
ther a positive association (i.e.   	   	 )
or a negative one; and we can distinguish between
them by checking whether 
 	



 	
.
Thus, we can split the set of cooccurring word
pairs into positively and negatively associated
pairs, and obtain a measure for each of the two as-
sociation types. The first type of association will
provide us with our (cleaner) lexicon, while the
second will allow us to estimate probabilities of
words not being translations of each other.
Before describing our new method more for-
mally, we address the notion of word cooc-
currence. In the work of Moore (2004a) and
Melamed (2000), two words cooccur if they are
present in a pair of aligned sentences in the parallel
training corpus. However, most of the words from
aligned sentences are actually unrelated; therefore,
this is a rather weak notion of cooccurrence. We
follow Resnik et. al (2001) and adopt a stronger
definition, based not on sentence alignment but
on word alignment: two words cooccur if they
are linked together in the word-aligned parallel
training corpus. We thus make use of the signifi-
cant amount of knowledge brought in by the word
alignment procedure.
We compute Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 305?312,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Bayesian Query-Focused Summarization
Hal Daume? III and Daniel Marcu
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
me@hal3.name,marcu@isi.edu
Abstract
We present BAYESUM (for ?Bayesian
summarization?), a model for sentence ex-
traction in query-focused summarization.
BAYESUM leverages the common case in
which multiple documents are relevant to a
single query. Using these documents as re-
inforcement for query terms, BAYESUM is
not afflicted by the paucity of information
in short queries. We show that approxi-
mate inference in BAYESUM is possible
on large data sets and results in a state-
of-the-art summarization system. Further-
more, we show how BAYESUM can be
understood as a justified query expansion
technique in the language modeling for IR
framework.
1 Introduction
We describe BAYESUM, an algorithm for perform-
ing query-focused summarization in the common
case that there are many relevant documents for a
given query. Given a query and a collection of rel-
evant documents, our algorithm functions by ask-
ing itself the following question: what is it about
these relevant documents that differentiates them
from the non-relevant documents? BAYESUM can
be seen as providing a statistical formulation of
this exact question.
The key requirement of BAYESUM is that mul-
tiple relevant documents are known for the query
in question. This is not a severe limitation. In two
well-studied problems, it is the de-facto standard.
In standard multidocument summarization (with
or without a query), we have access to known rel-
evant documents for some user need. Similarly, in
the case of a web-search application, an underly-
ing IR engine will retrieve multiple (presumably)
relevant documents for a given query. For both of
these tasks, BAYESUM performs well, even when
the underlying retrieval model is noisy.
The idea of leveraging known relevant docu-
ments is known as query expansion in the informa-
tion retrieval community, where it has been shown
to be successful in ad hoc retrieval tasks. Viewed
from the perspective of IR, our work can be inter-
preted in two ways. First, it can be seen as an ap-
plication of query expansion to the summarization
task (or, in IR terminology, passage retrieval); see
(Liu and Croft, 2002; Murdock and Croft, 2005).
Second, and more importantly, it can be seen as a
method for query expansion in a non-ad-hoc man-
ner. That is, BAYESUM is a statistically justified
query expansion method in the language modeling
for IR framework (Ponte and Croft, 1998).
2 Bayesian Query-Focused
Summarization
In this section, we describe our Bayesian query-
focused summarization model (BAYESUM). This
task is very similar to the standard ad-hoc IR task,
with the important distinction that we are compar-
ing query models against sentence models, rather
than against document models. The shortness of
sentences means that one must do a good job of
creating the query models.
To maintain generality, so that our model is ap-
plicable to any problem for which multiple rele-
vant documents are known for a query, we formu-
late our model in terms of relevance judgments.
For a collection of D documents and Q queries,
we assume we have a D ? Q binary matrix r,
where rdq = 1 if an only if document d is rele-
vant to query q. In multidocument summarization,
rdq will be 1 exactly when d is in the document set
corresponding to query q; in search-engine sum-
305
marization, it will be 1 exactly when d is returned
by the search engine for query q.
2.1 Language Modeling for IR
BAYESUM is built on the concept of language
models for information retrieval. The idea behind
the language modeling techniques used in IR is
to represent either queries or documents (or both)
as probability distributions, and then use stan-
dard probabilistic techniques for comparing them.
These probability distributions are almost always
?bag of words? distributions that assign a proba-
bility to words from a fixed vocabulary V .
One approach is to build a probability distri-
bution for a given document, pd(?), and to look
at the probability of a query under that distribu-
tion: pd(q). Documents are ranked according to
how likely they make the query (Ponte and Croft,
1998). Other researchers have built probability
distributions over queries pq(?) and ranked doc-
uments according to how likely they look under
the query model: pq(d) (Lafferty and Zhai, 2001).
A third approach builds a probability distribution
pq(?) for the query, a probability distribution pd(?)
for the document and then measures the similarity
between these two distributions using KL diver-
gence (Lavrenko et al, 2002):
KL (pq || pd) =
?
w?V
pq(w) log
pq(w)
pd(w)
(1)
The KL divergence between two probability
distributions is zero when they are identical and
otherwise strictly positive. It implicitly assumes
that both distributions pq and pd have the same
support: they assign non-zero probability to ex-
actly the same subset of V; in order to account
for this, the distributions pq and pd are smoothed
against a background general English model. This
final mode?the KL model?is the one on which
BAYESUM is based.
2.2 Bayesian Statistical Model
In the language of information retrieval, the query-
focused sentence extraction task boils down to es-
timating a good query model, pq(?). Once we have
such a model, we could estimate sentence models
for each sentence in a relevant document, and rank
the sentences according to Eq (1).
The BAYESUM system is based on the follow-
ing model: we hypothesize that a sentence ap-
pears in a document because it is relevant to some
query, because it provides background informa-
tion about the document (but is not relevant to a
known query) or simply because it contains use-
less, general English filler. Similarly, we model
each word as appearing for one of those purposes.
More specifically, our model assumes that each
word can be assigned a discrete, exact source, such
as ?this word is relevant to query q1? or ?this word
is general English.? At the sentence level, how-
ever, sentences are assigned degrees: ?this sen-
tence is 60% about query q1, 30% background
document information, and 10% general English.?
To model this, we define a general English
language model, pG(?) to capture the English
filler. Furthermore, for each document dk, we
define a background document language model,
pdk(?); similarly, for each query qj , we define
a query-specific language model pqj (?). Every
word in a document dk is modeled as being gen-
erated from a mixture of pG, pdk and {pqj :
query qj is relevant to document dk}. Supposing
there are J total queries and K total documents,
we say that the nth word from the sth sentence
in document d, wdsn, has a corresponding hidden
variable, zdsn that specifies exactly which of these
distributions is used to generate that one word. In
particular, zdsn is a vector of length 1 + J + K,
where exactly one element is 1 and the rest are 0.
At the sentence level, we introduce a second
layer of hidden variables. For the sth sentence in
document d, we let pids be a vector also of length
1 + J + K that represents our degree of belief
that this sentence came from any of the models.
The pidss lie in the J + K-dimensional simplex
?J+K = {? = ??1, . . . , ?J+K+1? : (?i) ?i ?
0, ?i ?i = 1}. The interpretation of the pi vari-
ables is that if the ?general English? component of
pi is 0.9, then 90% of the words in this sentence
will be general English. The pi and z variables are
constrained so that a sentence cannot be generated
by a document language model other than its own
document and cannot be generated by a query lan-
guage model for a query to which it is not relevant.
Since the pis are unknown, and it is unlikely that
there is a ?true? correct value, we place a corpus-
level prior on them. Since pi is a multinomial dis-
tribution over its corresponding zs, it is natural to
use a Dirichlet distribution as a prior over pi. A
Dirichlet distribution is parameterized by a vector
? of equal length to the corresponding multino-
mial parameter, again with the positivity restric-
306
tion, but no longer required to sum to one. It
has continuous density over a variable ?1, . . . , ?I
given by: Dir(? | ?) = ?(
?
i ?i)
?
i ?(?i)
?
i ?
?i?1
i . The
first term is a normalization term that ensures that
?
?I d? Dir(? | ?) = 1.
2.3 Generative Story
The generative story for our model defines a distri-
bution over a corpus of queries, {qj}1:J , and doc-
uments, {dk}1:K , as follows:
1. For each query j = 1 . . . J : Generate each
word qjn in qj by pqj (qjn)
2. For each document k = 1 . . .K and each
sentence s in document k:
(a) Select the current sentence degree piks
by Dir(piks | ?)rk(piks)
(b) For each word wksn in sentence s:
? Select the word source zksn accord-
ing to Mult(z | piks)
? Generate the word wksn by
?
?
?
pG(wksn) if zksn = 0
pdk(wksn) if zksn = k + 1
pqj (wksn) if zksn = j + K + 1
We used r to denote relevance judgments:
rk(pi) = 0 if any document component of pi ex-
cept the one corresponding to k is non-zero, or if
any query component of pi except those queries to
which document k is deemed relevant is non-zero
(this prevents a document using the ?wrong? doc-
ument or query components). We have further as-
sumed that the z vector is laid out so that z0 cor-
responds to general English, zk+1 corresponds to
document dk for 0 ? j < J and that zj+K+1 cor-
responds to query qj for 0 ? k < K.
2.4 Graphical Model
The graphical model corresponding to this gener-
ative story is in Figure 1. This model depicts the
four known parameters in square boxes (?, pQ, pD
and pG) with the three observed random variables
in shaded circles (the queries q, the relevance judg-
ments r and the words w) and two unobserved ran-
dom variables in empty circles (the word-level in-
dicator variables z and the sentence level degrees
pi). The rounded plates denote replication: there
are J queries and K documents, containing S sen-
tences in a given document and N words in a given
sentence. The joint probability over the observed
random variables is given in Eq (2):
w
z
rq
pQ
pG
pD
K
J
N
pi
?
S
Figure 1: Graphical model for the Bayesian
Query-Focused Summarization Model.
p (q1:J , r, d1:K) =
[
?
j
?
n
pqj (qjn)
]
? (2)
[
?
k
?
s
?
?
dpiks p (piks | ?, r)
?
n
?
zksn
p (zksn | piks) p (wksn | zksn)
]
This expression computes the probability of the
data by integrating out the unknown variables. In
the case of the pi variables, this is accomplished
by integrating over ?, the multinomial simplex,
according to the prior distribution given by ?. In
the case of the z variables, this is accomplished by
summing over all possible (discrete) values. The
final word probability is conditioned on the z value
by selecting the appropriate distribution from pG,
pD and pQ. Computing this expression and finding
optimal model parameters is intractable due to the
coupling of the variables under the integral.
3 Statistical Inference in BAYESUM
Bayesian inference problems often give rise to in-
tractable integrals, and a large variety of tech-
niques have been proposed to deal with this. The
most popular are Markov Chain Monte Carlo
(MCMC), the Laplace (or saddle-point) approxi-
mation and the variational approximation. A third,
less common, but very effective technique, espe-
cially for dealing with mixture models, is expec-
tation propagation (Minka, 2001). In this paper,
we will focus on expectation propagation; exper-
iments not reported here have shown variational
307
EM to perform comparably but take roughly 50%
longer to converge.
Expectation propagation (EP) is an inference
technique introduced by Minka (2001) as a gener-
alization of both belief propagation and assumed
density filtering. In his thesis, Minka showed
that EP is very effective in mixture modeling
problems, and later demonstrated its superiority
to variational techniques in the Generative As-
pect Model (Minka and Lafferty, 2003). The key
idea is to compute an integral of a product of
terms by iteratively applying a sequence of ?dele-
tion/inclusion? steps. Given an integral of the
form:
?
? dpi p(pi)
?
n tn(pi), EP approximates
each term tn by a simpler term t?n, giving Eq (3).
?
?
dpi q(pi) q(pi) = p(pi)
?
n
t?n(pi) (3)
In each deletion/inclusion step, one of the ap-
proximate terms is deleted from q(?), leaving
q?n(?) = q(?)/t?n(?). A new approximation for
tn(?) is computed so that tn(?)q?n(?) has the same
integral, mean and variance as t?n(?)q?n(?). This
new approximation, t?n(?) is then included back
into the full expression for q(?) and the process re-
peats. This algorithm always has a fixed point and
there are methods for ensuring that the approxi-
mation remains in a location where the integral is
well-defined. Unlike variational EM, the approx-
imation given by EP is global, and often leads to
much more reliable estimates of the true integral.
In the case of our model, we follow Minka and
Lafferty (2003), who adapts latent Dirichlet alo-
cation of Blei et al (2003) to EP. Due to space
constraints, we omit the inference algorithms and
instead direct the interested reader to the descrip-
tion given by Minka and Lafferty (2003).
4 Search-Engine Experiments
The first experiments we run are for query-focused
single document summarization, where relevant
documents are returned from a search engine, and
a short summary is desired of each document.
4.1 Data
The data we use to train and test BAYESUM
is drawn from the Text REtrieval Conference
(TREC) competitions. This data set consists of
queries, documents and relevance judgments, ex-
actly as required by our model. The queries are
typically broken down into four fields of increas-
ing length: the title (3-4 words), the summary (1
sentence), the narrative (2-4 sentences) and the
concepts (a list of keywords). Obviously, one
would expect that the longer the query, the better
a model would be able to do, and this is borne out
experimentally (Section 4.5).
Of the TREC data, we have trained our model
on 350 queries (queries numbered 51-350 and
401-450) and all corresponding relevant docu-
ments. This amounts to roughly 43k documents,
2.1m sentences and 65.8m words. The mean
number of relevant documents per query is 137
and the median is 81 (the most prolific query has
968 relevant documents). On the other hand, each
document is relevant to, on average, 1.11 queries
(the median is 5.5 and the most generally relevant
document is relevant to 20 different queries). In all
cases, we apply stemming using the Porter stem-
mer; for all other models, we remove stop words.
In order to evaluate our model, we had
seven human judges manually perform the query-
focused sentence extraction task. The judges were
supplied with the full TREC query and a single
document relevant to that query, and were asked to
select up to four sentences from the document that
best met the needs given by the query. Each judge
annotated 25 queries with some overlap to allow
for an evaluation of inter-annotator agreement,
yielding annotations for a total of 166 unique
query/document pairs. On the doubly annotated
data, we computed the inter-annotator agreement
using the kappa measure. The kappa value found
was 0.58, which is low, but not abysmal (also,
keep in mind that this is computed over only 25
of the 166 examples).
4.2 Evaluation Criteria
Since there are differing numbers of sentences se-
lected per document by the human judges, one
cannot compute precision and recall; instead, we
opt for other standard IR performance measures.
We consider three related criteria: mean average
precision (MAP), mean reciprocal rank (MRR)
and precision at 2 (P@2). MAP is computed by
calculating precision at every sentence as ordered
by the system up until all relevant sentences are se-
lected and averaged. MRR is the reciprocal of the
rank of the first relevant sentence. P@2 is the pre-
cision computed at the first point that two relevant
sentences have been selected (in the rare case that
308
humans selected only one sentence, we use P@1).
4.3 Baseline Models
As baselines, we consider four strawman models
and two state-of-the-art information retrieval mod-
els. The first strawman, RANDOM ranks sentences
randomly. The second strawman, POSITION,
ranks sentences according to their absolute posi-
tion (in the context of non-query-focused summa-
rization, this is an incredibly powerful baseline).
The third and fourth models are based on the vec-
tor space interpretation of IR. The third model,
JACCARD, uses standard Jaccard distance score
(intersection over union) between each sentence
and the query to rank sentences. The fourth, CO-
SINE, uses TF-IDF weighted cosine similarity.
The two state-of-the-art IR models used as com-
parative systems are based on the language mod-
eling framework described in Section 2.1. These
systems compute a language model for each query
and for each sentence in a document. Sentences
are then ranked according to the KL divergence
between the query model and the sentence model,
smoothed against a general model estimated from
the entire collection, as described in the case of
document retrieval by Lavrenko et al (2002). This
is the first system we compare against, called KL.
The second true system, KL+REL is based on
augmenting the KL system with blind relevance
feedback (query expansion). Specifically, we first
run each query against the document set returned
by the relevance judgments and retrieve the top n
sentences. We then expand the query by interpo-
lating the original query model with a query model
estimated on these sentences. This serves as a
method of query expansion. We ran experiments
ranging n in {5, 10, 25, 50, 100} and the interpo-
lation parameter ? in {0.2, 0.4, 0.6, 0.8} and used
oracle selection (on MRR) to choose the values
that performed best (the results are thus overly op-
timistic). These values were n = 25 and ? = 0.4.
Of all the systems compared, only BAYESUM
and the KL+REL model use the relevance judg-
ments; however, they both have access to exactly
the same information. The other models only run
on the subset of the data used for evaluation (the
corpus language model for the KL system and the
IDF values for the COSINE model are computed
on the full data set). EP ran for 2.5 hours.
MAP MRR P@2
RANDOM 19.9 37.3 16.6
POSITION 24.8 41.6 19.9
JACCARD 17.9 29.3 16.7
COSINE 29.6 50.3 23.7
KL 36.6 64.1 27.6
KL+REL 36.3 62.9 29.2
BAYESUM 44.1 70.8 33.6
Table 1: Empirical results for the baseline models
as well as BAYESUM, when all query fields are
used.
4.4 Performance on all Query Fields
Our first evaluation compares results when all
query fields are used (title, summary, description
and concepts1). These results are shown in Ta-
ble 1. As we can see from these results, the JAC-
CARD system alone is not sufficient to beat the
position-based baseline. The COSINE does beat
the position baseline by a bit of a margin (5 points
better in MAP, 9 points in MRR and 4 points in
P@2), and is in turn beaten by the KL system
(which is 7 points, 14 points and 4 points better
in MAP, MRR and P@2, respectively). Blind rel-
evance feedback (parameters of which were cho-
sen by an oracle to maximize the P@2 metric) ac-
tually hurts MAP and MRR performance by 0.3
and 1.2, respectively, and increases P@2 by 1.5.
Over the best performing baseline system (either
KL or KL+REL), BAYESUM wins by a margin of
7.5 points in MAP, 6.7 for MRR and 4.4 for P@2.
4.5 Varying Query Fields
Our next experimental comparison has to do with
reducing the amount of information given in the
query. In Table 2, we show the performance
of the KL, KL-REL and BAYESUM systems, as
we use different query fields. There are several
things to notice in these results. First, the stan-
dard KL model without blind relevance feedback
performs worse than the position-based model
when only the 3-4 word title is available. Sec-
ond, BAYESUM using only the title outperform
the KL model with relevance feedback using all
fields. In fact, one can apply BAYESUM without
using any of the query fields; in this case, only the
relevance judgments are available to make sense
1A reviewer pointed out that concepts were later removed
from TREC because they were ?too good.? Section 4.5 con-
siders the case without the concepts field.
309
MAP MRR P@2
POSITION 24.8 41.6 19.9
Title KL 19.9 32.6 17.8
KL-Rel 31.9 53.8 26.1
BAYESUM 41.1 65.7 31.6
+Description KL 31.5 58.3 24.1
KL-Rel 32.6 55.0 26.2
BAYESUM 40.9 66.9 31.0
+Summary KL 31.6 56.9 23.8
KL-Rel 34.2 48.5 27.0
BAYESUM 42.0 67.8 31.8
+Concepts KL 36.7 64.2 27.6
KL-Rel 36.3 62.9 29.2
BAYESUM 44.1 70.8 33.6
No Query BAYESUM 39.4 64.7 30.4
Table 2: Empirical results for the position-based
model, the KL-based models and BAYESUM, with
different inputs.
of what the query might be. Even in this cir-
cumstance, BAYESUM achieves a MAP of 39.4,
an MRR of 64.7 and a P@2 of 30.4, still bet-
ter across the board than KL-REL with all query
fields. While initially this seems counterintuitive,
it is actually not so unreasonable: there is signifi-
cantly more information available in several hun-
dred positive relevance judgments than in a few
sentences. However, the simple blind relevance
feedback mechanism so popular in IR is unable to
adequately model this.
With the exception of the KL model without rel-
evance feedback, adding the description on top of
the title does not seem to make any difference for
any of the models (and, in fact, occasionally hurts
according to some metrics). Adding the summary
improves performance in most cases, but not sig-
nificantly. Adding concepts tends to improve re-
sults slightly more substantially than any other.
4.6 Noisy Relevance Judgments
Our model hinges on the assumption that, for a
given query, we have access to a collection of
known relevant documents. In most real-world
cases, this assumption is violated. Even in multi-
document summarization as run in the DUC com-
petitions, the assumption of access to a collection
of documents all relevant to a user need is unreal-
istic. In the real world, we will have to deal with
document collections that ?accidentally? contain
irrelevant documents. The experiments in this sec-
tion show that BAYESUM is comparatively robust.
For this experiment, we use the IR engine that
performed best in the TREC 1 evaluation: In-
query (Callan et al, 1992). We used the offi-
0.4 0.5 0.6 0.7 0.8 0.9 1
28
30
32
34
36
38
40
42
44
R?precision of IR Engine
M
ea
n 
Av
er
ag
e 
Pr
ec
isi
on
 o
f S
en
te
nc
e 
Ex
tra
ct
io
n
KL?Rel (title only)
BayeSum (title only)
KL?Rel (title+desc+sum)
BayeSum (title+desc+sum)
KL?Rel (all fields)
BayeSum (all fields)
Figure 2: Performance with noisy relevance judg-
ments. The X-axis is the R-precision of the IR
engine and the Y-axis is the summarization per-
formance in MAP. Solid lines are BAYESUM, dot-
ted lines are KL-Rel. Blue/stars indicate title only,
red/circles indicated title+description+summary
and black/pluses indicate all fields.
cial TREC results of Inquery on the subset of
the TREC corpus we consider. The Inquery R-
precision on this task is 0.39 using title only, and
0.51 using all fields. In order to obtain curves
as the IR engine improves, we have linearly in-
terpolated the Inquery rankings with the true rel-
evance judgments. By tweaking the interpolation
parameter, we obtain an IR engine with improv-
ing performance, but with a reasonable bias. We
have run both BAYESUM and KL-Rel on the rel-
evance judgments obtained by this method for six
values of the interpolation parameter. The results
are shown in Figure 2.
As we can observe from the figure, the solid
lines (BAYESUM) are always above the dotted
lines (KL-Rel). Considering the KL-Rel results
alone, we can see that for a non-perfect IR engine,
it makes little difference what query fields we use
for the summarization task: they all obtain roughly
equal scores. This is because the performance in
KL-Rel is dominated by the performance of the IR
engine. Looking only at the BAYESUM results, we
can see a much stronger, and perhaps surprising
difference. For an imperfect IR system, it is better
to use only the title than to use the title, description
and summary for the summarization component.
We believe this is because the title is more on topic
than the other fields, which contain terms like ?A
relevant document should describe . . . .? Never-
310
theless, BAYESUM has a more upward trend than
KL-Rel, which indicates that improved IR will re-
sult in improved summarization for BAYESUM but
not for KL-Rel.
5 Multidocument Experiments
We present two results using BAYESUM in the
multidocument summarization settings, based on
the official results from the Multilingual Summa-
rization Evaluation (MSE) and Document Under-
standing Conference (DUC) competitions in 2005.
5.1 Performance at MSE 2005
We participated in the Multilingual Summariza-
tion Evaluation (MSE) workshop with a system
based on BAYESUM. The task for this competi-
tion was generic (no query) multidocument sum-
marization. Fortunately, not having a query is
not a hindrance to our model. To account for the
redundancy present in document collections, we
applied a greedy selection technique that selects
sentences central to the document cluster but far
from previously selected sentences (Daume? III and
Marcu, 2005a). In MSE, our system performed
very well. According to the human ?pyramid?
evaluation, our system came first with a score of
0.529; the next best score was 0.489. In the au-
tomatic ?Basic Element? evaluation, our system
scored 0.0704 (with a 95% confidence interval of
[0.0429, 0.1057]), which was the third best score
on a site basis (out of 10 sites), and was not statis-
tically significantly different from the best system,
which scored 0.0981.
5.2 Performance at DUC 2005
We also participated in the Document Understand-
ing Conference (DUC) competition. The chosen
task for DUC was query-focused multidocument
summarization. We entered a nearly identical sys-
tem to DUC as to MSE, with an additional rule-
based sentence compression component (Daume?
III and Marcu, 2005b). Human evaluators consid-
ered both responsiveness (how well did the sum-
mary answer the query) and linguistic quality. Our
system achieved the highest responsiveness score
in the competition. We scored more poorly on the
linguistic quality evaluation, which (only 5 out of
about 30 systems performed worse); this is likely
due to the sentence compression we performed on
top of BAYESUM. On the automatic Rouge-based
evaluations, our system performed between third
and sixth (depending on the Rouge parameters),
but was never statistically significantly worse than
the best performing systems.
6 Discussion and Future Work
In this paper we have described a model for au-
tomatically generating a query-focused summary,
when one has access to multiple relevance judg-
ments. Our Bayesian Query-Focused Summariza-
tion model (BAYESUM) consistently outperforms
contending, state of the art information retrieval
models, even when it is forced to work with sig-
nificantly less information (either in the complex-
ity of the query terms or the quality of relevance
judgments documents). When we applied our sys-
tem as a stand-alone summarization model in the
2005 MSE and DUC tasks, we achieved among
the highest scores in the evaluation metrics. The
primary weakness of the model is that it currently
only operates in a purely extractive setting.
One question that arises is: why does
BAYESUM so strongly outperform KL-Rel, given
that BAYESUM can be seen as Bayesian formalism
for relevance feedback (query expansion)? Both
models have access to exactly the same informa-
tion: the queries and the true relevance judgments.
This is especially interesting due to the fact that
the two relevance feedback parameters for KL-
Rel were chosen optimally in our experiments, yet
BAYESUM consistently won out. One explanation
for this performance win is that BAYESUM pro-
vides a separate weight for each word, for each
query. This gives it significantly more flexibility.
Doing something similar with ad-hoc query ex-
pansion techniques is difficult due to the enormous
number of parameters; see, for instance, (Buckley
and Salton, 1995).
One significant advantage of working in the
Bayesian statistical framework is that it gives us
a straightforward way to integrate other sources of
knowledge into our model in a coherent manner.
One could consider, for instance, to extend this
model to the multi-document setting, where one
would need to explicitly model redundancy across
documents. Alternatively, one could include user
models to account for novelty or user preferences
along the lines of Zhang et al (2002).
Our model is similar in spirit to the random-
walk summarization model (Otterbacher et al,
2005). However, our model has several advan-
tages over this technique. First, our model has
311
no tunable parameters: the random-walk method
has many (graph connectivity, various thresholds,
choice of similarity metrics, etc.). Moreover, since
our model is properly Bayesian, it is straightfor-
ward to extend it to model other aspects of the
problem, or to related problems. Doing so in a non
ad-hoc manner in the random-walk model would
be nearly impossible.
Another interesting avenue of future work is to
relax the bag-of-words assumption. Recent work
has shown, in related models, how this can be done
for moving from bag-of-words models to bag-of-
ngram models (Wallach, 2006); more interesting
than moving to ngrams would be to move to de-
pendency parse trees, which could likely be ac-
counted for in a similar fashion. One could also
potentially relax the assumption that the relevance
judgments are known, and attempt to integrate
them out as well, essentially simultaneously per-
forming IR and summarization.
Acknowledgments. We thank Dave Blei and Tom
Minka for discussions related to topic models, and to the
anonymous reviewers, whose comments have been of great
benefit. This work was partially supported by the National
Science Foundation, Grant IIS-0326276.
References
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research (JMLR), 3:993?1022, January.
Chris Buckley and Gerard Salton. 1995. Optimiza-
tion of relevance feedback weights. In Proceedings
of the Conference on Research and Developments in
Information Retrieval (SIGIR).
Jamie Callan, Bruce Croft, and Stephen Harding.
1992. The INQUERY retrieval system. In Pro-
ceedings of the 3rd International Conference on
Database and Expert Systems Applications.
Hal Daume? III and Daniel Marcu. 2005a. Bayesian
multi-document summarization at MSE. In ACL
2005 Workshop on Intrinsic and Extrinsic Evalua-
tion Measures.
Hal Daume? III and Daniel Marcu. 2005b. Bayesian
summarization at DUC and a suggestion for extrin-
sic evaluation. In Document Understanding Confer-
ence.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of the
Conference on Research and Developments in Infor-
mation Retrieval (SIGIR).
Victor Lavrenko, M. Choquette, and Bruce Croft.
2002. Crosslingual relevance models. In Proceed-
ings of the Conference on Research and Develop-
ments in Information Retrieval (SIGIR).
Xiaoyong Liu and Bruce Croft. 2002. Passage re-
trieval based on language models. In Processing
of the Conference on Information and Knowledge
Management (CIKM).
Thomas Minka and John Lafferty. 2003. Expectation-
propagation for the generative aspect model. In Pro-
ceedings of the Converence on Uncertainty in Artifi-
cial Intelligence (UAI).
Thomas Minka. 2001. A family of algorithms for ap-
proximate Bayesian inference. Ph.D. thesis, Mas-
sachusetts Institute of Technology, Cambridge, MA.
Vanessa Murdock and Bruce Croft. 2005. A transla-
tion model for sentence retrieval. In Proceedings of
the Joint Conference on Human Language Technol-
ogy Conference and Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 684?
691.
Jahna Otterbacher, Gunes Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In Proceedings of the
Joint Conference on Human Language Technology
Conference and Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP).
Jay M. Ponte and Bruce Croft. 1998. A language mod-
eling approach to information retrieval. In Proceed-
ings of the Conference on Research and Develop-
ments in Information Retrieval (SIGIR).
Hanna Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the International Con-
ference on Machine Learning (ICML).
Yi Zhang, Jamie Callan, and Thomas Minka. 2002.
Novelty and redundancy detection in adaptive filter-
ing. In Proceedings of the Conference on Research
and Developments in Information Retrieval (SIGIR).
312
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 769?776,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semi-Supervised Training for Statistical Word Alignment
Alexander Fraser
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
fraser@isi.edu
Daniel Marcu
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
We introduce a semi-supervised approach
to training for statistical machine transla-
tion that alternates the traditional Expecta-
tion Maximization step that is applied on a
large training corpus with a discriminative
step aimed at increasing word-alignment
quality on a small, manually word-aligned
sub-corpus. We show that our algorithm
leads not only to improved alignments
but also to machine translation outputs of
higher quality.
1 Introduction
The most widely applied training procedure for
statistical machine translation ? IBM model 4
(Brown et al, 1993) unsupervised training fol-
lowed by post-processing with symmetrization
heuristics (Och and Ney, 2003) ? yields low
quality word alignments. When compared with
gold standard parallel data which was manually
aligned using a high-recall/precision methodology
(Melamed, 1998), the word-level alignments pro-
duced automatically have an F-measure accuracy
of 64.6 and 76.4% (see Section 2 for details).
In this paper, we improve word alignment and,
subsequently, MT accuracy by developing a range
of increasingly sophisticated methods:
1. We first recast the problem of estimating the
IBM models (Brown et al, 1993) in a dis-
criminative framework, which leads to an ini-
tial increase in word-alignment accuracy.
2. We extend the IBM models with new
(sub)models, which leads to additional in-
creases in word-alignment accuracy. In the
process, we also show that these improve-
ments are explained not only by the power
of the new models, but also by a novel search
procedure for the alignment of highest prob-
ability.
3. Finally, we propose a training procedure that
interleaves discriminative training with max-
imum likelihood training.
These steps lead to word alignments of higher
accuracy which, in our case, correlate with higher
MT accuracy.
The rest of the paper is organized as follows.
In Section 2, we review the data sets we use to
validate experimentally our algorithms and the as-
sociated baselines. In Section 3, we present itera-
tively our contributions that eventually lead to ab-
solute increases in alignment quality of 4.8% for
French/English and 4.8% for Arabic/English, as
measured using F-measure for large word align-
ment tasks. These contributions pertain to the
casting of the training procedure in the discrim-
inative framework (Section 3.1); the IBM model
extensions and modified search procedure for the
Viterbi alignments (Section 3.2); and the in-
terleaved, minimum error/maximum likelihood,
training algorithm (Section 4). In Section 5, we as-
sess the impact that our improved alignments have
on MT quality. We conclude with a comparison of
our work with previous research on discriminative
training for word alignment and a short discussion
of semi-supervised learning.
2 Data Sets and Baseline
We conduct experiments on alignment and
translation tasks using Arabic/English and
French/English data sets (see Table 1 for details).
Both sets have training data and two gold stan-
dard word alignments for small samples of the
training data, which we use as the alignment
769
ARABIC/ENGLISH FRENCH/ENGLISH
A E F E
TRAINING
SENTS 3,713,753 2,842,184
WORDS 102,473,086 119,994,972 75,794,254 67,366,819
VOCAB 489,534 231,255 149,568 114,907
SINGLETONS 199,749 104,155 60,651 47,765
ALIGN DISCR.
SENTS 100 110
WORDS 1,712 2,010 1,888 1,726
LINKS 2,129 2,292
ALIGN TEST
SENTS 55 110
WORDS 1,004 1,210 1,899 1,716
LINKS 1,368 2,176
MAX BLEU SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)WORDS 17664 22.0K TO 24.5K 20,562 17,454
TRANS. TEST SENTS 663 (4 REFERENCES) 2,380 (1 REFERENCE)WORDS 16,075 19.0K TO 21.6K 58,990 49,182
Table 1: Datasets
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION)
F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED)
Table 2: Baseline Results. F-measures are presented on both the alignment discriminative training set
and the alignment test set sub-corpora, separated by /.
discriminative training set and alignment test set.
Translation quality is evaluated by translating
a held-out translation test set. An additional
translation set called the Maximum BLEU set is
employed by the SMT system to train the weights
associated with the components of its log-linear
model (Och, 2003).
The training corpora are publicly avail-
able: both the Arabic/English data and the
French/English Hansards were released by
LDC. We created the manual word alignments
ourselves, following the Blinker guidelines
(Melamed, 1998).
To train our baseline systems we follow a stan-
dard procedure. The models were trained two
times, first using French or Arabic as the source
language and then using English as the source
language. For each training direction, we run
GIZA++ (Och and Ney, 2003), specifying 5 iter-
ations of Model 1, 4 iterations of the HMM model
(Vogel et al, 1996), and 4 iterations of Model 4.
We quantify the quality of the resulting hypothe-
sized alignments with F-measure using the manu-
ally aligned sets.
We present the results for three different con-
ditions in Table 2. For the ?F to E? direction the
models assign non-zero probability to alignments
consisting of links from one Foreign word to zero
or more English words, while for ?E to F? the
models assign non-zero probability to alignments
consisting of links from one English word to zero
or more Foreign words. It is standard practice to
improve the final alignments by combining the ?F
to E? and ?E to F? directions using symmetriza-
tion heuristics. We use the ?union?, ?refined? and
?intersection? heuristics defined in (Och and Ney,
2003) which are used in conjunction with IBM
Model 4 as the baseline in virtually all recent work
on word alignment. In Table 2, we report the best
symmetrized results.
The low F-measure scores of the baselines mo-
tivate our work.
3 Improving Word Alignments
3.1 Discriminative Reranking of the IBM
Models
We reinterpret the five groups of parameters of
Model 4 listed in the first five lines of Table 3 as
sub-models of a log-linear model (see Equation 1).
Each sub-model hm has an associated weight ?m.
Given a vector of these weights ?, the alignment
search problem, i.e. the search to return the best
alignment a? of the sentences e and f according to
the model, is specified by Equation 2.
p?(f, a|e) =
exp(?i ?ihi(a, e, f))
?
a?,f ? exp(
?
i ?ihi(a?, e, f ?))
(1)
a? = argmax
a
?
i
?ihi(f, a, e) (2)
770
m Model 4 Description m Description
1 t(f |e) translation probs, f and e are words 9 translation table using approx. stems
2 n(?|e) fertility probs, ? is number of words generated by e 10 backoff fertility (fertility estimated
over all e)
3 null parameters used in generating Foreign words which
are unaligned
11 backoff fertility for words with count
<= 5
4 d1(4j) movement probs of leftmost Foreign word translated
from a particular e
12 translation table from HMM iteration 4
5 d>1(4j) movement probs of other Foreign words translated
from a particular e
13 zero fertility English word penalty
6 translation table from refined combination of both
alignments
14 non-zero fertility English word penalty
7 translation table from union of both alignments 15 NULL Foreign word penalty
8 translation table from intersection of both alignments 16 non-NULL Foreign word penalty
Table 3: Sub-Models. Note that sub-models 1 to 5 are IBM Model 4, sub-models 6 to 16 are new.
Log-linear models are often trained to maxi-
mize entropy, but we will train our model di-
rectly on the final performance criterion. We use
1?F-measure as our error function, comparing hy-
pothesized word alignments for the discriminative
training set with the gold standard.
Och (2003) has described an efficient exact
one-dimensional error minimization technique for
a similar search problem in machine translation.
The technique involves calculating a piecewise
constant function fm(x) which evaluates the er-
ror of the hypotheses which would be picked by
equation 2 from a set of hypotheses if we hold all
weights constant, except for the weight ?m (which
is set to x).
The discriminative reranking algorithm is ini-
tialized with the parameters of the sub-models ?,
an initial choice of the ? vector, gold standard
word alignments (labels) for the alignment dis-
criminative training set, the constant N specifying
the N-best list size used1, and an empty master set
of hypothesized alignments. The algorithm is a
three step loop:
1. Enrich the master set of hypothesized align-
ments by producing an N-best list using ?.
If all of the hypotheses in the N-best list are
already in the master set, the algorithm has
converged, so terminate the loop.
2. Consider the current ? vector and 999 addi-
tional randomly generated vectors, setting ?
to the vector with lowest error on the master
set.
3. Repeatedly run Och?s one-dimensional error
minimization step until there is no further er-
ror reduction (this results in a new vector ?).
1N = 128 for our experiments
3.2 Improvements to the Model and Search
3.2.1 New Sources of Knowledge
We define new sub-models to model factors not
captured by Model 4. These are lines 6 to 16
of Table 3, where we use the ?E to F? align-
ment direction as an example. We use word-level
translation tables informed by both the ?E to F?
and the ?F to E? translation directions derived us-
ing the three symmetrization heuristics, the ?E to
F? translation table from the final iteration of the
HMM model and an ?E to F? translation table de-
rived using approximative stemming. The approx-
imative stemming sub-model (sub-model 9) uses
the first 4 letters of each vocabulary item as the
stem for English and French while for Arabic we
use the full word as the stem. We also use sub-
models for backed off fertility, and direct penal-
ization of unaligned English words (?zero fertil-
ity?) and aligned English words, and unaligned
Foreign words (?NULL-generated? words) and
aligned Foreign words. This is a small sampling
of the kinds of knowledge sources we can use in
this framework; many others have been proposed
in the literature.
Table 4 shows an evaluation of discriminative
reranking. We observe:
1. The first line is the starting point, which is
the Viterbi alignment of the 4th iteration of
HMM training.
2. The 1-to-many alignments generated by dis-
criminatively reranking Model 4 are better
than the 1-to-many alignments of four itera-
tions of Model 4.
3. The 1-to-many alignments of the discrimina-
tively reranked extended model are much bet-
ter than four iterations of Model 4.
771
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E LAST ITERATION HMM 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION)
A/E MODEL 4 RERANKING 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION)
A/E EXTENDED MODEL RERANKING 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION)
A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION)
F/E LAST ITERATION HMM 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED)
F/E MODEL 4 RERANKING 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED)
F/E EXTENDED MODEL RERANKING 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED)
F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED)
Table 4: Discriminative Reranking with Improved Search. F-measures are presented on both the align-
ment discriminative training set and the alignment test set sub-corpora, separated by /.
4. The discriminatively reranked extended
model outperforms four iterations of Model
4 in both cases with the best heuristic
symmetrization, but some of the gain is
lost as we are optimizing the F-measure of
the 1-to-many alignments rather than the
F-measure of the many-to-many alignments
directly.
Overall, the results show our approach is better
than or competitive with running four iterations of
unsupervised Model 4 training.
3.2.2 New Alignment Search Algorithm
Brown et al (1993) introduced operations defin-
ing a hillclimbing search appropriate for Model 4.
Their search starts with a complete hypothesis and
exhaustively applies two operations to it, selecting
the best improved hypothesis it can find (or termi-
nating if no improved hypothesis is found). This
search makes many search errors2. We developed
a new alignment algorithm to reduce search errors:
? We perform an initial hillclimbing search (as
in the baseline algorithm) but construct a pri-
ority queue of possible other candidate align-
ments to consider.
? Alignments which are expanded are marked
so that they will not be returned to at a future
point in the search.
? The alignment search operates by consider-
ing complete hypotheses so it is an ?anytime?
algorithm (meaning that it always has a cur-
rent best guess). Timers can therefore be
used to terminate the processing of the pri-
ority queue of candidate alignments.
The first two improvements are related to the
well-known Tabu local search algorithm (Glover,
2A search error in a word aligner is a failure to find the
best alignment according to the model, i.e. in our case a fail-
ure to maximize Equation 2.
1986). The third improvement is important for
restricting total time used when producing align-
ments for large training corpora.
We performed two experiments. The first evalu-
ates the number of search errors. For each corpus
we sampled 1000 sentence pairs randomly, with
no sentence length restriction. Model 4 parameters
are estimated from the final HMM Viterbi align-
ment of these sentence pairs. We then search to
try to find the Model 4 Viterbi alignment with both
the new and old algorithms, allowing them both
to process for the same amount of time. The per-
centage of known search errors is the percentage
of sentences from our sample in which we were
able to find a more probable candidate by apply-
ing our new algorithm using 24 hours of compu-
tation for just the 1000 sample sentences. Table
5 presents the results, showing that our new algo-
rithm reduced search errors in all cases, but fur-
ther reduction could be obtained. The second ex-
periment shows the impact of the new search on
discriminative reranking of Model 4 (see Table 6).
Reduced search errors lead to a better fit of the dis-
criminative training corpus.
4 Semi-Supervised Training for Word
Alignments
Intuitively, in approximate EM training for Model
4 (Brown et al, 1993), the E-step corresponds to
calculating the probability of all alignments ac-
cording to the current model estimate, while the
M-step is the creation of a new model estimate
given a probability distribution over alignments
(calculated in the E-step).
In the E-step ideally all possible alignments
should be enumerated and labeled with p(a|e, f),
but this is intractable. For the M-step, we would
like to count over all possible alignments for each
sentence pair, weighted by their probability ac-
cording to the model estimated at the previous
772
SYSTEM F TO E ERRORS % E TO F ERRORS %
A/E OLD 19.4 22.3
A/E NEW 8.5 15.3
F/E OLD 32.5 25.9
F/E NEW 13.7 10.4
Table 5: Comparison of New Search Algorithm with Old Search Algorithm
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E MODEL 4 RERANKING OLD 64.1 / 58.1 54.0 / 48.8 67.9 / 63.0 (UNION)
A/E MODEL 4 RERANKING NEW 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION)
F/E MODEL 4 RERANKING OLD 77.3 / 77.8 78.3 / 77.2 79.2 / 79.1 (REFINED)
F/E MODEL 4 RERANKING NEW 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED)
Table 6: Impact of Improved Search on Discriminative Reranking of Model 4
step. Because this is not tractable, we make the
assumption that the single assumed Viterbi align-
ment can be used to update our estimate in the M-
step. This approximation is called Viterbi training.
Neal and Hinton (1998) analyze approximate EM
training and motivate this type of variant.
We extend approximate EM training to perform
a new type of training which we call Minimum Er-
ror / Maximum Likelihood Training. The intuition
behind this approach to semi-supervised training
is that we wish to obtain the advantages of both
discriminative training (error minimization) and
approximate EM (which allows us to estimate a
large numbers of parameters even though we have
very few gold standard word alignments). We in-
troduce the EMD algorithm, in which discrimina-
tive training is used to control the contributions
of sub-models (thereby minimizing error), while a
procedure similar to one step of approximate EM
is used to estimate the large number of sub-model
parameters.
A brief sketch of the EMD algorithm applied
to our extended model is presented in Figure 1.
Parameters have a superscript t representing their
value at iteration t. We initialize the algorithm
with the gold standard word alignments (labels) of
the word alignment discriminative training set, an
initial ?, N, and the starting alignments (the iter-
ation 4 HMM Viterbi alignment). In line 2, we
make iteration 0 estimates of the 5 sub-models of
Model 4 and the 6 heuristic sub-models which are
iteration dependent. In line 3, we run discrimi-
native training using the algorithm from Section
3.1. In line 4, we measure the error of the result-
ing ? vector. In the main loop in line 7 we align
the full training set (similar to the E-step of EM),
while in line 8 we estimate the iteration-dependent
sub-models (similar to the M-step of EM). Then
1: Algorithm EMD(labels, ??, N, starting alignments)
2: estimate ?0m for m = 1 to 11
3: ?0 = Discrim(?0, ??, labels, N)
4: e0 = E(?0, labels)
5: t = 1
6: loop
7: align full training set using ?t?1 and ?t?1m
8: estimate ?tm for m = 1 to 11
9: ?t = Discrim(?t, ???, labels, N)
10: et = E(?t, labels)
11: if et >= et?1 then
12: terminate loop
13: end if
14: t = t + 1
15: end loop
16: return hypothesized alignments of full training set
Figure 1: Sketch of the EMD algorithm
we perform discriminative reranking in line 9 and
check for convergence in lines 10 and 11 (conver-
gence means that error was not decreased from the
previous iteration). The output of the algorithm is
new hypothesized alignments of the training cor-
pus.
Table 7 evaluates the EMD semi-supervised
training algorithm. We observe:
1. In both cases there is improved F-measure
on the second iteration of semi-supervised
training, indicating that the EMD algorithm
performs better than one step discriminative
reranking.
2. The French/English data set has converged3
after the second iteration.
3. The Arabic/English data set converged after
improvement for the first, second and third
iterations.
We also performed an additional experiment for
French/English aimed at understanding the poten-
tial contribution of the word aligned data without
3Convergence is achieved because error on the word
alignment discriminative training set does not improve.
773
SYSTEM F-MEASURE F TO E F-MEASURE E TO F BEST SYMM.
A/E STARTING POINT 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION)
A/E EMD: ITERATION 1 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION)
A/E EMD: ITERATION 2 69.8 / 63.1 64.1 / 59.5 74.1 / 68.1 (UNION)
A/E EMD: ITERATION 3 70.6 / 65.4 64.3 / 59.2 74.7 / 69.4 (UNION)
F/E STARTING POINT 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED)
F/E EMD: ITERATION 1 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED)
F/E EMD: ITERATION 2 79.4 / 80.5 79.8 / 80.5 79.9 / 81.2 (REFINED)
Table 7: Semi-Supervised Training Task F-measure
the new algorithm4. Like Ittycheriah and Roukos
(2005), we converted the alignment discrimina-
tive training corpus links into a special corpus
consisting of parallel sentences where each sen-
tence consists only of a single word involved in
the link. We found that the information in the
links was ?washed out? by the rest of the data and
resulted in no change in the alignment test set?s
F-Measure. Callison-Burch et al (2004) showed
in their work on combining alignments of lower
and higher quality that the alignments of higher
quality should be given a much higher weight than
the lower quality alignments. Using this insight,
we found that adding 10,000 copies of the special
corpus to our training data resulted in the highest
alignment test set gain, which was a small gain
of 0.6 F-Measure. This result suggests that while
the link information is useful for improving F-
Measure, our improved methods for training are
producing much larger improvements.
5 Improvement of MT Quality
The symmetrized alignments from the last iter-
ation of EMD were used to build phrasal SMT
systems, as were the symmetrized Model 4 align-
ments (the baseline). Aside from the final align-
ment, all other resources were held constant be-
tween the baseline and contrastive SMT systems,
including those based on lower level alignments
models such as IBM Model 1. For all of our ex-
periments, we use two language models, one built
using the English portion of the training data and
the other built using additional English news data.
We run Maximum BLEU (Och, 2003) for 25 iter-
ations individually for each system.
Table 8 shows our results. We report BLEU (Pa-
pineni et al, 2001) multiplied by 100. We also
show the F-measure after heuristic symmetrization
of the alignment test sets. The table shows that
4We would like to thank an anonymous reviewer for sug-
gesting that this experiment would be useful even when using
a small discriminative training corpus.
our algorithm produces heuristically symmetrized
final alignments of improved F-measure. Us-
ing these alignments in our phrasal SMT system,
we produced a statistically significant BLEU im-
provement (at a 95% confidence interval a gain of
0.78 is necessary) on the French/English task and
a statistically significant BLEU improvement on
the Arabic/English task (at a 95% confidence in-
terval a gain of 1.2 is necessary).
5.1 Error Criterion
The error criterion we used for all experiments
is 1 ? F-measure. The formula for F-measure is
shown in Equation 3. (Fraser and Marcu, 2006) es-
tablished that tuning the trade-off between Preci-
sion and Recall in the F-Measure formula will lead
to the best BLEU results. We tuned ? by build-
ing a collection of alignments using our baseline
system, measuring Precision and Recall against
the alignment discriminative training set, build-
ing SMT systems and measuring resulting BLEU
scores, and then searching for an appropriate ?
setting. We searched ? = 0.1, 0.2, ..., 0.9 and set
? so that the resulting F-measure tracks BLEU to
the best extent possible. The best settings were
? = 0.2 for Arabic/English and ? = 0.7 for
French/English, and these settings of ? were used
for every result reported in this paper. See (Fraser
and Marcu, 2006) for further details.
F (A, S, ?) = 1
?Precision(A,S) +
(1??)
Recall(A,S)
(3)
6 Previous Research
Previous work on discriminative training for word-
alignment differed most strongly from our ap-
proach in that it generally views word-alignment
as a supervised task. Examples of this perspective
include (Liu et al, 2005; Ittycheriah and Roukos,
2005; Moore, 2005; Taskar et al, 2005). All
of these also used knowledge from one of the
IBM Models in order to obtain competitive results
774
SYSTEM BLEU F-MEASURE
A/E UNSUP. MODEL 4 UNION 49.16 64.6
A/E EMD 3 UNION 50.84 69.4
F/E UNSUP. MODEL 4 REFINED 30.63 76.4
F/E EMD 2 REFINED 31.56 81.2
Table 8: Evaluation of Translation Quality
with the baseline (with the exception of (Moore,
2005)). We interleave discriminative training with
EM and are therefore performing semi-supervised
training. We show that semi-supervised training
leads to better word alignments than running unsu-
pervised training followed by discriminative train-
ing.
Another important difference with previous
work is that we are concerned with generating
many-to-many word alignments. Cherry and Lin
(2003) and Taskar et al (2005) compared their re-
sults with Model 4 using ?intersection? by look-
ing at AER (with the ?Sure? versus ?Possible? link
distinction), and restricted themselves to consider-
ing 1-to-1 alignments. However, ?union? and ?re-
fined? alignments, which are many-to-many, are
what are used to build competitive phrasal SMT
systems, because ?intersection? performs poorly,
despite having been shown to have the best AER
scores for the French/English corpus we are using
(Och and Ney, 2003). (Fraser and Marcu, 2006)
recently found serious problems with AER both
empirically and analytically, which explains why
optimizing AER frequently results in poor ma-
chine translation performance.
Finally, we show better MT results by using F-
measure with a tuned ? value. The only previous
discriminative approach which has been shown to
produce translations of similar or better quality to
those produced by the symmetrized baseline was
(Ittycheriah and Roukos, 2005). They had access
to 5000 gold standard word alignments, consider-
ably more than the 100 or 110 gold standard word
alignments used here. They also invested signif-
icant effort in sub-model engineering (producing
both sub-models specific to Arabic/English align-
ment and sub-models which would be useful for
other language pairs), while we use sub-models
which are simple extensions of Model 4 and lan-
guage independent.
The problem of semi-supervised learning is of-
ten defined as ?using unlabeled data to help su-
pervised learning? (Seeger, 2000). Most work on
semi-supervised learning uses underlying distribu-
tions with a relatively small number of parame-
ters. An initial model is estimated in a supervised
fashion using the labeled data, and this supervised
model is used to attach labels (or a probability dis-
tribution over labels) to the unlabeled data, then a
new supervised model is estimated, and this is it-
erated. If these techniques are applied when there
are a small number of labels in relation to the num-
ber of parameters used, they will suffer from the
?overconfident pseudo-labeling problem? (Seeger,
2000), where the initial labels of poor quality as-
signed to the unlabeled data will dominate the
model estimated in the M-step. However, there
are tasks with large numbers of parameters where
there are sufficient labels. Nigam et al (2000) ad-
dressed a text classification task. They estimate
a Naive Bayes classifier over the labeled data and
use it to provide initial MAP estimates for unla-
beled documents, followed by EM to further re-
fine the model. Callison-Burch et al (2004) exam-
ined the issue of semi-supervised training for word
alignment, but under a scenario where they simu-
lated sufficient gold standard word alignments to
follow an approach similar to Nigam et al (2000).
We do not have enough labels for this approach.
We are aware of two approaches to semi-
supervised learning which are more similar in
spirit to ours. Ivanov et al (2001) used discrimi-
native training in a reinforcement learning context
in a similar way to our adding of a discriminative
training step to an unsupervised context. A large
body of work uses semi-supervised learning for
clustering by imposing constraints on clusters. For
instance, in (Basu et al, 2004), the clustering sys-
tem was supplied with pairs of instances labeled
as belonging to the same or different clusters.
7 Conclusion
We presented a semi-supervised algorithm based
on IBM Model 4, with modeling and search ex-
tensions, which produces alignments of improved
F-measure over unsupervised Model 4 training.
We used these alignments to produce transla-
tions of higher quality.
775
The semi-supervised learning literature gen-
erally addresses augmenting supervised learning
tasks with unlabeled data (Seeger, 2000). In con-
trast, we augmented an unsupervised learning task
with labeled data. We hope that Minimum Error /
Maximum Likelihood training using the EMD al-
gorithm can be used for a wide diversity of tasks
where there is not enough labeled data to allow
supervised estimation of an initial model of rea-
sonable quality.
8 Acknowledgments
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022. We would like to thank the USC Cen-
ter for High Performance Computing and Commu-
nications.
References
Sugato Basu, Mikhail Bilenko, and Raymond J.
Mooney. 2004. A probabilistic framework for semi-
supervised clustering. In KDD ?04: Proc. of the
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 59?68, New
York. ACM Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In
Proc. of the 42nd Annual Meeting of the Association
for Computational Linguistics, Barcelona, Spain,
July.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, July.
Alexander Fraser and Daniel Marcu. 2006. Measur-
ing word alignment quality for statistical machine
translation. In Technical Report ISI-TR-616. Avail-
able at http://www.isi.edu/ fraser/research.html,
ISI/University of Southern California, May.
Fred Glover. 1986. Future paths for integer program-
ming and links to artificial intelligence. Computers
and Operations Research, 13(5):533?549.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for Arabic-English
machine translation. In Proc. of Human Language
Technology Conf. and Conf. on Empirical Methods
in Natural Language Processing, Vancouver, BC.
Yuri A. Ivanov, Bruce Blumberg, and Alex Pentland.
2001. Expectation maximization for weakly labeled
data. In ICML ?01: Proc. of the Eighteenth Interna-
tional Conf. on Machine Learning, pages 218?225.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proc. of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 459?466, Ann Arbor, MI.
I. Dan Melamed. 1998. Manual annotation of trans-
lational equivalence: The blinker project. Techni-
cal Report 98-07, Institute for Research in Cognitive
Science, Philadelphia, PA.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proc. of Human
Language Technology Conf. and Conf. on Empirical
Methods in Natural Language Processing, Vancou-
ver, BC, October.
Radford M. Neal and Geoffrey E. Hinton. 1998. A
view of the EM algorithm that justifies incremental,
sparse, and other variants. In M. I. Jordan, editor,
Learning in Graphical Models. Kluwer.
Kamal Nigam, Andrew K. McCallum, Sebastian
Thrun, and Tom M. Mitchell. 2000. Text classifi-
cation from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103?134.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo, Japan.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY, September.
Matthias Seeger. 2000. Learning with labeled and un-
labeled data. In Technical report, 2000. Available at
http://www.dai.ed.ac.uk/ seeger/papers.html.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proc. of Human Language Technol-
ogy Conf. and Conf. on Empirical Methods in Natu-
ral Language Processing, Vancouver, BC, October.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING ?96: The 16th Int. Conf. on
Computational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
776
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1105?1112,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Stochastic Language Generation Using WIDL-expressions and its
Application in Machine Translation and Summarization
Radu Soricut
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
radu@isi.edu
Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
We propose WIDL-expressions as a flex-
ible formalism that facilitates the integra-
tion of a generic sentence realization sys-
tem within end-to-end language process-
ing applications. WIDL-expressions rep-
resent compactly probability distributions
over finite sets of candidate realizations,
and have optimal algorithms for realiza-
tion via interpolation with language model
probability distributions. We show the ef-
fectiveness of a WIDL-based NLG system
in two sentence realization tasks: auto-
matic translation and headline generation.
1 Introduction
The Natural Language Generation (NLG) com-
munity has produced over the years a consid-
erable number of generic sentence realization
systems: Penman (Matthiessen and Bateman,
1991), FUF (Elhadad, 1991), Nitrogen (Knight
and Hatzivassiloglou, 1995), Fergus (Bangalore
and Rambow, 2000), HALogen (Langkilde-Geary,
2002), Amalgam (Corston-Oliver et al, 2002), etc.
However, when it comes to end-to-end, text-to-
text applications ? Machine Translation, Summa-
rization, Question Answering ? these generic sys-
tems either cannot be employed, or, in instances
where they can be, the results are significantly
below that of state-of-the-art, application-specific
systems (Hajic et al, 2002; Habash, 2003). We
believe two reasons explain this state of affairs.
First, these generic NLG systems use input rep-
resentation languages with complex syntax and se-
mantics. These languages involve deep, semantic-
based subject-verb or verb-object relations (such
as ACTOR, AGENT, PATIENT, etc., for Penman
and FUF), syntactic relations (such as subject,
object, premod, etc., for HALogen), or lexi-
cal dependencies (Fergus, Amalgam). Such inputs
cannot be accurately produced by state-of-the-art
analysis components from arbitrary textual input
in the context of text-to-text applications.
Second, most of the recent systems (starting
with Nitrogen) have adopted a hybrid approach
to generation, which has increased their robust-
ness. These hybrid systems use, in a first phase,
symbolic knowledge to (over)generate a large set
of candidate realizations, and, in a second phase,
statistical knowledge about the target language
(such as stochastic language models) to rank the
candidate realizations and find the best scoring
one. The disadvantage of the hybrid approach
? from the perspective of integrating these sys-
tems within end-to-end applications ? is that the
two generation phases cannot be tightly coupled.
More precisely, input-driven preferences and tar-
get language?driven preferences cannot be inte-
grated in a true probabilistic model that can be
trained and tuned for maximum performance.
In this paper, we propose WIDL-expressions
(WIDL stands for Weighted Interleave, Disjunc-
tion, and Lock, after the names of the main op-
erators) as a representation formalism that facil-
itates the integration of a generic sentence real-
ization system within end-to-end language appli-
cations. The WIDL formalism, an extension of
the IDL-expressions formalism of Nederhof and
Satta (2004), has several crucial properties that
differentiate it from previously-proposed NLG
representation formalisms. First, it has a sim-
ple syntax (expressions are built using four oper-
ators) and a simple, formal semantics (probability
distributions over finite sets of strings). Second,
it is a compact representation that grows linearly
1105
in the number of words available for generation
(see Section 2). (In contrast, representations such
as word lattices (Knight and Hatzivassiloglou,
1995) or non-recursive CFGs (Langkilde-Geary,
2002) require exponential space in the number
of words available for generation (Nederhof and
Satta, 2004).) Third, it has good computational
properties, such as optimal algorithms for inter-
section with   -gram language models (Section 3).
Fourth, it is flexible with respect to the amount of
linguistic processing required to produce WIDL-
expressions directly from text (Sections 4 and 5).
Fifth, it allows for a tight integration of input-
specific preferences and target-language prefer-
ences via interpolation of probability distributions
using log-linear models. We show the effec-
tiveness of our proposal by directly employing
a generic WIDL-based generation system in two
end-to-end tasks: machine translation and auto-
matic headline generation.
2 The WIDL Representation Language
2.1 WIDL-expressions
In this section, we introduce WIDL-expressions, a
formal language used to compactly represent prob-
ability distributions over finite sets of strings.
Given a finite alphabet of symbols  , atomic
WIDL-expressions are of the form  , with 

.
For a WIDL-expression  , its semantics is
a probability distribution 	
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 803?810,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discourse Generation Using Utility-Trained Coherence Models
Radu Soricut
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
radu@isi.edu
Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
We describe a generic framework for inte-
grating various stochastic models of dis-
course coherence in a manner that takes
advantage of their individual strengths. An
integral part of this framework are algo-
rithms for searching and training these
stochastic coherence models. We evaluate
the performance of our models and algo-
rithms and show empirically that utility-
trained log-linear coherence models out-
perform each of the individual coherence
models considered.
1 Introduction
Various theories of discourse coherence (Mann
and Thompson, 1988; Grosz et al, 1995) have
been applied successfully in discourse analy-
sis (Marcu, 2000; Forbes et al, 2001) and dis-
course generation (Scott and de Souza, 1990; Kib-
ble and Power, 2004). Most of these efforts, how-
ever, have limited applicability. Those that use
manually written rules model only the most visi-
ble discourse constraints (e.g., the discourse con-
nective ?although? marks a CONCESSION relation),
while being oblivious to fine-grained lexical indi-
cators. And the methods that utilize manually an-
notated corpora (Carlson et al, 2003; Karamanis
et al, 2004) and supervised learning algorithms
have high costs associated with the annotation pro-
cedure, and cannot be easily adapted to different
domains and genres.
In contrast, more recent research has focused on
stochastic approaches that model discourse coher-
ence at the local lexical (Lapata, 2003) and global
levels (Barzilay and Lee, 2004), while preserving
regularities recognized by classic discourse theo-
ries (Barzilay and Lapata, 2005). These stochas-
tic coherence models use simple, non-hierarchical
representations of discourse, and can be trained
with minimal human intervention, using large col-
lections of existing human-authored documents.
These models are attractive due to their increased
scalability and portability. As each of these
stochastic models captures different aspects of co-
herence, an important question is whether we can
combine them in a model capable of exploiting all
coherence indicators.
A frequently used testbed for coherence models
is the discourse ordering problem, which occurs
often in text generation, complex question answer-
ing, and multi-document summarization: given  
discourse units, what is the most coherent order-
ing of them (Marcu, 1996; Lapata, 2003; Barzilay
and Lee, 2004; Barzilay and Lapata, 2005)? Be-
cause the problem is NP-complete (Althaus et al,
2005), it is critical how coherence model evalua-
tion is intertwined with search: if the search for the
best ordering is greedy and has many errors, one
is not able to properly evaluate whether a model is
better than another. If the search is exhaustive, the
ordering procedure may take too long to be useful.
In this paper, we propose an A  search al-
gorithm for the discourse ordering problem that
comes with strong theoretical guarantees. For a
wide range of practical problems (discourse order-
ing of up to 15 units), the algorithm finds an op-
timal solution in reasonable time (on the order of
seconds). A beam search version of the algorithm
enables one to find good, approximate solutions
for very large reordering tasks. These algorithms
enable us not only to compare head-to-head, for
the first time, a set of coherence models, but also
to combine these models so as to benefit from
their complementary strengths. The model com-
803
bination is accomplished using statistically well-
founded utility training procedures which auto-
matically optimize the contributions of the indi-
vidual models on a development corpus. We em-
pirically show that utility-based models of dis-
course coherence outperform each of the individ-
ual coherence models considered.
In the following section, we describe
previously-proposed and new coherence models.
Then, we present our search algorithms and the
input representation they use. Finally, we show
evaluation results and discuss their implications.
2 Stochastic Models of Discourse
Coherence
2.1 Local Models of Discourse Coherence
Stochastic local models of coherence work under
the assumption that well-formed discourse can be
characterized in terms of specific distributions of
local recurring patterns. These distributions can be
defined at the lexical level or entity-based levels.
Word-Coocurrence Coherence Models. We
propose a new coherence model, inspired
by (Knight, 2003), that models the intuition that
the usage of certain words in a discourse unit
(sentence) tends to trigger the usage of other
words in subsequent discourse units. (A similar
intuition holds for the Machine Translation mod-
els generically known as the IBM models (Brown
et al, 1993), which assume that certain words in a
source language sentence tend to trigger the usage
of certain words in a target language translation
of that sentence.)
We train models able to recognize local recur-
ring patterns of word usage across sentences in an
unsupervised manner, by running an Expectation-
Maximization (EM) procedure over pairs of con-
secutive sentences extracted from a large collec-
tion of training documents1 . We expect EM to
detect and assign higher probabilities to recur-
ring word patterns compared to casually occurring
word patterns.
A local coherence model based on IBM Model
1 assigns the following probability to a text   con-
sisting of  sentences 	
	 :

 ffAn Empirical Study in Multilingual Natural Language 
Generation: What Should A Text Planner Do? 
Danie l  Marcu  
Information Sciences Institute and 
Department of Computer Science 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina del Rey, CA 90292 
marcu@isi, edu 
Lynn Carlson 
U.S. Department of Defense 
, _Et .  Meade,_ MD~.20755 
lmcarls@afterlife, ncsc. rail 
Maki  Watanabe 
Department of Linguistics 
........ ? , ~n i .v ,  e rs i ty .o f  Southern~California 
Los Angeles, CA 90089 
m watanab@usc, edu 
Abstract 
We present discourse annotation work aimed at con- 
structing a parallel corpus of Rhetorical Structure 
trees for a collection of Japanese texts and their cor- 
responding English translations. We discuss impli- 
cations of our empirical findings for the task of text 
planning in the context of implementing multilingual 
natural anguage generation systems. 
1 I n t roduct ion  
The natural anguage generation community has em- 
phasized for a number of years the strengths of mul- 
tilingual generation (MGEN) systems (Iordanskaja 
et al, 1992; RTsner and Stede, 1992; Reiter and Mel- 
lish, 1993; Goldberg et al, 1994; Paris et al, 1995; 
Power and Scott, 1998). These strengths concern the 
reuse of knowledge, the support for early drafts in 
several anguages, the support for maintaining con- 
sistency when making changes, the support for pro- 
ducing alternative formulations, and the potential 
for producing higher quality outputs than machine 
translation. (The weaknesses concern the high-cost 
of building large, language-independent k owledge 
bases, and the dilficulty of producing high-quality. 
broad-coverage neration algorithms.) 
From an economic perspective, the more a sys- 
tem can rely on language independent modules for 
the purpose of multilingual generatiom the better. 
If an MGEN system needs to develop language de- 
pendent knowledge bases, and language dependent 
algorithms for content selection, text planning, and 
sentence planning, it-is difficult to justify its eco- 
nomic viability. However, if most of these compo- 
nents are language independent and/or much of the 
code can be re-used, an MGEN system becomes a
viable option. 
.Many of the earl3 implementations of MGEN sys- 
tems have adopted the perspective that text plan- 
ners can be implemented as language-independent 
modules (lordanskaja el, ;11., 1992: Goldberg et el., 
1994), possibly followed by a hm:aricatwn stage, 
in which discourse l.rees are re-written to refleet~ 
language-specific constraints (R6sner and Stede. 
1992; St,ede, 1999). Although such an approach may 
17 
be adequate for highly restricted text genres, such 
as weather forecasts, it usually poses problems for 
less restricted genres. Studies of instruction man- 
uals (RTsner and Stede, 1992; Delin et al, 1994: 
Delin et al, 1996) suggest hat there are variations 
with respect to the way high-level communicative 
goals  are realized across languages. For example, 
Delin et al (1994) noticed that sentences (1), (2), 
and (3), which were taken from a trilingual instruc- 
tion manual for a step-aerobics machine, yield non- 
isomorphic Rhetorical Structure (Mann and Thomp- 
son, 1988) analyses in English, French, and German 
respectively (see Figure 1). 
English: \[The stepping load can be altered I \] 
\[by loosening the locking lever 2\] \[and changing 
the position of the cylinder foota\]. 
(1) 
French: \[Pour modifier la charge d'appui, l\] 
\[desserrer l s levieres 2\] \[puis d6placer le pied 
des v6rins a\] (\[To modify the load stepping ~\] 
\[loosen the levers 2\] \[then change .the foot of 
the cylinder foot.el) 
(2) 
German: \[,Nach Lockern der Klelnmhebel 2\] (3) 
\[kann t \] \[durch Verschieben des Zylinderfudes 3 \]
\[die Tretbelastung verS.ndert werden.~ \] (\[After 
loosening of the levers 2\] \[can'\] \[by puslfing of 
the cylinder foot 3\] \[the load changed be, ~\]) 
Hmvever, previous.discourse ,studies do .not es- 
timate how ubiquitous uch non-isomorphic analy- 
ses are. Are the examples above an exception or 
the norm? Are non-isomorphic analyses specific 
to discourse structures built, across elementary dis- 
course units of single sentences, or do they also 
occur across sentences and paragraphs? If non- 
isomorphism is ubiquitous, how should an MGEN 
system be designed in order to effectively deal wit h 
non-isomorphic discourse structures when mapping 
knowledge bases into multiple languages? 
In this paper, we describe an experiment that was 
designed to answer these questions. To investigate 
English French German 
Circumstance 
_ .. - -.' ! : ,  !'{Modifier)- ~ ~ r - '  O-.?ckem ) 
2 3 2 3 3 1 
Loosen Change Loosen Change Change Alter 
(Desserrer) (Deplacer) (Verschieben) (Verandert) 
Figure 1: Contrasting multi l ingual discourse structure representations (Delin et al, 1994, p. 63) 
how discourse structures differ across languages, we 
manual ly built a parallel corpus of discourse trees of 
newspaper Japanese texts and their corresponding 
English translations. In section 2, we present some 
of the problems pecific to the construction of such 
a corpus. In section 3, we present our experiment 
and discuss our empirical findings. In Section 4, we 
discuss the implications of our work for the task of 
text planning, in the context of multi l ingual natural 
language generation. 
2 Towards bu i ld ing a paral le l  corpus 
of d iscourse trees: an example  
Consider, for example, Japanese sentence (4), a 
word-by-word "gloss" of it (5), and a two-sentence 
translation of it that was produced by a professional 
translator (6). 
~/rf~ ~ 4\] \ [~afL~, t~,  5\] \ [~< g-~.~oq~t-? 
{4) 
Thompson,  1988). If we analyze the text frag- 
ments closely, we will notice that in translat ing sen- 
tence (4), a professional translator chose to realize 
the information in Japanese unit 2 first (unit 2 in 
text (4) corresponds roughly to unit I in text (6)); 
to realize then some of the information in Japanese 
unit 1 (part of unit 1 in text (4) corresponds to unit 
2 in text (6)); to fuse then information given in units 
1, 3, and 5 in text (4) and realize it in English as 
unit 3; and so on. Also, the translator chose to re- 
package the information in the original Japanese sen- 
tence into two English sentences. 
At the elementary unit level, the correspondence 
between Japanese sentence (4) and its English trans- 
lation (6) can be represented as in (7), where j C e 
denotes the fact that the semantic content of unit 
j is realized fully in unit e; j D e denotes the fact 
that the semantic content of unit e is realized fully 
in unit. j; j = e denotes the fact that units j and e 
are semantical ly equivalent; and j ~ e denotes the 
fact that  there is a semantic overlap between units j 
and e, but neither proper inclusion nor proper equiv- 
alence. 
\[The Ministry of Health and Welfare last year 
revealed 1\] \[population of future estimate ac- 
cording to 2\] \[m future 1.499 persons as the 
lowest a\] \[that after *SAB* rising to turn that 4\] 
\[*they* estimated but 5\] \[already the estimate 
misses a point G\] \[prediction became, r\] 
\[In its future population estimateQ\] \[made 
public last yearf\] \[the Ministry of Health and 
Welfare predicted that the SAB would drop to 
a new low of 1.-199 in the future, a\] \[but would 
make a comeback after that .4\] \[increasing once 
again/;\] \[However. it looks as if that prediction 
will be quickly shattered, t; \] 
(.~) 
(6) 
The labeled spans o f  lext represent elementary 
discourse units (~dus). i.e.. minimal text spans that 
have an unambiguous discourse function (Mann and 
18 
J l  
.12 
33 
../4 
3s 
./6 
Jr 
e2; j l  ~ ca :  
----eel; 
C e3; 
e4; j4  ~ e5 ;  
e3; 
C e6; 
C e6 
(7) 
Hence, tile mappings in (7) provide an explicit, rep- 
resentation of the way information is re-ordered and 
re-packaged when translated from Japanese into En- 
glish. However, when translating text, it is not only 
that information is re-packaged and re-ordered; it 
is also that the rhetorical rendering changes. What  
is realized in Japanese using an ELABORATION rela- 
tion can be realized in English using, for example, a 
CON' I 'RAST  or  a CONCESSION relation. 
Figure 2 presents in the style of Mann and Thomp-  
C oncess~or ._~ " 
i _ . .  , . .  ? 
attriiout+or, a:tr~bmi:~n e aioorahor - oaject- ~.It,lolute- e
I 
e alaorzm~.-mje+t e : : .  . . . . .  " . . . . . . . .  .temporal- a l te r . . .+ .  +,~P~a:+m~+ ~~.+.~i=m +~+ +-++++ia ; i+- '> +": 
:1 ) (Z)  (~ (a)  " + . . . . . . . . . . . .  +~ . . . . . . . . . . . .  + . . . .  S + ' . . . . .  + + +" " 
b.?  (The Ministry ~ '(popu alien - X -E~.  'tfutur @g (that - a~er 
of Health and of future estimate e 1.499 persons \[SAB I rLqng - to 
Weffare last yeal - a:ccrcling - to) as the lowest) turn lhal) 
revealed) 
t - .=  
I-2 3-3 
e l a b o r ~ r  |buto--? 
I11 12) (3) ~.3 
In Its fulure made pobll? the Mlnl~ry el:l ~ ~t~,n_.~ d.d ~ i on.~ i 
I~Jl~liOn ~"  year, ~ Heolth and 
LIv.~r "~,.. 
estimates WeHare . (4) (5) but would increasing 
the SAB rmk~a c~nce again. 
comet~sc~r 
wo~rld rop to after that. 
a ni~a. low o~ 
1,4~0 In the 
future, 
(~ 
,,,~ie+gai 
Figure 2: The discourse structures of texts (4) and (6). 
son (1988) the discourse structures of text frag- 
ments (4) and (6). Each discourse structure is a 
tree whose leaves correspond to the edus and whose 
internal nodes correspond t.o contiguous text spans. 
Each node is characterized by a status (NUCLEUS or 
SATELLITE) and a rhetorical relation, which is a re- 
lation that holds between two non-overlapping text 
spans. (There are a few exceptions to this rule: some 
relations, such as the CONTRAST relation that holds 
between unit \[3\] and span \[4,.5\] in the structure of the 
English text are multinuclear.) The distinction be- 
tween nuclei and satellites comes from the empirical 
observation that the nucleus expresses what is more 
essential to the writer's intention than the satellite: 
and that the nucleus of a rhetorical relation is com- 
prehensible independent of the satellite, but not vice 
versa. Rhetorical relations'that end.in the.suffix :'-e". 
denote relations that correspond to embedded syn- 
tactic constituents. For example, the ELABORATION- 
OBJEC'T-ATTRIBUTE-E relation that holds between 
units 9. and 1 in the English discourse structure cor- 
responds to a restrictive relative. We chose to label 
these relations because we have noticed that they 
often dominate complex discourse trees, whose ele- 
nlentary units are fully fleshed clauses. 
If one knows the mappings at the ed~l level, 
one can determine the mappings at. the span (dis- 
course constituent) level as well. For example, us- 
19 
ing the elementary mappings in (7), one can deter- 
mine that Japanese span \[1,2\] corresponds to English 
span \[1,2\], Japanese unit \[4\] to English span \[4,5\]. 
,Japanese span \[6,7\] to English unit \[6\], Japanese 
span \[1,5\] to English span \[1,5\], and so on. As Fig- 
ure 2 shows, the CONCESSION relation that holds be- 
tween spans \[1,5\] and \[6,7\] in the Japanese tree corre- 
sponds to a similar relation that holds between span 
\[I.5\] and unit \[6\] in the English tree (modulo the fact 
that, in Japanese, the relation holds between sen- 
tence fragments, while in English it holds between 
fldl sentences). However, the TEMPORAL-AFTER re- 
lation that holds between units \[3\] and \[4\] in the 
Japanese tree is realized as a CONTRAST relation 
between unit  \[3\] and span \[4,5\] in the English tree: 
And because Japanese units \[6\] and \[7\] are fused 
into: unit \[6\] ing:nglish, the  rel,ation .ELA-BORAT.ION- 
OBJECT-ATTRIBUTE-E iS no longer made explicit in 
the English text. 
Assume now that it is the task of an MGEN sys- 
tem to produce from a knowledge base texts (4) 
and (6). The system will have to select, the ap- 
propriate information, generate text plans for the 
two texts, generate sentence plans, and realize them. 
Should the syst.en~ generate a text plan having a 
structure similar to the PtST analysis at. the top or 
the bottom of Figure 2"? Or something in between'? 
As one can see, the discourse trees in Figure 2 are 
quite different: they suggest hat depending on the 
output language, text plans should use different re- 
lations, different orderings of elementary units, dif- 
ferent aggregations across semantic units, etc. 
Some researchers may argue that the two RST 
analyses in Figure 2 are too specific. That  they, 
figures. 
In computing Position-Dependent (P-D) recall 
and precision figures, a Japanese span was consid- 
ered to match an English span when the Japanese 
span contained all the Japanese edus that cor- 
responded to the edus in the English span, and 
in fact, correspond t0.:text.,~plaz!s .,tha-t .l!.ave bee.n 
already refined by an aggregation module and ar- 
guably, even by a sentence planner. After all, the 
re-ordering of units 1 and 2 can be explained only 
in terms of different syntactic ontraints in Japanese 
and English. We agree with such a concern. Never- 
theless, as our experiment shows, significant differ- 
ences across discourse trees are found not only for 
trees built at the sentence level, but also for trees 
built at the paragraph and text levels. For these lev- 
els, it is difficult to explain the differences in terms 
of language-specific syntactic onstraints. Rather, it 
seems more adequate to assume that there are sig- 
nificant differences with respect o the way informa- 
tion is organized rhetorically across languages. The 
experiment described in the next section estimates 
quantitatively this difference. 
3 Exper iment  
In order to assess how similar discourse structures 
are across languages, we built manually a cor- 
pus of discourse trees for 40 Japanese texts and 
their corresponding translations. The texts, se- 
lected randomly from the ARPA corpus (White 
and O'Connell, 1994), contained on average about 
460 words. We developed a discourse annota- 
tion protocol for ,Japanese and English along the 
lines followed by Marcu et al (1999). We used 
Marcu's discourse annotation tool (1999) in order 
to manually construct he discourse structure of all 
Japanese and English texts it, the corpus. 10~. of 
the Japanese and English texts were rhetorically 
labeled by two of us. The agreement was sta- 
tistically significant (Kappa = 0.65.0 > 0.01 for 
Japanese and Kappa = 0.748,0 > 0.01 for En- 
glish (Carletta, 1996; Siegel-and Castellan, 1988)). 
The tool and the annotation protocol are available 
at. http://www, isi.edt,/~r, zarcu/softwa,-e/. For each 
pair of Japanese-English discourse, structures, we 
also built, manually an alignment file, which specified 
the correspondence b tween the edus of the Japanese 
and English texts. 
Using labeled recall and precision figures, we com- 
puted the similarity between English and Japanese 
discourse trees with respect t,o their assignment of 
edu boundaries, hierarchical spans, nuclearity, and 
rhetorical relations, Because the trees we comparod 
differ from one language to the other ill the ntnnber 
of elernent ary units, the order, of these units, and the 
way the units are grouped rectirsively into discourse 
spans, we comptlted two types of recall and precision 
, :+when :~e.J~laa~tese:~-and.~En~lish::spans -ap eared-in 
the same position linearly. For example, the En- 
glish tree in Figure 2 is characterized by 10 sub- 
sentential spans, which span across positions \[1,1\], 
\[2,2\], \[3,3\], \[4,4\], \[5,5\], \[6,6\], \[1,2\], \[4,5\], \[3,5\], and 
\[1,5\]. (Span \[1,6\] subsumes 2 sentences, so it is 
not sub-sentential.) The Japanese discourse tree has 
only 4 spans that could be matched in the same po- 
sitions with English spans, namely spans \[1,2\]. \[4,4\], 
\[5,5\], and \[1,5\]. Hence the similarity between the 
Japanese tree and the English tree with respect to 
their discourse structure below the sentence level has 
a recall of 4/10 and a precision of 4 / l l  (in Figure 2, 
there are 11 sub-sentential Japanese spans). 
In computing Position-Independent (P-I) recall 
and precision figures, even when a Japanese span 
"floated" during the translation to a position in the 
English tree that was different from the position 
in the initial tree, the P-I recall and precision fig- 
ures are affected less than when computing Position- 
Dependent figures. The position-independent fig- 
ures reflect the intuition that if two trees tl and t2 
both have a subtree t, tl and 12 are more similar 
than if they were if they didn't share ally subtree. 
For instance, for the spans at the sub-sentential level 
in the trees in Figure 2 the position-independent 
recall is 6/10 and the position-independent preci- 
sion is 6/11 because in addition to spans \[1,2\], \[4,4\], 
\[5,5\], and \[1,5\], one can also match Japanese spat, 
\[1,1\] to English spa,, \[2,2\] and Japanese spa,, \[2,2\] 
to Japanese span \[1,1\]. The Position-Independent 
figures offer a more optimistic metric for comparing 
discourse trees. They span a wider range of values 
than the Position-Dependent figures, which enables 
a finer grained comparison, which in turn enables 
a better characterization of the differ.ences between 
Japanese and English discourse structures. 
In order to provide a better estimate of how close 
two  discourse trees were, we computed Position- 
Dependent and -Independent recall and precision fig- 
ures for the sentential evel (where units are given 
by edus and spans are given by sets of edus or single 
sentences); paragraph level (where units are given by 
sentences and spans are given by sets of sentences or 
single paragraphs): and text level (where units are 
given by paragraphs and spans are given by sets of 
paragraphs). These figures offer a detailed picture of 
how discourse structures and relations are mapped 
-from one languageto the other. Some of the differ- 
ences at the sentence level can be explained by differ- 
ences between the syntactic structures of Japanese 
20 
Level 
Sentence 
Paragraph 
Text 
Weighted Average 
All .- 
Sentence 
Paragraph 
Text 
Weighted Average 
All 
Units 
P-D R P-D P 
29.1 25.0 
53.9 53.4 
41.3 42.6 
36.0 32.5 
Spans 
P-D R P-D P 
27.2 22.7 
46.8 47.3 
31.5 32.6 
31.8 28.4 
? 8 .2  ..... : 7,4 .. . . .  - .5 .9  . ...... 5.3.: 
P- IR  P - IP  
71.0 61.0 
62.1 61.6 
74.1 76.5 
69.6 63.0 
74.5 66.8 
P- IR  P - IP  
56.0 46.6 
53.2 53.8 
54.4 56.5 
55.2 49.2 
50.6 45.8 
Nuclei 
P-D R P-D P 
21.3 17.7 
38.6 39.0 
28.8 29.9 
26.0 23.1 
..... -.4A ............ 3~9:_ 
P- IR  P - IP  
44.3 36.9 
43.3 43.8 
48.5 50.4 
44.8 39.9 
39.4 35.7 
Relations 
P-D R P-D P 
14.9 12.4 
31.9 32.3 
26.1 27.1 
20.1 17.9 
. . . .  ..3.3 . . . . . . .  3 .0 .  
P - IR  P - IP  
30.5 25.4 
35.1 35.5 
41.1 42.7 
33.1 29.5 
26.8 24.3 
Table 1: Similarity of the Japanese and English discourse structures 
and English. The differences at the paragraph and 
text levels have a purely rhetorical explanation. 
As expected, when one computes the recall and 
precision figures with respect to the nuclearity and 
relation assignments, one also factors in the nucle- 
arity status and the rhetorical relation that is asso- 
ciated with each span. 
Table 1 summarizes the results (P-D and P-I 
(R)ecall and (P)recision figures) for each level (Sen- 
tence, Paragraph, and Text). It presents Recall and 
Precision figures with respect to span assignment, 
nuclearity status, and rhetorical relation labeling of 
discourse spans. The numbers in the "Weighted 
Average" line report averages of the Sentence-, 
Paragraph-, and Text-specific figures, weighted ac- 
cording to the number of units at each level. The 
numbers in the "All" line reflect recall and precision 
figures computed across the entire trees, with no at- 
tention paid t.o sentence and paragraph boundaries. 
Given the significantly different syntactic struc- 
tures of Japanese and English. we were not surprised 
by tile low recall and precision results that reflect 
the similarity between discourse trees built below 
the sentence level. However, as Table 1 shows, there 
are astonishing differences between discourse trees 
at the paragraph and text. levels as well. For exam- 
pie, the Position-Independent figures show that only 
about 62% of the sentences: and only :about 53% of 
the hierarchical spans built across sentences could be 
matched between the two corpora. When one looks 
at the nuclearity status and rhetorical relations as- 
sociated with the spans built across sentences, the 
P-I recall and precision figures drop to about 43c2~ 
and :/5~ respectively. 
The differences in recall and precision are ex- 
l)lained both by differen,-es in the way information is 
packaged rote paragraphs in the-two languages arid 
the way it is structured rhetorically both within and 
above the paragraph level. 
4 How shou ld  a mul t i l i ngua l  text  
p lanner  work?  
The results in Section 3 strongly suggest hat if one 
is to build text plans in the context of a Japanese- 
English multilingual generation system, a language- 
independent text planning module whose output is 
mapped straightforwardly into sentence plans (Ior- 
danskaja et al, 1992; Goldberg et al, 1994) will not 
do. The differences between the rhetorical structures 
of Japanese and English texts are simply too big to 
support the derivation of a unique text plan, which 
would subsume both the Japanese- and English- 
specific realizations. If we are to build MGEN sys- 
tems capable of generating rich texts in languages 
as distant as English and Japanese, we would need 
to use more sophisticated techniques. In the rest of 
this section, we discuss a set of possible approaches, 
which are consistent with work that has been carried 
out to date in the NLG field. 
4.1 Use text  p lan representat ions  that  are 
more  abst rac t  than  d iscourse trees 
Delin et al (1994) have shown that although tile 
rhetorical renderings in Figure 1 are non-isomorphic. 
dmy are alt subsumed by one .commol~, more.ab- 
stract t.ext.-plan representation language that for- 
realizes the procedural relations of Generation and 
Enablement (Goldman, 1970). One caa~ conceive of. 
text plans being represented as sequences of actions 
or hierarchies of actions and goals over which one can 
identify Generation and Enablement relations that 
hold between them. In such a framework, text plan- 
ning is carried out ill a language-independent man- 
ner. which is then followed by a rhetorical "'fleshing 
out". (Delin et al (1994) have shown how Gener- 
ation and Enablenlent relations are realized rhetor- 
ically in various languages using relations such as 
PURPOSE, 'SEQUENCE, CONDITION, and MEANS.) 
Bateman and Rondhuis (1997) suggest that the 
variability present in Delin et al's Rhetorical Struc- 
21 
ture analyses in Figure 1 can be explained by the 
inadequate mixture of intentional and semantic re- 
lations, at different levels of granularity. They pro- 
pose that discourse phenomena should be accounted 
for at a more abstract level than RST relations 
and they present a classification system in terms 
discourse-tree r writing module capable of rewriting 
P-specific discourse structures into O-specific dis- 
course structures. When generating texts in lan- 
guage P, the MGEN system works as a monolin- 
gum generator. When generating texts in language 
O, the MGEN system generates a text plan in lan- 
of "stratification", ..'!me?afunction?., ,,and .::p~radig,: ........ guage.-~, xnapsitdr~to.=taaag,uageO.,~ anti then ~proceeds - .. 
matic/syntagmatic axiality" that enables one to rep- 
resent discourse structures at multiple levels of ab- 
straction. 
Adopting such an approach could be an extremely 
rewarding enterprise. Unfortunately, the research 
of Delin et al (1994) and Bateman and Rond- 
huis (1997) cannot be applied yet to unrestricted o- 
mains. Generation and Enablement are only two of 
the abstract relations that can hold between actions 
and goals. And some texts, such as descriptions, are 
difficult to characterize only in terms of actions and 
goals. Building a "complete" taxonomy of such ab- 
stract relations and identifying adequate mappings 
between there relations and rhetorical relations are 
still open problems. 
4.2 Derive a language- independent 
discourse structure, and then l inearize 
it 
RSsner and Stede (1992) and Stede (1999) assume 
that a discourse representation g la Mann and 
Thompson imposes no contraints on the linear order 
of the leaves. For tile purpose of multilingual text 
planning, one can, hence, assume that a language- 
independent text planner derives first a language- 
independent rhetorical structure and then linearizes 
it, i.e., transforms it to make it language specific. 
The transformations that RSsner and Stede have ap- 
plied concern primarily re-orderings of the children 
of some nodes and re-assignment of rhetorical rela- 
tion labels. But given, for example, tile significant 
differences between the discourse structures in Fig- 
ure 2, it is difficult to envision what the language- 
independent text plan might look like. It is deft- 
nitely possible to conceive of such a text plan rep- 
resentation. However, the linearization module will 
need then to be much more sophisticated: it will 
need to be able to rewrite full structures, re-order 
constituents, aggregate_across possibly non-adjacent 
units, etc. 
4.3 hnp lement  a text  p lann ing  a lgor i thm 
for one language only. For all o ther  
languages,  dev ise d i scourse - t ree  
rewr i t ing  modu les  
In this approach, the system developer assigns a pre- 
ferrential status to one of the languages that are 
to be handled I) 3 ' the MGEN system. Lot's call 
this language P. The system developer implenlents 
text planning algorithms only for this language. For 
any other language O, the developer itnplements a 
22 
further with the sentence planning and realization 
stages. Marcu et al (2000) present and evaluate a 
discourse-tree r writing algorithm that exploits ma- 
chine learning methods in order to map Japanese 
discourse trees into discourse trees that resemble 
English-specific renderings. 
The advantage of such an approach is that the 
tree-rewriting modules can be also used in the con- 
text of machine translation systems in order to re- 
package and re-organize the input text rhetorically, 
to reflect constraints pecific to the target language. 
The disadvantage is that, from an NLG perspective, 
there is no guarantee that such a system could pro- 
duce better results than a system that implements 
language-dependent text planning modules. 
4.4 Derive language-dependent  text plans 
Another viable approach is to acknowledge that 
text plans vary significantly across languages and, 
therefore, should be derived by language-dependent 
planners. To this end, one could use both top- 
down (How, 1993; Moore and Paris, 1993) and 
bottom-up (Marcu, t997; Mellish et al, 1998) text 
planning algorithms. The advantage of this ap- 
proach is that it has the potential of producing trees 
that reflect tile peculiarities specific to any language. 
The disadvantage is that only the text planning al- 
gorithms are general: the plan operators and the 
rhetorical relations they operate with are language- 
dependent, and hence, more expensive to develop 
and maintain. 
4.5 Discuss ion 
Depending oil tile languages and text genres it op- 
erates with, all MGEN system may get away with 
a language-independent text planner. However, 
for sophisticated genres and distant languages, im- 
plementing a language-independent planner that is 
straightforwardly'mapped i:nto sentence, plans does 
not appear to be a felicitous solution. We enu- 
merated four possible alternatives for addressing the 
text planning problem in an MGEN system. Each 
of tile approaches has its own pluses and minuses. 
Which will eventually win in large-scale deployable 
MGEN systems remains an open question. 
Re ferences  
John A. Bateman and Klaas Jan Flondliuis. 1997. 
Coherence relations: Towards a general specifica- 
tion. Dtsco~u'se Processes, 24:3-49. 
Jean Carletta. 1996. Assessing agreement on clas- 
sification tasks: The kappa statistic. Computa- 
tional Linguistics, 22(2):249-254, June. 
Judy L. Delin, Anthony Hartley, C@ile L. Paris, Do- 
nia R. Scott, and Keith Vander Linden. 1994. Ex- 
pressing procedural relationships in multilingual 
-. instructions. In -P. roeeedirtgs.,of 4he .,geventh:-,tnter~ 
national Workshop on Natural Language Genera- 
tion, pages 61-70, Kennebunkport, Maine, June. 
J. Delin, D. Scott, and A. Hartley. 1996. Prag- 
matic congruence through language-specific map- 
pings from semantics to syntax. Technical report, 
ITRI Research Report ITRI-96-12, University of 
Brighton. 
E. Goldberg, N. Driedger, and R. Kittredge. 1994. 
Using natural-language processing to produce 
weather forecasts. IEEE Expert, 9(2):45-53. 
A.I. Goldman. 1970. A Theory of Human Action. 
Prentice Hall, Englewood Cliffs, NJ. 
Eduard H. Hovy. 1993. Automated iscourse gen- 
eration using discourse structure relations. Artifi- 
cial Intelligence, 63(1-2):341-386, October. 
L. Iordanskaja, M. Kim, R. Kittredge, B. Lavoie, 
and A. Polguere. 1992. Generation of extended 
bilingual statistical reports. In Proceedings of 
the 14th International Conference on Compu- 
tational Linguistics (COLING'92), pages 1019- 
1023, Nantes, France. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional 
theory of text organization. Text, 8(3):243-281. 
Daniel Marcu. 1997. From local to global coherence: 
A bottom-up approach to text planning. In Pro- 
ceedings of the Fourteenth National Conference on 
Artificial Intelligence (AAAI-97), pages 629-635, 
Providence, Rhode Island, July 28-31. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments in constructing a
corpus of discourse trees, tn Proceedings of the 
ACL'99 Workshop on Standards and Tools for 
Discourse Tagging, pages 48-.57, University of 
Maryland. June 22. 
Daniel Marcu, Lynn Carlson, and Maki Watan- 
abe. 2000. The automatic translation of discourse 
structures. In Proceedings of the First Annual 
Meeting of the A'orth American Chapter of the As- 
sociation for Computational Linguistics NAACL- 
2000, Seattle, Washington. April 29 - May 3. 
Chris Mellish. Alistair Knott. Jon Oberlander, 
and Mick O'Donnell. 1998. Experiments using 
stochastic search for text planning. In Proceed- 
}ngs of lbc 9lh International H'oHcstwp on .Vatvral 
Language. (;era'cation. pages 98 107. Niagara-on- 
the-Lake, Canada. August 5-7. 
,lohanna D. Moore and Cdcile L. Paris. 1993. Plan- 
ning text \['or advisory dialogues: Capturing inten- 
23 
tional and rhetorical information. Computational _ 
Linguistics, 19(4):651-694. 
C. Paris, K. Vander Linden, M. Fischer, A. Hart- 
Icy, L. Pemberton, R. Power, and D. Scott. 1995. 
A support tool for writing multilingual instruc- 
tions. In Proceedings. of the 14th International 
Joint. ,Gonfer~nee. ~on.:Artificial ~tnt~ttigenee (IJ- 
CA I'95), pages 1398-1404, Montreal, Canada. 
Richar Power and Donia Scott. 1998. Multilingual 
authoring using feedback texts. In Proceedings of 
the 36th Annual Meeting of the Association for 
Computational Linguistics (ACL'98), Montreal, 
Canada, August. 
Ehud Reiter and Chris Mellish. 1993. Optimizing 
the costs and benefits of natural anguage gen- 
eration. In Proceedings of the 131h International 
Joint Conference on Artificial Intelligence, pages 
1164-1169. 
Dietmar R6sner and Manfred Stede. 1992. Cus- 
tomizing RST for the automatic production 
of technical manuals. In R. Dale, E. How, 
D. R6sner, and O. Stock, editors, Aspects of Au- 
tomated Natural Language Generation; 6th Inter- 
national Workshop on Natural Language Gener- 
ation, number 587 in Lecture Notes in Artificial 
Intelligence, pages 199-214, Trento, Italy, April. 
Springer-Verlag. 
Sidney Siegel and N.J. Castellan. 1988. Non- 
parametric Statistics for the Behavioral Sciences. 
McGraw-Hill, second edition. 
Manfred Stede. 1999. Rhetorical structure and the- 
matic structure in text generation. In Working 
Notes of the Workshop on Levels of Represen- 
tation m Discourse, pages 117-123, Edinburgh, 
Scotland, July 7-9. 
J. White and T. O'Connell. 1994. Evalua- 
tion in the ARPA machine-translation pro- 
gram: 1993 methodology. In Proceedings of 
the .4RPA Human language Technology Work- 
shop, pages 135-140, Washington, D.C. See also 
h ttp :/ /  ursula, geowetown, edu/. 
Building a Discourse-Tagged Corpus in the Framework of
Rhetorical Structure Theory
Lynn Carlson
Department of Defense
Ft. George G. Meade
MD 20755
lmcarlnord@aol.com
Daniel Marcu
Information Sciences Institute
University of S. California
Marina del Rey, CA 90292
marcu@isi.edu
Mary Ellen Okurowski
Department of Defense
Ft. George G. Meade
MD 20755
meokuro@romulus.ncsc.mil
Abstract
We describe our experience in
developing a discourse-annotated
corpus for community-wide use.
Working in the framework of
Rhetorical Structure Theory, we were
able to create a large annotated
resource with very high consistency,
using a well-defined methodology and
protocol. This resource is made
publicly available through the
Linguistic Data Consortium to enable
researchers to develop empirically
grounded, discourse-specific
applications.
1 Introduction
The advent of large-scale collections of
annotated data has marked a paradigm shift in
the research community for natural language
processing. These corpora, now also common in
many languages, have accelerated development
efforts and energized the community.
Annotation ranges from broad characterization
of document-level information, such as topic or
relevance judgments (Voorhees and Harman,
1999; Wayne, 2000) to discrete analysis of a
wide range of linguistic phenomena. However,
rich theoretical approaches to discourse/text
analysis (Van Dijk and Kintsch, 1983; Meyer,
1985; Grosz and Sidner, 1986; Mann and
Thompson, 1988) have yet to be applied on a
large scale. So far, the annotation of discourse
structure of documents has been applied
primarily to identifying topical segments
(Hearst, 1997), inter-sentential relations
(Nomoto and Matsumoto, 1999; Ts?ou et al,
2000), and hierarchical analyses of small
corpora (Moser and Moore, 1995; Marcu et al,
1999).
In this paper, we recount our experience in
developing a large resource with discourse-level
annotation for NLP research. Our main goal in
undertaking this effort was to create a reference
corpus for community-wide use. Two essential
considerations from the outset were that the
corpus needed to be consistently annotated, and
that it would be made publicly available through
the Linguistic Data Consortium for a nominal
fee to cover distribution costs. The paper
describes the challenges we faced in building a
corpus of this level of complexity and scope ?
including selection of theoretical approach,
annotation methodology, training, and quality
assurance. The resulting corpus contains 385
documents of American English selected from
the Penn Treebank (Marcus et al, 1993),
annotated in the framework of Rhetorical
Structure Theory. We believe this resource
holds great promise as a rich new source of text-
level information to support multiple lines of
research for language understanding
applications.
2 Framework
Two principle goals underpin the creation of this
discourse-tagged corpus: 1) The corpus should
be grounded in a particular theoretical approach,
and 2) it should be sufficiently large enough to
offer potential for wide-scale use ? including
linguistic analysis, training of statistical models
of discourse, and other computational linguistic
applications. These goals necessitated a number
of constraints to our approach. The theoretical
framework had to be practical and repeatable
over a large set of documents in a reasonable
amount of time, with a significant level of
consistency across annotators. Thus, our
approach contributes to the community quite
differently from detailed analyses of specific
discourse phenomena in depth, such as
anaphoric relations (Garside et al, 1997) or
style types (Leech et al, 1997); analysis of a
single text from multiple perspectives (Mann
and Thompson, 1992); or illustrations of a
theoretical model on a single representative text
(Britton and Black, 1985; Van Dijk and Kintsch,
1983).
Our annotation work is grounded in the
Rhetorical Structure Theory (RST) framework
(Mann and Thompson, 1988). We decided to
use RST for three reasons:
? It is a framework that yields rich annotations
that uniformly capture intentional, semantic,
and textual features that are specific to a
given text.
? Previous research on annotating texts with
rhetorical structure trees (Marcu et al,
1999) has shown that texts can be annotated
by multiple judges at relatively high levels
of agreement. We aimed to produce
annotation protocols that would yield even
higher agreement figures.
? Previous research has shown that RST trees
can play a crucial role in building natural
language generation systems (Hovy, 1993;
Moore and Paris, 1993; Moore, 1995) and
text summarization systems (Marcu, 2000);
can be used to increase the naturalness of
machine translation outputs (Marcu et al
2000); and can be used to build essay-
scoring systems that provide students with
discourse-based feedback (Burstein et al,
2001). We suspect that RST trees can be
exploited successfully in the context of
other applications as well.
In the RST framework, the discourse
structure of a text can be represented as a tree
defined in terms of four aspects:
? The leaves of the tree correspond to text
fragments that represent the minimal units
of the discourse, called elementary
discourse units
? The internal nodes of the tree correspond to
contiguous text spans
? Each node is characterized by its nuclearity
? a nucleus indicates a more essential unit of
information, while a satellite indicates a
supporting or background unit of
information.
? Each node is characterized by a rhetorical
relation that holds between two or more
non-overlapping, adjacent text spans.
Relations can be of intentional, semantic, or
textual nature.
Below, we describe the protocol that we used
to build consistent RST annotations.
2.1 Segmenting Texts into Units
The first step in characterizing the discourse
structure of a text in our protocol is to determine
the elementary discourse units (EDUs), which
are the minimal building blocks of a discourse
tree. Mann and Thompson (1988, p. 244) state
that ?RST provides a general way to describe
the relations among clauses in a text, whether or
not they are grammatically or lexically
signalled.? Yet, applying this intuitive notion to
the task of producing a large, consistently
annotated corpus is extremely difficult, because
the boundary between discourse and syntax can
be very blurry. The examples below, which
range from two distinct sentences to a single
clause, all convey essentially the same meaning,
packaged in different ways:
1. [Xerox Corp.?s third-quarter net income
grew 6.2% on 7.3% higher revenue.] [This
earned mixed reviews from Wall Street
analysts.]
2. [Xerox Corp?s third-quarter net income
grew 6.2% on 7.3% higher revenue,] [which
earned mixed reviews from Wall Street
analysts.]
3. [Xerox Corp?s third-quarter net income
grew 6.2% on 7.3% higher revenue,]
[earning mixed reviews from Wall Street
analysts.]
4. [The 6.2% growth of Xerox Corp.?s third-
quarter net income on 7.3% higher revenue
earned mixed reviews from Wall Street
analysts.]
In Example 1, there is a consequential
relation between the first and second sentences.
Ideally, we would like to capture that kind of
rhetorical information regardless of the syntactic
form in which it is conveyed. However, as
examples 2-4 illustrate, separating rhetorical
from syntactic analysis is not always easy. It is
inevitable that any decision on how to bracket
elementary discourse units necessarily involves
some compromises.
Reseachers in the field have proposed a
number of competing hypotheses about what
constitutes an elementary discourse unit. While
some take the elementary units to be clauses
(Grimes, 1975; Givon, 1983; Longacre, 1983),
others take them to be prosodic units
(Hirschberg and Litman, 1993), turns of talk
(Sacks, 1974), sentences (Polanyi, 1988),
intentionally defined discourse segments (Grosz
and Sidner, 1986), or the ?contextually indexed
representation of information conveyed by a
semiotic gesture, asserting a single state of
affairs or partial state of affairs in a discourse
world,? (Polanyi, 1996, p.5). Regardless of their
theoretical stance, all agree that the elementary
discourse units are non-overlapping spans of
text.
Our goal was to find a balance between
granularity of tagging and ability to identify
units consistently on a large scale. In the end,
we chose the clause as the elementary unit of
discourse, using lexical and syntactic clues to
help determine boundaries:
5. [Although Mr. Freeman is retiring,] [he will
continue to work as a consultant for
American Express on a project basis.]wsj_1317
6. [Bond Corp., a brewing, property, media
and resources company, is selling many of
its assets] [to reduce its debts.]wsj_0630
However, clauses that are subjects, objects,
or complements of a main verb are not treated as
EDUs:
7. [Making computers smaller often means
sacrificing memory.]wsj_2387
8. [Insurers could see claims totaling nearly
$1 billion from the San Francisco
earthquake.]wsj_0675
Relative clauses, nominal postmodifiers, or
clauses that break up other legitimate EDUs, are
treated as embedded discourse units:
9. [The results underscore Sears?s difficulties]
[in implementing the ?everyday low
pricing? strategy?]wsj_1105
10. [The Bush Administration,] [trying to blunt
growing demands from Western Europe for
a relaxation of controls on exports to the
Soviet bloc,] [is questioning?]wsj_2326
Finally, a small number of phrasal EDUs are
allowed, provided that the phrase begins with a
strong discourse marker, such as because, in
spite of, as a result of, according to. We opted
for consistency in segmenting, sacrificing some
potentially discourse-relevant phrases in the
process.
2.2 Building up the Discourse Structure
Once the elementary units of discourse have
been determined, adjacent spans are linked
together via rhetorical relations creating a
hierarchical structure. Relations may be
mononuclear or multinuclear. Mononuclear
relations hold between two spans and reflect the
situation in which one span, the nucleus, is more
salient to the discourse structure, while the other
span, the satellite, represents supporting
information. Multinuclear relations hold among
two or more spans of equal weight in the
discourse structure. A total of 53 mononuclear
and 25 multinuclear relations were used for the
tagging of the RST Corpus. The final inventory
of rhetorical relations is data driven, and is
based on extensive analysis of the corpus.
Although this inventory is highly detailed,
annotators strongly preferred keeping a higher
level of granularity in the selections available to
them during the tagging process. More extensive
analysis of the final tagged corpus will
demonstrate the extent to which individual
relations that are similar in semantic content
were distinguished consistently during the
tagging process.
The 78 relations used in annotating the
corpus can be partitioned into 16 classes that
share some type of rhetorical meaning:
Attribution, Background, Cause, Comparison,
Condition, Contrast, Elaboration, Enablement,
Evaluation, Explanation, Joint, Manner-Means,
Topic-Comment, Summary, Temporal, Topic-
Change. For example, the class Explanation
includes the relations evidence, explanation-
argumentative, and reason, while Topic-
Comment includes problem-solution, question-
answer, statement-response, topic-comment, and
comment-topic. In addition, three relations are
used to impose structure on the tree: textual-
organization, span, and same-unit  (used to link
parts of units separated by embedded units or
spans).
3 Discourse Annotation Task
Our methodology for annotating the RST
Corpus builds on prior corpus work in the
Rhetorical Structure Theory framework by
Marcu et al (1999). Because the goal of this
effort was to build a high-quality, consistently
annotated reference corpus, the task required
that we employ people as annotators whose
primary professional experience was in the area
of language analysis and reporting, provide
extensive annotator training, and specify a
rigorous set of annotation guidelines.
3.1 Annotator Profile and Training
The annotators hired to build the corpus were all
professional language analysts with prior
experience in other types of data annotation.
They underwent extensive hands-on training,
which took place roughly in three phases.
During the orientation phase, the annotators
were introduced to the principles of Rhetorical
Structure Theory and the discourse-tagging tool
used for the project (Marcu et  al., 1999). The
tool enables an annotator to segment a text into
units, and then build up a hierarchical structure
of the discourse. In this stage of the training, the
focus was on segmenting hard copy texts into
EDUs, and learning the mechanics of the tool.
In the second phase, annotators began to
explore interpretations of discourse structure, by
independently tagging a short document, based
on an initial set of tagging guidelines, and then
meeting as a group to compare results. The
initial focus was on resolving segmentation
differences, but over time this shifted to
addressing issues of relations and nuclearity.
These exploratory sessions led to enhancements
in the tagging guidelines. To reinforce new
rules, annotators re-tagged the document.
During this process, we regularly tracked inter-
annotator agreement (see Section 4.2). In the
final phase, the annotation team concentrated on
ways to reduce differences by adopting some
heuristics for handling higher levels of the
discourse structure. Wiebe et al (1999) present
a method for automatically formulating a single
best tag when multiple judges disagree on
selecting between binary features. Because our
annotators had to select among multiple choices
at each stage of the discourse annotation
process, and because decisions made at one
stage influenced the decisions made during
subsequent stages, we could not apply Wiebe et
al.?s method.  Our methodology for determining
the ?best? guidelines was much more of a
consensus-building process, taking into
consideration multiple factors at each step. The
final tagging manual, over 80 pages in length,
contains extensive examples from the corpus to
illustrate text segmentation, nuclearity, selection
of relations, and discourse cues. The manual can
be downloaded from the following web site:
http://www.isi.edu/~marcu/discourse.
The actual tagging of the corpus progressed
in three developmental phases. During the initial
phase of about four months, the team created a
preliminary corpus of 100 tagged documents.
This was followed by a one-month reassessment
phase, during which we measured consistency
across the group on a select set of documents,
and refined the annotation rules. At this point,
we decided to proceed by pre-segmenting all of
the texts on hard copy, to ensure a higher overall
quality to the final corpus. Each text was pre-
segmented by two annotators; discrepancies
were resolved by the author of the tagging
guidelines. In the final phase (about six months)
all 100 documents were re-tagged with the new
approach and guidelines. The remainder of the
corpus was tagged in this manner.
3.2 Tagging Strategies
Annotators developed different strategies for
analyzing a document and building up the
corresponding discourse tree. There were two
basic orientations for document analysis ? hard
copy or graphical visualization with the tool.
Hard copy analysis ranged from jotting of notes
in the margins to marking up the document into
discourse segments. Those who preferred a
graphical orientation performed their analysis
simultaneously with building the discourse
structure, and were more likely to build the
discourse tree in chunks, rather than
incrementally.
We observed a variety of annotation styles
for the actual building of a discourse tree. Two
of the more representative styles are illustrated
below.
1. The annotator segments the text one unit at
a time, then incrementally builds up the
(26)
*elaboration-object-attribute-embedded
+attribution-embedded
(17) (18) (21)19-20 22-23 24-25
17-21 22-26explanation-argumentative consequence-s
same-unit
(25)
purpose
(24)
17-26 elaboration-additional(16)
example
+
(22) (23)
17-18
attribution 19-21 attribution
(19) (20)
*
Figure 1: Discourse sub-tree for multiple sentences
discourse tree by immediately attaching the
current node to a previous node. When
building the tree in this fashion, the
annotator must anticipate the upcoming
discourse structure, possibly for a large
span. Yet, often an appropriate choice of
relation for an unseen segment may not be
obvious from the current (rightmost) unit
that needs to be attached. That is why
annotators typically used this approach on
short documents, but resorted to other
strategies for longer documents.
2. The annotator segments multiple units at a
time, then builds discourse sub-trees for
each sentence. Adjacent sentences are then
linked, and larger sub-trees begin to
emerge. The final tree is produced by
linking major chunks of the discourse
structure. This strategy allows the annotator
to see the emerging discourse structure more
globally; thus, it was the preferred approach
for longer documents.
Consider the text fragment below, consisting
of four sentences, and 11 EDUs:
[Still, analysts don?t expect the buy-back to
significantly affect per-share earnings in the
short term.]16 [The impact won?t be that great,]17
[said Graeme Lidgerwood of First Boston
Corp.]18 [This is in part because of the effect]19
[of having to average the number of shares
outstanding,]20 [she said.]21 [In addition,]22 [Mrs.
Lidgerwood said,]23 [Norfolk is likely to draw
down its cash initially]24 [to finance the
purchases]25 [and thus forfeit some interest
income.]26 wsj_1111
The discourse sub-tree for this text fragment
is given in Figure 1. Using Style 1 the annotator,
upon segmenting unit [17], must anticipate the
upcoming example relation, which spans units
[17-26]. However, even if the annotator selects
an incorrect relation at that point, the tool allows
great flexibility in changing the structure of the
tree later on.
Using Style 2, the annotator segments each
sentence, and builds up corresponding sub-trees
for spans [16], [17-18], [19-21] and [22-26]. The
second and third sub-trees are then linked via an
explanation-argumentative relation, after which,
the fourth sub-tree is linked via an elaboration-
additional relation. The resulting span [17-26] is
finally attached to node [16] as an example
satellite.
4 Quality Assurance
A number of steps were taken to ensure the
quality of the final discourse corpus. These
involved two types of tasks: checking the
validity of the trees and tracking inter-annotator
consistency.
4.1 Tree Validation Procedures
Annotators reviewed each tree for syntactic and
semantic validity. Syntactic checking involved
ensuring that the tree had a single root node and
comparing the tree to the document to check for
missing sentences or fragments from the end of
the text. Semantic checking involved reviewing
nuclearity assignments, as well as choice of
relation and level of attachment in the tree.  All
trees were checked with a discourse parser and
tree traversal program which often identified
errors undetected by the manual validation
process. In the end, all of the trees worked
successfully with these programs.
4.2 Measuring Consistency
We tracked inter-annotator agreement during
each phase of the project, using a method
developed by Marcu et al (1999) for computing
kappa statistics over hierarchical structures. The
kappa coefficient (Siegel and Castellan, 1988)
has been used extensively in previous empirical
studies of discourse (Carletta et al, 1997;
Flammia and Zue, 1995; Passonneau and
Litman, 1997). It measures pairwise agreement
among a set of coders who make category
judgments, correcting for chance expected
agreement. The method described in Marcu et
al. (1999) maps hierarchical structures into sets
of units that are labeled with categorial
judgments. The strengths and shortcomings of
the approach are also discussed in detail there.
Researchers in content analysis (Krippendorff,
1980) suggest that values of kappa > 0.8 reflect
very high agreement, while values between 0.6
and 0.8 reflect good agreement.
Table 1 shows average kappa statistics
reflecting the agreement of three annotators at
various stages of the tasks on selected
documents. Different sets of documents were
chosen for each stage, with no overlap in
documents. The statistics measure annotation
reliability at four levels: elementary discourse
units, hierarchical spans, hierarchical nuclearity
and hierarchical relation assignments.
At the unit level, the initial (April 00) scores
and final (January 01) scores represent
agreement on blind segmentation, and are
shown in boldface. The interim June and
November scores represent agreement on hard
copy pre-segmented texts. Notice that even with
pre-segmenting, the agreement on units is not
100% perfect, because of human errors that
occur in segmenting with the tool. As Table 1
shows, all levels demonstrate a marked
improvement from April to November (when
the final corpus was completed), ranging from
about 0.77 to 0.92 at the span level, from 0.70 to
0.88 at the nuclearity level, and from 0.60 to
0.79 at the relation level. In particular, when
relations are combined into the 16 rhetorically-
related classes discussed in Section 2.2, the
November results of the annotation process are
extremely good. The Fewer-Relations column
shows the improvement in scores on assigning
Table 1: Inter-annotator agreement ? periodic results for three taggers
Taggers Units Spans Nuclearity Relations Fewer-
Relations
No. of
Docs
Avg. No.
EDUs
A, B, E
(Apr 00)
0.874407 0.772147 0.705330 0.601673 0.644851 4 128.750000
A, B, E
(Jun 00)
0.952721 0.844141 0.782589 0.708932 0.739616 5 38.400002
A, E
(Nov 00)
0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668
B, E
(Nov 00)
0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713
A, B
(Nov 00)
1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001
A, B, E
(Jan 01)
0.971613 0.899971 0.855867 0.755539 0.782312 5 68.599998
relations when they are grouped in this manner,
with November results ranging from 0.78 to
0.82. In order to see how much of the
improvement had to do with pre-segmenting, we
asked the same three annotators to annotate five
previously unseen documents in January,
without reference to a pre-segmented document.
The results of this experiment are given in the
last row of Table 1, and they reflect only a small
overall decline in performance from the
November results. These scores reflect very
strong agreement and represent a significant
improvement over previously reported results on
annotating multiple texts in the RST framework
(Marcu et al, 1999).
Table 2 reports final results for all pairs of
taggers who double-annotated four or more
documents, representing 30 out of the 53
documents that were double-tagged. Results are
based on pre-segmented documents.
Our team was able to reach a significant
level of consistency, even though they faced a
number of challenges which reflect differences
in the agreement scores at the various levels.
While operating under the constraints typical of
any theoretical approach in an applied
environment, the annotators faced a task in
which the complexity increased as support from
the guidelines tended to decrease. Thus, while
rules for segmenting were fairly precise,
annotators relied on heuristics requiring more
human judgment to assign relations and
nuclearity. Another factor is that the cognitive
challenge of the task increases as the tree takes
shape. It is relatively straightforward for the
annotator to make a decision on assignment of
nuclearity and relation at the inter-clausal level,
but this becomes more complex at the inter-
sentential level, and extremely difficult when
linking large segments.
This tension between task complexity and
guideline under-specification resulted from the
practical application of a theoretical model on a
broad scale. While other discourse theoretical
approaches posit distinctly different treatments
for various levels of the discourse (Van Dijk and
Kintsch, 1983; Meyer, 1985), RST relies on a
standard methodology to analyze the document
at all levels. The RST relation set is rich and the
concept of nuclearity, somewhat interpretive.
This gave our annotators more leeway in
interpreting the higher levels of the discourse
structure, thus introducing some stylistic
differences, which may prove an interesting
avenue of future research.
5 Corpus Details
The RST Corpus consists of 385 Wall Street
Journal articles from the Penn Treebank,
representing over 176,000 words of text. In
order to measure inter-annotator consistency, 53
of the documents (13.8%) were double-tagged.
The documents range in size from 31 to 2124
words, with an average of 458.14 words per
document. The final tagged corpus contains
21,789 EDUs with an average of 56.59 EDUs
per document. The average number of words per
EDU is 8.1.
The articles range over a variety of topics,
including financial reports, general interest
stories, business-related news, cultural reviews,
editorials, and letters to the editor. In selecting
these documents, we partnered with the
Linguistic Data Consortium to select Penn
Treebank texts for which the syntactic
bracketing was known to be of high caliber.
Thus, the RST Corpus provides an additional
level of linguistic annotation to supplement
existing annotated resources.
Table 2: Inter-annotator agreement ? final results fox six taggers
Taggers Units Spans Nuclearity Relations Fewer-
Relations
No. of
Docs
Avg. No.
EDUs
B, E 0.960384 0.890481 0.848976 0.782327 0.806389 7 88.285713
A, E 0.984471 0.904707 0.835040 0.755486 0.784435 6 57.666668
A, B 1.000000 0.929157 0.882437 0.792134 0.822910 5 58.200001
A, C 0.950962 0.840187 0.782688 0.676564 0.711109 4 116.500000
A, F 0.952342 0.777553 0.694634 0.597302 0.624908 4 26.500000
A, D 1.000000 0.868280 0.801544 0.720692 0.769894 4 23.250000
For details on obtaining the corpus,
annotation software, tagging guidelines, and
related documentation and resources,  see:
http://www.isi.edu/~marcu/discourse.
6 Discussion
A growing number of groups have developed or
are developing discourse-annotated corpora for
text. These can be characterized both in terms of
the kinds of features annotated as well as by the
scope of the annotation. Features may include
specific discourse cues or markers, coreference
links, identification of rhetorical relations, etc.
The scope of the annotation refers to the levels
of analysis within the document, and can be
characterized as follows:
? sentential: annotation of features at the
intra-sentential or inter-sentential level, at a
single level of depth  (Sundheim, 1995;
Tsou et al, 2000; Nomoto and Matsumoto,
1999; Rebeyrolle, 2000).
? hierarchical: annotation of features at
multiple levels, building upon lower levels
of analysis at the clause or sentence level
(Moser and Moore, 1995; Marcu, et al
1999)
? document-level: broad characterization of
document structure such as identification of
topical segments (Hearst, 1997), linking of
large text segments via specific relations
(Ferrari, 1998; Rebeyrolle, 2000), or
defining text objects with a text architecture
(Pery-Woodley and Rebeyrolle, 1998).
Developing corpora with these kinds of rich
annotation is a labor-intensive effort. Building
the RST Corpus involved more than a dozen
people on a full or part-time basis over a one-
year time frame (Jan. ? Dec. 2000). Annotation
of a single document could take anywhere from
30 minutes to several hours, depending on the
length and topic. Re-tagging of a large number
of documents after major enhancements to the
annotation guidelines was also time consuming.
In addition, limitations of the theoretical
approach became more apparent over time.
Because the RST theory does not differentiate
between different levels of the tree structure, a
fairly fine-grained set of relations operates
between EDUs and EDU clusters at the macro-
level. The procedural knowledge available at the
EDU level is likely to need further refinement
for higher-level text spans along the lines of
other work which posits a few macro-level
relations for text segments, such as Ferrari
(1998) or Meyer (1985).  Moreover, using the
RST approach, the resultant tree structure, like a
traditional outline, imposed constraints that
other discourse representations (e.g., graph)
would not. In combination with the tree
structure, the concept of nuclearity also guided
an annotator to capture one of a number of
possible stylistic interpretations. We ourselves
are eager to explore these aspects of the RST,
and expect new insights to appear through
analysis of the corpus.
We anticipate that the RST Corpus will be
multifunctional and support a wide range of
language engineering applications. The added
value of multiple layers of overt linguistic
phenomena enhancing the Penn Treebank
information can be exploited to advance the
study of discourse, to enhance language
technologies such as text summarization,
machine translation or information retrieval, or
to be a testbed for new and creative natural
language processing techniques.
References
Bruce Britton and John Black. 1985.
Understanding Expository Text. Hillsdale, NJ:
Lawrence Erlbaum Associates.
Jill Burstein, Daniel Marcu, Slava Andreyev,
and Martin Chodorow. 2001. Towards
automatic identification of discourse elements in
essays. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, Toulouse, France.
Jean Carletta, Amy Isard, Stephen Isard,
Jacqueline Kowtko, Gwyneth Doherty-Sneddon,
and Anne Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Linguistics 23(1): 13-32.
Giacomo Ferrari. 1998. Preliminary steps
toward the creation of a discourse and text
resource. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC 1998),
Granada, Spain, 999-1001.
Giovanni Flammia and Victor Zue. 1995.
Empirical evaluation of human performance and
agreement in parsing discourse constituents in
spoken dialogue. In Proceedings of the 4th
European Conference on Speech
Communication and Technology, Madrid, Spain,
vol. 3, 1965-1968.
Roger Garside, Steve Fligelstone and Simon
Botley. 1997. Discourse Annotation: Anaphoric
Relations in Corpora. In Corpus annotation:
Linguistic information from computer text
corpora, edited by R. Garside, G. Leech, and T.
McEnery. London: Longman, 66-84.
Talmy Givon. 1983. Topic continuity in
discourse. In Topic Continuity in Discourse: a
Quantitative Cross-Language Study.
Amsterdam/Philadelphia: John Benjamins, 1-41.
Joseph Evans Grimes. 1975. The Thread of
Discourse. The Hague, Paris: Mouton.
Barbara Grosz and Candice Sidner. 1986.
Attentions, intentions, and the structure of
discourse. Computational Linguistics, 12(3):
175-204.
Marti Hearst. 1997. TextTiling: Segmenting
text into multi-paragraph subtopic passages.
Computational Linguistics 23(1): 33-64.
Julia Hirschberg and Diane Litman. 1993.
Empirical studies on the disambiguation of cue
phrases. Computational Linguistics 19(3): 501-
530.
Eduard Hovy. 1993. Automated discourse
generation using discourse structure relations.
Artificial Intelligence 63(1-2): 341-386.
Klaus Krippendorff. 1980. Content Analysis:
An Introduction to its Methodology. Beverly
Hills, CA: Sage Publications.
Geoffrey Leech, Tony McEnery, and Martin
Wynne. 1997. Further levels of annotation. In
Corpus Annotation: Linguistic Information from
Computer Text Corpora, edited by R. Garside,
G. Leech, and T. McEnery. London: Longman,
85-101.
Robert Longacre. 1983. The Grammar of
Discourse. New York: Plenum Press.
William Mann and Sandra Thompson. 1988.
Rhetorical structure theory. Toward a functional
theory of text organization. Text, 8(3): 243-281.
William Mann and Sandra Thompson, eds.
1992. Discourse Description: Diverse Linguistic
Analyses of a Fund-raising Text.
Amsterdam/Philadelphia: John Benjamins.
Daniel Marcu. 2000. The Theory and
Practice of Discourse Parsing and
Summarization. Cambridge, MA: The MIT
Press.
Daniel Marcu, Estibaliz Amorrortu, and
Magdelena Romera. 1999. Experiments in
constructing a corpus of discourse trees. In
Proceedings of the ACL Workshop on Standards
and Tools for Discourse Tagging, College Park,
MD, 48-57.
Daniel Marcu, Lynn Carlson, and Maki
Watanabe. 2000. The automatic translation of
discourse structures. Proceedings of the First
Annual Meeting of the North American Chapter
of the Association for Computational
Linguistics, Seattle, WA, 9-17.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: the Penn
Treebank, Computational Linguistics 19(2),
313-330.
Bonnie Meyer. 1985. Prose Analysis:
Purposes, Procedures, and Problems. In
Understanding Expository Text, edited by B.
Britton and J. Black. Hillsdale, NJ: Lawrence
Erlbaum Associates, 11-64.
Johanna Moore. 1995. Participating in
Explanatory Dialogues: Interpreting and
Responding to Questions in Context.
Cambridge, MA: MIT Press.
Johanna Moore and Cecile Paris. 1993.
Planning text for advisory dialogues: capturing
intentional and rhetorical information.
Computational Linguistics 19(4): 651-694.
Megan Moser and Johanna Moore. 1995.
Investigating cue selection and placement in
tutorial discourse. Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics, Cambridge, MA,
130-135.
Tadashi Nomoto and Yuji Matsumoto. 1999.
Learning discourse relations with active data
selection. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
College Park, MD, 158-167.
Rebecca Passonneau and Diane Litman.
1997. Discourse segmentation by human and
automatic means. Computational Linguistics
23(1): 103-140.
Marie-Paule Pery-Woodley and Josette
Rebeyrolle. 1998. Domain and genre in
sublanguage text: definitional microtexts in
three corpora. In Proceedings of the First
International Conference on Language
Resources and Evaluation (LREC-1998),
Granada, Spain, 987-992.
Livia Polanyi. 1988. A formal model of the
structure of discourse. Journal of Pragmatics
12: 601-638.
Livia Polanyi. 1996. The linguistic structure
of discourse. Center for the Study of Language
and Information. CSLI-96-200.
Josette Rebeyrolle. 2000. Utilisation de
contextes d?finitoires pour l?acquisition de
connaissances ? partir de textes. In Actes
Journ?es Francophones d?Ing?nierie de la
Connaissance (IC?2000), Toulouse, IRIT, 105-
114.
Harvey Sacks, Emmanuel Schegloff, and
Gail Jefferson. 1974. A simple systematics for
the organization of turntaking in conversation.
Language 50: 696-735.
Sidney Siegal and N.J. Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences. New York: McGraw-Hill.
Beth Sundheim. 1995. Overview of results of
the MUC-6 evaluation. In Proceedings of the
Sixth Message Understanding Conference
(MUC-6), Columbia, MD, 13-31.
Benjamin K. T?sou, Tom B.Y. Lai, Samuel
W.K. Chan, Weijun Gao, and Xuegang Zhan.
2000. Enhancement of Chinese discourse
marker tagger with C.4.5. In  Proceedings of the
Second Chinese Language Processing
Workshop, Hong Kong, 38-45.
Teun A. Van Dijk and Walter Kintsch. 1983.
Strategies of Discourse Comprehension. New
York: Academic Press.
Ellen Voorhees and Donna Harman. 1999.
The Eighth Text Retrieval Conference (TREC-
8). NIST Special Publication 500-246.
Charles Wayne. 2000. Multilingual topic
detection and tracking: successful research
enabled by corpora and evaluation. In
Proceedings of the Second International
Conference on Language Resources and
Evaluation (LREC-2000), Athens, Greece,
1487-1493.
Janyce Wiebe, Rebecca Bruce, and Thomas
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics.
College Park, MD, 246-253.
A Phrase-Based, Joint Probability Model for Statistical Machine Translation
Daniel Marcu
Information Sciences Institute and
Department of Computer Science
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
marcu@isi.edu
William Wong
Language Weaver Inc.
1639 11th St., Suite 100A
Santa Monica, CA 90404
wwong@languageweaver.com
Abstract
We present a joint probability model for
statistical machine translation, which au-
tomatically learns word and phrase equiv-
alents from bilingual corpora. Transla-
tions produced with parameters estimated
using the joint model are more accu-
rate than translations produced using IBM
Model 4.
1 Motivation
Most of the noisy-channel-based models used in
statistical machine translation (MT) (Brown et al,
1993) are conditional probability models. In the
noisy-channel framework, each source sentence e in
a parallel corpus is assumed to ?generate? a target
sentence f by means of a stochastic process, whose
parameters are estimated using traditional EM tech-
niques (Dempster et al, 1977). The generative
model explains how source words are mapped into
target words and how target words are re-ordered
to yield well-formed target sentences. A variety
of methods are used to account for the re-ordering
stage: word-based (Brown et al, 1993), template-
based (Och et al, 1999), and syntax-based (Yamada
and Knight, 2001), to name just a few. Although
these models use different generative processes to
explain how translated words are re-ordered in a tar-
get language, at the lexical level they are quite sim-
ilar; all these models assume that source words are
individually translated into target words.1
1The individual words may contain a non-existent element,
called NULL.
We suspect that MT researchers have so far cho-
sen to automatically learn translation lexicons de-
fined only over words for primarily pragmatic rea-
sons. Large scale bilingual corpora with vocabu-
laries in the range of hundreds of thousands yield
very large translation lexicons. Tuning the probabil-
ities associated with these large lexicons is a difficult
enough task to deter one from trying to scale up to
learning phrase-based lexicons. Unfortunately, trad-
ing space requirements and efficiency for explana-
tory power often yields non-intuitive results.
Consider, for example, the parallel corpus of three
sentence pairs shown in Figure 1. Intuitively, if we
allow any Source words to be aligned to any Target
words, the best alignment that we can come up with
is the one in Figure 1.c. Sentence pair (S2, T2) of-
fers strong evidence that ?b c? in language S means
the same thing as ?x? in language T. On the basis
of this evidence, we expect the system to also learn
from sentence pair (S1, T1) that ?a? in language S
means the same thing as ?y? in language T. Unfortu-
nately, if one works with translation models that do
not allow Target words to be aligned to more than
one Source word ? as it is the case in the IBM mod-
els (Brown et al, 1993) ? it is impossible to learn
that the phrase ?b c? in language S means the same
thing as word ?x? in language T. The IBM Model
4 (Brown et al, 1993), for example, converges to the
word alignments shown in Figure 1.b and learns the
translation probabilities shown in Figure 1.a.2 Since
in the IBM model one cannot link a Target word
to more than a Source word, the training procedure
2To train the IBM-4 model, we used Giza (Al-Onaizan et al,
1999).
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 133-139.
                         Proceedings of the Conference on Empirical Methods in Natural
IBM?4 T?Table
p(y | a) = 1
p(x | c) = 1
p(z | b) = 0.98
p(x | b) = 0.02
Joint T?Table
p(x, b c) = 0.34
p(y, a) = 0.01
p(x y, a b c) = 0.32
p(z, b) = 0.33
Corresponding
Conditional Table
p(x y | a b c ) = 1
p(x | b c) = 1
p(y | a) = 1
p(z | b) = 1
S1: a   b   c
T1: x   y
S2: b   c
T2: x
S3: b
T3: z
S1: a   b   c
T1: x   y
S2: b   c
T2: x
S3: b
T3: z
S1: a   b   c
T1: x   y
S2: b   c
T2: x
S3: b
T3: z
Intuitive JointIBM?4
a) b) c) e)d)
Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model.
yields unintuitive translation probabilities. (Note
that another good word-for-word model is one that
assigns high probability to p(x   b) and p(z   b) and
low probability to p(x   c).)
In this paper, we describe a translation model that
assumes that lexical correspondences can be estab-
lished not only at the word level, but at the phrase
level as well. In constrast with many previous ap-
proaches (Brown et al, 1993; Och et al, 1999; Ya-
mada and Knight, 2001), our model does not try to
capture how Source sentences can be mapped into
Target sentences, but rather how Source and Tar-
get sentences can be generated simultaneously. In
other words, in the style of Melamed (2001), we es-
timate a joint probability model that can be easily
marginalized in order to yield conditional probabil-
ity models for both source-to-target and target-to-
source machine translation applications. The main
difference between our work and that of Melamed
is that we learn joint probability models of trans-
lation equivalence not only between words but also
between phrases and we show that these models can
be used not only for the extraction of bilingual lexi-
cons but also for the automatic translation of unseen
sentences.
In the rest of the paper, we first describe our
model (Section 2) and explain how it can be imple-
mented/trained (Section 3). We briefly describe a
decoding algorithm that works in conjunction with
our model (Section 4) and evaluate the performance
of a translation system that uses the joint-probability
model (Section 5). We end with a discussion of the
strengths and weaknesses of our model as compared
to other models proposed in the literature.
2 A Phrase-Based Joint Probability Model
2.1 Model 1
In developing our joint probability model, we started
out with a very simple generative story. We assume
that each sentence pair in our corpus is generated by
the following stochastic process:
1. Generate a bag of concepts  .
2. For each concept  , generate a pair of
phrases 
	  	

 , according to the distribution


	


	

 , where 	  and 	

 each contain at least
one word.
3. Order the phrases generated in each language
so as to create two linear sequences of phrases;
these sequences correspond to the sentence
pairs in a bilingual corpus.
For simplicity, we initially assume that the bag of
concepts and the ordering of the generated phrases
are modeled by uniform distributions. We do not
assume that  is a hidden variable that generates
the pair 	  	

 , but rather that 
	  	

 . Un-
der these assumptions, it follows that the probability
of generating a sentence pair (E, F) using concepts
 is given by the product of all phrase-to-
phrase translation probabilities, ff 
		The Perils and Rewards of Developing                                        
Restricted Domain Applications 
Daniel Marcu 
Information Sciences Institute and 
Department of Computer Science 
University of Southern California 
4676 Admiralty Way, Suite 1001 
Marina del Rey, CA 90292 
marcu@isi.edu 
Abstract 
Over the last three decades, the development of 
restricted domain applications has been an ongo-
ing theme in computational linguistic research. 
Speech Recognition, Machine Translation, Sum-
marization, and Question Answering researchers 
have all built at one time or another restricted 
domain systems. In retrospect, it is long due to 
examine both the successes and failures of these 
previous attempts. 
In this talk, I will examine the circumstances 
in which the development of restricted domain 
applications has led to significant advances in the 
state of the art and the circumstances in which 
restricted domain research has had little impact 
on our field. I will use the lessons learned from 
previous attempts to building restricted domain 
natural language processing applications in order 
to examine the potential impact of current re-
search in restricted domain question answering. 
  
   
Generic Sentence Fusion is an Ill-Defined Summarization Task
Hal Daume? III and Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{hdaume,marcu}@isi.edu
Abstract
We report on a series of human evaluations of the
task of sentence fusion. In this task, a human is
given two sentences and asked to produce a single
coherent sentence that contains only the important
information from the original two. Thus, this is a
highly constrained summarization task. Our inves-
tigations show that even at this restricted level, there
is no measurable agreement between humans re-
garding what information should be considered im-
portant. We further investigate the ability of sepa-
rate evaluators to assess summaries, and find simi-
larly disturbing lack of agreement.
1 Introduction and Motivation
The practices of automatic summarization vary
widely across many dimensions, including source
length, summary length, style, source, topic, lan-
guage, and structure. Most typical are summaries
of a single news document down to a headline or
short summary, or of a collection of news docu-
ments down to a headline or short summary (Hahn
and Harman, 2002). A few researchers have focused
on other aspects of summarization, including sin-
gle sentence (Knight and Marcu, 2002), paragraph
or short document (Daume? III and Marcu, 2002),
query-focused (Berger and Mittal, 2000), or speech
(Hori et al, 2003).
The techniques relevant to, and the challenges
faced in each of these tasks can be quite different.
Nevertheless, they all rely on one critical assump-
tion: there exists a notion of (relative) importance
between pieces of information in a document (or ut-
terance), regardless of whether we can detect this
or not. Indeed, recent research has looked at this
question in detail, and can be rather cleanly divided
into two partitions. The first partition aims to de-
velop manual evaluation criteria for determining the
quality of a summary, and is typified by the exten-
sive research done in single-document summariza-
tion by Halteren and Teufel (2003) and by the evalu-
ation strategy proposed by Nenkova and Passonneau
(2004). The other half aims to develop automatic
evaluation criteria to imitate the manual evaluation
methods (or at least to complement them). Work in
this area includes that of Lin and Hovy (2003) and
Pastra and Saggion (2003), both of whom inspect
the use of Bleu-like metrics (Papineni et al, 2002)
in summarization.
The results of these investigations have been
mixed. In the DUC competitions (Hahn and Har-
man, 2002), when manual evaluation has been em-
ployed, it has been commonly observed that human-
written summaries grossly outscore any machine-
produced summary. All machine-produced sum-
maries tend to show little (statistically significant)
difference from one another. Moreover, a baseline
system that simply takes the first sentences of a doc-
ument performs just as well or better than intelli-
gently crafted systems when summarizing news sto-
ries. Additionally, studies of vast numbers of sum-
maries of the same document (Halteren and Teufel,
2003) have shown that there is little agreement
among different humans as to what information be-
longs in a single document summary. This has been
leveraged by Nenkova and Passonneau (2004) to
produce a manual scoring method for summaries,
though the fact that humans show so little agree-
ment in this task is somewhat disheartening. All of
these evaluations rely strongly on the issue of mul-
tiple references, in order to achieve consensus.
Opinions voiced at DUC meetings indicate that
different researchers attribute this apparent lack of
agreement to one (or more) of many factors (in ad-
dition, see (Mani and Maybury, 1999)). Many be-
lieve that the fact that we are typically working in a
news genre is to blame, though this complaint tends
to be directed more at the excellent performance of
the baseline than at the issue of human agreement.
Others believe that in order to observe more agree-
ment, one needs to move to query-focused sum-
maries; it seems reasonable that if the person writ-
ing the summary knew how it would be used, he
would be more guided in what information to re-
tain. Yet others attribute the lack of agreement sim-
Connecting Point has become the single largest Mac retailer after tripling it ?s Macintosh sales since January 1989 .
Connecting Point Systems tripled it ?s sales of Apple Macintosh systems since last January .  It is now the single largest seller of Macintosh .
Figure 1: Example ?document, abstract? alignment.
ply to the vast space of possible choices a summa-
rizer could make, and see the disagreement simply
as par for the course.
2 Our Study
In this paper, we report on a study of the perfor-
mance of humans producing summaries. We con-
cern ourselves with the task of sentence fusion. In
this task, we assume that two sentences are provided
and that the summarizer must produce as output a
single sentence that contains the important informa-
tion contained in the input sentences (we will de-
scribe later how we obtain such data). We would
like to show that this task is well-defined: if we
show many humans the same two sentences, they
will produce similar summaries. Of course we do
not penalize one human for using different words
than another.
The sentence fusion task is interesting after per-
forming sentence extraction, the extracted sen-
tences often contain superfluous information. It
has been further observed that simply compress-
ing sentences individually and concatenating the re-
sults leads to suboptimal summaries (Daume? III and
Marcu, 2002). The use of sentence fusion in multi-
document summarization has been extensively ex-
plored by Barzilay in her thesis (Barzilay, 2003;
Barzilay et al, 1999), though in the multi-document
setting, one has redundancy to fall back on. Addi-
tionally, the sentence fusion task is sufficiently con-
strained that it makes possible more complex and
linguistically motivated manipulations than are rea-
sonable for full document or multi-document sum-
maries (and for which simple extraction techniques
are unlikely to suffice).
3 Data Collection
Our data comes from a collection of computer prod-
uct reviews from the Ziff-Davis corporation. This
corpus consists of roughly seven thousand docu-
ments paired with human written abstracts. The av-
erage document was 1080 words in length, with an
abstract of length 136 words, a compression rate of
roughly 87.5%.
3.1 Examples Based on Alignments
For 50 of these ?document, abstract? pairs, we
have human-created word-for-word and phrase-for-
phrase alignments. An example alignment is shown
in Figure 1. Moreover, using a generalization of a
hidden Markov model, we are able to create (in an
unsupervised fashion) similar alignments for all of
the documents (Daume? III and Marcu, 2004). This
system achieves a precision, recall and f-score of
0.528, 0.668 and 0.590, respectively (which is a sig-
nificant increase in performance (f = 0.407) over
the IBM models or the Cut & Paste method (Jing,
2002)).
Based on these alignments (be they manually cre-
ated or automatically created), we are able to look
for examples of sentence fusions within the data.
In particular, we search for sentences in the ab-
stracts which are aligned to exactly two document
sentences, for which at least 80% of the summary
sentence is aligned and for which at least 20% of
the words in the summary sentence come from each
of the two document sentences.
This leaves us with pairs that consist of two doc-
ument sentences and one abstract sentence, exactly
the sort of data we are looking to use. We randomly
select 25 such pairs from the data collected from the
human-aligned portion of the corpus and 25 pairs
from the automatically aligned portion, giving us 50
pairs in all.
3.2 Examples Based on Elicitation
In addition to collecting data from the Ziff-Davis
corpus, we also elicited data from human subjects
with a variety of different backgrounds (though
all were familiar with computers and technology).
These people were presented with the pairs of docu-
ment sentences and, independently of the rest of the
document, asked to produce a single summary sen-
tence that contained the ?important? information.
Their summary was to be about half the length of
the original (this is what was observed in the pairs
extracted from the corpus) They were given no ad-
ditional specific instructions.
The summaries thus elicited ranged rather dra-
matically from highly cut and paste summaries to
highly abstractive summaries. An example is shown
in Table 1. In this table, we show the original pair of
ORIG: After years of pursuing separate and conflicting paths, AT&T and Digital Equipment Corp. agreed
in June to settle their computer-to-PBX differences.
The two will jointly develop an applications interface that can be shared by computers and PBXs of
any stripe.
REF: AT&T and DEC have a joint agreement from June to develop an applications interface to be shared
by various models of computers and PBXs.
HUM 1: AT&T and Digital Equipment Corp. agreed in June to settle their computer-to-PBX differences and
develop an applications interface that can be shared by any computer or PBX.
HUM 2: After years of pursuing different paths, AT&T and Digital agreed to jointly develop an applications
interface that can be shared by computers and PBXs of any stripe.
HUM 3: After working separately for years, AT&T will jointly develop an interface between computers and
PBXs.
Table 1: Example of elicited data.
document sentences, the ?reference? summary (i.e.,
the one that came from the original abstract), and
the responses of three of the eight human subjects
are shown (the first is the most ?cut and paste,? the
second is typical of the ?middle set? and the last is
unusually abstractive).
3.3 Baseline Summaries
In addition to the human elicited data, we gener-
ate three baseline summaries. The first baseline,
LONGER, simply selects the longer of the two sen-
tences as the summary (typically the sentences are
roughly the same length; thus this is nearly random).
The second baseline, DROPSTOP first catenates the
sentences (in random order), then removes punctu-
ation and stop words, finally cutting off at the 50%
mark. The third baseline, COMP is the document
compression system developed by Daume? III and
Marcu (2002), which compresses documents by cut-
ting out constituents in a combined syntax and dis-
course tree.
4 Evaluation of Summaries
We perform three types of manual evaluation on the
summaries from the previous section. In the first,
the ranked evaluation, we present evaluators with
original two document sentences; they also see a
list of hypothesis summaries and are asked to rank
them relative to one another. In the second evalu-
ation, the absolute evaluation, evaluators are pre-
sented with the reference summary and a hypothe-
sis and are asked to produce an absolute score for
the hypothesis. In the third, the factoid evaluation,
we manually inspect the information content of each
hypothesis.
4.1 Ranked Evaluation
In the ranked evaluation, human evaluators are pre-
sented with the original two document sentences.
They also see a list of 12 hypothesis summaries:
the reference summary, the eight summaries elicited
from human subjects, and the three baseline sum-
maries. They are asked to produce a ranking of the
12 summaries based both on their faithfulness to the
original document sentences and on their grammat-
icality. They were allowed to assign the same score
to two systems if they felt neither was any better (or
worse) than the other. They ranked the systems from
1 (best) to 12 (worst), though typically enough sys-
tems performed ?equally well? that a rank of 12 was
not assigned. Three humans performed this evalua-
tion.
4.2 Absolute Evaluation
In the absolute evaluation, human evaluators are
shown the reference summary and a single hypoth-
esis summary. In order to partially assuage the is-
sue of humans doing little more than string match-
ing (Coughlin, 2001), the reference and hypothe-
sis were shown on separate pages and humans were
asked not to go ?back? during the evaluation. Due to
time constraints, only three systems were evaluated
in this manner, one of the humans (the human out-
put was selected so that it was neither too cut-and-
paste nor too generative), the LONGER and COMP
systems. Three humans performed this task (each
shown a single different system output for each ref-
erence summary) and scored outputs on a scale from
1 (best) to 5 (worst). They were told to deduct points
for any information contained in the reference not
contained in the hypothesis, any information con-
tained in the hypothesis not contained in the refer-
ence, and ungrammaticality.
4.3 Factoid Evaluation
The third evaluation we perform ourselves, due to
its difficulty. This follows the general rubric de-
scribed by Nenkova and Passonneau?s (2004) pyra-
mid scoring scheme, though it differs in the sense
that we base our evaluation not on a reference sum-
mary, but on the original two document sentences.
Our methodology is described below.
REF LONGER COMP HUM 1 HUM 2 HUM 3 Factoid
F F F F F CP has taken leadership
F F leadership by volume
doug kass is analysis at dataquest inc
dq is a market research co
dq is in san jose
kass said CP has taken leadership
F F F F F analysts say
F F F F F F CP has a wide variety of stores
F F F F F F CP endorsed apple?s earned investment program
F F F F CP has become the low-price leader
F F F F CP hasn?t sacrificed technical support
Table 2: Factoid-based evaluation scheme for the sentence pair ?Connecting Point has taken leadership by volume,
volume, volume,? said Doug Kass, an analyst at Dataquest Inc., a market research company in San Jose. Analysts and
observers say Connecting Point?s wide variety of stores and endorsement of Apple?s earned investment program have
helped it become the low-price leader without sacrificing technical support.?
We assume that we are given the original pair
of sentences from the document and the hypothesis
summaries for many systems (in our experiments,
we used the original reference summary, the outputs
of three representative humans, and the LONGER
and COMP baselines). Given this data, we first seg-
ment the original pair of sentences into ?factoids?
in the style of Halteren and Teufel (2003). Then, for
each hypothesis summary and each factoid, we in-
dicate whether the summary contained that factoid.
Grammaticality of summary hypotheses enters
into the calculation of the factoid agreement num-
bers. A system only gets credit for a factoid if
its summary contains that factoid in a sufficiently
grammatical form that the following test could be
passed: given any reasonable question one could
pose about this factoid, and given the hypothesis
summary, could one answer the question correctly.
An example is shown in Table 2.
Based on this information, it is possible to se-
lect one or more of the outputs as the ?gold stan-
dard? and compare the rest in the pyramid scor-
ing scheme described by Nenkova and Passonneau
(2004). If only one output is used as the gold stan-
dard, then it is sufficient to compute precision and
recall against that gold standard, and then use these
numbers to compute an F-score, which essentially
measures agreement between the chosen gold stan-
dard and another hypothesis. In the remainder of
this analysis, when we report an F-score over the
factoid, this is calculated when the REF summary is
taken as the standard.
5 Evaluation Results
The fundamental question we would like to answer
is whether humans agree in terms of what informa-
tion should be preserved in a summary. Given our
data, there are two ways of looking at this. First:
HUM 1 HUM 2 HUM 3
REF 0.182 0.188 0.251
HUM 1 - 0.201 0.347
HUM 2 - - 0.470
Table 3: Agreement (kappa) scores for different
combinations of systems and humans
do the humans from whom we elicited data select
the same information as the reference? Second: do
these humans agree with each other. Both of these
questions can be answered by looking at the results
of the factoid evaluation.
For any set of columns in the factoid evaluation,
we can compute the agreement based on the kappa
statistic (Krippendorff, 1980). Researchers have ob-
served that kappa scores over 0.8 indicate strong
agreement, while scores between 0.6 and 0.8 indi-
cate reasonable agreement. Kappa values below 0.6
indicate little to no agreement. The kappa values
for various combinations of columns are shown in
Table 3.
As we can see from this table, there is essen-
tially no agreement found anywhere. The maximum
agreement is between HUMAN 2 and HUMAN 3, but
even a kappa value of 0.470 is regarded as virtually
no agreement. Furthermore, the kappa values com-
paring the human outputs to the reference outputs is
even lower, attaining a maximum of 0.251; again,
no agreement. One is forced to conclude that in the
task of generic sentence fusion, people will not pro-
duce a summary containing the same information
as the original reference sentence, and will not pro-
duce summaries that contain the same information
as another person in the same situation.
Despite the fact that humans do not agree on
what information should go into a summary, there is
still the chance that when presented with two sum-
System F-Score Absolute Relative
HUM 4 0.652 2.605 2.066
HUM 3 0.608 - 2.276
HUM 5 0.574 - 2.434
LONGER 0.419 3.000 3.368
REF 1.000 - 3.500
COMP 0.475 3.842 4.184
Table 4: Factoid F-score, absolute score and relative
ranking for 6 outputs
maries, they will be able to distinguish one as some-
how better than another. Answering this question is
the aim of the other two evaluations.
First, we consider the absolute rankings. Recall
that in this evaluation, humans are presented with
the reference summary as the gold standard sum-
mary. Since, in addition to grammaticality, this is
supposed to measure the correctness of information
preservation, it is reasonable to compare these num-
bers to the F-scores that can be computed based on
the factoid evaluation. These results are shown in
Table 4. For the first column (F-Score), higher num-
bers are better; for the second and third columns,
lower scores are better. We can see that the evalua-
tion prefers the human output to the outputs of either
of the systems. However, the factoid scoring prefers
the COMP model to the LONGER model, though the
Absolute scoring rates them in the opposite direc-
tion.
As we can see from the Relative column in Ta-
ble 4, human elicited summaries are consistently
preferred to any of the others. This is good news:
even if people cannot agree on what information
should go into a summary, they at least prefer hu-
man written summaries to others. After the hu-
man elicited summaries, there is a relatively large
jump to the LONGER baseline, which is unfortu-
nately preferred to the REFERENCE summary. After
the reference summary, there are two large jumps,
first to the document compression model and then
to the DROPSTOP baseline. However, when com-
paring the relative scores to the F-Score, we see that,
again, the factoid metric prefers the COMP model to
the LONGER model, but this is not reflected in the
relative scoring metric.
6 Analysis of Results
There are two conclusions that can be drawn from
these data. The first, related specifically to the
kappa statistic over the factoids as depicted in Ta-
ble 3, is that even in this modest task of compress-
ing two sentences into one, the task is ill-defined.
The second, related to the two other evaluations, is
that while humans seem able to agree on the rela-
tive quality of sentence fusions, judgments elicited
by direct comparison do not reflect whether systems
are correctly able to select content.
6.1 Disagreement of Importance
As indicated in Section 5, when humans are given
the task of compressing two sentences into one,
there is no measurable agreement between any two
as to what information should be retained.
The first thing worth noting is that there is mod-
erately more agreement between two elicited, non-
expert data points than between the elicited data and
the original reference. This can be attributed either
to the lack of context available to the non-experts,
or to their respective lack of expertise. Regardless,
the level of agreement between such non-expert hu-
mans is so low that this matters little. Furthermore,
from an automatic sentence fusion perspective, a
computer program is much more like a non-expert
human with no context than an expert with an entire
document to borrow from.
It might be argued that looking at only two sen-
tences does not provide sufficient context for hu-
mans to be able to judge relative importance. This
argument is supported by the fact that, upon mov-
ing to multi-document summarization, there is (rel-
atively) more agreement between humans regarding
what pieces of information should be kept. In or-
der to make the transition from two-sentence fusion
to multi-document summarization, one essentially
needs to make two inductive steps: the first from
two sentences, to three and so on up to a full sin-
gle document; the second from a single document
to multiple documents.
The analysis we have performed does not com-
ment on either of these inductive steps. However,
it is much more likely that it is the second, not
the first, that breaks down and enables humans to
agree more when creating summaries of collections
of documents. On the one hand, it seems unrea-
sonable to posit that there is some ?magic? num-
ber of sentences needed, such that once two humans
read that many sentences, they are able to agree on
what information is relevant. On the other hand, in
all evaluations that have considered multi-document
summarization, the collection of documents to be
summarized has been selected by a human with a
particular interest in mind. While this interest is not
(necessarily) communicated to the summarizers di-
rectly, it is indirectly suggested by the selection of
documents. This is why the use of redundancy in
multi-document summarization is so important. If,
on the other hand, humans were given a set of mod-
erately related or unrelated documents, we believe
that there would be even less agreement on what
makes a good summary1.
6.2 Human Perception of Quality
We have presented two sets of results regarding hu-
man perception of the quality of summaries. In the
first (see Table 4), humans are presented with the
REF summary and then with either a human-elicited
summary, a summary that is simply the longer of the
two sentences (recall that they do not see the origi-
nal two sentences, so they have no way of knowing
how this summary was created) and the output of
the COMP system. If one accepts that the F-Score
over factoids is a high-quality measure of summary
quality, then there should be strong correlation be-
tween this F-Score and the absolute scoring of the
system outputs. This is not observed. In fact, the
F-Score strongly prefers the COMP system over the
LONGER system, while human scoring prefers the
LONGER system.
Since the humans performing this evaluation
were told explicitly to count off for missing infor-
mation, extraneous information or lack of grammat-
icality, the only reasonable explanation for this dis-
crepancy is that the evaluators were sufficiently put
off by the grammatical errors made by the COMP
system that they penalized it heavily. Grammatical-
ity does enter into the factoids evaluation, though
perhaps not as strongly.
In the relative ranking evaluation (see Table 4),
there are two disturbing observations we can make.
First, as in the absolute scoring, the factoid evalua-
tion prefers the COMP system to the LONGER sys-
tem, but the relative ranking puts them in the other
order. Second, the LONGER baseline outperforms
the reference summary.
As before, we can explain this first discrepancy
by the issue of grammaticality. This is especially
important in this case: since the evaluators are not
given a reference summary that explicitly tells them
what information is important and what information
is not, they are required to make this decision on
their own. As we have observed, this act is very
imprecise, and it is likely the people performing
the evaluation have recognized this. Since there is
no longer a clear cut distinction between important
and unimportant information, and since they are re-
quired to make a decision, they have no choice but
to fall back on grammaticality as the primary moti-
vating factor for their decisions.
1Summarizing a set of unrelated documents may be an un-
realistic and unimportant task; nevertheless, it is interesting to
consider such a task in order to better understand why humans
agree more readily in multi-document summarization than in
single document summarization or in sentence fusion.
The second discrepancy is particularly disturb-
ing. Before discussing its possible causes, we
briefly consider the implications of this finding. In
order to build an automatic sentence fusion sys-
tem, one would like to be able to automatically col-
lect training data. Our method for doing so is by
constructing word-for-word and phrase-for-phrase
alignments between documents and abstracts and
leveraging these alignments to select such pairs.
In theory, one could extract many thousands of
such examples from the plethora of existing docu-
ment/summary pairs available. Unfortunately, this
result tells us that even if we are able to build a
system that perfectly mimics these collected data,
a simple baseline will be preferred by humans in an
evaluation.
One might wish to attribute this discrepancy to er-
rors made by the largely imperfect automatic align-
ments. However, we have calculated the results sep-
arately for pairs derived from human alignments and
from automatic alignments, and observe no differ-
ences.
This leaves two remaining factors to explain this
difference. First, the original summary is created
by a trained human professional, who is very famil-
iar with the domain (while our elicited data comes
from technologically proficient adults, the topics
discussed in the data are typically about technical
systems from the late eighties, topics our summa-
rizers know very little about). Second, the original
summarizers had the rest of the document available
when creating these fusions. Though without per-
forming relevant experiments, it is impossible to say
what the results would be.
However, from a system-building perspective,
one can view fusion in many applications and it
is highly desirable to be able to perform such fu-
sions without knowing the rest of the document.
From a document summarization perspective, one
might wish to perform sentence extraction to re-
duce the document to a few sentences and then use
sentence fusion to compress these further. In this
case, the primary motivation for performing this in
a pipelined fashion would be to remove the com-
plexity of dealing with the entire document when
the more complex fusion models are applied. In
another possible application of question answering,
one can imagine answering a question by fusion to-
gether several sentences returned as the result of an
information retrieval engine. In this case, it is nearly
impossible to include the remainder of the docu-
ments in such an analysis.
7 Summary and Conclusions
We have performed an analysis of agreement be-
tween humans in the highly constrained task of fus-
ing two sentences together. This task has appli-
cations in summarization, question answering and
pure natural language generation. We have shown
that this task is not well defined, when viewed in
isolation. Furthermore, we have shown that us-
ing automatically extracted data for training cannot
lead to systems that outperform a simple baseline of
choosing the longer of the two sentences..
These results are disheartening, though by per-
forming such experiments a priori, we are able to
better judge which courses of research are and are
not worth pursuing. Questions regarding the agree-
ment between people in the area of single docu-
ment summarization and multi-document summa-
rization have already been raised and are currently
only partially answered (Halteren and Teufel, 2003;
Nenkova and Passonneau, 2004; Marcu and Ger-
ber, 2001). We have shown that even in this con-
strained domain, it is very unlikely that any signif-
icant agreement will be found, without specifically
guiding the summarizers, either by a query, a user
model, or some other external knowledge. We have
argued that it is likely that this lack of agreement
will not be subverted by adding more sentences,
though this should be confirmed experimentally.
The issues of multiple references and of adding
context (essentially by allowing the summarizers to
see the document from which these two sentences
were extracted) has not been addressed in this work;
either might serve to increase agreement. However,
one of the goals of this methodology for automat-
ically extracting pairs of sentences from automat-
ically aligned corpora is to be able to get data on
which to train and test a system without having hu-
mans write it. To require one to elicit multiple ref-
erences to obtain any agreement obviates this goal
(moreover, that agreement between humans and the
original summary sentence is even lower than be-
tween a pair of humans makes this practice ques-
tionable). Regarding context, it is reasonable to hy-
pothesize (though this would need to be verified)
that the addition of context would result in higher
kappa scores. Unfortunately, if a human is given
access to this information, it would only be fair to
give a system access to the same information. This
means that we would no longer be able to view
generic sentence fusion as an isolated task, making
fusion-specific research advances very difficult.
8 Acknowledgements
We wish to thank Kevin Knight, Eduard Hovy, Jerry
Hobbs and the anonymous reviewers for their help-
ful and insightful comments. This work was par-
tially supported by DARPA-ITO grant N66001-00-
1-9814, NSF grant IIS-0097846, and a USC Dean
Fellowship to Hal Daume? III.
References
R. Barzilay, K. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-
document summarization. In Proceedings of
ACL.
R. Barzilay. 2003. Information Fusion for Mut-
lidocument Summarization: Paraphrasing and
Generation. Ph.D. thesis, Columbia University.
A. Berger and V. Mittal. 2000. Query-relevant sum-
marization using FAQs. In Proceedings of ACL.
D. Coughlin. 2001. Correlating automated and hu-
man assessments of machine translation quality.
In Proceedings of MT Summit IX.
H. Daume? III and D. Marcu. 2002. A noisy-channel
model for document compression. In Proceed-
ings of ACL.
H. Daume? III and D. Marcu. 2004. A phrase-based
HMM approach to document/abstract alignment.
In preparation.
U. Hahn and D. Harman, editors. 2002. Second
Document Understanding Conference (DUC-
2002).
H. Halteren and S. Teufel. 2003. Examining the
consensus between human summaries: Initial ex-
periments with factoid analysis. In HLT-NAACL
DUC Workshop.
C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel.
2003. A statistical approach to automatic speech
summarization. Journal on Applied Signal Pro-
cessing, 3:128?139.
H. Jing. 2002. Using hidden Markov modeling to
decompose human-written summaries. Compu-
tational Linguistics, 28(4):527 ? 544, December.
K. Knight and D. Marcu. 2002. Summarization
beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intel-
ligence.
K. Krippendorff. 1980. Content analysis: An Intro-
duction to its Methodology. Sage Publications,
CA.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statis-
tics. In Proceedings of HLT-NAACL.
C.Y. Lin. 2003. Improving summarization perfor-
mance by sentence compression - a pilot study.
In Proceedings of IRAL Workshop.
I. Mani and M. Maybury, editors. 1999. Ad-
vances in Automatic Text Summarization. The
MIT Press, Cambridge, MA.
D. Marcu and L. Gerber. 2001. An inquiry into
the nature of multidocument abstracts, extracts,
and their evaluation. In NAACL Summarization
Workshop.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarization: The pyramid
method. In Proceedings of HLT-NAACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu.
2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of ACL.
K. Pastra and H. Saggion. 2003. Colouring sum-
maries BLEU. In EACL.
A Phrase-Based HMM Approach to Document/Abstract Alignment
Hal Daume? III and Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{hdaume,marcu}@isi.edu
Abstract
We describe a model for creating word-to-word and
phrase-to-phrase alignments between documents
and their human written abstracts. Such alignments
are critical for the development of statistical sum-
marization systems that can be trained on large cor-
pora of document/abstract pairs. Our model, which
is based on a novel Phrase-Based HMM, outper-
forms both the Cut & Paste alignment model (Jing,
2002) and models developed in the context of ma-
chine translation (Brown et al, 1993).
1 Introduction
There are a wealth of document/abstract pairs that
statistical summarization systems could leverage to
learn how to create novel abstracts. Detailed stud-
ies of such pairs (Jing, 2002) show that human ab-
stractors perform a range of very sophisticated op-
erations when summarizing texts, which include re-
ordering, fusion, and paraphrasing. Unfortunately,
existing document/abstract alignment models are
not powerful enough to capture these operations.
To get around directly tackling this problem, re-
searchers in text summarization have employed one
of several techniques.
Some researchers (Banko et al, 2000) have de-
veloped simple statistical models for aligning doc-
uments and headlines. These models, which imple-
ment IBM Model 1 (Brown et al, 1993), treat docu-
ments and headlines as simple bags of words and
learn probabilistic word-based mappings between
the words in the documents and the words in the
headlines. As our results show, these models are
too weak for capturing the operations that are em-
ployed by humans in summarizing texts beyond the
headline level.
Other researchers have developed models that
make unreasonable assumptions about the data,
which lead to the utilization of a very small per-
cent of available data. For instance, the docu-
ment and sentence compression models of Daume?
III, Knight, and Marcu (Knight and Marcu, 2002;
Daume? III and Marcu, 2002a) assume that sen-
tences/documents can be summarized only through
deletion of contiguous text segments. Knight and
Marcu found that from a corpus of 39, 060 abstract
sentences, only 1067 sentence extracts existed: a re-
call of only 2.7%.
An alternate techinque employed in a large vari-
ety of systems is to treat the summarization prob-
lem as a sentence extraction problem. Such sys-
tems can be trained either on human constructed ex-
tracts or extracts generated automatically from doc-
ument/abstract pairs (see (Marcu, 1999; Jing and
McKeown, 1999) for two such approaches).
None of these techniques is adequate. Even for a
relatively simple sentence from an abstract, we can
see that none of the assumptions listed above holds.
In Figure 1, we observe several phenomena:
? Alignments can occur at the granularity of
words and at the granularity of phrases.
? The ordering of phrases in an abstract can be
different from the ordering in the document.
? Some abstract words do not have direct cor-
respondents in the document, and some doc-
ument words are never used.
It is thus desirable to be able to automatically
construct alignments between documents and their
abstracts, so that the correspondences between the
pairs are obvious. One might be initially tempted
to use readily-available machine translation systems
like GIZA++ (Och and Ney, 2003) to perform such
Connecting Point has become the single largest Mac retailer after tripling it ?s Macintosh sales since January 1989 .
Connecting Point Systems tripled it ?s sales of Apple Macintosh systems since last January .  It is now the single largest seller of Macintosh .
Figure 1: Example abstract/text alignment.
alignments. However, as we will show, the align-
ments produced by such a system are inadequate for
this task.
The solution that we propose to this problem is
an alignment model based on a novel mathematical
structure we call the Phrase-Based HMM.
2 Designing a Model
As observed in Figure 1, our model needs to be able
to account for phrase-to-phrase alignments. It also
needs to be able to align abstract phrases with arbi-
trary parts of the document, and not require a mono-
tonic, left-to-right alignment.1
2.1 The Generative Story
The model we propose calculates the probability of
an alignment/abstract pair in a generative fashion,
generating the summary S = ?s1 . . . sm? from the
document D = ?d1 . . . dn?.
In a document/abstract corpus that we have
aligned by hand (see Section 3), we have observed
that 16% of abstract words are left unaligned. Our
model assumes that these ?null-generated? words
and phrases are produced by a unique document
word ?, called the ?null word.? The parame-
ters of our model are stored in two tables: a
rewrite/paraphrase table and a jump table. The
rewrite table stores probabilities of producing sum-
mary words/phrases from document words/phrases
and from the null word (namely, probabilities of the
form rewrite
(
s? d?
)
and rewrite (s? ?)); the jump ta-
ble stores the probabilities of moving within a doc-
ument from one position to another, and from and
to ?.
The generation of a summary from a document is
assumed to proceed as follows:
1In the remainder of the paper, we will use the words ?sum-
mary? and ?abstract? interchangeably. This is because we wish
to use the letter s to refer to summaries. We could use the letter
a as an abbreviation for ?abstract?; however, in the definition
of the Phrase-Based HMM, we reuse common notation which
ascribes a different interpretation to a.
1. Choose a starting index i and jump to po-
sition di in the document with probability
jump (i). (If the first summary phrase is null-
generated, jump to the null-word with proba-
bility jump (?).)
2. Choose a document phrase of length k ? 0 and
a summary phrase of length l ? 1. Generate
summary words sl1 from document words di+ki
with probability rewrite
(
sl1 di+ki
)
.
2
3. Choose a new document index i? and
jump to position di? with probability
jump (i? ? (i + k)) (or, if the new document
position is the empty state, then jump (?)).
4. Choose k? and l? as in step 2, and gener-
ate the summary words s1+l+l?1+l from the
document words di?+k?i? with probability
rewrite
(
s1+l+l?1+l d
i?+k?
i?
)
.
5. Repeat from step 3 until the entire summary
has been generated.
6. Jump to position dn+1 in the document with
probability jump (n + 1 ? (i? + k?)).
Note that such a formulation allows the same
document word/phrase to generate many summary
words: unlike machine translation, where such be-
havior is typically avoided, in summarization, we
observe that such phenomena do occur. However,
if one were to build a decoder based on this model,
one would need to account for this issue to avoid
degenerate summaries from being produced.
The formal mathematical model behind the align-
ments is as follows: An alignment ? defines both
a segmentation of the summary S and a mapping
from the segments of S to the segments of the doc-
ument D. We write si to refer to the ith segment of
S, and M to refer to the total number of segments
2We write xba for the subsequence ?xa . . . xb?.
in S. We write d?(i) to refer to the words in the
document which correspond to segment si. Then,
the probability of a summary/alignment pair given a
document (Pr (S,? D)), becomes:
M+1
?
i=1
(jump (?(i) ?(i ? 1)) rewrite (si d?(i)
))
Here, we implicitly define sm+1 to be the end-of-
document token ??? and d?(m+1) to generate this
with probability 1. We also define the initial posi-
tion in the document, ?(0) to be 0, and assume a
uniform prior on segmentations.
2.2 The Mathematical Model
Having decided to use this model, we must now
find a way to efficiently train it. The model is very
much like a Hidden Markov Model in which the
summary is the observed sequence. However, us-
ing a standard HMM would not allow us to account
for phrases in the summary. We therefore extend
a standard HMM to allow multiple observations to
be emitted on one transition. We call this model a
Phrase-Based HMM (PBHMM).
For this model, we have developed equiva-
lents of the forward and backward algorithms,
Viterbi search and forward-backward parameter re-
estimation. Our notation is shown in Table 1.
Here, S is the state space, and the observation se-
quences come from the alphabet K . pij is the prob-
ability of beginning in state j. The transition prob-
ability ai,j is the probability of transitioning from
state i to state j. bi,j,k? is the probability of emitting
(the non-empty) observation sequence k? while tran-
sitioning from state i to state j. Finally, xt denotes
the state after emitting t symbols.
The full derivation of the model is too lengthy to
include; the interested reader is directed to (Daume?
III and Marcu, 2002b) for the derivations and proofs
of the formulae. To assist the reader in understand-
ing the mathematics, we follow the same notation as
(Manning and Schutze, 2000). The formulae for the
calculations are summarized in Table 2.
2.2.1 Forward algorithm
The forward algorithm calculates the probability of
an observation sequence. We define ?j(t) as the
probability of being in state j after emitting the first
t ? 1 symbols (in whatever grouping we want).
2.2.2 Backward algorithm
Just as we can compute the probability of an obser-
vation sequence by moving forward, so can we cal-
culate it by going backward. We define ?i(t) as the
probability of emitting the sequence oTt given that
we are starting out in state i.
2.2.3 Best path
We define a path as a sequence P = ?p1 . . . pL? such
that pi is a tuple ?t, x? where t corresponds to the
last of the (possibly multiple) observations made,
and x refers to the state we were coming from when
we output this observation (phrase). Thus, we want
to find:
argmax
P
Pr
(
P oT1 , ?
)
= argmax
P
Pr
(
P, oT1 ?
)
To do this, as in a traditional HMM, we estimate
the ? table. When we calculate ?j(t), we essentially
need to choose an appropriate i and t?, which we
store in another table, so we can calculate the actual
path at the end.
2.2.4 Parameter re-estimation
We want to find the model ? which best explains
observations. There is no known analytic solution
for standard HMMs, so we are fairly safe in assum-
ing that we will not find an analytic solution for this
more complex problem. Thus, we also revert to an
iterative hill-climbing solution analogous to Baum-
Welch re-estimation (i.e., the Forward Backward al-
gorithm). The equations for the re-estimated values
a? and b? are shown in Table 2.
2.2.5 Dirichlet Priors
Using simple maximum likelihood estimation is in-
adequate for this model: the maximum likelihood
solution is simply to make phrases as long as pos-
sible; unfortunately, doing so will first cut down on
the number of probabilities that need to be multi-
plied and second make nearly all observed summary
phrase/document phrase alignments unique, thus re-
sulting in rewrite probabilities of 1 after normaliza-
tion. In order to account for this, instead of finding
the maximum likelihood solution, we instead seek
the maximum a posteriori solution.
The distributions we deal with in HMMs, and,
in particular, PBHMMs, are all multinomial. The
Dirichlet distribution is in the conjugate family to
the multinomial distribution3 . This makes Dirich-
let priors very appealing to work with, so long as
3This effectively means that the product of a Dirichlet and
multinomial yields a multinomial.
S set of states
K output alphabet
? = {pij : j ? S} initial state probabilities
A = {ai,j : i, j ? S} transition probabilities
B = {bi,j,k? : i, j ? S, k? ? K+} emission probabilities
Table 1: Notation used for the PBHMM
?j(t) = Pr
(
ot?11 , xt?1 = j ?
)
=
t?1
?
t?=0
?
i?S
(
?i(t? + 1) ? ai,j ? bi,j,ott?+1
)
?i(t) = Pr
(
oTt ?, xt?1 = i
)
=
T
?
t?=t
?
j?S
(
ai,j ? bi,j,ot?t ? ?j(t
? + 1)
)
?j(t) = max
l,pl?11
Pr
(
pl?11 , ot?11 , pl.t = t ? 1, pl.x = j ?
)
= ?i(t?)ai,jbi,j,ot?1t?
?i,j(t?, t) = E
[
# of transitions i ; j emitting ott?
]
=
?i(t?)ai,jbi,j,ott??j(t + 1)
Pr
(
oT1 ?
)
a?i,j =
E [# of transitions i ; j]
E [# of transitions i ;?] =
?T
t?=1
?T
t=t? ?i,j(t?, t)
?T
t?=1
?T
t=t?
?
j??S ?i,j?(t?, t)
b?i,j,k? =
E
[
# of transitions i ; j with k? observed
]
E [# of transitions i ; j] =
?T+1?|k?|
t=1 ?(k?, o
t+|k?|?1
t )?i,j(t, t + |k?| ? 1)
?T
t?=1
?T
t=t? ?i,j(t?, t)
Table 2: Summary of equations for a PBHMM
we can adequately express our prior beliefs in their
form. (See (Gauvain and Lee, 1994) for the appli-
cation to standard HMMs.)
Applying a Dirichlet prior effectively allows us to
add ?fake counts? during parameter re-estimation,
according to the prior. The prior we choose has a
form such that fake counts are added as follows:
word-to-word rewrites get an additional count of 2;
identity rewrites get an additional count of 4; stem-
identity rewrites get an additional count of 3.
2.3 Constructing the PBHMM
Given our generative story, we construct a PBHMM
to calculate these probabilities efficiently. The
structure of the PBHMM for a given document is
conceptually simple. We provide values for each of
the following: the set of possible states S; the out-
put alphabet K; the initial state probabilities ?; the
transition probabilities A; and the emission proba-
bilities B.
2.3.1 State Space
The state set is large, but structured. There is a
unique initial state p, a unique final state q, and a
state for each possible document phrase. That is, for
all 1 ? i ? i? ? n, there is a state that corresponds
to the document phrase beginning at position i and
ending at position i?, di?i , which we will refer to as
ri,i? . There is also a null state for each document po-
sition r   ,i, so that when jumping out of a null state,
we can remember what our previous position in the
document was. Thus, S = {p, q} ? {ri,i? : 1 ? i ?
i? ? n} ? {r   ,i : 1 ? i ? n}. Figure 2 shows the
schematic drawing of the PBHMM constructed for
the document ?a b?. K , the output alphabet, con-
sists of each word found in S, plus the token ?.
2.3.2 Initial State Probabilities
For initial state probabilities: since p is our initial
state, we say that pip = 1 and that pir = 0 for all
r 6= p.
2.3.3 Transition Probabilities
The transition probabilities A are governed by the
jump table. Each possible jump type and it?s as-
sociated probability is shown in Table 3. By these
calculations, regardless of document phrase lengths,
transitioning forward between two consecutive seg-
ments will result in jump (1). When transitioning
jump(2)
jump(1)
jump(1)
jump(0)
jump(2)
jump(1)
jump(2)
jump(0)
jump(1)
ba
ab
b
qp
jump(1)jump(   )
a
Figure 2: Schematic drawing of the PBHMM (with some transition probabilities) for the document ?a b?
source target probability
p ri,i? jump (i)
ri,i? rj,j? jump (j ? i?)
ri,j? q jump (m + 1 ? i?)
p r   ,i jump (?) jump (i)
r   ,i rj,j? jump (j ? i)
r   ,i r   ,j jump (?) jump (j ? i)
r   ,i q jump (m + 1 ? i)
ri,i? r   ,j jump (?) jump (j ? i?)
Table 3: Jump probability decomposition
from p to ri,i? , the value ap,ri,i? = jump (i). Thus,
if we begin at the first word in the document, we
incur a transition probability of jump (1). There are
no transitions into p.
2.3.4 Rewrite Probabilities
Just as the transition probabilities are governed by
the jump table, the emission probabilities B are
governed by the rewrite table. In general, we write
bx,y,k? to mean the probability of generating k? while
transitioning from state x to state y. However, in
our case we do not need the x parameter, so we
will refer to these as bj,k?, the probability of generat-
ing k? when jumping into state j. When j = ri,i? ,
this is rewrite
(
k? di?i
)
. When j = r   ,i, this is
rewrite
(
k? ?
)
. Finally, any state transitioning into
q generates the phrase ??? with probability 1 and
any other phrase with probability 0.
Consider again the document ?a b? (the PBHMM
for which is shown in Figure 2) in the case when
the corresponding summary is ?c d?. Suppose the
correct alignment is that ?c d? is aligned to ?a? and
?b? is left unaligned. Then, the path taken through
the PBHMM is p ? a ? q. During the transi-
tion p ? a, ?c d? is emitted. During the transition
a ? q, ? is emitted. Thus, the probability for the
alignment is: jump (1) rewrite (?cd? ?a?) jump (2).
The rewrite probabilities themselves are gov-
erned by a mixture model with unknown mixing pa-
rameters. There are three mixture component, each
of which is represented by a multinomial. The first
is the standard word-for-word and phrase-for-phrase
table seen commonly in machine translation, where
rewrite
(
s? d?
)
is simply a normalized count of how
many times we have seen s? aligned to d?. The sec-
ond is a stem-based table, in which suffixes (using
Porter?s stemmer) of the words in s? and d? are thrown
out before a comparison is made. The third is a
simple identity function, which has a constant zero
value when s? and d? are different (up to stem) and
a constant non-zero value when they have the same
stem. The mixing parameters are estimated simul-
taneously during EM.
2.3.5 Parameter Initialization
Instead of initializing the jump and rewrite tables
randomly or uniformly, as it typically done with
HMMs, we initialize the tables according to the dis-
tribution specified by the prior. This is not atypi-
cal practice in problems in which a MAP solution is
sought.
3 Evaluation and Results
In this section, we describe an intrinsic evaluation of
the PBHMM document/abstract alignment model.
All experiments in this paper are done on the Ziff-
Davis corpus (statistics are in Table 4). In order
to judge the quality of the alignments produced by
a system, we first need to create a set of ?gold
standard? alignments. Two human annotators man-
ually constructed such alignments between docu-
ments and their abstracts. Software for assisting this
process was developed and is made freely available.
An annotation guide, which explains in detail the
document/abstract alignment process was also pre-
pared and is freely available.4
4Both the software and documentation are available on the
first author?s web page. The alignments are also available; con-
tact the authors for a copy.
Abstracts Extracts
Documents 2033
Sentences 13k 41k
Words 261k 1m
Types 14k 26k
29k
Sentences/Doc 6.28 21.51
Words/Doc 128.52 510.99
Words/Sent 20.47 23.77
Table 4: Ziff-Davis extract corpus statistics
3.1 Human Annotation
From the Ziff-Davis corpus, we randomly selected
45 document/abstract pairs and had both annotators
align them. The first five were annotated separately
and then discussed; the last 40 were done indepen-
dently.
Annotators were asked to perform phrase-to-
phrase alignments between abstracts and documents
and to classify each alignment as either possible P
or sure S, where P ? S. In order to calculate
scores for phrase alignments, we convert all phrase
alignments to word alignments. That is, if we have
an alignment between phrases A and B, then this
induces word alignments between a and b for all
words a ? A and b ? B. Given an alignment A,
we could calculate precision and recall as (see (Och
and Ney, 2003)):
Precision = |A?P ||A| Recall =
|A?S|
|S|
One problem with these definitions is that phrase-
based models are fond of making phrases. That is,
when given an abstract containing ?the man? and a
document also containing ?the man,? a human may
prefer to align ?the? to ?the? and ?man? to ?man.?
However, a phrase-based model will almost always
prefer to align the entire phrase ?the man? to ?the
man.? This is because it results in fewer probabili-
ties being multiplied together.
To compensate for this, we define soft precision
(SoftP in the tables) by counting alignments where
?a b? is aligned to ?a b? the same as ones in which
?a? is aligned to ?a? and ?b? is aligned to ?b.? Note,
however, that this is not the same as ?a? aligned to
?a b? and ?b? aligned to ?b?. This latter alignment
will, of course, incur a precision error. The soft pre-
cision metric induces a new, soft F-Score, labeled
SoftF.
Often, even humans find it difficult to align func-
tion words and punctuation. A list of 58 function
words and punctuation marks which appeared in the
corpus (henceforth called the ignore-list) was as-
sembled. Agreement and precision/recall have been
calculated both on all words and on all words that
do not appear in the ignore-list.
Annotator agreement was strong for Sure align-
ments and fairly weak for Possible alignments (con-
sidering only the 40 independently annotated pairs).
When considering only Sure alignments, the kappa
statistic (over 7.2 million items, 2 annotators and 2
categories) for agreement was 0.63. When words
from the ignore-list were thrown out, this rose to
0.68. Carletta (1995) suggests that kappa values
over 0.80 reflect very strong agreement and that
kappa values between 0.60 and 0.80 reflect good
agreement.
3.2 Machine Translation Experiments
In order to establish a baseline alignment model,
we used the IBM Model 4 (Brown et al, 1993)
and the HMM model (Stephan Vogel and Tillmann,
1996) as implemented in the GIZA++ package (Och
and Ney, 2003). We modified this slightly to allow
longer inputs and higher fertilities.
Such translation models require that input be in
sentence-aligned form. In the summarization task,
however, one abstract sentence often corresponds
to multiple document sentences. In order to over-
come this problem, each sentence in an abstract was
paired with three sentences from the corresponding
document, selected using the techniques described
by Marcu (1999). In an informal evaluation, 20 such
pairs were randomly extracted and evaluated by a
human. Each pair was ranked as 0 (document sen-
tences contain little-to-none of the information in
the abstract sentence), 1 (document sentences con-
tain some of the information in the abstract sen-
tence) or 2 (document sentences contain all of the
information). Of the twenty random examples, none
were labeled as 0; five were labeled as 1; and 15
were labeled as 2, giving a mean rating of 1.75.
We ran experiments using the document sen-
tences as both the source and the target language
in GIZA++. When document sentences were used
as the target language, each abstract word needed
to produce many document words, leading to very
high fertilities. However, since each target word is
generated independently, this led to very flat rewrite
tables and, hence, to poor results. Performance in-
creased dramatically by using the document as the
source language and the abstract as the target lan-
guage.
In all MT cases, the corpus was appended with
one-word sentence pairs for each word where that
word is translated as itself. In the two basic mod-
els, HMM and Model 4, the abstract sentence is the
source language and the document sentences are the
target language. To alleviate the fertility problem,
we also ran experiments with the translation going
in the opposite direction. These are called HMM-
flipped and Model 4-flipped, respectively. These
tend to out-perform the original translation direc-
tion. In all of these setups, 5 iterations of Model
1 were run, followed by 5 iterations of the HMM
model. In the Model 4 cases, 5 iterations of Model
4 were run, following the HMM.
3.3 Cut and Paste Experiments
We also tested alignments using the Cut and
Paste summary decomposition method (Jing, 2002),
based on a non-trainable HMM. Briefly, the Cut and
Paste HMM searches for long contiguous blocks of
words in the document and abstract that are iden-
tical (up to stem). The longest such sequences are
aligned. By fixing a length cutoff of n and ignoring
sequences of length less than n, one can arbitrarily
increase the precision of this method. We found that
n = 2 yields the best balance between precision and
recall (and the highest F-measure). The results of
these experiments are shown under the header ?Cut
& Paste.? It clearly outperforms all of the MT-based
models.
3.4 PBHMM Experiments
While the PBHMM is based on a dynamic program-
ming algorithm, the effective search space in this
model is enormous, even for moderately sized doc-
ument/abstract pairs. We selected the 2000 shortest
document/abstract pairs from the Ziff-Davis corpus
for training; however, only 12 of the hand-annotated
documents were included in this set, so we addition-
ally added the other 33 hand-annotate documents to
this set, yielding 2033 document/abstract pairs. We
then performed sentence extraction on this corpus
exactly as in the MT case, using the technique of
(Marcu, 1999). The relevant data for this corpus is
in Table 4. We also restrict the state-space with a
beam, sized at 50% of the unrestricted state-space.
The PBHMM system was then trained on this ab-
stract/extract corpus. The precision/recall results
are shown in Table 5. Under the methodology for
combining the two human annotations by taking the
union, either of the human scores would achieve a
System SoftP Recall SoftF
Human1 0.727 0.746 0.736
Human2 0.680 0.695 0.687
HMM 0.120 0.260 0.164
Model 4 0.117 0.260 0.161
HMM-flipped 0.295 0.250 0.271
Model 4-flipped 0.280 0.247 0.262
Cut & Paste 0.349 0.379 0.363
PBHMM 0.456 0.686 0.548
PBHMM O 0.523 0.686 0.594
Table 5: Results on the Ziff-Davis corpus
precision and recall of 1.0. To give a sense of how
well humans actually perform on this task (in addi-
tion to the kappa scores reported earlier), we com-
pare each human against the other.
One common precision mistake made by the
PBHMM system is to accidentally align words on
the summary side to words on the document side,
when the summary word should be null-aligned.
The PBHMMO system is an oracle system in which
system-produced alignments are removed for sum-
mary words that should be null-aligned (according
to the hand-annotated data). Doing this results in a
rather significant gain in SoftP score.
As we can see from Table 5, none of the ma-
chine translation models is well suited to this task,
achieving, at best, an F-score of 0.298. The Cut &
Paste method performs significantly better, which is
to be expected, since it is designed specifically for
summarization. As one would expect, this method
achieves higher precision than recall, though not by
very much. Our method significantly outperforms
both the IBM models and the Cut & Paste method,
achieving a precision of 0.456 and a recall nearing
0.7, yielding an overall F-score of 0.548.
4 Conclusions and Future Work
Despite the success of our model, it?s performance
still falls short of human performance (we achieve
an F-score of 0.548 while humans achieve 0.736).
Moreover, this number for human performance is
a lower-bound, since it is calculated with only one
reference, rather than two.
We have begun to perform a rigorous error anal-
ysis of the model to attempt to identify its deficien-
cies: currently, these appear to primarily be due to
the model having a zeal for aligning identical words.
This happens for one of two reasons: either a sum-
mary word should be null-aligned (but it is not),
or a summary word should be aligned to a differ-
ent, non-identical document word. We can see the
PBHMMO model as giving us an upper bound on
performance if we were to fix this first problem. The
second problem has to do either with synonyms that
do not appear frequently enough for the system to
learn reliable rewrite probabilities, or with corefer-
ence issues, in which the system chooses to align,
for instance, ?Microsoft? to ?Microsoft,? rather than
?Microsoft? to ?the company,? as might be correct
in context. Clearly more work needs to be done
to fix these problems; we are investigating solving
the first problem by automatically building a list of
synonyms from larger corpora and using this in the
mixture model, and the second problem by inves-
tigating the possibility of including some (perhaps
weak) coreference knowledge into the model.
Finally, we are looking to incorporate the results
of this model into a real system. This can be done ei-
ther by using the word-for-word alignments to auto-
matically build sentence-to-sentence alignments for
training a sentence extraction system (in which case
the precision/recall numbers over full sentences are
likely to be much higher), or by building a system
that exploits the word-for-word alignments explic-
itly.
5 Acknowledgments
This work was partially supported by DARPA-ITO
grant N66001-00-1-9814, NSF grant IIS-0097846,
and a USC Dean Fellowship to Hal Daume? III.
Thanks to Franz Josef Och and Dave Blei for dis-
cussions related to the project.
References
Michele Banko, Vibhu Mittal, and Michael Wit-
brock. 2000. Headline generation based on sta-
tistical translation. In Proceedings of the 38th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2000), pages 318?325,
Hong Kong, October 1?8.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Jean Carletta. 1995. Assessing agreement on clas-
sification tasks: the kappa statistic. Computa-
tional Linguistics, 22(2):249?254.
Hal Daume? III and Daniel Marcu. 2002a. A noisy-
channel model for document compression. In
Proceedings of the Conference of the Association
of Computational Linguistics (ACL 2002).
Hal Daume? III and Daniel Marcu. 2002b.
A phrase-based HMM. Unpublished; avail-
able at http://www.isi.edu/?hdaume/
docs/daume02pbhmm.ps, December.
J. Gauvain and C. Lee. 1994. Maximum a-
posteriori estimation for multivariate gaussian
mixture observations of markov chains. IEEE
Transactions SAP, 2:291?298.
Hongyan Jing and Kathleen R. McKeown. 1999.
The decomposition of human-written summary
sentences. In Proceedings of the 22nd Confer-
ence on Research and Development in Informa-
tion Retrieval (SIGIR?99), Berkeley, CA, August
15?19.
Hongyan Jing. 2002. Using hidden markov mod-
eling to decompose human-written summaries.
Computational Linguistics, 28(4):527 ? 544, De-
cember.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: A proba-
bilistic approach to sentence compression. Arti-
ficial Intelligence, 139(1).
Christopher Manning and Hinrich Schutze. 2000.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research.
In Proceedings of the 22nd Conference on Re-
search and Development in Information Retrieval
(SIGIR?99), pages 137?144, Berkeley, CA, Au-
gust 15?19.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Hermann Ney Stephan Vogel and Christoph Till-
mann. 1996. HMM-based word alignment in sta-
tistical translation. In COLING ?96: The 16th Int.
Conf. on Computational Linguistics, pages 836?
841.
NP Bracketing by Maximum Entropy Tagging and SVM Reranking
Hal Daume? III and Daniel Marcu
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{hdaume,marcu}@isi.edu
Abstract
We perform Noun Phrase Bracketing by using a lo-
cal, maximum entropy-based tagging model, which
produces bracketing hypotheses. These hypothe-
ses are subsequently fed into a reranking frame-
work based on support vector machines. We solve
the problem of hierarchical structure in our tag-
ging model by modeling underspecified tags, which
are fully determined only at decoding time. The
tagging model performs comparably to competing
approaches and the subsequent reranking increases
our system?s performance from an f-score of 81.7 to
86.1, surpassing the best reported results to date of
83.8.
1 Introduction and Prior Work
Noun Phrase Bracketing (NP Bracketing) is the task
of identifying any and all noun phrases in a sen-
tence. It is a strictly more difficult problem than
NP Chunking (Ramshaw and Marcus, 1995), in
which only non-recursive (or ?base?) noun phrases
are identified. It is simultaneously strictly more sim-
ple than either full parsing (Collins, 2003; Charniak,
2000) or supertagging (Bangalore and Joshi, 1999).
NP Bracketing is both a useful first step toward full
parsing and also a meaningful task in its own right;
for instance as an initial step toward co-reference
resolution and noun-phrase translation.
While existing NP Bracketers (including the one
described in this paper) tend to achieve worse over-
all F-measures than a full statistical parser (eg.,
(Collins, 2003; Charniak, 2000)), they can be sig-
nificantly more computationally efficient. Statisti-
cal parsers tend to scale exponentially in sentence
length, unless a narrow beam is employed, which
leads to globally poorer parses. In contrast, the
bracketer described in this paper scales linearly in
[[Confidence] in [the pound]] is widely expected
to take [another sharp dive] if [[[trade figures] for
[September]] , due for [release] [tomorrow] ,] . . .
Figure 1: Sample sentence with NPs bracketed.
the length of the sentence to find the globally op-
timal solution. This trade-off is depicted graphi-
cally in Figure 2. This figure shows the amount of
time (excluding any startup overhead) spent pars-
ing or bracketing using this system (the two lowest
lines) versus the parsers of Collins (2003) and Char-
niak (2000) run with default settings.
NP Bracketing was the shared task of the Com-
putational Natural Language Learning workshop in
1999 (CoNLL-99). In this competition, NP Brack-
eting systems were trained on sections 15-18 of the
Wall Street Journal corpus, while section 20 was
used for testing. The bracketing information was
extracted directly from the Penn Treebank, essen-
tially disregarding all non-NP brackets. An example
bracketed sentence is in Figure 1.
There have been several successful approaches
reported in the literature to solve this task. Tjong
Kim Sang (1999) first used repeated chunking to at-
tain an f-score of 82.98 during the CoNLL compe-
tition and subsequently (Sang, 2002) an f-score of
83.79 using a combination of two different systems.
Krymolowski and Dagan (2000) have obtained sim-
ilar results using more training data and lexicaliza-
tion. Brandts (1999) has used cascaded HMMs to
solve the NP Bracketing problem; however, he eval-
uated his system only on German NPs, so his results
cannot be directly compared.
Obviously, the difficulty that arises in NP Brack-
eting that differentiates it from NP Chunking is the
issue of embedded NPs, thus requiring output in the
5 10 15 20 25 30 35 40 45 50 55
0
5
10
15
20
25
30
Sentence Length
Se
co
nd
s 
to
 P
ar
se
 (n
orm
ali
ze
d)
Charniak
Collins
Bracketer+SVM
Bracketer
Figure 2: Speed of different systems
form of a tree structure. Most solutions to prob-
lems involving building trees from sequences build
in to the model a concept of depth (in parsing, this
is typically in the form of a chart; in bracketing and
shallow parsing, this is typically in the form of em-
bedded finite-state automata). We elect to take a
completely different approach. The model we use
is agnostic to any sort of depth: it hypothesizes un-
derspecified tags and allows the matching bracket
constraint to select a solution.
Specifically, we approach the NP Bracketing
problem as a tagging and reranking problem. We
use an efficient maximum entropy-based tagger to
hypothesize possible bracketings (see Section 2)
and then rerank these hypotheses using a support
vector reranking system (see Section 3). Using only
the tagger (without reranking), we achieve compa-
rable results to those referenced above and, with the
addition of the reranking system, achieve, to our
knowledge, the best reported results to date.
2 Bracketing as a Tagging Problem
In any tagging problem, the task is to associate each
word in the input with a single tag. There are
many competing approaches to tagging problems
including Hidden Markov Models (HMMs), Maxi-
mum Entropy Markov Models (MEMMs) and Con-
ditional Random Fields (CRFs). We adopt a slight
variant of the MEMM framework.
2.1 Maximum Entropy Tagging Model
In the formulation of the maximum entropy tagging
model, we assume that the probability distribution
of tags takes the form of an exponential distribution,
parameterized by a sequence of feature weights,
?m1 , where there are m-many features. Thus, we
obtain a distribution for Pr?m1 (ti ti?1, w?) of the
form:
1
Zti?1,w?
exp
?
?
m
?
j=1
?jfj(ti, ti?1, w?)
?
? (1)
where Zti?1,w? is a normalizing factor.
Like other maximum entropy approaches, this
distribution is unimodal and optimal values for the
?s can be found through various algorithms; we
use GIS. A good introduction to maximum entropy
models can be found in (Berger et al, 1996).
In our approach, we use a tag set of exactly five
tags: {open, close, in, out, sing}. An open tag is
assigned to all words that open a bracketing (regard-
less of the number of brackets opened) and do not
also close a bracketing. A close tag is assigned to all
words that close a bracketing and do not also open
one. An in tag is assigned to all words enclosed in
an NP, but which neither open nor close one. An out
tag is assigned to all words which are not enclosed
in an NP. A sing(leton) tag is assigned to all words
that both open and close a bracketing (regardless of
whether they open or close more than just their own
bracketing).
Note that such a tagging does not uniquely deter-
mine a bracketing. For instance, the tag sequence
?sing sing? could correspond either to [[w1] [w2]]
or to [w1] [w2]. Nevertheless, due to the constraints
involved in the tagging process (namely that a close
tag cannot appear unless one is already within an
NP and that one cannot have two close tags when
the corresponding open tags appear at the same lo-
cation1), we hope that our system will be able to dis-
ambiguate sufficiently. In other words, although our
taggings are under-specified, we hope that the ad-
ditional constraints that we subsequently associate
with these tags will yield high quality bracketings.
2.2 Feature Functions
The probability distribution shown in Equation 1 is
based on m-many real-valued feature functions, fj .
We use two classes of features, closed features and
open features (these roughly correspond to whether
they look at closed class elements or open class ele-
ments). The open features for position i are applied
at positions i, i ? 1 and i + 1. The closed features
are applied at i, i ? 1, i ? 2, i ? 3 and i + 1, i + 2
and i + 3.
1For instance, the bracketing [[wi . . . wj ]] is disallowed;
this bracketing must appear simply as [wi . . . wj ].
Closed features include: part of speech tag (ac-
cording to Brill?s (1995) tagger); two character suf-
fix of word; first character of part of speech; initial
character capitalized; word fully capitalized; last
character is period; word position in sentence; and
two features for when the word is either the first or
last word in the sentence. Open features include: the
word itself; the word lower-cased; the lower-cased
stem (Porter, 1980); the lower-cased stem plus the
part of speech; and 3 features that are each true
when there is a CC in the next 2 through 5 words.
In addition, we include a feature for tag ti?1.
2.3 Maximum Entropy Training
We used generalized iterative scaling to train the
maximum entropy model2 on 929, 921 features and
211, 728 training instances from sections 15-18 of
the Penn Treebank (20% of which was set aside as
a validation set). Training was run for ten thousand
iterations and, at convergence, achieved a tagging
error rate of 2.1% on the training data and 6.9% on
the validation data.
2.4 Decoding Algorithm
We use a Viterbi-like dynamic programming de-
coding algorithm, where transition probabilities
are governed by the discriminative tagging model.
However, the tags generated by our decoder are not
the same as those predicted by the maximum en-
tropy model. Our decoder does not search in the
original space of tags (sing, in, out, . . . ) but rather
in a new space that yields only well-formed brack-
etings. In the secondary search space, the algorithm
is guaranteed to find the most likely well-formed
bracketing, even though this might not correspond
to the most likely tag sequence. While it would be
possible to simply tag using the original tag set and
allow the reranker (see Section 3) to select a well-
formed bracketing, it is unlikely that this will lead
to improved performance: the complexity of the de-
coders will be the same, yet the bracketer would
have to wade through significantly more bad tag-
gings to find a good solution.
Our decoding tags take one of five forms, capi-
talized to distinguish them from the maximum en-
tropy tags: On, Cn , N , OnC , OCn where n ? 1
for all but OCn where n ? 2. The meaning of the
tags is: On means n simultaneous open brackets:
Cn means n simultaneous close brackets. N means
that no brackets appear at this position. OnC corre-
2Using the YASMET maximum entropy training package:
http://www.isi.edu/?och/YASMET/.
0 20 40 60 80 100 120 140 160 180 200
82
84
86
88
90
92
94
96
98
Precision
Recall
F?Score
Figure 3: Plot of n versus maximal f-score (and as-
sociated precision and recall) for test data.
sponds to n open brackets and one close bracket,
while OCn corresponds to one open bracket and
n ? 2 close brackets. These tags are enough to
decode any well-formed bracketing.
Our decoder assumes a maximum depth of tags
d has been prespecified and then solves a dynamic
programming problem on an n ? d ? t array A,
where n is the sentence length and t denotes an inte-
ger corresponding to the highest possible decoding
tag in an enumeration. The value Ai,d,t stores the
probability of being at position i and depth d af-
ter applying tag t at that position. It is always the
case that t ? 4d. The time and space complexity
of this decoding problem is thus O(d2n). The dy-
namic programming problem is:
A1,d,t = Pr??
(
t?0
) (2)
Ap,d,t = maxt? Ap?1,d??t,t? ? Pr??
(
t?d t?
) (3)
where
t?d =
?
?
?
?
?
?
?
?
?
?
?
out t = N ? d = 0
in t = N ? d > 0
sing t ? {OnC,OCn}
begin t = On
end t = Cn
(4)
?t =
?
?
?
?
?
?
?
?
?
?
?
n t = On
n ? 1 t = OnC
?n t = Cn
?n + 1 t = OCn
0 t = N
(5)
The intuition for calculating the value of Ap,d,t
for p > 1 (see Equation 3) is that we first choose
the optimal previous tag, t?. Furthermore, based on
t and d, we can calculate the depth (d ? ?t, see
Equation 5) we must have been at previously. Thus,
we must take the value of Ap?1,d??t,t? which is the
probability of having arrived at position p ? 1 at
depth d ? ?t with tag t?. We then multiply this
by the probability of getting from that position to
the current position, which is given by Pr??
(
t?d t?
)
(note that the normalization occurs over the new
space of tags). The optimal tagging is given by
back-tracing through A, beginning at An,0,t for any
tag t. Even for long sentences, this algorithm re-
quires very little time and memory.
2.5 Model Deficiencies
While the bracketing model described above al-
ready performs comparably to competing ap-
proaches (see Section 4), it is still subject to mak-
ing categorical mistakes. Most of its errors are due
to the locality of the decisions made. Because of
the coarseness of the tags used in the maximum en-
tropy tagging framework, the model is unable to dis-
criminate between some bad bracketings and some
good ones. For instance, it must assign precisely
the same probability to both of the following brack-
etings, since the maximum entropy tags (shown be-
neath) are identical:
[[John,] [president] of [the company] ,]
[[John,] [[president] of [the company]]] ,]
sing sing in open close close
This limitation causes the model to make con-
sistent mistakes distinguishing between, for exam-
ple, lists and appositional phrases. To solve these
problems in the tagging model would be nearly im-
possible, without giving up on efficiency. However,
our decoder is able to produce n-best lists using ex-
act A? search that very frequently contain globally
superior taggings, even though the simple tagging
model cannot recognize them as such.
In Figure 3, we show the maximal f-score (and
corresponding precision and recall) for the best
bracketing chosen out of the n-best, as we let n
range from 1 to 400 for both the validation data
and the test data. As we can see from these graphs,
we have the possibility of improving our system?s f-
score performance by about ten points ? from 82%
to 93%, simply by being able to choose the correct
hypothesis from the n-best list; also working with
100-best lists is likely sufficient.
3 Hypothesis Reranking
In the previous section, we described a tagging
model for NP Bracketing that can produce n-best
lists. In this section, we describe a machine learn-
ing method for reranking these lists in an attempt to
choose a hypothesis which is superior to the first-
best output of the decoder. Reranking of n-best lists
has recently become popular in several natural lan-
guage problems, including parsing (Collins, 2003),
machine translation (Och and Ney, 2002) and web
search (Joachims, 2002). Each of these researchers
takes a different approach to reranking. Collins
(2003) uses both Markov Random Fields and boost-
ing, Och and Ney (2002) use a maximum entropy
ranking scheme, and Joachims (2002) uses a sup-
port vector approach. As SVMs tend to exhibit less
problems with over-fitting than other competing ap-
proaches in noisy scenarios, we also adopt the sup-
port vector approach.
3.1 Support Vector Reranking
A support vector classifier is a binary classifier with
a linear decision boundary. The selected decision
boundary is a hyperplane that is chosen in such a
way that the distance between it and the nearest data
points is maximized. Slack variables are commonly
introduced when the problem is not linearly separa-
ble, leading to soft margins.
For reranking, we assume that instead of having
binary classes for the yis, we have real values which
specify the relative ordering (higher values come
first). For this task, we get the following optimiza-
tion problem (Joachims, 2002):
minimize 12 ||w?||
2 + C
N
?
i=1
?i,j (6)
subject to w? ? x?i ? w? ? x?j + 1 ? ?i,j (7)
?i,j ? 0 (8)
Where the i, js are drawn from comparable data
points and yi ? yj and C is a regularization param-
eter that specifies how great the cost of mis-ordering
is. As noticed by Joachims, the condition in Equa-
tion 7 can be reduced to the standard SVM model
by subtracting w? ? x?j from both sides.
3.2 Reranking Feature Functions
Since our problem is closely related to that of
Collins? (2003), we use many of the same feature
functions he does, though we do introduce many of
our own (those which are copied from Collins are
marked with an asterisk). We view the hypothesized
bracketing as a tree in a context free grammar and
include features based on each rule used to gener-
ate the given tree. For concreteness, we will use the
CFG rule NP ? DT JJ NP (where the NP is selected
as the head) as an example.
Rules*: the full CFG rule; in this case, the active
rule would be NP ? DT JJ NP.
Markov 2 Rules: CFG rules where 2-level
Markovization has been applied. That is, we look
at the rule for generating the first two tags, then
the next two (given the previous one), then the next
two (given the previous one), and so on. A start
of branch tag ([S]) and end of branch tag ([/S]) are
added to the beginning and end of the children lists.
In this case, the rules that fire are: NP! ? [S] DT,
NP![S] ? DT JJ, NP!DT ? JJ NP and NP!JJ ? NP
[/S]. The notation is X!Y ? A B, where X is the true
parent, Y was the previous child in the Markoviza-
tion, and A B are the two children.
Lex-Rules*: full CFG rules, where terminal POS
tags are replaced with lexical items.
Markov 2 Lex-Rules: Markov 2-style rules, ter-
minal POS tags are replaced with lexical items.
Bigrams*: pairs of adjacent tags in the CFG
rule; in our example, the active pairs are ([S],DT),
(DT,JJ), (JJ,NP) and (NP,[/S]).
Lex-Bigrams*: same as BIGRAMS, but with lex-
ical heads instead of POS tags.
Head Pairs*: pairs of internal node tags with the
head type; in the example, (DT, NP), (JJ, NP) and
(NP, NP).
Sizes: the child count, conditioned on the internal
tag; eg., NP ? 3.
Word Count: pair of the SIZES and total number
of words under this constituent.
Boundary Heads: pairs of the first and last head
in the constituent.
POS-Counts: a scheme of features that count
the number of children whose part of speech tag
matches a given predicate. There are six of these:
(1) children whose tag begins with N, (2) children
whose tag begins with N but is not NP, (3) children
which are DTs, (4) children whose tag begin with V,
(5) children which are commas, (6) children whose
tag is CC. In this case, we get a count of 1 for rules
(2) and (3), and 2 for rule (1).
Lex-Tag/Head Pairs: same as HEAD PAIRS, but
where lexical items are used instead of POS tags.
Special Tag Pairs: count of the lexical heads to
the left and right of leaves tagged with each of POS,
CC, IN and TO.
Tag-Counts: another schema of features that
replicates some of the features used in the maxi-
mum entropy tagger. This schema includes all the
original maximum entropy tags, as well as a feature
for each maximum entropy tag at position i, paired
with (a) the part of speech tag at position i, i?1 and
i + 1, (b) the word at position i, i ? 1 and i + 1, (c)
the part of speech + word pair at those positions, (d)
the maximum entropy tag at that position.
3.3 SVM Training
We develop three reranking systems, differentiated
by the amount of training data used. The first,
RR1, is trained on the validation part of the train-
ing set (20% of sections 15-18). The second, RR2.
is trained on the entire training set through cross-
validation (all of sections 15-18). The final, RR3 is
trained on the entire Penn Treebank corpus, except
section 20.
Training the reranking system only on the valida-
tion data (RR1) results in only a marginal gain of
overall f-score, due primarily to the fact that most
of the features use lexical information to prefer one
bracketing over another. The validation data from
sections 15-18 gives rise to 2, 012 training instances
and 362, 415 features. In order to train the reranking
system on all of the training data (RR2), we built
five decoders, each with a different 20% of the train-
ing data held out. Each decoder is then used to tag
the held-out 20% (this is done so that the tagger does
not do ?too well? on its training data). This leads to
8, 935 sentences for training, with a total of 1.1 mil-
lion features. Training on all the WSJ data except
section 20 (RR3) gives rise to 39, 953 training in-
stances and a total of just over 2.1 million features.
These examples give 1, 462, 568 rank constraints.
4 Results
We compare our system against those reported in
the literature. In all, the evaluation is over 2, 012
sentences of test data. In Table 1, we display the re-
sults of state-of-the-art systems, and the system de-
scribed in this paper (both with and without rerank-
ing). The upper part of the table displays results
from systems which are trained only on sections 15-
18 of the WSJ. The lower part displays results based
on systems trained on more data.
System BR BP BF CB
TKS99 76.1 91.3 82.8 0.14
TKS02 78.4 90.0 83.8 -
TAG 81.0 86.0 83.4 0.26
RR1 82.1 88.8 85.3 0.18
RR2 82.7 89.8 86.1 0.14
COL03NP 68.6 68.9 68.7 0.91
COL03Full 88.2 87.7 87.9 0.31
CHUNK 73.0 100.0 84.4 -
COL03All 88.0 89.8 88.9 0.18
KD00 79.3 88.5 83.7 -
RR3 84.3 90.8 87.4 0.12
Table 1: Results on test data. The systems in the
lower half are not directly comparable, since they
were either trained or tested on different data.
In the table, TKS99 and TKS02 are the sys-
tems of Tjong Kim Sang (1999; 2002). KD00 is
the system of (Krymolowski and Dagan, 2000). All
the COL03 systems are results obtained using the
restriction of the output of Collins (2003) parser.
In particular, the two comparable numbers coming
from Collins? parser are COL03NP and COL03Full .
The difference between these two systems is that the
NP system is trained on parse trees, with all non-NP
nodes removed. The FULL system is trained on full
parse trees, and then the output is reduced to just in-
clude NPs. COL03All is trained on sections 2-21 of
WSJ and tested on section 23, and is thus an upper
bound, since these numbers are testing on training
data.3 Our RR3 system had the reranking compo-
nent (but not the tagging component) trained on all
of the WSJ except for section 20.
The CHUNK row in the results table is the per-
formance of an optimally performing NP chunker.
That is, this is the performance attainable given a
chunker that identifies base NPs perfectly (at 100%
precision). However, since this hypothetical sys-
tem only chunks base NPs, it misses all non-base
NPs and thus achieves a recall of only 73.0, yield-
ing an overall F-score below our system?s perfor-
mance. Note also that no chunker will perform this
well. Current systems attain approximately 94%
precision and recall on the chunking task (Sha and
Pereira, 2002; Kudo and Matsumoto, 2001), so the
3Collins independently reports a recall of 91.2 and preci-
sion of 90.3 for NPs (Collins, 2003); however, these numbers
are based on training on all the data and testing on section 0.
Moreover, it is possible that his evaluation of NP bracketing is
not identical to our own. The results in row COL03Full are
therefore perhaps more relevant.
actual performance for a real system would be sub-
stantially lower.
The four criteria these systems are evaluated
on are bracketing recall (BR), bracketing precision
(BP), bracketing f-score (BF) and average crossing
brackets (CB). Some systems do not report their
crossing bracket rate. All of these metrics are cal-
culated only on NP* and WHNP* brackets.
5 Comparison of Performance
The results depicted in Table 1 show that, when
comparing our system directly to Collins? parser,
his system tends to achieve significantly higher lev-
els of recall, while maintaining a slight advantage
in terms of precision. This table, however, does not
tell the full story. As is typically observed in these
sort of applications, it is not the case that Collins?
parser is ?winning? by a little on all the data, but
rather that Collins? parser wins on some of the data
and our bracketer wins on some of the data. In this
section, we analyze the differences.
Overall, there are 2, 012 sentences in the test
data. In 558 cases, both the bracketing system and
Collins? parser achieve perfect precision. In 505
cases, both achieve perfect recall. For the remainder
of the discussion in this section, when discussing
precision, we will only consider the cases in which
not both achieved perfect scores, and similarly for
recall.
In Figure 4, we depict (excluding the mutually
perfect sentences) the percentage of sentences on
which each system is better than the other by a dis-
tance of at least . Along the X-axes, the value of 
ranges from 0 to 20. At a given value of , the seg-
mentation along the Y-axes depict (a) along the top
(in yellow where available), the proportion of sen-
tences for which the bracketer?s precision (for the
left hand image) was at least  of that of Collins?;
(b) in the middle (in red), the proportion of sen-
tences for which Collins? was at least  better; and
(c) along the bottom (in blue), the proportion of sen-
tences where the two systems performed within  of
each other.
As should be expected, as  increases, the
?Equal? region also increases. However, it is worth
noticing that even at an  of 20 precision points,
there are still roughly 11% of the sentences for
which one system?s performance is noticeably dif-
ferent from the other?s (and furthermore, that these
are about even). As can be immediately seen from
the right-hand graph, Collins? parser consistently
outperforms the bracketer in terms of recall. How-
Figure 4: Proportion of sentences for which one system outperforms the other with difference at least .
Precision Recall
Tag RR2 COL03 RR2 COL03
NP 21.4 19.8 20.5 21.3
VP 7.49 8.52 8.31 7.57
NN 8.22 7.62 7.43 7.83
IN 6.01 5.89 5.31 6.15
PP 5.90 5.63 5.16 6.03
S 4.96 5.82 5.44 5.15
NNP 6.15 4.79 6.29 5.82
Table 2: Percentage of tags on superior system.
ever, in contrast to the Precision graph, for the
first 10 or so values of , these proportions remain
roughly the same (in fact, for a short period, Collins?
actually looses ground). This suggests that there are
a relatively large proportion of sentences for which
our system is performing abominably (with > 10
recall points difference) in comparison to Collins?.
However, once a critical mass of  > 10 is reached,
the relative differences become less strong.
Since neither system is winning in all cases, in an
effort to better understand the conditions in which
one system will outperform the other, we inspect
the sentences for which there was a difference in
performance of at least 10 (for precision and recall
separately). To perform this investigation, we look
at the distribution of tags in the true, full parse trees
for those sentences. These percentages, for the 7
most common tags, are summarized in Table 2 (for
example, the relative frequency of the NP tag in sen-
tences where the RR2 system achieved higher pre-
cision was 21.4, while for the sentences for which
COL03 achieved higher precision was 19.8).
The first thing worth noticing in this table is that
in general, when one system achieves higher preci-
sion, the other system achieves higher recall, which
is not surprising. However, in the last row, corre-
sponding to proper nouns, the RR2 system outper-
forms the COL03 (this is the ?Full? implementa-
tion) in both precision and recall, suggesting that
our system is better able to capture the phrasing
of proper nouns. We attribute this to the fact that
our model is specialized to identify noun phrases,
of which proper nouns comprise a large part. Simi-
larly, the largest gains in recall for COL03 over RR2
are in sentences with many PPs. This coincides with
our intuition about the syntactic parser being better
able to capture long, embedded noun phrases.
6 Conclusion
We have presented a method for performing noun
phrase bracketing, which outperforms competing
methods both in terms of f-score and recall. The
system is based on two separate components: a
maximum entropy-based tagging system and a sup-
port vector machine reranking system. The key
component of the tagging system is that it produces
underspecified tags that are determined only at de-
coding time by bracketing constraints. The tagging
system operates very quickly and can tag and rerank
at a rate of approximately two sentences per second.
The tagger alone achieves an f-score of 83.4. This
score is only 0.4% lower (absolute) than the best re-
ported result to date of 83.8.
After tagging, we have fed 100 best lists into a
support vector reranking system, which performs
global optimization to choose a good bracketing.
Our reranking system is able to increase the f-score
of our bracketing approach from 83.4 to 86.1, im-
proving our performance beyond the best reported
system to date.
As we can see from Table 1, by comparing the
output of our system to that of COL00Full , there is
much in the way of recall to be gained by using a
full syntactic parser. However, this gain comes at
two expenses. First, full syntactic parsers are com-
putationally more expensive to run. Moreover, per-
formance of Collins? parser degrades significantly
(from 87.9 to 68.7 in f-score) when it cannot take
advantage of other constituent information. This
has a strong influence when one is faced with the
task of moving to a new domain. On the one hand,
our system (as well as the other bracketing systems
cited) requires data to only be annotated at the NP
level in order to achieve high performance. Con-
versely, without full parses, using a parser for learn-
ing NPs is inadequate.
Despite these successes, there is still much that
can be improved upon. While the reranking is
very efficient in the classification phase, training
a support vector reranking system is computation-
ally very expensive. Other well grounded statistical
learning systems might allow us to train this com-
ponent on more data and using more features. We
also hope to be able to improve our system?s perfor-
mance from its current rate of 86.1 (on official data)
and 87.4 (on all data) closer to the n-best optimal,
depicted in Figure 3.
7 Acknowledgments
This work was partially supported by DARPA-ITO
grant N66001-00-1-9814, NSF grant IIS-0097846,
and a USC Dean Fellowship to Hal Daume? III.
References
Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An approach to alsmost parsing.
Computational Linguistics, 25(2):237?265.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Thorsten Brandts. 1999. Cascaded markov models.
In Proceedings of EACL 1999.
Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing:
a case study in part of speech tagging. Computa-
tional Linguistics, December.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First An-
nual Meeting of the North American Chapter
of the Association for Computational Linguistics
NAACL?2000, pages 132?139, Seattle, Washing-
ton, April 29 ? May 3.
Michael Collins. 2003. Head-driven statistical
models for natural language parsing. Computa-
tional Linguistics, 29(4), December.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the ACM Conference on Knowledge Discovery
and Data Mining (KDD). ACM.
Yuval Krymolowski and Ido Dagan. 2000. Incorpo-
rating compositional evidence in memory-based
partial parsing. In Proceedings of ACL 2000,
Hong Kong.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In ACL 02, pages
295?302, Philadelphia, PA, July.
M.F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14:130?137.
Lance A. Ramshaw and Michell P. Marcus. 1995.
Text chunking using transformation-based learn-
ing. In Proceedings of the Third ACL Workshop
on Very Large Corpora. Association for Compu-
tational Linguistics.
Erik F. Tjong Kim Sang. 1999. Noun phrase detec-
tion by repeated chunking. In CoNLL-99 Work-
shop, Bergen, Norway.
Erik F. Tjong Kim Sang. 2002. Memory-based
shallow parsing. Journal of Machine Learning
Research, 2:559 ? 594, March.
Fei Sha and Fernando Pereira. 2002. Shallow pars-
ing with conditional random fields. In HLT-
NAACL.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 91?94,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
ISI?s Participation in the Romanian-English Alignment Task
Alexander Fraser
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292-6601
fraser@isi.edu
Daniel Marcu
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292-6601
marcu@isi.edu
Abstract
We discuss results on the shared task of
Romanian-English word alignment. The
baseline technique is that of symmetrizing
two word alignments automatically gener-
ated using IBM Model 4. A simple vo-
cabulary reduction technique results in an
improvement in performance. We also
report on a new alignment model and a
new training algorithm based on alternat-
ing maximization of likelihood with mini-
mization of error rate.
1 Introduction
ISI participated in the WPT05 Romanian-English
word alignment task. The system used for baseline
experiments is two runs of IBM Model 4 (Brown et
al., 1993) in the GIZA++ (Och and Ney, 2003) im-
plementation, which includes smoothing extensions
to Model 4. For symmetrization, we found that Och
and Ney?s ?refined? technique described in (Och and
Ney, 2003) produced the best AER for this data set
under all experimental conditions.
We experimented with a statistical model for in-
ducing a stemmer cross-lingually, but found that the
best performance was obtained by simply lower-
casing both the English and Romanian text and re-
moving all but the first four characters of each word.
We also tried a new model and a new training
criterion based on alternating the maximization of
likelihood and minimization of the alignment error
rate. For these experiments, we have implemented
an alignment package for IBM Model 4 using a hill-
climbing search and Viterbi training as described in
(Brown et al, 1993), and extended this to use new
submodels. The starting point is the final alignment
generated using GIZA++?s implementation of IBM
Model 1 and the Aachen HMM model (Vogel et al,
1996).
Paper organization: Section 2 is on the baseline,
Section 3 discusses vocabulary reduction, Section 4
introduces our new model and training method, Sec-
tion 5 describes experiments, Section 6 concludes.
We use the following notation: e refers to an En-
glish sentence composed of English words labeled
ei. f refers to a Romanian sentence composed of
Romanian words labeled fj . a is an alignment of e
to f . We use the term ?Viterbi alignment? to denote
the most probable alignment we can find, rather than
the true Viterbi alignment.
2 Baseline
To train our systems, Model 4 was trained two times,
first using Romanian as the source language and
then using English as the source language. For each
training, we ran 5 iterations of Model 1, 5 iterations
of the HMM model and 3 iterations of Model 4.
For the distortion calculations of Model 4, we re-
moved the dependencies on Romanian and English
word classes. We applied the ?union?, ?intersection?
and ?refined? symmetrization metrics (Och and Ney,
2003) to the final alignments output from training, as
well as evaluating the two final alignments directly.
We tried to have a strong baseline. GIZA++ has
many free parameters which can not be estimated us-
ing Maximum Likelihood training. We did not use
91
the defaults, but instead used settings which produce
good AER results on French/English bitext. We
also optimized p0 on the 2003 test set (using AER),
rather than using likelihood training. Turning off the
extensions to GIZA++ and training p0 as in (Brown
et al, 1993) produces a substantial increase in AER.
3 Vocabulary Size Reduction
Romanian is a Romance language which has a sys-
tem of suffixes for inflection which is richer than
English. Given the small amount of training data,
we decided that vocabulary size reduction was de-
sirable. As a baseline for vocabulary reduction, we
tried reducing words to prefixes of varying sizes
for both English and Romanian after lowercasing
the corpora. We also tried Porter stemming (Porter,
1997) for English.
(Rogati et al, 2003) extended Model 1 with an ad-
ditional hidden variable to represent the split points
in Arabic between the prefix, the stem and the suf-
fix to generate a stemming for use in Cross-Lingual
Information Retrieval. As in (Rogati et al, 2003),
we can find the most probable stemming given the
model, apply this stemming, and retrain our word
alignment system. However, we can also use the
modified model directly to find the best word align-
ment without converting the text to its stemmed
form.
We introduce a variable rj for the Romanian stem
and a variable sj for the Romanian suffix (which
when concatenated together give us the Romanian
word fj) into the formula for the probability of gen-
erating a Romanian word fj using an alignment aj
given only an English sentence e. We use the index
z to denote a particular stemming possibility. For a
given Romanian word the stemming possibilities are
simply every possible split point where the stem is at
least one character (this includes the null suffix).
p(fj , aj |e) =
?
z
p(rj,z, sj,z, aj |e) (1)
If the assumption is made that the stem and the
suffix are generated independently from e, we can
assume conditional independence.
p(fj , aj |e) =
?
z
p(rj,z, aj |e)p(sj,z, aj |e) (2)
We performed two sets of experiments, one set
where the English was stemmed using the Porter
stemmer and one set where each English word was
stemmed down to its first four characters. We
tried the best performing scoring heuristic for Ara-
bic from (Rogati et al, 2003) where p(sj,z, aj |e) is
modeled using the heuristic p(sj,z|lj) where sj,z is
the Romanian suffix, and lj is the last letter of the
Romanian word fj ; these adjustments are updated
during EM training. We also tried several other ap-
proximations of p(sj,z, aj |e) with and without up-
dates in EM training. We were unable to produce
better results and elected to use the baseline vocab-
ulary reduction technique for the shared task.
4 New Model and Training Algorithm
Our motivation for a new model and a new training
approach which combines likelihood maximization
with error rate minimization is threefold:
? Maximum Likelihood training of Model 4 is
not sufficient to find good alignments
? We would like to model factors not captured by
IBM Model 4
? Using labeled data could help us produce better
alignments, but we have very few labels
We create a new model and train it using an al-
gorithm which has a step which increases likelihood
(like one iteration in the EM algorithm), alternating
with a step which decreases error. We accomplish
this by:
? grouping the parameters of Model 4 into 5 sub-
models
? implementing 6 new submodels
? combining these into a single log-linear model
with 11 weights, ?1 to ?11, which we group
into the vector ?
? defining a search algorithm for finding the
alignment of highest probability given the sub-
models and ?
? devising a method for finding a ? which min-
imizes alignment error given fixed submodels
and a set of gold standard alignments
? inventing a training method for alternating
steps which estimate the submodels by increas-
ing likelihood with steps which set ? to de-
crease alignment error
The submodels in our new alignment model are
listed in table 1, where for ease of exposition we
92
Table 1: Submodels used for alignment
1 t(fj |ei) TRANSLATION PROBABILITIES
2 n(?i|ei) FERTILITY PROBABILITIES, ?i IS THE NUMBER OF WORDS GENERATED BY THE ENGLISH WORD ei
3 null PARAMETERS USED IN GENERATING ROMANIAN WORDS FROM ENGLISH NULL WORD (INCLUDING p0, p1)
4 d1(4j) MOVEMENT (DISTORTION) PROBABILITIES OF FIRST ROMANIAN WORD GENERATED FROM ENGLISH WORD
5 d>1(4j) MOVEMENT (DISTORTION) PROBABILITIES OF OTHER ROMANIAN WORDS GENERATED FROM ENGLISH WORD
6 TTABLE ESTIMATED FROM INTERSECTION OF TWO STARTING ALIGNMENTS FOR THIS ITERATION
7 TRANSLATION TABLE FROM ENGLISH TO ROMANIAN MODEL 1 ITERATION 5
8 TRANSLATION TABLE FROM ROMANIAN TO ENGLISH MODEL 1 ITERATION 5
9 BACKOFF FERTILITY (FERTILITY ESTIMATED OVER ALL ENGLISH WORDS)
10 ZERO FERTILITY ENGLISH WORD PENALTY
11 NON-ZERO FERTILITY ENGLISH WORD PENALTY
consider English to be the source language and Ro-
manian the target language.
The log-linear alignment model is specified by
equation 3. The model assigns non-zero proba-
bilities only to 1-to-many alignments, like Model
4. (Cettolo and Federico, 2004) used a log-linear
model trained using error minimization for the trans-
lation task, 3 of the submodels were taken from
Model 4 in a similar way to our first 5 submodels.
p?(a, f |e) =
exp(?m ?mhm(f, a, e))
?
f,e,a exp(
?
m ?mhm(f, a, e))
(3)
Given ?, the alignment search problem is to find
the alignment a of highest probability according to
equation 3. We solve this using the local search de-
fined in (Brown et al, 1993).
We set ? as follows. Given a sequence A of align-
ments we can calculate an error function, E(A). For
these experiments average sentence AER was used.
We wish to minimize this error function, so we se-
lect ? accordingly:
argmin
?
?
a?
E(a?)?(a?, (argmax
a
p?(a, f |e))) (4)
Maximizing performance for all of the weights
at once is not computationally tractable, but (Och,
2003) has described an efficient one-dimensional
search for a similar problem. We search over each
?m (holding the others constant) using this tech-
nique to find the best ?m to update and the best value
to update it to. We repeat the process until no further
gain can be found.
Our new training method is:
REPEAT
? Start with submodels and lambda from previ-
ous iteration
? Find Viterbi alignments on entire training cor-
pus using new model (similar to E-step of
Model 4 training)
? Reestimate submodel parameters from Viterbi
alignments (similar to M-step of Model 4
Viterbi training)
? Find a setting for ? that reduces AER on dis-
criminative training set (new D-step)
We use the first 148 sentences of the 2003 test set
for the discriminative training set. 10 settings for ?
are found, the hypothesis list is augmented using the
results of 10 searches using these settings, and then
another 10 settings for ? are found. We then select
the best ?. The discriminative training regimen is
otherwise similar to (Och, 2003).
5 Experiments
Table 2 provides a comparison of our baseline sys-
tems using the ?refined? symmetrization metric with
the best limited resources track system from WPT03
(Dejean et al, 2003) on the 2003 test set. The best
results are obtained by stemming both English and
Romanian words to the first four letters, as described
in section 2.
Table 3 provides details on our shared task sub-
mission. RUN1 is the word-based baseline system.
RUN2 is the stem-based baseline system. RUN4
uses only the first 6 submodels, while RUN5 uses
all 11 submodels. RUN3 had errors in processing,
so we omit it.
Results:
? Our new 1-to-many alignment model and train-
ing method are successful, producing decreases
of 0.03 AER when the source is Romanian, and
0.01 AER when the source is English.
93
Table 2: Summary of results for 2003 test set
SYSTEM STEM SIZES AER
XEROX ?NOLEM-ER-56K? 0.289
BASELINE NO PROCESSING 0.284
BASELINE ENG PORTER / ROM 4 0.251
BASELINE ENG 4 / ROM 4 0.248
Table 3: Full results on shared task submissions (blind test 2005)
RUN NAMES STEM SIZES SOURCE ROM SOURCE ENG UNION INTERSECTION REFINED
ISI.RUN1 NO PROCESSING 0.3834 0.3589 0.3590 0.3891 0.3165
ISI.RUN2 ENG 4 / ROM 4 0.3056 0.2880 0.2912 0.3041 0.2675
ISI.RUN4 ENG 4 / ROM 4 0.2798 0.2833 0.2773 0.2862 0.2663
ISI.RUN5 ENG 4 / ROM 4 0.2761 0.2778 0.2736 0.2807 0.2655
? These decreases do not translate to a large im-
provement in the end-to-end task of producing
many-to-many alignments with a balanced pre-
cision and recall. We had a very small decrease
of 0.002 AER using the ?refined? heuristic.
? The many-to-many alignments produced using
?union? and the 1-to-1 alignments produced us-
ing ?intersection? were also improved.
? It may be a problem that we trained p0 using
likelihood (it is in submodel 3) rather than op-
timizing p0 discriminatively as we did for the
baseline.
6 Conclusion
? Considering multiple stemming possibilities
for each word seems important.
? Alternating between increasing likelihood and
decreasing error rate is a useful training ap-
proach which can be used for many problems.
? Our model and training method improve upon a
strong baseline for producing 1-to-many align-
ments.
? Our model and training method can be used
with the ?intersection? heuristic to produce
higher quality 1-to-1 alignments
? Models which can directly model many-to-
many alignments and do not require heuristic
symmetrization are needed to produce higher
quality many-to-many alignments. Our train-
ing method can be used to train them.
7 Acknowledgments
This work was supported by DARPA-ITO grant
NN66001-00-1-9814 and NSF grant IIS-0326276.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Mauro Cettolo and Marcello Federico. 2004. Minimum er-
ror training of log-linear translation models. In Proc. of the
International Workshop on Spoken Language Translation,
pages 103?106, Kyoto, Japan.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji Yamada.
2003. Reducing parameter space for word alignment. In
HLT-NAACL 2003 Workshop on Building and Using Paral-
lel Texts: Data Driven Machine Translation and Beyond, Ed-
monton, Alberta, July.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
M. F. Porter. 1997. An algorithm for suffix stripping. In Read-
ings in information retrieval, pages 313?316, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Monica Rogati, Scott McCarley, and Yiming Yang. 2003. Un-
supervised learning of arabic stemming using a parallel cor-
pus. In Proc. of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), Sapporo, Japan, July.
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, pages 836?841, Copenhagen, Denmark, August.
94
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 746?754, Prague, June 2007. c?2007 Association for Computational Linguistics
Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy
Wei Wang and Kevin Knight and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA, 90292
{wwang,kknight,dmarcu}@languageweaver.com
Abstract
We show that phrase structures in Penn Tree-
bank style parses are not optimal for syntax-
based machine translation. We exploit a se-
ries of binarization methods to restructure
the Penn Treebank style trees such that syn-
tactified phrases smaller than Penn Treebank
constituents can be acquired and exploited in
translation. We find that by employing the
EM algorithm for determining the binariza-
tion of a parse tree among a set of alternative
binarizations gives us the best translation re-
sult.
1 Introduction
Syntax-based translation models (Eisner, 2003; Gal-
ley et al, 2006; Marcu et al, 2006) are usually built
directly from Penn Treebank (PTB) (Marcus et al,
1993) style parse trees by composing treebank gram-
mar rules. As a result, often no substructures corre-
sponding to partial PTB constituents are extracted to
form translation rules.
Syntax translation models acquired by composing
treebank grammar rules assume that long rewrites
are not decomposable into smaller steps. This ef-
fectively restricts the generalization power of the in-
duced model. For example, suppose we have an
xRs (Knight and Graehl, 2004) rule R1 in Figure 1
that translates the Chinese phrase RUSSIA MINISTER
VIKTOR-CHERNOMYRDIN into an English NPB tree
fragment yielding an English phrase. Also suppose
that we want to translate a Chinese phrase
VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE
into English. What we desire is that if we have
another rule R2 as shown in Figure 1, we could
somehow compose it with R1 to obtain the desir-
able translation. We unfortunately cannot do this
because R1 and R2 are not further decomposable
and their substructures cannot be re-used. The re-
quirement that all translation rules have exactly one
root node does not enable us to use the translation of
VIKTOR-CHERNOMYRDIN in any other contexts than
those seen in the training corpus.
A solution to overcome this problem is to right-
binarize the left-hand side (LHS) (or the English-
side) tree of R1 such that we can decompose
R1 into R3 and R4 by factoring NNP(viktor)
NNP(chernomyrdin) out as R4 according to the
word alignments; and left-binarize the LHS of R2 by
introducing a new tree node that collapses the two
NNP?s, so as to generalize this rule, getting rule R5
and rule R6. We also need to consistently syntact-
ify the root labels of R4 and the new frontier label
of R6 such that these two rules can be composed.
Since labeling is not a concern of this paper, we sim-
ply label new nodes with X-bar where X here is the
parent label. With all these in place, we now can
translate the foreign sentence by composing R6 and
R4 in Figure 1.
Binarizing the syntax trees for syntax-based ma-
chine translation is similar in spirit to generalizing
parsing models via markovization (Collins, 1997;
Charniak, 2000). But in translation modeling, it is
unclear how to effectively markovize the translation
rules, especially when the rules are complex like
those proposed by Galley et al (2006).
In this paper, we explore the generalization abil-
ity of simple binarization methods like left-, right-,
and head-binarization, and also their combinations.
Simple binarization methods binarize syntax trees
in a consistent fashion (left-, right-, or head-) and
746
NPB
JJ         NNP      NNP     NNP
russia    minister    viktor   chernomyrdin
RUSSIA   MINISTER      V?C
NPB
?
NNP         NNP      AND  HIS   COLLEAGUE
V?C AND  HIS COLLEAGUE
NPB
JJ
russia
RUSSIA
minister
MINISTER NNP   NNP
viktor  chernomyrdin
V?C
NPB
colleague
and    his     colleague
COLLEAGUE
PRB$
his
HIS
CC
and
ANDNNP      NNP
x0:NNP  x1:NNP V?C AND  HIS COLLEAGUE
NNS
x0:NNP   x1:NNP   CC   PRB$   NNS
R1 R2
NPB
NPB
R3
NPB
NPBR4 R5
R6
R6
R4
NPB
Figure 1: Generalizing translation rules by binarizing trees.
thus cannot guarantee that all the substructures can
be factored out. For example, right binarization on
the LHS of R1 makes available R4, but misses R6
on R2. We then introduce a parallel restructuring
method, that is, one can binarize both to the left and
right at the same time, resulting in a binarization for-
est. We employ the EM (Dempster et al, 1977) algo-
rithm to learn the binarization bias for each tree node
from the parallel alternatives. The EM-binarization
yields best translation performance.
The rest of the paper is organized as follows.
Section 2 describes related research. Section 3 de-
fines the concepts necessary for describing the bina-
rizations methods. Section 4 describes the tree bina-
rization methods in details. Section 5 describes the
forest-based rule extraction algorithm, and section 6
explains how we restructure the trees using the EM
algorithm. The last two sections are for experiments
and conclusions.
2 Related Research
Several researchers (Melamed et al, 2004; Zhang
et al, 2006) have already proposed methods for bi-
narizing synchronous grammars in the context of
machine translation. Grammar binarization usually
maintains an equivalence to the original grammar
such that binarized grammars generate the same lan-
guage and assign the same probability to each string
as the original grammar does. Grammar binarization
is often employed to make the grammar fit in a CKY
parser. In our work, we are focused on binarization
of parse trees. Tree binarization generalizes the re-
sulting grammar and changes its probability distri-
bution. In tree binarization, synchronous grammars
built from restructured (binarized) training trees still
contain non-binary, multi-level rules and thus still
require the binarization transformation so as to be
employed by a CKY parser.
The translation model we are using in this paper
belongs to the xRs formalism (Knight and Graehl,
2004), which has been proved successful for ma-
chine translation in (Galley et al, 2004; Galley et
al., 2006; Marcu et al, 2006).
3 Concepts
We focus on tree-to-string (in noisy-channel model
sense) translation models. Translation models of
this type are typically trained on tuples of a source-
language sentence f, a target language (e.g., English)
parse tree pi that yields e and translates from f, and
the word alignments a between e and f. Such a tuple
is called an alignment graph in (Galley et al, 2004).
The graph (1) in Figure 2 is such an alignment graph.
747
(1) unbinarized tree
  
NPB
viktor chernomyrdin
VIKTOR?CHERNOMYRDIN
NNP1    NNP2   NNP3   NNP4*
(2) left-binarization (3) right-/head-binarization
NPB
NPB
NNP1 NNP2 NNP3
viktor
NNP?4
chernomyrdin
NPB
NNP1 NPB?
NNP2 NNP3
viktor
NNP4?
chernomyrdin
(4) left-binarization (5) right-binarization (6) left-binarization (7) right-/head-binarization
NPB
NPB
NPB
NNP1 NNP2
NNP3
viktor
NNP4?
chernomyrdin
- -
NPB
NNP1 NPB?
NNP2 NPB?
NNP3
viktor
NNP4?
chernomyrdin
Figure 2: Left, right, and head binarizations. Heads are marked with ??s. New nonterminals introduced by binarization are
denoted by X-bars.
A tree node in pi is admissible if the f string cov-
ered by the node is contiguous but not empty, and
if the f string does not align to any e string that is
not covered by pi. An xRs rule can be extracted only
from an admissible tree node, so that we do not have
to deal with dis-contiguous f spans in decoding (or
synchronous parsing). For example, in tree (2) in
Figure 2, node NPB is not admissible because the
f string that the node covers also aligns to NNP4,
which is not covered by the NPB. Node NPB in tree
(3), on the other hand, is admissible.
A set of sibling tree nodes is called factorizable
if we can form an admissible new node dominating
them. For example, in tree (1) of Figure 2, sibling
nodes NNP2 NNP3 and NNP4 are factorizable be-
cause we can factorize them out and form a new
node NPB, resulting in tree (3). Sibling tree nodes
NNP1 NNP2 and NNP3 are not factorizable. In syn-
chronous parse trees, not all sibling nodes are fac-
torizable, thus not all sub-phrases can be acquired
and syntactified. The main purpose of our paper is
to restructure parse trees by factorization such that
syntactified sub-phrases can be employed in transla-
tion.
4 Binarizing Syntax Trees
We are going to binarize a tree node n that domi-
nates r children n1, ..., nr. Restructuring will be
performed by introducing new tree nodes to domi-
nate a subset of the children nodes. To avoid over-
generalization, we allow ourselves to form only one
new node at a time. For example, in Figure 2, we
can binarize tree (1) into tree (2), but we are not
allowed to form two new nodes, one dominating
NNP1 NNP2 and the other dominating NNP3 NNP4.
Since labeling is not the concern of this paper, we re-
label the newly formed nodes as n.
4.1 Simple binarization methods
The left binarization of node n (i.e., the NPB in
tree (1) of Figure 2) factorizes the leftmost r ? 1
children by forming a new node n (i.e., NPB in
tree (2)) to dominate them, leaving the last child
nr untouched; and then makes the new node n the
left child of n. The method then recursively left-
binarizes the newly formed node n until two leaves
are reached. In Figure 2, we left-binarize tree (1)
into (2) and then into (4).
The right binarization of node n factorizes the
rightmost r ? 1 children by forming a new node n
(i.e., NPB in tree (3)) to dominate them, leaving the
748
first child n1 untouched; and then makes the new
node n the right child of n. The method then recur-
sively right-binarizes the newly formed node n. In
Figure 2, we right-binarize tree (1) into (3) and then
into (7).
The head binarization of node n left-binarizes
n if the head is the first child; otherwise, right-
binarizes n. We prefer right-binarization to left-
binarization when both are applicable under the head
restriction because our initial motivation was to gen-
eralize the NPB-rooted translation rules. As we will
show in the experiments, binarization of other types
of phrases contribute to the translation accuracy im-
provement as well.
Any of these simple binarization methods is easy
to implement, but is incapable of giving us all the
factorizable sub-phrases. Binarizing all the way to
the left, for example, from tree (1) to tree (2) and to
tree (4) in Figure 2, does not enable us to acquire a
substructure that yields NNP3 NNP4 and their trans-
lational equivalences. To obtain more factorizable
sub-phrases, we need to parallel-binarize in both di-
rections.
4.2 Parallel binarization
Simple binarizations transform a parse tree into an-
other single parse tree. Parallel binarization will
transform a parse tree into a binarization forest,
desirably packed to enable dynamic programming
when extracting translation rules from it.
Borrowing terms from parsing semirings (Good-
man, 1999), a packed forest is composed of addi-
tive forest nodes (?-nodes) and multiplicative forest
nodes (?-nodes). In the binarization forest, a ?-
node corresponds to a tree node in the unbinarized
tree; and this ?-node composes several ?-nodes,
forming a one-level substructure that is observed in
the unbinarized tree. A ?-node corresponds to al-
ternative ways of binarizing the same tree node in
the unbinarized tree and it contains one or more ?-
nodes. The same ?-node can appear in more than
one place in the packed forest, enabling sharing.
Figure 3 shows a packed forest obtained by pack-
ing trees (4) and (7) in Figure 2 via the following
parallel binarization algorithm.
To parallel-binarize a tree node n that has children
n1, ..., nr , we employ the following steps:
?1(NPB)
?2(NPB)
?3(NPB)
?4(NPB)
?5(NPB)
?6(NPB)
?7(NNP1) ?8(NNP2)
?9(NNP3)
?10(NNP4)
?11(NPB)
?7(NNP1) ?12(NPB)
?13(NPB)
?8(NNP2) ?14(NPB)
?15(NPB)
?9(NNP3) ?10(NNP4)
Figure 3: Packed forest obtained by packing trees (4) and (7)
in Figure 2
? We recursively parallel-binarize children nodes
n1, ..., nr, producing binarization ?-nodes
?(n1), ..., ?(nr), respectively.
? We right-binarize n, if any contiguous1 subset
of children n2, ..., nr is factorizable, by intro-
ducing an intermediate tree node labeled as n.
We recursively parallel-binarize n to generate
a binarization forest node ?(n). We form a
multiplicative forest node ?R as the parent of
?(n1) and ?(n).
? We left-binarize n if any contiguous subset
of n1, ..., nr?1 is factorizable and if this sub-
set contains n1. Similar to the above right-
binarization, we introduce an intermediate tree
node labeled as n, recursively parallel-binarize
n to generate a binarization forest node ?(n),
form a multiplicative forest node ?L as the par-
ent of ?(n) and ?(n1).
? We form an additive node ?(n) as the parent
of the two already formed multiplicative nodes
?L and ?R.
The (left and right) binarization conditions con-
sider any subset to enable the factorization of small
constituents. For example, in tree (1) of Figure 2,
although NNP1 NNP2 NNP3 of NPB are not factor-
izable, the subset NNP1 NNP2 is factorizable. The
binarization from tree (1) to tree (2) serves as a re-
laying step for us to factorize NNP1 NNP2 in tree
(4). The left-binarization condition is stricter than
1We factorize only subsets that cover contiguous spans to
avoid introducing dis-contiguous constituents for practical pur-
pose. In principle, the algorithm works fine without this bina-
rization condition.
749
the right-binarization condition to avoid spurious bi-
narization; i.e., to avoid the same subconstituent be-
ing reached via both binarizations. We could trans-
form tree (1) directly into tree (4) without bother-
ing to generate tree (3). However, skipping tree (3)
will create us difficulty in applying the EM algo-
rithm to choose a better binarization for each tree
node, since tree (4) can neither be classified as left
binarization nor as right binarization of the original
tree (1) ? it is the result of the composition of two
left-binarizations.
In parallel binarization, nodes are not always bi-
narizable in both directions. For example, we do not
need to right-binarize tree (2) because NNP2 NNP3
are not factorizable, and thus cannot be used to form
sub-phrases. It is still possible to right-binarize tree
(2) without affecting the correctness of the parallel
binarization algorithm, but that will spuriously in-
crease the branching factor of the search for the rule
extraction, because we will have to expand more tree
nodes.
A restricted version of parallel binarization is the
headed parallel binarization, where both the left and
the right binarization must respect the head propaga-
tion property at the same time.
A nice property of parallel binarization is that
for any factorizable substructure in the unbinarized
tree, we can always find a corresponding admissi-
ble ?-node in the parallel-binarized packed forest.
A leftmost substructure like the lowest NPB-subtree
in tree (4) of Figure 2 can be made factorizable
by several successive left binarizations, resulting in
?5(NPB)-node in the packed forest in Figure 3. A
substructure in the middle can be factorized by the
composition of several left- and right-binarizations.
Therefore, after a tree is parallel-binarized, to make
the sub-phrases available to the MT system, all we
need to do is to extract rules from the admissible
nodes in the packed forest. Rules that can be ex-
tracted from the original unrestructured tree can be
extracted from the packed forest as well.
Parallel binarization results in parse forests. Thus
translation rules need to be extracted from training
data consisting of (e-forest, f, a)-tuples.
5 Extracting translation rules from
(e-forest, f, a)-tuples
The algorithm to extract rules from (e-forest, f, a)-
tuples is a natural generalization of the (e-parse, f,
a)-based rule extraction algorithm in (Galley et al,
2006). The input to the forest-based algorithm is a
(e-forest, f, a)-triple. The output of the algorithm is
a derivation forest (Galley et al, 2006) composed of
xRs rules. The algorithm recursively traverses the e-
forest top-down and extracts rules only at admissible
forest nodes.
The following procedure transforms the packed e-
forest in Figure 3 into a packed synchronous deriva-
tion in Figure 4.
Condition 1: Suppose we reach an additive
e-forest node, e.g. ?1(NPB) in Figure 3. For
each of ?1(NPB)?s children, e-forest nodes
?2(NPB) and ?11(NPB), we go to condi-
tion 2 to recursively extract rules on these
two e-forest nodes, generating multiplicative
derivation forest nodes, i.e., ?(NPB(NPB :
x0 NNP3(viktor) NNP4(chernomyrdin)4) ?
x0 V-C) and ?(NPB(NNP1 NPB(NNP2 : x0 NPB :
x1)) ? x0 x1 x2) in Figure 4. We make these
new ? nodes children of ?(NPB) in the derivation
forest.
Condition 2: Suppose we reach a multiplicative
parse forest node, i.e., ?11(NPB) in Figure 3. We
extract rules rooted at it using the procedure in
(Galley et al, 2006), forming multiplicative deriva-
tion forest nodes, i.e., ?(NPB(NNP1 NPB(NNP2 :
x0 NPB : x1)) ? x0 x1 x2) We then go
to condition 1 to form the derivation forest on
the additive frontier e-forest nodes of the newly
extracted rules, generating additive derivation for-
est nodes, i.e., ?(NNP1), ?(NNP2) and ?(NPB).
We make these ? nodes the children of node
?(NPB(NNP1 NPB(NNP2 : x0 NPB : x1)) ?
x0 x1 x2) in the derivation forest.
This algorithm is a natural extension of the extrac-
tion algorithm in (Galley et al, 2006) in the sense
that we have an extra condition (1) to relay rule ex-
traction on additive e-forest nodes.
It is worthwhile to eliminate the spuriously am-
biguous rules that are introduced by the parallel bi-
750
?(NPB)
?
(
NPB(NPB : x0 NNP(viktor) NNP(chernomyrdin)) ? x0 V-C
)
?(NPB)
?
(
NPB(NNP : x0 NNP : x1 ? x0 x1)
)
?
(
NPB(NNP : x0 NPB(NNP : x1 NPB : x2)) ? x0 x1 x2
)
?(NNP) ?(NNP) ?(NPB)
?
(
NPB(NNP(viktor) NNP(chernomyrdin)) ? V-C)
Figure 4: Derivation forest.
narization. For example, we may extract the follow-
ing two rules:
- A(A(B:x0 C:x1)D:x2) ? x1 x0 x2
- A(B:x0 A(C:x1 D:x2)) ? x1 x0 x2
These two rules, however, are not really distinct.
They both converge to the following rules if we
delete the auxiliary nodes A.
- A(B:x0 C:x1 D:x2) ? x1 x0 x2
The forest-base rule extraction algorithm pro-
duces much larger grammars than the tree-based
one, making it difficult to scale to very large training
data. From a 50M-word Chinese-to-English parallel
corpus, we can extract more than 300 million trans-
lation rules, while the tree-based rule extraction al-
gorithm gives approximately 100 million. However,
the restructured trees from the simple binarization
methods are not guaranteed to give the best trees for
syntax-based machine translation. What we desire is
a binarization method that still produces single parse
trees, but is able to mix left binarization and right
binarization in the same tree. In the following, we
shall use the EM algorithm to learn the desirable bi-
narization on the forest of binarization alternatives
proposed by the parallel binarization algorithm.
6 Learning how to binarize via the EM
algorithm
The basic idea of applying the EM algorithm to
choose a restructuring is as follows. We perform a
set {?} of binarization operations on a parse tree ? .
Each binarization ? is the sequence of binarizations
on the necessary (i.e., factorizable) nodes in ? in pre-
order. Each binarization ? results in a restructured
tree ?? . We extract rules from (?? , f, a), generating a
translation model consisting of parameters (i.e., rule
e?parse
  (Galley et al, 2006)
composed rule extraction
1
2
parallel binarization e?forest
forest?based rule extraction
         of minimal rules
f,a
synchronous derivation forests
EM
3
4
viterbi derivationsproject e?parse
model
syntax translation 
Figure 5: Using the EM algorithm to choose restructuring.
probabilities) ?. Our aim is to obtain the binarization
?? that gives the best likelihood of the restructured
training data consisting of (?? , f , a)-tuples. That is
?? = arg max
?
p(??, f ,a|??) (1)
In practice, we cannot enumerate all the exponen-
tial number of binarized trees for a given e-parse.
We therefore use the packed forest to store all the
binarizations that operate on an e-parse in a com-
pact way, and then use the inside-outside algorithm
(Lari and Young, 1990; Knight and Graehl, 2004)
for model estimation.
The probability p(??, f ,a) of a (?? , f, a)-tuple
is what the basic syntax-based translation model is
concerned with. It can be further computed by ag-
gregating the rule probabilities p(r) in each deriva-
tion ? in the set of all derivations ? (Galley et al,
2004; Marcu et al, 2006). That is
p(??, f ,a) =
?
???
?
r??
p(r) (2)
Since it has been well-known that applying EM
with tree fragments of different sizes causes over-
fitting (Johnson, 1998), and since it is also known
that syntax MT models with larger composed rules
in the mix significantly outperform rules that min-
imally explain the training data (minimal rules) in
751
translation accuracy (Galley et al, 2006), we decom-
pose p(?b, f ,a) using minimal rules during running
of the EM algorithm, but, after the EM restructuring
is finished, we build the final translation model using
composed rules for evaluation.
Figure 5 is the actual pipeline that we use for
EM binarization. We first generate a packed e-forest
via parallel binarization. We then extract minimal
translation rules from the (e-forest, f, a)-tuples, pro-
ducing synchronous derivation forests. We run the
inside-outside algorithm on the derivation forests
until convergence. We obtain the Viterbi derivations
and project the English parses from the derivations.
Finally, we extract composed rules using Galley et
al. (2006)?s (e-tree, f, a)-based rule extraction algo-
rithm. This procedure corresponds to the path 13?42
in the pipeline.
7 Experiments
We carried out a series of experiments to compare
the performance of different binarization methods
in terms of BLEU on Chinese-to-English translation
tasks.
7.1 Experimental setup
Our bitext consists of 16M words, all in the
mainland-news domain. Our development set is a
925-line subset of the 993-line NIST02 evaluation
set. We removed long sentences from the NIST02
evaluation set to speed up discriminative training.
The test set is the full 919-line NIST03 evaluation
set.
We used a bottom-up, CKY-style decoder that
works with binary xRs rules obtained via a syn-
chronous binarization procedure (Zhang et al,
2006). The decoder prunes hypotheses using strate-
gies described in (Chiang, 2007).
The parse trees on the English side of the bitexts
were generated using a parser (Soricut, 2004) imple-
menting the Collins parsing models (Collins, 1997).
We used the EM procedure described in (Knight
and Graehl, 2004) to perform the inside-outside al-
gorithm on synchronous derivation forests and to
generate the Viterbi derivation forest.
We used the rule extractor described in (Galley et
al., 2006) to extract rules from (e-parse, f, a)-tuples,
but we made an important modification: new nodes
introduced by binarization will not be counted when
computing the rule size limit unless they appear as
the rule roots. The motivation is that binarization
deepens the parses and increases the number of tree
nodes. In (Galley et al, 2006), a composed rule
is extracted only if the number of internal nodes it
contains does not exceed a limit (i.e., 4), similar
to the phrase length limit in phrase-based systems.
This means that rules extracted from the restructured
trees will be smaller than those from the unrestruc-
tured trees, if the X nodes are deleted. As shown in
(Galley et al, 2006), smaller rules lose context, and
thus give lower translation performance. Ignoring X
nodes when computing the rule sizes preserves the
unstructured rules in the resulting translation model
and adds substructures as bonuses.
7.2 Experiment results
Table 1 shows the BLEU scores of mixed-cased and
detokenized translations of different systems. We
see that all the binarization methods improve the
baseline system that does not apply any binarization
algorithm. The EM-binarization performs the best
among all the restructuring methods, leading to 1.0
BLEU point improvement. We also computed the
bootstrap p-values (Riezler and Maxwell, 2005) for
the pairwise BLEU comparison between the base-
line system and any of the system trained from bina-
rized trees. The significance test shows that the EM
binarization result is statistically significant better
than the baseline system (p > 0.005), even though
the baseline is already quite strong. To our best
knowledge, 37.94 is the highest BLEU score on this
test set to date.
Also as shown in Table 1, the grammars trained
from the binarized training trees are almost two
times of the grammar size with no binarization. The
extra rules are substructures factored out by these bi-
narization methods.
How many more substructures (or translation
rules) can be acquired is partially determined by
how many more admissible nodes each binariza-
tion method can factorize, since rules are extractable
only from admissible tree nodes. According to
Table 1, binarization methods significantly increase
the number of admissible nodes in the training trees.
The EM binarization makes available the largest
752
EXPERIMENT NIST03-BLEU # RULES # ADMISSIBLE NODES IN TRAINING
no-bin 36.94 63.4M 7,995,569
left binarization 37.47 (p = 0.047) 114.0M 10,463,148
right binarization 37.49 (p = 0.044) 113.0M 10,413,194
head binarization 37.54 (p = 0.086) 113.8M 10,534,339
EM binarization 37.94 (p = 0.0047) 115.6M 10,658,859
Table 1: Translation performance, grammar size and # admissible nodes versus binarization algorithms. BLEU scores are for
mixed-cased and detokenized translations, as we usually do for NIST MT evaluations.
nonterminal left-binarization right-binarization
NP 96.97% 3.03%
NP-C 97.49% 2.51%
NPB 0.25% 99.75%
VP 93.90% 6.10%
PP 83.75% 16.25%
ADJP 87.83% 12.17%
ADVP 82.74% 17.26%
S 85.91% 14.09%
S-C 18.88% 81.12%
SBAR 96.69% 3.31%
QP 86.40% 13.60%
PRN 85.18% 14.82%
WHNP 97.93% 2.07%
NX 100% 0
SINV 87.78% 12.22%
PRT 100% 0
SQ 93.53% 6.47%
CONJP 18.08% 81.92%
Table 2: Binarization bias learned by EM.
number of admissible nodes, and thus results in the
most rules.
The EM binarization factorizes more admissible
nodes because it mixes both left and right binariza-
tions in the same tree. We computed the binarization
biases learned by the EM algorithm for each nonter-
minal from the binarization forest of headed-parallel
binarizations of the training trees, getting the statis-
tics in Table 2. Of course, the binarization bias
chosen by left-/right-binarization methods would be
100% deterministic. One noticeable message from
Table 2 is that most of the categories are actually bi-
ased toward left-binarization, although our motivat-
ing example in our introduction section is for NPB,
which needed right binarization. The main reason
might be that the head sub-constituents of most cat-
egories tend to be on the left, but according to the
performance comparison between head binarization
and EM binarization, head binarization does not suf-
fice because we still need to choose the binarization
between left and right if they both are head binariza-
tions.
8 Conclusions
In this paper, we not only studied the impact of
simple tree binarization algorithms on the perfor-
mance of end-to-end syntax-based MT, but also pro-
posed binarization methods that mix more than one
simple binarization in the binarization of the same
parse tree. Binarizing a tree node whether to the left
or to the right was learned by employing the EM
algorithm on a set of alternative binarizations and
by choosing the Viterbi one. The EM binarization
method is informed by word alignments such that
unnecessary new tree nodes will not be ?blindly? in-
troduced.
To our best knowledge, our research is the first
work that aims to generalize a syntax-based trans-
lation model by restructuring and achieves signifi-
cant improvement on a strong baseline. Our work
differs from traditional work on binarization of syn-
chronous grammars in that we are not concerned
with the equivalence of the binarized grammar to the
original grammar, but intend to generalize the orig-
inal grammar via restructuring of the training parse
trees to improve translation performance.
Acknowledgments
The authors would like to thank David Chiang,
Bryant Huang, and the anonymous reviewers for
their valuable feedbacks.
References
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Seattle, May.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
753
35th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 16?23, Madrid, Spain,
July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 205?208, Sapporo,
July.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a Translation Rule? In Proceedings of
the Human Language Technology Conference and the
North American Association for Computational Lin-
guistics (HLT-NAACL), Boston, Massachusetts.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference and
Training of Context-Rich Syntactic Models. In Pro-
ceedings of the 44th Annual Meeting of the Association
for Computational Linguistics (ACL).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
M. Johnson. 1998. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):71?76.
K. Knight and J. Graehl. 2004. Training Tree Transduc-
ers. In Proceedings of NAACL-HLT.
K. Lari and S. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech and Language, pages 35?56.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phraases.
In Proceedings of EMNLP-2006, pp. 44-52, Sydney,
Australia.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proc. ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summa-
rization.
Radu Soricut. 2004. A reimplementation of Collins?s
parsing models. Technical report, Information Sci-
ences Institute, Department of Computer Science Uni-
versity of Southern California.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the HLT-NAACL.
754
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 755?763, Prague, June 2007. c?2007 Association for Computational Linguistics
What Can Syntax-based MT Learn from Phrase-based MT?
Steve DeNeefe and Kevin Knight
Information Sciences Institute
The Viterbi School of Engineering
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{sdeneefe,knight}@isi.edu
Wei Wang and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
{wwang,dmarcu}@languageweaver.com
Abstract
We compare and contrast the strengths
and weaknesses of a syntax-based machine
translation model with a phrase-based ma-
chine translation model on several levels.
We briefly describe each model, highlight-
ing points where they differ. We include a
quantitative comparison of the phrase pairs
that each model has to work with, as well
as the reasons why some phrase pairs are
not learned by the syntax-based model. We
then evaluate proposed improvements to the
syntax-based extraction techniques in light
of phrase pairs captured. We also compare
the translation accuracy for all variations.
1 Introduction
String models are popular in statistical machine
translation. Approaches include word substitution
systems (Brown et al, 1993), phrase substitution
systems (Koehn et al, 2003; Och and Ney, 2004),
and synchronous context-free grammar systems (Wu
and Wong, 1998; Chiang, 2005), all of which train
on string pairs and seek to establish connections be-
tween source and target strings. By contrast, ex-
plicit syntax approaches seek to directly model the
relations learned from parsed data, including models
between source trees and target trees (Gildea, 2003;
Eisner, 2003; Melamed, 2004; Cowan et al, 2006),
source trees and target strings (Quirk et al, 2005;
Huang et al, 2006), or source strings and target trees
(Yamada and Knight, 2001; Galley et al, 2004).
It is unclear which of these important pursuits will
best explain human translation data, as each has ad-
vantages and disadvantages. A strength of phrase
models is that they can acquire all phrase pairs con-
sistent with computed word alignments, snap those
phrases together easily by concatenation, and re-
order them under several cost models. An advan-
tage of syntax-based models is that outputs tend to
be syntactically well-formed, with re-ordering influ-
enced by syntactic context and function words intro-
duced to serve specific syntactic purposes.
A great number of MT models have been re-
cently proposed, and other papers have gone over the
expressive advantages of syntax-based approaches.
But it is rare to see an in-depth, quantitative study
of strengths and weaknesses of particular models
with respect to each other. This is important for a
scientific understanding of how these models work
in practice. Our main novel contribution is a com-
parison of phrase-based and syntax-based extraction
methods and phrase pair coverage. We also add to
the literature a new method of improving that cover-
age. Additionally, we do a careful study of several
syntax-based extraction techniques, testing whether
(and how much) they affect phrase pair coverage,
and whether (and how much) they affect end-to-end
MT accuracy. The MT accuracy tests are needed
because we want to see the individual effects of par-
ticular techniques under the same testing conditions.
For this comparison, we choose a previously estab-
lished statistical phrase-based model (Och and Ney,
2004) and a previously established statistical string-
to-tree model (Galley et al, 2004). These two mod-
els are chosen because they are the basis of two of
the most successful systems in the NIST 2006 MT
755
evaluation1.
2 Phrase-based Extraction
The Alignment Template system (ATS) described by
Och and Ney (2004) is representative of statistical
phrase-based models. The basic unit of translation
is the phrase pair, which consists of a sequence of
words in the source language, a sequence of words
in the target language, and a vector of feature val-
ues which describe this pair?s likelihood. Decod-
ing produces a string in the target language, in or-
der, from beginning to end. During decoding, fea-
tures from each phrase pair are combined with other
features (e.g., re-ordering, language models) using a
log-linear model to compute the score of the entire
translation.
The ATS phrase extraction algorithm learns these
phrase pairs from an aligned, parallel corpus.
This corpus is conceptually a list of tuples of
<source sentence, target sentence, bi-directional
word alignments> which serve as training exam-
ples, one of which is shown in Figure 1.
Figure 1: a phrase-based training example
For each training example, the algorithm identi-
fies and extracts all pairs of <source sequence, tar-
get sequence> that are consistent with the align-
ments. It does this by first enumerating all source-
side word sequences up to a length limit L, and for
each source sequence, it identifies all target words
aligned to those source words. For example, in Fig-
ure 1, for the source phrase ? 	  , the target
words it aligns to are felt, obliged, and do.
These words, and all those between them, are the
proposed target phrase. If no words in the proposed
target phrase align to words outside of the source
phrase, then this phrase pair is extracted.
The extraction algorithm can also look to the left
and right of the proposed target phrase for neighbor-
ing unaligned words and extracts phrases. For ex-
ample, for the phrase pair ? 	  ? felt obliged,
1http://www.nist.gov/speech/tests/mt/
mt06eval official results.html
the word to is a neighboring unaligned word. It
constructs new target phrases by adding on con-
secutive unaligned words in both directions, and
extracts those in new pairs, too (e.g., ? 	  ?
felt obliged to). For efficiency reasons, imple-
mentations often skip this step.
Figure 2 shows the complete set of phrase pairs
up to length 4 that are extracted from the Figure 1
training example. Notice that no extracted phrase
pair contains the character ?. Because of the align-
ments, the smallest legal phrase pair, ? ? 	  
? i felt obliged to do my, is beyond the size
limit of 4, so it is not extracted in this example.
? ? felt
? 	  ? felt obliged
? 	   ? felt obliged to do
	  ? obliged
	   ? obliged to do
 ? do
 P ? part
 P ? ? part
 P ? . ? part .
? . ? .
. ? .
Figure 2: phrases up to length 4 extracted from the
example in Figure 1
Phrase pairs are extracted over the entire train-
ing corpus. Due to differing alignments, some
phrase pairs that cannot be learned from one exam-
ple may be learned from another. These pairs are
then counted, once for each time they are seen in a
training example, and these counts are used as the
basis for maximum likelihood probability features,
such as p(f |e) and p(e|f).
3 Syntax-based Extraction
The GHKM syntax-based extraction method for
learning statistical syntax-based translation rules,
presented first in (Galley et al, 2004) and expanded
on in (Galley et al, 2006), is similar to phrase-based
extraction in that it extracts rules consistent with
given word alignments. A primary difference is the
use of syntax trees on the target side, rather than se-
quences of words. The basic unit of translation is the
translation rule, consisting of a sequence of words
756
and variables in the source language, a syntax tree
in the target language having words or variables at
the leaves, and again a vector of feature values which
describe this pair?s likelihood. Translation rules can:
? look like phrase pairs with syntax decoration:
NPB(NNP(prime)
NNP(minister)
NNP(keizo)
NNP(obuchi))
? B ? ? ? D #
? carry extra contextual constraints:
VP(VBD(said)
x0:SBAR-C)
? ? x0
(according to this rule, ? can translate to
said only if some Chinese sequence to the
right of ? is translated into an SBAR-C)
? be non-constituent phrases:
VP(VBD(said)
SBAR-C(IN(that)
x0:S-C))
? ? x0
VP(VBD(pointed)
PRT(RP(out))
x0:SBAR-C)
? ? ? x0
? contain non-contiguous phrases, effectively
?phrases with holes?:
PP(IN(on)
NP-C(NPB(DT(the)
x0:NNP))
NN(issue))))
? ? x0 ?  ?
PP(IN(on)
NP-C(NPB(DT(the)
NN(issue))
x0:PP))
? ? x0 ?  ?
? be purely structural (no words):
S(x0:NP-C x1:VP)? x0 x1
? re-order their children:
NP-C(NPB(DT(the)
x0:NN)
PP(IN(of)
x1:NP-C))
? x1 { x0
Decoding with this model produces a tree in the
target language, bottom-up, by parsing the foreign
string using a CYK parser and a binarized rule set
(Zhang et al, 2006). During decoding, features from
each translation rule are combined with a language
model using a log-linear model to compute the score
of the entire translation.
The GHKM extractor learns translation rules from
an aligned parallel corpus where the target side has
been parsed. This corpus is conceptually a list of tu-
ples of <source sentence, target tree, bi-directional
word alignments> which serve as training exam-
ples, one of which is shown in Figure 3.
Figure 3: a syntax-based training example
For each training example, the GHKM extrac-
tor computes the set of minimally-sized translation
rules that can explain the training example while re-
maining consistent with the alignments. This is, in
effect, a non-overlapping tiling of translation rules
over the tree-string pair. If there are no unaligned
words in the source sentence, this is a unique set.
This set, ordered into a tree of rule applications, is
called the derivation tree of the training example.
Unlike the ATS model, there are no inherent size
limits, just the constraint that the rules be as small
as possible for the example.
Ignoring the unaligned ? for the moment, there
are seven minimal translation rules that are extracted
from the example in Figure 3, as shown in Fig-
ure 4. Notice that rule 6 is rather large and applies
to a very limited syntactic context. The only con-
stituent node that covers both i and my is the S,
so the rule rooted at S is extracted, with variables
for every branch below this top constituent that can
be explained by other rules. Note also that to be-
757
comes a part of this rule naturally. If the alignments
were not as constraining (e.g., if my was unaligned),
then instead of this one big rule many smaller rules
would be extracted, such as structural rules (e.g.,
VP(x0:VBD x1:VP-C)? x0 x1) and function word in-
sertion rules (e.g., VP(TO(to) x0:VP-C)? x0).
1. VBD(felt)? ?
2. VBN(obliged)? 	 
3. VB(do)? 
4. NN(part)?  P
5. PERIOD(.)? .
6. S(NP-C(NPB(PRP(I)))
VP(x0:VBD
VP-C(x1:VBN
SG-C(VP(TO(to)
VP-C(x2:VB
NP-C(NPB(PRP$(my)
x3:NN)))))))
x4:PERIOD)? ? x0 x1 x2 x3 x4
7. TOP(x0:S)? x0
Figure 4: rules extracted from training example
We ignored unaligned source words in the exam-
ple above. Galley et al (2004) attach the unaligned
source word to the highest possible location, in our
example, the S. Thus it is extracted along with our
large rule 6, changing the target language sequence
to ?? x0 x1 x2 x3 ? x4?. This treatment still re-
sults in a unique derivation tree no matter how many
unaligned words are present.
In Galley et al (2006), instead of a unique deriva-
tion tree, the extractor computes several derivation
trees, each with the unaligned word added to a dif-
ferent rule such that the data is still explained. For
example, for the tree-string pair in Figure 3, ?
could be added not only to rule 6, but alternatively
to rule 4 or 5, to make the new rules:
NN(part)?  P ?
PERIOD(.)? ? .
This results in three different derivations, one
with the ? character in rule 4 (with rules 5 and 6
as originally shown), another with the ? character
in rule 5 (with rules 4 and 6 as originally shown),
and lastly one with the ? character in rule 6 (with
rules 4 and 5 as originally shown) as in the origi-
nal paper (Galley et al, 2004). In total, ten different
rules are extracted from this training example.
As with ATS, translation rules are extracted and
counted over the entire training corpus, a count of
one for each time they appear in a training example.
These counts are used to estimate several features,
including maximum likelihood probability features
for p(etree, fwords|ehead), p(ewords|fwords), and
p(fwords|ewords).
4 Differences in Phrasal Coverage
Both the ATS model and the GHKM model extract
linguistic knowledge from parallel corpora, but each
has fundamentally different constraints and assump-
tions. To compare the models empirically, we ex-
tracted phrase pairs (for the ATS model) and transla-
tion rules (for the GHKM model) from parallel train-
ing corpora described in Table 1. The ATS model
was limited to phrases of length 10 on the source
side, and length 20 on the target side. A super-
set of the parallel data was word aligned by GIZA
union (Och and Ney, 2003) and EMD (Fraser and
Marcu, 2006). The English side of training data was
parsed using an implementation of Collins? model 2
(Collins, 2003).
Chinese Arabic
Document IDs LDC2003E07 LDC2004T17
LDC2003E14 LDC2004T18
LDC2005T06 LDC2005E46
# of segments 329,031 140,511
# of words in foreign corpus 7,520,779 3,147,420
# of words in English corpus 9,864,294 4,067,454
Table 1: parallel corpora used to train both models
Table 2 shows the total number of GHKM rules
extracted, and a breakdown of the different kinds
of rules. Non-lexical rules are those whose source
side is composed entirely of variables ? there are
no source words in them. Because of this, they
potentially apply to any sentence. Lexical rules
(their counterpart) far outnumber non-lexical rules.
Of the lexical rules, a rule is considered a phrasal
rule if its source side and the yield of its target
side contain exactly one contiguous phrase each, op-
tionally with one or more variables on either side
of the phrase. Non-phrasal rules include structural
rules, re-ordering rules, and non-contiguous phrases.
These rules are not easy to directly compare to any
phrase pairs from the ATS model, so we do not focus
on them here.
Phrasal rules can be directly compared to ATS
phrase pairs, the easiest way being to discard the
758
Statistic Chinese Arabic
total translation rules 2,487,110 662,037
non-lexical rules 110,066 15,812
lexical rules 2,377,044 646,225
phrasal rules 1,069,233 406,020
distinct GHKM-derived phrase pairs 919,234 352,783
distinct corpus-specific
GHKM-derived phrase pairs 203,809 75,807
Table 2: a breakdown of how many rules the
GHKM extraction algorithm produces, and how
many phrase pairs can be derived from them
syntactic context and look at the phrases contained
in the rules. The second to last line of Table 2 shows
the number of phrase pairs that can be derived from
the above phrasal rules. The number of GHKM-
derived phrase pairs is lower than the number of
phrasal rules because some rules represent the same
phrasal translation, but with different syntactic con-
texts. The last line of Table 2 shows the subset of
phrase pairs that contain source phrases found in our
development corpus.
Table 3 compares these corpus-specific GHKM-
derived phrase pairs with the corpus-specific ATS
phrase pairs. Note that the number of phrase pairs
derived from the GHKM rules is less than the num-
ber of phrase pairs extracted by ATS. Moreover, only
slightly over half of the phrase pairs extracted by the
ATS model are common to both models. The lim-
its and constraints of each model are responsible for
this difference in contiguous phrases learned.
Source of phrase pairs Chinese Arabic
GHKM-derived 203,809 75,807
ATS 295,537 133,576
Overlap between models 160,901 75,038
GHKM only 42,908 769
ATS only 134,636 58,538
ATS-useful only 1,994 2,199
Table 3: comparison of corpus-specific phrase pairs
from each model
GHKM learns some contiguous phrase pairs that
the phrase-based extractor does not. Only a small
portion of these are due to the fact that the GHKM
model has no inherent size limit, while the phrase
based system has limits. More numerous are cases
where unaligned English words are not added to an
ATS phrase pair while GHKM adopts them at a syn-
tactically motivated location, or where a larger rule
contains mostly syntactic structure but happens to
have some unaligned words in it. For example, con-
sider Figure 5. Because basic and will are un-
aligned, ATS will learn no phrase pairs that translate
to these words alone, though they will be learned as
a part of larger phrases.
Figure 5: Situation where GHKM is able to learn
rules that translate into basic and will, but ATS
is not
GHKM, however, will learn several phrasal rules
that translate to basic, based on the syntactic con-
text
NPB(x0:DT
JJ(basic)
x1:NN)
? x0  x1
NPB(x0:DT
JJ(basic)
x1:NN)
? x0  ? ? x1
NPB(x0:DT
JJ(basic)
x1:NN)
? x0 ? ? x1
and one phrasal rule that translates into will
VP(MD(will)
x0:RB
x1:VP-C)
? x0 ? ? x1
The quality of such phrases may vary. For example,
the first translation of  (literally: ?one? or ?a?) to
basic above is a phrase pair of poor quality, while
the other two for basic and one for will are ar-
guably reasonable.
However, Table 3 shows that ATS was able to
learn many more phrase pairs that GHKM was not.
Even more significant is the subset of these missing
phrase pairs that the ATS decoder used in its best2
2i.e. highest scoring
759
translation of the corpus. According to the phrase-
based system these are the most ?useful? phrase
pairs and GHKM could not learn them. Since this is
a clear deficiency, we will focus on analyzing these
phrase pairs (which we call ATS-useful) and the rea-
sons they were not learned.
Table 4 shows a breakdown, categorizing each of
these missing ATS-useful phrase pairs and the rea-
sons they were not able to be learned. The most
common reason is straightforward: by extracting
only the minimally-sized rules, GHKM is unable to
learn many larger phrases that ATS learns. If GHKM
can make a word-level analysis, it will do that, at
the expense of a phrase-level analysis. Galley et
al. (2006) propose one solution to this problem and
Marcu et al (2006) propose another, both of which
we explore in Sections 5.1 and 5.2.
Category of missing ATS-useful phrase pairs Chinese Arabic
Not minimal 1,320 1,366
Extra target words in GHKM rules 220 27
Extra source words in GHKM rules 446 799
Other (e.g. parse failures) 8 7
Total missing useful phrase pairs 1,994 2,199
Table 4: reasons that ATS-useful phrase pairs could
not be extracted by GHKM as phrasal rules
The second reason is that the GHKM model is
sometimes forced by its syntactic constraints to in-
clude extra words. Sometimes this is only target lan-
guage words, and this is often useful ? the rules are
learning to insert these words in their proper context.
But most of the time, source language words are also
forced to be part of the rule, and this is harmful ? it
makes the rules less general. This latter case is often
due to poorly aligned target language words (such as
the ? in our Section 3 rule extraction example), or
unaligned words under large, flat constituents.
Another factor here: some of the phrase pairs are
learned by both systems, but GHKM is more specific
about the context of use. This can be both a strength
and a weakness. It is a strength when the syntactic
context helps the phrase to be used in a syntactically
correct way, as in
VP(VBD(said)
x0:SBAR-C)
? ? x0
where the syntax rule requires a constituent of type
SBAR-C. Conversely its weakness is seen when the
context is too constrained. For example, ATS can
easily learn the phrase
 ? ? prime minister
and is then free to use it in many contexts. But
GHKM learns 45 different rules, each that translate
this phrase pair in a unique context. Figure 6 shows
a sampling. Notice that though many variations are
present, the decoder is unable to use any of these
rules to produce certain noun phrases, such as ?cur-
rent Japanese Prime Minister Shinzo Abe?, because
no rule has the proper number of English modifiers.
NPB(NNP(prime) NNP(minister) x0:NNP)? x0  ?
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(x0:JJ NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(NNP(prime) NNP(minister) x0:NNP)?  ? x0
NPB(NNP(prime) NNP(minister))?  ?
NPB(NNP(prime) NNP(minister) x0:NNP x1:NNP)? x0 x1  ?
NPB(x0:DT x1:JJ JJ(prime) NN(minister))? x0 x1  ?
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)? x0  ? x1
Figure 6: a sampling of the 45 rules that translate
 ? to prime minister
5 Coverage Improvements
Each of the models presented so far has advantages
and disadvantages. In this section, we consider ideas
that make up for deficiencies in the GHKM model,
drawing our inspiration from the strong points of the
ATS model. We then measure the effects of each
idea empirically, showing both what is gained and
the potential limits of each modification.
5.1 Composed Rules
Galley et al (2006) proposed the idea of composed
rules. This removes the minimality constraint re-
quired earlier: any two or more rules in a parent-
child relationship in the derivation tree can be com-
bined to form a larger, composed rule. This change
is similar in spirit to the move from word-based to
phrase-based MT models, or parsing with a DOP
model (Bod et al, 2003) rather than a plain PCFG.
Because this results in exponential variations, a
size limit is employed: for any two or more rules
to be allowed to combine, the size of the resulting
rule must be at most n. The size of a rule is de-
fined as the number of non-part-of-speech, non-leaf
760
constituent labels in a rule?s target tree. For exam-
ple, rules 1-5 shown in Section 3 have a size of 0,
and rule 6 has a size of 10. Composed rules are ex-
tracted in addition to minimal rules, which means
that a larger n limit always results in a superset of
the rules extracted when a smaller n value is used.
When n is set to 0, then only minimal rules are ex-
tracted. Table 5 shows the growth in the number of
rules extracted for several size limits.
Size limit (n) Chinese Arabic
0 (minimal) 2,487,110 662,037
2 12,351,297 2,742,513
3 26,917,088 4,824,928
4 55,781,061 8,487,656
Table 5: increasing the size limit of composed rules
significantly increases the number of rules extracted
In our previous analysis, the main reason that
GHKM did not learn translations for ATS-useful
phrase pairs was due to its minimal-only approach.
Table 6 shows the effect that composed rule extrac-
tion has on the total number of ATS-useful phrases
missing. Note that as the allowed size of composed
rule increases, we are able to extract an greater per-
centage of the missing ATS-useful phrase pairs.
Size limit (n) Chinese Arabic
0 (minimal) 1,994 2,199
2 1,478 1,528
3 1,096 1,210
4 900 1,041
Table 6: number of ATS-useful phrases still missing
when using GHKM composed rule extraction
Unfortunately, a comparison of Tables 5 and 6 in-
dicates that the number of ATS-useful phrase pairs
gained is growing at a much slower rate than the total
number of rules. From a practical standpoint, more
rules means more processing work and longer de-
coding times, so there are diminishing returns from
continuing to explore larger size limits.
5.2 SPMT Model 1 Rules
An alternative for extracting larger rules called
SPMT model 1 is presented by Marcu et al (2006).
Though originally presented as a separate model,
the method of rule extraction itself builds upon the
minimal GHKM method just as composed rules do.
For each training example, the method considers all
source language phrases up to length L. For each of
these phrases, it extracts the smallest possible syn-
tax rule that does not violate the alignments. Ta-
ble 7 shows that this method is able to extract rules
that cover useful phrases, and can be combined with
size 4 composed rules to an even better effect. Since
there is some overlap in these methods, when com-
bining the two methods we eliminate any redundant
rules.
Method Chinese Arabic
composed alone (size 4) 900 1,041
SPMT model 1 alone 676 854
composed + SPMT model 1 663 835
Table 7: ATS-useful phrases still missing after dif-
ferent non-minimal methods are applied
Note that having more phrasal rules is not the only
advantage of composed rules. Here, combining both
composed and SPMT model 1 rules, our gain in use-
ful phrases is not very large, but we do gain addi-
tional, larger syntax rules. As discussed in (Galley
et al, 2006), composed rules also allow the learning
of more context, such as
ADJP(ADVP(RB(far)
CC(and)
RB(away)
x0:JJ)
? ? ? x0
This rule is not learned by SPMT model 1 because
it is not the smallest rule that can explain the phrase
pair, but it is still valuable for its syntactic context.
5.3 Restructuring Trees
Table 8 updates the causes of missing ATS-useful
phrase pairs. Most are now caused by syntactic con-
straints, thus we need to address these in some way.
GHKM translation rules are affected by large,
flat constituents in syntax trees, as in the prime
minister example earlier. One way to soften this
constraint is to binarize the trees, so that wide con-
stituents are broken down into multiple levels of tree
structure. The approach we take here is head-out bi-
narization (Wang et al, 2007), where any constituent
with more than two children is split into partial con-
stituents. The children to the left of the head word
761
Category of ATS-useful phrase pairs Chinese Arabic
Too large 12 9
Extra target words in GHKM rules 218 27
Extra source words in GHKM rules 424 792
Other (e.g. parse failures) 9 7
Total missing useful phrase pairs 663 835
Table 8: reasons that ATS-useful phrase pairs are
still not extracted as phrasal rules, with composed
and SPMT model 1 rules in place
are binarized one direction, while the children to
the right are binarized the other direction. The top
node retains its original label (e.g. NPB), while the
new partial constituents are labeled with a bar (e.g.
NPB). Figure 7 shows an example.
Figure 7: head-out binarization in the target lan-
guage: S, NPB, and VP are binarized according to
the head word
Table 9 shows the effect of binarization on phrasal
coverage, using both composed and SPMT rules. By
eliminating some of the syntactic constraints we al-
low more freedom, which allows increased phrasal
coverage, but generates more rules.
Category of missing ATS-useful phrase pairs Chinese Arabic
Too large 16 12
Extra target words in GHKM rules 123 12
Extra source words in GHKM rules 307 591
Other (e.g. parse failures) 12 7
Total missing useful phrase pairs 458 622
Table 9: reasons that ATS-useful phrase pairs still
could not be extracted as phrasal rules after bina-
rization
6 Evaluation of Translations
To evaluate translation quality of each of these mod-
els and methods, we ran the ATS decoder using its
extracted phrase pairs and the syntax-based decoder
using all the rule sets mentioned above. Table 10 de-
scribes the development and test datasets used, along
with four references for measuring BLEU. Tun-
ing was done using Maximum BLEU hill-climbing
(Och, 2003). Features used for the ATS system were
the standard set. For the syntax-based translation
system, we used a similar set of features.
# of lines
Dataset Chinese Arabic
Development set NIST 2002 MT eval 925 696
(sentences < 47 tokens)
Test set NIST 2003 MT eval 919 663
Table 10: development and test corpora
Table 11 shows the case-insensitive NIST BLEU4
scores for both our development and test decod-
ings. The BLEU scores indicate, first of all, that
the syntax-based system is much stronger in trans-
lating Chinese than Arabic, in comparison to the
phrase-based system. Also, the ideas presented here
for improving phrasal coverage generally improve
the syntax-based translation quality. In addition,
composed rules are shown to be helpful as com-
pared to the minimal runs. This is true even when
SPMT model 1 is added, which indicates that the
size 4 composed rules bring more than just improved
phrasal coverage.
Chinese Arabic
Experiment Dev Test Dev Test
Baseline ATS 34.94 32.83 50.46 50.52
Baseline GHKM (minimal only) 38.02 37.67 49.34 49.99
GHKM composed size 2 40.24 39.75 50.76 50.94
GHKM composed size 3 40.95 40.44 51.56 51.48
GHKM composed size 4 41.36 40.69 51.60 51.71
GHKM minimal + SPMT model 1 39.78 39.16 50.17 51.27
GHKM composed + SPMT model 1 42.04 41.07 51.73 51.53
With binarization 42.17 41.26 52.50 51.79
Table 11: evaluation results (reported in case-
insensitive NIST BLEU4)
7 Conclusions
Both the ATS model for phrase-based machine
translation and the GHKM model for syntax-based
machine translation are state-of-the-art methods.
Each extraction method has strengths and weak-
nesses as compared to the other, and there are sur-
prising differences in phrasal coverage ? neither is
merely a superset of the other. We have shown that
it is possible to gain insights from the strengths of
the phrase-based extraction model to increase both
762
the phrasal coverage and translation accuracy of the
syntax-based model.
However, there is still room for improvement in
both models. For syntax models, there are still holes
in phrasal coverage, and other areas are needing
progress, such as decoding efficiency. For phrase-
based models, incorporating syntactic knowledge
and constraints may lead to improvements as well.
8 Acknowledgments
The authors wish to acknowledge our colleagues at
ISI, especially David Chiang, for constructive criti-
cism on an early draft of this document, and several
reviewers for their detailed comments which helped
us make the paper stronger. We are also grateful to
Jens-So?nke Vo?ckler for his assistance in setting up
an experimental pipeline, without which this work
would have been much more tedious and difficult.
This research was supported under DARPA Contract
No. HR0011-06-C-0022.
References
Rens Bod, Remko Scha, and Khalil Sima?an, editors. 2003.
Data-Oriented Parsing. CSLI Publications, University of
Chicago Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL 2005.
Michael Collins. 2003. Head-driven statistical models for nat-
ural language parsing. Computational Linguistics, 29(4).
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins. 2006.
A discriminative model for tree-to-tree translation. In Proc.
EMNLP 2006.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proc. ACL 2003.
Alexander Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In Proc. ACL 2006.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. HLT-
NAACL 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proc. ACL 2006.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. ACL 2003, companion volume.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Sta-
tistical syntax-directed translation with extended domain of
locality. In Proc. AMTA 2006.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. HLT-NAACL 2003.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical machine translation with
syntactified target language phrases. In Proc. EMNLP 2006.
I. Dan Melamed. 2004. Statistical machine translation by pars-
ing. In Proc. ACL 2004.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1).
Franz Josef Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics, 30.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. ACL 2003.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proc. ACL 2005.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing
syntax trees to improve syntax-based machine translation ac-
curacy. In Proc. EMNLP and CoNLL 2007.
Dekai Wu and Hongsing Wong. 1998. Machine translation
with a stochastic grammatical channel. In Proc. ACL 1998.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL 2001.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proc. NAACL HLT 2006.
763
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 16?17,
Vancouver, October 2005.
 
Translation Exercise Assistant: 
Automated Generation of Translation Exercises  
for Native-Arabic Speakers Learning English 
 
Jill Burstein 
Educational Testing Service 
Princeton, NJ 08541 
jburstein@ets.org 
Daniel Marcu 
Language Weaver, Inc 
Marina del Rey, CA 90292 
dmarcu@languageweaver.com 
 
1. Introduction 
 
Machine translation has clearly entered into 
the marketplace as a helpful technology. 
Commercial applications are used on the internet 
for automatic translation of web pages and news 
articles. In the business environment, companies 
offer software that performs automatic 
translations of web sites for localization 
purposes, and translations of business 
documents (e.g., memo and e-mails).  With 
regard to education, research using machine 
translation for language learning tools has been 
of interest since the early 1990?s (Anderson, 
1993, Richmond, 1994, and Yasuda, 2004), 
though little has been developed. Very recently, 
Microsoft introduced a product called Writing 
Wizard that uses machine translation to assist 
with business writing for native Chinese 
speakers. To our knowledge, this is currently the 
only deployed education-based tool that uses 
machine translation. 
Currently, all writing-based English 
language learning (ELL) writing-based products 
and services at Educational Testing Service rely 
on e-rater automated essay scoring and the 
Critique writing analysis tool capabilities 
(Burstein, Chodorow, and Leacock, 2004).  In 
trying to build on a portfolio of innovative 
products and services, we have explored using 
machine translation toward the development of 
new ELL-based capabilities. We have developed 
a prototype system for automatically generating 
translation exercises in Arabic --- the 
Translation Exercise Assistant.   
Translation exercises are one kind of task 
that teachers can offer to give students practice 
with specific grammatical structures in English. 
Our hypothesis is that teachers could use such a 
tool to help them create exercises for the 
classroom, homework, or quizzes. The idea 
behind our prototype is a capability that can be 
used either by classroom teachers to help them 
generate sentence-based translation exercises 
from an infinite number of Arabic language texts 
of their choice. The capability might be 
integrated into a larger English language 
learning application. In this latter application, 
these translation exercises could be created by 
classroom teachers for the class or for 
individuals who may need extra help with 
particular grammatical structures in English. 
Another potential use of this system that has 
been discussed is to use it in ESL classrooms in 
the United States, to allow teachers to offer 
exercises in students? native language, especially 
for students who are competent in their own 
language, but only beginners in English. 
We had two primary goals in mind in 
developing our prototype. First, we wanted to 
evaluate how well the machine translation 
capability itself would work with this 
application.  In other words, how useful were the 
system outputs that are based on the machine 
translations? We also wanted to know to what 
extent this kind of tool facilitated the task of 
creating translation exercise items.  So, how 
much time is involved for a teacher to manually 
create these kinds of items versus using the 
exercise assistant tool to create them? Manually 
creating such an item involves searching through 
numerous reference sources (e.g., paper or web-
based version of newspapers), finding sentences 
with the relevant grammatical structure in the 
source language (Arabic), and then manually 
producing an English translation that can be 
used as an answer key.   
To evaluate these aspects, we implemented a 
graphical user interface that offered our two 
users the ability to create sets of translation 
 16
 exercise items for six pre-selected, grammatical 
structures. For each structure the system 
automatically identified and offered a set of 200 
system-selected potential sentences per category. 
For the exercise creation task, we collected 
timing information that told us how long it took 
users to create 3 exercises of 10 sentences each, 
for each category. In addition, users rated a set 
of up to 200 Arabic sentences with regard to if 
they were usable as translation exercise items, so 
that we could gauge the proportion of sentences 
selected by the application. These were the 
sentences that remained in the set of 200 
because they were not selected for an exercise. 
Two teachers participated in the evaluation of 
our prototype. One of the users also did the task 
manually. 
 
2. Translation Exercise Selection 
 
2.1     Data Sets 
 
The source of the data was Arabic English 
Parallel News Part 1 and the Multiple 
Translation Arabic Part 1 corpus from the 
Linguistic Data Consortium.1   Across these data 
sets we had access to about 45,000 Arabic 
sentences from Arabic journalistic texts taken 
from Ummah Press Service, Xinhua News and 
the AFP News Service available for this 
research. We used approximately 10,000 of 
these Arabic sentences for system development, 
and selected sentences from the remaining 
Arabic sentences for use with the interface.2  
 
2.2 System Description 
 
We used Language Weaver?s3 Arabic-to-English 
system to translate the Arabic sentences in the 
data sets. We built a module to find the relevant 
grammatical structures in the English 
translations. This module first passes the English 
                                                 
1 The LDC reference numbers for these corpora are: 
LDC2004T18 and LDC2003T18. 
2 To avoid producing sentences with overly 
complicated structures, we applied two constraints to 
the English translation: 1) it contained 20 words or 
less, and 2) it contained only a single sentence.  
3 See http://www.languageweaver.com. 
 
translation to a part-of-speech tagger that assigns 
a part-of-speech to each word in the sentence. 
Another module identifies regular expressions 
for the relevant part-of-speech sequences in the 
sentences, corresponding to one of these six 
grammatical structures: a) subject-verb 
agreement, b) complex verbs, c) phrasal verbs, 
d) nominal compounds, e) prepositions, and f) 
adjective modifier phrases.  When the 
appropriate pattern was found in the English 
translation, the well-formed Arabic sentence that 
corresponds to that translation is added to the set 
of potential translation exercise sentence 
candidates in the interface.   
 
2.3 Results 
 
The outcome of the evaluation indicated 
that between 98% and 100% of automatically-
generated sentence-based translation items were 
selected by both users as usable for translation 
items.  In addition, the time involved to create 
the exercises using the tool was 2.6 times faster 
than doing the task manually. 
 
References 
 
Anderson, Don D. (1995) ?Machine Translation as a 
Tool in Second Language Learning?, CALICO 
Journal 13.1, 68?97.  
 
Burstein, J., Chodorow, M., & Leacock, C. (2004). 
Automated essay evaluation: The Criterion online 
writing service. AI Magazine, 25(3), 27-36. 
 
Johnson, Rod (1993) ?MT Technology and 
Computer-Aided Language Learning?, in Sergei 
Nirenburg (ed.) Progress in Machine Translation, 
Amsterdam: IOS and Tokyo: Ohmsha, pages 286?
287. 
 
Richmond, Ian M. (1994) ?Doing it backwards: 
Using translation software to teach target-language 
grammaticality?, Computer Assisted Language 
Learning 7, 65?78. 
 
Yasuda, K. Sugaya F., Sumita E, Takezawa T.,  
Kikui G., Yamamoto, S. (2004). Automatic 
Measuring of English Language Proficiency using 
MT Evaluation Technology. Proceedings of e-
Learning workshop, COLING 2004, Geneva, 
Switzerland. 
 17
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1?8,
New York, June 2006. c?2006 Association for Computational Linguistics
Capitalizing Machine Translation
Wei Wang and Kevin Knight and Daniel Marcu
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA, 90292
{wwang, kknight, dmarcu}@languageweaver.com
Abstract
We present a probabilistic bilingual capi-
talization model for capitalizing machine
translation outputs using conditional ran-
dom fields. Experiments carried out on
three language pairs and a variety of ex-
periment conditions show that our model
significantly outperforms a strong mono-
lingual capitalization model baseline, es-
pecially when working with small datasets
and/or European language pairs.
1 Introduction
Capitalization is the process of recovering case in-
formation for texts in lowercase. It is also called
truecasing (Lita et al, 2003). Usually, capitalization
itself tries to improve the legibility of texts. It, how-
ever, can affect the word choice or order when inter-
acting with other models. In natural language pro-
cessing, a good capitalization model has been shown
useful for tasks like name entity recognition, auto-
matic content extraction, speech recognition, mod-
ern word processors, and machine translation (MT).
Capitalization can be viewed as a sequence la-
beling process. The input to this process is a sen-
tence in lowercase. For each lowercased word in
the input sentence, we have several available cap-
italization tags: initial capital (IU), all uppercase
(AU), all lowercase (AL), mixed case (MX), and
all having no case (AN). The output of capital-
ization is a capitalization tag sequence. Associ-
ating a tag in the output with the corresponding
lowercased word in the input results in a surface
form of the word. For example, we can tag the
input sentence ?click ok to save your changes to
/home/doc.? into ?click IU ok AU to AL save AL
your AL changes AL to AL /home/doc MX . AN?,
getting the surface form ?Click OK to save your
changes to /home/DOC .?.
A capitalizer is a tagger that recovers the capi-
talization tag for each input lowercased word, out-
putting a well-capitalized sentence. Since each low-
ercased word can have more than one tag, and as-
sociating a tag with a lowercased word can result
in more than one surface form (e.g., /home/doc MX
can be either /home/DOC or /home/Doc), we need a
capitalization model to solve the capitalization am-
biguities. For example, Lita et al (2003) use a tri-
gram language model estimated from a corpus with
case information; Chelba and Acero (2004) use a
maximum entropy Markov model (MEMM) com-
bining features involving words and their cases.
Capitalization models presented in most previ-
ous approaches are monolingual because the models
are estimated only from monolingual texts. How-
ever, for capitalizing machine translation outputs,
using only monolingual capitalization models is not
enough. For example, if the sentence ?click ok to
save your changes to /home/doc .? in the above
example is the translation of the French sentence
?CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFI-
CATIONS DANS /HOME/DOC .?, the correct capitaliza-
tion result should probably be ?CLICK OK TO SAVE
YOUR CHANGES TO /HOME/DOC .?, where all words
are in all upper-case. Without looking into the case
1
of the MT input, we can hardly get the correct capi-
talization result.
Although monolingual capitalization models in
previous work can apply to MT output, a bilingual
model is more desirable. This is because MT out-
puts usually strongly preserve case from the input,
and because monolingual capitalization models do
not always perform as well on badly translated text
as on well-formed syntactic texts.
In this paper, we present a bilingual capitalization
model for capitalizing machine translation outputs
using conditional random fields (CRFs) (Lafferty et
al., 2001). This model exploits case information
from both the input sentence (source) and the out-
put sentence (target) of the MT system. We define a
series of feature functions to incorporate capitaliza-
tion knowledge into the model.
Experimental results are shown in terms of BLEU
scores of a phrase-based SMT system with the cap-
italization model incorporated, and in terms of cap-
italization precision. Experiments are performed
on both French and English targeted MT systems
with large-scale training data. Our experimental re-
sults show that the CRF-based bilingual capitaliza-
tion model performs better than a strong baseline
capitalizer that uses a trigram language model.
2 Related Work
A simple capitalizer is the 1-gram tagger: the case of
a word is always the most frequent one observed in
training data, with the exception that the sentence-
initial word is always capitalized. A 1-gram capital-
izer is usually used as a baseline for capitalization
experiments (Lita et al, 2003; Kim and Woodland,
2004; Chelba and Acero, 2004).
Lita et al (2003) view capitalization as a lexi-
cal ambiguity resolution problem, where the lexi-
cal choices for each lowercased word happen to be
its different surface forms. For a lowercased sen-
tence e, a trigram language model is used to find the
best capitalization tag sequence T that maximizes
p(T, e) = p(E), resulting in a case-sensitive sen-
tence E. Besides local trigrams, sentence-level
contexts like sentence-initial position are employed
as well.
Chelba and Acero (2004) frame capitalization as
a sequence labeling problem, where, for each low-
MT Decoder
Train Monolingual
Capitalization Model
Monolingual Cap Model Capitalization
Lower Case
Lower Case
f
Lower Case
e
Finput
Eoutput
Train
Translation Model
Train
Language Model
Translation
Model
Languagel
Model
{F}
{E}
{f}
{e}
Figure 1: The monolingual capitalization scheme employed
by most statistical MT systems.
ercased sentence e, they find the label sequence T
that maximizes p(T |e). They use a maximum en-
tropy Markov model (MEMM) to combine features
of words, cases and context (i.e., tag transitions).
Gale et al (1994) report good results on capital-
izing 100 words. Mikheev (1999) performs capital-
ization using simple positional heuristics.
3 Monolingual Capitalization Scheme
Translation and capitalization are usually performed
in two successive steps because removing case infor-
mation from the training of translation models sub-
stantially reduces both the source and target vocabu-
lary sizes. Smaller vocabularies lead to a smaller
translation model with fewer parameters to learn.
For example, if we do not remove the case informa-
tion, we will have to deal with at least nine prob-
abilities for the English-French word pair (click,
cliquez). This is because either ?click? or ?cliquez?
can have at least three tags (IU, AL, AU), and thus
three surface forms. A smaller translation model re-
quires less training data, and can be estimated more
accurately than otherwise from the same amount
of training data. A smaller translation model also
means less memory usage.
Most statistical MT systems employ the monolin-
gual capitalization scheme as shown in Figure 1. In
this scheme, the translation model and the target lan-
guage model are trained from the lowercased cor-
pora. The capitalization model is trained from the
case-sensitive target corpus. In decoding, we first
turn input into lowercase, then use the decoder to
generate the lowercased translation, and finally ap-
2
HYDRAULIC HEADER TILT CYLINDER KIT
Kit de ve?rin d?inclinaison hydraulique de la plate-forme
haut-parleur avant droit +
HAUT-PARLEUR AVANT DROIT +
Seat Controls, Standard
COMMANDES DU SIGE, STANDARD
loading a saved legend
Chargement d?une le?gende sauvegarde
Table 1: Errors made by monolingual capitalization model.
Each row contains a pair of MT input and MT output.
MT Decoder
CapitalizationBilingual
Cap Model
Train Bilingual
    Cap Model
alignment
 Word/Phrase Aligner
f
Lower Case
e
Finput
Eoutput
{F}
{E}
Figure 2: A bilingual capitalization scheme.
ply the capitalization model to recover the case of
the decoding output.
The monolingual capitalization scheme makes
many errors as shown in Table 1. Each cell in
the table contains the MT-input and the MT-output.
These errors are due to the capitalizer does not have
access to the source sentence.
Regardless, estimating mixed-cased translation
models, however, is a very interesting topic and
worth future study.
4 Bilingual Capitalization Model
4.1 The Model
Our probabilistic bilingual capitalization model ex-
ploits case information from both the input sentence
to the MT system and the output sentence from the
system (see Figure 2). An MT system translates a
capitalized sentence F into a lowercased sentence e.
A statistical MT system can also provide the align-
ment A between the input F and the output e; for
example, a statistical phrase-based MT system could
provide the phrase boundaries in F and e, and also
the alignment between the phrases.1
1We shall explain our capitalization model within the
phrase-based SMT framework, the model, however, could be
OK
Click OK
Cliquez
E
F
E?i
F?j
Figure 3: Alignment graph. Brackets mean phrase bound-
aries.
The bilingual capitalization algorithm recovers
the capitalized sentence E from e, according to the
input sentence F , and the alignment A. Formally,
we look for the best capitalized sentence E? such
that
E? = arg maxE?GEN(e)p(E|F,A) (1)
where GEN(e) is a function returning the set of
possible capitalized sentences consistent with e. No-
tice that e does not appear in p(E|F,A) because we
can uniquely obtain e from E. p(E|F,A) is the cap-
italization model of concern in this paper.2
To further decompose the capitalization model
p(E|F,A), we make some assumptions. As shown
in Figure 3, input sentence F , capitalized output E,
and their alignment can be viewed as a graph. Ver-
tices of the graph correspond to words in F and
E. An edge connecting a word in F and a word
in E corresponds to a word alignment. An edge
between two words in E represents the dependency
between them captured by monolingual n-gram lan-
guage models. We also assume that both E and
F have phrase boundaries available (denoted by the
square brackets), and that A is the phrase alignment.
In Figure 3, F?j is the j-th phrase of F , E?i is the i-th
phrase of E, and they align to each other. We do not
require a word alignment; instead we find it reason-
able to think that a word in E?i can be aligned to any
adapted to syntax-based machine translation, too. To this end,
the translational correspondence is described within a transla-
tion rule, i.e., (Galley et al, 2004) (or a synchronous produc-
tion), rather than a translational phrase pair; and the training
data will be derivation forests, instead of the phrase-aligned
bilingual corpus.
2The capitalization model p(E|F, A) itself does not require
the existence of e. This means that in principle this model can
also be viewed as a capitalized translation model that performs
translation and capitalization in an integrated step. In our paper,
however, we consider the case where the machine translation
output e is given, which is reflected by the the fact that GEN(e)
takes e as input in Formula 1.
3
word in F?j . A probabilistic model defined on this
graph is a Conditional Random Field. Therefore,
it is natural to formulate the bilingual capitalization
model using CRFs:3
p?(E|F, A) =
1
Z(F, A, ?)
exp
 I
X
i=1
?ifi(E, F, A)
!
(2)
where
Z(F, A, ?) =
X
E?GEN(e)
exp
 I
X
i=1
?ifi(E,F, A)
!
(3)
fi(E,F,A), i = 1...I are the I features, and
? = (?1, ..., ?I) is the feature weight vector. Based
on this capitalization model, the decoder in the cap-
italizer looks for the best E? such that
E? = arg maxE?GEN(e,F )
I
?
i=1
?ifi(E,F,A) (4)
4.2 Parameter Estimation
Following Roark et al (2004), Lafferty et al (2001)
and Chen and Rosenfeld (1999), we are looking for
the set of feature weights ? maximizing the regu-
larized log-likelihood LLR(?) of the training data
{E(n), F (n), A(n), n = 1, ..., N}.
LLR(?) =
N
X
n=1
log p
?
E(n)|F (n), A(n)
?
? ||?||
2
2?2 (5)
The second term at the right-hand side of For-
mula 5 is a zero-mean Gaussian prior on the pa-
rameters. ? is the variance of the Gaussian prior
dictating the cost of feature weights moving away
from the mean ? a smaller value of ? keeps feature
weights closer to the mean. ? can be determined
by linear search on development data.4 The use of
the Gaussian prior term in the objective function has
been found effective in avoiding overfitting, leading
to consistently better results. The choice of LLR as
an objective function can be justified as maximum
a-posteriori (MAP) training within a Bayesian ap-
proach (Roark et al, 2004).
3We chose CRFs over other sequence labeling models (i.e.
MEMM) because CRFs have no label bias and we do not need
to compute the partition function during decoding.
4In our experiment, we use an empirical value ? = 0.5 as in
(Roark et al, 2004).
4.3 Feature Functions
We define features based on the alignment graph
in Figure 3. Each feature function is defined on a
word.
Monolingual language model feature. The
monolingual LM feature of word Ei is the loga-
rithm of the probability of the n-gram ending at
Ei:
fLM(Ei, F,A) = log p(Ei|Ei?1, ..., Ei?n+1) (6)
p should be appropriately smoothed such that it
never returns zero.
Capitalized translation model feature. Sup-
pose E phrase ?Click OK? is aligned to F
phrase ?Cliquez OK?. The capitalized transla-
tion model feature of ?Click? is computed as
log p(Click|Cliquez)+log p(Click|OK). ?Click? is
assumed to be aligned to any word in the F phrase.
The larger the probability that ?Click? is translated
from an F word, i.e., ?Cliquez?, the more chances
that ?Click? preserves the case of ?Cliquez?. For-
mally, for word Ei, and an aligned phrase pair E?l
and F?m, where Ei ? E?l, the capitalized translation
model feature of Ei is
fcap?t1(Ei, F,A) = log
|F?m|
?
k=1
p(Ei|F?m,k) (7)
p(Ei|F?m,k) is the capitalized translation table. It
needs smoothing to avoid returning zero, and is esti-
mated from a word-aligned bilingual corpus.
Capitalization tag translation feature. The fea-
ture value of E word ?Click? aligning to F phrase
?Cliquez OK? is log p(IU|IU)p(click|cliquez) +
log p(IU|AU)p(click|ok). We see that this feature
is less specific than the capitalized translation model
feature. It is computed in terms of the tag transla-
tion probability and the lowercased word translation
probability. The lowercased word translation proba-
bility, i.e., p(click|ok), is used to decide how much
of the tag translation probability, i.e., p(IU|AU),
will contribute to the final decision. The smaller the
word translation probability, i.e., p(click|ok), is, the
smaller the chance that the surface form of ?click?
4
preserves case from that of ?ok?. Formally, this fea-
ture is defined as
fcap?tag?t1(Ei, F,A) =
log
|f?m|
?
k=1
p(ei|f?m,k) ? p(?(Ei)|?(F?m,k)) (8)
p(ei|f?m,k) is the t-table over lowercased word pairs,
which is the usual ?t-table? in a SMT system.
p(?(Ei)|?(F?m,k)) is the probability of a target cap-
italization tag given a source capitalization tag and
can be easily estimated from a word-aligned bilin-
gual corpus. This feature attempts to help when
fcap?t1 fails (i.e., the capitalized word pair is un-
seen). Smoothing is also applied to both p(ei|f?m,k)
and p(?(Ei)|?(F?m,k)) to handle unseen words (or
word pairs).
Upper-case translation feature. Word Ei is in
all upper case if all words in the corresponding F
phrase F?m are in upper case. Although this fea-
ture can also be captured by the capitalization tag
translation feature in the case where an AU tag in
the input sentence is most probably preserved in the
output sentence, we still define it to emphasize its
effect. This feature aims, for example, to translate
?ABC XYZ? into ?UUU VVV? even if all words are
unseen.
Initial capitalization feature. An E word is ini-
tially capitalized if it is the first word that contains
letters in the E sentence. For example, for sentence
?? Please click the button? that starts with a bul-
let, the initial capitalization feature value of word
?please? is 1 because ??? does not contain a letter.
Punctuation feature template. An E word is ini-
tially capitalized if it follows a punctuation mark.
Non-sentence-ending punctuation marks like com-
mas will usually get negative weights.
As one can see, our features are ?coarse-grained?
(e.g., the language model feature). In contrast, Kim
and Woodland (2004) and Roark et al (2004) use
?fine-grained? features. They treat each n-gram as
a feature for, respectively, monolingual capitaliza-
tion and language modeling. Feature weights tuned
at a fine granularity may lead to better accuracy,
but they require much more training data, and re-
sult in much slower training speed, especially for
large-scale learning problems. Coarse-grained fea-
tures enable us to efficiently get the feature values
from a very large training corpus, and quickly tune
the weights on small development sets. For exam-
ple, we can train a bilingual capitalization model on
a 70 million-word corpus in several hours with the
coarse-grained features presented above, but in sev-
eral days with fine-grained n-gram count features.
4.4 The GEN Function
Function GEN generates the set of case-sensitive
candidates from a lowercased token. For exam-
ple GEN(mt) = {mt, mT, Mt, MT}. The follow-
ing heuristics can be used to reduce the range of
GEN. The returned set of GEN on a lower-cased to-
ken w is the union of: (i) {w,AU(w), IU(w)}, (ii)
{v|v is seen in training data and AL(v) = w},
and (iii) {F?m,k|AL(F?m,k) = AL(w)}. The heuris-
tic (iii) is designed to provide more candidates for
w when it is translated from a very strange input
word F?m,k in the F phrase F?m that is aligned to the
phrase that w is in. This heuristic creates good capi-
talization candidates for the translation of URLs, file
names, and file paths.
5 Generating Phrase-Aligned Training
Data
Training the bilingual capitalization model requires
a bilingual corpus with phrase alignments, which are
usually produced from a phrase aligner. In practice,
the task of phrase alignment can be quite computa-
tionally expensive as it requires to translate the en-
tire training corpus; also a phrase aligner is not al-
ways available. We therefore generate the training
data using a na??ve phrase aligner (NPA) instead of
resorting to a real one.
The input to the NPA is a word-aligned bilingual
corpus. The NPA stochastically chooses for each
sentence pair one segmentation and phrase align-
ment that is consistent with the word alignment. An
aligned phrase pair is consistent with the word align-
ment if neither phrase contains any word aligning
to a word outside the other phrase (Och and Ney,
2004). The NPA chunks the source sentence into
phrases according to a probabilistic distribution over
source phrase lengths. This distribution can be ob-
tained from the trace output of a phrase-based MT
5
Entire Corpus (#W) Test-BLEU
Languages Training Dev Test-Prec. (#sents)
E?F (IT) 62M 13K 15K 763
F?E (news) 144M 11K 22K 241
C?E (news) 50M 8K 17K 919
Table 2: Corpora used in experiments.
decoder on a small development set. The NPA has
to retry if the current source phrase cannot find any
consistent target phrase. Unaligned target words are
attached to the left phrase. Heuristics are employed
to prevent the NPA from not coming to a solution.
Obviously, the NPA is a special case of the phrase
extractor in (Och and Ney, 2004) in that it considers
only one phrase alignment rather than all possible
ones.
Unlike a real phrase aligner, the NPA need not
wait for the training of the translation model to fin-
ish, making it possible for parallelization of transla-
tion model training and capitalization model train-
ing. However, we believe that a real phrase aligner
may make phrase alignment quality higher.
6 Experiments
6.1 Settings
We conducted capitalization experiments on three
language pairs: English-to-French (E?F) with a
bilingual corpus from the Information Technology
(IT) domain; French-to-English (F?E) with a bilin-
gual corpus from the general news domain; and
Chinese-to-English (C?E) with a bilingual corpus
from the general news domain as well. Each lan-
guage pair comes with a training corpus, a develop-
ment corpus and two test sets (see Table 2). Test-
Precision is used to test the capitalization precision
of the capitalizer on well-formed sentences drawn
from genres similar to those used for training. Test-
BLEU is used to assess the impact of our capitalizer
on end-to-end translation performance; in this case,
the capitalizer may operate on ungrammatical sen-
tences. We chose to work with these three language
pairs because we wanted to test our capitalization
model on both English and French target MT sys-
tems and in cases where the source language has no
case information (such as in Chinese).
We estimated the feature functions, such as the
log probabilities in the language model, from the
training set. Kneser-Ney smoothing (Kneser and
Ney, 1995) was applied to features fLM, fcap?t1,
and fcap?tag?t1. We trained the feature weights of
the CRF-based bilingual capitalization model using
the development set. Since estimation of the feature
weights requires the phrase alignment information,
we efficiently applied the NPA on the development
set.
We employed two LM-based capitalizers as base-
lines for performance comparison: a unigram-based
capitalizer and a strong trigram-based one. The
unigram-based capitalizer is the usual baseline for
capitalization experiments in previous work. The
trigram-based baseline is similar to the one in
(Lita et al, 2003) except that we used Kneser-Ney
smoothing instead of a mixture.
A phrase-based SMT system (Marcu and Wong,
2002) was trained on the bitext. The capitalizer
was incorporated into the MT system as a post-
processing module ? it capitalizes the lowercased
MT output. The phrase boundaries and alignments
needed by the capitalizer were automatically in-
ferred as part of the decoding process.
6.2 BLEU and Precision
We measured the impact of our capitalization model
in the context of an end-to-end MT system using
BLEU (Papineni et al, 2001). In this context, the
capitalizer operates on potentially ill-formed, MT-
produced outputs.
To this end, we first integrated our bilingual capi-
talizer into the phrase-based SMT system as a post-
processing module. The decoder of the MT sys-
tem was modified to provide the capitalizer with
the case-preserved source sentence, the lowercased
translation, and the phrase boundaries and their
alignments. Based on this information, our bilin-
gual capitalizer recovers the case information of the
lowercased translation, outputting a capitalized tar-
get sentence. The case-restored machine transla-
tions were evaluated against the target test-BLEU
set. For comparison, BLEU scores were also com-
puted for an MT system that used the two LM-based
baselines.
We also assessed the performance of our capital-
izer on the task of recovering case information for
well-formed grammatical texts. To this end, we used
the precision metric that counted the number of cor-
6
rectly capitalized words produced by our capitalizer
on well-formed, lowercased input
precision = #correctly capitalized words#total words (9)
To obtain the capitalization precision, we im-
plemented the capitalizer as a standalone program.
The inputs to the capitalizer were triples of a case-
preserved source sentence, a lowercased target sen-
tence, and phrase alignments between them. The
output was the case-restored version of the target
sentence. In this evaluation scenario, the capitalizer
output and the reference differ only in case infor-
mation ? word choices and word orders between
them are the same. Testing was conducted on Test-
Precision. We applied the NPA to the Test-Precision
set to obtain the phrases and their alignments be-
cause they were needed to trigger the features in
testing. We used a Test-Precision set that was dif-
ferent from the Test-BLEU set because word align-
ments were by-products only of training of transla-
tion models on the MT training data and we could
not put the Test-BLEU set into the MT training
data. Rather than implementing a standalone word
aligner, we randomly divided the MT training data
into three non-overlapping sets: Test-Precision set,
CRF capitalizer training set and dev set.
6.3 Results
The performance comparisons between our CRF-
based capitalizer and the two LM-based baselines
are shown in Table 3 and Table 4. Table 3 shows
the BLEU scores, and Table 4 shows the precision.
The BLEU upper bounds indicate the ceilings that a
perfect capitalizer can reach, and are computed by
ignoring the case information in both the capitalizer
outputs and the reference. Obviously, the precision
upper bounds for all language pairs are 100%.
The precision and end-to-end BLEU based com-
parisons show that, for European language pairs, the
CRF-based bilingual capitalization model outper-
forms significantly the strong LM-based baseline.
We got more than one BLEU point improvement on
the MT translation between English and French, a
34% relative reduction in capitalization error rate for
the French-to-English language pair, and a 42% rel-
ative error rate reduction for the English-to-French
language pair. These results show that source lan-
guage information provides significant help for cap-
italizing machine translation outputs. The results
also show that when the source language does not
have case, as in Chinese, the bilingual model equals
a monolingual one.
The BLEU difference between the CRF-based
capitalizer and the trigram one were larger than
the precision difference. This indicates that the
CRF-based capitalizer performs much better on non-
grammatical texts that are generated from an MT
system due to the bilingual feature of the CRF capi-
talizer.
6.4 Effect of Training Corpus Size
The experiments above were carried out on large
data sets. We also conducted experiments to exam-
ine the effect of the training corpus size on capital-
ization precision. Figure 4 shows the effects. The
experiment was performed on the E?F corpus. The
bilingual capitalizer performed significantly better
when the training corpus size was small (e.g., un-
der 8 million words). This is common in many do-
mains: when the training corpus size increases, the
difference between the two capitalizers decreases.
7 Conclusions
In this paper, we have studied how to exploit bilin-
gual information to improve capitalization perfor-
mance on machine translation output, and evaluated
the improvement over traditional methods that use
only monolingual language models.
We first presented a probabilistic bilingual cap-
italization model for capitalizing machine transla-
tion outputs using conditional random fields. This
model exploits bilingual capitalization knowledge as
well as monolingual information. We defined a se-
ries of feature functions to incorporate capitalization
knowledge into the model.
We then evaluated our CRF-based bilingual capi-
talization model both on well-formed texts in terms
of capitalization precision, and on possibly ungram-
matical end-to-end machine translation outputs in
terms of BLEU scores. Experiments were per-
formed on both French and English target MT sys-
tems with large-scale training data. Our experimen-
tal results showed that the CRF-based bilingual cap-
7
BLEU Scores
Translation UnigramCapitalizer
Trigram
Capitalizer
CRF-based
Capitalizer
Upper
Bound
F?E 24.96 26.73 27.92 28.85
E?F 32.63 34.66 36.10 36.17
C?E 23.81 25.92 25.89 -
Table 3: Impact of CRF-based capitalizer on end-to-end translation performance compared with two LM-based baselines.
Capitalization Precision (%)
Translation Unigram
capitalizer
Trigram
capitalizer
CRF-based
capitalizer
F?E 94.03 98.79 99.20
E?F 91.52 98.47 99.11
C?E 90.77 96.40 96.76
Table 4: Impact of CRF-based capitalizer on capitalization precision compared with two LM-based baselines.
100
99
98
97
96
95
94
93
92
64.032.016.08.04.02.01.00.50.20.1
Pr
ec
isi
on
 (x
%)
Training Corpus Size (MWs)
CRF-based capitalizer
LM-based capitalizer
Figure 4: Capitalization precision with respect to size of train-
ing corpus. LM-based capitalizer refers to the trigram-based
one. Results were on E?F corpus.
italization model performs significantly better than a
strong baseline, monolingual capitalizer that uses a
trigram language model.
In all experiments carried out at Language Weaver
with customer (or domain specific) data, MT sys-
tems trained on lowercased data coupled with the
CRF bilingual capitalizer described in this paper
consistently outperformed both MT systems trained
on lowercased data coupled with a strong monolin-
gual capitalizer and MT systems trained on mixed-
cased data.
References
Ciprian Chelba and Alex Acero. 2004. Adaptation of maxi-
mum entroy capitalizer: Little data can help a lot. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Barcelona, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing Maximum Entropy models. Technical Report
CMUCS-99-108, Carnegie Mellon University.
William A. Gale, Kenneth W. Church, and David Yarowsky.
1994. Discrimination decisions for 100,000-dimensional
spaces. In Current issues in computational linguistics.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a Translation Rule? In Proceedings of the Human
Language Technology Conference and the North American
Association for Computational Linguistics (HLT-NAACL),
Boston, Massachusetts.
Ji-Hwan Kim and Philip C. Woodland. 2004. Automatic capi-
talization generation for speech input. Computer Speech and
Language, 18(1):67?90, January.
Reinhard Kneser and Hermann Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Signal
Processing (ICASSP) 1995, pages 181?184, Detroit, Michi-
gan. IEEE.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for segmen-
tation and labeling sequence data.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda
Kambhatla. 2003. tRuEcasIng. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan, July.
Daniel Marcu and William Wong. 2002. A phrase-based, joint
probability model for statistical machine translation. In Pro-
ceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, PA.
A. Mikheev. 1999. A knowledge-free method fro capitalized
word disambiguation. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguistics
(ACL), College Park, Maryland, June.
Franz Och and Hermann Ney. 2004. The alignment template
approach to statistical machine translation. Computational
Linguistics, 30(4).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. BLEU: A method for automatic evaluation
of Machine Translation. Technical Report RC22176, IBM,
September.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004. Discriminative language modeling with condi-
tional random field and the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain.
8
Language Weaver Arabic->English MT 
Daniel MARCU, Alex FRASER, William WONG, Kevin KNIGHT 
Language Weaver, Inc.  
4640 Admiralty Way, Suite 1210 
Marina del Rey, CA, USA, 90292 
{marcu,afraser,wong,knight}@languageweaver.com 
 
Abstract 
This presentation is primarily a demonstration 
of a working statistical machine translation 
system which translates Modern Standard 
Arabic into English.  
1 Overview 
Language Weaver has produced a high-
performance statistical Arabic-to-English machine 
translation system, based on research work 
conducted at the University of Southern California, 
Information Sciences Institute (USC/ISI).  Getting 
resource-unlimited laboratory systems to run in 
real time, on a typical desktop Windows machine, 
is among Language Weaver?s contributions.  The 
system is designed to provide broad general 
coverage of Arabic news, and is currently used at 
various sites within the U.S. Government.  
 
 
 
 
 
The Arabic->English translation system to be 
demonstrated has been prepared in versions that 
require 1 or 2 GB of RAM, and run on a 1.5GHz or 
faster processor and translates at a minimum rate 
of 500 words per minute.  The system includes an 
option to trade off speed for quality in the 
translation process allowing users to select the 
fastest possible gisting-quality output, or the best 
possible translation quality for each sentence. 
2 Demonstration 
The translation system will be demonstrated on 
current news, and possibly other postings from 
Internet, or other files: 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 44?52,
Sydney, July 2006. c?2006 Association for Computational Linguistics
SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight
Language Weaver Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
{dmarcu,wwang,aechihabi,kknight}@languageweaver.com
Abstract
We introduce SPMT, a new class of sta-
tistical Translation Models that use Syn-
tactified target language Phrases. The
SPMT models outperform a state of the art
phrase-based baseline model by 2.64 Bleu
points on the NIST 2003 Chinese-English
test corpus and 0.28 points on a human-
based quality metric that ranks translations
on a scale from 1 to 5.
1 Introduction
During the last four years, various implemen-
tations and extentions to phrase-based statistical
models (Marcu and Wong, 2002; Koehn et al,
2003; Och and Ney, 2004) have led to signif-
icant increases in machine translation accuracy.
Although phrase-based models yield high-quality
translations for language pairs that exhibit simi-
lar word order, they fail to produce grammatical
outputs for language pairs that are syntactically
divergent. Recent models that exploit syntactic
information of the source language (Quirk et al,
2005) have been shown to produce better outputs
than phrase-based systems when evaluated on rel-
atively small scale, domain specific corpora. And
syntax-inspired formal models (Chiang, 2005), in
spite of being trained on significantly less data,
have shown promising results when compared on
the same test sets with mature phrase-based sys-
tems. To our knowledge though, no previous re-
search has demonstrated that a syntax-based sta-
tistical translation system could produce better re-
sults than a phrase-based system on a large-scale,
well-established, open domain translation task. In
this paper we present such a system.
Our translation models rely upon and naturally
exploit submodels (feature functions) that have
been initially developed in phrase-based systems
for choosing target translations of source language
phrases, and use new, syntax-based translation and
target language submodels for assembling target
phrases into well-formed, grammatical outputs.
After we introduce our models intuitively, we
discuss their formal underpinning and parameter
training in Section 2. In Section 3, we present our
decoder and, in Section 4, we evaluate our models
empirically. In Section 5, we conclude with a brief
discussion.
2 SPMT: statistical Machine Translation
with Syntactified Phrases
2.1 An intuitive introduction to SPMT
After being exposed to 100M+ words of parallel
Chinese-English texts, current phrase-based statis-
tical machine translation learners induce reason-
ably reliable phrase-based probabilistic dictionar-
ies. For example, our baseline statistical phrase-
based system learns that, with high probabilities,
the Chinese phrases ?ASTRO- -NAUTS?, ?FRANCE
AND RUSSIA? and ?COMINGFROM? can be trans-
lated into English as ?astronauts?/?cosmonauts?,
?france and russia?/?france and russian? and
?coming from?/?from?, respectively. 1 Unfortu-
nately, when given as input Chinese sentence 1,
our phrase-based system produces the output
shown in 2 and not the translation in 3, which
correctly orders the phrasal translations into a
grammatical sequence. We believe this hap-
pens because the distortion/reordering models that
are used by state-of-the-art phrase-based systems,
which exploit phrase movement and ngram target
1To increase readability, in this paper, we represent Chi-
nese words using fully capitalized English glosses and En-
glish words using lowercased letters.
44
language models (Och and Ney, 2004; Tillman,
2004), are too weak to help a phrase-based de-
coder reorder the target phrases into grammatical
outputs.
THESE 7PEOPLE INCLUDE COMINGFROM
FRANCE AND RUSSIA p-DE ASTRO- -NAUTS .
(1)
the 7 people including those from france
and the russian cosmonauts .
(2)
these 7 people include astronauts coming
from france and russia .
(3)
One method for increasing the ability of a de-
coder to reorder target language phrases is that
of decorating them with syntactic constituent in-
formation. For example, we may make ex-
plicit that the Chinese phrase ?ASTRO- -NAUTS?
may be translated into English as a noun phrase,
NP(NNS(astronauts)); that the phrase FRANCE AND
RUSSIA may be translated into a complex noun-
phrase, NP(NP(NNP(france)) CC(and) NP(NNP(russia)));
that the phrase COMINGFROM may be translated
into a partially realized verb phrase that is look-
ing for a noun phrase to its right in order to be
fully realized, VP(VBG(coming) PP(IN(from) NP:x0));
and that the Chinese particle p-DE, when occurring
between a Chinese string that was translated into
a verb phrase to its left and another Chinese string
that was translated into a noun phrase to its right,
VP:x1 p-DE NP:x0, should be translated to noth-
ing, while forcing the reordering of the two con-
stituents, NP(NP:x0, VP:x1). If all these translation
rules (labeled r1 to r4 in Figure 1) were available
to a decoder that derives English parse trees start-
ing from Chinese input strings, this decoder could
produce derivations such as that shown in Fig-
ure 2. Because our approach uses translation rules
with Syntactified target language Phrases (see Fig-
ure 1), we call it SPMT.
2.2 A formal introduction to SPMT
2.2.1 Theoretical foundations
We are interested to model a generative process
that explains how English parse trees pi and their
associated English string yields E, foreign sen-
tences, F , and word-level alignments, A, are pro-
duced. We assume that observed (pi, F,A) triplets
are generated by a stochastic process similar to
r1 :NP(NNS(astronauts)) ? ASTRO- -NAUTS
r2 :NP(NP(NNP(france)) CC(and) NP(NNP(russia)))?
FRANCE AND RUSSIA
r3 :VP(VBG(coming) PP(IN(from) NP:x0)) ?
COMINGFROM x0
r4 :NP(NP:x0, VP:x1) ? x1 p-DE x0
r5 :NNP(france) ? FRANCE
r6 :NP(NP(NNP(france)) CC(and) NP:x0) ? FRANCE AND x0
r7 :NNS(astronauts) ? ASTRO- -NAUTS
r8 :NNP(russia) ? RUSSIA
r9 :NP(NNS:x0)? x0
r10 :PP(IN:x0 NP:x1) ? x0 x1
r11 :NP(NP:x0 CC:x1 NP:x2) ? x0 x1 x2
r12 :NP(NNP:x0)? x0
r13 :CC(and) ? AND
r14 :NP(NP:x0 CC(and) NP:x1) ? x0 AND x1
r15 :NP(NP:x0 VP(VBG(coming) PP(IN(from) NP:x1))) ?
x1 COMINGFROM x0
Figure 1: Examples of xRS rules.
that used in Data Oriented Parsing models (Bon-
nema, 2002). For example, if we assume that the
generative process has already produced the top
NP node in Figure 2, then the corresponding par-
tial English parse tree, foreign/source string, and
word-level alignment could be generated by the
rule derivation r4(r1, r3(r2)), where each rule is
assumed to have some probability.
The extended tree to string transducers intro-
duced by Knight and Graehl (2005) provide a nat-
ural framework for expressing the tree to string
transformations specific to our SPMT models.
The transformation rules we plan to exploit are
equivalent to one-state xRS top-down transduc-
ers with look ahead, which map subtree patterns
to strings. For example, rule r3 in Figure 1 can
be applied only when one is in a state that has a
VP as its syntactic constituent and the tree pat-
tern VP(VBG(coming) PP(IN(from) NP)) immediately
underneath. The rule application outputs the string
?COMINGFROM? as the transducer moves to the
state co-indexed by x0; the outputs produced from
the new state will be concatenated to the right of
the string ?COMINGFROM?.
Since there are multiple derivations that could
lead to the same outcome, the probability of a
tuple (pi, F,A) is obtained by summing over all
derivations ?i ? ? that are consistent with the tu-
45
Figure 2: English parse tree derivation of the Chi-
nese string COMINGFROM FRANCE AND RUSSIA p-
DE ASTRO- -NAUTS.
ple, c(?) = (pi, F,A). The probability of each
derivation ?i is given by the product of the proba-
bilities of all the rules p(rj) in the derivation (see
equation 4).
Pr(pi, F,A) =
?
?i??,c(?)=(pi,F,A)
?
rj??i
p(rj) (4)
In order to acquire the rules specific to our
model and to induce their probabilities, we parse
the English side of our corpus with an in-house
implementation (Soricut, 2005) of Collins pars-
ing models (Collins, 2003) and we word-align the
parallel corpus with the Giza++2 implementation
of the IBM models (Brown et al, 1993). We
use the automatically derived ?English-parse-tree,
English-sentence, Foreign-sentence, Word-level-
alignment? tuples in order to induce xRS rules for
several models.
2.2.2 SPMT Model 1
In our simplest model, we assume that each
tuple (pi, F,A) in our automatically annotated
corpus could be produced by applying a com-
bination of minimally syntactified, lexicalized,
phrase-based compatible xRS rules, and mini-
mal/necessary, non-lexicalized xRS rules. We call
a rule non-lexicalized whenever it does not have
any directly aligned source-to-target words. Rules
r9?r12 in Figure 1 are examples of non-lexicalized
rules.
Minimally syntactified, lexicalized, phrase-
based-compatible xRS rules are extracted via a
2http://www.fjoch.com/GIZA++.html
simple algorithm that finds for each foreign phrase
F ji , the smallest xRS rule that is consistent with
the foreign phrase F ji , the English syntactic tree
pi, and the alignment A. The algorithm finds for
each foreign/source phrase span its projected span
on the English side and then traverses the En-
glish parse tree bottom up until it finds a node
that subsumes the projected span. If this node has
children that fall outside the projected span, then
those children give rise to rules that have variables.
For example, if the tuple shown in Figure 2 is in
our training corpus, for the foreign/source phrases
FRANCE, FRANCE AND, FRANCE AND RUSSIA, and
ASTRO- -NAUTS, we extract the minimally syntac-
tified, lexicalized phrase-based-compatible xRS
rules r5, r6, r2, and r7 in Figure 1, respectively.
Because, as in phrase-based MT, all our rules have
continuous phrases on both the source and target
language sides, we call these phrase-based com-
patible xRS rules.
Since these lexicalized rules are not sufficient to
explain an entire (pi, F,A) tuple, we also extract
the required minimal/necessary, non-lexicalized
xRS rules. The minimal non-lexicalized rules that
are licensed by the tuple in Figure 2 are labeled
r4, r9, r10, r11 and r12 in Figure 1. To obtain the
non-lexicalized xRS rules, we compute the set of
all minimal rules (lexicalized and non-lexicalized)
by applying the algorithm proposed by Galley et
al. (2006) and then remove the lexicalized rules.
We remove the Galley et al?s lexicalized rules
because they are either already accounted for by
the minimally syntactified, lexicalized, phrase-
based-compatible xRS rules or they subsume non-
continuous source-target phrase pairs.
It is worth mentioning that, in our framework,
a rule is defined to be ?minimal? with respect to a
foreign/source language phrase, i.e., it is the min-
imal xRS rule that yields that source phrase. In
contrast, in the work of Galley et al (2004; 2006),
a rule is defined to be minimal when it is necessary
in order to explain a (pi, F,A) tuple.
Under SPMT model 1, the tree in Figure 2 can
be produced, for example, by the following deriva-
tion: r4(r9(r7), r3(r6(r12(r8)))).
2.2.3 SPMT Model 1 Composed
We hypothesize that composed rules, i.e., rules
that can be decomposed via the application of a
sequence of Model 1 rules may improve the per-
formance of an SPMT system. For example, al-
though the minimal Model 1 rules r11 and r13 are
46
Figure 3: Problematic syntactifications of phrasal
translations.
sufficient for building an English NP on top of two
NPs separated by the Chinese conjunction AND,
the composed rule r14 in Figure 1 accomplishes
the same result in only one step. We hope that the
composed rules could play in SPMT the same role
that phrases play in string-based translation mod-
els.
To test our hypothesis, we modify our rule ex-
traction algorithm so that for every foreign phrase
F ji , we extract not only a minimally syntactified,
lexicalized xRS rule, but also one composed rule.
The composed rule is obtained by extracting the
rule licensed by the foreign/source phrase, align-
ment, English parse tree, and the first multi-child
ancestor node of the root of the minimal rule. Our
intuition is that composed rules that involve the ap-
plication of more than two minimal rules are not
reliable. For example, for the tuple in Figure 2,
the composed rule that we extract given the for-
eign phrases AND and COMINGFROM are respec-
tively labeled as rules r14 and r15 in Figure 1.
Under the SPMT composed model 1,
the tree in Figure 2 can be produced,
for example, by the following derivation:
r15(r9(r7), r14(r12(r5), r12(r8))).
2.2.4 SPMT Model 2
In many instances, the tuples (pi, F,A) in our
training corpus exhibit alignment patterns that can
be easily handled within a phrase-based SMT
framework, but that become problematic in the
SPMT models discussed until now.
Consider, for example, the (pi, F,A) tuple frag-
ment in Figure 3. When using a phrase-based
translation model, one can easily extract the
phrase pair (THE MUTUAL; the mutual) and use it
during the phrase-based model estimation phrase
and in decoding. However, within the xRS trans-
ducer framework that we use, it is impossible to
extract an equivalent syntactified phrase transla-
tion rule that subsumes the same phrase pair be-
cause valid xRS translation rules cannot be multi-
headed. When faced with this constraint, one has
several options:
? One can label such phrase pairs as non-
syntactifiable and ignore them. Unfortu-
nately, this is a lossy choice. On our par-
allel English-Chinese corpus, we have found
that approximately 28% of the foreign/source
phrases are non-syntactifiable by this defini-
tion.
? One can also traverse the parse tree upwards
until one reaches a node that is xRS valid, i.e.,
a node that subsumes the entire English span
induced by a foreign/source phrase and the
corresponding word-level alignment. This
choice is also inappropriate because phrase
pairs that are usually available to phrase-
based translation systems are then expanded
and made available in the SPTM models only
in larger applicability contexts.
? A third option is to create xRS compati-
ble translation rules that overcome this con-
straint.
Our SPMT Model 2 adopts the third option by
rewriting on the fly the English parse tree for each
foreign/source phrase and alignment that lead to
non-syntactifiable phrase pairs. The rewriting pro-
cess adds new rules to those that can be created
under the SPMT model 1 constraints. The process
creates one xRS rule that is headed by a pseudo,
non-syntactic nonterminal symbol that subsumes
the target phrase and corresponding multi-headed
syntactic structure; and one sibling xRS rule that
explains how the non-syntactic nonterminal sym-
bol can be combined with other genuine nonter-
minals in order to obtain genuine parse trees. In
this view, the foreign/source phrase THE MUTUAL
and corresponding alignment in Figure 3 licenses
the rules ?NPB? NN(DT(the) JJ(mutual)) ? THE MU-
TUAL and NPB(?NPB? NN:x0 NN:x1) ? x0 x1 even
though the foreign word UNDERSTANDING is
aligned to an English word outside the NPB con-
situent. The name of the non-syntactic nontermi-
nal reflects the intuition that the English phrase ?the
mutual? corresponds to a partially realized NPB that
needs an NN to its right in order to be fully real-
ized.
47
Our hope is that the rules headed by pseudo
nonterminals could make available to an SPMT
system all the rules that are typically available to
a phrase-based system; and that the sibling rules
could provide a sufficiently robust generalization
layer for integrating pseudo, partially realized con-
stituents into the overall decoding process.
2.2.5 SPMT Model 2 Composed
The SPMT composed model 2 uses all rule
types described in the previous models.
2.3 Estimating rule probabilities
For each model, we extract all rule instances that
are licensed by a symmetrized Giza-aligned paral-
lel corpus and the constraints we put on the model.
We condition on the root node of each rule and use
the rule counts f(r) and a basic maximum likeli-
hood estimator to assign to each rule type a condi-
tional probability (see equation 5).
p(r|root(r)) = f(r)?
r?:root(r?)=root(r) f(r?)
(5)
It is unlikely that this joint probability model
can be discriminative enough to distinguish be-
tween good and bad translations. We are not too
concerned though because, in practice, we decode
using a larger set of submodels (feature functions).
Given the way all our lexicalized xRS rules have
been created, one can safely strip out the syntac-
tic information and end up with phrase-to-phrase
translation rules. For example, in string-to-string
world, rule r5 in Figure 1 can be rewritten as ?france
? FRANCE?; and rule r6 can be rewritten as ?france
and ? FRANCE AND?. When one analyzes the lex-
icalized xRS rules in this manner, it is easy to as-
sociate with them any of the submodel probability
distributions that have been proven useful in statis-
tical phrase-based MT. The non-lexicalized rules
are assigned probability distributions under these
submodels as well by simply assuming a NULL
phrase for any missing lexicalized source or target
phrase.
In the experiments described in this paper, we
use the following submodels (feature functions):
Syntax-based-like submodels:
? proot(ri) is the root normalized conditional
probability of all the rules in a model.
? pcfg(ri) is the CFG-like probability of the
non-lexicalized rules in the model. The lexi-
calized rules have by definition pcfg = 1.
? is lexicalized(ri) is an indicator feature func-
tion that has value 1 for lexicalized rules, and
value 0 otherwise.
? is composed(ri) is an indicator feature func-
tion that has value 1 for composed rules.
? is lowcount(ri) is an indicator feature func-
tion that has value 1 for the rules that occur
less than 3 times in the training corpus.
Phrase-based-like submodels:
? lex pef(ri) is the direct phrase-based con-
ditional probability computed over the for-
eign/source and target phrases subsumed by
a rule.
? lex pfe(ri) is the inverse phrase-based condi-
tional probability computed over the source
and target phrases subsumed by a rule.
? m1(ri) is the IBM model 1 probability com-
puted over the bags of words that occur on
the source and target sides of a rule.
? m1inv(ri) is the IBM model 1 inverse prob-
ability computed over the bags of words that
occur on the source and target sides of a rule.
? lm(e) is the language model probability of
the target translation under an ngram lan-
guage model.
? wp(e) is a word penalty model designed to
favor longer translations.
All these models are combined log-linearly dur-
ing decoding. The weights of the models are
computed automatically using a variant of the
Maximum Bleu training procedure proposed by
Och (2003).
The phrase-based-like submodels have been
proved useful in phrase-based approaches to
SMT (Och and Ney, 2004). The first two syntax-
based submodels implement a ?fused? translation
and lexical grounded distortion model (proot) and
a syntax-based distortion model (pcfg). The indi-
cator submodels are used to determine the extent
to which our system prefers lexicalized vs. non-
lexicalized rules; simple vs. composed rules; and
high vs. low count rules.
48
3 Decoding
3.1 Decoding with one SPMT model
We decode with each of our SPMT models using
a straightforward, bottom-up, CKY-style decoder
that builds English syntactic constituents on the
top of Chinese sentences. The decoder uses a bina-
rized representation of the rules, which is obtained
via a syncronous binarization procedure (Zhang et
al., 2006). The CKY-style decoder computes the
probability of English syntactic constituents in a
bottom up fashion, by log-linearly interpolating all
the submodel scores described in Section 2.3.
The decoder is capable of producing nbest
derivations and nbest lists (Knight and Graehl,
2005), which are used for Maximum Bleu train-
ing (Och, 2003). When decoding the test cor-
pus, the decoder returns the translation that has the
most probable derivation; in other words, the sum
operator in equation 4 is replaced with an argmax.
3.2 Decoding with multiple SPMT models
Combining multiple MT outputs to increase per-
formance is, in general, a difficult task (Matusov
et al, 2006) when significantly different engines
compete for producing the best outputs. In our
case, combining multiple MT outputs is much
simpler because the submodel probabilities across
the four models described here are mostly iden-
tifical, with the exception of the root normalized
and CFG-like submodels which are scaled differ-
ently ? since Model 2 composed has, for example,
more rules than Model 1, the root normalized and
CFG-like submodels have smaller probabilities for
identical rules in Model 2 composed than in Model
1. We compare these two probabilities across the
submodels and we scale all model probabilities to
be compatible with those of Model 2 composed.
With this scaling procedure into place, we pro-
duce 6,000 non-unique nbest lists for all sentences
in our development corpus, using all SPMT sub-
models. We concatenate the lists and we learn a
new combination of weights that maximizes the
Bleu score of the combined nbest list using the
same development corpus we used for tuning the
individual systems (Och, 2003). We use the new
weights in order to rerank the nbest outputs on the
test corpus.
4 Experiments
4.1 Automatic evaluation of the models
We evaluate our models on a Chinese to English
machine translation task. We use the same training
corpus, 138.7M words of parallel Chinese-English
data released by LDC, in order to train several
statistical-based MT systems:
? PBMT, a strong state of the art phrase-based
system that implements the alignment tem-
plate model (Och and Ney, 2004); this is the
system ISI has used in the 2004 and 2005
NIST evaluations.
? four SPMT systems (M1, M1C, M2, M2C)
that implement each of the models discussed
in this paper;
? a SPMT system, Comb, that combines the
outputs of all SPMT models using the pro-
cedure described in Section 3.2.
In all systems, we use a rule extraction algo-
rithm that limits the size of the foreign/source
phrases to four words. For all systems, we use
a Kneser-Ney (1995) smoothed trigram language
model trained on 2.3 billion words of English. As
development data for the SPMT systems, we used
the sentences in the 2002 NIST development cor-
pus that are shorter than 20 words; we made this
choice in order to finish all experiments in time for
this submission. The PBMT system used all sen-
tences in the 2002 NIST corpus for development.
As test data, we used the 2003 NIST test set.
Table 1 shows the number of string-to-string or
tree-to-string rules extracted by each system and
the performance on both the subset of sentences in
the test corpus that were shorter than 20 words and
the entire test corpus. The performance is mea-
sured using the Bleu metric (Papineni et al, 2002)
on lowercased, tokenized outputs/references.
The results show that the SPMT models clearly
outperform the phrase-based systems ? the 95%
confidence intervals computed via bootstrap re-
sampling in all cases are around 1 Bleu point. The
results also show that the simple system combina-
tion procedure that we have employed is effective
in our setting. The improvement on the develop-
ment corpus transfers to the test setting as well.
A visual inspection of the outputs shows signif-
icant differences between the outputs of the four
models. The models that use composed rules pre-
fer to produce outputs by using mostly lexicalized
49
System # of rules Bleu score Bleu score Bleu score
(in millions) on Dev on Test on Test
(4 refs) (4 refs) (4 refs)
< 20 words < 20 words
PBMT 125.8 34.56 34.83 31.46
SPMT-M1 34.2 37.60 38.18 33.15
SPMT-M1C 75.7 37.30 38.10 32.39
SPMT-M2 70.4 37.77 38.74 33.39
SPMT-M2C 111.1 37.48 38.59 33.16
SPMT-Comb 111.1 39.44 39.56 34.10
Table 1: Automatic evaluation results.
rules; in contrast, the simple M1 and M2 mod-
els produce outputs in which content is translated
primarily using lexicalized rules and reorderings
and word insertions are explained primarily by the
non-lexical rules. It appears that the two strategies
are complementary, succeeding and failing in dif-
ferent instances. We believe that this complemen-
tarity and the overcoming of some of the search
errors in our decoder during the model rescoring
phase explain the success of the system combina-
tion experiments.
We suspect that our decoder still makes many
search errors. In spite of this, the SPTM outputs
are still significantly better than the PBMT out-
puts.
4.2 Human-based evaluation of the models
We also tested whether the Bleu score improve-
ments translate into improvements that can be per-
ceived by humans. To this end, we randomly se-
lected 138 sentences of less than 20 words from
our development corpus; we expected the transla-
tion quality of sentences of this size to be easier to
assess than that of sentences that are very long.
We prepared a web-based evaluation interface
that showed for each input sentence:
? the Chinese input;
? three English reference translations;
? the output of seven ?MT systems?.
The evaluated ?MT systems? were the six systems
shown in Table 1 and one of the reference trans-
lations. The reference translation presented as
automatically produced output was selected from
the set of four reference translations provided by
NIST so as to be representative of human transla-
tion quality. More precisely, we chose the second
best reference translation in the NIST corpus ac-
cording to its Bleu score against the other three
reference translations. The seven outputs were
randomly shuffled and presented to three English
speakers for assessment.
The judges who participated in our experiment
were instructed to carefully read the three refer-
ence translations and seven machine translation
outputs, and assign a score between 1 and 5 to
each translation output on the basis of its quality.
Human judges were told that the translation qual-
ity assessment should take into consideration both
the grammatical fluency of the outputs and their
translation adequacy. Table 2 shows the average
scores obtained by each system according to each
judge. For convenience, the table also shows the
Bleu scores of all systems (including the human
translations) on three reference translations.
The results in Table 2 show that the human
judges are remarkably consistent in preferring the
syntax-based outputs over the phrase-based out-
puts. On a 1 to 5 quality scale, the difference be-
tween the phrase-based and syntax-based systems
was, on average, between 0.2 and 0.3 points. All
differences between the phrase-based baseline and
the syntax-based outputs were statistically signif-
icant. For example, when comparing the phrase-
based baseline against the combined system, the
improvement in human scores was significant at
P = 4.04e?6(t = 4.67, df = 413).
The results also show that the LDC reference
translations are far from being perfect. Although
we selected from the four references the second
best according to the Bleu metric, this human ref-
erence was judged to be at a quality level of only
4.67 on a scale from 1 to 5. Most of the translation
errors were fluency errors. Although the human
outputs had most of the time the right meaning,
the syntax was sometimes incorrect.
In order to give readers a flavor of the types
of re-orderings enabled by the SPMT models, we
present in Table 3, several translation outputs pro-
duced by the phrase-based baseline and the com-
50
System Bleu score Judge 1 Judge 2 Judge 3 Judge
on Dev avg
(3 refs)
< 20 words
PBMT 31.00 3.00 3.34 2.95 3.10
SPMT-M1 33.79 3.28 3.49 3.04 3.27
SPMT-M1C 33.66 3.23 3.43 3.26 3.31
SPMT-M2 34.05 3.24 3.45 3.10 3.26
SPMT-M2C 33.42 3.24 3.48 3.13 3.28
SPMT-Combined 35.33 3.31 3.59 3.25 3.38
Human Ref 40.84 4.64 4.62 4.75 4.67
Table 2: Human-based evaluation results.
bined SPMT system. The outputs were selected to
reflect both positive and negative effects of large-
scale re-orderings.
5 Discussion
The SPMT models are similar to the models pro-
posed by Chiang (2005) and Galley et al (2006).
If we analyze these three models in terms of ex-
pressive power, the Galley et al (2006) model is
more expressive than the SPMT models, which
in turn, are more expressive than Chiang?s model.
The xRS formalism utilized by Galley et al (2006)
allows for the use of translation rules that have
multi-level target tree annotations and discontin-
uous source language phrases. The SPMT mod-
els are less general: they use translation rules that
have multi-level target tree annotations but require
that the source language phrases are continuous.
The Syncronous Grammar formalism utilized by
Chiang is stricter than SPMT since it allows only
for single-level target tree annotations.
The parameters of the SPMT models presented
in this paper are easier to estimate than those of
Galley et als (2006) and can easily exploit and
expand on previous research in phrase-based ma-
chine translation. Also, the SPMT models yield
significantly fewer rules that the model of Galley
et al In contrast with the model proposed by Chi-
ang, the SPMT models introduced in this paper are
fully grounded in syntax; this makes them good
candidates for exploring the impact that syntax-
based language models could have on translation
performance.
From a machine translation perspective, the
SPMT translation model family we have proposed
in this paper is promising. To our knowledge,
we are the first to report results that show that a
syntax-based system can produce results that are
better than those produced by a strong phrase-
based system in experimental conditions similar
to those used in large-scale, well-established in-
dependent evaluations, such as those carried out
annually by NIST.
Although the number of syntax-based rules
used by our models is smaller than the number
of phrase-based rules used in our state-of-the-art
baseline system, the SPMT models produce out-
puts of higher quality. This feature is encouraging
because it shows that the syntactified translation
rules learned in the SPMT models can generalize
better than the phrase-based rules.
We were also pleased to see that the Bleu
score improvements going from the phrase- to the
syntax-based models, as well as the Bleu improve-
ments going from the simple syntax-based models
to the combined models system are fully consis-
tent with the human qualitative judgments in our
subjective evaluations. This correlation suggests
that we can continue to use the Bleu metric to fur-
ther improve our models and systems.
Acknowledgements. This research was par-
tially supported by the National Institute of Stan-
dards and Technology?s Advanced Technology
Program Award 70NANB4H3050 to Language
Weaver Inc.
References
R. Bonnema. 2002. Probability models for DOP. In
Data-Oriented Parsing. CSLI publications.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June.
51
System Output
PBMT fujian is china ?s coastal areas most rapid development of foreign trade of the region .
SPMT-Combined china ?s coastal areas of fujian is one of the areas of the most rapid development of
foreign trade and economic cooperation .
PBMT investment in macao has become the largest foreign investors .
SPMT-Combined the chinese - funded enterprises have become the largest foreign investor in macao.
PBMT they are now two people were unaccounted for .
SPMT-Combined currently , both of them remain unaccounted for .
PBMT there was no further statement .
SPMT-Combined the statement did not explain further .
Table 3: Sample translations.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In HLT-NAACL?2004: Main Proceedings,
pages 273?280, Boston, Massachusetts, USA, May
2 - May 7.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training
of context-rich syntax translation models. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL?2006), Sydney,
Australia, July.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP?95),
volume 1, pages 181?184.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natu-
ral language processing. In Proc. of the Sixth In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing?2005),
pages 1?25. Springer Verlag.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT-NAACL?2003), Edmon-
ton, Canada, May 27?June 1.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?2002), pages 133?139, Philadelphia, PA,
July 6-7.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypothesis alignment. In Proceedings of the An-
nual Meeting of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL?2006),
Trento, Italy.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4), Decem-
ber.
Franz Joseph Och. 2003. Minimum error training
in statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL?2003), pages 160?167,
Sapooro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, John
Henderson, and Florence Reeder. 2002. Corpus-
based comprehensive and diagnostic MT evaluation:
Initial Arabic, Chinese, French, and Spanish results.
In Proceedings of the Human Language Technology
Conference (ACL?2002), pages 124?127, San Diego,
CA, March 24-27.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?2005), pages 271?279, Ann
Arbor, Michigan, June.
Radu Soricut. 2005. A reimplementation of Collins?s
parsing models.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Syncronous binarization for ma-
chine translation. In Proceding of the Human Lan-
guage Technology and North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?2006), New York, June.
52
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961?968,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Scalable Inference and Training of
Context-Rich Syntactic Translation Models
Michel Galley*, Jonathan Graehl?, Kevin Knight??, Daniel Marcu??,
Steve DeNeefe?, Wei Wang? and Ignacio Thayer?
*Columbia University
Dept. of Computer Science
New York, NY 10027
galley@cs.columbia.edu, {graehl,knight,marcu,sdeneefe}@isi.edu,
wwang@languageweaver.com, thayer@google.com
?University of Southern California
Information Sciences Institute
Marina del Rey, CA 90292
?Language Weaver, Inc.
4640 Admiralty Way
Marina del Rey, CA 90292
Abstract
Statistical MT has made great progress in the last
few years, but current translation models are weak
on re-ordering and target language fluency. Syn-
tactic approaches seek to remedy these problems.
In this paper, we take the framework for acquir-
ing multi-level syntactic translation rules of (Gal-
ley et al, 2004) from aligned tree-string pairs, and
present two main extensions of their approach: first,
instead of merely computing a single derivation that
minimally explains a sentence pair, we construct
a large number of derivations that include contex-
tually richer rules, and account for multiple inter-
pretations of unaligned words. Second, we pro-
pose probability estimates and a training procedure
for weighting these rules. We contrast different
approaches on real examples, show that our esti-
mates based on multiple derivations favor phrasal
re-orderings that are linguistically better motivated,
and establish that our larger rules provide a 3.63
BLEU point increase over minimal rules.
1 Introduction
While syntactic approaches seek to remedy word-
ordering problems common to statistical machine
translation (SMT) systems, many of the earlier
models?particularly child re-ordering models?
fail to account for human translation behavior.
Galley et al (2004) alleviate this modeling prob-
lem and present a method for acquiring millions
of syntactic transfer rules from bilingual corpora,
which we review below. Here, we make the fol-
lowing new contributions: (1) we show how to
acquire larger rules that crucially condition on
more syntactic context, and show how to com-
pute multiple derivations for each training exam-
ple, capturing both large and small rules, as well
as multiple interpretations for unaligned words;
(2) we develop probability models for these multi-
level transfer rules, and give estimation methods
for assigning probabilities to very large rule sets.
We contrast our work with (Galley et al, 2004),
highlight some severe limitations of probability
estimates computed from single derivations, and
demonstrate that it is critical to account for many
derivations for each sentence pair. We also use
real examples to show that our probability mod-
els estimated from a large number of derivations
favor phrasal re-orderings that are linguistically
well motivated. An empirical evaluation against
a state-of-the-art SMT system similar to (Och and
Ney, 2004) indicates positive prospects. Finally,
we show that our contextually richer rules provide
a 3.63 BLEU point increase over those of (Galley
et al, 2004).
2 Inferring syntactic transformations
We assume we are given a source-language (e.g.,
French) sentence f , a target-language (e.g., En-
glish) parse tree pi, whose yield e is a translation
of f , and a word alignment a between f and e.
Our aim is to gain insight into the process of trans-
forming pi into f and to discover grammatically-
grounded translation rules. For this, we need
a formalism that is expressive enough to deal
with cases of syntactic divergence between source
and target languages (Fox, 2002): for any given
(pi, f ,a) triple, it is useful to produce a derivation
that minimally explains the transformation be-
tween pi and f , while remaining consistent with a.
Galley et al (2004) present one such formalism
(henceforth ?GHKM?).
2.1 Tree-to-string alignments
It is appealing to model the transformation of pi
into f using tree-to-string (xRs) transducers, since
their theory has been worked out in an exten-
sive literature and is well understood (see, e.g.,
(Graehl and Knight, 2004)). Formally, transfor-
mational rules ri presented in (Galley et al, 2004)
are equivalent to 1-state xRs transducers mapping
a given pattern (subtree to match in pi) to a right
hand side string. We will refer to them as lhs(ri)
and rhs(ri), respectively. For example, some xRs
961
rules may describe the transformation of does not
into ne ... pas in French. A particular instance may
look like this:
VP(AUX(does), RB(not), x0:VB) ? ne, x0, pas
lhs(ri) can be any arbitrary syntax tree fragment.
Its leaves are either lexicalized (e.g. does) or vari-
ables (x0, x1, etc). rhs(ri) is represented as a se-
quence of target-language words and variables.
Now we give a brief overview of how such
transformational rules are acquired automatically
in GHKM.1 In Figure 1, the (pi, f ,a) triple is rep-
resented as a directed graph G (edges going down-
ward), with no distinction between edges of pi and
alignments. Each node of the graph is labeled with
its span and complement span (the latter in italic
in the figure). The span of a node n is defined by
the indices of the first and last word in f that are
reachable from n. The complement span of n is
the union of the spans of all nodes n? in G that
are neither descendants nor ancestors of n. Nodes
of G whose spans and complement spans are non-
overlapping form the frontier set F ? G.
What is particularly interesting about the fron-
tier set? For any frontier of graph G containing
a given node n ? F , spans on that frontier de-
fine an ordering between n and each other frontier
node n?. For example, the span of VP[4-5] either
precedes or follows, but never overlaps the span of
any node n? on any graph frontier. This property
does not hold for nodes outside of F . For instance,
PP[4-5] and VBG[4] are two nodes of the same
graph frontier, but they cannot be ordered because
of their overlapping spans.
The purpose of xRs rules in this framework is
to order constituents along sensible frontiers in G,
and all frontiers containing undefined orderings,
as between PP[4-5] and VBG[4], must be disre-
garded during rule extraction. To ensure that xRs
rules are prevented from attempting to re-order
any such pair of constituents, these rules are de-
signed in such a way that variables in their lhs can
only match nodes of the frontier set. Rules that
satisfy this property are said to be induced by G.2
For example, rule (d) in Table 1 is valid accord-
ing to GHKM, since the spans corresponding to
1Note that we use a slightly different terminology.
2Specifically, an xRs rule ri is extracted fromG by taking
a subtree ? ? pi as lhs(ri), appending a variable to each
leaf node of ? that is internal to pi, adding those variables to
rhs(ri), ordering them in accordance to a, and if necessary
inserting any word of f to ensure that rhs(ri) is a sequence of
contiguous spans (e.g., [4-5][6][7-8] for rule (f) in Table 1).
DT
CD
VBP
NNS
IN
NNP
NP
NNS
VBG
3
2
2
1
7-8
4
4
5
9
1
2
3
4
5
6
7
8
9
3 1-2,4
-9
2 1-9
2 1-9
1 2-9
7-8 1-5,9
4 1-9
4 1-9
5 1-4,7
-9
9 1-8
1-2 3-9
NP 7-8 1-5,9
NP 5 1-4, 7
-9
PP 4-5 1-4,7
-9
VP 4-5 1-3,7
-9
NP 4-8 1-3,9
VP 3-8 1-2,9
S 1-9 ?
7!
"#
$
%&
'(
)
*+
,
.
Thes
e
peop
le
inclu
de
astro
naut
s
com
ing
from
Fran
ce
..
7
-
Figure 1: Spans and complement-spans determine what
rules are extracted. Constituents in gray are members of the
frontier set; a minimal rule is extracted from each of them.
(a) S(x0:NP, x1:VP, x2:.) ? x0, x1, x2
(b) NP(x0:DT, CD(7), NNS(people)) ? x0, 7?
(c) DT(these) ??
(d) VP(x0:VBP, x1:NP) ? x0, x1
(e) VBP(include) ?-?
(f) NP(x0:NP, x1:VP) ? x1,?, x0
(g) NP(x0:NNS) ? x0
(h) NNS(astronauts) ??*,X
(i) VP(VBG(coming), PP(IN(from), x0:NP)) ?e?, x0
(j) NP(x0:NNP) ? x0
(k) NNP(France) ???
(l) .(.) ? .
Table 1: A minimal derivation corresponding to Figure 1.
its rhs constituents (VBP[3] and NP[4-8]) do not
overlap. Conversely, NP(x0:DT, x1:CD:, x2:NNS)
is not the lhs of any rule extractible from G, since
its frontier constituents CD[2] and NNS[2] have
overlapping spans.3 Finally, the GHKM proce-
dure produces a single derivation from G, which
is shown in Table 1.
The concern in GHKM was to extract minimal
rules, whereas ours is to extract rules of any arbi-
trary size. Minimal rules defined over G are those
that cannot be decomposed into simpler rules in-
duced by the same graph G, e.g., all rules in Ta-
ble 1. We call minimal a derivation that only con-
tains minimal rules. Conversely, a composed rule
results from the composition of two or more min-
imal rules, e.g., rule (b) and (c) compose into:
NP(DT(these), CD(7), NNS(people)) ??, 7?
3It is generally reasonable to also require that the root n
of lhs(ri) be part of F , because no rule induced by G can
compose with ri at n, due to the restrictions imposed on the
extraction procedure, and ri wouldn?t be part of any valid
derivation.
962
OR
NP
(x0
:NP
, x1
:V
P) 
!
x1
,!
, x0
VP
(x0
:VB
P, 
x1:
NP
) 
!
x0
 , x
1
S(x
0:N
P, 
x1:
VP
, x
2:.
) 
!
x0
 , x
1, x
2
NP
(x0
:DT
 CD
(7)
, N
NS
(pe
opl
e))
 
!
x0
, 7"
.(.)
 
!
.
DT
(th
ese
) 
!
#
VB
P(i
ncl
ude
) 
!
$%
&
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
  
!
'(
, x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
NP
(x0
:NN
S) 
!
x0
NP
(x0
:NN
S) 
!
!,
 x0
NP
(x0
:NN
P) 
!
x0
, !
NN
P(F
ran
ce)
 
!
)*
NN
S(a
stro
nau
ts) 
!
+,
, -
OR
OR N
NS
(as
tro
nau
ts) 
!!
,+
,,
 -
OR
NP
(x0
:NN
P) 
!
x0
NP
(x0
:NN
P) 
!
x0
NN
P(F
ran
ce)
 
!
)*
, !
NP
(x0
:NN
S) 
!
x0
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m),
 x0
:N
P))
 
!
'(
, x0
co
min
g
fro
m
NN
S
IN
NN
P
NP
VP
NP
VB
G
PP
NP
7-8
5
7-8
5
7-8
4
4
5
4
5
6
7
8
4
4
4-5
4-5
4-8
NN
P(F
ran
ce)
 
!)
*,
 !
NP
(x0
:NN
P) 
!
x0
, !
VP
(V
BG
(co
mi
ng)
, 
PP
(IN
(fro
m)
, 
x0:
NP
))  
!
'(
, x0
, !
NN
S(a
stro
nau
ts) 
!
! ,
 +
,,
 -
NP
(x0
:NN
S) 
!
! ,
 x0
NP
(x0
:NP
, x1
:V
P) 
!
x1
, !
, x0
(a)
(b)
-
'(
)*
!
+,
as
tro
na
uts
Fra
nce
Figure 2: (a) Multiple ways of aligning? to constituents in the tree. (b) Derivation corresponding to the parse tree in Figure 1,
which takes into account all alignments of? pictured in (a).
Note that these properties are dependent on G, and
the above rule would be considered a minimal rule
in a graph G? similar to G, but additionally con-
taining a word alignment between 7 and ?. We
will see in Sections 3 and 5 why extracting only
minimal rules can be highly problematic.
2.2 Unaligned words
While the general theory presented in GHKM ac-
counts for any kind of derivation consistent with
G, it does not particularly discuss the case where
some words of the source-language string f are
not aligned to any word of e, thus disconnected
from the rest of the graph. This case is highly fre-
quent: 24.1% of Chinese words in our 179 mil-
lion word English-Chinese bilingual corpus are
unaligned, and 84.8% of Chinese sentences con-
tain at least one unaligned word. The question is
what to do with such lexical items, e.g., ? in
Figure 2(a). The approach of building one mini-
mal derivation for G as in the algorithm described
in GHKM assumes that we commit ourselves to
a particular heuristic to attach the unaligned item
to a certain constituent of pi, e.g., highest attach-
ment (in the example, ? is attached to NP[4-8]
and the heuristic generates rule (f)). A more rea-
sonable approach is to invoke the principle of in-
sufficient reason and make no a priori assump-
tion about what is a ?correct? way of assigning
the item to a constituent, and return all derivations
that are consistent with G. In Section 4, we will
see how to use corpus evidence to give preference
to unaligned-word attachments that are the most
consistent across the data. Figure 2(a) shows the
six possible ways of attaching ? to constituents
of pi: besides the highest attachment (rule (f)),?
can move along the ancestors of France, since it is
to the right of the translation of that word, and be
considered to be part of an NNP, NP, or VP rule.
We make the same reasoning to the left: ? can
either start the NNS of astronauts, or start an NP.
Our account of all possible ways of consistently
attaching ? to constituents means we must ex-
tract more than one derivation to explain transfor-
mations in G, even if we still restrict ourselves to
minimal derivations (a minimal derivation for G
is unique if and only if no source-language word
in G is unaligned). While we could enumerate
all derivations separately, it is much more effi-
cient both in time and space to represent them as a
derivation forest, as in Figure 2(b). Here, the for-
est covers all minimal derivations that correspond
to G. It is necessary to ensure that for each deriva-
tion, each unaligned item (here ?) appears only
once in the rules of that derivation, as shown in
Figure 2 (which satisfies the property). That re-
quirement will prove to be critical when we ad-
dress the problem of estimating probabilities for
our rules: if we allowed in our example to spuri-
ously generate?s in multiple successive steps of
the same derivation, we would not only represent
the transformation incorrectly, but also ?-rules
would be disproportionately represented, leading
to strongly biased estimates. We will now see how
to ensure this constraint is satisfied in our rule ex-
traction and derivation building algorithm.
963
2.3 Algorithm
The linear-time algorithm presented in GHKM is
only a particular case of the more general one we
describe here, which is used to extract all rules,
minimal and composed, induced by G. Similarly
to the GHKM algorithm, ours performs a top-
down traversal of G, but differs in the operations
it performs at each node n ? F : we must explore
all subtrees rooted at n, find all consistent ways
of attaching unaligned words of f, and build valid
derivations in accordance to these attachments.
We use a table or-dforest[x, y, c] to store OR-
nodes, in which each OR-node can be uniquely
defined by a syntactic category c and a span [x, y]
(which may cover unaligned words of f). This ta-
ble is used to prevent the same partial derivation
to be followed multiple times (the in-degrees of
OR-nodes generally become large with composed
rules). Furthermore, to avoid over-generating un-
aligned words, the root and variables in each rule
are represented with their spans. For example, in
Figure 2(b), the second and third child of the top-
most OR-node respectively span across [4-5][6-8]
and [4-6][7-8] (after constituent reordering). In
the former case, ? will eventually be realized in
an NP, and in the latter case, in a VP.
The preprocessing step consists of assigning
spans and complement spans to nodes of G, in
the first case by a bottom-up exploration of the
graph, and in the latter by a top-down traversal.
To assign complement spans, we assign the com-
plement span of any node n to each of its children,
and for each of them, add the span of the child
to the complement span of all other children. In
another traversal of G, we determine the minimal
rule extractible from each node in F .
We explore all tree fragments rooted at n by
maintaining an open and a closed queue of rules
extracted from n (qo and qc). At each step, we
pick the smallest rule in qo, and for each of its
variable nodes, try to discover new rules (?succes-
sor rules?) by means of composition with minimal
rules, until a given threshold on rule size or maxi-
mum number of rules in qc is reached. There may
be more that one successor per rule, since we must
account for all possible spans than can be assigned
to non-lexical leaves of a rule. Once a threshold is
reached, or if the open queue is empty, we connect
a new OR-node to all rules that have just been ex-
tracted from n, and add it to or-dforest. Finally,
we proceed recursively, and extract new rules from
each node at the frontier of the minimal rule rooted
at n. Once all nodes of F have been processed, the
or-dforest table contains a representation encod-
ing only valid derivations.
3 Probability models
The overall goal of our translation system is to
transform a given source-language sentence f
into an appropriate translation e in the set E
of all possible target-language sentences. In a
noisy-channel approach to SMT, we uses Bayes?
theorem and choose the English sentence e? ? E
that maximizes:4
e? = argmax
e?E
{
Pr(e) ? Pr(f |e)
}
(1)
Pr(e) is our language model, and Pr(f |e) our
translation model. In a grammatical approach to
MT, we hypothesize that syntactic information
can help produce good translation, and thus
introduce dependencies on target-language syntax
trees. The function to optimize becomes:
e? = argmax
e?E
{
Pr(e) ?
?
pi??(e)
Pr(f |pi) ?Pr(pi|e)
}
(2)
?(e) is the set of all English trees that yield the
given sentence e. Estimating Pr(pi|e) is a prob-
lem equivalent to syntactic parsing and thus is not
discussed here. Estimating Pr(f |pi) is the task of
syntax-based translation models (SBTM).
Given a rule set R, our SBTM makes the
common assumption that left-most compositions
of xRs rules ?i = r1 ? ... ? rn are independent
from one another in a given derivation ?i ? ?,
where ? is the set of all derivations constructible
from G = (pi, f ,a) using rules of R. Assuming
that ? is the set of all subtree decompositions of pi
corresponding to derivations in ?, we define the
estimate:
Pr(f |pi) =
1
|?|
?
?i??
?
rj??i
p(rhs(rj)|lhs(rj)) (3)
under the assumption:
?
rj?R:lhs(rj)=lhs(ri)
p(rhs(rj)|lhs(rj)) = 1 (4)
It is important to notice that the probability
distribution defined in Equation 3 requires a
normalization factor (|?|) in order to be tight, i.e.,
sum to 1 over all strings fi ? F that can be derived
4We denote general probability distributions with Pr(?)
and use p(?) for probabilities assigned by our models.
964
Xa
Y b
a?
b?
c?c
(!,f 1
,a 1):
X
a
Y b
b?
a?
c?c
(!,f 2
,a 2):
Figure 3: Example corpus.
from pi. A simple example suffices to demonstrate
it is not tight without normalization. Figure 3
contains a sample corpus from which four rules
can be extracted:
r1: X(a, Y(b, c)) ? a?, b?, c?
r2: X(a, Y(b, c)) ? b?, a?, c?
r3: X(a, x0:Y) ? a?, x0
r4: Y(b, c) ? b?, c?
From Equation 4, the probabilities of r3 and r4
must be 1, and those of r1 and r2 must sum to
1. Thus, the total probability mass, which is dis-
tributed across two possible output strings a?b?c?
and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 +
p3 ? p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).
It is relatively easy to prove that the probabil-
ities of all derivations that correspond to a given
decomposition ?i ? ? sum to 1 (the proof is omit-
ted due to constraints on space). From this prop-
erty we can immediately conclude that the model
described by Equation 3 is tight.5
We examine two estimates p(rhs(r)|lhs(r)).
The first one is the relative frequency estimator
conditioning on left hand sides:
p(rhs(r)|lhs(r)) =
f(r)
?
r?:lhs(r?)=lhs(r) f(r
?)
(5)
f(r) represents the number of times rule r oc-
curred in the derivations of the training corpus.
One of the major negative consequences of
extracting only minimal rules from a corpus is
that an estimator such as Equation 5 can become
extremely biased. This again can be observed
from Figure 3. In the minimal-rule extraction of
GHKM, only three rules are extracted from the ex-
ample corpus, i.e. rules r2, r3, and r4. Let?s as-
sume now that the triple (pi, f1,a1) is represented
99 times, and (pi, f2,a2) only once. Given a tree
pi, the model trained on that corpus can generate
the two strings a?b?c? and b?a?c? only through two
derivations, r3 ? r4 and r2, respectively. Since
all rules in that example have probability 1, and
5If each tree fragment in pi is the lhs of some rule in R,
then we have |?| = 2n, where n is the number of nodes of
the frontier set F ? G (each node is a binary choice point).
given that the normalization factor |?| is 2, both
probabilities p(a?b?c?|pi) and p(b?a?c?|pi) are 0.5.
On the other hand, if all rules are extracted and
incorporated into our relative-frequency probabil-
ity model, r1 seriously counterbalances r2 and the
probability of a?b?c? becomes: 12 ?(
99
100+1) = .995
(since it differs from .99, the estimator remains bi-
ased, but to a much lesser extent).
An alternative to the conditional model of
Equation 3 is to use a joint model conditioning on
the root node instead of the entire left hand side:
p(r|root(r)) =
f(r)
?
r?:root(r?)=root(r) f(r
?)
(6)
This can be particularly useful if no parser or
syntax-based language model is available, and we
need to rely on the translation model to penalize
ill-formed parse trees. Section 6 will describe an
empirical evaluation based on this estimate.
4 EM training
In our previous discussion of parameter estima-
tion, we did not explore the possibility that one
derivation in a forest may be much more plau-
sible than the others. If we knew which deriva-
tion in each forest was the ?true? derivation, then
we could straightforwardly collect rule counts off
those derivations. On the other hand, if we had
good rule probabilities, we could compute the
most likely (Viterbi) derivations for each training
example. This is a situation in which we can em-
ploy EM training, starting with uniform rule prob-
abilities. For each training example, we would like
to: (1) score each derivation ?i as a product of the
probabilities of the rules it contains, (2) compute
a conditional probability pi for each derivation ?i
(conditioned on the observed training pair) by nor-
malizing those scores to add to 1, and (3) collect
weighted counts for each rule in each ?i, where
the weight is pi. We can then normalize the counts
to get refined probabilities, and iterate; the corpus
likelihood is guaranteed to improve with each it-
eration. While it is infeasible to enumerate the
millions of derivations in each forest, Graehl and
Knight (2004) demonstrate an efficient algorithm.
They also analyze how to train arbitrary tree trans-
ducers into two steps. The first step is to build a
derivation forest for each training example, where
the forest contains those derivations licensed by
the (already supplied) transducer?s rules. The sec-
ond step employs EM on those derivation forests,
running in time proportional to the size of the
965
Best minimal-rule derivation (Cm) p(r)
(a) S(x0:NP-C x1:VP x2:.) ? x0 x1 x2 .845
(b) NP-C(x0:NPB) ? x0 .82
(c) NPB(DT(the) x0:NNS) ? x0 .507
(d) NNS(gunmen) ??K .559
(e) VP(VBD(were) x0:VP-C) ? x0 .434
(f) VP-C(x0:VBN x1:PP) ? x1 x0 .374
(g) PP(x0:IN x1:NP-C) ? x0 x1 .64
(h) IN(by) ?? .0067
(i) NP-C(x0:NPB) ? x0 .82
(j) NPB(DT(the) x0:NN) ? x0 .586
(k) NN(police) ?f? .0429
(l) VBN(killed) ??? .0072
(m) .(.) ? . .981
.
 
The
gunm
enw
ere
killed
by
the
polic
e.
DT
VBD
VBN
DT
NN
NP
PP
VP-C
VPS
NNS
IN
NP
.
!"
#$
%
&'
Best composed-rule derivation (C4) p(r)
(o) S(NP-C(NPB(DT(the) NNS(gunmen))) x0:VP .(.)) ??K x0 . 1
(p) VP(VBD(were) VP-C(x0:VBN PP(IN(by) x1:NP-C))) ?? x1 x0 0.00724
(q) NP-C(NPB(DT(the) NN(police))) ?f? 0.173
(r) VBN(killed) ??? 0.00719
Figure 4: Two most probable derivations for the graph on the right: the top table restricted to minimal rules; the bottom one,
much more probable, using a large set of composed rules. Note: the derivations are constrained on the (pi, f ,a) triple, and thus
include some non-literal translations with relatively low probabilities (e.g. killed, which is more commonly translated as{?).
rule nb. of nb. of deriv- EM-
set rules nodes time time
Cm 4M 192M 2 h. 4 h.
C3 142M 1255M 52 h. 34 h.
C4 254M 2274M 134 h. 60 h.
Table 2: Rules and derivation nodes for a 54M-word, 1.95M
sentence pair English-Chinese corpus, and time to build
derivations (on 10 cluster nodes) and run 50 EM iterations.
forests. We only need to borrow the second step
for our present purposes, as we construct our own
derivation forests when we acquire our rule set.
A major challenge is to scale up this EM train-
ing to large data sets. We have been able to run
EM for 50 iterations on our Chinese-English 54-
million word corpus. The derivation forests for
this corpus contain 2.2 billion nodes; the largest
forest contains 1.1 million nodes. The outcome
is to assign probabilities to over 254 million rules.
Our EM runs with either lhs normalization or lhs-
root normalization. In the former case, each lhs
has an average of three corresponding rhs?s that
compete with each other for probability mass.
5 Model coverage
We now present some examples illustrating the
benefit of composed rules. We trained three
p(rhs(ri)|lhs(ri)) models on a 54 million-word
English-Chinese parallel corpus (Table 2): the first
one (Cm) with only minimal rules, and the two
others (C3 and C4) additionally considering com-
posed rules with no more than three, respectively
four, internal nodes in lhs(ri). We evaluated these
models on a section of the NIST 2002 evaluation
corpus, for which we built derivation forests and
lhs: S(x0:NP-C VP(x1:VBD x2:NP-C) x3:.)
corpus rhsi p(rhsi|lhs)
Chinese x1 x0 x2 x3 .3681
(minimal) x0 x1 , x3 x2 .0357
x2 , x0 x1 x3 .0287
x0 x1 , x3 x2 . .0267
Chinese x0 x1 x2 x3 .9047
(composed) x0 x1 , x2 x3 .016
x0 , x1 x2 x3 .0083
x0 x1 ? x2 x3 .0072
Arabic x1 x0 x2 x3 .5874
(composed) x0 x1 x2 x3 .4027
x1 x2 x0 x3 .0077
x1 x0 x2 " x3 .0001
Table 3: Our model transforms English subject-verb-object
(SVO) structures into Chinese SVO and into Arabic VSO.
With only minimal rules, Chinese VSO is wrongly preferred.
extracted the most probable one (Viterbi) for each
sentence pair (based on an automatic alignment
produced by GIZA). We noticed in general that
Viterbi derivations according to C4 make exten-
sive usage of composed rules, as it is the case in
the example in Figure 4. It shows the best deriva-
tion according to Cm and C4 on the unseen (pi,f,a)
triple displayed on the right. The second deriva-
tion (log p = ?11.6) is much more probable than
the minimal one (log p = ?17.7). In the case
of Cm, we can see that many small rules must be
applied to explain the transformation, and at each
step, the decision regarding the re-ordering of con-
stituents is made with little syntactic context. For
example, from the perspective of a decoder, the
word by is immediately transformed into a prepo-
sition (IN), but it is in general useful to know
which particular function word is present in the
sentence to motivate good re-orderings in the up-
966
lhs1: NP-C(x0:NPB PP(IN(of) x1:NP-C)) (NP-of-NP)
lhs2: PP(IN(of) NP-C(x0:NPB PP(IN(of) NP-C(x1:NPB x2:VP)))) (of-NP-of-NP-VP)
lhs3: VP(VBD(said) SBAR-C(IN(that) x0:S-C)) (said-that-S)
lhs4: SBAR(WHADVP(WRB(when)) S-C(x0:NP-C VP(VBP(are) x1:VP-C))) (when-NP-are-VP)
rhs1i p(rhs1i|lhs1) rhs2i p(rhs2i|lhs2) rhs3i p(rhs3i|lhs3) rhs4i p(rhs4i|lhs4)
x1 x0 .54 x2 ? x1 ? x0 .6754 ? , x0 .6062 ( x1 x0 ? .6618
x0 x1 .2351 ( x2 ? x1 ? x0 .035 ? x0 .1073 S x1 x0 ? .0724
x1 ? x0 .0334 x2 ? x1 ? x0 , .0263 h: , x0 .0591 ( x1 x0 ? , .0579
x1 x0 ? .026 x2 ? x1 ? x0 	 .0116 ? ? , x0 .0234 , ( x1 x0 ? .0289
Table 4: Translation probabilities promote linguistically motivated constituent re-orderings (for lhs1 and lhs2), and enable
non-constituent (lhs3) and non-contiguous (lhs4) phrasal translations.
per levels of the tree. A rule like (e) is particu-
larly unfortunate, since it allows the word were to
be added without any other evidence that the VP
should be in passive voice. On the other hand, the
composed-rule derivation of C4 incorporates more
linguistic evidence in its rules, and re-orderings
are motivated by more syntactic context. Rule
(p) is particularly appropriate to create a passive
VP construct, since it expects a Chinese passive
marker (?), an NP-C, and a verb in its rhs, and
creates the were ... by construction at once in the
left hand side.
5.1 Syntactic translation tables
We evaluate the promise of our SBTM by analyz-
ing instances of translation tables (t-table). Table 3
shows how a particular form of SVO construc-
tion is transformed into Chinese, which is also an
SVO language. While the t-table for Chinese com-
posed rules clearly gives good estimates for the
?correct? x0 x1 ordering (p = .9), i.e. subject be-
fore verb, the t-table for minimal rules unreason-
ably gives preference to verb-subject ordering (x1
x0, p = .37), because the most probable transfor-
mation (x0 x1) does not correspond to a minimal
rule. We obtain different results with Arabic, an
VSO language, and our model effectively learns
to move the subject after the verb (p = .59).
lhs1 in Table 4 shows that our model is able
to learn large-scale constituent re-orderings, such
as re-ordering NPs in a NP-of-NP construction,
and put the modifier first as it is more commonly
the case in Chinese (p = .54). If more syntac-
tic context is available as in lhs2, our model
provides much sharper estimates, and appropri-
ately reverses the order of three constituents with
high probability (p = .68), inserting modifiers first
(possessive markers? are needed here for better
syntactic disambiguation).
A limitation of earlier syntax-based systems is
their poor handling of non-constituent phrases.
Table 4 shows that our model can learn rules for
such phrases, e.g., said that (lhs3). While the that
has no direct translation, our model effectively
learns to separate? (said) from the relative clause
with a comma, which is common in Chinese.
Another promising prospect of our model seems
to lie in its ability to handle non-contiguous
phrases, a feature that state of the art systems
such as (Och and Ney, 2004) do not incorpo-
rate. The when-NP-are-VP construction of lhs4
presents such a case. Our model identifies that are
needs to be deleted, that when translates into the
phrase( ...?, and that the NP needs to be moved
after the VP in Chinese (p = .66).
6 Empirical evaluation
The task of our decoder is to find the most likely
English tree pi that maximizes all models involved
in Equation 2. Since xRs rules can be converted to
context-free productions by increasing the number
of non-terminals, we implemented our decoder as
a standard CKY parser with beam search. Its rule
binarization is described in (Zhang et al, 2006).
We compare our syntax-based system against
an implementation of the alignment template
(AlTemp) approach to MT (Och and Ney, 2004),
which is widely considered to represent the state
of the art in the field. We registered both systems
in the NIST 2005 evaluation; results are presented
in Table 5. With a difference of 6.4 BLEU points
for both language pairs, we consider the results
of our syntax-based system particularly promis-
ing, since these are the highest scores to date that
we know of using linguistic syntactic transforma-
tions. Also, on the one hand, our AlTemp sys-
tem represents quite mature technology, and in-
corporates highly tuned model parameters. On
the other hand, our syntax decoder is still work in
progress: only one model was used during search,
i.e., the EM-trained root-normalized SBTM, and
as yet no language model is incorporated in the
search (whereas the search in the AlTemp sys-
tem uses two phrase-based translation models and
967
Syntactic AlTemp
Arabic-to-English 40.2 46.6
Chinese-to-English 24.3 30.7
Table 5: BLEU-4 scores for the 2005 NIST test set.
Cm C3 C4
Chinese-to-English 24.47 27.42 28.1
Table 6: BLEU-4 scores for the 2002 NIST test set, with rules
of increasing sizes.
12 other feature functions). Furthermore, our de-
coder doesn?t incorporate any syntax-based lan-
guage model, and admittedly our ability to penal-
ize ill-formed parse trees is still limited.
Finally, we evaluated our system on the NIST-
02 test set with the three different rule sets (see
Table 6). The performance with our largest rule
set represents a 3.63 BLEU point increase (14.8%
relative) compared to using only minimal rules,
which indicates positive prospects for using even
larger rules. While our rule inference algorithm
scales to higher thresholds, one important area of
future work will be the improvement of our de-
coder, conjointly with analyses of the impact in
terms of BLEU of contextually richer rules.
7 Related work
Similarly to (Poutsma, 2000; Wu, 1997; Yamada
and Knight, 2001; Chiang, 2005), the rules dis-
cussed in this paper are equivalent to productions
of synchronous tree substitution grammars. We
believe that our tree-to-string model has several
advantages over tree-to-tree transformations such
as the ones acquired by Poutsma (2000). While
tree-to-tree grammars are richer formalisms that
provide the potential benefit of rules that are lin-
guistically better motivated, modeling the syntax
of both languages comes as an extra cost, and it
is admittedly more helpful to focus our syntac-
tic modeling effort on the target language (e.g.,
English) in cases where it has syntactic resources
(parsers and treebanks) that are considerably more
available than for the source language. Further-
more, we think there is, overall, less benefit in
modeling the syntax of the source language, since
the input sentence is fixed during decoding and is
generally already grammatical.
With the notable exception of Poutsma, most
related works rely on models that are restricted
to synchronous context-free grammars (SCFG).
While the state-of-the-art hierarchical SMT sys-
tem (Chiang, 2005) performs well despite strin-
gent constraints imposed on its context-free gram-
mar, we believe its main advantage lies in its
ability to extract hierarchical rules across phrasal
boundaries. Context-free grammars (such as Penn
Treebank and Chiang?s grammars) make indepen-
dence assumptions that are arguably often unrea-
sonable, but as our work suggests, relaxations
of these assumptions by using contextually richer
rules results in translations of increasing quality.
We believe it will be beneficial to account for this
finding in future work in syntax-based SMT and in
efforts to improve upon (Chiang, 2005).
8 Conclusions
In this paper, we developed probability models for
the multi-level transfer rules presented in (Galley
et al, 2004), showed how to acquire larger rules
that crucially condition on more syntactic context,
and how to pack multiple derivations, including
interpretations of unaligned words, into derivation
forests. We presented some theoretical arguments
for not limiting extraction to minimal rules, val-
idated them on concrete examples, and presented
experiments showing that contextually richer rules
provide a 3.63 BLEU point increase over the min-
imal rules of (Galley et al, 2004).
Acknowledgments
We would like to thank anonymous review-
ers for their helpful comments and suggestions.
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022.
References
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
H. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In Proc. of EMNLP, pages 304?311.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proc. of HLT/NAACL-04.
J. Graehl and K. Knight. 2004. Training tree transducers. In
Proc. of HLT/NAACL-04, pages 105?112.
F. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
A. Poutsma. 2000. Data-oriented translation. In Proc. of
COLING, pages 635?641.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of ACL, pages 523?530.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In Proc. of
HLT/NAACL.
968
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 497?507,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
)HDWXUH5LFK/DQJXDJH,QGHSHQGHQW6\QWD[%DVHG$OLJQPHQWIRU
6WDWLVWLFDO0DFKLQH7UDQVODWLRQ
-DVRQ5LHVD1 $QQ,UYLQH2 'DQLHO0DUFX1
1,QIRUPDWLRQ6FLHQFHV,QVWLWXWH
8QLYHUVLW\RI6RXWKHUQ&DOLIRUQLD
0DULQDGHO5H\ &$ 
{ULHVD PDUFX}#LVLHGX
2'HSDUWPHQWRI&RPSXWHU6FLHQFH
-RKQV+RSNLQV8QLYHUVLW\
%DOWLPRUH 0' 
DQQL#MKXHGX
$EVWUDFW
:HSUHVHQWDQDFFXUDWHZRUGDOLJQPHQWDOJR
ULWKPWKDWKHDYLO\H[SORLWVVRXUFHDQGWDUJHW
ODQJXDJHV\QWD[ 8VLQJDGLVFULPLQDWLYHIUDPH
ZRUNDQGDQHI?FLHQWERWWRPXSVHDUFKDOJR
ULWKP ZHWUDLQDPRGHORIKXQGUHGVRIWKRX
VDQGVRIV\QWDFWLFIHDWXUHV 2XUQHZPRGHO
KHOSVXVWRYHU\DFFXUDWHO\PRGHOV\QWDF
WLFWUDQVIRUPDWLRQVEHWZHHQODQJXDJHV LV
ODQJXDJHLQGHSHQGHQW DQGZLWKDXWRPDWLF
IHDWXUH  H[WUDFWLRQ DVVLVWV  V\VWHP GHYHORSHUV
LQREWDLQLQJJRRGZRUGDOLJQPHQWSHUIRUPDQFH
RIIWKHVKHOIZKHQWDFNOLQJQHZODQJXDJHSDLUV
:HDQDO\]HWKHLPSDFWRIRXUIHDWXUHV GHVFULEH
LQIHUHQFH XQGHU  WKH PRGHO DQG GHPRQVWUDWH
VLJQL?FDQW  DOLJQPHQW  DQG  WUDQVODWLRQ  TXDOLW\
LPSURYHPHQWVRYHUDOUHDG\SRZHUIXOEDVHOLQHV
WUDLQHG RQ YHU\  ODUJH  FRUSRUD :H REVHUYH
WUDQVODWLRQTXDOLW\LPSURYHPHQWVFRUUHVSRQG
LQJWRDQG%/(8 IRU$UDELF(QJOLVKDQG
&KLQHVH(QJOLVK UHVSHFWLYHO\
 ,QWURGXFWLRQ
,QUHFHQW\HDUV VHYHUDOVWDWHRIWKHDUWVWDWLVWLFDOPD
FKLQHWUDQVODWLRQ07 V\VWHPVKDYHLQFRUSRUDWHG
ERWKVRXUFHDQGWDUJHWV\QWD[LQWRWKHJUDPPDUVWKDW
WKH\JHQHUDWHDQGXVHWRWUDQVODWH :KLOHVRPHWUHH
WRWUHH V\VWHPVSDUVH VRXUFH DQG WDUJHW  VHQWHQFHV
VHSDUDWHO\ *DOOH\HWDO  =ROOPDQDQG9HQX
JRSDO  +XDQJDQG0L  RWKHUVSURMHFW
V\QWDFWLFSDUVHVDFURVVZRUGDOLJQPHQWV/LHWDO
 ,QERWKDSSURDFKHV DVLQODUJHO\DOOVWDWLVWLFDO
07WKHTXDOLW\RIWKHDOLJQPHQWVXVHGWRJHQHUDWH
WKHUXOHVRIWKHJUDPPDUDUHFULWLFDOWRWKHVXFFHVV
RIWKHV\VWHP +RZHYHU WRGDWH PRVWZRUGDOLJQ
PHQWV\VWHPVKDYHQRWFRQVLGHUHGWKHVDPHGHJUHH
RIV\QWDFWLFLQIRUPDWLRQWKDW07 V\VWHPVKDYH
([WHQGLQJ  XQVXSHUYLVHG PRGHOV OLNH  WKH  ,%0
PRGHOV %URZQ  HW  DO  JHQHUDOO\  UHTXLUHV
FKDQJLQJWKHHQWLUHJHQHUDWLYHVWRU\ 7KHDGGLWLRQDO
FRPSOH[LW\ZRXOGOLNHO\PDNHWUDLQLQJVXFKPRG
HOVTXLWHH[SHQVLYH $OUHDG\ ZLWKXELTXLWRXVWRROV
OLNH*,=$ 2FKDQG1H\  WUDLQLQJDFFXUDWH
PRGHOVRQODUJHFRUSRUDWDNHVXSZDUGVRIGD\V
5HFHQWZRUNLQGLVFULPLQDWLYHDOLJQPHQWKDVIR
FXVHGRQLQFRUSRUDWLQJIHDWXUHVWKDWDUHXQDYDLODEOH
RUGLI?FXOWWRLQFRUSRUDWHZLWKLQRWKHUPRGHOV HJ
0RRUH  ,WW\FKHULDKDQG5RXNRV  /LX
HW  DO  7DVNDU HW  DO E %OXQVRPDQG
&RKQ  /DFRVWH-XOLHQHWDO  0RRUHHWDO
 (YHQPRUHUHFHQWO\ PRWLYDWHGE\WKHULVHRI
V\QWD[EDVHGWUDQVODWLRQPRGHOV RWKHUVKDYHVRXJKW
WRLQIRUPDOLJQPHQWGHFLVLRQVZLWKV\QWDFWLFLQIRU
PDWLRQ)UDVHUDQG0DUFX Squibs and Discussions
Measuring Word Alignment Quality for Statistical
Machine Translation
Alexander Fraser?
University of Southern California
Daniel Marcu?
University of Southern California
Automatic word alignment plays a critical role in statistical machine translation. Unfortu-
nately, the relationship between alignment quality and statistical machine translation perform-
ance has not been well understood. In the recent literature, the alignment task has frequently
been decoupled from the translation task and assumptions have been made about measuring
alignment quality for machine translation which, it turns out, are not justified. In particular,
none of the tens of papers published over the last five years has shown that significant decreases
in alignment error rate (AER) result in significant increases in translation performance. This
paper explains this state of affairs and presents steps towards measuring alignment quality in a
way which is predictive of statistical machine translation performance.
1. Introduction
Automatic word alignment (Brown et al 1993) is a vital component of all statistical
machine translation (SMT) approaches. There were a number of research papers pre-
sented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so
forth, outlining techniques for attempting to increase word alignment quality. Despite
this high level of interest, none of these techniques has been shown to result in a large
gain in translation performance as measured by BLEU (Papineni et al 2001) or any
other metric. We find this lack of correlation between previous word alignment quality
metrics and BLEU counterintuitive, because we and other researchers have measured
this correlation in the context of building SMT systems that have benefited from using
the BLEU metric in improving performance in open evaluations such as the NIST
evaluations.1
We confirm experimentally that previous metrics do not predict BLEU well and
develop a methodology for measuring alignment quality that is predictive of BLEU. We
? USC/ISI - Natural Language Group, 4676 Admiralty Way, Suite 1001, Marina del Rey, CA 90292-6601.
E-mail: fraser@isi.edu, marcu@isi.edu.
1 Because in our experiments we use BLEU to compare the performance of systems built using a common
framework where the only difference is the word alignment, we make no claims about the utility of BLEU
for measuring translation quality in absolute terms, nor its utility for comparing two completely different
MT systems.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 3
also show that alignment error rate (AER) is not correctly derived from F-Measure and
is therefore unlikely to be useful as a metric.
2. Experimental Methodology
2.1 Data
To build an SMT system we require a bitext and a word alignment of that bitext, as well
as language models built from target language data. In all of our experiments, we will
hold the bitext and target language resources constant, and only vary how we construct
the word alignment.
The gold standard word alignment sets we use have been manually annotated
using links between words showing translational correspondence. Links which must
be present in a hypothesized alignment are called Sure links. Some of the alignment
sets also have links which are not Sure links but are Possible links (Och and Ney 2003).
Possible links which are not Sure2 may be present but need not be present.
We evaluate the translation performance of SMT systems by translating a held-
out translation test set and measuring the BLEU score of our hypothesized translations
against one or more reference translations. We also have an additional held-out transla-
tion set, the development set, which is employed by the MT system to train the weights
of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three
different language pairs, examining French to English, Arabic to English, and Romanian
to English translation tasks.
The training data for the French/English data set is taken from the LDC Canadian
Hansard data set, from which the word aligned data (presented in Och and Ney 2003)
was also taken. The English side of the bitext is 67.4 million words. We used a separate
Canadian Hansard data set (released by ISI) as the source of the translation test set and
development set. We evaluate two different tasks using this data, a medium task where
1/8 of the data (8.4 million English words) is used as the fixed bitext, and a large task
where all of the data is used as the fixed bitext. The 484 sentences in the gold standard
word alignments have 4,376 Sure links and 19,222 Possible links.
The Arabic/English training corpus is the data used for the NIST 2004 machine
translation evaluation.3 The English side of the bitext is 99.3 million words. The trans-
lation development set is the ?NIST 2002 Dry Run,? and the test set is the ?NIST 2003
evaluation set.? We have annotated gold standard alignments for 100 parallel sentences
using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure
links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8
of the data (12.4 million English words) and a large task using all of the data.
The Romanian/English training data was used for the tasks on Romanian/English
alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea,
and Pedersen 2005). We carefully removed two sections of news bitext to use as the
translation development and test sets. The English side of the training corpus is 964,000
words. The alignment set is the first 148 annotated sentences used for the 2003 task
(there were 3,181 Sure links).
2 Sure links are by definition also Possible.
3 http://www.nist.gov/speech/tests/summaries/2004/mt04.htm.
294
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
2.2 Measuring Translation Performance Changes Caused By Alignment
In phrased-based SMT (Koehn, Och, and Marcu 2003) the knowledge sources which
vary with the word alignment are the phrase translation lexicon (which maps source
phrases to target phrases using counts from the word alignment) and some of the word
level translation parameters (sometimes called lexical smoothing). However, many
knowledge sources do not vary with the final word alignment, such as rescoring with
IBM Model 1, n-gram language models, and the length penalty. In our experiments,
we use a state-of-the-art phrase-based system, similar to Koehn, Och, and Marcu.
The weights of the different knowledge sources in the log-linear model used by our
system are trained using Maximum BLEU (Och 2003), which we run for 25 iterations
individually for each system. Two language models are used, one built using the target
language training data and the other built using additional news data.
2.3 Generating Alignments of Varying Quality
We have observed in the past that generative models used for statistical word alignment
create alignments of increasing quality as they are exposed to more data. The intuition
behind this is simple; as more co-occurrences of source and target words are observed,
the word alignments are better. If we wish to increase the quality of a word alignment,
we allow the alignment process access to extra data, which is used only during the
alignment process and then removed. If we wish to decrease the quality of a word
alignment, we divide the bitext into pieces and align the pieces independently of one
another, finally concatenating the results together.
To generate word alignments we use GIZA++ (Och and Ney 2003), which im-
plements both the IBM Models of Brown et al (1993) and the HMM model (Vogel,
Ney, and Tillmann 1996). We use Model 1, HMM, and Model 4, in that order. The
output of these models is an alignment of the bitext which projects one language to
another. GIZA++ is run end-to-end twice. In one case we project the source language
to the target language, and in the other we project the target language to the source
language. The output of GIZA++ is then post-processed using the three ?symmetriza-
tion heuristics? described in Och and Ney (2003). We evaluate our approaches using
these heuristics because we would like to account for alignments generated in different
fashions. These three heuristics were used as the baselines in virtually all recent work
on automatic word alignment, and most of the best SMT systems use these techniques
as well.
When applying the union symmetrization heuristic, we take the transitive closure
of the bipartite graph created, which results in fully connected components indicating
translational correspondence.4 Each of the presented alignments are equivalent from a
translational correspondence perspective and the first two will be mapped to the third
4 We have no need to do this for the ?refined? and ?intersection? heuristics, because they only produce
alignments in which the components are fully connected.
295
Computational Linguistics Volume 33, Number 3
in order to ensure consistency between the number of links an alignment has and the
translational equivalences licensed by that alignment.
A B C
D















E







A B C
D















E














A B C
D















E














3. Word Alignment Quality Metrics
3.1 Alignment Error Rate is Not a Useful Measure
We begin our study of metrics for word alignment quality by testing AER (Och and Ney
2003). AER requires a gold standard manually annotated set of Sure links and Possible
links (referred to as S and P). Given a hypothesized alignment consisting of the link set
A, three measures are defined:
Precision(A, P) =
|P ? A|
|A| (1)
Recall(A, S) =
|S ? A|
|S| (2)
AER(A, P, S) = 1 ? |P ? A|+ |S ? A||S|+ |A| (3)
In our graphs, we will present 1 ? AER so that we have an accuracy measure.
We created alignments of varying quality for the medium French/English training
set. We broke the data into separate pieces corresponding to 1/16, 1/8, 1/4, and 1/2
of the original data to generate degraded alignments, and we used 2, 4, and 8 times
the original data to generate enhanced alignments. For the ?fractional? alignments we
report the average AER of the pieces.5
The graph in Figure 1 shows the correlation of 1 ? AER with BLEU. High correlation
would look like a line from the bottom left corner to the top right corner. As can be seen
by looking at the graph, there is low correlation between 1 ? AER and the BLEU score. A
concise mathematical description of correlation is the coefficient of determination (r2),
5 For example, for 1/16, we perform 16 pairs of alignments, each of which includes the full gold standard
text, and another 16 pairs of alignments without the gold standard text. We then apply the
symmetrization heuristics to these pairs. We use the symmetrized alignments including the text from the
gold standard set to measure AER and we concatenate those not including the gold standard text to build
SMT systems for measuring BLEU.
296
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Figure 1
French 1 ? AER versus BLEU, r2 = 0.16.
which is the square of the Pearson product-moment correlation coefficient (r). Here,
r2 = 0.16, which is low.
The correlation is low because of a significant shortcoming in the mathematical
formulation of AER, which to our knowledge has not been previously reported. Och
and Ney (2003) state that AER is derived from F-Measure. But AER does not share a
very important property of F-Measure, which is that unbalanced precision and recall
are penalized, where S ? P (i.e., when we make the Sure versus Possible distinction).6
We will show this using an example.
We first define the measure ?F-Measure with Sure and Possible? using Och and
Ney?s Precision and Recall formulae together with the standard F-Measure formula
(van Rijsbergen 1979). In the F-Measure formula (4) there is a parameter ? which sets
the trade-off between Precision and Recall. When an equal trade-off is desired, ? is set
to 0.5.
F-Measure with Sure and Possible(A, P, S, ?) = 1
?
Precision(A,P) +
(1??)
Recall(A,S)
(4)
We compare two hypothesized alignments where |A|, the number of hypothesized
alignment links, is the same, for instance, |A| = 100. Let |S| = 100. In the first case, let
|P ? A| = 50 and |S ? A| = 50. Precision is 0.50 and Recall is 0.50. In the second case, let
|P ? A| = 75 and |S ? A| = 25. Precision is 0.75 and Recall is 0.25.
The basic property of F-Measure, if we set ? equal to 0.5, is that unbalanced
precision and recall should be penalized. The first hypothesized alignment has an
F-Measure with Sure and Possible score of 0.50, whereas the second has a worse score,
0.375.
However, if we substitute the relevant values into the formula for AER (Equa-
tion (3)), we see that 1 ? AER for both of the hypothesized alignments is 0.5. Therefore
AER does not share the property of F-Measure (with ? = 0.5) that unbalanced precision
and recall are penalized. Because of this, it is possible to maximize AER by favoring
precision over recall, which can be done by simply guessing very few alignment links.
Unfortunately, when S ? P, this leads to strong biases, which makes AER not useful as
a metric.
6 Note that if S = P then AER reduces to balanced F-Measure.
297
Computational Linguistics Volume 33, Number 3
Figure 2
French F-Measure with Sure and Possible ? = 0.5 versus BLEU, r2 = 0.20.
Goutte, Yamada, and Gaussier (2004) previously observed that AER could be un-
fairly optimized by using a bias toward precision which was unlikely to improve the
usefulness of the alignments. Possible problems with AER were discussed at WPT 2003
and WPT 2005.
Examining the graph in Figure 2, we see that F-Measure with Sure and Possible has
some predictive power for the data points generated using a single heuristic, but the
overall correlation is still low, r2 = 0.20. We need a measure that predicts BLEU without
having a dependency on the way the alignments are generated.
3.2 Balanced F-Measure is Better, but Still Inadequate
We wondered whether the low correlation was caused by the Sure and Possible dis-
tinction. We reannotated the first 110 sentences of the French test set using the Blinker
guidelines (there were 2,292 Sure links). We define F-Measure without the Sure versus
Possible distinction (i.e., all links are Sure) in Equation (5), and set ? = 0.5. This measure
has been extensively used with other word alignment test sets. Figure 3 shows the
results. Correlation is higher: r2 = 0.67.
F-Measure(A, S, ?) = 1
?
Precision(A,S) +
(1??)
Recall(A,S)
(5)
3.3 Varying the Trade-Off Between Precision and Recall Works Well
We then hypothesized that the trade-off between precision and recall is important. This
is controlled in both formulae by the constant ?. We search ? = 0.1, 0.2, ..., 0.9. The best
results are: ? = 0.1 for the original annotation annotated with Sure and Possible (see
Figure 4), and ? = 0.4 for the first 110 sentences as annotated by us (see Figure 5).7 The
relevant r2 scores were 0.80 and 0.85, respectively. With a good ? setting, we are able
7 We also checked the first 110 sentences using the original annotation to ensure that the differences
observed were not an effect of restricting our annotation to these sentences.
298
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Figure 3
French F-Measure ? = 0.5 versus BLEU, r2 = 0.67.
Figure 4
French F-Measure with Sure and Possible ? = 0.1 versus BLEU, r2 = 0.80.
Figure 5
French F-Measure ? = 0.4 versus BLEU, r2 = 0.85.
to predict the machine translation results reasonably well. For the original annotation,
recall is very highly weighted, whereas for our annotation, recall is still more important
than precision.8 Our results also suggest that better correlation will be achieved when
using Sure-only annotation than with Sure and Possible annotation.
8 ? less than 0.5 weights recall higher, whereas ? greater than 0.5 weights precision higher; see the
F-Measure formulae.
299
Computational Linguistics Volume 33, Number 3
Figure 6
Arabic F-Measure ? = 0.1 versus BLEU, r2 = 0.93.
Figure 7
Large French F-Measure ? = 0.4 (110 sentences) versus BLEU, r2 = 0.64.
We then tried the medium Arabic training set. Results are shown in Figure 6, the
best setting of ? = 0.1, and r2 = 0.93. F-Measure is effective in predicting machine
translation performance for this set.
We also tried the larger tasks, where we can only decrease alignment quality, as
we have no additional data. For the large French/English corpus the best results are
with ? = 0.2 for the original annotation of 484 sentences and ? = 0.4 for the new
Figure 8
Large Arabic F-Measure ? = 0.1 (100 sentences) versus BLEU, r2 = 0.90.
300
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
annotation of 110 sentences with only Sure links (see Figure 7). Relevant r2 scores were
0.62 and 0.64, respectively. Disappointingly, our measures are not able to fully explain
MT performance for the large French/English task.
For the large Arabic/English corpus, the results were better: the best correlation
was at ? = 0.1, for which r2 = 0.90 (see Figure 8). We can predict MT performance for
this set. It is worth noting that the Arabic/English translation task and data set has
been tested in conjunction with our translation system over a long period, but the
French/English translation task and data has not. As a result, there may be hidden
factors that affect the performance of our MT system, which only appear in conjunction
with the large French/English task.
One well-studied task on a smaller data set is the Romanian/English shared word
alignment task from the Workshop on Parallel Text at ACL 2005 (Martin, Mihalcea, and
Pedersen 2005). We only decreased alignment quality and used 5 data points for each
symmetrization heuristic due to the small bitext. The best setting of ? was ? = 0.2, for
which r2 = 0.94, showing that F-Measure is again effective in predicting BLEU.
4. Conclusion
We have presented an empirical study of the use of simple evaluation metrics based on
gold standard alignment of a small number of sentences to predict machine translation
performance. Based on our experiments we can now draw the following conclusions:
1. When S ? P, AER does not share the important property of F-Measure
that unequal precision and recall are penalized, making it easy to obtain
good AER scores by simply guessing fewer alignment links. As a result
AER is a misleading metric that should no longer be used.
2. Good correlation was obtained for the medium French and Arabic data
sets, the large Arabic data set, and the small Romanian data set. We have
explained most of the effect of alignment quality on these sets, and if we
are given the F-Measure of a hypothesized word alignment for the bitext,
we can make a reasonable prediction as to what the resulting BLEU score
will be.
3. We have only partially explained the effect of alignment quality on BLEU
for the large French data set, and further investigation is warranted.
4. We recommend using the Blinker guidelines as a starting point for new
alignment annotation efforts, and that Sure-only annotation be used. If a
larger gold standard is available and was already annotated using the Sure
versus Possible distinction, this is likely to have only slightly worse results.
Although we have addressed measuring alignment quality for phrasal SMT, similar
work is now required to see how to measure alignment quality for other settings of
machine translation and for other tasks. For an evaluation campaign the organizers
should pick a specific task, such as improving phrasal SMT, and calculate an appropriate
? to be used. Individual researchers working on the same phrasal SMT tasks as those
reported here (or on very similar tasks) could use the values of ? we calculated.
Our work invalidates some of the conclusions of recent alignment work which
presented only evaluations based on metrics like AER or balanced F-Measure, and
explains the lack of correlation in the few works which presented both such a metric
301
Computational Linguistics Volume 33, Number 3
and final MT results. A good example of the former are our own results (Fraser and
Marcu 2005). The work presented there had the highest balanced F-Measure scores for
the Romanian/English WPT05 shared task, but based on the findings here it is possible
that a different algorithm tuned for the correct criterion would have had better MT
performance. Other work includes many papers working on alignment models where
words are allowed to participate in a maximum of one link. These models generally
have higher precision and lower recall than IBM Model 4 symmetrized using the ?Re-
fined? or ?Union? heuristics. Recall that in Section 3.1 we showed that AER is broken in
a way that favors precision. It is therefore likely that the results reported in these papers
are affected by the AER bias and that the corresponding improvements in AER score do
not correlate with increases in phrasal SMT performance.
We suggest comparing alignment algorithms by measuring performance in an
identified final task such as machine translation. F-Measure with an appropriate setting
of ? will be useful during the development process of new alignment models, or as a
maximization criterion for discriminative training of alignment models (Cherry and Lin
2003; Ayan, Dorr, and Monz 2005; Ittycheriah and Roukos 2005; Liu, Liu, and Lin 2005;
Fraser and Marcu 2006; Lacoste-Julien et al 2006; Moore, Yih, and Bode 2006).
Acknowledgments
This work was supported by DARPA-ITO
grant NN66001-00-1-9814, NSF grant
IIS-0326276, and the DARPA GALE Program.
We thank USC High Performance
Computing & Communications.
References
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP,
pages 65?72, Vancouver, Canada.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL,
pages 88?95, Sapporo, Japan.
Fraser, Alexander and Daniel Marcu. 2005.
Isi?s participation in the Romanian-English
alignment task. In Proceedings of the ACL
Workshop on Building and Using Parallel
Texts, pages 91?94, Ann Arbor, MI.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for word
alignment. In Proceedings of COLING-ACL,
pages 769?776, Sydney, Australia.
Goutte, Cyril, Kenji Yamada, and Eric
Gaussier. 2004. Aligning words using
matrix factorisation. In Proceedings of ACL,
pages 502?509, Barcelona, Spain.
Ittycheriah, Abraham and Salim Roukos.
2005. A maximum entropy word aligner
for Arabic-English machine translation. In
Proceedings of HLT-EMNLP, pages 89?96,
Vancouver, Canada.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton, Canada.
Lacoste-Julien, Simon, Dan Klein,
Ben Taskar, and Michael Jordan.
2006. Word alignment via quadratic
assignment. In Proceedings
of HLT-NAACL, pages 112?119,
New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin.
2005. Log-linear models for word
alignment. In Proceedings of ACL,
pages 459?466, Ann Arbor, MI.
Martin, Joel, Rada Mihalcea, and
Ted Pedersen. 2005. Word
alignment for languages with
scarce resources. In Proceedings of the
ACL Workshop on Building and
Using Parallel Texts, pages 65?74,
Ann Arbor, MI.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The
Blinker project. Technical Report 98?07,
Institute for Research in Cognitive
Science, University of Pennsylvania,
Philadelphia, PA.
Mihalcea, Rada and Ted Pederson. 2003.
An evaluation exercise for word
alignment. In Proceedings of the
HLT-NAACL Workshop on Building
and Using Parallel Texts, pages 1?10,
Edmonton, Canada.
302
Fraser and Marcu Measuring Word Alignment Quality for Statistical Machine Translation
Moore, Robert C., Wen-Tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL, pages 513?520,
Sydney, Australia.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo, Japan.
Och, Franz J. and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Papineni, Kishore A., Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2001. BLEU: A method for
automatic evaluation of machine
translation. Technical Report
RC22176 (W0109-022), IBM Research
Division, Thomas J. Watson
Research Center, Yorktown
Heights, NY.
van Rijsbergen, Keith. 1979. Information
Retrieval. Butterworth-Heinemann,
Newton, MA.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In Proceedings of COLING, pages 836?841,
Copenhagen, Denmark.
303

Re-structuring, Re-labeling, and Re-aligning
for Syntax-Based Machine Translation
Wei Wang?
Language Weaver, Inc.
Jonathan May??
USC/Information Sciences Institute
Kevin Knight?
USC/Information Sciences Institute
Daniel Marcu?
Language Weaver, Inc.
This article shows that the structure of bilingual material from standard parsing and alignment
tools is not optimal for training syntax-based statistical machine translation (SMT) systems.
We present three modifications to the MT training data to improve the accuracy of a state-of-the-
art syntax MT system: re-structuring changes the syntactic structure of training parse trees to
enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context;
and re-aligning unifies word alignment across sentences to remove bad word alignments and
refine good ones. Better structures, labels, and word alignments are learned by the EM algorithm.
We show that each individual technique leads to improvement as measured by BLEU, and we
also show that the greatest improvement is achieved by combining them. We report an overall
1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English
translation.
1. Background
Syntactic methods have recently proven useful in statistical machine translation (SMT).
In this article, we explore different ways of exploiting the structure of bilingual material
for syntax-based SMT. In particular, we ask what kinds of tree structures, tree labels,
and word alignments are best suited for improving end-to-end translation accuracy.
We begin with structures from standard parsing and alignment tools, then use the EM
algorithm to revise these structures in light of the translation task. We report an overall
+1.48 BLEU improvement on a standard Chinese-to-English test.
? 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: wwang@languageweaver.com.
?? 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: jonmay@isi.edu.
? 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: knight@isi.edu.
? 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: dmarcu@languageweaver.com.
Submission received: 6 November 2008; revised submission received: 10 September 2009; accepted for
publication: 1 January 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
We carry out our experiments in the context of a string-to-tree translation system.
This system accepts a Chinese string as input, and it searches through a multiplicity of
English tree outputs, seeking the one with the highest score. The string-to-tree frame-
work is motivated by a desire to improve target-language grammaticality. For example,
it is common for string-basedMT systems to output sentences with no verb. By contrast,
the string-to-tree framework forces the output to respect syntactic requirements?for
example, if the output is a syntactic tree whose root is S (sentence), then the S will gen-
erally have a child of type VP (verb phrase), which will in turn contain a verb. Another
motivation is better treatment of function words. Often, these words are not literally
translated (either by themselves or as part of a phrase), but rather they control what
happens in the translation, as with case-marking particles or passive-voice particles.
Finally, much of the re-ordering we find in translation is syntactically motivated, and
this can be captured explicitly with syntax-based translation rules. Tree-to-tree systems
are also promising, but in this work we concentrate only on target-language syntax. The
target-language generation problem presents a difficult challenge, whereas the source
sentence is fixed and usually already grammatical.
To prepare training data for such a system, we begin with a bilingual text that has
been automatically processed into segment pairs. We require that the segments be single
sentences on the English side, whereas the corresponding Chinese segments may be
sentences, sentence fragments, or multiple sentences. We then parse the English side of
the bilingual text using a re-implementation of the Collins (1997) parsing model, which
we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
Finally, we word-align the segment pairs according to IBMModel 4 (Brown et al 1993).
Figure 1 shows a sample (tree, string, alignment) triple.
We build two generative statistical models from this data. First, we construct a
smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the
English side of the bilingual data. This model assigns a probability P(e) to any candidate
translation, rewarding translations whose subsequences have been observed frequently
in the training data.
Second, we build a syntax-based translation model that we can use to produce
candidate English trees fromChinese strings. Following previouswork in noisy-channel
Figure 1
A sample learning case for the syntax-based machine translation system described in this article.
248
Wang et al Re-structuring, Re-labeling, and Re-aligning
SMT (Brown et al 1993), our model operates in the English-to-Chinese direction?
we envision a generative top?down process by which an English tree is gradually
transformed (by probabilistic rules) into an observed Chinese string. We represent a
collection of such rules as a tree transducer (Knight and Graehl 2005). In order to
construct this transducer from parsed and word-aligned data, we use the GHKM rule
extraction algorithm of Galley et al (2004). This algorithm computes the unique set of
minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the
minimal rules extracted from the example (tree, string, alignment) triple in Figure 1.
Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and
deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example
form a derivation tree.
We collect all rules over the entire bilingual corpus, and we normalize rule counts
in this way: P(rule) = count(rule)count(LHS-root(rule)) . When we apply these probabilities to derive an
English sentence e and a corresponding Chinese sentence c, we wind up computing the
joint probability P(e, c). We smooth the rule counts with Good?Turing smoothing (Good
1953).
This extraction method assigns each unaligned Chinese word to a default rule in
the derivation tree. We next follow Galley et al (2006) in allowing unaligned Chinese
words to participate in multiple translation rules. In this case, we obtain a derivation
forest of minimal rules. Galley et al show how to use EM to count rules over deriva-
tion forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley
et al in collecting composed rules, namely, compositions of minimal rules. These larger
rules have been shown to substantially improve translation accuracy (Galley et al 2006;
DeNeefe et al 2007). Figure 3 shows some of the additional rules.
With these models, we can decode a new Chinese sentence by enumerating and
scoring all of the English trees that can be derived from it by rule. The score is a
weighted product of P(e) and P(e, c). To search efficiently, we employ the CKY dynamic-
programming parsing algorithm (Yamada and Knight 2002; Galley et al 2006). This
algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix,
we store the non-terminal symbol at the root of the English tree being built up. We also
Figure 2
Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure.
249
Computational Linguistics Volume 36, Number 2
Figure 3
Additional rules extracted from the learning case in Figure 1.
store English words that appear at the left and right corners of the tree, as these are
needed for computing the P(e) score when cells are combined. For CKY to work, all
transducer rules must be broken down, or binarized, into rules that contain at most two
variables?more efficient search can be gained if this binarization produces rules that
can be incrementally scored by the language model (Melamed, Satta, and Wellington
2004; Zhang et al 2006). Finally, we employ cube pruning (Chiang 2007) for further
efficiency in the search.
When scoring translation candidates, we add several smaller models. One model
rewards longer translation candidates, off-setting the language model?s desire for short
output. Other models punish rules that drop Chinese content words or introduce
spurious English content words. We also include lexical smoothing models (Gale and
Sampson 1996; Good 1953) to help distinguish good low-count rules from bad low-
count rules. The final score of a translation candidate is a weighted linear combination
of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain
weights through minimum error-rate training (Och 2003).
The system thus constructed performs fairly well at Chinese-to-English translation,
as reflected in the NIST06 common evaluation of machine translation quality.1
However, it would be surprising if the parse structures and word alignments in our
bilingual data were somehow perfectly suited to syntax-based SMT?we have so far
used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and
Knight (2006) already investigated whether different syntactic labels would be more
appropriate for SMT, though their study was carried out on a weak baseline translation
system. In this article, we take a broad view and investigate how changes to syntactic
structures, syntactic labels, and word alignments can lead to substantial improvements
in translation quality on top of a strong baseline. We design our methods around
problems that arise in MT data whose parses and alignments use some Penn Treebank-
style annotations. We believe that some of the techniques will apply to other annotation
schemes, but conclusions here are limited to Penn Treebank-style trees.
The rest of this article is structured as follows. Section 2 describes the corpora and
model configurations used in our experiments. In each of the next three sections we
present a technique for modifying the training data to improve syntax MT accuracy:
tree re-structuring in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5.
In each of these three sections, we also present experiment results to show the impact
of each individual technique on end-to-end MT accuracy. Section 6 shows the improve-
ment made by combining all three techniques. We conclude in Section 7.
1 http://nist.gov/speech/tests/mt/2006/doc/mt06eval official results.html.
250
Wang et al Re-structuring, Re-labeling, and Re-aligning
2. Corpora for Experiments
For our experiments, we use a 245 million word Chinese/English bitext, available from
LDC. A re-implementation of the Collins (1997) parser runs on the English half of the
bitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 word
alignments. We extract a subset of 36 million words from the entire bitext, by selecting
only sentences in the mainland news domain. We extract translation rules from these
selected 36 million words. Experiments show that our Chinese/English syntax MT
systems built from this selected bitext give as high BLEU scores as from the entire bitext.
Our development set consists of 1,453 lines and is extracted from the NIST02?
NIST05 evaluation sets, for tuning of feature weights. The development set is from the
newswire domain, andwe chose it to represent awide period of time rather than a single
year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set
is a mix of newswire text andWeb text, we also report the BLEU scores on the newswire
portion.
We use two 5-gram languagemodels. One is trained on the English half of the bitext.
The other is trained on one billion words of monolingual data. Kneser?Ney smoothing
(Kneser and Ney 1995) is applied to both language models. Language models are
represented using randomized data structures similar to those of Talbot and Osborne
(2007) in decoding for efficient RAM usage.
To test the significance of improvements over the baseline, we compute paired boot-
strap p-values (Koehn 2004) for BLEU between the baseline system and each improved
system.
3. Re-structuring Trees for Training
Our translation system is trained on Chinese/English data, where the English side
has been automatically parsed into Penn Treebank-style trees. One striking fact about
these trees is that they contain many flat structures. For example, base noun phrases
frequently have five or more direct children. It is well known in monolingual parsing
research that these flat structures cause problems. Although thousands of rewrite rules
can be learned from the Penn Treebank, these rules still do not cover the new rewrites
observed in held-out test data. For this reason, and to extract more general knowl-
edge, many monolingual parsing models aremarkovized so that they can produce flat
structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing
systems binarize the training trees in a pre-processing step, then learn to model the
binarized corpus (Petrov et al 2006); after parsing, their results are flattened back
in a post-processing step. In addition, Johnson (1998b) shows that different types of
tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II
representation) can have a large effect on the parsing performance of a PCFG estimated
from these trees.
We find that flat structures are also problematic for syntax-based machine transla-
tion. The rules we learn from tree/string/alignment triples often lack sufficient gener-
alization power. For example, consider the training samples in Figure 4. We should be
able to learn enough from these two samples to translate the new phrase
?.W-#L?? Z ?{ 3?
VIKTOR CHERNOMYRDIN AND HIS COLLEAGUE
into its English equivalent victor chernomyrdin and his colleagues.
251
Computational Linguistics Volume 36, Number 2
Figure 4
Learning translation rules from flat English structures.
However, the learned rules R12 and R13 do not fit together nicely. R12 can translate
?.W-#L?? into an English base noun phrase (NPB) that includes viktor
chernomyrdin, but only if it is preceded by words that translate into an English JJ and
an English NNP. Likewise, R13 can translate
Z ?{ 3?
into an NPB that includes
and his colleagues, but only if preceded by two NNPs. Both rules want to create an NPB,
and neither can supply the other with what it needs.
If we re-structure the training trees as shown in Figure 5, we get much better
behavior. Now rule R14 translates
?.W-#L?? into a free-standing NPB. This
gives rule R15 the ability to translate
Z ?{ 3?
, because it finds the necessary
NPB to its left.
Here, we are re-structuring the trees in our MT training data by binarizing them.
This allows us to extract better translation rules, though of course an extracted rule may
have more than two variables. Whether the rules themselves should be binarized is a
separate question, addressed in Melamed, Satta, andWellington (2004) and Zhang et al
(2006). One can decide to re-structure training data trees, binarize translation rules, or
do both, or do neither. Here we focus on English tree re-structuring.
In this section, we explore the generalization ability of simple re-structuring meth-
ods like left-, right-, and head-binarization, and also their combinations. Simple bina-
rization methods binarize syntax trees in a consistent fashion (left-, right-, or head-)
and thus cannot guarantee that all the substructures can be factored out. For example,
consistent right binarization of the training examples in Figure 4 makes available R14,
but misses R15. We therefore also introduce a parallel re-structuring method in which
we binarize both to the left and right at the same time, resulting in a binarization
forest. We employ the EM algorithm (Dempster, Laird, and Rubin 1977) to learn the
binarization bias for each tree node in the corpus from the parallel alternatives.
252
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 5
Learning translation rules from binarized English structures.
3.1 Some Concepts
We now explain some concepts to facilitate the descriptions of the re-structuring meth-
ods. We train our translation model on alignment graphs (Galley et al 2004). An
alignment graph is a tuple of a source-language sentence f, a target-language parse tree
that yields e and translates from f, and the word alignments a between e and f. The
graphs in Figures 1, 4, and 5 are examples of alignment graphs.
In the alignment graph, a node in the parse tree is called admissible if rules can be
extracted from it.We can extract rules from a node if and only if the yield of the tree node
is consistent with the word alignments?the f string covered by the node is contiguous
but not empty, and the f string does not align to any e string that is not covered by
the node. An admissible tree node is one where rules overlap. Figure 6 shows different
binarizations of the left tree in Figure 4. In this figure, the NPB node in tree (1) is not
admissible because the f string, V-C, that the node covers also aligns to NNP3, which is
not covered by the NPB. Node NPB in tree (2), on the other hand, is admissible.
A set of sibling tree nodes is called factorizable if we can form an admissible
new node dominating them. In Figure 6, sibling nodes NNP1, NNP2, and NNP3 are
factorizable because we can factorize them out and form a new node NPB, resulting
in tree (2). Sibling tree nodes JJ, NNP1, and NNP2 are not factorizable. Not all sibling
nodes are factorizable, so not all sub-phrases can be acquired and syntactified. Ourmain
purpose is to re-structure parse trees by factorization such that syntactified sub-phrases
can be employed in translation.
With these concepts defined, we now present the re-structuring methods.
253
Computational Linguistics Volume 36, Number 2
Figure 6
Left, right, and head binarizations on the left tree in Figure 4. Tree leaves of nodes JJ and NNP1
are omitted for convenience. Heads are marked with ?. New nonterminals introduced by
binarization are denoted by X-bars.
3.2 Binarizing Syntax Trees
We re-structure parse trees by binarizing the trees. We are going to binarize a tree node
n that dominates r children n1, ..., nr. Binarization is performed by introducing new tree
nodes to dominate a subset of the children nodes. We allow ourselves to form only one
new node at a time to avoid over-generalization. Because labeling is not the concern of
this section, we re-label the newly formed nodes as n.
3.2.1 Simple Binarization Methods. The left binarization of node n (e.g., the NPB in tree
(1) of Figure 6) factorizes the leftmost r ? 1 children by forming a new node n (i.e., the
NPB in tree (1)) to dominate them, leaving the last child nr untouched; and then makes
the new node n the left child of n. The method then recursively left-binarizes the newly
formed node n until two leaves are reached. We left-binarize the left tree in Figure 4 into
Figure 6 (1).
The right binarization of node n factorizes the rightmost r ? 1 children by forming
a new node n (i.e., the NPB in tree (2)) to dominate them, leaving the first child n1
untouched; and then makes the new node n the right child of n. The method then
recursively right-binarizes the newly formed node n. For instance, we right-binarize
the left tree in Figure 4 into Figure 6 (2) and then into Figure 6 (6).
The head binarization of node n left-binarizes n if the head is the first child;
otherwise, it right-binarizes n. We prefer right-binarization to left-binarization when
both are applicable under the head restriction because our initial motivation was to
generalize the NPB-rooted translation rules. As we show in experiments, binarization
of other types of phrases contributes to translation accuracy as well.
Any of these simple binarization methods is easy to implement, but each in itself
is incapable of giving us all the factorizable sub-phrases. Binarizing all the way to the
left, for example, from unbinarized tree to tree (1) and to tree (3) in Figure 6, does not
enable us to acquire a substructure that yields NNP2, NNP3, and their translational
equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in
both directions.
254
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 7
Packed forest obtained by packing trees (3) and (6) in Figure 6.
3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single
parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed
to enable dynamic programming when we extract translation rules from it.
Borrowing terms from parsing semirings (Goodman 1999), a packed forest is com-
posed of additive forest nodes (?-nodes) and multiplicative forest nodes (?-nodes). In
the binarization forest, a ?-node corresponds to a tree node in the unbinarized tree or a
new tree node introduced during tree binarization; and this ?-node composes several
?-nodes, forming a one-level substructure that is observed in the unbinarized tree or
in one of its binarized tree. A ?-node corresponds to alternative ways of binarizing the
same tree node and it contains one or more ?-nodes. The same ?-node can appear in
more than one place in the packed forest, enabling sharing. Figure 7 shows a packed
forest obtained by packing trees (3) and (6) in Figure 6 via the following tree parallel
binarization algorithm.
We use a memoization procedure to recursively parallel-binarize a parse tree.
To parallel-binarize a tree node n that has children n1, ...,nr, we employ the following
steps:
 If r ? 2, parallel-binarize tree nodes n1, ..., nr, producing binarization
?-nodes ?(n1), ..., ?(nr), respectively. Construct node ?(n) as the parent
of ?(n1), ...,?(nr). Construct an additive node ?(n) as the parent of ?(n).
Otherwise, execute the following steps.
 Right-binarize n, if any contiguous2 subset of children n2, ...,nr is
factorizable, by introducing an intermediate tree node labeled as n. We
recursively parallel-binarize n to generate a binarization forest node ?(n).
We also recursively parallel-binarize n1, forming a binarization forest
node ?(n1). We form a multiplicative forest node ?R as the parent of
?(n1) and ?(n).
 Left-binarize n if any contiguous subset of n1, ...,nr?1 is factorizable and
if this subset contains n1. Similar to the previous right-binarization,
we introduce an intermediate tree node labeled as n, recursively
parallel-binarize n to generate a binarization forest node ?(n), recursively
2 For practical purposes we factorize only subsets that cover contiguous spans to avoid introducing
discontiguous constituents. In principle, the algorithm works fine without this condition.
255
Computational Linguistics Volume 36, Number 2
parallel-binarize nr to generate a binarization forest node ?(nr), and then
form a multiplicative forest node ?L as the parent of ?(n) and ?(nr).
 Form an additive node ?(n) as the parent of the two already formed
multiplicative nodes ?L and ?R.
The (left and right) binarization conditions consider any subset to enable the fac-
torization of small constituents. For example, in the left tree of Figure 4, although the
JJ, NNP1, and NNP2 children of the NPB are not factorizable, the subset JJ NNP1 is
factorizable. The binarization from this tree to the tree in Figure 6 (1) serves as a relaying
step for us to factorize JJ and NNP1 in the tree in Figure 6 (3). The left-binarization
condition is stricter than the right-binarization condition to avoid spurious binarization,
that is, to avoid the same subconstituent being reached via both binarizations.
In parallel binarization, nodes are not always binarizable in both directions. For
example, we do not need to right-binarize tree (2) because NNP2 and NNP3 are not
factorizable, and thus cannot be used to form sub-phrases. It is still possible to right-
binarize tree (2) without affecting the correctness of the parallel binarization algorithm,
but that will spuriously increase the branching factor of the search for the rule extrac-
tion, because we will have to expand more tree nodes.
A special version of parallel binarization is the parallel head binarization, where
both the left and the right binarization must respect the head propagation property
at the same time. Parallel head binarization guarantees that new nodes introduced by
binarization always contain the head constituent, which will become convenient when
head-driven syntax-based language models are integrated into a bottom?up decoding
search by intersecting with the trees inferred from the translation model.
Our re-structuring of MT training trees is realized by tree binarization, but this does
not mean that our re-structuring method can factor out phrases covered only by two
(binary) constituents. In fact, a nice property of parallel binarization is that for any
factorizable substructure in the unbinarized tree, we can always find a corresponding
admissible ?-node in the parallel-binarized packed forest, and thus we can always
extract that phrase. A leftmost substructure like the lowest NPB-subtree in tree (3) of
Figure 6 can be made factorizable by several successive left binarizations, resulting in
the ?5(NPB)-node in the packed forest in Figure 7. A substructure in the middle can be
factorized by the composition of several left- and right-binarizations. Therefore, after a
tree is parallel-binarized, to make the sub-phrases available to the MT system, all we
need to do is to extract rules from the admissible nodes in the packed forest. Rules that
can be extracted from the original unrestructured tree can be extracted from the packed
forest as well.
Parallel binarization results in parse forests. Thus translation rules need to be ex-
tracted from training data consisting of (e-forest, f, a)-tuples.
3.3 Extracting Translation Rules from (e-forest, f, a)-tuples
Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization
of the (e-parse, f, a)-based rule extraction algorithm in Galley et al (2006). A similar
problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based
rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation
forest (Galley et al 2006), which consists of overlapping translation rules. The algorithm
recursively traverses the e-forest top?down, extracts rules only at admissible e-forest
256
Wang et al Re-structuring, Re-labeling, and Re-aligning
nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the
following two procedures, depending on which condition is met.
 Condition 1: If we reach an additive e-forest node, for each of its children,
which are multiplicative e-forest nodes, we go to condition 2 to recursively
extract rules from it to obtain a set of multiplicative derivation-forest
nodes, respectively. We form an additive derivation-forest node, and take
these newly produced multiplicative derivation-forest nodes (by going to
condition 2) as children. After this, we return the additive
derivation-forest node.
For instance, at node ?1(NPB) in Figure 7, for each of its children, e-forest
nodes ?2(NPB) and ?11(NPB), we go to condition 2 to extract rules on it,
to form multiplicative derivation forest nodes, ?(R16) and ?(R17) in
Figure 8.
 Condition 2: If we reach a multiplicative e-forest node, we extract a set of
rules rooted at it using the procedure in Galley et al (2006); and for each
rule, we form a multiplicative derivation-forest node, and go to condition 1
to form the additive derivation-forest nodes for the additive frontier
e-forest nodes of the newly extracted rule, and then make these additive
derivation-forest nodes the children of the multiplicative derivation-forest
node. After this, we return a set of multiplicative derivation-forest nodes,
each corresponding to one rule extracted from the multiplicative e-forest
node we just reached.
Figure 8
A synchronous derivation forest built from a (e-forest, f, a) triple. The e-forest is shown in
Figure 7.
257
Computational Linguistics Volume 36, Number 2
For example, at node ?11(NPB) in Figure 7, we extract a rule from it and
form derivation-forest node ?(R17) in Figure 8. We then go to condition 1
to obtain, for each of the additive frontier e-forest nodes (in Figure 7) of
this rule, a derivation-forest node, namely, ?(NNP), ?(NNP), and ?(NPB)
in Figure 8. We make these derivation-forest ?-nodes the children of
derivation-forest node ?(R17).
This procedure transforms the packed e-forest in Figure 7 into a packed synchro-
nous derivation in Figure 8. This algorithm is an extension of the extraction algorithm
in Galley et al (2006), in the sense that we have an extra condition (1) to relay rule
extraction on additive e-forest nodes.
The forest-based rule extraction algorithm produces much larger grammars than
the tree-based one, making it difficult to scale to very large training data. From a
50M-word Chinese-to-English parallel corpus, we can extract more than 300 million
translation rules, while the tree-based rule extraction algorithm gives approximately
100 million. However, the restructured trees from the simple binarization methods are
not guaranteed to give the best trees for syntax-based machine translation. What we
desire is a binarization method that still produces single parse trees, but is able to mix
left binarization and right binarization in the same tree. In the following, we use the EM
algorithm to learn the desirable binarization on the forest of binarization alternatives
proposed by the parallel binarization algorithm.
3.4 Learning How to Binarize Via the EM Algorithm
The basic idea of applying the EM algorithm to choose a re-structuring is as follows. We
perform a set {?} of binarization operations on a parse tree ?. Each binarization ? is
the sequence of binarizations on the necessary (i.e., factorizable) nodes in ? in pre-order.
Each binarization ? results in a restructured tree ??. We extract rules from (??, f, a),
generating a translation model consisting of parameters (i.e., rule probabilities) ?. Our
aim is to first obtain the rule probabilities that are the maximum likelihood estimate
of the training tuples, and then produce the Viterbi binarization tree for each training
tuple.
The probability P(??, f, a) of a (??, f, a)-tuple is what the basic syntax-based trans-
lation model is concerned with. It can be further computed by aggregating the rule
probabilities P(r) in each derivation? in the set of all derivations ? (Galley et al 2004).
That is,
P(??, f, a) =
?
???
?
r??
P(r) (1)
The rule probabilities are estimated by the inside?outside algorithm (Lari and
Young 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests.
Our previous sections have already presented algorithms to transform a parse tree into
a binarization forest, and then transform the (e-forest, f, a)-tuples into derivation forests
(e.g., Figure 8), on which the inside?outside algorithm can then be applied.
In the derivation forests, an additive node labeled as A dominates several mul-
tiplicative nodes, each corresponding to a translation rule resulting from either left
binarization or right binarization of the original structure. We use rule r to either refer
to a rule or to a multiplicative node in the derivation forest. We use root(r) to represent
the root label of the rule, and parent(r) to refer to the additive node that is the parent
258
Wang et al Re-structuring, Re-labeling, and Re-aligning
of the node corresponding to the r. Each rule node (or multiplicative node) dom-
inates several other additive children nodes, and we present the ith child node as
childi(r), among the total number of n children. For example, in Figure 8, for the rule r
corresponding to the left child of the forest root (labeled as NPB), parent(r) is NPB, and
child1(NPB) = r. Based on these notations, we can compute the inside probability ?(A)
of an additive node labeled as A and the outside probability ?(B) of an additive forest
node labeled as B as follows.
?(A) =
?
r?{child(A)}
P(r)?
?
i=1...n
?(childi(r)) (2)
?(B) =
?
r:B?{child(r)}
P(r)? ?(parent(r))?
?
C?{child(r)}?{B}
?(C) (3)
In the expectation step, the contribution of each occurrence of a rule in a derivation-
forest to the total expected count of that rule is computed as
?(parent(r))? P(r)?
?
i=1...n
?(childi(r)) (4)
In the maximization step, we use the expected counts of rules, #r, to update the proba-
bilities of the rules.
P(r) = #r?
rule q:root(q)=root(r) #q
(5)
Because it is well known that applying EM with tree fragments of different sizes
causes overfitting (Johnson 1998a), and because it is also known that syntax MT models
with larger composed rules in the mix significantly outperform rules that minimally
explain the training data (minimal rules) in translation accuracy (Galley et al 2006), we
use minimal rules to construct the derivation forests from (e-binarization-forest, f, a)-
tuples during running of the EM algorithm, but, after the EM re-structuring is finished,
we build the final translation model using composed rules for MT evaluation.
Figure 9 is the actual pipeline that we use for EM binarization. We first generate a
packed e-forest via parallel binarization. We then extract minimal translation rules from
Figure 9
Using the EM algorithm to choose re-structuring.
259
Computational Linguistics Volume 36, Number 2
the (e-forest, f, a)-tuples, producing synchronous derivation forests. We run the inside?
outside algorithm on the derivation forests until convergence. We obtain the Viterbi
derivations and project the English parses from the derivations. Finally, we extract
composed rules using Galley et al (2006)?s (e-tree, f, a)-based rule extraction algorithm.
When extracting composed rules from (e-parse, f, a)-tuples, we use an ?ignoring-X-
node? trick to the rule extraction method in Galley et al (2006) to avoid breaking the
local dependencies captured in complex rules. The trick is that new nodes introduced
by binarization are not counted when computing the rule size limit unless they appear
as the rule roots. The motivation is that newly introduced nodes break the local depen-
dencies, deepening the parses. In Galley et al, a composed rule is extracted only if the
number of internal nodes it contains does not exceed a limit, similar to the phrase length
limit in phrase-based systems. This means that rules extracted from the restructured
trees will be smaller than those from the unrestructured trees, if the X nodes are deleted
from the rules. As shown in Galley et al, smaller rules lose context, and thus give
lower translation accuracy. Ignoring X nodes when computing the rule sizes preserves
the unrestructured rules in the resulting translation model and adds substructures as
bonuses.
3.5 Experimental Results
We carried out experiments to evaluate different tree binarization methods in terms
of translation accuracy for Chinese-to-English translation. The baseline syntax MT sys-
tem was trained on the original, non-restructured trees. We also built one MT system
by training on left-binarizations of training trees, and another by training on EM-
binarizations of training trees.
Table 1 shows the results on end-to-endMT. The bootstrap p-values were computed
for the pairwise BLEU comparison between the EM binarization and the baseline. The
results show that tree binarization improves MT system accuracy, and that EM binariza-
tion outperforms left binarization. The results also show that the EM re-structuring sig-
nificantly outperforms (p <0.05) the no re-structuring baseline on the NIST08 eval set.
The MT improvement by tree re-structuring is also validated by our previous work
(Wang, Knight, and Marcu 2007), in which we reported a 1 BLEU point gain from EM
binarization under other training/testing conditions; other simple binarizationmethods
were examined in that work aswell, showing that simple binarizations also improveMT
accuracy, and that EM binarization consistently outperforms the simple binarization
methods.
Table 1
Translation accuracy versus binarization algorithms. In this and all other tables reporting BLEU
performance, statistically significant improvements over the baseline are highlighted. p = the
paired bootstrap p-value computed between each system and the baseline, showing the level at
which the two systems are significantly different.
EXPERIMENT NIST08 NIST08-NW
BLEU p BLEU p
no binarization (baseline) 29.12 ? 35.33 ?
left binarization 29.35 0.184 35.46 0.360
EM binarization 29.74 0.010 36.12 0.016
260
Wang et al Re-structuring, Re-labeling, and Re-aligning
Table 2
# admissible nodes, # rules versus re-structuring methods.
RE-STRUCTURING METHOD # ADMISSIBLE NODES (M) # RULES (M)
no binarization 13 76.0
left binarization 17.2 153.4
EM binarization 17.4 154.8
We think that these improvements are explained by the fact that tree re-structuring
introduces more admissible trees nodes in the training trees and enables the forming
of additional rules. As a result, re-structuring produces more rules. Table 2 shows the
number of admissible nodes made available by each re-structuring method, as well as
by the baseline. Table 2 also shows the sizes of the resulting grammars.
The EM binarization is able to introduce more admissible nodes because it mixes
both left and right binarizations in the same tree. We computed the binarization biases
learned by the EM algorithm for each nonterminal from the binarization forest of
parallel head binarizations of the training trees (Table 3). Of course, the binarization
bias chosen by left-/right-binarization methods would be 100% deterministic. One
noticeable message from Table 3 is that most of the categories are actually biased toward
left-binarization. The reason might be that the head sub-constituents of most English
categories tend to be on the left.
Johnson (1998b) argues that the more nodes there are in a treebank, the stronger
the independence assumptions implicit in the PCFG model are, and the less accurate
the estimated PCFG will usually be?more nodes break more local dependencies. Our
experiments, on the other hand, show MT accuracy improvement by introducing more
admissible nodes. This initial contradiction actually makes sense. The key is that we use
composed rules to build our final MT system and that we introduce the ?ignoring-X-
node? trick to preserve the local dependencies. More nodes in training trees weaken the
accuracy of a translation model of minimal rules, but boost the accuracy of a translation
model of composed rules.
Table 3
Binarization bias learned by the EM re-structuring method on the model 4 word alignments.
nonterminal left-binarization (%) right-binarization (%)
NP 98 2
NPB 1 99
VP 95 5
PP 86 14
ADJP 67 33
ADVP 76 24
S 94 6
S-C 17 83
SBAR 93 7
QP 89 11
WHNP 98 2
SINV 94 6
CONJP 69 31
261
Computational Linguistics Volume 36, Number 2
4. Re-Labeling Trees for Training
The syntax translation model explains (e-parse, f, a)-tuples by a series of applications
of translation rules. At each derivation step, which rule to apply next depends only
on the nonterminal label of the frontier node being expanded. In the Penn Treebank
annotation, the nonterminal labels are too coarse to encode enough context information
to accurately predict the next translation rule to apply. As a result, using the Penn
Treebank annotation can license ill-formed subtrees (Figure 10). This subtree contains
an error that induces a VP as an SG-C when the head of the VP is the finite verb dis-
cussed. The translation error leads to the ungrammatical ?... confirmed discussed ... ?. This
translation error occurs due to the fact that there is no distinction between finite VPs
and non-finite VPs in Penn Treebank annotation. Monolingual parsing suffers similarly,
but to a lesser degree.
Re-structuring of training trees enables the reuse of sub-constituent structures, but
further introduces new nonterminals and actually reduces the context for rules, thus
making this ?coarse nonterminal? problem more severe. In Figure 11, R23 may be
extracted from a construct like S(S CC S) via tree binarization, and R24 may be extracted
from a construct like S(NP NP-C VP) via tree binarization. Composing R23 and R24
forms the structure in Figure 11(b), which, however, is ill-formed. This wrong structure
in Figure 11(b) yields ungrammatical translations like he likes reading she does not like
reading. Tree binarization enables the reuse of substructures, but causes over-generation
of trees at the same time.
We solve the coarse-nonterminal problem by refining/re-labeling the training tree
labels. Re-labeling is done by enriching the nonterminal label of each tree node based
on its context information.
Re-labeling has already been used in monolingual parsing research to improve
parsing accuracy of PCFGs. We are interested in two types of re-labeling methods:
Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches
the labels of parser training trees using parent labels, head word tag labels, and/or
sibling labels. Automatic category splitting (Petrov et al 2006) refines a nonterminal
Figure 10
MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule
overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22.
262
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 11
Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired
from binarized training trees, aiming for reuse of substructures. Composing R23 and R24,
however, results in an ill-formed tree. The new nonterminal S introduced in tree binarization
needs to be refined into different sub-categories to prevent R23 and R24 from being composed.
Automatic category splitting can be employed for refining the S.
by classifying the nonterminal into a fine-grained sub-category, and this sub-classing
is learned via the EM algorithm. Category splitting is realized by several splitting-and-
merging cycles. In each cycle, the nonterminals in the PCFG rules are split by splitting
each nonterminal into two. The EM algorithm is employed to estimate the split PCFG on
the Penn Treebank training trees. After that, 50% of the new nonterminals are merged
based on some loss function, to avoid overfitting.
4.1 Linguistic Re-labeling
In the linguistically motivated approach, we employ the following set of rules to re-label
tree nodes. In our MT training data:
 SPLIT-VP: annotates each VP nodes with its head tag, and then merges all
finite VP forms to a single VPF.
 SPLIT-IN: annotates each IN node with the combination of IN and its
parent node label. IN is frequently overloaded in the Penn Treebank. For
instance, its parent can be PP or SBAR.
These two operations re-label the tree in Figure 12(a1) to Figure 12(b1). Example
rules extracted from these two trees are shown in Figure 12(a2) and Figure 12(b2),
respectively. We apply this re-labeling on the MT training tree nodes, and then acquire
rules from these re-labeled trees. We chose to split only these two categories because
our syntax MT system tends to frequently make parse errors in these two categories,
and because, as shown by Klein and Manning (2003), further refining the VP and IN
categories is very effective in improving monolingual parsing accuracy.
This type of re-labeling fixes the parse error in Figure 10. SPLIT-VP transforms the
R18 root to VPF, and the R17 frontier node to VP.VBN. Thus R17 and R18 can never be
composed, preventing the wrong tree being formed by the translation model.
4.2 Statistical Re-labeling
Our second re-labeling approach is to learn the split categories for the node labels of
the training trees via the EM algorithm, as in Petrov et al (2006). Rather than using
263
Computational Linguistics Volume 36, Number 2
Figure 12
Re-labeling of parse trees.
264
Wang et al Re-structuring, Re-labeling, and Re-aligning
their parser to directly produce category-split parse trees for the MT training data, we
separate the parsing step and the re-labeling step. The re-labeling method is as follows.
1. Run a parser to produce the MT training trees.
2. Binarize the MT training trees via the EM binarization algorithm.
3. Learn an n-way split PCFG from the binarized trees via the algorithm
described in Petrov et al (2006).
4. Produce the Viterbi split annotations on the binarized training trees with
the learned category-split PCFG.
As we mentioned earlier, tree binarization sometimes makes the decoder over-
generalize the trees in the MT outputs, but we still binarize the training trees before
performing category splitting, for two reasons. The first reason is that the improve-
ment on MT accuracy we achieved by tree re-structuring indicates that the benefit
we obtained from structure reuse triumphs the problem of tree over-generalization.
The second is that carrying out category splitting on unbinarized training trees blows
up the grammar?splitting a CFG rule of rank 10 results in 211 split rules. This re-
labeling procedure tries to achieve further improvement by trying to fix the tree over-
generalization problem of re-structuring while preserving the gain we have already
obtained from tree re-structuring.
Figure 12(c1) shows a category-split tree, and Figure 12(c2) shows the minimal xRs
rules extracted from the split tree. In Figure 12(c2), the two VPs (VP-0 and VP-2) now
belong to two different categories and cannot be used in the same context.
In this re-labeling procedure, we separate the re-labeling step from the parsing step,
rather than using a parser like the one in Petrov et al (2006) to directly produce category-
split parse trees on the English corpus. We think that we benefit from this separation in
the following ways: First, this gives us the freedom to choose the parser to produce the
initial trees. Second, this enables us to train the re-labeler on the domains where the MT
system is trained, instead of on the Penn Treebank. Third, this enables us to choose our
own tree binarization methods.
Tree re-labeling fragments the translation rules. Each refined rule now fits in fewer
contexts than its corresponding coarse rule. Re-labeling, however, does not explode
the grammar size, nor does re-labeling deteriorate the reuse of substructures. This
is because the re-labeling (whether linguistic or automatic) results in very consistent
annotations. Table 4 shows the sizes of the translation grammars from different re-
labelings of the training trees, as well as that from the unrelabeled ones.
Table 4
Grammar size vs. re-labeling methods. Re-labeling does not explode the grammar size.
RE-LABELING METHOD # RULES (M) NONTERMINAL SET SIZE
No re-labeling 154.80 144
Linguistically motivated re-labeling 154.97 210
4-way splitting (90% merging) 158.89 178
8-way splitting (90% merging) 160.62 195
4-way splitting (50% merging) 164.15 326
265
Computational Linguistics Volume 36, Number 2
It would be very interesting to perform automatic category splitting with synchro-
nous translation rules and run the EM algorithm on the synchronous derivation forests.
Synchronous category splitting is computationally much more expensive, so we do not
study it here.
4.3 Experimental Results
We ran end-to-end MT experiments by re-labeling the MT training trees. Our two base-
line systems were a syntax MT system with neither re-structuring nor re-labeling, and
a syntax MT system with re-structuring but no re-labeling. The linguistically motivated
re-labeling method was applied directly on the original (unrestructured) training trees,
so that it could be compared to the first baseline. The automatic category splitting re-
labeling method was applied to binarized trees so as to avoid the explosion of the split
grammar, so it is compared to the second baseline. The experiment results are shown in
Table 5.
Both re-labeling methods help MT accuracy. Putting both re-structuring and re-
labeling together results in 0.93 BLEU points improvement on NIST08 set, and 1 BLEU
point improvement on the newswire subset. All p-values are computed between the
re-labeling systems and Baseline1. The improvement made by the linguistically moti-
vated re-labeling method is significant at the 0.05 level. Because the automatic category
splitting is carried out on the top of EM re-structuring and because, as we have already
shown, EM re-structuring significantly improves Baseline1, putting them together re-
sults in better translations with more confidence.
If we compare these results to those in Table 1, we notice that re-structuring tends
to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring
overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and
Table 1 show substructure reuse mitigates structure over-generalization in our tree re-
structuring method.
5. Re-aligning (Tree, String) Pairs for Training
So far, we have improved the English structures in our parsed, aligned training corpus.
We now turn to improving the word alignments.
Some MT systems use the same model for alignment and translation?examples
include Brown et al (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada
and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al
for alignment, then collect counts for a completely different model, such as Och andNey
Table 5
Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was
carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values
are computed against Baseline1.
EXPERIMENT NIST08 NIST08-NW
BLEU p BLEU p
Baseline1 (no re-structuring and no re-labeling) 29.12 ? 35.33 ?
Linguistically motivated re-labeling 29.57 0.029 35.85 0.050
Baseline2 (EM re-structuring but no re-labeling) 29.74 ? 36.12 ?
4-way splitting (w/ 90% merging) 30.05 0.001 36.42 0.003
266
Wang et al Re-structuring, Re-labeling, and Re-aligning
(2004) or Chiang (2007). Our basic syntax-based system falls into this second category,
as we learn our syntactic translation model from a corpus aligned with word-based
techniques. We would like to inject more syntactic reasoning into the alignment process.
We start by contrasting two generative translation models.
5.1 The Traditional IBM Alignment Model
IBMModel 4 (Brown et al 1993) learns a set of four probability tables to compute P( f |e)
given a foreign sentence f and its target translation e via the following (simplified)
generative story:
1. A fertility y for each word ei in e is chosen with probability Pfert(y|ei).
2. A null word is inserted next to each fertility-expanded word with
probability Pnull.
3. Each token ei in the fertility-expanded word and null string is translated
into some foreign word fi in f with probability Ptrans( fi|ei).
4. The position of each foreign word fi that was translated from ei is changed
by? (which may be positive, negative, or zero) with probability
Pdistortion(?|A(ei),B( fi)), where A and B are functions over the source and
target vocabularies, respectively.
Brown et al (1993) describe an EM algorithm for estimating values for the four
tables in the generative story. With those values in hand, we can calculate the highest-
probability (Viterbi) alignment for any given string pair.
Two scale problems arise in this algorithm. The first is the time complexity of enu-
merating alignments for fractional count collection. This is solved by considering only
a subset of alignments, and by bootstrapping the Ptrans table with a simpler model that
admits fast count collection via dynamic programming, such as IBM Model 1 (Brown
et al 1993) or Aachen HMM (Vogel, Ney, and Tillmann 1996). The second problem is
one of space. In theory, the initial Ptrans table contains a cell for every English word
paired with every Chinese word?this would be infeasible. Fortunately, in practice, the
table can be initialized with only those word pairs observed co-occurring in the parallel
training text.
5.2 A Syntax Re-alignment Model
Our syntax translation model learns a single probability table to compute P(etree, f )
given a foreign sentence f and a parsed target translation etree. In the following gen-
erative story we assume a starting variable with syntactic type v.
1. Choose a rule r to replace v, with probability Prule(r|v).
2. For each variable with syntactic type vi in the partially completed (tree,
string) pair, continue to choose rules ri with probability Prule(ri|vi) to
replace these variables until there are no variables remaining.
We can use this model to explain unaligned (tree, string) pairs from our training
data. With a large enough rule set, any given (tree, string) pair will admit many deriva-
tions. Consider again the example from Figure 1. The particular alignment associated
267
Computational Linguistics Volume 36, Number 2
with that (tree, string) pair yields the minimal rules of Figure 2. A different alignment
yields different rules. Figure 13 shows two other alignments and their corresponding
minimal rules. As noted before, a set of minimal rules in proper sequence forms
a derivation tree of rules that explains the (tree, string) pair. Because rules explain
variable-size fragments (e.g., R35 vs. R6), the possible derivation trees of rules that
explain a sentence pair have varying sizes. The smallest such derivation tree has a single
large rule (which does not appear in Figure 13). When our model chooses a particular
derivation tree of minimal rules to explain a given (tree, string) pair it implicitly chooses
the alignment that produced these rules as well.3 Our model can choose a derivation by
using any of the rules in Figures 13 and 2. We would prefer it select the derivation that
yields the good alignment in Figure 1.
We can also develop an EM learning approach for this model. As in the IBM
approach, we have both time and space issues. Time complexity, as we will see sub-
sequently, is O(mn3), where m is the number of nodes in the English training tree and
n is the length of the corresponding Chinese string. Space is more of a problem. We
would like to initialize EM with all the rules that might conceivably be used to explain
the training data. However, this set is too large to practically enumerate.
To reduce the model space we first create a bootstrap alignment using a simpler
word-based model. Then we acquire a set of minimal translation rules from the (tree,
string, alignment) triples. Armed with these rules, we can discard the word-based
alignments and re-alignwith the syntax translation model.
We summarize the approach described in this section as:
1. Obtain bootstrap alignments for a training corpus using word-based
alignment.
2. Extract minimal rules from the corpus and alignments using GHKM,
noting the partial alignment that is used to extract each rule.
3. Construct derivation forests for each (tree, string) pair, ignoring the
alignments, and run EM to obtain Viterbi derivation trees, then use the
annotated partial alignments to obtain Viterbi alignments.
4. Use the new alignments to re-train the full MT system, this time collecting
composed rules as well as minimal rules.
5.3 EM Training for the Syntax Translation Model
Consider the example of Figure 13 again. The top alignment was the bootstrap align-
ment, and thus prior to any experiment we obtained the corresponding indicated
minimal rule set and derivation. This derivation is reasonable but there are some poorly
motivated rules, from a linguistic standpoint. The Chinese word
??
roughly means
the two shores in this context, but the rule R35 learned from the alignment incorrectly
includes between. However, other sentences in the training corpus have the correct
3 Strictly speaking there is actually a one-to-many mapping between a derivation tree of minimal rules and
the alignment that yields these rules, due to the handling of unaligned words. However, the choice of one
partial alignment over another does not affect results and in practice we impose a one-to-one mapping
between minimal rules and the partial alignments that imply them by selecting the most frequently
observed partial alignment for a given minimal rule.
268
Wang et al Re-structuring, Re-labeling, and Re-aligning
Figure 13
The minimal rules extracted from two different alignments of the sentence in Figure 1.
alignment, which yields rules in Figure 2, such as R8. Figure 2 also contains rules R4
and R6, learned from yet other sentences in the training corpus, which handle the
?
...
?
structure (which roughly translates to in between), thus allowing a derivation which
contains the minimal rule set of Figure 2 and implies the alignment in Figure 1.
EM distributes rule probabilities in such a way as to maximize the probability of the
training corpus. It thus prefers to use one rule many times instead of several different
269
Computational Linguistics Volume 36, Number 2
rules for the same situation over several sentences, if possible. R35 is a possible rule
in 46 of the 329,031 sentence pairs in the training corpus, and R8 is a possible rule in
100 sentence pairs. Well-formed rules are more usable than ill-formed rules and the
partial alignments behind these rules, generally also well-formed, become favored as
well. The top row of Figure 14 contains an example of an alignment learned by the
bootstrap alignment model that includes an incorrect link. Rule R25, which is extracted
from this alignment, is a poor rule. A set of commonly seen rules learned from other
training sentences provide a more likely explanation of the data, and the consequent
alignment omits the spurious link.
Figure 14
The impact of a bad alignment on rule extraction. Including the alignment link indicated by the
dotted line in the example leads to the rule set in the second row. The re-alignment procedure
described in Section 5.2 learns to prefer the rule set at bottom, which omits the bad link.
270
Wang et al Re-structuring, Re-labeling, and Re-aligning
Table 6
Translation performance, grammar size versus the re-alignment algorithm proposed in
Section 5.2, and re-alignment as modified in Section 5.4.
EXPERIMENT NIST08 NIST08-NW # RULES (M)
BLEU p BLEU p
no re-alignment (baseline) 29.12 ? 35.33 ? 76.0
EM re-alignment 29.18 0.411 35.52 0.296 75.1
EM re-alignment with size prior 29.37 0.165 35.96 0.050 110.4
Now we need an EM algorithm for learning the parameters of the rule set that
maximize
?
corpus
P(tree, string). Knight, Graehl, andMay (2008) present a generic such algo-
rithm for tree-to-string transducers that runs in O(mn3) time, as mentioned earlier. The
algorithm consists of two components: DERIV, which is a procedure for constructing
a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus
given that corpus and a rule set, and TRAIN, which is an iterative parameter-setting
procedure.
We initially attempted to use the top-down DERIV algorithm of Knight, Graehl, and
May (2008), but as the constraints of the derivation forests are largely lexical, too much
time was spent on exploring dead-ends. Instead we build derivation forests using the
following sequence of operations:
1. Binarize rules using the synchronous binarization algorithm for
tree-to-string transducers described in Zhang et al (2006).
2. Construct a parse chart with a CKY parser simultaneously constrained on
the foreign string and English tree, similar to the bilingual parsing of Wu
(1997).4
3. Recover all reachable edges by traversing the chart, starting from the
topmost entry.
Because the chart is constructed bottom-up, leaf lexical constraints are encountered
immediately, resulting in a narrower search space and faster running time than the
top-down DERIV algorithm for this application. The Viterbi derivation tree tells us
which English words produce which Chinese words, so we can extract a word-to-word
alignment from it.
Although in principle the re-alignment model and translation model learn parame-
ter weights over the same rule space, in practice we limit the rules used for re-alignment
to the set of minimal rules.
5.4 Adding a Rule Size Prior
An initial re-alignment experiment shows a small rise in BLEU scores from the baseline
(Table 6), but closer inspection of the rules favored by EM implies we can do even better.
4 In the cases where a rule is not synchronous-binarizable, standard left?right binarization is performed
and proper permutation of the disjoint English tree spans must be verified when building the part of the
chart that uses this rule.
271
Computational Linguistics Volume 36, Number 2
EM has a tendency to favor a few large rules over many small rules, even when the
small rules are more useful. Referring to the rules in Figures 2 and 13, note that possible
derivations for (taiwan?s,
?l
)5 are R33, R2?R3, and R38?R40. Clearly the third deriva-
tion is not desirable, and we do not discuss it further. Between the first two derivations,
R2?R3 is preferred over R33, as the conditioning for possessive insertion is not related to
the specific Chinese word being inserted. Of the 1,902 sentences in the training corpus
where this pair is seen, the bootstrap alignments yield the R33 derivation 1,649 times
and the R2?R3 derivation 0 times. Re-alignment does not change the result much; the
new alignments yield the R33 derivation 1,613 times and again never choose R2?R3. The
rules in the second derivation themselves are not rarely seen?R2 is in 13,311 forests
other than those where R33 is seen, and R3 is in 2,500 additional forests. EM gives R2 a
probability of e?7.72?better than 98.7% of rules, and R3 a probability of e?2.96. But R33
receives a probability of e?6.32 and is preferred over the R2?R3 derivation, which has a
combined probability of e?10.68.
The preference for shorter derivations containing large rules over longer derivations
containing small rules is due to a general tendency for EM to prefer derivations with
few atoms. Marcu and Wong (2002) note this preference but consider the phenomenon
a feature, rather than a bug. Zollmann and Sima?an (2005) combat the overfitting aspect
for parsing by using a held-out corpus and a straight maximum likelihood estimate,
rather than EM. DeNero, Bouchard-Co?te?, and Klein (2008) encourage small rules with a
modeling approach; they put a Dirichlet process prior of rule size over their model and
learn the parameters of the geometric distribution of that prior with Gibbs sampling.We
use a simpler modeling approach to accomplish the same goals as DeNero, Bouchard-
Co?te?, and Klein which, although less elegant, is more scalable and does not require a
separate Bayesian inference procedure.
As the probability of a derivation is determined by the product of its atom probabil-
ities, longer derivations with more probabilities to multiply have an inherent disadvan-
tage against shorter derivations, all else being equal. EM is an iterative procedure and
thus such a bias can lead the procedure to converge with artificially raised probabilities
for short derivations and the large rules that constitute them. The relatively rare ap-
plicability of large rules (and thus lower observed partial counts) does not overcome the
inherent advantage of large coverage. To combat this, we introduce size terms into our
generative story, ensuring that all competing derivations for the same sentence contain
the same number of atoms:
1. Choose a rule size s with cost csize(s)
s?1.
2. Choose a rule r (of size s) to replace the start symbol with probability
Prule(r|s, v).
3. For each variable in the partially completed (tree, string) pair, continue to
choose sizes followed by rules, recursively to replace these variables until
there are no variables remaining.
This generative story changes the derivation comparison from R33 vs. R2?R3 to S2?
R33 vs. R2?R3, where S2 is the atom that represents the choice of size 2 (the size of a rule
5 The Chinese gloss is simply ?taiwan?.
272
Wang et al Re-structuring, Re-labeling, and Re-aligning
in this context is the number of non-leaf and non-root nodes in its tree fragment). Note
that the variable number of inclusions implied by the exponent in the generative story
above ensures that all derivations have the same size. For example, a derivation with
one size-3 rule, a derivation with one size-2 and one size-1 rule, and a derivation with
three size-1 rules would each have three atoms. With this revised model that allows for
fair comparison of derivations, the R2?R3 derivation is chosen 1,636 times, and S2?R33
is not chosen. R33 does, however, appear in the translation model, as the expanded rule
extraction described in Section 1 creates R33 by joining R2 and R3.
The probability of size atoms, like that of rule atoms, is decided by EM. The revised
generative story tends to encourage smaller sizes by virtue of the exponent. This does
not, however, simply ensure the largest number of rules per derivation is used in all
cases. Ill-fitting and poorly motivated rules such as R42, R43, and R44 in Figure 13 are
not preferred over R8, even though they are smaller. However, R6 and R8 are preferred
over R35, as the former are useful rules. Although the modified model does not sum to
1, it can nevertheless lead to an improvement in BLEU score.
5.5 Experimental Results
The end-to-end MT experiment used as baseline and described in Sections 3.5 and 4.3
was trained on IBMModel 4 word alignments, obtained by running GIZA, as described
in Section 2. We compared this baseline to an MT system that used alignments obtained
by re-aligning the GIZA alignments using the method of Section 5.2 with the 36 million
word subset of the training corpus used for re-alignment learning. We next compared
the baseline to an MT system that used re-alignments obtained by also incorporating
the size prior described in Section 5.4. As can be seen by the results in Table 6, the size
prior method is needed to obtain reasonable improvement in BLEU. These results are
consistent with those reported in May and Knight (2007), where gains in Chinese and
ArabicMT systemswere observed, though over aweaker baseline andwith less training
data than is used in this work.
6. Combining Techniques
We have thus far seen gains in BLEU score by independent improvements in training
data tree structure, syntax labeling, and alignment. This naturally raises the question
of whether the techniques can be combined, that is, if improvement in one aspect of
training data aids in improvement of another. As reported in Section 4.3 and Table 5,
we were able to improve re-labeling efforts and take advantage of the split-and-merge
technique of Petrov et al (2006) by first re-structuring via the method described in Sec-
tion 3.4. It is unlikely that such re-structuring or re-labeling would aid in a subsequent
re-alignment procedure like that of Section 5.2, for re-structuring changes trees based
on a given alignment, and re-alignment can only change links when multiple instances
of a (subtree, substring) tuple are found in the data with different partial alignments.
Re-structuring beforehand changes the trees over different alignments differently. It is
unlikely that many (subtree, substring) tuples with more than one partial alignment
would remain after a re-structuring.
However, re-structuring may benefit from a prior re-alignment. We do not want re-
structuring decisions to be made over bad alignments, so unifying alignments based
on common syntax should lead EM to make a more confident binarization decision.
273
Computational Linguistics Volume 36, Number 2
Table 7
Summary of experiments in this article, including a combined experiment with re-alignment,
re-structuring, and re-labeling.
EXPERIMENT NIST08 NIST08-NW # RULES (M)
BLEU p BLEU p
Baseline (no binarization, no re-labeling, 29.12 ? 35.33 ? 76.0
Model 4 alignments)
left binarization 29.35 0.184 35.46 0.360 153.4
EM binarization 29.74 0.010 36.12 0.016 154.8
Linguistic re-labeling 29.57 0.029 35.85 0.050 154.97
EM binarization + EM re-labeling 30.05 0.001 36.42 0.003 158.89
(4-way splitting w/ 90% merging)
EM re-alignment 29.18 0.411 35.52 0.296 75.1
Size prior EM re-alignment 29.37 0.165 35.96 0.050 110.4
Size prior EM re-alignment + 30.6 0.001 36.73 0.002 222.0
EM binarization + EM re-labeling
Better re-structuring should in turn lead to better re-labeling, and this should increase
the performance of the overall MT pipeline.
To test this hypothesis we pre-processed alignments using the modified re-
alignment procedure described in Section 5.4. We next used those alignments to obtain
new binarizations of trees following the EM binarization method described in Sec-
tion 3.4. Finally, re-labeling was done on these binarized trees using 4-way splitting with
90% merging, as described in Section 4. The final trees, along with the alignments used
to get these trees and of course the parallel Chinese sentences, were then used as the
training data of our MT pipeline. The results of this combined experiment are shown
in Table 7 along with the other experiments from this article, for ease of comparison.
As can be seen from this table, the progressive improvement of training data leads
to an overall improvement in MT system performance. As noted previously, there
tends to be a correspondence between the number of unique rules extracted and MT
performance. The final combined experiment has the greatest number of unique rules.
The improvements made to syntax and alignment described in this article unify these
two independently determined annotations over the bitext, and this thus leads to more
admissible nodes and a greater ability to extract rules. Such a unification can lead to
over-generalization, as rules lacking sufficient context may be extracted and used to the
system?s detriment. This is why a re-labeling technique is also needed, to ensure that
sufficient rule specificity is maintained.
7. Conclusion
This article considered three modifications to MT training data that encourage im-
proved performance in a state-of-the-art syntactic MT system. The improvements
changed syntactic structure, altered bracket labels, and unified alignment across sen-
tences, and when combined led to an improvement of 1.48 BLEU points over a strong
baseline in Chinese?English translation. The techniques herein described require only
274
Wang et al Re-structuring, Re-labeling, and Re-aligning
the training data used in the original MT task and are thus applicable to a string-to-tree
MT system for any language pair.
Acknowledgments
Some of the results in this article appear
in Wang, Knight, and Marcu (2007) and
May and Knight (2007). The linguistically
motivated re-labeling method is due to
Steve DeNeefe, Kevin Knight, and David
Chiang. The authors also wish to thank
Slav Petrov for his help with the Berkeley
parser, and the anonymous reviewers
for their helpful comments. This research
was supported under DARPA Contract
No. HR0011-06-C-0022, BBN subcontract
9500008412.
References
Alshawi, Hiyan, Srinivas Bangalore,
and Shona Douglas. 1998. Automatic
acquisition of hierarchical transduction
models for machine translation. In
Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
(ACL) and 17th International Conference on
Computational Linguistics (COLING) 1998,
pages 41?47, Montre?al.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?312.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st North American Chapter of the
Association for Computational Linguistics
Conference (NAACL), pages 132?139,
Seattle, WA.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Cohn, Trevor and Phil Blunsom. 2009.
A Bayesian model of syntax-directed
tree to string grammar induction. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 352?361, Singapore.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 16?23, Madrid.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
EM algorithm. Journal of the Royal
Statistical Society, 39(1):1?38.
DeNeefe, Steve, Kevin Knight, Wei Wang,
and Daniel Marcu. 2007. What can
syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP?CoNLL-2007,
pages 755?763, Prague.
DeNero, John, Alexandre Bouchard-Co?te?,
and Dan Klein. 2008. Sampling alignment
structure under a Bayesian translation
model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 314?323,
Honolulu, HI.
Gale, William A. and Geoffrey Sampson.
1996. Good-Turing frequency estimation
without tears. Journal of Quantitative
Linguistics, 2(3):217?237.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the 21st International
Conference on Computational Linguistics
(COLING) and 44th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the
Human Language Technology Conference and
the North American Association for
Computational Linguistics (HLT-NAACL),
pages 273?280, Boston, MA.
Good, Irving J. 1953. The population
frequencies of species and the estimation
of population parameters. Biometrika,
40(3):237?264.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573?605.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
accuracy. In Proceedings of the main
conference on Human Language Technology
Conference of the North American Chapter
of the Association of Computational
Linguistics (NAACL-HLT), pages 240?247,
New York, NY.
Johnson, Mark. 1998a. The DOP estimation
method is biased and inconsistent.
Computational Linguistics, 28(1):71?76.
Johnson, Mark. 1998b. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Klein, Dan and Chris Manning. 2003.
Accurate unlexicalized parsing. In
275
Computational Linguistics Volume 36, Number 2
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 423?430, Sapporo.
Kneser, Reinhard and Hermann Ney.
1995. Improved backing-off for
m-gram language modeling. In
Proceedings of the International Conference
on Acoustics, Speech, and Signal
Processing (ICASSP) 1995, pages 181?184,
Detroit, MI.
Knight, Kevin and Jonathan Graehl. 2005.
An overview of probabilistic tree
transducers for natural language
processing. In Proceedings of the Sixth
International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing), pages 1?25,
Mexico City.
Knight, Kevin, Jonathan Graehl, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 388?395,
Barcelona.
Lari, Karim and Steve Young. 1990. The
estimation of stochastic context-free
grammars using the inside-outside
algorithm. Computer Speech and Language,
4:35?56.
Marcu, Daniel and William Wong. 2002.
A phrase-based, joint probability model
for statistical machine translation.
In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 133?139,
Philadelphia, PA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
May, Jonathan and Kevin Knight. 2007.
Syntactic re-alignment models for
machine translation. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP?CoNLL), pages 360?368,
Prague.
Melamed, I. Dan, Giorgio Satta, and
Benjamin Wellington. 2004. Generalized
multitext grammars. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 662?669, Barcelona.
Mi, Haitao and Liang Huang. 2008.
Forest-based translation rule extraction.
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 206?214,
Honolulu, HI.
Och, Franz and Hermann Ney. 2004. The
alignment template approach to statistical
machine translation. Computational
Linguistics, 30(4):417?449.
Och, Franz Josef. 2003. Minimum error rate
training for machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo.
Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning
accurate, compact, and interpretable
tree annotation. In Proceedings of the
21st International Conference on
Computational Linguistics (COLING) and
44th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 433?440, Sydney.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP) 2002, pages 901?904,
Denver, CO.
Talbot, David and Miles Osborne. 2007.
Randomised language modelling for
statistical machine translation. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL), pages 512?519, Prague.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation. In
Proceedings of the International Conference on
Computational Linguistics (COLING) 1996,
pages 836?841, Copenhagen.
Wang, Wei, Kevin Knight, and Daniel
Marcu. 2007. Binarizing syntax trees
to improve syntax-based machine
translation accuracy. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP?CoNLL), pages 746?754,
Prague.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2001.
A syntax-based statistical translation
model. In Proceedings of the 39th
Annual Meeting of the Association for
276
Wang et al Re-structuring, Re-labeling, and Re-aligning
Computational Linguistics (ACL),
pages 523?530, Toulouse.
Yamada, Kenji and Kevin Knight. 2002.
A decoder for syntax-based statistical
MT. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 303?310,
Philadelphia, PA.
Zhang, Hao, Liang Huang, Daniel Gildea,
and Kevin Knight. 2006. Synchronous
binarization for machine translation.
In Proceedings of the main conference
on Human Language Technology
Conference of the North American Chapter
of the Association of Computational
Linguistics (HLT-NAACL), pages 256?263,
New York, NY.
Zollmann, Andreas and Khalil Sima?an.
2005. A consistent and efficient estimator
for data-oriented parsing. Journal of
Automata, Languages and Combinatorics,
10(2/3):367?388.
277

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 162?171,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HyTER: Meaning-Equivalent Semantics for Translation Evaluation
Markus Dreyer
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA
mdreyer@sdl.com
Daniel Marcu
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA
dmarcu@sdl.com
Abstract
It is common knowledge that translation is
an ambiguous, 1-to-n mapping process, but
to date, our community has produced no em-
pirical estimates of this ambiguity. We have
developed an annotation tool that enables us
to create representations that compactly en-
code an exponential number of correct trans-
lations for a sentence. Our findings show that
naturally occurring sentences have billions of
translations. Having access to such large sets
of meaning-equivalent translations enables us
to develop a new metric, HyTER, for transla-
tion accuracy. We show that our metric pro-
vides better estimates of machine and human
translation accuracy than alternative evalua-
tion metrics.
1 Motivation
During the last decade, automatic evaluation met-
rics (Papineni et al, 2002; Snover et al, 2006; Lavie
and Denkowski, 2009) have helped researchers ac-
celerate the pace at which they improve machine
translation (MT) systems. And human-assisted met-
rics (Snover et al, 2006) have enabled and sup-
ported large-scale U.S. government sponsored pro-
grams, such as DARPA GALE (Olive et al, 2011).
However, these metrics have started to show signs of
wear and tear.
Automatic metrics are often criticized for provid-
ing non-intuitive scores ? few researchers can ex-
plain to casual users what a BLEU score of 27.9
means. And researchers have grown increasingly
concerned that automatic metrics have a strong bias
towards preferring statistical translation outputs; the
NIST (2008, 2010), MATR (Gao et al, 2010) and
WMT (Callison-Burch et al, 2011) evaluations held
during the last five years have provided ample ev-
idence that automatic metrics yield results that are
inconsistent with human evaluations when compar-
ing statistical, rule-based, and human outputs.
In contrast, human-informed metrics have other
deficiencies: they have large variance across human
judges (Bojar et al, 2011) and produce unstable re-
sults from one evaluation to another (Przybocki et
al., 2011). Because evaluation scores are not com-
puted automatically, systems developers cannot au-
tomatically tune to human-based metrics.
Table 1 summarizes the dimensions along which
evaluation metrics should do well and the strengths
and weaknesses of the automatic and human-
informed metrics proposed to date. Our goal is
to develop metrics that do well along all these di-
mensions. The fundamental insight on which our
research relies is that the failures of current auto-
matic metrics are not algorithmic: BLEU, Meteor,
TER (Translation Edit Rate), and other metrics ef-
ficiently and correctly compute informative distance
functions between a translation and one or more hu-
man references. We believe that these metrics fail
simply because they have access to sets of human
references that are too small. If we had access to
the set of all correct translations of a given sentence,
we could measure the minimum distance between a
translation and the set. When a translation is perfect,
it can be found in the set, so it requires no editing to
produce a perfect translation. Therefore, its score
should be zero. If the translation has errors, we can
162
Desiderata Auto. Manu. HyTER
Metric is intuitive N Y Y
Metric is computed automatically Y N Y
Metric is stable and reproducible from one evaluation to another Y N Y
Metric works equally well when comparing human and automatic outputs
and when comparing rule-based, statistical-based, and hybrid engines
N Y Y
System developers can tune to the metric Y N Y
Metric helps developers identify deficiencies of MT engines N N Y
Table 1: Desiderata of evaluation metrics: Current automatic and human metrics, proposed metric.
efficiently compute the minimum number of edits
(substitutions, deletions, insertions, moves) needed
to rewrite the translation into the ?closest? reference
in the set. Current automatic evaluation metrics do
not assign their best scores to most perfect transla-
tions because the set of references they use is too
small; their scores can therefore be perceived as less
intuitive.
Following these considerations, we developed an
annotation tool that enables one to efficiently create
an exponential number of correct translations for a
given sentence, and present a new evaluation met-
ric, HyTER, which efficiently exploits these mas-
sive reference networks. In the rest of the paper, we
first describe our annotation environment, process,
and meaning-equivalent representations that we cre-
ate (Section 2). We then present the HyTER met-
ric (Section 3). We show that this new metric pro-
vides better support than current metrics for machine
translation evaluation (Section 4) and human trans-
lation proficiency assessment (Section 5).
2 Annotating sentences with exponential
numbers of meaning equivalents
2.1 Annotation tool
We have developed a web-based annotation tool
that can be used to create a representation encoding
an exponential number of meaning equivalents
for a given sentence. The meaning equivalents
are constructed in a bottom-up fashion by typing
translation equivalents for larger and larger phrases.
For example, when building the meaning equiv-
alents for the Spanish phrase ?el primer ministro
italiano Silvio Berlusconi?, the annotator first types
in the meaning equivalents for ?primer ministro?
? ?prime-minister; PM; prime minister; head of
government; premier; etc.?; ?italiano? ? ?Italian?;
and ?Silvio Berlusconi? ? ?Silvio Berlusconi;
Berlusconi?. The tool creates a card that stores
all the alternative meanings for a phrase as a
determinized FSA and gives it a name in the target
language that is representative of the underly-
ing meaning-equivalent set: [PRIME-MINISTER],
[ITALIAN], and [SILVIO-BERLUSCONI]. Each base
card can be thought of expressing a semantic con-
cept. A combination of existing cards and additional
words can be subsequently used to create larger
meaning equivalents that cover increasingly larger
source sentence segments. For example, to create
the meaning equivalents for ?el primer ministro ital-
iano? one can drag-and-drop existing cards or type
in new words: ?the [ITALIAN] [PRIME-MINISTER];
the [PRIME-MINISTER] of Italy?; to create the
meaning equivalents for ?el primer ministro italiano
Silvio Berlusconi?, one can drag-and-drop and type:
?[SILVIO-BERLUSCONI] , [THE-ITALIAN-PRIME-
MINISTER]; [THE-ITALIAN-PRIME-MINISTER] ,
[SILVIO-BERLUSCONI]; [THE-ITALIAN-PRIME-
MINISTER] [SILVIO-BERLUSCONI] ?. All meaning
equivalents associated with a given card are ex-
panded and used when that card is re-used to create
larger meaning-equivalent sets.
The annotation tool supports, but does not en-
force, re-use of annotations created by other anno-
tators. The resulting meaning equivalents are stored
as recursive transition networks (RTNs), where each
card is a subnetwork; if needed, these non-cyclic
RTNs can be automatically expanded into finite-
state acceptors (FSAs, see Section 3).
2.2 Data and Annotation Protocols
Using the annotation tool, we have created meaning-
equivalent annotations for 102 Arabic and 102 Chi-
nese sentences ? a subset of the ?progress set? used
in the 2010 Open MT NIST evaluation (the average
163
sentence length was 24 words). For each sentence,
we had access to four human reference translations
produced by LDC and five MT system outputs,
which were selected by NIST to cover a variety of
system architectures (statistical, rule-based, hybrid)
and performances. For each MT output, we also had
access to sentence-level HTER scores (Snover et al,
2006), which were produced by experienced LDC
annotators.
We have experimented with three annotation pro-
tocols:
? Ara-A2E and Chi-C2E: Foreign language natives
built English networks starting from foreign lan-
guage sentences.
? Eng-A2E and Eng-C2E: English natives built En-
glish networks starting from ?the best translation?
of a foreign language sentence, as identified by
NIST.
? Eng*-A2E and Eng*-C2E: English natives built
English networks starting from ?the best transla-
tion?, but had access to three additional, indepen-
dently produced human translations to boost their
creativity.
Each protocol was implemented independently by
at least three annotators. In general, annotators need
to be fluent in the target language, familiar with the
annotation tool we provide and careful not to gen-
erate incorrect paths, but they do not need to be lin-
guists.
2.3 Exploiting multiple annotations
For each sentence, we combine all networks that
were created by the different annotators. We eval-
uate two different combination methods, each of
which combines networks N1 and N2 of two anno-
tators (see an example in Figure 1):
(a) Standard union U(N1, N2): The standard
finite-state union operation combines N1 and N2
on the whole-network level. When traversing
U(N1, N2), one can follow a path that comes from
either N1 or N2.
(b) Source-phrase-level union SPU(N1, N2): As
an alternative, we introduce SPU, a more fine-
grained union which operates on sub-sentence seg-
ments. Here we exploit the fact that each annotator
explicitly aligned each of her various subnetworks
N1
the level of approval
was
close to
zero
the approval rate
practically
the approval level
was
close to
zero
the approval rate
about equal to
(a)
was
zero
the approval rate
the level of approval
the approval level
close to
practically
about equal to
(b)
N2
Figure 1: (a) Finite-state union versus (b) source-phrase-
level union (SPU). The former does not contain the path
?the approval level was practically zero?.
for a given sentence to a source span of that sen-
tence. Now for each pair of subnetworks (S1, S2)
from N1 and N2, we build their union if they are
compatible; two subnetworks S1, S2 are defined to
be compatible if they are aligned to the same source
span and have at least one path in common.
The purpose of SPU is to create new paths by mix-
ing paths from N1 and N2. In Figure 1, for example,
the path ?the approval level was practically zero? is
contained in the SPU, but not in the standard union.
We build SPUs using a dynamic programming al-
gorithm that builds subnetworks bottom-up, build-
ing unions of intermediate results. Two larger sub-
networks can be compatible only if their recursive
smaller subnetworks are compatible. Each SPU con-
tains at least all paths from the standard union.
2.4 Empirical findings
Now that we have described how we created partic-
ular networks for a given dataset, we describe some
empirical findings that characterize our annotation
process and the created networks.
Meaning-equivalent productivity. When we
compare the productivity of the three annotation
protocols in terms of the number of reference trans-
lations that they enable, we observe that target lan-
guage natives that have access to multiple human
references produce the largest networks. The me-
dian number of paths produced by one annotator un-
der the three protocols varies from 7.7 ? 105 paths
for Ara-A2E, to 1.4 ? 108 paths for Eng-A2E, to
5.9 ? 108 paths for Eng*-A2E; in Chinese, the me-
164
dians vary from 1.0? 105 for Chi-C2E, to 1.7? 108
for Eng-C2E, to 7.8? 109 for Eng*-C2E.
Protocol productivity. When we compare the
annotator time required by the three protocols, we
find that foreign language natives work faster ? they
need about 2 hours per sentence ? while target lan-
guage natives need 2.5 hours per sentence. Given
that target language natives build significantly larger
networks and that bilingual speakers are in shorter
supply than monolingual ones, we conclude that us-
ing target language annotators is more cost-effective
overall.
Exploiting multiple annotations. Overall, the me-
dian number of paths produced by a single annota-
tor for A2E is 1.5 ? 106, two annotators (randomly
picked per sentence) produce a median number of
4.7 ? 107 (Union), for all annotators together it is
2.1? 1010 (Union) and 2.1? 1011 (SPU). For C2E,
these numbers are 5.2? 106 (one), 1.1? 108 (two),
and 2.6?1010 (all, Union) and 8.5?1011 (all, SPU).
Number of annotators and annotation time. We
compute the minimum number of edits and length-
normalized distance scores required to rewrite ma-
chine and human translations into translations found
in the networks produced by one, two, and three
annotators. We find that the length-normalized dis-
tances do not vary by more than 1% when adding the
meaning equivalents produced by a third annotator.
We conclude that 2-3 annotators per sentence pro-
duce a sufficiently large set of alternative meaning
equivalents, which takes 4-7.5 hours. We are cur-
rently investigating alternative ways to create net-
works more efficiently.
Grammaticality. For each of the four human trans-
lation references and each of the five machine trans-
lation outputs (see Section 2.2), we algorithmically
find the closest path in the annotated networks of
meaning equivalents (see Section 3). We presented
the resulting 1836 closest paths extracted from the
networks (2 language pairs ?102 sentences ?9 hu-
man/machine translations) to three independent En-
glish speakers. We asked each English path to be
labeled as grammatical, grammatical-but-slightly-
odd, or non-grammatical. The metric is harsh: paths
such as ?he said that withdrawing US force with-
out promoting security would be cataclysmic? are
judged as non-grammatical by all three judges al-
though a simple rewrite of ?force? into ?forces?
would make this path grammatical. We found that
90% of the paths are judged as grammatical and
96% as grammatical or grammatical-but-slightly-
odd, by at least one annotator. We interpret these
results as positive: the annotation process leads to
some ungrammatical paths being created, but most
of the closest paths to human and machine outputs,
those that matter from an evaluation perspective, are
judged as correct by at least one judge.
Coverage. We found it somewhat disappoint-
ing that networks that encode billions of meaning-
equivalent translations for a given sentence do not
contain every independently produced human ref-
erence translation. The average length-normalized
edit distance (computed as described in Section 3)
between an independently produced human refer-
ence and the corresponding network is 19% for
Arabic-English and 34% for Chinese-English across
the entire corpus. Our analysis shows that about
half of the edits are explained by several non-
content words (?the?, ?of?, ?for?, ?their?, ?,?) be-
ing optional in certain contexts; several ?obvious?
equivalents not being part of the networks (?that??
?this?; ?so???accordingly?); and spelling alterna-
tives/errors (?rockstrom???rockstroem?). We hy-
pothesize that most of these ommissions/edits can be
detected automatically and dealt with in an appropri-
ate fashion. The rest of the edits would require more
sophisticated machinery, to figure out, for example,
that in a particular context pairs like ?with???and?
or ?that???therefore? are interchangeable.
Given that Chinese is significantly more under-
specified compared to Arabic and English, it is con-
sistent with our intuition to see that the average mini-
mal distance is higher between Chinese-English ref-
erences and their respective networks (34%) than
between Arabic-English references and their respec-
tive networks (19%).
3 Measuring translation quality with large
networks of meaning equivalents
In this section, we present HyTER (Hybrid Trans-
lation Edit Rate), a novel metric that makes use of
large reference networks.
165


	


	


	


	





	


	


	
 

	



	
	




	

	

  	
		
 		

<ts>
 
	

	

	





 


 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538?542,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Automatic Parallel Fragment Extraction from Noisy Data
Jason Riesa and Daniel Marcu
Information Sciences Institute
Viterbi School of Engineering
University of Southern California
{riesa, marcu}@isi.edu
Abstract
We present a novel method to detect parallel
fragments within noisy parallel corpora. Isolat-
ing these parallel fragments from the noisy data
in which they are contained frees us from noisy
alignments and stray links that can severely
constrain translation-rule extraction. We do
this with existing machinery, making use of an
existing word alignment model for this task.
We evaluate the quality and utility of the ex-
tracted data on large-scale Chinese-English and
Arabic-English translation tasks and show sig-
nificant improvements over a state-of-the-art
baseline.
1 Introduction
A decade ago, Banko and Brill (2001) showed that
scaling to very large corpora is game-changing for a
variety of tasks. Methods that work well in a small-
data setting often lose their luster when moving to
large data. Conversely, other methods that seem to
perform poorly in that same small-data setting, may
perform markedly differently when trained on large
data.
Perhaps most importantly, Banko and Brill
showed that there was no significant variation in per-
formance among a variety of methods trained at-
scale with large training data. The takeaway? If you
desire to scale to large datasets, use a simple solution
for your task, and throw in as much data as possible.
The community at large has taken this message to
heart, and in most cases it has been an effective way
to increase performance.
Today, for machine translation, more data than
what we already have is getting harder and harder
to come by; we require large parallel corpora to
Figure 1: Example of a word alignment resulting from
noisy parallel data. The structure of the resulting align-
ment makes it difficult to find and extract parallel frag-
ments via the standard heuristics or simply by inspection.
How can we discover automatically those parallel frag-
ments hidden within such data?
train state-of-the-art statistical, data-driven models.
Groups that depend on clearinghouses like LDC for
their data increasingly find that there is less of a man-
date to gather parallel corpora on the scale of what
was produced in the last 5-10 years. Others, who di-
rectly exploit the entire web to gather such data will
necessarily run up against a wall after all that data
has been collected.
We need to learn how to do more with the data
we already have. Previous work has focused on
detecting parallel documents and sentences on the
web, e.g. (Zhao and Vogel, 2002; Fung and Che-
ung, 2004; Wu and Fung, 2005). Munteanu and
Marcu (2006), and later Quirk et al (2007), extend
the state-of-the-art for this task to parallel fragments.
In this paper, we present a novel method for de-
tecting parallel fragments in large, existing and po-
tentially noisy parallel corpora using existing ma-
538
chinery and show significant improvements to two
state-of-the-art MT systems. We also depart from
previous work in that we only consider parallel cor-
pora that have previously been cleaned, sanitized,
and thought to be non-noisy, e.g. parallel corpora
available from LDC.
2 Detecting Noisy Data
In order to extract previously unextractable good
parallel data, we must first detect the bad data. In
doing so, we will make use of existing machinery in
a novel way. We directly use the alignment model to
detect weak or undesirable data for translation.
2.1 Alignment Model as Noisy Data Detector
The alignment model we use in our experiments is
that described in (Riesa et al, 2011), modified to
output full derivation trees and model scores along
with alignments. Our reasons for using this particu-
lar alignment method are twofold: it provides a natu-
ral way to hierarchically partition subsentential seg-
ments, and is also empirically quite accurate in mod-
eling word alignments, in general. This latter quality
is important, not solely for downstream translation
quality, but also for the basis of our claims with re-
spect to detecting noisy or unsuitable data:
The alignment model we employ is discrimina-
tively trained to know what good alignments be-
tween parallel data look like. When this model pre-
dicts an alignment with a low model score, given an
input sentence pair, we might say the model is ?con-
fused.? In this case, the alignment probably doesn?t
look like the examples it has been trained on.
1. It could be that the data is parallel, but the model
is very confused. (modeling problem)
2. It could be that the data is noisy, and the model
is very confused. (data problem)
The general accuracy of the alignment model we
employ makes the former case unlikely. Therefore,
a key assumption we make is to assume a low model
score accompanies noisy data, and use this data as
candidates from which to extract non-noisy parallel
segments.
2.2 A Brief Example
As an illustrative example, consider the follow-
ing sentence pair in our training corpus taken from
LDC2005T10. This is the sentence pair shown in
Figure 1:
fate brought us together on that wonderful summer day
and one year later , shou ? tao and i were married not only
in the united states but also in taiwan .
? ?? ? ?? , ? ? ? ???? ? ??? ? ? ???
; ? ?? ? ?? ? ? ? ? , ? ? ? ?? ? ? ?? .
In this sentence pair there are only two parallel
phrases, corresponding to the underlined and double-
underlined strings. There are a few scattered word
pairs which may have a natural correspondence,1 but
no other larger phrases.2
In this work we are concerned with finding large
phrases,3 since very small phrases tend to be ex-
tractible even when data is noisy. Bad alignments
tend to cause conflicts when extracting large phrases
due to unexpected, stray links in the alignment ma-
trix; smaller fragments will have less opportunity to
come into conflict with incorrect, stray links due to
noisy data or alignment model error. We consider
large enough phrases for our purposes to be phrases
of size greater than 3, and ignore smaller fragments.
2.3 Parallel Fragment Extraction
2.3.1 A Hierarchical Alignment Model and its
Derivation Trees
The alignment model we use, (Riesa et al,
2011), is a discriminatively trained model which at
alignment-time walks up the English parse-tree and,
at every node in the tree, generates alignments by re-
cursively scoring and combining alignments gener-
ated at the current node?s children, building up larger
and larger alignments. This process works similarly
to a CKY parser, moving bottom-up and generating
larger and larger constituents until it has predicted
the full tree spanning the entire sentence. How-
1For example, (I, ?) and (Taiwan, ??)
2The rest of the Chinese describes where the couple is from;
the speaker, she says, is an American raised in New Jersey.
3We count the size of the phrase according to the number of
English words it contains; one could be more conservative by
constraining both sides.
539
??
?
?
??
a
IN
f
a
n
t
a
s
t
i
c
y
e
t
r
e
a
l
i
s
t
i
c
JJ
CC
JJ
ADJP
NP
NN
a
d
v
e
n
t
u
r
e
??
[14.2034] PP [9.5130]
NP [-0.5130]
with multi-sensory experiences
Figure 2: From LDC2004T08, when the NP fragment
shown here is combined to make a larger span with a sis-
ter PP fragment, the alignment model objects due to non-
parallel data under the PP, voicing a score of -0.5130. We
extract and append to our training corpus the NP fragment
depicted, from which we later learn 5 additional transla-
tion rules.
ever, instead of generating syntactic structures, we
are generating alignments.
In moving bottom-up along the tree, just as there
is a derivation tree for a CKY parse, we can also fol-
low backpointers to extract the derivation tree of the
1-best alignment starting from the root node. This
derivation tree gives a hierarchical partitioning of the
alignment and the associated word-spans. We can
also inspect model scores at each node in the deriva-
tion tree.
2.3.2 Using the Alignment Model to Detect
Parallel Fragments
For each training example in our parallel cor-
pus, we have an alignment derivation tree. Be-
cause the derivation tree is essentially isomorphic
to the English parse tree, the derivation tree repre-
sents a hierarchical partitioning of the training ex-
ample into syntactic segments. We traverse the tree
top-down, inspecting the parallel fragments implied
by the derivation at each point, and their associated
model scores.
The idea behind this top-down traversal is that al-
though some nodes, and perhaps entire derivations,
may be low-scoring, there are often high-scoring
fragments that make up the larger derivation which
are worthy of extraction. Figure 2 shows an ex-
ample. We recursively traverse the derivation, top-
down, extracting the largest fragment possible at
any derivation node whose alignment model score is
higher than some threshold ?, and whose associated
English and foreign spans meet a set of important
constraints:
1. The parent node in the derivation has a score less
than ?.
2. The length of the English span is > 3.
3. There are no unaligned foreign words inside the
fragment that are also aligned to English words
outside the fragment.
Once a fragment has been extracted, we do not re-
curse any further down the subtree.
Constraint 1 is a candidate constraint, and forces
us to focus on segments of parallel sentences with
low model scores; these are segments likely to con-
sist of bad alignments due to noisy data or aligner
error.
Constraint 2 is a conservativity constraint ? we
are more confident in model scores over larger frag-
ments with more context than smaller ones with min-
imal context. This constraint also parameterizes the
notion that larger fragments are the type more often
precluded from extraction due to stray or incorrect
word-alignment links; additionally, we are already
likely to be able to extract smaller fragments using
standard methods, and as such, they are less useful
to us here.
Constraint 3 is a content constraint, limiting us
from extracting fragments with blocks of unaligned
foreign words that don?t belong in this particular
fragment because they are aligned elsewhere. If we
threw out this constraint, then in translating from
Chinese to English, we would erroneously learn to
delete blocks of Chinese words that otherwise should
be translated. When foreign words are unaligned ev-
erywhere within a parallel sentence, then they can
be included within the extracted fragment. Common
examples in Chinese are function words such as ?,
?, and ?. Put another way, we only allow globally
unaligned words in extracted fragments.
Computing ?. In computing our extraction thresh-
old ?, we must decide what proportion of fragments
we consider to be low-scoring and least likely to be
useful for translation. We make the rather strong as-
540
sumption that this is the bottom 10% of the data.4
3 Evaluation
We evaluate our parallel fragment extraction in a
large-scale Chinese-English and Arabic-English MT
setting. In our experiments we use a tree-to-string
syntax-based MT system (Galley et al, 2004), and
evaluate on a standard test set, NIST08. We parse the
English side of our parallel corpus with the Berkeley
parser (Petrov et al, 2006), and tune parameters of
theMT systemwithMIRA (Chiang et al, 2008). We
decode with an integrated language model trained on
about 4 billion words of English.
Chinese-English We align a parallel corpus of
8.4M parallel segments, with 210M words of En-
glish and 193M words of Chinese. From this we
extract 868,870 parallel fragments according to the
process described in Section 2, and append these
fragments to the end of the parallel corpus. In doing
so, we have created a larger parallel corpus of 9.2M
parallel segments, consisting of 217M and 198M
words of English and Chinese, respectively.
Arabic-English We align a parallel corpus of
9.0M parallel segments, with 223M words of En-
glish and 194M words of Arabic. From this we ex-
tract 996,538 parallel fragments, and append these
fragments to the end of the parallel corpus. The re-
sulting corpus has 10M parallel segments, consisting
of 233M and 202Mwords of English and Arabic, re-
spectively.
Results are shown in Table 1. Using our parallel
fragment extraction, we learn 68M additional unique
Arabic-English rules that are not in the baseline sys-
tem; likewise, we learn 38M new unique Chinese-
English rules not in the baseline system for that lan-
guage pair. Note that we are not simply duplicat-
ing portions of the parallel data. While each se-
quence fragment of source and target words we ex-
tract will be found elsewhere in the larger parallel
corpus, these fragments will largely not make it into
fruitful translation rules to be used in the downstream
MT system.
We see gains in BLEU score across two differ-
ent language pairs, showing empirically that we are
4One may wish to experiment with different ranges here, but
each requires a separate time-consuming downstream MT ex-
periment. In this work, it turns out that scrutinizing 10% of the
data is productive and empirically reasonable.
Corpus Extracted Rules BLEU
Baseline (Ara-Eng) 750M 50.0
+Extracted fragments 818M 50.4
Baseline (Chi-Eng) 270M 31.5
+Extracted fragments 308M 32.0
Table 1: End-to-end translation experiments with and
without extracted fragments. We are learning many more
unique rules; BLEU score gains are significant with p <
0.05 for Arabic-English and p < 0.01 for Chinese-
English.
learning new and useful translation rules we previ-
ously were not in our grammars. These results are
significant with p < 0.05 for Arabic-English and
p < 0.01 for Chinese-English.
4 Discussion
All alignment models we have experimented with
will fall down in the presence of noisy data. Impor-
tantly, even if the alignment model were able to yield
?perfect? alignments with no alignment links among
noisy sections of the parallel data precluding us from
extracting reasonable rules or phrase pairs, wewould
still have to deal with downstream rule extraction
heuristics and their tendency to blow up a translation
grammar in the presence of large swaths of unaligned
words. Absent a mechanism within the alignment
model itself to deal with this problem, we provide a
simple way to recover from noisy data without the
introduction of new tools.
Summing up, parallel data in the world is not
unlimited. We cannot always continue to double
our data for increased performance. Parallel data
creation is expensive, and automatic discovery is
resource-intensive (Uszkoreit et al, 2010). We have
presented a technique that helps to squeeze more out
of an already large, state-of-the-art MT system, us-
ing existing pieces of the pipeline to do so in a novel
way.
Acknowledgements
This work was supported byDARPABOLT via BBN sub-
contract HR0011-12-C-0014. We thank our three anony-
mous reviewers for thoughtful comments. Thanks also to
Kevin Knight, David Chiang, Liang Huang, and Philipp
Koehn for helpful discussions.
541
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proc. of the ACL, pages 26?33.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP, pages
224?233.
Pascale Fung and Percy Cheung. 2004. Mining very non-
parallel corpora: Parallel sentence and lexicon extrac-
tion via boostrapping and EM. In Proc. of EMNLP,
pages 57?63.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc. of
HLT-NAACL, pages 273?280.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proc. of COLING/ACL, Sydney,
Australia.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative models of noisy translations with ap-
plications to parallel fragment extraction. In Proceed-
ings of MT Summit XI.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In Proc.
of EMNLP, pages 497?507.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, andMoshe Du-
biner. 2010. Large scale parallel document mining
for machine translation. In Proc. of COLING, pages
1101?1109.
Dekai Wu and Pascale Fung. 2005. Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora. In Proc. of IJCNLP,
pages 257?268.
Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web bilingual news collec-
tion. In IEEE International Conference on Data Min-
ing, pages 745?748, Maebashi City, Japan.
542
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157?166,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical Search for Word Alignment
Jason Riesa and Daniel Marcu
Information Sciences Institute
Viterbi School of Engineering
University of Southern California
{riesa, marcu}@isi.edu
Abstract
We present a simple yet powerful hier-
archical search algorithm for automatic
word alignment. Our algorithm induces
a forest of alignments from which we
can efficiently extract a ranked k-best list.
We score a given alignment within the
forest with a flexible, linear discrimina-
tive model incorporating hundreds of fea-
tures, and trained on a relatively small
amount of annotated data. We report re-
sults on Arabic-English word alignment
and translation tasks. Our model out-
performs a GIZA++ Model-4 baseline by
6.3 points in F-measure, yielding a 1.1
BLEU score increase over a state-of-the-art
syntax-based machine translation system.
1 Introduction
Automatic word alignment is generally accepted
as a first step in training any statistical machine
translation system. It is a vital prerequisite for
generating translation tables, phrase tables, or syn-
tactic transformation rules. Generative alignment
models like IBM Model-4 (Brown et al, 1993)
have been in wide use for over 15 years, and while
not perfect (see Figure 1), they are completely un-
supervised, requiring no annotated training data to
learn alignments that have powered many current
state-of-the-art translation system.
Today, there exist human-annotated alignments
and an abundance of other information for many
language pairs potentially useful for inducing ac-
curate alignments. How can we take advantage
of all of this data at our fingertips? Using fea-
ture functions that encode extra information is one
good way. Unfortunately, as Moore (2005) points
out, it is usually difficult to extend a given genera-
tive model with feature functions without chang-
ing the entire generative story. This difficulty
Visualization generated by riesa: Feb 12, 2010 20:06:24
683.g (a1)
683.union.a (a2)
683.e (e)
683.f (f)
Sentence 1
t
h
e
f
i
v
e
p
r
e
v
i
o
u
s
t
e
s
t
s
h
a
v
e
b
e
e
n
l
i
m
i
t
e
d
t
o
t
h
e
t
a
r
g
e
t
m
i
s
s
i
l
e
a
n
d
o
n
e
o
t
h
e
r
b
o
d
y
.
!
"#$%
!
&
!
' ()
"
*
+,-
*
!
&. (
/0
1
2
3
4(
!
5
!
67
*
,8.(
9:
;
<)+,=.(
1
>?@
!
A8B
C
(
DEFG
*
)
#
1
G(
?H()
*
1
Figure 1: Model-4 alignment vs. a gold stan-
dard. Circles represent links in a human-annotated
alignment, and black boxes represent links in the
Model-4 alignment. Bold gray boxes show links
gained after fully connecting the alignment.
has motivated much recent work in discriminative
modeling for word alignment (Moore, 2005; Itty-
cheriah and Roukos, 2005; Liu et al, 2005; Taskar
et al, 2005; Blunsom and Cohn, 2006; Lacoste-
Julien et al, 2006; Moore et al, 2006).
We present in this paper a discriminative align-
ment model trained on relatively little data, with
a simple, yet powerful hierarchical search proce-
dure. We borrow ideas from both k-best pars-
ing (Klein and Manning, 2001; Huang and Chi-
ang, 2005; Huang, 2008) and forest-based, and
hierarchical phrase-based translation (Huang and
Chiang, 2007; Chiang, 2007), and apply them to
word alignment.
Using a foreign string and an English parse
tree as input, we formulate a bottom-up search
on the parse tree, with the structure of the tree
as a backbone for building a hypergraph of pos-
sible alignments. Our algorithm yields a forest of
157
the man
ate the
NP
VP
S
NP
t
h
e
???
?????
t
h
e
???
?????
t
h
e
???
??????
m
a
n
???
?????
t
h
e
m
a
n
a
t
e
t
h
e
b
r
e
a
d
?????
????? ?????
?????
bread
b
r
e
a
d
???
?????
?????
Figure 2: Example of approximate search through a hypergraph with beam size = 5. Each black square
implies a partial alignment. Each partial alignment at each node is ranked according to its model score.
In this figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NP
node is constructed by composing the best hypothesis at the terminal node labeled ?the? and the 2nd-
best hypothesis at the terminal node labeled ?man?. (We ignore terminal nodes in this toy example.)
Hypotheses at the root node imply full alignment structures.
word alignments, from which we can efficiently
extract the k-best. We handle an arbitrary number
of features, compute them efficiently, and score
alignments using a linear model. We train the
parameters of the model using averaged percep-
tron (Collins, 2002) modified for structured out-
puts, but can easily fit into a max-margin or related
framework. Finally, we use relatively little train-
ing data to achieve accurate word alignments. Our
model can generate arbitrary alignments and learn
from arbitrary gold alignments.
2 Word Alignment as a Hypergraph
Algorithm input The input to our alignment al-
gorithm is a sentence-pair (en1, f
m
1 ) and a parse tree
over one of the input sentences. In this work,
we parse our English data, and for each sentence
E = en1, let T be its syntactic parse. To gener-
ate parse trees, we use the Berkeley parser (Petrov
et al, 2006), and use Collins head rules (Collins,
2003) to head-out binarize each tree.
Overview We present a brief overview here and
delve deeper in Section 2.1. Word alignments are
built bottom-up on the parse tree. Each node v in
the tree holds partial alignments sorted by score.
158
u11
u
12
u
13
u
21
2.2 4.1 5.5
u
22
2.4 3.5 7.2
u
23
3.2 4.5 11.4
u
11
u
12
u
13
u
21
2.2 4.1 5.5
u
22
2.4 3.5 7.2
u
23
3.2 4.5 11.4
u
11
u
12
u
13
u
21
2.2 4.1 5.5
u
22
2.4 3.5 7.2
u
23
3.2 4.5 11.4
(a) Score the left corner align-
ment first. Assume it is the 1-
best. Numbers in the rest of the
boxes are hidden at this point.
u
1
u
12
u
13
u
21
2.2 4.1 5.5
u
2
2.4 3.5 7.2
u
23
3.2 4.5 1 .4
u
1
u
12
u
13
u
21
2.2 4.1 5.5
u
2
2.4 3.5 7.2
u
23
3.2 4.5 1 .4
u
1
u
12
u
13
u
21
2.2 4.1 5.5
u
2
2.4 3.5 7.2
u
23
3.2 4.5 1 .4
(b) Expand the frontier of align-
ments. We are now looking for
the 2nd best.
u
1
u
12
u
13
u
21
2. 4.1 5.
u
2
2.4 3.5 7.2
u
23
3.2 4.5 1 .4
u
1
u
12
u
13
u
21
2. 4.1 5.
u
2
2.4 3.5 7.2
u
23
3.2 4.5 1 .4
u
1
u
12
u
13
u
21
2. 4.1 5.
u
2
2.4 3.5 7.2
u
23
3.2 4.5 1 .4
(c) Expand the frontier further.
After this step we have our top
k alignments.
Figure 3: Cube pruning with alignment hypotheses to select the top-k alignments at node v with children
?u1, u2?. In this example, k = 3. Each box represents the combination of two partial alignments to create
a larger one. The score in each box is the sum of the scores of the child alignments plus a combination
cost.
Each partial alignment comprises the columns of
the alignment matrix for the e-words spanned by
v, and each is scored by a linear combination of
feature functions. See Figure 2 for a small exam-
ple.
Initial partial alignments are enumerated and
scored at preterminal nodes, each spanning a sin-
gle column of the word alignment matrix. To
speed up search, we can prune at each node, keep-
ing a beam of size k. In the diagram depicted in
Figure 2, the beam is size k = 5.
From here, we traverse the tree nodes bottom-
up, combining partial alignments from child nodes
until we have constructed a single full alignment at
the root node of the tree. If we are interested in the
k-best, we continue to populate the root node until
we have k alignments.1
We use one set of feature functions for preter-
minal nodes, and another set for nonterminal
nodes. This is analogous to local and nonlo-
cal feature functions for parse-reranking used by
Huang (2008). Using nonlocal features at a non-
terminal node emits a combination cost for com-
posing a set of child partial alignments.
Because combination costs come into play, we
use cube pruning (Chiang, 2007) to approxi-
mate the k-best combinations at some nonterminal
node v. Inference is exact when only local features
are used.
Assumptions There are certain assumptions re-
lated to our search algorithm that we must make:
1We use approximate dynamic programming to store
alignments, keeping only scored lists of pointers to initial
single-column spans. Each item in the list is a derivation that
implies a partial alignment.
(1) that using the structure of 1-best English syn-
tactic parse trees is a reasonable way to frame and
drive our search, and (2) that F-measure approxi-
mately decomposes over hyperedges.
We perform an oracle experiment to validate
these assumptions. We find the oracle for a given
(T ,e, f ) triple by proceeding through our search al-
gorithm, forcing ourselves to always select correct
links with respect to the gold alignment when pos-
sible, breaking ties arbitrarily. The the F1 score of
our oracle alignment is 98.8%, given this ?perfect?
model.
2.1 Hierarchical search
Initial alignments We can construct a word
alignment hierarchically, bottom-up, by making
use of the structure inherent in syntactic parse
trees. We can think of building a word alignment
as filling in an M?N matrix (Figure 1), and we be-
gin by visiting each preterminal node in the tree.
Each of these nodes spans a single e word. (Line
2 in Algorithm 1).
From here we can assign links from each e word
to zero or more f words (Lines 6?14). At this
level of the tree the span size is 1, and the par-
tial alignment we have made spans a single col-
umn of the matrix. We can make many such partial
alignments depending on the links selected. Lines
5 through 9 of Algorithm 1 enumerate either the
null alignment, single-link alignments, or two-link
alignments. Each partial alignment is scored and
stored in a sorted heap (Lines 9 and 13).
In practice enumerating all two-link alignments
can be prohibitive for long sentence pairs; we set
a practical limit and score only pairwise combina-
159
Algorithm 1: Hypergraph Alignment
Input:
Source sentence en1
Target sentence fm1
Parse tree T over en1
Set of feature functions h
Weight vector w
Beam size k
Output:
A k-best list of alignments over en1 and f
m
1
1 function A????(en1, f
m
1 ,T )
2 for v ? T in bottom-up order do
3 ?v ? ?
4 if ??-P??????????N???(v) then
5 i? index-of(v)
6 for j = 0 to m do
7 links? (i, j)
8 score? w ? h(links, v, en1, f
m
1 )
9 P???(?v, ?score, links?, k )
10 for k = j + 1 to m do
11 links? (i, j), (i, k)
12 score? w ? h(links, v, en1, f
m
1 )
13 P???(?v, ?score, links?, k )
14 end
15 end
16 else
17 ?v ? G???S???(children(v), k)
18 end
19 end
20 end
21 function G???S???(?u1, u2?, k)
22 return C???P??????(??u1 , ?u2?, k,w,h)
23 end
tions of the top n = max
{
| f |
2 , 10
}
scoring single-
link alignments.
We limit the number of total partial alignments
?v kept at each node to k. If at any time we wish to
push onto the heap a new partial alignment when
the heap is full, we pop the current worst off the
heap and replace it with our new partial alignment
if its score is better than the current worst.
Building the hypergraph We now visit internal
nodes (Line 16) in the tree in bottom-up order. At
each nonterminal node v we wish to combine the
partial alignments of its children u1, . . . , uc. We
use cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) to select the k-best combinations of the
partial alignments of u1, . . . , uc (Line 19). Note
Sentence 1
TOP
1
S
2
NP-C
1
NPB
2
DT
NPB-BAR
2
CD
NPB-BAR
2
JJ NNS
S-BAR
1
VP
1
VBP
VP-C
1
VBN
VP-C
1
VBN
PP
1
IN
NP-C
1
NP-C-BAR
1
NP
1
NPB
2
DT
NPB-BAR
2
NN NN CC
NP
1
NPB
2
CD
NPB-BAR
2
JJ NN
.
t
h
e
f
i
v
e
p
r
e
v
i
o
u
s
t
e
s
t
s
h
a
v
e
b
e
e
n
l
i
m
i
t
e
d
t
o
t
h
e
t
a
r
g
e
t
m
i
s
s
i
l
e
a
n
d
o
n
e
o
t
h
e
r
b
o
d
y
.
!
"#$%
!
&
!
' ()
"
*
+,-
*
!
&. (
/0
1
2
3
4(
!
5
!
67
*
,8.(
9:
;
<)+,=.(
1
>?@
!
A8B
C
(
DEFG
*
)
#
1
G(
?H()
*
Figure 4: Correct version of Figure 1 after hyper-
graph alignment. Subscripts on the nonterminal
labels denote the branch containing the head word
for that span.
that Algorithm 1 assumes a binary tree2, but is not
necessary. In the general case, cube pruning will
operate on a d-dimensional hypercube, where d is
the branching factor of node v.
We cannot enumerate and score every possibil-
ity; without the cube pruning approximation, we
will have kc possible combinations at each node,
exploding the search space exponentially. Figure 3
depicts how we select the top-k alignments at a
node v from its children ? u1, u2 ?.
3 Discriminative training
We incorporate all our new features into a linear
model and learn weights for each using the on-
line averaged perceptron algorithm (Collins, 2002)
with a few modifications for structured outputs in-
spired by Chiang et al (2008). We define:
2We find empirically that using binarized trees reduces
search errors in cube pruning.
160
in
i
n
?
?
. . .
.
.
.
.
.
.
Figure 5: A common problem with GIZA++
Model 4 alignments is a weak distortion model.
The second English ?in? is aligned to the wrong
Arabic token. Circles show the gold alignment.
?(y) = `(yi, y) + w ? (h(yi) ? h(y)) (1)
where `(yi,y) is a loss function describing how bad
it is to guess y when the correct answer is yi. In our
case, we define `(yi,y) as 1?F1(yi,y). We select the
oracle alignment according to:
y+ = arg min
y?????(x)
?(y) (2)
where ????(x) is a set of hypothesis alignments
generated from input x. Instead of the traditional
oracle, which is calculated solely with respect to
the loss `(yi,y), we choose the oracle that jointly
minimizes the loss and the difference in model
score to the true alignment. Note that Equation 2
is equivalent to maximizing the sum of the F-
measure and model score of y:
y+ = arg max
y?????(x)
(F1(yi, y) + w ? h(y)) (3)
Let y? be the 1-best alignment according to our
model:
y? = arg max
y?????(x)
w ? h(y) (4)
Then, at each iteration our weight update is:
w? w + ?(h(y+) ? h(y?)) (5)
where ? is a learning rate parameter.3 We find
that this more conservative update gives rise to a
much more stable search. After each iteration, we
expect y+ to get closer and closer to the true yi.
4 Features
Our simple, flexible linear model makes it easy to
throw in many features, mapping a given complex
3We set ? to 0.05 in our experiments.
alignment structure into a single high-dimensional
feature vector. Our hierarchical search framework
allows us to compute these features when needed,
and affords us extra useful syntactic information.
We use two classes of features: local and non-
local. Huang (2008) defines a feature h to be lo-
cal if and only if it can be factored among the lo-
cal productions in a tree, and non-local otherwise.
Analogously for alignments, our class of local fea-
tures are those that can be factored among the local
partial alignments competing to comprise a larger
span of the matrix, and non-local otherwise. These
features score a set of links and the words con-
nected by them.
Feature development Our features are inspired
by analysis of patterns contained among our gold
alignment data and automatically generated parse
trees. We use both local lexical and nonlocal struc-
tural features as described below.
4.1 Local features
These features fire on single-column spans.
? From the output of GIZA++ Model 4, we
compute lexical probabilities p(e | f ) and
p( f | e), as well as a fertility table ?(e).
From the fertility table, we fire features ?0(e),
?1(e), and ?2+(e) when a word e is aligned
to zero, one, or two or more words, respec-
tively. Lexical probability features p(e | f )
and p( f | e) fire when a word e is aligned to
a word f .
? Based on these features, we include a binary
lexical-zero feature that fires if both p(e | f )
and p( f | e) are equal to zero for a given word
pair (e, f ). Negative weights essentially pe-
nalize alignments with links never seen be-
fore in the Model 4 alignment, and positive
weights encourage such links. We employ a
separate instance of this feature for each En-
glish part-of-speech tag: p( f | e, t).
We learn a different feature weight for each.
Critically, this feature tells us how much to
trust alignments involving nouns, verbs, ad-
jectives, function words, punctuation, etc.
from the Model 4 alignments from which our
p(e | f ) and p( f | e) tables are built. Ta-
ble 1 shows a sample of learned weights. In-
tuitively, alignments involving English parts-
of-speech more likely to be content words
(e.g. NNPS, NNS, NN) are more trustworthy
161
PP
IN
NP
e
prep
e
head
...
f
NP
DT
NP
e
det
e
head
...
f
VP
VBD
VP
e
verb
e
head
...
f
Figure 6: Features PP-NP-head, NP-DT-head, and VP-VP-head fire on these tree-alignment patterns. For
example, PP-NP-head fires exactly when the head of the PP is aligned to exactly the same f words as the
head of it?s sister NP.
Penalty
NNPS ?1.11
NNS ?1.03
NN ?0.80
NNP ?0.62
VB ?0.54
VBG ?0.52
JJ ?0.50
JJS ?0.46
VBN ?0.45
... ...
POS ?0.0093
EX ?0.0056
RP ?0.0037
WP$ ?0.0011
TO 0.037
Reward
Table 1: A sampling of learned weights for the lex-
ical zero feature. Negative weights penalize links
never seen before in a baseline alignment used to
initialize lexical p(e | f ) and p( f | e) tables. Posi-
tive weights outright reward such links.
than those likely to be function words (e.g.
TO, RP, EX), where the use of such words is
often radically different across languages.
? We also include a measure of distortion.
This feature returns the distance to the diag-
onal of the matrix for any link in a partial
alignment. If there is more than one link, we
return the distance of the link farthest from
the diagonal.
? As a lexical backoff, we include a tag prob-
ability feature, p(t | f ) that fires for some
link (e, f ) if the part-of-speech tag of e is t.
The conditional probabilities in this table are
computed from our parse trees and the base-
line Model 4 alignments.
? In cases where the lexical probabilities are
too strong for the distortion feature to
overcome (see Figure 5), we develop the
multiple-distortion feature. Although local
features do not know the partial alignments at
other spans, they do have access to the entire
English sentence at every step because our in-
put is constant. If some e exists more than
once in en1 we fire this feature on all links con-
taining word e, returning again the distance to
the diagonal for that link. We learn a strong
negative weight for this feature.
? We find that binary identity and
punctuation-mismatch features are im-
portant. The binary identity feature fires if
e = f , and proves useful for untranslated
numbers, symbols, names, and punctuation
in the data. Punctuation-mismatch fires on
any link that causes nonpunctuation to be
aligned to punctuation.
Additionally, we include fine-grained versions of
the lexical probability, fertility, and distortion fea-
tures. These fire for for each link (e, f ) and part-
of-speech tag. That is, we learn a separate weight
for each feature for each part-of-speech tag in our
data. Given the tag of e, this affords the model the
ability to pay more or less attention to the features
described above depending on the tag given to e.
Arabic-English specific features We describe
here language specific features we implement to
exploit shallow Arabic morphology.
162
PP
IN
NP
from
...
?
.
.
.
Figure 7: This figure depicts the tree/alignment
structure for which the feature PP-from-prep
fires. The English preposition ?from? is aligned
to Arabic word 	??. Any aligned words in the span
of the sister NP are aligned to words following 	??.
English preposition structure commonly matches
that of Arabic in our gold data. This family of fea-
tures captures these observations.
? We observe the Arabic prefix ?, transliterated
w- and generally meaning and, to prepend to
most any word in the lexicon, so we define
features p?w(e | f ) and p?w( f | e). If f be-
gins with w-, we strip off the prefix and return
the values of p(e | f ) and p( f | e). Otherwise,
these features return 0.
? We also include analogous feature functions
for several functional and pronominal pre-
fixes and suffixes.4
4.2 Nonlocal features
These features comprise the combination cost
component of a partial alignment score and may
fire when concatenating two partial alignments
to create a larger span. Because these features
can look into any two arbitrary subtrees, they
are considered nonlocal features as defined by
Huang (2008).
? Features PP-NP-head, NP-DT-head, and
VP-VP-head (Figure 6) all exploit head-
words on the parse tree. We observe English
prepositions and determiners to often align to
the headword of their sister. Likewise, we ob-
serve the head of a VP to align to the head of
an immediate sister VP.
4Affixes used by our model are currently: K
.
, ?, ? @, ?AK
.
,
?


, ??, A??, ??, A??. Others either we did not experiment
with, or seemed to provide no significant benefit, and are not
included.
In Figure 4, when the search arrives at the
left-most NPB node, the NP-DT-head fea-
ture will fire given this structure and links
over the span [the ... tests]. When
search arrives at the second NPB node, it
will fire given the structure and links over the
span [the ... missle], but will not fire at
the right-most NPB node.
? Local lexical preference features compete
with the headword features described above.
However, we also introduce nonlocal lexical-
ized features for the most common types of
English and foreign prepositions to also com-
pete with these general headword features.
PP features PP-of-prep, PP-from-prep, PP-
to-prep, PP-on-prep, and PP-in-prep fire at
any PP whose left child is a preposition and
right child is an NP. The head of the PP is one
of the enumerated English prepositions and is
aligned to any of the three most common for-
eign words to which it has also been observed
aligned in the gold alignments. The last con-
straint on this pattern is that all words un-
der the span of the sister NP, if aligned, must
align to words following the foreign preposi-
tion. Figure 7 illustrates this pattern.
? Finally, we have a tree-distance feature to
avoid making too many many-to-one (from
many English words to a single foreign word)
links. This is a simplified version of and sim-
ilar in spirit to the tree distance metric used
in (DeNero and Klein, 2007). For any pair of
links (ei, f ) and (e j, f ) in which the e words
differ but the f word is the same token in
each, return the tree height of first common
ancestor of ei and e j.
This feature captures the intuition that it is
much worse to align two English words at
different ends of the tree to the same foreign
word, than it is to align two English words
under the same NP to the same foreign word.
To see why a string distance feature that
counts only the flat horizontal distance from
ei to e j is not the best strategy, consider the
following. We wish to align a determiner
to the same f word as its sister head noun
under the same NP. Now suppose there are
several intermediate adjectives separating the
determiner and noun. A string distance met-
163
ric, with no knowledge of the relationship be-
tween determiner and noun will levy a much
heavier penalty than its tree distance analog.
5 Related Work
Recent work has shown the potential for syntac-
tic information encoded in various ways to sup-
port inference of superior word alignments. Very
recent work in word alignment has also started to
report downstream effects on BLEU score.
Cherry and Lin (2006) introduce soft syntac-
tic ITG (Wu, 1997) constraints into a discrimi-
native model, and use an ITG parser to constrain
the search for a Viterbi alignment. Haghighi et
al. (2009) confirm and extend these results, show-
ing BLEU improvement for a hierarchical phrase-
based MT system on a small Chinese corpus.
As opposed to ITG, we use a linguistically mo-
tivated phrase-structure tree to drive our search
and inform our model. And, unlike ITG-style ap-
proaches, our model can generate arbitrary align-
ments and learn from arbitrary gold alignments.
DeNero and Klein (2007) refine the distor-
tion model of an HMM aligner to reflect tree
distance instead of string distance. Fossum et
al. (2008) start with the output from GIZA++
Model-4 union, and focus on increasing precision
by deleting links based on a linear discriminative
model exposed to syntactic and lexical informa-
tion.
Fraser and Marcu (2007) take a semi-supervised
approach to word alignment, using a small amount
of gold data to further tune parameters of a
headword-aware generative model. They show
a significant improvement over a Model-4 union
baseline on a very large corpus.
6 Experiments
We evaluate our model and and resulting align-
ments on Arabic-English data against those in-
duced by IBM Model-4 using GIZA++ (Och and
Ney, 2003) with both the union and grow-diag-
final heuristics. We use 1,000 sentence pairs and
gold alignments from LDC2006E86 to train model
parameters: 800 sentences for training, 100 for
testing, and 100 as a second held-out development
set to decide when to stop perceptron training. We
also align the test data using GIZA++5 along with
50 million words of English.
5We use a standard training procedure: 5 iterations of
Model-1, 5 iterations of HMM, 3 iterations of Model-3, and 3
iterations of Model-4.
0 5 10 15 20 25 30 35 400.73
0.735
0.74
0.745
0.75
0.755
0.76
0.765
0.77
0.775
Training epoch
Tra
inin
g F
?m
eas
ure
Figure 8: Learning curves for 10 random restarts
over time for parallel averaged perceptron train-
ing. These plots show the current F-measure on
the training set as time passes. Perceptron training
here is quite stable, converging to the same general
neighborhood each time.
0.67
0.68
0.69
0.70
0.71
0.72
0.73
0.74
0.75
0.76
Model 1 HMM Model 4
F
-
m
e
a
s
u
r
e
Initial alignments
Figure 9: Model robustness to the initial align-
ments from which the p(e | f ) and p( f | e) features
are derived. The dotted line indicates the baseline
accuracy of GIZA++ Model 4 alone.
6.1 Alignment Quality
We empirically choose our beam size k from the
results of a series of experiments, setting k=1, 2,
4, 8, 16, 32, and 64. We find setting k = 16 to yield
the highest accuracy on our held-out test data. Us-
ing wider beams results in higher F-measure on
training data, but those gains do not translate into
higher accuracy on held-out data.
The first three columns of Table 2 show the
balanced F-measure, Precision, and Recall of our
alignments versus the two GIZA++ Model-4 base-
lines. We report an F-measure 8.6 points over
Model-4 union, and 6.3 points over Model-4 grow-
diag-final.
164
F P R Arabic/English # Unknown
BLEU Words
M4 (union) .665 .636 .696 45.1 2,538
M4 (grow-diag-final) .688 .702 .674 46.4 2,262
Hypergraph alignment .751 .780 .724 47.5 1,610
Table 2: F-measure, Precision, Recall, the resulting BLEU score, and number of unknown words on a
held-out test corpus for three types of alignments. BLEU scores are case-insensitive IBM BLEU. We
show a 1.1 BLEU increase over the strongest baseline, Model-4 grow-diag-final. This is statistically
significant at the p < 0.01 level.
Figure 8 shows the stability of the search proce-
dure over ten random restarts of parallel averaged
perceptron training with 40 CPUs. Training ex-
amples are randomized at each epoch, leading to
slight variations in learning curves over time but
all converge into the same general neighborhood.
Figure 9 shows the robustness of the model to
initial alignments used to derive lexical features
p(e | f ) and p( f | e). In addition to IBM Model 4,
we experiment with alignments from Model 1 and
the HMM model. In each case, we significantly
outperform the baseline GIZA++ Model 4 align-
ments on a heldout test set.
6.2 MT Experiments
We align a corpus of 50 million words with
GIZA++ Model-4, and extract translation rules
from a 5.4 million word core subset. We align
the same core subset with our trained hypergraph
alignment model, and extract a second set of trans-
lation rules. For each set of translation rules, we
train a machine translation system and decode a
held-out test corpus for which we report results be-
low.
We use a syntax-based translation system for
these experiments. This system transforms Arabic
strings into target English syntax trees Translation
rules are extracted from (e-tree, f -string, align-
ment) triples as in (Galley et al, 2004; Galley et
al., 2006).
We use a randomized language model (similar
to that of Talbot and Brants (2008)) of 472 mil-
lion English words. We tune the the parameters
of the MT system on a held-out development cor-
pus of 1,172 parallel sentences, and test on a held-
out parallel corpus of 746 parallel sentences. Both
corpora are drawn from the NIST 2004 and 2006
evaluation data, with no overlap at the document
or segment level with our training data.
Columns 4 and 5 in Table 2 show the results
of our MT experiments. Our hypergraph align-
ment algorithm allows us a 1.1 BLEU increase over
the best baseline system, Model-4 grow-diag-final.
This is statistically significant at the p < 0.01
level. We also report a 2.4 BLEU increase over
a system trained with alignments from Model-4
union.
7 Conclusion
We have opened up the word alignment task to
advances in hypergraph algorithms currently used
in parsing and machine translation decoding. We
treat word alignment as a parsing problem, and
by taking advantage of English syntax and the hy-
pergraph structure of our search algorithm, we re-
port significant increases in both F-measure and
BLEU score over standard baselines in use by most
state-of-the-art MT systems today.
Acknowledgements
We would like to thank our colleagues in the Nat-
ural Language Group at ISI for many meaningful
discussions and the anonymous reviewers for their
thoughtful suggestions. This research was sup-
ported by DARPA contract HR0011-06-C-0022
under subcontract to BBN Technologies, and a
USC CREATE Fellowship to the first author.
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
In Proceedings of the 44th Annual Meeting of the
ACL. Sydney, Australia.
Peter F. Brown, Stephen A. Della Pietra, Vincent Della
J. Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
312. MIT Press. Camrbidge, MA. USA.
165
Colin Cherry and Dekang Lin. 2006. Soft Syntactic
Constraints for Word Alignment through Discrimi-
native Training. In Proceedings of the 44th Annual
Meeting of the ACL. Sydney, Australia.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics. 33(2):201?228.
MIT Press. Cambridge, MA. USA.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and
Structural Translation Features. In Proceedings of
EMNLP. Honolulu, HI. USA.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Computational
Linguistics. 29(4):589?637. MIT Press. Cam-
bridge, MA. USA.
Michael Collins 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In
Proceedings of the 45th Annual Meeting of the ACL.
Prague, Czech Republic.
Alexander Fraser and Daniel Marcu. 2007. Getting
the Structure Right for Word Alignment: LEAF. In
Proceedings of EMNLP-CoNLL. Prague, Czech Re-
public.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using Syntax to Improve Word Alignment
Precision for Syntax-Based Machine Translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation. Columbus, Ohio.
Dan Klein and Christopher D. Manning. 2001. Parsing
and Hypergraphs. In Proceedings of the 7th Interna-
tional Workshop on Parsing Technologies. Beijing,
China.
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better Word Alignments with Supervised ITG Mod-
els. In Proceedings of ACL-IJCNLP 2009. Singa-
pore.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies. Vancouver, BC.
Canada.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the ACL. Prague, Czech Republic.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings
of the 46th Annual Meeting of the ACL. Columbus,
OH. USA.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In Proceedings of NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Models In Proceedings of
the 44th Annual Meeting of the ACL. Sydney, Aus-
tralia.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT-EMNLP.
Vancouver, BC. Canada.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
Quadratic Assignment. In Proceedings of HLT-
EMNLP. New York, NY. USA.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear Models for Word Alignment In Proceedings
of the 43rd Annual Meeting of the ACL. Ann Arbor,
Michigan. USA.
Robert C. Moore. 2005. A Discriminative Framework
for Word Alignment. In Proceedings of EMNLP.
Vancouver, BC. Canada.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved Discriminative Bilingual Word
Alignment In Proceedings of the 44th Annual Meet-
ing of the ACL. Sydney, Australia.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics. 29(1):19?52.
MIT Press. Cambridge, MA. USA.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation In Proceedings of the
44th Annual Meeting of the ACL. Sydney, Australia.
Kishore Papineni, Salim Roukos, T. Ward, and W-J.
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation In Proceedings of the
40th Annual Meeting of the ACL. Philadelphia, PA.
USA.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach to
Word Alignment. In Proceedings of HLT-EMNLP.
Vancouver, BC. Canada.
David Talbot and Thorsten Brants. 2008. Random-
ized Language Models via Perfect Hash Functions.
In Proceedings of ACL-08: HLT. Columbus, OH.
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics. 23(3):377?404. MIT
Press. Cambridge, MA. USA.
166
