Gene/protein/family name recognition in biomedical literature  
 
 
Asako Koike 1,2 
1Central Research Laboratory, Hitatchi, 
Ltd. 
1-280 Higashi-koigakubo Kokubunji, 
Tokyo, 185-8601 
 akoike@hgc.jp 
Toshihisa Takagi2 
2University of Tokyo. Dept. of Comp. 
Biol. Graduate School of Frontier Science 
Kiban-3A1(CB01) 1-5-1 Kashiwanoha Ka-
shiwa-shi Chiba 277-8561, Japan 
tt@k.u-tokyo.ac.jp 
 
 
Abstract 
Rapid advances in the biomedical field have 
resulted in the accumulation of numerous ex-
perimental results, mainly in text form. To ex-
tract knowledge from biomedical papers, or 
use the information they contain to interpret 
experimental results, requires improved tech-
niques for retrieving information from the 
biomedical literature. In many cases, since the 
information is required in gene units, recogni-
tion of the named entity is the first step in 
gathering and using knowledge encoded in 
these papers. Dictionary-based searching is 
useful for retrieving biological information in 
gene units. However, since many genes in the 
biomedical literature are written using am-
biguous names, such as family names, we 
need a way of constructing dictionaries. In our 
laboratory, we have developed a gene name 
dictionary:GENA and a family name diction-
ary. The latter contains ambiguous hierarchi-
cal gene names to compensate GENA. In 
addition, to address the problem of trivial 
gene name variations and polysemy, heuristics 
were used to search gene/protein/family 
names in MEDLINE abstracts. Using these 
algorithms to match dictionary and 
gene/protein/family names, about 95, 91, and 
89% of protein/gene/family names in abstracts 
on Saccharomyces cerevisiae, Drosophila 
melanogaster, and Homo sapiens were de-
tected with a precision of 96, 92, and 94%, in 
respective organisms. The effect of our 
gene/protein/family recognition method on 
protein-interaction and protein-function ex-
traction using these dictionaries is also dis-
cussed.  
1 Introduction 
With the increasing number of biomedical papers, 
and their electronic publication in NCBI-PUBMED, 
there is a growing focus on information retrieval from 
texts. In particular, the recent development of proce-
dures for large-scale experiments, such as yeast-two 
hybrid screening, mass spectrometry, and DNA/protein 
microarrays, has brought about many changes in the 
knowledge required by biologists and chemists. Because 
they produce large amounts of data on genes at one time, 
biologists require extensive knowledge of numerous 
genes to analyze the data obtained and these are beyond 
the capability of manual acquisition from the vast bio-
medical literature. Since, in many cases, the main objec-
tive of text processing is extraction of protein-
protein/gene interaction or gene function, the first prob-
lem to solve is gene/protein/compound name recogni-
tion. To date, various methods of protein/gene name 
taggers have been proposed, mainly relating to Homo 
sapiens.  These methods can be roughly divided into 
rule-based approaches (Fukuda et al 1998), statistical 
approaches, including machine learning (Collier et al 
2000, Nobata et al 1999), dictionary/knowledge-based 
approaches Humphreys et al 2000, Jenssen et al 2001, 
Koike et al 2003), or a combination of these approaches 
(Tanabe and Wilbur, 2002). Since merely recognizing 
gene/protein names is insufficient to keep the extracted 
information in gene order, dictionary-based name rec-
ognition appears useful for assigning the locus of the 
extracted gene/protein name. Naming conventions are 
quite different for different organisms. Therefore, an 
appropriate approach is required for each organism.  
                                            Association for Computational Linguistics.
                    Linking Biological Literature, Ontologies and Databases, pp. 9-16.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
There are three main problems in dictionary-based 
searching: (1) the existence of multi-sense words; (2) 
variations in gene names; and (3) the existence of am-
biguous names. The first problem is mainly seen in 
symbol (abbreviated) types. For example, HAC1 is a 
synonym for both ?tripartite motif-containing 3? and 
?hyperpolarization activated cyclic nucleotide-gated 
potassium channel 2? in H. sapiens. Further, some gene 
names, especially in Drosophila melanogaster, have the 
same spelling with verb(lack, ...), adjective(white, yel-
low...), common nouns (spot, twin, ...), and prepositions 
(of, ...). The second problem is trivial variations in gene 
names (orthographical, morphological, syntactic, lexico-
semantic, insertion/deletion, permutation, or pragmatic). 
For example, ?mitogen-activated protein kinase 1? and 
?protein kinase mitogen-activated, 1?, ?NIK ser-
ine/threonine protein kinase?, and ?NIK protein kinase? 
indicate the same gene. The third problem is caused by 
ambiguous expression of the gene name in the text. The 
problems of multi-sense words and the ambiguity are 
well summarized by Tuason et al (2004) 
In many cases, the family name is used instead of 
the gene name. A unique gene locus may not have been 
specified, especially for genes with multiple paralogs, or 
to avoid repeating the same expression, the family name 
may frequently be used. For example, in 1996, the ?14-
3-3? family name was counted 107 times in abstracts 
using mesh terms for human, while ?14-3-3 alpha, beta, 
delta, gamma? gene name expressions did not appear at 
all. Thus, a family name dictionary is also required 
along with a gene name dictionary to specify the gene 
locus or loci. In this study, the above-mentioned prob-
lems were, as far as possible, solved simply using heu-
ristics. 
 
2 Construction of the gene name diction-
ary 
    The gene name dictionary, GENA, was constructed 
using the major databases, GenAtlas 
(http://www.dsi.univ-paris5.fr/genatlas/), HUGO 
(http://www.gene.ucl.ac.uk/hugo/), LocusLink 
(http://www.ncbi.nlm.nih.gov/LocusLink/), GDB 
(http://gdb.weizmann.ac.il/index.shtml), SGD 
(http://www.yeastgenome.org/), MIPS 
(http://mips.gsf.de/genre/proj/yeast/index.jsp), Worm-
base (http://www.wormbase.org/), OMIM 
(http://www.ncbi.nlm.nih.gov/omim/), MGI 
(http://www.informatics.jax.org/), RGD 
(http://rgd.mcw.edu/), FlyBase 
(http://flybase.bio.indiana.edu/), S. pombe geneDB 
(http://www.sanger.ac.uk/Projects/S_pombe/), SWISS-
PROT, TrEMBL (http://us.expasy.org/sprot/), and PIR 
(http://pir.georgetown.edu/) for Schizosaccharomyces  
pombe, Saccharomyces cerevisiae, Caenorhabditis ele-
gans, Drosophila melanogaster, Mus musculus, Rattus 
norvegicus, and Homo sapiens, respectively. A merge of 
each database entry was done using the ?official sym-
bol? or ORF name and link data provided by each entry 
and the protein-sequence data entry. The priority of the 
database was given in advance. For example, in H. 
sapiens, HUGO, Locuslink, GDB, and GenAtlas were 
registered in this order, using the merged entry for the 
same ?official symbol?. LocusLink?s ?preferred symbol?, 
which is not yet administered by HUGO, was also used. 
Merging the entries in SWISS-PROT, TrEMBL, and 
these registered data was done using the link data for 
?Genew? provided by SWISS-PROT and TrEMBL. The 
rest of the entries were merged using the protein-IDs for 
LocusLink, SWISS-PROT, and TrEMBL. For example, 
LocusLink provides unique representative mRNA and 
protein sequences, and related sequences belonging to 
the same gene. If the protein-sequence entry for SWISS-
PROT and TrEMBL matched with any of these se-
quence entries for LocusLink, the entries were merged. 
Linking these registered data with the PIR entries was 
also done using protein-ID entries. In principle, for all 
organisms, protein sequences without ?official or pre-
ferred symbols? were not registered. The entries con-
sisted of ?official symbols? and ?official full names?, 
which were provided by representative institutions, such 
as HUGO, for each organism, and ?synonyms? and 
?gene products?. S. cerevisiae and C. elegans do not 
have ?official full names?. The distinction between these 
elements of each ?name? simply depends on the ?item 
headings? for each database. Although gene names and 
their product names are registered separately for one 
locus, and whether the entry?s product is protein or 
RNA is also registered in GENA, we do not distinguish 
between them here. Hereafter, we do not distinguish 
?gene product? from the gene name ?synonym?. Unfor-
tunately, databases contain numerous mistakes or inap-
propriate gene/protein names. The reliability of each 
synonym was judged according to the database source. 
To meet our information extraction purposes, only gene 
names over a certain reliability can be used. Meaning-
less names (ex. hypothetical protein), higher concept 
names (ex. membrane protein) and apparently wrong 
names (ex. OK ) were removed from the data semi-
automatically using word-net vocabularies and term 
frequencies of all abstracts of one year. In an evaluation 
of this study, synonym names entered only in TrEMBL 
or PIR, except for names manually checked in our labo-
ratory, were removed due to their low reliability.  
   In addition to these data, we added synonym names 
using the following methods. (1) Abbreviations of 
synonyms were added using an abbreviation extraction 
algorithm (Schwartz and Hears, 2003). (2) Plausible 
gene names were extracted from the subject and object 
noun of some verbs, which restricted such subjects and 
objects as ?phosphorylate? and ?methylate? (both sub-
jects must be protein/gene/family names). These are by-
products of protein-interaction extraction in our project. 
The corresponding ?official symbol? was searched using 
a partial match of registered names, and finally was 
checked manually.  
Compound names were gathered from the index of 
the biochemical dictionary, KEGG 
(http://www.genome.ad.jp/kegg/kegg2.html), mesh 
terms, and UMLS 
(http://www.nlm.nih.gov/research/umls/)  and were reg-
istered in GENA. Some high-concept terms were re-
moved manually. Compound name searches were not 
evaluated in this study. Currently (January, 2004), it 
contains about 920,000 registered gene/protein names 
and 210,000 compound names. 
 
 GENA was managed using Postgres, which pro-
vides command line searching and Web searching 
(http://www.gena.ontology.ims.u-tokyo.ac.jp). Searches 
can be done considering the word order replacement of 
long gene names using indexing all words consisting 
names. 
3 Construction of family name dictionary 
The construction of the family name dictionary was 
done using SWISS-PROT family names, PIR family 
names, INTERPRO family names 
(http://www.ebi.ac.uk/interpro/), gene/protein names in 
GENA, and clustering sequence similarities. These have 
hierarchical named entities. For example, ?MAPK1? is 
a member of the ?MAPK family? and the ?MAPK fam-
ily? is a member of the family of the ?Ser/Thr protein 
kinase family?; in turn, this family is a member of ?pro-
tein kinase?, and ?protein kinase? is a type of ?kinase?. 
Although ?family? is usually used to indicate ?similar 
sequence groups that probably have the same origin?, 
sometimes it is also used to mean ?sequence groups that 
have almost the same function?. In this paper, we use 
?family? as ?ambiguous gene/protein names that indi-
cate similar sequences or biological functions?. Plausi-
ble family names based on gene names are the common 
parts of multiple gene names, such as ?MAPK? of 
?MAPK[number]?, ?14-3-3? of ?14-3-3 [Greek alpha-
bet[alpha-delta/alphabet[a-d]]?,  ?protein kinase? of 
?Tyr protein kinase? and ?Ser/Thr protein kinase?, and 
?kinase? of ?Inositol kinase? and ?protein kinase?. The 
backbone of the family hierarchy was constructed based 
on the INTERPRO family hierarchy. As far as possible, 
the remaining hierarchy was manually constructed con-
sidering sequence similarities, using Markov clustering 
(Enright et al 2002) based on all-versus-all blast. The 
hierarchy has a directed acyclic graph structure. The 
family names are across organims and the family name 
dictionary is common to each organism. The family 
database is available from http://marine.ims.u-
tokyo.ac.jp:8080/Dict/family. Currently (January, 2004), 
it contains about 16,000 entries and 70,000 registered 
names. 
4 Gene/protein/family name searches us-
ing a devised trie 
A gene/protein/family name search of texts was car-
ried out using a devised trie for faster gene name search-
ing. The trie was provided for each organism separately. 
The core terms implemented for the trie were generated 
based on GENA. Here, the following main heuristics 
were used.  
(1) Special characters are replaced by a space.  
(2) In principle, both numerical and Roman numer-
als are prepared.  
(3) The space before a numerical number is removed. 
However, if the previous character before the space is a 
number, the space is not removed (e.g., 14-3-3 is ?14 3 
3?).  
(4) With space and without space terms are used for 
?Greek alphabet and alphabet a/A, b/B, c/C, ...?. For 
example, ?14 3 3 alpha, 14 3 3alpha, 14 3 3 a, 14 3 3a?.  
(5) Common words at the end of gene names, such 
as ?protein?, ?gene?, ?sub-family?, ?family?, and 
?group?, are removed. However, if the meaning of 
names is changed with/without these words, they are 
left. For example, ?T-cell surface protein? indicates 
?protein on the T-cell surface?, while ?T-cell surface? 
usually indicates ?the surface of the T-cell?, and remov-
ing ?protein? from ?memory-related protein? causes 
faulty recognition of ?memory-related function? as 
?memory related /gene-name? ?function?. When ?pro-
tein?, ?gene?, ?sub-family?, ?group?, and ?family? ap-
pear within gene names, gene words with and without 
these words are generated.  
(6) For symbol-type names (less than seven charac-
ters), the initial of the organism is added to the spelt-out 
type. For example, in MAPK1 for H. sapiens, hMAPK1 
and h MAPK1 are used. For S. cerevisiae, the protein 
name is generated by adding ?p? at the end of the name. 
For example, the protein of STE7 is STE7p. For muta-
tions of D. melanogaster, + added names are used. For 
example, lt+ for lt. 
 (7) All names are converted into small characters 
and plurals are also generated. Some names are ?case 
sensitive? and some require ?all capital letters?. In prin-
ciple, when the name is the common spelling of a 
?common noun, adverb, or adjective?, ?all capital letter 
names? are adopted in H. sapiens, M. musculus, and R. 
norvegicus (using ?word net vocabularies? with less 
than five characters. Word length is limited to remove 
words that happen to have the same spelling but without 
removing biological names registered in the word net). 
?All capital letters names? were recognized in the trie. 
Case-sensitive words such as cAMP and CAMP were 
selected experientially and checked after the trie search. 
Since many of Drosophila melanogaster genes have the 
same spelling with verb, adjective, common nouns, and 
preposition. These gene names are replaced by ?gene 
name + specified names? using word-net vocabularies 
to decrease false positive. For example, the gene name 
?yellow? is replaced by ?yellow locus?, ?yellow gene?, 
?yellow protein?, ?yellow allele?...  etc. 
The trie search starts from the next characters after a 
?space?, ?-?, ?/?, or ?period? or the head of sentence. 
When multiple gene names are hit in duplicate, the 
longest name ID is outputted. When specific terms, such 
as ?antagonist?, ?receptor?, ?cell?, and ?inhibi-
tor?, ....are next to the gene name, the hit gene name ID 
is not outputted, since these indicate different 
gene/protein names or are not gene/protein names. Also, 
when terms such as ?promoter? and ?mutant? are lo-
cated next to the gene name, they do not show the 
gene/protein/family themselves. However, for our pur-
poses of extracting the genetic interaction, they are 
treated the same as gene/protein/family names. Specific 
terms such as ?number? are located before the gene 
name and the hit gene name ID is not outputted since 
they are multi-sense words and, in most cases, are not 
gene/protein names. Parentheses are also specially 
treated, so ?mitogen activated protein kinase (MAPK) 
1? --> is recognized as ?mitogen activated protein 
kinase 1 (MAPK1)?. The continuous gene description 
such as ?GATA-4/5/6? is also specially treated as 
shown in Figure 1. If the gene names are synonyms of 
multi-genes, the multiple gene IDs are outputted in this 
stage. 
 
  Figure 1. The schematic drawing of a devised trie. 
5 Resolving multi sense words 
  To resolve the problem of multi-sense words, we 
used information from the whole text. When the hit 
name is shorter than a certain gene name length (seven 
characters for H. sapiens; the length is different for each 
organism), there is a possibility that the hit name is an 
abbreviation of another word (not only gene names, but 
also an experimental method or name of an apparatus). 
To avoid false-positive words as far as possible, we 
used the following heuristics in M. musculus, R. 
norvegicus, and H. sapiens.  
1) If the corresponding full name, or a name longer 
than six characters, is written in the same abstract, 
the hit gene ID is used.  
When the full name and abbreviation pairs are writ-
ten in the abstract as ?plausible full name (the hit 
name)? or ?plausible full name [the hit name]?, the fol-
lowing procedures are carried out.  
2) If the full/long name is a complete match for the 
synonyms or full name of the corresponding ID, the hit 
gene ID is used.  
3) If the full/long name is not a complete match for 
these corresponding IDs using the abbreviation extrac-
tion algorithm (Schwartz and Hearst, 2003), but its 
spelling consists of words used in any name of the cor-
responding ID, the hit ID is adopted. If not, the hit ID is 
discarded (i.e., the full/long name considering the re-
placement of the word order).  
4) If information on full names or long names is not 
found in the abstract, a key-word search of all the ab-
stracts is carried out. If at least one key word is detected, 
the ID is used.  
The summary of these steps were shown in Figure 2. 
(The numbers in Fig.2 correspond to the above head 
numbers.) 
However, treatment (2) is not sufficient in some cases 
because some abbreviations are written only once for 
one family kind.  For example, in PUBMED-ID 
8248212, ...?the recently described TAP (transporter 
associated with antigen processing) genes have been 
mapped approximately midway between DP and DQ. ... 
In addition to the alleles of TAP1 that have been de-
scribed, others were identified during this study.?  
?TAP1? is the synonym for ?transporter 1, ATP-binding 
cassette, sub-family B (MDR/TAP)?, and ?transient 
receptor potential cation channel, subfamily C, member 
4 associated protein.? In most cases, the full name is 
written only once for the same family. In this case, the 
former (?transporter 1, ATP-binding cassette, sub-
family B (MDR/TAP)?) is correct. Accordingly, the full 
name and abbreviation pair ?TAP? without the number 
is also checked. Since all vocabularies (?transporter?, 
?associated?, ?antigen?, ?processing?) are components 
of synonyms of TAP1, the TAP1 is recognized by 
?transporter 1, ATP-binding cassette, and sub-family B 
(MDR/TAP)?. In considering syntactic variations, some 
GATA-4/5/6 expression constructs ...  G 
A 
T 
4 
5 
/ 
/ 
6 
A If next word is not stop word, the ID is 
outputted. 
GHS007219 
GHS007220 
P
GHS007219/GHS007221/GHS007222 
space,  
hyphen, ... 
... 
... 
GHS007221 
prepositions such as ?of? and ?with?, and frequently 
used words such as ?sub-family? and ?family?, are 
skipped in this process. Further regarding the lexico-
semantic pattern, as far as possible, adjectives and 
nouns are provided for each vocabulary using word-net 
vocabularies and UMLS. 
 
   Figure 2. The schematic drawing of each gene names. 
With this treatment, only when pairs of full names, or 
close to the full name, and abbreviations appear, the 
distinctions between some synonyms are completed. In 
some cases, the name belongs to the same family. For 
example, LRE2 is a synonym for ?LINE retrotranspos-
able element 2? and ?LINE retrotransposable element 
3?. In this case, the distinction between them is very 
fine and seems unimportant. In some abstracts, full 
names are not written in the text. To resolve this issue, 
we used key words for each gene, which were selected 
from all words/terms (continuous words) composing 
synonym names and their family names as shown in the 
procedures in (4). When at least one keyword is de-
tected, the ID is accepted. The key words appear less 
than 50 times (only for words extracted from gene 
names, in the case of words from family name, this limi-
tation is not used) in genes and appear less than a cer-
tain frequency in all abstracts and are not common to 
different genes that have synonyms with the same spell-
ing. Even if a key word search is performed, except for 
famous names such as p53 and p38, the locus identifica-
tion for ?# kDa?, meaning a ?#p? expression such as 
p60 and p61, is quite difficult. In relation to famous 
name-Ids, such as cAMP(cyclic AMP), CD2(cluster 
designation 2),  the IDs are used to recover a false nega-
tive even if the full/longer name is not written in the 
abstracts and the keywords are not detected.  
The automatic keyword selection using conventional 
methods such as tf-idf (Salton and Yang, 1973) and 
SMART (Singhal et al 1996) may be applicable. How-
ever, the number of abstracts per gene is too small in 
many cases and the effective keywords selection could 
not be achieved. Therefore, this approach was not ap-
plied, in this study. 
For S. cerevisiae, C. elegans, and D. melanogaster, in 
most cases, the full names of symbols are not written. 
Only when the symbol name has a symbol (abbrevia-
tion)-full name pairs, and the full name is not the corre-
sponding gene name or contains a word that is not a 
component of the synonyms, the hit-ID is discarded.  
Although, as far as possible, we removed what we as-
sumed were wrong or inappropriate gene names, some 
names either do not seem to be synonyms or are rarely 
used ones. These can cause errors. For example, LPS is 
a synonym for ?interferon regulatory factor 6? (for ex-
ample, LocusLink, GenAtlas) and ?lipopolysaccharide? 
in H. sapiens. However, our investigations indicate that 
LPS is not used to indicate ?interferon regulatory factor 
6? in abstracts. 
 
6 Experiment and Results 
To validate the recall and precision of our method 
for gene/protein/family name recognition, we made 
manually pre-tagged 100 abstracts (1996 year) on each 
of the following organisms: S. cerevisiae, D. 
melanogaster, and H. sapiens with mesh terms ?sac-
charomyces cerevisiae?, ?drosophila melanogaster?, 
and ?human?, respectively. Table 1 shows the results. 
In this evaluation, whether each gene/family ID was 
correctly assigned in the abstract or not was investigated. 
(each ID was counted only once per abstract.) When the 
precision and recall of all gene/family name descrip-
tions? recognition were calculated (each ID can be 
counted more than once per abstract), they did not 
change largely and  were within 2-5% error spans of 
Table 1.  
 
Table 1 The summary of precision and recall of 
gene/protein/family name recognition 
Organism* Precision 
=TP/(TP+FP) : 
total(gene/family) 
Recall 
=TP/(TP+FN): 
total(gene/family) 
HS 94.3 
(95.2/93.2)% 
88.6 
(92.0/85.0) % 
DM 92.1 
(90.3/94.5)% 
91.2 
(91.8/90.4)% 
SC 95.5 
(94.6/96.0)% 
94.6 
(96.0/93.7)% 
*HS:H. sapiens, DM:D. melanogaster, SC:S. cerevisiae 
The corpus size and the number of deficient name 
entries in GENA and family name dictionary were 
summarized in Table 2.  
 
Table 2 The corpus size and num. of deficient 
gene/family entries. 
Organism Num of 
gene/family in the 
corpus: 
total (gene/family)  
Num of deficient 
name entries: 
total(gene/family) 
HS 167 (87/80) 10 (1/9) 
DM 547 (317/230) 31 (16/18) 
SC 277 (100/177) 14 (2/11) 
 
In judging family name recognition, slightly soft cri-
teria were used. If a complete matching entry was not 
registered in the family name dictionary, a higher con-
cept ID was assigned. For example, ?lactate dehydro-
genase? was not registered in the family name 
dictionary, so this name was assigned the ID ?dehydro-
genase?. Even if the other organisms are written in the 
same abstracts, their gene names are not extracted in 
principle. However, human, rat, and mouse are not dis-
tinguished in this validation. The family names in other 
organisms are also extracted in this evaluation. 
 
As shown in Table 2, in all organisms, more than 
one-third of the gene names were written as family 
names. This indicates the necessity for hierarchical gene 
names, as in the family dictionary, although conven-
tional methods scarcely mentioned. The recall and pre-
cision of these organisms as shown in Table 1 are 
relatively high roughly compared to previous reports. 
(precision:72-93%, recall:76-94%: The summary is re-
viewed by Hirschman 2002). The details of errors were 
as followings. Only 4 and 1 names, which were regis-
tered in GENA and family name dictionary, were rec-
ognized as gene/family names at once, but they were 
erroneously discarded by the procedures used to con-
firm ambiguous names, in H. sapiens. Many of them are 
caused by the key-word search fails.  Especially, in fam-
ily names, the key-words seem to be insufficient. 
Probably, these will be addressed in some extent by use 
of the key words of the higher/lower concept IDs. In 
some cases, the full-name and abbreviation match failed. 
For example, in ?urokinase-type plasminogen activator 
receptor (uPAR, CD87)?, the full-name and abbrevia-
tion match failed due to the existence of ?two names? in 
the parenthesis. These errors will be recovered by the 
keyword search. However, in the present program, re-
covering step is not used. The recall of family names in 
H. sapiens is slightly low because of varieties of fami-
lies as shown in Table 1. 6, 4 names were false positive 
gene/protein names in S. cerevisiae and H. sapiens, re-
spectively. 7, 5 names were false positive family names 
in S. cerevisiae and H. sapien, respectively. Most of 
them were short names and were not removed due to 
their in-appropriate keywords.  Some of them are 
caused by inappropriate GENA entries.  
In relation to D. melanogaster, 10 gene/protein 
names that were registered in GENA were not recog-
nized as gene/family names. Many of them were general 
nouns/adjective and were not used as the ?gene name + 
specified words? phrase in the abstracts. Rest of them 
were gene/protein names removed in trie implementa-
tion steps due to their confusing spellings such as ?10-
4?. Also mutant gene name recognition was quite diffi-
cult in this method, since the superscript for the muta-
tion was converted in the normal characters in NCBI-
abstracts and newly developed mutant was expressed by 
changing the superscript.  4 family names were recog-
nized once and erroneously discarded in the keyword 
search steps. 31 gene/protein names and 12 family 
names were false positive. Most of them in gene/protein 
names were misleading names such as 19A. These mis-
leading names were removed or replaced by the ?gene 
name + specified words? phrase as far as possible with 
some heuristics and term frequencies in abstracts. How-
ever, some remained. Some false positive were wrongly 
extracted other organisms? gene names. 
 In the strict criteria of family name recognition, 10, 
18, 10 names were recognized as higher concepts in H. 
sapiens, D. melanogaster, and S.cerevisiae, respectively. 
The registration of detailed entries for the family name 
dictionary is required. 
The heuristics of the name detection seem to be suf-
ficient so that no name detections failed due to trivial 
name variations in H. sapiens and S. cerevisiae, and 
only one name in D. melanogaster except mutant varia-
tion failed. There is some room to be improved in ambi-
guity resolution steps using sophisticated keyword 
searching. 
In our laboratory, protein interaction information and 
protein function were automatically extracted and stored 
in PRIME (http://prime.ontology.ims.u-tokyo.ac.jp) and 
in the protein kinase database 
(http://kinasedb.ontology.ims.u-tokyo.ac.jp, Koike et al, 
2003). With this procedure, some false positives were 
not extracted since the phrase patterns did not match the 
extracted protein interaction and protein function. That 
is, some wrongly recognized names were removed as a 
result of considering the local context. In this stage, the 
wrongly recognized false positive names was 0, 4, and 3 
for S. cerevisiae, D. melanogaster, and H. sapiens, re-
spectively. Using the family name dictionary greatly 
increased the recognition of ambiguous names. How-
ever, a new difficulty was found in extracting informa-
tion. Many family names are common to functional 
nouns. Therefore, even if a phrase pattern is used, the 
wrong interaction may be extracted. For example, from 
PUBMED_11279098: ?We also identified key residue 
pairs in the hydrophobic core of the Cet1 protomer that 
support the active site tunnel and stabilize the triphos-
phatase in vivo.? It is difficult to automatically judge 
from this sentence whether ?triphosphatase? means the 
Cet1 function or another protein family name. All the 
interaction information in this abstract indicates that 
?triphosphatase? is the activity of Cet1. Our program 
wrongly extracted ?Cet1/gene-name? stabilize ?triphos-
phatase/family-name?. Additional heuristics are re-
quired to remove these wrongly extracted data. 
 
7 Related Work 
Various protein/gene recognition methods have been 
reported and some successes were gained as briefly re-
viewed in introduction and well reviewed in the refer-
ences (Hirshman et al, 2002). However, most of them 
did not specify the gene locus. Further, they were de-
veloped mainly for H. sapiens. Since the naming con-
vention is different in organisms, their recognition 
performance in other organisms is unknown. 
Hirshman et al (2002) have reported the dictionary-
based name recognition. This report discussed the diffi-
culty of the gene name recognition of D. melanogaster 
and showed the increase of the precision by removing 
the gene names that have meanings as normal English 
words. Tuason et al (2004) have investigated that the 
ambiguity within each organism and among organisms 
(mouse, worm, fly, and yeast) and with general English 
words. Tsuruoka and Tsujii (2003) also reported the 
dictionary-based named recognition and our method is 
similar to them. They resolved the trivial gene variation 
problems using dynamic programming and tries, while 
in our method, by normalizing dictionary names and 
devising the trie structure, the trivial variations were 
addressed without dynamic programming and the re-
quired CPU time is expected to be largely reduced 
without decreasing precision and recall. The protein 
name recognition standard is a little different from them 
and the direct comparison of precision and recall with 
their results seem meaningless. In their methods, they 
focus on protein names (without gene names) and seem 
not to distinguish whether the protein name candidate 
represents the protein itself or not in the context. (ex. 
?IL-1 receptor antagonist? and ?IL-1 receptor expres-
sion?: only the latter description means the IL-1 recep-
tor itself.)  Further, in our method, addressing the 
ambiguity of gene names (common gene names among 
multiple gene names) is tried. Since long protein names 
are usually written with abbreviated names, the name 
variations caused of permutation and insertion/deletion 
of long name words are picked up in the ambiguity reso-
lution process.  
 
8 Conclusions: 
We constructed gene name and family name dic-
tionaries to link each gene name to a gene locus and to 
relate ambiguous names to gene families. Our prelimi-
nary investigations showed that more than one-third to 
one-half of gene/protein names in abstracts are written 
using ambiguous names such as family/super-family 
level names. This indicates that dictionary-based 
gene/protein/family name recognition requires not only 
a gene name dictionary but also a hierarchical family 
name dictionary. Using the gene name dictionary 
GENA and the family name dictionary we constructed 
and our searching method, 95, 91, and 89% of pro-
tein/gene/family names in abstracts on S. cerevisiae, D. 
melanogaster, and H. sapiens were detected with a pre-
cision of 96, 92, and 94%, respectively. The simple heu-
ristics we developed seem to be useful for matching 
gene/family names in texts with dictionary entry names, 
although additional trivial changes are required to ad-
dress ambiguity of gene names. These methods are also 
useful for extracting data on protein interaction and pro-
tein function. However, the gene/protein/family name 
recognition subject is deep. For example, ?NFkappaB? 
represents ?NFKB1? and ?RELA? complex in many 
contexts and sometimes represents ?NFKB1?. Unfortu-
nately, these complicated recognitions were not re-
solved. 
 Although different organisms have different naming 
conventions, the nomenclature for mammals is similar 
to that for H. sapiens, and most bacteria and archaea 
gene/protein/family names are similar to the nomencla-
ture for S. cerevisiae. Problems in gene name recogni-
tion for most organisms will be able to be addressed 
using our method. Dictionary-based name recognition 
cannot search new gene name/synonym names. How-
ever, the whole human/drosophila/yeast genomes have 
already been sequenced and the appearance of new 
synonym names can be expected to decrease or be in-
ferable from the referenced known name. In addition, 
with the introduction of the family name dictionary, 
parts of new genes can be retrieved using the higher 
concept name (family name), even if the new gene name 
itself is not registered in GENA. Accordingly, the dic-
tionary-based name recognition will be expected to be 
sufficient for the information extraction in these organ-
isms. 
 Protein-interaction and protein-function information 
extracted using these procedures for gene/protein/family 
name recognition are available from 
http://prime.ontlogy.ims.u-toky.ac.jp. 
 
Acknowledgements 
 
We wish to acknowledge Yo Shidahara and 
Kouichiro Yamada for reading many abstracts and help-
ing us by constructing the family name dictionary. We 
would like to thank Chiharu Kikuchi in Nittetsu Hitachi 
System Engineering for helping us by programming 
GENA. 
This work is supported in part by Grant-in aid for 
scientific research on priority areas (c) genome informa-
tion science from the Ministry of Education, Culture, 
Sports, Science, and Technology of Japan. 
References 
Collier, N., Nobata, C. and Tsujii, J. 2000. Proc. of the 
18th Int. Conf. on Comp. Ling. 201-207. 
Enright, AJ, Van Dongen, S, and Ouzounis, CA. 2002. 
Nucleic Acids Res. 30(7):1575-84.l 
Fukuda, K., Tsunoda, T. Tamura, A. and Takagi, T. 
1998 Proceedings of the Pacific Symposium on Bio-
computing, 705-716.  
Hatzivassiloglou, V., Duboue, P.A. and Rzhetsky, A. 
2001. Bioinformatics, 17S(1), S97-S106. 
Hirschman, L., Morgan, A.A., and Yeh, A.S. 2002 J. 
Biomed. Inform. 35:247-259.  
Humphreys, K., Demetriou, G., and Gaizauskas, R. 
2000 Proc. of the Pacific Symposium on Biocomput-
ing, 5:502-513. 
Jenssen TK, Laegreid A, Komorowski J, Hovig E. 2001. 
Nat Genet. 28(1):21-8. 
Koike, A., Kobayashi, Y., and Takagi, T. 2003. Genome 
Research, 13:1231-1243. 
Nobata, C., Collier, N., and Tsujii, J. 1999. Proc. of Nat. 
Lang. Paci. Rim Symp. 369-374. 
Salton, G. and Yang, C.S. (1973) J. Document. 29(4), 
351-372. 
Schwartz, A.S., Hearst M.A., 2003. Pacific Symposium 
on Biocomputing 8:451-462. 
Singhal, A. Buckley, C., and Cochrane, P.A. 1996. Proc. 
of ACM SIGIR, 26-133. 
Tanabae, L and Wilbur,WJ. 2002. Bioinformatics, 
18(8):1124-1132. 
Tsuruoka, Y. and Tsujii, J. 2004. Proc. of the ACL 2003 
Workshop on Natural Language Processing in Bio-
medicine 41-48. 
Tuason, O. and Chen, L., Liu, H., Blake, J.A., and 
Friedman, C. 2004. Proc. of Pacific Symposium on Bio-
computing, 238-249. 
 
 
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 90?91,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Prediction of Protein Sub-cellular Localization
using Information from Texts and Sequences
Hong-Woo Chun1,2,3 Chisato Yamasaki2,3 Naomi Saichi2,3 Masayuki Tanaka2,3
chun@dbcls.rois.ac.jp, {chisato-yamasaki, nao-saichi, masa-tanaka}@aist.go.jp
Teruyoshi Hishiki3 Tadashi Imanishi3,5 Takashi Gojobori3,6
{t-hishiki, t.imanishi, t-gojobori}@aist.go.jp
Jin-Dong Kim4 Jun?ichi Tsujii4,7,8 Toshihisa Takagi1,9
{jdkim, tsujii}@is.s.u-tokyo.ac.jp, takagi@dbcls.rois.ac.jp
1 Database Center for Life Science, Research Organization of Information and System,
Engineering 12th Bldg., University of Tokyo, 2-11-16 Yayoi, Bunkyo-ku, Tokyo, 113-0032, Japan
2 Japan Biological Information Research Center, Japan Biological Informatics Consortium
3 Biological Information Research Center,
National Institute of Advanced Industrial Science and Technology, Japan
4 Department of Computer Science, University of Tokyo, Japan
5 Graduate School of Information Science and Technology, Hokkaido University, Japan
6 Center for Information Biology and DNA Data Bank of Japan, National Institute of Genetics
7 School of Informatics, University of Manchester, UK
8 National Centre for Text Mining, UK
9 Department of Computational Biology, University of Tokyo, Japan
Abstract
This paper presents a novel prediction ap-
proach for protein sub-cellular localization. We
have incorporated text and sequence-based ap-
proaches.
1 Introduction
Natural Language Processing (NLP) has tackled and
solved a lot of prediction problems in Biology. One
practical research issue is Protein Sub-Cellular Lo-
calization (PSL) Prediction. Many previous ap-
proaches have combined information from both texts
and sequences by a machine learning (ML) technique
(Shatkay et al, 2007). All of them have not used tra-
ditional NLP techniques such as parsing. Our aim
is to develop a novel PSL prediction system using
information from texts and sequences. At the same
time, we demonstrated the effectiveness of the tra-
ditional NLP and the sequence-based features in the
viewpoint of the text-based approach.
2 Methodology
A Maximum Entropy-based ML technique has been
used to combine information from both texts and se-
quences. To develop a supervised ML-based predic-
tion system, an annotated corpus is needed to train
the system. However, there is no publicly available
corpus that contains the PSL. Therefore, we have
constructed a corpus using GENIA corpus as an ini-
tial data, because the annotation of Protein and Cel-
lular component in GENIA corpus is already done
by human experts. The new types of annotation con-
tain two tasks. The first annotation is to classify
1,117 cellular components in GENIA corpus into 11
locations, and the second annotation is to catego-
rize a relation between a protein and a location into
positive, negative, and neutral. Biologists selected
11 locations based on Gene Ontology: Cytoplasm,
Cytoskeleton, Endoplasmic reticulum, Extracellular,
Golgi apparatus, Granule, Lysosome, Mitochondria,
Nucleus, Peroxisome, and Plasma membrane. The
number of co-occurrences in GENIA corpus is 864.
1 Three human experts annotated with 79.49% of
inter-annotator agreement. For calculating the inter-
annotator agreement, all annotators annotated 117
1The co-occurrence in the proposed approach is a sentence
that contains at least one pair of protein and cellular component
names.
90
# Relevant Performance : F-score (Precision, Recall)
Location relations Baseline Text Sequence Text + Sequence
Nucleus 173 0.282 (0.164, 1.0) 0.764 (0.736, 0.794) 0.725 (0.569, 1.000) 0.778 (0.758, 0.798)
Cytoplasm 94 0.163 (0.089, 1.0) 0.828 (0.804, 0.852) 0.788 (0.657, 0.984) 0.828 (0.804, 0.852)
Plasma membrane 23 0.043 (0.022, 1.0) 0.875 (0.814, 0.946) 0.857 (0.766, 0.973) 0.885 (0.841, 0.932)
Table 1: Performance of protein sub-cellular localization prediction for each location.
co-occurrences. From the texts, we used eight fea-
tures: (1) protein and cellular component names an-
notated by human experts, (2) adjacent one and two
words of names, (3) bag of words, (4) order of names,
(5) distance between names, (6) syntactic category
of names, (7) predicates of names, and (8) part-of-
speech of predicates. To analyze the syntactic struc-
ture, we used the ENJU full parser whose output is
predicate-argument structures of a sentence.
To combine the information from sequences, we
attempted to predict PSL for all proteins in GE-
NIA corpus by two existing sequence-based meth-
ods: WoLF PSORT (Horton et al, 2006) and SOSUI
(Hirokawa et al, 1998). Approximately 14% of pro-
tein names in GENIA corpus obtained results. From
the sequences, we used two features: (1) existence
of the sequence-based results, and (2) the number of
sequence-based results.
3 Experimental results and Conclusion
The proposed approach has integrated text and
sequence-based approaches. To evaluate the system,
we performed 10-fold cross validation using 864 co-
occurrences including positive, negative, and neutral
relations. We measured the precision, recall, and
F-score of the system for all experiments. Among
864 co-occurrences in GENIA corpus, 301 positive
or negative co-occurrences have been considered as
relevant relations, and the remaining 563 neutral re-
lations have been considered as irrelevant relations.
Four approaches have been compared based on
three locations in Table 1. The four approaches are
baseline, text-based approach, sequence-based ap-
proach, and integration of the text and sequence-
based approaches. Baseline experiment used an as-
sumption: there is a relevant relation if a protein and
a cellular component names occur together in a co-
occurrence. The three locations selected when there
are the sequence-based results and the number of rel-
evant relations is more than one. All experiments
showed that the integration of text and sequence-
based approaches is the best, even though the exper-
iments for Cytoplasm showed the best performance
at both the text-based approach and the integration
approach.
A new prediction method has been developed for
protein sub-cellular localization, and it has integrated
text and sequence-based approach using an ML tech-
nique. The traditional NLP techniques contributed
to improve performance of the text-based approach,
and the text and sequence-based approaches recipro-
cally contributed to obtain a improved PSL predic-
tion method. The newly constructed corpus will be
included in the next version of GENIA corpus. There
are weak points in the proposed approach. The cur-
rent evaluation method has been focusing on eval-
uating the text-based approach, and the results of
the sequence-based approach were obtained for only
14% of proteins in GENIA corpus, so these situations
might be the reason that the sequence-based approach
did contribute a little. Thus, we need to evaluate the
proposed approach with a more reasonable method.
Acknowledgments
We acknowledge Fusano Todokoro for her technical
assistance.
References
Paul Horton, Keun-Joon Park, Takeshi Obayashi and
Kenta Nakai. 2006. Protein Subcellular Localization
Prediction with WoLF PSORT. Asia Pacific Bioinfor-
matics Conference (APBC), pp. 39?48.
Takatsugu Hirokawa, Seah Boon-Chieng and Shigeki Mi-
taku. 1998. SOSUI: classification and secondary
structure prediction system for membrane proteins.
Bioinformatics, 14(4): pp. 378?379.
Hagit Shatkay, Annette Ho?glund, Scott Brady, Torsten
Blum, Pierre Do?nnes and Oliver Kohlbacher. 2007.
SherLoc: high-accuracy prediction of protein subcellu-
lar localization by integrating text and protein sequence
data. Bioinformatics., 23(11): pp. 1410?1417
91
Proceedings of the Workshop on BioNLP: Shared Task, pages 41?49,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Markov Logic Approach to Bio-Molecular Event Extraction
Sebastian Riedel
??
Hong-Woo Chun
??
Toshihisa Takagi
??
Jun'ichi Tsujii
???
?
Database Center for Life Science, Research Organization of Information and System, Japan
?
Department of Computer Science, University of Tokyo, Japan
?
Department of Computational Biology, University of Tokyo, Japan
?
School of Informatics, University of Manchester, UK
?
National Centre for Text Mining, UK
{sebastian,chun,takagi}@dbcls.rois.ac.jp
tsujii@is.s.u-tokyo.ac.jp
Abstract
In this paper we describe our entry to the
BioNLP 2009 Shared Task regarding bio-
molecular event extraction. Our work can
be described by three design decisions: (1)
instead of building a pipeline using local
classifier technology, we design and learn
a joint probabilistic model over events in
a sentence; (2) instead of developing spe-
cific inference and learning algorithms for
our joint model, we apply Markov Logic, a
general purpose Statistical Relation Learn-
ing language, for this task; (3) we represent
events as relational structures over the to-
kens of a sentence, as opposed to structures
that explicitly mention abstract event en-
tities. Our results are competitive: we
achieve the 4th best scores for task 1 (in
close range to the 3rd place) and the best
results for task 2 with a 13 percent point
margin.
1 Introduction
The continuing rapid development of the Inter-
net makes it very easy to quickly access large
amounts of data online. However, it is impossi-
ble for a single human to read and comprehend a
significant fraction of the available information.
Genomics is not an exception, with databases
such as MEDLINE storing a vast amount of
biomedical knowledge.
A possible way to overcome this is informa-
tion extraction (IE) based on natural language
processing (NLP) techniques. One specific IE
sub-task concerns the extraction of molecular
events that are mentioned in biomedical liter-
ature. In order to drive forward research in this
domain, the BioNLP Shared task 2009 (Kim
et al, 2009) concerned the extraction of such
events from text. In the course of the shared task
the organizers provided a training/development
set of abstracts for biomedical papers, annotated
with the mentioned events. Participants were
required to use this data in order to engineer
a event predictor which was then evaluated on
unseen test data.
The shared task covered three sub-tasks. The
first task concerned the extraction of events
along with their clue words and their main argu-
ments. Figure 1 shows a typical example. The
second task was an extension of the first one,
requiring participants to not only predict the
core arguments of each event, but also the cel-
lular locations the event is associated with in
the text. The events in this task were simi-
lar in nature to those in figure 1, but would
also contain arguments that are neither events
nor proteins but cellular location terms. In con-
trast to the protein terms, cellular location terms
were not given as input and had to be predicted,
too. Finally, for task 3 participants were asked
to extract negations and speculations regarding
events. However, in our work we only tackled
Task 1 and Task 2, and hence we omit further
details on Task 3 for brevity.
Our approach to biomedical event extraction
is inspired by recent work on Semantic Role La-
belling (Meza-Ruiz and Riedel, 2009; Riedel and
Meza-Ruiz, 2008) and can be characterized by
three decisions that we will illustrate in the fol-
lowing. First, we do not build a pipelined sys-
tem that first predicts event clues and cellular
locations, and then relations between these; in-
41
stead, we design and learn a joint discrimina-
tive model of the complete event structure for
a given sentence. This allows us to incorporate
global correlations between decisions in a prin-
cipled fashion. For example, we know that any
event that has arguments which itself are events
(such as the positive regulation event in figure
1) has to be a regulation event. This means that
when we make the decision about the type of
an event (e.g., in the first step of a classifica-
tion pipeline) independently from the decisions
about its arguments and their type, we run the
risk of violating this constraint. However, in a
joint model this can be easily avoided.
Our second design choice is the following: in-
stead of designing and implementing specific in-
ference and training methods for our structured
model, we useMarkov Logic, a Statistical Re-
lational Learning language, and define our global
model declaratively. This simplified the imple-
mentation of our system significantly, and al-
lowed us to construct a very competitive event
extractor in three person-months. For example,
the above observation is captured by the simple
formula:
eventType (e, t) ? role (e, a, r) ? event (a) ?
regType (t) (1)
Finally, we represent event structures as rela-
tional structures over tokens of a sentence,
as opposed to structures that explicitly mention
abstract event entities (compare figure 1 and 2).
The reason is as follows. Markov Logic, for now,
is tailored to link prediction problems where we
may make inferences about the existence of rela-
tions between given entities. However, when the
identity and number of objects of our domain is
unknown, things become more complicated. By
mapping to relational structure over grounded
text, we also show a direct connection to recent
formulations of Semantic Role Labelling which
may be helpful in the future.
The remainder of this paper is organized as
follows: we will first present the preprocessing
steps we perform (section 2), then the conversion
to a link prediction problem (section 3). Subse-
quently, we will describe Markov Logic (section
4) and our Markov Logic Network for event ex-
!"# !"$ !"%
&'()*
+,*-*+,*-*
+,*-*
1 2 3 4 5 6 7 8 9
Figure 1: Example gold annotation for task 1 of the
shared task.
1 2 3 4 5 6 7 8 9
Figure 2: Link Prediction version of the events in
figure 1.
traction (section 5). Finally, we present our re-
sults (in section 6) and conclude (section 7).
2 Preprocessing
The original data format provided by the shared
task organizers consists of (a) a collection
biomedical abstracts, and (b) standoff anno-
tation that describes the proteins, events and
sites mentioned in these abstracts. The organiz-
ers also provided a set of dependency and con-
stituent parses for the abstracts. Note that these
parses are based on a different tokenisation of the
text in the abstracts.
In our first preprocessing step we convert the
standoff annotation in the original data to stand-
off annotation for the tokenisation used in the
parses. This allows us to formulate our proba-
bilistic model in terms of one consistent tokeni-
sation (and be able to speak of token instead of
character offsets). Then we we retokenise the
input text (for the parses) according the protein
boundaries that were given in the shared task
data (in order to split strings such as p50/p55).
Finally, we use this tokenisation to once again
adapt the stand-off annotation (using the previ-
ously adapted version as input).
3 Link Prediction Representation
As we have mentioned earlier, before we learn
and apply our Statistical Relational Model, we
convert the task to link prediction over a se-
quence of tokens. In the following we will present
this transformation in detail.
42
To simplify our later presentation we will first
introduce a formal representation of the events,
proteins and locations mentioned in a sentence.
Let us simply identify both proteins and cellular
location entities with their token position in the
sentence. Furthermore, let us describe an event e
as a tuple (i, t, A) where i is the token position of
the clue word of e and t is the event type of e; A
is a set of labelled arguments (a, r) where each a
is either a protein, location or event, and r is the
role a plays with respect to e. We will identify
the set of all proteins, locations and events for a
sentence with P , L and E, respectively.
For example, in figure 1 we have P =
{4, 7} , L = ? and E = {e13, e14, e15} with
e15 = (5, gene_expr, {(4,Theme)})
e14 = (2, pos_reg, {(e15,Theme) , (7,Cause)})
e13 = (1, neg_reg, {(e14,Theme)})
3.1 Events to Links
As we mentioned in section 1, Markov Logic (or
its interpreters) are not yet able to deal with
cases where the number and identity of entities is
unknown, while relations/links between known
objects can be readily modelled. In the follow-
ing we will therefore present a mapping of an
event structure E to a labelled relation over to-
kens. Essentially, we project E to a pair (L,C)
where L is a set of labelled token-to-token links
(i, j, r), and C is a set of labelled event clues
(i, t). Note that this mapping has another ben-
efit: it creates a predicate-argument structure
very similar to most recent formulations of Se-
mantic Role Labelling (Surdeanu et al, 2008).
Hence it may be possible to re-use or adapt the
successful approaches in SRL in order to improve
bio-molecular event extraction. Since our ap-
proach is inspired by the Markov Logic role la-
beller in (Riedel and Meza-Ruiz, 2008), this work
can be seen as an attempt in this direction.
For a sentence with given P , L and E, algo-
rithm 1 presents our mapping from E to (L,C).
For brevity we omit a more detailed description
of the algorithm. Note that for our running ex-
ample eventsToLinks would return
C = {(1, neg_reg) , (2, pos_reg) , (5, gene_expr)}
(2)
Algorithm 1 Event to link conversion
/* returns all clues C and links L given
by the events in E */
1 function eventsToLinks (E):
2 C ? ?, L? ?
3 for each event (i, t, A) ? E do
4 C ? C?{(i, t)}
5 for each argument (a, r) ? A do
6 if a is an event (i?, t?, A?) do
7 L? L?{(i, i?, r)} with a = (i?, t?, A?)
8 else
9 L? L ? {(i, a, r)}
10 return (C,L)
and
L = {(1, 2,Theme) , (2, 5,Theme) ,
(2, 7,Cause) , (5, 4,Theme)} . (3)
3.2 Links to Events
The link-based representation allows us to sim-
plify the design of our Markov Logic Network.
However, after we applied the MLN to our data,
we still need to transform this representation
back to an event structure (in order to use or
evaluate it). This mapping is presented in al-
gorithm 2 and discussed in the following. Note
that we expect the relational structure L to be
cycle free. We again omit a detailed discussion of
this algorithm. However, one thing to notice is
the special treatment we give to binding events.
Roughly speaking, for the binding event clue c
we create an event with all arguments of c in
L. For a non-binding event clue c we first col-
lect all roles for c, and then create one event per
assignment of argument tokens to these roles.
If we would re-convert C and L from equation
2 and 3, respectively, we could return to our orig-
inal event structure in figure 1. However, con-
verting back and forth is not loss-free in general.
For example, if we have a non-binding event in
the original E set with two arguments A and B
with the same role Theme, the round-trip con-
version would generate two events: one with A
as Theme and one with B as Theme.
4 Markov Logic
Markov Logic (Richardson and Domingos, 2006)
is a Statistical Relational Learning language
43
Algorithm 2 link to event conversion. Assume:
no cycles; tokens can only be one of protein, site
or event; binding events have only protein argu-
ments.
/* returns all events E specified
by clues C and links L */
1 function linksToEvents (C,L)
2 return S(i,t)?C resolve (i, C, L)
/* returns all events for
the given token i */
1 function resolve (i, C, L)
2 if no t with (i, t) ? C return {i}
3 t? type (i, C)
4 if t = binding return {(i, t, A)} with
5 A = {(a, r) | (i, a, r) ? L}
6 Ri ? {r?|?a : (i, a, r) ? L}
7 for each role r ? Ri do
8 Ar ? {a| (i, a, r) ? L}
9 Br ?
S
a?Ar {(resolve (a) , r)}
10 return SA?expand(Br1 ,...,Brn ) {(i, t, A)}
/* returns all possible argument
sets for Br1 , . . . , Brn */
1 function expand (Br1 , . . . , Brn )
2 if n = 1 return Brn
3 return
S
a?Br1
S
A?expand(Br2 ,...,Brn ) {(a, r1)} ?A
based on First Order Logic and Markov Net-
works. It can be seen as a formalism that ex-
tends First Order Logic to allow formulae that
can be violated with some penalty. From an al-
ternative point of view, it is an expressive tem-
plate language that uses First Order Logic for-
mulae to instantiate Markov Networks of repet-
itive structure.
Let us introduce Markov Logic by considering
the event extraction task (as relational structure
over tokens as generated by algorithm 1). In
Markov Logic we can model this task by first
introducing a set of logical predicates such as
eventType(Token,Type), role(Token,Token,Role)
and word(Token,Word). Then we specify a set of
weighted first order formulae that define a distri-
bution over sets of ground atoms of these pred-
icates (or so-called possible worlds). Note that
we will refer predicates such as word as observed
because they are known in advance. In contrast,
role is hidden because we need to infer its ground
atoms at test time.
Ideally, the distribution we define with these
weighted formulae assigns high probability to
possible worlds where events are correctly iden-
tified and a low probability to worlds where this
is not the case. For example, in our running ex-
ample a suitable set of weighted formulae would
assign a higher probability to the world
{word (1, prevented) , eventType (1, neg_reg) ,
role(1, 2,Theme), event(2), . . .}
than to the world
{word (1, prevented) , eventType (1, binding) ,
role(1, 2,Theme), event(2), . . .}
In Markov Logic a set of weighted first order for-
mulae is called a Markov Logic Network (MLN).
Formally speaking, an MLN M is a set of pairs
(?,w) where ? is a first order formula and w a
real weigh t. M assigns the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
?
(4)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with
the constants of our domain. f?c is a feature
function that returns 1 if in the possible world y
the ground formula we get by replacing the free
variables in ? by the constants in the binding
c is true and 0 otherwise. Z is a normalisation
constant.
4.1 Inference and Learning
Assuming that we have an MLN, a set of weights
and a given sentence, we need to predict the
choice of event clues and roles with maximal
a posteriori probability (MAP). To this end
we apply a method that is both exact and ef-
ficient: Cutting Plane Inference Riedel (2008,
CPI) with Integer Linear Programming (ILP) as
base solver.
In order to learn the weights of the MLN
we use the 1-best MIRA Crammer and Singer
(2003) Online Learning method. As MAP infer-
ence method that is applied in the inner loop of
the online learner we apply CPI, again with ILP
as base solver. The loss function for MIRA is a
44
weighted sum FP +?FN where FP is the num-
ber of false positives, FN the number of false
negatives and ? = 0.01.
5 Markov Logic Network for Event
Extraction
We define four hidden predicates our task:
event(i) indicates that there is an event with
clue word i; eventType(i,t) denotes that at token
i there is an event with type t; site(i) denotes a
cellular location mentioned at token i; role(i,j,r)
indicates that token i has the argument j with
role r. In other words, the four hidden predicates
represent the set of sites L (via site), the set of
event clues C (via event and eventType) and the
set of links L (via role) presented in section 3.
There are numerous observed predicates we
use. Firstly, the provided information about
protein mentions is captured by the predicate
protein(i), indicating there is a protein mention
ending at token i. We also describe event types
and roles in more detail: regType( t) holds for
an event type t iff it is a regulation event type;
task1Role(r) and task2Role(r) hold for a role r
if is a role of task 1 (Theme, Cause) or task 2
(Site, CSite, etc.).
Furthermore, we use predicates that de-
scribe properties of tokens (such as the word
or stem of a token) and token pairs (such
as the dependency between two tokens); this
set is presented in table 1. Here the path
and pathNL predicates may need some fur-
ther explanation. When path(i,j,p,parser) is
true, there must be a labelled dependency
path p between i and j according to the
parser parser. For example, in figure 1 we
will observe path(1,5,dobj?prep_of?,mcclosky-
charniak). pathNL just omits the depen-
dency labels, leading to path(1,5,??,mcclosky-
charniak) for the same example.
We use two parses per sentence: the outputs
of a self-trained reranking parser Charniak and
Johnson (2005); McClosky and Charniak (2008)
and a CCG parser (Clark and Curran, 2007),
provided as part of the shared task dataset. As
dictionaries we use a collection of cellular lo-
cation terms taken from the Genia event cor-
pus (Kim et al, 2008), a small handpicked set of
event triggers and a list of English stop words.
Predicate Description
word(i,w) Token i has word w.
stem(i,s) i has (Porter) stem s.
pos(i,p) i has POS tag p.
hyphen(i,w) i has word w after last hyphen.
hyphenStem(i,s) i has stem s after last hyphen.
dict(i,d) i appears in dictionary d.
genia(i,p) i is event clue in the Genia
corpus with precision p.
dep(i,j,d,parser) i is head of token j with
dependency d according to
parser parser.
path(i,j,p,parser) Labelled Dependency path
according to parser parser
between tokens i and j is p.
pathNL(i,j,p,parser) Unlabelled dependency path
according to parser p between
tokens i and j is path.
Table 1: Observable predicates for token and token
pair properties.
5.1 Local Formulae
A formula is local if its groundings relate any
number of observed ground atoms to exactly one
hidden ground atom. For example, the ground-
ing
dep (1, 2, dobj, ccg) ? word (1, prevented) ?
eventType (2, pos_reg) (5)
of the local formula
dep(h, i, d, parser) ? word (h,+w) ?
eventType(i,+t) (6)
connects a single hidden eventType ground atom
with an observed word and dep atom. Note that
the + prefix for variables indicates that there is
a different weight for each possible pair of word
and event type (w, t).
5.1.1 Local Entity Formulae
The local formulae for the hidden event/1
predicate can be summarized as follows. First,
we add a event (i) formula that postulates the
existence of an event for each token. The weight
of this formulae serves as a general bias for or
against the existence of events.
45
Next, we add one formula
T (i,+t) ? event (i) (7)
for each simple token property predicate T in
table 1 (those in the first section of the table).
For example, when we plug in word for T we get
a formula that encourages or discourages the ex-
istence of an event token based on the word form
of the current token: word (i,+t) ? event (i).
We also add the formula
genia (i, p) ? event (i) (8)
and multiply the feature-weight product for each
of its groundings with the precision p. This is
corresponds to so-called real-valued feature func-
tions, and allows us to incorporate probabili-
ties and other numeric quantities in a principled
fashion.
Finally, we add a version of formula 6 where
we replace eventType(i,t) with event(i).
For the cellular location site predicate we
use exactly the same set of formulae but re-
place every occurrence of event(i) with site(i).
This demonstrates the ease with which we could
tackle task 2: apart from a small set of global
formulae we introduce later, we did not have to
do more than copy one file (the event model file)
and perform a search-and-replace. Likewise, in
the case of the eventType predicate we simply
replace event(i) with eventType(i,+t).
5.1.2 Local Link Formulae
The local formulae for the role/3 predicate
are different in nature because they assess two
tokens and their relation. However, the first for-
mula does look familiar: role (i, j,+r). This for-
mula captures a (role-dependent) bias for the ex-
istence of a role between any two tokens.
The next formula we add is
dict (i,+di) ? dict (j,+dj) ? role (i, j,+r) (9)
and assesses each combination of dictionaries
that the event and argument token are part of.
Furthermore, we add the formula
path (i, j,+p,+parser) ? role (i, j,+r) (10)
that relates the dependency path between two
tokens i and j with the role that j plays with
respect to i. We also add an unlabelled version
of this formula (using pathNL instead of path).
Finally, we add a formula
P (i, j,+p,+parser) ? T (i,+t) ?
role (i, j,+r) (11)
for each P in {path,pathNL} and T in
{word,stem,pos,dict,protein}. Note that for
T=protein we replace T (i,+t) with T (i).
5.2 Global Formulae
Global formulae relate two or more hidden
ground atoms. For example, the formula in
equation 1 is global. While local formulae can be
used in any conventional classifier (in the form
of feature functions conditioned only on the in-
put data) this does not hold for global ones.
We could enforce global constraints such as the
formula in equation 1 by building up structure
incrementally (e.g. start with one classifier for
events and sites, and then predict roles between
events and arguments with another). However,
this does not solve the typical chicken-and-egg
problem: evidence for possible arguments could
help us to predict the existence of event clues,
and evidence for events help us to predict argu-
ments. By contrast, global formulae can capture
this type of correlation very naturally.
Table 2 shows the global formulae we use. We
divide them into three parts. The first set of for-
mulae (CORE) ensures that event and eventType
atoms are consistent. In all our experiments we
will always include all CORE formulae; without
them we might return meaningless solutions that
have events with no event types, or types with-
out events.
The second set of formulae (VALID) consist
of CORE and formulae that ensure that the link
structure represents a valid set of events. For
example, this includes formula 12 that enforces
each event to have at least one theme.
Finally, FULL includes VALID and two con-
straints that are not strictly necessary to enforce
valid event structures. However, they do help us
to improve performance. Formula 14 forbids a
token to be argument of more than one event. In
fact, this formula does not hold all the time, but
46
# Formula Description
1 event (i)? ?t.eventType (i, t) If there is an event there should be an event type.
2 eventType (i, t)? event (i) If there is an event type there should be an event.
3 eventType (i, t) ? t 6= o? ?eventType (i, o) There cannot be more than one event type per token.
4 ?site (i) ? ?event (i) A token cannot be both be event and site.
5 role (i, j, r)? event (i) If j plays the role r for i then i has to be an event.
6 role (i, j, r1) ? r1 6= r2 ? ?role (i, j, r2) There cannot be more than one role per argument.
7 eventType (e, t) ? role (e, a, r) ? event (a)? regType (t) Only reg. type events can have event arguments.
9 role (i, j, r) ? taskOne (r)? event (j) ? protein (j) For task 1 roles arguments must be proteins or events
10 role (i, j, r) ? taskTwo (r)? site (j) Task 2 arguments must be cellular locations (site).
11 site (j)? ?i, r.role (i, j, r) ? taskTwo (r) Sites are always associated with an event.
12 event (i)? ?j.role (i, j,Theme) Every events need a theme.
13 eventType (i, t) ? ?allowed (t, r)? ?role (i, j, r) Certain events may not have certain roles.
14 role (i, j, r1) ? k 6= i? ?role (k, j, r2) A token cannot be argument of more than one event.
15 j < k ? i < j ? role (i, j, r1)? ?role (i, k, r2) No inside outside chains.
Table 2: All three sets of global formulae used: CORE (1-3), VALID (1-13), FULL (1-15).
by adding it we could improve performance. For-
mula 15 is our answer to a type of event chain
that earlier models would tend to produce.
Note that all formulae but formula 15 are de-
terministic. This amounts to giving them a very
high/infinite weight in advance (and not learn-
ing it during training).
6 Results
In table 3 we can see our results for task 1 and
2 of the shared task. The measures we present
here correspond to the approximate span, ap-
proximate recursive match criterion that counts
an event as correctly predicted if all arguments
are extracted and the event clue tokens approx-
imately match the gold clue tokens. For more
details on this metric we refer the reader to the
shared task overview paper.
To put our results into context: for task 1 we
reached the 4th place among 20 participants, are
in close range to place 2 and 3, and significantly
outperform the 5th best entry. Moreover, we
had highest scoring scores for task 2 with a 13%
margin to the runner-up. Using both training
and development set for training (as allowed by
the task organisers), our task 1 score rises to
45.1, slightly higher than the score of the current
third.
In terms of accuracy across different event
types our model performs worse for binding, reg-
ulation type and transcription events. Binding
events are inherently harder to correctly extract
because they often have multiple core arguments
while other non-regulation events have only one;
just missing one of the binding arguments will
lead to an event that is considered as error with
no partial credit given. If we would give credit
for binding with partially correct arguments our
F-score for binding events would rise to 49.8.
One reason why regulation events are difficult
to extract is the fact that they often have argu-
ments which themselves are events, too. In this
case our recall is bound by the recall for argu-
ment events because we can never find a regu-
lation event if we cannot predict the argument
event. Note that we are still unsure about tran-
scription events, in particular because we ob-
serve 49% F-score for such events in the devel-
opment set.
How does our model benefit from the global
formulae we describe in section 5 (and which
represent one of the core benefits of a Markov
Logic approach)? To evaluate this we compare
our FULL model with CORE and VALID from
table 2. Note that because the evaluation inter-
face rejects invalid event structures, we cannot
use the evaluation metrics of the shared task.
Instead we use table 4 to present an evaluation
in terms of ground atom F1-score for the hidden
predicates of our model. This amounts to a per-
47
Task 1 Task 2
R P F R P F
Loc 37.9 88.0 53.0 32.8 76.0 45.8
Bind 23.1 48.2 31.2 22.4 47.0 30.3
Expr 63.0 75.1 68.5 63.0 75.1 68.5
Trans 16.8 29.9 21.5 16.8 29.9 21.5
Cata 64.3 81.8 72.0 64.3 81.8 72.0
Phos 78.5 77.4 77.9 69.1 70.1 69.6
Total 48.3 68.9 56.8 46.8 67.0 55.1
Reg 23.7 40.8 30.0 22.3 38.5 28.2
Pos 26.8 42.8 32.9 26.7 42.3 32.7
Neg 27.2 40.2 32.4 26.1 38.6 31.2
Total 26.3 41.8 32.3 25.8 40.8 31.6
Total 36.9 55.6 44.4 35.9 54.1 43.1
Table 3: (R)ecall, (P)recision, and (F)-Score for task
1 and 2 in terms of event types.
role, per-site and per-event-clue evaluation. The
numbers here will not directly correspond to ac-
tual scores, but generally we can assume that if
we do better in our metrics, we will likely have
better scores.
In table 4 we notice that ensuring consistency
between all predicates has a significant impact
on the performance across the board (see the
VALID results). Furthermore, when adding ex-
tra formulae that are not strictly necessary for
consistency, but which encourage more likely
event structure, we again see significant improve-
ments (see FULL results). Interestingly, al-
though the extra formulae only directly consider
role atoms, they also have a significant impact
on event and particularly site extraction perfor-
mance. This reflects how in a joint model deci-
sions which would appear in the end of a tradi-
tional pipeline (e.g., extracting roles for events)
can help steps that would appear in the begin-
ning (extracting events and sites).
For the about 7500 sentences in the training
set we need about 3 hours on a MacBook Pro
with 2.8Ghz and 4Gb RAM to learn the weights
of our MLN. This allowed us to try different sets
of formulae in relatively short time.
7 Conclusion
Our approach the BioNLP Shared Task 2009 can
be characterized by three decisions: (a) jointly
CORE VALID FULL
eventType 52.8 63.2 64.3
role 44.0 53.5 55.7
site 42.0 46.0 51.5
Total 50.7 60.1 61.9
Table 4: Ground atom F-scores for global formulae.
modelling the complete event structure for a
given sentence; (b) using Markov Logic as gen-
eral purpose-framework in order to implement
our joint model; (c) framing the problem as a
link prediction problem between tokens of a sen-
tence.
Our results are competitive: we reach the 4th
place in task 1 and the 1st place for task 2 (with
a 13% margin). Furthermore, the declarative na-
ture of Markov Logic helped us to achieve these
results with a moderate amount of engineering.
In particular, we were able to tackle task 2 by
copying the local formulae for event prediction,
and adding three global formulae (4, 10 and 11
in table 2). Finally, our system was fast to train
(3 hours) . This greatly simplified the search for
good sets of formulae.
We have also shown that global formulae sig-
nificantly improve performance in terms of event
clue, site and argument prediction. While a sim-
ilar effect may be possible with reranking archi-
tectures, we believe that in terms of implemen-
tation efforts our approach is at least as simple.
In fact, our main effort lied in the conversion to
link prediction, not in learning or inference. In
future work we will therefore investigate means
to extend Markov Logic (interpreter) in order to
directly model event structure.
Acknowledgements
We thank Dr. Chisato Yamasaki and Dr.
Tadashi Imanishi, BIRC, AIST, for their help.
This work is supported by the Integrated
Database Project (MEXT, Japan), the Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan) and the Genome Network Project
(MEXT, Japan).
48
References
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics (ACL' 05). pages
173180.
Clark, Stephen and James R. Curran. 2007.
Wide-coverage efficient statistical parsing
with ccg and log-linear models. Comput. Lin-
guist. 33(4):493552.
Crammer, Koby and Yoram Singer. 2003. Ultra-
conservative online algorithms for multiclass
problems. Journal of Machine Learning Re-
search 3:951991.
Kim, Jin D., Tomoko Ohta, and Jun'ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformat-
ics 9(1).
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo,
Yoshinobu Kano, and Jun'ichi Tsujii. 2009.
Overview of bionlp'09 shared task on event ex-
traction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL
2009 Workshop. To appear.
McClosky, David and Eugene Charniak. 2008.
Self-training for biomedical parsing. In
Proceedings of the 46rd Annual Meeting of
the Association for Computational Linguistics
(ACL' 08).
Meza-Ruiz, Ivan and Sebastian Riedel. 2009.
Jointly identifying predicates, arguments and
senses using markov logic. In Joint Human
Language Technology Conference/Annual
Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL '09).
Richardson, Matt and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107136.
Riedel, Sebastian. 2008. Improving the accuracy
and efficiency of map inference for markov
logic. In Proceedings of the 24th Annual Con-
ference on Uncertainty in AI (UAI '08).
Riedel, Sebastian and Ivan Meza-Ruiz. 2008.
Collective semantic role labelling with markov
logic. In Proceedings of the 12th Conference
on Computational Natural Language Learning
(CoNLL' 08). pages 193197.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Llu?s M?rquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependen-
cies. In Proceedings of the 12th Conference
on Computational Natural Language Learning
(CoNLL-2008).
49
Proceedings of BioNLP Shared Task 2011 Workshop, pages 7?15,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of Genia Event Task in BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Yue Wang
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
wang@dbcls.rois.ac.jp
Toshihisa Takagi
University of Tokyo
5-1-5 Kashiwa-no-ha, Kashiwa, Chiba
tt@k.u-tokyo.ac.jp
Akinori Yonezawa
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
yonezawa@dbcls.rois.ac.jp
Abstract
The Genia event task, a bio-molecular event
extraction task, is arranged as one of the main
tasks of BioNLP Shared Task 2011. As its sec-
ond time to be arranged for community-wide
focused efforts, it aimed to measure the ad-
vance of the community since 2009, and to
evaluate generalization of the technology to
full text papers. After a 3-month system de-
velopment period, 15 teams submitted their
performance results on test cases. The re-
sults show the community has made a sig-
nificant advancement in terms of both perfor-
mance improvement and generalization.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
is a series of efforts to promote a community-
wide collaboration towards fine-grained informa-
tion extraction (IE) in biomedical domain. The
first event, BioNLP-ST 2009, introducing a bio-
molecular event (bio-event) extraction task to the
community, attracted a wide attention, with 42 teams
being registered for participation and 24 teams sub-
mitting final results (Kim et al, 2009).
To establish a community effort, the organizers
provided the task definition, benchmark data, and
evaluations, and the participants competed in devel-
oping systems to perform the task. Meanwhile, par-
ticipants and organizers communicated to develop a
better setup of evaluation, and some provided their
tools and resources for other participants, making it
a collaborative competition.
The final results enabled to observe the state-of-
the-art performance of the community on the bio-
event extraction task, which showed that the auto-
matic extraction of simple events - those with unary
arguments, e.g. gene expression, localization, phos-
phorylation - could be achieved at the performance
level of 70% in F-score, but the extraction of com-
plex events, e.g. binding and regulation, was a lot
more challenging, having achieved 40% of perfor-
mance level.
After BioNLP-ST 2009, all the resources from the
event were released to the public, to encourage con-
tinuous efforts for further advancement. Since then,
several improvements have been reported (Miwa et
al., 2010b; Poon and Vanderwende, 2010; Vlachos,
2010; Miwa et al, 2010a; Bjo?rne et al, 2010).
For example, Miwa et al (Miwa et al, 2010b)
reported a significant improvement with binding
events, achieving 50% of performance level.
The task introduced in BioNLP-ST 2009 was re-
named to Genia event (GE) task, and was hosted
again in BioNLP-ST 2011, which also hosted four
other IE tasks and three supporting tasks (Kim et al,
2011). As the sole task that was repeated in the two
events, the GE task was referenced during the devel-
opment of other tasks, and took the role of connect-
ing the results of the 2009 event to the main tasks of
2011. The GE task in 2011 received final submis-
sions from 15 teams. The results show the commu-
nity made a significant progress with the task, and
also show the technology can be generalized to full
papers at moderate cost of performance.
This paper presents the task setup, preparation,
and discusses the results.
7
Event Type Primary Argument Secondary Argument
Gene expression Theme(Protein)
Transcription Theme(Protein)
Protein catabolism Theme(Protein)
Phosphorylation Theme(Protein) Site(Entity)
Localization Theme(Protein) AtLoc(Entity), ToLoc(Entity)
Binding Theme(Protein)+ Site(Entity)+
Regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Positive regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Negative regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Table 1: Event types and their arguments for Genia event task. The type of each filler entity is specified in parenthesis.
Arguments that may be filled more than once per event are marked with ?+?.
2 Task Definition
The GE task follows the task definition of BioNLP-
ST 2009, which is briefly described in this section.
For more detail, please refer to (Kim et al, 2009).
Table 1 shows the event types to be addressed in
the task. For each event type, the primary and sec-
ondary arguments to be extracted with an event are
defined. For example, a Phosphorylation event is
primarily extracted with the protein to be phospho-
rylated. As secondary information, the specific site
to be phosphorylated may be extracted.
From a computational point of view, the event
types represent different levels of complexity. When
only primary arguments are considered, the first five
event types in Table 1 are classified as simple event
types, requiring only unary arguments. The Bind-
ing and Regulation types are more complex: Bind-
ing requires detection of an arbitrary number of ar-
guments, and Regulation requires detection of recur-
sive event structure.
Based on the definition of event types, the entire
task is divided to three sub-tasks addressing event
extraction at different levels of specificity:
Task 1. Core event extraction addresses the ex-
traction of typed events together with their pri-
mary arguments.
Task 2. Event enrichment addresses the extrac-
tion of secondary arguments that further spec-
ify the events extracted in Task 1.
Task 3. Negation/Speculation detection
addresses the detection of negations and
speculations over the extracted events.
Task 1 serves as the backbone of the GE task and is
mandatory for all participants, while the other two
are optional.
The failure of p65 translocation to the nucleus ?
Protein Localization Location
theme ToLoc
Negated
Figure 1: Event annotation example
Figure 1 shows an example of event annotation.
The event encoded in the text is represented in a
standoff-style annotation as follows:
T1 Protein 15 18
T2 Localization 19 32
T3 Entity 40 46
E1 Localization:T2 Theme:T1 ToLoc:T1
M1 Negation E1
The annotation T1 identifies the entity referred
to by the string (p65) between the character offsets,
15 and 18 to be a Protein. T2 identifies the string,
translocation, to refer to a Localization event. Enti-
ties other than proteins or event type references are
classified into a default class Entity, as in T3. E1
then represents the event defined by the three enti-
ties, as defined in Table 1. Note that for Task 1, the
entity, T3, does not need to be identified, and the
event, E1, may be identified without specification of
the secondary argument, ToLoc:T1:
E1? Localization:T2 Theme:T1
Finding the full representation of E1 is the goal of
Task 2. In the example, the localization event, E1,
is negated as expressed in the failure of . Finding the
negation, M1 is the goal of Task 3.
8
Training Devel Test
Item Abs. Full Abs. Full Abs. Full
Articles 800 5 150 5 260 4
Words 176146 29583 33827 30305 57256 21791
Proteins 9300 2325 2080 2610 3589 1712
Events 8615 1695 1795 1455 3193 1294
Gene expression 1738 527 356 393 722 280
Transcription 576 91 82 76 137 37
Protein catabolism 110 0 21 2 14 1
Phosphorylation 169 23 47 64 139 50
Localization 265 16 53 14 174 17
Binding 887 101 249 126 349 153
Regulation 961 152 173 123 292 96
Positive regulation 2847 538 618 382 987 466
Negative regulation 1062 247 196 275 379 194
Table 2: Statistics of annotations in training, development, and test sets
3 Data preparation
The data sets are prepared in two collections: the
abstract and the full text collections. The abstract
collection includes the same data used for BioNLP-
ST 2009, and is meant to be used to measure the
progress of the community. The full text collection
includes full papers which are newly annotated, and
is meant to be used to measure the generalization
of the technology to full papers. Table 2 shows the
statistics of the annotations in the GE task data sets.
Since the training data from the full text collection is
relatively small despite of the expected rich variety
of expressions in full text, it is expected that ?gener-
alization? of a model from the abstract collection to
full papers would be a key technique to get a reason-
able performance.
A full paper consists of several sections includ-
ing the title, abstract, introduction, results, conclu-
sion, methods, and so on. Different sections would
be written with different purposes, which may af-
fect the type of information that are found in the sec-
tions. Table 3 shows the distribution of annotations
in different sections. It indicates that event men-
tions, according to the event definition in Table 1, in
Methods and Captions are much less frequent than
in the other TIAB, Intro. and R/D/C sections. Fig-
ure 2 illustrates the different distribution of anno-
tated event types in the five sections. It is notable
that the Methods section (depicted in blue) shows
very different distribution compared to others: while
Gene_expression
Transcrip.
Binding
Regulation
Pos_regul.
Neg_regul.
TIAB Intro. R/D/C Methods Caption
Figure 2: Event distribution in different sections
Regulation and Positive regulation events are not as
frequent as in other sections, Negative regulation is
relatively much more frequent. It may agree with
an intuition that experimental devices, which will be
explained in Methods sections, often consists of ar-
tificial processes that are designed to cause a nega-
tive regulatory effect, e.g. mutation, addition of in-
hibitor proteins, etc. This observation suggests a dif-
ferent event annotation scheme, or a different event
extraction strategy would be required for Methods
sections.
9
Full Paper
Item Abstract Whole TIAB Intro. R/D/C Methods Caption
Words 267229 80962 3538 7878 43420 19406 6720
Proteins 14969 6580 336 597 3980 916 751
(Density: P / W) (5.60%) (8.13%) (9.50%) (7.58%) (9.17%) (4.72%) (11.18%)
Events 13603 4436 272 427 3234 198 278
(Density: E / W) (5.09%) (5.48%) (7.69%) (5.42%) (7.51%) (1.02%) (4.14%)
(Density: E / P) (90.87%) (67.42%) (80.95%) (71.52%) (81.93%) (21.62%) (37.02%)
Gene expression 2816 1193 62 98 841 80 112
Transcription 795 204 7 7 140 30 20
Protein catabolism 145 3 0 0 3 0 0
Phosphorylation 355 137 12 12 101 10 2
Localization 492 47 3 15 22 7 0
Binding 1485 380 16 74 266 6 18
Regulation 1426 371 35 30 281 4 21
Positive regulation 4452 1385 98 131 1087 15 54
Negative regulation 1637 716 39 60 520 46 51
Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstraction collection
(1210 titles and abstracts), and the following columns are of full paper collection (14 full papers). TIAB = title and
abstract, Intro. = introduction and background, R/D/C = results, discussions, and conclusions, Methods = methods,
materials, and experimental procedures. Some minor sections, supporting information, supplementary material, and
synopsis, are ignored. Density = relative density of annotation (P/W = Protein/Word, E/W = Event/Word, and E/P =
Event/Protein).
4 Participation
In total, 15 teams submitted final results. All 15
teams participated in the mandatory Task 1, four
teams in Task 2, and two teams in Task 3. Only one
team, UTurku, completed all the three tasks.
Table 4 shows the profile of the teams, except-
ing three who chose to remain anonymous. A brief
examination on the team organization (the People
column) suggests the importance of a computer sci-
ence background, C and BI, to perform the GE task,
which agrees with the same observation made in
2009. It is interpreted as follows: the role of com-
puter scientists may be emphasized in part due to
the fact that the task requires complex computational
modeling, demanding particular efforts in frame-
work design and implementation and computational
resources. The ?09 column suggests that previous
experience in the task may have affected to the per-
formance of the teams, especially in a complex task
like the GE task.
Table 5 shows the profile of the systems. A
notable observation is that four teams developed
their systems based on the model of UTurku09
(Bjo?rne et al, 2009) which was the winning sys-
tem of BioNLP-ST 2009. It may show an influence
of the BioNLP-ST series in the task. For syntac-
tic analyses, the prevailing use of Charniak John-
son re-ranking parser (Charniak and Johnson, 2005)
using the self-trained biomedical model from Mc-
Closky (2008) (McCCJ) which is converted to Stan-
ford Dependency (de Marneffe et al, 2006) is no-
table, which may also be an influence from the re-
sults of BioNLP-ST 2009. The last two teams,
XABioNLP and HCMUS, who did not use syntactic
analyses could not get a performance comparable to
the others, which may suggest the importance of us-
ing syntactic analyses for a complex IE task like GE
task.
5 Results
5.1 Task 1
Table 6 shows the final evaluation results of Task 1.
For reference, the reported performance of the two
systems, UTurku09 and Miwa10 is listed in the
top. UTurku09 was the winning system of Task 1
in 2009 (Bjo?rne et al, 2009), and Miwa10 was
the best system reported after BioNLP-ST 2009
(Miwa et al, 2010b). Particularly, the latter made
10
Team ?09 Task People reference
FAUST
?
12- 3C (Riedel et al, 2011)
UMASS
?
12- 1C (Riedel and McCallum, 2011)
UTurku
?
123 1BI (Bjrne and Salakoski, 2011)
MSR-NLP 1-- 4C (Quirk et al, 2011)
ConcordU
?
1-3 2C (Kilicoglu and Bergler, 2011)
UWMadison
?
1-- 2C (Vlachos and Craven, 2011)
Stanford 1-- 3C+1.5L (McClosky et al, 2011)
BMI@ASU
?
12- 3C (Emadzadeh et al, 2011)
CCP-BTMG
?
1-- 3BI (Liu et al, 2011)
TM-SCS 1-- 1C (Bui and Sloot, 2011)
XABioNLP 1-- 4C (Casillas et al, 2011)
HCMUS 1-- 6L (Minh et al, 2011)
Table 4: Team profiles: The ?09 column indicates whether at least one team member participated in BioNLP-ST 2009.
In People column, C=Computer Scientist, BI=Bioinformatician, B=Biologist, L=Linguist
NLP Task Other resources
Team Lexical Proc. Syntactic Proc. Trig. Arg. group Dictionary Other
FAUST SnowBall, CNLP McCCJ+SD Stacking (UMASS + Stanford)
UMASS SnowBall, CNLP McCCJ+SD Joint infer., Dual Decomposition
UTurku Porter McCCJ+SD SVM SVM SVM S. cues
MSR-NLP Porter McCCJ+SD, Enju SVM MaxEnt rules Coref(Hobbs)
ConcordU - McCCJ+SD dic rules rules S./N. cues
UWMadison Morpha, Porter MCCCJ+SD Joint infer., SEARN
Stanford Morpha, CNLP McCCJ+SD MaxEnt MSTParser word clusters
BMI@ASU Porter, WordNet Stanford+SD SVM SVM - MeSH
CCP-BTMG Porter, WordNet Stanford+SD Subgraph Isomorphism
TM-SCS Stanford Stanford dic rules rules
XABioNLP KAF - rules
HCMUS OpenNLP - dic, rules rules UIMA
Table 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization), KAF=Kyoto An-
notation Format McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, SD=Stanford Dependency
Conversion, S.=Speculation, N.=Negation
an impressive improvement with Binding events
(44.41%?52.62%).
The best performance in Task 1 this time is
achieved by the FAUST system, which adopts a
combination model of UMass and Stanford. Its
performance on the abstract collection, 56.04%,
demonstrates a significant improvement of the com-
munity in the repeated GE task, when compared to
both UTurku09, 51.95% and Miwa10, 53.29%.
The biggest improvement is made to the Regulation
events (40.11%?46.97%) which requires a com-
plex modeling for recursive event structure - an
event may become an argument of another event.
The second ranked system, UMass, shows the best
performance on the full paper collection. It suggests
that what FAUST obtained from the model combi-
nation might be a better optimization to abstracts.
The ConcordU system is notable as it is the sole
rule-based system that is ranked above the average.
It shows a performance optimized for precision with
relatively low recall. The same tendency is roughly
replicated by other rule-based systems, CCP-BTMG,
TM-SCS, XABioNLP, and HCMUS. It suggests that
a rule-based system might not be a good choice if a
high coverage is desired. However, the performance
of ConcordU for simple events suggests that a high
precision can be achieved by a rule based system
with a modest loss of recall. It might be more true
when the task is less complex.
This time, three teams achieved better results than
Miwa10, which indicates some role of focused ef-
forts like BioNLP-ST. The comparison between the
11
performance on abstract and full paper collections
shows that generalization to full papers is feasible
with very modest loss in performance.
5.2 Task 2
Tables 7 shows final evaluation results of Task 2.
For reference, the reported performance of the task-
winning system in 2009, UT+DBCLS09 (Riedel et
al., 2009), is shown in the top. The first and second
ranked system, FAUST and UMass, which share a
same author with Riedel09, made a significant
improvement over Riedel09 in the abstract col-
lection. UTurku achieved the best performance in
finding sites arguments but did not produce location
arguments. In table 7, the performance of all the
systems in full text collection suggests that finding
secondary arguments in full text is much more chal-
lenging.
In detail, a significant improvement was made for
Location arguments (36.59%?50.00%). A further
breakdown of the results of site extraction, shown
in table 8, shows that finding site arguments for
Phosphorylation, Binding and Regulation events are
all significantly improved, but in different ways.
The extraction of protein sites to be phosphory-
lated is approaching a practical level of performance
(84.21%), while protein sites to be bound or to be
regulated remains challenging to be extracted.
5.3 Task 3
Table 9 shows final evaluation results of Task 3.
For reference, the reported performance of the task-
winning system in 2009, Kilicoglu09(Kilicoglu
and Bergler, 2009), is shown in the top. Among the
two teams participated in the task, UTurku showed
a better performance in extracting negated events,
while ConcordU showed a better performance in
extracting speculated events.
6 Conclusions
The Genia event task which was repeated for
BioNLP-ST 2009 and 2011 took a role of measur-
ing the progress of the community and generaliza-
tion IE technology to full papers. The results from
15 teams who made their final submissions to the
task show that a clear advance of the community in
terms of the performance on a focused domain and
also generalization to full papers. To our disappoint-
ment, however, an effective use of supporting task
results was not observed, which thus remains as fu-
ture work for further improvement.
Acknowledgments
This work is supported by the ?Integrated Database
Project? funded by the Ministry of Education, Cul-
ture, Sports, Science and Technology of Japan.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 10?
18, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Jari Bjrne and Tapio Salakoski. 2011. Generaliz-
ing Biomedical Event Extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extracting
biological events from text using simple syntactic pat-
terns. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Arantza Casillas, Arantza Daz de Ilarraza, Koldo Go-
jenola, Maite Oronoz, and German Rigau. 2011. Us-
ing Kybots for Extracting Events in Biomedical Texts.
In Proceedings of the BioNLP 2011 Workshop Com-
panion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double Layered Learning for Bi-
ological Event Extraction from Text. In Proceedings
12
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task, pages 119?127,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
General Semantic Interpretation Approach to Biolog-
ical Event Extraction. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event Extraction as Dependency Parsing
for BioNLP 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for Biomedical Event Anno-
tation . In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwend. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Robust
Biomedical Event Extraction with Dual Decomposi-
tion and Minimal Domain Adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49, Boulder, Colorado, June.
Association for Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Andreas Vlachos and Mark Craven. 2011. Biomedical
Event Extraction from Abstracts and Full Papers using
Search-based Structured Prediction. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
13
Team Simple Event Binding Regulation All
UTurku09 A 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
Miwa10 A 70.44 52.62 40.60 48.62 / 58.96 / 53.29
W 68.47 / 80.25 / 73.90 44.20 / 53.71 / 48.49 38.02 / 54.94 / 44.94 49.41 / 64.75 / 56.04
FAUST A 66.16 / 81.04 / 72.85 45.53 / 58.09 / 51.05 39.38 / 58.18 / 46.97 50.00 / 67.53 / 57.46
F 75.58 / 78.23 / 76.88 40.97 / 44.70 / 42.75 34.99 / 48.24 / 40.56 47.92 / 58.47 / 52.67
W 67.01 / 81.40 / 73.50 42.97 / 56.42 / 48.79 37.52 / 52.67 / 43.82 48.49 / 64.08 / 55.20
UMass A 64.21 / 80.74 / 71.54 43.52 / 60.89 / 50.76 38.78 / 55.07 / 45.51 48.74 / 65.94 / 56.05
F 75.58 / 83.14 / 79.18 41.67 / 47.62 / 44.44 34.72 / 47.51 / 40.12 47.84 / 59.76 / 53.14
W 68.22 / 76.47 / 72.11 42.97 / 43.60 / 43.28 38.72 / 47.64 / 42.72 49.56 / 57.65 / 53.30
UTurku A 64.97 / 76.72 / 70.36 45.24 / 50.00 / 47.50 40.41 / 49.01 / 44.30 50.06 / 59.48 / 54.37
F 78.18 / 75.82 / 76.98 37.50 / 31.76 / 34.39 34.99 / 44.46 / 39.16 48.31 / 53.38 / 50.72
W 68.99 / 74.30 / 71.54 42.36 / 40.47 / 41.39 36.64 / 44.08 / 40.02 48.64 / 54.71 / 51.50
MSR-NLP A 65.99 / 74.71 / 70.08 43.23 / 44.51 / 43.86 37.14 / 45.38 / 40.85 48.52 / 56.47 / 52.20
F 78.18 / 73.24 / 75.63 40.28 / 32.77 / 36.14 35.52 / 41.34 / 38.21 48.94 / 50.77 / 49.84
W 59.99 / 85.53 / 70.52 29.33 / 49.66 / 36.88 35.72 / 45.85 / 40.16 43.55 / 59.58 / 50.32
ConcordU A 56.51 / 84.56 / 67.75 29.97 / 49.76 / 37.41 36.24 / 47.09 / 40.96 43.09 / 60.37 / 50.28
F 70.65 / 88.03 / 78.39 27.78 / 49.38 / 35.56 34.58 / 43.22 / 38.42 44.71 / 57.75 / 50.40
W 59.67 / 80.95 / 68.70 29.33 / 49.66 / 36.88 34.10 / 49.46 / 40.37 42.56 / 61.21 / 50.21
UWMadison A 54.99 / 79.85 / 65.13 34.87 / 56.81 / 43.21 34.54 / 50.67 / 41.08 42.17 / 62.30 / 50.30
F 74.03 / 83.58 / 78.51 15.97 / 29.87 / 20.81 33.11 / 46.87 / 38.81 43.53 / 58.73 / 50.00
W 65.79 / 76.83 / 70.88 39.92 / 49.87 / 44.34 27.55 / 48.75 / 35.21 42.36 / 61.08 / 50.03
Stanford A 62.61 / 77.57 / 69.29 42.36 / 54.24 / 47.57 28.25 / 49.95 / 36.09 42.55 / 62.69 / 50.69
F 75.58 / 75.00 / 75.29 34.03 / 40.16 / 36.84 26.01 / 46.08 / 33.25 41.88 / 57.36 / 48.41
W 62.09 / 76.55 / 68.57 27.90 / 44.92 / 34.42 22.30 / 40.26 / 28.70 36.91 / 56.63 / 44.69
BMI@ASU A 58.71 / 78.51 / 67.18 26.22 / 47.40 / 33.77 22.99 / 40.47 / 29.32 36.61 / 57.82 / 44.83
F 72.47 / 72.09 / 72.28 31.94 / 40.71 / 35.80 20.78 / 39.74 / 27.29 37.65 / 53.93 / 44.34
W 53.61 / 75.13 / 62.57 22.61 / 49.12 / 30.96 19.01 / 43.80 / 26.51 31.57 / 58.99 / 41.13
CCP-BTMG A 50.93 / 74.50 / 60.50 25.65 / 53.29 / 34.63 19.54 / 43.47 / 26.96 31.87 / 59.02 / 41.39
F 61.82 / 76.77 / 68.49 15.28 / 37.29 / 21.67 17.83 / 44.63 / 25.48 30.82 / 58.92 / 40.47
W 57.33 / 71.34 / 63.57 34.01 / 44.77 / 38.66 16.39 / 25.37 / 19.91 32.73 / 45.84 / 38.19
TM-SCS A 53.65 / 71.66 / 61.36 36.02 / 49.41 / 41.67 18.29 / 27.07 / 21.83 33.36 / 47.09 / 39.06
F 68.57 / 70.59 / 69.57 29.17 / 35.00 / 31.82 12.20 / 21.02 / 15.44 31.14 / 42.83 / 36.06
W 43.71 / 47.18 / 45.38 05.30 / 50.00 / 09.58 05.79 / 26.94 / 09.54 19.07 / 42.08 / 26.25
XABioNLP A 39.76 / 45.90 / 42.61 06.34 / 56.41 / 11.40 04.72 / 23.21 / 07.84 17.91 / 40.74 / 24.89
F 55.84 / 50.23 / 52.89 02.78 / 30.77 / 05.10 08.18 / 33.89 / 13.17 21.96 / 45.09 / 29.54
W 24.82 / 35.14 / 29.09 04.68 / 12.92 / 06.88 01.63 / 10.40 / 02.81 10.12 / 27.17 / 14.75
HCMUS A 22.42 / 37.38 / 28.03 04.61 / 10.46 / 06.40 01.69 / 10.37 / 02.91 09.71 / 27.30 / 14.33
F 32.21 / 31.16 / 31.67 04.86 / 28.00 / 08.28 01.47 / 10.48 / 02.59 11.14 / 26.89 / 15.75
Table 6: Evaluation results (recall / precision / f-score) of Task 1 in (W)hole data set, (A)bstracts only, and (F)ull
papers only. Some notable figures are emphasized in bold.
14
Team Sites (222) Locations (66) All (288)
UT+DBCLS09 A 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52
W 32.88 / 70.87 / 44.92 36.36 / 75.00 / 48.98 33.68 / 71.85 / 45.86
FAUST A 43.51 / 71.25 / 54.03 36.92 / 77.42 / 50.00 41.33 / 72.97 / 52.77
F 17.58 / 69.57 / 28.07 - 17.39 / 66.67 / 27.59
W 31.98 / 71.00 / 44.10 36.36 / 77.42 / 49.48 32.99 / 72.52 / 45.35
UMass A 42.75 / 70.00 / 53.08 36.92 / 77.42 / 50.00 40.82 / 72.07 / 52.12
F 16.48 / 75.00 / 27.03 - 16.30 / 75.00 / 26.79
W 32.88 / 62.93 / 43.20 22.73 / 83.33 / 35.71 30.56 / 65.67 / 41.71
BMI@ASU A 37.40 / 67.12 / 48.04 23.08 / 83.33 / 36.14 32.65 / 70.33 / 44.60
F 26.37 / 55.81 / 35.82 - 26.09 / 55.81 / 35.56
W 40.09 / 65.44 / 49.72 00.00 / 00.00 / 00.00 30.90 / 65.44 / 41.98
UTurku A 48.09 / 69.23 / 56.76 00.00 / 00.00 / 00.00 32.14 / 69.23 / 43.90
F 28.57 / 57.78 / 38.24 - 28.26 / 57.78 / 37.96
Table 7: Evaluation results of Task 2 in (W)hole data set, (A)bstracts only, and (F)ull papers only
Team Phospho. (67) Binding (84) Reg. (71)
Riedel?09 A 71.43 / 71.43 / 71.43 04.76 / 50.00 / 08.70 12.96 / 58.33 / 21.21
W 71.64 / 84.21 / 77.42 05.95 / 38.46 / 10.31 28.17 / 60.61 / 38.46
FAUST A 71.43 / 81.63 / 76.19 04.76 / 14.29 / 07.14 29.63 / 66.67 / 41.03
F 72.73 / 100.0 / 84.21 06.35 / 66.67 / 11.59 23.53 / 44.44 / 30.77
W 76.12 / 79.69 / 77.86 04.76 / 36.36 / 08.42 22.54 / 64.00 / 33.33
UMass A 76.79 / 76.79 / 76.79 04.76 / 14.29 / 07.14 22.22 / 70.59 / 33.80
F 72.73 / 100.0 / 84.21 04.76 / 75.00 / 08.96 23.53 / 50.00 / 32.00
W 52.24 / 97.22 / 67.96 20.24 / 53.12 / 29.31 29.58 / 43.75 / 35.29
BMI@ASU A 53.57 / 96.77 / 68.97 09.52 / 22.22 / 13.33 31.48 / 51.52 / 39.08
F 45.45 / 100.0 / 62.50 23.81 / 65.22 / 34.88 23.53 / 26.67 / 25.00
W 76.12 / 91.07 / 82.93 21.43 / 51.43 / 30.25 28.17 / 44.44 / 34.48
UTurku A 78.57 / 89.80 / 83.81 09.52 / 18.18 / 12.50 31.48 / 54.84 / 40.00
F 63.64 / 100.0 / 77.78 25.40 / 66.67 / 36.78 17.65 / 21.43 / 19.35
Table 8: Evaluation results of Site information for different event types in (A)bstracts
Team Negation Speculation All
Kilicoglu09 A 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27 15.86 / 50.74 / 24.17
W 22.87 / 48.85 / 31.15 17.86 / 32.54 / 23.06 20.30 / 39.67 / 26.86
UTurku A 22.03 / 49.02 / 30.40 19.23 / 38.46 / 25.64 20.69 / 43.69 / 28.08
F 25.76 / 48.28 / 33.59 15.00 / 23.08 / 18.18 19.28 / 30.85 / 23.73
W 18.77 / 44.26 / 26.36 21.10 / 38.46 / 27.25 19.97 / 40.89 / 26.83
ConcordU A 18.06 / 46.59 / 26.03 23.08 / 40.00 / 29.27 20.46 / 42.79 / 27.68
F 21.21 / 38.24 / 27.29 17.00 / 34.69 / 22.82 18.67 / 36.14 / 24.63
Table 9: Evaluation results of Task 3 in (W)hole data set, (A)bstracts only, and (F)ull papers only
15
