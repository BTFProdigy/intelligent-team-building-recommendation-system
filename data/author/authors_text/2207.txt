Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 313?320
Manchester, August 2008
Tracking the Dynamic Evolution of Participant Salience in a Discussion
Ahmed Hassan
University of Michigan
hassanam@umich.edu
Anthony Fader
University of Michigan
afader@umich.edu
Michael H. Crespin
University of Georgia
crespin@uga.edu
Kevin M. Quinn
Harvard University
kquinn@fsa.harvard.edu
Burt L. Monroe
Pennsylvania State University
burtmonroe@psu.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a technique for analyzing the
temporal evolution of the salience of par-
ticipants in a discussion. Our method can
dynamically track how the relative impor-
tance of speakers evolve over time using
graph based techniques. Speaker salience
is computed based on the eigenvector cen-
trality in a graph representation of partici-
pants in a discussion. Two participants in a
discussion are linked with an edge if they
use similar rhetoric. The method is dy-
namic in the sense that the graph evolves
over time to capture the evolution inher-
ent to the participants salience. We used
our method to track the salience of mem-
bers of the US Senate using data from the
US Congressional Record. Our analysis
investigated how the salience of speakers
changes over time. Our results show that
the scores can capture speaker centrality
in topics as well as events that result in
change of salience or influence among dif-
ferent participants.
1 Introduction
There are several sources of data that record
speeches or participations in debates or discus-
sions among a group of speakers or participants.
Those include parliamentary records, blogs, and
news groups. This data represents a very important
and unexploited source of information that con-
tains several trends and ideas. In any debate or
discussion, there are certain types of persons who
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
influence other people and pass information or ad-
vice to them. Those persons are often regarded
as experts in the field or simply influential peo-
ple and they tend to affect the ideas and rhetoric
of other participants. This effect can be tracked
down by tracking the similarity between different
speeches. We can then imagine a debate with many
people arguing about many different things as a
network of speeches or participations interacting
with each other. We can then try to identify the
most salient or important participants by identify-
ing the most central speeches in this network and
associating them with their speakers. When we
have a large dataset of debates and conversations
that expand over a long period of time, the salience
of participants becomes a dynamic property that
changes over time. To capture this dynamic nature
of the process, the graph of speeches must evolve
over time such that we have a different graph at
each instance of time that reflects the interaction
of speeches at this instant.
We apply our method to the US Congressional
Record. The US Congressional Record documents
everything said and done in the US Congress
House and Senate. The speeches in this data set
are made by a large number of people over a long
period of time. Using political speeches as test
data for the proposed method adds an extra layer
of meaning onto the measure of speakers salience.
Speaker salience of the Congress members can re-
flect the importance or influence in the US leg-
islative process. The way salience scores evolve
over time can answer several interesting issues like
how the influence of the speakers vary with major-
ity status and change of party control. It can also
study the dynamics of the relative distribution of
attention to each topic area in different time peri-
ods.
313
The rest of this paper will proceed as follows.
Section 2 reviews some related work. In Section 3,
we describe how the data can be clustered into dif-
ferent topic clusters. In Section 4, we describe
our method for computing the salience of different
participant in a discussion, we also describe how
to the network of speakers varies over time. Sec-
tion 5 describes the experimental setup. Finally,
we present the conclusions in Section 6.
2 Related Work
Several methods have been proposed for identify-
ing the most central nodes in a network. Degree
centrality, closeness, and betweenness (Newman,
2003) are among the most known methods for
measuring centrality of nodes in a network. Eigen-
vector centrality is another powerful method that
that has been applied to several types of networks.
For example it has been used to measure cen-
trality in hyperlinked web pages networks (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et
al., 2004).
The interest of applying natural language pro-
cessing techniques in the area of political science
has been recently increasing.
(Quinn et al, 2006) introduce a multinomial
mixture model to cluster political speeches into
topics or related categories. In (Porter et al, 2005),
a network analysis of the members and committees
of the US House of Representatives is performed.
The authors prove that there are connections link-
ing some political positions to certain committees.
This suggests that there are factors affecting com-
mittee membership and that they are not deter-
mined at random. In (Thomas et al, 2006), the au-
thors try to automatically classify speeches, from
the US Congress debates, as supporting or oppos-
ing a given topic by taking advantage of the voting
records of the speakers. (Fader et al, 2007) in-
troduce MavenRank , which is a method based on
lexical centrality that identifies the most influen-
tial members of the US Senate. It computes a sin-
gle salience score for each speaker that is constant
over time.
In this paper, we introduce a new method for
tracking the evolution of the salience of partici-
pants in a discussion over time. Our method is
based on the ones described in (Erkan and Radev,
2004; Mihalcea and Tarau, 2004; Fader et al,
2007), The objective of this paper is to dynami-
cally rank speakers or participants in a discussion.
The proposed method is dynamic in the sense that
the computed importance varies over time.
3 Topic Clusters
Before applying the proposed method to a data
set with speeches in multiple topics, we first need
to divide the speech documents into topic clus-
ters. We used the model described in (Quinn et al,
2006) for this purpose. The model presented in this
paper assumes that the probabilities of a document
belonging to a certain topic varies smoothly over
time and the words within a given document have
exactly the same probability of being drawn from
a particular topic (Quinn et al, 2006). These two
properties make the model different than standard
mixture models (McLachlan and Peel, 2000) and
the latent Dirichlet alocation model of (Blei et al,
2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006).
The output from the topic model is a D ? K
matrix Z where D is the number of speeches , K
is the number of topics and the element z
dk
repre-
sents the probability of the dth speech being gen-
erated by topic k. We then assign each speech d
to the kth cluster where k = argmax
j
z
dj
. If the
maximum value is not unique, one of the clusters
having the maximum value is arbitrary selected.
4 Speaker Centrality
In this section we describe how to build a network
of speeches and use it to identify speaker centrality.
We also describe how to generate different projec-
tions of the network at different times, and how
to use those projection to get dynamic salience
scores.
4.1 Computing Speaker Salience
The method we used is similar to the methods de-
scribed in (Erkan and Radev, 2004; Mihalcea and
Tarau, 2004; Kurland and Lee, 2005), which were
originally used for ranking sentences and docu-
ments in extractive summarization and information
retrieval systems.
A collection of speeches can be represented as
a network where similar speeches are linked to
each other. The proposed method is based on
314
the premise that important speeches tend to be
lexically similar to other important speeches, and
important speeches tend to belong to important
speakers. Hence given a collection of speeches and
a similarity measure, we can build a network and
define the centrality score of a speech recursively
in terms of the scores of other similar speeches.
Later, we can compute the salience of a speaker
as the sum of the centrality measure of all his
speeches.
To measure the similarity between two
speeches, we use the bag-of-words model to repre-
sent each sentence as an N-dimensional vector of
tf-idf scores, where N is the number of all possible
words in the target language. The similarity
between two speeches is then computed using the
cosine similarity between the two vectors.
A vector of term frequencies is used to represent
each speech. Those term frequencies are weighted
according to the relative importance of the given
term in the cluster.
The vectors representing speeches contain term
frequencies (or tf), which are weighted according
to their inverse document frequencies to account
for the relative importance of the given term in the
cluster. The inverse document frequency of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
n
w
)
(1)
where n
w
is the number of speeches in the clus-
ter containing the term w, and N is the number of
documents in the cluster. We calculated idf values
specific to each topic, rather than to all speeches.
We preferred to use topic-specific idf values be-
cause the relative importance of words may vary
from one topic to the other.
The tf-idf cosine similarity measure is computed
as the cosine of the angle between the tf-idf vec-
tors. It is defined as follows:
P
w?u,v
tf
u
(w) tf
v
(w) idf(w)
2
?
P
w?u
(tf
u
(w) idf(w))
2
?
P
w?v
(tf
v
(w) idf(w))
2
, (2)
The choice of tf-idf scores to measure speech
similarity is an arbitrary choice. Some other possi-
ble similarity measures are edit distance, language
models (Kurland and Lee, 2005), or generation
probabilities (Erkan, 2006).
The recursive definition of the score of any
speech s in the speeches network is given by
p(s) =
?
t?adj[s]
p(t)
deg(t)
(3)
where deg(t) is the degree of node t, and adj[s] is
the set of all speeches adjacent to s in the network.
This can be rewritten in matrix notation as:
p = pB (4)
where p = (p(s
1
), p(s
2
), . . . , p(s
N
)) and the ma-
trix B is the row normalized similarity matrix of
the graph
B(i, j) =
S(i, j)
?
k
S(i, k)
(5)
where S(i, j) = sim(s
i
, s
j
). Equation (4) shows
that the vector of salience scores p is the left eigen-
vector of B with eigenvalue 1.
The matrix B can be thought of as a stochastic
matrix that acts as transition matrix of a Markov
chain. An element X(i, j) of a stochastic matrix
specifies the transition probability from state i to
state j in the corresponding Markov chain. And
the whole process can be seen as a Markovian ran-
dom walk on the speeches graph. To help the ran-
dom walker escape from periodic or disconnected
components, (Brin and Page, 1998) suggests re-
serving a small escape probability at each node
that represents a chance of jumping to any node
in the graph, making the Markov chain irreducible
and aperiodic, which guarantees the existence of
the eigenvector.
Equation (4) can then be rewritten, assuming a
uniform escape probability, as:
p = p[dU+ (1 ? d)B] (6)
where N is the total number of nodes, U is a
square matrix with U(i, j) = 1/N for all i, j, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998).
4.2 Dynamic Salience Scores
We use the time stamps associated with the data to
compute dynamic salience scores p
T
(u) that iden-
tify central speakers at some time T . To do this,
we create a speech graph that evolves over time.
Let T be the current date and let u and v be two
speech documents that occur on days t
u
and t
v
.
Our goal is to discount the lexical similarity of u
and v based on how far apart they are. One way
to do this is by defining a new similarity measure
s(u, v;T ) as:
s(u, v;T ) = tf-idf-cosine(u, v) ? f(u, v;T ) (7)
315
where f(u, v;T ) is a function taking values in
[0, 1].
If f(u, v;T ) = 1 for all u, v, and T , then time is
ignored when calculating similarity and p
T
(u) =
p(u). On the other hand, suppose we let
f(u, v;T ) =
{
1 if t
u
= t
v
= T ,
0 else.
(8)
This removes all edges that link a speech, occur-
ring at some time T , to all other speeches occur-
ring at some time other than T and the ranking al-
gorithm will be run on what is essentially the sub-
graph of documents restricted to time T (although
the isolated speech documents will receive small
non-zero scores because of the escape probability
from Section 4.1). These two cases act as the ex-
treme boundaries of possible functions f : in the
first case time difference has no effect on document
similarity, while in the second case two documents
must occur on the same day to be similar.
We use the following time weight functions in
our experiments. In each case, we assume that the
speeches represented by speech documents u and v
have already occurred, that is, t
u
, t
v
? T . We will
use the convention that f(u, v;T ) = 0 if t
u
> T
or t
v
> T for all time weight functions, which
captures the idea that speeches that have not yet
occurred have no influence on the graph at time T .
Also define
age(u, v;T ) = T ? min{t
u
, t
v
} (9)
which gives the age of the oldest speech document
from the pair u, v at time T .
? Exponential: Given a parameter a > 0, define
f
exp,a
(u, v;T ) = e
?a age(u,v;T )
. (10)
This function will decrease the impact of sim-
ilarity as time increases in an exponential
fashion. a is a parameter that controls how
fast this happens, where a larger value of a
makes earlier speeches have a small impact
on current scores and a smaller value of a
means that earlier speeches will have a larger
impact on current scores.
? Linear: Given b > 0, define
f
lin,d
(u, v;T ) =
?
?
?
?
?
1 ?
1
b
age(u, v;T )
if age(u, v;T ) ? b
0 if age(u, v;T ) > b
(11)
Figure 1: The Dynamic boundary cases for Sena-
tor Santorum.
This function gives speech documents that
occur at time T full weight and then decreases
their weight linearly towards time T + b,
where it becomes 0.
? Boundary: Given d ? 0, define
f
bnd,d
(u, v;T ) =
{
1 if age(u, v;T ) ? d
0 if age(u, v;T ) > d
(12)
This function gives speech documents occur-
ring within d days of T the regular tf-idf sim-
ilarity score, but sets the similarity of speech
documents occurring outside of d days to 0.
The case when d = 0 is one of the boundary
cases explained above.
Figure 1 gives an example of different time
weighting functions for Senator Rick Santorum
(R - Pennsylvania) on topic 22 (Abortion) during
1997, the first session of the 105th Congress. The
dashed line shows the case when time has no ef-
fect on similarity (his score is constant over time),
while the solid line shows the case where only
speeches on the current day are considered simi-
lar (his score spikes only on days where he speaks
and is near zero otherwise). The dotted line shows
the case when the influence of older speeches de-
creases exponentially, which is more dynamic than
the first case but smoother than the second case.
5 Experiments and Results
5.1 Data
We used the United States Congressional Speech
corpus (Monroe et al, 2006) in our experiment.
316
This corpus is in XML formatted version of the
electronic United States Congressional Record
from the Library of Congress
1
. The Congressional
Record is a verbatim transcript of the speeches
made in the US House of Representatives and Sen-
ate and includes tens of thousands of speeches per
year (Monroe et al, 2006). The data we used cover
the period from January 2001 to January 2003.
5.2 Experimental Setup
We used results from (Quinn et al, 2006) to get
topic clusters from the data, as described in Sec-
tion 3. The total number of topics was 42. The
average sized topic cluster had several hundred
speech documents (Quinn et al, 2006).
We set up a pipeline using a Perl implementa-
tion of the proposed method We ran it on the topic
clusters and ranked the speakers based on the cen-
trality scores of their speeches. The graph nodes
were speech documents. A speaker?s score was
determined by the average of the scores of the
speeches given by that speaker. After comparing
the different time weighting function as shown in
Figure 1, we decided to use the exponential time
weight function for all the experiments discussed
below. Exponential time weighting function de-
creases the impact of similarity as time increases
in an exponential fashion. It also allows us to con-
trol the rate of decay using the parameter a.
5.3 Baseline
We compare the performance of our system to
a simple baseline that calculates the salience of
a speaker as a weighted count of the number of
times he has spoken. The baseline gives high
weight to recent speeches . The weight decreases
as the speeches gets older. The salience score of a
speaker is calculate as follows:
BS(i) =
?
d
?
d
0
?d
? S
i
d
(13)
Where BS(i) is the baseline score of speaker i,
? is the discounting factor, d
0
is the current date,
and S
i
d
is the number of speeches made by speaker
i at date d. We used ? = 0.9 for all our experi-
ments.
5.4 Results
One way to evaluate the dynamic salience scores,
is to look at changes when party control of the
1
http://thomas.loc.gov
chamber switches. Similar to (Hartog and Mon-
roe, 2004), we exploit the party switch made by
Senator Jim Jeffords of Vermont and the result-
ing change in majority control of the Senate dur-
ing the 107th Congress as a quasi-experimental
design. In short, Jeffords announced his switch
on May 24, 2001 from Republican to Independent
status, effective June 6, 2001. Jeffords stated that
he would vote with the Democrats to organize the
Senate, giving the Democrats a one-seat advantage
and change control of the Senate from the Repub-
licans back to the Democrats. This change of ma-
jority status during the 107th Congress allows us
to ignore many of the factors that could potentially
influence dynamic salience scores at the start of a
new congress.
On average, we expect committee chairs or a
member of the majority party to be the most im-
portant speaker on each topic followed by ranking
members or a member of the minority party. If
our measure is capturing dynamics in the central-
ity of Senators, we expect Republicans to be more
central before the Jeffords switch and Democrats
becoming central soon afterwards, assuming the
topic is being discussed on the Senate floor. We
show that the proposed technique captures several
interesting events in the data and also show that the
baseline explained above fails to capture the same
set of events.
Figure 2(a) shows the dynamic salience scores
over time for Senator John McCain (R - Arizona)
and Senator Carl Levin (D - Michigan) on topic
5 (Armed Forces 2) for the 107th Senate. Mc-
Cain was the most salient speaker for this topic
until June 2001. Soon after the change in major-
ity status a switch happened and Levin, the new
chair of Senate Armed Services, replaced McCain
as the most salient speaker. On the other hand,
Figure 2(b) shows the baseline scores for the same
topic and same speakers. We notice here that the
baseline failed to capture the switch of salience
near June 2001.
We can also observe similar behavior in Fig-
ure 3(a). This figure shows how Senate Majority
Leader Trent Lott (R - Mississippi) was the most
salient speaker on topic 35 (Procedural Legisla-
tion) until July 2001. Topic 35 does not map to
a specific committee but rather is related to ma-
neuvering bills through the legislative process on
the floor, a job generally delegated to members in
the Senate leadership. Just after his party gained
317
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2 (Infra
structure
)??, Expo
nential, a
=0.02, th
=
MCCA
IN
LEVIN
CARL
(a) Dynamic Lexrank
 
0
 
2
 
4
 
6
 
8
 
10 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
MCCA
IN
LEVIN
CARL
(b) Baseline
Figure 2: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2).
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Proce
dural 4
 (Legisla
ton 2)??, E
xponenti
al, a=0.0
2, th= REID LOTT
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Proced
ural 4 (L
egislaton
 2)??
REID LOTT
(b) Baseline
Figure 3: The Switch of Speakers Salience near Jun 2001 for Topic 35(Procedural Legislation).
majority status, Senator Harry Reid (D - Nevada)
became the most salient speaker for this topic. This
is consistent with Reid?s switch from Assistant mi-
nority Leader to Assistant majority Leader. Again
the baseline scores for the same topic and speakers
in Figure 3(b) fails to capture the switch.
An even more interesting test would be to check
whether the Democrats in general become more
central than Republicans after the Jeffords switch.
Figure 4(a) shows the normalized sum of the
scores of all the Democrats and all the Republicans
on topic 5 (Armed Forces 2) for the 107th Senate.
The figure shows how the Republicans were most
salient until soon after the Jeffords switch when the
Democrats regained the majority and became more
salient. We even discovered similar behavior when
we studied how the average salience of Democrats
and Republicans change across all topics. This is
shown in Figure 5(a) where we can see that the
Republicans were more salient on average for all
topics until June 2001. Soon after the change in
majority status, Democrats became more central.
Figures 4(b) and 5(b) show the same results using
the baseline system. We notice that the number of
speeches made by the Democrats and the Repub-
licans is very similar in most of the times. Even
when one of the parties has more speeches than
the other, it does not quite reflect the salience of
the speakers or the parties in general.
An alternative approach to evaluate the dynamic
scores is to exploit the cyclical nature of the leg-
islative process as some bills are re-authorized on
a fairly regular time schedule. For example, the
farm bill comes due about every five years. As a
new topic is coming up for debate, we expect the
saliency scores for relevant legislators to increase.
Figure 6 shows the dynamic scores of Senator
Thomas Harkin (D - Iowa), and Senator Richard
318
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2??
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20
 
25
 
30 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
Democ
rates
Repub
licans
(b) Baseline
Figure 4: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2), Republicans vs
Democrats.
 
10
 
12
 
14
 
16
 
18
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
100
 
200
 
300
 
400
 
500
 
600
 
700 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107
Democ
rates
Repub
licans
(b) Baseline
Figure 5: The Switch of Speakers Salience near Jun 2001 for All Topics, Republicans vs Democrats.
Lugar (R - Indiana) during the 107th senate on
topic 24 (Agriculture). The two senators were
identified, by the proposed method, as the most
salient speakers for this topic, as expected, since
they both served as chairmen of the Senate Com-
mittee on Agriculture, Nutrition, and Forestry
when their party was in the majority during the
107th Senate. This committee was in charge of
shepherding the Farm Bill through the Senate. The
scores of both senators on the agriculture topic sig-
nificantly increased starting late 2001 until June
2002. The debate began on the bill starting in
September of 2001 and it was not passed until May
2002.
6 Conclusion
We presented a graph based method for analyz-
ing the temporal evolution of the salience of par-
ticipants in a discussion. We used this method to
track the evolution of salience of speakers in the
US Congressional Record. We showed that the
way salience scores evolve over time can answer
several interesting issues. We tracked how the in-
fluence of the speakers vary with majority status
and change of party control. We also show how
a baseline system that depends on the number of
speeches fails to capture the interesting events cap-
tured by the proposed system. We also studied the
dynamics of the relative distribution of attention to
each topic area in different time periods and cap-
tured the cyclical nature of the legislative process
as some bills are re-authorized on a fairly regular
time schedule.
319
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Agric
ulture??
, Expon
ential, 
a=0.02
, th= LUG
AR HARKI
N
Figure 6: The Farm Bill Discussions on the Rela-
tive Distribution of Attention to Topic 24 (Agricul-
ture).
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
References
Blei, David and John Lafferty. 2006. Dynamic topic
models. In ICML 2006.
Blei, David, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
CNIS, 30(1?7):107?117.
Erkan, G?unes? and Dragomir Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Erkan, Gunes. 2006. Language model-based document
clustering using random walks. In HLT/NAACL
2006, pages 479?486. Association for Computa-
tional Linguistics.
Fader, Anthony, Dragomir Radev, Michael Crespin,
Burt Monroe, Kevin Quinn, and Michael Colaresi.
2007. Mavenrank: Identifying influential members
of the us senate using lexical centrality. In EMNLP
2007.
Hartog, Chris Den and Nathan Monroe. 2004. The
value of majority status: The effect of jeffords?s
switch on asset prices of republican and democratic
firms. Legislative Studies Quarterly, 33:63?84.
Kleinberg, Jon. 1998. Authoritative sources in a hyper-
linked environment. In the ACM-SIAM Symposium
on Discrete Algorithms, pages 668?677.
Kurland, Oren and Lillian Lee. 2005. PageRank with-
out hyperlinks: Structural re-ranking using links in-
duced by language models. In SIGIR 2005, pages
306?313.
Kurland, Oren and Lillian Lee. 2006. Respect my au-
thority! HITS without hyperlinks, utilizing cluster-
based language models. In SIGIR 2006, pages 83?
90.
McLachlan, Geoffrey and David Peel. 2000. Finite
Mixture Models. New York: Wiley.
Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In EMNLP 2004.
Mihalcea, Rada, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application
to word sense disambiguation. In COLING 2004,
pages 1126?1132.
Monroe, Burt, Cheryl Monroe, Kevin Quinn, Dragomir
Radev, Michael Crespin, Michael Colaresi, Anthony
Fader, Jacob Balazer, and Steven Abney. 2006.
United states congressional speech corpus. Depart-
ment of Political Science, The Pennsylvania State
University.
Newman, Mark. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Porter, Mason, Peter Mucha, Miark Newman, and
Casey Warmbrand. 2005. A network analysis of
committees in the U.S. House of Representatives.
PNAS, 102(20).
Quinn, Kevin, Burt Monroe, Michael Colaresi, Michael
Crespin, and Dragomir Radev. 2006. An automated
method of topic-coding legislative speech over time
with application to the 105th-108th U.S. senate. In
Midwest Political Science Association Meeting.
Sparck-Jones, Karen. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28(1):11?20.
Thomas, Matt, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
320
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 658?666, Prague, June 2007. c?2007 Association for Computational Linguistics
MavenRank: Identifying Influential Members of the US Senate Using
Lexical Centrality
Anthony Fader
University of Michigan
afader@umich.edu
Dragomir Radev
University of Michigan
radev@umich.edu
Michael H. Crespin
The University of Georgia
crespin@uga.edu
Burt L. Monroe
The Pennsylvania State University
burtmonroe@psu.edu
Kevin M. Quinn
Harvard University
kevin quinn@harvard.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Abstract
We introduce a technique for identifying the
most salient participants in a discussion. Our
method, MavenRank is based on lexical cen-
trality: a random walk is performed on a
graph in which each node is a participant in
the discussion and an edge links two partici-
pants who use similar rhetoric. As a test, we
used MavenRank to identify the most influ-
ential members of the US Senate using data
from the US Congressional Record and used
committee ranking to evaluate the output.
Our results show that MavenRank scores are
largely driven by committee status in most
topics, but can capture speaker centrality in
topics where speeches are used to indicate
ideological position instead of influence leg-
islation.
1 Introduction
In a conversation or debate between a group of
people, we can think of two remarks as interact-
ing if they are both comments on the same topic.
For example, if one speaker says ?taxes should
be lowered to help business,? while another argues
?taxes should be raised to support our schools,? the
speeches are interacting with each other by describ-
ing the same issue. In a debate with many people
arguing about many different things, we could imag-
ine a large network of speeches interacting with each
other in the same way. If we associate each speech
in the network with its speaker, we can try to iden-
tify the most important people in the debate based
on how central their speeches are in the network.
To describe this type of centrality, we borrow a
term from The Tipping Point (Gladwell, 2002), in
which Gladwell describes a certain type of person-
ality in a social network called a maven. A maven
is a trusted expert in a specific field who influences
other people by passing information and advice. In
this paper, our goal is to identify authoritative speak-
ers who control the spread of ideas within a topic. To
do this, we introduce MavenRank, which measures
the centrality of speeches as nodes in the type of net-
work described in the previous paragraph.
Significant research has been done in the area
of identifying central nodes in a network. Vari-
ous methods exist for measuring centrality, includ-
ing degree centrality, closeness, betweenness (Free-
man, 1977; Newman, 2003), and eigenvector cen-
trality. Eigenvector centrality in particular has
been successfully applied to many different types
of networks, including hyperlinked web pages (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et al,
2004). The authors of (Lin and Kan, 2007) extended
these methods to include timestamped graphs where
nodes are added over time and applied it to multi-
document summarization. In (Tong and Faloutsos,
2006), the authors use random walks on a graph as
a method for finding a subgraph that best connects
some or all of a set of query nodes. In our paper,
we introduce a new application of eigenvector cen-
trality for identifying the central speakers in the type
of debate or conversation network described above.
Our method is based on the one described in (Erkan
658
and Radev, 2004) and (Mihalcea and Tarau, 2004),
but modified to rank speakers instead of documents
or sentences.
In our paper, we apply our method to analyze the
US Congressional Record, which is a verbatim tran-
script of speeches given in the United States House
of Representatives and Senate. The Record is a
dense corpus of speeches made by a large number
of people over a long period of time. Using the tran-
scripts of political speeches adds an extra layer of
meaning onto the measure of speaker centrality. The
centrality of speakers in Congress can be thought of
as a measure of relative importance or influence in
the US legislative process. We can also use speaker
centrality to analyze committee membership: are the
central speakers on a given issue ranking members
of a related committee? Is there a type of impor-
tance captured through speaker centrality that isn?t
obvious in the natural committee rankings?
There has been growing interest in using tech-
niques from natural language processing in the area
of political science. In (Porter et al, 2005) the
authors performed a network analysis of members
and committees of the US House of Representatives.
They found connections between certain commit-
tees and political positions that suggest that com-
mittee membership is not determined at random.
In (Thomas et al, 2006), the authors use the tran-
scripts of debates from the US Congress to auto-
matically classify speeches as supporting or oppos-
ing a given topic by taking advantage of the vot-
ing records of the speakers. In (Wang et al, 2005),
the authors use a generative model to simultane-
ously discover groups of voters and topics using
the voting records and the text from bills of the
US Senate and the United Nations. The authors
of (Quinn et al, 2006) introduce a multinomial mix-
ture model to perform unsupervised clustering of
Congressional speech documents into topically re-
lated categories. We rely on the output of this model
to cluster the speeches from the Record in order to
compare speaker rankings within a topic to related
committees.
We take advantage of the natural measures of
prestige in Senate committees and use them as a
standard for comparison with MavenRank. Our hy-
pothesis is that MavenRank centrality will capture
the importance of speakers based on the natural
committee rankings and seniority. We can test this
claim by clustering speeches into topics and then
mapping the topics to related committees. If the hy-
pothesis is correct, then the speaker centrality should
be correlated with the natural committee rankings.
There have been other attempts to link floor par-
ticipation with topics in political science. In (Hall,
1996), the author found that serving on a commit-
tee can positively predict participation in Congress,
but that seniority was not a good predictor. His
measure only looked at six bills in three commit-
tees, so his method is by far not as comprehensive
as the one that we present here. Our approach with
MavenRank differs from previous work by provid-
ing a large scale analysis of speaker centrality and
bringing natural language processing techniques to
the realm of political science.
2 Data
2.1 The US Congressional Speech Corpus
The text used in the experiments is from the United
States Congressional Speech corpus (Monroe et
al., 2006), which is an XML formatted version of
the electronic United States Congressional Record
from the Library of Congress1. The Congressional
Record is a verbatim transcript of the speeches made
in the US House of Representatives and Senate be-
ginning with the 101st Congress in 1998 and in-
cludes tens of thousands of speeches per year. In
our experiments we focused on the records from the
105th and 106th Senates. The basic unit of the US
Congressional Speech corpus is a record, which cor-
responds to a single subsection of the print version
of the Congressional Record and may contain zero
or more speakers. Each paragraph of text within
a record is tagged as either speech or non-speech
and each paragraph of speech text is tagged with the
unique id of the speaker. Figure 1 shows an example
record file for the sixth record on July 14th, 1997 in
the 105th Senate.
In our experiments we use a smaller unit of anal-
ysis called a speech document by taking all of the
text of a speaker within a single record. The cap-
italization and punctuation is then removed from
the text as in (Monroe et al, 2006) and then the
1http://thomas.loc.gov
659
text stemmed using Porter?s Snowball II stemmer2.
Figure 1 shows an example speech document for
speaker 15703 (Herb Kohl of Wisconsin) that has
been generated from the record in Figure 1.
In addition to speech documents, we also use
speaker documents. A speaker document is the
concatenation of all of a speaker?s speech docu-
ments within a single session and topic (so a sin-
gle speaker may have multiple speaker documents
across topics). For example within the 105th Senate
in topic 1 (?Judicial Nominations?), Senator Kohl
has four speech documents, so the speaker document
attributed to him within this session and topic would
be the text of these four documents treated as a sin-
gle unit. The order of the concatenation does not
matter since we will look at it as a vector of weighted
term frequencies (see Section 3.2).
2.2 Topic Clusters
We used the direct output of the 42-topic model of
the 105th-108th Senates from (Quinn et al, 2006)
to further divide the speech documents into topic
clusters. In their paper, they use a model where the
probabilities of a document belonging to a certain
topic varies smoothly over time and the words within
a given document have exactly the same probabil-
ity of being drawn from a particular topic. These
two properties make the model different than stan-
dard mixture models (McLachlan and Peel, 2000)
and the latent Dirichlet alocation model of (Blei et
al., 2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006). Table 1 lists the 42
topics and their related committees.
The output from the topic model is a D ? 42 ma-
trix Z where D is the number of speech documents
and the element zdk represents the probability of the
dth speech document being generated by topic k.
We clustered the speech documents by assigning a
speech document d to the kth cluster where
k = argmax
j
zdj .
If the maximum value is not unique, we arbitrarily
assign d to the lowest numbered cluster where zdj is
2http://snowball.tartarus.org/
algorithms/english/stemmer.html
a maximum. A typical topic cluster contains several
hundred speech documents, while some of the larger
topic clusters contain several thousand.
2.3 Committee Membership Information
The committee membership information that we
used in the experiments is from Stewart and
Woon?s committee assignment codebook (Stewart
and Woon, 2005). This provided us with a roster
for each committee and rank and seniority informa-
tion for each member. In our experiments we use
the rank within party and committee seniority mem-
ber attributes to test the output of our pipeline. The
rank within party attribute orders the members of a
committee based on the Resolution that appointed
the members with the highest ranking members hav-
ing the lowest number. The chair and ranking mem-
bers always receive a rank of 1 within their party. A
committee member?s committee seniority attribute
corresponds to the number of years that the member
has served on the given committee.
2.4 Mapping Topics to Committees
In order to test our hypothesis that lexical centrality
is correlated with the natural committee rankings,
we needed a map from topics to related commit-
tees. We based our mapping on Senate Rule XXV,3
which defines the committees, and the descriptions
on committee home pages. Table 1 shows the map,
where a topic?s related committees are listed in ital-
ics below the topic name. Because we are matching
short topic names to the complex descriptions given
by Rule XXV, the topic-committee map is not one
to one or even particularly well defined: some top-
ics are mapped to multiple committees, some top-
ics are not mapped to any committees, and two dif-
ferent topics may be mapped to the same commit-
tee. This is not a major problem because even if a
one to one map between topics and committees ex-
isted, speakers from outside a topic?s related com-
mittee are free to participate in the topic simply by
giving a speech. Therefore there is no way to rank
all speakers in a topic using committee information.
To test our hypotheses, we focused our attention on
topics that have at least one related committee. In
Section 4.3 we describe how the MavenRank scores
3http://rules.senate.gov/senaterules/
rule25.php
660
<?xml version="1.0" standalone="no"?>
<!DOCTYPE RECORD SYSTEM "record.dtd">
<RECORD>
<HEADER>
<CHAMBER>Senate</CHAMBER>
<TITLE>NOMINATION OF JOEL KLEIN TO BE ASSISTANT ATTORNEY
GENERAL IN CHARGE OF THE ANTITRUST DIVISION </TITLE>
<DATE>19970714</DATE>
</HEADER>
<BODY>
<GRAF>
<PAGEREF></PAGEREF>
<SPEAKER>NULL</SPEAKER>
<NONSPEECH>NOMINATION OF JOEL KLEIN TO BE ASSISTANT
ATTORNEY GENERAL IN CHARGE OF THE ANTITRUST DIVISION
(Senate - July 14, 1997)</NONSPEECH>
</GRAF>
<GRAF>
<PAGEREF>S7413</PAGEREF>
<SPEAKER>15703</SPEAKER>
<SPEECH> Mr. President, as the ranking Democrat on the
Antitrust Subcommittee, let me tell you why I support Mr.
Klein?s nomination, why he is a good choice for the job,
and why we ought to confirm him today.
</SPEECH>
</GRAF>
. . .
<GRAF>
<PAGEREF>S7414</PAGEREF>
<SPEAKER>UNK1</SPEAKER>
<SPEECH> Without objection, it is so ordered. </SPEECH>
</GRAF>
</BODY>
</RECORD>
mr presid a the rank democrat on the antitrust subcommitte
let me tell you why i support mr klein nomin why he i a
good choic for the job and why we ought to confirm him
todai
first joel klein i an accomplish lawyer with a distinguish
career he graduat from columbia univers and harvard law
school and clerk for the u court of appeal here in
washington then for justic powel just a importantli he i
the presid choic to head the antitrust divis and i believ
that ani presid democrat or republican i entitl to a strong
presumpt in favor of hi execut branch nomine second joel
klein i a pragmatist not an idealogu hi answer at hi confirm
hear suggest that he i not antibusi a some would claim the
antitrust divis wa in the late 1970 nor anticonsum a some
argu the divis wa dure the 1980 instead he will plot a middl
cours i believ that promot free market fair competit and
consum welfar
the third reason we should confirm joel klein i becaus no on
deserv to linger in thi type of legisl limbo here in congress
we need the input of a confirm head of the antitrust divis
to give u the administr view on a varieti of import polici
matter defens consolid electr deregul and telecommun merger
among other we need someon who can speak with author for the
divis without a cloud hang over hi head
more than that without a confirm leader moral at the
antitrust divis i suffer and given the pace at which the
presid ha nomin and the senat ha confirm appointe if we fail
to approv mr klein it will be at least a year befor we confirm
a replac mayb longer and mayb never so we need to act now we
can?t afford to let the antitrust divis continu to drift
final mr presid i have great respect for the senat from south
carolina a well a the senat from nebraska and north dakota
thei have been forc advoc for consum on telecommun matter and
. . .
Figure 1: A sample of the text from record 105.sen.19970714.006.xml and the speech document for Senator
Herb Kohl of Wisconsin (id 15703) generated from it. The ?. . . ? represents omitted text.
1 Judicial Nominations 15 Health 2 (Economics - Seniors) 27 Procedural 1 (Housekeeping 1)
Judiciary Health, Education, Labor, and Pensions 28 Procedural 2 (Housekeeping 2)
2 Law & Crime 1 (Violence / Drugs) Veterans? Affairs 29 Campaign Finance
Judiciary Agriculture, Nutrition, and Forestry Rules and Administration
3 Banking / Finance Aging (Special Committee) 30 Law & Crime 2 (Federal)
Banking, Housing, and Urban Affairs Finance Judiciary
4 Armed Forces 1 (Manpower) 16 Gordon Smith re Hate Crime 31 Child Protection
Armed Services 17 Debt / Deficit / Social Security Health, Education, Labor, and Pensions
5 Armed Forces 2 (Infrastructure) Appropriations Agriculture, Nutrition, and Forestry
Armed Services Budget 32 Labor 1 (Workers, esp. Retirement)
6 Symbolic (Tribute - Living) Finance Health, Education, Labor, and Pensions
7 Symbolic (Congratulations - Sports) Aging (Special Committee) Aging (Special Committee)
8 Energy 18 Supreme Court / Constitutional Small Business and Entrepreneurship
Energy and Natural Resources Judiciary 33 Environment 2 (Regulation)
9 Defense (Use of Force) 19 Commercial Infrastructure Environment and Public Works
Armed Services Commerce, Science, and Transportation Agriculture, Nutrition, and Forestry
Homeland Security and Governmental Affairs 20 Symbolic (Remembrance - Military) Energy and Natural Resources
Intelligence (Select Committee) 21 International Affairs (Diplomacy) 34 Procedural 3 (Legislation 1)
10 Jesse Helms re Debt Foreign Relations 35 Procedural 4 (Legislation 2)
11 Environment 1 (Public Lands) 22 Abortion 36 Procedural 5 (Housekeeping 3)
Energy and Natural Resources Judiciary 37 Procedural 6 (Housekeeping 4)
Agriculture, Nutrition, and Forestry Health, Education, Labor, and Pensions 38 Taxes
12 Health 1 (Medical) 23 Symbolic (Tribute - Constituent) Finance
Health, Education, Labor, and Pensions 24 Agriculture 39 Symbolic (Remembrance - Nonmilitary)
13 International Affairs (Arms Control) Agriculture, Nutrition, and Forestry 40 Labor 2 (Employment)
Foreign Relations 25 Intelligence Health, Education, Labor, and Pensions
14 Social Welfare Intelligence (Select Committee) Small Business and Entrepreneurship
Agriculture, Nutrition, and Forestry Homeland Security and Governmental Affairs 41 Foreign Trade
Banking, Housing, and Urban Affairs 26 Health 3 (Economics - General) Finance
Health, Education, Labor, and Pensions Health, Education, Labor, and Pensions Banking, Housing, and Urban Affairs
Finance Finance 42 Education
Health, Education, Labor, and Pensions
Table 1: The numbers and names of the 42 topics from (Quinn et al, 2006) with our mappings to related
committees (listed below the topic name, if available).
661
of speakers who are not members of related commit-
tees were taken into account when we measured the
rank correlations.
3 MavenRank and Lexical Similarity
The following sections describe MavenRank, a mea-
sure of speaker centrality, and tf-idf cosine similar-
ity, which is used to measure the lexical similarity of
speeches.
3.1 MavenRank
MavenRank is a graph-based method for finding
speaker centrality. It is similar to the methods
in (Erkan and Radev, 2004; Mihalcea and Tarau,
2004; Kurland and Lee, 2005), which can be used
for ranking sentences in extractive summaries and
documents in an information retrieval system. Given
a collection of speeches s1, . . . , sN and a measure
of lexical similarity between pairs sim(si, sj) ? 0,
a similarity graph can be constructed. The nodes
of the graph represent the speeches and a weighted
similarity edge is placed between pairs that exceed
a similarity threshold smin. MavenRank is based on
the premise that important speakers will have cen-
tral speeches in the graph, and that central speeches
should be similar to other central speeches. A recur-
sive explanation of this concept is that the score of
a speech should be proportional to the scores of its
similar neighbors.
Given a speech s in the graph, we can express the
recursive definition of its score p(s) as
p(s) =
?
t?adj[s]
p(t)
wdeg(t)
(1)
where adj[s] is the set of all speeches adjacent to
s and wdeg(t) =
?
u?adj[t] sim(t, u), the weighted
degree of t. Equation (1) captures the idea that the
MavenRank score of a speech is distributed to its
neighbors. We can rewrite this using matrix notation
as
p = pB (2)
where p = (p(s1), p(s2), . . . , p(sN )) and the ma-
trixB is the row normalized similarity matrix of the
graph
B(i, j) =
S(i, j)
?
k S(i, k)
(3)
where S(i, j) = sim(si, sj). Equation (2) shows
that the vector of MavenRank scores p is the left
eigenvector of B with eigenvalue 1.
We can prove that the eigenvector p exists by us-
ing a techinque from (Page et al, 1999). We can
treat the matrix B as a Markov chain describing
the transition probabilities of a random walk on the
speech similarity graph. The vector p then repre-
sents the stationary distribution of the random walk.
It is possible that some parts of the graph are dis-
connected or that the walk gets trapped in a com-
ponent. These problems are solved by reserving
a small escape probability at each node that repre-
sents a chance of jumping to any node in the graph,
making the Markov chain irreducible and aperiodic,
which guarantees the existence of the eigenvector.
Assuming a uniform escape probability for each
node on the graph, we can rewrite Equation (2) as
p = p[dU+ (1? d)B] (4)
where U is a square matrix with U(i, j) = 1/N
for all i and j, N is the number of nodes, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998). Equation (4) is
known as PageRank (Page et al, 1999) and is used
for determining prestige on the web in the Google
search engine.
3.2 Lexical Similarity
In our experiments, we used tf-idf cosine similarity
to measure lexical similarity between speech docu-
ments. We represent each speech document as a vec-
tor of term frequencies (or tf), which are weighted
according to the relative importance of the given
term in the cluster. The terms are weighted by their
inverse document frequency or idf. The idf of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
nw
)
(5)
where N is the number of documents in the corpus
and nw is the number of documents in the corpus
containing the term w. It follows that very common
words like ?of? or ?the? have a very low idf, while
the idf values of rare words are higher. In our experi-
ments, we calculated the idf values for each topic us-
ing all speech documents across sessions within the
662
 20
 30
 40
 50
 60
 70
 80
 90
 100
Abortion ChildProtection Education Workers,Retirement
SantorumBoxerKennedy
Figure 2: MavenRank percentiles for three speakers
over four topics.
given topic. We calculated topic-specific idf values
because some words may be relatively unimportant
in one topic, but important in another. For example,
in topic 22 (?Abortion?), the idf of the term ?abort?
is near 0.20, while in topic 38 (?Taxes?), its idf is
near 7.18.
The tf-idf cosine similarity measure
tf-idf-cosine(u, v) is defined as
P
w?u,v tfu(w) tfv(w) idf(w)
2
?P
w?u(tfu(w) idf(w))
2
?P
w?v(tfv(w) idf(w))
2
, (6)
which is the cosine of the angle between the tf-idf
vectors.
There are other alternatives to tf-idf cosine sim-
ilarity. Some other possible similarity measures
are document edit distance, the language models
from (Kurland and Lee, 2005), or generation proba-
bilities from (Erkan, 2006). For simplicity, we only
used tf-idf similarities in our experiments, but any of
these measures could be used in this case.
4 Experiments and Results
4.1 Data
We used the topic clusters from the 105th Senate
as training data to adjust the parameter smin and
observe trends in the data. We did not run experi-
ments to test the effect of different values of smin on
MavenRank scores, but our chosen value of 0.25 has
shown to give acceptable results in similar experi-
ments (Erkan and Radev, 2004). We used the topic
clusters from the 106th Senate as test data. For the
speech document networks, there was an average of
351 nodes (speech documents) and 2142 edges per
topic. For the speaker document networks, there was
an average of 63 nodes (speakers) and 545 edges per
topic.
4.2 Experimental Setup
We set up a pipeline using a Perl implementation
of tf-idf cosine similarity and MavenRank. We ran
MavenRank on the topic clusters and ranked the
speakers based on the output. We used two different
types granularities of the graphs as input: one where
the nodes are speech documents and another where
the nodes are speaker documents (see Section 2.1).
For the speech document graph, a speaker?s score is
determined by the sum of the MavenRank scores of
the speeches given by that speaker.
4.3 Evaluation Methods
To evaluate our output, we estimate independent
ordinary least squares linear regression models of
MavenRank centrality for topics with at least one re-
lated committee (there are 29 total):
MavenRankik = ?0k + ?skSeniorityik +
+?rkRankingMemberjk + ik (7)
where i indexes Senators, k indexes topics,
Seniorityik is the number of years Senator i has
served on the relevant committee for topic k (value
zero for those not on a relevant committee) and
RankingMemberjk has the value of one only for
the Chair and ranking minority member of a rele-
vant committee. We are interested primarily in the
overall significance of the estimated model (indicat-
ing committee effects) and, secondarily, in the spe-
cific source of any committee effect in seniority or
committee rank.
4.4 Results
Table 2 summarizes the results. ?Maven? status on
most topics does appear to be driven by committee
status, as expected. There are particularly strong ef-
fects of seniority and rank in topics tied to the Judi-
ciary, Foreign Relations, and Armed Services com-
mittees, as well as legislation-rich areas of domestic
policy. Perhaps of greater interest are the topics that
do not have committee effects. These are of three
distinct types. The first are highly politicized top-
ics for which speeches are intended not to influence
663
Topic p(F )a p(?s > 0)b p(?r > 0)c Topic p(F ) p(?s > 0) p(?r > 0)
Seniority and Ranking Status Both Significant Seniority and Ranking Status Jointly Significant
2 Law & Crime 1 [Violent] < .001 0.016 < .001 26 Health 3 [Economics] 0.001 0.106 0.064
18 Constitutional < .001 0.003 < .001 32 Labor 1 [Workers] 0.007 0.156 0.181
33 Environment 2 [Regulation] 0.007 0.063 0.056
Seniority Significant 3 Banking / Finance 0.042 0.141 0.579
12 Health 1 [Medical] < .001 < .001 0.567
42 Education < .001 < .001 0.337 No Significant Effects of Committee Status
41 Trade < .001 < .001 0.087 11 Environment 1 [Public Lands] 0.104 0.102 0.565
21 Int?l Affairs [Nonmilitary] < .001 0.007 0.338 22 Abortion 0.419 0.609 0.252
9 Defense [Use of Force] 0.002 0.001 0.926 5 Armed Forces 2 [Infrastructure] 0.479 0.267 0.919
19 Commercial Infrastructure 0.007 0.032 0.332 24 Agriculture 0.496 0.643 0.425
40 Labor 2 [Employment] 0.029 0.010 0.114 17 Debt / Social Security 0.502 0.905 0.295
38 Taxes 0.037 0.033 0.895 15 Health 2 [Seniors] 0.706 0.502 0.922
25 Intelligence 0.735 0.489 0.834
Ranking Status Significant 29 Campaign Finance 0.814 0.748 0.560
30 Crime 2 [Federal] < .001 0.334 < .001 31 Child Protection 0.856 0.580 0.718
8 Energy < .001 0.145 < .001
1 Judicial Nominations < .001 0.668 < .001
14 Social Welfare < .001 0.072 0.005
13 Int?l Affairs [Arms] < .001 0.759 0.001
4 Armed Forces 1 [Manpower] 0.007 0.180 0.049
aF-test for joint significance of committee variables.
bT-test for significance of committee seniority.
cT-test for significance of chair or ranking member status.
Table 2: Significance tests for ordinary least squares (OLS) linear regressions ofMavenRank scores (Speech-
documents graph) on committee seniority (in years) and ranking status (chair or ranking member), 106th
Senate, topic-by-topic. Results for the speaker-documents graph are similar.
legislation as much as indicate an ideological or par-
tisan position, so the mavens are not on particular
committees (abortion, children, seniors, the econ-
omy). The second are ?distributive politics? topics
where many Senators speak to defend state or re-
gional interests, so debate is broadly distributed and
there are no clear mavens (agriculture, military base
closures, public lands). Third are topics where there
are not enough speeches for clear results, because
most debate occurred after 1999-2000 (post-9/11
intelligence reform, McCain-Feingold campaign fi-
nance reform).
Alternative models, using measures of centrality
based on the centroid were also examined. Dis-
tance to centroid provides broadly similar results as
MavenRank, with several marginal significance re-
sults reversed in each direction. Cosine similarity
with centroid, on the other hand, appears to have no
relationship with committee structure.
Figure 2 shows the MavenRank percentiles (us-
ing the speech document network) for Senators Rick
Santorum, Barbara Boxer, and Edward Kennedy
across a few topics in the 106th Senate. These
sample scores conform to the expected rankings for
these speakers. In this session, Santorum was the
sponsor of a bill to ban partial birth abortions and
was a spokesman for Social Security reform, which
support his high ranking in abortion and work-
ers/retirement. Boxer acted as the lead opposition
to Santorum?s abortion bill and is known for her
support of child abuse laws. Kennedy was ranking
member of the Health, Education, Labor, and Pen-
sions committee and the Judiciary committee (which
was involved with the abortion bill).
4.5 MavenRank in Other Contexts
MavenRank is a general method for finding central
speakers in a discussion and can be applied to areas
outside of political science. One potential applica-
tion would be analyzing blog posts to find ?Maven?
bloggers by treating blogs as speakers and posts as
speeches. Similarly, MavenRank could be used to
find central participants in a newsgroup, a forum, or
a collection of email conversations.
5 Conclusion
We have presented a technique for identifying lexi-
cally central speakers using a graph based method
called MavenRank. To test our method for find-
ing central speakers, we analyzed the Congressional
664
Record by creating a map from the clusters of
speeches to Senate committees and comparing the
natural ranking committee members to the output of
MavenRank. We found evidence of a possible rela-
tionship between the lexical centrality and commit-
tee rank of a speaker by ranking the speeches us-
ing MavenRank and computing the rank correlation
with the natural ordering of speakers. Some spe-
cific committees disagreed with our hypothesis that
MavenRank and committee position are correlated,
which we propose is because of the non-legislative
aspects of those specific committees. The results
of our experiment suggest that MavenRank can in-
deed be used to find central speakers in a corpus of
speeches.
We are currently working on applying our meth-
ods to the US House of Representatives and other
records of parliamentary speech from the United
Kingdom and Australia. We have also developed a
dynamic version of MavenRank that takes time into
account when finding lexical centrality and plan on
using it with the various parliamentary records. We
are interested in dynamic MavenRank to go further
with the idea of tracking how ideas get propagated
through a network of debates, including congres-
sional records, blogs, and newsgroups.
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the views of the National Sci-
ence Foundation.
References
David Blei and John Lafferty. 2006. Dynamic topic
models. In Machine Learning: Proceedings of the
Twenty-Third International Conference (ICML).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Gunes Erkan. 2006. Language model-based document
clustering using random walks. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 479?486, New York
City, USA, June. Association for Computational Lin-
guistics.
L. C. Freeman. 1977. A set of measures of central-
ity based on betweenness. Sociometry, 40(1):35?41,
March.
Malcolm Gladwell. 2002. The Tipping Point: How Little
Things Can Make a Big Difference. Back Bay Books,
January.
Richard L. Hall. 1996. Participation in Congress. Yale
University Press.
Jon M. Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th An-
nual ACM-SIAM Symposium on Discrete Algorithms,
pages 668?677.
Oren Kurland and Lillian Lee. 2005. PageRank without
hyperlinks: Structural re-ranking using links induced
by language models. In Proceedings of SIGIR, pages
306?313.
Oren Kurland and Lillian Lee. 2006. Respect my author-
ity! HITS without hyperlinks, utilizing cluster-based
language models. In Proceedings of SIGIR, pages 83?
90.
Ziheng Lin and Min-Yen Kan. 2007. Timestamped
graphs: Evolutionary models of text for multi-
document summarization. In Proceedings of the Sec-
ond Workshop on TextGraphs: Graph-Based Algo-
rithms for Natural Language Processing, pages 25?32,
Rochester, NY, USA. Association for Computational
Linguistics.
Geoffrey McLachlan and David Peel. 2000. Finite Mix-
ture Models. New York: Wiley.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into texts. In Proceedings of the Ninth Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ?04).
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application to
word sense disambiguation. In Proceedings of the
Twentieth International Conference on Computational
Linguistics (COLING ?04), pages 1126?1132.
665
Burt L. Monroe, Cheryl L. Monroe, Kevin M. Quinn,
Dragomir Radev, Michael H. Crespin, Michael P. Co-
laresi, Anthony Fader, Jacob Balazer, and Steven P.
Abney. 2006. United states congressional speech cor-
pus. Department of Political Science, The Pennsylva-
nia State University.
Mark E. J. Newman. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank citation ranking:
Bringing order to the Web. Technical Report 1999-66,
Stanford Digital Library Technologies Project, Stan-
ford University, November 11,.
Mason A. Porter, Peter J. Mucha, M. E. J. Newman, and
Casey M. Warmbrand. 2005. A network analysis of
committees in the u.s. house of representatives. PNAS,
102(20), May.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th-108th
U.S. senate. In Midwest Political Science Association
Meeting.
K. Sparck-Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28(1):11?20.
Charles Stewart and Jonathan Woon. 2005. Con-
gressional committee assignments, 103rd to 105th
congresses, 1993?1998: Senate, july 12, 2005.
http://web.mit.edu/17.251/www/
data_page.html.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
Hanghang Tong and Christos Faloutsos. 2006. Center-
piece subgraphs: problem definition and fast solutions.
In Tina Eliassi-Rad, Lyle H. Ungar, Mark Craven, and
Dimitrios Gunopulos, editors, KDD, pages 404?413.
ACM.
Xuerui Wang, Natasha Mohanty, and Andrew McCallum.
2005. Group and topic discovery from relations and
their attributes. In NIPS.
666
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 45?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LexNet: A Graphical Environment for Graph-Based NLP
Dragomir R. Radev
 
, Gu?nes? Erkan
 
, Anthony Fader

,
Patrick Jordan
 
, Siwei Shen
 
, and James P. Sweeney

Department of Electrical Engineering and Computer Science
School of Information
Department of Mathematics
University of Michigan
Ann Arbor, MI 48109

radev, gerkan, afader, prjordan, shens, jpsweeney@umich.edu
Abstract
This interactive presentation describes
LexNet, a graphical environment for
graph-based NLP developed at the Uni-
versity of Michigan. LexNet includes
LexRank (for text summarization), bi-
ased LexRank (for passage retrieval), and
TUMBL (for binary classification). All
tools in the collection are based on random
walks on lexical graphs, that is graphs
where different NLP objects (e.g., sen-
tences or phrases) are represented as nodes
linked by edges proportional to the lexi-
cal similarity between the two nodes. We
will demonstrate these tools on a variety of
NLP tasks including summarization, ques-
tion answering, and prepositional phrase
attachment.
1 Introduction
We will present a series of graph-based tools for a
variety of NLP tasks such as text summarization,
passage retrieval, prepositional phrase attachment,
and binary classification in general.
Recently proposed graph-based methods
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002b; Zhu and Ghahramani, 2002a;
Toutanova et al, 2004) are particularly well
suited for transductive learning (Vapnik, 1998;
Joachims, 1999). Transductive learning is based
on the idea (Vapnik, 1998) that instead of splitting
a learning problem into two possibly harder
problems, namely induction and deduction, one
can build a model that covers both labeled and
unlabeled data. Unlabeled data are abundant as
well as significantly cheaper than labeled data in
a variety of natural language applications. Parsing
and machine translation both offer examples of
this relationship, with unparsed text from the Web
and untranslated texts being computationally less
costly. These can then be used to supplement
manually translated and aligned corpora. Hence
transductive methods are of great potential for
NLP problems and, as a result, LexNet includes a
number of transductive methods.
2 LexRank: text summarization
LexRank (Erkan and Radev, 2004) embodies the
idea of representing a text (e.g., a document or a
collection of related documents) as a graph. Each
node corresponds to a sentence in the input and the
edge between two nodes is related to the lexical
similarity (either cosine similarity or n-gram gen-
eration probability) between the two sentences.
LexRank computes the steady-state distribution of
the random walk probabilities on this similarity
graph. The LexRank score of each node gives
the probability of a random walk ending up in
that node in the long run. An extractive summary
is generated by retrieving the sentences with the
highest score in the graph. Such sentences typ-
ically correspond to the nodes that have strong
connections to other nodes with high scores in the
graph. Figure 1 demonstrates LexRank.
3 Biased LexRank: passage retrieval
The basic idea behind Biased LexRank is to label
a small number of sentences (or passages) that are
relevant to a particular query and then propagate
relevance from these sentences to other (unanno-
tated) sentences. Relevance propagation is per-
formed on a bipartite graph. In that graph, one
of the modes corresponds to the sentences and
the other ? to certain words from these sentences.
Each sentence is connected to the words that ap-
pear in it. Thus indirectly, each sentence is two
hops away from any other sentence that shares
words in it. Intuitively, the sentences that are
close to the labeled sentences tend to get higher
scores. However, the relevance propagation en-
45
Figure 1: A sample snapshot of LexRank. A 3-
sentence summary is produced from a set of 11
related input sentences. The summary sentences
are shown as larger squares.
ables us to mark certain sentences that are not im-
mediate neighbors of the labeled sentences via in-
direct connections. The effect of the propagation
is discounted by a parameter at each step so that
the relationships between closer nodes are favored
more. Biased LexRank also allows for negative
relevance to be propagated through the network as
the example shows. See Figures 2? 3 for a demon-
stration of Biased LexRank.
Figure 2: Display of Biased LexRank. One sen-
tence at the top is annotated as positive while an-
other at the bottom is marked negative. Sentences
are displayed as circles and the word features are
shown as squares.
Figure 3: After convergence of Biased LexRank.
4 TUMBL: prepositional phrase
attachment
A number of NLP problems such as word sense
disambiguation, text categorization, and extractive
summarization can be cast as classification prob-
lems. This fact is used to great effect in the de-
sign and application of many machine learning
methods used in modern NLP, including TUMBL,
through the utilization of vector representations.
Each object is represented as a vector   of fea-
tures. The main assumption made is that a pair of
objects   and  will be classified the same way
if the distance between them in some space  is
small (Zhu and Ghahramani, 2002a).
This algorithm propagates polarity information
first from the labeled data to the features, capturing
whether each feature is more indicative of posi-
tive class or more negative learned. Such informa-
tion is further transferred to the unlabeled set. The
backward steps update feature polarity with infor-
mation learned from the structure of the unlabeled
data. This process is repeated with a damping fac-
tor to discount later rounds. This process is illus-
tracted in Figure 4. TUMBL was first described
in (Radev, 2004). A series of snapshots showing
TUMBL in Figures 5? 7.
5 Technical information
5.1 Code implementation
The LexRank and TUMBL demonstrations are
provided as both an applet and an application.
The user is presented with a graphical visualiza-
tion of the algorithm that was conveniently de-
veloped using the JUNG API (http://jung.
sourceforge.net/faq.html).
46
(a) Initial graph (b) Forward pass
(c) Backward
pass
(d) Convergence
Figure 4: TUMBL snapshots: the circular vertices
are objects while the square vertices are features.
(a) The initial graph with features showing no bias.
(b) The forward pass where objects propagate la-
bels forward. (c) The backward pass where fea-
tures propagate labels backward. (d) Convergence
of the TUMBL algorithm after successive itera-
tions.
Figure 5: A 10-pp prepositional phrase attachment
problem is displayed. The head of each preposi-
tional phrase is ine middle column. Four types of
features are represented in four columns. The first
column is Noun1 in the 4-tuple. The second col-
umn is Noun2. The first column from the right is
verb of the 4-tuple while the second column from
the right is the actual head of the prepositional
phrase. At this time one positive and one negative
example (high and low attachment) are annotated.
The rest of the circles correspond to the unlabeled
examples.
Figure 6: The final configuration.
47
Figure 7: XML file corresponding to the PP at-
tachment problem. The XML DTD allows layout
information to be encoded along with algorithmic
information such as label and polarity.
In TUMBL, each object is represented by a cir-
cular vertex in the graph and each feature as a
square. Vertices are assigned a color according to
their label. The colors are assignable by the user
and designate the probability of membership of a
class.
To allow for a range of uses, data can be
entered either though the GUI or read in from
an XML file. The schema for TUMBL files is
shown at http://tangra.si.umich.edu/
clair/tumbl.
In the LexRank demo, each sentence becomes a
node. Selected nodes for the summary are shown
in larger size and in blue while the rest are smaller
and drawn in red. The link between two nodes has
a weight proportional to the lexical similarity be-
tween the two corresponding sentences. The demo
also reports the metrics precision, recall, and F-
measure.
5.2 Availability
The demos are available both as locally based and
remotely accessible from http://tangra.
si.umich.edu/clair/lexrank and
http://tangra.si.umich.edu/clair/
tumbl.
6 Acknowledgments
This work was partially supported by the U.S.
National Science Foundation under the follow-
ing two grants: 0329043 ?Probabilistic and link-
based Methods for Exploiting Very Large Textual
Repositories? administered through the IDM pro-
gram and 0308024 ?Collaborative Research: Se-
mantic Entity and Relation Extraction from Web-
Scale Text Document Collections? run by the HLC
program. All opinions, findings, conclusions, and
recommendations in this paper are made by the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
References
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML ?99.
Dragomir Radev. 2004. Weakly supervised graph-
based methods for classification. Technical Report
CSE-TR-500-04, University of Michigan.
Martin Szummer and Tommi Jaakkola. 2001. Partially
labeled classification with Markov random walks. In
NIPS ?01, volume 14. MIT Pres.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk mod-
els for inducing word dependency distributions. In
ICML ?04, New York, New York, USA. ACM Press.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Xiaojin Zhu and Zoubin Ghahramani. 2002a. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, CMU-CALD-02-107.
Xiaojin Zhu and Zoubin Ghahramani. 2002b. Towards
semi-supervised classification with Markov random
fields. Technical report, CMU-CALD-02-106.
48
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535?1545,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identifying Relations for Open Information Extraction
Anthony Fader, Stephen Soderland, and Oren Etzioni
University of Washington, Seattle
{afader,soderlan,etzioni}@cs.washington.edu
Abstract
Open Information Extraction (IE) is the task
of extracting assertions from massive corpora
without requiring a pre-specified vocabulary.
This paper shows that the output of state-of-
the-art Open IE systems is rife with uninfor-
mative and incoherent extractions. To over-
come these problems, we introduce two sim-
ple syntactic and lexical constraints on bi-
nary relations expressed by verbs. We im-
plemented the constraints in the REVERB
Open IE system, which more than doubles the
area under the precision-recall curve relative
to previous extractors such as TEXTRUNNER
and WOEpos. More than 30% of REVERB?s
extractions are at precision 0.8 or higher?
compared to virtually none for earlier systems.
The paper concludes with a detailed analysis
of REVERB?s errors, suggesting directions for
future work.1
1 Introduction and Motivation
Typically, Information Extraction (IE) systems learn
an extractor for each target relation from la-
beled training examples (Kim and Moldovan, 1993;
Riloff, 1996; Soderland, 1999). This approach to IE
does not scale to corpora where the number of target
relations is very large, or where the target relations
cannot be specified in advance. Open IE solves this
problem by identifying relation phrases?phrases
that denote relations in English sentences (Banko
et al, 2007). The automatic identification of rela-
1The source code for REVERB is available at http://
reverb.cs.washington.edu/
tion phrases enables the extraction of arbitrary re-
lations from sentences, obviating the restriction to a
pre-specified vocabulary.
Open IE systems have achieved a notable measure
of success on massive, open-domain corpora drawn
from the Web, Wikipedia, and elsewhere. (Banko et
al., 2007; Wu and Weld, 2010; Zhu et al, 2009). The
output of Open IE systems has been used to support
tasks like learning selectional preferences (Ritter et
al., 2010), acquiring common sense knowledge (Lin
et al, 2010), and recognizing entailment (Schoen-
mackers et al, 2010; Berant et al, 2011). In ad-
dition, Open IE extractions have been mapped onto
existing ontologies (Soderland et al, 2010).
We have observed that two types of errors are fre-
quent in the output of Open IE systems such as TEX-
TRUNNER and WOE: incoherent extractions and un-
informative extractions.
Incoherent extractions are cases where the ex-
tracted relation phrase has no meaningful interpre-
tation (see Table 1 for examples). Incoherent ex-
tractions arise because the learned extractor makes a
sequence of decisions about whether to include each
word in the relation phrase, often resulting in incom-
prehensible predictions. To solve this problem, we
introduce a syntactic constraint: every multi-word
relation phrase must begin with a verb, end with a
preposition, and be a contiguous sequence of words
in the sentence. Thus, the identification of a relation
phrase is made in one fell swoop instead of on the
basis of multiple, word-by-word decisions.
Uninformative extractions are extractions that
omit critical information. For example, consider the
sentence ?Faust made a deal with the devil.? Previ-
1535
ous Open IE systems return the uninformative
(Faust, made, a deal)
instead of
(Faust, made a deal with, the devil).
This type of error is caused by improper handling
of relation phrases that are expressed by a combi-
nation of a verb with a noun, such as light verb
constructions (LVCs). An LVC is a multi-word ex-
pression composed of a verb and a noun, with the
noun carrying the semantic content of the predi-
cate (Grefenstette and Teufel, 1995; Stevenson et al,
2004; Allerton, 2002). Table 2 illustrates the wide
range of relations expressed this way, which are not
captured by existing open extractors. Our syntactic
constraint leads the extractor to include nouns in the
relation phrase, solving this problem.
Although the syntactic constraint significantly re-
duces incoherent and uninformative extractions, it
allows overly-specific relation phrases such as is of-
fering only modest greenhouse gas reduction targets
at. To avoid overly-specific relation phrases, we in-
troduce an intuitive lexical constraint: a binary rela-
tion phrase ought to appear with at least a minimal
number of distinct argument pairs in a large corpus.
In summary, this paper articulates two simple but
surprisingly powerful constraints on how binary re-
lationships are expressed via verbs in English sen-
tences, and implements them in the REVERB Open
IE system. We release REVERB and the data used in
our experiments to the research community.
The rest of the paper is organized as follows. Sec-
tion 2 analyzes previous work. Section 3 defines our
constraints precisely. Section 4 describes REVERB,
our implementation of the constraints. Section 5 re-
ports on our experimental results. Section 6 con-
cludes with a summary and discussion of future
work.
2 Previous Work
Open IE systems like TEXTRUNNER (Banko et al,
2007), WOEpos, and WOEparse (Wu and Weld, 2010)
focus on extracting binary relations of the form
(arg1, relation phrase, arg2) from text. These sys-
tems all use the following three-step method:
1. Label: Sentences are automatically labeled
with extractions using heuristics or distant su-
pervision.
Sentence Incoherent Relation
The guide contains dead links
and omits sites.
contains omits
The Mark 14 was central to the
torpedo scandal of the fleet.
was central torpedo
They recalled that Nungesser
began his career as a precinct
leader.
recalled began
Table 1: Examples of incoherent extractions. In-
coherent extractions make up approximately 13% of
TEXTRUNNER?s output, 15% of WOEpos?s output, and
30% of WOEparse?s output.
is is an album by, is the author of, is a city in
has has a population of, has a Ph.D. in, has a cameo in
made made a deal with, made a promise to
took took place in, took control over, took advantage of
gave gave birth to, gave a talk at, gave new meaning to
got got tickets to, got a deal on, got funding from
Table 2: Examples of uninformative relations (left) and
their completions (right). Uninformative relations oc-
cur in approximately 4% of WOEparse?s output, 6% of
WOEpos?s output, and 7% of TEXTRUNNER?s output.
2. Learn: A relation phrase extractor is learned
using a sequence-labeling graphical model
(e.g., CRF).
3. Extract: the system takes a sentence as in-
put, identifies a candidate pair of NP arguments
(arg1, arg2) from the sentence, and then uses
the learned extractor to label each word be-
tween the two arguments as part of the relation
phrase or not.
The extractor is applied to the successive sentences
in the corpus, and the resulting extractions are col-
lected.
This method faces several challenges. First,
the training phase requires a large number of la-
beled training examples (e.g., 200, 000 heuristically-
labeled sentences for TEXTRUNNER and 300, 000
for WOE). Heuristic labeling of examples obviates
hand labeling but results in noisy labels and distorts
the distribution of examples. Second, the extrac-
tion step is posed as a sequence-labeling problem,
where each word is assigned its own label. Because
each assignment is uncertain, the likelihood that the
extracted relation phrase is flawed increases with
the length of the sequence. Finally, the extractor
1536
chooses an extraction?s arguments heuristically, and
cannot backtrack over this choice. This is problem-
atic when a word that belongs in the relation phrase
is chosen as an argument (for example, deal from
the ?made a deal with? sentence).
Because of the feature sets utilized in previous
work, the learned extractors ignore both ?holistic?
aspects of the relation phrase (e.g., is it contiguous?)
as well as lexical aspects (e.g., how many instances
of this relation are there?). Thus, as we show in Sec-
tion 5, systems such as TEXTRUNNER are unable
to learn the constraints embedded in REVERB. Of
course, a learning system, utilizing a different hy-
pothesis space, and an appropriate set of training ex-
amples, could potentially learn and refine the con-
straints in REVERB. This is a topic for future work,
which we consider in Section 6.
The first Open IE system was TEXTRUNNER
(Banko et al, 2007), which used a Naive Bayes
model with unlexicalized POS and NP-chunk fea-
tures, trained using examples heuristically generated
from the Penn Treebank. Subsequent work showed
that utilizing a linear-chain CRF (Banko and Et-
zioni, 2008) or Markov Logic Network (Zhu et al,
2009) can lead to improved extraction. The WOE
systems introduced by Wu and Weld make use of
Wikipedia as a source of training data for their ex-
tractors, which leads to further improvements over
TEXTRUNNER (Wu and Weld, 2010). Wu and Weld
also show that dependency parse features result in a
dramatic increase in precision and recall over shal-
low linguistic features, but at the cost of extraction
speed.
Other approaches to large-scale IE have included
Preemptive IE (Shinyama and Sekine, 2006), On-
Demand IE (Sekine, 2006), and weak supervision
for IE (Mintz et al, 2009; Hoffmann et al, 2010).
Preemptive IE and On-Demand IE avoid relation-
specific extractors, but rely on document and en-
tity clustering, which is too costly for Web-scale IE.
Weakly supervised methods use an existing ontol-
ogy to generate training data for learning relation-
specific extractors. While this allows for learn-
ing relation-specific extractors at a larger scale than
what was previously possible, the extractions are
still restricted to a specific ontology.
Many systems have used syntactic patterns based
on verbs to extract relation phrases, usually rely-
ing on a full dependency parse of the input sentence
(Lin and Pantel, 2001; Stevenson, 2004; Specia and
Motta, 2006; Kathrin Eichler and Neumann, 2008).
Our work differs from these approaches by focus-
ing on relation phrase patterns expressed in terms
of POS tags and NP chunks, instead of full parse
trees. Banko and Etzioni (Banko and Etzioni, 2008)
showed that a small set of POS-tag patterns cover a
large fraction of relationships in English, but never
incorporated the patterns into an extractor. This pa-
per reports on a substantially improved model of bi-
nary relation phrases, which increases the recall of
the Banko-Etzioni model (see Section 3.3). Further,
while previous work in Open IE has mainly focused
on syntactic patterns for relation extraction, we in-
troduce a lexical constraint that boosts precision and
recall.
Finally, Open IE is closely related to semantic role
labeling (SRL) (Punyakanok et al, 2008; Toutanova
et al, 2008) in that both tasks extract relations and
arguments from sentences. However, SRL systems
traditionally rely on syntactic parsers, which makes
them susceptible to parser errors and substantially
slower than Open IE systems such as REVERB. This
difference is particularly important when operating
on the Web corpus due to its size and heterogeneity.
Finally, SRL requires hand-constructed semantic re-
sources like Propbank and Framenet (Martha and
Palmer, 2002; Baker et al, 1998) as input. In con-
trast, Open IE systems require no relation-specific
training data. ReVerb, in particular, relies on its ex-
plicit lexical and syntactic constraints, which have
no correlate in SRL systems. For a more detailed
comparison of SRL and Open IE, see (Christensen
et al, 2010).
3 Constraints on Relation Phrases
In this section we introduce two constraints on re-
lation phrases: a syntactic constraint and a lexical
constraint.
3.1 Syntactic Constraint
The syntactic constraint serves two purposes. First,
it eliminates incoherent extractions, and second, it
reduces uninformative extractions by capturing rela-
tion phrases expressed by a verb-noun combination,
including light verb constructions.
1537
V | V P | VW ?P
V = verb particle? adv?
W = (noun | adj | adv | pron | det)
P = (prep | particle | inf. marker)
Figure 1: A simple part-of-speech-based regular expres-
sion reduces the number of incoherent extractions like
was central torpedo and covers relations expressed via
light verb constructions like gave a talk at.
The syntactic constraint requires the relation
phrase to match the POS tag pattern shown in Fig-
ure 1. The pattern limits relation phrases to be either
a verb (e.g., invented), a verb followed immediately
by a preposition (e.g., located in), or a verb followed
by nouns, adjectives, or adverbs ending in a preposi-
tion (e.g., has atomic weight of). If there are multiple
possible matches in a sentence for a single verb, the
longest possible match is chosen. Finally, if the pat-
tern matches multiple adjacent sequences, we merge
them into a single relation phrase (e.g., wants to ex-
tend). This refinement enables the model to readily
handle relation phrases containing multiple verbs. A
consequence of this pattern is that the relation phrase
must be a contiguous span of words in the sentence.
The syntactic constraint eliminates the incoherent
relation phrases returned by existing systems. For
example, given the sentence
Extendicare agreed to buy Arbor Health Care for
about US $432 million in cash and assumed debt.
TEXTRUNNER returns the extraction
(Arbor Health Care, for assumed, debt).
The phrase for assumed is clearly not a valid rela-
tion phrase: it begins with a preposition and splices
together two distant words in the sentence. The syn-
tactic constraint prevents this type of error by sim-
ply restricting relation phrases to match the pattern
in Figure 1.
The syntactic constraint reduces uninformative
extractions by capturing relation phrases expressed
via LVCs. For example, the POS pattern matched
against the sentence ?Faust made a deal with the
Devil,? would result in the relation phrase made a
deal with, instead of the uninformative made.
Finally, we require the relation phrase to appear
between its two arguments in the sentence. This is a
common constraint that has been implicitly enforced
in other open extractors.
3.2 Lexical Constraint
While the syntactic constraint greatly reduces unin-
formative extractions, it can sometimes match rela-
tion phrases that are so specific that they have only a
few possible instances, even in a Web-scale corpus.
Consider the sentence:
The Obama administration is offering only modest
greenhouse gas reduction targets at the conference.
The POS pattern will match the phrase:
is offering only modest greenhouse gas reduction targets at
(1)
Thus, there are phrases that satisfy the syntactic con-
straint, but are not relational.
To overcome this limitation, we introduce a lexi-
cal constraint that is used to separate valid relation
phrases from overspecified relation phrases, like the
example in (1). The constraint is based on the in-
tuition that a valid relation phrase should take many
distinct arguments in a large corpus. The phrase in
(1) is specific to the argument pair (Obama admin-
istration, conference), so it is unlikely to represent a
bona fide relation. We describe the implementation
details of the lexical constraint in Section 4.
3.3 Limitations
Our constraints represent an idealized model of re-
lation phrases in English. This raises the question:
How much recall is lost due to the constraints?
To address this question, we analyzed Wu and
Weld?s set of 300 sentences from a set of random
Web pages, manually identifying all verb-based re-
lationships between noun phrase pairs. This resulted
in a set of 327 relation phrases. For each rela-
tion phrase, we checked whether it satisfies our con-
straints. We found that 85% of the relation phrases
do satisfy the constraints. Of the remaining 15%,
we identified some of the common cases where the
constraints were violated, summarized in Table 3.
Many of the example relation phrases shown in
Table 3 involve long-range dependencies between
words in the sentence. These types of dependen-
cies are not easily representable using a pattern over
POS tags. A deeper syntactic analysis of the input
sentence would provide a much more general lan-
guage for modeling relation phrases. For example,
one could create a model of relations expressed in
1538
Binary Verbal Relation Phrases
85% Satisfy Constraints
8% Non-Contiguous Phrase Structure
Coordination: X is produced and maintained by Y
Multiple Args: X was founded in 1995 by Y
Phrasal Verbs: X turned Y off
4% Relation Phrase Not Between Arguments
Intro. Phrases: Discovered by Y, X . . .
Relative Clauses: . . . the Y that X discovered
3% Do Not Match POS Pattern
Interrupting Modifiers: X has a lot of faith in Y
Infinitives: X to attack Y
Table 3: Approximately 85% of the binary verbal relation
phrases in a sample of Web sentences satisfy our con-
straints.
terms of dependency parse features that would cap-
ture the non-contiguous relation phrases in Table 3.
Previous work has shown that dependency paths do
indeed boost the recall of relation extraction systems
(Wu and Weld, 2010; Mintz et al, 2009). While us-
ing dependency path features allows for a more flex-
ible model of relations, it significantly increases pro-
cessing time, which is problematic for Web-scale ex-
traction. Further, we have found that this increased
recall comes at the cost of lower precision on Web
text (see Section 5).
The results in Table 3 are similar to Banko and Et-
zioni?s findings that a set of eight POS patterns cover
a large fraction of binary verbal relation phrases.
However, their analysis was based on a set of sen-
tences known to contain either a company acquisi-
tion or birthplace relationship, while our results are
on a random sample of Web sentences. We applied
Banko and Etzioni?s verbal patterns to our random
sample of 300 Web sentences, and found that they
cover approximately 69% of the relation phrases in
the corpus. The gap in recall between this and the
85% shown in Table 3 is largely due to LVC relation
phrases (made a deal with) and phrases containing
multiple verbs (refuses to return to), which their pat-
terns do not cover.
In sum, our model is by no means complete.
However, we have empirically shown that the ma-
jority of binary verbal relation phrases in a sample
of Web sentences are captured by our model. By
focusing on this subset of language, our model can
be used to perform Open IE at significantly higher
precision than before.
4 REVERB
This section introduces REVERB, a novel open ex-
tractor based on the constraints defined in the previ-
ous section. REVERB first identifies relation phrases
that satisfy the syntactic and lexical constraints, and
then finds a pair of NP arguments for each identified
relation phrase. The resulting extractions are then
assigned a confidence score using a logistic regres-
sion classifier.
This algorithm differs in three important ways
from previous methods (Section 2). First, the re-
lation phrase is identified ?holistically? rather than
word-by-word. Second, potential phrases are fil-
tered based on statistics over a large corpus (the
implementation of our lexical constraint). Finally,
REVERB is ?relation first? rather than ?arguments
first?, which enables it to avoid a common error
made by previous methods?confusing a noun in the
relation phrase for an argument, e.g. the noun deal in
made a deal with.
4.1 Extraction Algorithm
REVERB takes as input a POS-tagged and NP-
chunked sentence and returns a set of (x, r, y)
extraction triples.2 Given an input sentence s,
REVERB uses the following extraction algorithm:
1. Relation Extraction: For each verb v in s,
find the longest sequence of words rv such that
(1) rv starts at v, (2) rv satisfies the syntactic
constraint, and (3) rv satisfies the lexical con-
straint. If any pair of matches are adjacent or
overlap in s, merge them into a single match.
2. Argument Extraction: For each relation
phrase r identified in Step 1, find the nearest
noun phrase x to the left of r in s such that x is
not a relative pronoun, WHO-adverb, or exis-
tential ?there?. Find the nearest noun phrase y
to the right of r in s. If such an (x, y) pair could
be found, return (x, r, y) as an extraction.
We check whether a candidate relation phrase
rv satisfies the syntactic constraint by matching it
against the regular expression in Figure 1.
2REVERB uses OpenNLP for POS tagging and NP chunk-
ing: http://opennlp.sourceforge.net/
1539
To determine whether rv satisfies the lexical con-
straint, we use a large dictionary D of relation
phrases that are known to take many distinct argu-
ments. In an offline step, we construct D by find-
ing all matches of the POS pattern in a corpus of
500 million Web sentences. For each matching re-
lation phrase, we heuristically identify its arguments
(as in Step 2 above). We set D to be the set of all
relation phrases that take at least k distinct argument
pairs in the set of extractions. In order to allow for
minor variations in relation phrases, we normalize
each relation phrase by removing inflection, auxil-
iary verbs, adjectives, and adverbs. Based on ex-
periments on a held-out set of sentences, we found
that a value of k = 20 works well for filtering out
overspecified relations. This results in a set of ap-
proximately 1.7 million distinct normalized relation
phrases, which are stored in memory at extraction
time.
As an example of the extraction algorithm in ac-
tion, consider the following input sentence:
Hudson was born in Hampstead, which is a
suburb of London.
Step 1 of the algorithm identifies three relation
phrases that satisfy the syntactic and lexical con-
straints: was, born in, and is a suburb of. The first
two phrases are adjacent in the sentence, so they are
merged into the single relation phrase was born in.
Step 2 then finds an argument pair for each relation
phrase. For was born in, the nearest NPs are (Hud-
son, Hampstead). For is a suburb of, the extractor
skips over the NP which and chooses the argument
pair (Hampstead, London). The final output is
e1: (Hudson, was born in, Hampstead)
e2: (Hampstead, is a suburb of, London).
4.2 Confidence Function
The extraction algorithm in the previous section has
high recall, but low precision. Like with previous
open extractors, we want way to trade recall for pre-
cision by tuning a confidence threshold. We use a
logistic regression classifier to assign a confidence
score to each extraction, which uses the features
shown in Table 4. All of these features are efficiently
computable and relation independent. We trained
the confidence function by manually labeling the ex-
tractions from a set of 1, 000 sentences from the Web
and Wikipedia as correct or incorrect.
Weight Feature
1.16 (x, r, y) covers all words in s
0.50 The last preposition in r is for
0.49 The last preposition in r is on
0.46 The last preposition in r is of
0.43 len(s) ? 10 words
0.43 There is a WH-word to the left of r
0.42 r matches VW*P from Figure 1
0.39 The last preposition in r is to
0.25 The last preposition in r is in
0.23 10 words < len(s) ? 20 words
0.21 s begins with x
0.16 y is a proper noun
0.01 x is a proper noun
-0.30 There is an NP to the left of x in s
-0.43 20 words < len(s)
-0.61 r matches V from Figure 1
-0.65 There is a preposition to the left of x in s
-0.81 There is an NP to the right of y in s
-0.93 Coord. conjunction to the left of r in s
Table 4: REVERB uses these features to assign a confi-
dence score to an extraction (x, r, y) from a sentence s
using a logistic regression classifier.
Previous open extractors require labeled training
data to learn a model of relations, which is then used
to extract relation phrases from text. In contrast,
REVERB uses a specified model of relations for ex-
traction, and requires labeled data only for assigning
confidence scores to its extractions. Learning a con-
fidence function is a much simpler task than learning
a full model of relations, using two orders of magni-
tude fewer training examples than TEXTRUNNER or
WOE.
4.3 TEXTRUNNER-R
The model of relation phrases used by REVERB
is specified, but could a TEXTRUNNER-like sys-
tem learn this model from training data? While
it is difficult to answer such a question for all
possible permutations of features sets, training ex-
amples, and learning biases, we demonstrate that
TEXTRUNNER itself cannot learn REVERB?s model
even when re-trained using the output of REVERB
as labeled training data. The resulting system,
TEXTRUNNER-R, uses the same feature representa-
tion as TEXTRUNNER, but different parameters, and
a different set of training examples.
To generate positive instances, we ran REVERB
1540
on the Penn Treebank, which is the same dataset
that TEXTRUNNER is trained on. To generate neg-
ative instances from a sentence, we took each noun
phrase pair in the sentence that does not appear as
arguments in a REVERB extraction. This process
resulted in a set of 67, 562 positive instances, and
356, 834 negative instances. We then passed these
labeled examples to TEXTRUNNER?s training proce-
dure, which learns a linear-chain CRF using closed-
class features like POS tags, capitalization, punctu-
ation, etc.TEXTRUNNER-R uses the argument-first
extraction algorithm described in Section 2.
5 Experiments
We compare REVERB to the following systems:
? REVERB?lex - The REVERB system described
in the previous section, but without the lexical
constraint. REVERB?lex uses the same confi-
dence function as REVERB.
? TEXTRUNNER - Banko and Etzioni?s 2008 ex-
tractor, which uses a second order linear-chain
CRF trained on extractions heuristically gener-
ated from the Penn Treebank. TEXTRUNNER
uses shallow linguistic features in its CRF,
which come from the same POS tagger and NP-
chunker that REVERB uses.
? TEXTRUNNER-R - Our modification to
TEXTRUNNER, which uses the same extrac-
tion code, but with a model of relations trained
on REVERB extractions.
? WOEpos - Wu and Weld?s modification to
TEXTRUNNER, which uses a model of re-
lations learned from extractions heuristically
generated from Wikipedia.
? WOEparse - Wu and Weld?s parser-based ex-
tractor, which uses a large dictionary of depen-
dency path patterns learned from heuristic ex-
tractions generated from Wikipedia.
Each system is given a set of sentences as input,
and returns a set of binary extractions as output. We
created a test set of 500 sentences sampled from the
Web, using Yahoo?s random link service.3 After run-
3http://random.yahoo.com/bin/ryl
REVERB REVERB WOE TEXT- WOE TEXT-
0.0
0.1
0.2
0.3
0.4
0.5
A
r
e
a
U
n
d
e
r
P
R
C
u
r
v
e
?lex
parse
RUNNER-R
pos
RUNNER
A
r
e
a
U
n
d
e
r
P
R
C
u
r
v
e
Figure 2: REVERB outperforms state-of-the-art open
extractors, with an AUC more than twice that of
TEXTRUNNER or WOEpos, and 38% higher than
WOEparse.
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Comparison of REVERB-Based Systems
REVERB
REVERB
?lex
TEXTRUNNER-R
P
r
e
c
i
s
i
o
n
Figure 3: The lexical constraint gives REVERB
a boost in precision and recall over REVERB?lex.
TEXTRUNNER-R is unable to learn the model used by
REVERB, which results in lower precision and recall.
ning each extractor over the input sentences, two hu-
man judges independently evaluated each extraction
as correct or incorrect. The judges reached agree-
ment on 86% of the extractions, with an agreement
score of ? = 0.68. We report results on the subset
of the data where the two judges concur.
The judges labeled uninformative extractions con-
servatively. That is, if critical information was
dropped from the relation phrase but included in the
second argument, it is labeled correct. For example,
both the extractions (Ackerman, is a professor of, bi-
ology) and (Ackerman, is, a professor of biology) are
considered correct.
Each system returns confidence scores for its ex-
tractions. For a given threshold, we can measure
the precision and recall of the output. Precision
is the fraction of returned extractions that are cor-
rect. Recall is the fraction of correct extractions in
1541
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Extractions
REVERB
WOE
parse
WOE
pos
TEXTRUNNER
P
r
e
c
i
s
i
o
n
Figure 4: REVERB achieves significantly higher preci-
sion than state-of-the-art Open IE systems, and compara-
ble recall to WOEparse.
the corpus that are returned. We use the total num-
ber of extractions labeled as correct by the judges
as our measure of recall for the corpus. In order to
avoid double-counting, we treat extractions that dif-
fer superficially (e.g., different punctuation or drop-
ping inessential modifiers) as a single extraction. We
compute a precision-recall curve by varying the con-
fidence threshold, and then compute the area under
the curve (AUC).
5.1 Results
Figure 2 shows the AUC of each system. REVERB
achieves an AUC that is 30% higher than WOEparse
and is more than double the AUC of WOEpos or
TEXTRUNNER. The lexical constraint provides a
significant boost in performance, with REVERB
achieving an AUC 23% higher than REVERB?lex.
REVERB proves to be a useful source of train-
ing data, with TEXTRUNNER-R having an AUC
71% higher than TEXTRUNNER and performing
on par with WOEpos. From the training data,
TEXTRUNNER-R was able to learn a model that
predicts contiguous relation phrases, but still re-
turned incoherent relation phrases (e.g., starting with
a preposition) and overspecified relation phrases.
These errors are due to TEXTRUNNER-R overfitting
the training data and not having access to the lexical
constraint.
Figure 3 shows the precision-recall curves of the
systems introduced in this paper. TEXTRUNNER-R
has much lower precision than REVERB and
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Relations Only
REVERB
WOE
parse
WOE
pos
TEXTRUNNER
P
r
e
c
i
s
i
o
n
Figure 5: On the subtask of identifying relations phrases,
REVERB is able to achieve even higher precision and re-
call than other systems.
REVERB?lex at all levels of recall. The lexi-
cal constraint gives REVERB a boost in precision
over REVERB?lex, reducing overspecified extrac-
tions from 20% of REVERB?lex?s output to 1% of
REVERB?s. The lexical constraint also boosts recall
over REVERB?lex, since REVERB is able to find a
correct relation phrase where REVERB?lex finds an
overspecified one.
Figure 4 shows the precision-recall curves of
REVERB and the external systems. REVERB has
much higher precision than the other systems at
nearly all levels of recall. In particular, more than
30% of REVERB?s extractions are at precision 0.8
or higher, compared to virtually none for the other
systems. WOEparse achieves a slightly higher recall
than REVERB (0.62 versus 0.64), but at the cost of
lower precision.
In order to highlight the role of the relational
model of each system, we also evaluate their per-
formance on the subtask of extracting just the rela-
tion phrases from the input text. Figure 5 shows the
precision-recall curves for each system on the rela-
tion phrase-only evaluation. In this case, REVERB
has both higher precision and recall than the other
systems.
REVERB?s biggest improvement came from the
elimination of incoherent extractions. Incoher-
ent extractions were a large fraction of the errors
made by previous systems, accounting for approxi-
mately 13% of TEXTRUNNER?s extractions, 15% of
WOEpos?s, and 30% of WOEparse?s. Uninformative
1542
REVERB - Incorrect Extractions
65% Correct relation phrase, incorrect arguments
16% N-ary relation
8% Non-contiguous relation phrase
2% Imperative verb
2% Overspecified relation phrase
7% Other, including POS/chunking errors
Table 5: The majority of the incorrect extractions re-
turned by REVERB are due to errors in argument extrac-
tion.
extractions had a smaller effect on other systems?
precision, accounting for 4% of WOEparse?s extrac-
tions, 5% of WOEpos?s, and 7% of TEXTRUNNER?s,
while only appearing in 1% of REVERB?s extrac-
tions. REVERB?s reduction in uninformative extrac-
tions resulted in a boost in recall, capturing many
LVC relation phrases missed by other systems (like
those shown in Table 2).
To test the systems? speed, we ran each extrac-
tor on a set of 100, 000 sentences using a Pen-
tium 4 machine with 4GB of RAM. The process-
ing times were 16 minutes for REVERB, 21 min-
utes for TEXTRUNNER, 21 minutes for WOEpos, and
11 hours for WOEparse. The times for REVERB,
TEXTRUNNER, and WOEpos are all approximately
the same, since they all use the same POS-tagging
and NP-chunking software. WOEparse processes
each sentence with a dependency parser, resulting
in much longer processing time.
5.2 REVERB Error Analysis
To better understand the limitations of REVERB, we
performed a detailed analysis of its errors in pre-
cision (incorrect extractions returned by REVERB)
and its errors in recall (correct extractions that
REVERB missed).
Table 5 summarizes the types of incorrect extrac-
tions that REVERB returns. We found that 65% of
the incorrect extractions returned by REVERB were
cases where a relation phrase was correctly identi-
fied, but the argument-finding heuristics failed. The
remaining errors were cases where REVERB ex-
tracted an incorrect relation phrase. One common
mistake that REVERB made was extracting a rela-
tion phrase that expresses an n-ary relationship via
a ditransitive verb. For example, given the sentence
REVERB - Missed Extractions
52% Could not identify correct arguments
23% Relation filtered out by lexical constraint
17% Identified a more specific relation
8% POS/chunking error
Table 6: The majority of extractions that were missed by
REVERB were cases where the correct relation phrase
was found, but the arguments were not correctly identi-
fied.
?I gave him 15 photographs,? REVERB extracts (I,
gave, him). These errors are due to the fact that
REVERB only models binary relations.
Table 6 summarizes the correct extractions that
were extracted by other systems and were not ex-
tracted by REVERB. As with the false positive ex-
tractions, the majority of false negatives (52%) were
due to the argument-finding heuristics choosing the
wrong arguments, or failing to extract all possible ar-
guments (in the case of coordinating conjunctions).
Other sources of failure were due to the lexical con-
straint either failing to filter out an overspecified re-
lation phrase or filtering out a valid relation phrase.
These errors hurt both precision and recall, since
each case results in the extractor overlooking a cor-
rect relation phrase and choosing another.
5.3 Evaluation At Scale
Section 5.1 shows that REVERB outperforms ex-
isting Open IE systems when evaluated on a sam-
ple of sentences. Previous work has shown that
the frequency of an extraction in a large corpus is
useful for assessing the correctness of extractions
(Downey et al, 2005). Thus, it is possible a pri-
ori that REVERB?s gains over previous systems will
diminish when extraction frequency is taken into ac-
count.
In fact, we found that REVERB?s advantage over
TEXTRUNNER when run at scale is qualitatively
similar to its advantage on single sentences. We ran
both REVERB and TEXTRUNNER on Banko and Et-
zioni?s corpus of 500 million Web sentences and ex-
amined the effect of redundancy on precision.
As Downey?s work predicts, precision increased
in both systems for extractions found multiple
times, compared with extractions found only once.
However, REVERB had higher precision than
1543
TEXTRUNNER at all frequency thresholds. In fact,
REVERB?s frequency 1 extractions had a precision
of 0.75, which TEXTRUNNER could not approach
even with frequency 10 extractions, which had a
precision of 0.34. Thus, REVERB is able to return
more correct extractions at a higher precision than
TEXTRUNNER, even when redundancy is taken into
account.
6 Conclusions and Future Work
The paper?s contributions are as follows:
? We have identified and analyzed the problems
of incoherent and uninformative extractions for
Open IE systems, and shown their prevalence
for systems such as TEXTRUNNER and WOE.
? We articulated general, easy-to-enforce con-
straints on binary, verb-based relation phrases
in English that ameliorate these problems and
yield richer and more informative relations
(see, for example, Table 2).
? Based on these constraints, we designed, im-
plemented, and evaluated the REVERB extrac-
tor, which substantially outperforms previous
Open IE systems in both recall and precision.
? We make REVERB and the data used in our
experiments available to the research commu-
nity.4
In future work, we plan to explore utilizing our
constraints to improve the performance of learned
CRF models. Roth et al have shown how to incor-
porate constraints into CRF learners (Roth and Yih,
2005). It is natural, then, to consider whether the
combination of heuristically labeled training exam-
ples, CRF learning, and our constraints will result
in superior performance. The error analysis in Sec-
tion 5.2 also suggests natural directions for future
work. For instance, since many of REVERB?s errors
are due to incorrect arguments, improved methods
for argument extraction are in order.
Acknowledgments
We would like to thank Mausam, Dan Weld, Yoav
Artzi, Luke Zettlemoyer, members of the KnowItAll
4http://reverb.cs.washington.edu
group, and the anonymous reviewers for their help-
ful comments. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, and DARPA contract FA8750-09-C-0179,
and carried out at the University of Washington?s
Turing Center.
References
David J. Allerton. 2002. Stretched Verb Constructions in
English. Routledge Studies in Germanic Linguistics.
Routledge (Taylor and Francis), New York.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In In the Proceedings
of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, January.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, FAM-LbR ?10, pages 52?60, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034?1041.
Gregory Grefenstette and Simone Teufel. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proceedings of the sev-
enth conference on European chapter of the Associa-
tion for Computational Linguistics, pages 98?103, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 286?
295, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
1544
Holmer Hemsen Kathrin Eichler and Gnter Neu-
mann. 2008. Unsupervised relation extraction
from web documents. In LREC. http://www.lrec-
conf.org/proceedings/lrec2008/.
J. Kim and D. Moldovan. 1993. Acquisition of semantic
patterns for information extraction from corpora. In
Procs. of Ninth IEEE Conference on Artificial Intelli-
gence for Applications, pages 171?176.
Dekang Lin and Patrick Pantel. 2001. DIRT-Discovery
of Inference Rules from Text. In Proceedings of
ACM Conference on Knowledge Discovery and Data
Mining(KDD-01), pages pp. 323?328.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing Functional Relations in Web Text. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1266?1276, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ?09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003?1011, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
E. Riloff. 1996. Automatically constructing extraction
patterns from untagged text. In Procs. of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044?1049.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Pref-
erences. In ACL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ?05, pages 736?743, New
York, NY, USA. ACM.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731?738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 304?311, New York City, USA,
June. Association for Computational Linguistics.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open in-
formation extraction to domain-specific relations. AI
Magazine, 31(3):93?102.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272.
Lucia Specia and Enrico Motta. 2006. M.: A hybrid
approach for extracting semantic relations from texts.
In In. Proceedings of the 2 nd Workshop on Ontology
Learning and Population, pages 57?64.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity
of light verb constructions. In 2nd ACL Workshop on
Multiword Expressions, pages 1?8.
M. Stevenson. 2004. An unsupervised WordNet-based
algorithm for relation extraction. In Proceedings of
the ?Beyond Named Entity? workshop at the Fourth
International Conference on Language Resources and
Evalutaion (LREC?04).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 118?127, Morristown,
NJ, USA. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW ?09:
Proceedings of the 18th international conference on
World wide web, pages 101?110, New York, NY, USA.
ACM.
1545
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 12?16,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Chinese Open Relation Extraction for Knowledge Acquisition Yuen-Hsien Tseng1, Lung-Hao Lee1,2, Shu-Yen Lin1, Bo-Shun Liao1,  Mei-Jun Liu1, Hsin-Hsi Chen2, Oren Etzioni3, Anthony Fader4   1Information Technology Center, National Taiwan Normal University  2Dept. of Computer Science and Information Engineering, National Taiwan University 3Allen Institute for Artificial Intelligence, Seattle, WA 4Dept. of Computer Science and Engineering, University of Washington  {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu  Abstract 
This study presents the Chinese Open Relation Extraction (CORE) system that is able to extract entity-relation triples from Chinese free texts based on a series of NLP techniques, i.e., word segmentation, POS tagging, syntactic parsing, and extraction rules. We employ the proposed CORE techniques to extract more than 13 million entity-relations for an open domain question answering application. To our best knowledge, CORE is the first Chinese Open IE system for knowledge acquisition.  1 Introduction  Traditional Information Extraction (IE) involves human intervention of handcrafted rules or tagged examples as the input for machine learning to recognize the assertion of a particular relationship between two entities in texts (Riloff, 1996; Soderland, 1999). Although machine learning helps enumerate potential relation patterns for extraction, this approach is often limited to extracting the relation sets that are predefined. In addition, traditional IE has focused on satisfying pre-specified requests from small homogeneous corpora, leaving the question open whether it can scale up to massive and heterogeneous corpora such as the Web (Banko and Etzioni, 2008; Etzioni et al., 2008, 2011). Open IE, a new domain-independent knowledge discovery paradigm that extracts a diverse set of relations without requiring any relation-specific human inputs and a pre-specified vocabulary, is especially suited to 
massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. The relatively rich morpho-syntactic marking system of English (e.g., verbal inflection, nominal case, clausal markers) makes the syntactic roles of many words detectable from their surface forms. A tensed verb in English, for example, generally indicates its main verb status of a clause. The pinning down of the main verb in a Chinese clause, on the other hand, must rely on other linguistic cues such as word context due to the lack of tense markers. In contrast to the syntax-oriented English language, Chinese is discourse-oriented and rich in ellipsis ? meaning is often construable in the absence of explicit linguistic devices such that many obligatory grammatical categories (e.g., pronouns and BE verbs) can be elided in Chinese.  For example, the three Chinese sentences ???????? (?Apples nutritious?), ????????? ? (?Apples are nutritious?), and ???????? 
12
(?Apples are rich in nutrition?) are semantically synonymous sentences, but the first one, which lacks an overt verb, is used far more often than the other two. Presumably, an adequate multilingual IE system must take into account those intrinsic differences between languages. This paper introduces the Chinese Open Relation Extraction (CORE) system, which utilizes a series of NLP techniques to extract relations embedded in Chinese sentences. Given a Chinese text as the input, CORE employs word segmentation, part-of-speech (POS) tagging, and syntactic parsing, to automatically annotate the Chinese sentences. Based on this rich information, the input sentences are chunked and the entity-relation triples are extracted. Our evaluation shows the effectiveness of CORE, and its deficiency as well. 2 Related Work TextRunner (Banko et al., 2007) was the first Open IE system, which trains a Na?ve Bayes classifier with POS and NP-chunk features to extract relationships between entities. The subsequent work showed that employing the classifiers capable of modeling the sequential information inherited in the texts, like linear-chain CRF (Banko and Etzioni, 2008) and Markov Logic Network (Zhu et al., 2009), can result in better extraction performance. The WOE system (Wu and Weld, 2010) adopted Wikipedia as the training source for their extractor. Experimental results indicated that parsed dependency features lead to further improvements over TextRunner.  ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-Verb-Argument triple. This approach involves only POS tagging for English and ?regular expression?-like matching. As such, it is suitable for large corpora, and likely to be applicable to Chinese.  
For multilingual open IE, Gamallo et al. (2012) adopts a rule-based dependency parser to extract relations represented in English, Spanish, Portuguese, and Galician. For each parsed sentence, they separate each verbal clause and then identify each one?s verb participants, including their functions: subject, direct object, attribute, and prepositional complements. A set of rules is then applied on the clause constituents to extract the target triples. For Chinese open IE, we adopt a similar general approach. The main differences are the processing steps specific to Chinese language. 3 Chinese Open Relation Extraction This section describes the components of CORE. Not requiring any predefined vocabulary, CORE?s sole input is a Chinese corpus and its output is an extracted set of relational tuples. The system consists of three key modules, i.e., word segmentation and POS tagging, syntactic parsing, and entity-relation triple extraction, which are introduced as follows: Chinese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-of-vocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with part-of-speeches (Tsai and Chen, 2004). Take the Chinese sentence ?????????? (?Edison invented the light bulb?) for instance. It was segmented and tagged as follows: ???/Nb  ??/VC  ?/Di  ??/Na. Among these words, the translation of a foreign proper name ????? (?Edison?) is not likely to be included in a lexicon and therefore is extracted by the unknown word detection method. In this case, 
13
the special POS tag ?Di? is a tag to represent a verb?s tense when its character ??? follows immediately after its precedent verb. The complete set of part-of-speech tags is defined in the technical report (CKIP, 1993). In the above sentence, ?? ? could represent a complete different meaning if it is associate with other character, such as ???? meaning ?understand?. Therefore, ????????? ? (?Edison invented a cure?) would be segmented incorrectly once ?? ? is associated with its following character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the context-dependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ????/Nb? and ??? /Na? were annotated as two nominal phrases (i.e., ?NP?), and ???/VC  ?/Di? was annotated as a verbal phrase (i.e., ?VP?). CKIP parser also adopts dependency decision-making and example-based approaches to label the semantic role ?Head?, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the ?Head?-labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head?s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (???/Edison, ???/invented, ??/light bulb) is extracted by this approach. Figure 1 shows the parsed tree of a Chinese sentence for the relation extraction by CORE. The Chinese sentence ???????????
????????? (?Democrats on the House Budget Committee released a report on Monday?) is the manual translation of one of the English sentences evaluated by ReVerb (Fader et al., 2011). The first step of CORE involves word-segmentation and POS-tagging, thus returning eight word/POS pairs: ??/Nc, ??/Na, ???/Nc, ?/DE, ???/Nb, ???/Nd, ??/VE, ?? /Na. Next, ???? /Nd ?? /VE? is identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP ??? ?? ??? ? ???? and NP ???? are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., ??????????? / ????? / ??, ?Democrats on the House Budget Committee / on Monday released / a report?) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains ?Head?-labeled words only, can be applied to strain out from each component of this triple the most prominent word: ???? / ?? / ??? (?Democrats / released / report?). 
 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and Evaluation We adopted the same test set released by ReVerb for performance evaluation. The test set consists of 500 English sentences randomly sampled from the Web and were annotated using a pooling method. To obtain ?gold standard? relation triples in Chinese, the 500 test sentences were manually translated from English to Chinese by a 
14
trained native Chinese speaker and verified by another. Additionally, two other native Chinese speakers annotated the relation triples for each Chinese sentence. In total, 716 Chinese entity-relation triples with an agreement score of 0.79 between the two annotators were obtained and regarded as gold standard.  Performance evaluation of CORE was conducted based on: 1) exact match; and 2) relation-only match. For exact match, each component of the extracted triple must be identical with the gold standard. For relation-only match, the extracted triple is regarded as a correct case if an extracted relation agreed with the relation of the gold standard.  Without another Chinese Open IE system for performance comparison, we compared CORE with a modification of ReVerb system capable of handling Chinese sentences. The modification of ReVerb?s verb-driven regular expression matching was kept to a minimum to deal with language-specific processing. As such, ReVerb remains mostly the same as its English counterpart so that a bilingual (Chinese/English) Open IE system can be easily implemented. Table 1 shows the experimental results. Our CORE system obviously performs better than ReVerb when recall is considered for both exact and relation-only match. The results suggest that utilizing more sophisticated NLP techniques is effective to extract relations without any specific human intervention. In addition, there is a slight decrease in the precision of exact match for CORE. This reveals that ReVerb?s original syntactic and lexical constraints are also useful to identify the arguments and their relationship precisely. In summary, CORE achieved relatively promising F1 scores. These results imply that CORE method is more suitable for Chinese open relation extraction. 
Chinese Open IE Precision Recall F1 Exact Match ReVerb 0.5820 0.0987 0.1688 CORE 0.5579 0.3291 0.4140 Relation Only ReVerb 0.8361 0.1425 0.2435 CORE 0.8463 0.5000 0.6286 Table 1: Performance evaluation on Chinese Open IE. 
We also analyzed the errors made by the CORE model. Almost all the errors resulted from incorrect parsing. Enhancing the parsing effectiveness is most likely to improve the performance of CORE. The relatively low recall rate also indicates that CORE misses many types of relation expression. Ellipsis and flexibility in Chinese syntax are so difficult not only to fail the parser, but also the extraction attempts to bypass the parsing errors. To demonstrate the applicability of CORE, we implement a Chinese Question-Answering (QA) system based on two million news articles from 2002 to 2009 published by the United Daily News Group (udn.com/NEWS). CORE extracted more than 13 million unique entity-relation triples from this corpus. These extracted relations are useful for knowledge acquisition. Take the question ????????? ? (?What is originated from China??) as an example, the relation is automatically identified as ?? ? (?originate?) that heads the following entity ??? ? (?China?). Our open QA system then searched the triples and returned the first entity as the answers. In addition to the obvious answer ???? (?Chinese medicine?), which is usually considered as common-sense knowledge, we also obtained those that are less known, such as the traditional Japanese food ???? (?natto?) and the musical instrument ????? (?accordion?). 5 Conclusions This work demonstrates the feasibility of extracting relations from Chinese corpus without the input of any predefined vocabulary to IE systems. This work is the first to explore Chinese open relation extraction to our best knowledge.  Acknowledgments 
This research was partially supported by National Science Council, Taiwan under grant NSC102-2221-E-002-103-MY3, and the ?Aim for the Top University Project? of National Taiwan Normal University, sponsored by the Ministry of Education, Taiwan. 
15
References  Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. Proceedings of EMNLP?11, pages 1535-1545. Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-Ming Gao, and Kuang-Yu Chen. 2000. Sinina Treebank: design criteria, annotation guidelines, and on-line interface. Proceedings of SIGHAN?00, pages 29-37. Chinese Knowledge Information Processing (CKIP) Group. 1993. Categorical analysis of Chinese. ACLCLP Technical Report # 93-05, Academia Sinica.  Fei Wu and Daniel S. Weld. 2010. Open information extraction using Wikipedia. Proceedings of ACL?10, pages 118-127. Jia-Ming You, and Keh-Jiann Chen. 2004. Automatic semantic role assignment for a tree structure.  In Proceedings of SIGHAN?04, pages 1-8. Jun Zhu, Zaiqing Nie, Xiaojiang Lium Bo Zhang, and Ji-Rong Wen. 2009. StatSnowball: a statistical approach to extracting entity relationships. In Proceedings of WWW?09, pages 101-110. Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word extraction for Chinese documents. In Proceedings of COLING?02, pages 169-175. Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. Proceedings of IJCAI?07, pages 2670-2676. 
Michele Banko, and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. Proceedings of ACL?08, pages 28-26.   Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open information extraction: the second generation. In Proceedings of IJCAI?11, pages 3-10. Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68-74. Pablo Gamallo, Marcos Garcia, and Santiago Fern?ndez-Lanza. 2012. Dependency-based open information extraction. In Proceedings of ROBUS-UNSUP?12, pages 10-18.  Elleen Riloff. 1996. Automatically constructing extraction patterns from untagged text. In Proceedings of AAAI?96, pages 1044-1049.  Stephen Soderland. 1999. Learning information extraction rules for semi-structured and free text.  Machine Learning, 34(1-3):233-272. Yu-Ming Hsieh, Ming-Hong Bai, Jason S. Chang, and Keh-Jiann Chen. 2012. Improving PCFG Chinese Parsing with Context-Dependent Probability Re-estimation. Proceedings of CLP?12, pages 216-221. Yu-Fang Tsai, and Keh-Jiann Chen. 2004. Reliable and cost-effective pos-tagging. International Journal of Computational Linguistics and Chinese Language Processing, 9(1):83-96. Yuen-Hsien Tseng, Lung-Hao Lee, and Liang-Chih Yu 2012. Traditional Chinese parsing evaluation at SIGHAN Bake-offs 2012. Proceedings of CLP?12, pages 199-205.   
16
Proceedings of the ACL 2010 Conference Short Papers, pages 286?290,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extracting Sequences from the Web
Anthony Fader, Stephen Soderland, and Oren Etzioni
University of Washington, Seattle
{afader,soderlan,etzioni}@cs.washington.edu
Abstract
Classical Information Extraction (IE) sys-
tems fill slots in domain-specific frames.
This paper reports on SEQ, a novel
open IE system that leverages a domain-
independent frame to extract ordered se-
quences such as presidents of the United
States or the most common causes of death
in the U.S. SEQ leverages regularities
about sequences to extract a coherent set
of sequences from Web text. SEQ nearly
doubles the area under the precision-recall
curve compared to an extractor that does
not exploit these regularities.
1 Introduction
Classical IE systems fill slots in domain-specific
frames such as the time and location slots in sem-
inar announcements (Freitag, 2000) or the terror-
ist organization slot in news stories (Chieu et al,
2003). In contrast, open IE systems are domain-
independent, but extract ?flat? sets of assertions
that are not organized into frames and slots
(Sekine, 2006; Banko et al, 2007). This paper
reports on SEQ?an open IE system that leverages
a domain-independent frame to extract ordered se-
quences of objects from Web text. We show that
the novel, domain-independent sequence frame in
SEQ substantially boosts the precision and recall
of the system and yields coherent sequences fil-
tered from low-precision extractions (Table 1).
Sequence extraction is distinct from set expan-
sion (Etzioni et al, 2004; Wang and Cohen, 2007)
because sequences are ordered and because the ex-
traction process does not require seeds or HTML
lists as input.
The domain-independent sequence frame con-
sists of a sequence name s (e.g., presidents of the
United States), and a set of ordered pairs (x, k)
where x is a string naming a member of the se-
quence with name s, and k is an integer indicating
Most common cause of death in the United States:
1. heart disease, 2. cancer, 3. stroke, 4. COPD,
5. pneumonia, 6. cirrhosis, 7. AIDS, 8. chronic liver
disease, 9. sepsis, 10. suicide, 11. septic shock.
Largest tobacco company in the world:
1. Philip Morris, 2. BAT, 3. Japan Tobacco,
4. Imperial Tobacco, 5. Altadis.
Largest rodent in the world:
1. Capybara, 2. Beaver, 3. Patagonian Cavies. 4. Maras.
Sign of the zodiac:
1. Aries, 2. Taurus, 3. Gemini, 4. Cancer, 5. Leo,
6. Virgo, 7. Libra, 8. Scorpio, 9. Sagittarius,
10. Capricorn, 11. Aquarius, 12. Pisces, 13. Ophiuchus.
Table 1: Examples of sequences extracted by SEQ
from unstructured Web text.
its position (e.g., (Washington, 1) and (JFK, 35)).
The task of sequence extraction is to automatically
instantiate sequence frames given a corpus of un-
structured text.
By definition, sequences have two properties
that we can leverage in creating a sequence ex-
tractor: functionality and density. Functionality
means position k in a sequence is occupied by a
single real-world entity x. Density means that if
a value has been observed at position k then there
must exist values for all i < k, and possibly more
after it.
2 The SEQ System
Sequence extraction has two parts: identify-
ing possible extractions (x, k, s) from text, and
then classifying those extractions as either cor-
rect or incorrect. In the following section, we
describe a way to identify candidate extractions
from text using a set of lexico-syntactic patterns.
We then show that classifying extractions based
on sentence-level features and redundancy alone
yields low precision, which is improved by lever-
aging the functionality and density properties of
sequences as done in our SEQ system.
286
Pattern Example
the ORD the fifth
the RB ORD the very first
the JJS the best
the RB JJS the very best
the ORD JJS the third biggest
the RBS JJ the most popular
the ORD RBS JJ the second least likely
Table 2: The patterns used by SEQ to detect ordi-
nal phrases are noun phrases that begin with one
of the part-of-speech patterns listed above.
2.1 Generating Sequence Extractions
To obtain candidate sequence extractions (x, k, s)
from text, the SEQ system finds sentences in its
input corpus that contain an ordinal phrase (OP).
Table 2 lists the lexico-syntactic patterns SEQ uses
to detect ordinal phrases. The value of k is set to
the integer corresponding to the ordinal number in
the OP.1
Next, SEQ takes each sentence that contains an
ordinal phrase o, and finds candidate items of the
form (x, k) for the sequence with name s. SEQ
constrains x to be an NP that is disjoint from o, and
s to be an NP (which may have post-modifying
PPs or clauses) following the ordinal number in o.
For example, given the sentence ?With help
from his father, JFK was elected as the 35th Pres-
ident of the United States in 1960?, SEQ finds
the candidate sequences with names ?President?,
?President of the United States?, and ?President of
the United States in 1960?, each of which has can-
didate extractions (JFK, 35), (his father, 35), and
(help, 35). We use heuristics to filter out many of
the candidate values (e.g., no value should cross a
sentence-like boundary, and x should be at most
some distance from the OP).
This process of generating candidate ex-
tractions has high coverage, but low preci-
sion. The first step in identifying correct ex-
tractions is to compute a confidence measure
localConf(x, k, s|sentence), which measures
how likely (x, k, s) is given the sentence it came
from. We do this using domain-independent syn-
tactic features based on POS tags and the pattern-
based features ?x {is,are,was,were} the kth s? and
?the kth s {is,are,was,were} x?. The features are
then combined using a Naive Bayes classifier.
In addition to the local, sentence-based features,
1Sequences often use a superlative for the first item (k =
1) such as ?the deepest lake in Africa?, ?the second deepest
lake in Africa? (or ?the 2nd deepest ...?), etc.
we define the measure totalConf that takes into
account redundancy in an input corpus C. As
Downey et al observed (2005), extractions that
occur more frequently in multiple distinct sen-
tences are more likely to be correct.
totalConf(x, k, s|C) =
?
sentence?C
localConf(x, k, s|sentence) (1)
2.2 Challenges
The scores localConf and totalConf are not suffi-
cient to identify valid sequence extractions. They
tend to give high scores to extractions where the
sequence scope is too general or too specific. In
our running example, the sequence name ?Presi-
dent? is too general ? many countries and orga-
nizations have a president. The sequence name
?President of the United States in 1960? is too spe-
cific ? there were not multiple U.S. presidents in
1960.
These errors can be explained as violations of
functionality and density. The sequence with
name ?President? will have many distinct candi-
date extractions in its positions, which is a vio-
lation of functionality. The sequence with name
?President of the United States in 1960? will not
satisfy density, since it will have extractions for
only one position.
In the next section, we present the details of how
SEQ incorporates functionality and density into its
assessment of a candidate extraction.
Given an extraction (x, k, s), SEQ must clas-
sify it as either correct or incorrect. SEQ breaks
this problem down into two parts: (1) determining
whether s is a correct sequence name, and (2) de-
termining whether (x, k) is an item in s, assuming
s is correct.
A joint probabilistic model of these two deci-
sions would require a significant amount of la-
beled data. To get around this problem, we repre-
sent each (x, k, s) as a vector of features and train
two Naive Bayes classifiers: one for classifying s
and one for classifying (x, k). We then rank ex-
tractions by taking the product of the two classi-
fiers? confidence scores.
We now describe the features used in the two
classifiers and how the classifiers are trained.
Classifying Sequences To classify a sequence
name s, SEQ uses features to measure the func-
tionality and density of s. Functionality means
287
that a correct sequence with name s has one cor-
rect value x at each position k, possibly with ad-
ditional noise due to extraction errors and synony-
mous values of x. For a fixed sequence name s
and position k, we can weight each of the candi-
date x values in that position by their normalized
total confidence:
w(x|k, s, C) =
totalConf(x, k, s|C)
?
x
? totalConf(x?, k, s|C)
For overly general sequences, the distribution of
weights for a position will tend to be more flat,
since there are many equally-likely candidate x
values. To measure this property, we use a func-
tion analogous to information entropy:
H(k, s|C) = ?
?
x
w(x|k, s, C) log
2
w(x|k, s, C)
Sequences s that are too general will tend to have
high values of H(k, s|C) for many values of k.
We found that a good measure of the overall non-
functionality of s is the average value of H(k, s|C)
for k = 1, 2, 3, 4.
For a sequence name s that is too specific, we
would expect that there are only a few filled-in po-
sitions. We model the density of s with two met-
rics. The first is numFilledPos(s|C), the num-
ber of distinct values of k such that there is some
extraction (x, k) for s in the corpus. The second
is totalSeqConf(s|C), which is the sum of the
scores of most confident x in each position:
totalSeqConf(s|C) =
?
k
max
x
totalConf(x, k, s|C) (2)
The functionality and density features are com-
bined using a Naive Bayes classifier. To train the
classifier, we use a set of sequence names s labeled
as either correct or incorrect, which we describe in
Section 3.
Classifying Sequence Items To classify (x, k)
given s, SEQ uses two features: the total con-
fidence totalConf(x, k, s|C) and the same total
confidence normalized to sum to 1 over all x, hold-
ing k and s constant. To train the classifier, we use
a set of extractions (x, k, s) where s is known to
be a correct sequence name.
3 Experimental Results
This section reports on two experiments. First, we
measured how the density and functionality fea-
tures improve performance on the sequence name
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Both Feature Sets
Only Density
Only Functionality
Max localConf
Figure 1: Using density or functionality features
alone is effective in identifying correct sequence
names. Combining both types of features outper-
forms either by a statistically significant margin
(paired t-test, p < 0.05).
classification sub-task (Figure 1). Second, we
report on SEQ?s performance on the sequence-
extraction task (Figure 2).
To create a test set, we selected all sentences
containing ordinal phrases from Banko?s 500M
Web page corpus (2008). To enrich this set O,
we obtained additional sentences from Bing.com
as follows. For each sequence name s satis-
fying localConf(x, k, s|sentence) ? 0.5 for
some sentence in O, we queried Bing.com for
?the kth s? for k = 1, 2, . . . until no more hits
were returned.2 For each query, we downloaded
the search snippets and added them to our cor-
pus. This procedure resulted in making 95, 611
search engine queries. The final corpus contained
3, 716, 745 distinct sentences containing an OP.
Generating candidate extractions using the
method from Section 2.1 resulted in a set of over
40 million distinct extractions, the vast majority
of which are incorrect. To get a sample with
a significant number of correct extractions, we
filtered this set to include only extractions with
totalConf(x, k, s|C) ? 0.8 for some sentence,
resulting in a set of 2, 409, 211 extractions.
We then randomly sampled and manually la-
beled 2, 000 of these extractions for evaluation.
We did a Web search to verify the correctness of
the sequence name s and that x is the kth item in
the sequence. In some cases, the ordering rela-
tion of the sequence name was ambiguous (e.g.,
2We queried for both the numeric form of the ordinal and
the number spelled out (e.g ?the 2nd ...? and ?the second ...?).
We took up to 100 results per query.
288
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
SEQ
REDUND
LOCAL
Figure 2: SEQ outperforms the baseline systems,
increasing the area under the curve by 247% rela-
tive to LOCAL and by 90% relative to REDUND.
?largest state in the US? could refer to land area or
population), which could lead to merging two dis-
tinct sequences. In practice, we found that most
ordering relations were used in a consistent way
(e.g., ?largest city in? always means largest by
population) and only about 5% of the sequence
names in our sample have an ambiguous ordering
relation.
We compute precision-recall curves relative to
this random sample by changing a confidence
threshold. Precision is the percentage of correct
extractions above a threshold, while recall is the
percentage correct above a threshold divided by
the total number of correct extractions. Because
SEQ requires training data, we used 15-fold cross
validation on the labeled sample.
The functionality and density features boost
SEQ?s ability to correctly identify sequence
names. Figure 1 shows how well SEQ can iden-
tify correct sequence names using only functional-
ity, only density, and using functionality and den-
sity in concert. The baseline used is the maximum
value of localConf(x, k, s) over all (x, k). Both
the density features and the functionality features
are effective at this task, but using both types of
features resulted in a statistically significant im-
provement over using either type of feature in-
dividually (paired t-test of area under the curve,
p < 0.05).
We measure SEQ?s efficacy on the complete
sequence-extraction task by contrasting it with two
baseline systems. The first is LOCAL, which
ranks extractions by localConf .3 The second is
3If an extraction arises from multiple sentences, we use
REDUND, which ranks extractions by totalConf .
Figure 2 shows the precision-recall curves for each
system on the test data. The area under the curves
for SEQ, REDUND, and LOCAL are 0.59, 0.31,
and 0.17, respectively. The low precision and flat
curve for LOCAL suggests that localConf is not
informative for classifying extractions on its own.
REDUND outperformed LOCAL, especially at
the high-precision part of the curve. On the subset
of extractions with correct s, REDUND can iden-
tify x as the kth item with precision of 0.85 at re-
call 0.80. This is consistent with previous work on
redundancy-based extractors on the Web. How-
ever, REDUND still suffered from the problems
of over-specification and over-generalization de-
scribed in Section 2. SEQ reduces the negative ef-
fects of these problems by decreasing the scores
of sequence names that appear too general or too
specific.
4 Related Work
There has been extensive work in extracting lists
or sets of entities from the Web. These extrac-
tors rely on either (1) HTML features (Cohen
et al, 2002; Wang and Cohen, 2007) to extract
from structured text or (2) lexico-syntactic pat-
terns (Hearst, 1992; Etzioni et al, 2005) to ex-
tract from unstructured text. SEQ is most similar
to this second type of extractor, but additionally
leverages the sequence regularities of functionality
and density. These regularities allow the system to
overcome the poor performance of the purely syn-
tactic extractor LOCAL and the redundancy-based
extractor REDUND.
5 Conclusions
We have demonstrated that an extractor leveraging
sequence regularities can greatly outperform ex-
tractors without this knowledge. Identifying likely
sequence names and then filling in sequence items
proved to be an effective approach to sequence ex-
traction.
One line of future research is to investigate
other types of domain-independent frames that ex-
hibit useful regularities. Other examples include
events (with regularities about actor, location, and
time) and a generic organization-role frame (with
regularities about person, organization, and role
played).
the maximal localConf .
289
6 Acknowledgements
This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-08-1-
0431, DARPA contract FA8750-09-C-0179, and
an NSF Graduate Research Fellowship, and was
carried out at the University of Washington?s Tur-
ing Center.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, pages 2670?2676.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
gap: Learning-based information extraction rival-
ing knowledge-engineering methods. In ACL, pages
216?223.
William W. Cohen, Matthew Hurst, and Lee S. Jensen.
2002. A flexible learning system for wrapping ta-
bles and lists in html documents. In In International
World Wide Web Conference, pages 232?241.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI, pages 1034?1041.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2004. Methods for domain-independent informa-
tion extraction from the Web: An experimental com-
parison. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence (AAAI-2004),
pages 391?398.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Er Yates. 2005. Unsupervised
named-entity extraction from the web: An experi-
mental study. Artificial Intelligence, 165:91?134.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2-3):169?202.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, pages
539?545.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 731?738, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. In ICDM, pages 342?350. IEEE
Computer Society.
290
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1608?1618,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Paraphrase-Driven Learning for Open Question Answering
Anthony Fader Luke Zettlemoyer Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195
{afader, lsz, etzioni}@cs.washington.edu
Abstract
We study question answering as a ma-
chine learning problem, and induce a func-
tion that maps open-domain questions to
queries over a database of web extrac-
tions. Given a large, community-authored,
question-paraphrase corpus, we demon-
strate that it is possible to learn a se-
mantic lexicon and linear ranking func-
tion without manually annotating ques-
tions. Our approach automatically gener-
alizes a seed lexicon and includes a scal-
able, parallelized perceptron parameter es-
timation scheme. Experiments show that
our approach more than quadruples the re-
call of the seed lexicon, with only an 8%
loss in precision.
1 Introduction
Open-domain question answering (QA) is a long-
standing, unsolved problem. The central challenge
is to automate every step of QA system construc-
tion, including gathering large databases and an-
swering questions against these databases. While
there has been significant work on large-scale in-
formation extraction (IE) from unstructured text
(Banko et al, 2007; Hoffmann et al, 2010; Riedel
et al, 2010), the problem of answering questions
with the noisy knowledge bases that IE systems
produce has received less attention. In this paper,
we present an approach for learning to map ques-
tions to formal queries over a large, open-domain
database of extracted facts (Fader et al, 2011).
Our system learns from a large, noisy, question-
paraphrase corpus, where question clusters have
a common but unknown query, and can span
a diverse set of topics. Table 1 shows exam-
ple paraphrase clusters for a set of factual ques-
tions. Such data provides strong signal for learn-
ing about lexical variation, but there are a number
Who wrote the Winnie the Pooh books?
Who is the author of winnie the pooh?
What was the name of the authur of winnie the pooh?
Who wrote the series of books for Winnie the poo?
Who wrote the children?s storybook ?Winnie the Pooh??
Who is poohs creator?
What relieves a hangover?
What is the best cure for a hangover?
The best way to recover from a hangover?
Best remedy for a hangover?
What takes away a hangover?
How do you lose a hangover?
What helps hangover symptoms?
What are social networking sites used for?
Why do people use social networking sites worldwide?
Advantages of using social network sites?
Why do people use social networks a lot?
Why do people communicate on social networking sites?
What are the pros and cons of social networking sites?
How do you say Santa Claus in Sweden?
Say santa clause in sweden?
How do you say santa clause in swedish?
How do they say santa in Sweden?
In Sweden what is santa called?
Who is sweden santa?
Table 1: Examples of paraphrase clusters from the
WikiAnswers corpus. Within each cluster, there is
a wide range of syntactic and lexical variations.
of challenges. Given that the data is community-
authored, it will inevitably be incomplete, contain
incorrectly tagged paraphrases, non-factual ques-
tions, and other sources of noise.
Our core contribution is a new learning ap-
proach that scalably sifts through this para-
phrase noise, learning to answer a broad class
of factual questions. We focus on answer-
ing open-domain questions that can be answered
with single-relation queries, e.g. all of the para-
phrases of ?Who wrote Winnie the Pooh?? and
?What cures a hangover?? in Table 1. The
algorithm answers such questions by mapping
them to executable queries over a tuple store
containing relations such as authored(milne,
winnie-the-pooh) and treat(bloody-mary,
hangover-symptoms).
1608
The approach automatically induces lexical
structures, which are combined to build queries for
unseen questions. It learns lexical equivalences for
relations (e.g., wrote, authored, and creator), en-
tities (e.g., Winnie the Pooh or Pooh Bear), and
question templates (e.g., Who r the e books? and
Who is the r of e?). Crucially, the approach
does not require any explicit labeling of the ques-
tions in our paraphrase corpus. Instead, we use
16 seed question templates and string-matching to
find high-quality queries for a small subset of the
questions. The algorithm uses learned word align-
ments to aggressively generalize the seeds, pro-
ducing a large set of possible lexical equivalences.
We then learn a linear ranking model to filter the
learned lexical equivalences, keeping only those
that are likely to answer questions well in practice.
Experimental results on 18 million paraphrase
pairs gathered from WikiAnswers1 demonstrate
the effectiveness of the overall approach. We
performed an end-to-end evaluation against a
database of 15 million facts automatically ex-
tracted from general web text (Fader et al, 2011).
On known-answerable questions, the approach
achieved 42% recall, with 77% precision, more
than quadrupling the recall over a baseline system.
In sum, we make the following contributions:
? We introduce PARALEX, an end-to-end open-
domain question answering system.
? We describe scalable learning algorithms that
induce general question templates and lexical
variants of entities and relations. These algo-
rithms require no manual annotation and can
be applied to large, noisy databases of rela-
tional triples.
? We evaluate PARALEX on the end-task of an-
swering questions from WikiAnswers using a
database of web extractions, and show that it
outperforms baseline systems.
? We release our learned lexicon and
question-paraphrase dataset to the
research community, available at
http://openie.cs.washington.edu.
2 Related Work
Our work builds upon two major threads of re-
search in natural language processing: informa-
tion extraction (IE), and natural language inter-
faces to databases (NLIDB).
1http://wiki.answers.com/
Research in IE has been moving towards the
goal of extracting facts from large text corpora,
across many domains, with minimal supervision
(Mintz et al, 2009; Hoffmann et al, 2010; Riedel
et al, 2010; Hoffmann et al, 2011; Banko et al,
2007; Yao et al, 2012). While much progress
has been made in converting text into structured
knowledge, there has been little work on an-
swering natural language questions over these
databases. There has been some work on QA over
web text (Kwok et al, 2001; Brill et al, 2002), but
these systems do not operate over extracted rela-
tional data.
The NLIDB problem has been studied for
decades (Grosz et al, 1987; Katz, 1997). More
recently, researchers have created systems that
use machine learning techniques to automatically
construct question answering systems from data
(Zelle and Mooney, 1996; Popescu et al, 2004;
Zettlemoyer and Collins, 2005; Clarke et al, 2010;
Liang et al, 2011). These systems have the abil-
ity to handle questions with complex semantics
on small domain-specific databases like GeoQuery
(Tang and Mooney, 2001) or subsets of Freebase
(Cai and Yates, 2013), but have yet to scale to the
task of general, open-domain question answering.
In contrast, our system answers questions with
more limited semantics, but does so at a very large
scale in an open-domain manner. Some work has
been made towards more general databases like
DBpedia (Yahya et al, 2012; Unger et al, 2012),
but these systems rely on hand-written templates
for question interpretation.
The learning algorithms presented in this pa-
per are similar to algorithms used for paraphrase
extraction from sentence-aligned corpora (Barzi-
lay and McKeown, 2001; Barzilay and Lee, 2003;
Quirk et al, 2004; Bannard and Callison-Burch,
2005; Callison-Burch, 2008; Marton et al, 2009).
However, we use a paraphrase corpus for extract-
ing lexical items relating natural language patterns
to database concepts, as opposed to relationships
between pairs of natural language utterances.
3 Overview of the Approach
In this section, we give a high-level overview of
the rest of the paper.
Problem Our goal is to learn a function that will
map a natural language question x to a query z
over a database D. The database D is a collection
of assertions in the form r(e1, e2) where r is a bi-
1609
nary relation from a vocabulary R, and e1 and e2
are entities from a vocabulary E. We assume that
the elements of R and E are human-interpretable
strings like population or new-york. In our
experiments, R and E contain millions of en-
tries representing ambiguous and overlapping con-
cepts. The database is equipped with a simple in-
terface that accepts queries in the form r(?, e2) or
r(e1, ?). When executed, these queries return all
entities e that satisfy the given relationship. Thus,
our task is to find the query z that best captures the
semantics of the question x.
Model The question answering model includes a
lexicon and a linear ranking function. The lexicon
L associates natural language patterns to database
concepts, thereby defining the space of queries
that can be derived from the input question (see
Table 2). Lexical entries can pair strings with
database entities (nyc and new-york), strings with
database relations (big and population), or ques-
tion patterns with templated database queries (how
r is e? and r(?,e)). We describe this model in
more detail in Section 4.
Learning The learning algorithm induces a lex-
icon L and estimates the parameters ? of the
linear ranking function. We learn L by boot-
strapping from an initial seed lexicon L0 over a
corpus of question paraphrases C = {(x, x?) :
x? is a paraphrase of x}, like the examples in Ta-
ble 1. We estimate ? by using the initial lexicon to
automatically label queries in the paraphrase cor-
pus, as described in Section 5.2. The final result
is a scalable learning algorithm that requires no
manual annotation of questions.
Evaluation In Section 8, we evaluate our system
against various baselines on the end-task of ques-
tion answering against a large database of facts
extracted from the web. We use held-out known-
answerable questions from WikiAnswers as a test
set.
4 Question Answering Model
To answer questions, we must find the best query
for a given natural language question.
4.1 Lexicon and Derivations
To define the space of possible queries, PARALEX
uses a lexicon L that encodes mappings from nat-
ural language to database concepts (entities, rela-
tions, and queries). Each entry in L is a pair (p, d)
Entry Type NL Pattern DB Concept
Entity nyc new-york
Relation big population
Question (1-Arg.) how big is e population(?, e)
Question (2-Arg.) how r is e r(?, e)
Table 2: Example lexical entries.
where p is a pattern and d is an associated database
concept. Table 2 gives examples of the entry types
in L: entity, relation, and question patterns.
Entity patterns match a contiguous string of
words and are associated with some database en-
tity e ? E.
Relation patterns match a contiguous string of
words and are associated with a relation r ? R and
an argument ordering (e.g. the string child could
be modeled as either parent-of or child-of with
opposite argument ordering).
Question patterns match an entire question
string, with gaps that recursively match an en-
tity or relation patterns. Question patterns are as-
sociated with a templated database query, where
the values of the variables are determined by the
matched entity and relation patterns. A question
pattern may be 1-Argument, with a variable for
an entity pattern, or 2-Argument, with variables
for an entity pattern and a relation pattern. A 2-
argument question pattern may also invert the ar-
gument order of the matched relation pattern, e.g.
who r e? may have the opposite argument order
of who did e r?
The lexicon is used to generate a derivation y
from an input question x to a database query z.
For example, the entries in Table 2 can be used
to make the following derivation from the ques-
tion How big is nyc? to the query population(?,
new-york):
This derivation proceeds in two steps: first match-
ing a question form like How r is e? and then
mapping big to population and nyc to new-york.
Factoring the derivation this way allows the lexi-
cal entries for big and nyc to be reused in semanti-
1610
cally equivalent variants like nyc how big is it? or
approximately how big is nyc? This factorization
helps the system generalize to novel questions that
do not appear in the training set.
We model a derivation as a set of (pi, di) pairs,
where each pi matches a substring of x, the sub-
strings cover all words in x, and the database con-
cepts di compose to form z. Derivations are rooted
at either a 1-argument or 2-argument question en-
try and have entity or relation entries as leaves.
4.2 Linear Ranking Function
In general, multiple queries may be derived from a
single input question x using a lexicon L. Many of
these derivations may be incorrect due to noise in
L. Given a question x, we consider all derivations
y and score them with ? ??(x, y), where ?(x, y) is
a n-dimensional feature representation and ? is a
n-dimensional parameter vector. Let GEN(x;L)
be the set of all derivations y that can be generated
from x using L. The best derivation y?(x) accord-
ing to the model (?, L) is given by:
y?(x) = argmax
y?GEN(x;L)
? ? ?(x, y)
The best query z?(x) can be computed directly
from the derivation y?(x).
Computing the set GEN(x;L) involves finding
all 1-Argument and 2-Argument question patterns
that match x, and then enumerating all possible
database concepts that match entity and relation
strings. When the database and lexicon are large,
this becomes intractable. We prune GEN(x;L)
using the model parameters ? by only considering
the N -best question patterns that match x, before
additionally enumerating any relations or entities.
For the end-to-end QA task, we return a ranked
list of answers from the k highest scoring queries.
We score an answer a with the highest score of all
derivations that generate a query with answer a.
5 Learning
PARALEX uses a two-part learning algorithm; it
first induces an overly general lexicon (Section
5.1) and then learns to score derivations to increase
accuracy (Section 5.2). Both algorithms rely on an
initial seed lexicon, which we describe in Section
7.4.
5.1 Lexical Learning
The lexical learning algorithm constructs a lexi-
con L from a corpus of question paraphrases C =
{(x, x?) : x? is a paraphrase of x}, where we as-
sume that all paraphrased questions (x, x?) can be
answered with a single, initially unknown, query
(Table 1 shows example paraphrases). This as-
sumption allows the algorithm to generalize from
the initial seed lexicon L0, greatly increasing the
lexical coverage.
As an example, consider the paraphrase pair x
= What is the population of New York? and x? =
How big is NYC? Suppose x can be mapped to a
query under L0 using the following derivation y:
what is the r of e = r(?, e)
population = population
new york = new-york
We can induce new lexical items by aligning the
patterns used in y to substrings in x?. For example,
suppose we know that the words in (x, x?) align in
the following way:
Using this information, we can hypothesize that
how r is e, big, and nyc should have the same in-
terpretations as what is the r of e, population, and
new york, respectively, and create the new entries:
how r is e = r(?, e)
big = population
nyc = new-york
We call this procedure InduceLex(x, x?, y, A),
which takes a paraphrase pair (x, x?), a derivation
y of x, and a word alignment A, and returns a new
set of lexical entries. Before formally describing
InduceLex we need to introduce some definitions.
Let n and n? be the number of words in x and
x?. Let [k] denote the set of integers {1, . . . , k}.
A word alignment A between x and x? is a subset
of [n] ? [n?]. A phrase alignment is a pair of in-
dex sets (I, I ?) where I ? [n] and I ? ? [n?]. A
phrase alignment (I, I ?) is consistent with a word
alignment A if for all (i, i?) ? A, i ? I if and only
if i? ? I ?. In other words, a phrase alignment is
consistent with a word alignment if the words in
the phrases are aligned only with each other, and
not with any outside words.
We will now define InduceLex(x, x?, y, A) for
the case where the derivation y consists of a 2-
argument question entry (pq, dq), a relation entry
1611
function LEARNLEXICON
Inputs:
- A corpus C of paraphrases (x, x?). (Table 1)
- An initial lexicon L0 of (pattern, concept) pairs.
- A word alignment function WordAlign(x, x?).
(Section 6)
- Initial parameters ?0.
- A function GEN(x;L) that derives queries from
a question x using lexicon L. (Section 4)
- A function InduceLex(x, x?, y, A) that induces
new lexical items from the paraphrases (x, x?) us-
ing their word alignment A and a derivation y of
x. (Section 5.1)
Output: A learned lexicon L.
L = {}
for all x, x? ? C do
if GEN(x;L0) is not empty then
A?WordAlign(x, x?)
y? ? argmaxy?GEN(x;L0) ?0 ? ?(x, y)
L? L ? InduceLex(x, x?, y?, A)
return L
Figure 1: Our lexicon learning algorithm.
(pr, dr), and an entity entry (pe, de), as shown in
the example above.2 InduceLex returns the set of
all triples (p?q, dq), (p?r, dr), (p?e, de) such that for
all p?q, p?r, p?e such that
1. p?q, p?r, p?e are a partition of the words in x?.
2. The phrase pairs (pq, p?q), (pr, p?r), (pe, p?e)
are consistent with the word alignment A.
3. The p?r and p?e are contiguous spans of words
in x?.
Figure 1 shows the complete lexical learning al-
gorithm. In practice, for a given paraphrase pair
(x, x?) and alignment A, InduceLex will gener-
ate multiple sets of new lexical entries, resulting
in a lexicon with millions of entries. We use an
existing statistical word alignment algorithm for
WordAlign (see Section 6). In the next section,
we will introduce a scalable approach for learning
to score derivations to filter out lexical items that
generalize poorly.
5.2 Parameter Learning
Parameter learning is necessary for filtering out
derivations that use incorrect lexical entries like
new mexico = mexico, which arise from noise in
the paraphrases and noise in the word alignment.
2InduceLex has similar behavior for the other type of
derivation, which consists of a 1-argument question entry
(pq, dq) and an entity (pe, de).
We use the hidden variable structured perceptron
algorithm to learn ? from a list of (question x,
query z) training examples. We adopt the itera-
tive parameter mixing variation of the perceptron
(McDonald et al, 2010) to scale to a large number
of training examples.
Figure 2 shows the parameter learning algo-
rithm. The parameter learning algorithm operates
in two stages. First, we use the initial lexicon
L0 to automatically generate (question x, query z)
training examples from the paraphrase corpus C.
Then we feed the training examples into the learn-
ing algorithm, which estimates parameters for the
learned lexicon L.
Because the number of training examples is
large, we adopt a parallel perceptron approach.
We first randomly partition the training data T
into K equally-sized subsets T1, . . . , TK . We then
perform perceptron learning on each partition in
parallel. Finally, the learned weights from each
parallel run are aggregated by taking a uniformly
weighted average of each partition?s parameter
vector. This procedure is repeated for T iterations.
The training data consists of (question x, query
z) pairs, but our scoring model is over (question
x, derivation y) pairs, which are unobserved in
the training data. We use a hidden variable ver-
sion of the perceptron algorithm (Collins, 2002),
where the model parameters are updated using the
highest scoring derivation y? that will generate the
correct query z using the learned lexicon L.
6 Data
For our database D, we use the publicly avail-
able set of 15 million REVERB extractions (Fader
et al, 2011).3 The database consists of a set
of triples r(e1, e2) over a vocabulary of ap-
proximately 600K relations and 2M entities, ex-
tracted from the ClueWeb09 corpus.4 The RE-
VERB database contains a large cross-section of
general world-knowledge, and thus is a good
testbed for developing an open-domain QA sys-
tem. However, the extractions are noisy, unnor-
malized (e.g., the strings obama, barack-obama,
and president-obama all appear as distinct en-
tities), and ambiguous (e.g., the relation born-in
contains facts about both dates and locations).
3We used version 1.1, downloaded from http://
reverb.cs.washington.edu/.
4The full set of REVERB extractions from ClueWeb09
contains over six billion triples. We used the smaller subset
of triples to simplify our experiments.
1612
function LEARNPARAMETERS
Inputs:
- A corpus C of paraphrases (x, x?). (Table 1)
- An initial lexicon L0 of (pattern, db concept)
pairs.
- A learned lexiconL of (pattern, db concept) pairs.
- Initial parameters ?0.
- Number of perceptron epochs T .
- Number of training-data shards K.
- A function GEN(x;L) that derives queries from
a question x using lexicon L. (Section 4)
- A function PerceptronEpoch(T , ?, L) that runs
a single epoch of the hidden-variable structured
perceptron algorithm on training set T with initial
parameters ?, returning a new parameter vector
??. (Section 5.2)
Output: A learned parameter vector ?.
// Step 1: Generate Training Examples T
T = {}
for all x, x? ? C do
if GEN(x;L0) is not empty then
y? ? argmaxy?GEN(x;L0) ?0 ? ?(x, y)
z? ? query of y?
Add (x?, z?) to T
// Step 2: Learn Parameters from T
Randomly partition T into shards T1, . . . , TK
for t = 1 . . . T do
// Executed on k processors
?k,t = PerceptronEpoch(Tk, ?t?1, L)
// Average the weights
?t = 1K
?
k ?k,t
return ?T
Figure 2: Our parameter learning algorithm.
Our paraphrase corpus C was constructed from
the collaboratively edited QA site WikiAnswers.
WikiAnswers users can tag pairs of questions as
alternate wordings of each other. We harvested
a set of 18M of these question-paraphrase pairs,
with 2.4M distinct questions in the corpus.
To estimate the precision of the paraphrase cor-
pus, we randomly sampled a set of 100 pairs and
manually tagged them as ?paraphrase? or ?not-
paraphrase.? We found that 55% of the sampled
pairs are valid paraphrased. Most of the incorrect
paraphrases were questions that were related, but
not paraphrased e.g. How big is the biggest mall?
and Most expensive mall in the world?
We word-aligned each paraphrase pair using
the MGIZA++ implementation of IBM Model 4
(Och and Ney, 2000; Gao and Vogel, 2008). The
word-alignment algorithm was run in each direc-
tion (x, x?) and (x?, x) and then combined using
the grow-diag-final-and heuristic (Koehn et al,
2003).
7 Experimental Setup
We compare the following systems:
? PARALEX: the full system, using the lexical
learning and parameter learning algorithms
from Section 5.
? NoParam: PARALEX without the learned
parameters.
? InitOnly: PARALEX using only the initial
seed lexicon.
We evaluate the systems? performance on the end-
task of QA on WikiAnswers questions.
7.1 Test Set
A major challenge for evaluation is that the RE-
VERB database is incomplete. A system may cor-
rectly map a test question to a valid query, only
to return 0 results when executed against the in-
complete database. We factor out this source of
error by semi-automatically constructing a sample
of questions that are known to be answerable us-
ing the REVERB database, and thus allows for a
meaningful comparison on the task of question un-
derstanding.
To create the evaluation set, we identified ques-
tions x in a held out portion of the WikiAnswers
corpus such that (1) x can be mapped to some
query z using an initial lexicon (described in Sec-
tion 7.4), and (2) when z is executed against the
database, it returns at least one answer. We then
add x and all of its paraphrases as our evaluation
set. For example, the question What is the lan-
guage of Hong-Kong satisfies these requirements,
so we added these questions to the evaluation set:
What is the language of Hong-Kong?
What language do people in hong kong use?
How many languages are spoken in hong kong?
How many languages hong kong people use?
In Hong Kong what language is spoken?
Language of Hong-kong?
This methodology allows us to evaluate the sys-
tems? ability to handle syntactic and lexical varia-
tions of questions that should have the same an-
swers. We created 37 question clusters, result-
ing in a total of 698 questions. We removed all
of these questions and their paraphrases from the
training set. We also manually filtered out any in-
correct paraphrases that appeared in the test clus-
ters.
We then created a gold-standard set of (x, a, l)
triples, where x is a question, a is an answer, and l
1613
Question Pattern Database Query
who r e r(?, e)
what r e r(?, e)
who does e r r(e, ?)
what does e r r(e, ?)
what is the r of e r(?, e)
who is the r of e r(?, e)
what is r by e r(e, ?)
who is e?s r r(?, e)
what is e?s r r(?, e)
who is r by e r(e, ?)
when did e r r-in(e, ?)
when did e r r-on(e, ?)
when was e r r-in(e, ?)
when was e r r-on(e, ?)
where was e r r-in(e, ?)
where did e r r-in(e, ?)
Table 3: The question patterns used in the initial
lexicon L0.
is a label (correct or incorrect). To create the gold-
standard, we first ran each system on the evalua-
tion questions to generate (x, a) pairs. Then we
manually tagged each pair with a label l. This
resulted in a set of approximately 2, 000 human
judgments. If (x, a) was tagged with label l and x?
is a paraphrase of x, we automatically added the
labeling (x?, a, l), since questions in the same clus-
ter should have the same answer sets. This process
resulted in a gold standard set of approximately
48, 000 (x, a, l) triples.
7.2 Metrics
We use two types of metrics to score the systems.
The first metric measures the precision and recall
of each system?s highest ranked answer. Precision
is the fraction of predicted answers that are cor-
rect and recall is the fraction of questions where a
correct answer was predicted. The second metric
measures the accuracy of the entire ranked answer
set returned for a question. We compute the mean
average precision (MAP) of each systems? output,
which measures the average precision over all lev-
els of recall.
7.3 Features and Settings
The feature representation ?(x, y) consists of in-
dicator functions for each lexical entry (p, d) ? L
used in the derivation y. For parameter learning,
we use an initial weight vector ?0 = 0, use T = 20
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
NoParam 0.30 0.53 0.20 0.08
InitOnly 0.18 0.84 0.10 0.04
Table 4: Performance on WikiAnswers questions
known to be answerable using REVERB.
F1 Precision Recall MAP
PARALEX 0.54 0.77 0.42 0.22
No 2-Arg. 0.40 0.86 0.26 0.12
No 1-Arg 0.35 0.81 0.22 0.11
No Relations 0.18 0.84 0.10 0.03
No Entity 0.36 0.55 0.27 0.15
Table 5: Ablation of the learned lexical items.
0.0 0.1 0.2 0.3 0.4 0.5
Recall
0.5
0.6
0.7
0.8
0.9
1.0
Pr
eci
sio
n
PARALEX
No 2-Arg.
Initial Lexicon
Figure 3: Precision-recall curves for PARALEX
with and without 2-argument question patterns.
iterations and shard the training data into K = 10
pieces. We limit each system to return the top 100
database queries for each test sentence. All input
words are lowercased and lemmatized.
7.4 Initial Lexicon
Both the lexical learning and parameter learning
algorithms rely on an initial seed lexicon L0. The
initial lexicon allows the learning algorithms to
bootstrap from the paraphrase corpus.
We construct L0 from a set of 16 hand-written
2-argument question patterns and the output of the
identity transformation on the entity and relation
strings in the database. Table 3 shows the question
patterns that were used in L0.
8 Results
Table 4 shows the performance of PARALEX on
the test questions. PARALEX outperforms the
baseline systems in terms of both F1 and MAP.
The lexicon-learning algorithm boosts the recall
by a factor of 4 over the initial lexicon, show-
ing the utility of the InduceLex algorithm. The
1614
String Learned Database Relations for String
get rid of treatment-for, cause, get-rid-of, cure-for, easiest-way-to-get-rid-of
word word-for, slang-term-for, definition-of, meaning-of, synonym-of
speak speak-language-in, language-speak-in, principal-language-of, dialect-of
useful main-use-of, purpose-of, importance-of, property-of, usefulness-of
String Learned Database Entities for String
smoking smoking, tobacco-smoking, cigarette, smoking-cigar, smoke, quit-smoking
radiation radiation, electromagnetic-radiation, nuclear-radiation
vancouver vancouver, vancouver-city, vancouver-island, vancouver-british-columbia
protein protein, protein-synthesis, plasma-protein, monomer, dna
Table 6: Examples of relation and entity synonyms learned from the WikiAnswers paraphrase corpus.
parameter-learning algorithm also results in a
large gain in both precision and recall: InduceLex
generates a noisy set of patterns, so selecting the
best query for a question is more challenging.
Table 5 shows an ablation of the different types
of lexical items learned by PARALEX. For each
row, we removed the learned lexical items from
each of the types described in Section 4, keeping
only the initial seed lexical items. The learned 2-
argument question templates significantly increase
the recall of the system. This increased recall
came at a cost, lowering precision from 0.86 to
0.77. Thresholding the query score allows us to
trade precision for recall, as shown in Figure 3.
Table 6 shows some examples of the learned en-
tity and relation synonyms.
The 2-argument question templates help PAR-
ALEX generalize over different variations of the
same question, like the test questions shown in
Table 7. For each question, PARALEX combines
a 2-argument question template (shown below the
questions) with the rules celebrate = holiday-of
and christians = christians to derive a full
query. Factoring the problem this way allows
PARALEX to reuse the same rules in different
syntactic configurations. Note that the imperfect
training data can lead to overly-specific templates
like what are the religious r of e, which can lower
accuracy.
9 Error Analysis
To understand how close we are to the goal of
open-domain QA, we ran PARALEX on an unre-
stricted sample of questions from WikiAnswers.
We used the same methodology as described in the
previous section, where PARALEX returns the top
answer for each question using REVERB.
We found that PARALEX performs significantly
worse on this dataset, with recall maxing out at ap-
Celebrations for Christians?
r for e?
Celebrations of Christians?
r of e?
What are some celebrations for Christians?
what are some r for e?
What are some celebrations of the Christians?
what are some r of e?
What are some of Christians celebrations?
what are some of e r?
What celebrations do Christians do?
what r do e do?
What did Christians celebrate?
what did e r?
What are the religious celebrations of Christians?
what are the religious r of e?
What celebration do Christians celebrate?
what r do e celebrate?
Table 7: Questions from the test set with 2-
argument question patterns that PARALEX used to
derive a correct query.
proximately 6% of the questions answered at pre-
cision 0.4. This is not surprising, since the test
questions are not restricted to topics covered by
the REVERB database, and may be too complex to
be answered by any database of relational triples.
We performed an error analysis on a sample
of 100 questions that were either incorrectly an-
swered or unanswered. We examined the can-
didate queries that PARALEX generated for each
question and tagged each query as correct (would
return a valid answer given a correct and com-
plete database) or incorrect. Because the input
questions are unrestricted, we also judged whether
the questions could be faithfully represented as a
r(?, e) or r(e, ?) query over the database vocabu-
lary. Table 8 shows the distribution of errors.
The largest source of error (36%) were on com-
1615
plex questions that could not be represented as a
query for various reasons. We categorized these
questions into groups. The largest group (14%)
were questions that need n-ary or higher-order
database relations, for example How long does
it take to drive from Sacramento to Cancun? or
What do cats and dogs have in common? Approx-
imately 13% of the questions were how-to ques-
tions like How do you make axes in minecraft?
whose answers are a sequence of steps, instead
of a database entity. Lastly, 9% of the questions
require database operators like joins, for example
When were Bobby Orr?s children born?
The second largest source of error (32%) were
questions that could be represented as a query, but
where PARALEX was unable to derive any cor-
rect queries. For example, the question Things
grown on Nigerian farms? was not mapped to
any queries, even though the REVERB database
contains the relation grown-in and the entity
nigeria. We found that 13% of the incorrect
questions were cases where the entity was not rec-
ognized, 12% were cases where the relation was
not recognized, and 6% were cases where both the
entity and relation were not recognized.
We found that 28% of the errors were cases
where PARALEX derived a query that we judged to
be correct, but returned no answers when executed
against the database. For example, given the ques-
tion How much can a dietician earn? PARALEX
derived the query salary-of(?, dietician) but
this returned no answers in the REVERB database.
Finally, approximately 4% of the questions in-
cluded typos or were judged to be inscrutable, for
example Barovier hiriacy of evidence based for
pressure sore?
Discussion Our experiments show that the learn-
ing algorithms described in Section 5 allow PAR-
ALEX to generalize beyond an initial lexicon and
answer questions with significantly higher accu-
racy. Our error analysis on an unrestricted set of
WikiAnswers questions shows that PARALEX is
still far from the goal of truly high-recall, open-
domain QA. We found that many questions asked
on WikiAnswers are either too complex to be
mapped to a simple relational query, or are not
covered by the REVERB database. Further, ap-
proximately one third of the missing recall is due
to entity and relation recognition errors.
Incorrectly Answered/Unanswered Questions
36% Complex Questions
Need n-ary or higher-order relations (14%)
Answer is a set of instructions (13%)
Need database operators e.g. joins (9%)
32% Entity or Relation Recognition Errors
Entity recognition errors (13%)
Relation recognition errors (12%)
Entity & relation recognition errors (7%)
28% Incomplete Database
Derived a correct query, but no answers
4% Typos/Inscrutable Questions
Table 8: Error distribution of PARALEX on an un-
restricted sample of questions from the WikiAn-
swers dataset.
10 Conclusion
We introduced a new learning approach that in-
duces a complete question-answering system from
a large corpus of noisy question-paraphrases. Us-
ing only a seed lexicon, the approach automat-
ically learns a lexicon and linear ranking func-
tion that demonstrated high accuracy on a held-out
evaluation set.
A number of open challenges remain. First,
precision could likely be improved by adding
new features to the ranking function. Second,
we would like to generalize the question under-
standing framework to produce more complex
queries, constructed within a compositional se-
mantic framework, but without sacrificing scala-
bility. Third, we would also like to extend the
system with other large databases like Freebase or
DBpedia. Lastly, we believe that it would be pos-
sible to leverage the user-provided answers from
WikiAnswers as a source of supervision.
Acknowledgments
This research was supported in part by ONR grant
N00014-11-1-0294, DARPA contract FA8750-09-
C-0179, a gift from Google, a gift from Vulcan
Inc., and carried out at the University of Washing-
ton?s Turing Center. We would like to thank Yoav
Artzi, Tom Kwiatkowski, Yuval Marton, Mausam,
Dan Weld, and the anonymous reviewers for their
helpful comments.
1616
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of the 39th Annual Meeting on Associ-
ation for Computational Linguistics.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An Analysis of the AskMSR Question-Answering
System. In Proceedings of Empirical Methods in
Natural Language Processing.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexicon
Extension. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving Semantic Parsing from
the World?s Response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. In Proc. of the
ACL 2008 Software Engineering, Testing, and Qual-
ity Assurance Workshop.
Barbara J. Grosz, Douglas E. Appelt, Paul A. Mar-
tin, and Fernando C. N. Pereira. 1987. TEAM:
An Experiment in the Design of Transportable
Natural-Language Interfaces. Artificial Intelligence,
32(2):173?243.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-Based Weak Supervision for Informa-
tion Extraction of Overlapping Relations. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics.
Boris Katz. 1997. Annotating the World Wide Web
using Natural Language. In RIAO, pages 136?159.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling Question Answering to the Web. ACM
Trans. Inf. Syst., 19(3):242?262.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning Dependency-Based Compositional Se-
mantics. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved Statistical Machine Trans-
lation Using Monolingually-Derived Paraphrases.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant Supervision for Relation Ex-
traction Without Labeled Data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
Natural Language Interfaces to Databases: Compos-
ing Statistical Parsing with Semantic Tractability. In
Proceedings of the Twentieth International Confer-
ence on Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Pro-
cessing.
1617
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling Relations and Their Mentions with-
out Labeled Text. In Proceedings of the 2010 Euro-
pean conference on Machine learning and Knowl-
edge Discovery in Databases.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing Multiple Clause Constructors in Inductive Logic
Programming for Semantic Parsing.
Christina Unger, Lorenz Bu?hmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-Based Question
Answering over RDF Data. In Proceedings of the
21st World Wide Web Conference 2012.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for
the Web of Data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised Relation Discovery with Sense
Disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries Using Inductive Logic
Programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In Proceedings of the 21st Conference
in Uncertainty in Artificial Intelligence.
1618
