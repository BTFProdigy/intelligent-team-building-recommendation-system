Bootstrapping Parallel Treebanks
Martin VOLK and Yvonne SAMUELSSON
Stockholm University
Department of Linguistics
10691 Stockholm
Sweden
volk@ling.su.se
Abstract
This paper argues for the development of par-
allel treebanks. It summarizes the work done
in this area and reports on experiments for
building a Swedish-German treebank. And it
describes our approach for reusing resources
from one language while annotating another
language.
1 Introduction
Treebanks have become valuable resources in
natural language processing (NLP) in recent
years (Abeille?, 2003). A treebank is a collection
of syntactically annotated sentences in which
the annotation has been manually checked so
that the treebank can serve as training cor-
pus for natural language parsers, as repository
for linguistic research, or as evaluation corpus
for NLP systems. The current interest in tree-
banks is documented in international workshop
series like ?Linguistically Interpreted Corpora
(LINC)? or ?Treebanks and Linguistic Theo-
ries? (TLT). But also the recent international
CL conferences have seen a wide variety of pa-
pers that involved treebanks. Treebanks have
become a necessary resource for many research
activities in NLP.
On the other hand recent years have seen
an increasing interest in parallel corpora (of-
ten called bitexts). See for example (Melamed,
2001) or (Borin, 2002) for a broad picture of
this area.
But surprisingly little work has been reported
on combining these two areas: parallel tree-
banks. We define a parallel treebank as a bi-
text where the sentences of each language are
annotated with a syntactic tree, and the sen-
tences are aligned below the clause level. This
leaves room for various kinds of tree structure
(e.g. dependency structure trees or constituent
structure trees) and does not specify a precise
requirement for tree alignments but rather for
some sort of sub-clausal alignment (e.g. word
alignment or phrase alignment).
But why has there been so little work done
on parallel treebanks? The benefits of hav-
ing such a treebank for training statistical ma-
chine translation systems, experimenting with
example-based translation systems, or evalu-
ating word alignment programs seem so over-
whelming. We speculate that this scarcity is
mainly due to the expenses necessary for build-
ing a parallel treebank (in terms of time and
human resources). It is well known that the
manual labor involved in building a monolin-
gual treebank is high (For the Penn Treebank
(Taylor et al, 2003) report on 750 - 1000
words per hour for an experienced annotator,
which translates to 35 - 50 sentences per hour).
And the cross-language alignment requires ad-
ditional work. Therefore every approach to fa-
cilitate and speed up this process will be highly
welcome.
The goal of this paper is to summarize the
(little) work that has been done on parallel
treebanks and related areas such as annota-
tion projection. In particular we will report on
our experiments for building a Swedish-German
parallel treebank. As a side issue we investi-
gated whether the German treebank annota-
tion guidelines (from the NEGRA / TIGER
projects) can be applied to Swedish. We have
chosen Swedish and German because they are
our mother tongues, but also because they are
similar and still interestingly different.
2 Previous Work on Parallel
Treebanks
The field of parallel treebanks is only now evolv-
ing into a research field. (Cmejrek et al, 2003)
at the Charles University in Prague have built
a treebank for the specific purpose of machine
translation, the Czech-English Penn Treebank
with tectogrammatical dependency trees. They
have asked translators to translate part of the
Penn Treebank into Czech with the clear direc-
tive to translate every English sentence with one
in Czech and to stay as close as possible to the
original.
This directive seems strange at first sight but
it makes sense with regard to their objective.
Since they specifically construct the treebank
for training and evaluating machine translation
systems, a close human translation is a valid
starting point to get good automatic transla-
tions.
At the University of Mu?nster (Germany)
(Cyrus et al, 2003) have started working on
FuSe, a syntactically analyzed parallel corpus.
The goal is a treebank with English and German
texts (currently with examples from the Eu-
roparl corpus). The annotation is multi-layered
in that they use PoS-tags, constituent structure,
functional relations, predicate-argument struc-
ture and alignment information. However their
focus is on the predicate-argument structure.
The Nordic Treebank Network1 has started
an initiative to syntactically annotate the first
chapter of ?Sophie?s World?2 in the nordic lan-
guages. This text was chosen since it has been
translated into a vast number of languages and
since it includes interesting linguistic properties
such as direct speech. Currently a prototype
of this parallel treebank with the first 50 sen-
tences in Swedish, Norwegian, Danish, Estonian
and German has been finished. The challenge
in this project is that all involved researchers
annotate the Sophie sentences of their language
in their format of choice (ranging from depen-
dency structures for Danish and Swedish to con-
stituency structures for Estonian and German).
In order to make the results exchangeable and
comparable all results have been converted into
TIGER-XML so that TIGERSearch3 can be
used to display and search the annotated sen-
tences monolingually. The alignment across lan-
guages is still open.
3 Bootstrapping a German-Swedish
parallel treebank
We have built a small German-Swedish parallel
treebank with 25 sentence pairs taken from the
Europarl corpus. First, the German sentences
1The Nordic Treebank Network is headed by
Joakim Nivre. See www.masda.vxu.se/?nivre/research/
nt.html
2The Norwegian original is: Jostein Gaarder (1991):
Sofies verden: roman om filosofiens historie. Aschehoug.
3TIGERSearch is a treebank query tool developed at
the University of Stuttgart. See also section 5.2.
were tokenized and loaded into the Annotate
treebank editor4. Annotate includes Thorsten
Brants? Part-of-Speech Tagger and Chunker for
German. The PoS tagger employs the STTS, a
set of around 50 PoS-tags for German. The set
is so large because it incorporates some morpho-
syntactic features (e.g. it distinguishes between
finite and non-finite verb forms). The chun-
ker assigns a flat constituent structure with the
usual node labels (e.g. AP, NP, PP, S, VP), but
also special labels for coordinated phrases (e.g.
CAP, CNP, CPP, CS, CVP). In addition the
chunker suggests syntactic functions (like sub-
ject, object, head or modifier) as edge labels.
The human treebank annotator controls the
suggestions made by the tagger and the chun-
ker and modifies them where necessary. Tagger
and chunker help to speed up the annotation
process for German sentences enormously. The
upper tree in figure 1 shows the structure for
the following sentence (taken from Europarl):
(1) Doch sind Bu?rger einiger unserer
Mitgliedstaaten Opfer von schrecklichen
Naturkatastrophen geworden.
(EN: But citizens of some of our
member states have become victims of
terrible natural disasters.)
Now let us look at the resources available for
Swedish. First there is SUC (the Stockholm-
Ume?a-Corpus), a 1 million word corpus of writ-
ten Swedish designed as a representative corpus
along the lines of the Brown corpus. SUC con-
tains PoS-tags, morphological tags and lemmas
for all tokens as well as proper name classes.
All the information is hand-checked. So this
is proper training material for a PoS tagger.
Compared to the 50 tags of the STTS, the 22
SUC PoS-tags (e.g. only one verb tag) are rather
coarse-grained, but of course we can use the
combination of PoS-tags and morphological in-
formation to automatically derive a richer tag
set.
Training material for a Swedish chunker is
harder to come by. There are two early
Swedish treebanks, Mamba and SynTag (dating
back to the 1970s (!) and 1980s respectively),
but they are rather small (about 5000 sen-
tences each), very heterogeneously annotated
and somewhat faulty (cf. (Nivre, 2002)). There-
fore, the most serious attempt at training a
4Annotate is a treebank editor developed at the Uni-
versity of Saarbru?cken. See www.coli.uni-sb.de/sfb378/
negra-corpus/annotate.html
Figure 1: Parallel trees with lines showing the alignment.
chunker for Swedish was based on an automat-
ically created ?treebank? which of course con-
tained a certain error rate (Megyesi, 2002). Es-
sentially there exists no constituent structure
treebank for Swedish that could be used for
training a chunker with resulting structures cor-
responding to the German sentences.
Therefore we have worked with a different
approach (described in detail in (Samuelsson,
2004)). We first trained a PoS tagger on SUC
and used it to assign PoS-tags to our Swedish
sentences. We then converted the Swedish PoS-
tags in these sentences into the corresponding
German STTS tags.5 We loaded the Swedish
sentences into Annotate (now with STTS tags),
and we were then able to reuse the German
chunker to make structural decisions over the
Swedish sentences. This worked surprisingly
5An alternative approach could have been to map all
tags in the SUC to STTS and then train a Swedish tagger
on this converted material.
well due to the structural similarities of Swedish
and German. After the semi-automatic an-
notation of the syntactic structure, the PoS-
tags were converted back to the usual Swedish
tag set. This is a straight-forward example of
how resources for one language (in this case
German) can be reused to bootstrap linguis-
tic structure in another albeit related language
(here Swedish).
The lower tree in figure 1 shows the structure
for the Swedish sentence which corresponds to
the German sentence in example 1.
(2) Da?remot har inv?anarna i ett antal av
v?ara medlemsla?nder drabbats av
naturkatastrofer som verkligen varit
fo?rskra?ckliga.
(EN: However inhabitants of a number
of our member states were affected by
natural disasters which indeed were
terrible.)
Since the German STTS is more fine-grained
than the SUC tag set, the mapping from the
SUC tag set to STTS does not entail loosing
any information. When converting in this di-
rection the problem is rather which option to
choose. For example, the SUC tag set has one
tag for adjectives, but the STTS distinguishes
between attributive adjectives (ADJA) and ad-
verbial or predicative adjectives (ADJD). We
decided to map all Swedish adjectives to ADJA
since the information in SUC does not give us
any clue about the usage difference. The human
annotator then needs to correct the ADJA tag
to ADJD if appropriate, in order to enable the
chunker to work as intended.
Other tag mapping problems come with the
SUC tags for adverb, determiner, pronoun and
possessive all of which are marked as ?interrog-
ative or relative? in the guidelines. There is no
clear mapping of these tags to STTS. We de-
cided to use the mapping in table 1.
The benefit of using the German chunker for
annotating the Swedish sentences is hard to
quantify. A precise experiment would require
one group of annotators to work with this chun-
ker and another to work without it on the same
sentences for a comparison of the time needed.
We performed a small experiment to see how
often the German chunker suggests the correct
node labels and edge labels for the Swedish sen-
tences (when the children tags/nodes were man-
ually selected). In 100 trials we observed 89 cor-
rect node labels and 93% correct edge labels (for
305 edges). If we assume that manual inspec-
tion of correct suggestions takes about a third
of the time of manual annotation, and if we also
assume that the correction of erroneous sugges-
tions takes the same amount of time as manual
annotation, then the employment of the Ger-
man chunker for Swedish saves about 60% of
the annotation time.
Reusing a chunker for bootstrapping a paral-
lel treebank between closely related languages
like German and Swedish is only a first step to-
wards reusing annotation (be it automatic or
manual) in one language for another language.
But it points to a promising research direc-
tion. (Yarowsky et al, 2001) have reported
interesting results of an annotation-projection
technique for PoS tagging, named entities and
morphology. And (Cabezas et al, 2001) have
explored projecting syntactic dependency rela-
tions from English to Basque. This idea was
followed by (Hwa et al, 2002) who investi-
gated English to Chinese projections based on
the direct correspondence assumption. They
conclude that annotation projections are nearly
70% accurate (in terms of unlabelled dependen-
cies) when some linguistic knowledge is used.
We believe that annotation projection is a diffi-
cult field but even if we only succeed in a limited
number of cases, it will be valuable for increased
speed in the development of parallel treebanks.
3.1 Alignment
The alignment in our experimental treebank is
based on the nodes, not the edge labels. Fig-
ure 1 shows the phrase alignment as thick lines
across the trees. All of the alignment mapping
was done by hand.
We decided to make the alignment determin-
istic, i.e. a node in one language can only be
aligned with one node in the other language.
There are, of course, a lot of problems with
the alignment. We have looked at the meaning,
rather than the exact wording. Sometimes dif-
ferent words are used in an S or VP, but we still
feel that the meaning is the same, and therefore
we have aligned them. We might have align-
ment on one constituent level, while there are
differences (i.e. no alignment) on lower levels of
the tree. Therefore we consider it important to
make the parse trees sufficiently deep. We need
to be able to draw the alignment on as many
levels as possible.
Another problem arises when the sentences
are constructed in different ways, due to e.g.
passivisation or topicalisation. Although Ger-
man and Swedish are structurally close, there
are some clear differences.
? German separable prefix verbs (e.g. fangen
an = begin) do not have a direct corres-
pondence in Swedish. However, Swedish
has frequent particle verbs (e.g. ta upp =
bring up). But whereas the German sep-
arated verb prefix occupies a specific posi-
tion at the end of a clause (?Rechte Satzk-
lammer?), the Swedish verb particle occurs
at the end of the verb group.
? The general word order in Swedish subordi-
nate clauses is the same as in main clauses.
Unlike in German there is no verb-final or-
der in subordinate clauses.
? German uses accusative and dative case
endings to mark direct and indirect objects.
This is reflected in the German function
labels for accusative object (OA) and for
SUC tag STTS tag
HA int. or rel. adverb PWAV adverbial interrog. or relative pronoun
HD int. or rel. determiner PWS (stand-alone) interrog. pronoun
HP int. or rel. pronoun PRELS (stand-alone) relative pronoun
HS int. or rel. possessive PPOS (stand-alone) possessive pronoun
Table 1: Mapping of SUC tags to STTS
dative object (DO). Swedish has lost these
case endings and the labels therefore need
not reflect case but rather object function.
Our overall conclusion is that applying the
German treebank annotation guidelines to
Swedish works well when the few peculiarities
of Swedish are taken care of.
4 Corpus representation
After annotating the sentences in both lan-
guages with the Annotate treebank editor, the
tree structures were exported in the NEGRA
export format from the MySQL database. The
file in NEGRA format is easily loaded into
TIGERSearch via the TIGERRegistry which
provides an import filter for this format. This
import process creates a TIGER-XML file
which contains the same information as the NE-
GRA file. The difference is that the pointers in
the NEGRA format go from the tokens to the
pre-terminal nodes (and from nodes to parent
nodes) in a bottom-up fashion, whereas in the
TIGER-XML file the nodes point to their chil-
dren by listing their id numbers (idref) and their
edge label (in a top-down perspective).
In this file the tokens of the sentence (ter-
minals) are listed beneath each other with their
corresponding PoS-tag (PPER for personal pro-
noun, VVFIN for finite verb, APPRART for
contracted preposition etc.). The nodes (non-
terminals) are listed with their name and their
outgoing edges with labels such as HD for head,
NK for noun kernel, SB for subject etc.
<s id="s1">
<graph root="522">
<terminals>
<t id="1" word="Ich" pos="PPER" />
<t id="2" word="erkla?re" pos="VVFIN"/>
<t id="3" word="die" pos="ART" />
<t id="4" word="am" pos="APPRART"/>
<t id="5" word="Freitag" pos="NN" />
[...]
</terminals>
<nonterminals>
<nt id="500" cat="NP">
<edge label="HD" idref="1" />
</nt>
[...]
<nt id="522" cat="S">
<edge label="HD" idref="2" />
<edge label="SB" idref="500" />
<edge label="MO" idref="511" />
<edge label="OA" idref="521" />
</nt>
</nonterminals>
</graph>
</s>
Since all tokens and all nodes are uniquely
numbered, these numbers can be used for the
phrase alignment. For the representation of the
alignment we adapted a DTD that was devel-
oped for the Linko?ping Word Aligner (Ahren-
berg et al, 2002). The XML-file with the align-
ment information then looks like this. The
sentLink-tags each contain one sentence pair,
while each phraseLink represents one aligned
node pair.
<!DOCTYPE DeSv SYSTEM "align.dtd">
<DeSv fromDoc="De.xml" toDoc="Sv.xml">
<linkList>
<sentLink xtargets="1 ; 1">
<phraseLink xtargets="500; 500"/>
<phraseLink xtargets="501; 503"/>
[...]
</sentLink>
</linkList>
</DeSv>
This fragment first specifies the two involved
XML files for German (De.xml) and Swedish
(Sv.xml). It then states the phrase pairs for
the sentence pair 1 - 1 from these files. For
example, phrase number 501 from the German
sentence 1 is aligned with phrase number 503 of
the Swedish sentence.
5 Tools for Parallel Treebanks
Treebank tools are usually of two types. First
there are tools for producing the treebank, i.e.
for automatically adding information (taggers,
chunkers, parsers) and for manual inspection
and correction (treebank editors). On the other
hand we need tools for viewing and searching a
treebank.
5.1 Treebank Editors
Of course the tools for monolingual treebank
production can also be used for building the
language-specific parts of a parallel treebank.
Thus a treebank editor such as Annotate with
built-in PoS tagger and chunker is an invaluable
resource. But such a tool should include or be
complemented with a completeness and consis-
tency checker.
In addition the parallel treebank needs to be
aligned on the sub-sentence level. Automatic
word alignment systems will help ((Tiedemann,
2003) discusses some interesting approaches).
But tools for checking and correcting this align-
ment will be needed. For example the I*Link
system (Ahrenberg et al, 2002) could be used
for this task. I*Link comes with a graphical
user interface for creating and storing associa-
tions between segments in a bitext. I*Link is
aimed at word and phrase associations and re-
quires bitexts that are pre-aligned at the sen-
tence level.
5.2 Treebank Search Tools
With the announcement of the Penn Treebank,
some 10 years ago, came a search tool called
tgrep. It is a UNIX-based program that allows
querying a treebank specifying dominance and
precedence relations over trees (plus regular ex-
pressions and boolean operators). The search
results are bracketed trees in line-based or in-
dented format catering for the needs of different
users. For example, the following tgrep query
searches for a VP that dominates (not necessar-
ily directly) an NP which immediately precedes
a PP.
VP << (NP . PP)
More recently TIGERSearch was launched.
It is a Java-based program that comes with a
graphical user interface and a powerful feature-
value-oriented query language. The output
are graphical tree representations in which the
matched part of the tree is highlighted and fo-
cused. TIGERSearch?s ease of installation and
friendly user interface have made it the tool of
choice for many treebank researchers.
According to our knowledge no specific search
tools for parallel treebanks exist. In addition to
the above sketched search options of tgrep and
TIGERSearch a search tool for parallel tree-
banks will have to allow queries that combine
constraints over two trees. For example one
wants to issue queries such as ?Find a tree
in language 1 with a relative clause where the
parallel tree in language 2 uses a prepositional
phrase for the same content.?
5.3 Displaying Parallel Trees
There is currently no off-the-shelf tool that can
display parallel trees so that one could view two
phrase structure trees at the same time with
their alignment. Therefore we discuss possible
display options of such a future program.
One alternative is to show the two trees above
each other (as in figure 1). And there are
many ways to visualize the alignment: Either
by drawing lines between the nodes (as we did),
or by color marking the nodes, or by opening an-
other window where only chosen parallel nodes
are shown. The latter case corresponds to a
zoom function, but this also entails that the user
has to click on a node to view the alignment.
Another alternative would be a mirror imag-
ing. One language would have its tree with the
root at the top and the tree of the other lan-
guage would be below with the root at the bot-
tom. The alignment could be portrayed in the
same ways as above.
But then the display problem is mainly a
problem concerning the computer screens of to-
day, where a large picture partly lands outside
of the screen, while a smaller scale picture might
result in words that are too small to be read-
able. One solution could be to use two screens
(as is done in complex layout tasks), but then
we cannot have a solution with the trees above
each other, but rather next to each other, pos-
sibly with some kind of color marking of the
nodes.
A last alternative is to use vertical trees,
where the words are listed below each other,
showing phrase depth horizontally. Then the
alignment could be shown by having the nodes
side by side instead of above each other. This
is the least space consuming alternative, but it
is also the least intuitive one. Furthermore, this
is not a viable alternative if the trees contain
crossing branches.
We currently favor the first approach with
two trees above each other, and we have writ-
ten a program that takes the SVG (scalable
vector graphics) representation of two trees (as
exported from TIGERSearch), merges the two
graphs into a single graph and adds the phrase
alignment lines based on the information in the
alignment file.
6 Conclusions
We have reported on our experiments for build-
ing a German-Swedish parallel treebank. We
have shown that by mapping the German PoS
tag set to the Swedish tag set we were able
to reuse the German chunker for the semi-
automatic annotation of the Swedish sentences.
Our experiments have also shown that the Ger-
man annotation guidelines with minor adapta-
tions are well-suited for Swedish.
We have argued that tools for building mono-
lingual treebanks can be used for parallel tree-
banks as well, and that tools for sub-sentence
alignment are available but they are not enough
evaluated yet for aligning tree structures. Tools
for viewing and searching through parallel tree-
banks are missing.
7 Acknowledgements
We would like to thank the anonymous review-
ers for useful comments, the members of the
Nordic Treebank Network for many interesting
discussions, and David Hagstrand for handling
our annotation databases.
References
Anne Abeille?, editor. 2003. Building and Us-
ing Parsed Corpora, volume 20 of Text,
Speech and Language Technology. Kluwer,
Dordrecht.
Lars Ahrenberg, Magnus Merkel, and Mikael
Andersson. 2002. A system for incremental
and interactive word linking. In Proceedings
from The Third International Conference on
Language Resources and Evaluation (LREC-
2002), pages 485?490, Las Palmas.
Lars Borin, editor. 2002. Parallel Corpora,
Parallel Worlds. Selected Papers from a Sym-
posium on Parallel and Comparable Corpora
at Uppsala University, Sweden, 22-23 April,
1999., volume 43 of Language and Comput-
ers. Rodopi, Amsterdam.
Clara Cabezas, Bonnie Dorr, and Philip Resnik.
2001. Spanish language processing at Univer-
sity of Maryland: Building infrastructure for
multilingual applications. In Proceedings of
the Second International Workshop on Span-
ish Language Processing and Language Tech-
nologies (SLPLT-2), Jaen, Spain, September.
Martin Cmejrek, Jan Curin, and Jiri Havelka.
2003. Treebanks in machine translation. In
Proc. Of the 2nd Workshop on Treebanks and
Linguistic Theories, Va?xjo?, Sweden.
Lea Cyrus, Hendrik Feddes, and Frank Schu-
macher. 2003. FuSe - a multi-layered paral-
lel treebank. In Proc. Of the 2nd Workshop
on Treebanks and Linguistic Theories, Va?xjo?,
Sweden.
Rebecca Hwa, Philip Resnik, Amy Weinberg,
and Okan Kolak. 2002. Evaluating transla-
tional correspondence using annotation pro-
jection. In Proceedings of the 40th Annual
Meeting of the ACL, Philadelphia.
Bea?ta Megyesi. 2002. Data-Driven Syn-
tactic Analysis. Methods and Applications
for Swedish. Doctoral dissertation, Kungl.
Tekniska Ho?gskolan. Department of Speech,
Music and Hearing, Stockholm.
I. Dan Melamed. 2001. Empirical Methods for
Exploiting Parallel Texts. MIT Press, Cam-
bridge, MA.
Joakim Nivre. 2002. What kinds of trees grow
in Swedish soil? A comparison of four anno-
tation schemes for Swedish. In Proc. Of First
Workshop on Treebanks and Linguistic The-
ory, Sozopol, Bulgaria.
Yvonne Samuelsson. 2004. Parallel phrases.
Experiments towards a German-Swedish par-
allel treebank. C-uppsats, Stockholms Uni-
versitet.
Ann Taylor, Mitchell Marcus, and Beatrice
Santorini. 2003. The Penn Treebank: An
overview. In Anne Abeille?, editor, Build-
ing and Using Parsed Corpora, volume 20
of Text, Speech and Language Technology.
Kluwer, Dordrecht.
Jo?rg Tiedemann. 2003. Recycling Transla-
tions. Extraction of Lexical Data from Par-
allel Corpora and Their Application in Nat-
ural Language Processing. Acta universitatis
upsaliensis, Uppsala University.
D. Yarowsky, G. Ngai, and R. Wicentowski.
2001. Inducing multilingual text analysis
tools via robust projection across aligned cor-
pora. In Proceedings of HLT 2001, First In-
ternational Conference on Human Language
Technology Research.
XML-based Phrase Alignment in Parallel Treebanks
Martin Volk, Sofia Gustafson-Capkova?, Joakim Lundborg,
Torsten Marek, Yvonne Samuelsson, Frida Tidstro?m
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
volk@ling.su.se
Abstract
This paper describes the usage of XML for
representing cross-language phrase align-
ments in parallel treebanks. We have de-
veloped a TreeAligner as a tool for interac-
tively inserting and correcting such align-
ments as an independent level of treebank
annotation.
1 Introduction
The combined research on treebanks and paral-
lel corpora has recently led to parallel treebanks.
A parallel treebank consists of syntactically anno-
tated sentences in two or more languages, taken
from translated (i.e. parallel) documents. In ad-
dition, the syntax trees of two corresponding sen-
tences are aligned on a sub-sentential level. This
means word level, phrase level and clause level,
but we will refer to it as phrase alignment since
it best represents the idea. Parallel treebanks can
be used as training or evaluation corpora for word
and phrase alignment, as input for example-based
machine translation (EBMT), as training corpora
for transfer rules, or for translation studies.
We are developing an English-German-Swedish
parallel treebank. In this paper we will focus on
the representation of the treebank and the align-
ment. We will briefly explain the steps for building
the parallel treebank and describe our new align-
ment tool. This paper is a follow-up and revision
of (Samuelsson and Volk, 2005) based on fresh in-
sights from this tool.
2 Building the treebanks
Our parallel treebank contains the first two chap-
ters of Jostein Gaarder?s novel ?Sofie?s World?
with about 500 sentences.1 In addition it contains
500 sentences from economy texts (a quarterly re-
port by a multinational company as well as part of
a bank?s annual report).
In creating the parallel treebank, we have
first annotated the monolingual treebanks with
the ANNOTATE treebank editor.2 It includes
Thorsten Brants? statistical Part-of-Speech Tagger
and Chunker. The chunker follows the TIGER
annotation guidelines for German (Brants and
Hansen, 2002), which gives a flat phrase structure
tree. This means, for instance, no unary nodes,
no ?unnecessary? NPs (noun phrases) within PPs
(prepositional phrases) and no finite VPs (verb
phrases).
Using a flat tree structure for manual treebank
annotation has two advantages for the human an-
notator: fewer annotation decisions, and a better
overview of the trees. This comes at the prize
of the trees not being complete from a linguistic
point of view. Moreover, flat syntax trees are also
problematic for node alignment in a parallel tree-
bank. We prefer to have ?deep trees? to be able to
draw the alignment on as many levels as possible;
in fact, the more detailed the sentence structure is,
the more expressive our alignment can become.
As an example, let us look at the work
flow for the German-Swedish parallel treebank.
We first annotated the German sentences semi-
automatically in the flat manner, and we then auto-
matically deepened the flat syntax trees (Samuels-
son and Volk, 2004).
1A prototype of the parallel treebank was developed by
Yvonne Samuelsson and contains the first chapter of the
novel in German and Swedish. Later, a French version was
added and aligned to the Swedish treebank by (Tidstro?m,
2005). We would like to thank Eckhard Bick, Declan Groves
and Jo?rg Tiedemann for their help.
2www.coli.uni-sb.de/sfb378/negra-corpus/annotate.html
93
We annotated the Swedish sentences by first
tagging them with a Part-of-Speech tagger trained
on SUC (the Stockholm-Umea? Corpus). Since we
did not have a Swedish treebank to train a Swedish
chunker, we used a trick to apply the German
chunker for Swedish sentences. We mapped the
Swedish Part-of-Speech tags in the Swedish sen-
tences to the corresponding German tags. Since
the German chunker works on these tags, it then
suggested constituents for the Swedish sentences,
assuming they were German sentences. These
experiments and the resulting time gain were re-
ported in (Volk and Samuelsson, 2004). Upon
completion of the Swedish treebank with flat syn-
tax trees, we applied the same deepening method
as for German, and we then converted the Part-of-
Speech labels back to the Swedish labels.
Finally, we annotated the English sentences ac-
cording to the Penn Treebank guidelines. We
trained the PoS tagger and the chunker on the Penn
Treebank and integrated them into ANNOTATE.
The English guidelines lead to complete trees so
that the deepening step is not needed.
3 XML Representation of the Trees
After finishing the monolingual treebanks with
ANNOTATE, the trees were exported from the
accompanying SQL database and converted into
TIGER-XML. TIGER-XML is a line-based (i.e.
not nested and thus database-friendly) representa-
tion for graph structures, which includes syntax
trees with node labels, edge labels, multiple fea-
tures on the word level and even crossing edges.3
In a TIGER-XML graph each leaf (= token) and
each node (= linguistic constituent) has a unique
identifier which is prefixed with the sentence num-
ber. Leaves are numbered from 1 to 499 and nodes
starting from 500 (under the plausible assumption
that no sentence will ever have more than 499 to-
kens). As can be seen in the following exam-
ple, node 500 in sentence 12 is of the category
PP (prepositional phrase). The phrase consists
of word number 4, which is the preposition in,
plus node 502 which in turn is marked as an NP
(noun phrase), consisting of the words 5 and 6. It
should be noted that the id attribute in the token
lines serves a dual purpose of identifier and order
marker. This makes it possible to represent cross-
ing branches.
<s id="s12">
3See www.ims.uni-stuttgart.de/projekte/TIGER
<graph root="s12_501">
<terminals>
<t id="s12_1" word="Jetzt" pos="ADV" />
<t id="s12_2" word="bog" pos="VVFIN" />
<t id="s12_3" word="sie" pos="PPER" />
<t id="s12_4" word="in" pos="APPR" />
<t id="s12_5" word="den" pos="ART" />
<t id="s12_6" word="Kl?verveien" pos="NE"/>
<t id="s12_7" word="ein" pos="PTKVZ" />
<t id="s12_8" word="." pos="$." />
</terminals>
<nonterminals>
<nt id="s12_500" cat="PP">
<edge label="HD" idref="s12_4" />
<edge label="NK" idref="s12_502" />
</nt>
<nt id="s12_502" cat="NP">
<edge label="NK" idref="s12_5" />
<edge label="HD" idref="s12_6" />
</nt>
[...]
</nonterminals>
</graph>
</s>
This means that the token identifiers and con-
stituent identifiers are used as pointers to represent
the nested tree structure. This example thus repre-
sents the upper tree in figure 1.
One might wonder why tree nesting is not di-
rectly mapped into XML nesting. But the require-
ment that the representation format must support
crossing edges rules out this option. TIGER-XML
is a powerful representation format and is typically
used with constituent symbols on the nodes and
functional information on the edge labels. This
constitutes a combination of constituent structure
and dependency structure information.
4 XML Representation of the Alignment
Phrase alignment can be regarded as an additional
layer of information on top of the syntax struc-
ture. We use the unique node identifiers for the
phrase alignment across parallel trees. We also
use an XML representation for storing the align-
ment. The alignment file first stores the names of
the treebank files and assigns identifiers to them.
Every single phrase alignment is then stored with
the tag align. Thus the entry in the following
example represents the alignment of node 505 in
sentence 13 of language one (German) to the node
506 in sentence 14 of language two (Swedish).
<treebanks>
<tbank file="Sofie_DE.xml" id="De"/>
<tbank file="Sofie_SV.xml" id="Sv"/>
</treebanks>
<align type="exact">
<node node_id="s13_505" tbank_id="De"/>
<node node_id="s14_506" tbank_id="Sv"/>
</align>
94
This representation allows phrase alignments
within m:n sentence alignments, which we have
used in our project. The XML also allows m:n
phrase alignments, which we however have not
used for reasons of simplicity and clarity. Two
nodes are aligned if the words which they span
convey the same meaning and could serve as trans-
lation units.
The alignment format allows alignments to be
specified between an arbitrary number of nodes,
for example nodes from three languages. And
it includes an attribute type which we currently
use to distinguish between exact and approximate
alignments.
5 Our Tree Alignment Tool
After finishing the monolingual trees we want to
align them on the phrase level. For this purpose
we have developed a ?TreeAligner?. This program
is a graphical user interface to insert (or correct)
alignments between pairs of syntax trees.4 The
TreeAligner can be seen in the line of tools such
as I*Link (Ahrenberg et al, 2002) or Cairo (Smith
and Jahr, 2000) but it is especially tailored to visu-
alize and align full syntax trees.
The TreeAligner requires three input files. One
TIGER-XML file with the trees from language
one, another TIGER-XML file with the trees from
language two, plus the alignment file as described
above. The alignment file might initially be empty
when we want to start manual alignment from
scratch, or it might contain automatically com-
puted alignments for correction. The TreeAligner
displays tree pairs with the trees in mirror orien-
tation (one top-up and one top-down). See fig-
ure 1 for an example. This has the advantage that
the alignment lines cross fewer parts of the lower
tree. The trees are displayed with node labels and
greyed-out edge labels. The PoS labels are omit-
ted in the display since they are not relevant for the
task.
Each alignment is displayed as a dotted line be-
tween one node (or word) from each tree. Clicking
on a node (or a word) in one tree and dragging the
mouse pointer to a node (or a word) in the other
tree inserts an alignment line. Figure 2 shows an
example of a tree pair with alignment lines. Cur-
rently the TreeAligner supports two types of align-
4The TreeAligner has been implemented in Python by
Joakim Lundborg and is freely available at www.ling.su.se/
DaLi/downloads/treealigner/index.htm
Figure 1: Tree pair German-Swedish in the
TreeAligner.
ment lines (displayed in different colors) which
are used to indicate exact translation correspon-
dence vs. approximate translation correspondence.
However, our experiments indicate that eventually
more alignment types will be needed to precisely
represent different translation deviations.
Often one tree needs to be aligned to two trees
in the other language. We therefore provide the
option to scroll the trees independently. For in-
stance, if we have aligned only a part of tree 20
from language one to tree 18 of language two, we
may scroll to tree 19 of language two in order to
align the remaining parts of tree 20.5
The TreeAligner is designed as a stand-alone
tool (i.e. it is not prepared for collaborative anno-
tation). It stores every alignment in an XML file
(in the format described above) as soon as the user
moves to a new tree pair. It has been tested on
parallel treebanks with several hundred trees each.
6 Conclusion
We have shown a straightforward way to tie in
XML-based phrase alignment information with
syntax trees represented in TIGER-XML. The
alignment information is stored independently
from the treebank files. This independence allows
for a modularization and separation of the anno-
tation but it entails that the synchronization of the
5The final result of an m:n tree alignment can be visual-
ized with an SVG-based display which we have described in
(Samuelsson and Volk, 2005). SVG (Scalable Vector Graph-
ics) describes vector graphics in XML.
95
Figure 2: Tree pair German-Swedish with alignment in the TreeAligner.
treebanks with the alignment needs to be guarded
separately. If any of the treebanks is modified, the
modification of the alignment needs to follow.
We have argued for the use of a graphical
TreeAligner to display and interactively modify
the alignment between parallel syntax trees. The
TreeAligner allows for m:n sentence alignment,
word alignment and node alignment. And it sup-
ports the distinction between exact and approxi-
mate alignments.
As a next step we plan to integrate a com-
ponent for automatic phrase alignment into the
TreeAligner. The user can then select a tree pair
and will get automatic phrase alignment predic-
tions. We have already experimented with the
projection of automatically computed word align-
ments to predict phrase alignment. Of course, the
automatic phrase alignment has to be manually
checked if we want to ensure high quality align-
ment data.
Another avenue of further research is the inclu-
sion of yet more levels of annotation. For exam-
ple, we are currently experimenting with the anno-
tation of semantic frames on top of the treebanks.
We use the SALSA tool developed at Saarbru?cken
University (Erk and Pado, 2004) which also as-
sumes TIGER-XML input. So, TIGER-XML has
become the lingua franca of treebank annotation
which allows for the addition of arbitrary layers.
References
Lars Ahrenberg, Magnus Merkel, and Mikael Anders-
son. 2002. A system for incremental and interactive
word linking. In Proc. of LREC-2002, pages 485?
490, Las Palmas.
Sabine Brants and Silvia Hansen. 2002. Developments
in the TIGER annotation scheme and their realiza-
tion in the corpus. In Proc. of LREC-2002, pages
1643?1649, Las Palmas.
Katrin Erk and Sebastian Pado. 2004. A powerful and
versatile XML format for representing role-semantic
annotation. In Proc. of LREC-2004, Lisbon.
Yvonne Samuelsson and Martin Volk. 2004. Au-
tomatic node insertion for treebank deepening. In
Proc. of 3rd Workshop on Treebanks and Linguistic
Theories, Tu?bingen, December.
Yvonne Samuelsson and Martin Volk. 2005. Presen-
tation and representation of parallel treebanks. In
Proc. of the Treebank-Workshop at Nodalida, Joen-
suu, May.
Noah A. Smith and Michael E. Jahr. 2000. Cairo:
An alignment visualization tool. In Proc. of LREC-
2000, Athens.
Frida Tidstro?m. 2005. Extending a parallel treebank
with data in French. C-uppsats, Department of Lin-
guistics, Stockholm University, April.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
96
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 51?57
Manchester, August 2008
Human Judgements in Parallel Treebank Alignment
Martin Volk and Torsten Marek
University of Zurich
Institute of Computational Linguistics
8050 Zurich, Switzerland
volk@cl.uzh.ch
Yvonne Samuelsson
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
yvonne.samuelsson@ling.su.se
Abstract
We have built a parallel treebank that
includes word and phrase alignment.
The alignment information was manually
checked using a graphical tool that al-
lows the annotator to view a pair of trees
from parallel sentences. We found the
compilation of clear alignment guidelines
to be a difficult task. However, experi-
ments with a group of students have shown
that we are on the right track with up to
89% overlap between the student annota-
tion and our own. At the same time these
experiments have helped us to pin-point
the weaknesses in the guidelines, many of
which concerned unclear rules related to
differences in grammatical forms between
the languages.
1 Introduction
Establishing translation correspondences is a dif-
ficult task. This task is traditionally called align-
ment and is usually performed on the paragraph
level, sentence level and word level. Alignment
answers the question: Which part of a text in lan-
guage L1 corresponds in meaning to which part of
a text in language L2 (under the assumption that
the two texts represent the same meaning in differ-
ent languages). This may mean that one text is the
translation of the other or that both are translations
derived from a third text.
There is considerable interest in automating the
alignment process. Automatic sentence alignment
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of legacy translations helps to fill translation mem-
ories. Automatic word alignment is a crucial step
in training statistical machine translation systems.
Both sentence and word alignment have to deal
with 1:many alignments, i.e. sometimes a sentence
in one language is translated as two or three sen-
tences in the other language.
In other respects sentence alignment and word
alignment are fundamentally different. It is rela-
tively safe to assume the same sentence order in
both languages when computing sentence align-
ment. But such a monotonicity assumption is not
possible for word alignment which needs to allow
for word order differences and thus for crossing
alignments. And while algorithms for sentence
alignment usually focus on length comparisons (in
terms of numbers of characters), word alignment
algorithms use cross-language cooccurrence fre-
quencies as a key feature.
Our work focuses on word alignment and on an
intermediate alignment level which we call phrase
alignment. Phrase alignment encompasses the
alignment from simple noun phrases and preposi-
tional phrases all the way to complex clauses. For
example, on the word alignment level we want to
establish the correspondence of the German ?verb
form plus separated prefix? fing an with the Eng-
lish verb form began. While in phrase alignment
we mark the correspondence of the verb phrases
ihn in den Briefkasten gesteckt and dropped it in
the mail box.
We regard phrase alignment as alignment be-
tween linguistically motivated phrases, in con-
trast to some work in statistical machine trans-
lation where phrase alignment is defined as the
alignment between arbitrary word sequences. Our
phrase alignment is alignment between nodes in
constituent structure trees. See figure 1 for an ex-
51
ample of a tree pair with word and phrase align-
ment.
We believe that such linguistically motivated
phrase alignment provides useful phrase pairs for
example-based machine translation, and provides
interesting insights for translation science and
cross-language comparisons. Phrase alignments
are particularly useful for annotating correspon-
dences of idiomatic or metaphoric language use.
2 The Parallel Treebank
We have built a trilingual parallel treebank in Eng-
lish, German and Swedish. The treebank consists
of around 500 trees from the novel Sophie?s World
and 500 trees from economy texts (an annual re-
port from a bank, a quarterly report from an inter-
national engineering company, and the banana cer-
tification program of the Rainforest Alliance). The
sentences in Sophie?s World are relatively short
(14.8 tokens on average in the English version),
while the sentences in the economy texts are much
longer (24.3 tokens on average; 5 sentences in the
English version have more than 100 tokens).
The treebanks in English and German consist of
constituent structure trees that follow the guide-
lines of existing treebanks, the NEGRA/TIGER
guidelines for German and the Penn treebank
guidelines for English. There were no guidelines
for Swedish constituent structure trees. We have
therefore adapted the German treebank guidelines
for Swedish. Both German trees and Swedish trees
are annotated with flat structures but subsequently
automatically deepened to result in richer and lin-
guistically more plausible tree structures.
When the monolingual treebanks were finished,
we started with the word and phrase alignment.
For this purpose we have developed a special tool
called the Stockholm TreeAligner (Lundborg et
al., 2007) which displays two trees and allows the
user to draw alignment lines by clicking on nodes
and words. This tool is similar to word alignment
tools like ILink (Ahrenberg et al, 2003) or Cairo
(Smith and Jahr, 2000). As far as we know our tool
is unique in that it allows the alignments of lin-
guistically motivated phrases via node alignments
in parallel constituent structure trees (cf. (Samuels-
son and Volk, 2007)).
After having solved the technical issues, the
challenge was to compile precise and comprehen-
sive guidelines to ensure smooth and consistent
alignment decisions. In (Samuelsson and Volk,
2006) we have reported on a first experiment to
evaluate inter-annotator agreement from our align-
ment tasks.
In this paper we report on another recently con-
ducted experiment in which we tried to identify
the weaknesses in our alignment guidelines. We
asked 12 students to alignment 20 tree pairs (Eng-
lish and German) taken from our parallel treebank.
By comparing their alignments to our Gold Stan-
dard and to each other we gained valuable insights
into the difficulty of the alignment task and the
quality of our guidelines.
3 Related Research
Our research on word and phrase alignment is re-
lated to previous work on word alignment as e.g.
in the Blinker project (Melamed, 1998) or in the
UPLUG project (Ahrenberg et al, 2003). Align-
ment work on parallel treebanks is rare. Most
notably there is the Prague Czech-English tree-
bank (Kruijff-Korbayova? et al, 2006) and the
Linko?ping Swedish-English treebank (Ahrenberg,
2007). There has not been much work on the align-
ment of linguistically motivated phrases. Tinsley
et al (2007) and Groves et al (2004) report on
semi-automatic phrase alignment as part of their
research on example-based machine translation.
Considering the fact that the alignment task is
essentially a semantic annotation task, we may
also compare our results to other tasks in seman-
tic corpus annotation. For example, we may con-
sider the methods for resolving annotation con-
flicts and the figures for inter-annotator agreement
in frame-semantic annotation as found in the Ger-
man SALSA project (cf. (Burchardt et al, 2006)).
4 Our Alignment Guidelines
We have compiled alignment guidelines for word
and phrase alignment between annotated syntax
trees. The guidelines consist of general principles,
concrete rules and guiding principles.
The most important general principles are:
1. Align items that can be re-used as units in a
machine translation system.
2. Align as many items (i.e. words and phrases)
as possible.
3. Align as close as possible to the tokens.
The first principle is central to our work. It
defines the general perspective for our alignment.
52
Figure 1: Tree pair German-English with word and phrase alignments.
We do not want to know which part of a sentence
has possibly given rise to which part of the cor-
respondence sentence. Instead our perspective is
on whether a phrase pair is general enough to be
re-used as translation unit in a machine translation
system. For example, we do not want to align die
Verwunderung u?ber das Leben with their astonish-
ment at the world although these two phrases were
certainly triggered by the same phrase in the orig-
inal and both have a similar function in the two
corresponding sentences. These two phrases seen
in isolation are too far apart in meaning to license
their re-use. We are looking for correspondences
like was fu?r eine seltsame Welt and what an ex-
traordinary world which would make for a good
translation in many other contexts.
Some special rules follow from this principle.
For example, we have decided that a pronoun in
one language shall never be aligned with a full
noun in the other, since such a pair is not directly
useful in a machine translation system.
Principles 2 and 3 are more technical. Princi-
ple 2 tells our annotators that alignment should be
exhaustive. We want to re-use as much as pos-
sible from the treebank, so we have to look for
as many alignments as possible. And principle 3
says that in case of doubt the alignment should go
to the node that is closest to the terminals. For
example, our German treebank guidelines require
a multi-word proper noun to first be grouped in
a PN phrase which is a daughter node of a noun
phrase [[Sofie Amundsen]PN ]NP whereas
the English guidelines only require the NP node
[Sophie Amundsen]NP. When we align the
two names, principle 3 tells us to draw the align-
ment line between the German PN node and the
English NP node since the PN node is closer to the
tokens than the German NP node.
Often we are confronted with phrases that are
not exact translation correspondences but approx-
imate translation correspondences. Consider the
phrases mehr als eine Maschine and more than a
piece of hardware. This pair does not represent the
closest possible translation but it represents a pos-
sible translation in many contexts. In a way we
could classify this pair as the ?second-best? trans-
lation. To allow for such distinctions we provide
our annotators with a choice between exact transla-
tion correspondences and approximate correspon-
dences. We also use the term fuzzy correspon-
dence to refer to and give an intuitive picture of
these approximate correspondences. The option to
53
distinguish between different alignment strengths
sounded very attractive at the start but it turned out
to be the source for some headaches later. Where
and how can we draw the line between exact and
fuzzy translation correspondences?
We have formulated some clear-cut rules:
1. If an acronym is to be aligned with a spelled-
out term, it is always an approximate align-
ment. For example, in our economy reports
the English acronym PT stands for Power
Technology and is aligned to the German En-
ergietechnik as a fuzzy correspondence.
2. Proper names shall be aligned as exact align-
ments (even if they are spelled differently
across languages; e.g. Sofie vs. Sophie).
But many open questions persist. Is einer der
ersten Tage im Mai an exact or rather a fuzzy trans-
lation correspondence of early May? We decided
that it is not an exact correspondence. How shall
we handle zu dieser Jahreszeit vs. at this time of
the year where a literal translation would be in this
season? We decided that the former is still an exact
correspondence. These examples illustrate the dif-
ficulties that make us wonder how useful the dis-
tinction between exact and approximate translation
correspondence really is.
Automatically ensuring the overall consistency
of the alignment decisions is a difficult task.
But we have used a tool to ensure the consis-
tency within the exact and approximate alignment
classes. The tool computes the token span for each
alignment and checks if the same tokens pairs have
always received the same alignment type. For ex-
ample, if the phrase pair mit einer blitzschnellen
Bewegung and with a lightning movement is once
annotated as exact alignment, then it should always
be annotated as exact alignment. Figure 1 shows
approximate alignments between the PPs in der
Hand and in her hand. It was classified as approxi-
mate rather than exact alignment since the German
PP lacks the possessive determiner.
Currently our alignment guidelines are 6 pages
long with examples for English-German and
English-Swedish alignments.
5 Experiments with Student Annotators
In order to check the inter-annotator agreement for
the alignment task we performed the following ex-
periment. We gave 20 tree pairs in German and
English to 12 advanced undergraduate students in
a class on ?Machine Translation and Parallel Cor-
pora?. Half of the tree pairs were taken from our
Sophie?s World treebank and the other half from
our Economy treebank. We made sure that there
was one 1:2 sentence alignment in the sample. The
students did not have access to the Gold Standard
alignment.
In class we demonstrated the alignment tool to
the students and we introduced the general align-
ment principles to them. Then the students were
given a copy of the alignment guidelines. We
asked them to do the alignments independently of
each other and to the best of their knowledge ac-
cording to the guidelines.
In our own annotation of the 20 tree pairs (= the
Gold Standard alignment) we have the following
numbers of alignments:
type exact fuzzy total
Sophie part word 75 3 78
phrase 46 12 58
Economy part word 159 19 178
phrase 62 9 71
In the Sophie part of the experiment treebank we
have 78 word-to-word alignments and 58 phrase-
to-phrase alignments. Note that some phrases con-
sist only of one word and thus the same alignment
information is represented twice. We have deliber-
ately kept this redundancy.
The alignments in the Sophie part consist of
125 times 1:1 alignments, 4 times 1:2 alignments
and one 1:3 alignment (wa?re vs. would have been)
when viewed from the German side. There are 3
times 1:2 alignments (e.g. introducing vs. stellte
vor) and no other 1:many alignment when viewed
from the English side.
In the Economy part the picture is similar. The
vast majority are 1:1 alignments. There are 207
times 1:1 alignments and 21 times 1:2 alignments
(many of which are German compound nouns)
when viewed from German. And there are 235
times 1:1 alignments, plus 4 times 1:2 alignments,
plus 2 times 1:3 alignments when viewed from
English (e.g. the Americas was aligned to the three
tokens Nord- und Su?damerika).
The student alignments showed a huge vari-
ety in terms of numbers of alignments. In the
Sophie part they ranged from 125 alignments to
bare 47 alignments (exact alignments and fuzzy
alignments taken together). In the Economy part
the variation was between 259 and 62 alignments.
54
On closer inspection we found that the student
with the lowest numbers works as a translator
and chose to use a very strict criterion of transla-
tion equivalence rather than translation correspon-
dence. Three other students at the end of the list are
not native speakers of either German and English.
We therefore decided to exclude these 4 students
from the following comparison.
The student alignments allow for the investiga-
tion of a number of interesting questions:
1. How did the students? alignments differ from
the Gold Standard?
2. Which were the alignments done by all stu-
dents?
3. Which were the alignments done by single
students only?
4. Which alignments varied most between exact
and fuzzy alignment?
When we compared each student?s alignments
to the Gold Standard alignments, we computed
three figures:
1. How often did the student alignment and the
Gold Standard alignment overlap?
2. How many Gold Standard alignments did the
student miss?
3. How many student alignments were not in the
Gold Standard?
The remaining 8 students reached between 81%
and 48% overlap with the Gold Standard on the
Sophie part, and between 89% and 66% overlap
with the Gold Standard on the Economy texts. This
can be regarded as their recall values if we assume
that the Gold Standard represents the correct align-
ments. These same 8 students additionally had
between 2 and 22 own alignments in the Sophie
part and between 12 and 55 own alignments in the
Economy part.
So the interesting question is: What kind of
alignments have they missed, and which were
the additional own alignments that they suggested
(alignments that are not in the gold standard)? We
first checked the students with the highest numbers
of own alignments. We found that some of these
alignments were due to the fact that students had
ignored the rule to align as close to the tokens as
possible (principle 3 above).
Another reason was that students sometimes
aligned a word (or some words) with a node.
For example, one student had aligned the word
natu?rlich to the phrase of course instead of to the
word sequence of course. Our alignment tool al-
lows that, but the alignment guidelines discour-
age such alignments. There might be exceptional
cases where a word-to-phrase alignment is neces-
sary in order to keep valuable information, but in
general we try to stick to word-to-word and phrase-
to-phrase alignments.
Another discrepancy occurred when the stu-
dents aligned a German verb group with a single
verb form in English (e.g. ist zuru?ckzufu?hren vs.
reflecting). We have decided to only align the full
verb to the full verb (independent of the inflection).
This means that we align only zuru?ckzufu?hren to
reflecting in this example.
The uncertainties on how to deal with different
grammatical forms led to the most discrepancies.
Shall we align the definite NP die Umsa?tze with
the indefinite NP revenues since it is much more
common to drop the article in an English plural NP
than in German? Shall we align a German genitive
NP with an of-PP in English (der beiden Divisio-
nen vs. of the two divisions)? We have decided to
give priority to form over function and thus to align
the NP der beiden Divisionen with the NP the two
divisions. But of course this choice is debatable.
When we compute the intersection of the align-
ments done by all students (ignoring the difference
between exact and fuzzy alignments), we find that
about 50% of the alignments done by the student
with the smallest number of alignments is shared
by all other students. All of the alignments in the
intersection are in our Gold Standard file. This in-
dicates that there is a core of alignments that are
obvious and uncontroversial. Most of them are
word alignments.
When we compute the union of the alignments
done by all students (again ignoring the difference
between exact and fuzzy alignments), we find that
the number of alignments in the union is 40% to
50% higher than the number of alignments done by
the student with the highest number of alignments.
It is also about 40% to 50% higher than the number
of alignments in the Gold Standard. This means
that there is considerable deviation from the Gold
Standard.
Comparing the union of the students? align-
ments to the Gold Standard points to some weak-
55
nesses of the guidelines. For example, one align-
ment in the Gold Standard that was missed by all
students concerns the alignment of a German pro-
noun (wenn sie die Hand ausstreckte) to an empty
token in English (herself shaking hands). Our
guidelines recommend to align such cases as fuzzy
alignments, but of course it is difficult to determine
that the empty token really corresponds to the Ger-
man word.
Other discrepancies concern cases of differing
grammatical forms, e.g. a German definite singu-
lar noun phrase (die Hand) that was aligned to an
English plural noun phrase (Hands) in the Gold
Standard but missed by all students. Finally there
are a few cases where obvious noun phrase cor-
respondences were simply overlooked by all stu-
dents (sich - herself ) although the tokens them-
selves were aligned. Such cases should be handled
by an automated process in the alignment tool that
projects from aligned tokens to their mother nodes
(in particular in cases of single token phrases).
We also investigated how many exact align-
ments and how many fuzzy alignments the stu-
dents had used. The following table gives the fig-
ures.
exact fuzzy overlap total
Sophie part 152 106 69 189
Economy part 296 188 119 366
The alignments done by all students resulted in a
union set of 189 alignments for the Sophie part and
366 alignments for the Economy part. The align-
ments in the Sophie part consisted of 152 exact
alignments and 106 fuzzy alignments. This means
that 69 alignments were marked as both exact and
fuzzy. In other words, in 69 cases at least one stu-
dent has marked an alignment as fuzzy while at
least one other student has marked the same align-
ment as good. So there is still considerable con-
fusion amongst the annotators on how to decide
between exact and fuzzy alignments. And in case
of doubt many students have decided in favor of
fuzzy alignments.
6 Conclusions
We have shown the difficulties in creating cross-
language word and phrase alignments. Experi-
ments with a group of students have helped to iden-
tify the weaknesses in our alignment guidelines
and in our Gold Standard alignment. We have re-
alized that the guidelines need to contain a host
of fine-grained alignment rules and examples that
will clarify critical cases.
In order to evaluate a set of alignment experi-
ments with groups of annotators it is important to
have good visualization tools to present the results.
We have worked with Perl scripts for the compar-
ison and with our own TreeAligner tool for the vi-
sualization. For example we have used two colors
to visualize a student?s alignment overlap with the
Gold Standard in one color and his own alignments
(that are not in the Gold Standard) in another color.
In order to visualize the agreements of the whole
group it would be desirable to have the option to in-
crease the alignment line width in proportion to the
number of annotators that have chosen a particular
alignment link. This would give an intuitive im-
pression of strong alignment links and weak align-
ment links.
Another option for future extension of this work
is an even more elaborate classification of the
alignment links. (Hansen-Schirra et al, 2006) have
demonstrated how a fine-grained distinction be-
tween different alignment types could look like.
Annotating such a corpus will be labor-intensive
but provide for a wealth of cross-language obser-
vations.
References
Ahrenberg, Lars, Magnus Merkel, and Michael Petter-
stedt. 2003. Interactive word alignment for language
engineering. In Proc. of EACL-2003, Budapest.
Ahrenberg, Lars. 2007. LinES: An English-Swedish
parallel treebank. In Proc. of Nodalida, Tartu.
Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. The SALSA corpus: A Ger-
man corpus resource for lexical semantics. In Pro-
ceedings of LREC 2006, pages 969?974, Genoa.
Groves, Declan, Mary Hearne, and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In Proceedings of Coling 2004, pages 1072?
1078, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Hansen-Schirra, Silvia, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and
alignment in an English-German translation corpus.
In Proceedings of the EACL Workshop on Multidi-
mensional Markup in Natural Language Processing
(NLPXML-2006), pages 35? 42, Trento.
Kruijff-Korbayova?, Ivana, Kla?ra Chva?talova?, and Oana
Postolache. 2006. Annotation guidelines for the
Czech-English word alignment. In Proceedings of
LREC, Genova.
56
Lundborg, Joakim, Torsten Marek, Mae?l Mettler,
and Martin Volk. 2007. Using the Stockholm
TreeAligner. In Proc. of The 6th Workshop on Tree-
banks and Linguistic Theories, Bergen, December.
Melamed, Dan. 1998. Manual annotation of transla-
tional equivalence: The blinker project. Technical
Report 98-06, IRCS, Philadelphia PA.
Samuelsson, Yvonne and Martin Volk. 2006. Phrase
alignment in parallel treebanks. In Hajic, Jan and
Joakim Nivre, editors, Proc. of the Fifth Workshop on
Treebanks and Linguistic Theories, pages 91?102,
Prague, December.
Samuelsson, Yvonne and Martin Volk. 2007. Align-
ment tools for parallel treebanks. In Proceedings of
GLDV Fru?hjahrstagung 2007.
Smith, Noah A. and Michael E. Jahr. 2000. Cairo:
An alignment visualization tool. In Proc. of LREC-
2000, Athens.
Tinsley, John, Ventsislav Zhechev, Mary Hearne, and
Andy Way. 2007. Robust language pair-independent
sub-tree alignment. In Machine Translation Summit
XI Proceedings, Copenhagen.
57
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248?252
Manchester, August 2008
Mixing and Blending Syntactic and Semantic Dependencies
Yvonne Samuelsson
Dept. of Linguistics
Stockholm University
yvonne.samuelsson@ling.su.se
Johan Eklund
SSLIS
University College of Bor?as
johan.eklund@hb.se
Oscar T
?
ackstr
?
om
Dept. of Linguistics and Philology
SICS / Uppsala University
oscar@sics.se
Mark Fi
?
sel
Dept. of Computer Science
University of Tartu
fishel@ut.ee
Sumithra Velupillai
Dept. of Computer and Systems Sciences
Stockholm University / KTH
sumithra@dsv.su.se
Markus Saers
Dept. of Linguistics and Philology
Uppsala University
markus.saers@lingfil.uu.se
Abstract
Our system for the CoNLL 2008 shared
task uses a set of individual parsers, a set of
stand-alone semantic role labellers, and a
joint system for parsing and semantic role
labelling, all blended together. The system
achieved a macro averaged labelled F
1
-
score of 79.79 (WSJ 80.92, Brown 70.49)
for the overall task. The labelled attach-
ment score for syntactic dependencies was
86.63 (WSJ 87.36, Brown 80.77) and the
labelled F
1
-score for semantic dependen-
cies was 72.94 (WSJ 74.47, Brown 60.18).
1 Introduction
This paper presents a system for the CoNLL 2008
shared task on joint learning of syntactic and se-
mantic dependencies (Surdeanu et al, 2008), com-
bining a two-step pipelined approach with a joint
approach.
In the pipelined system, eight different syntac-
tic parses were blended, yielding the input for two
variants of a semantic role labelling (SRL) system.
Furthermore, one of the syntactic parses was used
with an early version of the SRL system, to pro-
vide predicate predictions for a joint syntactic and
semantic parser. For the final submission, all nine
syntactic parses and all three semantic parses were
blended.
The system is outlined in Figure 1; the dashed
arrow indicates the potential for using the predi-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Process
Parse + SRL
Process
8 MaltParsers
Parser Blender
Process
2 Pipelined SRLs
SRL Blender
Joint Parser/SRL
Possible
Iteration
Figure 1: Overview of the submitted system.
cate prediction to improve the joint syntactic and
semantic system.
2 Dependency Parsing
The initial parsing system was created using Malt-
Parser (Nivre et al, 2007) by blending eight dif-
ferent parsers. To further advance the syntactic ac-
curacy, we added the syntactic structure predicted
by a joint system for syntactic and semantic depen-
dencies (see Section 3.4) in the blending process.
2.1 Parsers
The MaltParser is a dependency parser genera-
tor, with three parsing algorithms: Nivre?s arc
standard, Nivre?s arc eager (see Nivre (2004)
for a comparison between the two Nivre algo-
rithms), and Covington?s (Covington, 2001). Both
of Nivre?s algorithms assume projectivity, but
the MaltParser supports pseudo-projective parsing
(Nilsson et al, 2007), for projectivization and de-
projectivization.
248
WSJ Brown
Best single parse 85.22% 78.37%
LAS weights 87.00% 80.60%
Learned weights 87.36% 80.77%
Table 1: Labelled attachment score on the two test
sets of the best single parse, blended with weights
set to PoS labelled attachment score (LAS) and
blended with learned weights.
Four parsing algorithms (the two Nivre al-
gorithms, and Covington?s projective and non-
projective version) were used, creating eight
parsers by varying the parsing direction, left-to-
right and right-to-left. The latter was achieved by
reversing the word order in a pre-processing step
and then restoring it in post-processing. For the fi-
nal system, feature models and training parameters
were adapted from Hall et al (2007).
2.2 Blender
The single parses were blended following the pro-
cedure of Hall et al (2007). The parses of each
sentence were combined into a weighted directed
graph. The Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967) was then used to find
the maximum spanning tree (MST) of the graph,
which was considered the final parse of the sen-
tence. The weight of each graph edge was calcu-
lated as the sum of the weights of the correspond-
ing edges in each single parse tree.
We used a simple iterative weight updating algo-
rithm to learn the individual weights of each single
parser output and part-of-speech (PoS) using the
development set. To construct an initial MST, the
labelled attachment score was used. Each single
weight, corresponding to an edge of the hypoth-
esis tree, was then iteratively updated by slightly
increasing or decreasing the weight, depending on
whether it belonged to a correct or incorrect edge
as compared to the reference tree.
2.3 Results
The results are summarized in Table 1; the parse
with LAS weights and the best single parse
(Nivre?s arc eager algorithm with left-to-right pars-
ing direction) are also included for comparison.
3 Semantic Role Labelling
The SRL system is a pipeline with three chained
stages: predicate identification, argument identifi-
cation, and argument classification. Predicate and
argument identification are treated as binary clas-
sification problems. In a simple post-processing
predicate classification step, a predicted predicate
is assigned the most frequent sense from the train-
ing data. Argument classification is treated as a
multi-class learning problem, where the classes
correspond to the argument types.
3.1 Learning and Parameter Optimization
For learning and prediction we used the freely
available support vector machine (SVM) imple-
mentation LIBSVM (version 2.86) (Chang and
Lin, 2001). The choice of cost and kernel parame-
ter values will often significantly influence the per-
formance of the SVM classifier. We therefore im-
plemented a parameter optimizer based on the DI-
RECT optimization algorithm (Gablonsky, 2001).
It iteratively divides the search space into smaller
hyperrectangles, sampling the objective function
in the centroid of each hyperrectangle, and select-
ing those hyperrectangles that are potentially opti-
mal for further processing. The search space con-
sisted of the SVM parameters to optimize and the
objective function was the cross-validation accu-
racy reported by LIBSVM.
Tests performed during training for predicate
identification showed that the use of runtime opti-
mization of the SVM parameters for nonlinear ker-
nels yielded a higher average F
1
-score effective-
ness. Surprisingly, the best nonlinear kernels were
always outperformed by the linear kernel with de-
fault settings, which indicates that the data is ap-
proximately linearly separable.
3.2 Filtering and Data Set Splitting
To decrease the number of instances during train-
ing, all predicate and argument candidates with
PoS-tags that occur very infrequently in the
training set were filtered out. Some PoS-tags
were filtered out for all three stages, e.g. non-
alphanumerics, HYPH, SYM, and LS. This ap-
proach was effective, e.g. removing more than half
of the total number of instances for predicate pre-
diction.
To speed up the SVM training and allow for
parallelization, each data set was split into several
bins. However, there is a trade-off between speed
and accuracy. Performance consistently deterio-
rated when splitting into smaller bins. The final
system contained two variants, one with more bins
based on a combination of PoS-tags and lemma
frequency information, and one with fewer bins
249
based only on PoS-tag information. The three
learning tasks used different splits. In general, the
argument identification step was the most difficult
and therefore required a larger number of bins.
3.3 Features
We implemented a large number of features (over
50)
1
for the SRL system. Many of them can be
found in the literature, starting from Gildea and
Jurafsky (2002) and onward. All features, except
bag-of-words, take nominal values, which are bi-
narized for the vectors used as input to the SVM
classifier. Low-frequency feature values (except
for Voice, Initial Letter, Number of Words, Rela-
tive Position, and the Distance features), below a
threshold of 20 occurrences, were given a default
value.
We distinguish between single node and node
pair features. The following single node features
were used for all three learning tasks and for both
the predicate and argument node:
2
? Lemma, PoS, and Dependency relation (DepRel) for
the node itself, the parent, and the left and right sibling
? Initial Letter (upper-case/lower-case), Number of
Words, and Voice (based on simple heuristics, only for
the predicate node during argument classification)
? PoS Sequence and PoS bag-of-words (BoW) for the
node itself with children and for the parent with chil-
dren
? Lemma and PoS for the first and last child of the node
? Sequence and BoW of Lemma and PoS for content
words
? Sequence and BoW of PoS for the immediate children?s
content words
? Sequence and BoW of PoS for the parent?s content
words and for the parent?s immediate children
? Sequence and BoW of DepRels for the node itself, for
the immediate children, and for the parent?s immediate
children
All extractors of node pair features, where the pair
consists of the predicate and the argument node,
can be used both for argument identification and
argument classification. We used the following
node pair features:
? Relative Position (the argument is before/after the pred-
icate), Distance in Words, Middle Distance in DepRels
? PoS Full Path, PoS Middle Path, PoS Short Path
1
Some features were discarded for the final system based
on Information Gain, calculated using Weka (Witten and
Frank, 2005).
2
For all features using lemma or PoS the (predicted) split
value is used.
The full path feature contains the PoS-tag of the ar-
gument node, all dependency relations between the
argument node and the predicate node and finally
the PoS-tag of the predicate node. The middle path
goes to the lowest common ancestor for argument
and predicate (this is also the distance calculated
by Middle Distance in DepRels) and the short path
only contains the dependency relation of the argu-
ment and predicate nodes.
3.4 Joint Syntactic and Semantic Parsing
When considering one predicate at a time, SRL be-
comes a regular labelling problem. Given a pre-
dicted predicate, joint learning of syntactic and se-
mantic dependencies can be carried out by simulta-
neously assigning an argument label and a depen-
dency relation. This is possible because we know
a priori where to attach the argument, since there
is only one predicate candidate
3
. The MaltParser
system for English described in Hall et al (2007)
was used as a baseline, and then optimized for this
new task, focusing on feature selection.
A large feature model was constructed, and
backward selection was carried out until no fur-
ther gain could be observed. The feature model of
MaltParser consists of a number of feature types,
each describing a starting point, a path through the
structure so far, and a column of the node arrived
at. The number of feature types was reduced from
37 to 35 based on the labelled F
1
-score.
As parsing is done at the same time as argu-
ment labelling, different syntactic structures risk
being assigned to the same sentence, depending
on which predicate is currently processed. This
means that several, possibly different, parses have
to be combined into one. In this experiment, the
head and the dependency label were concatenated,
and the most frequent one was used. In case of
a tie, the first one to appear was used. The like-
lihood of the chosen labelling was also used as a
confidence measure for the syntactic blender.
3.5 Blending and Post-Processing
Combining the output from several different sys-
tems has been shown to be beneficial (Koomen
et al, 2005). For the final submission, we com-
bined the output of two variants of the pipelined
SRL system, each using different data splits, with
3
The version of the joint system used in the submission
was based on an early predicate prediction. More accurate
predicates would give a major improvement for the results.
250
Test set Pred PoS Labelled F
1
Unlabelled F
1
WSJ All 82.90 90.90
NN* 81.12 86.39
VB* 85.52 96.49
Brown All 67.48 85.49
NN* 58.34 75.35
VB* 73.24 91.97
Table 2: Semantic predicate results on the test sets.
the SRL output of the joint system. A simple uni-
form weight majority vote heuristic was used, with
no combinatorial constraints on the selected argu-
ments. For each sentence, all predicates that were
identified by a majority of the systems were se-
lected. Then, for each selected predicate, its ar-
guments were picked by majority vote (ignoring
the systems not voting for the predicate). The best
single SRL system achieved a labelled F
1
-score
of 71.34 on the WSJ test set and 57.73 on the
Brown test set, compared to 74.47 and 60.18 for
the blended system.
As a final step, we filtered out all verbal and
nominal predicates not in PropBank or NomBank,
respectively, based on the predicted PoS-tag and
lemma. Each lexicon was expanded with lemmas
from the training set, due to predicted lemma er-
rors in the training data. This turned out to be a
successful strategy for the individual systems, but
slightly detrimental for the blended system.
3.6 Results
Semantic predicate results for WSJ and Brown can
be found in Table 2. Table 4 shows the results for
identification and classification of arguments.
4 Analysis and Conclusions
In general, the mixed and blended system performs
well on all tasks, rendering a sixth place in the
CoNLL 2008 shared task. The overall scores for
the submitted system can be seen in Table 3.
4.1 Parsing
For the blended parsing system, the labelled at-
tachment score drops from 87.36 for the WSJ test
set to 80.77 for the Brown test set, while the unla-
belled attachment score only drops from 89.88 to
86.28. This shows that the system is robust with
regards to the overall syntactic structure, even if
picking the correct label is more difficult for the
out-of-domain text.
The parser has difficulties finding the right head
for punctuation and symbols. Apart from errors re-
WSJ + Brown WSJ Brown
Syn + Sem 79.79 80.92 70.49
Syn 86.63 87.36 80.77
Sem 72.94 74.47 60.18
Table 3: Syntactic and semantic scores on the test
sets for the submitted system. The scores, from top
to bottom, are labelled macro F
1
, labelled attach-
ment score and labelled F
1
.
garding punctuation, most errors occur for IN and
TO. A majority of these problems are related to as-
signing the correct dependency. This is not surpris-
ing, since these are categories that focus on form
rather than function.
There is no significant difference in score for left
and right dependencies, presumably because of the
bi-directional parsing. However, the system over-
predicts dependencies to the root. This is mainly
due to the way MaltParser handles tokens not be-
ing attached anywhere during parsing. These to-
kens are by default assigned to the root.
4.2 SRL
Similarly to the parsing results, the blended SRL
system is less robust with respect to labelled F
1
-
score, dropping from 74.47 on the WSJ test set to
60.18 on the Brown test set. The corresponding
drop in unlabelled F
1
-score is from 82.90 to 75.49.
The simple method of picking the most com-
mon sense from the training data works quite well,
but the difference in domain makes it more diffi-
cult to find the correct sense for the Brown corpus.
In the future, a predicate classification module is
needed. For the WSJ corpus, assigning the most
common predicate sense works better with nomi-
nal than with verbal predicates, while verbal pred-
icates are handled better for the Brown corpus.
In general, verbal predicate-argument structures
are handled better than nominal ones, for both
test sets. This is not surprising, since nominal
predicate-argument structures tend to vary more in
their composition.
Since we do not use global constraints for the
argument labelling (looking at the whole argument
structure for each predicate), the system can out-
put the same argument label for a predicate several
times. For the WSJ test set, for instance, the ra-
tio of repeated argument labels is 5.4% in the sys-
tem output, compared to 0.3% in the gold standard.
However, since there are no confidence scores for
predictions it is difficult to handle this in the cur-
rent system.
251
PPOSS(pred) + ARG WSJ F
1
Brown F
1
NN* + A0 61.42 38.99
NN* + A1 67.07 53.10
NN* + A2 57.02 26.19
NN* + A3 63.08 (16.67)
NN* + AM-ADV 4.65 (-)
NN* + AM-EXT 44.78 (40.00)
NN* + AM-LOC 49.45 (-)
NN* + AM-MNR 53.51 21.82
NN* + AM-NEG 79.37 (46.15)
NN* + AM-TMP 67.23 (25.00)
VB* + A0 81.72 73.58
VB* + A1 81.77 67.99
VB* + A2 60.91 50.67
VB* + A3 61.49 (14.28)
VB* + A4 77.84 (40.00)
VB* + AM-ADV 47.49 30.33
VB* + AM-CAU 55.12 (35.29)
VB* + AM-DIR 41.86 37.14
VB* + AM-DIS 71.91 37.04
VB* + AM-EXT 60.38 (-)
VB* + AM-LOC 55.69 37.50
VB* + AM-MNR 49.54 36.25
VB* + AM-MOD 94.85 82.42
VB* + AM-NEG 93.45 77.08
VB* + AM-PNC 50.00 (62.50)
VB* + AM-TMP 69.59 49.07
VB* + C-A1 70.76 55.32
VB* + R-A0 83.68 70.83
VB* + R-A1 68.87 51.43
VB* + R-AM-LOC 38.46 (25.00)
VB* + R-AM-TMP 56.82 (58.82)
Table 4: Semantic argument results on the two
test sets, showing arguments with more than 20
instances in the gold test set (fewer instances for
Brown are given in parentheses).
Acknowledgements
This project was carried out within the course Ma-
chine Learning 2, organized by GSLT (Swedish
National Graduate School of Language Tech-
nology), with additional support from NGSLT
(Nordic Graduate School of Language Technol-
ogy). We thank our supervisors Joakim Nivre,
Bj?orn Gamb?ack and Pierre Nugues for advice and
support. Computations were performed on the
BalticGrid and UPPMAX (projects p2005008 and
p2005028) resources. We thank Tore Sundqvist at
UPPMAX for technical assistance.
References
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
A library for support vector machines.
Chu, Y. J. and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual Association for Computing Machinery
Southeast Conference, Athens, Georgia.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Gablonsky, J?org M. 2001. Modifications of the DI-
RECT algorithm. Ph.D. thesis, North Carolina State
University, Raleigh, North Carolina.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Hall, Johan, Jens Nilsson, Joakim Nivre, G?uls?en
Eryi?git, Be?ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, Prague, Czech Republic.
Koomen, Peter, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), Ann Arbor,
Michigan.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, Prague, Czech Republic.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
2(13):95?135.
Nivre, Joakim. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, Barcelona, Spain.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), Manchester, Great
Britain.
Witten, Ian H. and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, Amsterdam, 2nd edition.
252
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 38?46,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Consistency Checking for Treebank Alignment
Markus Dickinson
Indiana University
md7@indiana.edu
Yvonne Samuelsson
Stockholm University
yvonne.samuelsson@ling.su.se
Abstract
This paper explores ways to detect errors
in aligned corpora, using very little tech-
nology. In the first method, applicable
to any aligned corpus, we consider align-
ment as a string-to-string mapping. Treat-
ing the target string as a label, we ex-
amine each source string to find incon-
sistencies in alignment. Despite setting
up the problem on a par with grammat-
ical annotation, we demonstrate crucial
differences in sorting errors from legiti-
mate variations. The second method ex-
amines phrase nodes which are predicted
to be aligned, based on the alignment of
their yields. Both methods are effective in
complementary ways.
1 Introduction
Parallel corpora?texts and their translations?
have become essential in the development of
machine translation (MT) systems. Alignment
quality is crucial to these corpora; as Tiede-
mann (2003) states, ?[t]he most important fea-
ture of texts and their translations is the corre-
spondence between source and target segments?
(p. 2). While being useful for translation studies
and foreign language pedagogy (see, e.g., Botley
et al, 2000; McEnery and Wilson, 1996), PARAL-
LEL TREEBANKS?syntactically-annotated paral-
lel corpora?offer additional useful information
for machine translation, cross-language infor-
mation retrieval, and word-sense disambiguation
(see, e.g., Tiedemann, 2003),
While high-quality alignments are desirable,
even gold standard annotation can contain anno-
tation errors. For other forms of linguistic an-
notation, the presence of errors has been shown
to create various problems, from unreliable train-
ing and evaluation of NLP technology (e.g., Padro
and Marquez, 1998) to low precision and recall
of queries for already rare linguistic phenomena
(e.g., Meurers and Mu?ller, 2008). Even a small
number of errors can have a significant impact
on the uses of linguistic annotation, e.g., chang-
ing the assessment of parsers (e.g., Habash et al,
2007). One could remove potentially unfavorable
sentence pairs when training a statistical MT sys-
tem, to avoid incorrect word alignments (Okita,
2009), but this removes all relevant data from
those sentences and does not help evaluation.
We thus focus on detecting errors in the anno-
tation of alignments. Annotation error detection
has been explored for part-of-speech (POS) anno-
tation (e.g., Loftsson, 2009) and syntactic anno-
tation (e.g., Ule and Simov, 2004; Dickinson and
Meurers, 2005), but there have been few, if any, at-
tempts to develop general approaches to error de-
tection for aligned corpora. Alignments are differ-
ent in nature, as the annotation does not introduce
abstract categories such as POS, but relies upon
defining translation units with equivalent mean-
ings.
We use the idea that variation in annotation can
indicate errors (section 2), for consistency check-
ing of alignments, as detailed in section 3. In sec-
tion 4, we outline language-independent heuristics
to sort true ambiguities from errors, and evaluate
them on a parallel treebank in section 5. In sec-
tion 6 we turn to a complementary method, ex-
ploiting compositional properties of aligned tree-
banks, to align more nodes. The methods are sim-
ple, effective, and applicable to any aligned tree-
bank. As far as we know, this is the first attempt to
thoroughly investigate and empirically verify er-
ror detection methods for aligned corpora.
38
2 Background
2.1 Variation N -gram Method
As a starting point for an error detection method
for aligned corpora, we use the variation n-gram
approach for syntactic annotation (Dickinson and
Meurers, 2003, 2005). The approach is based on
detecting strings which occur multiple times in
the corpus with varying annotation, the so-called
VARIATION NUCLEI. The nucleus with repeated
surrounding context is referred to as a VARIATION
n-GRAM. The basic heuristic for detecting anno-
tation errors requires one word of recurring con-
text on each side of the nucleus, which is suffi-
cient for detecting errors in grammatical annota-
tion with high precision (Dickinson, 2008).
The approach detects bracketing and labeling
errors in constituency annotation. For example,
the variation nucleus last month occurs once in
the Penn Treebank (Taylor et al, 2003) with the
label NP and once as a non-constituent, handled
through a special label NIL. As a labeling error
example, next Tuesday occurs three times, twice
as NP and once as PP (Dickinson and Meur-
ers, 2003). The method works for discontinuous
constituency annotation (Dickinson and Meurers,
2005), allowing one to apply it to alignments,
which may span over several words.
2.2 Parallel Treebank Consistency Checking
For the experiments in this paper we will use
the SMULTRON parallel treebank of Swedish,
German, and English (Gustafson-C?apkova? et al,
2007), containing syntactic annotation and align-
ment on both word and phrase levels.1 Addition-
ally, alignments are marked as showing either an
EXACT or a FUZZY (approximate) equivalence.
Corpora with alignments often have under-
gone some error-checking. Previous consistency
checks for SMULTRON, for example, consisted
of running one script for comparing differences
in length between the source and target language
items, and one script for comparing alignment
labels, to detect variation between EXACT and
FUZZY links. For example, the pair and (English)
and samt (German, ?together with?) had 20 FUZZY
matches and 1 (erroneous) EXACT match. Such
1SMULTRON is freely available for research purposes, see
http://www.cl.uzh.ch/kitt/smultron/.
methods are limited, in that they do not, e.g., han-
dle missing alignments.
The TreeAligner2 tool for annotating and
querying aligned parallel treebanks (Volk et al,
2007) employs its own consistency checking, re-
cently developed by Torsten Marek. One method
uses 2 ? 2 contingency tables over words, look-
ing, e.g., at the word-word or POS-POS combina-
tions, pinpointing anomalous translation equiva-
lents. While potentially effective, this does not ad-
dress the use of alignments in context, i.e., when
we might expect to see a rare translation.
A second, more treebank-specific method
checks for so-called branch link locality: if two
nodes are aligned, any node dominating one of
them can only be aligned to a node dominating the
other one. While this constraint can flag erroneous
links, it too does not address missing alignments.
The two methods we propose in this paper address
these limitations and can be used to complement
this work. Furthermore, these methods have not
been evaluated, whereas we evaluate our methods.
3 Consistency of Alignment
To adapt the variation n-gram method and deter-
mine whether strings in a corpus are consistently
aligned, we must: 1) define the units of data we
expect to be consistently annotated (this section),
and 2) define which information effectively iden-
tifies the erroneous cases (section 4).
3.1 Units of Data
Alignment relates words in a source language and
words in a target language, potentially mediated
by phrase nodes. Following the variation n-gram
method, we define the units of data, i.e., the vari-
ation nuclei, as strings. Then, we break the prob-
lem into two different source-to-target mappings,
mapping a source variation nucleus to a target lan-
guage label. With a German-English aligned cor-
pus, for example, we look for the consistency of
aligning German words to their English counter-
parts and separately examine the consistency of
aligning English words with their German ?la-
bels.? Because a translated word can be used in
different parts of a sentence, we also normalize all
target labels into lower-case, preventing variation
between, e.g., the and The.
2http://www.cl.uzh.ch/kitt/treealigner
39
.$.bl?htenVVFINInAPPR einigenPIAT G?rtenNN unterAPPR denART Obstb?umenNN Kr?nzeNNdichteADJA vonAPPR OsterglockenNN
..theDT fruitNN treesNNSInIN someDT ofIN theDT gardensNNS wereVBD encircledVBN ---NONE- withIN denseJJ clustersNNS ofIN daffodilsNNS
*
HD
HD
NK HDNP
NKPP
MO
HD
NK HDNP
NKPP
MO
HD
HDAP
NK
HD
HDNP
NKPP
MNRNP
SBS
VROOT
NP
SBJ
NP
PP
NP
PP
LOC
NP NP NP
PP
NP
PPCLRVP
VP
S
VROOT
.$.bl?hteennVFI NAbPRRi PhegTGeII ?trPiu dGtONbbrIIsArthberPmKPhtl?eRuzIcd rthONAPmnmNbPiu DJ?l?rhII tOPRRiPiu votrkrAII
..u?rmu ktGAII thFI e?rmu OtGGTGII ?t?nfm hTeif GrNlenf gte?FI Nbif OSl?if NbFI Nmu egtel?II
am
am
amNP
IzPP
Dw
Iz am
amAP
Iz
Id amASP
Dw NP
wP
Iz am
am
amNP
IzPP
DIiNP
vf V
SROOT
NP NP
PPBw-NP
vfK
A	SP NP
PP
PP-BiSP
SP
V
SROOT
Figure 1: Word and phrase alignments span the
same string on the left, but not on the right.
Although alignment maps strings to strings for
this method, complications arise when mediated
by phrase nodes: if a phrase node spans over only
one word, it could have two distinct mappings,
one as a word and one as a phrase, which may
or may not result in the same yield. Figure 1 il-
lustrates this. On the left side, Osterglocken is
aligned to daffodils at the word level, and the same
string is aligned on the phrase level (NP to NP).
In contrast, on the right side, the word Spiegel is
aligned to the word mirror, while at the phrase
level, Spiegel (NP) is aligned to the mirror (NP).
As word and phrase level strings can behave dif-
ferently, we split error detection into word-level
and phrase-level methods, to avoid unnecessary
variation. By splitting the problem first into differ-
ent source-to-target mappings and then into words
and phrases, we do not have to change the under-
lying way of finding consistency.
Multiple Alignment The mapping between
source strings and target labels handles n-to-m
alignments. For example, if Ga?rten maps to the
gardens, the and gardens is considered one string.
Likewise, in the opposite direction, the gardens
maps as a unit to Ga?rten, even if discontinuous.
Unary Branches With syntactic annotation,
unary branches present a potential difficulty, in
that a single string could have more than one la-
bel, violating the assumption that the string-to-
.
$.
b
$l
l
$l
?htene
VVFIN
APPnR
NN
ign
TG?
TRerude
NN
ihRR
TOV
snRR
mKzc
DdhJnR
VVFIN
rgd
vvAG
onkhRinR
vIc
gR
TvvG
ngRnk
TG?
fhRi
NN
kge
TvvG
StRJndPRue
NN
.. lleanO? hRPrndNN gPVwB DuuiNNIDIN rnvGv hP-Vwv PuknuRnNN ?gEgRJVw* uRIN eanO? niJnNN uDIN PehdEheguRNN
SO
SO
NP
vO
Nm SO
NP
cw
vS
Hv SO
SO
NP
cw
SO
NP
KT
SO
Nm SO
SO
SO
NPNm
PPMNG
NPNm
PPMK
AGA
SVPMK
A
VROOT
NP
cwL
NP
vGOVP
NP
cwL
NP
cwL
NP NP
PP
NP
PP
HfGVP
A
VP
A
A	SR
TOV A
VROOT
.
$b
l?ht
enVFI
NAt
PPRi
gTGG
n?e
Trud
n?e
tAGt
niO
TGgtht
PFs
lmrKgz? .bNdtPiP cdtGiD dTJteD vttGeDI NmotmGtII tKNtiD
k?
k?
NPsD
k?
ASPzf
k?
ASPzf
IS IS
NPP?
V
SROOT
NP
sDa
A	SP A	SP
NP
Pi?SP
SP
V

SROOT
Figure 2: The word someone aligned as a phrase
on the left, but not a phrase by itself on the right.
label mapping is a function. For example, in
Penn Treebank-style annotation, an NP node can
dominate a QP (quantifier phrase) node via a
unary branch. Thus, an annotator could (likely
erroneously) assign different alignments to each
phrasal node, one for the NP and one for the QP,
resulting in different target labels.
We handle all the (source) unary branch align-
ments as a conjunction of possibilities, ordered
from top to bottom. Just as the syntactic struc-
ture can be relabeled as NP/QP (Dickinson and
Meurers, 2003), we can relabel a string as, e.g.,
the man/man. If different unary nodes result in the
same string (the man/the man), we combine them
(the man). Note that unary branches are unprob-
lematic in the target language since they always
yield the same string, i.e., are still one label.
3.2 Consistency and Completeness
Error detection for syntactic annotation finds in-
consistencies in constituent labeling (e.g., NP vs.
QP) and inconsistencies in bracketing (e.g., NP vs.
NIL). Likewise, we can distinguish inconsistency
in labeling (different translations) from inconsis-
tency in alignment (aligned/unaligned). Detecting
inconsistency in alignment deals with the com-
pleteness of the annotation, by using the label NIL
for unaligned strings.
We use the method from Dickinson and Meur-
ers (2005) to generate NILs, but using NIL for un-
aligned strings is too coarse-grained for phrase-
level alignment. A string mapping to NIL might
be a phrase which has no alignment, or it might
40
not be a phrase and thus could not possibly have
an alignment. Thus, we create NIL-C as a new
label, indicating a constituent with no alignment,
differing from NIL strings which do not even form
a phrase. For example, on the left side of Fig-
ure 2, the string someone aligns to jemanden on
the phrase level. On the right side of Figure 2,
the string someone by itself does not constitute a
phrase (even though the alignment in this instance
is correct) and is labeled NIL. If there were in-
stances of someone as an NP with no alignment,
this would be NIL-C. NIL-C cases seem to be use-
ful for inconsistency detection, as we expect con-
sistency for items annotated as a phrase.
3.3 Alignment Types
Aligned corpora often specify additional informa-
tion about each alignment, e.g., a ?sure? or ?pos-
sible? alignment (Och and Ney, 2003). In SMUL-
TRON, for instance, an EXACT alignment means
that the strings are considered direct translation
equivalents outside the current sentence context,
whereas a FUZZY one is not as strict an equiva-
lent. For example, something in English EXACT-
aligns with etwas in German. However, if some-
thing and irgend etwas (?something or other?) are
constituents on the phrase level, <something, ir-
gend etwas> is an acceptable alignment (since the
corpus aligns as much as possible), but is FUZZY.
Since EXACT alignments are the ones we expect
to consistently align with the same string across
the corpus, we attach information about the align-
ment type to each corpus position. This can be
used to filter out variations involving, e.g., FUZZY
alignments (see section 4.4). When multiple
alignments form a single variation nucleus, there
could be different types of alignment for each link,
e.g., dog EXACT-aligning and the FUZZY-aligning
with Hund. We did not observe this, but one can
easily allow for a mixed type (EXACT-FUZZY).
3.4 Algorithm
The algorithm first splits the data into appropriate
units (SL=source language, TL=target language):
1. Divide the alignments into two SL-to-TL mappings.
2. Divide each SL-to-TL alignment set into word-level
and phrase-level alignments.
For each of the four sets of alignments:
1. Map each string in SL with an alignment to a label
? Label = <(lower-cased) TL translation, EX-
ACT|FUZZY|EXACT-FUZZY>
? (For phrases) Constituent phrases with no align-
ment are given the special label, NIL-C.
? (For phrases) Constituent phrases which are
unary branches are given a single, normalized la-
bel representing all target strings.
2. Generate NIL alignments for string tokens which occur
in SL, but have no alignment to TL, using the method
described in Dickinson and Meurers (2005).
3. Find SL strings which have variation in labeling.
4. Filter the variations from step 3, based on likelihood of
being an error (see section 4).
4 Identifying Inconsistent Alignments
As words and phrases have acceptable variants for
translation, the method in section 3 will lead to
detecting acceptable variations. We use several
heuristics to filter the set of variations.
4.1 NIL-only Variation
As discussed in section 3.2, we use the label NIL-
C to refer to syntactic constituents which do not
receive an alignment, while NIL refers to non-
constituent strings without an alignment. A string
which varies between NIL and NIL-C, then, is not
really varying in its alignment?i.e., it is always
unaligned. We thus remove cases varying only be-
tween NIL and NIL-C.
4.2 Context-based Filtering
The variation n-gram method has generally relied
upon immediate lexical context around the vari-
ation nucleus, in order to sort errors from ambi-
guities (Dickinson, 2008). However, while use-
ful for grammatical annotation, it is not clear how
useful the surrounding context is for translation
tasks, given the wide range of possible translations
for the same context. Further, requiring identical
context around source words is very strict, leading
to sparse data problems, and it ignores alignment-
specific information (see sections 4.3 and 4.4).
We test three different notions of context.
Matching the variation n-gram method, we first
employ a filter identifying those nuclei which
share the ?shortest? identical context, i.e., one
word of context on every side of a nucleus. Sec-
ondly, we relax this to require only one word of
41
context, on either the left or right side. Finally, we
require no identical context in the source language
and rely only on other filters. For example, with
the nucleus come in the context Where does the
world come from, the first notion requires world
come from to recur, the second either world come
or come from, and the third only requires that the
nucleus itself recur (come).
4.3 Target Language Filtering
Because translation is open-ended, there can be
different translations in a corpus. We want to
filter out cases where there is variation in align-
ment stemming from multiple translation possibil-
ities. We implement a TARGET LANGUAGE FIL-
TER, which keeps only the variations where the
target words are present in the same sentence. If
word x is sometimes aligned to y1 and sometimes
to y2 , and word y2 occurs in at least one sentence
where y1 is the chosen target, then we keep the
variation. If y1 and y2 do not occur in any of the
same sentences, we remove the variation: given
the translations, there is no possibility of having
the same alignment.
This also works for NIL labels, given sentence
alignments.3 For NILs, the check is in only one
direction: the aligned sentence must contain the
target string used as the label elsewhere in the cor-
pus. For instance, the word All aligns once with
alle and twice with NIL. We check the two NIL
cases to see whether one of them contains alle.
Sentences which are completely unaligned lead
to NILs for every word and phrase, and we always
keep the variation. In practice, the issue of having
no alignment should be handled separately.
4.4 Alignment Type Filtering
A final filter relies on alignment type informa-
tion. Namely, the FUZZY label already indicates
that the alignment is not perfect, i.e., not nec-
essarily applicable in other contexts. For exam-
ple, the English word dead FUZZY-aligns with the
German verschwunden (?gone, missing?), the best
translation in its context. In another part of the
corpus, dead EXACT-aligns with leblosen (?life-
less?). While this is variation between verschwun-
den and leblosen, the presence of the FUZZY label
3In SMULTRON, sentence alignments are not given di-
rectly, but can be deduced from the set of word alignments.
word phrase
all 540 251
oneword 340 182
shortest 96 21
all-TL 194 140
oneword-TL 130 94
shortest-TL 30 16
Table 1: Number of variations across contexts
alerts us to the fact that it should vary with another
word. The ALIGNMENT TYPE FILTER removes
cases varying between one EXACT label and one
or more FUZZY labels.
5 Evaluation
Evaluation was done for English to German on
half of SMULTRON (the part taken from the novel
Sophie?s World), with approximately 7500 words
from each language and 7600 alignments (roughly
4800 word-level and 2800 phrase-level). Basic
statistics are in Table 1. We filter based on the
target language (TL) and provide three different
contextual definitions: no context, i.e., all varia-
tions (all); one word of context on the left or right
(oneword); and one word of context on the left and
right, i.e., the shortest surrounding context (short-
est). The filters reduce the number of variations,
with a dramatic loss for the shortest contexts.
A main question concerns the impact of the fil-
tering conditions on error detection. To gauge this,
we randomly selected 50 (all) variations for the
word level and 50 for the phrase level, each corre-
sponding to just under 400 corpus instances. The
variations were checked manually to see which
were true variations and which were errors.
We report the effect of different filters on preci-
sion and recall in Table 2, where recall is with re-
spect to the all condition.4 Adding too much lexi-
cal context in the source language (i.e., the short-
est conditions) results in too low a recall to be
practically effective. Using one word of context
on either side has higher recall, but the precision
is no better than using no source language con-
text at all. What seems to be most effective is to
only use the target language filter (all-TL). Here,
we find higher precision?higher than any source
language filter?and the recall is respectable.
4Future work should test for recall of all alignment errors,
by first manually checking a small section of the corpus.
42
Word Phrase
Cases Errors P R Cases Errors P R
all 50 17 34% 100% 50 15 30% 100%
oneword 33 12 36% 71% 33 8 24% 53%
shortest 8 2 25% 12% 4 1 25% 7%
all-TL 20 11 55% 65% 27 12 44% 80%
oneword-TL 15 6 40% 35% 14 7 50% 47%
shortest-TL 2 1 50% 6% 3 1 33% 7%
Table 2: Error precision and recall
TL filter An advantage of the target language
filter is its ability to handle lexical (e.g., case) vari-
ations. One example of this is the English phrase
a dog, which varies between German einem Hund
(dative singular), einen Hund (accusative singu-
lar) and Hunde (accusative plural). Similar to us-
ing lower-case labels, one could map strings to
canonical forms. However, the target language
filter naturally eliminates such unwanted varia-
tion, without any language-specific information,
because the other forms do not appear across sen-
tences.
Several of the variations which the target lan-
guage filter incorrectly removes would, once the
error is fixed, still have variation. As an example,
consider cat, which varies between Katze (5 to-
kens) and NIL (2 tokens). In one of the NIL cases,
the word needs to be FUZZY-aligned with the Ger-
man Tigerkatze. The variation points out the error,
but there would still be variation (between Katze,
Tigerkatze, and NIL) after correction. This shows
the limitation of the heuristic in identifying the re-
quired non-exact alignments.
Another case the filter misses is the variation
nucleus heard, which varies between geho?rt (2 to-
kens) and ho?ren (1 token). In this case, one of the
instances of <heard, geho?rt> should be <heard,
geho?rt hatte>. Note that here the erroneous case
is not variation-based at all; it is a problem with
the label geho?rt. What is needed is a method to
detect more translation possibilities.
As an example of a problem for phrases, con-
sider the variation for the nucleus end with 5 in-
stances of NIL and 1 of ein Ende. In one NIL
instance, the proper alignment should be <the
end, Ende>, with a longer source string. Since
the target label is Ende and not ein Ende, the fil-
ter removes this variation. One might explore
more fuzzily matching NIL strings, so that Ende
matches with ein Ende. We explore a different
method for phrases next, which deals with some
of these NIL cases.
6 A Complementary Method
Although it works for any type of aligned corpus,
the string-based variation method of detecting er-
rors is limited in the types of errors it can de-
tect. There might be ways to generalize the vari-
ation n-gram method (cf. Dickinson, 2008), but
this does not exploit properties inherent to aligned
treebanks. We pursue a complementary approach,
as this can fill in some gaps a string-based method
cannot deal with (cf. Loftsson, 2009).
6.1 Phrase Alignment Based on Word Links
Using the existing word alignments, we can search
for missing or erroneous phrase alignments. If
the words dominated by a phrase are aligned, the
phrases generally should be, too (cf. Lavie et al,
2008). We take the yield of a constituent in one
side of a corpus, find the word alignments of this
yield, and use these alignments to predict a phrasal
alignment for the constituent. If the predicted
alignment is not annotated, it is flagged as a possi-
ble error. This is similar to the branch link locality
of the TreeAligner (see section 2.2), but here as a
prediction, rather than a restriction, of alignment.
For example, consider the English VP choose
her own friends in (1). Most of the words are
aligned to words within Ihre Freunde vielleicht
wa?hlen (?possibly choose her friends?), with no
alignment to words outside of this German VP. We
want to predict that the phrases be aligned.
(1) a. [VP choose1 her2 own friends3 ]
b. [VP Ihre2 Freunde3 vielleicht wa?hlen1 ]
The algorithm works as follows:
1. For every phrasal node s in the source treebank:
(a) Predict a target phrase node t to align with,
where t could be non-alignment (NIL):
43
i. Obtain the yield (i.e., child nodes) of the
phrase node s: s1 , ... sn .
ii. Obtain the alignments for each child node
si , resulting in a set of child nodes in the
target language (t1 , ... tm ).
iii. Store every mother node t? covering all the
target child nodes, i.e., all <s, t?> pairs.
(b) If a predicted alignment (<s, t?>) is not in the
set of actual alignments (<s, t>), add it to the
set of potential alignments, AS 7?T .
i. For nodes which are predicted to have non-
alignment (but are actually aligned), output
them to a separate file.
2. Perform step 1 with the source and target reversed,
thereby generating both AS 7?T and AT 7?S .
3. Intersect AS 7?T and AT 7?S , to obtain the set of pre-
dicted phrasal alignments not currently aligned.
The main idea in 1a is to find the children of a
source node and their alignments and then obtain
the target nodes which have all of these aligned
nodes as children. A node covering all these target
children is a plausible candidate for alignment.
Consider example (2). Within the 8-word En-
glish ADVP (almost twice . . . ), there are six words
which align to words in the corresponding Ger-
man sentence, all under the same NP.5 It does not
matter that some words are unaligned; the fact
that the English ADVP and the German NP cover
basically the same set of words suggests that the
phrases should be aligned, as is the case here.
(2) a. Sophie lived on2 [NP1 the2 outskirts3 of a4
sprawling5? suburb6?] and had [ADVP almost7
twice8 as9 far10 to school as11 Joanna12?] .
b. Sophie wohnte am2 [NP1 Ende3 eines4
ausgedehnten5? Viertels6? mit Einfam-
ilienha?usern] und hatte [NP einen fast7
doppelt8 so9 langen10 Schulweg wie11
Jorunn12?] .
The prediction of an aligned node in 1a allows
for multiple possibilities: in 1aiii, we only check
that a mother node t? covers all the target children,
disregarding extra children, since translations can
contain extra words. In general, many such dom-
inating nodes exist, and most are poor candidates
for alignment of the node in question. This is the
reason for the bidirectional check in steps 2 and 3.
For example, in (3), we correctly predict align-
ment between the NP dominating you in English
and the NP dominating man in German. From
the word alignment, we generate a list of mother
5FUZZY labels are marked by an asterisk, but are not used.
nodes of man as potential alignments for the you
NP. Two of these (six) nodes are shown in (3b).
In the other direction, there are eight nodes con-
taining you; two are shown in (3a). These are the
predicted alignment nodes for the NP dominating
man. In either direction, this overgenerates; the
intersection, however, only contains alignment be-
tween the lowest NPs.
(3) a. But it ?s just as impossible to realize [S [NP
you1 ] have to die without thinking how incred-
ibly amazing it is to be alive ] .
b. [S Und es ist genauso unmo?glich , daru?ber
nachzudenken , dass [NP man1 ] sterben muss
, ohne zugleich daran zu denken , wie phan-
tastisch das Leben ist . ]
While generally effective, certain predictions
are less likely to be errors. In figure 3, for ex-
ample, the sentence pair is an entire rephrasing;
<her, ihr> is the only word alignment. For each
phrasal node in the SL, the method only requires
that all its words be aligned with the words under
the TL node. Thus, the English PP on her, the VP
had just been dumped on her, and the two VPs in
between are predicted as possible alignments with
the German VP ihr einfach in die Wiege gelegt
worden or its immediate VP daughter: they all
have her and ihr aligned, and no contradicting
alignments. Sparse word alignments lead to mul-
tiple possible phrase alignments. After intersect-
ing, we mark cases with more than one predicted
source or target phrase and do not evaluate them.
If in step 1aiii, no target mother (t?) exists, but
there is alignment in the corpus, then in step 1bi,
we output predicted non-alignment. In Example
(2), for instance, the English NP the outskirts of
a sprawling suburb is (incorrectly) predicted to
have no alignment, although most words align to
words within the same German NP. This predic-
tion arises because the aligns to a word (am) out-
side of the German NP, due to am being a contrac-
tion of the preposition an and the article dem, (cf.
on and the, respectively). The method for predict-
ing phrase alignments, however, relies upon words
being within the constituent. We thus conclude
that: 1) the cases in step 1bi are unlikely to be er-
rors, and 2) there are types of alignments which
we simply will not find, a problem also for au-
tomatic alignment based on similar assumptions
(e.g., Zhechev and Way, 2008). In (2), for in-
stance, were there not already alignment between
44
.$.bl?htenVFlINFA bP?RightNNTiGiT?hhNNru?NNdO irgslmutFh rgtNNO RritOK zriTiVV
..ci?NON$ GPPDIVVA ulRhJF voI?OJ kiighJV RofSiRhJV aaaVwVda PgnV ui?NON
B
cF
cFNP
AJ
cF
cF
cFNP
Ft
cFASP
-w
cF
VE cFNP
VEPP
-wSP
w* SP
w*V
NP
AJH
ARSP NP NP
PP*MOSP
SP
SP
V
Figure 3: A sentence with minimal alignment
the NPs, we would not predict it.
6.2 Evaluation
The method returns 318 cases, in addition to 135
cases with multiple source/target phrases and 104
predicted non-alignments. To evaluate, we sam-
pled 55 of the 318 flagged phrases and found that
25 should have been aligned as suggested. 21
of the phrases have zero difference in length be-
tween source and target, while 34 have differences
of up to 9 tokens. Of the phrases with zero-
length difference, 18 should have been aligned
(precision=85.7%), while only 7 with length dif-
ferences should have been aligned. This is in line
with previous findings that length difference can
help predict alignment (cf., e.g., Gale and Church,
1993). About half of all phrase pairs that should
be aligned should be EXACT, regardless of the
length difference.
The method is good at predicting the alignment
of one-word phrases, e.g., pronouns, as in (3). Of
the 11 suggested alignments where both source
and target have a length of 1, all were correct sug-
gestions. This is not surprising, since all words
under the phrases are (trivially) aligned. Although
shorter phrases with short length differences gen-
erally means a higher rate of correct suggestions,
we do not want to filter out items based on phrase
length, since there are outliers that are correct sug-
gestions, e.g., phrase pairs with lengths of 15 and
13 (difference=2) or 31 and 36 (difference=5). It
is worth noting that checking the suggestions took
very little time.
7 Summary and Outlook
This paper explores two simple, language-
independent ways to detect errors in aligned cor-
pora. In the first method, applicable to any aligned
corpus, we consider alignment as a string-to-string
mapping, where a string could be the yield of a
phrase. Treating the target string as a label, we
find inconsistencies in the labeling of each source
string. Despite setting the problem up in a similar
way to grammatical annotation, we also demon-
strated that new heuristics are needed to sort er-
rors. The second method examines phrase nodes
which are predicted to be aligned, based on the
alignment of their yields. Both methods are ef-
fective, in complementary ways, and can be used
to suggest alignments for annotators or to suggest
revisions for incorrect alignments.
The wide range of possible translations and the
linguistic information which goes into them indi-
cate that there should be other ways of finding er-
rors. One possibility is to use more abstract source
or target language representations, such as POS,
to overcome the limitations of string-based meth-
ods. This will likely also be a useful avenue to
explore for language pairs more dissimilar than
English and German. By investigating different
ways to ensure alignment consistency, one can be-
gin to provide insights into automatic alignment
(Zhechev and Way, 2008). Additionally, by cor-
recting the errors, one can determine the effect on
machine translation evaluation.
Acknowledgments
We would like to thank Martin Volk and Thorsten
Marek for useful discussion and feedback of ear-
lier versions of this paper and three anonymous
reviewers for their comments.
45
References
Botley, S. P., McEnery, A. M., and Wilson, A.,
editors (2000). Multilingual Corpora in Teach-
ing and Research. Rodopi, Amsterdam, Atlanta
GA.
Dickinson, M. (2008). Representations for cat-
egory disambiguation. In Proceedings of
COLING-08, pages 201?208, Manchester.
Dickinson, M. and Meurers, W. D. (2003). Detect-
ing inconsistencies in treebanks. In Proceed-
ings of TLT-03, pages 45?56, Va?xjo?, Sweden.
Dickinson, M. and Meurers, W. D. (2005). De-
tecting errors in discontinuous structural anno-
tation. In Proceedings of ACL-05, pages 322?
329.
Gale, W. A. and Church, K. W. (1993). A pro-
gram for aligning sentences in bilingual cor-
pora. Computational Linguistics, 19(1):75?
102.
Gustafson-C?apkova?, S., Samuelsson,
Y., and Volk, M. (2007). SMUL-
TRON (version 1.0) - The Stock-
holm MULtilingual parallel TReebank.
www.ling.su.se/dali/research/smultron/index.htm.
Habash, N., Gabbard, R., Rambow, O., Kulick, S.,
and Marcus, M. (2007). Determining case in
Arabic: Learning complex linguistic behavior
requires complex linguistic features. In Pro-
ceedings of EMNLP-CoNLL-07, pages 1084?
1092.
Lavie, A., Parlikar, A., and Ambati, V. (2008).
Syntax-driven learning of sub-sentential trans-
lation equivalents and translation rules from
parsed parallel corpora. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2),
pages 87?95, Columbus, OH.
Loftsson, H. (2009). Correcting a POS-tagged
corpus using three complementary methods.
In Proceedings of EACL-09, pages 523?531,
Athens, Greece.
McEnery, T. and Wilson, A. (1996). Corpus Lin-
guistics. Edinburgh University Press, Edin-
burgh.
Meurers, D. and Mu?ller, S. (2008). Corpora
and syntax (article 44). In Lu?deling, A. and
Kyto?, M., editors, Corpus Linguistics. An In-
ternational Handbook, Handbooks of Linguis-
tics and Communication Science. Mouton de
Gruyter, Berlin.
Och, F. J. and Ney, H. (2003). A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Okita, T. (2009). Data cleaning for word align-
ment. In Proceedings of the ACL-IJCNLP 2009
Student Research Workshop, pages 72?80, Sun-
tec, Singapore.
Padro, L. and Marquez, L. (1998). On the eval-
uation and comparison of taggers: the effect
of noise in testing corpora. In Proceedings of
ACL-COLING-98, pages 997?1002, San Fran-
cisco, California.
Taylor, A., Marcus, M., and Santorini, B. (2003).
The penn treebank: An overview. In Abeille?,
A., editor, Treebanks: Building and using syn-
tactically annotated corpora, chapter 1, pages
5?22. Kluwer, Dordrecht.
Tiedemann, J. (2003). Recycling Translations -
Extraction of Lexical Data from Parallel Cor-
pora and their Application in Natural Lan-
guage Processing. PhD thesis, Uppsala univer-
sity.
Ule, T. and Simov, K. (2004). Unexpected pro-
ductions may well be errors. In Proceedings of
LREC-04, Lisbon, Portugal.
Volk, M., Lundborg, J., and Mettler, M. (2007).
A search tool for parallel treebanks. In Pro-
ceedings of the Linguistic Annotation Workshop
(LAW) at ACL, pages 85?92, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Zhechev, V. and Way, A. (2008). Automatic gen-
eration of parallel treebanks. In Proceedings
of Coling 2008, pages 1105?1112, Manchester,
UK.
46
