Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 217?225,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning to Tell Tales: A Data-driven Approach to Story Generation
Neil McIntyre and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
n.d.mcintyre@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Computational story telling has sparked
great interest in artificial intelligence,
partly because of its relevance to educa-
tional and gaming applications. Tradition-
ally, story generators rely on a large repos-
itory of background knowledge contain-
ing information about the story plot and
its characters. This information is detailed
and usually hand crafted. In this paper we
propose a data-driven approach for gen-
erating short children?s stories that does
not require extensive manual involvement.
We create an end-to-end system that real-
izes the various components of the gen-
eration pipeline stochastically. Our system
follows a generate-and-and-rank approach
where the space of multiple candidate sto-
ries is pruned by considering whether they
are plausible, interesting, and coherent.
1 Introduction
Recent years have witnessed increased interest in
the use of interactive language technology in ed-
ucational and entertainment applications. Compu-
tational story telling could play a key role in these
applications by effectively engaging learners and
assisting them in creating a story. It could also al-
low teachers to generate stories on demand that
suit their classes? needs. And enhance the enter-
tainment value of role-playing games1. The major-
ity of these games come with a set of pre-specified
plots that the players must act out. Ideally, the plot
should adapt dynamically in response to the play-
ers? actions.
Computational story telling has a longstanding
tradition in the field of artificial intelligence. Early
work has been largely inspired by Propp?s (1968)
1A role-playing game (RPG) is a game in which the par-
ticipants assume the roles of fictional characters and act out
an adventure.
typology of narrative structure. Propp identified in
Russian fairy tales a small number of recurring
units (e.g., the hero is defeated, the villain causes
harm) and rules that could be used to describe their
relation (e.g., the hero is pursued and the rescued).
Story grammars (Thorndyke, 1977) were initially
used to capture Propp?s high-level plot elements
and character interactions. A large body of more
recent work views story generation as a form of
agent-based planning (Theune et al, 2003; Fass,
2002; Oinonen et al, 2006). The agents act as
characters with a list of goals. They form plans
of action and try to fulfill them. Interesting stories
emerge as agents? plans interact and cause failures
and possible replanning.
Perhaps the biggest challenge faced by compu-
tational story generators is the amount of world
knowledge required to create compelling stories.
A hypothetical system must have information
about the characters involved, how they inter-
act, what their goals are, and how they influence
their environment. Furthermore, all this informa-
tion must be complete and error-free if it is to be
used as input to a planning algorithm. Tradition-
ally, this knowledge is created by hand, and must
be recreated for different domains. Even the sim-
ple task of adding a new character requires a whole
new set of action descriptions and goals.
A second challenge concerns the generation
task itself and the creation of stories character-
ized by high-quality prose. Most story genera-
tion systems focus on generating plot outlines,
without considering the actual linguistic structures
found in the stories they are trying to mimic (but
see Callaway and Lester 2002 for a notable ex-
ception). In fact, there seems to be little com-
mon ground between story generation and natural
language generation (NLG), despite extensive re-
search in both fields. The NLG process (Reiter and
Dale, 2000) is often viewed as a pipeline consist-
ing of content planning (selecting and structuring
the story?s content), microplanning (sentence ag-
217
gregation, generation of referring expressions, lex-
ical choice), and surface realization (agreement,
verb-subject ordering). However, story generation
systems typically operate in two phases: (a) creat-
ing a plot for the story and (b) transforming it into
text (often by means of template-based NLG).
In this paper we address both challenges fac-
ing computational story telling. We propose a
data-driven approach to story generation that does
not require extensive manual involvement. Our
goal is to create stories automatically by leverag-
ing knowledge inherent in corpora. Stories within
the same genre (e.g., fairy tales, parables) typically
have similar structure, characters, events, and vo-
cabularies. It is precisely this type of information
we wish to extract and quantify. Of course, build-
ing a database of characters and their actions is
merely the first step towards creating an automatic
story generator. The latter must be able to select
which information to include in the story, in what
order to present it, how to convert it into English.
Recent work in natural language generation has
seen the development of learning methods for re-
alizing each of these tasks automatically with-
out much hand coding. For example, Duboue and
McKeown (2002) and Barzilay and Lapata (2005)
propose to learn a content planner from a paral-
lel corpus. Mellish et al (1998) advocate stochas-
tic search methods for document structuring. Stent
et al (2004) learn how to combine the syntactic
structure of elementary speech acts into one or
more sentences from a corpus of good and bad ex-
amples. And Knight and Hatzivassiloglou (1995)
use a language model for selecting a fluent sen-
tence among the vast number of surface realiza-
tions corresponding to a single semantic represen-
tation. Although successful on their own, these
methods have not been yet integrated together into
an end-to-end probabilistic system. Our work at-
tempts to do this for the story generation task,
while bridging the gap between story generators
and NLG systems.
Our generator operates over predicate-argument
and predicate-predicate co-occurrence statistics
gathered from corpora. These are used to pro-
duce a large set of candidate stories which are
subsequently ranked based on their interesting-
ness and coherence. The top-ranked candidate
is selected for presentation and verbalized us-
ing a language model interfaced with RealPro
(Lavoie and Rambow, 1997), a text generation
engine. This generate-and-rank architecture cir-
cumvents the complexity of traditional generation
This is a fat hen.
The hen has a nest in the box.
She has eggs in the nest.
A cat sees the nest, and can get the eggs.
The sun will soon set.
The cows are on their way to the barn.
One old cow has a bell on her neck.
She sees the dog, but she will not run.
The dog is kind to the cows.
Figure 1: Children?s stories from McGuffey?s
Eclectic Primer Reader; it contains primary read-
ing matter to be used in the first year of school
work.
systems, where numerous, often conflicting con-
straints, have to be encoded during development
in order to produce a single high-quality output.
As a proof of concept we initially focus on
children?s stories (see Figure 1 for an example).
These stories exhibit several recurrent patterns and
are thus amenable to a data-driven approach. Al-
though they have limited vocabulary and non-
elaborate syntax, they nevertheless present chal-
lenges at almost all stages of the generation pro-
cess. Also from a practical point of view, chil-
dren?s stories have great potential for educational
applications (Robertson and Good, 2003). For in-
stance, the system we describe could serve as an
assistant to a person who wants suggestions as to
what could happen next in a story. In the remain-
der of this paper, we first describe the components
of our story generator (Section 2) and explain how
these are interfaced with our story ranker (Sec-
tion 3). Next, we present the resources and evalu-
ation methodology used in our experiments (Sec-
tion 4) and discuss our results (Section 5).
2 The Story Generator
As common in previous work (e.g., Shim and Kim
2002), we assume that our generator operates in an
interactive context. Specifically, the user supplies
the topic of the story and its desired length. By
topic we mean the entities (or characters) around
which the story will revolve. These can be a list
of nouns such as dog and duck or a sentence, such
as the dog chases the duck. The generator next
constructs several possible stories involving these
entities by consulting a knowledge base containing
information about dogs and ducks (e.g., dogs bark,
ducks swim) and their interactions (e.g., dogs
chase ducks, ducks love dogs). We conceptualize
218
the dog chases the duck
the dog barks the duck runs away
the dog catches the duck the duck escapes
Figure 2: Example of a simplified story tree.
the story generation process as a tree (see Figure 2)
whose levels represent different story lengths. For
example, a tree of depth 3 will only generate sto-
ries with three sentences. The tree encodes many
stories efficiently, the nodes correspond to differ-
ent sentences and there is no sibling order (the
tree in Figure 2 can generate three stories). Each
sentence in the tree has a score. Story generation
amounts to traversing the tree and selecting the
nodes with the highest score
Specifically, our story generator applies two
distinct search procedures. Although we are ul-
timately searching for the best overall story at
the document level, we must also find the most
suitable sentences that can be generated from the
knowledge base (see Figure 4). The space of pos-
sible stories can increase dramatically depending
on the size of the knowledge base so that an ex-
haustive tree search becomes computationally pro-
hibitive. Fortunately, we can use beam search to
prune low-scoring sentences and the stories they
generate. For example, we may prefer sentences
describing actions that are common for their char-
acters. We also apply two additional criteria in se-
lecting good stories, namely whether they are co-
herent and interesting. At each depth in the tree
we maintain the N-best stories. Once we reach the
required length, the highest scoring story is pre-
sented to the user. In the following we describe
the components of our system in more detail.
2.1 Content Planning
As mentioned earlier our generator has access to
a knowledge base recording entities and their in-
teractions. These are essentially predicate argu-
ment structures extracted from a corpus. In our ex-
periments this knowledge base was created using
the RASP relational parser (Briscoe and Carroll,
2002). We collected all verb-subject, verb-object,
verb-adverb, and noun-adjective relations from the
parser?s output and scored them with the mutual
dog:SUBJ:bark whistle:OBJ:dog
dog:SUBJ:bite treat:OBJ:dog
dog:SUBJ:see give:OBJ:dog
dog:SUBJ:like have: OBJ:dog
hungry:ADJ:dog lovely:ADJ:dog
Table 1: Relations for the noun dog with high
MI scores (SUBJ is a shorthand for subject-of,
OBJ for object-of and ADJ for adjective-of).
information-based metric proposed in Lin (1998):
MI = ln
(
? w,r,w? ? ? ? ?,r,? ?
? w,r,? ? ? ? ?,r,w? ?
)
(1)
where w and w? are two words with relation type r.
? denotes all words in that particular relation and
? w,r,w? ? represents the number of times w,r,w?
occurred in the corpus. These MI scores are used
to inform the generation system about likely entity
relationships at the sentence level. Table 1 shows
high scoring relations for the noun dog extracted
from the corpus used in our experiments (see Sec-
tion 4 for details).
Note that MI weighs binary relations which in
some cases may be likely on their own without
making sense in a ternary relation. For instance, al-
though both dog:SUBJ:run and president:OBJ:run
are probable we may not want to create the sen-
tence ?The dog runs for president?. Ditransitive
verbs pose a similar problem, where two incongru-
ent objects may appear together (the sentence John
gives an apple to the highway is semantically odd,
whereas John gives an apple to the teacher would
be fine). To help reduce these problems, we need
to estimate the likelihood of ternary relations. We
therefore calculate the conditional probability:
p(a1,a2 | s,v) =
? s,v,a1,a2 ?
? s,v,?,? ?
(2)
where s is the subject of verb v, a1 is the first argu-
ment of v and a2 is the second argument of v and
v,s,a1 6= ?. When a verb takes two arguments, we
first consult (2), to see if the combination is likely
before backing off to (1).
The knowledge base described above can only
inform the generation system about relationships
on the sentence level. However, a story created
simply by concatenating sentences in isolation
will often be incoherent. Investigations into the
interpretation of narrative discourse (Asher and
Lascarides, 2003) have shown that lexical infor-
mation plays an important role in determining
219
SUBJ:chase
OBJ:chase
SUBJ:run
SUBJ:escape
SUBJ:fall
OBJ:catch SUBJ:frighten
SUBJ:jump
1 2
2
6
5
8
1
5
Figure 3: Graph encoding (partially ordered)
chains of events
the discourse relations between propositions. Al-
though we don?t have an explicit model of rhetor-
ical relations and their effects on sentence order-
ing, we capture the lexical inter-dependencies be-
tween sentences by focusing on events (verbs)
and their precedence relationships in the corpus.
For every entity in our training corpus we extract
event chains similar to those proposed by Cham-
bers and Jurafsky (2008). Specifically, we identify
the events every entity relates to and record their
(partial) order. We assume that verbs sharing the
same arguments are more likely to be semantically
related than verbs with no arguments in common.
For example, if we know that someone steals and
then runs, we may expect the next action to be that
they hide or that they are caught.
In order to track entities and their associated
events throughout a text, we first resolve entity
mentions using OpenNLP2. The list of events per-
formed by co-referring entities and their gram-
matical relation (i.e., subject or object) are sub-
sequently stored in a graph. The edges between
event nodes are scored using the MI equation
given in (1). A fragment of the action graph
is shown in Figure 3 (for simplicity, the edges
in the example are weighted with co-occurrence
frequencies). Contrary to Chambers and Juraf-
sky (2008) we do not learn global narrative
chains over an entire corpus. Currently, we con-
sider local chains of length two and three (i.e.,
chains of two or three events sharing gram-
matical arguments). The generator consults the
graph when selecting a verb for an entity. It
will favor verbs that are part of an event chain
(e.g., SUBJ:chase ? SUBJ:run ? SUBJ:fall in
Figure 3). This way, the search space is effectively
pruned as finding a suitable verb in the current sen-
tence is influenced by the choice of verb in the next
sentence.
2See http://opennlp.sourceforge.net/.
2.2 Sentence Planning
So far we have described how we gather knowl-
edge about entities and their interactions, which
must be subsequently combined into a sentence.
The backbone of our sentence planner is a gram-
mar with subcategorization information which we
collected from the lexicon created by Korhonen
and Briscoe (2006) and the COMLEX dictionary
(Grishman et al, 1994). The grammar rules act
as templates. They each take a verb as their head
and propose ways of filling its argument slots. This
means that when generating a story, the choice of
verb will affect the structure of the sentence. The
subcategorization templates are weighted by their
probability of occurrence in the reference dictio-
naries. This allows the system to prefer less elab-
orate grammatical structures. The grammar rules
were converted to a format compatible with our
surface realizer (see Section 2.3) and include in-
formation pertaining to mood, agreement, argu-
ment role, etc.
Our sentence planner aggregates together infor-
mation from the knowledge base, without how-
ever generating referring expressions. Although
this would be a natural extension, we initially
wanted to assess whether the stochastic approach
advocated here is feasible at all, before venturing
towards more ambitious components.
2.3 Surface Realization
The surface realization process is performed by
RealPro (Lavoie and Rambow (1997)). The sys-
tem takes an abstract sentence representation and
transforms it into English. There are several gram-
matical issues that will affect the final realization
of the sentence. For nouns we must decide whether
they are singular or plural, whether they are pre-
ceded by a definite or indefinite article or with no
article at all. Adverbs can either be pre-verbal or
post-verbal. There is also the issue of selecting
an appropriate tense for our generated sentences,
however, we simply assume all sentences are in
the present tense. Since we do not know a priori
which of these parameters will result in a gram-
matical sentence, we generate all possible combi-
nations and select the most likely one according to
a language model. We used the SRI toolkit to train
a trigram language model on the British National
Corpus, with interpolated Kneser-Ney smoothing
and perplexity as the scoring metric for the gener-
ated sentences.
220
root
dog
. . . bark
bark(dog) bark at(dog,OBJ)
bark at(dog,duck) bark at(dog,cat)
bark(dog,ADV)
bark(dog,loudly)
hide run
duck
quack
. . .
run
. . .
fly
. . .
Figure 4: Simplified generation example for the in-
put sentence the dog chases the duck.
2.4 Sentence Generation Example
It is best to illustrate the generation procedure with
a simple example (see Figure 4). Given the sen-
tence the dog chases the duck as input, our gen-
erator assumes that either dog or duck will be the
subject of the following sentence. This is a some-
what simplistic attempt at generating coherent sto-
ries. Centering (Grosz et al, 1995) and other dis-
course theories argue that topical entities are likely
to appear in prominent syntactic positions such as
subject or object. Next, we select verbs from the
knowledge base that take the words duck and dog
as their subject (e.g., bark, run, fly). Our beam
search procedure will reduce the list of verbs to
a small subset by giving preference to those that
are likely to follow chase and have duck and dog
as their subjects or objects.
The sentence planner gives a set of possible
frames for these verbs which may introduce ad-
ditional entities (see Figure 4). For example, bark
can be intransitive or take an object or adver-
bial complement. We select an object for bark,
by retrieving from the knowledge base the set
of objects it co-occurs with. Our surface real-
izer will take structures like ?bark(dog,loudly)?,
?bark at(dog,cat)?, ?bark at(dog,duck)? and gen-
erate the sentences the dog barks loudly, the dog
barks at the cat and the dog barks at the duck. This
procedure is repeated to create a list of possible
candidates for the third sentence, and so on.
As Figure 4 illustrates, there are many candidate
sentences for each entity. In default of generating
all of these exhaustively, our system utilizes the
MI scores from the knowledge base to guide the
search. So, at each choice point in the generation
process, e.g., when selecting a verb for an entity or
a frame for a verb, we consider the N best alterna-
tives assuming that these are most likely to appear
in a good story.
3 Story Ranking
We have so far described most modules of our
story generator, save one important component,
namely the story ranker. As explained earlier, our
generator produces stories stochastically, by rely-
ing on co-occurrence frequencies collected from
the training corpus. However, there is no guaran-
tee that these stories will be interesting or coher-
ent. Engaging stories have some element of sur-
prise and originality in them (Turner, 1994). Our
stories may simply contain a list of actions typi-
cally performed by the story characters. Or in the
worst case, actions that make no sense when col-
lated together.
Ideally, we would like to be able to discern in-
teresting stories from tedious ones. Another im-
portant consideration is their coherence. We have
to ensure that the discourse smoothly transitions
from one topic to the next. To remedy this, we
developed two ranking functions that assess the
candidate stories based on their interest and coher-
ence. Following previous work (Stent et al, 2004;
Barzilay and Lapata, 2007) we learn these ranking
functions from training data (i.e., stories labeled
with numeric values for interestingness and coher-
ence).
Interest Model A stumbling block to assessing
how interesting a story may be, is that the very no-
tion of interestingness is subjective and not very
well understood. Although people can judge fairly
reliably whether they like or dislike a story, they
have more difficulty isolating what exactly makes
it interesting. Furthermore, there are virtually no
empirical studies investigating the linguistic (sur-
face level) correlates of interestingness. We there-
fore conducted an experiment where we asked par-
ticipants to rate a set of human authored stories in
terms of interest. Our stories were Aesop?s fables
since they resemble the stories we wish to gener-
ate. They are fairly short (average length was 3.7
sentences) and with a few characters. We asked
participants to judge 40 fables on a set of crite-
ria: plot, events, characters, coherence and interest
(using a 5-point rating scale). The fables were split
into 5 sets of 8; each participant was randomly as-
signed one of the 5 sets to judge. We obtained rat-
221
ings (440 in total) from 55 participants, using the
WebExp3 experimental software.
We next investigated if easily observable syn-
tactic and lexical features were correlated with in-
terest. Participants gave the fables an average in-
terest rating of 3.05. For each story we extracted
the number of tokens and types for nouns, verbs,
adverbs and adjectives as well as the number
of verb-subject and verb-object relations. Using
the MRC Psycholinguistic database4 tokens were
also annotated along the following dimensions:
number of letters (NLET), number of phonemes
(NPHON), number of syllables (NSYL), written
frequency in the Brown corpus (Kucera and Fran-
cis 1967; K-F-FREQ), number of categories in the
Brown corpus (K-F-NCATS), number of samples
in the Brown corpus (K-F-NSAMP), familiarity
(FAM), concreteness (CONC), imagery (IMAG),
age of acquisition (AOA), and meaningfulness
(MEANC and MEANP).
Correlation analysis was used to assess the de-
gree of linear relationship between interest ratings
and the above features. The results are shown in
Table 2. As can be seen the highest predictor is the
number of objects in a story, followed by the num-
ber of noun tokens and types. Imagery, concrete-
ness and familiarity all seem to be significantly
correlated with interest. Story length was not a
significant predictor. Regressing the best predic-
tors from Table 2 against the interest ratings yields
a correlation coefficient of 0.608 (p < 0.05). The
predictors account uniquely for 37.2% of the vari-
ance in interest ratings. Overall, these results indi-
cate that a model of story interest can be trained
using shallow syntactic and lexical features. We
used the Aesop?s fables with the human ratings as
training data fromwhich we extracted features that
shown to be significant predictors in our correla-
tion analysis. Word-based features were summed
in order to obtain a representation for the en-
tire story. We used Joachims?s (2002) SVMlight
package for training with cross-validation (all pa-
rameters set to their default values). The model
achieved a correlation of 0.948 (Kendall?s tau)
with the human ratings on the test set.
Coherence Model As well as being interesting
we have to ensure that our stories make sense
to the reader. Here, we focus on local coher-
ence, which captures text organization at the level
3See http://www.webexp.info/.
4http://www.psy.uwa.edu.au/mrcdatabase/uwa_
mrc.htm
Interest Interest
NTokens 0.188?? NLET 0.120?
NTypes 0.173?? NPHON 0.140??
VTokens 0.123? NSYL 0.125??
VTypes 0.154?? K-F-FREQ 0.054
AdvTokens 0.056 K-F-NCATS 0.137??
AdvTypes 0.051 K-F-NSAMP 0.103?
AdjTokens 0.035 FAM 0.162??
AdjTypes 0.029 CONC 0.166??
NumSubj 0.150?? IMAG 0.173??
NumObj 0.240?? AOA 0.111?
MEANC 0.169?? MEANP 0.156??
Table 2: Correlation values for the human ratings
of interest against syntactic and lexical features;
? : p < 0.05, ?? : p < 0.01.
of sentence to sentence transitions. We created a
model of local coherence using using the Entity
Grid approach described in Barzilay and Lapata
(2007). This approach represents each document
as a two-dimensional array in which the columns
correspond to entities and the rows to sentences.
Each cell indicates whether an entity appears in a
given sentence or not and whether it is a subject,
object or neither. This entity grid is then converted
into a vector of entity transition sequences. Train-
ing the model required examples of both coher-
ent and incoherent stories. An artificial training set
was created by permuting the sentences of coher-
ent stories, under the assumption that the original
story is more coherent than its permutations. The
model was trained and tested on the Andrew Lang
fairy tales collection5 on a random split of the data.
It ranked the original stories higher than their cor-
responding permutations 67.40% of the time.
4 Experimental Setup
In this section we present our experimental set-up
for assessing the performance of our story genera-
tor. We give details on our training corpus, system,
parameters (such as the width of the beam), the
baselines used for comparison, and explain how
our system output was evaluated.
Corpus The generator was trained on 437 sto-
ries from the Andrew Lang fairy tale corpus.6 The
stories had an average length of 125.18 sentences.
The corpus contained 15,789 word tokens. We
5Aesop?s fables were too short to learn a coherence
model.
6See http://www.mythfolklore.net/andrewlang/.
222
discarded word tokens that did not appear in the
Children?s Printed Word Database7, a database of
printed word frequencies as read by children aged
between five and nine.
Story search When searching the story space,
we set the beam width to 500. This means that
we allow only 500 sentences to be considered at
a particular depth before generating the next set of
sentences in the story. For each entity we select the
five most likely events and event sequences. Anal-
ogously, we consider the five most likely subcate-
gorization templates for each verb. Considerable
latitude is available when applying the ranking
functions. We may use only one of them, or one
after the other, or both of them. To evaluate which
system configuration was best, we asked two hu-
man evaluators to rate (on a 1?5 scale) stories pro-
duced in the following conditions: (a) score the
candidate stories using the interest function first
and then coherence (and vice versa), (b) score the
stories simultaneously using both rankers and se-
lect the story with the highest score. We also ex-
amined how best to prune the search space, i.e., by
selecting the highest scoring stories, the lowest
scoring one, or simply at random. We created ten
stories of length five using the fairy tale corpus for
each permutation of the parameters. The results
showed that the evaluators preferred the version
of the system that applied both rankers simultane-
ously and maintained the highest scoring stories in
the beam.
Baselines We compared our system against two
simpler alternatives. The first one does not use
a beam. Instead, it decides deterministically how
to generate a story on the basis of the most
likely predicate-argument and predicate-predicate
counts in the knowledge base. The second one
creates a story randomly without taking any co-
occurrence frequency into account. Neither of
these systems therefore creates more than one
story hypothesis whilst generating.
Evaluation The system generated stories for
10 input sentences. These were created using com-
monly occurring sentences in the fairy tales corpus
(e.g., The family has the baby, The monkey climbs
the tree, The giant guards the child). Each sys-
tem generated one story for each sentence result-
ing in 30 (3?10) stories for evaluation. All sto-
ries had the same length, namely five sentences.
Human judges (21 in total) were asked to rate the
7http://www.essex.ac.uk/psychology/cpwd/
System Fluency Coherence Interest
Random 1.95? 2.40? 2.09?
Deterministic 2.06? 2.53? 2.09?
Rank-based 2.20 2.65 2.20
Table 3: Human evaluation results: mean story rat-
ings for three versions of our system; ?: signifi-
cantly different from Rank-based.
stories on a scale of 1 to 5 for fluency (was the
sentence grammatical?), coherence (does the story
make sense overall?) and interest (how interesting
is the story?). The stories were presented in ran-
dom order. Participants were told that all stories
were generated by a computer program. They were
instructed to rate more favorably interesting sto-
ries, stories that were comprehensible and overall
grammatical.
5 Results
Our results are summarized in Table 3 which lists
the average human ratings for the three systems.
We performed an Analysis of Variance (ANOVA)
to examine the effect of system type on the story
generation task. Statistical tests were carried out
on the mean of the ratings shown in Table 3 for
fluency, coherence, and interest. We observed a re-
liable effect of system type by subjects and items
on all three dimensions. Post-hoc Tukey tests re-
vealed that the stories created with our rank-
based system are perceived as significantly better
in terms of fluency, interest, and coherence than
those generated by both the deterministic and ran-
dom systems (? < 0.05). The deterministic system
is not significantly better than the random one ex-
cept in terms of coherence.
These results are not entirely surprising. The
deterministic system maintains a local restricted
view of what constitutes a good story. It creates
a story by selecting isolated entity-event relation-
ships with high MI scores. As a result, the stories
are unlikely to have a good plot. Moreover, it tends
to primarily favor verb-object or verb-subject re-
lations, since these are most frequent in the cor-
pus. The stories thus have little structural varia-
tion and feel repetitive. The random system uses
even less information in generating a story (entity-
action relationships are chosen at random without
taking note of the MI scores). In contrast to these
baselines, the rank-based system assesses candi-
date stories more globally. It thus favors coher-
ent stories, with varied word choice and structure.
223
The family has the baby The giant guards the child
R
an
do
m The family has the baby. The family is how to emptyup to a fault. The baby vanishes into the cave. The fam-
ily meets with a stranger. The baby says for the boy to
fancy the creature.
The giant guards the child. The child calls for the
window to order the giant. The child suffers from a
pleasure. The child longer hides the forest. The child
reaches presently.
D
et
er
m The family has the baby. The family rounds up the
waist. The family comes in. The family wonders. The
family meets with the terrace.
The giant guards the child. The child rescues the clutch.
The child beats down on a drum. The child feels out of
a shock. The child hears from the giant.
R
an
k-
ba
se
d The family has the baby. The baby is to seat the lady at
the back. The baby sees the lady in the family. The fam-
ily marries a lady for the triumph. The family quickly
wishes the lady vanishes.
The giant guards the child. The child rescues the son
from the power. The child begs the son for a pardon.
The giant cries that the son laughs the happiness out of
death. The child hears if the happiness tells a story.
Table 4: Stories generated by the random, deterministic, and rank-based systems.
A note of caution here concerns referring expres-
sions which our systems cannot at the moment
generate. This may have disadvantaged the stories
overall, rendering them stylistically awkward.
The stories generated by both the determinis-
tic and random systems are perceived as less in-
teresting in comparison to the rank-based system.
This indicates that taking interest into account is a
promising direction even though the overall inter-
estingness of the stories we generate is somewhat
low (see third column in Table 3). Our interest
ranking function was trained on well-formed hu-
man authored stories. It is therefore possible that
the ranker was not as effective as it could be sim-
ply because it was applied to out-of-domain data.
An interesting extension which we plan for the
future is to evaluate the performance of a ranker
trained on machine generated stories.
Table 4 illustrates the stories generated by each
system for two input sentences. The rank-based
stories read better overall and are more coherent.
Our subjects also gave them high interest scores.
The deterministic system tends to select simplis-
tic sentences which although read well by them-
selves do not lead to an overall narrative. Interest-
ingly, the story generated by the random system
for the input The family has the baby, scored high
on interest too. The story indeed contains interest-
ing imagery (e.g. The baby vanishes into the cave)
although some of the sentences are syntactically
odd (e.g. The family is how to empty up to a fault).
6 Conclusions and Future Work
In this paper we proposed a novel method to
computational story telling. Our approach has
three key features. Firstly, story plot is created
dynamically by consulting an automatically cre-
ated knowledge base. Secondly, our generator re-
alizes the various components of the generation
pipeline stochastically, without extensive manual
coding. Thirdly, we generate and store multiple
stories efficiently in a tree data structure. Story
creation amounts to traversing the tree and select-
ing the nodes with the highest score. We develop
two scoring functions that rate stories in terms
of how coherent and interesting they are. Experi-
mental results show that these bring improvements
over versions of the system that rely solely on
the knowledge base. Overall, our results indicate
that the overgeneration-and-ranking approach ad-
vocated here is viable in producing short stories
that exhibit narrative structure. As our system can
be easily rertrained on different corpora, it can po-
tentially generate stories that vary in vocabulary,
style, genre, and domain.
An important future direction concerns a more
detailed assessment of our search procedure. Cur-
rently we don?t have a good estimate of the type of
stories being overlooked due to the restrictions we
impose on the search space. An appealing alterna-
tive is the use of Genetic Algorithms (Goldberg,
1989). The operations of mutation and crossover
have the potential of creating more varied and
original stories. Our generator would also bene-
fit from an explicit model of causality which is
currently approximated by the entity chains. Such
a model could be created from existing resources
such as ConceptNet (Liu and Davenport, 2004),
a freely available commonsense knowledge base.
Finally, improvements such as the generation of
referring expressions and the modeling of selec-
tional restrictions would create more fluent stories.
Acknowledgements The authors acknowledge
the support of EPSRC (grant GR/T04540/01).
We are grateful to Richard Kittredge for his help
with RealPro. Special thanks to Johanna Moore
for insightful comments and suggestions.
224
References
Asher, Nicholas and Alex Lascarides. 2003. Logics of Con-
versation. Cambridge University Press.
Barzilay, Regina and Mirella Lapata. 2005. Collective con-
tent selection for concept-to-text generation. In Proceed-
ings of the HLT/EMNLP. Vancouver, pages 331?338.
Barzilay, Regina and Mirella Lapata. 2007. Modeling local
coherence: An entity-based approach. Computational Lin-
guistics 34(1):1?34.
Briscoe, E. and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Callaway, Charles B. and James C. Lester. 2002. Narrative
prose generation. Artificial Intelligence 2(139):213?252.
Chambers, Nathanael and Dan Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Proceedings of ACL-
08: HLT . Columbus, OH, pages 789?797.
Duboue, Pablo A. and Kathleen R. McKeown. 2002. Con-
tent planner construction via evolutionary algorithms and
a corpus-based fitness function. In Proceedings of the 2nd
INLG. Ramapo Mountains, NY.
Fass, S. 2002. Virtual Storyteller: An Approach to Compu-
tational Storytelling. Master?s thesis, Dept. of Computer
Science, University of Twente.
Goldberg, David E. 1989. Genetic Algorithms in Search, Op-
timization and Machine Learning. Addison-Wesley Long-
man Publishing Co., Inc., Boston, MA.
Grishman, Ralph, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational lexi-
con. In Proceedings of the 15th COLING. Kyoto, Japan,
pages 268?272.
Grosz, Barbara J., Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguistics
21(2):203?225.
Joachims, Thorsten. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the 8th ACM
SIGKDD. Edmonton, AL, pages 133?142.
Knight, Kevin and Vasileios Hatzivassiloglou. 1995. Two-
level, many-paths generation. In Proceedings of the 33rd
ACL. Cambridge, MA, pages 252?260.
Korhonen, Y. Krymolowski, A. and E.J. Briscoe. 2006. A
large subcategorization lexicon for natural language pro-
cessing applications. In Proceedings of the 5th LREC.
Genova, Italy.
Kucera, Henry and Nelson Francis. 1967. Computational
Analysis of Present-day American English. Brown Uni-
versity Press, Providence, RI.
Lavoie, Benoit and Owen Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings of the
5th ANCL. Washington, D.C., pages 265?268.
Lin, Dekang. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th COLING. Montre?al,
QC, pages 768?774.
Liu, Hugo and Glorianna Davenport. 2004. ConceptNet: a
practical commonsense reasoning toolkit. BT Technology
Journal 22(4):211?226.
Mellish, Chris, Alisdair Knott, Jon Oberlander, and Mick
O?Donnell. 1998. Experiments using stochastic search for
text planning. In Eduard Hovy, editor, Proceedings of the
9th INLG. New Brunswick, NJ, pages 98?107.
Oinonen, K.M., M. Theune, A. Nijholt, and J.R.R. Uijlings.
2006. Designing a story database for use in automatic
story generation. In R. Harper, M. Rauterberg, and
M. Combetto, editors, Entertainment Computing ? ICEC
2006. Springer Verlag, Berlin, volume 4161 of Lecture
Notes in Computer Science, pages 298?301.
Propp, Vladimir. 1968. The Morphology of Folk Tale. Uni-
versity of Texas Press, Austin, TX.
Reiter, E and R Dale. 2000. Building Natural-Language Gen-
eration Systems. Cambridge University Press.
Robertson, Judy and Judith Good. 2003. Ghostwriter: A nar-
rative virtual environment for children. In Proceedings of
IDC2003. Preston, England, pages 85?91.
Shim, Yunju and Minkoo Kim. 2002. Automatic short story
generator based on autonomous agents. In Proceedings of
PRIMA. London, UK, pages 151?162.
Stent, Amanda, Rashmi Prasad, and Marilyn Walker. 2004.
Trainable sentence planning for complex information pre-
sentation in spoken dialog systems. In Proceedings of the
42nd ACL. Barcelona, Spain, pages 79?86.
Theune, M., S. Faas, D.K.J. Heylen, and A. Nijholt. 2003.
The virtual storyteller: Story creation by intelligent agents.
In S. Gbel, N. Braun, U. Spierling, J. Dechau, and H. Di-
ener, editors, TIDSE-2003. Fraunhofer IRB Verlag, Darm-
stadt, pages 204?215.
Thorndyke, Perry W. 1977. Cognitive structures in compre-
hension and memory of narrative discourse. Cognitive
Psychology 9(1):77?110.
Turner, Scott T. 1994. The creative process: A computer
model of storytelling and creativity. Erlbaum, Hillsdale,
NJ.
225
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1562?1572,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Plot Induction and Evolutionary Search for Story Generation
Neil McIntyre and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB, UK
n.d.mcintyre@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we develop a story genera-
tor that leverages knowledge inherent in
corpora without requiring extensive man-
ual involvement. A key feature in our ap-
proach is the reliance on a story planner
which we acquire automatically by record-
ing events, their participants, and their
precedence relationships in a training cor-
pus. Contrary to previous work our system
does not follow a generate-and-rank archi-
tecture. Instead, we employ evolutionary
search techniques to explore the space of
possible stories which we argue are well
suited to the story generation task. Experi-
ments on generating simple children?s sto-
ries show that our system outperforms pre-
vious data-driven approaches.
1 Introduction
Computer story generation has met with fasci-
nation since the early days of artificial intelli-
gence. Indeed, over the years, several genera-
tors have been developed capable of creating sto-
ries that resemble human output. To name only
a few, TALE-SPIN (Meehan, 1977) generates sto-
ries through problem solving, MINSTREL (Turner,
1992) relies on an episodic memory scheme, es-
sentially a repository of previous hand-coded sto-
ries, to solve the problems in the current story,
and MAKEBELIEVE (Liu and Singh, 2002) uses
commonsense knowledge to generate short stories
from an initial seed story (supplied by the user). A
large body of more recent work views story gener-
ation as a form of agent-based planning (Swartjes
and Theune, 2008; Pizzi et al, 2007). The agents
act as characters with a list of goals. They form
plans of action and try to fulfill them. Interesting
stories emerge as plans interact and cause failures
and possible replanning.
The broader appeal of computational story gen-
eration lies in its application potential. Examples
include the entertainment industry and the devel-
opment of tools that produce large numbers of
plots automatically that might provide inspiration
to professional screen writers (Agudo et al, 2004);
rendering video games more interesting by allow-
ing the plot to adapt dynamically to the players?
actions (Barros and Musse, 2007); and assisting
teachers to create or personalize stories for their
students (Riedl and Young, 2004).
A major stumbling block for the widespread use
of computational story generators is their reliance
on expensive, manually created resources. A typi-
cal story generator will make use of a knowledge
base for providing detailed domain-specific infor-
mation about the characters and objects involved
in the story and their relations. It will also have a
story planner that specifies how these characters
interact, what their goals are and how their ac-
tions result in different story plots. Finally, a sen-
tence planner (coupled with a surface realizer) will
render an abstract story specification into natural
language text. Traditionally, most of this knowl-
edge is created by hand, and the effort must be re-
peated for new domains, new characters and plot
elements.
Fortunately, recent work in natural language
processing has taken significant steps towards de-
veloping algorithms that learn some of this knowl-
edge automatically from natural language cor-
pora. Chambers and Jurafsky (2009, 2008) pro-
pose an unsupervised method for learning narra-
tive schemas, chains of events whose arguments
are filled with participant semantic roles defined
over words. An example schema is {X arrest, X
charge, X raid, X seize, X confiscate, X detain, X
deport}, where X stands for the argument types
{police, agent, authority, government}. Their ap-
proach relies on the intuition that in a coherent
text events that are about the same participants are
1562
likely to be part of the same story or narrative.
Their model extracts narrative chains, essentially
events that share argument slots and merges them
into schemas. The latter could be used to construct
or enrich the knowledge base of a story generator.
In McIntyre and Lapata (2009) we presented a
story generator that leverages knowledge inherent
in corpora without requiring extensive manual in-
volvement. The generator operates over predicate-
argument and predicate-predicate co-occurrence
tuples gathered from training data. These are used
to produce a large set of candidate stories which
are subsequently ranked based on their interest-
ingness and coherence. The approach is unusual
in that it does not involve an explicit story plan-
ning component. Stories are created stochastically
by selecting entities and the events they are most
frequently attested with.
In this work we develop a story generator that
is also data-driven but crucially relies on a story
planner for creating meaningful stories. Inspired
by Chambers and Jurafsky (2009) we acquire story
plots automatically by recording events, their par-
ticipants, and their precedence relationships as at-
tested in a training corpus. Entities give rise to
different potential plots which in turn generate
multiple stories. Contrary to our previous work
(McIntyre and Lapata, 2009), we do not follow a
generate-and-rank architecture. Instead, we search
the space of possible stories using Genetic Algo-
rithms (GAs) which we argue are advantageous
in the story generation setting, as they can search
large fitness landscapes while greatly reducing the
risk of getting stuck in local optima. By virtue of
exploring the search space more broadly, we are
able to generate creative stories without an explicit
interest scoring module.
In the remainder of this paper we give a brief
overview of the system described in McIntyre and
Lapata (2009) and discuss previous applications of
GAs in natural language generation (Section 2).
Next, we detail our approach, specifically how
plots are created and used in conjunction with ge-
netic search (Sections 3 and 4). Finally, we present
our experimental results (Sections 6 and 7) and
conclude the paper with discussion of future work.
2 Related Work
Our work builds on and extends the story genera-
tor developed in McIntyre and Lapata (2009). The
system creates simple children?s stories in an in-
teractive context: the user supplies the topic of the
story and its desired length (number of sentences).
The generator creates a story following a pipeline
architecture typical of natural language generation
systems (Reiter and Dale, 2000) consisting of con-
tent selection, sentence planning, and surface real-
ization.
The content of a story is determined by consult-
ing a data-driven knowledge base that records the
entities (i.e., nouns) appearing in a corpus and the
actions they perform. These are encoded as depen-
dency relations (e.g., subj-verb, verb-obj). In order
to promote between-sentence coherence the gen-
erator also make use of an action graph that con-
tains action-role pairs and the likelihood of tran-
sitioning from one to another. The sentence plan-
ner aggregates together entities and their actions
into a sentence using phrase structure rules. Fi-
nally, surface realization is performed by interfac-
ing RealPro (Lavoie and Rambow, 1997) with a
language model. The system searches for the best
story overall as well as the best sentences that can
be generated from the knowledge base. Unlikely
stories are pruned using beam search. In addition,
stories are reranked using two scoring functions
based on coherence and interest. These are learnt
from training data, i.e., stories labeled with nu-
meric values for interest and coherence.
Evolutionary search techniques have been pre-
viously employed in natural language generation,
especially in the context of document planning.
Structuring a set of facts into a coherent text is ef-
fectively a search problem that may lead to com-
binatorial explosion for large domains. Mellish
et al (1998) (and subsequently Karamanis and
Manurung 2002) advocate genetic algorithms as
an alternative to exhaustively searching for the op-
timal ordering of descriptions of museum arte-
facts. Rather than requiring a global optimum to
be found, the genetic algorithm selects an order
(based on coherence) that is good enough for peo-
ple to understand. Cheng and Mellish (2000) focus
on the interaction of aggregation and text planning
and use genetic algorithms to search for the best
aggregated document that satisfies coherence con-
straints.
The application of genetic algorithms to story
generation is novel to our knowledge. Our work
also departs from McIntyre and Lapata (2009) in
two important ways. Firstly, our generator does
not rely on a knowledge base of seemingly un-
related entities and relations. Rather, we employ
1563
a document planner to create and structure a plot
for a story. The planner is built automatically from
a training corpus and creates plots dynamically
depending on the protagonists of the story. Sec-
ondly, our search procedure is simpler and more
global; instead of searching for the best story twice
(i.e., by first finding the n-best stories and then
subsequently reranking them based on coherence
and interest), our genetic algorithm explores the
space of possible stories once.
3 Plot Generation
Following previous work (e.g., Shim and Kim
2002; McIntyre and Lapata 2009) we assume that
the user supplies a sentence (e.g., the princess
loves the prince) from which the system creates
a story. Each entity in this sentence (e.g., princess,
prince) is associated with its own narrative
schema, a set of key events and actors co-
occurring with it in the training corpus. Our nar-
rative schemas differ slightly from Chambers and
Jurafsky (2009). They acquire schematic represen-
tations of situations akin to FrameNet (Fillmore
et al, 2003): schemas consists of semantically
similar predicates and the entities evoked by them.
In our setting, every entity has its own schema, and
predicates associated with it are ordered. Plots are
generated by merging the entity-specific narrative
schemas which subsequently serve as the input to
the genetic algorithm. In the following we describe
how the narrative schemas are extracted and plots
merged, and then discuss our evolutionary search
procedure.
Entity-based Schema Extraction Before we
can generate a plot for a story we must have an
idea of the actions associated with the entities in
the story, the order in which these actions are per-
formed and also which other entities can partici-
pate. This information is stored in a directed graph
which we explain below. Our algorithm processes
each document at a time, it operates over depen-
dency structures and assumes that entity mentions
have been resolved. In our experiments we used
Rasp (Briscoe and Carroll, 2002), a broad cover-
age dependency parser, and the OpenNLP1 coref-
erence resolution engine.2 However, any depen-
dency parser or coreference tool could serve our
1See http://opennlp.sourceforge.net/.
2The coreference resolution tool we employ is not
error-free and on occasion will fail to resolve a pronoun. We
map unresolved pronouns to the generic labels person or ob-
ject.
purpose. We also assume that the actions associ-
ated with a given entity are ordered and that lin-
ear order corresponds to temporal order. This is a
gross simplification as it is well known that tem-
poral relationships between events are not limited
to precedence, they may overlap, occur simultane-
ously, or be temporally unrelated. We could have
obtained a more accurate ordering using a tempo-
ral classifier (see Chambers and Jurafsky 2008),
however we leave this to future work.
For each entity e in the corpus we build a di-
rected graph G = (V,E) whose nodes V denote
predicate argument relationships, and edges E rep-
resent transitions from node Vi to node Vj. As
an example of our schema construction process,
consider a very small corpus consisting of the
two documents shown in Figure 1. The schema
for princess after processing the first document is
given on the left hand side. Each node in this graph
corresponds to an action attested with princess (we
also record who performs it and where or how).
Nodes are themselves dependency trees (see Fig-
ure 4a), but are linearized in the figure for the
sake of brevity. Edges in the graph indicate order-
ing and are weighted using the mutual informa-
tion metric proposed in Lin (1998) (the weights
are omitted from the example).3 The first sentence
in the text gives rise to the first node in the graph,
the second sentence to the second node, and so on.
Note that the third sentence is not present in the
graph as it is not about the princess.
When processing the second document, we sim-
ply expand this graph. Before inserting a new
node, we check if it can be merged with an al-
ready existing one. Nodes are merged only if they
have the same verb and similar arguments, with
the focal entity (i.e., princess) appearing in the
same argument slot. In our example, the nodes
?prince marry princess in castle? and ?prince
marry princess in temple? can be merged as they
contain the same verb and number of similar ar-
guments. The nodes ?princess have influence?
and ?princess have baby? cannot be merged as
influence and baby are semantically unrelated.
We compute argument similarity using WordNet
(Fellbaum, 1998) and the measure proposed by
Wu and Palmer (1994) which is based on path
length. We merge nodes with related arguments
only if their similarity exceeds a threshold (deter-
mined empirically).
3We use mutual information to identify event sequences
strongly associated with the graph entity.
1564
The goblin holds the princess in a lair.
The prince rescues the princess and
marries her in a castle. The ceremony
is beautiful. The princess has influence
as the prince rules the country.
The dragon holds the princess in a
cave. The prince slays the dragon. The
princess loves the prince. The prince
asks the king?s permission. The prince
marries the princess in the temple. The
princess has a baby.
goblin hold princess in lair
prince rescue princess
prince marry princess in castle
princess have influence
[
goblin
dragon
]
hold princess in
[
lair
cave
]
prince rescue princess princess love prince
prince marry princess in
[
castle
temple
]
princess have influence princess have baby
Figure 1: Example of schema construction for the entity princess
The schema construction algorithm terminates
when graphs like the ones shown in Figure 1 (right
hand side) have been created for all entities in the
corpus.
Building a Story Plot Our generator takes an in-
put sentence and uses it to instantiate several plots.
We achieve this by merging the schemas associ-
ated with the entities in the sentence into a plot
graph. As an example, consider again the sentence
the princess loves the prince which requires comb-
ing the schemas representing prince and princess
shown in Figures 2 and 1 (right hand side), re-
spectively. Again, we look for nodes that can be
merged based on the identity of the actions in-
volved and the (WordNet) similarity of their ar-
guments. However, we disallow the merging of
nodes with focal entities appearing in the same ar-
gument slot (e.g., ?[prince, princess] cries?).
Once the plot graph is created, a depth first
search starting from the node corresponding to
the input sentence, finds all paths with length
matching the desired story length (cycles are dis-
allowed). Assuming we wish to generate a story
consisting of three sentences, the graph in Figure 3
would create four plots. These are (princess love
prince, prince marry princess in [castle, temple],
princess have influence), (princess love prince,
prince marry princess in [castle, temple], princess
have baby), (princess love prince, prince marry
princess in [castle, temple], prince rule country),
and (princess love prince, prince ask king?s per-
mission prince marry princess in [castle, temple]).
Each of these plots represents two different stories
one with castle and one with temple in it.
Sentence Planning The sentence planner is in-
terleaved with the story planner and influences
the final structure of each sentence in the story.
To avoid generating short sentences ? note that
nodes in the plot graph consist of a single ac-
tion and would otherwise correspond to a sentence
with a single clause ? we combine pairs of nodes
within the same graph by looking at intrasenten-
tial verb-verb co-occurrences in the training cor-
pus. For example, the nodes (prince have prob-
lem, prince keep secret) could become the sen-
tence the prince has a problem keeping a secret.
We leave it up to the sentence planner to decide
how the two actions should be combined.4 The
sentence planner will also insert adverbs and ad-
jectives, using co-occurrence likelihoods acquired
from the training corpus. It is essentially a phrase
structure grammar compiled from the lexical re-
sources made available by Korhonen and Briscoe
(2006) and Grishman et al (1994). The grammar
rules act as templates for combining clauses and
filling argument slots.
4We only turn an action into a subclause if its subject en-
tity is same as that of the previous action.
1565
prince slay dragon
prince rescue princess
princess love prince
prince marry princess in
[
castle
temple
]
prince ask king?s permission
prince rule country
Figure 2: Narrative schema for the entity prince.
4 Genetic Algorithms
The example shown in Figure 3 is a simplified ver-
sion of a plot graph. The latter would normally
contain hundreds of nodes and give rise to thou-
sands of stories once lexical variables have been
expanded. Searching the story space is a difficult
optimization problem, that must satisfy several
constraints: the story should be of a certain length,
overall coherent, creative, display some form of
event progression, and generally make sense. We
argue that evolutionary search is appealing here, as
it can find global optimal solutions in a more effi-
cient way than traditional optimization methods.
In this study we employ genetic algorithms
(GAs) a well-known search technique for finding
approximate (or exact) solutions to optimization
problems. The basic idea behind GAs is based
on ?natural selection? and the Darwinian princi-
ple of the survival of the fittest (Mitchell, 1998).
An initial population is randomly created contain-
ing a predefined number of individuals (or solu-
tions), each represented by a genetic string (e.g., a
population of chromosomes). Each individual is
evaluated according to an objective function (also
called a fitness function). A number of individu-
als are then chosen as parents from the population
according to their fitness, and undergo crossover
(also called recombination) and mutation in order
to develop the new population. Offspring with bet-
ter fitness are then inserted into the population,
replacing the inferior individuals in the previous
generation.
The algorithm thus identifies the individuals
with the optimizing fitness values, and those with
lower fitness will naturally get discarded from the
population. This cycle is repeated for a given num-
ber of generations, or stopped when the solution
[
goblin
dragon
]
hold princess in
[
lair
cave
]
prince rescue princess princess love prince
prince marry princess in
[
castle
temple
]
princess have influence
princess have baby
prince slay dragon
prince ask king?s
permission
prince rule country
Figure 3: Plot graph for the input sentence the
princess loves the prince.
obtained is considered optimal. This process leads
to the evolution of a population in which the in-
dividuals are more and more suited to their envi-
ronment, just as natural adaptation. We describe
below how we developed a genetic algorithm for
our story generation problem.
Initial Population Rather than start with a ran-
dom population, we seed the initial population
with story plots generated from our plot graph.
For an input sentence, we generate all possible
plots. The latter are then randomly sampled until a
population of the desired size is created. Contrary
to McIntyre and Lapata (2009), we initialize the
search with complete stories, rather than generate
one sentence at a time. The genetic algorithm will
thus avoid the pitfall of selecting early on a solu-
tion that will later prove detrimental.
Crossover Each plot is represented as an or-
dered graph of dependency trees (corresponding
to sentences). We have decided to use crossover of
a single point between two selected parents. The
children will therefore contain sentences up to the
crossover point of the first parent and sentences
after that point of the second. Figure 4a shows
two parents (prince rescue princess, prince marry
princess in castle, princess have baby) and (prince
rescue princess, prince love princess, princess kiss
prince) and how two new plots are created by
swapping their last nodes.
1566
a) rescue
prince princess
marry
prince princess castle
have
princess baby
rescue
prince princess
love
prince princess
kiss
princess prince
=?
rescue
prince princess
marry
prince princess castle
kiss
prince princess
rescue
prince princess
love
prince princess
have
princess baby
in in
b) marry
prince princess castle
hall
temple
forest
kingdom
c) rescue
prince princess
marry
prince princess castle
kiss
prince princess
in
rescue
prince princess
marry
prince princess castle
kiss
prince princess
in
d) rescue
prince princess
marry
prince princess castle
kiss
prince princess
in
=?
hold
prince princess
e) knows
prince
loves
princess child
=?
escape
princess dragon
Figure 4: Example of genetic algorithm operators as they are applied to plot structures: a) crossover of
two plots on a single point, indicated by the dashed line, resulting in two children which are a recombi-
nation of the parents; b) mutation of a lexical node, church can be replaced from a list of semantically
related candidates; c) sentences can be switched under mutation to create a potentially more coherent
structure; d) if the matrix verb undergoes mutation then, a random sentence is generated to replace it; e)
if the verb chosen for mutation is the head of a subclause, then a random subclause replaces it.
Mutation Mutation can occur on any verb,
noun, adverb, or adjective in the plot. If a noun,
adverb or adjective is chosen to undergo mutation,
then we simply substitute it with a new lexical item
that is sufficiently similar (see Figure 4b for an
example). Verbs, however, have structural impor-
tance in the stories and we cannot simply replace
them without taking account of their arguments.
If a matrix verb is chosen to undergo mutation,
then a new random sentence is generated to re-
place the entire sentence (see Figure 4d). If it is
a subclause, then it is replaced with a randomly
generated clause, headed by a verb that has been
seen in the corpus to co-occur with the matrix verb
(Figure 4e). The sentence planner selects and fills
template trees for generating random clauses. Mu-
tation may also change the order of any two nodes
in the graph in the hope that this will increase the
story?s coherence or create some element of sur-
prise (see Figure 4c).
Selection To choose the plots for the next gener-
ation, we used fitness proportional selection (also
know as roulette-wheel selection, Goldberg 1989)
which chooses candidates randomly but with a
bias towards those with a larger proportion of the
population?s combined fitness. We do not want to
always select the fittest candidates as there may
be valid partial solutions held within less fit mem-
bers of the population. However, we did employ
some elitism by allowing the top 1% of solutions
to be copied straight from one generation to the
next. Note that our candidates may also represent
invalid solutions. For instance, through crossover
it is possible to create a plot in which all or some
nodes are identical. If any such candidates are
identified, they are assigned a low fitness, without
however being eliminated from the population as
some could be used to create fitter solutions.
In a traditional GA, the fitness function deals
with one optimization objective. It is possible to
optimize several objectives either using a vot-
1567
ing model or more sophisticated methods such as
Pareto ranking (Goldberg, 1989). Following previ-
ous work (Mellish et al, 1998) we used a single fit-
ness function that scored candidates based on their
coherence. Our function was learned from training
data using the Entity Grid document representa-
tion proposed in Barzilay and Lapata (2007). An
entity grid is a two-dimensional array in which
columns correspond to entities and rows to sen-
tences. Each cell indicates whether an entity ap-
pears in a given sentence or not and whether it is a
subject, object or neither. For training, this repre-
sentation is converted into a feature vector of en-
tity transition sequences and a model is learnt from
examples of coherent and incoherent stories. The
latter can be easily created by permuting the sen-
tences of coherent stories (assuming that the orig-
inal story is more coherent than its permutations).
In addition to coherence, in McIntyre and La-
pata (2009) we used a scoring function based on
interest which we approximated with lexical and
syntactic features such as the number of noun/verb
tokens/types, the number of subjects/objects, the
number of letters, word familiarity, imagery, and
so on. An interest-based scoring function made
sense in our previous setup as a means of selecting
unusual stories. However, in the context of genetic
search such a function seems redundant as inter-
esting stories emerge naturally through the opera-
tions of crossover and mutation.
5 Surface Realization
Once the final generation of the population has
been reached, the fittest story is selected for sur-
face realization. The realizer takes each sentence
in the story and reformulates it into input com-
patible with the RealPro (Lavoie and Rambow,
1997) text generation engine. Realpro creates sev-
eral variants of the same story differing in the
choice of determiners, number (singular or plural),
and prepositions. A language model is then used
to select the most probable realization (Knight
and Hatzivassiloglou, 1995). Ideally, the realizer
should also select an appropriate tense for the sen-
tence. However, we make the simplifying assump-
tion that all sentences are in the present tense.
6 Experimental Setup
In this section we present our experimental set-up
for assessing the performance of our story genera-
tor. We give details on our training corpus, system,
parameters (such as the population size for the GA
search), the baselines used for comparison, and ex-
plain how our system output was evaluated.
Corpus The generator was trained on the same
corpus used in McIntyre and Lapata (2009), 437
stories from the Andrew Lang fairy tales collec-
tion.5 The average story length is 125.18 sen-
tences. The corpus contains 15,789 word tokens.
Following McIntyre and Lapata, we discarded to-
kens that did not appear in the Children?s Printed
Word Database6, a database of printed word fre-
quencies as read by children aged between five
and nine. From this corpus we extracted narrative
schemas for 667 entities in total. We disregarded
any graph that contained less than 10 nodes as too
small. The graphs had on average 61.04 nodes,
with an average clustering rate7 of 0.027 which in-
dicates that they are substantially connected.
Parameter Setting Considerable latitude is
available when selecting parameters for the GA.
These involve the population size, crossover, and
mutation rates. To evaluate which setting was best,
we asked two human evaluators to rate (on a 1?5
scale) stories produced with a population size
ranging from 1,000 to 10,000, crossover rate of 0.1
to 0.6 and mutation rate of 0.001 to 0.1. For each
run of the system a limit was set to 5,000 genera-
tions. The human ratings revealed that the best sto-
ries were produced for a population size of 10,000,
a crossover rate of 0.1% and a mutation rate
of 0.1%. Compared to previous work (e.g., Kara-
manis and Manurung 2002) our crossover rate
may seem low and the mutation rate high. How-
ever, it makes intuitively sense, as high crossover
may lead to incoherence by disrupting canonical
action sequences found in the plots. On the other
hand, a higher mutation will raise the likelihood of
a lexical item being swapped for another and may
improve overall coherence and interest. The fit-
ness function was trained on 200 documents from
the fairy tales collection using Joachims?s (2002)
SVMlight package and entity transition sequences
of length 2. The realizer was interfaced with a tri-
gram language model trained on the British Na-
tional Corpus with the SRI toolkit.
5Available from http://homepages.inf.ed.ac.uk/
s0233364/McIntyreLapata09/.
6http://www.essex.ac.uk/psychology/cpwd/
7Clustering rate (or transitivity) is the number of triangles
in the graph ? sets of three vertices each of which is con-
nected to each of the others.
1568
Evaluation We compared the stories gener-
ated by the GA against those produced by the
rank-based system described in McIntyre and La-
pata (2009) and a system that creates stories from
the plot graph, without any stochastic search.
Since plot graphs are weighted, we can simply se-
lect the graph with the highest weight. After ex-
panding all lexical variables, the chosen plot graph
will give rise to different stories (e.g., castle or
temple in the example above). We select the story
ranked highest according to our coherence func-
tion. In addition, we included a baseline which
randomly selects sentences from the training cor-
pus provided they contain either of the story pro-
tagonists (i.e., entities in the input sentence). Sen-
tence length was limited to 12 words or less as this
was on average the length of the sentences gener-
ated by our GA system.
Each system created stories for 12 input sen-
tences, resulting in 48 (4?12) stories for eval-
uation. The sentences used commonly occurring
entities in the fairy tales corpus (e.g., The child
watches the bird, The emperor rules the kingdom.,
The wizard casts the spell.). The stories were split
into three sets containing four stories from each
system but with only one story from each input
sentence. All stories had the same length, namely
five sentences. Human judges were presented with
one of the three sets and asked to rate the stories
on a scale of 1 to 5 for fluency (was the sentence
grammatical?), coherence (does the story make
sense overall?) and interest (how interesting is the
story?). The stories were presented in random or-
der and participants were told that all of them
were generated by a computer program. They were
instructed to rate more favorably interesting sto-
ries, stories that were comprehensible and overall
grammatical. The study was conducted over the
Internet using WebExp (Keller et al, 2009) and
was completed by 56 volunteers, all self reported
native English speakers.
7 Results
Our results are summarized in Table 1 which lists
the average human ratings for the four systems.
We performed an Analysis of Variance (ANOVA)
to examine the effect of system type on the story
generation task. Statistical tests were carried out
on the mean of the ratings shown in Table 1 for
fluency, coherence, and interest.
In terms of interest, the GA-based system is sig-
System Fluency Coherence Interest
GA-based 3.09 2.48 2.36
Plot-based 3.03 2.36 2.14?
Rank-based 1.96?? 1.65? 1.85?
Random 3.10 2.23? 2.20?
Table 1: Human evaluation results: mean story
ratings for four story generators; ? : p < 0.05,
?? : p < 0.01, significantly different from
GA-based system.
nificantly better than the Rank-based, Plot-based
and Random ones (using a Post-hoc Tukey test,
? < 0.05). With regard to fluency, the Rank-
based system is significantly worse than the rest
(? < 0.01). Interestingly, the sentences generated
by the GA and Plot-based systems are as fluent as
those created by humans. Recall that the Random
system, simply selects sentences from the train-
ing corpus. Finally, the GA system is significantly
more coherent than the Rank-based and Random
systems (? < 0.05), but not the Plot-based one.
This is not surprising, the GA and Plot-based sys-
tems rely on similar plots to create a coherent
story. The performance of the Random system is
also inferior as it does not have any explicit coher-
ence enforcing mechanism. The Rank-based sys-
tem is perceived overall worse. As this system is
also the least fluent, we conjecture that partici-
pants are influenced in their coherence judgments
by the grammaticality of the stories.
Overall our results indicate that an explicit story
planner improves the quality of the generated sto-
ries, especially when coupled with a search mech-
anism that advantageously explores the search
space. It is worth noting that the Plot-based sys-
tem is relatively simple, however the explicit use
of a story plot, seems to make up for the lack of
sophisticated search and more elaborate linguis-
tic information. Example stories generated by the
four systems are shown in Table 2 for the input
sentences The emperor rules the kingdom and The
child watches the bird.
Possible extensions and improvements to the
current work are many and varied. Firstly, we
could improve the quality of our plot graphs by
taking temporal knowledge into account and mak-
ing use of knowledge bases such as WordNet
and ConceptNet (Liu and Davenport, 2004), a
freely available commonsense knowledge base.
Secondly, our fitness function optimizes one ob-
1569
P
lo
tG
A
The emperor rules the kingdom. The kingdom
holds on to the emperor. The emperor rides
out of the kingdom. The kingdom speaks out
against the emperor. The emperor lies.
The child watches the bird. The bird weeps
for the child. The child begs the bird to lis-
ten.The bird dresses up the child. The child
grows up.
P
lo
t
The emperor rules the kingdom. The emperor
takes over. The emperor goes on to feel for the
kingdom. Possibly the emperor sleeps. The
emperor steals.
The child watches the bird. The bird comes
to eat away at the child. The child does thor-
oughly. The bird sees the child. The child sits
down.
R
an
k
The emperor rules the kingdom. The kingdom
lives from the reign to the emperor. The em-
peror feels that the brothers tempt a beauty
into the game. The kingdom saves the life
from crumbling the earth into the bird. The
kingdom forces the whip into wiping the tears
on the towel.
The child watches the bird. The bird lives
from the reign to the child. The child thanks
the victory for blessing the thought. The child
loves to hate the sun with the thought. The
child hopes to delay the duty from the happi-
ness.
R
an
do
m
Exclaimed the emperor when Petru had put
his question. In the meantime, mind you take
good care of our kingdom. At first the em-
peror felt rather distressed. The dinner of an
emperor! Thus they arrived at the court of the
emperor.
They cried, ?what a beautiful child!? ?No,
that I cannot do, my child? he said at last.
?What is the matter, dear child?? ?You wicked
child,? cried the Witch. Well, I will watch till
the bird comes.
Table 2: Stories generated by a system that uses plots and genetic search (PlotGA), a system that uses
only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly
pastes together sentences from the training corpus (Random).
jective, namely coherence. In the future we plan to
explore multiple objectives, such as whether the
story is verbose, readable (using existing readabil-
ity metrics), has two many or two few protago-
nists, and so on.
Thirdly, our stories would benefit from some ex-
plicit modeling of discourse structure. Although
the plot graph captures the progression of the ac-
tions in a story, we would also like to know where
in the story these actions are likely to occur?
some tend to appear in the beginning and others in
the end. Such information would allow us to struc-
ture the stories better and render them more natu-
ral sounding. For example, an improvement would
be the inclusion of proper endings, as the stories
are currently cut off at an arbitrary point when the
desired maximum length is reached.
Finally, the fluency of the stories would bene-
fit from generating referring expressions, multiple
tense forms, indirect speech, aggregation and gen-
erally more elaborate syntactic structure.
References
Agudo, Bele?n Dia?z, Pablo Gerva?s, and Fred-
erico Peinado. 2004. A case based reason-
ing approach to story plot generation. In
Proceedings of the 7th European Conference
on Case-Based Reasoning. Springer, Madrid,
Spain, pages 142?156.
Barros, Leandro Motta and Soraia Raupp Musse.
2007. Planning algorithms for interactive story-
telling. In Computers in Entertainment (CIE),
Association for Computing Machinery (ACM),
volume 5.
Barzilay, Regina and Mirella Lapata. 2007. Mod-
eling local coherence: An entity-based ap-
proach. Computational Linguistics 34(1):1?34.
Briscoe, E. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation. Las Pal-
mas, Gran Canaria, pages 1499?1504.
Chambers, Nathanael and Dan Jurafsky. 2008.
Unsupervised learning of narrative event chains.
In Proceedings of 46th Annual Meeting of the
Association for Computational Linguistics: Hu-
man Language Technologies. Columbus, Ohio,
pages 789?797.
Chambers, Nathanael and Dan Jurafsky. 2009.
1570
Unsupervised learning of narrative schemas and
their participants. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.
Singapore, pages 602?610.
Cheng, Hua and Chris Mellish. 2000. Captur-
ing the interaction between aggregation and text
planning in two generation systems. In Pro-
ceedings of the 1st International Conference on
Natural Language Generation. Mitzpe Ramon,
Israel, pages 186?193.
Fellbaum. 1998. WordNet: An Electronic Lexi-
cal Database (Language, Speech, and Commu-
nication). The MIT Press, Cambridge, Mas-
sachusetts.
Fillmore, Charles J., Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicogra-
phy 16:235?250.
Goldberg, David E. 1989. Genetic Algorithms
in Search, Optimization and Machine Learning.
Addison-Wesley Longman Publishing Co., Inc.,
Boston, Massachusetts.
Grishman, Ralph, Catherine Macleod, and Adam
Meyers. 1994. COMLEX syntax: Building a
computational lexicon. In Proceedings of the
15th COLING. Kyoto, Japan, pages 268?272.
Joachims, Thorsten. 2002. Optimizing search en-
gines using clickthrough data. In Proceed-
ings of the 8th Proceedings of the eighth ACM
SIGKDD international conference on Knowl-
edge discovery and data mining. Edmonton, Al-
berta, pages 133?142.
Karamanis, Nikiforos and Hisar Maruli Manu-
rung. 2002. Stochastic text structuring using
the principle of continuity. In Proceedings of
the 2nd International Natural Language Gener-
ation Conference (INLG?02). pages 81?88.
Keller, Frank, Subahshini Gunasekharan, Neil
Mayo, and Martin Corley. 2009. Timing accu-
racy of web experiments: A case study using the
WebExp software package. Behavior Research
Methods 41(1):1?12.
Knight, Kevin and Vasileios Hatzivassiloglou.
1995. Two-level, many-paths generation. In
Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics
(ACL?95). Cambridge, Massachusetts, pages
252?260.
Korhonen, Y. Krymolowski, A. and E.J. Briscoe.
2006. A large subcategorization lexicon for nat-
ural language processing applications. In Pro-
ceedings of the 5th LREC. Genova, Italy.
Lavoie, Benoit and Owen Rambow. 1997. A fast
and portable realizer for text generation sys-
tems. In Proceedings of the 5th Conference on
Applied Natural Language Processing. Wash-
ington, D.C., pages 265?268.
Lin, Dekang. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of
the 17th International Conference on Compu-
tational Linguistic. Montreal, Quebec, pages
768?774.
Liu, Hugo and Glorianna Davenport. 2004. Con-
ceptNet: a practical commonsense reasoning
toolkit. BT Technology Journal 22(4):211?226.
Liu, Hugo and Push Singh. 2002. Using com-
monsense reasoning to generate stories. In Pro-
ceedings of the 18th National Conference on Ar-
tificial Intelligence. Edmonton, Alberta, pages
957?958.
McIntyre, Neil and Mirella Lapata. 2009. Learn-
ing to tell tales: A data-driven approach to story
generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP. Singa-
pore, pages 217?225.
Meehan, James. 1977. An interactive program that
writes stories. In Proceedings of the 5th In-
ternational Joint Conference on Artificial Intel-
ligence. Cambridge, Massachusetts, pages 91?
98.
Mellish, Chris, Alisdair Knott, Jon Oberlander,
and Mick O?Donnell. 1998. Experiments using
stochastic search for text planning. In Proceed-
ings of the 9th International Conference on Nat-
ural Language Generation. New Brunswick,
New Jersey, pages 98?107.
Mitchell, Melanie. 1998. An Introduction to Ge-
netic Algorithms. MIT Press, Cambridge, Mas-
sachusetts.
Pizzi, David, Fred Charles, Jean-Luc Lugrin, and
Marc Cavazza. 2007. Interactive storytelling
with literary feelings. In Proceedings of the 2nd
International Conference on Affective Comput-
ing and Intelligent Interaction. Lisbon, Portu-
gal, pages 630?641.
1571
Reiter, E and R Dale. 2000. Building Natural-
Language Generation Systems. Cambridge
University Press, Cambridge, UK.
Riedl, Mark O. and R. Michael Young. 2004. A
planning approach to story generation and his-
tory education. In Proceedings of the 3rd In-
ternational Conference on Narrative and Inter-
active Learning Environments. Edinburgh, UK,
pages 41?48.
Shim, Yunju and Minkoo Kim. 2002. Automatic
short story generator based on autonomous
agents. In Proceedings of the 5th Pacific Rim In-
ternational Workshop on Multi Agents. Tokyo,
pages 151?162.
Swartjes, I.M.T. and M. Theune. 2008. The vir-
tual storyteller: story generation by simulation.
In Proceedings of the 20th Belgian-Netherlands
Conference on Artificial Intelligence, BNAIC
2008. Enschede, the Netherlands, pages 257?
264.
Turner, Scott R. 1992. Ministrel: A Computer
Model of Creativity and Sotrytelling. University
of California, Los Angeles, California.
Wu, Zhibiao and Martha Palmer. 1994. Verb se-
mantics and lexical selection. In Proceedings
of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics. Las Cruces,
New Mexico, pages 133?138.
1572
