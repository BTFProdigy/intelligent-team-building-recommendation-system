Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 193?200
Manchester, August 2008
Re-estimation of Lexical Parameters for Treebank PCFGs
Tejaswini Deoskar
Department of Linguistics
Cornell University
td72@cornell.edu
Abstract
We present procedures which pool lexical
information estimated from unlabeled data
via the Inside-Outside algorithm, with lex-
ical information from a treebank PCFG.
The procedures produce substantial im-
provements (up to 31.6% error reduction)
on the task of determining subcategoriza-
tion frames of novel verbs, relative to a
smoothed Penn Treebank-trained PCFG.
Even with relatively small quantities of
unlabeled training data, the re-estimated
models show promising improvements in
labeled bracketing f-scores on Wall Street
Journal parsing, and substantial benefit
in acquiring the subcategorization prefer-
ences of low-frequency verbs.
1 Introduction
In order to obtain the meaning of a sentence au-
tomatically, it is necessary to have access to its
syntactic analysis at some level of complexity.
Many NLP applications like translation, question-
answering, etc. might benefit from the avail-
ability of syntactic parses. Probabilistic parsers
trained over labeled data have high accuracy on in-
domain data: lexicalized parsers get an f-score of
up to 90.0% on Wall Street Journal data (Charniak
and Johnson (2005)?s re-ranking parser), while re-
cently, unlexicalized PCFGs have also been shown
to perform much better than previously believed
(Klein and Manning, 2003). However, the limited
size of annotated training data results in many pa-
rameters of a PCFG being badly estimated when
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
trained on annotated data. The Zipfian nature of
a text corpus results in PCFG parameters related
to the properties of specific words being espe-
cially badly estimated. For instance, about 38% of
verbs in the training sections of the Penn Treebank
(PTB) (Marcus et al, 1993) occur only once ? the
lexical properties of these verbs (such as their most
common subcategorization frames ) cannot be rep-
resented accurately in a model trained exclusively
on the Penn Treebank.
The research reported here addresses this issue.
We start with an unlexicalized PCFG trained on
the PTB. We then re-estimate the parameters of
this PCFG from raw text using an unsupervised
estimation method based on the Inside-Outside al-
gorithm (Lari and Young, 1990), an instance of
the Expectation Maximization algorithm (Demp-
ster et al, 1977) for PCFG induction. The re-
estimation improves f-score on the standard test
section of the PTB significantly. Our focus is on
learning lexical parameters i.e. those parameters
related to the lexico-syntactic properties of open-
class words. Examples of such properties are: sub-
categorization frames of verbs and nouns, attach-
ment preference of adverbs to sentential, verbal or
nominal nodes, attachment preference of PPs to a
verbal or nominal node, etc.
The current research is related to semi-
supervised training paradigms like self-training ?
these methods are currently being explored to im-
prove the performance of existing PCFG models
by utilizing unlabeled data. For example, Mc-
Closkey et al (2006) achieve a 1.1% improvement
in labeled bracketing f-score by the use of un-
labeled data to self-train the parser-reranker sys-
tem from Charniak and Johnson (2005). Ear-
lier research on inside-outside estimation of PCFG
models has reported some positive results as well
193
(Pereira and Schabes, 1992; Carroll and Rooth,
1998; Beil et al, 1999; imWalde, 2002). In
some of these cases, an initial model is derived
by other means ? inside-outside is used to re-
estimate the initial model. However, many ques-
tions still remain open about its efficacy for PCFG
re-estimation. Grammars used previously have
not been treebank grammars (for e.g., Carroll and
Rooth (1998) and Beil et al (1999) used hand-
crafted grammars), hence these models could not
be evaluated according to standardized evaluations
in the parsing literature. In the current work, we
use a Penn Treebank based grammar; hence all re-
estimated grammars can be evaluated using stan-
dardized criteria.
The rest of the paper is organized as follows:
First, we describe in brief the construction of an
unlexicalized PCFG from the PTB. We then de-
scribe a procedure based on the inside-outside al-
gorithm to re-estimate the lexical parameters of
this PCFG from unlabeled Wall Street Journal
data. Finally, we present evaluations of the re-
estimated models, based on labeled bracketing
measures and on the detection of subcategorization
frames of verbs: there is a 31.6% reduction in er-
ror for novel verbs and up to 8.97% reduction in
overall subcategorization error.
2 Unlexicalized treebank PCFG
We build an unlexicalized PCFG from the stan-
dard training sections of the PTB. As is common
(Collins, 1997; Johnson, 1998; Klein and Man-
ning, 2003; Schmid, 2006), the treebank is first
transformed in various ways, in order to give an ac-
curate PCFG. In our framework, treebank trees are
augmented with extra features; the methodology
involves constructing a feature-constraint grammar
from a context-free treebank backbone grammar.
The detailed methodology is described in Deoskar
and Rooth (2008)1. A PCFG is trained on the
transformed treebank, with these added features
incorporated into the PCFG?s non-terminal cate-
gories. The framework affords us the flexibility
to stipulate the features to be incorporated in the
PCFG categories, as parameters of the PCFG.
Our features are largely designed to have a
linguistically relevant interpretation2 . For exam-
1The reason for using this framework (as opposed to using
available unlexicalized PCFGs) is that it allows us flexibility
in designing features of interest, and can also be used for lan-
guages other than English with existing treebanks.
2In addition we also have some features that do not have a
this paper Schmid K&M
Recall 86.5 86.3 85.1
Precision 86.7 86.9 86.3
F-score 86.6 86.6 85.7
Table 1: Labeled bracketing scores, PTB sec. 23.
ple, there is a feature on verbs which denotes the
subcategorization frame of the verb (with values
like intransitive, transitive, etc.). Similarly, there
are features which denote the type of clause (fi-
nite, infinite, small clause, etc.), the subject type
of clausal nodes, the attachment of adverbs, va-
lence of nouns, etc. Unlike most existing treebank
PCFGs, all PTB function tags are retained, as are
all empty categories.
As a measure of the quality of the transformed-
PTB based PCFG, Table 1 gives the labeled brack-
eting scores on the standard test section 23 of
the PTB, comparing them to unlexicalized PCFG
scores in (Schmid, 2006) and (Klein and Man-
ning, 2003) (K&M). The current PCFG f-score is
comparable to the state-of-the-art in unlexicalized
PCFGs ((Schmid, 2006), to our knowledge). We
stopped grammar development when the f-score
reached state-of-the-art since our goal was to use
this grammar as the initial model and baseline
for the unsupervised re-estimation procedure, de-
scribed in the next section.
3 Inside-Outside Re-estimation
As a basic unsupervised estimation method, we
use standard inside-outside estimation of PCFGs,
which realizes EM estimation (Lari and Young,
1990; Pereira and Schabes, 1992). We use the
notation I(C, e) to designate the new frequency
model, computed via inside-outside from the cor-
pus C by using a probability model based on the
frequency model e3. The iterative inside-outside
re-estimation procedure has the following sim-
ple form (Eq.1), where each successive frequency
model e
i+1
is estimated from the corpus C using a
probability model determined by the previous fre-
quency model e
i
. Our notation always refers to fre-
linguistic interpretation, but result in a good PCFG, such as a
parent feature on some categories, following Johnson (1998).
3The inside-outside algorithm uses an existing grammar
model and a raw text corpus (incomplete data) to obtain cor-
responding complete data (a set of analyses/parses for the cor-
pus sentences). A new grammar model is then estimated from
this complete data. See (Prescher, 2003) for an explanation
using the standard EM notions of incomplete/complete data.
194
quency models such as e
i
, rather than the relative-
frequency probability models they determine4.
e
1
= I(C, e
0
)
...
e
i+1
= I(C, e
i
)
(1)
3.1 Interleaved Inside-Outside
It is well-known that while lexicalization is use-
ful, lexical parameters determined from the tree-
bank are poorly estimated because of the sparse-
ness of treebank data for particular words (e.g.
Hindle and Rooth (1993)). Gildea (2001) and
Bikel (2004) show that removing bilexical de-
pendencies hardly hurts the performance of the
Collins Model2 parser, although there is the ben-
efit of lexicalization in the form of lexico-syntactic
dependencies ? structures being conditioned on
words. On the other hand, structural parameters
are comparatively well-estimated from treebanks
since they are not keyed to particular words. Thus,
it might be beneficial to use a combination of su-
pervised and unsupervised estimation for lexical
parameters, while obtaining syntactic (structural)
parameters solely by supervised estimation (i.e.
from a treebank). The experiments in this paper
are based on this idea. In an unlexicalised PCFG
like the one described in ?2, it is easy to make
the distinction between structural parameters (non-
terminal rules) and lexical parameters (preterminal
to terminal rules).
To this end, we define a modified inside-outside
procedure in which a frequency transformation
T (c, t) is interleaved between the iterations of the
standard inside-outside procedure. The form of
this interleaved procedure is shown in Eq. 2. In
Eq. 2, t designates a smoothed treebank model (the
smoothing procedure is described later in ?3.1.1).
This smoothed treebank model is used as the prior
model for the inside-outside re-estimation proce-
dure. For each iteration i, c
i
represent models ob-
tained by inside-outside estimation. d
i
represent
derived models obtained by performing a transfor-
mation T on c
i
. The transformation T combines
the re-estimated model c
i
and the smoothed tree-
4We use a frequency-based notation because we use out-
of-the-box software Bitpar (Schmid, 2004) which implements
inside-outside estimation ? Bitpar reads in frequency models
and converts them to relative frequency models. We justify
the use of the frequency-based notation by ensuring that all
marginal frequencies in the treebank model are always pre-
served in all other models.
bank model t (hence represented as T (c
i
, t)).
d
0
= t smoothed treebank model
c
1
= I(C, d
0
) estimation step
d
1
= T (c
1
, t) transformation step
...
c
i+1
= I(C, d
i
) estimation step
d
i+1
= T (c
i+1
, t) transformation step
(2)
The lexical parameters for the treebank model t
or the re-estimated models c
i
are represented as
t(w, ?, ?) or c
i
(w, ?, ?), where w is the terminal
word, ? is the PTB-style PoS tag, and ? is the
sequence of additional features incorporated into
the PoS tag (the entries in our lexicon have the
form w.?.? with an associated frequency). The
transformation T preserves the marginal frequen-
cies seen in the treebank model. A marginal tag-
incorporation frequency is defined by summation:
f(?, ?) =
?
w
f(w, ?, ?). (3)
The transformation T is used to obtain the derived
models d
i
and consists of two parts, corresponding
to the syntactic and the lexical parameters of d
i
:
? The syntactic parameters of d
i
are copied
from t.
? To obtain the lexical parameters of d
i
, lex-
ical parameters from the treebank model t
and lexical parameters from the re-estimated
model are linearly combined, shown in Eq. 4.
d
i
(w, ?, ?) = (1? ?
?,?
)t(w, ?, ?) + ?
?,?
c?
i
(w, ?, ?)
(4)
where ?
?,?
is a parameter with 0 < ?
?,?
< 1 which
may depend on the tag and incorporation. The
term c?
i
(w, ?, ?) in Eq. 4 is obtained by scaling the
frequencies in c
i
(w, ?, ?), as shown in Eq. 5.
c?
i
(w, ?, ?) =
t(?, ?)
c
i
(?, ?)
c
i
(w, ?, ?). (5)
In terms of probability models determined from
the frequency models, the effect of T is to allocate
a fixed proportion of the probability mass for each
?, ? to the corpus, and share it out among words w
in proportion to relative frequencies ci(w,?,?)
c
i
(?,?)
in the
inside-outside estimate c
i
. Eqs. 6 and 7 verify that
marginals are preserved in the derived model d.
c?(?, ?) =
?
w
c?(w, ?, ?) =
?
w
t(?,?)
c(?,?)
c(w, ?, ?)
=
t(?,?)
c(?,?)
?
w
c(w, ?, ?)
=
t(?,?)
c(?,i)
c(?, ?) = t(?, ?).
(6)
195
d(?, ?) =
?
w
d(w, ?, ?)
=
?
w
(1? ?
?,?
)t(w, ?, ?) + ?
?,?
c?(w, ?, ?)
= (1? ?
?,?
)
?
w
t(w, ?, ?)
+ ?
?,?
?
w
c?(w, ?, ?)
= (1? ?
?,?
)t(?, ?) + ?
?,?
c?(?, ?)
= (1? ?
?,?
)t(?, ?) + ?
?,?
t(?, ?)
= t(?, ?).
(7)
3.1.1 Smoothing the treebank model
To initialize the iterative procedures, a smooth-
ing scheme is required which allocates frequency
to combinations of words w and PoS tags ? which
are not present in the treebank model but are
present in the corpus, and also to all possible in-
corporations of a PoS tag. Otherwise, if the un-
smoothed treebank model (t
0
) has zero frequency
for some lexical parameter, the inside-outside es-
timate I(C, t
0
) for that parameter would also be
zero, and new lexical entries would never be in-
duced.
The smoothed treebank model t is obtained from
the unsmoothed model t
0
as follows. First a PoS
tagger (Treetagger, (Schmid, 1994)) is run on the
unsupervised corpus C , which assigns PTB-style
PoS tags to the corpus. Tokens of words and
PoS tags are tabulated to obtain a frequency table
g(w, ?). Each frequency g(w, ?) is split among
possible incorporations ? in proportion to a ratio of
marginal frequencies in t
0
:
g(w, ?, ?) =
t
0
(?, ?)
t
0
(?)
g(w, ?) (8)
The smoothed model t is defined as an interpola-
tion of g and t
0
for lexical parameters as shown in
9, with syntactic parameters copied from t
0
.
t(w, ?, ?) = (1? ?
?,?
)t
0
(w, ?, ?) + ?
?,?
g(w, ?, ?)
(9)
3.2 Experimental setup
The treebank grammar is trained over sections 0-
22 of the transformed PTB (minus about 7000 sen-
tences held out for testing). Testset I contains 1331
sentences and is constructed as follows: First, we
select 117 verbs whose frequency in PTB sections
0-22 is between 10-20 (mid-frequency verbs). All
sentences containing occurrences of these verbs
are held out from the training data to form Test-
set I. The effect of holding out these sentences is
to make these 117 verbs novel (i.e. unseen in train-
ing). This testset is used to evaluate the learning of
subcategorization frames of novel verbs. We also
construct another testset (Testset II) by holding out
every 10th sentence in PTB sections 0-22 (4310
sentences).
The corpus used for re-estimation is about 4 mil-
lion words of unannotated Wall Street Journal text
(year 1997) (sentence length<25 words). The re-
estimation was carried out using Bitpar (Schmid,
2004) for inside-outside estimation. The parame-
ter ? in Eq. 4 was set to 0.5 for all ? and ?, giving
equal weight to the treebank and the re-estimated
lexicons. Starting from a smoothed treebank gram-
mar t, we separately ran 6 iterations of the inter-
leaved estimation procedure defined in Eq. 2, and
4 iterations of standard inside-outside estimation.
This gave us two series of models corresponding
to the two procedures.
4 Labeled Bracketing Results
As a basic evaluation of the re-estimated gram-
mars, we report the labeled bracketing scores on
the standard test section 23 of the PTB (Table 2).
Using the re-estimated models, maximum proba-
bility (viterbi) parses were obtained for all sen-
tences in sec. 23, after stripping away the treebank
annotation, including the pre-terminal tag. The
baseline is the treebank model t
0t
5
. The scores
for re-estimated grammars from successive itera-
tions are under columns It 1, It 2, etc. All models
obtained using the interleaved procedure show an
improvement over the baseline. The best model
is obtained after 2 iterations, after which the score
reduces a little. Statistically significant improve-
ments are marked with *, with p<0.005 for recall
and p<0.0001 for precision for the best model. Ta-
ble 2 also shows scores for grammars estimated
using the standard inside-outside procedure. The
first re-estimated model is better than any model
obtained from either procedure. Notice however,
the disparity in precision and recall ? precision
is much lower than recall. This is not surpris-
ing; inside-outside is known to converge to incor-
rect solutions for PCFGs (Lari and Young, 1990;
de Marcken, 1995). This causes the f-score to de-
teriorate in successive iterations.
5This baseline is slightly lower than that reported in Ta-
ble 1 due to holding out an additional 7000 sentences from
the treebank training set. In order to accommodate unknown
words from the test data (sec 23), the treebank model t
0
is
smoothed in a manner similar to that shown in Eq. 9, with
the test words (tagged using Treetagger) forming g(w, ? ) and
? = 0.1. A testset is always merged with a given model in
this manner before parsing, to account for unknown words.
196
t0t
It 1 It 2 It 3 It 4 It 5 It 6
Interleaved Recall 86.48 86.72 *86.79 *86.79 *86.78 86.81 86.72
Procedure Precision 86.61 86.95 *87.07 *87.06 *87.07 87.04 87.01
f-score 86.55 86.83 *86.93 *86.92 *86.92 86.92 86.86
Standard Recall 86.48 87.95 87.11 86.42 85.55
Procedure Precision 86.61 85.99 84.79 83.37 82.06
f-score 86.5 86.96 85.93 84.87 83.77
Table 2: Labeled Bracketing scores for various models, on PTB section 23.
The improvement in labeled bracketing f-score
for the interleaved procedure is small, but is an en-
couraging result. The benefit to the re-estimated
models comes only from better estimates of lex-
ical parameters. We expect that re-estimation
will benefit parameters associated with low fre-
quency words - lexical parameters for high fre-
quency words are bound to be estimated accurately
from the treebank. We did not expect a large im-
pact on labeled bracketing scores, given that low
frequency words have correspondingly few occur-
rences in this test dataset. It is possible that the im-
pact on f-score will be higher for a test set from a
different domain. Note also that the size of our un-
labeled training corpus (?4M words) is relatively
small ? only about 4 times the PTB.
5 Verbal Subcategorization
We focus on learning verbal subcategorization, as
a typical case of lexico-syntactic information. The
subcategorization frame (SF) of verbs is a parame-
ter of our PCFG - verbal tags in the PCFG are fol-
lowed by an incorporation sequence that denotes
the SF for that verb. We evaluate the re-estimated
models on the task of detecting correct SFs of verbs
in maximum-probability (viterbi) parses obtained
using the models. All tokens of verbs and their
preterminal symbols (consisting of a PoS tag and
an incorporation sequence encoding the SF) are ex-
tracted from the viterbi parses of sentences in a
testset. This tag-SF sequence is compared to a gold
standard, and is scored correct if the two match ex-
actly. PoS errors are scored as incorrect, even if the
SF is correct. The gold standard is obtained from
the transformed PTB trees.
The incorporation sequence corresponding to
the SF consists of 3 features: The first one denotes
basic categories of subcategorization such as tran-
sitive, intransitive, ditransitive, NP-PP, S, etc. The
second feature denotes, for clausal complements,
the type of clause (finite, infinite, small clause,
Figure 1: A subcat. frame for control verb want.
etc.). The third feature encodes the nature of the
subject of the clausal complements (empty cate-
gory or non-empty). For example, the verb con-
sidered in the treebank sentence They are officially
considered strategic gets a preterminal sequence of
VBD.s.e.sc. This sequence indicates a past tense
verb (VBD) with a clausal complement (s) which
has an empty subject (e) since the sentence is pas-
sive and is of the type small clause (sc). A control
verb (with an infinitival complement) in the sen-
tence fragment ..did not want to fund X.. gets the
frame s.e.to (see Fig. 1 for an example of a verb
with its complement, as parsed by our PCFG). We
have a total of 81 categories of SFs (without count-
ing specific prepositions for prepositional frames),
making fairly fine-grained distinctions of verbal
categories.
5.1 Learning Subcat Frames of Novel Verbs
We measure the error rate in the detection of the
subcategorization frame of 1360 tokens of 117
verbs in Testset I. Recall from ?3.2 that these
verbs are novel verbs with respect to the treebank
model. Table 3 shows this error rate (i.e. the
fraction of test items which receive incorrect tag-
incorporations in viterbi parses) for various mod-
els obtained using the interleaved and standard re-
estimation procedures. t
0t1
is the treebank model
t
0
with the test data from Testset I merged in (to
197
Iteration i Interleaved Standard
Procedure Procedure
t
0t1
33.36 33.36
1 *24.40 28.69
2 *23.45 25.56
3 *23.05 27.86
4 *22.89 28.41
5 *22.81 -
6 *22.83 -
Table 3: Subcat. error for novel verbs (Testset I).
account for unknown words) using the smoothing
scheme given in Eq. 9. This model has no verb
specific information for the test verbs. For each
test verb, it has a smoothed SF distribution pro-
portional to the SF distribution for all verbs of that
tag. The baseline error is 33.36%. This means that
there is enough information in the average distri-
bution of all verbs to correctly assign the subcat-
egorization frame to novel verbs in 66.64% cases.
For the models obtained using the interleaved re-
estimation, the error rate falls to the lowest value
of 22.81% for the model obtained in the 5th iter-
ation: an absolute reduction of 10.55 points, and
a percentage error-reduction of 31.6%. The er-
ror reduction is statistically significant for all it-
erations compared to the baseline, with the 5th it-
eration being also significantly better than the 1st.
The models obtained using standard re-estimation
do not perform as well. Even for the model from
the first iteration, whose labeled bracketing score
was highest, the SF error is higher than the cor-
responding model from the interleaved procedure
(possibly due to the low precision of this model).
The error rate for the standard procedure starts to
increase after the 2nd iteration in contrast to the in-
terleaved procedure.
5.2 Analysis of subcategorization learning
While the re-estimation clearly results in gains in
SF detection for novel verbs, we also perform an
evaluation for all verbs (novel and non-novel) in a
given testset (Testset II as described in ?3.2). The
overall error reduction using the interleaved proce-
dure is 8.97% (in Iteration 1). In order to better un-
derstand the relative efficacy of the supervised and
unsupervised estimation for lexical items of differ-
ent frequencies, we break up the set of test verbs
into subsets based on their frequency of occurrence
in the PTB training data, and evaluate them sepa-
TB Freq t
0t2
It 1 Abs.Reduc %Reduc
all 18.5 16.84 1.66 *8.97
0 41.26 33.01 8.25 *19.99
1 32.69 24.52 8.17 *24.99
2 36.55 22.76 13.79 *37.73
3 26.59 19.08 7.51 *28.24
4 22.38 20.28 2.1 9.38
5 24.63 19.40 5.23 *21.23
6-10 22.24 19.59 2.65 **11.92
11-20 21.54 18.02 3.52 *16.34
21-50 19.41 19.11 0.3 1.55
51-100 19.44 19.09 0.35 1.80
101-200 18.71 18.57 0.14 0.75
201-500 23.06 22.31 0.75 3.25
501-1K 18.07 16.82 1.25 6.92
1K-2K 12.38 12.25 0.13 1.05
2K-5K 9.42 7.62 1.8 *19.11
>5K 10.54 10.13 0.41 3.89
Table 4: Subcat. error breakup (Testset II)
rately. Table 4 shows the error rates for verbs di-
vided into these sets. We present error rates only
for Iteration 1 in Table 4, since most of the error
reduction takes place with the 1st iteration. Sta-
tistically significant reductions are marked with *
(confidence>99.9) and ** (>95). The second row
shows error rates for verbs which have zero fre-
quency in the treebank training data (i.e. novel
verbs): Note that this error reduction is much less
than the 31.6% in Testset I. These verbs are truly
rare and hence have much fewer occurrences in
the unlabeled corpus than Testset I verbs, which
were artificially made novel (but are really mid-
frequency verbs). This might indicate that error
rates will decrease further if the size of the unla-
beled corpus is increased. There is substantial er-
ror reduction for low-frequency verbs (<21 PTB
occurrences). This is not hard to understand: the
PTB does not provide enough data to have good
parameter estimates for these verbs. For mid-to-
high frequency verbs (from 21 to 500), the benefit
of the unsupervised procedure reduces, though er-
ror reduction is still positive. Surprisingly, the er-
ror reduction for very high frequency verbs (more
than 500 occurrences in the treebank) is also fairly
high: we expected that parameters for high fre-
quency words would benefit the least from the un-
supervised estimation, given that they are already
common enough in the PTB to be accurately esti-
mated from it. The high frequency verbs (>500
198
occurrences) consist of very few types? mainly
auxiliaries, some light verbs (make, do) and a few
others (rose, say). It is possible that re-estimation
from large data is beneficial for light verbs since
they have a larger number of frames. The fre-
quency range 2K-5K consists solely of auxiliary
verbs. Examination of viterbi parses shows that
improved results are largely due to better detection
of predicative frames in re-estimated models.
To measure the impact of more unlabeled train-
ing data, we ran the interleaved procedure with 8M
words of WSJ text. The SF error for novel verbs re-
duces to 22.06% in the 2nd iteration (significantly
different from the best error of 22.81% in the 5th
iteration for 4M words of training data)). We also
get an improved overall error reduction of 9.9% on
Testset II for the larger training data, as compared
to 8.97% previously.
5.3 Previous Work
While there has been substantial previous work
on the task of SF acquisition from corpora (Brent
(1991); Manning (1993); Briscoe and Carroll
(1997); Korhonen (2002), amongst others), we find
that relatively few parsing-based evaluations are
reported. Since their goal is to build probabilistic
SF dictionaries, these systems are evaluated either
against existing dictionaries, or on distributional
similarity measures. Most are evaluated on testsets
of high-frequency verbs (unlike the present work),
in order to gauge the effectiveness of the acquisi-
tion strategy. Briscoe and Carroll (1997) report a
token-based evaluation for seven verb types? their
system gets an average recall accuracy of 80.9%
for these verbs (which appear to be high-frequency
verbs). This is slightly lower than the present sys-
tem, which has an overall accuracy of 83.16% (on
Testset II (It 1), as shown in Table 4). However,
for low frequency verbs (exemplars <10) they re-
port that their results are around chance. A parsing
evaluation of their lexicon using an unlexicalized
grammar as baseline, on 250 sentences from the
Suzanne treebank gave a small (but not statistically
significant) improvement in f-score (from 71.49 to
72.14%). Korhonen (2002) reports a parsing-based
evaluation on 500 test sentences. She found a
small increase in f-score (of grammatical relations
markup) from 76.03 to 76.76. In general PARSE-
VAL measures are not very sensitive to subcatego-
rization (Carroll et al, 1998); they therefore use
a dependency-based evaluation. In the present re-
search as well, we obtain statistically significant
but quite small improvements in f-score (?4). Since
we are interested in acquisition of PCFG lexicons,
we focus our evaluations on verbal subcategoriza-
tion of token occurrences of verbs in viterbi parses.
6 Conclusions
We have presented a methodology for incorporat-
ing additional lexical information from unlabeled
data into an unlexicalized treebank PCFG. We ob-
tain a large error reduction (31.6%) in SF detection
for novel verbs as compared to a treebank base-
line. The interleaved re-estimation scheme gives
a significant increase in labeled bracketing scores
from a relatively small unlabeled corpus. The in-
terleaved scheme has an advantage over standard
inside-outside PCFG estimation, as measured both
by labeled bracketing scores and on the task of de-
tecting SFs of novel verbs. Since our re-estimated
models are treebank models, all evaluations are
against treebank standards.
The grammar we worked with has very few in-
corporated features compared to the grammar used
by, say Klein and Manning (2003). It would make
sense to experiment with grammars with much
richer sets of incorporated features. Features re-
lated to structure-selection by categories other than
verbs ? nouns, adverbs and adjectives ? might be
beneficial. These features should be incorporated
as PCFG parameters, similar to verbal subcate-
gorization. Experiments with 8 million words of
training data gave significantly better results than
with 4 million words, indicating that larger train-
ing sets will be beneficial as well. It would also be
useful to make the transformation T of lexical pa-
rameters sensitive to treebank frequency of words.
For instance, more weight should be given to the
treebank model rather than the corpus model for
mid-to-high frequency words, by making the pa-
rameter ? in T sensitive to frequency.
This methodology is relevant to the task of
domain-adaption. Hara et al (2007) find that re-
training a model of HPSG lexical entry assign-
ments is more critical for domain adaptation than
re-training a structural model alone. Our PCFG
captures many of the important dependencies cap-
tured in a framework like HPSG; in addition, we
can use unlabeled data from a new domain in an
unsupervised fashion for re-estimating lexical pa-
rameters, an important consideration in domain-
adaption. Preliminary experiments on this task us-
199
ing New York Times unlabeled data with the PTB-
trained PCFG show promising results.
Acknowledgments
I am grateful to Mats Rooth for extensive com-
ments and guidance during the course of this re-
search. The inside-outside re-estimation was con-
ducted using the resources of the Cornell Univer-
sity Center for Advanced Computing.
References
F. Beil, G. Carroll, D. Prescher, S. Riezler, and
M. Rooth. 1999. Inside-outside estimation of
a lexicalized PCFG for German. In Proceedings
of the 37th meeting of ACL.
Dan Bikel. 2004. Intricacies of Collins? Parser.
Computational Linguistics, 30(4):479?511.
M. Brent. 1991. Automatic acquisition of subcat-
egorization frames from untagged text. In Pro-
ceedings of the 29th meeting of ACL.
Ted Briscoe and John Carroll. 1997. Automatic
Extraction of Subcategorization from Corpora.
In Proceedings of the 5th ACL Conference on
Applied NLP.
G. Carroll and M. Rooth. 1998. Valence induction
with a head-lexicalized PCFG. In Proceedings
of EMNLP 1998.
J. Carroll, G. Minnen, and E. Briscoe. 1998. Can
subcategorization probabilities help parsing. In
Proceedings of 6th ACL/SIGDAT Workshop on
Very Large Corpora.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proceedings of 43rd
meeting of ACL.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th meeting of ACL.
Carl de Marcken. 1995. On the unsupervised in-
duction of Phrase Structure grammars. In 3rd
Workshop on Very Large Corpora.
A. P. Dempster, N. M. Laird, and D. B. Rubin.
1977. Maximum likelihood estimation from in-
complete data via the EM algorithm. J. Royal
Statistical Society, 39(B):1?38.
Tejaswini Deoskar and Mats Rooth. 2008. Induc-
tion of Treebank-Aligned Lexical Resources. In
Proceedings of 6th LREC.
Daniel Gildea. 2001. Corpus Variation and Parser
Performance. In Proceedings of EMNLP 2001.
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evalu-
ating Impact of Re-training a Lexical Disam-
biguation Model on Domain Adaptation of an
HPSG Parser. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies.
Donald Hindle and Mats Rooth. 1993. Structural
ambiguity and lexical relations. Computational
Linguistics, 18(2):103?120.
Sabine Schulte imWalde. 2002. A Subcategori-
sation Lexicon for German Verbs induced from
a Lexicalised PCFG. In Proceedings of LREC
2002.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4).
D. Klein and C. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st ACL.
Anna Korhonen. 2002. Subcategorization Acqui-
sition. Ph.D. thesis, Univ. of Cambridge.
K. Lari and S. J. Young. 1990. The estimation
of stochastic context-free grammars using the
Inside-Outside algorithm. Computer Speech
and Language, 4:35?56.
C. Manning. 1993. Automatic acquisition of a
large subcategorization dictionary from corpora.
In Proceedings of the 31st meeting of ACL.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313?330.
D. McCloskey, E. Charniak, and M. Johnson.
2006. Effective Self-Training for Parsing. In
Proceedings of HLT-NAACL 2006.
Pereira and Schabes. 1992. Inside-Outside re-
estimation from partially bracketed corpora. In
Proceedings of the 30th meeting of ACL.
Detlef Prescher. 2003. A Tutorial on the
Expectation-Maximization Algorithm Includ-
ing Maximum-Likelihood Estimation and EM
Training of Probabilistic Context-Free Gram-
mars. ESSLLI 2003.
Helmut Schmid. 1994. Probabilistic Part-of-
Speech Tagging Using Decision Trees. In Pro-
ceedings of International Conference on New
Methods in Language Processing.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit
Vectors. In Proceedings of the 20th COLING.
Helmut Schmid. 2006. Trace Prediction and Re-
covery with Unlexicalised PCFGs and Slash
Features. In Proceedings of the 21st Conference
on Computational Linguistics (COLING).
200
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214?217,
Paris, October 2009. c?2009 Association for Computational Linguistics
Smoothing fine-grained PCFG lexicons
Tejaswini Deoskar
ILLC
University of Amsterdam
t.deoskar@uva.nl
Mats Rooth
Dept. of Linguistics and CIS
Cornell University
mr249@cornell.edu
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
We present an approach for smoothing
treebank-PCFG lexicons by interpolating
treebank lexical parameter estimates with
estimates obtained from unannotated data
via the Inside-outside algorithm. The
PCFG has complex lexical categories,
making relative-frequency estimates from
a treebank very sparse. This kind of
smoothing for complex lexical categories
results in improved parsing performance,
with a particular advantage in identify-
ing obligatory arguments subcategorized
by verbs unseen in the treebank.
1 Introduction
Lexical scarcity is a problem faced by all sta-
tistical NLP applications that depend on anno-
tated training data, including parsing. One way
of alleviating this problem is to supplement super-
vised models with lexical information from unla-
beled data. In this paper, we present an approach
for smoothing the lexicon of a treebank PCFG
with frequencies estimated from unannotated data
with Inside-outside estimation (Lari and Young,
1990). The PCFG is an unlexicalised PCFG, but
contains complex lexical categories (akin to su-
pertags in LTAG (Bangalore and Joshi, 1999) or
CCG (Clark and Curran, 2004)) encoding struc-
tural preferences of words, like subcategorization.
The idea behind unlexicalised parsing is that the
syntax and lexicon of a language are largely inde-
pendent, being mediated by ?selectional? proper-
ties of open-class words. This is the intuition be-
hind lexicalised formalisms like CCG: here lexical
categories are fine-grained and syntactic in nature.
Once a word is assigned a lexical category, the
word itself is not taken into consideration further
in the syntactic analysis. Fine-grained categories
imply that lexicons estimated from treebanks will
be extremely sparse, even for a language like En-
glish with a large treebank resource like the Penn
Treebank (PTB) (Marcus et al, 1993). Smoothing
a treebank lexicon with an external wide-coverage
lexicon is problematic due to their respective rep-
resentations being incompatible and without an
obvious mapping, assuming that the external lexi-
con is probabilistic to begin with. In this paper, we
start with a treebank PCFG with fine-grained lex-
ical categories and re-estimate its parameters on a
large corpus of unlabeled data. We then use re-
estimates of lexical parameters (i.e. pre-terminal
to terminal rule probabilities) to smooth the orig-
inal treebank lexical parameters by interpolation
between the two. Since the treebank PCFG itself is
used to propose analyses of new data, the mapping
problem is inherently taken care of. The smooth-
ing procedure takes into account the fact that unsu-
pervised estimation has benefits for unseen or low-
frequency lexical items, but the treebank relative-
frequency estimates are more reliable in the case
of high-frequency items.
2 Treebank PCFG
In order to have fine-grained and linguistic lexi-
cal categories (like CCG) within a simple formal-
ism with well-understood estimation methods, we
first build a PCFG containing such categories from
the PTB. The PCFG is unlexicalised (with lim-
ited lexicalization of certain function words, like
in Klein and Manning (2003)). It is created by
first transforming the PTB (Johnson, 1998) in an
appropriate way and then extracting a PCFG from
the transformed trees (Deoskar and Rooth, 2008).
All functional tags in the PTB (such as NP-SBJ,
PP-TMP, etc.) are maintained, as are all empty
categories, making long-distance dependencies re-
coverable. The PCFG is trained on the standard
training sections of the PTB and performs at the
state-of-the-art level for unlexicalised PCFGs, giv-
ing 86.6% f-score on Sec. 23.
214
VP
VB.np
add
NP
four more
Boeings
PP-TMP
by 1994
PP-CLR
to the
two units.
(a) An NP PP subcategorization frame marked on the
verb ?add? as np. Note that the arguments NP and PP-
CLR are part of the subcategorization frame and are
represented locally on the verb but the adjunct PP-
TMP is not.
VP
VBG.s.e.to
seeking
S.e.to
+E-NP+ VP.to
TO
to
VP
avoid..
(b) An S frame on the verb ?seeking?: +E-
NP+ represents the empty subject of the
S. Note that structure internal to S is also
marked on the verb.
VP
Vb.sb
think
SBAR
+C+ S
the consumer
is right
(c) An SBAR frame: +C+ is the
empty complementizer.
Figure 1: Subcategorized structures are marked as features on the verbal POS category.
An important feature of our PCFG is that pre-
terminal categories for open-class items like verbs,
nouns and adverbs are more complex than PTB
POS tags. They encode information about the
structure selected by the lexical item, in effect,
its subcategorization frame. A pre-terminal in our
PCFG consists of the standard PTB POS tag, fol-
lowed by a sequence of features incorporated into
it. Thus, each PTB POS tag can be considered to
be divided into multiple finer-grained ?supertags?
by the incorporated features. These features en-
code the structure selected by the words. We fo-
cus on verbs in this paper, as they are important
structural determiners. A sequence of one or more
features forms the ?subcategorization frame? of a
verb: three examples are shown in Figure 1. The
features are determined by a fully automated pro-
cess based on PTB tree structure and node labels.
There are 81 distinct subcategorization frames for
verbal categories. The process can be repeated for
other languages with a treebank annotated in the
PTB style which marks arguments like the PTB.
3 Unsupervised Re-estimation
Inside-outside (henceforth I-O) (Lari and Young,
1990), an instance of EM, is an iterative estima-
tion method for PCFGs that, given an initial model
and a corpus of unannotated data, produces mod-
els that assign increasingly higher likelihood to
the corpus at each iteration. I-O often leads to
sub-optimal grammars, being subject to the well-
known problem of local maxima, and dependence
on initial conditions (de Marcken, 1995) (although
there have been positive results using I-O as well,
for e.g. Beil et al (1999)). More recently, Deoskar
(2008) re-estimated an unlexicalised PTB PCFG
using unlabeled Wall Street Journal data. They
compared models for which all PCFG parameters
were re-estimated from raw data to models for
which only lexical parameters were re-estimated,
and found that the latter had better parsing results.
While it is common to constrain EM either by
good initial conditions or by heuristic constraints,
their approach used syntactic parameters from a
treebank model to constrain re-estimation of lex-
ical parameters. Syntactic parameters are rela-
tively well-estimated from a treebank, not being as
sparse as lexical parameters. At each iteration, the
re-estimated lexicon was interpolated with a tree-
bank lexicon, ensuring that re-estimated lexicons
did not drift away from the treebank lexicon.
We follow their methodology of constrained
EM re-estimation. Using the PCFG with fine
lexical categories (as described in ?2) as the ini-
tial model, we re-estimate its parameters from an
unannotated corpus. The lexical parameters of
the re-estimated PCFG form its probabilistic ?lex-
icon?, containing the same fine-grained categories
as the original treebank PCFG. We use this re-
estimated ?lexicon? to smooth the lexical proba-
bilities in the treebank PCFG.
4 Smoothing based on a POS tagger : the
initial model.
In order to use the treebank PCFG as an initial
model for unsupervised estimation, new words
from the unannotated training corpus must be in-
cluded in it ? if not, parameter values for new
words will never be induced. Since the treebank
model contains no information regarding correct
feature sequences for unseen words, we assign all
possible sequences that have occurred in the tree-
bank model with the POS tag of the word. We
assign all possible sequences to seen words as
215
well ? although the word is seen, the correct fea-
ture sequence for a structure in a training sentence
might still be unseen with that word. This is done
as follows: a standard POS-tagger (TreeTagger,
(Schmid, 1994)) is used to tag the unlabeled cor-
pus. A frequency table cpos(w, ?) consisting of
words and POS-tags is extracted from the result-
ing corpus, where w is the word and ? its POS
tag. The frequency cpos(w, ?) is split amongst all
possible feature sequences ? for that POS tag in
proportion to treebank marginals t(?, ?) and t(?)
cpos(w, ?, ?) = t(?, ?)t(?) cpos(w, ?) (1)
Then the treebank frequency t(w, ?, ?) and the
scaled corpus frequency are interpolated to get a
smoothed model tpos. We use ?=0.001, giving a
small weight initially to the unlabeled corpus.
tpos(w, ?, ?) = (1? ?)t(w, ?, ?) + ?cpos(w, ?, ?)
(2)
The first term will be zero for words unseen in the
treebank: their distribution in the smoothed model
will be the average treebank distribution over all
possible feature sequences for a POS tag. For
seen words, the treebank distribution over feature
sequence is largely maintained, but a small fre-
quency is assigned to unseen sequences.
5 Smoothing based on EM re-estimation
After each iteration i of I-O, the expected counts
cemi(w, ?, ?) under the model instance at itera-
tion (i ? 1) are obtained. A smoothed treebank
lexicon temi is obtained by linearly interpolating
the smoothed treebank lexicon tpos(w, ?, ?) and a
scaled re-estimated lexicon c?emi(w, ?, ?).
temi(w, ?, ?) = (1??)tpos(w, ?, ?)+?c?emi (w, ?, ?)
(3)
where 0 < ? < 1. The term c?emi(w, ?, ?) is ob-
tained by scaling the frequencies cemi(w, ?, ?) ob-
tained by I-O, ensuring that the treebank lexicon is
not swamped with the large training corpus1.
c?emi(w, ?, ?) = t(?, ?)?
w cemi(w, ?, ?)
cemi(w, ?, ?)
(4)
? determines the relative weights given to the
treebank and re-estimated model for a word. Since
parameters of high-frequency words are likely
to be more accurate in the treebank model, we
parametrize ? as ?f according to the treebank fre-
quency f = t(w, ?).
1Note that in Eq. 4, the ratio of the two terms involving
cemi is the conditional, lexical probability Pemi(w|?, ?).
6 Experiments
The treebank PCFG is trained on sections 0-22 of
the PTB, with 5000 sentences held-out for evalu-
ation. We conducted unsupervised estimation us-
ing Bitpar (Schmid, 2004) with unannotated Wall
Street Journal data of 4, 8 and 12 million words,
with sentence length <25 words. The treebank
and re-estimated models are interpolated with ? =
0.5 (in Eq. 3). We also parametrize ? for treebank
frequency of words ? optimizing over a develop-
ment set gives us the following values of ?f for
different ranges of treebank word frequencies.
if t(w, ?) <= 5 , ?f = 0.5
if 5 < t(w, ?) <= 15 , ?f = 0.25
if 15 < t(w, ?) <= 50 , ?f = 0.05
if t(w, ?) > 50 , ?f = 0.005
(5)
Evaluations are on held-out data from the PTB
by stripping all PTB annotation and obtaining
Viterbi parses with the parser Bitpar. In addition
to standard PARSEVAL measures, we also eval-
uate parses by another measure specific to sub-
categorization2 : the POS-tag+feature sequence on
verbs in the Viterbi parse is compared against the
corresponding tag+feature sequence on the trans-
formed PTB gold tree, and errors are counted. The
tag-feature sequence correlates to the structure se-
lected by the verb, as exemplified in Fig. 1.
7 Results
There is a statistically significant improvement3
in labeled bracketing f-score on Sec. 23 when
the treebank lexicon is smoothed with an EM-re-
estimated lexicon. In Table 1, tt refers to the base-
line treebank model, smoothed using the POS-
tag smoothing method (from ?4) on the test data
(Sec. 23) in order to incorporate new words from
the test data4. tpos refers to the initial model for
re-estimation, obtained by smoothed the treebank
model with the POS-tag smoothing method with
the large unannotated corpus (4 million words).
This model understandably does not improve over
tt for parsing Sec. 23. tem1,?=0.5 is the model
obtained by smoothing with an EM-re-estimated
model with a constant interpolation factor ? =
0.5. This model gives a statistically significant im-
provement in f-score over both tt and tpos. The
last model tem1,?f is obtained by smoothing with
2PARSEVAL measures are known to be insensitive to sub-
categorization (Carroll et al, 1998).
3A randomized version of a paired-sample t-test is used.
4This is always done before parsing test data.
216
tt tpos tem1,?=0.5 tem1,?f
Recall 86.48 86.48 86.72 87.44
Precision 86.61 86.63 86.95 87.15
f-score 86.55 86.56 *86.83 *87.29
Table 1: Labeled bracketing F-score on section 23.
an interpolation factor as in Eq. 5 : this is the best
model with a statistically significant improvement
in f-score over tt, tpos and tem1,?=0.5.
Since we expect that smoothing will be advanta-
geous for unseen or low-frequency words, we per-
form an evaluation targeted at identifying struc-
tures subcategorized by unseen verbs. Table 2
shows the error reduction in identifying subcat.
frames in Viterbi parses, of unseen verbs and also
of all verbs (seen and unseen) in the testset. A
breakup of error by frame type for unseen verbs is
also shown (here, only frames with >10 token oc-
currences in the test data are shown). In all cases
(unseen verbs and all verbs) we see a substantial
error reduction. The error reduction improves with
larger amounts of unannotated training data.
8 Discussion and Conclusions
We have shown that lexicons re-estimated with I-
O can be used to smooth unlexicalised treebank
PCFGs, with a significant increase in f-score even
in the case of English with a large treebank re-
source. We expect this method to have more
impact for languages with a smaller treebank or
richer tag-set. An interesting aspect is the substan-
tial reduction in subcategorization error for un-
seen verbs for which no word-specific information
about subcategorization exists in the unsmoothed
or POS-tag-smoothed lexicon. The error reduction
in identifying subcat. frames implies that some
constituents (such as PPs) are not only attached
correctly but also identified correctly as arguments
(such as PP-CLR) rather than as adjuncts.
There have been previous attempts to use POS-
tagging technologies (such as HMM or maximum-
entropy based taggers) to enhance treebank-
trained grammars (Goldberg et al (2009) for He-
brew, (Clark and Curran, 2004) for CCG). The re-
estimation method we use builds full parse-trees,
rather than use local features like taggers do, and
hence might have a benefit over such methods. An
interesting option would be to train a ?supertag-
ger? on fine-grained tags from the PTB and to su-
pertag a large corpus to harvest lexical frequen-
Frame # tokens %Error %Error %Error
(test) tpos tem1 Reduc.
All unseen (4M words) 1258 33.47 22.81 31.84
All unseen (8M words) 1258 33.47 22.26 33.49
All unseen (12M words) 1258 33.47 21.86 34.68
transitive 662 23.87 18.73 21.52
intransitive 115 38.26 33.91 11.36
NP PP-CLR 121 34.71 32.23 7.14
PP-CLR 73 27.4 20.55 25
SBAR 124 12.1 12.1 0
S 12 83.33 58.33 30
NP NP 10 90 80 11.11
PRT NP 21 38.1 33.33 12.5
s.e.to (see Fig.1b) 50 16 12 25
NP PP-DIR 11 63.64 54.55 14.28
All verbs (4M) 11710 18.5 16.84 8.97
Table 2: Subcat. error for verbs in Viterbi parses.
cies. This would form another (possibly higher)
baseline for the I-O re-estimation approach pre-
sented here and is the focus of our future work.
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An Ap-
proach to Almost Parsing. Computational Linguistics,
25:237?265.
F. Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth.
1999. Inside-outside estimation of a lexicalized PCFG for
German. In ACL 37.
J. Carroll, G. Minnen, and E. Briscoe. 1998. Can subcate-
gorization probabilities help parsing. In 6th ACL/SIGDAT
Workshop on Very Large Corpora.
S. Clark and J. R. Curran. 2004. The Importance of Supertag-
ging for Wide-Coverage CCG Parsing. In 22nd COLING.
Carl de Marcken. 1995. On the unsupervised induction of
Phrase Structure grammars. In Proceedings of the 3rd
Workshop on Very Large Corpora.
T. Deoskar. 2008. Re-estimation of Lexical Parameters for
Treebank PCFGs. In 22nd COLING.
Tejaswini Deoskar and Mats Rooth. 2008. Induction of
Treebank-Aligned Lexical Resources. In 6th LREC.
Y. Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad. 2009.
Enhancing Unlexicalized Parsing Performance using a
Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and
EM-HMM-based Lexical Probabilities. In EACL-09.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL 41.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the Inside-Outside algo-
rithm. Computer Speech and Language, 4:35?56.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In International Conference on New
Methods in Language Processing.
H. Schmid. 2004. Efficient Parsing of Highly Ambiguous
CFGs with Bit Vectors. In 20th COLING.
217
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126?134,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Generalizing a Strongly Lexicalized Parser using Unlabeled Data
Tejaswini Deoskar
1
, Christos Christodoulopoulos
2
, Alexandra Birch
1
, Mark Steedman
1
1
School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB
2
University of Illinois, Urbana-Champaign, Urbana, IL 61801
{tdeoskar,abmayne,steedman}@inf.ed.ac.uk, christod@illinois.edu
Abstract
Statistical parsers trained on labeled data
suffer from sparsity, both grammatical and
lexical. For parsers based on strongly
lexicalized grammar formalisms (such as
CCG, which has complex lexical cate-
gories but simple combinatory rules), the
problem of sparsity can be isolated to
the lexicon. In this paper, we show that
semi-supervised Viterbi-EM can be used
to extend the lexicon of a generative CCG
parser. By learning complex lexical entries
for low-frequency and unseen words from
unlabeled data, we obtain improvements
over our supervised model for both in-
domain (WSJ) and out-of-domain (ques-
tions and Wikipedia) data. Our learnt
lexicons when used with a discriminative
parser such as C&C also significantly im-
prove its performance on unseen words.
1 Introduction
An important open problem in natural language
parsing is to generalize supervised parsers, which
are trained on hand-labeled data, using unlabeled
data. The problem arises because further hand-
labeled data in the amounts necessary to signif-
icantly improve supervised parsers are very un-
likely to be made available. Generalization is also
necessary in order to achieve good performance on
parsing in textual domains other than the domain
of the available labeled data. For example, parsers
trained on Wall Street Journal (WSJ) data suffer a
fall in accuracy on other domains (Gildea, 2001).
In this paper, we use self-training to generalize
the lexicon of a Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000) parser. CCG is a
strongly lexicalized formalism, in which every
word is associated with a syntactic category (sim-
ilar to an elementary syntactic structure) indicat-
ing its subcategorization potential. Lexical en-
tries are fine-grained and expressive, and contain
a large amount of language-specific grammatical
information. For parsers based on strongly lexical-
ized formalisms, the problem of grammar general-
ization can be cast largely as a problem of lexical
extension.
The present paper focuses on learning lexi-
cal categories for words that are unseen or low-
frequency in labeled data, from unlabeled data.
Since lexical categories in a strongly lexicalized
formalism are complex, fine-grained (and far more
numerous than simple part-of-speech tags), they
are relatively sparse in labeled data. Despite per-
forming at state-of-the-art levels, a major source
of error made by CCG parsers is related to unseen
and low-frequency words (Hockenmaier, 2003;
Clark and Curran, 2007; Thomforde and Steed-
man, 2011). The unseen words for which we learn
categories are surprisingly commonplace words of
English; examples are conquered, apprehended,
subdivided, scoring, denotes, hunted, obsessed,
residing, migrated (Wikipedia). Correctly learn-
ing to parse the predicate-argument structures as-
sociated with such words (expressed as lexical cat-
egories in the case of CCG), is important for open-
domain parsing, not only for CCG but indeed for
any parser.
We show that a simple self-training method,
Viterbi-EM (Neal and Hinton, 1998) when used
to enhance the lexicon of a strongly-lexicalized
parser can be an effective strategy for self-training
and domain-adaptation. Our learnt lexicons im-
prove on the lexical category accuracy of two su-
pervised CCG parsers (Hockenmaier (2003) and
the Clark and Curran (2007) parser, C&C) on
within-domain (WSJ) and out-of-domain test sets
(a question corpus and a Wikipedia corpus).
In most prior work, when EM was initialized
based on labeled data, its performance did not im-
prove over the supervised model (Merialdo, 1994;
126
Charniak, 1993). We found that in order for per-
formance to improve, unlabeled data should be
used only for parameters which are not well cov-
ered by the labeled data, while those that are well
covered should remain fixed.
In an additional contribution, we compare two
strategies for treating unseen words (a smoothing-
based, and a part-of-speech back-off method) and
find that a smoothing-based strategy for treat-
ing unseen words is more effective for semi-
supervised learning than part-of-speech back-off.
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a strongly lexicalized grammar
formalism, in which the lexicon contains all
language-specific grammatical information. The
lexical entry of a word consists of a syntactic cat-
egory which expresses the subcategorization po-
tential of the word, and a semantic interpretation
which defines the compositional semantics (Lewis
and Steedman, 2013). A small number of combi-
natory rules are used to combine constituents, and
it is straightforward to map syntactic categories to
a logical form for semantic interpretation.
For statistical CCG parsers, the lexicon is learnt
from labeled data, and is subject to sparsity due
to the fine-grained nature of the categories. Fig-
ure 1 illustrates this with a simple CCG deriva-
tion. In this sentence, bake is used as a ditransi-
tive verb and is assigned the ditransitive category
S\NP/NP/NP . This category defines the verb syn-
tactically as mapping three NP arguments to a sen-
tence S , and semantically as a ternary relation be-
tween its three arguments, thus providing a com-
plete analysis of the sentence.
[
NNP
John ] [
VBD
baked ] [
NNP
Mary] [
DT
a ] [
NN
cake]
NP S\NP/NP/NP NP NP/N N
> >
S\NP/NP NP
>
S\NP
<
S
?John baked Mary a cake?
Figure 1: Example CCG derivation
For a CCG parser to obtain the correct deriva-
tion above, its lexicon must include the ditransitive
category S\NP/NP/NP for the verb bake. It is not
sufficient to have simply seen the verb in another
context (say a transitive context like ?John baked a
cake?, which is a more common context). This is
in contrast to standard treebank parsers where the
verbal category is simply VBD (past tense verb)
and a ditransitive analysis of the sentence is not
ruled out as a result of the lexical category.
In addition to sparsity related to open-class
words like verbs as in the above example, there are
also missing categories in labeled data for closed-
class words like question words, due to the small
number of questions in the Penn Treebank. In gen-
eral, lexical sparsity for a statistical CCG parser
can be broken down into three types: (i) where a
word is unseen in training data but is present in
test data, (ii) where a word is seen in the train-
ing data but not with the category type required
in the test data (but the category type is seen with
other words) and (iii) where a word bears a cate-
gory type required in the test data but the category
type is completely unseen in the training data.
In this paper, we deal with the first two kinds.
The third kind is more prevalent when the size
of labeled data is comparatively small (although,
even in the case of the English WSJ CCG tree-
bank, there are several attested category types that
are entirely missing from the lexicon, Clark et al.,
2004). We make the assumption here that all cat-
egory types in the language have been seen in the
labeled data. In principle new category types may
be introduced independently without affecting our
semi-supervised process (for instance, manually,
or via a method that predicts new category types
from those seen in labeled data).
3 Related Work
Previous attempts at harnessing unlabeled data to
improve supervised CCG models using methods
like self-training or co-training have been unsat-
isfactory (Steedman et al., 2003, 43-44). Steed-
man et al. (2003) experimented with self-training
a generative CCG parser, and co-training a genera-
tive parser with an HMM-based supertagger. Co-
training (but not self-training) improved the results
of the parser when the seed labeled data was small.
When the seed data was large (the full treebank),
i.e., the supervised baseline was high, co-training
and self-training both failed to improve the parser.
More recently, Honnibal et al. (2009) improved
the performance of the C&C parser on a domain-
adaptation task (adaptation to Wikipedia text) us-
ing self-training. Instead of self-training the pars-
ing model, they re-train the supertagging model,
which in turn affects parsing accuracy. They
obtained an improvement of 1.09% (dependency
127
score) on supertagger accuracy on Wikipedia (al-
though performance on WSJ text dropped) but did
not attempt to re-train the parsing model.
An orthogonal approach for extending a CCG
lexicon using unlabeled data is that of Thomforde
and Steedman (2011), in which a CCG category for
an unknown word is derived from partial parses
of sentences with just that one word unknown.
The method is capable of inducing unseen cate-
gories types (the third kind of sparsity mentioned
in ?2.1), but due to algorithmic and efficiency is-
sues, it did not achieve the broad-coverage needed
for grammar generalisation of a high-end parser. It
is more relevant for low-resource languages which
do not have substantial labeled data and category
type discovery is important.
Some notable positive results for non-CCG
parsers are McClosky et al. (2006) who use a
parser-reranker combination. Koo et al. (2008)
and Suzuki et al. (2009) use unsupervised word-
clusters as features in a dependency parser to get
lexical dependencies. This has some notional sim-
ilarity to categories, since, like categories, clus-
ters are less fine-grained than words but more fine-
grained than POS-tags.
4 Supervised Parser
The CCG parser used in this paper is a re-
implementation of the generative parser of Hock-
enmaier and Steedman (2002) and Hockenmaier
(2003)
1
, except for the treatment of unseen and
low-frequency words.
We use a model (the LexCat model in Hock-
enmaier (2003)) that conditions the generation of
constituents in the parse tree on the lexical cate-
gory of the head word of the constituent, but not on
the head word itself. While fully-lexicalized mod-
els that condition on words (and thus model word-
to-word dependencies) are more accurate than un-
lexicalized ones like the LexCat model, we use
an unlexicalized model
2
for two reasons: first,
1
These generative models are similar to the Collins? head-
based models (Collins, 1997), where for every node, a head is
generated first, and then a sister conditioned on the head. De-
tails of the models are in Hockenmaier and Steedman (2002)
and Hockenmaier 2003:pg 166.
2
A terminological clarification: unlexicalized here refers
to the model, in the sense that head-word information is
not used for rule-expansion. The formalism itself (CCG)
is referred to as strongly-lexicalized, as used in the title of
the paper. Formalisms like CCG and LTAG are consid-
ered strongly-lexicalized since linguistic knowledge (func-
tions mapping words to syntactic structures/semantic inter-
pretations) is included in the lexicon.
our lexicon smoothing procedure (described in the
next section) introduces new words and new cat-
egories for words into the lexicon. Lexical cate-
gories are added to the lexicon for seen and un-
seen words, but no new category types are intro-
duced. Since the LexCat model conditions rule ex-
pansions on lexical categories, but not on words, it
is still able to produce parses for sentences with
new words. In contrast, a fully lexicalized model
would need all components of the grammar to be
smoothed, a task that is far from trivial due to the
resulting explosion in grammar size (and one that
we leave for future work).
Second, although lexicalized models perform
better on in-domain WSJ data (the LexCat model
has an accuracy of 87.9% on Section 23, as op-
posed to 91.03% for the head-lexicalized model
in Hockenmaier (2003) and 91.9% for the C&C
parser), our parser is more accurate on a question
corpus, with a lexical category accuracy of 82.3%,
as opposed to 71.6% and 78.6% for the C&C and
Hockenmaier (2003) respectively.
4.1 Handling rare and unseen words
Existing CCG parsers (Hockenmaier (2003) and
Clark and Curran (2007)) back-off rare and unseen
words to their POS tag. The POS-backoff strategy
is essentially a pipeline approach, where words
are first tagged with coarse tags (POS tags) and
finer tags (CCG categories) are later assigned, by
the parser (Hockenmaier, 2003) or the supertag-
ger (Clark and Curran, 2007). As POS-taggers
are much more accurate than parsers, this strat-
egy has given good performance in general for
CCG parsers, but it has the disadvantage that POS-
tagging errors are propagated. The parser can
never recover from a tagging error, a problem that
is serious for words in the Zipfian tail, where these
words might also be unseen for the POS tagger
and hence more likely to be tagged incorrectly.
This issue is in fact more generally relevant than
for CCG parsers alone?the dependence of parsers
on POS-taggers was cited as one of the problems
in domain-adaptation of parsers in the NAACL-
2012 shared task on parsing the web (Petrov and
McDonald, 2012). Lease and Charniak (2005)
obtained an improvement in the accuracy of the
Charniak (2000) parser on a biomedical domain
simply by training a new POS tagger model.
In the following section, we describe an alter-
native smoothing-based approach to handling un-
128
seen and rare words. This method is less sen-
sitive to POS tagging errors, as described below.
In this approach, in a pre-processing step prior
to parsing, categories are introduced into the lex-
icon for unseen and rare words from the data to
be parsed. Some probability mass is taken from
seen words/categories and given to unseen word
and category pairs. Thus, at parse time, no word is
unseen for the parser.
4.1.1 Smoothing
In our approach, we introduce lexical entries for
words from the unlabeled corpus that are unseen
in the labeled data, and also add categories to ex-
isting entries for rarely seen words. The most gen-
eral case of this would be to assign all known cat-
egories to a word. However, doing this reduces
the lexical category accuracy.
3
A second option,
chosen here, is to limit the number of categories
assigned to the word by using some information
about the word (for instance, its part-of-speech).
Based on the part-of-speech of an unseen word in
the unlabeled or test corpus, we add an entry to the
lexicon of the word with the top n categories that
have been seen with that part-of-speech in the la-
beled data. Each new entry of (w, cat), where w
is a word and cat is a CCG category, is associated
with a count c(w, cat), obtained as described be-
low. Once all (w, cat) entries are added to the lex-
icon along with their counts, a probability model
P (w|cat) is calculated over the entire lexicon.
Our smoothing method is based on a method
used in Deoskar (2008) for smoothing a PCFG
lexicon. Eq. 1 and 2 apply it to CCG entries for
unseen and rare words. In the first step, an out-
of-the-box POS tagger is used to tag the unlabeled
or test corpus (we use the C&C tagger). Counts
of words and POS-tags c
corpus
(w, T ) are obtained
from the tagged corpus. For the CCG lexicon, we
ultimately need a count for a word w and a CCG
category cat. To get this count, we split the count
of a word and POS-tag amongst all categories seen
with that tag in the supervised data in the same
ratio as the ratio of the categories in the super-
vised data. In Eq. 1, this ratio is c
tb
(cat
T
)/c
tb
(T )
where c
tb
(cat
T
) is the treebank count of a cate-
gory cat
T
seen with a POS-tag T , and c
tb
(T ) is the
marginal count of the tag T in the treebank. This
3
For instance, we find that assigning all categories to un-
seen verbs gives a lexical category accuracy of 52.25 %, as
opposed to an accuracy of 65.4% by using top 15 categories,
which gave us the best results, as reported later in Table 3.
ratio makes a more frequent category type more
likely than a rarer one for an unseen word. For ex-
ample, for unseen verbs, it would make the transi-
tive category more likely than a ditransitive one
(since transitives are more frequent than ditran-
sitives). There is an underlying assumption here
that relative frequencies of categories and POS-
tags in the labeled data are maintained in the un-
labeled data, which in fact can be thought of as
a prior while estimating from unlabeled data (De-
oskar et al., 2012).
c
corpus
(w, cat) =
c
tb
(cat
T
)
c
tb
(T )
? c
corpus
(w, T ) (1)
Additionally, for seen but low-frequency words,
we make use of the existing entry in the lexicon.
Thus in a second step, we interpolate the count
c
corpus
(w, cat) of a word and category with the
supervised count of the same c
tb
(w, cat) (if it ex-
ists) to give the final smoothed count of a word and
category c
smooth
(w, cat) (Eq. 2).
c
smooth
(w, cat) = ? ? c
tb
(w, cat) +
(1? ?) ? c
corpus
(w, cat)
(2)
When this smoothed lexicon is used with a
parser, POS-backoff is not necessary since all
needed words are now in the lexicon. Lexical en-
tries for words in the parse are determined not by
the POS-tag from a tagger, but directly by the pars-
ing model, thus making the parse less susceptible
to tagging errors.
5 Semi-supervised Learning
We use Viterbi-EM (Neal and Hinton, 1998) as
the self-training method. Viterbi-EM is an alter-
native to EM where instead of using the model
parameters to find a true posterior from unlabeled
data, a posterior based on the single maximum-
probability (Viterbi) parse is used. Viterbi-EM
has been used in various NLP tasks before and
often performs better than classic EM (Cohen
and Smith, 2010; Goldwater and Johnson, 2005;
Spitkovsky et al., 2010). In practice, a given pars-
ing model is used to obtain Viterbi parses of un-
labeled sentences. The Viterbi parses are then
treated as training data for a new model. This pro-
cess is iterated until convergence.
Since we are interested in learning the lexi-
con, we only consider lexical counts from Viterbi
parses of the unlabeled sentences. Other parame-
ters of the model are held at their supervised val-
ues. We conducted some experiments where we
129
self-trained all components of the parsing model,
which is the usual case of self-training. We ob-
tained negative results similar to Steedman et al.
(2003), where self-training reduced the perfor-
mance of the parsing model. We do not report
them here. Thus, using unlabeled data only to es-
timate parameters that are badly estimated from
labeled data (lexical entries in CCG, due to lexi-
cal sparsity) results in improvements, in contrast
to prior work with semi-supervised EM.
As is common in semi-supervised settings, we
treated the count of each lexical event as the
weighted count of that event in the labeled data
(treebank)
4
and the count from the Viterbi-parses
of unlabeled data. Here we follow Bacchiani et al.
(2006) and McClosky et al. (2006) who show that
count merging is more effective than model inter-
polation.
We placed an additional constraint on the con-
tribution that the unlabeled data makes to the semi-
supervised model?we only use counts (from un-
labeled data) of lexical events that are rarely
seen/unseen in the labeled data. Our reasoning
was that many lexical entries are estimated accu-
rately from the treebank (for example, those re-
lated to function words and other high-frequency
words) and estimation from unlabeled data might
hurt them. We thus had a cut-off frequency (of
words in labeled data) above which we did not
allow the unlabeled counts to affect the semi-
supervised model. In practise, our experiments
turned out to be fairly insensitive to the value of
this parameter, on evaluations over rare or un-
seen verbs. However, overall accuracy would drop
slightly if this cut-off was increased. We experi-
mented with cut-offs of 5, 10 and 15, and found
that the most conservative value (of 5) gave the
best results on in-domain WSJ experiments, and a
higher value of 10 gave the best results for out-of-
domain experiments.
We also conducted some limited experiments
with classical semi-supervised EM, with similar
settings of weighting labeled counts, and using un-
labeled counts only for rare/unseen events. Since
it is a much more computationally expensive pro-
cedure, and most of the results did not come close
to the results of Viterbi-EM, we did not pursue it.
4
The labeled count is weighted in order to scale up the la-
beled data which is usually smaller in size than the unlabeled
data, to avoid swamping the labeled counts with much larger
unlabeled counts.
5.1 Data
Labeled: Sec. 02-21 of CCGbank (Hockenmaier
and Steedman, 2007). In one experiment, we used
Sec. 02-21 minus 1575 sentences that were held
out to simulate test data containing unseen verbs?
see ?6.2 for details.
Unlabeled: For in-domain experiments, we used
sentences from the unlabeled WSJ portion of the
ACL/DCI corpus (LDC93T1, 1993), and the WSJ
portion of the ANC corpus (Reppen et al., 2005),
limited to sentences containing 20 words or less,
creating datasets of approximately 10, 20 and 40
million words each. Additionally, we have a
dataset of 140 million words ? 40M WSJ words
plus an additional 100M from the New York
Times.
For domain-adaptation experiments, we use
two different datasets. The first one consists
of question-sentences ? 1328 unlabeled ques-
tions, obtained by removing the manual annota-
tion of the question corpus from Rimell and Clark
(2008). The second out-of-domain dataset con-
sists of Wikipedia data, approximately 40 million
words in size, with sentence length < 20 words.
5.2 Experimental setup
We ran our semi-supervised method using our
parser with a smoothed lexicon (from ?4.1.1) as
the initial model, on unlabeled data of different
sizes/domains. For comparison, we also ran ex-
periments using a POS-backed off parser (the orig-
inal Hockenmaier and Steedman (2002) LexCat
model) as the initial model. Viterbi-EM converged
at 4-5 iterations. We then parsed various test sets
using the semi-supervised lexicons thus obtained.
In all experiments, the labeled data was scaled to
match the size of the unlabeled data. Thus, the
scaling factor of labeled data was 10 for unlabeled
data of 10M words, 20 for 20M words, etc.
5.3 Evaluation
We focused our evaluations on unseen and low-
frequency verbs, since verbs are the most impor-
tant open-class lexical entries and the most am-
biguous to learn from unlabeled data (approx. 600
categories, versus 150 for nouns). We report lexi-
cal category accuracy in parses produced using our
semi-supervised lexicon, since it is a direct mea-
sure of the effect of the lexicon.
5
We discuss four
5
Dependency recovery accuracy is also used to evaluate
performance of CCG parsers and is correlated with lexical
130
All words All Verbs Unseen
Verbs
SUP 87.76 78.10 52.54
SEMISUP 88.14 78.46 **57.28
SUP
bkoff
87.91 76.08 54.14
SEMISUP
bkoff
87.79 75.68 54.60
Table 1: Lexical category accuracy on TEST-4SEC
**: p < 0.004, McNemar test
experiments below. The first two are on in-domain
(WSJ) data. The last two are on out-of-domain
data ? a question corpus and a Wikipedia corpus.
6 Results
6.1 In-domain: WSJ unseen verbs
Our first testset consists of a concatenation of 4
sections of CCGbank (01, 22, 24, 23), a total of
7417 sentences, to form a testset called TEST-
4SEC. We use all these sections in order to get
a reasonable token count of unseen verbs, which
was not possible with Sec. 23 alone.
Table 1 shows the performance of the smoothed
supervised model (SUP) and the semi-supervised
model (SEMISUP) on this testset. There is a sig-
nificant improvement in performance on unseen
verbs, showing that the semi-supervised model
learns good entries for unseen verbs over and
above the smoothed entry in the supervised lexi-
con. This results in an improvement in the over-
all lexical category accuracy of the parser on all
words, and all verbs.
We also performed semi-supervised training us-
ing a supervised model that treated unseen words
with a POS-backoff strategy SUP
bkoff
. We used
the same settings of cut-off and the same scal-
ing of labeled counts as before. The supervised
backed-off model performs somewhat better than
the supervised smoothed model. However, it did
not improve as much as the smoothed one from
unlabeled data. Additionally, the overall accuracy
of SEMISUP
bkoff
fell below the supervised level,
in contrast to the smoothed model, where overall
numbers improved. This could indicate that the
accuracy of a POS tagger on unseen words, es-
pecially verbs, may be an important bottleneck in
semi-supervised learning.
Low-frequency verbs We also obtain improve-
ments on verbs that are seen but with a low fre-
quency in the labeled data (Table 2). We divided
category accuracy, but a dependency evaluation is more rele-
vant when comparing performance with parsers in other for-
malisms and does not have much utility here.
Freq. Bin 1-5 6-10 11-20
SUP 64.13 75.19 77.6
SEMISUP 66.72 76.21 79.8
Table 2: Seen but rare verbs, TEST-4SEC
verbs occurring in TEST-4SEC into different bins
according to their occurrence frequency in the la-
beled data (bins of frequency 1-5, 6-10 and 11-20).
Semi-supervised training improves over the super-
vised baseline for all bins of low-frequency verbs.
Note that our cut-off frequency for using unlabeled
data is 5, but there are improvements in the 6-10
and 11-20 bins as well, suggesting that learning
better categories for rare words (below the cut-off)
impacts the accuracy of words above the cut-off as
well, by affecting the rest of the parse positively.
6.2 In-domain : heldout unseen verbs
The previous section showed significant improve-
ment in learning categories for verbs that are un-
seen in the training sections of CCGbank. How-
ever, these verbs are in the Zipfian tail, and for this
reason have fairly low occurrence frequencies in
the unlabeled corpus. In order to estimate whether
our method will give further improvements in the
lexical categories for these verbs, we would need
unlabeled data of a much larger size. We there-
fore designed an experimental scenario in which
we would be able to get high counts of unseen
verbs from a similar size of unlabeled data. We
first made a list of N verbs from the treebank and
then extracted all sentences containing them (ei-
ther as verbs or otherwise) from CCGbank training
sections. These sentences form a testset of 1575
sentences, called TEST-HOV (for held out verbs).
The verbs in the list were chosen based on occur-
rence frequency f in the treebank, choosing all
verbs that occurred with a frequency of f = 11.
This number gave us a large enough set and a
good type/token ratio to reliably evaluate and ana-
lyze our semi-supervised models?112 verb types,
with 1115 token occurrences
6
. Since these verbs
are actually mid-frequency verbs in the supervised
data, they have a correspondingly large occurrence
frequency in the unlabeled data, occurring much
more often than true unseen verbs. Thus, the un-
labeled data size is effectively magnified?as far
as these verbs are concerned, the unlabeled data is
approximately 11 times larger than it actually is.
Table 3 shows lexical category accuracy on
6
Selecting a different but close value of f such as f = 10
or f = 12 would have also served this purpose.
131
All Words All Verbs Unseen
Verbs
SUP 87.26 74.55 65.49
SEMISUP 87.78 75.30 *** 70.43
SUP
bkoff
87.58 73.06 67.25
SEMISUP
bkoff
87.52 72.89 68.05
Table 3: Lexical category accuracy in TEST-HOV.
***p<0.0001, McNemar test
55
60
65
70
0 10 20 40 140
Size of Unlabelled Data (in millions of words)
Lexic
al Ca
tegor
y Acc
uracy
 for U
nsee
n Ver
bs
Test:HOVTest:4Sec
Figure 2: Increasing accuracy on unseen verbs
with increasing amounts of unlabeled data.
this testset. The baseline accuracy of the parser
on these verbs is much higher than that on the
truly unseen verbs.
7
The semi-supervised model
(SEMISUP) improves over the supervised model
SUP very significantly on these unseen verbs. We
also see an overall improvement on all verbs (seen
and unseen) in the test data, and in the over-
all lexical category accuracy as well. Again, the
backed-off model does not improve as much as
the smoothed model, and moreover, overall per-
formance falls below the supervised level.
Figure 2 shows the effect of different sizes of
unlabeled data on accuracy of unseen verbs for
the two testsets TEST-HOV and TEST-4SEC . Im-
provements are monotonic with increasing unla-
beled data sizes, up to 40M words. The additional
100M words of NYT also improve the models but
to a lesser degree, possibly due to the difference in
domain. The graphs indicate that the method will
lead to more improvements as more unlabeled data
(especially WSJ data) is added.
7
This could be because verbs in the Zipfian tail have more
idiosyncratic subcategorization patterns than mid-frequency
verbs, and thus are harder for a parser. Another reason is that
they may have been seen as nouns or other parts of speech,
leading to greater ambiguity in their case.
QUESTIONS WIKIPEDIA
All wh All Unseen
words words words words
SUP 82.36 61.77 84.31 79.5
SEMISUP *83.21 63.22 *85.6 80.25
Table 4: Out-of-domain: Questions and
Wikipedia, *p<0.05, McNemar test
6.2.1 Out-of-Domain
Questions The question corpus is not strictly a
different domain (since questions form a differ-
ent kind of construction rather than a different do-
main), but it is an interesting case of adaptation
for several reasons: WSJ parsers perform poorly
on questions due to the small number of questions
in the Penn Treebank/CCGbank. Secondly, unsu-
pervised adaptation to questions has not been at-
tempted before for CCG (Rimell and Clark (2008)
did supervised adaptation of their supertagger).
The supervised model SUP already performs
at state-of-the-art on this corpus, on both overall
scores and on wh(question)-words alone. C&C
and Hockenmaier (2003) get 71.6 and 78.6% over-
all accuracies respectively, and only 33.6 and 50.7
on wh-words alone. To our original unlabeled
WSJ data (40M words), we add 1328 unlabeled
question-sentences from Rimell and Clark, 2008,
scaled by ten, so that each is counted ten times. We
then evaluated on a testset containing questions
(500 question sentences, from Rimell and Clark
(2008)). The overall lexical category accuracy on
this testset improves significantly as a result of the
semi-supervised learning (Table 4). The accuracy
on the question words alone (who, what, where,
when, which, how, whose, whom) also improves
numerically, but by a small amount (the number
of tokens that improve are only 7). This could be
an effect of the small size of the testset (500 sen-
tences, i.e. 500 wh-words).
Wikipedia We obtain statistically significant im-
provements in overall scores over a testset consist-
ing of Wikipedia sentences hand-annotated with
CCG categories (from Honnibal et al. (2009)) (Ta-
ble 4). We also obtained improvements in lexical
category accuracy on unseen words, and on un-
seen verbs alone (not shown), but could not prove
significance. This testset contains only 200 sen-
tences, and counts for unseen words are too small
for significance tests, although there are numeric
improvements. However, the overall improvement
is statistically significantly, showing that adapting
the lexicon alone is effective for a new domain.
132
6.3 Using semi-supervised lexicons with the
C&C parser
To show that the learnt lexical entries may be use-
ful to parsers other than our own, we incorpo-
rate our semi-supervised lexical entries into the
C&C parser to see if it benefits performance. We
do this in a naive manner, as a proof of concept,
making no attempt to optimize the performance
of the C&C parser (since we do not have access
to its internal workings). We take all entries of
unseen words from our best semi-supervised lex-
icon (word, category and count) and add them to
the dictionary of the C&C supertagger (tagdict).
The C&C is a discriminative, lexicalized model
that is more accurate than an unlexicalized model.
Even so, the lexical entries that we learn improve
the C&C parsers performance over and above its
back-off strategy for unseen words. Table 5 shows
the results on WSJ data TEST-4SEC and TEST-
HOV. There were numeric improvements on the
TEST-4SEC test set as shown in Table 5
8
. We ob-
tain significance on the TEST-HOV testset which
has a larger number of tokens of unseen verbs and
entries that were learnt from effectively larger un-
labeled data. We tested two cases: when these
verbs were seen for the POS tagger used to tag
the test data, and when they were unseen for the
POS tagger, and found statistically significant im-
provement for the case when the verbs were un-
seen for the POS tagger
9
, indicating sensitivity to
POS-tagger errors.
6.4 Entropy and KL-divergence
We also evaluated the quality of the semi-
supervised lexical entries by measuring the over-
all entropy and the average Kullback-Leibler (KL)
divergence of the learnt entries of unseen verbs
from entries in the gold testset. The gold entry
for each verb from the TEST-HOV testset was ob-
tained from the heldout gold treebank trees. Su-
pervised (smoothed) and semi-supervised entries
were obtained from the respective lexicons. These
metrics use the conditional probability of a cate-
gory given a word, which is not a factor in the
generative model (which considers probabilities of
8
There were also improvements on the question and
Wikipedia testsets (not shown) (8 and 6 tokens each) but the
size of these testsets is too small for significance.
9
Note that for this testset TEST-HOV, the numbers are the
supertagger?s accuracy, and not the parser?s. We were only
able to retrain the supertagger on training data with TEST-
HOV sentences heldout, but could not retrain the parser, de-
spite consultation with the authors.
TEST-4SEC TEST-HOV
POS-seen POS-unseen
(590) (1134) (1134)
C&C 62.03 (366) 76.71 (870) 72.39 (821)
C&C
(enhanced) 63.89 (377) 77.34 (877) *73.98 (839)
Table 5: TEST-4SEC: Lexical category accuracy of
C&C parser on unseen verbs. Numbers in brackets
are the number of tokens.*p<0.05, McNemar test
words given categories), but provide a good mea-
sure of how close the learnt lexicons are to the gold
lexicon. We find that the average KL divergence
reduces from 2.17 for the baseline supervised en-
tries to 1.40 for the semi-supervised entries. The
overall entropy for unseen verb distributions also
goes down from 2.23 (supervised) to 1.37 (semi-
supervised), showing that semi-supervised distri-
butions are more peaked, and bringing them closer
to the true entropy of the gold distribution (0.93).
7 Conclusions
We have shown that it is possible to learn CCG lex-
ical entries for unseen and low-frequency words
from unlabeled data. When restricted to learning
only lexical entries, Viterbi-EM improved the per-
formance of the supervised parser (both in-domain
and out-of-domain). Updating all parameters of
the parsing model resulted in a decrease in the ac-
curacy of the parser. We showed that the entries
we learnt with an unlexicalized model were accu-
rate enough to also be useful to a highly-accurate
lexicalized parser. It is likely that a lexicalized
parser will provide even better lexical entries. The
lexical entries continued to improve with increas-
ing size of unlabeled data. For the out-of-domain
testsets, we obtained statistically significant over-
all improvements, but we were hampered by the
small sizes of the testsets in evaluating unseen/wh
words.
In future work, we would like to add unseen but
predicted category types to the initial lexicon using
an independent method, and then apply the same
semi-supervised learning to words of these types.
Acknowledgements
We thank Mike Lewis, Shay Cohen and the three
anonymous EACL reviewers for helpful com-
ments. This work was supported by the ERC Ad-
vanced Fellowship 249520 GRAMPLUS.
133
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. MAP adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Eugene Charniak. 1993. Statistical Language Learning. MIT
Press.
Stephen Clark and James R. Curran. 2007. Wide-Coverage
Efficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
Stephen Clark, Mark Steedman, and James Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of EMNLP 2004.
Shay Cohen and Noah Smith. 2010. Viterbi Training for
PCFGs: Hardness Results and Competitiveness of Uni-
form Initialization. In Proceedings of ACL 2010.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th ACL.
Tejaswini Deoskar. 2008. Re-estimation of Lexical Param-
eters for Treebank PCFGs. In Proceedings of COLING
2008.
Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima?an.
2012. Learning Structural Dependencies of Words in the
Zipfian Tail. Journal of Logic and Computation.
Daniel Gildea. 2001. Corpus Variation and Parser Perfor-
mance. In Proceedings of EMNLP 2001.
Sharon Goldwater and Mark Johnson. 2005. Bias in learning
syllable structure. In Proceedings of CoNLL05.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2002. Generative
Models for Statistical Parsing with Combinatory Catego-
rial Grammar. In ACL40.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A
Corpus of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computational Lin-
guistics, 33:355?396.
Matthew Honnibal, Joel Nothman, and James R. Curran.
2009. Evaluating a Statistial CCG Parser on Wikipedia.
In Proceedings of the 2009 Workshop on the People?s Web
Meets NLP, ACL-IJCNLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple Semi-supervised Dependency Parsing. In Proceedings
of ACL-08: HLT , pages 595?603. Association for Com-
putational Linguistics, Columbus, Ohio.
LDC93T1. 1993. LDC93T1. Linguistic Data Consortium,
Philadelphia.
Matthew Lease and Eugene Charniak. 2005. Parsing Biomed-
ical Literature. In R. Dale, K.-F. Wong, J. Su, and
O. Kwong, eds., Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJC-
NLP?05), vol. 3651 of Lecture Notes in Computer Science,
pages 58 ? 69. Springer-Verlag, Jeju Island, Korea.
Mike Lewis and Mark Steedman. 2013. Combined Distribu-
tional and Logical Semantics. Transactions of the Associ-
ation for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Proceedings
of HLT-NAACL 2006.
Bernard Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational Linguistics, 20(2):155?
171.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view of
the EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models, pages
355 ? 368. Kluwer Academic Publishers.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In First Work-
shop on Syntactic Analysis of Non-Canonical Language
(SANCL) Workshop at NAACL 2012.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
LDC2005T35, American National Corpus (ANC) Second
Release. Linguistic Data Consortium, Philadelphia.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-08).
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and
Christopher D. Manning. 2010. Viterbi Training Improves
Unsupervised Dependency Parsing. In Proceedings of
CoNLL-2010.
Mark Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Mark Steedman, Steven Baker, Jeremiah Crim, Stephen
Clark, Julia Hockenmaier, Rebecca Hwa, Miles Osbornn,
Paul Ruhlen, and Anoop Sarkar. 2003. Semi-Supervised
Training for Statistical Parsing. Tech. rep., CLSP WS-02.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An Empirical Study of Semi-supervised
Structured Conditional Models for Dependency Parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 551?
560. Association for Computational Linguistics, Singa-
pore.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG Lexicon Extension. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, Edinburgh UK.
134
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 159?163,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Dependency Parsers using Combinatory Categorial Grammar
Bharat Ram Ambati Tejaswini Deoskar
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
bharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.uk
Mark Steedman
Abstract
Subcategorization information is a useful
feature in dependency parsing. In this
paper, we explore a method of incorpo-
rating this information via Combinatory
Categorial Grammar (CCG) categories
from a supertagger. We experiment with
two popular dependency parsers (Malt
and MST) for two languages: English
and Hindi. For both languages, CCG
categories improve the overall accuracy
of both parsers by around 0.3-0.5% in
all experiments. For both parsers, we
see larger improvements specifically on
dependencies at which they are known to
be weak: long distance dependencies for
Malt, and verbal arguments for MST. The
result is particularly interesting in the case
of the fast greedy parser (Malt), since im-
proving its accuracy without significantly
compromising speed is relevant for large
scale applications such as parsing the web.
1 Introduction
Dependency parsers can recover much of the
predicate-argument structure of a sentence, while
being relatively efficient to train and extremely
fast at parsing. Dependency parsers have been
gaining in popularity in recent times due to
the availability of large dependency treebanks
for several languages and parsing shared tasks
(Buchholz and Marsi, 2006; Nivre et al., 2007a;
Bharati et al., 2012).
Ambati et al. (2013) showed that the perfor-
mance of Malt (Nivre et al., 2007b) on the free
word order language, Hindi, is improved by using
lexical categories from Combinatory Categorial
Grammar (CCG) (Steedman, 2000). In this paper,
we extend this work and show that CCG categories
are useful even in the case of English, a typolog-
ically different language, where parsing accuracy
of dependency parsers is already extremely high.
In addition, we also demonstrate the utility of
CCG categories to MST (McDonald et al., 2005)
for both languages. CCG lexical categories
contain subcategorization information regarding
the dependencies of predicates, including long-
distance dependencies. We show that providing
this subcategorization information in the form of
CCG categories can help both Malt and MST on
precisely those dependencies for which they are
known to have weak rates of recovery. The result
is particularly interesting for Malt, the fast greedy
parser, as the improvement in Malt comes without
significantly compromising its speed, so that it
can be practically applied in web scale parsing.
Our results apply both to English, a fixed word
order and morphologically simple language, and
to Hindi, a free word order and morphologically
rich language, indicating that CCG categories
from a supertagger are an easy and robust way
of introducing lexicalized subcategorization
information into dependency parsers.
2 Related Work
Parsers using different grammar formalisms
have different strengths and weaknesses, and
prior work has shown that information from one
formalism can improve the performance of a
parser in another formalism. Sagae et al. (2007)
achieved a 1.4% improvement in accuracy over a
state-of-the-art HPSG parser by using dependen-
cies from a dependency parser for constraining
wide-coverage rules in the HPSG parser. Coppola
and Steedman (2013) incorporated higher-order
dependency features into a cube decoding phrase-
structure parser and obtained significant gains
on dependency recovery for both in-domain and
out-of-domain test sets.
Kim et al. (2012) improved a CCG parser using
dependency features. They extracted n-best parses
from a CCG parser and provided dependency
159
Pierre Vinken will join the board as a nonexecutive director Nov. 29
N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N
> > > >
N NP N (S\NP)\(S\NP)
T >
NP NP
> >
(S[b]\NP)/PP PP
>
S[b]\NP
<
S[b]\NP
>
S[dcl]\NP
>
S[dcl]
Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence.
features from a dependency parser to a re-ranker
with an improvement of 0.35% in labelled F-score
of the CCGbank test set. Conversely, Ambati
et al. (2013) showed that a Hindi dependency
parser (Malt) could be improved by using CCG
categories. Using an algorithm similar to Cakici
(2005) and Uematsu et al. (2013), they first cre-
ated a Hindi CCGbank from a Hindi dependency
treebank and built a supertagger. They provided
CCG categories from a supertagger as features to
Malt and obtained overall improvements of 0.3%
and 0.4% in unlabelled and labelled attachment
scores respectively.
3 Data and Tools
Figure 1 shows a CCG derivation with CCG
lexical categories for each word and Stanford
scheme dependencies (De Marneffe et al., 2006)
for an example English sentence. (Details of CCG
and dependency parsing are given by Steedman
(2000) and K?ubler et al. (2009).)
3.1 Treebanks
In English dependency parsing literature, Stanford
and CoNLL dependency schemes are widely
popular. We used the Stanford parser?s built-in
converter (with the basic projective option) to
generate Stanford dependencies and Penn2Malt
1
to generate CoNLL dependencies from Penn
Treebank (Marcus et al., 1993). We used standard
splits, training (sections 02-21), development
(section 22) and testing (section 23) for our
experiments. For Hindi, we worked with the
Hindi Dependency Treebank (HDT) released
as part of Coling 2012 Shared Task (Bharati et
al., 2012). HDT contains 12,041 training, 1,233
development and 1,828 testing sentences.
We used the English (Hockenmaier and Steed-
man, 2007) and Hindi CCGbanks (Ambati et al.,
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
2013) for our experiments. For Hindi we used two
lexicons: a fine-grained one (with morphological
information) and a coarse-grained one (without
morphological information).
3.2 Supertaggers
We used Clark and Curran (2004)?s supertagger
for English, and Ambati et al. (2013)?s supertag-
ger for Hindi. Both are Maximum Entropy based
CCG supertaggers. The Clark and Curran (2004)
supertagger uses different features like word, part-
of-speech, and contextual and complex bi-gram
features to obtain a 1-best accuracy of 91.5% on
the development set. In addition to the above
mentioned features, Ambati et al. (2013) em-
ployed morphological features useful for Hindi.
The 1-best accuracy of Hindi supertagger for fine-
grained and coarse-grained lexicon is 82.92% and
84.40% respectively.
3.3 Dependency Parsers
There has been a significant amount of work on
parsing English and Hindi using the Malt and
MST parsers in the recent past (Nivre et al.,
2007a; Bharati et al., 2012). We first run these
parsers with previous best settings (McDonald et
al., 2005; Zhang and Nivre, 2012; Bharati et
al., 2012) and treat them as our baseline. In
the case of English, Malt uses arc-standard and
stack-projective parsing algorithms for CoNLL
and Stanford schemes respectively and LIBLIN-
EAR learner (Fan et al., 2008) for both the
schemes. MST uses 1st-order features, and a pro-
jective parsing algorithm with 5-best MIRA train-
ing for both the schemes. For Hindi, Malt uses
the arc-standard parsing algorithm with a LIBLIN-
EAR learner. MST uses 2nd-order features, non-
projective algorithm with 5-best MIRA training.
For English, we assigned POS-tags using a per-
ceptron tagger (Collins, 2002). For Hindi, we also
did all our experiments using automatic features
160
Language Experiment
Malt MST
UAS LAS UAS LAS
English
Stanford Baseline 90.32 87.87 90.36 87.18
Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87.73** (4.3)
CoNLL Baseline 89.99 88.73 90.94 89.69
CoNLL + CCG 90.38** (4.0) 89.19** (4.1) 91.48** (5.9) 90.23** (5.3)
Hindi
Baseline 88.67 83.04 90.52 80.67
Fine CCG 88.93** (2.2) 83.23* (1.1) 90.97** (4.8) 80.94* (1.4)
Coarse CCG 89.04** (3.3) 83.35* (1.9) 90.88** (3.8) 80.73* (0.4)
Table 1: Impact of CCG categories from a supertagger on dependency parsing. Numbers in brackets
are percentage of errors reduced. McNemar?s test compared to baseline, * = p < 0.05 ; ** = p < 0.01
(Hindi Malt results (grey background) are from Ambati et al. (2013)).
(POS, chunk and morphological information)
extracted using a Hindi shallow parser
2
.
4 CCG Categories as Features
Following Ambati et al. (2013), we used supertags
which occurred at least K times in the training
data, and backed off to coarse POS-tags otherwise.
For English K=1, i.e., when we use CCG cate-
gories for all words, gave the best results. K=15
gave the best results for Hindi due to sparsity is-
sues, as the data for Hindi is small. We provided
a supertag as an atomic symbol similar to a POS
tag and didn?t split it into a list of argument and
result categories. We explored both Stanford and
CoNLL schemes for English and fine and coarse-
grained CCG categories for Hindi. All feature and
parser tuning was done on the development data.
We assigned automatic POS-tags and supertags to
the training data.
4.1 Experiments with Supertagger output
We first used gold CCG categories extracted from
each CCGbank as features to the Malt and MST,
to get an upper bound on the utility of CCG cate-
gories. As expected, gold CCG categories boosted
the Unlabelled Attachment Score (UAS) and La-
belled Attachment Score (LAS) by a large amount
(4-7% in all the cases).
We then experimented with using automatic
CCG categories from the English and Hindi su-
pertaggers as a feature to Malt and MST. With au-
tomatic categories from a supertagger, we got sta-
tistically significant improvements (McNemar?s
test, p < 0.05 for Hindi LAS and p < 0.01 for the
rest) over the baseline parsers, for all cases (Table
1). Since the CCGbanks used to train the supertag-
gers are automatically generated from the con-
stituency or dependency treebanks used to train
2
http://ltrc.iiit.ac.in/analyzer/hindi/
the dependency parsers, the improvements are
indeed due to reparameterization of the model to
include CCG categories and not due to additional
hand annotations in the CCGbanks. This shows
that the rich subcategorization information pro-
vided by automatically assigned CCG categories
can help Malt and MST in realistic applications.
For English, in case of Malt, we achieved
0.3% improvement in both UAS and LAS for
Stanford scheme. For CoNLL scheme, these
improvements were 0.4% and 0.5% in UAS and
LAS respectively. For MST, we got around 0.5%
improvements in all cases.
In case of Hindi, fine-grained supertags gave
larger improvements for MST. We got final
improvements of 0.5% and 0.3% in UAS and LAS
respectively. In contrast, for Malt, Ambati et al.
(2013) had shown that coarse-grained supertags
gave larger improvements of 0.3% and 0.4% in
UAS and LAS respectively. Due to better handling
of error propagation in MST, the richer informa-
tion in fine-grained categories may have surpassed
the slightly lower supertagger performance,
compared to coarse-grained categories.
4.2 Analysis: English
We analyze the impact of CCG categories on
different labels (label-wise) and distance ranges
(distance-wise) for CoNLL scheme dependencies
(We observed a similar impact for the Stanford
scheme dependencies as well). Figure 2a shows
the F-score for three major dependency labels,
namely, ROOT (sentence root), SUBJ (subject),
OBJ (object). For Malt, providing CCG categories
gave an increment of 1.0%, 0.3% for ROOT and
SUBJ labels respectively. For MST, the improve-
ments for ROOT and SUBJ were 0.5% and 0.8%
respectively. There was no significant improve-
ment for OBJ label, especially in the case of Malt.
161
87.7
92.5
88.7
92.893.4 92.5
88.2
93.9 93.3
88.58990
9192
9394
95 MaltMalt + CCGMSTMST + CCG
86.5 86.5
8687
88
ROOT SUBJ DOBJ
(a) Label-wise impact
98.2 
78.6 80.8 
98.3 
79.2 81.7 
98.4 
80.8 
84.5 
98.5 
81.8 
85.5 
78 
83 
88 
93 
98 
1-5 6-10 >10 
Malt Malt + CCG MST MST + CCG 
(b) Distance-wise impact
Figure 2: Label-wise and Distance-wise impact of supertag features on Malt and MST for English
Figure 2b shows the F-score of dependencies
based on the distance ranges between words. The
percentage of dependencies in the 1?5, 6?10 and
>10 distance ranges are 88.5%, 6.6% and 4.9% re-
spectively out of the total of around 50,000 depen-
dencies. For both Malt and MST, there was very
slight improvement for short distance dependen-
cies (1?5) but significant improvements for longer
distances (6?10 and >10). For Malt, there was
an improvement of 0.6% and 0.9% for distances
6?10, and >10 respectively. For MST, these
improvements were 1.0% and 1.0% respectively.
4.3 Analysis: Hindi
In the case of Hindi, for MST, providing CCG
categories gave an increment of 0.5%, 0.4% and
0.3% for ROOT, SUBJ and OBJ labels respec-
tively in F-score over the baseline. Ambati et al.
(2013) showed that for Hindi, providing CCG
categories as features improved Malt in better
handling of long distance dependencies.
The percentage of dependencies in the 1?5,
6?10 and >10 distance ranges are 82.2%,
8.6% and 9.2% respectively out of the total of
around 40,000 dependencies. Similar to English,
there was very slight improvement for short
distance dependencies (1?5). But for longer
distances, 6?10, and >10, there was significant
improvement of 1.3% and 1.3% respectively
for MST. Ambati et al. (2013) reported similar
improvements for Malt as well.
4.4 Discussion
Though valency is a useful feature in dependency
parsing (Zhang and Nivre, 2011), Zhang and Nivre
(2012) showed that providing valency information
dynamically, in the form of the number of depen-
dencies established in a particular state during
parsing, did not help Malt. However, as we have
shown above, providing this information as a static
lexical feature in the form of CCG categories does
help Malt. In addition to specifying the number of
arguments, CCG categories also contain syntactic
type and direction of those arguments. However,
providing CCG categories as features to zpar
(Zhang and Nivre, 2011) didn?t have significant
impact as it is already using similar information.
4.5 Impact on Web Scale Parsing
Greedy parsers such as Malt are very fast and are
practically useful in large-scale applications such
as parsing the web. Table 2, shows the speed of
Malt, MST and zpar on parsing English test data
in CoNLL scheme (including POS-tagging and
supertagging time). Malt parses 310 sentences per
second, compared to 35 and 11 of zpar and MST
respectively. Clearly, Malt is orders of magnitude
faster than MST and zpar. After using CCG
categories from the supertagger, Malt parses 245
sentences per second, still much higher than other
parsers. Thus we have shown a way to improve
Malt without significantly compromising speed,
potentially enhancing its usefulness for web scale
parsing.
Parser Ave. Sents / Sec Total Time
MST 11 3m 36s
zpar 35 1m 11s
Malt 310 0m 7.7s
Malt + CCG 245 0m 10.2s
Table 2: Time taken to parse English test data.
5 Conclusion
We have shown that informative CCG categories,
which contain both local subcategorization infor-
mation and capture long distance dependencies
elegantly, improve the performance of two de-
pendency parsers, Malt and MST, by helping
in recovering long distance relations for Malt
and local verbal arguments for MST. This is
true both in the case of English (a fixed word
order language) and Hindi (free word order and
morphologically richer language), extending the
result of Ambati et al. (2013). The result is
particularly interesting in the case of Malt which
cannot directly use valency information, which
CCG categories provide indirectly. It leads to an
improvement in performance without significantly
compromising speed and hence promises to be
applicable to web scale processing.
162
References
Bharat Ram Ambati, Tejaswini Deoskar, and Mark
Steedman. 2013. Using CCG categories to improve
Hindi dependency parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
604?609, Sofia, Bulgaria.
Akshar Bharati, Prashanth Mannem, and Dipti Misra
Sharma. 2012. Hindi Parsing Shared Task. In Pro-
ceedings of Coling Workshop on Machine Transla-
tion and Parsing in Indian Languages, Kharagpur,
India.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164,
New York City, New York.
Ruken Cakici. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of the ACL
Student Research Workshop, pages 73?78, Ann Ar-
bor, Michigan.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282?288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing, EMNLP ?02, pages 1?8.
Greg Coppola and Mark Steedman. 2013. The effect
of higher-order dependency features in discrimina-
tive phrase-structure parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
610?616, Sofia, Bulgaria.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Sunghwan Mac Kim, Dominick Ng, Mark Johnson,
and James Curran. 2012. Improving combina-
tory categorial grammar parse reranking with depen-
dency grammar features. In Proceedings of COL-
ING 2012, pages 1441?1458, Mumbai, India.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Clay-
pool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98, Ann Arbor, Michigan.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 624?
631, Prague, Czech Republic.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,
Yusuke Miyao, and Hideki Mima. 2013. Inte-
grating multiple dependency corpora for inducing
wide-coverage Japanese CCG resources. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1042?1051, Sofia, Bulgaria.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391?1400,
Mumbai, India, December.
163
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?609,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using CCG categories to improve Hindi dependency parsing
Bharat Ram Ambati Tejaswini Deoskar
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
bharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.uk
Mark Steedman
Abstract
We show that informative lexical cate-
gories from a strongly lexicalised for-
malism such as Combinatory Categorial
Grammar (CCG) can improve dependency
parsing of Hindi, a free word order lan-
guage. We first describe a novel way to
obtain a CCG lexicon and treebank from
an existing dependency treebank, using a
CCG parser. We use the output of a su-
pertagger trained on the CCGbank as a
feature for a state-of-the-art Hindi depen-
dency parser (Malt). Our results show that
using CCG categories improves the accu-
racy of Malt on long distance dependen-
cies, for which it is known to have weak
rates of recovery.
1 Introduction
As compared to English, many Indian languages
including Hindi have a freer word order and are
also morphologically richer. These characteristics
pose challenges to statistical parsers. Today, the
best dependency parsing accuracies for Hindi are
obtained by the shift-reduce parser of Nivre et
al. (2007) (Malt). It has been observed that Malt
is relatively accurate at recovering short distance
dependencies, like arguments of a verb, but is less
accurate at recovering long distance dependencies
like co-ordination, root of the sentence, etc
(Mcdonald and Nivre, 2007; Ambati et al, 2010).
In this work, we show that using CCG lexical
categories (Steedman, 2000), which contain sub-
categorization information and capture long dis-
tance dependencies elegantly, can help Malt with
those dependencies. Section 2 first shows how we
extract a CCG lexicon from an existing Hindi de-
pendency treebank (Bhatt et al, 2009) and then
use it to create a Hindi CCGbank. In section 3, we
develop a supertagger using the CCGbank and ex-
plore different ways of providing CCG categories
from the supertagger as features to Malt. Our re-
sults show that using CCG categories can help
Malt by improving the recovery of long distance
relations.
2 A CCG Treebank from a Dependency
Treebank
There have been some efforts at automatically ex-
tracting treebanks of CCG derivations from phrase
structure treebanks (Hockenmaier and Steedman,
2007; Hockenmaier, 2006; Tse and Curran, 2010),
and CCG lexicons from dependency treebanks
(C?ak?c?, 2005). Bos et al (2009) created a
CCGbank from an Italian dependency treebank by
converting dependency trees into phrase structure
trees and then applying an algorithm similar
to Hockenmaier and Steedman (2007). In this
work, following C?ak?c? (2005), we first extract a
Hindi CCG lexicon from a dependency treebank.
We then use a CKY parser based on the CCG
formalism to automatically obtain a treebank
of CCG derivations from this lexicon, a novel
methodology that may be applicable to obtaining
CCG treebanks in other languages as well.
2.1 Hindi Dependency Treebank
In this paper, we work with a subset of the Hindi
Dependency Treebank (HDT ver-0.5) released
as part of Coling 2012 Shared Task on parsing
(Bharati et al, 2012). HDT is a multi-layered
dependency treebank (Bhatt et al, 2009) an-
notated with morpho-syntactic (morphological,
part-of-speech and chunk information) and
syntactico-semantic (dependency) information
(Bharati et al, 2006; Bharati et al, 2009).
Dependency labels are fine-grained, and mark de-
pendencies that are syntactico-semantic in nature,
such as agent (usually corresponding to subject),
patient (object), and time and place expressions.
There are special labels to mark long distance
relations like relative clauses, co-ordination etc
604
(Bharati et al, 1995; Bharati et al, 2009).
The treebank contains 12,041 training, 1,233
development and 1,828 testing sentences with an
average of 22 words per sentence. We used the
CoNLL format1 for our purposes, which contains
word, lemma, pos-tag, and coarse pos-tag in the
WORD, LEMMA, POS, and CPOS fields respectively
and morphological features and chunk information
in the FEATS column.
2.2 Algorithm
We first made a list of argument and adjunct
dependency labels in the treebank. For e.g.,
dependencies with the label k1 and k2 (corre-
sponding to subject and object respectively) are
considered to be arguments, while labels like
k7p and k7t (corresponding to place and time
expressions) are considered to be adjuncts. For
readability reasons, we will henceforth refer to
dependency labels with their English equivalents
(e.g., SUBJ, OBJ, PURPOSE, CASE for k1, k2,
rt, lwg psp respectively).
Starting from the root of the dependency tree,
we traverse each node. The category of a node de-
pends on both its parent and children. If the node
is an argument of its parent, we assign the chunk
tag of the node (e.g., NP, PP) as its CCG category.
Otherwise, we assign it a category of X|X, where
X is the parent?s result category and | is direction-
ality (\ or /), which depends on the position of
the node w.r.t. its parent. The result category of
a node is the category obtained once its arguments
are resolved. For example, S, is the result category
for (S\NP)\NP. Once we get the partial category
of a node based on the node?s parent information,
we traverse through the children of the node. If
a child is an argument, we add that child?s chunk
tag, with appropriate directionality, to the node?s
category. The algorithm is sketched in Figure 1
and an example of a CCG derivation for a simple
sentence (marked with chunk tags; NP and VGF
are the chunk tags for noun and finite verb chunks
respectively.) is shown in Figure 2. Details of
some special cases are described in the following
subsections.
We created two types of lexicon. In Type 1,
we keep morphological information in noun cate-
gories and in Type 2, we don?t. For example, con-
sider a noun chunk ?raam ne (Ram ERG)?. In Type
1, CCG categories for ?raam? and ?ne? are NP and
1http://nextens.uvt.nl/depparse-wiki/DataFormat
ModifyTree(DependencyTree tree);
for (each node in tree):
handlePostPositionMarkers(node);
handleCoordination(node);
handleRelativeClauses(node);
if (node is an argument of parent):
cat = node.chunkTag;
else:
prescat = parent.resultCategory;
cat = prescat + getDir(node, parent) + prescat;
for(each child of node):
if (child is an argument of node):
cat = cat + getDir(child, node) + child.chunkTag;
Figure 1: Algorithm for extracting CCG lexicon
from a dependency tree.
ROOT mohan ne raam ke lie kitaab khariidii
Mohan Erg Ram for book buy-past-fem
ROOT
OBJ
PURPOSE
CASE
SUBJ
CASE
[NP mohan ne] [NP raam ke lie] [NP kitaab] [VGF khariidii]
NP NP\NP NP (VGF/VGF)\NP NP (VGF\NP)\NP
< < <NP VGF/VGF VGF\NP
< B?VGF\NP
<VGF?Mohan bought a book for Ram.?
Figure 2: An example dependency tree with its
CCG derivation (Erg = Ergative case).
NP[ne]\NP respectively. In Type 2, respective
CCG categories for ?raam? and ?ne? are NP and
NP\NP. Morphological information such as case
(e.g., Ergative case - ?ne?) in noun categories is ex-
pected to help with determining their dependency
labels, but makes the lexicon more sparse.
2.3 Morphological Markers
In Hindi, morphological information is encoded in
the form of post-positional markers on nouns, and
tense, aspect and modality markers on verbs. A
post-positional marker following a noun plays the
role of a case-marker (e.g., ?raam ne (Ram ERG)?,
here ?ne? is the ergative case marker) and can also
have a role similar to English prepositions (e.g.,
?mej par (table on)?). Post-positional markers on
nouns can be simple one word expressions like
?ne? or ?par? or can be multiple words as in ?raam
ke lie (Ram for)?. Complex post position markers
as a whole give information about how the head
noun or verb behaves. We merged complex post
position markers into single words like ?ke lie? so
605
that the entire marker gets a single CCG category.
For an adjunct like ?raam ke lie (for Ram)?
in Figure 2, ?raam? can have a CCG category
VGF/VGF as it is the head of the chunk and
?ke lie? a category of (VGF/VGF)\(VGF/VGF).
Alternatively, if we pass the adjunct information
to the post-position marker (?ke lie?), and use the
chunk tag ?NP? as the category for the head word
(?raam?), then categories of ?raam? and ?ke lie? are
NP and (VGF/VGF)\NP respectively. Though
both these analysis give the same semantics, we
chose the latter as it leads to a less sparse lexi-
con. Also, adjuncts that modify adjacent adjuncts
are assigned identical categories X/X making use
of CCG?s composition rule and following C?ak?c?
(2005).
2.4 Co-ordination
The CCG category of a conjunction is (X\X)/X,
where a conjunction looks for a child to its right
and then a child to its left. To handle conjunc-
tion with multiple children, we modified the de-
pendency tree, as follows.
For the example given below, in the original de-
pendency tree, the conjunction ora ?and? has three
children ?Ram? , ?Shyam? and ?Sita?. We modified
the original dependency tree and treat the comma
?,? as a conjunction. As a result, ?,? will have ?Ram?
and ?Shyam? as children and ?and? will have ?,? and
?Sita? as children. It is straightforward to convert
this tree into the original dependency tree for the
purpose of evaluation/comparison with other de-
pendency parsers.
ROOT raam , shyam ora siitaa skoola gaye
ROOT
DEST
SUBJ
COORD
COORD
COORDCOORD
raam , shyam ora siitaa skoola gayeRam , Shyam and Sita school went
NP (NP\NP)/NP NP (NP\NP)/NP NP NP (VGF\NP)\NP
> > <NP\NP NP\NP VGF\NP
<NP <NP <VGF?Ram , Shyam and Sita went to school.?
2.5 Relative Clauses
In English, relative clauses have the category type
NP\NP, where they combine with a noun phrase
on the left to give a resulting noun phrase. Hindi,
due to its freer word order, has relative clauses of
the type NP\NP or NP/NP based on the position of
the relative clause with respect to the head noun.
Similar to English, the relative pronoun has a CCG
category of (NP|NP)|X where directionality de-
pends on the position of the relative pronoun in the
clause and the category X depends on the gram-
matical role of the relative pronoun. In the follow-
ing example, X is VGF\NP
ROOT vaha ladakaa jo baithaa hai raam hai
ROOT
OBJ
SUBJ
DEM RELCSUBJ AUX
vaha ladakaa jo baithaa hai raam haithat boy who sitting be-1P-pres Ram be-1P-pres
NP/NP NP (NP\NP)/X VGF\NP VGF\VGF NP (VGF\NP)\NP
> > B? <NP VGF\NP VGF\NP
>NP\NP
>NP <VGF
?The boy who is sitting is Ram?
2.6 CCG Lexicon to Treebank conversion
We use a CCG parser to convert the CCG lexicon
to a CCG treebank as conversion to CCG trees
directly from dependency trees is not straight-
forward. Using the above algorithm, we get one
CCG category for every word in a sentence. We
then run a non-statistical CKY chart parser based
on the CCG formalism2, which gives CCG deriva-
tions based on the lexical categories. This gives
multiple derivations for some sentences. We rank
these derivations using two criteria. The first cri-
terion is correct recovery of the gold dependency
tree. Derivations which lead to gold dependencies
are given higher weight. In the second criterion,
we prefer derivations which yield intra-chunk de-
pendencies (e.g., verb and auxiliary) prior to inter-
chunk (e.g., verb and its arguments). For exam-
ple, morphological markers (which lead to intra-
chunk dependencies) play a crucial role in identi-
fying correct dependencies . Resolving these de-
pendencies first helps parsers in better identifica-
tion of inter-chunk dependencies such as argument
structure of the verb (Ambati, 2011). We thus ex-
tract the best derivation for each sentence and cre-
ate a CCGbank for Hindi. Coverage, i.e., number
of sentences for which we got at least one com-
plete derivation, using this lexicon is 96%. The
remaining 4% are either cases of wrong annota-
tions in the original treebank, or rare constructions
which are currently not handled by our conversion
algorithm.
2http://openccg.sourceforge.net/
606
3 Experiments
In this section, we first describe the method of de-
veloping a supertagger using the CCGbank. We
then describe different ways of providing CCG
categories from the supertagger as features to a
state-of-the-art Hindi Dependency parser (Malt).
We did all our experiments using both gold fea-
tures (pos, chunk and morphological information)
provided in the treebank and automatic features
extracted using a Hindi shallow parser3. We re-
port results with automatic features but we also
obtained similar improvements with gold features.
3.1 Category Set
For supertagging, we first obtained a category set
from the CCGbank training data. There are 2,177
and 718 category types in Type 1 (with morph. in-
formation) and Type 2 (without morph. informa-
tion) data respectively. Clark and Curran (2004)
showed that using a frequency cutoff can signif-
icantly reduce the size of the category set with
only a small loss in coverage. We explored dif-
ferent cut-off values and finally used a cutoff of
10 for building the tagger. This reduced the cat-
egory types to 376 and 202 for Type 1 and Type
2 respectively. The percent of category tokens in
development data that don?t appear in the category
set entrailed by this cut-off are 1.39 & 0.47 for
Type 1 and Type 2 respectively.
3.2 Supertagger
Following Clark and Curran (2004), we used
a Maximum Entropy approach to build our su-
pertagger. We explored different features in the
context of a 5-word window surrounding the tar-
get word. We used features based on WORD (w),
LEMMA (l), POS (p), CPOS (c) and the FEATS (f )
columns of the CoNLL format. Table 1 shows the
impact of different features on supertagger perfor-
mance. Experiments 1, 2, 3 have current word (wi)
features while Experiments 4, 5, 6 show the im-
pact of contextual and complex bi-gram features.
Accuracy of the supertagger after Experiment 6
is 82.92% and 84.40% for Type 1 and Type 2 data
respectively. As the number of category types in
Type 1 data (376) are much higher than in Type 2
(202), it is not surprising that the performance of
the supertagger is better for Type 2 as compared to
Type 1.
3http://ltrc.iiit.ac.in/analyzer/hindi/
Experiments: Features Accuracy
Type 1 Type 2
Exp 1: wi, pi 75.14 78.47
Exp 2: Exp 1 + li, ci 77.58 80.17
Exp 3: Exp 2 + fi 80.43 81.88
Exp 4: Exp 3 +wi?1,wi?2, pi?1,pi?2, 82.72 84.15
wi+1, wi+2, pi+1, pi+2
Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29
Exp 6: Exp 5 + wi?2wi?1, wi?1wi, 82.92 84.40
wiwi+1, wi+1wi+2, pi?2pi?1,
pi?1pi, pipi+1, pi+1pi+2
Table 1: Impact of different features on the su-
pertagger performance for development data.
3.3 Dependency Parsing
There has been a significant amount of work on
Hindi dependency parsing in the recent past (Hu-
sain, 2009; Husain et al, 2010; Bharati et al,
2012). Out of all these efforts, state-of-the-art ac-
curacy is achieved using the Malt parser. We first
run Malt with previous best settings (Bharati et
al., 2012) which use the arc-standard parsing al-
gorithm with a liblinear learner, and treat this as
our baseline. We compare and analyze results af-
ter adding supertags as features with this baseline.
3.4 Using Supertags as Features to Malt
C?ak?c? (2009) showed that using gold CCG cate-
gories extracted from dependency trees as features
to MST parser (McDonald et al, 2006) boosted
the performance for Turkish. But using automatic
categories from a supertagger radically decreased
performance in their case as supertagger accuracy
was very low. We have explored different ways
of incorporating both gold CCG categories and
supertagger-provided CCG categories into depen-
dency parsing. Following C?ak?c? (2009), instead
of using supertags for all words, we used supertags
which occurred at least K times in the training
data, and backed off to coarse POS-tags otherwise.
We experimented with different values of K and
found that K=15 gave the best results.
We first provided gold CCG categories as fea-
tures to the Malt parser and then provided the out-
put of the supertagger described in section 3.2. We
did all these experiments with both Type 1 and
Type 2 data. Unlabelled Attachment Scores (UAS)
and Labelled Attachment Scores (LAS) for Malt
607
are shown in Table 2. As expected, gold CCG
categories boosted UAS and LAS by around 6%
and 7% respectively, for both Type 1 and Type 2
data. This clearly shows that the rich subcatego-
rization information provided by CCG categories
can help a shift-reduce parser. With automatic cat-
egories from a supertagger, we also got improve-
ments over the baseline, for both Type 1 and Type
2 data. All the improvements are statistically sig-
nificant (McNemar?s test, p < 0.01).
With gold CCG categories, Type 1 data gave
slightly better improvements over Type 2 as Type
1 data has richer morphological information. But,
in the case of supertagger output, Type 2 data
gave more improvements over the baseline Malt
as compared to Type 1. This is because the perfor-
mance of the supertagger on Type 2 data is slightly
better than that of Type 1 data (see Table 1).
Experiment Development Testing
UAS LAS UAS LAS
Malt: Baseline 89.09 83.46 88.67 83.04
Malt + Type 1 Gold 95.87* 90.79* 95.27* 90.22*
Malt + Type 2 Gold 95.73* 90.70* 95.26* 90.18*
Malt + Type 1 ST 89.54* 83.68* 88.93* 83.23*
Malt + Type 2 ST 89.90* 83.96* 89.04* 83.35*
Table 2: Supertagger impact on Hindi dependency
parsing (ST=Supertags). McNemar?s test, * = p <
0.01.
It is interesting to notice the impact of using
automatic CCG categories from a supertagger on
long distance dependencies. It is known that Malt
is weak at long-distance relations (Mcdonald and
Nivre, 2007; Ambati et al, 2010). Providing
CCG categories as features improved handling of
long-distance dependencies for Malt. Figure 3
shows the F-score of the impact of CCG categories
on three dependency labels, which take the ma-
jor share of long distance dependencies, namely,
ROOT, COORD, and RELC, the labels for sentence
root, co-ordination, and relative clause respec-
tively. For these relations, providing CCG cate-
gories gave an increment of 1.2%, 1.4% and 1.6%
respectively over the baseline.
We also found that the impact of CCG cate-
gories is higher when the span of the dependency
is longer. Figure 4 shows the F-score of the impact
of CCG categories on dependencies based on the
distance between words. Using CCG categories
73.12 80.9 
29.89 
74.35 82.28 
31.46 
25 
50 
75 
100 
ROOT COORD RELC 
F-sco
re 
Dependency Labels 
Malt Malt + Type 2 ST 
Figure 3: Label-wise impact of supertag features.
does not have much impact on short distance de-
pendencies (1?5), which Malt is already good at.
For longer range distances, 6?10, and >10, there
is an improvement of 1.8% and 1.4% respectively.
97.3 
79.0 76.4 
97.5 
80.8 77.8 
70 
80 
90 
100 
1 - 5 6 - 10 > 10  
F-sco
re 
Distance ranges  
Malt Malt + Type 2 ST 
Figure 4: Impact of supertags on distance ranges.
4 Conclusion and Future Direction
We have presented an approach for automatically
extracting a CCG lexicon from a dependency tree-
bank for Hindi. We have also presented a novel
way of creating a CCGbank from a dependency
treebank using a CCG parser and the CCG lex-
icon. Unlike previous work, we obtained im-
provements in dependency recovery using auto-
matic supertags, as well as gold information. We
have shown that informative CCG categories im-
prove the performance of a shift-reduce depen-
dency parser (Malt) in recovering some long dis-
tance relations. In future work, we would like to
directly train a CCG shift-reduce parser (such as
Zhang and Clark (2011)?s English parser) on the
Hindi CCGbank. We would also like to see the
impact of generalisation of our lexicon using the
free-word order formalism for CCG categories of
Baldridge (2002).
Acknowledgements
We would like to thank three anonymous review-
ers for their useful suggestions. This work was
supported by ERC Advanced Fellowship 249520
GRAMPLUS.
608
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010.
Two Methods to Incorporate ?Local Morphosyntac-
tic? Features in Hindi Dependency Parsing. In Pro-
ceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 22?30, Los Angeles, CA, USA, June.
Bharat Ram Ambati. 2011. Hindi Dependency Parsing
and Treebank Validation. Master?s Thesis, Interna-
tional Institute of Information Technology - Hyder-
abad, India.
Jason M. Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, UK.
Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.
1995. Natural Language Processing: A Paninian
Perspective. Prentice-Hall of India, pages 65?106.
Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma,
and Lakshmi Bai. 2006. AnnCorra: Annotating
Corpora Guidelines for POS and Chunk Annotation
for Indian Languages. In Technical Report (TR-
LTRC-31), LTRC, IIIT-Hyderabad.
Akshar Bharati, Dipti Misra Sharma, Samar
Husain, Lakshmi Bai, Rafiya Begum, and
Rajeev Sangal. 2009. AnnCorra: Tree-
Banks for Indian Languages, Guidelines for
Annotating Hindi TreeBank (version 2.0).
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf.
Akshar Bharati, Prashanth Mannem, and Dipti Misra
Sharma. 2012. Hindi Parsing Shared Task. In Pro-
ceedings of Coling Workshop on Machine Transla-
tion and Parsing in Indian Languages, Kharagpur,
India.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for Hindi/Urdu. In Proceedings of the
Third Linguistic Annotation Workshop at 47th ACL
and 4th IJCNLP, pages 186?189, Suntec, Singapore.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a Dependency Treebank to a Cat-
egorial Grammar Treebank for Italian. In M. Pas-
sarotti, Adam Przepio?rkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27?38, Milan, Italy.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of Student Re-
search Workshop, 43rd Annual Meeting of the ACL,
pages 73?78.
Ruket C?ak?c?. 2009. Parser Models for a Highly In-
flected Language. Ph.D. thesis, University of Edin-
burgh, UK.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282?288.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396, Septem-
ber.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 505?512, Sydney, Aus-
tralia.
Samar Husain, Prashanth Mannem, Bharat Ram Am-
bati, and Phani Gadde. 2010. The ICON-
2010 Tools Contest on Indian Language Depen-
dency Parsing. In Proceedings of ICON-2010 Tools
Contest on Indian Language Dependency Parsing,
Kharagpur, India.
Samar Husain. 2009. Dependency Parsers for Indian
Languages. In Proceedings of the ICON09 NLP
Tools Contest: Indian Language Dependency Pars-
ing, India.
Ryan Mcdonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
and Natural Language Learning.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216?220, New
York City, New York.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the Penn
Chinese Treebank. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1083?1091, Beijing, China.
Yue Zhang and Stephen Clark. 2011. Shift-Reduce
CCG Parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
683?692, Portland, Oregon, USA, June.
609
