Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 249?252,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Introduction of a new paraphrase generation tool
based on Monte-Carlo sampling
Jonathan Chevelu
1,2
Thomas Lavergne Yves Lepage
1
Thierry Moudenc
2
(1) GREYC, universit? de Caen Basse-Normandie
(2) Orange Labs; 2, avenue Pierre Marzin, 22307 Lannion
{jonathan.chevelu,thierry.moudenc}@orange-ftgroup.com,
thomas.lavergne@reveurs.org, yves.lepage@info.unicaen.fr
Abstract
We propose a new specifically designed
method for paraphrase generation based
on Monte-Carlo sampling and show how
this algorithm is suitable for its task.
Moreover, the basic algorithm presented
here leaves a lot of opportunities for fu-
ture improvement. In particular, our algo-
rithm does not constraint the scoring func-
tion in opposite to Viterbi based decoders.
It is now possible to use some global fea-
tures in paraphrase scoring functions. This
algorithm opens new outlooks for para-
phrase generation and other natural lan-
guage processing applications like statis-
tical machine translation.
1 Introduction
A paraphrase generation system is a program
which, given a source sentence, produces a differ-
ent sentence with almost the same meaning.
Paraphrase generation is useful in applications
to choose between different forms to keep the
most appropriate one. For instance, automatic
summary can be seen as a particular paraphrasing
task (Barzilay and Lee, 2003) with the aim of se-
lecting the shortest paraphrase.
Paraphrases can also be used to improve natu-
ral language processing (NLP) systems. (Callison-
Burch et al, 2006) improved machine translations
by augmenting the coverage of patterns that can
be translated. Similarly, (Sekine, 2005) improved
information retrieval based on pattern recognition
by introducing paraphrase generation.
In order to produce paraphrases, a promising
approach is to see the paraphrase generation prob-
lem as a translation problem, where the target lan-
guage is the same as the source language (Quirk et
al., 2004; Bannard and Callison-Burch, 2005).
A problem that has drawn less attention is the
generation step which corresponds to the decoding
step in SMT. Most paraphrase generation tools use
some standard SMT decoding algorithms (Quirk et
al., 2004) or some off-the-shelf decoding tools like
MOSES (Koehn et al, 2007). The goal of a de-
coder is to find the best path in the lattice produced
from a paraphrase table. This is basically achieved
by using dynamic programming and especially the
Viterbi algorithm associated with beam searching.
However decoding algorithms were designed
for translation, not for paraphrase generation. Al-
though left-to-right decoding is justified for trans-
lation, it may not be necessary for paraphrase
generation. A paraphrase generation tool usually
starts with a sentence which may be very similar to
some potential solution. In other words, there is no
need to "translate" all of the sentences. Moreover,
decoding may not be suitable for non-contiguous
transformation rules.
In addition, dynamic programming imposes an
incremental scoring function to evaluate the qual-
ity of each hypothesis. For instance, it cannot cap-
ture some scattered syntactical dependencies. Im-
proving on this major issue is a key point to im-
prove paraphrase generation systems.
This paper first presents an alternative to decod-
ing that is based on transformation rule application
in section 2. In section 3 we propose a paraphrase
generation method for this paradigm based on an
algorithm used in two-player games. Section 4
briefly explain experimental context and its asso-
ciated protocol for evaluation of the proposed sys-
tem. We compare the proposed algorithm with a
baseline system in section 5. Finally, in section 6,
we point to future research tracks to improve para-
phrase generation tools.
2 Statistical paraphrase generation using
transformation rules
The paraphrase generation problem can be seen as
an exploration problem. We seek the best para-
phrase according to a scoring function in a space
249
to search by applying successive transformations.
This space is composed of states connected by ac-
tions. An action is a transformation rule with a
place where it applies in the sentence. States are a
sentence with a set of possible actions. Applying
an action in a given state consists in transforming
the sentence of the state and removing all rules that
are no more applicable. In our framework, each
state, except the root, can be a final state. This
is modelised by adding a stop rule as a particular
action. We impose the constraint that any trans-
formed part of the source sentence cannot be trans-
formed anymore.
This paradigm is more approriate for paraphrase
generation than the standard SMT approach in re-
spect to several points: there is no need for left-
to-right decoding because a transformation can be
applied anywhere without order; there is no need
to transform the whole of a sentence because each
state is a final state; there is no need to keep the
identity transformation for each phrase in the para-
phrase table; the only domain knowledge needed
is a generative model and a scoring function for
final states; it is possible to mix different genera-
tive models because a statistical paraphrase table,
an analogical solver and a paraphrase memory for
instance; there is no constraint on the scoring func-
tion because it only scores final states.
Note that the branching factor with a paraphrase
table can be around thousand actions per states
which makes the generation problem a difficult
computational problem. Hence we need an effi-
cient generation algorithm.
3 Monte-Carlo based Paraphrase
Generation
UCT (Kocsis and Szepesv?ri, 2006) (Upper Con-
fidence bound applied to Tree) is a Monte-Carlo
planning algorithm that have some interesting
properties: it grows the search tree non-uniformly
and favours the most promising sequences, with-
out pruning branch; it can deal with high branch-
ing factor; it is an any-time algorithm and returns
best solution found so far when interrupted; it does
not require expert domain knowledge to evaluate
states. These properties make it ideally suited for
games with high branching factor and for which
there is no strong evaluation function.
For the same reasons, this algorithm sounds in-
teresting for paraphrase generation. In particular,
it does not put constraint on the scoring function.
We propose a variation of the UCT algorithm for
paraphrase generation named MCPG for Monte-
Carlo based Paraphrase Generation.
The main part of the algorithm is the sampling
step. An episode of this step is a sequence of states
and actions, s
1
, a
1
, s
2
, a
2
, . . . , s
T
, from the root
state to a final state. During an episode construc-
tion, there are two ways to select the action a
i
to
perfom from a state s
i
.
If the current state was already explored in a
previous episode, the action is selected accord-
ing to a compromise between exploration and ex-
ploitation. This compromise is computed using
the UCB-Tunned formula (Auer et al, 2001) as-
sociated with the RAVE heuristic (Gelly and Sil-
ver, 2007). If the current state is explored for
the first time, its score is estimated using Monte-
Carlo sampling. In other word, to complete the
episode, the actions a
i
, a
i+1
, . . . , a
T?1
, a
T
are se-
lected randomly until a stop rule is drawn.
At the end of each episode, a reward is com-
puted for the final state s
T
using a scoring func-
tion and the value of each (state, action) pair of the
episode is updated. Then, the algorithm computes
an other episode with the new values.
Periodically, the sampling step is stopped and
the best action at the root state is selected. This
action is then definitely applied and a sampling
is restarted from the new root state. The action
sequence is built incrementally and selected af-
ter being enough sampled. For our experiments,
we have chosen to stop sampling regularly after a
fixed amount ? of episodes.
Our main adaptation of the original algorithm
is in the (state, action) value updating procedure.
Since the goal of the algorithm is to maximise a
scoring function, we use the maximum reachable
score from a state as value instead of the score ex-
pectation. This algorithm suits the paradigm pro-
posed for paraphrase generation.
4 Experimental context
This section describes the experimental context
and the methodology followed to evaluate our sta-
tistical paraphrase generation tool.
4.1 Data
For the experiment reported in section 5, we use
one of the largest, multi-lingual, freely available
aligned corpus, Europarl (Koehn, 2005). It con-
sists of European parliament debates. We choose
250
French as the language for paraphrases and En-
glish as the pivot language. For this pair of lan-
guages, the corpus consists of 1, 487, 459 French
sentences aligned with 1, 461, 429 English sen-
tences. Note that the sentences in this corpus
are long, with an average length of 30 words per
French sentence and 27.1 for English. We ran-
domly extracted 100 French sentences as a test
corpus.
4.2 Language model and paraphrase table
Paraphrase generation tools based on SMT meth-
ods need a language model and a paraphrase table.
Both are computed on a training corpus.
The language models we use are n-gram lan-
guage models with back-off. We use SRILM (Stol-
cke, 2002) with its default parameters for this pur-
pose. The length of the n-grams is five.
To build a paraphrase table, we use the con-
struction method via a pivot language proposed
in (Bannard and Callison-Burch, 2005).
Three heuristics are used to prune the para-
phrase table. The first heuristic prunes any entry
in the paraphrase table composed of tokens with a
probability lower than a threshold . The second,
called pruning pivot heuristic, consists in deleting
all pivot clusters larger than a threshold ? . The
last heuristic keeps only the ? most probable para-
phrases for each source phrase in the final para-
phrase table. For this study, we empirically fix
 = 10
?5
, ? = 200 and ? = 10.
4.3 Evaluation Protocol
We developed a dedicated website to allow the hu-
man judges with some flexibility in workplaces
and evaluation periods. We retain the principle of
the two-step evaluation, common in the machine
translation domain and already used for para-
phrase evaluation (Bannard and Callison-Burch,
2005).
The question asked to the human evaluator for
the syntactic task is: Is the following sentence in
good French? The question asked to the human
evaluator for the semantic task is: Do the following
two sentences express the same thing?
In our experiments, each paraphrase was evalu-
ated by two native French evaluators.
5 Comparison with a SMT decoder
In order to validate our algorithm for paraphrase
generation, we compare it with an off-the-shelf
SMT decoder.
We use the MOSES decoder (Koehn et al, 2007)
as a baseline. The MOSES scoring function is
set by four weighting factors ?
?
, ?
LM
, ?
D
, ?
W
.
Conventionally, these four weights are adjusted
during a tuning step on a training corpus. The
tuning step is inappropriate for paraphrase because
there is no such tuning corpus available. We em-
pirically set ?
?
= 1, ?
LM
= 1, ?
D
= 10 and
?
W
= 0. Hence, the scoring function (or reward
function for MCPG) is equivalent to:
R(f
?
|f, I) = p(f
?
)? ?(f |f
?
, I)
where f and f
?
are the source and target sen-
tences, I a segmentation in phrases of f , p(f
?
)
the language model score and ?(f |f
?
, I) =
?
i?I
p(f
i
|f
?i
) the paraphrase table score.
The MCPG algorithm needs two parameters.
One is the number of episodes ? done before se-
lecting the best action at root state. The other is
k, an equivalence parameter which balances the
exploration/exploitation compromise (Auer et al,
2001). We empirically set ? = 1, 000, 000 and
k = 1, 000.
For our algorithm, note that identity paraphrase
probabilities are biased: for each phrase it is
equal to the probability of the most probable para-
phrase. Moreover, as the source sentence is the
best meaning preserved "paraphrase", a sentence
cannot have a better score. Hence, we use a
slightly different scoring function:
R(f
?
|f, I) = min
?
?
?
?
p(f
?
)
p(f)
?
i?I
f
i
6=f
?i
p(f
i
|f
?i
)
p(f
i
|f
i
)
, 1
?
?
?
?
Note that for this model, there is no need to know
the identity transformations probability for un-
changed part of the sentence.
Results are presented in Table 1. The Kappa
statistics associated with the results are 0.84, 0.64
and 0.59 which are usually considered as a "per-
fect", "substantial" and "moderate" agreement.
Results are close to evaluations from the base-
line system. The main differences are from Kappa
statistics which are lower for the MOSES system
evaluation. Judges changed between the two ex-
periments. We may wonder whether an evaluation
with only two judges is reliable. This points to the
ambiguity of any paraphrase definition.
251
System MOSES MCPG
Well formed (Kappa) 64%(0.57) 63%(0.84)
Meaning preserved (Kappa) 58%(0.48) 55%(0.64)
Well formed and meaning preserved (Kappa) 50%(0.54) 49%(0.59)
Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan-
guage. Comparison between the baseline system MOSES and our algorithm MCPG.
By doing this experiment, we have shown that
our algorithm with a biased paraphrase table is
state-of-the-art to generate paraphrases.
6 Conclusions and further research
In this paper, we have proposed a different
paradigm and a new algorithm in NLP field
adapted for statistical paraphrases generation.
This method, based on large graph exploration by
Monte-Carlo sampling, produces results compa-
rable with state-of-the-art paraphrase generation
tools based on SMT decoders.
The algorithm structure is flexible and generic
enough to easily work with discontinous patterns.
It is also possible to mix various transformation
methods to increase paraphrase variability.
The rate of ill-formed paraphrase is high at
37%. The result analysis suggests an involvement
of the non-preservation of the original meaning
when a paraphrase is evaluated ill-formed. Al-
though the mesure is not statistically significant
because the test corpus is too small, the same trend
is also observed in other experiments. Improv-
ing on the language model issue is a key point to
improve paraphrase generation systems. Our al-
gorithm can work with unconstraint scoring func-
tions, in particular, there is no need for the scor-
ing function to be incremental as for Viterbi based
decoders. We are working to add, in the scoring
function, a linguistic knowledge based analyzer to
solve this problem.
Because MCPG is based on a different paradigm,
its output scores cannot be directly compared to
MOSES scores. In order to prove the optimisa-
tion qualities of MCPG versus state-of-the-art de-
coders, we are transforming our paraphrase gener-
ation tool into a translation tool.
References
P. Auer, N. Cesa-Bianchi, and C. Gentile. 2001. Adap-
tive and self-confident on-line learning algorithms.
Machine Learning.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Annual
Meeting of ACL, pages 597?604, Morristown, NJ,
USA. Association for Computational Linguistics.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In HLT-NAACL
2003: Main Proceedings, pages 16?23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In HLT-NAACL 2006: Main
Proceedings, pages 17?24, Morristown, NJ, USA.
Association for Computational Linguistics.
Sylvain Gelly and David Silver. 2007. Combining on-
line and offline knowledge in UCT. In 24th Interna-
tional Conference on Machine Learning (ICML?07),
pages 273?280, June.
Levente Kocsis and Csaba Szepesv?ri. 2006. Bandit
based monte-carlo planning. In 17th European Con-
ference on Machine Learning, (ECML?06), pages
282?293, September.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of ACL, Demonstra-
tion Session, pages 177?180, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Chris Quirk, Chris Brockett, and Bill Dolan. 2004.
Monolingual machine translation for paraphrase
generation. In Dekang Lin and Dekai Wu, edi-
tors, the 2004 Conference on Empirical Methods
in Natural Language Processing, pages 142?149.,
Barcelona, Spain, 25-26 July. Association for Com-
putational Linguistics.
Satoshi Sekine. 2005. Automatic paraphrase discov-
ery based on context and keywords between ne pairs.
In Proceedings of International Workshop on Para-
phrase (IWP2005).
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing.
252
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504?513,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Practical very large scale CRFs
Thomas Lavergne
LIMSI ? CNRS
lavergne@limsi.fr
Olivier Cappe?
Te?le?com ParisTech
LTCI ? CNRS
cappe@enst.fr
Franc?ois Yvon
Universite? Paris-Sud 11
LIMSI ? CNRS
yvon@limsi.fr
Abstract
Conditional Random Fields (CRFs) are
a widely-used approach for supervised
sequence labelling, notably due to their
ability to handle large description spaces
and to integrate structural dependency be-
tween labels. Even for the simple linear-
chain model, taking structure into account
implies a number of parameters and a
computational effort that grows quadrati-
cally with the cardinality of the label set.
In this paper, we address the issue of train-
ing very large CRFs, containing up to hun-
dreds output labels and several billion fea-
tures. Efficiency stems here from the spar-
sity induced by the use of a `1 penalty
term. Based on our own implementa-
tion, we compare three recent proposals
for implementing this regularization strat-
egy. Our experiments demonstrate that
very large CRFs can be trained efficiently
and that very large models are able to im-
prove the accuracy, while delivering com-
pact parameter sets.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et
al., 2001; Sutton and McCallum, 2006) constitute
a widely-used and effective approach for super-
vised structure learning tasks involving the map-
ping between complex objects such as strings and
trees. An important property of CRFs is their abil-
ity to handle large and redundant feature sets and
to integrate structural dependency between out-
put labels. However, even for simple linear chain
CRFs, the complexity of learning and inference
This work was partly supported by ANR projects CroTaL
(ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-
02).
grows quadratically with respect to the number of
output labels and so does the number of structural
features, ie. features testing adjacent pairs of la-
bels. Most empirical studies on CRFs thus ei-
ther consider tasks with a restricted output space
(typically in the order of few dozens of output la-
bels), heuristically reduce the use of features, es-
pecially of features that test pairs of adjacent la-
bels1, and/or propose heuristics to simulate con-
textual dependencies, via extended tests on the ob-
servations (see discussions in, eg., (Punyakanok
et al, 2005; Liang et al, 2008)). Limitating the
feature set or the number of output labels is how-
ever frustrating for many NLP tasks, where the
type and number of potentially relevant features
are very large. A number of studies have tried to
alleviate this problem. Pal et al (2006) propose
to use a ?sparse? version of the forward-backward
algorithm during training, where sparsity is en-
forced through beam pruning. Related ideas are
discussed by Dietterich et al (2004); by Cohn
(2006), who considers ?generalized? feature func-
tions; and by Jeong et al (2009), who use approx-
imations to simplify the forward-backward recur-
sions. In this paper, we show that the sparsity that
is induced by `1-penalized estimation of CRFs can
be used to reduce the total training time, while
yielding extremely compact models. The benefits
of sparsity are even greater during inference: less
features need to be extracted and included in the
potential functions, speeding up decoding with a
lesser memory footprint. We study and compare
three different ways to implement `1 penalty for
CRFs that have been introduced recently: orthant-
wise Quasi Newton (Andrew and Gao, 2007),
stochastic gradient descent (Tsuruoka et al, 2009)
and coordinate descent (Sokolovska et al, 2010),
concluding that these methods have complemen-
1In CRFsuite (Okazaki, 2007), it is even impossible to
jointly test a pair of labels and a test on the observation, bi-
grams feature are only of the form f(yt?1, yt).
504
tary strengths and weaknesses. Based on an effi-
cient implementation of these algorithms, we were
able to train very large CRFs containing more than
a hundred of output labels and up to several billion
features, yielding results that are as good or better
than the best reported results for two NLP bench-
marks, text phonetization and part-of-speech tag-
ging.
Our contribution is therefore twofold: firstly a
detailed analysis of these three algorithms, dis-
cussing implementation, convergence and com-
paring the effect of various speed-ups. This
comparison is made fair and reliable thanks to
the reimplementation of these techniques in the
same software package. Second, the experimen-
tal demonstration that using large output label sets
is doable and that very large feature sets actually
help improve prediction accuracy. In addition, we
show how sparsity in structured feature sets can
be used in incremental training regimes, where
long-range features are progressively incorporated
in the model insofar as the shorter range features
have proven useful.
The rest of the paper is organized as follows: we
first recall the basics of CRFs in Section 2, and dis-
cuss three ways to train CRFs with a `1 penalty in
Section 3. We then detail several implementation
issues that need to be addressed when dealing with
massive feature sets in Section 4. Our experiments
are reported in Section 5. The main conclusions of
this study are drawn in Section 6.
2 Conditional Random Fields
In this section, we recall the basics of Conditional
Random Fields (CRFs) (Lafferty et al, 2001; Sut-
ton and McCallum, 2006) and introduce the nota-
tions that will be used throughout.
2.1 Basics
CRFs are based on the following model
p?(y|x) =
1
Z?(x)
exp
{
K?
k=1
?kFk(x,y)
}
(1)
where x = (x1, . . . , xT ) and y = (y1, . . . , yT )
are, respectively, the input and output sequences2,
and Fk(x,y) is equal to
?T
t=1 fk(yt?1, yt, xt),
where {fk}1?k?K is an arbitrary set of feature
2Our implementation also includes a special label y0, that
is always observed and marks the beginning of a sequence.
functions and {?k}1?k?K are the associated pa-
rameter values. We denote by Y and X , respec-
tively, the sets in which yt and xt take their values.
The normalization factor in (1) is defined by
Z?(x) =
?
y?Y T
exp
{
K?
k=1
?kFk(x,y)
}
. (2)
The most common choice of feature functions is to
use binary tests. In the sequel, we distinguish be-
tween two types of feature functions: unigram fea-
tures fy,x, associated with parameters ?y,x, and bi-
gram features fy?,y,x, associated with parameters
?y?,y,x. These are defined as
fy,x(yt?1, yt, xt) = 1(yt = y, xt = x)
fy?,y,x(yt?1, yt, xt) = 1(yt?1 = y
?, yt = y, xt = x)
where 1(cond.) is equal to 1 when the condition
is verified and to 0 otherwise. In this setting, the
number of parametersK is equal to |Y |2?|X|train,
where | ? | denotes the cardinal and |X|train refers to
the number of configurations of xt observed dur-
ing training. Thus, even in moderate size applica-
tions, the number of parameters can be very large,
mostly due to the introduction of sequential de-
pendencies in the model. This also explains why it
is hard to train CRFs with dependencies spanning
more than two adjacent labels. Using only uni-
gram features {fy,x}(y,x)?Y?X results in a model
equivalent to a simple bag-of-tokens position-
by-position logistic regression model. On the
other hand, bigram features {fy?,y,x}(y,x)?Y 2?X
are helpful in modelling dependencies between
successive labels. The motivations for using si-
multaneously both types of feature functions are
evaluated experimentally in Section 5.
2.2 Parameter Estimation
Given N independent sequences {x(i),y(i)}Ni=1,
where x(i) and y(i) contain T (i) symbols, condi-
tional maximum likelihood estimation is based on
the minimization, with respect to ?, of the negated
conditional log-likelihood of the observations
l(?) = ?
N?
i=1
log p?(y
(i)|x(i)) (3)
=
N?
i=1
{
logZ?(x
(i))?
K?
k=1
?kFk(x
(i),y(i))
}
This term is usually complemented with an addi-
tional regularization term so as to avoid overfitting
505
(see Section 3.1 below). The gradient of l(?) is
?l(?)
??k
=
N?
i=1
T (i)?
t=1
Ep?(y|x(i)) fk(yt?1, yt, x
(i)
t )
?
N?
i=1
T (i)?
t=1
fk(y
(i)
t?1, y
(i)
t , x
(i)
t ) (4)
where Ep?(y|x) denotes the conditional expecta-
tion given the observation sequence, i.e.
Ep?(y|x) fk(yt?1, yt, x
(i)
t ) =
?
(y?,y)?Y 2
fk(y, y
?, xt) P?(yt?1 = y
?, yt = y|x) (5)
Although l(?) is a smooth convex function, its op-
timum cannot be computed in closed form, and
l(?) has to be optimized numerically. The com-
putation of its gradient implies to repeatedly com-
pute the conditional expectation in (5) for all in-
put sequences x(i) and all positions t. The stan-
dard approach for computing these expectations
is inspired by the forward-backward algorithm for
hidden Markov models: using the notations intro-
duced above, the algorithm implies the computa-
tion of the forward
{
?1(y) = exp(?y,x1 + ?y0,y,x1)
?t+1(y) =
?
y? ?t(y
?) exp(?y,xt+1 + ?y?,y,xt+1)
and backward recursions
{
?Ti(y) = 1
?t(y?) =
?
y ?t+1(y) exp(?y,xt+1 + ?y?,y,xt+1),
for all indices 1 ? t ? T and all labels y ? Y .
Then, Z?(x) =
?
y ?T (y) and the pairwise prob-
abilities P?(yt = y?, yt+1 = y|x) are given by
?t(y
?) exp(?y,xt+1 + ?y?,y,xt+1)?t+1(y)/Z?(x)
These recursions require a number of operations
that grows quadratically with |Y |.
3 `1 Regularization in CRFs
3.1 Regularization
The standard approach for parameter estimation in
CRFs consists in minimizing the logarithmic loss
l(?) defined by (3) with an additional `2 penalty
term ?22 ???
2
2, where ?2 is a regularization parame-
ter. The objective function is then a smooth convex
function to be minimized over an unconstrained
parameter space. Hence, any numerical optimiza-
tion strategy may be used and practical solutions
include limited memory BFGS (L-BFGS) (Liu
and Nocedal, 1989), which is used in the popu-
lar CRF++ (Kudo, 2005) and CRFsuite (Okazaki,
2007) packages; conjugate gradient (Nocedal and
Wright, 2006) and Stochastic Gradient Descent
(SGD) (Bottou, 2004; Vishwanathan et al, 2006),
used in CRFsgd (Bottou, 2007). The only caveat
is to avoid numerical optimizers that require the
full Hessian matrix (e.g., Newton?s algorithm) due
to the size of the parameter vector in usual appli-
cations of CRFs.
The most significant alternative to `2 regulariza-
tion is to use a `1 penalty term ?1???1: such regu-
larizers are able to yield sparse parameter vectors
in which many component have been zeroed (Tib-
shirani, 1996). Using a `1 penalty term thus im-
plicitly performs feature selection, where ?1 con-
trols the amount of regularization and the number
of extracted features. In the following, we will
jointly use both penalty terms, yielding the so-
called elastic net penalty (Zhou and Hastie, 2005)
which corresponds to the objective function
l(?) + ?1???1 +
?2
2
???22 (6)
The use of both penalty terms makes it possible
to control the number of non zero coefficients and
to avoid the numerical problems that might occur
in large dimensional parameter settings (see also
(Chen, 2009)). However, the introduction of a `1
penalty term makes the optimization of (6) more
problematic, as the objective function is no longer
differentiable in 0. Various strategies have been
proposed to handle this difficulty. We will only
consider here exact approaches and will not dis-
cuss heuristic strategies such as grafting (Perkins
et al, 2003; Riezler and Vasserman, 2004).
3.2 Quasi Newton Methods
To deal with `1 penalties, a simple idea is that of
(Kazama and Tsujii, 2003), originally introduced
for maxent models. It amounts to reparameteriz-
ing ?k as ?k = ?
+
k ??
?
k , where ?
+
k and ?
?
k are pos-
itive. The `1 penalty thus becomes ?1(?+ ? ??).
In this formulation, the objective function recovers
its smoothness and can be optimized with conven-
tional algorithms, subject to domain constraints.
Optimization is straightforward, but the number
of parameters is doubled and convergence is slow
506
(Andrew and Gao, 2007): the procedure lacks a
mechanism for zeroing out useless parameters.
A more efficient strategy is the orthant-wise
quasi-Newton (OWL-QN) algorithm introduced in
(Andrew and Gao, 2007). The method is based on
the observation that the `1 norm is differentiable
when restricted to a set of points in which each
coordinate never changes its sign (an ?orthant?),
and that its second derivative is then zero, mean-
ing that the `1 penalty does not change the Hessian
of the objective on each orthant. An OWL-QN
update then simply consists in (i) computing the
Newton update in a well-chosen orthant; (ii) per-
forming the update, which might cause some com-
ponent of the parameter vector to change sign; and
(iii) projecting back the parameter value onto the
initial orthant, thereby zeroing out those compo-
nents. In (Gao et al, 2007), the authors show that
OWL-QN is faster than the algorithm proposed by
Kazama and Tsujii (2003) and can perform model
selection even in very high-dimensional problems,
with no loss of performance compared to the use
of `2 penalty terms.
3.3 Stochastic Gradient Descent
Stochastic gradient (SGD) approaches update the
parameter vector based on an crude approximation
of the gradient (4), where the computation of ex-
pectations only includes a small batch of observa-
tions. SGD updates have the following form
?k ? ?k + ?
?l(?)
??k
, (7)
where ? is the learning rate. In (Tsuruoka et al,
2009), various ways of adapting this update to `1-
penalized likelihood functions are discussed. Two
effective ideas are proposed: (i) only update pa-
rameters that correspond to active features in the
current observation, (ii) keep track of the cumu-
lated penalty zk that ?k should have received, had
the gradient been computed exactly, and use this
value to ?clip? the parameter value. This is imple-
mented by patching the update (7) as follows
{
if (?k > 0) ?k ? max(0, ?k ? zk)
else if (?k < 0) ?k ? min(0, ?k ? zk)
(8)
Based on a study of three NLP benchmarks, the
authors of (Tsuruoka et al, 2009) claim this ap-
proach to be much faster than the orthant-wise ap-
proach and yet to yield very comparable perfor-
mance, while selecting slightly larger feature sets.
3.4 Block Coordinate Descent
The coordinate descent approach of Dud??k et
al. (2004) and Friedman et al (2008) uses the
fact that optimizing a mono-dimensional quadratic
function augmented with a `1 penalty can be per-
formed analytically. For arbitrary functions, this
idea can be adapted by considering quadratic ap-
proximations of the objective around the current
value ??
lk,??(?k) =
?l(??)
??k
(?k ? ??k) +
1
2
?2l(??)
??2k
(?k ? ??k)
2
+ ?1|?k|+
?2
2
?2k + C
st (9)
The minimizer of the approximation (9) is simply
?k =
s
{
?2l(??)
??2k
??k ?
?l(??)
??k
, ?1
}
?2l(??)
??2k
+ ?2
(10)
where s is the soft-thresholding function
s(z, ?) =
?
??
??
z ? ? if z > ?
z + ? if z < ??
0 otherwise
(11)
Coordinate descent is ported to CRFs in
(Sokolovska et al, 2010). Making this scheme
practical requires a number of adaptations,
including (i) approximating the second order
term in (10), (ii) performing updates in block,
where a block contains the |Y | ? |Y + 1| fea-
tures ?y?,y,x and ?y,x for a fixed test x on the
observation sequence and (iii) approximating the
Hessian for a block by its diagonal terms. (ii)
is specially critical, as repeatedly cycling over
individual features to perform the update (10)
is only possible with restricted sets of features.
The block update schemes uses the fact that
all features within a block appear in the same
set of sequences, which means that most of the
computations needed to perform theses updates
can be shared within the block. One advantage
of the resulting algorithm, termed BCD in the
following, is that the update of ?k only involves
carrying out the forward-backward recursions for
the set of sequences that contain symbols x such
that at least one {fk(y?, y, x)}(y,y?)?Y 2 is non
null, which can be much smaller than the whole
training set.
507
4 Implementation Issues
Efficiently processing very-large feature and ob-
servation sets requires to pay attention to many
implementation details. In this section, we present
several optimizations devised to speed up training.
4.1 Sparse Forward-Backward Recursions
For all algorithms, the computation time is domi-
nated by the evaluations of the gradient: our im-
plementation takes advantage of the sparsity to ac-
celerate these computations. Assume the set of bi-
gram features {?y?,y,xt+1}(y?,y)?Y 2 is sparse with
only r(xt+1)  |Y |2 non null values and define
the |Y | ? |Y | sparse matrix
Mt(y
?, y) = exp(?y?,y,xt)? 1.
Using M , the forward-backward recursions are
?t(y) =
?
y?
ut?1(y
?) +
?
y?
ut?1(y
?)Mt(y
?, y)
?t(y
?) =
?
y
vt+1(y) +
?
y
Mt+1(y
?, y)vt+1(y)
with ut?1(y) = exp(?y,xt)?t?1(y) and
vt+1(y) = exp(?y,xt+1)?t+1(y). (Sokolovska et
al., 2010) explains how computational savings can
be obtained using the fact that the vector/matrix
products in the recursions above only involve
the sparse matrix Mt+1(y?, y). They can thus be
computed with exactly r(xt+1) multiplications
instead of |Y |2. The same idea can be used
when the set {?y,xt+1}y?Y of unigram features is
sparse. Using this implementation, the complexity
of the forward-backward procedure for x(i) can be
made proportional to the average number of active
features per position, which can be much smaller
than the number of potentially active features.
For BCD, forward-backward can even be made
slightly faster. When computing the gradient wrt.
features ?y,x and ?y?,y,x (for all the values of y
and y?) for sequence x(i), assuming that x only
occurs once in x(i) at position t, all that is needed
is ??t(y), ?t
? ? t and ??t(y),?t
? ? t. Z?(x) is then
recovered as
?
y ?t(y)?t(y). Forward-backward
recursions can thus be truncated: in our experi-
ments, this divided the computational cost by 1,8
on average.
Note finally that forward-backward is per-
formed on a per-observation basis and is easily
parallelized (see also (Mann et al, 2009) for more
powerful ways to distribute the computation when
dealing with very large datasets). In our imple-
mentation, it is distributed on all available cores,
resulting in significant speed-ups for OWL-QN
and L-BFGS; for BCD the gain is less acute, as
parallelization only helps when updating the pa-
rameters for a block of features that are occur in
many sequences; for SGD, with batches of size
one, this parallelization policy is useless.
4.2 Scaling
Most existing implementations of CRFs, eg.
CRF++ and CRFsgd perform the forward-
backward recursions in the log-domain, which
guarantees that numerical over/underflows are
avoided no matter the length T (i) of the sequence.
It is however very inefficient from an implementa-
tion point of view, due to the repeated calls to the
exp() and log() functions. As an alternative way
of avoiding numerical problems, our implementa-
tion, like crfSuite?s, resorts to ?scaling?, a solution
commonly used for HMMs. Scaling amounts to
normalizing the values of ?t and ?t to one, making
sure to keep track of the cumulated normalization
factors so as to compute Z?(x) and the conditional
expectations Ep?(y|x). Also note that in our imple-
mentation, all the computations of exp(x) are vec-
torized, which provides an additional speed up of
about 20%.
4.3 Optimization in Large Parameter Spaces
Processing very large feature vectors, up to bil-
lions of components, is problematic in many ways.
Sparsity has been used here to speed up forward-
backward, but we have made no attempt to accel-
erate the computation of the OWL-QN updates,
which are linear in the size of the parameter vector.
Of the three algorithms, BCD is the most affected
by increases in the number of features, or more
precisely, in the number of features blocks, where
one block correspond to a specific test of the ob-
servation. In the worst case scenario, each block
may require to visit all the training instances,
yielding terrible computational wastes. In prac-
tice though, most blocks only require to process
a small fraction of the training set, and the ac-
tual complexity depends on the average number of
blocks per observations. Various strategies have
been tried to further accelerate BCD, such as pro-
cessing blocks that only visit one observation in
parallel and updating simultaneously all the blocks
that visit all the training instances, leading to a
small speed-up on the POS-tagging task.
508
Working with billions of features finally re-
quires to worry also about memory usage. In this
respect, BCD is the most efficient, as it only re-
quires to store one K-dimensional vector for the
parameter itself. SGD requires two such vectors,
one for the parameter and one for storing the zk
(see Eq. (8)). In comparison, OWL-QN requires
much more memory, due to the internals of the
update routines, which require several histories of
the parameter vector and of its gradient. Typi-
cally, our implementation necessitates in the order
of a dozen K-dimensional vectors. Parallelization
only makes things worse, as each core will also
need to maintain its own copy of the gradient.
5 Experiments
Our experiments use two standard NLP tasks,
phonetization and part-of-speech tagging, chosen
here to illustrate two very different situations, and
to allow for comparison with results reported else-
where in the literature. Unless otherwise men-
tioned, the experiments use the same protocol: 10
fold cross validation, where eight folds are used
for training, one for development, and one for test-
ing. Results are reported in terms of phoneme er-
ror rates or tag error rates on the test set.
Comparing run-times can be a tricky matter, es-
pecially when different software packages are in-
volved. As discussed above, the observed run-
times depend on many small implementation de-
tails. As the three algorithms share as much code
as possible, we believe the comparison reported
hereafter to be fair and reliable. All experiments
were performed on a server with 64G of memory
and two Xeon processors with 4 cores at 2.27 Ghz.
For comparison, all measures of run-times include
the cumulated activity of all cores and give very
pessimistic estimates of the wall time, which can
be up to 7 times smaller. For OWL-QN, we use 5
past values of the gradient to approximate the in-
verse of the Hessian matrix: increasing this value
had no effect on accuracy or convergence and was
detrimental to speed; for SGD, the learning rate
parameter was tuned manually.
Note that we have not spent much time optimiz-
ing the values of ?1 and ?2. Based on a pilot study
on Nettalk, we found that taking ?1 = .5 and ?2 in
the order of 10?5 to yield nearly optimal perfor-
mance, and have used these values throughout.
5.1 Tasks and Settings
5.1.1 Nettalk
Our first benchmark is the word phonetization
task, using the Nettalk dictionary (Sejnowski and
Rosenberg, 1987). This dataset contains approxi-
mately 20,000 English word forms, their pronun-
ciation, plus some prosodic information (stress
markers for vowels, syllabic parsing for con-
sonants). Grapheme and phoneme strings are
aligned at the character level, thanks to the use of
a ?null sound? in the latter string when it is shorter
than the former; likewise, each prosodic mark is
aligned with the corresponding letter. We have de-
rived two test conditions from this database. The
first one is standard and aims at predicting the pro-
nunciation information only. In this setting, the set
of observations (X) contains 26 graphemes, and
the output label set contains |Y | = 51 phonemes.
The second condition aims at jointly predict-
ing phonemic and prosodic information3. The rea-
sons for designing this new condition are twofold:
firstly, it yields a large set of composite labels
(|Y | = 114) and makes the problem computation-
ally challenging. Second, it allows to quantify how
much the information provided by the prosodic
marks help predict the phonemic labels. Both in-
formation are quite correlated, as the stress mark
and the syllable openness, for instance, greatly in-
fluence the realization of some archi-phonemes.
The features used in Nettalk experiments take
the form fy,w (unigram) and fy?,y,w (bigram),
where w is a n-gram of letters. The n-grm feature
sets (n = {1, 3, 5, 7}) includes all features testing
embedded windows of k letters, for all 0 ? k ? n;
the n-grm- setting is similar, but only includes
the window of length n; in the n-grm+ setting,
we add features for odd-size windows; in the n-
grm++ setting, we add all sequences of letters up
to size n occurring in current window. For in-
stance, the active bigram features at position t = 2
in the sequence x=?lemma? are as follows: the 3-
grm feature set contains fy,y? , fy,y?,e and fy?,y,lem;
only the latter appears in the 3-grm- setting. In
the 3-grm+ feature set, we also have fy?,y,le and
fy?,y,em. The 3-grm++ feature set additionally in-
cludes fy?,y,l and fy?,y,m. The number of features
ranges from 360 thousands (1-grm setting) to 1.6
billion (7-grm).
3Given the design of the Nettalk dictionary, this experi-
ment required to modify the original database so as to reas-
sign prosodic marks to phonemes, rather than to letters.
509
Features With Without
Nettalk
3-grm 10.74% 14.3M 14.59% 0.3M
5-grm 8.48% 132.5M 11.54% 2.5M
POS tagging
base 2.91% 436.7M 3.47% 70.2M
Table 1: Features jointly testing label pairs and
the observation are useful (error rates and features
counts.)
`2 `1-sparse `1 % zero
1-grm 84min 41min 57min 44.6%
3-grm- 65min 16min 44min 99.6%
3-grm 72min 48min 58min 19.9%
Table 2: Sparse vs standard forward-backward
(training times and percentages of sparsity of M )
5.1.2 Part-of-Speech Tagging
Our second benchmark is a part-of-speech (POS)
tagging task using the PennTreeBank corpus
(Marcus et al, 1993), which provides us with a
quite different condition. For this task, the number
of labels is smaller (|Y | = 45) than for Nettalk,
and the set of observations is much larger (|X| =
43207). This benchmark, which has been used in
many studies, allows for direct comparisons with
other published work. We thus use a standard ex-
perimental set-up, where sections 0-18 of the Wall
Street Journal are used for training, sections 19-21
for development, and sections 22-24 for testing.
Features are also standard and follow the design
of (Suzuki and Isozaki, 2008) and test the current
words (as written and lowercased), prefixes and
suffixes up to length 4, and typographical charac-
teristics (case, etc.) of the words. Our baseline
feature set alo contains tests on individual and
pairs of words in a window of 5 words.
5.2 Using Large Feature Sets
The first important issue is to assess the benefits
of using large feature sets, notably including fea-
tures testing both a bigram of labels and an obser-
vation. Table 1 compares the results obtained with
and without these features for various setting (us-
ing OWL-QN to perform the optimization), sug-
gesting that for the tasks at hand, these features
are actually helping.
`2 `1 Elastic-net
1-grm 17.81% 17.86% 17.79%
3-grm 10.62% 10.74% 10.70%
5-grm 8.50% 8.45% 8.48%
Table 3: Error rates of the three regularizers on the
Nettalk task.
5.3 Speed, Sparsity, Convergence
The training speed depends of two main factors:
the number of iterations needed to achieve conver-
gence and the computational cost of one iteration.
In this section, we analyze and compare the run-
time efficiency of the three optimizers.
5.3.1 Convergence
As far as convergence is concerned, the two forms
of regularization (`2 and `1) yield the same per-
formance (see Table 3), and the three algorithms
exhibit more or less the same behavior. They
quickly reach an acceptable set of active param-
eters, which is often several orders of magnitude
smaller than the whole parameter set (see results
below in Table 4 and 5). Full convergence, re-
flected by a stabilization of the objective function,
is however not so easily achieved. We have of-
ten observed a slow, yet steady, decrease of the
log-loss, accompanied with a diminution of the
number of active features as the number of iter-
ations increases. Based on this observation, we
have chosen to stop all algorithms based on their
performance on an independent development set,
allowing a fair comparison of the overall training
time; for OWL-QN, it allowed to divide the total
training time by almost 2.
It has finally often been found useful to fine
tune the non-zero parameters by running a final
handful of L-BFGS iterations using only a small
`2 penalty; at this stage, all the other features are
removed from the model. This had a small impact
BCD and SGD?s performance and allowed them to
catch up with OWL-QN?s performance.
5.3.2 Sparsity and the Forward-Backward
As explained in section 4.1, the forward-backward
algorithm can be written so as to use the sparsity
of the matrix My,y?,x. To evaluate the resulting
speed-up, we ran a series of experiments using
Nettalk (see Table 2). In this table, the 3-grm- set-
ting corresponds to maximum sparsity for M , and
training with the sparse algorithm is three times
faster than with the non-sparse version. Throwing
510
Method Iter. # Feat. Error Time
O
W
L
-Q
N 1-grm 63.4 4684 17.79% 11min
7-grm 140.2 38214 8.12% 1h02min
5-grm+ 141.0 43429 7.89% 1h37min
S
G
D 1-grm 21.4 3540 18.21% 9min
5-grm+ 28.5 34319 8.01% 45min
B
C
D
1-grm 28.2 5017 18.27% 27min
7-grm 9.2 3692 8.21% 1h22min
5-grm+ 8.7 47675 7.91% 2h18min
Table 4: Performance on Nettalk
in more features has the effect of making M much
more dense, mitigating the benefits of the sparse
recursions. Nevertheless, even for very large fea-
ture sets, the percentage of zeros in M averages
20% to 30%, and the sparse version remains 10 to
20% faster than the non-sparse one. Note that the
non-sparse version is faster with a `1 penalty term
than with only the `2 term: this is because exp(0)
is faster to evaluate than exp(x) when x 6= 0.
5.3.3 Training Speed and Test Accuracy
Table 4 displays the results achieved on the Nettalk
task. The three algorithms yield very compara-
ble accuracy results, and deliver compact models:
for the 5-gram+ setting, only 50,000 out of 250
million features are selected. SGD is the fastest
of the three, up to twice as fast as OWL-QN and
BCD depending on the feature set. The perfor-
mance it achieves are consistently slightly worst
than the other optimizers, and only catch up when
the parameters are fine-tuned (see above). There
are not so many comparisons for Nettalk with
CRFs, due to the size of the label set. Our results
compare favorably with those reported in (Pal et
al., 2006), where the accuracy attains 91.7% us-
ing 19075 examples for training and 934 for test-
ing, and with those in (Jeong et al, 2009) (88.4%
accuracy with 18,000 (2,000) training (test) in-
stances). Table 5 gives the results obtained for
the larger Nettalk+prosody task. Here, we only
report the results obtained with SGD and BCD.
For OWL-QN, the largest model we could han-
dle was the 3-grm model, which contained 69 mil-
lion features, and took 48min to train. Here again,
performance steadily increase with the number of
features, showing the benefits of large-scale mod-
els. We lack comparisons for this task, which
seems considerably harder than the sole phone-
tization task, and all systems seem to plateau
around 13.5% accuracy. Interestingly, simulta-
Method Error Time
S
G
D 5-grm 14.71% / 8.11% 55min
5-grm+ 13.91% / 7.51% 2h45min
B
C
D
5-grm 14.57% / 8.06% 2h46min
7-grm 14.12% / 7.86% 3h02min
5-grm+ 13.85% / 7.47% 7h14min
5-grm++ 13.69% / 7.36% 16h03min
Table 5: Performance on Nettalk+prosody. Error
is given for both joint labels and phonemic labels.
neously predicting the phoneme and its prosodic
markers allows to improve the accuracy on the pre-
diction of phonemes, which improves of almost a
half point as compared to the best Nettalk system.
For the POS tagging task, BCD appears to be
unpractically slower to train than the others ap-
proaches (SGD takes about 40min to train, OWL-
QN about 1 hour) due the simultaneous increase
in the sequence length and in the number of ob-
servations. As a result, one iteration of BCD typi-
cally requires to repeatedly process over and over
the same sequences: on average, each sequence is
visited 380 times when we use the baseline fea-
ture set. This technique should reserved for tasks
where the number of blocks is small, or, as below,
when memory usage is an issue.
5.4 Structured Feature Sets
In many tasks, the ambiguity of tokens can be re-
duced by looking up increasingly large windows
of local context. This strategy however quickly
runs into a combinatorial increase of the number
of features. A side note of the Nettalk experiments
is that when using embedded features, the active
feature set tends to reflect this hierarchical organi-
zation. This means that when a feature testing a
n-gram is active, in most cases, the features for all
embedded k-grams are also selected.
Based on this observation, we have designed
an incremental training strategy for the POS tag-
ging task, where more specific features are pro-
gressively incorporated into the model if the cor-
responding less specific feature is active. This ex-
periment used BCD, which is the most memory ef-
ficient algorithm. The first iteration only includes
tests on the current word. During the second it-
eration, we add tests on bigram of words, on suf-
fixes and prefixes up to length 4. After four itera-
tions, we throw in features testing word trigrams,
subject to the corresponding unigram block being
active. After 6 iterations, we finally augment the
511
model with windows of length 5, subject to the
corresponding trigram being active. After 10 iter-
ations, the model contains about 4 billion features,
out of which 400,000 are active. It achieves an
error rate of 2.63% (resp. 2.78%) on the develop-
ment (resp. test) data, which compares favorably
with some of the best results for this task (for in-
stance (Toutanova et al, 2003; Shen et al, 2007;
Suzuki and Isozaki, 2008)).
6 Conclusion and Perspectives
In this paper, we have discussed various ways to
train extremely large CRFs with a `1 penalty term
and compared experimentally the results obtained,
both in terms of training speed and of accuracy.
The algorithms studied in this paper have com-
plementary strength and weaknesses: OWL-QN is
probably the method of choice in small or moder-
ate size applications while BCD is most efficient
when using very large feature sets combined with
limited-size observation alphabets; SGD comple-
mented with fine tuning appears to be the preferred
choice in most large-scale applications. Our anal-
ysis demonstrate that training large-scale sparse
models can be done efficiently and allows to im-
prove over the performance of smaller models.
The CRF package developed in the course of this
study implements many algorithmic optimizations
and allows to design innovative training strategies,
such as the one presented in section 5.4. This
package is released as open-source software and
is available at http://wapiti.limsi.fr.
In the future, we intend to study how spar-
sity can be used to speed-up training in the face
of more complex dependency patterns (such as
higher-order CRFs or hierarchical dependency
structures (Rozenknop, 2002; Finkel et al, 2008).
From a performance point of view, it might also
be interesting to combine the use of large-scale
feature sets with other recent improvements such
as the use of semi-supervised learning techniques
(Suzuki and Isozaki, 2008) or variable-length de-
pendencies (Qian et al, 2009).
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of the International Conference on Machine
Learning, pages 33?40, Corvalis, Oregon.
Le?on Bottou. 2004. Stochastic learning. In Olivier
Bousquet and Ulrike von Luxburg, editors, Ad-
vanced Lectures on Machine Learning, Lecture
Notes in Artificial Intelligence, LNAI 3176, pages
146?168. Springer Verlag, Berlin.
Le?on Bottou. 2007. Stochastic gradient descent (sgd)
implementation. http://leon.bottou.org/projects/sgd.
Stanley Chen. 2009. Performance prediction for ex-
ponential language models. In Proceedings of the
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 450?458, Boulder, Colorado, June.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of the 17th
European Conference on Machine Learning, pages
606?613, Berlin, September.
Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav
Bulatov. 2004. Training conditional random fields
via gradient tree boosting. In Proceedings of
the International Conference on Machine Learning,
Banff, Canada.
Miroslav Dud??k, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
John Shawe-Taylor and Yoram Singer, editors, Pro-
ceedings of the 17th annual Conference on Learning
Theory, volume 3120 of Lecture Notes in Computer
Science, pages 472?486. Springer.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics, pages 959?967, Columbus, Ohio.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear
models via coordinate descent. Technical report,
Department of Statistics, Stanford University.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech republic.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of crfs for large-scale nat-
ural language data. In Proceedings of the Joint Con-
ference of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing,
pages 281?284, Suntec, Singapore.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 137?144.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net/.
512
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Percy Liang, Hal Daume?, III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592?599.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Gideon Mann, Ryan McDonald, Mehryar Mohri,
Nathan Silberman, and Dan Walker. 2009. Efficient
large-scale distributed training of conditional maxi-
mum entropy models. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A.Culotta, editors,
Advances in Neural Information Processing Systems
22, pages 1231?1239.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn treebank. Com-
putational Linguistics, 19(2):313?330.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer.
Naoaki Okazaki. 2007. CRFsuite: A fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing, Toulouse, France.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333?1356.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of the International
Joint Conference on Artificial Intelligence, pages
1124?1129.
Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing
Huang, and Lide Wu. 2009. Sparse higher order
conditional random fields for improved sequence la-
beling. In Proceedings of the Annual International
Conference on Machine Learning, pages 849?856.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of the confer-
ence on Empirical Methods in Natural Language
Processing, pages 174?181, Barcelona, Spain, July.
Antoine Rozenknop. 2002. Mode`les syntaxiques
probabilistes non-ge?ne?ratifs. Ph.D. thesis, Dpt.
d?informatique, E?cole Polytechnique Fe?de?rale de
Lausanne.
Terrence J. Sejnowski and Charles R. Rosenberg.
1987. Parallel networks that learn to pronounce en-
glish text. Complex Systems, 1.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic.
Nataliya Sokolovska, Thomas Lavergne, Olivier
Cappe?, and Franc?ois Yvon. 2010. Efficient learning
of sparse conditional random fields for supervised
sequence labelling. IEEE Selected Topics in Signal
Processing.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors, In-
troduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the
Conference of the Association for Computational
Linguistics on Human Language Technology, pages
665?673, Columbus, Ohio.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. J.R.Statist.Soc.B, 58(1):267?
288.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
173?180.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of the Joint Conference
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
477?485, Suntec, Singapore.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23th In-
ternational Conference on Machine Learning, pages
969?976. ACM Press, New York, NY, USA.
Hui Zhou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. J. Royal. Stat.
Soc. B., 67(2):301?320.
513
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 542?553,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
From n-gram-based to CRF-based Translation Models
Thomas Lavergne Josep Maria Crego
LIMSI/CNRS
BP 133
F-91 403 Orsay Ce?dex
{lavergne,jmcrego}@limsi.fr
Alexandre Allauzen Franc?ois Yvon
LIMSI/CNRS & Uni. Paris Sud
BP 133
F-91 403 Orsay Ce?dex
{allauzen,yvon}@limsi.fr
Abstract
A major weakness of extant statistical ma-
chine translation (SMT) systems is their lack
of a proper training procedure. Phrase extrac-
tion and scoring processes rely on a chain of
crude heuristics, a situation judged problem-
atic by many. In this paper, we recast the ma-
chine translation problem in the familiar terms
of a sequence labeling task, thereby enabling
the use of enriched feature sets and exact train-
ing and inference procedures. The tractabil-
ity of the whole enterprise is achieved through
an efficient implementation of the conditional
random fields (CRFs) model using a weighted
finite-state transducers library. This approach
is experimentally contrasted with several con-
ventional phrase-based systems.
1 Introduction
A weakness of existing phrase-based SMT systems,
that has been repeatedly highlighted, is their lack
of a proper training procedure. Attempts to de-
sign probabilistic models of phrase-to-phrase align-
ments (e.g. (Marcu and Wong, 2002)) have thus far
failed to overcome the related combinatorial prob-
lems (DeNero and Klein, 2008) and/or to yield im-
proved training heuristics (DeNero et al, 2006).
Phrase extraction and scoring thus rely on a chain
of heuristics see (Koehn et al, 2003), which evolve
phrase alignments from ?symmetrized? word-to-
word alignments obtained with IBM models (Brown
et al, 1990) and the like (Liang et al, 2006b; Deng
and Byrne, 2006; Ganchev et al, 2008). Phrase
scoring is also mostly heuristic and relies on an op-
timized interpolation of several simple frequency-
based scores. Overall, the training procedure of
translation models within conventional phrase-based
(or hierarchical) systems is generally considered un-
satisfactory and the design of better estimation pro-
cedures remains an active research area (Wuebker et
al., 2010).
To overcome the NP-hard problems that derive
from the need to consider all possible permutations
of the source sentence, we make here a radical
simplification and consider training the translation
model given a fixed segmentation and reordering.
This idea is not new, and is one of the grounding
principle of n-gram-based approaches (Casacuberta
and Vidal, 2004; Marin?o et al, 2006) in SMT. The
novelty here is that we will use this assumption to re-
cast machine translation (MT) in the familiar terms
of a sequence labeling task.
This reformulation allows us to make use of the
efficient training and inference tools that exists for
such tasks, most notably linear CRFs (Lafferty et
al., 2001; Sutton and McCallum, 2006). It also en-
ables to easily integrate linguistically informed (de-
scribing morphological or morpho-syntactical prop-
erties of phrases) and/or contextual features into the
translation model. In return, in addition to having
a better trained model, we also expect (i) to make
estimation less sensible to data sparsity issues and
(ii) to improve the ability of our system to make
the correct lexical choices based on the neighbor-
ing source words. As explained in Section 2, this
reformulation borrows much from the general ar-
chitecture of n-gram MT systems and implies to
solve several computational challenges. In our ap-
542
proach, the tractability of the whole enterprise is
achieved through an efficient reimplementation of
CRFs using a public domain library for weighted
finite-state transducers (WFSTs) (see details in Sec-
tion 3). This approach is experimentally contrasted
with more conventional n-gram based and phrase-
based approaches on a standard benchmark in Sec-
tion 4, where we also evaluate the benefits of various
feature sets and training regimes. We finally relate
our new system with alternative proposals for train-
ing discriminatively SMT systems in Section 5, be-
fore drawing some lessons and discussing possible
extensions of this work.
The main contribution of this work are thus (i) a
detailed presentation of the CRF in translation in-
cluding all necessary implementation details and (ii)
an experimental study of various feature functions
and of various ways to integrate target side LM in-
formation.
2 MT as sequence labeling
In this section, we briefly review the n-gram based
approach to SMT, originally introduced in (Casacu-
berta and Vidal, 2004; Marin?o et al, 2006), which
constitutes our starting point. We then describe our
new proposal, which, in essence, consists in replac-
ing the modeling of compound source-target trans-
lation units by a conditional model where the prob-
ability of each target side phrase is conditioned on
the source sentence.
2.1 The n-gram based approach in SMT
The n-gram based approach of (Marin?o et al, 2006)
is a variation of the standard phrase-based model,
characterized by the peculiar form of the translation
model. In this approach, the translation model is
based on bilingual units called tuples. Tuples are
the analogous of phrase pairs, as they represent a
matching u = (e, f) between a source f and a tar-
get e word sequence. The probability of a sequence
of tuples is computed using a conventional n-gram
model as:
p(u1 . . . uI) =
I?
i=1
p(ul|ui?1 . . . ui?n+1).
The probability of a sentence pair (f , e) is then ei-
ther recovered by marginalization, or approximated
by maximization, over all possible joint segmenta-
tions of f and e into tuples.
As for any n-gram model, the parameters are es-
timated using statistics collected in a training corpus
made of sequences of tuples derived from the par-
allel sentences in a two step process. First, a word
alignment is computed using a standard alignment
pipeline1 based on the IBM models. Source words
are then reordered so as to disentangle the align-
ment links and to synchronize the source and tar-
get texts. Special care has to be paid to non-aligned
source words, which have to be collapsed with their
neighbor words. A byproduct of this process is a de-
terministic joint segmentation of parallel sentences
into minimal bilingual units, the tuples, that consti-
tute the basic elements in the model. This process is
illustrated on Figure 1, where the unfolding process
enables the extraction of tuples such as: (demanda,
said ) or (de nouveau, again).
f : demanda de nouveau la femme voile?e
e: the veiled dame said again
f? : la voile?e femme demanda de nouveau
Figure 1: The tuple extraction process
The original (top) and reordered (bottom) French
sentence aligned with its translation.
At test time, the source text is reordered so as
to match the reordering implied by the disentangle-
ment procedure. Various proposals has been made
to perform such source side reordering (Collins et
al., 2005; Xia and McCord, 2004), or even learn-
ing reordering rules based on syntactic or morpho-
syntactic information (Crego and Marin?o, 2007).
The latter approach amounts to accumulate reorder-
ing patterns during the training; test source sen-
tences are then non-deterministically reordered in
all possible ways yielding a word graph. This graph
is then monotonously decoded, where the score of
a translation hypothesis combines information from
the translation models as well as from other infor-
mation sources (lexicalized reordering model, target
1Here, using the MGIZA++ package (Gao and Vogel, 2008).
543
side language model (LM), word and phrase penal-
ties, etc).
2.2 Translating with CRFs
A discriminative version of the n-gram approach
consists in modeling P (e|f) instead of P (e, f),
which can be efficiently performed with CRFs (Laf-
ferty et al, 2001; Sutton and McCallum, 2006). As-
suming matched sequences of observations (x =
xL1 ) and labels (y = y
L
1 ), CRFs express the con-
ditional probability of labels as:
P (yL1 |x
L
1 ) =
1
Z(xL1 ; ?)
exp(?TG(xL1 , y
L
1 )),
where ? is a parameter vector and G denotes a vec-
tor of feature functions testing various properties of
x and y. In the linear-chain CRF, each compo-
nent Gk(xI1, y
I
1) of G is decomposed as a sum of
local features: Gk(xI1, y
I
1) =
?
i gk(x
I
1, yi?1, yi)
2.
CRFs are trained by maximizing the (penalized) log-
likelihood of a corpus containing observations and
their labels.
In principle, the data used to train n-gram trans-
lation models provide all the necessary information
required to train a CRF3. It suffices to consider that
the alphabet of possible observations ranges over all
possible source side fragments, and that each tar-
get side of a tuple is a potential label. The model
thus defines the probability of a segmented target
e? = e?I1 given the segmented and reordered source
sentence f? = f? I1 . To complete the model, one just
needs to define a distribution over source segmen-
tations P (f? |f). Given the deterministic relationship
between e and e? expressed by the ?unsegmentation?
function ? which maps e? with e = ?(e?), we then
have:
P (e|f) =
?
f? ,e|?(e)=e
P (e?, f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f? , f)P (f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f?)P (f? |f)
2Assuming first order dependencies.
3This is a significant difference with (Blunsom et al, 2008),
as we do not need to introduce latent variables during training.
In practice, we will only consider a restricted
number of possible segmentation/reorderings of the
source, denoted L(f), and compute the best transla-
tion e? as ?(e??), where:
e?? = arg max
e
P (e?|f)
? arg max
f??L(f),e
P (e?, |f? , f)P (f? |f) (1)
Even with these simplifying assumptions, this
approach raises several challenging computational
problems. First, training a CRF is quadratic in the
number of labels, of which we will have plenty (typ-
ically hundreds of thousands). A second issue is de-
coding: as we need to consider at test time a combi-
natorial number of possible source reorderings and
segmentations, we can no longer dispense with the
computation of the normalizer Z(f? ; ?) which is re-
quired to compute P (e?, f? |f) as P (f? |f)P (e?|f?) and to
compare hypotheses associated with different values
of f? . We discuss our solutions to these problems in
the next section.
3 Implementation issues
3.1 Training
Basic training The main difficulties in training are
caused by the unusually large number of labels, each
of which corresponds to a (small) sequence of target
words. Hopefully, each observation (source side tu-
ple) occurs with a very small number of different
labels. A first simplification is thus to consider that
the set of possible ?labels? e? for a source sequence
f? is limited to those that are seen in training: all
the other associations (f? , e?) are deemed impossible,
which amounts to setting the corresponding param-
eter value to ??.
A second speed-up is to enforce sparsity in the
model, through the use of a `1 regularization term
(Tibshirani, 1996): on the one hand, this greatly re-
duces the memory usage; furthermore, sparse mod-
els are also prone to various optimization of the
forward-backward computations (Lavergne et al,
2010). As discussed in (Ng, 2004; Turian et al,
2007), this feature selection strategy is well suited
to the task at hand, where the number of possible
features is extremely large. Optimization is per-
544
formed using the Rprop algorithm4 (Riedmiller and
Braun, 1993), which provides the memory efficiency
needed to cope with the very large feature sets con-
sidered here.
Training with a target language model One of
the main strength of the phrase-based ?log-linear?
models is their ability to make use of powerful
target side language models trained on very large
amounts of monolingual texts. This ability is crucial
to achieve good performance and has to be preserved
no matter the difficulties that occur when one moves
away from conventional phrase-based systems (Chi-
ang, 2005; Huang and Chiang, 2007; Blunsom and
Osborne, 2008; Ka?a?ria?inen, 2009). It thus seems
appropriate to include a LM feature function in our
model or alternatively to define:
P (e?|f?) =
1
Z(f? ; ?)
PLM (e?) exp(?
TG(f? , e?)),
where PLM is the target language model and
Z(f? ; ?) =
?
e PLM (e?) exp(?
TG(f? , e?)). Imple-
menting this approach implies to deal with the lack
of synchronization between the units of the trans-
lation models, which are variable-length (possibly
empty) tuples, and the units of the language models,
which are plain words.
In practice, this extension is implemented by per-
forming training and inference over a graph whose
nodes are not only indexed by their position and the
left target context, but also by the required n-gram
(target) history. In most cases, for small values of
n such as considered in this study, the n-gram his-
tory can be deduced from the left target tuple. The
most problematic case is when the left target tuple
is NULL, which require to copy the history from the
previous states. As a consequence, for the values of
n considered here, the impact of this extension on
the total training time is limited.
Reference reachability A recurring problem for
discriminative training approaches is reference un-
reachability (Liang et al, 2006a): this happens when
the model cannot predict the reference translation,
which means in our case that the probability of the
reference cannot be computed. In our implementa-
tion, this only happens when the reference involves
4Adapted to handle a locally non-differentiable objective.
a tuple (f? ,e?) that is too rare to be included in the
model. As a practical workaround, when this hap-
pens for a given training sentence, we make sure
to ?locally? augment the tuple dictionary with the
missing part of the reference, which is then removed
for processing the rest of the training corpus.
3.2 Inference
Our decoder is implemented as a cascade of
weighted finite-state transducers (WFSTs) using the
functionalities of the OpenFst library (Allauzen et
al., 2007). This library provides many basic opera-
tion for WFSTs, notably the left (pi1) and right (pi2)
projections as well as the composition operation (?).
The related notions and algorithms are presented in
detail in (Mohri, 2009), to which we refer the reader.
In essence, our decoder is implemented of a finite-
state cascade involoving the following steps: (i)
source reordering and segmentation (ii) application
of the translation model and (optionally) (iii) com-
position with a target side language model, an ar-
chitecture that is closely related to the proposal of
(Kumar et al, 2006). A more precise account of
these various steps is given below, where we de-
scribe the main finite-state transducers involved in
our decoder:
? S, the acceptor for the source sentence f ;
? R, which implements segmentation and re-
ordering rules;
? T , the tuple dictionary, associating source side
sequences with possible translations based on
the inventory of tuples;
? F , the feature matcher, mapping each feature
with the corresponding parameter value;
Source reordering The computation of R mainly
follows the approach of (Crego and Marin?o, 2007)
and uses a part-of-speech tagged version of the re-
ordered training data. Each reordering pattern seen
in training is generalized as a non-deterministic re-
ordering rule which expresses a possible rearrange-
ment of some subpart of the source sentence. Each
rule is implemented as an elementary finite-state
transducer, and the set of possible word reorderings
is computed as the composition of these transducers.
R is finally obtained by composing the result with a
545
transducer computing all the possible segmentations
of its input into sequences of source side tuples5.
The output of S ? R are sequences of source side
tuples f? ; each path in this transducer is addition-
ally weighted with a simplistic n-tuple segmentation
model, estimated using the source side of the paral-
lel training corpus. Note that these scores are nor-
malized, so that the weight of each path labelled f? in
S ?R is logP (f? |f).
The feature matcher F The feature matcher is
also implemented as a series of elementary weighted
transducers, each transducer being responsible for a
given class of feature functions. The simplest trans-
ducer in this family deals with the class of unigram
feature functions, ie. feature functions that only test
the current observation and label. It is represented
on the left part of Figure 3.2, where for the sake of
readability we only display one example for each
test pattern (here: an unconditional feature that al-
ways returns true for a given label, a test on the
source word, and a test on the source POS label).
As long as dependencies between source and/or tar-
get symbols remain local, they can be captured by
finite-state transducers such as the ones on the mid
and right part of Figure 3.2, which respectively com-
pute bigram target features, and joint bigram source
and target features.
The feature matcher F is computed as the com-
position of these elementary transducers, where we
only include source and target labels that can occur
given the current input sentence. Weights in F are
interpreted in the tropical semiring. exp(F ) is ob-
tained by replacing weights w in F with exp(w) in
the real semiring.
Decoding a word graph If the input segmentation
and reordering were deterministically set, meaning
that the automaton I = pi1(S ? R ? T ) would only
contain one path, decoding would amount to finding
the best path in S ?R ? T ?F . However, we need to
compute:
arg max
e
P (e?|f) = arg max
e
?
f?
P (e?, f? |f)
= arg max
e
?
f?
P (e?|f?)P (f? |f).
5When none is found, we also consider a maximal segmen-
tation into isolated words.
This requires to compare model scores for mul-
tiple source segmentations and reorderings f? , hence
to compute P (f? |f) and P (e?|f?), rather than just the
non-normalized value that is usually used in CRFs.
Computing the normalizer Z(f? ; ?) for all se-
quences in S ?R is performed efficiently using stan-
dard finite-state operations as :
D = det(pi1(pi2(S ?R) ? T ? exp(F ))).
In fact, determinization (in the real semiring) has the
effect of accumulating for each f? the corresponding
normalizer Z(f? ; ?). Replacing each weight w in D
by ? log(w) and using the log semiring enables to
compute? log(Z(f? ; ?)). The best translation is then
obtained as: bestpath(pi2(S?R)??log(D)?T ?F )
in the tropical semiring.
Decoding and Rescoring with a target language
model An alternative manner of using a (large)
target side language model is to use it for rescoring
purposes. The consistent use of finite-state machines
and operations makes it fairly easy to include one
during decoding : it suffices to perform the search in
pi2(S?R)?? log(D)?T ?F ?L, where L represents
a n-gram language model. When combining several
models, notably a source segmentation model and/or
a target language model for rescoring, we have made
sure to rescale the (log)probabilities so as to balance
the language model scores with the CRF scores, and
to use a fixed word bonus to make hypotheses of dif-
ferent length more comparable. All these parameters
are tuned as part of the decoder development pro-
cess. It is finally noteworthy that, in our architecture,
alternative decoding strategies, such as MBR (Ku-
mar and Byrne, 2004) are also readily implemented.
4 Experiments
4.1 Corpora and metrics
For these experiments, we have used a medium size
training corpus, extracted from the datasets made
available for WMT 20116 evaluation campaign, and
have focused on one translation direction, from
French to English7.
Translation model training uses the entire News-
Commentary subpart of the WMT?2011 training
6statmt.org/wmt11
7Results in the other direction suggest similar conclusions.
546
0le : the/?le,the
DET : the/?DET,the
? : the/?the 0 1
? : the/0
? : cat/?the,cat
0 1
? : the/0
chat : cat/?chat,cat
Figure 2: Feature matchers. The star symbol (*) matches any possible observation.
French English
sent? token types token types
train 115 K 3 339 K 60 K 2 816 K 58 K
test 2008 2.0 K 55 K 9 K 49 K 8 K
test 2009 2.5 K 72 K 11 K 65 K 10 K
test 2010 2.5 K 69 K 10 K 61 K 9 K
Table 1: Corpora used for the experiments
data; for language models, we have considered two
approaches (i) a ?large? bigram model highly opti-
mized using all the available monolingual data and
(ii) a ?small? trigram language model trained on
just the English side of the NewsCommentary cor-
pus. The regularization parameters used in training
are tuned using the WMT 2009 test set; the various
parameters implied in the decoding are tuned (for
BLEU) on WMT 2008 test set; the internal tests re-
ported below are performed on the 2010 test lines
(see Table 1) using the best parameters found during
tuning. Various statistics regarding these corpora are
reproduced on Table 1.
All the training corpora were aligned using
MGIZA++ with standard parameters8, and pro-
cessed in the standard tuple extraction pipeline. The
development and test corpora were also processed
analogously. For the sake of comparison, we also
trained a standard n-gram-based and a Moses sys-
tem (Koehn et al, 2007) with default parameters
and a 3-gram target LM trained using only the tar-
get side of our parallel corpus. The development set
(test 2009) was used to tune these two systems. All
performance are measured using BLEU (Papineni et
al., 2002).
8As part of a much larger batch of texts.
4.2 Features
The baseline system is composed only of transla-
tion features [trs] and target bigram features [t2g].
The former correspond to functions of the form
gus,t(f? , e?, i) = I(f?i = s ? e?i = t), where s
and t respectively denote source and target phrases
and I() is the indicator function. These are also
generalized to part-of-speech and also to any pos-
sible source phrase, giving rise to features such as
gu?,t = (f? , e?, i) = I(e?i = t). Target bigram features
correspond to functions of the form gbt,t?(f? , e?, i) =
I(e?i?1 = t? e?i = t?). The last baseline feature is the
copy feature, which fires whenever the source and
target segments are identical.
Supplementary groups of features are considered
in further stages:
? suffix/prefix features [ix]. These features allow
to generalize baseline features on the source
side to fixed length prefixes and suffixes, thus
smoothing the parameters.
? context features [ctx]. These features are sim-
ilar to unigram features, but also test the left
source tuple and the corresponding part-of-
speech.
? segmentation features [seg]. These features are
meant to express a preference for longer tuples
and to regulate the number of target words per
source word. We consider the following feature
functions (|e| denotes the length of e):
? target length features :
gl?,l(f? , e?, i) = I(|e?i| = l)
? source-target length features :
gll,l?(f? , e?, i) = I(|f?i| = l ? |e?i| = l
?)
? source-target length ratio :
gll(f? , e?, i) = I(round(
| efi|
|ei|
) = l)
547
Note that all these features are further condi-
tioned on the target label.
? reordering features [ord]. These features are
meant to model preferences for specific lo-
cal reordering patterns and take into account
neighbor source fragments in e? together with
the current label. Each source side segment
f?i is made of some source words that, prior
to source reordering, were located at indices
i1 . . . il, so that f?i = fi1 . . . fil . The high-
est (resp. lowest) index in this sequence is df?ie
(resp. bf?ic). The leftmost (resp. rightmost) in-
dex is [f?i[ (resp. ]f?i]).
Using these notations, our model includes the
following patterns:
? distortion features, measuring the gaps be-
tween consecutive source fragments :
gol,t(f? , e?, i)=I(?(f?i, e?i)= l ? e?i= t),
where ?(f?i, e?i) ={
bf?ic ? df?i?1e if (df?i?1e ? bf?ic)
df?ie ? bf?i?1c otherwise .
? lexicalized reordering, identifying mono-
tone, swap and discontinuous configura-
tions (Tillman, 2004). The monotonous
test is defined as: gom(f? , e?, i) =
I(]ei?1] = [ei[); the swap and discon-
tinuous configurations are defined analo-
gously.
? ?gappiness? test : this feature is activated
whenever the source indices i1...il contain
one or several gaps.
4.3 Experiments and lessons learned
Training time The first lesson learned is that
training can be performed efficiently. Our baseline
system, which only contains trs and trg contains ap-
proximately 87 million features, out of which a lit-
tle bit more than 600K are selected. Adding up all
supplementary features raises the number of param-
eters to about 130M features, out of which 1.5M are
found useful. All these systems require between 3
and 5 hours to train9. These numbers are obtained
with a `1 penalty term ? 1, which offers a good bal-
ance between accuracy and sparsity.
9All experiments run on a server with 64G of memory and
two Xeon processors with 4 cores at 2.27 Ghz.
Test conditions In order to better assess the
strengths and weaknesses of our approach, we com-
pare several test settings: the most favorable con-
siders only one possible segmentation/reordering f?
for each f , obtained through forced alignment with
the reference; we then consider the more challeng-
ing case where the reordering is fixed, but several
segmentations are considered; then the regular de-
coding task, where both segmentation and reorder-
ing are unknown and where the entire space of all
segmentations and reordering is searched. For each
condition, we also vary (i) the set of features used
and (ii) the target language model used, if any.
Wherever applicable, we also report contrasts with
n-gram-based systems subject to the same input and
comparable resources, varying the order of the tuple
language model, as well as with Moses. Results are
in Table 2.
dev test # feat.
decoding with optimal segmentation/reordering
CRF (trs,trg) 23.8 25.1 660K
CRF +ctx 24.1 25.4 1.5M
CRF +ix,ord,seg 24.3 25.6 1.5M
decoding with optimal reordering
n-gram (2g,3g) 20.6 24.1 755K
n-gram (3g,3g) 21.5 25.2 755K
CRF trs,trg - 22.8 660K
CRF +ctx - 23.1 1.5M
CRF +ix,ord,seg - 23.5 1.5M
regular decoding
Moses (3g) 21.2 20.5
n-gram (2g,3g) 20.6 20.2 755K
n-gram (3g,3g) 21.5 21.2 755K
CRF (trs,trg) - 18.3 660K
CRF +ctx - 18.8 1.5M
CRF +ix,ord,seg - 19.1 1.5M
CRF +ix,ord,seg+3g - 19.1 1.5M
Table 2: Translation performance
Extending the feature set As expected, the use
of increasingly complex feature sets seems benefi-
cial in all experimented conditions. It is noteworthy
that throwing in reordering and contextual features
is helping, even when decoding one single segmen-
tation and reordering. This is because these features
do not help to select the best input reordering, but
548
help choose the best target phrase.
Searching a larger space Going from the sim-
pler to the more difficult conditions yields signif-
icant degradations in the model, as our best score
drops down from 25.6 to 23.5 (with known reorder-
ing) then to 19.1 (regular decoding). This is a clear
indication that our current segmentation/reordering
model is not delivering very useful scores. A similar
loss is incurred by the n-gram system, which loses
4 bleu points between the two conditions.
LM rescoring Our results to date with target side
language models have proven inconclusive, which
might explain why our best results remain between
one and two BLEU points behind the n-gram based
system using comparable information. Note also
that preliminary experiments with incorporating a
large bigram during training have also failed to date
to provide us with improvements over the baseline.
Summary In sum, the results accumulated during
this first round of experiments tend to show that our
CRF model is still underperforming the more es-
tablished baseline by approximately 1 to 1.5 BLEU
point, when provided with comparable resources.
Sources of improvements that have been clearly
identified is the scoring of reordering and segmen-
tations, and the use of a target language model in
training and/or decoding.
5 Related work
Discriminative learning approaches have proven
successful for many NLP tasks, notably thanks to
their ability to cope with flexible linguistic repre-
sentations and to accommodate potentially redun-
dant descriptions. This is especially appealing for
machine translation, where the mapping between
a source word or phrase and its target correlate(s)
seems to involve an large array of factors, such as its
morphology, its syntactic role, its meaning, its lexi-
cal context, etc. (see eg. (Och et al, 2004; Gimpel
and Smith, 2008; Chiang et al, 2009), for inspira-
tion regarding potentially useful features in SMT).
Discriminative learning requires (i) a parameter-
ized scoring function and (ii) a training objective.
The scoring function is usually assumed to be linear
and ranks candidate outputs y for input x accord-
ing to ?TG(x, y), where ? is the parameter vector. ?
andG deterministically imply the input/output map-
ping as x ? arg maxy ?
TG(x, y). Given a set of
training pairs {xi, yi, i = 1 . . . N}, parameters are
learned by optimizing some regularized loss func-
tion of ?, so as to make the inferred input/output
mapping faithfully replicate the observed instances.
Machine translation, like most NLP tasks, does
not easily lend itself to that approach, due to the
complexity of the input/output objects (word or la-
bel strings, parse trees, dependency structures, etc).
This complexity makes inference and learning in-
tractable, as both steps imply the resolution of
the arg max problem over a combinatorially large
space of candidates y. Structured learning tech-
niques (Bakir et al, 2007), developed over the last
decade, rely on decompositions of these objects into
sub-parts as part of a derivation process, and use
conditional independence assumptions between sub-
parts to render the learning and inference problem
tractable. For machine translation, this only pro-
vides part of the solution, as the training data only
contain pairs of word aligned sentences (f , e), but
lack the explicit derivation h from f to e that is re-
quired to train the model in a fully supervised way.
The approach of (Liang et al, 2006a) circumvents
the issue by assuming that the hidden derivation h
can be approximated through forced decoding. As-
suming that h is in fact observed as the optimal
(Viterbi) derivation h? from f to e given the cur-
rent parameter value10, it is straightforward to re-
cast the training of a phrase-based system as a stan-
dard structured learning problem, thus amenable to
training algorithms such as the averaged perceptron
of (Collins, 2002). This approximation is however
not genuine, and the choice of the most appropriate
derivation seems to raises intriguing issues (Watan-
abe et al, 2007; Chiang et al, 2008).
The authors of (Blunsom et al, 2008; Blunsom
and Osborne, 2008) consider models for which it is
computationally possible to marginalize out all pos-
sible derivations of a given translation. As demon-
strated in these papers, this approach is tractable
even when the derivation process is a based on syn-
chronous context-free grammars, rather that finite-
state devices. However, the computational cost as-
10If one actually exists in the model, thus raising the issue of
reference reachability, see discussion in Section 3.
549
sociated with training and inference remains very
high, especially when using a target side language
model, which seems to preclude the application to
large-scale translation tasks11. The recent work of
(Dyer and Resnik, 2010) proceeds from a similar
vein: translation is however modeled as a two step
process, where a set of possible source reorderings,
represented as a parse forest, are associated with
possible target sentences, using, as we do, a finite-
state translation model. This translation model is
trained discriminatively by marginalizing out the
(unobserved) reordering variables; inference can be
performed effectively by intersecting the input parse
forest with a transducer representing translation op-
tions.
A third strategy is to consider a simpler class of
derivation process, which only partly describe the
mapping between f and e. This is, for instance,
the approach of (Bangalore et al, 2007), where a
simple bag-of-word representation of the target sen-
tence is computed using a battery of boolean clas-
sifiers (one for each target word). In this approach,
discriminative training is readily applicable, as the
required supervision is overtly present in example
source-target pairs (f , e); however, a complemen-
tary reshaping/reordering step is necessary to turn
the bag-of-word into a full-fledged translation. This
work was recently revisited in (Mauser et al, 2009),
where a conditional model predicting the presence
of each target phrase provides a supplementary score
for the standard ?log-linear? model.
This line of research has been continued notably
in (Ka?a?ria?inen, 2009), which introduces an exponen-
tial model of bag of phrases (allowing some over-
lap), that enables to capture localized dependencies
between target words, while preserving (to some ex-
tend) the efficiency of training and inference. Su-
pervision is here indirectly provided by word align-
ment and correlated phrase extraction processes
implemented in conventional phrase-based systems
(Koehn et al, 2003). If this model seems to deliver
state-of-the-art performance on large-scale tasks, it
does so at a very high computational cost. More-
over, for lack of an internal modeling of reordering
processes, this approach, like the bag-of-word ap-
11For instance, the experiments reported in (Blunsom and Os-
borne, 2008) use the English-Chinese BTEC, where the average
sentence length is lesser than 10.
proach, seems only appropriate for language pairs
with similar or related word ordering.
The approach developed in this paper fills a gap
between the hierarchical model of (Blunsom et
al., 2008) and the phrase-based model (Ka?a?ria?inen,
2009), with whom we share several important as-
sumptions, such as the use of alignment information
to provide supervision, and the resort to a an ?ex-
ternal?, albeit a more powerful, reordering compo-
nent. Using a finite-state model enables to process
reasonably large corpora, and gives some hopes as to
the scalability of the whole enterprise; it also makes
the integration of a target side language model much
easier than in hierarchical models.
6 Discussion and future work
In this paper, we have given detailed description of
an original phrase-based system implementing a dis-
criminative version of the n-gram model, where the
translation model probabilities are computed with
conditional random fields. We have showed how
to implement this approach using a memory effi-
cient implementation of the optimization algorithms
needed for training: in our approach, training a mid-
scale translation system with hundred of thousands
sentence pairs and millions of features only takes a
couple of hours on a standalone desktop machine.
Using `1 regularization has enabled to assess the
usefulness of various families of features.
We have also detailed a complete decoder im-
plemented as a pipeline of finite-state transducers,
which allows to efficiently combine several models,
to produce n-best lists and word lattices.
The results obtained in a series of preliminary ex-
periments show that our system is already deliver-
ing competitive translations, as acknowledged by a
comparison with two strong phrase-based baselines.
We have already started to implement various opti-
mizations and to experiment with somewhat larger
datasets (up to 500K sentence pairs) and larger fea-
ture sets, notably incorporating word sense disam-
biguation features: this work needs to be contin-
ued. In addition, we intend to explore a number
of extensions of this architecture, such as imple-
menting MBR decoding (Kumar and Byrne, 2004)
or adapting the translation model to new domains
and conditions, using, for instance, the proposal of
550
(Daume III, 2007)12.
One positive side effect of experimenting with
new translation models is that they help reevalu-
ate the performance of the whole translation system
pipeline: in particular, discriminative training seems
to be more sensible to alignments errors than the cor-
responding n-gram system, which suggests to pay
more attention to possible errors in the training data;
we have also seen that the current reordering model
defines a too narrow search space and delivers in-
sufficiently discriminant scores: we will investigate
various ways to further improve the computation and
scoring of hypothetical source reorderings.
Acknowledgements
The authors wish to thank the reviewers for com-
ments and suggestions. This work was achieved as
part of the Quaero Programme, funded by OSEO,
French State agency for innovation.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Go?khan Bakir, Thomas Hofmann, Bernhard Scho?lkopf,
Alexander J.Smola, Ben Taskar, and S.V.N. Vish-
wanathan. 2007. Predicting structured output. MIT
Press.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 152?159,
Prague, Czech Republic.
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 215?223, Honolulu,
Hawaii.
12In a nutshell, this proposal amounts to having three dif-
ferent parameters for each feature; one parameter is trained
as usual; the other two parameters are updated conditionally,
depending whether the training instance comes from the in-
domain or from the out-domain training dataset.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-08: HLT,
pages 200?208, Columbus, Ohio.
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218?226. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Josep M. Crego and Jose? B. Marin?o. 2007. Improving
SMT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. Association for Compu-
tational Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
08: HLT, Short Papers, pages 25?28, Columbus, Ohio.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
551
surface heuristics. In Proceedings of the ACL work-
shop on Statistical Machine Translation, pages 31?38,
New York City, NY.
Yonggang Deng and William Byrne. 2006. MTTK: An
alignment toolkit for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Demon-
strations, pages 265?268, New York City, USA.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 858?866, Los Angeles,
California. Association for Computational Linguistics.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations ? In Pro-
ceedings of ACL-08: HLT, pages 986?993, Columbus,
Ohio.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In SETQA-NLP ?08.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Matti Ka?a?ria?inen. 2009. Sinuhe ? statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1027?1036, Singapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistic, pages 127?133, Edmond-
ton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
Annual Meeting of the Association for Computational
Linguistics (ACL), demonstration session, pages 177?
180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages
169?176, Boston, Massachusetts, USA. Association
for Computational Linguistics.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the International Conference on Ma-
chine Learning, pages 282?289. Morgan Kaufmann,
San Francisco, CA.
Thomas Lavergne, Olivier Capp, and Franois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504?513, Uppsala,
Sweden.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006a. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia.
Percy Liang, Ben Taskar, and Dan Klein. 2006b. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 133?139.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer Verlag.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning, pages 78?86.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
552
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Martin Riedmiller and Heinrich Braun. 1993. A direct
adaptive method for faster backpropagation learning:
The RPROP algorithm. In Proceedings of the IEEE
International Conference on Neural Networks, pages
586?591, San Francisco, USA.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. J.R.Statist.Soc.B, 58(1):267?288.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Susan Du-
mais, Daniel Marcu, and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 101?104, Boston,
Massachusetts, USA.
J. Turian, B. Wellington, and I.D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In Proc. Neural Information Pro-
cessing Systems (NIPS), volume 19, pages 1409?1417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484, Uppsala, Sweden.
Fei Xia and Michael McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING), pages
508?514, Geneva, Switzerland.
553
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joint WMT 2012 Submission of the QUAERO Project
?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,
?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,
?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,
?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2012 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems and finally the RWTH system combi-
nation combined these translations in our final
submission. Experimental results show im-
provements of up to 1.7 points in BLEU and
3.4 points in TER compared to the best single
system.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
This paper is structured as follows. In Section
2, the different engines of all four groups are in-
troduced. In Section 3, the RWTH Aachen system
combination approach is presented. Experiments
with different system selections for system combi-
nation are described in Section 4. Finally in Section
5, we discuss the results.
2 Translation Systems
For WMT 2012 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned
on the newstest2009 or newstest2010 development
set. The newstest2011 dev set was used to train
the system combination parameters. Finally, the
newstest2008-newstest2010 dev sets were used to
compare the results of the different system combina-
tion settings. In this Section all four different system
engines are presented.
2.1 RWTH Aachen Single Systems
For the WMT 2012 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
parallel corpus, the translation probabilities are esti-
mated by relative frequencies. The standard feature
322
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. The model weights are optimized with standard
Mert (Och, 2003) on 200-best lists. The optimiza-
tion criterium is BLEU.
2.1.2 Hierarchical System
For the hierarchical setups (HPBT) described in
this paper, the open source Jane toolkit (Vilar et
al., 2010) is employed. Jane has been developed at
RWTH and implements the hierarchical approach as
introduced by Chiang (2007) with some state-of-the-
art extensions. In hierarchical phrase-based transla-
tion, a weighted synchronous context-free grammar
is induced from parallel text. In addition to contigu-
ous lexical phrases, hierarchical phrases with up to
two gaps are extracted. The search is typically car-
ried out using the cube pruning algorithm (Huang
and Chiang, 2007). The model weights are opti-
mized with standard Mert (Och, 2003) on 100-best
lists. The optimization criterium is 4BLEU ?TER.
2.1.3 Preprocessing
In order to reduce the source vocabulary size
translation, the German text was preprocessed
by splitting German compound words with the
frequency-based method described in (Koehn and
Knight, 2003a). To further reduce translation com-
plexity for the phrase-based approach, we performed
the long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.4 Language Model
For both decoders a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl, the
109 French-English, UN and LDC Gigaword Fourth
Edition corpora. For the 109 French-English, UN
and LDC Gigaword corpora RWTH applied the data
selection technique described in (Moore and Lewis,
2010).
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogenous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003b). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation and op-
timization with regard to the BLEU score is done us-
ing Minimum Error Rate Training as described in
Venugopal et al (2005).
2.2.3 Translation Models
The translation model is trained on the Europarl
and News Commentary Corpus and the phrase ta-
ble is based on a discriminative word alignment
(Niehues and Vogel, 2008).
In addition, the system applies a bilingual lan-
guage model (Niehues et al, 2011) to extend the
context of source language words available for trans-
lation.
Furthermore, we use a discriminative word lexi-
con as introduced in (Mauser et al, 2009). The lex-
icon was trained and integrated into our system as
described in (Mediani et al, 2011).
At last, we tried to find translations for
out-of-vocabulary (OOV) words by using quasi-
morphological operations as described in Niehues
and Waibel (2011). For each OOV word, we try to
find a related word that we can translate. We modify
the ending letters of the OOV word and learn quasi-
morphological operations to be performed on the
known translation of the related word to synthesize
a translation for the OOV word. By this approach
we were for example able to translate Kaminen into
chimneys using the known translation Kamin # chim-
ney.
2.2.4 Language Models
We use two 4-gram SRI language models, one
trained on the News Shuffle corpus and one trained
1http://hunspell.sourceforge.net/
323
on the Gigaword corpus. Furthermore, we use a 5-
gram cluster-based language model trained on the
News Shuffle corpus. The word clusters were cre-
ated using the MKCLS algorithm. We used 100
word clusters.
2.2.5 Reordering Model
Reordering is performed based on part-of-speech
tags obtained using the TreeTagger (Schmid, 1994).
Based on these tags we learn probabilistic continu-
ous (Rottmann and Vogel, 2007) and discontinuous
(Niehues and Kolss, 2009) rules to cover short and
long-range reorderings. The rules are learned from
the training corpus and the alignment. In addition,
we learned tree-based reordering rules. Therefore,
the training corpus was parsed by the Stanford parser
(Rafferty and Manning, 2008). The tree-based rules
consist of the head node of a subtree and all its
children as well as the new order and a probability.
These rules were applied recursively. The reordering
rules are applied to the source sentences and the re-
ordered sentence variants as well as the original se-
quence are encoded in a word lattice which is used
as input to the decoder. For the test sentences, the
reordering based on parts-of-speech and trees allows
us to change the word order in the source sentence
so that the sentence can be translated more easily.
In addition, we build reordering lattices for all train-
ing sentences and then extract phrase pairs from the
monotone source path as well as from the reordered
paths.
2.3 LIMSI-CNRS Single System
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine translation
system based on bilingual n-gram2. In this approach,
the translation model relies on a specific decomposi-
tion of the joint probability of a sentence pair P(s, t)
using the n-gram assumption: a sentence pair is de-
composed into a sequence of bilingual units called
tuples, defining a joint segmentation of the source
and target. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing which ultimately derives from initial word and
phrase alignments.
2http://ncode.limsi.fr/
2.3.1 An Overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using a
n-gram model of (source,target) pairs (Casacuberta
and Vidal, 2004). Training this model requires to
reorder source sentences so as to match the target
word order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a ?weak?
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
used in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 development set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2.3.2 Continuous Space Translation Models
One critical issue with standard n-gram transla-
tion models is that the elementary units are bilingual
pairs, which means that the underlying vocabulary
can be quite large. Unfortunately, the parallel data
available to train these models are typically smaller
than the corresponding monolingual corpora used to
train target language models. It is very likely then,
that such models should face severe estimation prob-
lems. In such setting, using neural network language
3Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
324
model techniques seem all the more appropriate. For
this study, we follow the recommendations of Le et
al. (2012), who propose to factor the joint proba-
bility of a sentence pair by decomposing tuples in
two (source and target) parts, and further each part
in words. This yields a word factored translation
model that can be estimated in a continuous space
using the SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the computa-
tional cost of computing n-gram probabilities. The
solution used here was to resort to a two pass ap-
proach: the first pass uses a conventional back-off
n-gram model to produce a k-best list; in the second
pass, the k-best list is reordered using the probabil-
ities of m-gram SOUL translation models. In the
following experiments, we used a fixed context size
for SOUL of m = 10, and used k = 300.
2.3.3 Corpora and Data Preprocessing
The parallel data is word-aligned using
MGIZA++4 with default settings. For the En-
glish monolingual training data, we used the same
setup as last year5 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we took advantage of our in-house
text processing tools for tokenization and detok-
enization steps (De?chelotte et al, 2008) and our sys-
tem was built in ?true-case?. As German is mor-
phologically more complex than English, the default
policy which consists in treating each word form
independently is plagued with data sparsity, which
is detrimental both at training and decoding time.
Thus, the German side was normalized using a spe-
cific pre-processing scheme (Allauzen et al, 2010;
Durgar El-Kahlout and Yvon, 2010), which notably
aims at reducing the lexical redundancy by (i) nor-
malizing the orthography, (ii) neutralizing most in-
flections and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
a system composed of the standard SYSTRAN MT
engine in combination with a statistical post editing
(SPE) component.
4http://geek.kyloo.net/software
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k -
800k entries per language pair).
The SYSTRAN phrase-based SPE component
views the output of the rule-based system as the
source language, and the (human) reference trans-
lation as the target language, see (L. Dugast and
Koehn, 2007). It performs corrections and adaptions
learned from the 5-gram language model trained on
the parallel target-to-target corpus. Moreover, the
following measures - limiting unwanted statistical
effects - were applied:
? Named entities, time and numeric expressions
are replaced by special tokens on both sides.
This usually improves word alignment, since
the vocabulary size is significantly reduced. In
addition, entity translation is handled more re-
liably by the rule-based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus to help to improve word
alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
The SPE language model was trained on 2M bilin-
gual phrases from the news/Europarl corpora, pro-
vided as training data for WMT 2012. An addi-
tional language model built from 15M phrases of
the English LDC Gigaword corpus using Kneser-
Ney (Kneser and Ney, 1995) smoothing was added.
Weights for these separate models were tuned by
the Mert algorithm provided in the Moses toolkit
(P. Koehn et al, 2007), using the provided news de-
velopment set.
325
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical models
is selected as consensus translation.
4 Experiments
This year, we tried different sets of single systems
for system combination. As RWTH has two dif-
ferent translation systems, we put the output of
both systems into system combination. Although
both systems have the same preprocessing and lan-
guage model, their hypotheses differ because of
their different decoding approach. Compared to
the other systems, the system by SYSTRAN has a
completely different approach (see section 2.4). It
is mainly based on a rule-based system. For the
German?English pair, SYSTRAN achieves a lower
BLEU score in each test set compared to the other
groups. However, since the SYSTRAN system is
very different to the others, we still obtain an im-
provement when we add it also to system combina-
tion.
We did experiments with different optimization
criteria for the system combination optimization.
All results are listed in Table 1 (unoptimized), Table
2 (optimized on BLEU) and Table 3 (optimized on
TER-BLEU). Further, we investigated, whether we
will loose performance, if a single system is dropped
from the system combination. The results show that
for each optimization criteria we need all systems to
achieve the best results.
For the BLEU optimized system combination, we
obtain an improvement compared to the best sin-
gle systems for all dev sets. For newstest2008, we
get an improvement of 1.5 points in BLEU and 1.5
points in TER compared to the best single system of
Karlsruhe Institute of Technology. For newstest2009
we get an improvement of 1.9 points in BLEU and
1.5 points in TER compared to the best single sys-
tem. The system combination of all systems outper-
forms the best single system with 1.9 points in BLEU
and 1.9 points in TER for newstest2010. For new-
stest2011 the improvement is 1.3 points in BLEU
and 2.9 points in TER.
For the TER-BLEU optimized system combina-
tion, we achieved more improvement in TER com-
pared to the BLEU optimized system combination.
For newstest2008, we get an improvement of 0.8
points in BLEU and 3.0 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. The system combinations performs better
on newstest2009 with 1.3 points in BLEU and 2.7
points in TER. For newstest2010, we get an im-
provement of 1.7 points in BLEU and 3.4 points in
TER and for newstest2011 we get an improvement
of 0.7 points in BLEU and 2.5 points in TER.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally, the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU and
a lower TER score compared to each single sys-
tem. For each optimization criteria the system com-
binations using all single systems outperforms the
system combinations using one less single system.
Although the single system of SYSTRAN has the
worst error scores and the RWTH single systems are
similar, we achieved the best result in using all single
systems. For the WMT 12 evaluation, we submitted
the system combination of all systems optimized on
BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
326
Table 1: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are unoptimized.
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
KIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9
RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7
Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2
RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4
SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8
sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5
sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6
sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6
sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0
sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1
sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3
Table 2: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6
sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1
sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5
sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2
sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5
sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2
Table 3: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on TER-BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6
sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9
sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0
sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3
sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5
sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4
327
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003a. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn and K. Knight. 2003b. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In NAACL ?12: Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology.
Jose? B. Marin?o, R. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4).
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT English-
French Translation Systems for IWSLT 2011. In Pro-
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. In Proc.
of Third ACL Workshop on Statistical Machine Trans-
lation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In Pro-
328
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Evelyne
Tzoukermann and SusanEditors Armstrong, editors,
Proceedings of the ACL SIGDATWorkshop, pages 47?
50. Kluwer Academic Publishers.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
329
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 260?265,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
LIMSI?s Participation in the 2013 Shared Task
on Native Language Identification
Thomas Lavergne, Gabriel Illouz, Aure?lien Max
LIMSI-CNRS
Univ. Paris Sud
Orsay, France
{firstname.lastname}@limsi.fr
Ryo Nagata
LIMSI-CNRS & Konan University
8-9-1 Okamoto
Kobe 658-0072 Japan
rnagata@konan-u.ac.jp
Abstract
This paper describes LIMSI?s participation to
the first shared task on Native Language Iden-
tification. Our submission uses a Maximum
Entropy classifier, using as features character
and chunk n-grams, spelling and grammati-
cal mistakes, and lexical preferences. Perfor-
mance was slightly improved by using a two-
step classifier to better distinguish otherwise
easily confused native languages.
1 Introduction
This paper describes the submission from LIMSI to
the 2013 shared task on Native Language Identifica-
tion (Tetreault et al, 2013). The creation of this new
challenge provided us with a dataset (12,100 TOEFL
essays by learners of English of eleven native lan-
guages (Blanchard et al, 2013)) that was necessary
to us to develop an initial framework for studying
Native Language Identification in text. We expect
that this challenge will draw conclusions that will
provide the community with new insights into the
impact of native language in foreign language writ-
ing. We believe that such a research domain is
crucial, not only for improving our understanding
of language learning and language production pro-
cesses, but also for developing Natural Language
Processing applications to support text improve-
ment.
This article is organized as follows. We first de-
scribe in Section 2 our maximum entropy system
used for the classification of a given text in English
into the native languages of the shared task. We then
introduce the various sets of features that we have in-
cluded in our submission, comprising basic n-gram
features (3.1) and features to capture spelling mis-
takes (3.2), grammatical mistakes (3.3), and lexical
preference (3.4). We next report the performance of
each of our sets of features (4.1) and our attempt to
perform a two-step classification to reduce frequent
misclassifications (4.2). We finally conclude with a
short discussion (section 5).
2 A Maximum Entropy model
Our system is based on a classical maximum entropy
model (Berger et al, 1996):
p?(y|x) =
1
Z?(x)
exp(?>F (x, y))
whereF is a vector of feature functions, ? a vector of
associated parameter values, and Z?(x) the partition
function.
Given N independent samples (xi, yi), the model
is trained by minimizing, with respect to ?, the neg-
ative conditional log-likelihood of the observations:
L(?) = ?
N?
i=1
log p(yi|xi).
This term is complemented with an additional regu-
larization term so as to avoid overfitting. In our case,
an `1 regularization is used, with the additional ef-
fect to produce a sparse model.
The model is trained with a gradient descent algo-
rithm (L-BFGS) using the Wapiti toolkit (Lavergne
et al, 2010). Convergence is determined either by
error rate stability on an held-out dataset or when
limits of numerical precision are reached.
260
3 Features
Our submission makes use of basic features, includ-
ing n-grams of characters and part-of-speech tags.
We further experimented with several sets of fea-
tures that will be described and compared in the fol-
lowing sections.
3.1 Basic features
We used n-grams of characters up to length 4 as fea-
tures. In order to reduce the size of the feature space
and the sparsity of these features, we used a hash
kernel (Shi et al, 2009) of size 216 with a hash fam-
ily of size 4. This allowed us to significantly reduce
the training time with no noticeable impact on the
model?s performance.
Our set of basic features also includes n-grams of
part-of-speech (POS) tags and chunks up to length 3.
Both were computed using an in-house CRF-based
tagger trained on PennTreeBank (Marcus et al,
1993). The POS tags sequences were post-processed
so that word tokens were used in lieu of their cor-
responding POS tags for the following: coordinat-
ing conjunctions, determiners, prepositions, modals,
predeterminers, possessives, pronouns, and question
adverbs (Nagata, 2013).
For instance, from this sentence excerpt:
[NP Some/DT people/NNS] [VP
might/MD think/VB] [SBAR that/IN]
[VP traveling/VBG] [PP in/IN]. . .
we extract n-grams from the pseudo POS-tag se-
quence:
Some NNS MD VB that VBG in. . .
and n-grams from the chunk sequence:
NP VP SBAR VP PP. . .
The length of chunks is encoded as separate fea-
tures that correspond to mean length of each type of
chunks. As shown in (Nagata, 2013), length of noun
sequences is also informative and thus was encoded
as a feature.
3.2 Capturing spelling mistakes
We added a set of features to capture information
about spelling mistakes in the model, following the
intuition that some spelling mistakes may be at-
tributed to the influence of the writer?s native lan-
guage.
To extract these features, each document is pro-
cessed using the ispell1 spell checker. This re-
sults in a list of incorrectly written word forms and
a set of potential corrections. For each word, the
best correction is next selected using a set of rules,
which were built manually after a careful study of
the training dataset.
When a corrected word is found, the incorrect
fragment of the word is isolated by striping from
the original and corrected words common prefix and
suffix, keeping only the inner-most substring differ-
ence. For example, given the following mistake and
correction:
appartment? apartment
this procedure generates the following feature:
pp? p
Such a feature may for instance help to identify na-
tive languages (using latin scripts) where doubling
of letters is frequent.
3.3 Capturing grammatical mistakes
Errors at the grammatical level are captured using
the ?language tool? toolkit (Milkowski, 2010), a
rule-based grammar and style checker. Each rule fir-
ing in a document is mapped to an individual feature.
This triggers features such as
BEEN PART AGREEMENT, corresponding to
cases where the auxiliary be is not followed by a
past participle, or EN A VS AN, corresponding to
confusions between the correct form the articles a
and an.
3.4 Capturing lexical preferences
Learners of a foreign language may have some pref-
erence for lexical choice given some semantic con-
tent that they want to convey2. We made the follow-
ing assumption: the lexical variant chosen for each
word may correspond to the less ambiguous choice
if mapping from the native language to English3.
1http://www.gnu.org/software/ispell/
2We assumed that we should not expect thematic differences
in the contents of the essays across original languages, as the
prompts for the essays were evenly distributed.
3This assumption of course could not hold for advanced
learners of English, who should make their lexical choices in-
dependently of their native language.
261
Thus, for each word in an English essay, if we
knew a corresponding word (or sense) that a writer
may have thought of in her native language, we
would like to consider the most likely translation
into English, according to some reliable probabilis-
tic model of lexical translation into English, as the
lexical choice most likely to be made by a learner of
this native language.
As we obviously do not have access to the word
in the native language of the writer, we approximate
this information by searching for the word that max-
imizes the translation probability of translating back
from the native language after translating from the
original English word. This in fact corresponds to a
widely used way of computing paraphrase probabili-
ties from bilingual translation distributions (Bannard
and Callison-Burch, 2005):
e?l ? argmax
e
?
f
pl(f |e).pl(e|f)
where f ranges over all possible translations of En-
glish word e in a given native language l.
Preferably, we would like to obtain candidate
translations into the native language in context,
that is, by translating complete sentences and us-
ing a posteriori translation probabilities. We could
not do this for a number of reasons, the main one
being that we did not have the possibility of using
or building Statistical Machine Translation systems
for all the language pairs involving English and the
native languages of the shared task. We therefore
resorted to simply finding, for each English word,
the most likely back-translation into English via a
given native language. Using the Google Transla-
tion online Statistical Machine Translation service4,
which proposed translations from and to English and
all the native languages of the shared task, a further
approximation had to be made as, in practice, we
were only able to access the most likely translations
for words in isolation: we considered only the best
translation of the original English word in the native
language, and then kept its best back-translation into
English. We here note some common intuitions with
the use of roundtrip translation as a Machine Trans-
lation evaluation metrics (Rapp, 2009).
4http://translate.google.com
Table 1 provides various examples of back-
translations for English adjectives obtained via each
native language. The samples from the Table show
that our procedure produces a significant number of
non identical back-translations. They also illustrate
some types of undesirable results obtained, which
led us to only consider as features for our classi-
fier the proportion of words in essays for which
the above-defined back-translation yielded the same
word, considering all possible native languages. We
only considered content words, as out-of-context
back-translation for function words would be too un-
reliable. Table 2 shows values for some documents
of the training set. As can be seen, there are impor-
tant differences across languages, some languages
obtaining high scores on average (e.g. French and
Japanese) and others obtaining low scores on aver-
age (e.g. Korean, Turkish). Furthermore, the high-
est score is only rarely obtained for the actual native
language of each document, showing that keeping
the most probable language according to this value
alone would not allow to obtain a good classification
performance.
4 Experiments
4.1 Results per set of features
For all our experiments reported here, we used the
full training data provided using cross-validation to
tune the regularization parameter. Our results are
presented in the top part of Table 3. Using our com-
plete set of features yields our best performance on
accuracy, corresponding to a 0.75% absolute im-
provement over using our basic n-gram features
only. No type of features allows a significant im-
provement over the n-gram features when added in-
dividually.
4.2 Two-step classification
Table 4 contains the confusion matrix for our system
across languages. It clearly stands out that two lan-
guage pairs were harder to distinguish: Hindi (hin)
and Telugu (tel) on the one hand, and Korean (kor)
and Japanese (jpn) on the other.
In order to improve the performance of our model,
we performed a two-step classification focused on
these difficult pairs. For this, we built additional
classifiers for each difficult pairs. Both are built
262
eng abrupt affirmative amazing ambiguous anarchic atrocious attentive awkward
ara sudden positive amazing mysterious messy terrible heedful inappropriate
chi sudden sure amazing ambiguous anarchic atrocious careful awkward
fre sudden affirmative amazing ambiguous anarchic atrocious careful awkward
ger abrupt affirmative incredible ambiguous anarchical gruesome attentively awkward
hin suddenly positive amazing vague chaotic brutal observant clumsy
ita abrupt affirmative amazing ambiguous anarchist atrocious careful uncomfortable
jap sudden positive surprising ambiguous anarchy heinous cautious awkward
kor fortuitous positive amazing ambiguous anarchic severe kind awkward
spa abrupt affirmative surprising ambiguous anarchic atrocious attentive clumsy
tel abrupt affirmative amazing ambiguous anarchic formidable attentive awkward
tur sudden positive amazing uncertain anarchic brutal attentive strange
Table 1: Examples of back translations for English adjectives from the training set via each of the eleven native
languages of the shared task. Back-translations that differ from the original word are indicated using a bold face.
Doc id. Native l. ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
976 ARA 0.80 0.88 0.91 0.95 0.75 0.91 0.87 0.73 0.89 0.79 0.71
29905 CHI 0.84 0.81 0.93 0.87 0.79 0.89 0.89 0.56 0.93 0.62 0.75
61765 FRE 0.73 0.84 0.90 0.71 0.73 0.83 0.86 0.50 0.91 0.58 0.66
100416 GER 0.78 0.80 0.86 0.83 0.72 0.89 0.86 0.70 0.90 0.67 0.67
26649 HIN 0.68 0.75 0.88 0.89 0.67 0.85 0.86 0.69 0.86 0.75 0.77
39189 ITA 0.68 0.85 0.92 0.94 0.74 0.93 0.89 0.69 0.92 0.72 0.72
3044 JPN 0.83 0.81 0.89 0.83 0.68 0.94 0.91 0.71 0.94 0.83 0.70
3150 KOR 0.75 0.86 0.91 0.84 0.76 0.88 0.87 0.55 0.88 0.67 0.73
6614 SPA 0.79 0.90 0.86 0.85 0.78 0.85 0.92 0.67 0.90 0.70 0.68
12600 TEL 0.65 0.74 0.84 0.73 0.71 0.92 0.90 0.76 0.95 0.82 0.58
5565 TUR 0.70 0.77 0.88 0.78 0.70 0.84 0.86 0.72 0.84 0.74 0.71
Table 2: Values corresponding to the proportion of content words in a random essay for each native language for which
back-translation yielded the same word.
FRE GER ITA SPA TUR ARA HIN TEL KOR JPN CHI
FRE 79 4 4 3 2 3 0 0 2 2 1
GER 0 89 2 4 1 0 1 0 2 1 0
ITA 6 1 83 6 1 1 0 0 0 1 1
SPA 4 4 5 72 2 3 3 2 1 1 3
TUR 3 2 1 3 81 1 3 2 0 3 1
ARA 3 0 1 3 3 81 5 2 1 0 1
HIN 1 1 1 3 2 1 64 26 1 0 0
TEL 0 0 1 0 0 1 17 81 0 0 0
KOR 1 1 0 0 3 1 0 0 80 12 2
JPN 1 0 2 2 0 3 0 1 13 73 5
CHI 0 1 0 0 2 2 0 2 3 3 87
Table 4: Confusion matrix on the Test set.
263
Features X-Val Test
ngm 74.83% 75.27%
ngm+ort 74.98% 75.29%
ngm+grm 75.18% 75.63%
ngm+lex 74.85% 75.47%
all 75.57% 75.81%
2-step (a) 75.46% 75.69%
2-step (b) 75.89% 75.98%
Table 3: Accuracy results obtained by cross-validation
and using the provided Test set for various combina-
tions of features and our two 2-step strategies. The fea-
ture sets are: character and part-of-speech n-grams fea-
tures (ngm), spelling features (ort), grammatical features
(grm), and lexical preference features (lex).
from the same feature sets as for the first-step model
but with only three labels: one for each language of
the pair and one for any other language.
The training data used for these new models in-
clude all documents from both languages as well as
document misclassified as one of them by the first-
step classifier (using cross-validation to label the full
training set). The formers keep their original labels
while the later are relabeled as other.
Document classified in one of the difficult pairs
by the first-step classifier were post-processed with
these new models. When the new label predicted is
other, the second best choice of the first step is used.
We investigated two setups for the first classifier:
(a) using the original 11 native languages classi-
fier, and (b) using a new classifier with languages
of the difficult pairs merged, resulting in 9 native
?languages?.
Our results, shown in Figure 3 for easy com-
parison, improve over our system using all fea-
tures only when the first-pass classifier uses the set
of 9 merged pseudo-languages (b). We obtain a
moderate 0.32% absolute improvement in accuracy
over one-step classification on cross-validation, and
0.17% improvement on the Test set.
5 Discussion and conclusion
We have submitted on maximum entropy system to
the shared task on Native Language Identification,
for which our basic set of n-gram features already
obtained a level of performance, around 75% in ac-
curacy, close to the best performance reported in our
submission. The additional feature sets that we have
included in our system, while improving the model,
did not allow us to capture a deeper influence of the
native language.
A first analysis reveals that the model fails to fully
use the additional feature sets due to lack of context.
Future experiments will need to link more closely
these features to the documents for which they pro-
vide useful information.
Due to time constraints and engineering issues,
the two-pass system was not ready by the time of
submission. The results that we have included in
this report show that it is a promising approach that
we should continue to explore. We also plan to con-
duct experiments that exploit the information about
the level of English available in the essays, some-
thing that we did not consider for this submission.
While this information is not directly available, it
may be infered from the data as a first-step classifi-
cation. We believe that studying its influence on the
mistakes make learners of different native language
is a promising direction.
The approach that we have described in this sub-
mission, as most of previously published approaches
for this task, attempts to find mistakes in the text of
the documents. The most typical mistakes are then
used by the classifier to detect the native language.
This does not take into consideration the fact that na-
tive English writers also make errors. It would be in-
teresting to explore the divergence between various
sets of writers/learners, not from the mean of non-
native writers, but from the mean of native writers.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 597?604,
Ann Arbor, Michigan.
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1), March.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
264
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics, July.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
Marcin Milkowski. 2010. Developing an open-source,
rule-based proofreading tool. Software - Practice and
Experience, 40(7):543?566.
Ryo Nagata. 2013. Generating a language family tree
from indo-european non-native english texts (to ap-
pear). In Proceedings the 51th Annual Meeting of the
Association for Computational Linguistics (ACL). As-
sociation for Computational Linguistics.
Reinhard Rapp. 2009. The backtranslation score: Auto-
matic mt evalution at the sentence level without refer-
ence translations. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 133?136, Sun-
tec, Singapore.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash kernels for structured data. Journal of Machine
Learning Research, 10:2615?2637, December.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A Report on the First Native Language Identification
Shared Task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
265
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI @ WMT?13
Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,
Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Vietnamese Academy of Science and Technology3, Hanoi, Vietnam
lehaison@ioit.ac.vn
Abstract
This paper describes LIMSI?s submis-
sions to the shared WMT?13 translation
task. We report results for French-English,
German-English and Spanish-English in
both directions. Our submissions use
n-code, an open source system based on
bilingual n-grams, and continuous space
models in a post-processing step. The
main novelties of this year?s participation
are the following: our first participation
to the Spanish-English task; experiments
with source pre-ordering; a tighter integra-
tion of continuous space language mod-
els using artificial text generation (for Ger-
man); and the use of different tuning sets
according to the original language of the
text to be translated.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Eighth Workshop on
Statistical Machine Translation. LIMSI partici-
pated in the French-English, German-English and
Spanish-English tasks in both directions. For this
evaluation, we used n-code, an open source in-
house Statistical Machine Translation (SMT) sys-
tem based on bilingual n-grams1, and continuous
space models in a post-processing step, both for
translation and target language modeling.
This paper is organized as follows. Section 2
contains an overview of the baseline systems built
with n-code, including the continuous space mod-
els. As in our previous participations, several
steps of data pre-processing, cleaning and filter-
ing are applied, and their improvement took a non-
negligible part of our work. These steps are sum-
marized in Section 3. The rest of the paper is de-
voted to the novelties of the systems submitted this
1http://ncode.limsi.fr/
year. Section 4 describes the system developed for
our first participation to the Spanish-English trans-
lation task in both directions. To translate from
German into English, the impact of source pre-
ordering is investigated, and experimental results
are reported in Section 5, while for the reverse di-
rection, we explored a text sampling strategy us-
ing a 10-gram SOUL model to allow a tighter in-
tegration of continuous space models during the
translation process (see Section 6). A final section
discusses the main lessons of this study.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o
et al, 2006; Crego and Marin?o, 2006). In this
framework, translation is divided in two steps: a
source reordering step and a (monotonic) transla-
tion step. Source reordering is based on a set of
learned rewrite rules that non-deterministically re-
order the input words. Applying these rules result
in a finite-state graph of possible source reorder-
ings, which is then searched for the best possible
candidate translation.
2.1 Features
Given a source sentence s of I words, the best
translation hypothesis t? is defined as the sequence
of J words that maximizes a linear combination of
feature functions:
t? = argmax
t,a
{ M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature
function hm and a denotes an alignment between
source and target phrases. Among the feature
functions, the peculiar form of the translation
model constitutes one of the main difference be-
tween the n-gram approach and standard phrase-
based systems.
62
In addition to the translation model (TM), four-
teen feature functions are combined: a target-
language model; four lexicon models; six lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two
lexical weights are estimated from the automatic
word alignments. The weight vector ? is learned
using the Minimum Error Rate Training frame-
work (MERT) (Och, 2003) and BLEU (Papineni
et al, 2002) measured on nt09 (newstest2009) as
the optimization criteria.
2.2 Translation Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to repro-
duce the word order modifications introduced dur-
ing the tuple extraction process. Hence, only those
reordering hypotheses are translated and are intro-
duced using a set of reordering rules automatically
learned from the word alignments. Part-of-speech
(POS) information is used to increase the gen-
eralization power of these rules. Hence, rewrite
rules are built using POS, rather than surface word
forms (Crego and Marin?o, 2006).
2.3 SOUL rescoring
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al, 2003; Schwenk
et al, 2006) as a potential means to improve dis-
crete language models (LMs). As for our last year
participation (Le et al, 2012c), we take advantage
of the recent proposal of Le et al (2011). Using
a specific neural network architecture (the Struc-
tured OUtput Layer or SOUL model), it becomes
possible to estimate n-gram models that use large
vocabulary, thereby making the training of large
neural network LMs (NNLMs) feasible both for
target language models and translation models (Le
et al, 2012a). We use the same models as last year,
meaning that the SOUL rescoring was used for all
systems, except for translating into Spanish. See
section 6 and (Le et al, 2012c) for more details.
3 Corpora and data pre-processing
Concerning data pre-processing, we started from
our submissions from last year (Le et al, 2012c)
and mainly upgraded the corpora and the associ-
ated language-dependent pre-processing routines.
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte
et al, 2008). Previous experiments have demon-
strated that better normalization tools provide bet-
ter BLEU scores: all systems are thus built using
the ?true-case? scheme.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to un-
known forms). When translating from German
into English, the German side is thus normalized
using a specific pre-processing scheme (Allauzen
et al, 2010; Durgar El-Kahlout and Yvon, 2010)
which aims at reducing the lexical redundancy by
(i) normalizing the orthography, (ii) neutralizing
most inflections and (iii) splitting complex com-
pounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition,
for German, fine-grained POS labels were also
needed for pre-processing and were obtained us-
ing the RFTagger (Schmid and Laws, 2008).
For Spanish, all the availaible data are tokenized
using FreeLing2 toolkit (Padro? and Stanilovsky,
2012), with default settings and some added rules.
Sentence splitting and morphological analysis are
disabled except for del ? de el and al ? a el.
Moreover, a simple ?true-caser? based on upper-
case word frequency is used, and the specific
Spanish punctuation signs ??? and ??? are removed
and heuristically reintroduced in a post-processing
step. All Spanish texts are POS-tagged also using
Freeling. The EAGLES tag set is however sim-
plified by truncating the category label to the first
two symbols, in order to reduce the sparsity of the
reordering rules estimated by n-code.
For the CommonCrawl corpus, we found that
many sentences are not in the expected language.
For example, in the French side of the French-
English version, most of the first sentences are
in English. Therefore, foreign sentence pairs are
filtered out with a MaxEnt classifier that uses n-
grams of characters as features (n is between 1
and 4). This filter discards approximatively 10%
2http://nlp.lsi.upc.edu/freeling/
63
of the sentence pairs. Moreover, we also observe
that a lot of sentence pairs are not translation of
each other. Therefore, an extra sentence alignment
step is carried out using an in-house implementa-
tion of the tool described in (Moore, 2002). This
last step discards approximately 20% of the cor-
pus. For the Spanish-English task, the same filter-
ing is applied to all the available corpora.
4 System development for the
Spanish-English task
This is our first participation to the Spanish-
English translation task in both directions. This
section provides details about the development of
n-code systems for this language pair.
4.1 Data selection and filtering
The CommonCrawl and UN corpora can be con-
sidered as very noisy and out-of-domain. As de-
scribed in (Allauzen et al, 2011), to select a subset
of parallel sentences, trigram LMs were trained for
both Spanish and English languages on a subset of
the available News data: the Spanish (resp. En-
glish) LM was used to rank the Spanish (resp. En-
glish) side of the corpus, and only those sentences
with perplexity above a given threshold were se-
lected. Finally, the two selected sets were in-
tersected. In the following experiments, the fil-
tered versions of these corpora are used to train
the translation systems unless explicitly stated.
4.2 Spanish language model
To train the language models, we assumed that the
test set would consist in a selection of recent news
texts and all the available monolingual data for
Spanish were used, including the Spanish Giga-
word, Third Edition. A vocabulary is first defined
by including all tokens observed in the News-
Commentary and Europarl corpora. This vocab-
ulary is then expanded with all words that occur
more than 10 times in the recent news texts (LDC-
2007-2011 and news-crawl-2011-2012). This pro-
cedure results in a vocabulary containing 372k
words. Then, the training data are divided into
7 sets based on dates or genres. On each set, a
standard 4-gram LM is estimated from the vocab-
ulary using absolute discounting interpolated with
lower order models (Kneser and Ney, 1995; Chen
and Goodman, 1998). The resulting LMs are then
linearly interpolated using coefficients chosen so
Corpora BLEU
dev nt11 test nt12
es2en N,E 30.2 33.2
N,E,C 30.6 33.7
N,E,U 30.3 33.6
N,E,C,U 30.6 33.7
N,E,C,U (nf) 30.7 33.6
en2es N,E 32.2 33.3
N,E,C,U 32.3 33.6
N,E,C,U (nf) 32.5 33.9
Table 1: BLEU scores achieved with different
sets of parallel corpora. All systems are base-
line n-code with POS factor models. The follow-
ing shorthands are used to denote corpora, : ?N?
stands for News-Commentary, ?E? for Europarl,
?C? for CommonCrawl, ?U? for UN and (nf) for
non filtered corpora.
as to minimise the perplexity evaluated on the de-
velopment set (nt08).
4.3 Experiments
All reported results are averaged on 3 MERT runs.
Table 1 shows the BLEU scores obtained with dif-
ferent corpora setups. We can observe that us-
ing the CommonCrawl corpus improves the per-
formances in both directions, while the impact of
the UN data is less important, especially when
combined with CommonCrawl. The filtering strat-
egy described in Section 4.2 has a slightly posi-
tive impact of +0.1 BLEU point for the Spanish-
to-English direction but yields a 0.2 BLEU point
decrease in the opposite direction.
For the following experiments, all the available
corpora are therefore used: News-Commentary,
Europarl, filtered CommonCrawl and UN. For
each of these corpora, a bilingual n-gram model
is estimated and used by n-code as one individual
model score. An additionnal TM is trained on the
concatenation all these corpora, resulting in a to-
tal of 5 TMs. Moreover, n-code is able to handle
additional ?factored? bilingual models where the
source side words are replaced by the correspond-
ing lemma or even POS tag (Koehn and Hoang,
2007). Table 2 reports the scores obtained with
different settings.
In Table 2, big denotes the use of a wider
context for n-gram TMs (n = 4, 5, 4 instead
of 3, 4, 3 respectively for word-based, POS-based
and lemma-based TMs). Using POS factored
64
Condition BLEU
dev nt11 test nt12
es2en base 30.3 33.5
pos 30.6 33.7
big-pos 30.7 33.7
big-pos-lem 30.7 33.8
en2es base 32.0 33.4
pos 32.3 33.6
big-pos 32.3 33.8
big-pos-pos+ 32.2 33.4
Table 2: BLEU scores for different configuration
of factored translation models. The big prefix de-
notes experiments with the larger context for n-
gram translation models.
models yields a significant BLEU improvement,
as well as using a wider context for n-gram TMs.
Since Spanish is morphologically richer than En-
glish, lemmas are introduced only on the Span-
ish side. An additionnal BLEU improvement is
achieved by adding factored models based on lem-
mas when translating from Spanish to English,
while in the opposite direction it does not seem
to have any clear impact.
For English to Spanish, we also experimented
with a 5-gram target factored model, using the
whole morphosyntactic EAGLES tagset, (pos+ in
Table 2), to add some syntactic information, but
this, in fact, proved harmful.
As several tuning sets were available, experi-
ments were carried out with the concatenation of
nt09 to nt11 as a tuning data set. This yields an im-
provement between 0.1 and 0.3 BLEU point when
testing on nt12 when translating from Spanish to
English.
4.4 Submitted systems
For both directions, the submitted systems are
trained on all the available training data, the cor-
pora CommonCrawl and UN being filtered as de-
scribed previously. A word-based TM and a POS
factored TM are estimated for each training set.
To translate from Spanish to English, the system
is tuned on the concatenation of the nt09 to nt11
datasets with an additionnal 4-gram lemma-based
factored model, while in the opposite direction, we
only use nt11.
dev nt09 test nt11
en2de 15.43 15.35
en-mod2de 15.06 15.00
Table 3: BLEU scores for pre-ordering experi-
ments with a n-code system and the approach pro-
posed by (Neubig et al, 2012)
5 Source pre-ordering for English to
German translation
While distorsion models can efficiently handle
short range reorderings, they are inadequate to
capture long-range reorderings, especially for lan-
guage pairs that differ significantly in their syn-
tax. A promising workaround is the source pre-
ordering method that can be considered similar,
to some extent, to the reordering strategy imple-
mented in n-code; the main difference is that the
latter uses one deterministic (long-range) reorder-
ing on top of conventional distortion-based mod-
els, while the former only considers one single
model delivering permutation lattices. The pre-
ordering approach is illustrated by the recent work
of Neubig et al (2012), where the authors use a
discriminatively trained ITG parser to infer a sin-
gle permutation of the source sentence.
In this section, we investigate the use of this
pre-ordering model in conjunction with the bilin-
gual n-gram approach for translating English into
German (see (Collins et al, 2005) for similar ex-
periments with the reverse translation direction).
Experiments are carried out with the same settings
as described in (Neubig et al, 2012): given the
source side of the parallel data (en), the parser is
estimated to modify the original word order and to
generate a new source side (en-mod); then a SMT
system is built for the new language pair (en-mod
? de). The same reordering model is used to re-
order the test set, which is then translated with the
en-mod? de system.
Results for these experiments are reported in Ta-
ble 3, where nt09 and nt11 are respectively used
as development and test sets. We can observe that
applying pre-ordering on source sentences leads to
small drops in performance for this language pair.
To explain this degradation, the histogram of to-
ken movements performed by the model on the
pre-ordered training data is represented in Fig-
ure 1. We can observe that most of the movements
are in the range [?4,+6] (92% of the total occur-
65
Figure 1: Histogram of token movement size ver-
sus its occurrences performed by the model Neu-
big on the source english data.
rences), which can be already taken into account
by the standard reordering model of the baseline
system. This is reflected also by the following
statistics: surprisingly, only 16% of the total num-
ber of sentences are changed by the pre-ordering
model, and the average sentence-wise Kendall?s ?
and the average displacement of these small parts
of modified sentences are, respectively, 0.027 and
3.5. These numbers are striking for two reasons:
first, English and German have in general quite
different word order, thus our experimental con-
dition should be somehow similar to the English-
Japanese scenario studied in (Neubig et al, 2012);
second, since the model is able to perform pre-
ordering basically at any distance, it is surprising
that a large part of the data remains unmodified.
6 Artificial Text generation with SOUL
While the context size for BOLMs is limited (usu-
ally up to 4-grams) because of sparsity issues,
NNLMs can efficiently handle larger contexts up
to 10-grams without a prohibitive increase of the
overall number of parameters (see for instance the
study in (Le et al, 2012b)). However the major
bottleneck of NNLMs is the computation cost dur-
ing both training and inference. In fact, the pro-
hibitive inference time usually implies to resort to
a two-pass approach: the first pass uses a conven-
tional BOLM to produce a k-best list (the k most
likely translations); in the second pass, the prob-
ability of a NNLM is computed for each hypoth-
esis, which is then added as a new feature before
the k-best list is reranked. Note that to produce the
k-best list, the decoder uses a beam search strategy
to prune the search space. Crucially, this pruning
does not use the NNLMs scores and results in po-
tentially sub-optimal k-best-lists.
6.1 Sampling texts with SOUL
In language modeling, a language is represented
by a corpus that is approximated by a n-gram
model. Following (Sutskever et al, 2011; Deoras
et al, 2013), we propose an additionnal approxi-
mation to allow a tighter integration of the NNLM:
a 10-gram NNLM is first estimated on the training
corpus; texts then are sampled from this model to
create an artificial training corpus; finally, this arti-
ficial corpus is approximated by a 4-gram BOLM.
The training procedure for the SOUL NNLM is
the same as the one described in (Le et al, 2012c).
To sample a sentence from the SOUL model, first
the sentence length is randomly drawn from the
empirical distribution, then each word of the sen-
tence is sampled from the 10-gram distribution es-
timated with the SOUL model.
The convergence of this sampling strategy can
be evaluated by monitoring the perplexity evolu-
tion vs. the number of sentences that are gener-
ated. Figure 2 depicts this evolution by measuring
perplexity on the nt08 set with a step size of 400M
sampled sentences. The baseline BOLM (std) is
estimated on all the available training data that
consist of approximately 300M of running words.
We can observe that the perplexity of the BOLM
estimated on sampled texts (generated texts) de-
creases when the number of sample sentences in-
creases, and tends to reach slowly the perplex-
ity of the baseline BOLM. Moreover, when both
BOLMs are interpolated, an even lower perplex-
ity is obtained, which further decreases with the
amount of sampled training texts.
6.2 Translation results
Experiments are run for translation into German,
which lacks a GigaWord corpus. An artificial cor-
pus containing 3 billions of running words is first
generated as described in Section 6.1. This corpus
is used to estimate a BOLM with standard settings,
that is then used for decoding, thereby approxi-
mating the use of a NNLM during the first pass.
Results reported in Table 4 show that adding gen-
erated texts improves the BLEU scores even when
the SOUL model is added in a rescoring step. Also
note that using the LM trained on the sampled cor-
pus yields the same BLEU score that using the
standard LM.
66
 190 200 210 220 230 240 250 260
 270 280
 2  4  6  8  10  12ppx times 400M sampled sentences
artificial textsartificial texts+stdstd
Figure 2: Perplexity measured on nt08 with the
baseline LM (std), with the LM estimated on the
sampled texts (generated texts), and with the inter-
polation of both.
Therefore, to translate from English to German,
the submitted system includes three BOLMs: one
trained on all the monolingual data, one on artifi-
cial texts and a third one that uses the freely avail-
able deWack corpus3 (1.7 billion words).
target LM BLEU
dev nt09 test nt10
base 15.3 16.5
+genText 15.5 16.8
+SOUL 16.4 17.6
+genText+SOUL 16.5 17.8
Table 4: Impact of the use of sampled texts.
7 Different tunings for different original
languages
As shown by Lembersky et al (2012), the original
language of a text can have a significant impact on
translation performance. In this section, this effect
is assessed on the French to English translation
task. Training one SMT system per original lan-
guage is impractical, since the required informa-
tion is not available for most of parallel corpora.
However, metadata provided by the WMT evalua-
tion allows us to split the development and test sets
according to the original language of the text. To
ensure a sufficient amount of texts for each con-
dition, we used the concatenation of newstest cor-
pora for the years 2008, 2009, 2011, and 2012,
leaving nt10 for testing purposes.
Five different development sets have been cre-
ated to tune five different systems. Experimental
results are reported in Table 7 and show a drastic
3http://wacky.sslmit.unibo.it/doku.php
baseline adapted
original language tuning
cz 22.31 23.83
en 36.41 39.21
fr 31.61 32.41
de 18.46 18.49
es 30.17 29.34
all 29.43 30.12
Table 5: BLEU scores for the French-to-English
translation task measured on nt10 with systems
tuned on development sets selected according to
their original language (adapted tuning).
improvement in terms of BLEU score when trans-
lating back to the original English and a significant
increase for original text in Czech and French. In
this year?s evaluation, Russian was introduced as
a new language, so for sentences originally in this
language, the baseline system was used. This sys-
tem is used as our primary submission to the eval-
uation, with additional SOUL rescoring step.
8 Conclusion
In this paper, we have described our submis-
sions to the translation task of WMT?13 for
the French-English, German-English and Spanish-
English language pairs. Similarly to last year?s
systems, our main submissions use n-code, and
continuous space models are introduced in a post-
processing step, both for translation and target lan-
guage modeling. To translate from English to
German, we showed a slight improvement with
a tighter integration of the continuous space lan-
guage model using a text sampling strategy. Ex-
periments with pre-ordering were disappointing,
and the reasons for this failure need to be better
understood. We also explored the impact of using
different tuning sets according to the original lan-
guage of the text to be translated. Even though the
gain vanishes when adding the SOUL model in a
post-processing step, it should be noted that due to
time limitation this second step was not tuned ac-
cordingly to the original language. We therefore
plan to assess the impact of using different tuning
sets on the post-processing step.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
67
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard Un iversity.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Marin?o.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, and
Kenneth Church. 2013. Approximate inference: A
sampling based modeling technique to capture com-
plex dependencies in a language model. Speech
Communication, 55(1):162 ? 177.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012b. Measuring the influence of long range de-
pendencies with neural network language models.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages 1?
10, Montre?al, Canada.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012c. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012. Language models for machine trans-
lation: Original vs. translated texts. Comput. Lin-
guist., 38(4):799?825, December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Proceed-
ings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, AMTA
?02, pages 135?144, Tiburon, CA, USA. Springer-
Verlag.
68
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02:
Proc. of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318. Associ-
ation for Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of Interna-
tional Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ?11, pages
1017?1024, New York, NY, USA, June. ACM.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
69
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168?177,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Named Entity Pre-Annotation
for Out-of-Domain Human Annotation
Sophie Rosset?, Cyril Grouin?, Thomas Lavergne?,? , Mohamed Ben Jannet?,?,?,?
Je?re?my Leixa, Olivier Galibert? , Pierre Zweigenbaum?.
?LIMSI?CNRS ?Universite? Paris-Sud ?LNE
?LPP, Universite? Sorbonne Nouvelle ELDA
{rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr
leixa@elda.org, olivier.galibert@lne.fr
Abstract
Automatic pre-annotation is often used to
improve human annotation speed and ac-
curacy. We address here out-of-domain
named entity annotation, and examine
whether automatic pre-annotation is still
beneficial in this setting. Our study de-
sign includes two different corpora, three
pre-annotation schemes linked to two an-
notation levels, both expert and novice an-
notators, a questionnaire-based subjective
assessment and a corpus-based quantita-
tive assessment. We observe that pre-
annotation helps in all cases, both for
speed and for accuracy, and that the sub-
jective assessment of the annotators does
not always match the actual benefits mea-
sured in the annotation outcome.
1 Introduction
Human corpus annotation is a difficult, time-
consuming, and hence costly process. This mo-
tivates research into methods which reduce this
cost (Leech, 1997). One such method consists of
automatically pre-annotating the corpus (Marcus
et al, 1993; Dandapat et al, 2009) using an ex-
isting system, e.g., a POS tagger, syntactic parser,
named entity recognizer, according to the task for
which the annotations aim to provide a gold stan-
dard. The pre-annotations are then corrected by
the human annotators. The underlying hypothe-
sis is that this should reduce annotation time while
possibly at the same time increasing annotation
completeness and consistency.
We study here corpus pre-annotation in a spe-
cific setting, out-of-domain named entity annota-
tion, in which we examine specific questions that
we present below. We produced corpora and an-
notation guidelines for named entities which are
both hierarchical and compositional (Grouin et al,
2011),1 and which we used in contrastive stud-
ies of news texts in French (Rosset et al, 2012).
We want to rely on the same named entity def-
initions for studies on two types of data we did
not cover: parliament debates (Europarl corpus)
and regional, contemporary written news (L?Est
Re?publicain), both in French. To help the annota-
tion process we could reuse our system (Dinarelli
and Rosset, 2011), but needed first to examine
whether a system trained on one type of text (our
first Broadcast News data) could be used to pro-
duce a useful pre-annotation for different types of
text (our two corpora).
We therefore set up the present study in which
we aim to answer the following questions linked
to this point and to related annotation issues:
? can a system trained on data from one spe-
cific domain be useful on data from another
domain in a pre-annotation task?
? does this pre-annotation help human annota-
tors or bias them?
? what importance can we give to the annota-
tors? subjective assessment of the usefulness
of the pre-annotation?
? can we observe differences in the use of pre-
annotation depending on the level of exper-
tise of human annotators?
Moreover, as the aforementioned annotation
scheme is based on two annotation levels (entities
and components), we want to answer these ques-
tions taking into account these two levels.
We first examine related work on pre-annotation
(Section 2), then present our corpora and annota-
tion task (Section 3). We describe and discuss ex-
periments in Section 4, and make subjective and
1Corpora, guidelines and tools are available through
ELRA under references ELRA-S0349 and ELRA-W0073.
168
quantitative observations in Sections 5 and 6. Fi-
nally, we conclude and present some perspectives
in Section 7.
2 Related Work
Facilitating human annotations has been the topic
of a large amount of research. Two different
approaches can be distinguished: active learn-
ing (Ringger et al, 2007; Settles et al, 2008) and
pre-annotation (Marcus et al, 1993; Dandapat et
al., 2009). Our work falls into the latter type.
Pre-annotation can be used in several ways. The
first is to provide annotations to be corrected by
human annotators (Fort and Sagot, 2010). A vari-
ant consists of merging multiple automatic anno-
tations before having them corrected by human
curators to produce a gold-standard (Rebholz-
Schuhmann et al, 2011). The second type con-
sists of providing clues to help human annotators
perform the annotation task (Mihaila et al, 2013).
This work addresses the first type, a single-
system pre-annotation with human correction. An
objective is to examine whether a system trained
on one type of text can be useful to pre-annotate
texts of a different type. Most previous studies
have been performed on well-behaved tasks such
as part-of-speech tagging on in-domain data, i.e.,
the model used for pre-annotating the target data
had been trained on similar data. For instance, Fort
and Sagot (2010) provide a precise evaluation of
the usefulness of pre-annotation and compare the
impact of different quality levels in POS taggers
on the Penn TreeBank corpus. They first trained
different models on the training part of the cor-
pus and applied them to the test corpus. The pre-
annotated test corpus was then corrected by hu-
mans. They reported gains in accuracy and inter-
annotator agreement. The study focused on the
minimal quality (accuracy threshold) of automatic
annotation that would prove useful for human an-
notation. They reported a gain for human annota-
tion when accuracy ranged from 66.5% to 81.6%.
On the contrary, for a semantic-frame annotation
task, Rehbein et al (2009) observed no significant
gain in quality and speed of annotation even when
using a state-of-the-art system.
Generally speaking, annotators find the pre-
annotation stage useful (Rehbein et al, 2009;
South et al, 2011; Huang et al, 2011). Anno-
tation managers consider that a bias may occur
depending on how much human annotators trust
the pre-annotation (Rehbein et al, 2009; Fort and
Sagot, 2010; South et al, 2011). In their frame-
semantic argument structure annotation, Rehbein
et al (2009) addressed a specific question consid-
ering a two-level annotation scheme: is the pre-
annotation of frame assignment (low-level anno-
tation) useful for annotating semantic roles (high-
level annotation)? Although for the low-level an-
notation task they observed a significant difference
in quality of final annotation, for the high-level
task they found no difference.
Most of these studies used a pre-annotation sys-
tem trained on the same kind of data as those
which were to be annotated manually. Neverthe-
less some system-oriented studies have focused on
the results obtained by systems trained on one type
of corpus and applied to another type of corpus,
e.g., for a Latin POS tagger (Poudat and Longre?e,
2009; Skj?rholt, 2011) or for a CoNLL named en-
tity tagger for German (Faruqui and Pado?, 2010)
for which the authors noticed noticed a reduc-
tion of the F-measure when going from in-domain
(newswire data, F=0.782 for their best system) to
out-of-domain (Europarl data, F=0.656).
One of our objectives is then to examine
whether a system trained on one type of text can
be useful to pre-annotate texts of a different type.
We set up experiments to study precisely the pos-
sible induced bias and whether the level of experi-
ence of the annotators would make a difference in
such a context. In this study, we used two different
kinds of corpora, which were both different from
the corpus used to train the pre-annotation system.
3 Task and corpus description
3.1 Task
In this work, we used the structured named entity
definition we proposed in a previous study (Grouin
et al, 2011): entities are both hierarchical (types
have subtypes) and compositional (types and com-
ponents are included in entities) as in Figure 1.
func.coll
org.ent
name
BEIde la
kind
analystes financiersles
Figure 1: Multi-level annotation of entity sub-
types (red tags) and components (blue tags): the
financial analysts of the EBI
169
This taxonomy of entity types is composed of
7 types (person, location, organization, amount,
time, production and function) and 32 sub-types
(individual person pers.ind vs. group of persons
pers.coll; administrative organization org.adm vs.
services org.ent; etc.). Types and subtypes consti-
tute the first level of annotation.
Within these categories, components are
second-level elements (kind, name, first.name,
etc.), and can never be used outside the scope of a
type or subtype element.
3.2 Corpora
Two French corpora were sampled from larger
ones:
Europarl: Prepared speech (Parliament
Debates?Europarl): 15,306 word extract;
Press: Local, contemporary written news (L?Est
Re?publicain): 11,146 word extract.
These corpora were automatically annotated us-
ing the system described in (Dinarelli and Rosset,
2011). This system relies on a Conditional Ran-
dom Field (CRF) model for the detection of com-
ponents and on a probabilistic context-free gram-
mar (PCFG) model for types and sub-types. These
models have been trained on Broadcast News data.
This system achieved a Slot Error Rate (Makhoul
et al, 1999) of 37.0% on Broadcast conversation
and 29.7% on Broadcast news, and ranked first in
the Quaero evaluation campaign (Galibert et al,
2011).
4 Experiments
In this section we present the protocol we designed
to study the usefulness of pre-annotation under
different conditions, and its overall results.
4.1 Protocol
We defined the following protocol, similar to the
one used in Rehbein et al (2009).
Corpora. Four versions of our two corpora were
prepared: (i) raw text, (ii) pre-annotation of
types, (iii) pre-annotation of components, and
(iv) full pre-annotation of both types and compo-
nents. Each of these four versions was split into
four quarters.
Annotators. Eight human annotators were in-
volved in this task. Among them, four are con-
sidered as expert annotators (they annotated cor-
pora in the previous years) while the four re-
maining ones are novice annotators (this was the
first time they annotated such corpora; they were
given training sessions before starting actual anno-
tation). We defined four pairs of annotators, where
each pair was composed of an expert and a novice
annotator.
Quarter allocation. We allocated each corpus
quarter in such a way that each pair of annotators
processed, in each corpus, material from each one
of the four pre-annotated versions (see Table 3).
The same allocation was made in both corpora.
4.2 Results
For each corpus part, a reference was built based
on a majority vote by confronting all annotations.
The resulting reference corpus is presented in Ta-
ble 1.
Corpus # comp. # types # entities # words
P
re
ss
Q1 481 310 791 3047
Q2 367 246 673 2628
Q3 495 327 822 2971
Q4 413 282 695 2600
E
ur
op
ar
l Q1 362 259 621 3926
Q2 309 221 530 3809
Q3 378 247 625 3604
Q4 413 299 712 3967
Table 1: General description of the reference an-
notations: number of components, types, entities
(the sum of components and types), and words
Table 2 presents the performance of the au-
tomatic pre-annotation system against the refer-
ence corpus. We used the well known F-measure
and in addition the Slot Error Rate as it allows
to weight different error classes (deletions, inser-
tions, type or frontier errors). Fort and Sagot
(2010) reported a gain in human annotation when
pre-annotation accuracy ranged from 66.5% to
81.6%. Given their results we can hope for a gain
in both accuracy and annotation time when using
pre-annotation.
Table 3 presents all results obtained by each an-
notators given each pre-annotation condition (raw,
components, types and full) in terms of precision,
recall and F-measure.
170
Corpus #
Raw text Components Types Full
R P F R P F R P F R P F
Press
Q1
0.874 0.777 0.823 0.876 0.741 0.803 0.824 0.870 0.846 0.852 0.800 0.825
0.810 0.766 0.788 0.815 0.777 0.796 0.645 0.724 0.683 0.844 0.785 0.813
Q2
0.765 0.796 0.780 0.870 0.773 0.819 0.822 0.801 0.812 0.917 0.773 0.839
0.558 0.654 0.602 0.826 0.775 0.800 0.815 0.777 0.795 0.816 0.752 0.783
Q3
0.835 0.715 0.771 0.888 0.809 0.847 0.884 0.796 0.837 0.887 0.859 0.873
0.792 0.689 0.736 0.904 0.780 0.837 0.876 0.771 0.820 0.780 0.827 0.803
Q4
0.802 0.757 0.779 0.845 0.876 0.860 0.900 0.702 0.789 0.914 0.840 0.876
0.794 0.727 0.759 0.696 0.715 0.705 0.812 0.701 0.752 0.802 0.757 0.779
Europarl
Q1
0.809 0.728 0.766 0.800 0.568 0.665 0.776 0.862 0.817 0.754 0.720 0.736
0.754 0.720 0.736 0.720 0.609 0.660 0.687 0.607 0.644 0.736 0.638 0.683
Q2
0.776 0.792 0.784 0.782 0.617 0.690 0.797 0.645 0.713 0.821 0.526 0.641
0.563 0.498 0.529 0.802 0.619 0.699 0.698 0.553 0.617 0.769 0.566 0.652
Q3
0.747 0.459 0.569 0.749 0.624 0.681 0.805 0.800 0.803 0.735 0.744 0.739
0.732 0.598 0.658 0.736 0.717 0.726 0.822 0.738 0.777 0.808 0.734 0.769
Q4
0.742 0.624 0.678 0.874 0.760 0.813 0.732 0.480 0.580 0.743 0.608 0.669
0.721 0.566 0.634 0.695 0.652 0.672 0.707 0.600 0.649 0.738 0.603 0.664
Table 3: Overall recall, precision and F-measure for each pair of annotators (blue: pair #1, ocre: pair
#2, green: pair #3, white: pair #4) on each corpus quarter (Q1, Q2, Q3, Q4), depending on the kind of
pre-annotation (raw text, only components, only types, full pre-annotation). Expert annotator is on the
upper line of each quarter, novice annotator is on the lower line. Boldface indicates the best F-measure
for each novice and expert annotator among all pre-annotation tasks in a given corpus quarter
Corpus
Components Types Full
F SER F SER F SER
P
re
ss
Q1 72.4 37.9 63.5 46.3 68.9 41.0
Q2 77.2 32.2 66.8 43.5 73.1 36.6
Q3 76.1 34.1 68.3 41.7 73.1 36.9
Q4 76.1 33.3 63.3 45.7 71.0 38.2
E
ur
op
ar
l Q1 61.9 49.9 57.5 55.4 60.1 52.2
Q2 61.2 51.3 54.6 54.3 58.5 52.5
Q3 61.6 50.1 53.3 55.7 58.2 52.2
Q4 57.1 57.0 48.1 59.7 53.3 58.1
Broad. 88.3 29.1 73.1 39.1 73.2 33.1
Table 2: F-measure and Slot Error Rate achieved
by the automatic system on each kind of annota-
tion and on in-domain broadcast data
We also computed inter-annotator agreement
(IAA) for each corpus considering two groups of
annotators, experts and novices. We consider that
the inter-annotator agreement is somewhere be-
tween the F-measure and the standard IAA con-
sidering as markables all the units annotated by at
least one of the annotators (Grouin et al, 2011).
We computed Scott?s Pi (Scott, 1955), and Co-
hen?s Kappa (Cohen, 1960). The former considers
one model for all annotators while the latter con-
siders one model per annotator. In our case, these
two values are almost the same, which means that
the proportions and kinds of annotations are very
similar across experts and novices. Figure 2 shows
the IAA (Cohen?s Kappa and F-measure) obtained
on the two corpora given the four pre-annotation
conditions (no pre-annotation, components, types,
and full pre-annotation). As we can see, IAA is
systematically higher for the Press corpus than
for the Europarl corpus, which can be linked
to the higher performance of the automatic pre-
annotation system on this corpus. We also can see
that pre-annotation always improves agreement
and that full pre-annotation yields the best result.
We observe that, as expected, pre-annotation leads
human annotators to obtain higher consistency.
5 Subjective assessment
An important piece of information in any anno-
tation campaign is the feelings of the annotators
about the task. This can give interesting clues
about the expected quality of their work and on the
usefulness of the pre-annotation step. We asked
the annotators a few questions concerning sev-
eral features of this project, such as the annotation
171
 0.5
 0.6
 0.7
 0.8
 0.9
 1
raw comp types full
Press: Cohen's kappaPress: F-measureEuroparl: Cohen's kappaEuroparl: F-measure
Figure 2: Cohen?s Kappa (red and blue) and F-
measure (green and pink) measuring agreement of
experts and novices on Press and Europarl corpora
in four pre-annotation conditions. Each measure
compares the concatenated annotations of the four
experts with the four novices.
manual, or how they assessed the benefits of pre-
annotation in the different corpora (Section 5.1).
Another important point is the experience of the
annotators, which we also examine in the light of
theirs answers to the questionnaire (Section 5.2).
5.1 Questionnaire
The questionnaire submitted to the annotators con-
tained 4 questions, dealing with their feedback on
the annotation process:
1. According to you, which level of pre-
annotation has been the most helpful during
the annotation process? Types, components,
or both?
2. To what extent would you say that pre-
annotation helped you in terms of precision
and speed? Did it produce many errors you
had to correct?
3. If you had to choose between the Europarl
corpus and the Press corpus, could you say
that one has been easier to annotate than the
other?
4. Concerning the annotation manual, are there
topics that you would like to change, or cor-
rect? In the same way, which named entities
caused you the most difficulties to deal with?
All 8 annotators answered these questions. We
summarize below what we found in their answers.
5.1.1 Level of pre-annotation
Most of the annotators preferred the corpora that
were pre-annotated with types only. The reason,
for the most part, is that a pre-annotation of types
allows the annotator to work faster on their files,
because guessing the components from the types
is easier than guessing types from components.2
Indeed, the different types of entities defined in
the manual always imply the same components,
be they specific (to one entity type) or transverse
(common to several entity types). On the contrary,
a transverse component, such as <kind>, can be
part of any type of named entity. The other rea-
son for this choice of pre-annotation concerns the
readability brought to the corpora. An annotation
with types only is easier to read than an annotation
with components, and less exhausting after many
hours of work on the texts.
5.1.2 Gain in precision and speed
What motivated the answers to the second ques-
tion mainly concerns the accuracy of the different
pre-annotation methods. While all of them pre-
sented errors that needed to be corrected, the pre-
annotation of types was the one that they felt pre-
sented the smaller number of errors. Thus, annota-
tors spent less time reviewing the corpora in search
of errors, compared to the other pre-annotated cor-
pora (with components, and with both types and
components), where more errors had to be spot-
ted and corrected. This search for incorrect pre-
annotations impacted the time spent on each cor-
pus. Indeed, most annotators declared that pre-
annotation with types was quicker to deal with
than other pre-annotation schemes.
5.1.3 Corpus differences
About one half of the annotators agreed that the
Europarl corpus had been more difficult to anno-
tate. Despite obvious differences in register, sen-
tence structure and vocabulary between the two
corpora, Europarl seemed more redundant and
complex than the other corpus. For instance, one
of the annotators declared:
The Europarl corpus is more difficult
to annotate in the sense that the exist-
ing types and components do not al-
ways match the realities found in the
corpus, either because their definitions
2This feeling is supported by results about ambiguity pre-
sented in Fort et al (2012).
172
cannot apply exactly, or because the re-
quired types and components are miss-
ing (mainly for frequencies: ?five times
per year?).
The other half of the annotators did not feel any
specific difficulties in annotating one corpus or the
other. According to them, both corpora are the
same in terms of register and sentence structure.
5.1.4 Improvements in guidelines
All of the annotators were unanimous in think-
ing that two points need to be modified in the
manual. First of all, the distinction between the
<org.adm> and <org.ent> subtypes is too diffi-
cult to apprehend, above all in the Europarl corpus
where these entities are too ambiguous to be anno-
tated correctly. Secondly, the distinction between
the <pers> and <func> types has also been diffi-
cult to deal with. The other remarks about poten-
tial changes mainly concerned the introduction of
explicit rules for frequencies, which are recurrent
in the Europarl corpus.
5.2 Experience
As mentioned earlier in Section 4.1, we will now
see if the differences in experience between an-
notators impacted their difficulty in annotating the
corpora. First of all, when we look at the answers
given to question 3, we notice that both novice and
expert annotators consider the Europarl corpus the
most difficult to annotate. Most of their answers
deal with the redundancy and the formal register
of the data. Moreover, as everyone answered in
question 4, both <func> and <org> entities have
to be modified to be easier to understand and to
use. This unanimous opinion about what needs
to be reviewed in the manual allows us to think
that the annotators? level of experience has a low
impact on their apprehension of the corpora, both
Europarl and Press. To confirm this, we can look
at the answers given to questions 1 and 2, as indi-
cated in the previous paragraph. As has been ex-
plained, every annotator correctly pointed at the
many errors found in pre-annotation, regardless
of their experience. Besides, the assessment of
the benefits of pre-annotation is the same for al-
most everyone, regardless of their experience too:
both novice and expert annotators agree that pre-
annotation with type adds efficiency and speed to
annotation.
To conclude, according to our observations
based on the questionnaire, we cannot assert that
there has been a difference between novice and ex-
pert annotators. Both groups agreed on the same
difficulties, pointed at the same errors, and crit-
icized the same entities, saying that their defini-
tions needed to be clarified.
6 Quantitative observations
In this section we provide results of quantitative
observations in order to support, or not, the anno-
tators? subjective assessment.
6.1 Corpus statistics
The annotators reported different feelings depend-
ing on the corpora. Some of them reported that
the Europarl corpus was more difficult to annotate,
with more complex sentence structures, or usage
of fewer proper nouns.
To explore these differences, we computed
some statistics over the two original, un-annotated
corpora (which are much larger than the samples
annotated in this experiment) as well as over the
original broadcast news corpus used to train the
pre-annotation system. Each of these corpora con-
tains several million words.
Table 4 reports simple statistics about sentences
in the three corpora. Based on these statistics,
while the Europarl (Euro) corpus is very similar to
the original Broadcast News (BN), the Press cor-
pus shows differences: sentences are 20% shorter,
with fewer but larger chunks, confirming the im-
pression of simpler, less convoluted sentences.
BN Press Euro
Mean sentence length 30.2 23.9 29.7
Mean chunk count 10.9 6.7 10.4
Mean chunk length 2.7 3.6 2.8
Table 4: Sentence summary of the three corpora
Looking more closely at the contents of these
sentences, Figure 3 summarizes the proportions of
grammatical word classes. The sentiment of ex-
tensive naming of entities in the Press corpus is
confirmed by the four times higher rate of proper
nouns. On the other hand, entities are more often
referred to using nouns with an optional adjective
in the Europarl corpus, leading to a more frequent
usage of the latter.
173
Figure 3: Frequency of word classes in the three
corpora (BN = Broadcast News, Est = Press, Euro
= Europarl). TOOL = grammatical words, PCT/NB
= punctuation and numbers, ADJ/ADV = adjectives
and adverbs, NAM = proper name, NOM = noun,
VER = verb.
6.2 Influence of pre-annotation on the
behaviour of annotators
As already mentioned, it is often reported that a
bias may occur depending on human confidence
in the pre-annotation (Fort and Sagot, 2010; Re-
hbein et al, 2009; South et al, 2011). An im-
portant unknown is always the influence of pre-
annotation on the behaviour of annotators, and at
which point pre-annotation induces more errors
than it helps. This may obviously depend on pre-
annotation quality. Table 5 summarizes the er-
ror rates of the automatic annotator in the stud-
ied data (Press + Europarl) and in comparison to
in-domain data. Insertions (Ins) are extra anno-
tations, deletions (Del) missing annotations, and
substitutions (Subs) are annotations that are incor-
rect in type, boundaries, or both. We can see that
Domain Pre-annotation Ins Del Subs
Components 4.4% 33.6% 7.8%
Out Types 7.0% 36.2% 12.7%
Full 5.5% 34.6% 9.7%
In Full 3.7% 23.4% 10.6%
Table 5: Pre-annotation errors and comparison
with in-domain (Broadcast News) data
going out-of-domain increased deletions, proba-
bly through a lack of knowledge of domain vo-
cabulary. But it did not influence the other error
rates significantly. It is also noticeable that dele-
tion is the type of error most produced by the sys-
tem, with every third entity missed. Automatic,
full pre-annotation of Press + Europarl obtains a
precision of 0.79 and a recall of 0.56.
Human annotator performance can then be mea-
sured over the same three error types (Table 6). We
Pre-annotation Ins Del Subs
Raw 8.9% 18.9% 12.8%
Components 5.9% 16.7% 11.3%
Types 7.1% 16.5% 12.0%
Full 7.1% 16.5% 10.1%
Table 6: Mean human annotation error levels for
each pre-annotation scheme
can see that annotation quality was systematically
improved by pre-annotation, with the best global
result obtained by full pre-annotation. In addition
there was no increase in deletions (had the human
stopped looking at the unannotated text) or inser-
tions (had the human always trusted the system) as
might have been feared. This may be a side effect
of the high deletion rate, making it obvious to the
human that the system was missing things. In any
case, the annotation was clearly beneficial in our
experiment with no ill effects seen in error rates
compared to the gold standard.
6.3 Is pre-annotation useful and to whom?
All annotators asserted that pre-annotation is use-
ful, specifically with types. In this section, we pro-
vide observations concerning variations in annota-
tion both in terms of accuracy (F-measure is used)
and duration.
Raw Comp. Types Full
Experts 0.748 0.786 0.778 0.791
Novices 0.682 0.737 0.721 0.742
Table 7: Mean F-measure of experts and novices,
for each pre-annotation scheme
Raw Comp. Types Full
Experts 109.0 52.5 64.0 39.13
Novices 151.7 135.5 117.9 103.88
Table 8: Mean duration (in minutes) of annotation
for experts and novices, for each pre-annotation
scheme (two corpus quarters)
Tables 7 and 8 confirm the hypothesis that auto-
matic pre-annotation helps annotators to annotate
174
faster and to be more efficient. All pre-annotation
levels (components, types and both) seem to be
helpful for both experts and novices. Experts
reached a higher accuracy (F=0.791) and they
were more than twice faster with components or
full pre-annotation. Similarly, novices performed
better when working on a full pre-annotation
(F=0.742) and reached a faster working time
(48mn less than with no pre-annotation). This last
observation contradicts the annotators? reported
experience: the annotators felt more comfortable
and faster with a types-only pre-annotation than
with full pre-annotation (see Section 5.1.2). The
results show that full pre-annotation was the best
choice for both quality and speed.
These results confirm that pre-annotation is use-
ful, even with a moderate level of performance of
the system. Does it help to annotate components
and types equally? To answer this question, we
computed the F-measure of novices and experts
for both components and types separately (see Fig-
ure 4).
 60
 65
 70
 75
 80
 85
 90
raw comp types full
types/novicescomponents/novicestypes/expertscomponents/experts
Figure 4: Mean F-measure on each pre-annotation
level for expert and novice annotators
For experts we can see that all pre-annotation
levels allow them to improve their performance on
both types and components. However for novices,
pre-annotation with types does not improve their
performance in labeling components. We also no-
tice that pre-annotation in both types and compo-
nents allows experts and novices to reach their best
performance for both types and components.
7 Conclusion and Perspectives
Conclusion. In this paper, we studied the inter-
est of a pre-annotation process for a complex an-
notation task with only an out-of-domain annota-
tion system available. We also designed our exper-
iments to check whether the level of experience of
the annotators made a difference in such a context.
The experiment produced in the end a high-quality
gold standard (8-way merge including 2 versions
without pre-annotation) which enabled us to mea-
sure quantitatively the performance of every pre-
annotation scheme.
We noticed that the pre-annotation system
proved relatively precise for such a complex task,
with 79% correct pre-annotations, but with a poor
recall at 56%. This may be a good operating point
for a pre-annotation system to reduce bias though.
In our quantitative experiments we found that
the fullest pre-annotation helped most, both in
terms of quality and annotation speed, even though
the quality of the pre-annotation system varied de-
pending on the annotation layer. This contradicted
the feelings of the annotators who thought that a
type-only pre-annotation was the most efficient.
This shows that in such a setting self-evaluation
cannot be trusted. On the other hand their remarks
about the problems in the annotation guide itself
seemed rather pertinent.
When it comes to experts vs. novices, we noted
that their behaviour and remarks were essentially
identical. Experts were both better and faster
at annotating, but had similar reactions to pre-
annotation and essentially the same feelings.
In conclusion, even with an out-of-domain sys-
tem, a pre-annotation step proves extremely useful
in both annotation speed and annotation quality,
and at least in our setting, with a reasonably pre-
cise system (at the expense of recall) no bias was
detectable. In addition, no matter what the anno-
tators feel, as long as precision is good enough,
the more pre-annotations the better. Pre-filtering
either of our two levels did not help.
Perspectives. Based upon this conclusion, we
plan to use automatic pre-annotation in further an-
notation work, beginning with the present corpora.
As a first use, we plan to propose a few changes
to the annotation principles in the guidelines we
used. To annotate existing corpora with these
changes, automatic pre-annotation will be useful.
As a second piece of future work, we plan to
annotate new corpora with the existing annotation
framework. We also plan to add new types of
named entities (e.g., events) to extend the anno-
tation of existing annotated corpora, using the pre-
annotation process to reduce the overall workload.
175
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program and by the French ANR
VERA project.
References
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex linguistic an-
notation ? no easy way out! A case from Bangla and
Hindi POS labeling tasks. In Proc of 3rd Linguistic
Annotation Workshop (LAW-III), pages 10?18, Sun-
tec, Singapore, August. ACL.
Marco Dinarelli and Sophie Rosset. 2011. Models
cascade for tree-structured named entity detection.
In Proc of IJCNLP, pages 1269?1278, Chiang Mai,
Thailand.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a German named entity recognizer
with semantic generalization. In Proc of Konvens,
Saarbru?cken, Germany.
Kare?n Fort and Beno??t Sagot. 2010. Influence of pre-
annotation on POS-tagged corpus development. In
Proc of 4th Linguistic Annotation Workshop (LAW-
IV), pages 56?63, Uppsala, Sweden. ACL.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In Proceedings of
COLING 2012, pages 895?910, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Olivier Galibert, Sophie Rosset, Cyril Grouin, Pierre
Zweigenbaum, and Ludovic Quintard. 2011. Struc-
tured and extended named entity evaluation in au-
tomatic speech transcriptions. In Proc of IJCNLP,
Chiang Mai, Thailand.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Kare?n Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: From guidelines to evalua-
tion, an overview. In Proc of 5th Linguistic Anno-
tation Workshop (LAW-V), pages 92?100, Portland,
OR. ACL.
Minlie Huang, Aure?lie Ne?ve?ol, and Zhiyong Lu.
2011. Recommending MeSH terms for annotating
biomedical articles. Journal of the American Medi-
cal Informatics Association, 18(5):660?7.
Geoffrey Leech. 1997. Introducing corpus annota-
tion. In Roger Garside, Geoffrey Leech, and Tony
McEnery, editors, Corpus annotation: Linguistic in-
formation from computer text corpora, pages 1?18.
Longman, London.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proc. of DARPA Broad-
cast News Workshop, pages 249?252.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn TreeBank. Com-
putational Linguistics, 19(2):313?330.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Ce?line Poudat and Doninique Longre?e. 2009. Vari-
ations langagie`res et annotation morphosyntaxique
du latin classique. Traitement Automatique des
Langues, 50(2):129?148.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen
Li, Senay Kafkas, Ian Lewin, Ning Kang, Peter Cor-
bett, David Milward, Ekaterina Buyko, Elena Beiss-
wanger, Kerstin Hornbostel, Alexandre Kouznetsov,
Rene? Witte, Jonas B Laurila, Christopher JO Baker,
Cheng-Ju Kuo, Simone Clematide, Fabio Rinaldi,
Richa?rd Farkas, Gyo?rgy Mo?ra, Kazuo Hara, Laura I
Furlong, Michael Rautschka, Mariana Lara Neves,
Alberto Pascual-Montano, Qi Wei, Nigel Collier,
Md Faisal Mahbub Chowdhury, Alberto Lavelli,
Rafael Berlanga, Roser Morante, Vincent Van Asch,
Walter Daelemans, Jose? L Marina, Erik van Mulli-
gen, Jan Kors, and Udo Hahn. 2011. Assessment of
NER solutions against the first and second CALBC
silver standard corpus. J Biomed Semantics, 2.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of partial
automatic pre-labeling for frame-semantic annota-
tion. In Proc of 3rd Linguistic Annotation Workshop
(LAW-III), pages 19?26, Suntec, Singapore. ACL.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proc of Linguistic Annotation Workshop
(LAW), pages 101?108. ACL.
Sophie Rosset, Cyril Grouin, Kare?n Fort, Olivier Gal-
ibert, Juliette Kahn, and Pierre Zweigenbaum. 2012.
Structured named entities in two distinct press cor-
pora: Contemporary broadcast news and old news-
papers. In Proc of 6th Linguistic Annotation Work-
shop (LAW-VI), pages 40?48, Jeju, South Korea.
ACL.
William A Scott. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proc
of the NIPS Workshop on Cost-Sensitive Learning.
176
Arne Skj?rholt. 2011. More, faster: Accelerated cor-
pus annotation with statistical taggers. Journal for
Language Technology and Computational Linguis-
tics, 26(2):151?163.
Brett R South, Shuying Shen, Robyn Barrus, Scott L
DuVall, O?zlem Uzuner, and Charlene Weir. 2011.
Qualitative analysis of workflow modifications used
to generate the reference standard for the 2010
i2b2/VA challenge. In Proc of AMIA.
177
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246?253,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIMSI @ WMT?14 Medical Translation Task
Nicolas P
?
echeux
1,2
, Li Gong
1,2
, Quoc Khanh Do
1,2
, Benjamin Marie
2,3
,
Yulia Ivanishcheva
2,4
, Alexandre Allauzen
1,2
, Thomas Lavergne
1,2
,
Jan Niehues
2
, Aur
?
elien Max
1,2
, Franc?ois Yvon
2
Univ. Paris-Sud
1
, LIMSI-CNRS
2
B.P. 133, 91403 Orsay, France
Lingua et Machina
3
, Centre Cochrane franc?ais
4
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submission
to the first medical translation task at
WMT?14. We report results for English-
French on the subtask of sentence trans-
lation from summaries of medical ar-
ticles. Our main submission uses a
combination of NCODE (n-gram-based)
and MOSES (phrase-based) output and
continuous-space language models used in
a post-processing step for each system.
Other characteristics of our submission in-
clude: the use of sampling for building
MOSES? phrase table; the implementation
of the vector space model proposed by
Chen et al. (2013); adaptation of the POS-
tagger used by NCODE to the medical do-
main; and a report of error analysis based
on the typology of Vilar et al. (2006).
1 Introduction
This paper describes LIMSI?s submission to the
first medical translation task at WMT?14. This
task is characterized by high-quality input text
and the availability of large amounts of training
data from the same domain, yielding unusually
high translation performance. This prompted us
to experiment with two systems exploring differ-
ent translation spaces, the n-gram-based NCODE
(?2.1) and an on-the-fly variant of the phrase-
based MOSES (?2.2), and to later combine their
output. Further attempts at improving translation
quality were made by resorting to continuous lan-
guage model rescoring (?2.4), vector space sub-
corpus adaptation (?2.3), and POS-tagging adap-
tation to the medical domain (?3.3). We also per-
formed a small-scale error analysis of the outputs
of some of our systems (?5).
2 System Overview
2.1 NCODE
NCODE implements the bilingual n-gram ap-
proach to SMT (Casacuberta and Vidal, 2004;
Mari?no et al., 2006; Crego and Mari?no, 2006) that
is closely related to the standard phrase-based ap-
proach (Zens et al., 2002). In this framework, the
translation is divided into two steps. To translate
a source sentence f into a target sentence e, the
source sentence is first reordered according to a
set of rewriting rules so as to reproduce the tar-
get word order. This generates a word lattice con-
taining the most promising source permutations,
which is then translated. Since the translation step
is monotonic, the peculiarity of this approach is to
rely on the n-gram assumption to decompose the
joint probability of a sentence pair in a sequence
of bilingual units called tuples.
The best translation is selected by maximizing
a linear combination of feature functions using the
following inference rule:
e
?
= argmax
e,a
K
?
k=1
?
k
f
k
(f , e,a) (1)
where K feature functions (f
k
) are weighted by
a set of coefficients (?
k
) and a denotes the set of
hidden variables corresponding to the reordering
and segmentation of the source sentence. Along
with the n-gram translation models and target n-
gram language models, 13 conventional features
are combined: 4 lexicon models similar to the ones
used in standard phrase-based systems; 6 lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. Features
are estimated during the training phase. Training
source sentences are first reordered so as to match
246
the target word order by unfolding the word align-
ments (Crego and Mari?no, 2006). Tuples are then
extracted in such a way that a unique segmenta-
tion of the bilingual corpus is achieved (Mari?no et
al., 2006) and n-gram translation models are then
estimated over the training corpus composed of tu-
ple sequences made of surface forms or POS tags.
Reordering rules are automatically learned during
the unfolding procedure and are built using part-
of-speech (POS), rather than surface word forms,
to increase their generalization power (Crego and
Mari?no, 2006).
2.2 On-the-fly System (OTF)
We develop an alternative approach implement-
ing an on-the-fly estimation of the parameter of
a standard phrase-based model as in (Le et al.,
2012b), also adding an inverse translation model.
Given an input source file, it is possible to compute
only those statistics which are required to trans-
late the phrases it contains. As in previous works
on on-the-fly model estimation for SMT (Callison-
Burch et al., 2005; Lopez, 2008), we first build
a suffix array for the source corpus. Only a lim-
ited number of translation examples, selected by
deterministic random sampling, are then used by
traversing the suffix array appropriately. A coher-
ent translation probability (Lopez, 2008) (which
also takes into account examples where translation
extraction failed) is then estimated. As we cannot
compute exactly an inverse translation probability
(because sampling is performed independently for
each source phrase), we resort to the following ap-
proximation:
p(
?
f |e?) = min
(
1.0,
p(e?|
?
f)? freq(
?
f)
freq(e?)
)
(2)
where the freq(?) is the number of occurrences of
the given phrase in the whole corpus, and the nu-
merator p(e?|
?
f)?freq(
?
f) represents the predicted
joint count of
?
f and e?. The other models in this
system are the same as in the default configuration
of MOSES.
2.3 Vector Space Model (VSM)
We used the vector space model (VSM) of Chen
et al. (2013) to perform domain adaptation. In
this approach, each phrase pair (
?
f, e?) present in
the phrase table is represented by a C-dimensional
vector of TF-IDF scores, one for each sub-corpus,
where C represents the number of sub-corpora
(see Table 1). Each component w
c
(
?
f, e?) is a stan-
dard TF-IDF weight of each phrase pair for the
c
th
sub-corpus. TF(
?
f, e?) is the raw joint count of
(
?
f, e?) in the sub-corpus; the IDF(
?
f, e?) is the in-
verse document frequency across all sub-corpora.
A similar C-dimensional representation of the
development set is computed as follows: we first
perform word alignment and phrase pairs extrac-
tion. For each extracted phrase pair, we compute
its TF-IDF vector and finally combine all vectors
to obtain the vector for the develompent set:
w
dev
c
=
J
?
j=0
K
?
k=0
count
dev
(
?
f
j
, e?
k
)w
c
(
?
f
j
, e?
k
) (3)
where J and K are the total numbers of source
and target phrases extracted from the development
data, respectively, and count
dev
(
?
f
j
, e?
k
) is the joint
count of phrase pairs (
?
f
j
, e?
k
) found in the devel-
opment set. The similarity score between each
phrase pair?s vector and the development set vec-
tor is added into the phrase table as a VSM fea-
ture. We also replace the joint count with the
marginal count of the source/target phrase to com-
pute an alternative average representation for the
development set, thus adding two VSM additional
features.
2.4 SOUL
Neural networks, working on top of conventional
n-gram back-off language models, have been in-
troduced in (Bengio et al., 2003; Schwenk et al.,
2006) as a potential means to improve discrete
language models. As for our submitted transla-
tion systems to WMT?12 and WMT?13 (Le et al.,
2012b; Allauzen et al., 2013), we take advantage
of the recent proposal of (Le et al., 2011). Using
a specific neural network architecture, the Struc-
tured OUtput Layer (SOUL), it becomes possible
to estimate n-gram models that use large vocab-
ulary, thereby making the training of large neural
network language models feasible both for target
language models and translation models (Le et al.,
2012a). Moreover, the peculiar parameterization
of continuous models allows us to consider longer
dependencies than the one used by conventional
n-gram models (e.g. n = 10 instead of n = 4).
Additionally, continuous models can also be
easily and efficiently adapted as in (Lavergne et
al., 2011). Starting from a previously trained
SOUL model, only a few more training epochs are
247
Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lm
in-domain
COPPA 454 246 10-12M -3 -15
EMEA 324 189 6-7M 26 -1
PATTR-ABSTRACTS 634 616 20-24M 22 21
PATTR-CLAIMS 888 725 32-36M 6 2
PATTR-TITLES 385 829 3-4M 4 -17
UMLS 2 166 612 8-8M term dictionary -7 -22
WIKIPEDIA 8 421 17-18k short titles -5 -13
out-of-domain
NEWSCOMMENTARY 171 277 4-5M 6 16
EUROPARL 1 982 937 54-60M -7 -33
GIGA 9 625 480 260-319M 27 52
all parallel all 17M 397-475M concatenation 33 69
target-lm
medical-data -146M 69 -
wmt13-data -2 536M 49 -
devel/test
DEVEL 500 10-12k khresmoi-summary
LMTEST 3 000 61-69k see Section 3.4
NEWSTEST12 3 003 73-82k from WMT?12
TEST 1 000 21-26k khresmoi-summary
Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English
and French tokens, respectively. Weights (?
k
) from our best NCODE configuration are indicated for each
sub-corpora?s bilingual word language model (wrd-lm) and POS factor language model (pos-lm).
needed on a new corpus in order to adapt the pa-
rameters to the new domain.
3 Data and Systems Preparation
3.1 Corpora
We use all the available (constrained) medical data
extracted using the scripts provided by the orga-
nizers. This resulted in 7 sub-corpora from the
medical domain with distinctive features. As out-
of-domain data, we reuse the data processed for
WMT?13 (Allauzen et al., 2013).
For pre-processing of medical data, we closely
followed (Allauzen et al., 2013) so as to be able to
directly integrate existing translation and language
models, using in-house text processing tools for
tokenization and detokenization steps (D?echelotte
et al., 2008). All systems are built using a
?true case? scheme, but sentences fully capital-
ized (plentiful especially in PATTR-TITLES) are
previously lowercased. Duplicate sentence pairs
are removed, yielding a sentence reduction up to
70% for EMEA. Table 1 summarizes the data used
along with some statistics after the cleaning and
pre-processing steps.
3.2 Language Models
A medical-domain 4-gram language model is built
by concatenating the target side of the paral-
lel data and all the available monolingual data
1
,
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1996), using the
SRILM (Stolcke, 2002) and KENLM (Heafield,
2011) toolkits. Although more similar to term-to-
term dictionaries, UMLS and WIKIPEDIA proved
better to be included in the language model.
The large out-of-domain language model used for
WMT?13 (Allauzen et al., 2013) is additionaly
used (see Table 1).
3.3 Part-of-Speech Tagging
Medical data exhibit many peculiarities, includ-
ing different syntactic constructions and a specific
vocabulary. As standard POS-taggers are known
not to perform very well for this type of texts, we
use a specific model trained on the Penn Treebank
and on medical data from the MedPost project
(Smith et al., 2004). We use Wapiti (Lavergne
et al., 2010), a state-of-the-art CRF implementa-
tion, with a standard feature set. Adaptation is per-
formed as in (Chelba and Acero, 2004) using the
out-of-domain model as a prior when training the
in-domain model on medical data. On a medical
test set, this adaptation leads to a 8 point reduc-
tion of the error rate. A standard model is used for
WMT?13 data. For the French side, due to the lack
of annotaded data for the medical domain, corpora
are tagged using the TreeTagger (Schmid, 1994).
1
Attempting include one language model per sub-corpora
yielded a significant drop in performance.
248
3.4 Proxy Test Set
For this first edition of a Medical Translation Task,
only a very small development set was made avail-
able (DEVEL in Table 1). This made both system
design and tuning challenging. In fact, with such a
small development set, conventional tuning meth-
ods are known to be very unstable and prone to
overfitting, and it would be suboptimal to select
a configuration based on results on the develop-
ment set only.
2
To circumvent this, we artificially
created our own internal test set by randomly se-
lecting 3 000 sentences out from the 30 000 sen-
tences from PATTR-ABSTRACTS having the low-
est perplexity according to 3-gram language mod-
els trained on both sides of the DEVEL set. This
test set, denoted by LMTEST, is however highly
biaised, especially because of the high redundancy
in PATTR-ABSTRACTS, and should be used with
great care when tuning or comparing systems.
3.5 Systems
NCODE We use NCODE with default settings, 3-
gram bilingual translation models on words and 4-
gram bilingual translation factor models on POS,
for each included corpora (see Table 1) and for the
concatenation of them all.
OTF When using our OTF system, all in-
domain and out-of-domain data are concatenated,
respectively. For both corpora, we use a maxi-
mum random sampling size of 1 000 examples and
a maximum phrase length of 15. However, all
sub-corpora but GIGA
3
are used to compute the
vectors for VSM features. Decoding is done with
MOSES
4
(Koehn et al., 2007).
SOUL Given the computational cost of com-
puting n-gram probabilities with neural network
models, we resort to a reranking approach. In
the following experiments, we use 10-gram SOUL
models to rescore 1 000-best lists. SOUL models
provide five new features: a target language model
score and four translation scores (Le et al., 2012a).
We reused the SOUL models trained for our par-
ticipation to WMT?12 (Le et al., 2012b). More-
over, target language models are adapted by run-
ning 6 more epochs on the new medical data.
2
This issue is traditionally solved in Machine Learning by
folded cross-validation, an approach that would be too pro-
hibitive to use here.
3
The GIGA corpus is actually very varied in content.
4
http://www.statmt.org/moses/
System Combination As NCODE and OTF dif-
fer in many aspects and make different errors, we
use system combination techniques to take advan-
tage of their complementarity. This is done by
reranking the concatenation of the 1 000-best lists
of both systems. For each hypothesis within this
list, we use two global features, corresponding
either to the score computed by the correspond-
ing system or 0 otherwise. We then learn rerank-
ing weights using Minimum Error Rate Training
(MERT) (Och, 2003) on the development set for
this combined list, using only these two features
(SysComb-2). In an alternative configuration, we
use the two systems without the SOUL rescoring,
and add instead the five SOUL scores as features in
the system combination reranking (SysComb-7).
Evaluation Metrics All BLEU scores (Pap-
ineni et al., 2002) are computed using cased
multi-bleu with our internal tokenization. Re-
ported results correspond to the average and stan-
dard deviation across 3 optimization runs to bet-
ter account for the optimizer variance (Clark et al.,
2011).
4 Experiments
4.1 Tuning Optimization Method
MERT is usually used to optimize Equation 1.
However, with up to 42 features when using
SOUL, this method is known to become very sen-
sitive to local minima. Table 2 compares MERT,
a batch variant of the Margin Infused Relaxation
Algorithm (MIRA) (Cherry and Foster, 2012) and
PRO (Hopkins and May, 2011) when tuning an
NCODE system. MIRA slightly outperforms PRO
on DEVEL, but seems prone to overfitting. How-
ever this was not possible to detect before the re-
lease of the test set (TEST), and so we use MIRA
in all our experiments.
DEVEL TEST
MERT 47.0? 0.4 44.1? 0.8
MIRA 47.9? 0.0 44.8? 0.1
PRO 47.1? 0.1 45.1? 0.1
Table 2: Impact of the optimization method during
the tuning process on BLEU score, for a baseline
NCODE system.
249
4.2 Importance of the Data Sources
Table 3 shows that using the out-of-domain data
from WMT?13 yields better scores than only using
the provided medical data only. Moreover, com-
bining both data sources drastically boosts perfor-
mance. Table 1 displays the weights (?
k
) given by
NCODE to the different sub-corpora bilingual lan-
guage models. Three corpora seems particulary
useful: EMEA, PATTR-ABSTRACTS and GIGA.
Note that several models are given a negative
weight, but removing them from the model sur-
prisingly results in a drop of performance.
DEVEL TEST
medical 42.2? 0.1 39.6? 0.1
WMT?13 43.0? 0.1 41.0? 0.0
both 48.3? 0.1 45.4? 0.0
Table 3: BLEU scores obtained by NCODE trained
on medical data only, WMT?13 data only, or both.
4.3 Part-of-Speech Tagging
Using the specialized POS-tagging models for
medical data described in Section 3.3 instead of a
standart POS-tagger, a 0.5 BLEU points increase
is observed. Table 4 suggests that a better POS
tagging quality is mainly beneficial to the reorder-
ing mechanism in NCODE, in contrast with the
POS-POS factor models included as features.
Reordering Factor model DEVEL TEST
std std 47.9? 0.0 44.8? 0.1
std spec 47.9? 0.1 45.0? 0.1
spec std 48.4? 0.1 45.3? 0.1
spec spec 48.3? 0.1 45.4? 0.0
Table 4: BLEU results when using a standard POS
tagging (std) or our medical adapted specialized
method (spec), either for the reordering rule mech-
anism (Reordering) or for the POS-POS bilingual
language models features (Factor model).
4.4 Development and Proxy Test Sets
In Table 5, we assess the importance of domain
adaptation via tuning on the development set used
and investigate the benefits of our internal test set.
Best scores are obtained when using the pro-
vided development set in the tuning process. Us-
DEVEL LMTEST NEWSTEST12 TEST
48.3? 0.1 46.8? 0.1 26.2? 0.1 45.4? 0.0
41.8? 0.2 48.9? 0.1 18.5? 0.1 40.1? 0.1
39.8? 0.1 37.4? 0.2 29.0? 0.1 39.0? 0.3
Table 5: Influence of the choice of the develop-
ment set when using our baseline NCODE system.
Each row corresponds to the choice of a develop-
ment set used in the tuning process, indicated by a
surrounded BLEU score.
Table 6: Contrast of our two main systems and
their combination, when adding SOUL language
(LM) and translation (TM) models. Stars indicate
an adapted LM. BLEU results for the best run on
the development set are reported.
DEVEL TEST
NCODE 48.5 45.2
+ SOUL LM 49.4 45.7
+ SOUL LM
?
49.8 45.9
+ SOUL LM + TM 50.1 47.0
+ SOUL LM
?
+ TM 50.1 47.0
OTF 46.6 42.5
+ VSM 46.9 42.8
+ SOUL LM 48.6 44.0
+ SOUL LM
?
48.4 44.2
+ SOUL LM + TM 49.6 44.8
+ SOUL LM
?
+ TM 49.7 44.9
SysComb-2 50.5 46.6
SysComb-7 50.7 46.5
ing NEWSTEST12 as development set unsurpris-
ingly leads to poor results, as no domain adapta-
tion is carried out. However, using LMTEST does
not result in much better TEST score. We also note
a positive correlation between DEVEL and TEST.
From the first three columns, we decided to use the
DEVEL data set as development set for our sub-
mission, which is a posteriori the right choice.
4.5 NCODE vs. OTF
Table 6 contrasts our different approaches. Prelim-
inary experiments suggest that OTF is a compara-
ble but cheaper alternative to a full MOSES sys-
tem.
5
We find a large difference in performance,
5
A control experiment for a full MOSES system (using a
single phrase table) yielded a BLEU score of 45.9 on DEVEL
and 43.2 on TEST, and took 3 more days to complete.
250
extra missing incorrect unknown
word content filler disamb. form style term order word term all
syscomb 4 13 20 47 62 8 18 21 1 11 205
OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248
Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.
NCODE outperforming OTF by 2.8 BLEU points
on the TEST set. VSM does not yield any signifi-
cant improvement, contrarily to the work of Chen
et al. (2013); it may be the case all individual sub-
corpus are equally good (or bad) at approximating
the stylistic preferences of the TEST set.
4.6 Integrating SOUL
Table 6 shows the substantial impact of adding
SOUL models for both baseline systems. With
only the SOUL LM, improvements on the test set
range from 0.5 BLEU points for NCODE system
to 1.2 points for the OTF system. The adaptation
of SOUL LM with the medical data brings an ad-
ditional improvement of about 0.2 BLEU points.
Adding all SOUL translation models yield an
improvement of 1.8 BLEU points for NCODE and
of 2.4 BLEU points with the OTF system using
VSM models. However, the SOUL adaptation step
has then only a modest impact. In future work, we
plan to also adapt the translation models in order
to increase the benefit of using in-domain data.
4.7 System Combination
Table 6 shows that performing the system combi-
nation allows a gain up to 0.6 BLEU points on the
DEVEL set. However this gain does not transfer to
the TEST set, where instead a drop of 0.5 BLEU
is observed. The system combination using SOUL
scores showed the best result over all of our other
systems on the DEVEL set, so we chose this (a
posteriori sub-obtimal) configuration as our main
system submission.
Our system combination strategy chose for DE-
VEL about 50% hypotheses among those produced
by NCODE and 25% hypotheses from OTF, the
remainder been common to both systems. As ex-
pected, the system combination prefers hypothe-
ses coming from the best system. We can observe
nearly the same distribution for TEST.
5 Error Analysis
The high level of scores for automatic metrics
encouraged us to perform a detailed, small-scale
analysis of our system output, using the error types
proposed by Vilar et al. (2006). A single annota-
tor analyzed the output of our main submission, as
well as our OTF variant. Results are in Table 7.
Looking at the most important types of errors,
assuming the translation hypotheses were to be
used for rapid assimilation of the text content, we
find a moderate number of unknown terms and in-
correctly translated terms. The most frequent er-
ror types include missing fillers, incorrect disam-
biguation, form and order, which all have some
significant impact on automatic metrics. Compar-
ing more specifically the two systems used in this
small-scale study, we find that our combination
(which reused more than 70% of hypotheses from
NCODE) mostly improves over the OTF variant on
the choice of correct word form and word order.
We may attribute this in part to a more efficient
reordering strategy that better exploits POS tags.
6 Conclusion
In this paper, we have demonstrated a successful
approach that makes use of two flexible transla-
tion systems, an n-gram system and an on-the-fly
phrase-based model, in a new medical translation
task, through various approaches to perform do-
main adaptation. When combined with continu-
ous language models, which yield additional gains
of up to 2 BLEU points, moderate to high-quality
translations are obtained, as confirmed by a fine-
grained error analysis. The most challenging part
of the task was undoubtedly the lack on an internal
test to guide system development. Another inter-
esting negative result lies in the absence of success
for our configuration of the vector space model
of Chen et al. (2013) for adaptation. Lastly, a more
careful integration of medical terminology, as pro-
vided by the UMLS, proved necessary.
7 Acknowledgements
We would like to thank Guillaume Wisniewski and
the anonymous reviewers for their helpful com-
ments and suggestions.
251
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-son Le, and Franc?ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Workshkop on
Statistical Machine Translation, pages 62?69, Sofia,
Bulgaria.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(6):1137?1155.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL, Ann Arbor, USA.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Stanley F. Chen and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 310?318, Santa Cruz, NM.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of ACL, Sofia,
Bulgaria.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability. In Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability, pages 176?181, Portland, Oregon.
Josep M. Crego and Jos?e B. Mari?no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jos?e B. Mari?no.
2011. N-code: an open-source bilingual N-gram
SMT toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel D?echelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, H?el`ene May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1352?1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Thomas Lavergne, Hai-Son Le, Alexandre Allauzen,
and Franc?ois Yvon. 2011. LIMSI?s experiments
in domain adaptation for IWSLT11. In Mei-Yuh
Hwang and Sebastian St?uker, editors, Proceedings
of the heigth International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39?48, Montr?eal, Canada, June.
Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
252
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. LIMSI @ WMT12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montr?eal,
Canada.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Proceedings of COLING,
Manchester, UK.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
USA, July. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 723?730, Morristown, NJ, USA. Association
for Computational Linguistics.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
post: a part of speech tagger for biomedical text.
Bioinformatics, 20(14):2320?2321.
A. Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, Colorado,
September.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL, pages 101?104.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In LREC, Genoa, Italy.
Richard Zens, Franz Joseph Och, and Herman Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
KI-2002: Advances in artificial intelligence, volume
2479 of LNAI, pages 18?32. Springer Verlag.
253
LAW VIII - The 8th Linguistic Annotation Workshop, pages 54?58,
Dublin, Ireland, August 23-24 2014.
Optimizing annotation efforts to build reliable annotated corpora
for training statistical models
Cyril Grouin
1
Thomas Lavergne
1,2
Aur
?
elie N
?
ev
?
eol
1
1
LIMSI?CNRS, 91405 Orsay, France
2
Universit?e Paris Sud 11, 91400 Orsay, France
firstname.lastname@limsi.fr
Abstract
Creating high-quality manual annotations on text corpus is time-consuming and often requires the
work of experts. In order to explore methods for optimizing annotation efforts, we study three key
time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and
(iii) careful annotations. Through a series of experiments using a corpus of clinical documents
annotated for personally identifiable information written in French, we address each of these
aspects and draw conclusions on how to make the most of an annotation effort.
1 Introduction
Statistical and Machine Learning methods have become prevalent in Natural Language Processing (NLP)
over the past decades. These methods sucessfully address NLP tasks such as part-of-speech tagging or
named entity recognition by relying on large annotated text corpora. As a result, developping high-
quality annotated corpora representing natural language phenomena that can be processed by statistical
tools has become a major challenge for the scientific community. Several aspects of the annotation task
have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA)
has been used as an indicator of annotation quality. Early work showed that the use of automatic pre-
annotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation
guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006).
Efforts have investigated methods to reduce the human workload while annotating corpora. In par-
ticular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most
benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double an-
notation and found that double annotation could be limited to carefully selected portions of a corpus.
They produced an algorithm that automatically selects portions of a corpus for double annotation. Their
approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and
maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic
pre-annotations was shown to increase annotation consistency and result in producing quality annotation
with a time gain over annotating raw data (Fort and Sagot, 2010; N?ev?eol et al., 2011; Rosset et al., 2013).
With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there
are ethic aspects to consider in addition to technical and monetary cost when using a microworking plat-
form for annotation. While selecting the adequate methods for computing IAA is important (Artstein
and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to
all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate
annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality
manual annotations is time-consuming and often requires the work of experts. The time burden is dis-
tributed between the sheer creation of the annotations, the act of producing multiple annotations for the
same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation of a
consensus. Research has addressed methods for reducing the time burden associated to these annotation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
54
activities (for example, adequate annotation tools such as automatic pre-annotations can reduce the time
burden of annotation creation) with the final goal of producing the highest quality of annotations.
In contrast, our hypothesis in this work is that annotations are being developed for the purpose of train-
ing a machine learning model. Therefore, our experiments consist in training a named entity recognizer
on a training set comprising annotations of varying quality to study the impact of training annotation
quality on model performance. In order to explore methods for optimizing annotation efforts for the de-
velopment of training corpora, we revisit the three key time burdens of the annotation process on textual
corpora: (i) careful annotations, (ii) multiple annotations, and (iii) consensus annotations. Through a
series of experiments using a corpus of French clinical documents annotated for personally identifiable
information (PHI), we address each of these aspects and draw conclusions on how to make the most of
an annotation effort.
2 Material and methods
2.1 Annotated corpus
Experiments were conducted with a corpus of clinical documents in French annotated for 10 categories of
PHI. The distribution of the categories over the corpus varies with some categories being more prevalent
than others. In addition, the performance of entity recognition for each type of PHI also varies (Grouin
and N?ev?eol, 2014). The datasets were split to obtain a training corpus (200 documents) and a test
corpus (100 documents). For all documents in the training corpus, three types of human annotations
are available: annotations performed independently by two human annotators and consensus annotations
obtained after adjudication to resolve conflicts between the two annotators. Inter-annotator agreement
on the training corpus was above 85% F-measure, which is considered high (Artstein and Poesio, 2008).
The distribution of annotations over all PHI categories on both corpora (train/dev) is: address
(188/100), zip code (197/97), date (1025/498), e-mail (119/57), hospital (448/208), identifier (135/76),
last name (1855/855), first name (1568/724), telephone (802/386) and city (450/217).
2.2 Automatic Annotation Methods
We directly applied the MEDINA rule-based de-identification tool (Grouin, 2013) to obtain baseline
automatic annotations. We used the CRF toolkit Wapiti (Lavergne et al., 2010) to train a series of models
on the various sets of annotations available for the training corpus.
Features set For each CRF experiment, we used the following set of features with a l1 regularization:
? Lexical features: unigram and bigram of tokens;
? Morphological features: (i) the token case (all in upper/lower case, combination of both), (ii) the
token is a digit, (iii) the token is a punctuation mark, (iv) the token belongs to a specific list (first
name, last name, city), (v) the token was not identified in a dictionary of inflected forms, (vi) the
token is a trigger word for specific categories (hospital, last name) ;
? Syntactic features: (i) the part-of-speech (POS) tag of the token, as provided by the Tree Tagger
tool (Schmid, 1994), (ii) the syntactic chunk the token belongs to, from a home made chunker
based upon the previouses POS tags;
? External features: (i) we created 320 classes of tokens using Liang?s implementation (Liang, 2005)
of the Brown clustering algorithm (Brown et al., 1992), (ii) the position of the token within the
document (begining, middle, end).
Design of experiments The models were built to assess three annotation time-saving strategies:
1. Careful annotation: (i) AR=based on automatic annotations from the rule-based system,
(ii) AR?H2=intersection of automatic annotations from the rule-based system with annotations
from annotator 2. This model captures a situation where the human annotator would quickly revise
the automatic annotations by removing errors: some annotations would be missing (average recall),
but the annotations present in the set would be correct (very high precision), (iii) ARH2=automatic
annotations from the rule-based system, with replacement of the three most difficult categories by
55
annotations from annotator 2. This model captures a situation where the human annotator would
focus on revising targeted categories, and (iv) ARHC=automatic annotations from the rule-based
system, with replacement of the three most difficult categories by consensus annotations;
2. Double annotation: (i) H1=annotations from annotator 1, (ii) H2=annotations from annotator 2,
(iii) H12=first half of the annotations from annotator 1, second half from annotator 2, and
(iv) H21=first half of the annotations from annotator 2, second half from annotator 1;
3. Consensus annotation: (i) H1?H2=all annotations from annotator 1 and 2 (concatenation without
adjudication), and (ii) HC=consensus annotations (after adjudication between annotator 1 and 2).
3 Results
Table 1 presents an overview of the global performance of each annotation run (H12 and H21 achieved
similar results) across all PHI categories in terms of precision, recall and F
1
-measure (Manning and
Sch?utze, 2000). Table 2 presents the detailed performance of each annotation run for individual PHI
categories in terms of F-measure.
Baseline AR AR?H2 ARH2* ARHC H1* H12 H1?H2 H2 HC
Precision .820 .868 .920 .942 .943 .959 .962 .969 .974 .974
Recall .806 .796 .763 .854 .854 .927 .934 .935 .936 .942
F-measure .813 .830 .834 .896 .896 .943 .948 .951 .955 .958
Table 1: Overall performance for all automatic PHI detection. A star indicates statistically significant
difference in F-measure over the previous model (Wilcoxon test, p<0.05)
Category Baseline AR AR?H2 ARH2 ARHC H1 H12 H1?H2 H2 HC
Address .648 .560 .000 .800 .800 .716 .744 .789 .795 .791
Zip code .950 .958 .947 .964 .958 .974 .984 .974 .984 .990
Date .958 .968 .962 .963 .967 .965 .963 .963 .959 .970
E-mail .937 .927 .927 .927 .927 1.000 1.000 1.000 1.000 1.000
Hospital .201 .248 .039 .856 .868 .789 .809 .856 .861 .867
Identifier .000 .000 .000 .762 .797 .870 .892 .823 .836 .876
Last name .816 .810 .834 .832 .828 .953 .957 .954 .961 .963
First name .849 .858 .900 .901 .902 .960 .956 .961 .965 .960
Telephone 1.000 .980 .978 .983 .980 .987 .994 .999 .999 1.000
City .869 .874 .883 .887 .887 .948 .972 .962 .965 .972
Table 2: Performance per PHI category (F-measure)
4 Discussion
4.1 Model performance
Overall, the task of automatic PHI recognition has been well studied and the rule-based tool provides a
strong baseline with .813 F-measure on the test set. Table 1 shows that there are three different types
of models, in terms of performance: the lower-performing category corresponds to models with no hu-
man input. The next category corresponds to models with some human input, and the higher-performing
models correspond to models with the most human input. This reflects the expectation that model per-
formance increases with training corpus quality. However, it also shows that, within the two categories
that include human input, there is no statistical difference in model performance with respect to the type
of human input. We observed that the model trained on annotations from the H2 human annotator per-
formed better (F=0.955) than the model trained on annotations from the H1 annotator (F=0.943). This
observation reflects the agreement of the annotators with consensus annotations, where H2 had higher
agreement than H1 (Grouin and N?ev?eol, 2014). This is also true at the category level: H2 achieved
56
higher agreement with the consensus compared to H1 on categories ?address? (F=0.985>0.767) and
?hospital? (F=0.947>0.806) but H2 had lower agreement with the consensus on the category ?identifier?
(F=0.840<0.933).
4.2 Error Analysis
The performance of CRF models depends on the size of the training corpus and the level of diversity of
the mentions. Error analysis on our test data shows that a few specific mentions are not tagged in the test
corpus, even though they occur in the training corpus. For example, some hospital names occur in the
clinical narratives either as acronyms or as full forms (e.g. ?GWH? for ?George Washington Hospital?
in transfer patient from GWH). The acronyms are overall much less prevalent than the full forms and
also happen to be difficult to identify for human annotators (depending on the context, a given acronym
could refer to either a medical procedure, a physician or a hospital). We observed that the only hospital
acronym present in the test corpus was not annotated by any of the CRF models. Nevertheless, only five
occurrences of this acronym were found in the training corpus which is not enough for the CRF to learn.
Other errors occur in recognizing sequences of doctors? names that appear without separators in sig-
natures lines at the end of documents (e.g. ?Jane BROWN John DOE Mary SMITH?). In our test set
we observed that models trained on automatic annotations correctly predicted the beginning of such se-
quences and then produced erroneous predictions for the rest of the sequence (models AR, AR?H2,
ARHC and ARH2). In contrast, models built on human annotations produced correct predictions on the
entire sequence (models H1, H12, H1?H2, H2 and HC). Similarly, for last names containing a nobiliary
particle, the models trained on automatic annotations only identified part of the last name as a PHI. We
also observed that spelling errors (e.g. ?Jhn DOE?) only resulted in correct predictions from the mod-
els trained on the human annotations. We did not find cases where the models built on the automatic
annotations performed better than the models built on the human annotations.
4.3 Annotation strategy
Table 1 indicates that for the purpose of training a machine learning entity recognizer, all types of human
input are equivalent. In practice, this means that double annotations or consensus annotations are not
necessary. The high inter-annotator agreement on our dataset may be a contributing factor for this finding.
Indeed, (Esuli et al., 2013) found that with low inter-annotator agreement, models are biased towards
the annotation style of the annotator who produced the training data. Therefore, we believe that inter-
annotator should be established on a small dataset before annotators work independently. Table 2 shows
that using human annotations for selected categories results in strong improvement of the performance
over these categories (?address?, ?hospital? and ?identifier? categories in ARHC and ARH2 vs. AR) with
little impact on the performance of the model on other categories. Therefore, careful human annotations
are not necessarily needed for the entire corpus. Targeting ?hard? categories for human annotations can
be a good time-saving strategy. While the difference between the models using some human input vs.
all human input is statistically significant, the performance gain is lower than between models without
human input and some human input. Using data with partial human input for training statistical models
can cut annotation cost.
5 Conclusion and future work
Herein we have shown that full double annotation of a corpus is not necessary for the purpose of training a
competitive CRF-based model. Our results suggest that a three-step annotation strategy can optimize the
annotation effort: (i) double annotate a small subset of the corpus to ensure human annotators understand
the guidelines; (ii) have annotators work independently on different sections of the corpus to obtain wide
coverage; and (iii) train a machine-learning based model on the human annotations and apply this model
on a new dataset.
In future work, we plan to re-iterate these experiments on a different type of entity recognition task
where inter-annotator agreement may be more difficult to achieve, and may vary more between categories
in order to investigate the influence of inter-annotator-agreement on our conclusions.
57
Acknowledgements
This work was supported by the French National Agency for Research under grant CABeRneT
1
ANR-
13-JS02-0009-01 and by the French National Agency for Medicines and Health Products Safety under
grant Vigi4MED
2
ANSM-2013-S-060.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4):555?596.
Peter F Brown, Vincent J Della Pietra, Peter V de Souza, Jenifer C Lai, and Robert L Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?79.
Dmitriy Dligach and Martha Palmer. 2011. Reducing the need for double annotation. In Proc of Linguistic
Annotation Workshop (LAW), pages 65?73. Association for Computational Linguistics.
Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2013. An enhanced CRFs-based system for informa-
tion extraction from radiology reports. J Biomed Inform, 46(3):425?35, Jun.
Kar?en Fort and Beno??t Sagot. 2010. Influence of pre-annotation on POS-tagged corpus development. In Proc of
Linguistic Annotation Workshop (LAW), pages 56?63. Association for Computational Linguistics.
Kar?en Fort, Gilles Adda, and Kevin Bretonnel Cohen. 2011. Amazon Mechanical Turk: Gold Mine or Coal Mine?
Computational Linguistics, pages 413?420.
Cyril Grouin and Aur?elie N?ev?eol. 2014. De-identification of clinical notes in french: towards a protocol for
reference corpus development. J Biomed Inform.
Cyril Grouin. 2013. Anonymisation de documents cliniques : performances et limites des m?ethodes symboliques
et par apprentissage statistique. Ph.D. thesis, University Pierre et Marie Curie, Paris, France.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for
Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Chiristopher D. Manning and Hinrich Sch?utze. 2000. Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, Massachusetts.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn TreeBank. Computational Linguistics, 19(2):313?330.
Aur?elie N?ev?eol, Rezarta Islamaj Do?gan, and Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed
queries: a study on quality, efficiency, satisfaction. J Biomed Inform, 44(2):310?8.
Sophie Rosset, Cyril Grouin, Thomas Lavergne, Mohamed Ben Jannet, J?er?emy Leixa, Olivier Galibert, and Pierre
Zweigenbaum. 2013. Automatic named entity pre-annotation for out-of-domain human annotation. In Proc of
Linguistic Annotation Workshop (LAW), pages 168?177. Association for Computational Linguistics.
Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur. 2009. How to get the most out of your curation effort. PLoS
Comput Biol, 5(5):e1000391.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc of International Confer-
ence on New Methods in Language.
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proc of the
NIPS Workshop on Cost-Sensitive Learning.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006. New directions in biomedical text annotation:
definitions, guidelines and corpus construction. BMC Bioinformatics, 25(7):356.
1
CABeRneT: Compr?ehension Automatique de Textes Biom?edicaux pour la Recherche Translationnelle
2
Vigi4MED: Vigilance dans les forums sur les M?edicaments
58
