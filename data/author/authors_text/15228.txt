Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1290?1301, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Besting the Quiz Master:
Crowdsourcing Incremental Classification Games
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Brianna Satinoff, He He, and Hal Daume? III
Department of Computer Science
University of Maryland
{bsonrisa, hhe, hal}@cs.umd.edu
Abstract
Cost-sensitive classification, where the features
used in machine learning tasks have a cost, has
been explored as a means of balancing knowl-
edge against the expense of incrementally ob-
taining new features. We introduce a setting
where humans engage in classification with
incrementally revealed features: the collegiate
trivia circuit. By providing the community with
a web-based system to practice, we collected
tens of thousands of implicit word-by-word
ratings of how useful features are for eliciting
correct answers. Observing humans? classifi-
cation process, we improve the performance
of a state-of-the art classifier. We also use the
dataset to evaluate a system to compete in the
incremental classification task through a reduc-
tion of reinforcement learning to classification.
Our system learns when to answer a question,
performing better than baselines and most hu-
man players.
1 Introduction
A typical machine learning task takes as input a set
of features and learns a mapping from features to a
label. In such a setting, the objective is to minimize
the error of the mapping from features to labels. We
call this traditional setting, where all of the features
are consumed, rapacious machine learning.1
This not how humans approach the same task.
They do not exhaustively consider every feature. Af-
ter a certain point, a human has made a decision
and no longer needs additional features. Even in-
defatigable computers cannot always exhaustively
consider every feature. This is because the result
1Earlier drafts called this ?batch? machine learning, which
confused the distinction between batch and online learning. We
gladly adopt ?rapacious? to make this distinction clearer and
to cast traditional machine learning?that always examines all
features?as a resource hungry approach.
is time sensitive, such as in interactive systems, or
because processing time is limited by the sheer quan-
tity of data, as in sifting e-mail for spam (Pujara et
al., 2011). In such settings, often the best solution
is incremental: allow a decision to be made without
seeing all of an instance?s features. We discuss the
incremental classification framework in Section 2.
Our understanding of how humans conduct incre-
mental classification is limited. This is because com-
plicating an already difficult annotation task is often
an unwise tradeoff. Instead, we adapt a real world
setting where humans are already engaging (eagerly)
in incremental classification?trivia games?and de-
velop a cheap, easy method for capturing human
incremental classification judgments.
After qualitatively examining how humans con-
duct incremental classification (Section 3), we show
that knowledge of a human?s incremental classifi-
cation process improves state-of-the-art rapacious
classification (Section 4). Having established that
these data contain an interesting signal, we build
Bayesian models that, when embedded in a Markov
decision process, can engage in effective incremental
classification (Section 5), and develop new hierar-
chical models combining local and thematic content
to better capture the underlying content (Section 7).
Finally, we conclude in Section 8 and discuss exten-
sions to other problem areas.
2 Incremental Classification
In this section, we discuss previous approaches that
explore how much effort or resources a classifier
needs to come to a decision, a problem not as thor-
oughly examined as the question of whether the de-
cision is right or not.2 Incremental classification is
2When have an externally interrupted feature stream, the
setting is called ?any time? (Boddy and Dean, 1989; Horsch and
Poole, 1998). Like ?budgeted? algorithms (Wang et al 2010),
these are distinct but related problems.
1290
not equivalent to missing features, which have been
studied at training time (Cesa-Bianchi et al 2011),
test time (Saar-Tsechansky and Provost, 2007), and
in an online setting (Rostamizadeh et al 2011). In
contrast, incremental classification allows the learner
to decide whether to acquire additional features.
A common paradigm for incremental classification
is to view the problem as a Markov decision process
(MDP) (Zubek and Dietterich, 2002). The incremen-
tal classifier can either request an additional feature
or render a classification decision (Chai et al 2004;
Ji and Carin, 2007; Melville et al 2005), choosing
its actions to minimize a known cost function. Here,
we assume that the environment chooses a feature
in contrast to a learner, as in some active learning
settings (Settles, 2011). In Section 5, we use a MDP
to decide whether additional features need to be pro-
cessed in our application of incremental classification
to a trivia game.
2.1 Trivia as Incremental Classification
A real-life setting where humans classify documents
incrementally is quiz bowl, an academic competition
between schools in English-speaking countries; hun-
dreds of teams compete in dozens of tournaments
each year (Jennings, 2006). Note the distinction be-
tween quiz bowl and Jeopardy, a recent application
area (Ferrucci et al 2010). While Jeopardy also uses
signaling devices, these are only usable after a ques-
tion is completed (interrupting Jeopardy?s questions
would make for bad television). Thus, Jeopardy is
rapacious classification followed by a race to see?
among those who know the answer?who can punch
a button first. Moreover, buzzes before the question?s
end are penalized.
Two teams listen to the same question.3 In this
context, a question is a series of clues (features) re-
ferring to the same entity (for an example question,
see Figure 1). We assume a fixed feature ordering
for a test sequence (i.e., you cannot request specific
features). Teams interrupt the question at any point
by ?buzzing in?; if the answer is correct, the team
gets points and the next question is read. Otherwise,
the team loses points and the other team can answer.
3Called a ?starter? (UK) or ?tossup? (US) in the lingo, as it
often is followed by a ?bonus? given to the team that answers the
starter; here we only concern ourselves with tossups answerable
by both teams.
After losing a race for the Senate, this politician edited the Om-
aha World-Herald. This man resigned 3 from one of his posts
when the President sent a letter to Germany protesting the Lusi-
tania 3 sinking, and 3 he advocated 3 coining 3 silver at a 16
3 to 1 33 rate 3 compared to 3 gold. He was the 3 three-time
Democratic 3 Party 333 nominee for 3 President 3 but 333
lost to McKinley twice 33 and then Taft, although he served as
Secretary of State 33 under Woodrow Wilson, 3 and he later
argued 3 against Clarence Darrow 3 in the Scopes 33 Monkey
Trial. For ten points, name this 3 man who famously declared
that ?we shall not be crucified on a Cross of 3 Gold?. 3
Figure 1: Quiz bowl question on William Jennings Bryan,
a late nineteenth century American politician; obscure
clues are at the beginning while more accessible clues are
at the end. Words (excluding stop words) are shaded based
on the number of times the word triggered a buzz from any
player who answered the question (darker means more
buzzes; buzzes contribute to the shading of the previous
five words). Diamonds (3) indicate buzz positions.
The answers to quiz bowl questions are well-
known entities (e.g., scientific laws, people, battles,
books, characters, etc.), so the answer space is rel-
atively limited; there are no open-ended questions
of the form ?why is the sky blue?? However, there
are no multiple choice questions?as there are in
Who Wants to Be a Millionaire (Lam et al 2003)?
or structural constraints?as there are in crossword
puzzles (Littman et al 2002).
Now that we introduced the concepts of questions,
answers, and buzzes, we pause briefly to define them
more formally and explicitly connect to machine
learning. In the sequel, we will refer to: questions,
sequences of words (tokens) associated with a single
answer; features, inputs used for decisions (derived
from the tokens in a question); labels, a question?s
correct response; answers, the responses (either cor-
rect or incorrect) provided; and buzzes, positions in
a question where users halted the stream of features
and gave an answer.
Quiz bowl is not a typical problem domain for natu-
ral language processing; why should we care about it?
First, it is a real-world instance of incremental classi-
fication that happens hundreds of thousands of times
most weekends. Second, it is a classification problem
intricately intertwined with core computational lin-
guistics problems such as anaphora resolution, online
sentence processing, and semantic priming. Finally,
quiz bowl?s inherent fun makes it easy to acquire
human responses, as we describe in the next section.
1291
Number of Tokens Revealed
Ac
cu
rac
y
0.4
0.6
0.8
1.0
40 60 80 100
Total
500
1000
1500
2000
2500
3000
Figure 2: Users plotted based on accuracy vs. the number
of tokens?on average?the user took to give an answer.
Dot size and colour represent the total number of ques-
tions answered. Users that answered questions later in the
question had higher accuracy. However, there were users
that were able to answer questions relatively early without
sacrificing accuracy.
3 Getting a Buzz through Crowdsourcing
We built a corpus with 37,225 quiz bowl questions
with 25,498 distinct labels from 121 tournaments
written for tournaments between 1999 and 2010. We
created a webapp4 that simulates the experience of
playing quiz bowl. Text is incrementally revealed
(at a pace adjustable by the user) until users press
the space bar to ?buzz?. Users then answer, and the
webapp judges correctness using a string matching
algorithm. Players can override the automatic check
if the system mistakenly judged an answer incorrect.
Answers of previous users are displayed after answer-
ing a question; this enhances the sense of community
and keeps users honest (e.g., it?s okay to say that ?wj
bryan? is an acceptable answer for the label ?william
jennings bryan?, but ?asdf? is not). We did not see
examples of nonsense answers from malicious users;
in contrast, users were stricter than we expected, per-
haps because protesting required effort.
To collect a set of labels with many buzzes, we
focused on the 1186 labels with more than four dis-
tinct questions. Thus, we shuffled the labels into a
canonical order shown to all users (e.g., everyone
saw a question on ?Jonathan Swift? and then a ques-
tion on ?William Jennings Bryan?, but because these
labels have many questions the specific questions
4Play online or download the datasets at http://umiacs.
umd.edu/?jbg/qb.
Figure 3: A screenshot of the webapp used to collect data.
Users see a question revealed one word at a time. They
signal buzzes by clicking on the answer button and input
an answer.
were different for each user). Participants were ea-
ger to answer questions; over 7000 questions were
answered in the first day, and over 43000 questions
were answered in two weeks by 461 users.
To represent a ?buzz?, we define a function b(q, f)
(?b? for buzz) as the number of times that feature
f occurred in question q at most five tokens before
a user correctly buzzed on that question.5 Aggre-
gating buzzes across questions (summing over q)
shows different features useful for eliciting a buzz
(Figure 4(a)). Some features coarsely identify the
type of answer sought, e.g., ?author?, ?opera?, ?city?,
?war?, or ?god?. Other features are relational, con-
necting the answer to other clues, e.g., ?namesake?,
?defeated?, ?husband?, or ?wrote?. The set of buzzes
help narrow which words are important for matching
a question to its answer; for an example, see how
the word cloud for all of the buzzes on ?Wuthering
5This window was chosen qualitatively by examining the
patterns of buzzes; this is person-dependent, based on reading
comprehension, reaction time, and what reveal speed the user
chose. We leave explicitly modeling this for future work.
1292
(a) Buzzes over all Questions (b) Wuthering Heights Question Text (c) Buzzes on Wuthering Heights
Figure 4: Word clouds representing all words that were a part of a buzz (a), the original text appearing in seven questions
on the book ?Wuthering Heights? by Emily Bro?nte (b), and the buzzes of users on those questions (c). The buzzes
reflect what users remember about the work and is more focused than the complete question text.
Heights? (Figure 4(c)) is much more focused than the
word cloud for all of the words from the questions
with that label (Figure 4(b)).
4 Buzzes Reveal Useful Features
If we restrict ourselves to a finite set of labels, the
process of answering questions is a multiclass clas-
sification problem. In this section, we show that in-
formation gleaned from humans making a similar de-
cision can help improve rapacious machine learning
classification. This validates that our crowdsourcing
technique is gathering useful information.
We used a state-of-the-art maximum entropy clas-
sification model, MEGAM (Daume? III, 2004), that
accepts a per-class mean prior for feature weights
and applied MEGAM to the 200 most frequent labels
(11,663 questions, a third of the dataset). The prior
mean of the feature weight is a convenient, simple
way to incorporate human feature utility; apart from
the mean, all default options are used.
Specifying the prior requires us to specify a weight
for each pair of label and feature. The weight com-
bines buzz information (described in Section 3) and
tf-idf (Salton, 1968). The tf-idf value is computed by
treating the training set of questions with the same
label as a single document.
Buzzes and tf-idf information were combined into
the prior ? for label a and feature f as ?a,f =
[
?b(a, f) + ?I [b(a, f) > 0] + ?
]
tf-idf(a, f). (1)
We describe our weight strategies in increasing order
of human knowledge. If ?, ?, and ? are zero, this
is a na??ve zero prior. If ? only is nonzero, this is a
linear transformation of features? tf-idf. If only ?
is nonzero, this is a linear transformation of buzzed
Weighting ? ? ? Error
zero - - - 0.37
tf-idf - - 3.5 0.14
buzz-binary 7.1 - - 0.10
buzz-linear - 1.5 - 0.16
buzz-tier - 1.1 0.1 0.09
Table 1: Classification error of a rapacious classifier able
to draw on human incremental classification. The best
weighting scheme for each dataset is in bold. Missing
parameter values (-) mean that the parameter is fixed to
zero for that weighting scheme.
words? tf-idf weights. If only ? is non-zero, num-
ber of buzzes is now a linear multiplier of the tf-idf
weight (buzz-linear). Finally we allow unbuzzed
words to have a separate linear transformation if both
? and ? are non-zero (buzz-tier).
Grid search (width of 0.1) on development set error
was used to set parameters. Table 1 shows test error
for weighting schemes and demonstrates that adding
human information as a prior improves classification
error, leading to a 36% error reduction over tf-idf
alone. While not directly comparable (this classifier
is rapacious, not incremental, and has a predefined
answer space), the average user had an error rate of
16.7%.
5 Building an Incremental Classifier
In the previous section we improved rapacious classi-
fication using humans? incremental classification. A
more interesting problem is how to compete against
humans in incremental classification. While in the
previous section we used human data for a training
set, here we use human data as an evaluation set.
Doing so requires us to formulate an incremental rep-
resentation of the contents of questions and to learn
a strategy to decide when to buzz.
1293
Because this is the first machine learning algo-
rithm for quiz bowl, we attempt to provide reason-
able rapacious baselines and compare against our new
strategies. We believe that our attempts represent a
reasonable explanation of the problem space, but ad-
ditional improvements could improve performance,
as discussed in Section 8.
A common way to represent state-dependent strate-
gies is via a Markov decision process (MDP). The
most salient component of a MDP is the policy, i.e., a
mapping from the state space to an action. In our con-
text, a state is a sequence of (thus far revealed) tokens,
and the action is whether to buzz or not. To learn a
policy, we use a standard reinforcement learning tech-
nique (Langford and Zadrozny, 2005; Abbeel and
Ng, 2004; Syed et al 2008): given a representation
of the state space, learn a classifier that can map from
a state to an action. This is also a common paradigm
for other incremental tasks, e.g., shift-reduce pars-
ing (Nivre, 2008).
Given examples of the correct answer given a con-
figuration of the state space, we can learn a MDP
without explicitly representing the reward function.
In this section, we define our method of defining
actions and our representation of the state space.
5.1 Action Space
We assume that there are only two possible actions:
buzz now or wait. An alternative would be a more
expressive action space (e.g., an action for every pos-
sible answer). However, this conflates the question
of when to buzz with what to answer. Instead, we call
the distinct component that provides what to answer
the content model. We describe an initial content
model in Section 5.2, below, and improve the models
further in Section 7. For the moment, assume that
a content model maintains a posterior distribution
over labels and when needed can provide its best
guess (e.g., given the features seen, the best answer
is ?William Jennings Bryan?).
Given the action space, we need to specify where
examples of state space and action come from. In
the language of classification, we need to provide
(x, y) pairs to learn a mapping x 7? y. The clas-
sifier attempts to learn that action y is (?buzz?) in
all states where the content model gave a correct re-
sponse given state x. Negative examples (?wait?)
are applied to states where the content model gave
a wrong answer. Every token in our training set cor-
responds to a classification example; both states are
prevalent enough that we do not to explicitly need to
address class imbalance. This resembles approaches
that merge different classifiers (Riedel et al 2011) or
attempt to estimate confidence of models (Blatz et al
2004). However, here we use partial observations.
This is a simplification of the problem and corre-
sponds to a strategy of ?buzz as soon as you know the
answer?, ignoring all other factors. While reasonable,
this is not always optimal. For example, if you know
your opponent is unlikely to answer a question, it is
better to wait until you are more confident. Incorrect
answers might also help your opponent, e.g., by elim-
inating an incorrect answer. Moreover, strategies in a
game setting (rather than a single question) are more
complicated. For example, if a right answer is worth
+10 points and the penalty for an incorrect question
is ?5, then a team leading by 15 points on the last
question should never attempt to answer. Investigat-
ing such gameplay strategies would require a ?roll
out? of game states (Tesauro and Galperin, 1996) to
explore the efficacy of such strategies. While inter-
esting, we leave these issues to future work.
We also investigated learning a policy directly
from users? buzzes directly (Abbeel and Ng, 2004),
but this performed poorly because the content model
is incompatible with the players? abilities and the
high variation in players? ability and styles (compare
Figure 2).
5.2 State Space
Recall that our goal is to learn a classifier that maps
states to actions; above, we defined the action space
(the classifier?s output) but not the state space, the
classifier?s input. The straightforward parameteriza-
tion of the state space would be all of the words that
have been revealed. However, such a feature set is
very sparse.
We use three components to form the state space:
what information has been observed, what the content
model believes is the correct answer, how confident
the content model is, and whether the content model?s
confidence is changing. We describe each in more
detail below.
Text In addition to the obvious, sparse parameter-
ization that contains all of the features thus far ob-
1294
served, we also include the total number of tokens
revealed and whether the phrase ?for ten points? has
appeared.6
Guess An additional feature that we used to repre-
sent the state space is the current guess of the content
model; i.e., the argmax of the posterior.
Posterior The posterior feature (Pos for short) cap-
tures the shape of the posterior distribution: the prob-
ability of the current guess (the max of the poste-
rior), the difference between the top two probabilities
and the probabilities associated with the fifteen most
probable labels under the posterior.
Change As features are revealed, there is often
a rapid transition from a state of confusion?when
there are many candidates with no clear best choice?
to a state of clarity with the posterior pointing to only
one probable label. To capture when this happens,
we add a binary feature to reflect when the best guess
has changed when a single feature has been revealed.
Other Features We thought that other features
would be useful. While useful on their own, no
features that we tried were useful when the content
model?s posterior was also used as a feature. Fea-
tures that we attempted to use were: a logistic re-
gression model attempting to capture the probability
that any player would answer (Silver et al 2008), a
regression predicting how many individuals would
buzz in the next n words, the year the question was
written, the category of the question, etc.
5.3 Na??ve Content Model
The action space is only deciding when to answer,
having abdicated responsibility for what to answer.
So where does do the answers come from? We as-
sume that at any point we can ask ?what is the highest
probability label given my current feature observa-
tions?? We call the component of our model that
answers this question the content model.
Our first content model is a na??ve Bayes
model (Lewis, 1998) trained over a text collection.
This generative model assumes labels for questions
come from a multinomial distribution ? ? Dir(?)
6The phrase ?for ten points? (abbreviated FTP) appears in
all quiz bowl questions to signal the question?s last sentence or
clause. It is a signal to answer soon, as the final ?giveaway? clue
is next.
and assumes that label l has a word distribution
?l ? Dir(?). Each question n has a label zn and
its words are generated from ?zn . Given labeled ob-
servations, we use the maximum a posteriori (MAP)
estimate of ?l.
Why use a generative model when a discriminative
classifier could use a richer feature space? The most
important reason is that, by definition, it makes sense
to ask a generative model the probability of a label
given a partial observation; such a question is not
well-formed for discriminative models, which expect
a complete feature set. Another important consid-
eration is that generative models can predict future,
unrevealed features (Chai et al 2004); however, we
do not make use of that capability here.
In addition to providing our answers, the content
model also provides an additional, critically impor-
tant feature for our state space: its posterior (pos
for short) probability. With every revealed feature,
the content model updates its posterior distribution
over labels given that t tokens have been revealed in
question n,
p(zn |w1 . . . wt, ?,?). (2)
To train our na??ve Bayes model, we semi-
automatically associate labels with a Wikipedia page
(correcting mistakes manually) and then form the
MAP estimate of the class multinomial distribution
from the Wikipedia page?s text. We did this for the
1065 labels that had at least three human answers,
excluding ambiguous labels associated with multiple
concepts (e.g., ?europa?, ?steppenwolf?, ?georgia?,
?paris?, and ?v?).
Features were taken to be the 25,000 most frequent
tokens and bigrams7 that were not stop words; fea-
tures were extracted from the Wikipedia text in the
same manner as from the question tokens.8
After demonstrating our ability to learn an incre-
mental classifier using this simple content model, we
extend the content model to capture local context and
correlations between similar labels in Section 7.
7We used NLTK (Loper and Bird, 2002) to filter stop words
and we used a ?2 test to identify bigrams with that rejected the
null hypothesis at the 0.01 level.
8The Dirichlet scaling parameter ? was set to 10,000 given
our relatively large vocabulary (25,000) and to not penalize a
label?s posterior probability if there were unseen features; this
corresponds to a pseudocount of 0.4. ? was set to 1.0.
1295
6 Pitting the Algorithm Against Humans
With a state space and a policy, we now have all the
necessary ingredients to have our algorithm compete
against humans. Classification, which allows us to
learn a policy, was done using the default settings of
LIBLINEAR (Fan et al 2008). To determine where
the algorithm buzzes, we provide a sequence of state
spaces until the policy classifier determines that it is
time to buzz.
We simulate competition by taking the human an-
swers and buzzes as a given and ask our algorithm
(independently) to provide its decision on when to
buzz on a test set. We compare the two buzz positions.
If the algorithm buzzed earlier with the right answer,
we consider it to have ?won? the question; equiva-
lently, if the algorithm buzzed later, we consider it to
have ?lost? that question. Ties are rare (less than 1%
of cases), as the algorithm had significantly different
behavior from humans; in the case where there was a
tie, ties were broken in favor of the machine.
Because we have a large, diverse population an-
swering questions, we need aggregate measures of
human performance to get a comprehensive view of
algorithm performance. We use the following metrics
for each question in the test set:
? best: the earliest anyone buzzed correctly
? median: the first buzz after 50% of human buzzes
? mean: for each recorded buzz compute a reward and
we average over all rewards
We compare the algorithm against baseline strategies:
? rap The rapacious strategy waits until the end of the
question and answers the best answer possible.
? ftp Waiting until when ?for 10 points? is said, then
giving the best answer possible.
? indexn Waiting until the first feature after the nth to-
ken has been processed, then giving the best answer
possible. The indices were chosen as the quartiles
for question length (by convention, most questions
are of similar length).
We compare these baselines against policies that de-
cide when to buzz based on the state.
Recall that the non-oracle algorithms were un-
aware of the true reward function. To best simulate
conventional quiz bowl settings, a correct answer
was +10 and the incorrect answer was ?5. The full
payoff matrix for the computer is shown in Table 2.
Cases where the opponent buzzes first but is wrong
are equivalent to rapacious classification, as there is
no longer any incentive to answer early. Thus we
exclude such situations (Outcomes 3, 5, 6 in Table 2)
from the dataset to focus on the challenge of process-
ing clues incrementally.
Computer Human Payoff
1 first and wrong right ?15
2 ? first and correct ?10
3 first and wrong wrong ?5
4 first and correct ? +10
5 wrong first and wrong +5
6 right first and wrong +15
Table 2: Payoff matrix (from the computer?s perspective)
for when agents ?buzz? during a question. To focus on
incremental classification, we exclude instances where the
human interrupts with an incorrect answer, as after an
opponent eliminates themselves, the answering reduces to
rapacious classification.
Table 3 shows the algorithm did much better when
it had access to the posterior. While incremental
algorithms outperform rapacious baselines, they lose
to humans. Against the median and average players,
they lose between three and four points per question,
and nearly twice that against the best players.
Although the content model is simple, this poor
performance is not from the content model never
producing the correct answer. To see this, we also
computed the optimal actions that could be executed.
We called this strategy the oracle strategy; it was able
to consistently win against its opponents. Thus, while
the content model was able to come up with correct
answers often enough to on average win against oppo-
nents (even the best human players), we were unable
to consistently learn winning policies.
There are two ways to solve this problem: create
deeper, more nuanced policies (or the features that
feed into them) or refine content models that provide
the signal needed for our policies to make sound
decisions. We chose to refine the content model, as
we felt we had added all of the obvious features for
learning effective policies.
7 Expanding the Content Model
When we asked quiz bowlers how they answer ques-
tions, they said that they first determine the category
1296
Strategy Features Mean Best Median Index
Classify
text -8.72 -10.04 -6.50 40.36
+guess -5.71 -8.40 -3.95 66.02
+pos -4.13 -7.56 -2.70 67.97
+change -4.02 -7.41 -2.63 77.33
Oracle text 3.36 0.61 4.35 49.90
all -6.61 -9.03 -4.42 100.19
ftp -5.22 -8.62 -4.23 88.65
Rapacious index30 -7.89 -8.71 -6.41 32.23
Baseline index60 -5.16 -7.56 -3.71 61.90
index90 -5.02 -8.62 -3.50 87.13
Table 3: Performance of strategies against users. The
human scoring columns show the average points per ques-
tion (positive means winning on average, negative means
losing on average) that the algorithm would expect to ac-
cumulate per question versus each human amalgam metric.
The index column notes the average index of the token
when the strategy chose to buzz.
of a question, which substantially narrows the an-
swer space. Ideally, the content model should con-
duct the same calculus?if a question seems to be
about mathematics, all answers related with mathe-
matics should be more likely in the posterior. This
was consistent with our error analysis; many errors
were non-sensical (e.g., answering ?entropy? for ?Jo-
hannes Brahms?, when an answer such as ?Robert
Schumann?, another composer, would be better).
In addition, assuming independence between fea-
tures given a label causes us to ignore potentially
informative multiword expressions such as quota-
tions, titles, or dates. Adding a language model to
our content model allows us to capture some of these
phenomena.
To create a model that jointly models categories
and local context, we propose the following model:
1. Draw a distribution over labels ? ? Dir(?)
2. Draw a background distribution over words ?0 ?
Dir(?0~1)
(a) For each category c of questions, draw a distribution
over words ?c ? Dir(?1?0).
i. For each label l in category c, draw a distribu-
tion over words ?l,c ? Dir(?2?c)
A. For each type v, draw a bigram distribution
?l,c,v ? Dir(?3?l,c)
3. Draw a distribution over labels ? ? Dir(?).
4. For each question with category c and N words, draw
answer l ? Mult(?):
(a) Assume w0 ? START
(b) Draw wn ? Mult(?l,c,wn?1) for n ? {1 . . . N}
This creates a language model over categories, la-
bels, and observed words (we use ?words? loosely, as
bigrams replace some word pairs). By constructing
the word distributions using hierarchical distributions
based on domain and ngrams (a much simpler para-
metric version of more elaborate methods (Wood and
Teh, 2009)), we can share statistical strength across
related contexts. We assume that labels are (only)
associated with their majority category as seen in our
training data and that category assignments are ob-
served. All scaling parameters ? were set to 10,000,
? was 1.0, and the vocabulary was still 25,000.
We used the maximal seating assignment (Wallach,
2008) for propagating counts through the Dirichlet
hierarchy. Thus, if the word v appeared Bl,u,v times
in label l following a preceding word u, Sl,v times in
label l, Tc,v times in category c, andGv times in total,
we estimate the probability of a word v appearing
in label k, category t, and after word u as p(wn =
v | lab = l, cat = c, wn?1 = u;~?) =
Bl,u,v + ?3
Sl,v+?2
Tc,v+?1
Gv+?0/V
G?+?0
Tc,?+?2
Sl,?+?2
Bl,u,? + ?3
, (3)
where we use ? to represent marginalization, e.g.
Tc,? =
?
v? Tc,v? . As with na??ve Bayes, Bayes? rule
provides posterior label probabilities (Equation 2).
We compare the na??ve model with models that
capture more of the content in the text in Table 4;
these results also include intermediate models be-
tween na??ve Bayes and the full content model: ?cat?
(omit 2.a.i.A) and ?bigram? (omit 2.a). These models
perform much better than the na??ve Bayes models
seen in Table 3. They are about even against the
mean and median players and lose four points per
question against top players.
7.1 Qualitative Analysis
In this section, we explore what defects are prevent-
ing the model presented here from competing with
top players, exposing challenges in reinforcement
learning, interpreting pragmatic cues, and large data.
Three examples of failures of the model are in Fig-
ure 5. This model is the best performing model of
the previous section.
Too Slow The first example is a question on Mau-
rice Ravel, a French composer known for Bole?ro. The
question leads off with Ravel?s orchestral version of
1297
Strategy Model Mean Best Median Index
Classify
na??ve -4.02 -7.41 -2.63 77.33
cat -1.69 -5.22 0.12 67.97
bigram -3.80 -7.66 -2.51 78.69
bgrm+cat -0.86 -4.46 0.83 63.42
Oracle
naive 3.36 0.61 4.35 49.90
cat 4.48 1.64 5.47 47.88
bigram 3.58 0.87 4.61 49.34
bgrm+cat 4.67 1.99 5.74 46.49
Table 4: As in Table 3, performance of strategies against
users, but with enhanced content models. Modeling both
bigrams and label categories improves overall perfor-
mance.
Mussorgsky?s piano piece ?Pictures at an Exhibition?.
Based on that evidence, the algorithm considers ?Pic-
tures at an Exhibition? the most likely but does not
yet buzz. When it receives enough information to be
sure about the correct answer, over half of the players
had already buzzed. Correcting this problem would
require a more aggressive strategy, perhaps incorpo-
rating the identity of the opponent or estimating the
difficulty of the question.
Mislead by the Content Model The second ex-
ample is a question on Enrico Fermi, an Italian-
American physicist. The first clues are about mag-
netic fields near a Fermi surface, which causes the
content model to view ?magnetic field? as the most
likely answer. The question?s text, however, has
pragmatic cues ?this man? and ?this Italian? which
would have ruled out the abstract answer ?magnetic
field?. Correcting this would require a model that
jointly models content and bigrams (Hardisty et
al., 2010), has a coreference system as its content
model (Haghighi and Klein, 2007), or determines the
correct question type (Moldovan et al 2000).
Insufficient Data The third example is where our
approach had no chance. The question is a very diffi-
cult question about George Washington, America?s
first president. As a sign of its difficulty, only half
the players answered correctly, and only near the end
of the question. The question concerns lesser known
episodes from Washington?s life, including a mistress
caught in the elements. To the content model, of the
several hypotheses it considers, the closest match
it can find is ?Yasunari Kawabata?, who wrote the
novel Snow Country, whose plot matches some of
these keywords. To answer these types of question,
george washington
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
0 10 20 30 40 50 60
maurice ravel
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
1.0
0 10 20 30 40 50 60 70
enrico fermi
Tokens Revealed
0.0
0.2
0.4
0.6
0.8
1.0
0 20 40 60 80 100
charlemagne yasunari kawabata
generals
chill
mistress
language
magnetic field neutrinomagnetic field
magnetic
paradox
zero
this_man
this_italian
pictures
orchestrated
pictures at an exhibition
maurice ravel
this_french
composer
bolero
Prediction
answer
Observation
feature
Buzz
Posterior Opponent
Figure 5: Three questions where our algorithm performed
poorly. It gets ?Maurice Ravel? (top) right but only after
over half the humans had answered correctly (i.e., the
buzz?s hexagon appears when the cyan line is above 0.6);
on ?Enrico Fermi? (middle) it confuses the correct type
of answer (person vs. concept); on ?George Washington?
(bottom) it lacks information to answer correctly. Lines
represent the current estimate posterior probability of the
answer (red) and the proportion of opponents who have
answered the question correctly (cyan). The label of each
of the three questions is above each chart. Words are in
black with arrows and arrows, and the current argmax
answer is at the bottom of the graph in red. The buzz
location is the hexagon.
1298
the repository used to train the content model would
have to be orders of magnitude larger to be able to
link the disparate clues in the question to a consistent
target. The content model would also benefit from
weighting later (more informative) features higher.
7.2 Assumptions
We have made assumptions to solve a problem that
is subtly different that the game of quiz bowl that
a human would play. Some of these were simpli-
fying assumptions, such as our assumption that the
algorithm has a closed set of possible answers (Sec-
tion 5.3). Even with this advantage, the algorithm is
unable to compete with human players, who choose
answers from an unbounded set. On the other hand,
to focus on incremental classification, we idealized
our human opponents so that they never give incor-
rect answers (Section 6). This causes our estimates
of our performance to be lower than they would be
against real players.
8 Conclusion and Future Work
We make three contributions. First, we introduce a
new setting for exploring the problem of incremental
classification: trivia games. This problem is intrin-
sically interesting because of its varied topics and
competitive elements, has a great quantity of stan-
dardized, machine-readable data, and also has the
boon of being cheaply and easily annotated. We took
advantage of that ease and created a framework for
quickly and efficiently gathering examples of humans
doing incremental classification.
There are other potential uses for the dataset; the
progression of clues from obscure nuggets to could
help determine how ?known? a particular aspect of
an entity is (e.g., that William Jennings Bryant gave
the ?Cross of Gold? speech is better known his resig-
nation after the Lusitania sinking, Figure 1). Which
could be used in educational settings (Smith et al
2008) or summarization (Das and Martins, 2007).
The second contribution shows that humans? incre-
mental classification improves state-of-the-art rapa-
cious classification algorithms. While other frame-
works (Zaidan et al 2008) have been proposed to
incorporate user clues about features, the system de-
scribed here provides analogous features without the
need for explicit post-hoc reflection, has faster anno-
tation throughput, and is much cheaper.
The problem of answering quiz bowl questions is
itself a challenging task that combines issues from
language modeling, large data, coreference, and re-
inforcement learning. While we do not address all
of these problems, our third contribution is a sys-
tem that learns a policy in a MDP for incremental
classification even in very large state spaces; it can
successfully compete with skilled human players.
Incorporating richer content models is one of our
next steps. This would allow us to move beyond the
closed-set model and use a more general coreference
model (Haghighi and Klein, 2007) for identifying
answers and broader corpora for training. In addi-
tion, using larger corpora would allow us to have
more comprehensive doubly-hierarchical language
models (Wood and Teh, 2009). We are also inter-
ested in adding richer models of opponents to the
state space that would adaptively adjust strategies as
it learned more about the strengths and weaknesses
of its opponent (Waugh et al 2011).
Further afield, our presentation of sentences
closely resembles paradigms for cognitive experi-
ments in linguistics (Thibadeau et al 1982) but are
much cheaper to conduct. If online processing ef-
fects (Levy et al 2008; Levy, 2011) could be ob-
served in buzzing behavior; e.g., if a confusingly
worded phrase depresses buzzing probability, it could
help validate cognitively-inspired models of online
sentence processing.
Incremental classification is a natural problem,
both for humans and resource-limited machines.
While our data set is trivial (in a good sense), learn-
ing how humans process data and make decisions
in a cheap, easy crowdsourced application can help
us apply new algorithms to improve performance in
settings where features aren?t free, either because of
computational or annotation cost.
1299
Acknowledgments
We thank the many players who played our online
quiz bowl to provide our data (and hopefully had fun
doing so) and Carlo Angiuli, Arnav Moudgil, and
Jerry Vinokurov for providing access to quiz bowl
questions. This research was supported by NSF grant
#1018625. Jordan Boyd-Graber is also supported by
the Army Research Laboratory through ARL Cooper-
ative Agreement W911NF-09-2-0072. Any opinions,
findings, conclusions, or recommendations expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of International Conference of Machine
Learning.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confi-
dence estimation for machine translation. In Proceed-
ings of the Association for Computational Linguistics.
Mark Boddy and Thomas L. Dean. 1989. Solving time-
dependent planning problems. In International Joint
Conference on Artificial Intelligence, pages 979?984.
Morgan Kaufmann Publishers, August.
Nicolo` Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad
Shamir. 2011. Efficient learning with partially ob-
served attributes. Journal of Machine Learning Re-
search, 12:2857?2878.
Xiaoyong Chai, Lin Deng, Qiang Yang, and Charles X.
Ling. 2004. Test-cost sensitive naive bayes classi-
fication. In IEEE International Conference on Data
Mining.
Dipanjan Das and Andre Martins. 2007. A survey on
automatic text summarization. Engineering and Tech-
nology, 4:192?195.
Hal Daume? III. 2004. Notes on CG and LM-
BFGS optimization of logistic regression. Pa-
per available at http://pub.hal3.name/
?daume04cg-bfgs, implementation available at
http://hal3.name/megam/.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building Watson:
An Overview of the DeepQA Project. AI Magazine,
31(3).
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the Association for Computational
Linguistics.
Eric Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Michael C. Horsch and David Poole. 1998. An anytime
algorithm for decision making under uncertainty. In
Proceedings of Uncertainty in Artificial Intelligence.
Ken Jennings. 2006. Brainiac: adventures in the curious,
competitive, compulsive world of trivia buffs. Villard.
Shihao Ji and Lawrence Carin. 2007. Cost-sensitive fea-
ture acquisition and classification. Pattern Recognition,
40:1474?1485, May.
Shyong K. Lam, David M. Pennock, Dan Cosley, and
Steve Lawrence. 2003. 1 billion pages = 1 million
dollars? mining the web to play ?who wants to be a mil-
lionaire??. In Proceedings of Uncertainty in Artificial
Intelligence.
John Langford and Bianca Zadrozny. 2005. Relating
reinforcement learning performance to classification
performance. In Proceedings of International Confer-
ence of Machine Learning.
Roger P. Levy, Florencia Reali, and Thomas L. Griffiths.
2008. Modeling the effects of memory on human on-
line sentence processing with particle filters. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Roger Levy. 2011. Integrating surprisal and uncertain-
input models in online sentence comprehension: formal
techniques and empirical results. In Proceedings of the
Association for Computational Linguistics.
David D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
Ne?dellec and Ce?line Rouveirol, editors, Proceedings
of European Conference of Machine Learning, number
1398.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artif. Intell., 134(1-2):23?55, January.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost,
and Raymond J. Mooney. 2005. An expected utility
approach to active feature-value acquisition. In Inter-
national Conference on Data Mining, November.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
1300
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Association for Computational Linguistics.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34(4):513?553, December.
Jay Pujara, Hal Daume III, and Lise Getoor. 2011. Using
classifier cascades for scalable e-mail classification.
In Collaboration, Electronic Messaging, Anti-Abuse
and Spam Conference, ACM International Conference
Proceedings Series.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model combination for event extraction in bionlp 2011.
In Proceedings of the BioNLP Workshop.
Afshin Rostamizadeh, Alekh Agarwal, and Peter L.
Bartlett. 2011. Learning with missing features. In
Proceedings of Uncertainty in Artificial Intelligence.
Maytal Saar-Tsechansky and Foster Provost. 2007. Han-
dling missing values when applying classification mod-
els. Journal of Machine Learning Research, 8:1623?
1657, December.
Gerard. Salton. 1968. Automatic Information Organiza-
tion and Retrieval. McGraw Hill Text.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of Emperical Methods
in Natural Language Processing.
David Silver, Richard S. Sutton, and Martin Mu?ller. 2008.
Sample-based learning and search with permanent and
transient memories. In International Conference on
Machine Learning.
Noah A. Smith, Michael Heilman, and Rebecca Hwa.
2008. Question generation as a competitive under-
graduate course project. In Proceedings of the NSF
Workshop on the Question Generation Shared Task and
Evaluation Challenge.
Umar Syed, Michael Bowling, and Robert E. Schapire.
2008. Apprenticeship learning using linear program-
ming. In Proceedings of International Conference of
Machine Learning.
Gerald Tesauro and Gregory R. Galperin. 1996. On-line
policy improvement using monte-carlo search. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Robert Thibadeau, Marcel A. Just, and Patricia A. Carpen-
ter. 1982. A model of the time course and content of
reading. Cognitive Science, 6.
Hanna M Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.
Lidan Wang, Donald Metzler, and Jimmy Lin. 2010.
Ranking Under Temporal Constraints. In Proceedings
of the ACM International Conference on Information
and Knowledge Management.
Kevin Waugh, Brian D. Ziebart, and J. Andrew Bagnell.
2011. Computational rationalization: The inverse equi-
librium problem. In Proceedings of International Con-
ference of Machine Learning.
F. Wood and Y. W. Teh. 2009. A hierarchical nonpara-
metric Bayesian approach to statistical language model
domain adaptation. In Proceedings of Artificial Intelli-
gence and Statistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2008.
Machine learning with annotator rationales to reduce
annotation cost. In Proceedings of the NIPS*2008
Workshop on Cost Sensitive Learning.
Valentina Bayer Zubek and Thomas G. Dietterich. 2002.
Pruning improves heuristic search for cost-sensitive
learning. In International Conference on Machine
Learning.
1301
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 248?257,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Interactive Topic Modeling
Yuening Hu
Department of Computer Science
University of Maryland
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool
University of Maryland
jbg@umiacs.umd.edu
Brianna Satinoff
Department of Computer Science
University of Maryland
bsonrisa@cs.umd.edu
Abstract
Topic models have been used extensively as a
tool for corpus exploration, and a cottage in-
dustry has developed to tweak topic models
to better encode human intuitions or to better
model data. However, creating such extensions
requires expertise in machine learning unavail-
able to potential end-users of topic modeling
software. In this work, we develop a frame-
work for allowing users to iteratively refine
the topics discovered by models such as la-
tent Dirichlet alocation (LDA) by adding con-
straints that enforce that sets of words must ap-
pear together in the same topic. We incorporate
these constraints interactively by selectively
removing elements in the state of a Markov
Chain used for inference; we investigate a va-
riety of methods for incorporating this infor-
mation and demonstrate that these interactively
added constraints improve topic usefulness for
simulated and actual user sessions.
1 Introduction
Probabilistic topic models, as exemplified by prob-
abilistic latent semantic indexing (Hofmann, 1999)
and latent Dirichlet alocation (LDA) (Blei et al,
2003) are unsupervised statistical techniques to dis-
cover the thematic topics that permeate a large cor-
pus of text documents. Topic models have had con-
siderable application beyond natural language pro-
cessing in computer vision (Rob et al, 2005), bi-
ology (Shringarpure and Xing, 2008), and psychol-
ogy (Landauer et al, 2006) in addition to their canon-
ical application to text.
For text, one of the few real-world applications
of topic models is corpus exploration. Unannotated,
noisy, and ever-growing corpora are the norm rather
than the exception, and topic models offer a way to
quickly get the gist a large corpus.1
1For examples, see Rexa http://rexa.info/, JSTOR
Contrary to the impression given by the tables
shown in topic modeling papers, topics discovered
by topic modeling don?t always make sense to os-
tensible end users. Part of the problem is that the
objective function of topic models doesn?t always cor-
relate with human judgements (Chang et al, 2009).
Another issue is that topic models ? with their bag-
of-words vision of the world ? simply lack the nec-
essary information to create the topics as end-users
expect.
There has been a thriving cottage industry adding
more and more information to topic models to cor-
rect these shortcomings; either by modeling perspec-
tive (Paul and Girju, 2010; Lin et al, 2006), syn-
tax (Wallach, 2006; Gruber et al, 2007), or author-
ship (Rosen-Zvi et al, 2004; Dietz et al, 2007). Sim-
ilarly, there has been an effort to inject human knowl-
edge into topic models (Boyd-Graber et al, 2007;
Andrzejewski et al, 2009; Petterson et al, 2010).
However, these are a priori fixes. They don?t help
a frustrated consumer of topic models staring at a
collection of topics that don?t make sense. In this
paper, we propose interactive topic modeling (ITM),
an in situ method for incorporating human knowl-
edge into topic models. In Section 2, we review prior
work on creating probabilistic models that incorpo-
rate human knowledge, which we extend in Section 3
to apply to ITM sessions. Section 4 discusses the
implementation of this process during the inference
process. Via a motivating example in Section 5, simu-
lated ITM sessions in Section 6, and a real interactive
test in Section 7, we demonstrate that our approach is
able to focus a user?s desires in a topic model, better
capture the key properties of a corpus, and capture
diverse interests from users on the web.
http://showcase.jstor.org/blei/, and the NIH
https://app.nihmaps.org/nih/.
248
2 Putting Knowledge in Topic Models
At a high level, topic models such as LDA take as
input a number of topics K and a corpus. As output,
a topic model discovers K distributions over words
? the namesake topics ? and associations between
documents and topics. In LDA both of these out-
puts are multinomial distributions; typically they are
presented to users in summary form by listing the
elements with highest probability. For an example
of topics discovered from a 20-topic model of New
York Times editorials, see Table 1.
When presented with poor topics learned from
data, users can offer a number of complaints:2
these documents should have similar topics but
don?t (Daume? III, 2009); this topic should have syn-
tactic coherence (Gruber et al, 2007; Boyd-Graber
and Blei, 2008); this topic doesn?t make any sense
at all (Newman et al, 2010); this topic shouldn?t be
associated with this document but is (Ramage et al,
2009); these words shouldn?t be the in same topic
but are (Andrzejewski et al, 2009); or these words
should be in the same topic but aren?t (Andrzejewski
et al, 2009).
Many of these complaints can be addressed by
using ?must-link? constraints on topics, retaining An-
drzejewski et als (2009) terminology borrowed from
the database literature. A ?must-link? constraint is a
group of words whose probability must be correlated
in the topic. For example, Figure 1 shows an example
constraint: {plant, factory}. After this constraint is
added, the probabilities of ?plant? and ?factory? in
each topic are likely to both be high or both be low.
It?s unlikely for ?plant? to have high probability in a
topic and ?factory? to have a low probability. In the
next section, we demonstrate how such constraints
can be built into a model and how they can even be
added while inference is underway.
In this paper, we view constraints as transitive; if
?plant? is in a constraint with ?factory? and ?factory?
is in a constraint with ?production,? then ?plant? is
in a constraint with ?production.? Making this as-
sumption can simplify inference slightly, which we
take advantage of in Section 3.1, but the real reason
for this assumption is because not doing so would
2Citations in this litany of complaints are offline solutions for
addressing the problem; the papers also give motivation why
such complaints might arise.
Constraints Prior Structure
{}
dogbark tree plant factory leash
?
?
?
?
?
?
{plant, factory}
dogbark tree
plant
factory
leash
?
? ?
?
2?
?
?
{plant, factory}
{dog, bark, leash}
dogbark
tree
plant factoryleash
?
?
?
?
2?
?
?
3?
Figure 1: How adding constraints (left) creates new topic
priors (right). The trees represent correlated distributions
(assuming ? >> ?). After the {plant, factory} constraint
is added, it is now highly unlikely for a topic drawn from
the distribution to have a high probability for ?plant? and
a low probability for ?factory? or vice versa. The bottom
panel adds an additional constraint, so now dog-related
words are also correlated. Notice that the two constraints
themselves are uncorrelated. It?s possible for both, either,
or none of ?bark? and ?plant? (for instance) to have high
probability in a topic.
introduce ambiguity over the path associated with an
observed token in the generative process. As long as
a word is either in a single constraint or in the general
vocabulary, there is only a single path. The details of
this issue are further discussed in Section 4.
3 Constraints Shape Topics
As discussed above, LDA views topics as distribu-
tions over words, and each document expresses an
admixture of these topics. For ?vanilla? LDA (no con-
straints), these are symmetric Dirichlet distributions.
A document is composed of a number of observed
words, which we call tokens to distinguish specific
observations from the more abstract word (type) as-
sociated with each token. Because LDA assumes
a document?s tokens are interchangeable, it treats
the document as a bag-of-words, ignoring potential
relations between words.
This problem with vanilla LDA can be solved by
encoding constraints, which will ?guide? different
words into the same topic. Constraints can be added
to vanilla LDA by replacing the multinomial distri-
bution over words for each topic with a collection of
249
tree-structured multinomial distributions drawn from
a prior as depicted in Figure 1. By encoding word
distributions as a tree, we can preserve conjugacy
and relatively simple inference while encouraging
correlations between related concepts (Boyd-Graber
et al, 2007; Andrzejewski et al, 2009; Boyd-Graber
and Resnik, 2010). Each topic has a top-level dis-
tribution over words and constraints, and each con-
straint in each topic has second-level distribution
over the words in the constraint. Critically, the per-
constraint distribution over words is engineered to be
non-sparse and close to uniform. The top level distri-
bution encodes which constraints (and unconstrained
words) to include; the lower-level distribution forces
the probabilities to be correlated for each of the con-
straints.
In LDA, a document?s token is produced in the
generative process by choosing a topic z and sam-
pling a word from the multinomial distribution ?z of
topic z. For a constrained topic, the process now can
take two steps. First, a first-level node in the tree is
selected from ?z . If that is an unconstrained word,
the word is emitted and the generative process for
that token is done. Otherwise, if the first level node
is constraint l, then choose a word to emit from the
constraint?s distribution over words piz,l.
More concretely, suppose for a corpus with M
documents we have a set of constraints ?. The prior
structure has B branches (one branch for each word
not in a constraint and one for each constraint). Then
the generative process for constrained LDA is:
1. For each topic i ? {1, . . .K}:
(a) draw a distribution over the B branches (words and
constraints) ?i ? Dir(~?), and
(b) for each constraint ?j ? ?, draw a distribution over
the words in the constraint pii,j ? Dir(?), where
pii,j is a distribution over the words in ?j
2. Then for each document d ? {1, . . .M}:
(a) first draw a distribution over topics ?d ? Dir(?),
(b) then for each token n ? {1, . . . Nd}:
i. choose a topic assignment zd,n ? Mult(?d),
and then
ii. choose either a constraint or word from
Mult(?zd,n):
A. if we chose a word, emit that word wd,n
B. otherwise if we chose a constraint index ld,n,
emit a word wd,n from the constraint?s dis-
tribution over words in topic zd,n: wd,n ?
Mult(pizd,n,ld,n).
In this model, ?, ?, and ? are Dirichlet hyperpa-
rameters set by the user; their role is explained below.
3.1 Gibbs Sampling for Topic Models
In topic modeling, collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004) is a standard procedure for
obtaining a Markov chain over the latent variables
in the model. Given certain technical conditions,
the stationary distribution of the Markov chain is
the posterior (Neal, 1993). Given M documents the
state of a Gibbs sampler for LDA consists of topic
assignments for each token in the corpus and is rep-
resented as Z = {z1,1 . . . z1,N1 , z2,1, . . . zM,NM }. In
each iteration, every token?s topic assignment zd,n
is resampled based on topic assignments for all the
tokens except for zd,n. (This subset of the state is
denoted Z?(d,n)). The sampling equation for zd,n is
p(zd,n = k|Z?(d,n), ?, ?) ?
Td,k + ?
Td,? +K?
Pk,wd,n + ?
Pk,? + V ?
(1)
where Td,k is the number of times topic k is used in
document d, Pk,wd,n is the number of times the type
wd,n is assigned to topic k, and ?, ? are the hyperpa-
rameters of the two Dirichlet distributions, and B is
the number of top-level branches (this is the vocab-
ulary size for vanilla LDA). When a dot replaces a
subscript of a count, it represents the marginal sum
over all possible topics or words, e.g. Td,? =
?
k Td,k.
The count statistics P and T provide summaries of
the state. Typically, these only change based on as-
signments of latent variables in the sampler; in Sec-
tion 4 we describe how changes in the model?s struc-
ture (in addition to the latent state) can be reflected
in these count statistics.
Contrasting with the above inference is the infer-
ence for a constrained model. (For a derivation, see
Boyd-Graber, Blei, and Zhu (2007) for the general
case or Andrzejewski, Zhu, and Craven (2009) for
the specific case of constraints.) In this case the
sampling equation for zd,n is changed to p(zd,n =
k|Z?(d,n), ?, ?, ?)
?
?
??
??
Td,k+?
Td,?+K?
Pk,wd,n+?
Pk,?+V ?
if ?l, wd,n 6? ?l
Td,k+?
Td,?+K?
Pk,l+Cl?
Pk,?+V ?
Wk,l,wd,n+?
Wk,l,?+Cl?
wd,n ? ?l
, (2)
where Pk,wd,n is the number of times the uncon-
strained word wd,n appears in topic k; Pk,l is the
250
number of times any word of constraint ?l appears in
topic k; Wk,l,wd,n is the number of times word wd,n
appears in constraint ?l in topic k; V is the vocabu-
lary size; Cl is the number of words in constraint ?l.
Note the differences between these two samplers for
constrained words; however, for unconstrained LDA
and for unconstrained words in constrained LDA, the
conditional probability is the same.
In order to make the constraints effective, we set
the constraint word-distribution hyperparameter ?
to be much larger than the hyperparameter for the
distribution over constraints and vocabulary ?. This
gives the constraints higher weight. Normally, esti-
mating hyperparameters is important for topic mod-
eling (Wallach et al, 2009). However, in ITM, sam-
pling hyperparameters often (but not always) undoes
the constraints (by making ? comparable to ?), so we
keep the hyperparameters fixed.
4 Interactively adding constraints
For a static model, inference in ITM is the same as
in previous models (Andrzejewski et al, 2009). In
this section, we detail how interactively changing
constraints can be accommodated in ITM, smoothly
transitioning from unconstrained LDA (n.b. Equa-
tion 1) to constrained LDA (n.b. Equation 2) with one
constraint, to constrained LDA with two constraints,
etc.
A central tool that we will use is the strategic unas-
signment of states, which we call ablation (distinct
from feature ablation in supervised learning). As
described in the previous section, a sampler stores
the topic assignment of each token. In the implemen-
tation of a Gibbs sampler, unassignment is done by
setting a token?s topic assignment to an invalid topic
(e.g. -1, as we use here) and decrementing any counts
associated with that word.
The constraints created by users implicitly signal
that words in constraints don?t belong in a given
topic. In other models, this input is sometimes used
to ?fix,? i.e. deterministically hold constant topic as-
signments (Ramage et al, 2009). Instead, we change
the underlying model, using the current topic assign-
ments as a starting position for a new Markov chain
with some states strategically unassigned. How much
of the existing topic assignments we use leads to four
different options, which are illustrated in Figure 2.
Previous New
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:-1 dog:-1]
[bark:-1, bark:-1, plant:-1, tree:-1]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:3]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:3 dog:-1]
[bark:-1, bark:-1, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:2, dog:3, leash:3 dog:2]
[bark:2, bark:2, plant:2, tree:3]
[tree:2,play:2,forest:1,leash:2]
[bark:-1, dog:-1, leash:-1 dog:-1]
[bark:-1, bark:-1, plant:-1, tree:-1]
[tree:-1,play:-1,forest:-1,leash:-1]
None
Term
Doc
All
Figure 2: Four different strategies for state ablation after
the words ?dog? and ?bark? are added to the constraint
{?leash,? ?puppy?} to make the constraint {?dog,? ?bark,?
?leash,? ?puppy?}. The state is represented by showing the
current topic assignment after each word (e.g. ?leash? in
the first document has topic 3, while ?forest? in the third
document has topic 1). On the left are the assignments
before words were added to constraints, and on the right
are the ablated assignments. Unassigned words are given
the new topic assignment -1 and are highlighted in red.
All We could revoke all state assignments, essen-
tially starting the sampler from scratch. This does
not allow interactive refinement, as there is nothing
to enforce that the new topics will be in any way
consistent with the existing topics. Once the topic
assignments of all states are revoked, the counts for
T , P and W (as described in Section 3.1) will be
zero, retaining no information about the state the user
observed.
Doc Because topic models treat the document con-
text as exchangeable, a document is a natural context
for partial state ablation. Thus if a user adds a set of
words S to constraints, then we have reason to sus-
pect that all documents containing any one of S may
have incorrect topic assignments. This is reflected
in the state of the sampler by performing the UNAS-
SIGN (Algorithm 1) operation for each word in any
document containing a word added to a constraint.
Algorithm 1 UNASSIGN(d, n, wd,n, zd,n = k)
1: T : Td,k ? Td,k ? 1
2: If wd,n /? ?old,
P : Pk,wd,n ? Pk,wd,n ? 1
3: Else: suppose wd,n ? ?oldm ,
P : Pk,m ? Pk,m ? 1
W : Wk,m,wd,n ?Wk,m,wd,n ? 1
251
This is equivalent to the Gibbs2 sampler of Yao
et al (2009) for incorporating new documents in
a streaming context. Viewed in this light, a user
is using words to select documents that should be
treated as ?new? for this refined model.
Term Another option is to perform ablation only
on the topic assignments of tokens whose words have
added to a constraint. This applies the unassignment
operation (Algorithm 1) only to tokens whose corre-
sponding word appears in added constraints (i.e. a
subset of the Doc strategy). This makes it less likely
that other tokens in similar contexts will follow the
words explicitly included in the constraints to new
topic assignments.
None The final option is to move words into con-
straints but keep the topic assignments fixed. Thus,
P and W change, but not T , as described in Algo-
rithm 2.3 This is arguably the simplest option, and
in principle is sufficient, as the Markov chain should
find a stationary distribution regardless of the starting
position. In practice, however, this strategy is less
interactive, as users don?t feel that their constraints
are actually incorporated in the model, and inertia
can keep the chain from reflecting the constraints.
Algorithm 2 MOVE(d, n, wd,n, zd,n = k,?l)
1: If wd,n /? ?old,
P : Pk,wd,n ? Pk,wd,n ? 1, Pk,l ? Pk,l + 1
W : Wk,l,wd,n ?Wk,l,wd,n + 1
2: Else, suppose wd,n ? ?oldm ,
P : Pk,m ? Pk,m ? 1, Pk,l ? Pk,l + 1
W : Wk,m,wd,n ?Wk,m,wd,n ? 1
Wk,l,wd,n ?Wk,l,wd,n + 1
Regardless of what ablation scheme is used, after
the state of the Markov chain is altered, the next
step is to actually run inference forward, sampling
assignments for the unassigned tokens for the ?first?
time and changing the topic assignment of previously
assigned tokens. How many additional iterations are
3This assumes that there is only one possible path in the con-
straint tree that can generate a word; in other words, this as-
sumes that constraints are transitive, as discussed at the end of
Section 2. In the more general case, when words lack a unique
path in the constraint tree, an additional latent variable specifies
which possible paths in the constraint tree produced the word;
this would have to be sampled. All other updating strategies
are immune to this complication, as the assignments are left
unassigned.
required after adding constraints is a delicate tradeoff
between interactivity and effectiveness, which we
investigate further in the next sections.
5 Motivating Example
To examine the viability of ITM, we begin with a
qualitative demonstration that shows the potential
usefulness of ITM. For this task, we used a corpus
of about 2000 New York Times editorials from the
years 1987 to 1996. We started by finding 20 initial
topics with no constraints, as shown in Table 1 (left).
Notice that topics 1 and 20 both deal with Russia.
Topic 20 seems to be about the Soviet Union, with
topic 1 about the post-Soviet years. We wanted to
combine the two into a single topic, so we created a
constraint with all of the clearly Russian or Soviet
words (boris, communist, gorbachev, mikhail, russia,
russian, soviet, union, yeltsin ). Running inference
forward 100 iterations with the Doc ablation strat-
egy yields the topics in Table 1 (right). The two
Russia topics were combined into Topic 20. This
combination also pulled in other relevant words that
not near the top of either topic before: ?moscow?
and ?relations.? Topic 1 is now more about elections
in countries other than Russia. The other 18 topics
changed little.
While we combined the Russian topics, other re-
searchers analyzing large corpora might preserve the
Soviet vs. post-Soviet distinction but combine topics
about American government. ITM allows tuning for
specific tasks.
6 Simulation Experiment
Next, we consider a process for evaluating our ITM
using automatically derived constraints. These con-
straints are meant to simulate a user with a predefined
list of categories (e.g. reviewers for journal submis-
sions, e-mail folders, etc.). The categories grow more
and more specific during the session as the simulated
users add more constraint words.
To test the ability of ITM to discover relevant
subdivisions in a corpus, we use a dataset with pre-
defined, intrinsic labels and assess how well the dis-
covered latent topic structure can reproduce the cor-
pus?s inherent structure. Specifically, for a corpus
with M classes, we use the per-document topic dis-
tribution as a feature vector in a supervised classi-
252
Topic Words
1
election, yeltsin, russian, political, party, democratic, russia, presi-
dent, democracy, boris, country, south, years, month, government, vote,
since, leader, presidential, military
2
new, york, city, state, mayor, budget, giuliani, council, cuomo, gov,
plan, year, rudolph, dinkins, lead, need, governor, legislature, pataki,
david
3
nuclear, arms, weapon, defense, treaty, missile, world, unite, yet, soviet,
lead, secretary, would, control, korea, intelligence, test, nation, country,
testing
4
president, bush, administration, clinton, american, force, reagan, war,
unite, lead, economic, iraq, congress, america, iraqi, policy, aid, inter-
national, military, see
...
20
soviet, lead, gorbachev, union, west, mikhail, reform, change, europe,
leaders, poland, communist, know, old, right, human, washington,
western, bring, party
Topic Words
1
election, democratic, south, country, president, party, africa, lead, even,
democracy, leader, presidential, week, politics, minister, percent, voter,
last, month, years
2
new, york, city, state, mayor, budget, council, giuliani, gov, cuomo,
year, rudolph, dinkins, legislature, plan, david, governor, pataki, need,
cut
3 nuclear, arms, weapon, treaty, defense, war, missile, may, come, test,american, world, would, need, lead, get, join, yet, clinton, nation
4
president, administration, bush, clinton, war, unite, force, reagan, amer-
ican, america, make, nation, military, iraq, iraqi, troops, international,
country, yesterday, plan
...
20
soviet, union, economic, reform, yeltsin, russian, lead, russia, gor-
bachev, leaders, west, president, boris, moscow, europe, poland,
mikhail, communist, power, relations
Table 1: Five topics from a 20 topic topic model on the editorials from the New York times before adding a constraint
(left) and after (right). After the constraint was added, which encouraged Russian and Soviet terms to be in the same
topic, non-Russian terms gained increased prominence in Topic 1, and ?Moscow? (which was not part of the constraint)
appeared in Topic 20.
fier (Hall et al, 2009). The lower the classification
error rate, the better the model has captured the struc-
ture of the corpus.4
6.1 Generating automatic constraints
We used the 20 Newsgroups corpus, which contains
18846 documents divided into 20 constituent news-
groups. We use these newsgroups as ground-truth
labels.5
We simulate a user?s constraints by ranking words
in the training split by their information gain (IG).6
After ranking the top 200 words for each class
by IG, we delete words associated with multiple
labels to prevent constraints for different labels
from merging. The smallest class had 21 words
remaining after removing duplicates (due to high
4Our goal is to understand the phenomena of ITM, not classifica-
tion, so these classification results are well below state of the
art. However, adding interactively selected topics to the state
of the art features (tf-idf unigrams) gives a relative error reduc-
tion of 5.1%, while just adding topics from vanilla LDA gives
a relative error reduction of 1.1%. Both measurements were
obtained without tuning or weighting features, so presumably
better results are possible.
5http://people.csail.mit.edu/jrennie/20Newsgroups/
In preprocessing, we deleted short documents, leaving 15160
documents, including 9131 training documents and 6029 test
documents (default split). Tokenization, lemmatization, and
stopword removal was performed using the Natural Language
Toolkit (Loper and Bird, 2002). Topic modeling was performed
using the most frequent 5000 lemmas as the vocabulary.
6IG is computed by the Rainbow toolbox
http://www.cs.umass.edu/ mccallum/bow/rainbow/
overlaps of 125 words between ?talk.religion.misc?
and ?soc.religion.christian,? and 110 words between
?talk.religion.misc? and ?alt.atheism?), so the top 21
words for each class were the ingredients for our
simulated constraints. For example, for the class
?soc.religion.christian,? the 21 constraint words in-
clude ?catholic, scripture, resurrection, pope, sab-
bath, spiritual, pray, divine, doctrine, orthodox.? We
simulate a user?s ITM session by adding a word to
each of the 20 constraints until each of the constraints
has 21 words.
6.2 Simulation scheme
Starting with 100 base iterations, we perform suc-
cessive rounds of refinement. In each round a new
constraint is added corresponding to the newsgroup
labels. Next, we perform one of the strategies for
state ablation, add additional iterations of Gibbs sam-
pling, use the newly obtained topic distribution of
each document as the feature vector, and perform
classification on the test / train split. We do this for
21 rounds until each label has 21 constraint words.
The number of LDA topics is set to 20 to match the
number of newsgroups. The hyperparameters for all
experiments are ? = 0.1, ? = 0.01, and ? = 100.
At 100 iterations, the chain is clearly not con-
verged. However, we chose this number of iterations
because it more closely matches the likely use case as
users do not wait for convergence. Moreover, while
investigations showed that the patterns shown in Fig-
253
ure 4 were broadly consistent with larger numbers
of iterations, such configurations sometimes had too
much inertia to escape from local extrema. More iter-
ations make it harder for the constraints to influence
the topic assignment.
6.3 Investigating Ablation Strategies
First, we investigate which ablation strategy best al-
lows constraints to be incorporated. Figure 3 shows
the classification error of six different ablation strate-
gies based on the number of words in each constraint,
ranging from 0 to 21. Each is averaged over five dif-
ferent chains using 10 additional iterations of Gibbs
sampling per round (other numbers of iterations are
discussed in Section 6.4). The model runs forward 10
iterations after the first round, another 10 iterations
after the second round, etc. In general, as the number
of words per constraint increases, the error decreases
as models gain more information about the classes.
Strategy Null is the non-interactive baseline that
contains no constraints (vanilla LDA), but runs infer-
ence for a comparable number of rounds. All Initial
and All Full are non-interactive baselines with all
constraints known a priori. All Initial runs the model
for the only the initial number of iterations (100 it-
erations in this experiment), while All Full runs the
model for the total number of iterations added for the
interactive version. (That is, if there were 21 rounds
and each round of interactive modeling added 10 iter-
ations, All Full would have 210 iterations more than
All Initial).
While Null sees no constraints, it serves as an
upper baseline for the error rate (lower error being
better) but shows the effect of additional inference.
All Full is a lower baseline for the error rate since
it both sees the constraints at the beginning and also
runs for the maximum number of total iterations. All
Initial sees the constraints before the other ablation
techniques but it has fewer total iterations.
The Null strategy does not perform as well as
the interactive versions, especially with larger con-
straints. Both All Initial and All Full, however, show
a larger variance (as denoted by error bands around
the average trends) than the interactive schemes. This
can be viewed as akin to simulated annealing, as the
interactive search has more freedom to explore in
early rounds. As more constraint words are added
each round, the model is less free to explore.
Words per constraint
Error
0.380.40
0.420.44
0.460.48
0.50
0 5 10 15 20
StrategyAll FullAll InitialDocNoneNullTerm
Figure 3: Error rate (y-axis, lower is better) using different
ablation strategies as additional constraints are added (x-
axis). Null represents standard LDA, as the unconstrained
baseline. All Initial and All Full are non-interactive, con-
strained baselines. The results of None, Term, Doc are
more stable (as denoted by the error bars), and the error
rate is reduced gradually as more constraint words are
added.
The error rate of each interactive ablation strategy
is (as expected) between the lower and upper base-
lines. Generally, the constraints will influence not
only the topics of the constraint words, but also the
topics of the constraint words? context in the same
document. Doc ablation gives more freedom for the
constraints to overcome the inertia of the old topic
distribution and move towards a new one influenced
by the constraints.
6.4 How many iterations do users have to wait?
Figure 4 shows the effect of using different numbers
of Gibbs sampling iterations after changing a con-
straint. For each of the ablation strategies, we run
{10, 20, 30, 50, 100} additional Gibbs sampling iter-
ations. As expected, more iterations reduce error,
although improvements diminish beyond 100 itera-
tions. With more constraints, the impact of additional
iterations is lessened, as the model has more a priori
knowledge to draw upon.
For all numbers of additional iterations, while the
Null serves as the upper baseline on the error rate
in all cases, the Doc ablation clearly outperforms
the other ablation schemes, consistently yielding a
lower error rate. Thus, there is a benefit when the
model has a chance to relearn the document context
when constraints are added. The difference is even
larger with more iterations, suggesting Doc needs
more iterations to ?recover? from unassignment.
The luxury of having hundreds or thousands of
additional iterations for each constraint would be im-
254
Words per constraint
Err
or
0.40
0.42
0.44
0.46
0.48
0.50
 10
0 5 10 15 20
 20
0 5 10 15 20
 30
0 5 10 15 20
 50
0 5 10 15 20
100
0 5 10 15 20
Strategy
Doc
None
Null
Term
Figure 4: Classification accuracy by strategy and number of additional iterations. The Doc ablation strategy performs
best, suggesting that the document context is important for ablation constraints. While more iterations are better, there
is a tradeoff with interactivity.
practical. For even moderately sized datasets, even
one iteration per second can tax the patience of in-
dividuals who want to use the system interactively.
Based on these results and an ad hoc qualitative ex-
amination of the resulting topics, we found that 30
additional iterations of inference was acceptable; this
is used in later experiments.
7 Getting Humans in the Loop
To move beyond using simulated users adding the
same words regardless of what topics were discov-
ered by the model, we needed to expose the model
to human users. We solicited approximately 200
judgments from Mechanical Turk, a popular crowd-
sourcing platform that has been used to gather lin-
guistic annotations (Snow et al, 2008), measure topic
quality (Chang et al, 2009), and supplement tradi-
tional inference techniques for topic models (Chang,
2010). After presenting our interface for collecting
judgments, we examine the results from these ITM
sessions both quantitatively and qualitatively.
7.1 Interface for soliciting refinements
Figure 5 shows the interface used in the Mechanical
Turk tests. The left side of the screen shows the
current topics in a scrollable list, with the top 30
words displayed for each topic.
Users create constraints by clicking on words from
the topic word lists. The word lists use a color-coding
scheme to help the users keep track of which words
they are currently grouping into constraints. The right
side of the screen displays the existing constraints.
Users can click on icons to edit or delete each one.
The constraint currently being built is also shown.
Figure 5: Interface for Mechanical Turk experiments.
Users see the topics discovered by the model and select
words (by clicking on them) to build constraints to be
added to the model.
Clicking on a word will remove that word from the
current constraint.
As in Section 6, we can compute the classification
error for these users as they add words to constraints.
The best users, who seemed to understand the task
well, were able to decrease classification error. (Fig-
ure 6). The median user, however, had an error re-
duction indistinguishable from zero. Despite this, we
can examine the users? behavior to better understand
their goals and how they interact with the system.
7.2 Untrained users and ITM
Most of the large (10+ word) user-created constraints
corresponded to the themes of the individual news-
groups, which users were able to infer from the
discovered topics. Common constraint themes that
255
Round
Re
lat
ive
 E
rro
r
0.94
0.96
0.98
1.00
0 1 2 3 4
Best Session
 10 Topics 
 20 Topics 
 50 Topics 
 75 Topics 
Figure 6: The relative error rate (using round 0 as a base-
line) of the best Mechanical Turk user session for each of
the four numbers of topics. While the 10-topic model does
not provide enough flexibility to create good constraints,
the best users could clearly improve classification with
more topics.
matched specific newsgroups included religion, space
exploration, graphics, and encryption. Other com-
mon themes were broader than individual news-
groups (e.g. sports, government and computers). Oth-
ers matched sub-topics of a single newsgroup, such
as homosexuality, Israel or computer programming.
Some users created inscrutable constraints, like
(?better, people, right, take, things?) and (?fbi, let,
says?). They may have just clicked random words to
finish the task quickly. While subsequent users could
delete poor constraints, most chose not to. Because
we wanted to understand broader behavior we made
no effort to squelch such responses.
The two-word constraints illustrate an interesting
contrast. Some pairs are linked together in the corpus,
like (?jesus, christ?) and (?solar, sun?). With others,
like (?even, number?) and (?book, list?), the users
seem to be encouraging collocations to be in the
same topic. However, the collocations may not be in
any document in this corpus. Another user created a
constraint consisting of male first names. A topic did
emerge with these words, but the rest of the words
in that topic seemed random, as male first names are
not likely to co-occur in the same document.
Not all sensible constraints led to successful topic
changes. Many users grouped ?mac? and ?windows?
together, but they were almost never placed in the
same topic. The corpus includes separate newsgroups
for Macintosh and Windows hardware, and divergent
contexts of ?mac? and ?windows? overpowered the
prior distribution.
The constraint size ranged from one word to over
40. In general, the more words in the constraint,
the more likely it was to noticeably affect the topic
distribution. This observation makes sense given
our ablation method. A constraint with more words
will cause the topic assignments to be reset for more
documents.
8 Discussion
In this work, we introduced a means for end-users
to refine and improve the topics discovered by topic
models. ITM offers a paradigm for non-specialist
consumers of machine learning algorithms to refine
models to better reflect their interests and needs. We
demonstrated that even novice users are able to under-
stand and build constraints using a simple interface
and that their constraints can improve the model?s
ability to capture the latent structure of a corpus.
As presented here, the technique for incorporating
constraints is closely tied to inference with Gibbs
sampling. However, most inference techniques are
essentially optimization problems. As long as it is
possible to define a transition on the state space that
moves from one less-constrained model to another
more-constrained model, other inference procedures
can also be used.
We hope to engage these algorithms with more
sophisticated users than those on Mechanical Turk
to measure how these models can help them better
explore and understand large, uncurated data sets. As
we learn their needs, we can add more avenues for
interacting with topic models.
Acknowledgements
We would like to thank the anonymous reviewers, Ed-
mund Talley, Jonathan Chang, and Philip Resnik for
their helpful comments on drafts of this paper. This
work was supported by NSF grant #0705832. Jordan
Boyd-Graber is also supported by the Army Research
Laboratory through ARL Cooperative Agreement
W911NF-09-2-0072 and by NSF grant #1018625.
Any opinions, findings, conclusions, or recommenda-
tions expressed are the authors? and do not necessar-
ily reflect those of the sponsors.
256
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In Proceedings of
International Conference of Machine Learning.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Proceedings of Advances in Neural
Information Processing Systems.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet alocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean
Gerrish, and David M. Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
Jonathan Chang. 2010. Not-so-latent Dirichlet alocation:
Collapsed Gibbs sampling using human judgments. In
NAACL Workshop: Creating Speech and Language
Data With Amazon?ss Mechanical Turk.
Hal Daume? III. 2009. Markov random topic fields. In
Proceedings of Artificial Intelligence and Statistics.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of International Conference of Machine Learn-
ing.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Proceedings of the National Academy
of Sciences, 101(Suppl 1):5228?5235.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov models. In Artificial Intelligence
and Statistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1):10?18.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence.
Thomas K. Landauer, Danielle S. McNamara, Dennis S.
Marynick, and Walter Kintsch, editors. 2006. Proba-
bilistic Topic Models. Laurence Erlbaum.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching.
Radford M. Neal. 1993. Probabilistic inference using
Markov chain Monte Carlo methods. Technical Report
CRG-TR-93-1, University of Toronto.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Conference of the North American Chapter of
the Association for Computational Linguistics.
Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In Association for the Advancement of
Artificial Intelligence.
James Petterson, Smola Alex, Tiberio Caetano, Wray Bun-
tine, and Narayanamurthy Shravan. 2010. Word fea-
tures for latent Dirichlet alocation. In Neural Informa-
tion Processing Systems.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of Emperical Methods
in Natural Language Processing.
Fergus Rob, Li Fei-Fei, Perona Pietro, and Zisserman An-
drew. 2005. Learning object categories from Google?s
image search. In International Conference on Com-
puter Vision.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In Proceedings of Uncertainty
in Artificial Intelligence.
Suyash Shringarpure and Eric P. Xing. 2008. mStruct:
a new admixture model for inference of population
structure in light of both genetic admixing and allele
mutations. In Proceedings of International Conference
of Machine Learning.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast?but is it good? Evalu-
ating non-expert annotations for natural language tasks.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
Hanna Wallach, David Mimno, and Andrew McCallum.
2009. Rethinking LDA: Why priors matter. In Pro-
ceedings of Advances in Neural Information Processing
Systems.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of International Conference
of Machine Learning.
Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.
257
