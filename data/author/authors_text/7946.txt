Characterizing and Predicting Corrections
in Spoken Dialogue Systems
Diane Litman?
University of Pittsburgh
Julia Hirschberg?
Columbia University
Marc Swerts?
Tilburg University
This article focuses on the analysis and prediction of corrections, defined as turns where a
user tries to correct a prior error made by a spoken dialogue system. We describe our labeling
procedure of various corrections types and statistical analyses of their features in a corpus
collected from a train information spoken dialogue system. We then present results of machine-
learning experiments designed to identify user corrections of speech recognition errors. We
investigate the predictive power of features automatically computable from the prosody of the
turn, the speech recognition process, experimental conditions, and the dialogue history. Our
best-performing features reduce classification error from baselines of 25.70?28.99% to 15.72%.
1. Introduction
Compared to many other systems, spoken dialogue systems (SDS) tend to have more
difficulties in correctly interpreting user input. Whereas a car normally goes left if the
driver turns the steering wheel in that direction or a vacuum cleaner starts working if
one pushes the on button, interactions between a user and a spoken dialogue system
are often hampered by mismatches between the action intended by the user and the
action executed by the system. Such mismatches are mainly due to errors in the Auto-
matic Speech Recognition (ASR) and/or the Natural Language Understanding (NLU)
component of these systems; they can also be due to wrong default assumptions of the
system or the fact that a user asks out-of-topic questions for which the system was not
designed. To solve these mismatches, users often have to put considerable effort into
trying to make it clear to the system that there was a problem, and trying to correct it by
reentering misrecognized or misinterpreted information. Previous research has already
brought to light that it is not always easy for users to determine whether their intended
actions were carried out correctly or not, in particular when the dialogue system does
not give appropriate feedback about its internal representation at the right moment.
In addition, users? corrections may miss their goal because corrections themselves are
more difficult for the system to recognize and interpret correctly, which may lead to
so-called cyclic (or spiral) errors.
? E-mail: litman@cs.pitt.edu.
? E-mail: julia@cs.columbia.edu.
? E-mail: m.g.j.swerts@uvt.nl.
Submission received: 12 January 2005; revised submission received: 3 April 2006; accepted for publication:
4 May 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
Given that current spoken dialogue systems are not sufficiently robust, there is
need for sophisticated error-handling strategies to gracefully solve communication
problems between the system and the user. Ideally, apart from strategies to prevent
errors, error handling would consist of steps to immediately detect an error when
it occurs and to interact with the user to correct the error in subsequent exchanges.
To date, attempts to improve system performance have largely focused on improv-
ing ASR accuracy or simplifying the task, either by further constraining the domain
and functionality of the system or by further restricting the vocabulary the system
must recognize. Such studies include work on improved acoustic and semantic con-
fidence scores (Ammicht, Potamianos, and Fosler-Lussier 2001; Andorno, Laface, and
Gemello 2002; Bouwman, Sturm, and Boves 1999; Falavigna, Gretter, and Riccardi
2002; Guillevic, Gandrabur, and Normandin 2002; Moreno, Logan, and Raj 2001; Wang
and Lin 2002; Zhang and Rudnicky 2001), on new system architectures for error han-
dling (McTear et al 2005; Prodanov and Drygajlo 2005; Torres et al 2005), on new
interfaces that are more user-friendly for error recovery (Bulyko et al 2005; Karsenty
and Botherel 2005; Sturm and Boves 2005), and on the use of error-recovery strategies
that are based on analyses of human?human dialogues (Skantze 2005), including the
use of facial expressions (Barkhuysen, Krahmer, and Swerts 2005).
However, as ASR accuracy improves, dialogue systems will be called upon to
handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely
that spoken dialogue systems will, for the foreseeable future, always require effective
error detection and repair strategies. In previous research (Hirschberg, Litman, and
Swerts 1999, 2004), we identified new procedures to detect recognition errors, which
perform well when tested on two different corpora, the TOOT and W99 corpora (train
information and conference registration dialogues) collected using two different ASR
systems (Sharp et al 1997; Kamm et al 1997). We found that prosodic features, in
combination with information already available to the recognizer, such as acoustic
confidence scores, grammar, and recognized string, can distinguish speaker turns that
are misrecognized far better than traditional methods for ASR rejection (the system
decision that its hypothesis is so weak that it should reprompt for fresh input), which
use acoustic confidence scores alone. Related work has been done by Lendvai (2004)
and Batliner et al (2003). In the current study, we turn to the question of how people try
to correct ASR errors in their interactions with machines and the role that prosody may
play in identifying user corrections and in helping to analyze them.
Understanding how users attempt to correct system failures and why their attempts
succeed or fail is important to improve the design of future spoken dialogue systems.
For example, knowing whether they are more likely to repeat or rephrase their utter-
ances, add new information or shorten their input, and how system behavior influences
these choices can suggest appropriate on-line modifications to the system?s interaction
strategy or to the recognition procedure it employs. Determining which speaker behav-
iors are more successful in correcting system errors can also lead to improvements in the
help information such systems provide. There is growing evidence that there is much
variance in the way people react to system errors and that the variance can be explained
on the basis of particular properties of the dialogue system or the dialogue context. In
particular, dialogue confirmation strategies may hinder users? ability to correct system
error. For instance, if a system wrongly presents information as being correct, as when it
verifies information implicitly, users become confused about how to respond (Krahmer
et al 2001). Other studies have shown that speakers tend to switch to a prosodically
?marked? speaking style after communication errors, comparing repetition corrections
with the speech being repeated (Wade, Shriberg, and Price 1992; Oviatt et al 1996;
418
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Levow 1998; Bell and Gustafson 1999). Although this speaking style may be effective in
problematic human?human communicative settings, there is evidence that suggests it
leads to further errors in human?machine interactions (Levow 1998; Soltau and Waibel
2000). That corrections are difficult for ASR systems is generally explained by the fact
that they tend to be hyperarticulated?higher, louder, longer?than other turns (Wade,
Shriberg, and Price 1992; Oviatt et al 1996; Levow 1998; Bell and Gustafson 1999;
Shimojima, et al 2001; Soltau and Waibel 1998, 2000; Soltau, Metze, and Waibel 2002),
where ASR models are not well adapted to handle this special speaking style, although
recent studies suggest that ASR systems are becoming less vulnerable to hyperarticula-
tion (Bulyko et al 2005).
So, repair strategies in human?machine interactions can be more or less effective.
Therefore, increased knowledge about the efficiency of different correction strategies
can lead to a number of possible courses of action. System strategy might be chosen to
favor the type(s) of correction the system can most easily process. Or, having chosen a
particular interaction strategy, the system repair strategy might be tuned to handle the
correction types that that strategy is likely to produce. Alternatively, the system?s dia-
logue manager might use the detection of corrections as a signal that it should modify its
interaction strategy, either locally, by beginning a subdialogue for faster error recovery,
or globally, by changing its initiative or confirmation strategies, or even directing the
user to a human operator. Or, since corrections are often hyperarticulated, detection of
a correction could serve as a signal to the ASR engine to run a recognizer trained on
hyperarticulated speech in parallel with its normal processor, to better transcribe the
speech. All of these possibilities, however, assume that user corrections can be detected
by the system reliably during the dialogue.
In this article, we describe an analysis of user corrections of system error collected in
the TOOT spoken dialogue system. In the next section, we describe the corpus itself and
how it was collected and labeled. The corpus is suitable to gain insight into the different
correction strategies that speakers exploit in different dialogue contexts and interaction
styles. Then, we characterize the nature of corrections in this corpus in terms of when
they occur, how well they are handled by the system, what distinguishes their prosody
from other utterances, their relationship to the utterances they correct, and how they
differ according to dialogue strategy. Then we present results of some machine-learning
experiments designed to automatically distinguish corrections from other user input,
using features that we derived as potentially useful from our descriptive analyses.
2. The Data
2.1 The TOOT Corpus
Our corpus consists of dialogues between human subjects and TOOT, a spoken dialogue
system that allows access to train information from the Web via telephone. TOOT
was collected to study variations in dialogue strategy and in user-adapted interac-
tion (Litman and Pan 1999). It is implemented using an interactive voice response
(IVR) platform developed at AT&T, combining ASR and text-to-speech with a phone
interface (Kamm et al 1997). The system?s speech recognizer is a speaker-independent,
hidden Markov model system with context-dependent phone models for telephone
speech and constrained (rule-based) grammars defining vocabulary at any dialogue
state. Whereas a ?universal? grammar specifying all legal utterances was used at some
points in the dialogue, seven smaller grammars were also used at many points in the
dialogue (e.g., to recognize city names, days of the week, answers to yes/no questions,
419
Computational Linguistics Volume 32, Number 3
etc.). Grammars were only written for originally expected answers; in other words,
no specific grammar for corrections was built in.1 Confidence scores for recognition
were available only at the turn level and were based on acoustic likelihoods; thresh-
olds for rejecting an utterance based on confidence scores were specified manually by
the system designers and were set differently for different grammars. The platform
supports barge-in. Subjects performed four tasks with one of several versions of the
system that differed in terms of locus of initiative (system, user, or mixed), confirmation
strategy (explicit, implicit, or none), and whether these conditions could be changed
by the user during the task (adaptive vs. non-adaptive). In the adaptive version of
the system, users were allowed to say change strategy at any point(s) in the dialogue.
TOOT would then ask the user to specify new initiative and confirmation strategies,
for example, You are using the no confirmation strategy. Which confirmation strategy do
you want to change to? No confirmation, implicit confirmation, or explicit confirmation?
TOOT?s initiative strategy specifies who has control of the dialogue, whereas TOOT?s
confirmation strategy specifies how and whether TOOT lets the user know what it just
understood. The fragments in Figure 1 provide some illustrations of how dialogues
vary with strategy. For example, in user initiative mode, the system allows the user
to specify any number of attributes in a single utterance. Thus, the system will let the
user ignore specific questions. In the example in Figure 1, although the system asks for
the day of the week, the user answers with the time, which can be recognized due to the
use of the ?universal grammar.? In contrast, in both mixed and system initiative mode,
when a specific question is asked, one of the restricted grammars is used to recognize
the response. Finally, in universal and mixed but not system initiative mode, the sys-
tem can ask both specific questions and open-ended questions (e.g., How may I help
you?). Subjects were 39 students: 20 native speakers and 19 non-native, 16 women
and 23 men. Dialogues were recorded and system and user behavior were logged
automatically. The concept accuracy (CA) of each turn was manually labeled. If the
ASR correctly captured all task-related information in the turn (e.g., time, departure,
and arrival cities), the turn?s CA score was 1 (semantically correct). Otherwise, the CA
score reflected the percentage of correctly recognized task information in the turn. The
dialogues were also transcribed and automatically scored in comparison to the ASR
recognized string (the best hypothesis output by the ASR engine) to produce a word
error rate (WER) for each turn. For the study described below, we examined 2,328 user
turns (all user input between two system inputs) from 152 dialogues.
2.2 Labeling
To identify corrections in the corpus two authors independently labeled each turn as
to whether or not it constituted a correction of a prior system failure (a rejection or CA
error, which were the only system failure subjects were aware of) and subsequently
decided upon a consensus label. Note that many of the discrepancies between labels
were due to tiredness or incidental sloppiness of individual annotators, rather than true
disagreement. Each turn labeled ?correction? was further classified as belonging to one
of the following categories: REP (repetition, including repetitions with differences in
pronunciation or fluency), PAR (paraphrase), ADD (task-relevant content added), OMIT
1 Thus, if the system prompted for a single city, but the user also included a correction of a prior utterance
(e.g., No, at 10:30 p.m. I want to go to New York City), the turn would be out of grammar. Coding our
corpus for out of vocabulary turns and examining whether corrections are more likely to be out of
grammar is an area for future work.
420
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Figure 1
Illustrations of various dialogue strategies in TOOT.
(content omitted), and ADD/OMIT (content both added and omitted). Repetitions
were further divided into repetitions with pronunciation variation (PRON) (e.g., yes
correcting yeah) and repetitions where the correction was pronounced using the same
pronunciation as the original turn, but this distinction was difficult to make and turned
out not to be useful. User turns that included both corrections and other speech acts
were so distinguished by labeling them ?2+.? For example, the turn I would like to go
to Chicago from Baltimore change strategies system contains not only an ADD correction,
but also a request to adapt the system?s dialogue strategies, followed by an inform
of the desired initiative value. As another example, the turn yes help contains a REP
correction, followed by a request for help. For user turns containing a correction plus
one or more additional dialogue acts, only the correction is used for purposes of analysis
below. We also labeled as ?restarts? user corrections that followed non-initial system-
initial prompts (e.g., How may I help you? or What city do you want to go to?); in such
cases, system and user essentially started the dialogue over from the beginning.2 Table 1
shows examples of each correction type and additional label for corrections of system
failures on I want to go to Boston on Sunday. Note that the utterance on the last line of this
figure is labeled 2+PAR, given that this turn consists of two speech acts: The goal of the
no-part of this turn is to signal a problem, whereas the remainder of this turn serves to
correct a prior error.
Each correction was also indexed with an identifier representing the closest prior
turn it was correcting, so that we could investigate ?chains? of corrections of a single
failed turn by tracing back through subsequent corrections of that turn. Figure 2 shows
a fragment of a TOOT dialogue with corrections labeled as discussed above.
3. Descriptive Analyses
This section presents the results of some descriptive analyses of the corrections we
labeled in the TOOT corpus. We provide data on the distribution of different correction
2 Restarts occurred when either the user said the phrase I?m done here at any point in the dialogue, or
answered no to the system?s request to perform a database query (e.g., Do you want me to find the trains
from Baltimore to Chicago on Tuesday around 8:45 now?).
421
Computational Linguistics Volume 32, Number 3
Table 1
Example corrections of I want to go to Boston on Sunday.
Corr Type Correction
REP I want to go to Boston on Sunday
PAR To Boston on Sunday
OMIT I want to go to Boston
ADD To Boston on Sunday at 8 p.m.
ADD/OMIT I want to arrive Sunday at 8 p.m.
2+PAR No, to Boston on Sunday
types, prosodic features of corrections, characteristics of correction chains, and variation
in features of corrections as a function of dialogue strategy.
3.1 Correction Types
Of the correction types we labeled, the largest numbers were REPs and OMITs, as
shown in Table 2, which shows overall distribution of correction types, and distri-
butions for each type of system failure corrected, following either a misrecognized
turn (with respect to concept accuracy) (Post-Misrec) or a rejected turn (Post-Rej) or
correcting an earlier system failure (Non-Immed). (The last group includes corrections
of earlier utterances that do not immediately follow a rejection or misrecognition.)
Table 2 shows that 39% of TOOT corrections were simple repetitions of a previously
rejected or misrecognized turn. Although this strategy is often suboptimal in correcting
ASR errors (Levow 1998), REPs (45% WER error) and OMITs (52% error) were better
recognized than ADDs (90% WER error) and PARs (72% WER error).
Figure 2
Toot dialogue fragment with correction labels.
422
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Table 2
Distribution of correction types.
Type ADD ADD/OMIT OMIT PAR REP N
Total 51 8% 14 2% 215 32% 127 19% 265 39% 672
Post-Misrec 39 7% 13 3% 203 40% 90 18% 173 32% 518
Post-Rej 8 6% 0 0% 9 7% 36 28% 75 59% 128
Non-Immed 4 15% 1 4% 3 12% 1 4% 17 65% 26
There was no significant difference either in the number of corrections produced
(? = 2.44, p = .12) or in correction type (?2 = 5.07, p = .28) between our native speaker
subjects and non-native speakers. However, what the user was correcting did influence
the type of correction chosen. Table 2 shows that corrections of misrecognitions (Post-
Misrec) were more likely to omit information present in the original turn (OMITs),
whereas corrections of rejections (Post-Rej) were more likely to be simple repetitions.
The latter finding is not surprising because the rejection message for tasks was always
a close paraphrase of Sorry, I can?t understand you. Can you please repeat your utterance?
However, it does suggest the surprising power of system directions and how impor-
tant it is to craft prompts to favor the type of correction most easily recognized by
the system.
3.2 Prosodic Features of Corrections
In part to test the hypothesis that corrections tend to be hyperarticulated (slower and
louder speech that contains wider pitch excursions and more internal silence), we
examined the following features for each user turn: maximum and mean fundamental
frequency values (f0 Max, f0 Mean); maximum and mean energy values (RMS Max,
RMS Mean); total duration; length of pause preceding the turn (Prior Pause); speaking
rate (Tempo), calculated in syllables per second (sps); and amount of silence within
the turn (% Silence).3 f0 and RMS values, representing measures of pitch excursion
and loudness, were calculated from the output of Entropic Research Laboratory?s pitch
tracker, get f0 (Talkin 1995), with no postcorrection. Timing variation was represented
by four features: Duration of turn and length of pause between turns was hand labeled.
Speaking rate was approximated in terms of syllables in the recognized string per
second. % Silence was defined as the percentage of zero frames in the turn, calculated
from the pitch track; this feature approximates the percentage of time within the turn
that the speaker was silent.
To ensure that our results were speaker independent, we calculated mean val-
ues for each speaker?s corrections and non-corrections for every feature. Then, for
each feature, we created vectors of speaker means for correction and non-correction
turns and performed paired t tests on the paired vectors. For example, for the feature
?f0 Max,? we calculated mean maxima for correction turns and for non-corrections
for each of our thirty-nine speakers. We then performed a paired t test on these
3 Although the features were automatically computed, turn beginnings and endings were hand segmented
in dialogue-level speech files, as the turn-level files created by TOOT were not available. Because of some
system/user overlap in the recordings, we were able to calculate prosodic features for only 1,975 user
turns.
423
Computational Linguistics Volume 32, Number 3
thirty-nine pairs of means to derive speaker-independent results for differences in
f0 maxima between corrections and non-corrections. Note, however, that there were
overall differences in the corrections produced by native and non-native speakers,
normalized by value of first turn in task: mean f0 was higher for native speakers than
for non-native speakers (t stat = ?2.72, df = 602, p = .0067), tempo was faster (t stat =
?3.18, df = 670, p = .0015), and duration was shorter (t stat = 2.20, df = 670, p = .028).
These differences do not occur in non-correction utterances.
Our results provide some explanation for why corrections are more poorly re-
cognized than non-corrections because they indicate that corrections are indeed
characterized by prosodic features associated with hyperarticulation. Table 3 shows
that corrections differ from other turns in that they are longer, louder, higher in
pitch excursion, follow longer pauses, and contain less internal silence than non-
corrections. All but the latter difference supports the hypothesis that corrections tend
to be hyperarticulated.
To confirm this hypothesis, two of the authors labeled each turn in the corpus for
evidence of perceptual hyperarticulation, following (Wade, Shriberg, and Price 1992).
Fifty-two percent of corrections in the corpus have some perceptual hyperarticulation,
compared with only 12% of other turns. Too, hyperarticulated corrections are more
likely to be misrecognized than other corrections (70% misrecognitions vs. 52%). How-
ever, it is important to note that only 59% of misrecognized corrections in the corpus are
also hyperarticulated, so recognition failure for a considerable portion of corrections
must be explained in some other way. There is still a large number of misrecognized
corrections that show no perceptual evidence of hyperarticulation.
In our earlier analysis of prosodic differences between correct and incorrectly
recognized turns (Hirschberg, Litman, and Swerts 2004), we also found that misrecog-
nized turns differed from correctly recognized turns in f0, loudness, duration, and
timing?all features associated with hyperarticulation. In addition, more misrecogni-
tions are hyperarticulated than are correctly recognized turns. But when we excluded
perceptually hyperarticulated turns from our prosodic analysis, we found that mis-
recognized turns were still prosodically different from correctly recognized turns, in
the same ways. We hypothesized there that misrecognitions might exhibit tendencies
toward hyperarticulation that are imperceptible to human listeners, but not to ASR
engines. The same may also be true of non-hyperarticulated, but still prosodically
distinct corrections. When we exclude hyperarticulated utterances from our corpus
Table 3
Corrections versus non-corrections by prosodic feature.
Feature t stat Mean corr p
- non-corr
f0 Max* 3.79 17.76 Hz < .001
f0 Mean 0.23 ?4.12 Hz .823
RMS Max* 4.88 347.75 < .001
RMS Mean* 2.57 63.44 .014
Duration* 6.68 1.16 sec < .001
Prior pause* 2.17 0.186 sec .036
Tempo 1.78 ?0.15 sps .246
% Silence* 4.75 ?0.05% < .001
*Significant at a 95% confidence level (p ? .05)
424
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
and reanalyze prosodic features of corrections versus non-corrections, we find signif-
icant differences in duration, rms maximum, rms mean, tempo, and amount of turn-
internal silence as we did with the corpus as a whole. So, again, even when corrections
are not perceptibly hyperarticulated, they share some acoustic tendencies with turns
that are.
3.3 Correction Chains
As noted above, corrections in the TOOT corpus often take the form of chains of
corrections of a single original error. Looking back at Figure 2, for example, we see
two chains of corrections: In the first, which begins with the misrecognition of turn 776
(Um, tomorrow), the user repeats the original phrase and then provides a paraphrase
(Saturday), which is correctly recognized. In the second, beginning with turn 780, the
time of departure is misrecognized. The user omits some information (a.m.) in turn
781, but without success; an ADD correction follows, with the previously omitted
information restored, in turn 783.
Distance of a correction from the original misrecognized turn?whether calculated
as position in chain (e.g., Saturday in Figure 2 is the second in the chain correcting
turn 776) or further in number of turns from that original error (e.g., Saturday here is
also two turns from the original error)?correlates significantly with prosodic variation.
An analysis of the relationship between both distance measures and our prosodic
features (using Pearson?s product?moment correlation) shows significant correlations
of distance in chain or from original error with f0 maximum (r = .20, p < .001; r = .21,
p < .001) and mean (r = .27, p < .001; r = .29, p < .001), rms maximum (r =?.09, p < .02;
r = ?.12, p < .005) and mean (r = ?.12, p < .0025; r = ?.16, p < .001), absolute duration
(r = .14, p < 0; r = .16, p < .001) and duration in number of words (r = .11, p < .01; r =
.12, p < .005), length of preceding pause (r = .11, p < .005; r = .10, p < .01), and speaking
rate (r = ?.05, p < .01; r = ?.10, p < .02). The more distant a correction is, in short, the
higher it is in pitch, the softer it is, the longer it is, the greater is its preceding pause, and
the more slowly it is spoken. In addition, more distant corrections are also more likely
to be misrecognized; for distance in turns there is a (negative) significant correlation for
concept accuracy (r = ?.13, p < .001), whereas both word and concept accuracy decline
significantly by position in chain (r = ?.08, p < .05; r = ?.15, p < .001). Table 4 shows
the mean concept accuracy of corrections for chain position through 8 (higher numbers
are very small) in the corpus. So, as speakers must try again and again to correct an
error, their attempts appear to become ever less likely to succeed, perhaps because their
prosodic behavior changes in ways that do not help the recognition process. Curiously,
however, our perceptual measure of hyperarticulation is not significantly correlated
with either of these distance measures. So, although speakers modify their speech in
ways generally consistent with hyperarticulation, their corrections do not necessarily
become more hyperarticulated as their attempts to correct continue. Another curious
Table 4
Mean concept accuracy by correction position in Chain.
Position 1 2 3 4 5 6 7 8
N 311 143 84 49 25 15 10 4
Error .43 .57 .63 .51 .60 .87 .70 1.00
425
Computational Linguistics Volume 32, Number 3
finding is that corrections that are more distant from the turn they immediately correct
(e.g., in Figure 2, turn 783 is more distant from the turn it corrects (781) than turn 781 is
from the turn it corrects, which is 780) tend to be more accurately recognized than turns
that are closer. Yet, prosodically, these turns are very like distant turns in a chain or from
the original error, being higher in f0 maximum and mean, lower in rms maximum and
mean, and longer in seconds and number of words. So, in the one case these prosodic
changes might be thought to lead to recognition error, where in the other they occur
with better recognized corrections.
3.4 Variation by Dialogue Strategy
Dialogue strategy clearly affects the type of correction users make and whether it
is successful or not. For example, users more frequently repeat their misrecognized
utterance in the SystemExplicit (75% of corrections are repetitions) condition than in
the MixedImplicit or UserNoConfirm (both 37% REP); the latter conditions have larger
proportions of OMITs and paraphrases. Perhaps this disparity is partly explained by the
larger proportion of corrections that follow rejections in the SystemExplicit condition
(39% vs. 22% and 19%). Overall, SystemExplicit turns are rejected 6% of the time,
whereas the other conditions have about 10% rejections. Table 5 shows differences in
mean length of tasks, number of corrections, number of misrecognitions, and number
of misrecognized corrections by dialogue strategy. Again, misrecognitions were defined
in terms of concept accuracy (turns with CA < 1); misrecognized corrections refer to
the intersection of user terms that were coded as both corrections and misrecognitions.
The fewer misrecognitions, corrections, and misrecognized corrections per task in the
SystemExplicit condition may well explain user ratings of the various systems (non-
adapt) in the original experiments (Litman and Pan 1999): When asked to say whether
they would be likely to use such a system in the future, on a 1?5 scale, subjects scored
SystemExplicit 3.5, MixedImplicit 2.6, and UserNoConfirm 1.7. User satisfaction scores
were similar: Where 40 is the highest score, users gave SystemExplicit 31.25, Mixed-
Implicit 24, and UserNoConfirm 22.1. So, SystemExplicit is preferred by users, even
though MixedImplicit on average takes fewer turns to accomplish a task, suggesting
that the large number of misrecognitions and consequent need for correction has a large
impact on user preferences. This is consistent with performance functions derived from
evaluations of TOOT (Litman and Pan 1999).
Perhaps because correction chains often end unsuccessfully, users frequently
?restart? a task within a session. Most restarts occurred in the MixedImplicit and
UserNoConfirm conditions and were rarely successful. In non-adaptive tasks, 42% of
corrections in the MixedImplicit condition were restarts and 31% in the UserNoConfirm,
Table 5
Corrections by system strategy.
Means SystemExplicit MixedImplicit UserNoConfirm
per task
# Turns 13.4 11.7 16.2
# Corrs 1.3 4.6 7.1
# Misrecs 2.8 6.4 9.4
# Misrec?d Corrs 0.3 3.2 4.8
426
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
whereas none occurred in the SystemExplicit condition. Restarts were misrecognized
77% of the time, compared to 65% of first turns in task. They thus seem to have been
a worse strategy than initiating a new task and might prove a useful diagnostic for
changing system strategy?or summoning a human operator.
4. Predicting Corrections
The previous section showed that corrections differ significantly from non-corrections
prosodically, being higher in pitch, louder, longer, with longer pauses preceding them
and less internal silence. In addition, they are misrecognized more frequently than
non-corrections?although they are no more likely to be rejected by the system. And
corrections more distant from the error they correct tend to exhibit greater prosodic
differences and are recognized more poorly, suggesting that users are not learning to
modify their own behavior to improve system performance. So, dealing with corrections
is a particularly difficult task for both users and systems. We also found that system dia-
logue strategy?the amount of initiative users are allowed to exercise in controlling the
flow of the dialogue and the type of confirmation strategy the system adopts?affects
users? choice of correction type (e.g., directly repeating vs. paraphrasing misrecognized
information). In the following, we turn to the question of identifying user corrections
automatically, from prosodic features as well as other features that are readily available
to a spoken dialogue system. In Section 4.1, we describe the features we use for our
machine-learning experiments. Section 4.2 presents the results of those experiments.
Section 4.3 presents further experiments using additional classifications and features,
motivated by our descriptive results. In the final section, we summarize our conclusions
and describe future research directions.
4.1 Features
In this section we describe the features used in the machine-learning experiments
and the motivation behind their selection. The entire feature set is presented in
Figure 3 and includes only features that could be automatically available to a dialogue
system.
4.1.1 Prosodic Features. Above we showed that corrections were significantly longer,
louder, higher in pitch excursion, and followed longer pauses than other turns. Thus,
we expected that these features would be useful in identifying corrections automatically.
We examined maximum and mean fundamental frequency values (f0max, f0mn) as
indicators of pitch range; maximum and mean energy values (rmsmax, rmsmn) as in-
dicators of loudness; total duration of the speaker turn (dur); length of pause preceding
the turn (ppau); speaking rate (tempo); and amount of silence within the turn (zeros).
The features were measured as indicated above. Table 6 shows the overall means and
standard deviations for these features over the corpus.
4.1.2 ASR Features. Since corrections in our corpus were misrecognized more frequently
than non-corrections (Swerts, Litman, and Hirschberg 2000), we included a set of ASR
features that were derived from TOOT?s speech recognition component and its outputs:
the grammar used as the ASR language model at each dialogue state (gram), the string
recognized by the ASR system as its best hypothesis (str), and the turn-level acoustic
427
Computational Linguistics Volume 32, Number 3
Figure 3
Feature set for predicting corrections.
Table 6
Means and standard deviations for prosodic features over all turns.
f0max f0mn rmsmax rmsmn dur ppau tempo zeros
(Hz) (Hz) (sec) (sec) (sps) (%)
Mean 227 163 1612 396 1.92 .71 2.48 44
S.D. 77 44 1020 261 2.44 .79 1.37 17
confidence score it produced (conf).4 As subcases of the str feature, we included Boolean
features representing whether or not the recognized string included the strings yes or no
(ynstr), some variant of no, such as nope (nofeat), cancel (canc), or help (help), as these
lexical items often occurred during problem resolution. To estimate durational features,
we approximated the length of the user turn in words (wordsstr) and in syllables (syls)
from the str feature, and we added a Boolean feature identifying whether or not the turn
had been rejected by the system (rejbool).
4.1.3 System Experimental Features. Our descriptive study showed that differences in
dialogue strategy affect the type and success of user corrections. For example, TOOT
users more frequently repeat their misrecognized turns and produce the fewest cor-
rections per task when TOOT has the initiative and explicitly confirms all user input.
So, we hypothesized that system conditions might prove important in our learning
experiments. We thus include features representing the system?s current initiative
and confirmation strategies (inittype, conftype), whether users could adapt the sys-
tem?s dialogue strategies (adapt), and the combined initiative and confirmation setting
(realstrat).
4 Confidence scores ranged from ?0.087662 to ?9.884418.
428
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
4.1.4 Dialogue Position and History Features. We also showed that the further a cor-
rection is from the original error, the less likely it is to be recognized correctly, and the
stronger the correlation with prosodic deviation from the mean values over a speaker?s
turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first
approximation of this distance feature, we included the feature diadist?distance of the
current turn from the beginning of the dialogue.
In addition, previous research (Litman, Walker, and Kearns 1999; Walker et al
2000) has shown that features of the dialogue as a whole and features of more local
context can be helpful in predicting ?problematic? dialogues. So we looked at a set
of features summarizing aspects of the prior dialogue for both the absolute number
of times prior turns exhibited certain characteristics (e.g., contained a key word like
cancel?priorcancnum) and the percentage of the prior dialogue containing one of these
features (e.g., priorcancpct). We also examined means for all our continuous-valued
features over the entire dialogue preceding the turn to be predicted (pmn ), such as
pmnsyls, the mean length of prior turns calculated in number of syllables per turn.
Finally, we examined more local contexts, including all features of the preceding turn
(pre ) and for the turn preceding that (ppre ).
It seemed particularly likely that lexical features of the local context?such as
whether a user had asked for help recently, or tried to cancel out of an exchange, or
replied no to a system query?might prove useful in identifying corrections.5 Also,
whether a prior turn had been rejected was clearly a useful cue to the identification
of the current turn as a correction, since users generally supplied a correction when
explicitly asked.
4.2 Machine-learning Experiments
In this section we investigate whether the features described in Section 4.1 (or inter-
esting subsets of them) can in fact be used to accurately predict whether a turn will
be a correction or not. We describe experiments using the machine-learning program
RIPPER (Cohen 1996) to automatically induce such prediction models. RIPPER takes as
input the classes to be learned, the names and possible values of a set of features, and
training data specifying the class and feature values for each training example. For our
experiments, the features presented in Figure 3 comprise the independent variables for
our learning experiments. The dependent variable to be learned, correction (T) ver-
sus non-correction (F), corresponds to the hand-labeled observations described above.
Given a vector of values for the independent and dependent variables for each speaker
turn, RIPPER outputs a classification model for classifying future examples. The model
is learned using greedy search guided by an information gain metric and is expressed as
an ordered set of if-then rules. When multiple rules are applicable, RIPPER uses the first
rule it finds. When no rules are applicable, RIPPER classifies the turn as a non-correction
(F) by default.
Table 7 shows the performance of the learned classification models for some
of the feature sets we examined; all performance figures are estimated using 25-fold
cross-validation on the 2,328 turns in our corpus. The Features column identifies the
set of features (as defined in Figure 3) used to learn the model. The second column,
DIA, indicates which type of dialogue history features (PreTurn, PrepreTurn, Prior,
5 Recall that these are lexical features from the recognized string, not from the actual user transcript.
429
Computational Linguistics Volume 32, Number 3
Table 7
Estimated error, recall, precision, and F? = 1 for predicting corrections.
class = T class = F
Features DIA Error ? SE Rec. Prec. F? = 1 Rec. Prec. F? = 1
Raw+ASR+SYS+POS PreTurn 15.72 ? 0.80 70.61 74.96 .72 89.95 88.28 .89
Raw+ASR+SYS+POS all 16.16 ? 0.58 69.80 74.65 .72 90.12 87.82 .89
PROS+ASR+SYS+POS all 16.38 ? 0.61 69.01 74.05 .71 89.60 87.61 .88
ASR all 16.41 ? 0.93 69.93 72.39 .70 88.76 87.7 .88
ASR+SYS+POS all 17.01 ? 0.78 73.73 73.38 .73 88.68 89.00 .89
ASR+SYS+POS none 18.60 ? 0.81 56.48 72.79 .63 91.33 83.76 .87
Raw+ASR+SYS+POS none 18.68 ? 0.67 58.45 71.64 .64 90.37 84.17 .87
ASR+PROS none 19.29 ? 0.78 54.54 69.97 .61 90.25 82.90 .86
POS+PROS none 19.59 ? 0.73 52.96 69.70 .60 90.38 82.47 .86
Raw all 19.68 ? 0.78 55.62 70.89 .62 90.64 83.33 .87
PROS all 20.33 ? 0.90 56.45 69.23 .61 89.43 83.42 .86
ASR+POS none 20.40 ? 0.79 52.20 71.99 .60 91.43 82.41 .87
PROS none 20.53 ? 0.81 54.86 71.72 .62 90.78 83.07 .87
conf+rejbool all 21.23 ? 0.93 59.70 65.97 .62 87.05 84.05 .85
ASR+SYS none 23.46 ? 0.72 51.55 63.40 .56 87.53 81.65 .84
ASR none 24.19 ? 0.84 45.93 60.99 .52 87.80 79.90 .84
Raw none 25.35 ? 0.93 42.26 59.46 .48 88.29 78.97 .83
POS none 29.00 ? 1.02 0.00 ? ? 99.94 70.99 .83
SYS none 29.00 ? 1.02 0.00 ? ? 100.00 71.00 .83
Prerejbool baseline error = 25.70; majority baseline error = 28.99
and/or PMean) were also included in the feature set; these features represent the
same types of information (e.g., f0max) that the Features column denotes, but for one
or more previous turns in the dialogue. The third column shows the mean error and
standard error (SE) predicted for the model specified by the first two columns. When
error estimates in different rows differ by plus or minus twice the standard error,
they are significantly different (Cohen 1995). The remaining columns show the mean
recall, precision, and F? = 1 for corrections (focus class = T) and non-corrections (focus
class = F), respectively.6 For comparison purposes, we compare our predictions to two
potential baselines. The Majority baseline predicts that all turns are non-corrections
(the majority class of F), and has a classification error of 28.99%. The Prerejbool base-
line predicts that all turns following rejected turns (prerejbool = T) are corrections?
since after rejections, TOOT asks users to repeat their turn7?and all others are non-
corrections; this baseline gives a classification error of 25.70%.
The first question addressed in our experiments is whether or not corrections can be
predicted significantly better than our baselines. Table 7 shows that in fact they can. Our
best-performing feature set (Raw+ASR+SYS+POS, DIA = PreTurn) cuts the majority
baseline error almost in half, from 28.99% to 15.72%, and predicts significantly better
than the rejection-based baseline as well. This feature set includes raw versions of all our
prosodic features and all of the non-prosodic features, for both the turn being classified
6 Recall is the percentage of actual members of a class that are identified, whereas precision is the percentage
of predicted class members that are in fact members. The definition of F? is
(?2+1)PrecisionRecall
?2Precicison+Recall
; ? = 1
equally weights precision and recall. These values are computed using our own cross-validation
program, while error is computed using RIPPER?s cross-validation option.
7 Although users are asked to repeat their turn, 29% of the turns after rejections are not in fact corrections
(e.g., the user instead asks for help or asks the system to repeat the prompt).
430
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
and the immediately prior turn. Note that even if all of the available features are used
for learning (i.e., the normalized versions of prosodic features and all of the various
history features (PROS+ASR+SYS+POS, DIA = all, error = 16.38%)), performance is
statistically comparable to this model.8 In addition, the recall, precision and F? = 1
values in Table 7 show that corrections are generally predicted with better precision
than recall whereas the reverse holds for non-corrections, and that non-corrections (the
majority class) are easier to accurately predict than corrections.
We next turn to an examination of the contribution of the different types of features
we used for prediction. First, we consider the utility of our non-prosodic features.
Table 7 shows that, using only non-prosodic features (ASR, SYS, POS), corrections can
still be predicted with an accuracy statistically equivalent to our best results. That is,
using all feature types (PROS+ASR+SYS+POS, DIA = all, error = 16.38%) is equivalent
to using only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%). Simi-
larly, restricting our feature set to the ASR-derived subset of our non-prosodic features
(ASR, DIA = all, error = 16.41%) or removing all dialogue history (ASR+SYS+POS,
DIA = none, error = 18.60%) yields results equivalent to our best-performing classifier.
However, when only those ASR features derived from the acoustic confidence score
(i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejbool-
num, priorrejboolpct) are used for prediction, then performance does significantly
degrade (conf+rejbool, DIA = all, error = 21.23%). So, it appears that there are numerous
ways to classify corrections successfully, using various combinations of feature types.
This finding is an important one because it suggests that systems that have access to
restricted kinds of information can still hope to identify user corrections with some
confidence. In particular, simply using information available to current ASR systems,
such as acoustic confidence score, recognized string, grammar, and features derived
from these, produces classification results equivalent to our best-performing classifier.
A caveat here is that some of the features in this ASR feature set (e.g., grammar and
recognized string) are less likely to generalize from task to task.
Turning now to the role of prosodic features in classifying corrections, Table 7 shows
that use of only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%)9
slightly (but not quite significantly) outperforms use of only raw prosodic features (Raw,
DIA = all, error = 19.68%). However, using raw prosodic features alone (error = 19.68%)
is comparable to using only ASR features alone (ASR, DIA = all, error = 16.41%). And
both significantly outperform the majority class and rejection-based baselines. Note
also that prediction from raw prosodic features alone (19.68%) is not improved by the
inclusion of their normalized versions (PROS, DIA = all, error = 20.33%). Thus, ASR-
derived features and prosodic features seem to provide equally successful classifications
of user corrections. Since ASR-derived features, in particular, acoustic confidence score,
are currently used by spoken dialogue systems to determine when to reject a turn,
our results suggest that such features can also be useful for identifying corrections.
Although prosodic features are rarely made use of in spoken dialogue systems, they
would, in fact, seem more likely to generalize across tasks and recognizers than the
ASR features.
Now we turn to the issue of how useful features of the dialogue history are in
classifying corrections. Recall that our best-performing ruleset used only a limited dia-
8 Note that removing features sometimes changes performance, which might indicate a weakness in
RIPPER?s feature selection process.
9 Recall that DIA = all includes only the same type of features as for the current utterance, in this case only
non-prosodic history features.
431
Computational Linguistics Volume 32, Number 3
Figure 4
Best performing ruleset (Raw+ASR+SYS+POS, DIA = PreTurn).
logue history?features from the preceding turn (Raw+ASR+SYS+POS, DIA = PreTurn,
error = 15.72%). While adding features of the turn two turns back (PrepreTurn ) and
of the dialogue as a whole (Prior and PMean ) does not significantly change the
error (Raw+ASR+SYS+POS, DIA = all, error = 16.16%), removing the features of the
immediately previous turn from the dialogue history does in fact cause a significant
increase in error rate (Raw+ASR+SYS+POS, DIA = none, error = 18.68%). However,
as discussed above, when only non-prosodic features are considered (ASR+SYS+POS),
there is no significant difference between DIA = all and DIA = none. So, it seems that
features of the immediate local context can improve our ability to classify corrections
accurately when prosodic features are included, but adding a larger local context win-
dow and a global context does not improve over these results. Contextual features seem
particularly important to performance when only raw prosodic features are considered
(Raw, DIA = all, error = 19.68%). When the raw prosodic features of the dialogue
history are removed, the error rate dramatically increases (Raw, DIA = none, error =
25.35%). However, if the normalized prosodic features (which themselves encode much
of the historical information) are also included, then removing the DIA versions of these
features does not significantly degrade performance (PROS, DIA = all, error = 20.33%
vs. PROS, DIA = none, error = 20.53%). We might explain the larger role that prosodic
context plays in classification by returning to the differences we found between prosodic
features of corrections and non-corrections, described in Section 3. In our descriptive
analyses we found that prosodic features such as pitch, duration, and loudness reliably
distinguish corrections based on relative differences between the two types of turns,
not absolute differences. In prediction also, it seems that some form of normalization by
context improves the performance of prosodic features.
When we examine which class of features performs best in the absence of contextual
information, we see that the prosodic features (PROS, DIA = none, error = 20.53%)
significantly outperform the ASR-derived features (ASR, DIA = none, error = 24.19%),
which in turn significantly outperform either of the remaining feature types (POS and
SYS). Table 7 also shows the cases in which the addition of new sources of knowl-
edge improves prediction performance. For DIA = none, the statistically significant
improvements involve adding the feature diadist (distance of the current turn from
the beginning of the dialogue): For example, ASR+POS (error = 20.4%) outperforms
both ASR (error = 24.19%) and POS (error = 29%), and ASR+SYS+POS (error = 18.6%)
outperforms ASR+SYS (error = 23.46%). Again, these are features that are easily made
available to current spoken dialogue systems.
The classification model learned from the best-performing feature set in Table 7 is
shown in Figure 4. Rules are presented in order of importance in classifying data. The
432
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
first rule RIPPER finds with this feature set specifies that if the duration of the current
turn is ? 3.89046 seconds, and if the acoustic confidence score of the prior turn is
? ?0.645234, and if the percentage of silence in the current turn is ? 53.9474%, then
predict that the turn is a correction; this rule correctly predicts 153 corrections and
incorrectly predicts that 10 non-corrections are corrections. So, this rule applies when
the previous turn has a low confidence score and the current turn exhibits some marked
prosodic features. The fourth rule predicts a correction after a previous rejection, but
only when the rejected turn was relatively short with a low confidence score. The
fifth rule predicts a correction when TOOT uses a particular confirmation strategy for
turns that are relatively long and far from the beginning of the dialogue. The sixth
rule predicts a correction when the previous turn is spoken soon after the prompt,
and contains the problem indicator help. Note that this use of the domain-independent
help is the only reference to a lexical item in this ruleset. This ruleset includes features
from all of the feature subsets in our inventory (PROS, ASR, SYS, POS, DIA). For the
current turn, the feature types that appear in the rules are PROS (dur, zeros), ASR (conf,
gram, syls), SYS (conftype), and POS (diadist). Of the previous turn?s features, only
two feature sets emerge as important: PROS (pref0mn, predur, preppau, pretempo)
and ASR (preconf, prestr, prewordstr, prerejbool). Furthermore, within a feature set
such as PROS, the useful features of the current and previous turns differ somewhat
(e.g., zeros is useful for the current turn, whereas tempo is useful for the prior turn),
suggesting important differences in the prosodic characteristics of corrections versus
the turns they follow.
When we look at a ruleset produced using only features commonly available to
current dialogue systems, such as ASR+SYS+POS (DIA = all), we see that creative use
of these features could in fact support correction classification (Figure 5). For example,
the fourth rule predicts that the current turn is a correction when it is not too short, and
when the pre turn indicates awareness (evidenced by the presence of no) of a problem
in the ppre turn (which was recognized with low confidence). This ruleset uses both
ASR (gram, nofeat, syls) and SYS (conftype) features of the current turn; although only
one rule in fact makes use of SYS features. For the contextual DIA features, only the
ASR features occur in the rule-set: PreTurn (preconf, prestr, prenofeat, prerejbool),
PrepreTurn (ppreconf, ppreynstr), and Prior and PMean (pmnconf, priorynstrpct,
pmnwordsstr, priorrejnum). Comparing this ruleset to the previous one (Figure 4), we
see that where timing features (dur, predur, zeros, pretempo, preppau) appear often
when prosodic features are available, related features such as syls and wordsstr (from
which, e.g., tempo is estimated) may be compensating in this ruleset. And of course the
rejection feature (prerejbool) itself is a function of the confidence score of the prior turn.
Note also that lexical features of the recognized string (nofeat, prenofeat, ppreynstr,
Figure 5
Ruleset for non-prosodic features (ASR+SYS+POS, DIA = all).
433
Computational Linguistics Volume 32, Number 3
prestr, priorynstrpct) emerge as quite useful in this ruleset?especially as contextual
features. So, what the system has recognized in prior turns is a good predictor of
whether the current turn is a correction. Also note that the overall verbosity of the
previous dialogue (pmnwordsstr) appears in two of the rules.
An example of a ruleset learned from only prosodic features (Raw, DIA = all, from
Table 7) is shown in Figure 6. This ruleset is notably terser than those shown in Figures 4
and 5 and includes primarily timing-based features (current turn features dur, zeros,
and tempo; local contextual feature pretempo; and dialogue-level features pmndur and
pmnppau). However, all prosodic feature types but f0 appear at least once in the ruleset,
and features specific to the current turn differ from those relevant to different types of
dialogue history. As with our previous descriptive findings discussed in Section 3, this
ruleset shows that corrections are longer, louder, follow longer pauses, and contain less
internal silence than non-corrections, and that these features can be used successfully to
identify them.
4.3 Other Experiments
The machine-learning experiments described in Section 4.2 were motivated by our
long-term goal to incorporate a correction predictor into future versions of our spoken
dialogue system. As such, the experiments were limited to a binary prediction task
(predicting whether a turn was a correction or a non-correction) and only considered
features readily available to our dialogue system. In this section we present additional
experiments removing some of these restrictions, with the goal of further investigating
some of the descriptive findings discussed in Section 3.
Recall from Section 3.2 that there were some differences in the prosodic features of
corrections produced by native versus non-native speakers when such features were
normalized by the first turn in the dialogue. We thus investigated whether adding a
native speaker feature (currently manually labeled) to the prosodic feature set Norm1
(DIA = all) would improve prediction accuracy. Although the error was reduced from
24.32% to 22.68%, this difference was not statistically significant. Furthermore, when
we added the native speaker feature to both the best-performing ruleset in Table 7
(Raw+ASR+SYS+POS, DIA = PreTurn) and the best-performing prosodic feature set
(Raw, DIA = all), the error rates actually increased; again, however, the differences were
not statistically significant.
Also, in Sections 3.1 and 3.4, we identified differences between different types of cor-
rections, which suggests that our features might be more effectively used to predict each
correction type differently. In other words, what would happen if instead of predicting
whether a turn was a correction (T) or not (F) (the binary classification task investigated
above), we predicted whether a turn was ADD, ADD/OMIT, OMIT, PAR, REP, or F (i.e.,
not a correction). Because, as Table 2 shows, we only have limited amounts of data for
Figure 6
Ruleset for raw prosodic features (Raw, DIA = all).
434
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
several of our classes (e.g., only 2% of our corrections are ADD/OMIT); we performed a
simpler version of this experiment, combining our three lowest frequency classes (ADD,
ADD/OMIT, and PAR) into the single class MISC.
Using the best feature set from Table 7 (Raw+ASR+SYS+POS, DIA = PreTurn),
Table 8 shows our results using 25-fold cross-validation. First, note that our overall
estimated error is now 24.13% ? 0.89%. Although this is a huge increase compared to
the 15.72% error rate of our original binary classifier, it should be noted that considering
correction types separately makes our class distribution quite skewed, with the data
for our three correction classes much smaller than the majority class. Nevertheless,
our classifier yields a slight but significant decrease compared to the majority baseline
error, and a nonsignificant decrease compared to the Prerejbool baseline error (both
baselines remain the same as in Table 7). With respect to precision and recall, although
the absolute numbers for corrections are much lower than in Table 7, we again see
that predicting corrections yields higher precision than recall, whereas predicting non-
corrections yields higher recall than precision. Finally, an examination of the learned
ruleset (which contains four rules for predicting MISC, two rules for predicting OMIT,
and seven rules for predicting REP) does show that features are used differently across
correction types. For example, the feature prestr is only used to predict repetition correc-
tions (in particular, after a turn containing help). Our rules also show some overlap with
our earlier descriptive findings. For example, we noted that corrections of rejections
were more likely to be repetitions, and find the feature prerejbool in two of the rules
for predicting repetitions. These findings suggest that if more data were available,
predicting corrections by type might prove a useful strategy.
5. Conclusions
In this article we have presented results of an analysis of corrections in the TOOT
spoken dialogue corpus. We first introduced the TOOT spoken dialogue corpus and
our labeling scheme to identify different types of corrections. The TOOT corpus is
representative of many current research and commercial dialog systems in focusing on
the travel domain. Also, since data were collected using a variety of dialog strategies
with different types of initiative and confirmation, results obtained with this corpus
are more likely to have general usefulness for builders of other spoken dialogue
systems.
We next presented a statistical description of the corrections we labeled. In general,
it appears that corrections are a serious problem for ASR, being recognized much
more poorly than non-corrections but not being rejected any more frequently. Some
corrections types are more difficult to handle for systems than others, with repetitions
Table 8
Predicting correction types (error ? SE = 24.13 ? 0.89)
Class Recall Precision F? = 1
FALSE 93.30 82.37 .87
REP 33.86 56.00 .41
MISC 36.17 48.36 .38
OMIT 25.47 50.13 .32
435
Computational Linguistics Volume 32, Number 3
and corrections that omit information from the original turn being much better recog-
nized than corrections that add or paraphrase such information. Confirming previous
studies of repetition corrections, we found that corrections in general differ from non-
corrections prosodically: They are higher in f0, softer, longer, follow longer pauses, and
contain less internal silence than non-corrections. Also, corrections more distant from
the error they are correcting are louder, higher in pitch, longer, slower, and follow
longer pauses than closer corrections. Both findings suggest a correlation between
corrections and hyperarticulation; however, most prosodic differences persist even
when perceptually hyperarticulated turns are removed from the sample, and perceptual
hyperarticulation is not significantly correlated with distance from original error. We
hypothesize that recognizers may be more sensitive to hyperarticulatory tendencies
than humans.
The second part of this article discusses results of machine-learning experiments
designed to evaluate how well we can distinguish user corrections from non-corrections
using features automatically available to dialogue systems. Clearly, new techniques
must be developed to interpret such corrections, but such techniques can only be
effective if corrections can be reliably identified as such for special handling. Using a
large set of prosodic, ASR-derived, and system-specific features, both for the current
turn and for contextual windows, and using summary features of the prior dialogue, we
have demonstrated that it is possible to classify user corrections significantly better than
either of two baseline classifiers (15.72% error vs. 25.70?28.99%). More usefully perhaps
for current spoken dialogue systems, we have found that we can derive classifiers
that perform equivalently well using only features currently available to most speech
recognizers, such as acoustic confidence score, recognized string, grammar, and features
easily derived from these data. For example, using only such features, we can classify
user corrections with an estimated success rate of 16.41%. So, it does, in fact, seem quite
feasible for current systems to identify user corrections using data they typically do not
make use of.
Given that our findings show that corrections can be classified well using quite dis-
tinct feature sets, a possible future line of research would be to try classification combi-
nation schemes. For instance, one could envision a form of metalearning or boosting that
combines classifications using different feature sets (e.g., ASR vs. prosodic vs. context),
or that combines the output of different learning algorithms (e.g., Ripper combined
with memory-based learning; see, e.g., Lendvai 2004). Kirchhoff (2001) presents some
results of classifier combination schemes, showing some improvements in detection of
corrections when using cascading, but especially when using boosting.
The next steps, developing techniques to interpret these turns more accurately
and to use correction prediction to drive modifications in dialogue strategy, are both
subjects of our future research. Also, whereas our analyses so far have given us overall
information about the relative contribution of various feature sets for the automatic
classification of corrections, one interesting problem for the future is to get more specific
information about the cues that characterize corrections, especially for the development
of on-line error-correction detection. In this respect, an interesting observation has been
made by Kirchhoff (2001), who reports that correction classification using only features
of the first half of a turn performs equally well as a classification using features of the
turn as a whole; this could be explained by the fact that speakers tend to put character-
istic cue phrases, such as ?no? or ?help,? in the beginning of a turn. Additional research
is needed to find strategies that use the detection of corrections to look back in the
dialogue history to identify the utterance being corrected or even the actual problematic
words in these turns. Finally, it would be worthwhile to investigate speaker-specific
436
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
correction strategies in more detail, the possible effect on such strategies of the user?s
experience with a system, and his or her linguistic background.
Acknowledgments
Marc Swerts is also affiliated with the
University of Antwerp. His research is
sponsored by the Netherlands Organisation
for Scientific Research (NWO). This work
was performed when the authors were at
AT&T Labs?Research.
References
Ammicht, E., A. Potamianos, and
E. Fosler-Lussier. 2001. Ambiguity
representation and resolution in spoken
dialogue systems. In Proceedings of
EUROSPEECH-01, pages 2217?2220,
Aalborg.
Andorno, M., P. Laface, and R. Gemello.
2002. Experiments in confidence scoring
for word and sentence verification. In
Proceedings of International Conference on
Spoken Language Processing-02,
pages 1377?1381, Denver.
Barkhuysen, P., E. Krahmer, and M. Swerts.
2005. Problem detection in
human?machine interactions based on
facial expressions of users. Speech
Communication, 45:343?359.
Batliner, A., K. Fischer, R. Huber, J. Spilker,
and E. No?th. 2003. How to find trouble in
communication. Speech Communication,
40:117?143.
Bell, L. and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating
a Swedish database of spontaneous
computer-directed speech. In Proceedings
of International Congress of Phonetic
Sciences-99, pages 1221?1224, San
Francisco.
Bouwman, A. G., J. Sturm, and L. Boves.
1999. Incorporating confidence
measures in the Dutch train timetable
information system developed
in the ARISE project. In Proceedings
International Conference on Acoustics,
Speech and Signal Processing, volume 1,
pages 493?496, Phoenix.
Bulyko, I., K. Kirchhoff, M. Ostendorf, and
J. Goldberg. 2005. Error-correction
detection and response generation in a
spoken dialogue system. Speech
Communication, 45:271?288.
Cohen, P. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Cohen, W. 1996. Learning trees and rules
with set-valued features. In 14th Conference
of the American Association of Artificial
Intelligence, AAAI, pages 709?716,
Portland.
Falavigna, D., R. Gretter, and G. Riccardi.
2002. Acoustic and word lattice based
algorithms for confidence scores. In
Proceedings of International Conference
on Spoken Language Processing-02,
pages 1621?1624, Denver.
Guillevic, D., S. Gandrabur, and
Y. Normandin. 2002. Robust semantic
confidence scoring. In Proceedings of
International Conference on Spoken Language
Processing-02, pages 853?856, Denver.
Hirschberg, J., D. Litman, and M. Swerts.
1999. Prosodic cues to recognition errors.
In Proceedings of the Automatic Speech
Recognition and Understanding Workshop
(ASRU?99), pages 349?352, Keystone.
Hirschberg, J., D. Litman, and M. Swerts.
2004. Prosodic and other cues to speech
recognition failures. Speech Communication,
43:155?175.
Kamm, C., S. Narayanan, D. Dutton, and
R. Ritenour. 1997. Evaluating spoken
dialog systems for telecommunication
services. In Proceedings of
EUROSPEECH-97, pages 2203?2206,
Rhodes.
Karsenty, L. and V. Botherel. 2005.
Transparency strategies to help users
handle system errors. Speech
Communication, 45:305?324.
Kirchhoff, Katrin. 2001. A comparison
of classification techniques for the
automatic detection of error corrections
in human?computer dialogues. In
Proceedings of the NAACL Workshop
on Adaptation in Dialogue Systems,
pages 33?40, Pittsburgh, PA.
Krahmer, E., M. Swerts, M. Theune, and
M. Weegels. 2001. Error detection in
spoken human?machine interaction.
International Journal of Speech Technology,
4(1):19?30.
Levow, Gina-Anne. 1998. Characterizing
and recognizing spoken corrections in
human?computer dialogue. In Proceedings
of the 36th Annual Meeting of the Association
of Computational Linguistics, COLING/ACL
98, pages 736?742, Montreal.
Lendvai, Piroska. 2004. Extracting
information from spoken user input. A
machine-learning approach. Unpublished
Ph.D. dissertation, Tilburg University.
Litman, D. and S. Pan. 1999. Empirically
evaluating an adaptable spoken
437
Computational Linguistics Volume 32, Number 3
dialogue system. In Proceedings of the
7th International Conference on User
Modeling (UM), pages 55?64, Banff.
Litman, D., M. Walker, and M. Kearns. 1999.
Automatic detection of poor speech
recognition at the dialogue level. In
Proceedings of the 37th Annual Meeting of the
Association of Computational Linguistics,
ACL99, pages 309?316, College Park.
McTear, M., I. A. O?Neill, P. Hanna, and
X. Liu. 2005. Handling errors and
determining confirmation strategies?an
object-based approach. Speech
Communication, 45:249?269.
Moreno, P. J., B. Logan, and B. Raj. 2001. A
boosting approach for confidence scoring.
In Proceedings of EUROSPEECH-01,
pages 2109?2112, Aalborg.
Oviatt, S. L., G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error
resolution. In Proceedings of International
Conference on Spoken Language
Processing-96, pages 801?804, Philadelphia.
Prodanov, P. and A. Drygajlo. 2005. Bayesian
networks based multimodality fusion for
error handling in human?robot dialogues
under noisy conditions. Speech
Communication, 45:231?248.
Sharp, R. D., E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley,
and J. Rowland. 1997. The Watson
speech recognition engine. In
Proceedings International Conference on
Acoustics, Speech and Signal Processing97,
pages 4065?4068, Munich.
Shimojima, A., Y. Katagiri, H. Koiso, and
M. Swerts. 2001. An experimental study on
the informational and grounding functions
of prosodic features of Japanese echoic
responses. Speech Communication,
43:155?175.
Skantze, G. 2005. Exploring human error
recovery strategies: Implications for
spoken dialogue systems. Speech
Communication, 45:325?341.
Soltau, Hagen and Alex Waibel. 1998. On the
influence of hyperarticulated speech on
recognition performance. In Proceedings of
International Conference on Spoken Language
Processing-98, pages 225?228, Sydney.
Soltau, Hagen and Alex Waibel. 2000.
Specialized acoustic models for
hyperarticulated speech. In Proceedings
of International Conference on Acoustics,
Speech and Signal Processing 2000,
pages 1779?1782, Istanbul.
Soltau, H., H. Metze, and A. Waibel. 2002.
Compensating for hyperarticulation by
modeling articulatory properties. In
Proceedings of International Conference
on Spoken Language Processing-02,
pages 83?86, Denver.
Sturm, J. and L. Boves. 2005. Effective error
recovery strategies for multimodal
form-filling applications. Speech
Communication, 45:289?303.
Swerts, M., D. Litman, and J. Hirschberg.
2000. Corrections in spoken dialogue
systems. In Proceedings of International
Conference on Spoken Language
Processing-00, pages 615?618, Beijing.
Talkin, D. 1995. A Robust algorithm for
pitch tracking (RAPT). In W. B. Klein
and K. K. Paliwal, editors, Speech
Coding and Synthesis. Elsevier Science,
Athens, pages 495?518.
Torres, F., L. Hurtado, F. Garc??a, E. Sanchis,
and E. Segarra. 2005. Error handling in a
stochastic dialog system through
confidence measures. Speech
Communication, 45:211?229.
Wade, E., E. E. Shriberg, and P. J. Price.
1992. User behaviors affecting
speech recognition. In Proceedings of
International Conference on Spoken
Language Processing-92, volume 2,
pages 995?998, Banff.
Walker, M., I. Langkilde, J. Wright,
A. Gorin, and D. Litman. 2000.
Learning to predict problematic
situations in a spoken dialogue
system: Experiments with How may
I help you? In Proceedings of NAACL-00,
pages 210?217, Seattle.
Wang, H.-M. and Y.-C. Lin. 2002. Error-
tolerant spoken language understanding
with confidence measuring. In Proceedings
of International Conference on Spoken
Language Processing-02, pages 1625?1628,
Denver.
Zhang, R. and A. Rudnicky. 2001. Word
level confidence annotation using
combinations of features. In Proceedings
of EUROSPEECH-01, pages 2105?2108,
Aalborg.
438
Predicting Automatic Speech Recognition Performance Using 
Prosodic Cues 
Diane  J .  L i tman and Ju l ia  B .  H i rschberg  Marc  Swer ts  
AT&T Labs - -  Research IPO,  Center for User-System Interact ion 
F lo rham Park,  NJ 07932-0971 USA Eindhoven, The Nether lands 
{ diane,julia} @research. att.com swerts@ipo.tue.nl 
Abst rac t  
In spoken dialogue systems, it is important for a 
system to know how likely a speech recognition hy- 
pothesis is to be correct, so it can reprompt for 
fresh input, or, in cases where many errors have 
occurred, change its interaction strategy or switch 
the caller to a human attendant. We have discov- 
ered prosodic features which more accurately predict 
when a recognition hypothesis contains a word error 
than the acoustic onfidence score thresholds tradi- 
tionally used in automatic speech recognition. We 
present analytic results indicating that there are sig- 
nificant prosodic differences between correctly and 
incorrectly recognized turns in the TOOT train in- 
formation corpus. We then present machine learn- 
ing results howing how the use of prosodic features 
to automatically predict correct versus incorrectly 
recognized turns improves over the use of acoustic 
confidence scores alone. 
1 I n t roduct ion  
One of the central tasks of the dialogue manager 
in most current spoken dialogue systems (SDSs) is 
error handling. The automatic speech recognition 
(ASR) component of such systems i prone to error, 
especially when the system has to operate in noisy 
conditions or when the domain of the system is large. 
Given that it is impossible to fully prevent ASR er- 
rors, it is important for a system to know how likely 
a speech recognition hypothesis to be correct, so 
it can take appropriate action, since users have con- 
siderable difficulty correcting incorrect information 
that is presented by the system as true (Krahmer 
et al, 1999). Such action may include verifying the 
user's input, reprompting for fresh input, or, in cases 
where many errors have occurred, changing the in- 
teraction strategy or switching the caller to a human 
attendant (Smith, 1998; Litman et al, 1999; Langk- 
ilde et al, 1999). Traditionally, the decision to re- 
ject a recognition hypothesis i based on acoustic 
confidence score thresholds, which provide a relia- 
bility measure on the hypothesis and are set in the 
application (Zeljkovic, 1996). However, this process 
often fails, as there is no simple one-to-one mapping 
between low confidence scores and incorrect recog- 
nitions, and the setting of a rejection threshold is 
a matter of trial and error (Bouwman et al, 1999). 
Also, some incorrect recognitions do not necessarily 
lead to misunderstandings at aconceptual level (e.g. 
"a.m." recognized as "in the morning"). 
The current paper looks at prosody as one possible 
predictor of ASR performance. ASR performance 
is known to vary based upon speaking style (Wein- 
traub et al, 1996), speaker gender and age, na- 
tive versus non-native speaker status, and, in gen- 
eral, the deviation of new speech from the training 
data. Some of this variation is linked to prosody, as 
prosodic differences have been found to character- 
ize differences in speaking style (Blaauw, 1992) and 
idiosyncratic differences (Kraayeveld, 1997). Sev- 
eral other studies (Wade et al, 1992; Oviatt et al, 
1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell 
and Gustafson, 1999) report that hyperarticulated 
speech, characterized by careful enunciation, slowed 
speaking rate, and increase in pitch and loudness, 
often occurs when users in human-machine interac- 
tions try to correct system errors. Still others have 
shown that such speech also decreases recognition 
performance (Soltau and Waibel, 1998). Prosodic 
features have also been shown to be effective in 
ranking recognition hypotheses, asa post-processing 
filter to score ASR hypotheses (Hirschberg, 1991; 
Veilleux, 1994; Hirose, 1997). 
In this paper we present results of empirical stud- 
ies testing the hypothesis that prosodic features pro- 
vide an important clue to ASR performance. We 
first present results comparing prosodic analyses of 
correctly and incorrectly recognized speaker turns 
in TOOT, an experimental SDS for obtaining train 
information over the phone. We then describe ma- 
chine learning experiments based on these results 
that explore the predictive power of prosodic fea- 
tures alone and in combination with other automat- 
ically available information, including ASR confi- 
dence scores and recognized string. Our results in- 
dicate that there are significant prosodic differences 
between correctly and incorrectly recognized utter- 
ances. These differences can in fact be used to pre- 
218 
dict whether an utterance has been misrecognized, 
with a high degree of accuracy. 
2 The  TOOT Corpus  
Our corpus consists of a set of dialogues between 
humans and TOOT, an SDS for accessing train 
schedules from the web via telephone, which was 
collected to study both variations in SDS strat- 
egy and user-adapted interaction (Litman and Pan, 
1999). TOOT is implemented on a platform com- 
bining ASR, text-to-speech, a phone interface, a 
finite-state dialogue manager, and application func- 
tions (Kamm et al, 1997). The speech recognizer is 
a speaker-independent hidden Markov model system 
with context-dependent phone models for telephone 
speech and constrained grammars for each dialogue 
state. Confidence scores for recognition were avail- 
able only at the turn, not the word, level (Zeljkovic, 
1996). An example TOOT dialogue is shown in Fig- 
ure 1. 
Subjects performed four tasks with one of sev- 
eral versions of TOOT, that differed in terms of locus 
of initiative (system, user, or mixed), confirmation 
strategy (explicit, implicit, or none), and whether 
these conditions could be changed by the user during 
the task. Subjects were 39 students, 20 native speak- 
ers of standard American English and 19 non-native 
speakers; 16 subjects were female and 23 male. Dia- 
logues were recorded and system and user behavior 
logged automatically. The concept accuracy (CA) of 
each turn was manually labeled by one of the exper- 
imenters. If the ASR output correctly captured all 
the task-related information in the turn (e.g. time, 
departure and arrival cities), the turn was given a 
CA score of 1 (a semantically correct recognition). 
Otherwise, the CA score reflected the percentage of 
correctly recognized task information in the turn. 
The dialogues were also transcribed by hand and 
these transcriptions automatically compared to the 
ASR recognized string to produce a word error rate 
(WEPt) for each turn. Note that a concept can be 
correctly recognized even though all words are not, 
so the CA metric does not penalize for errors that 
are unimportant to overall utterance interpretation. 
For the study described below, we examined 1994 
user turns from 152 dialogues in this corpus. The 
speech recognizer was able to generate a recognized 
string and an associated acoustic confidence score 
per turn for 1975 of these turns. 1 1410 of these 1975 
turns had a CA score of 1 (for an overall conceptual 
accuracy score of 71%) and 961 had a WER of 0 (for 
an overall transcription accuracy score of 49%, with 
a mean WER per turn of 47%). 
1For the remaining turns, ASR output "no speech" (and 
TOOT played a timeout message) or "garbage" (TOOT played 
a rejection message). 
3 D is t ingu ish ing  Cor rect  f rom 
Incor rect  Recogn i t ions  
We first looked for distinguishing prosodic charac- 
teristics of misrecognitions, defining misrecognitions 
in two ways: a) as turns with WER>0; and b) as 
turns with CA<I.  As noted in Section 1, previous 
studies have speculated that hyperarticulated speech 
(slower and louder speech which contains wider pitch 
excursions) may be associated with recognition fail- 
ure. So, we examined the following features for each 
user turn: 2 
? maximum and mean fundamental frequency 
values (F0 Max, F0 Mean) 
? maximum and mean energy values (RMS Max, 
RMS Mean) 
? total duration 
? length of pause preceding the turn (Prior Pause) 
* speaking rate (Tempo) 
? amount of silence within the turn (% Silence) 
F0 and I:LMS values, representing measures of pitch 
excursion and loudness, were calculated from the 
output of Entropic Research Laboratory's pitch 
tracker, get_fO, with no post-correction. Timing vari- 
ation was represented by four features. Duration 
within and length of pause between turns was com- 
puted from the temporal labels associated with each 
turn's beginning and end. Speaking rate was ap- 
proximated in terms of syllables in the recognized 
string per second, while % Silence was defined as the 
percentage of zero frames in the turn, i.e., roughly 
the percentage of time within the turn that the 
speaker was silent. These features were chosen based 
upon previous findings (see Section 1) and observa- 
tions from our data. 
To ensure that our results were speaker indepen- 
dent, we calculated mean values for each speaker's 
recognized turns and their misrecognized turns for 
every feature. Then, for each feature, we created 
vectors of speaker means for recognized and misrec- 
ognized turns and performed paired t-tests on the 
vectors. For example, for the feature "F0 max", 
we calculated mean maxima for misrecognized turns 
and for correctly recognized turns for each of our 
thirty-nine speakers. We then performed a paired 
t-test on these thirty-nine pairs of means to de- 
rive speaker-independent results for differences in F0 
maxima between correct and incorrect recognitions. 
Tables 1 and 2 show results of these compar- 
isons when we calculate misrecognition i terms of 
2While the features were automatically computed, turn 
beginnings and endings were hand segmented in dialogue-level 
speech files, as the turn-level files created by TOOT were not 
available. 
219 
Toot: 
User: 
Toot: 
User: 
Toot: 
Hi, this is AT&T Amtrak schedule system. This is TOOT. How may I help you? 
I want the trains from New York City to Washington DC on Monday at 9:30 in the evening. 
Do you want me to find the trains from New York City to Washington DC on Monday 
approximately at 9:30 in the evening now? 
Yes. 
I am going to get the train schedule for you ...  
Figure 1: Example Dialogue Excerpt with TOOT. 
Table 1: Comparison of Misrecognized (WER>0) 
vs. Recognized Turns by Prosodic Feature Across 
Speakers. Fe tur0 I st tlMeanMisrecdRocd PII 
*F0 Max 7.83 30.31 Hz 0 
*F0 Mean 3.66 ~I.12 Hz 0 
*RMS Max 5.70 235.93 0 
RMS Mean -.57 -8.50 .57 
*Duration 10.30 2.20 sec 0 
*Prior Pause 5.55 .35 sec 0 
Tempo -.05 .15 sps .13 
*% Silence -5.15 -.06% 0 
*significant at a 95% confidence level 
Table 2: Comparison of Misrecognized (CA<I)  
vs. Recognized Turns by Prosodic Feature Across 
Speakers. Fe turo I st t  ? nMisrecdl rlq ecd 
*F0 Max 5.60 29.64 Hz 0 
F0 Mean 1.70 2.10 Hz .10 
*RMS Max 2.86 173.87 .007 
RMS Mean -1.85 -27.75 .07 
*Duration 9.80 2.15 sec 0 
*Prior Pause 4.05 .38 sec 0 
*Tempo -4.21 -.58 sps 0 
% Silence -1.42 -.02% .16 
*significant at a 95% confidence level (p< .05) 
WER>0 and CA<l ,  respectively. These results in- 
dicate that misrecognized turns do differ from cor- 
rectly recognized ones in terms of prosodic features, 
although the features on which they differ vary 
slightly, depending upon the way "misrecognition" 
is defined. Whether defined by WER or CA, mis- 
recognized turns exhibit significantly higher F0 and 
RMS maxima, longer durations, and longer preced- 
ing pauses than correctly recognized speaker turns. 
For a traditional WER definition of misrecognition, 
misrecognitions are slightly higher in mean F0 and 
contain a lower percentage of internal silence. For a 
CA definition, on the other hand, tempo is a signif- 
icant factor, with misrecognitions spoken at a faster 
rate than correct recognitions - - contrary to our hy- 
pothesis about the role of hyperarticulation in recog- 
nition error. 
While the comparisons in Tables 1 and 2 were 
made on the means of raw values for all prosodic fea- 
tures, little difference is found when values are nor- 
malized by value of first or preceding turn, or by con- 
verting to z scores. 3 From this similarity between the 
performance of raw and normalized values, it would 
seem to be relative differences in speakers' prosodic 
values, not deviation from some 'acceptable' range, 
that distinguishes recognition failures from success- 
ful recognitions. A given speaker's turns that are 
The only differences occur for CA defined misrecognition, 
where normalizing by first utterance results in significant dif- 
ferences in mean RMS, and normalizing by preceding turn 
results in no significant differences in tempo. 
higher in pitch or loudness, or that are longer, or 
that follow longer pauses, are less likely to be recog- 
nized correctly than that same speaker's turns that 
are lower in pitch or loudness, shorter, and follow 
shorter pauses - -  however correct recognition is de- 
fined. 
It is interesting to note that the features we found 
to be significant indicators of failed recognitions (F0 
excursion, loudness, long prior pause, and longer du- 
ration) are all features previously associated with 
hyperarticulated speech. Since prior research has 
suggested that speakers may respond to failed recog- 
nition attempts by hyperarticulating, which itself 
may lead to more recognition failures, had we in fact 
simply identified a means of characterizing and iden- 
tifying hyperarticulated speech prosodically? 
Since we had independently labeled all speaker 
turns for evidence of hyperarticulation (two of the 
authors labeled each turn as "not hyperarticulated", 
"some hyperarticulation in the turn", and "hyperar- 
ticulated", following Wade et al (1992)), we were 
able to test this possibility. We excluded any turn 
either labeler had labeled as partially or fully hy- 
perarticulated, and again performed paired t-tests 
on mean values of misrecognized versus recognized 
turns for each speaker. Results show that for both 
WER-defined and CA-defined misrecognitions, not 
only are the same features ignificant differentiators 
when hyperarticulated turns are excluded from the 
analysis, but in addition, tempo also is significant 
for WER-defined misrecognition. So, our findings 
220 
for the prosodic characteristics of recognized and of 
misrecognized turns hold even when perceptibly hy- 
perarticulated turns are excluded from the corpus. 
4 P red ic t ing  M is recogn i t ions  Us ing  
Mach ine  Learn ing  
Given the prosodic differences between misrecog- 
nized and correctly recognized utterances in our 
corpus, is it possible to predict accurately when a 
particular utterance will be misrecognized or not? 
This section describes experiments using the ma- 
chine learning program RIPPER (Cohen, 1996) to au- 
tomatically induce prediction models, using prosodic 
as well as additional features. Like many learning 
programs, RIPPER takes as input the classes to be 
learned, a set of feature names and possible values, 
and training data specifying the class and feature 
values for each training example. RIPPER outputs 
a classification model for predicting the class of fu- 
ture examples. The model is learned using greedy 
search guided by an information gain metric, and is 
expressed as an ordered set of if-then rules. 
Our predicted classes correspond to correct recog- 
nition (T) or not (F). As in Section 3, we examine 
both WER-defined and CA-defined notions of cor- 
rect recognition, and represent each user turn as a 
set of features. The features used in our learning 
experiments include the raw prosodic features in Ta- 
bles 1 and 2 (which we will refer to as the feature set 
"Prosody"), the hyperarticulation score discussed in 
Section 3, and the following additional potential pre- 
dictors of misrecognition (described in Section 2): 
? ASR grammar 
? ASR confidence 
? ASR string 
? system adaptability 
? dialogue strategy 
? task number 
? subject 
? gender 
? native speaker 
The first three features are derived from the ASR 
process (the context-dependent grammar used to 
recognize the turn, the turn-level acoustic onfidence 
score output by the recognizer, and the recognized 
string). We included these features as a baseline 
against which to test new methods of predicting 
misrecognitions, although, currently, we know of no 
ASR system that includes recognized string in its 
rejection calculations. 4 TOOT itself used only the 
4Note that, while the entire recognized string is provided 
to the learning algorithm, RIPPER rules test for the presence 
of particular words in the string. 
first two features to calculate rejections and ask the 
user to repeat the utterance, whenever the confi- 
dence score fell below a pre-defined grammar-specific 
threshold. The other features represent he exper- 
imental conditions under which the data was col- 
lected (whether users could adapt TOOT's dialogue 
strategies, TOOT's initial initiative and confirmation 
strategies, experimental task, speaker's name and 
characteristics). We included these features to de- 
termine the extent o which particulars of task, sub- 
ject, or interaction influenced ASR success rates or 
our ability to predict them; previous work showed 
that these factors impact TOOT's performance (Lit- 
man and Pan, 1999; Hirschberg et al, 1999). Except 
for the task, subject, gender, native language, and 
hyperarticulation scores, all of our features are au- 
tomatically available. 
Table 3 shows the relative performance of a num- 
ber of the feature sets we examined; results here 
are for misrecognition defined in terms of WER. 5 A 
baseline classifier for misrecognition, predicting that 
ASR is always wrong (the majority class of F), has 
an error of 48.66%. The best performing feature 
set includes only the raw prosodic and ASR features 
and reduces this error to an impressive 6.53% +/ -  
.63%. Note that this performance is not improved 
by adding manually labeled features or experimen- 
tal conditions: the feature set corresponding to ALL 
features yielded the statistically equivalent 6.68% 
+/ -  0.63%. 
With respect o the performance of prosodic fea- 
tures, Table 3 shows that using them in conjunction 
with ASR features (error of 6.53%) significantly out- 
performs prosodic features alone (error of 12.76%), 
which, in turn, significantly outperforms any single 
prosodic feature; duration, with an error of 17.42%, 
is the best such feature. Although not shown in 
the table, the unnormalized prosodic features ig- 
nificantly outperform the normalized versions by 7- 
13%. Recall that prosodic features normalized by 
first task utterance, by previous utterance, or by 
z scores showed little performance difference in the 
analyses performed in Section 3. This difference may 
indicate that there are indeed limits on the ranges 
in features uch as F0 and RMS maxima, duration 
and preceding pause within which recognition per- 
formance is optimal. It seems reasonable that ex- 
treme deviation from characteristics of the acoustic 
training material should in fact impact ASR perfor- 
mance, and our experiments may have uncovered, if
not the critical variants, at least important acoustic 
correlates of them. However, it is difficult to com- 
SThe errors and standard errors (SE) result from 25-fold 
cross-validation the 1975 turns where ASR yielded a string 
and confidence. When two errors plus or minus twice the stan- 
dard error do not overlap, they are statistically significantly 
different. 
221 
Table 3: Estimated Error for Predicting Misrecognized Turns (WER>0). 
Features Used Error \] SE 
Prosody, ASR Confidence, ASR String, ASR Grammar 6.53% .63 
ALL 6.68% .63 
Prosody, ASR String 7.34% .75 
ASR Confidence, ASR String, ASR Grammar 9.01% .70 
Prosody, ASR Confidence, ASR Grammar 10.63% .88 
Prosody, ASR Confidence 10.99% .87 
Prosody 12.76% .79 
ASR String 15.24% 1.11 
Duration 17.42% .88 
ASR Confidence, ASR Grammar 17.77% .72 
ASR Confidence 22.23% 1.16 
ASR Grammar 26.28% .84 
Tempo 32.76% 1.03 
Hyperarticulation 35.24% 1.46 
% Silence 36.46% .79 
Prior Pause 36.61% .97 
F0 Max 38.73% .82 
RMS Max 42.23% .96 
F0 Mean 46.33% 1.10 
RMS Mean 48.35% 1.15 
II Majority Baseline J. 48.66%_%_\[___~ 
pare our machine learning results with the statisti- 
cal analyses, since a) the statistical analyses looked 
at only a single prosodic variable at a time, and b) 
data points for that analysis were means calculated 
per speaker, while the learning algorithm operated 
on all utterances, allowing for unequal contributions 
by speaker. 
We now address the issue of what prosodic fea- 
tures are contributing to misrecognition identifica- 
tion, relative to the more traditional ASR tech- 
niques. Do our prosodic features imply correlate 
with information already in use by ASR systems 
(e.g., confidence score, grammar), or at least avail- 
able to them (e.g., recognized string)? First, the 
error using ASR confidence score alone (22.23%) 
is significantly worse than the error when prosodic 
features are combined with ASR confidence scores 
(10.99%) - -  and is also significantly worse than 
the use of prosodic features alone (12.76%). Simi- 
larly, the error using ASR confidence scores and the 
ASR grammar (17.77%) is significantly worse than 
prosodic features alone (12.76%). Thus, prosodic 
features, either alone or in conjunction with tradi- 
tional ASR features, significantly outperform these 
traditional features alone for predicting WER-based 
misrecognitions. 
Another interesting finding from our experiments 
is the predictive power of information available to 
current ASR systems but not made use of in calcu- 
lating rejection likelihoods, the identity of the recog- 
nized string. This feature is in fact the best perform- 
ing single feature in predicting our data (15.24%). 
And, at a 95% confidence level, the error using 
ASR confidence scores, the recognized string, and 
grammar (9.01%) matches the performance of our 
best performing feature set (6.53%). It seems that, 
at least in our task and for our ASR system, the 
appearance of particular words in the recognized 
strings is an extremely useful cue to recognition ac- 
curacy. So, even by making use of information cur- 
rently available from the traditional ASR process, 
ASR systems could improve their performance on 
identifying rejections by a considerable margin. A 
caveat here is that this feature, like grammar state, 
is unlikely to generalize from task to task or recog- 
nizer to recognizer, but these findings suggest hat 
both should be considered as a means of improving 
rejection performance in stable systems. 
The classification model earned from the best per- 
forming feature set in Table 3 is shown in Figure 2. 6 
The first rule RIPPER finds with this feature set is 
that if the user turn is less than .9 seconds and the 
recognized string contains the word "yes" (and possi- 
bly other words as well), with an acoustic onfidence 
score > -2.6, then predict hat the turn will be cor- 
rectly recognized.7 Note that all of the prosodic fea- 
6Rules are presented in order of importance in classifying 
data. When multiple rules are applicable, RIPPER uses the 
first rule. 
7The confidence scores observed in our data ranged from 
a high of -0.087662 to a low of-9.884418. 
222 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
else F 
< 0.897073) A (confidence > -2.62744 ) A (string contains 'yes') then T 
< 1.03872 ) A (confidence > -2.69775) A (string contains 'no') then T 
< 0.982051) A (confidence > -1.99705) A (tempo > 3.1147) then T 
< 0.813633) A (duration > 0.642652) A (confidence > -3.33945) A (F0 Mean > 176.794) then T 
< 1.30312) A (confidence > -3.37301) A (% silences ~_ 0.647059) then T 
0.610734) A (confidence > -3.37301) A (% silences > 0.521739) then T 
< 1.09537) A (string contains 'Baltimore') then T 
< 0.982051) A (string contains 'no') then T 
< 1.1803) A (confidence > -2.93085) A (grammar ---- date) then T 
< 1.09537) A (confidence > -2.30717) A (% silences > 0.356436) A (F0 Max > 249.225) then T 
< 0.868743) A (confidence > -4.14926 ) A (% silences > 0.51923) A (F0 Max > 205.296) then T 
< 1.18036) A (string contains 'Philadelphia') then T 
Figure 2: Ruleset for Predicting Correctly Recognized Turns (WER = 0) from Prosodic and ASR Features. 
tures except for RMS mean, max, and prior pause 
appear in at least one rule, and that the features 
shown to be significant in our statistical analyses 
(Section 3) are not the same features as in the rules. 
But, as noted above, our data points in these two 
experiments differ. It is useful to note though, that 
while this ruleset contains all three ASR features, 
none of the experimental parameters was found to 
be a useful predictor, suggesting that our results are 
not specific to the particular conditions of and par- 
ticipants in the corpus collection, although they are 
specific to the lexicon and grammars. 
Results of our learning experiments with mis- 
recognition defined in terms of CA rather than WER 
show the overall role of the features which predict 
WER-defined misrecognition to be less successful 
in predicting CA-defined error. Table 4 shows the 
relative performance of the same feature sets dis- 
cussed above, with misrecognition ow defined in 
terms of CA<I.  As with the WER experiments, the 
best performing feature set makes use of prosodic 
and ASR-derived features. However, the predictive 
power of prosodic over ASR features decreases when 
misrecognition is defined in terms of CA - -  which is 
particularly interesting since ASR confidence scores 
are intended to predict WER rather than CA; the er- 
ror rate using ASR confidence scores alone (13.52%) 
is now significantly lower than the error obtained 
using prosody (18.18%). However, prosodic features 
still improve the predictive power of ASR confidence 
scores, to 11.34%, although this difference is not sig- 
nificant at a 95% confidence level. And the error 
rate of the three ASR features combined (11.70%) is 
reduced to the lowest error rate in our table when 
prosodic features are added (10.43%); this error rate 
is (just) significantly different from the use of ASR 
confidence scores alone. Thus, for CA-defined mis- 
recognitions, our experiments have uncovered only 
minor improvements over traditional ASR rejection 
calculation procedures. 
5 D iscuss ion  
A statistical comparison of recognized versus mis- 
recognized utterances indicates that F0 excursion, 
loudness, longer prior pause, and longer duration 
are significant prosodic haracteristics of both WER 
and CA-defined failed recognition attempts. Results 
from a set of machine learning experiments show 
that prosodic differences can in fact be used to im- 
prove the prediction of misrecognitions with a high 
degree of accuracy (12.76% error) for WER-based 
misrecognit ions-  and an even higher degree (6.53% 
error) when combined with information currently 
available from ASR systems. The use of ASR confi- 
dence scores alone had a predicted WER of 22.23%, 
so the improvement over traditional methods is quite 
considerable. For CA-defined misrecognitions, the 
improvement provided by prosodic features is con- 
siderably less. One of our future research directions 
will be to understand this difference. 
Another future direction will be to address the is- 
sue of just why  prosodic features provide such use- 
ful indicators of recognition failure. Do the features 
themselves make recognition difficult, or are they 
instead indirect correlates of other phenomena not 
captured in our study? While the negative influence 
of speaking rate variation on ASR has been reported 
before (e.g. (Ostendorf et al, 1996), it is tradition- 
ally assumed that ASR is impervious to differences 
in F0 and RMS; yet, it is known that F0 and RMS 
variations co-vary to some extent with spectral char- 
acteristics (e.g. (Swerts and Veldhuis, 1997; Fant et 
al., 1995)), so that it is not unlikely that utterances 
with extreme values for these may differ critically 
from the training data. Other prosodic features may 
be more indirect indicators of errors. Longer ut- 
terances may simply provide more chance for error 
than shorter ones, while speakers who pause longer 
before utterances and take more time making them 
may also produce more disfluencies than others. 
We are currently replicating our experiment on a 
new domain with a new speech recognizer. We are 
examining the W99 corpus, which was collected in a 
223 
Table 4: Estimated Error for Predicting Misrecognized Turns (CA<l). 
Features Used \[ Error 
Prosody, ASR Confidence~ ASR String, ASR Grammar 10.43% .63 
ALL 10.68% .71 
Prosody, ASR Confidence, ASR Grammar 11.24% .68 
Prosody, ASR Confidence 11.34% .64 
ASR Confidence, ASR String, ASR Grammar 11.70% .68 
ASR Confidence 13.52% .82 
ASR Confidence, ASR Grammar 13.52% .84 
ASR String 13.62% .83 
Prosody, ASR String 15.04% .84 
Prosody 18.18% .85 
Duration 18.38% .90 
ASR Grammar 22.73% .96 
Tempo 24.61% 1.28 
Hyperarticulation 25.27% 1.05 
F0 Mean 28.61% 1.19 
F0 Max 28.76% .90 
RMS Mean 28.86% 1.17 
% Silence 28.91% 1.23 
RMS Max 29.01% 1.16 
Prior Pause 29.22% 1.26 
Majority Baseline \[ 28.61% 
spoken dialogue system that supported registration, 
checking paper status, and information access for the 
IEEE Automatic Speech Recognition and Under- 
standing Workshop (ASRU99) (Rahim et al, 1999). 
This system employed the AT&T WATSON speech 
recognition technology (Sharp et al, 1997). Prelim- 
inary results indicate that our TOOT results do in 
fact hold up across recognizers. We also are extend- 
ing our TOOT corpus analysis to include prosodic 
analyses of turns in which users become aware of 
misrecognitions and correct them. In addition, we 
are exploring whether prosodic differences can help 
explain the "goat" phenomenon - -  the fact that 
some voices are recognized much more poorly than 
others (Doddington et al, 1998; Hirschberg et al, 
1999). Our ultimate goal is to provide prosodically- 
based mechanisms for identifying and reacting to 
ASR failures in SDS systems. 
Acknowledgements  
We would like to thank Jennifer Chu-Carroll, Candy 
Kamm, participants in the AT&T "SLUG" seminar 
series, and participants in the 1999 JHU Summer 
Language Engineering Workshop, for providing us 
with useful comments on this research and on earlier 
versions of this paper. 
References 
Linda Bell and Joakim Gustafson. 1999. Repe- 
tition and its phonetic realizations: Investigat- 
ing a Swedish database of spontaneous computer- 
directed speech. In Proceedings of ICPhS-99, San 
Francisco. International Congress of Phonetic Sci- 
ences. 
E. Blaauw. 1992. Phonetic differences between read 
and spontaneous speech. In Proceedings of IC- 
SLP92, volume 1, pages 751-758, Banff. 
A. G. Bouwman, J. Sturm, and L. Boves. 1999. 
Incorporating confidence measures in the dutch 
train timetable information system developed in 
the ARISE project. In Proc. International Con- 
ference on Acoustics, Speech and Signal Process- 
ing, volume 1, pages 493-496, Phoenix. 
William Cohen. 1996. Learning trees and rules with 
set-valued features. In l$th Conference of the 
American Association of Artificial Intelligence, 
AAAI. 
George Doddington, Walter Liggett, Alvin Martin, 
Mark Przybocki, and Douglas Reynolds. 1998. 
Sheep, goats, lambs and wolves: A statistical anal- 
ysis of speaker performance in the NIST 1998 
speaker ecognition evaluation. In Proceedings of 
ICSLP-98. 
G. Fant, J. Liljencrants, I. Karlsson, and 
M. B?veg?rd. 1995. Time and frequency do- 
main aspects of voice source modelling. BR 
Speechmaps 6975, ESPRIT. Deliverable 27 WP 
1.3. 
Keikichi Hirose. 1997. Disambiguating recogni- 
tion results by prosodic features. In Computing 
224 
Prosody: Computational Models for Processing 
Spontaneous Speech, pages 327-342. Springer. 
Julia Hirschberg, Diane Litman, and Marc Swerts. 
1999. Prosodic cues to recognition errors. In Pro- 
ceedings of the Automatic Speech Recognition and 
Understandin9 Workshop (ASRU'99). 
Julia Hirschberg. 1991. Using text analysis to pre- 
dict intonational boundaries. In Proceedings of the 
Second European Conference on Speech Commu- 
nication and Technology, Genova. ESCA. 
C. Kamm, S. Narayanan, D. Dutton, and R. Rite- 
nour. 1997. Evaluating spoken dialog systems 
for telecommunication services. In 5th European 
Conference on Speech Technology and Communi- 
cation, EUROSPEECH 97. 
Hans Kraayeveld. 1997. Idiosyncrasy in prosody. 
Speaker and speaker group identification i  Dutch 
using melodic and temporal information. Ph.D. 
thesis, Nijmegen University. 
E. Krahmer, M. Swerts, M. Theune, and 
M. Weegels. 1999. Error spotting in human- 
machine interactions. In Proceedings of 
E UR OSPEECH- 99. 
Irene Langkilde, Marilyn Walker, Jerry Wright, 
A1 Gorin, and Diane Litman. 1999. Automatic 
prediction of problematic human-computer dia- 
logues in 'how may i help you?'. In Proceedings 
of the Automatic Speech Recognition and Under- 
standin 9 Workshop (ASRU'99). 
Gina-Anne Levow. 1998. Characterizing and recog- 
nizing spoken corrections in human-computer dia- 
logue. In Proceedings of the 36th Annual Meeting 
of the Association of Computational Linguistics, 
COLING/ACL 98, pages 736-742. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. 
In Proceedings of the 7th International Conference 
on User Modeling (UM). 
Diane J. Litman, Marilyn A. Walker, and Michael J. 
Kearns. 1999. Automatic detection of poor 
speech recognition at the dialogue level. In Pro- 
ceedings of the 37th Annual Meeting of the As- 
sociation of Computational Linguistics , ACL99, 
pages 309-316. 
M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, 
A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, 
D. Talkin, A. Waibel, B. Wheatley, and T. Zep- 
penfeld. 1996. Modeling systematic variations 
in pronunciation via a language-dependent hid- 
den speaking mode. Report on 1996 CLSP/JHU 
Workshop on Innovative Techniques for Large Vo- 
cabulary Continuous Speech Recognition. 
S. L. Oviatt, G. Levow, M. MacEarchern, and 
K. Kuhn. 1996. Modeling hyperarticulate speech 
during human-computer error resolution. In Pro- 
ceedings of ICSLP-96, pages 801-804, Philadel- 
phia. 
M. Rahim, R. Pieracini, W. Eckert, E. Levin, G. Di 
Fabbrizio, G. Riccardi, C. Lin, and C. Kamm. 
1999. W99 - a spoken dialogue system for the 
asru'99 workshop. In Proc. ASRU'99. 
R.D. Sharp, E. Bocchieri, C. Castillo, 
S. Parthasarathy, C. Rath, M. Riley, and 
J Rowland. 1997. The watson speech recognition 
engine. In Proc. ICASSP97, pages 4065-4068. 
Ronnie W. Smith. 1998. An evaluation of strate- 
gies for selectively verifying utterance meanings 
in spoken natural language dialog. International 
Journal of Human- Computer Studies, 48:627-647. 
Hagen Soltau and Alex Waibel. 1998. On the in- 
fluence of hyperarticulated speech on recognition 
performance. In Proceedings of ICSLP-98, Syd- 
ney. International Conference on Spoken Lan- 
guage Processing. 
M. Swerts and M. Ostendorf. 1997. Prosodic 
and lexical indications of discourse structure in 
human-machine interactions. Speech Communica- 
tion, 22:25-41. 
Marc Swerts and Raymond Veldhuis. 1997. Interac- 
tions between intonation and glottal-pulse char- 
acteristics. In A. Botinis, G. Kouroupetroglou, 
and G. Carayiannis, editors, Intonation: Theory, 
Models and Applications, pages 297-300, Athens. 
ESCA. 
Nanette Veilleux. 1994. Computational Models of 
the Prosody/Syntax Mapping for Spoken Language 
Systems. Ph.D. thesis, Boston University. 
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User 
behaviors affecting speech recognition. In Pro- 
ceedings of ICSLP-92, volume 2, pages 995-998, 
Banff. 
M. Weintraub, K. Taussig, K. Hunicke-Smith, and 
A. Snodgrass. 1996. Effect of speaking style on 
LVCSR performance. In Proceedings of ICSLP- 
96, Philadelphia. International Conference on 
Spoken Language Processing. 
Ilija Zeljkovic. 1996. Decoding optimal state se- 
quences with smooth state likelihoods. In Interna- 
tional Conference on Acoustics, Speech, and Sig- 
nal Processing, ICASSP 96, pages 129-132. 
225 
 
		Detecting problematic turns in human-machine interactions:
Rule-induction versus memory-based learning approaches
Antal van den Bosch  
 ILK / Comp. Ling.
KUB, Tilburg
The Netherlands
antalb@kub.nl
Emiel Krahmer

 

IPO
TU/e, Eindhoven
The Netherlands
E.J.Krahmer@tue.nl
Marc Swerts
 

CNTS
UIA, Antwerp
Belgium
M.G.J.Swerts@tue.nl
Abstract
We address the issue of on-line detec-
tion of communication problems in
spoken dialogue systems. The useful-
ness is investigated of the sequence of
system question types and the word
graphs corresponding to the respective
user utterances. By applying both rule-
induction and memory-based learning
techniques to data obtained with a
Dutch train time-table information
system, the current paper demonstrates
that the aforementioned features indeed
lead to a method for problem detec-
tion that performs significantly above
baseline. The results are interesting
from a dialogue perspective since they
employ features that are present in the
majority of spoken dialogue systems
and can be obtained with little or no
computational overhead. The results
are interesting from a machine learning
perspective, since they show that the
rule-based method performs signific-
antly better than the memory-based
method, because the former is better
capable of representing interactions
between features.
1 Introduction
Given the state of the art of current language and
speech technology, communication problems are
unavoidable in present-day spoken dialogue sys-
tems. The main source of these problems lies
in the imperfections of automatic speech recogni-
tion, but also incorrect interpretations by the nat-
ural language understanding module or wrong de-
fault assumptions by the dialogue manager are
likely to lead to confusion. If a spoken dialogue
system had the ability to detect communication
problems on-line and with high accuracy, it might
be able to correct certain errors or it could in-
teract with the user to solve them. For instance,
in the case of communication problems, it would
be beneficial to change from a relatively natural
dialogue strategy to a more constrained one in
order to resolve the problems (see e.g., Litman
and Pan 2000). Similarly, it has been shown that
users switch to a ?marked?, hyperarticulate speak-
ing style after problems (e.g., Soltau and Waibel
1998), which itself is an important source of re-
cognition errors. This might be solved by using
two recognizers in parallel, one trained on nor-
mal speech and one on hyperarticulate speech. If
there are communication problems, then the sys-
tem could decide to focus on the recognition res-
ults delivered by the engine trained on hyperartic-
ulate speech.
For such approaches to work, however, it is
essential that the spoken dialogue system is able
to automatically detect communication problems
with a high accuracy. In this paper, we investigate
the usefulness for problem detection of the word
graph and the history of system question types.
These features are present in many spoken dia-
logue systems and do not require additional com-
putation, which makes this a very cheap method
to detect problems. We shall see that on the basis
of the previous and the current word graph and the
six most recent system question types, communic-
ation problems can be detected with an accuracy
of 91%, which is a significant improvement over
the relevant baseline. This shows that spoken dia-
logue systems may use these features to better pre-
dict whether the ongoing dialogue is problematic.
In addition, the current work is interesting from
a machine learning perspective. We apply two
machine learning techniques: the memory-based
IB1-IG algorithm (Aha et al 1991, Daelemans et
al. 1997) and the RIPPER rule induction algorithm
(Cohen 1996). As we shall see, some interesting
differences between the two approaches arise.
2 Related work
Recently there has been an increased interest in
developing automatic methods to detect problem-
atic dialogue situations using machine learning
techniques. For instance, Litman et al (1999)
and Walker et al (2000a) use RIPPER (Cohen
1996) to classify problematic and unproblematic
dialogues. Following up on this, Walker et al
(2000b) aim at detecting problems at the utter-
ance level, based on data obtained with AT&Ts
How May I Help You (HMIHY) system (Gorin et
al. 1997). Walker and co-workers apply RIPPER to
43 features which are automatically generated by
three modules of the HMIHY system, namely the
speech recognizer (ASR), the natural language un-
derstanding module (NLU) and the dialogue man-
ager (DM). The best result is obtained using all
features: communication problems are detected
with an accuracy of 86%, a precision of 83% and
a recall of 75%. It should be noted that the NLU
features play first fiddle among the set of all fea-
tures. In fact, using only the NLU features per-
forms comparable to using all features. Walker et
al. (2000b) also briefly compare the performance
of RIPPER with some other machine learning ap-
proaches, and show that it performs comparable
to a memory-based (instance-based) learning al-
gorithm (IB, see Aha et al 1991).
The results which Walker and co-workers de-
scribe show that it is possible to automatically de-
tect communication problems in the HMIHY sys-
tem, using machine learning techniques. Their ap-
proach also raises a number of interesting follow-
up questions, some concerned with problem de-
tection, others with the use of machine learning
techniques. (1) Walker et al train their classi-
fier on a large set of features, and show that the
set of features produced by the NLU module are
the most important ones. However, this leaves an
important general question unanswered, namely
which particular features contribute to what ex-
tent? (2) Moreover, the set of features which the
NLU module produces appear to be rather spe-
cific to the HMIHY system and indicate things like
the percentage of the input covered by the relev-
ant grammar fragment, the presence or absence of
context shifts, and the semantic diversity of sub-
sequent utterances. Many current day spoken dia-
logue systems do not have such a sophisticated
NLU module, and consequently it is unlikely that
they have access to these kinds of features. In
sum, it is uncertain whether other spoken dialogue
systems can benefit from the findings described by
Walker et al (2000b), since it is unclear which fea-
tures are important and to what extent these fea-
tures are available in other spoken dialogue sys-
tems. Finally, (3) we agree with Walker et al (and
the machine learning community at large) that it is
important to compare different machine learning
techniques to find out which techniques perform
well for which kinds of tasks. Walker et al found
that RIPPER does not perform significantly better
or worse than a memory-based learning technique.
Is this incidental or does it reflect a general prop-
erty of the problem detection task?
The current paper uses a similar methodology
for on-line problem detection as Walker et al
(2000b), but (1) we take a bottom-up approach,
focussing on a small number of features and in-
vestigating their usefulness on a per-feature basis
and (2) the features which we study are automat-
ically available in the majority of current spoken
dialogue system: the sequence of system ques-
tion types and the word graphs corresponding to
the respective user utterances. A word graph
is a lattice of word hypotheses, and we conjec-
ture that various features which have been shown
to cue communication problems (prosodic, lin-
guistic and ASR features, see e.g., Hirschberg et
al. 1999, Krahmer et al 1999 and Swerts et al
2000) have correlates in the word graph. The se-
quence of system question types is taken to model
the dialogue history. Finally, (3) to gain further in-
sight into the adequacy of various machine learn-
ing techniques for problem detection we use both
RIPPER and the memory-based IB1-IG algorithm.
3 Approach
3.1 Data and Labeling The corpus we used con-
sisted of 3739 question-answer pairs, taken from
444 complete dialogues. The dialogues consist
of users interacting with a Dutch spoken dialogue
system which provides information about train
time tables. The system prompts the user for un-
known slots, such as departure station, arrival sta-
tion, date, etc., in a series of questions. The sys-
tem uses a combination of implicit and explicit
verification strategies.
The data were annotated with a highly limited
set of labels. In particular, the kind of system
question and whether the reply of the user gave
rise to communication problems or not. The latter
feature is the one to be predicted. The following
labels are used for the system questions.
O open questions (?From where to where do you
want to travel??)
I implicit verification (?When do you want to
travel from Tilburg to Schiphol Airport??)
E explicit verification (?So you want to travel
from Tilburg to Schiphol Airport??)
Y yes/no question (?Do you want me to repeat the
connection??)
M Meta-questions (?Can you please correct
me??)
The difference between an explicit verification
and a yes/no question is that the former but not
the latter is aimed at checking whether what the
system understood or assumed corresponds with
what the user wants. If the current system ques-
tion is a repetition of the previous question it
asked, this is indicated by the suffix R. A ques-
tion only counts as a repetition when it has the
same contents as the previous system question. Of
the user inputs, we only labeled whether they gave
rise to a communication problem or not. A com-
munication problem arises when the value which
the system assigns to a particular slot (departure
station, date, etc.) does not coincide with the
value given for that particular slot by the user in
his or her most recent contribution to the dialogue
or when the system makes an incorrect default as-
sumption (e.g., the dialogue manager assumes that
the date slot should be filled with the current date,
i.e., that the user wants to travel today). Commu-
nication problems are generally easy to label since
the spoken dialogue system under consideration
here always provides direct feedback (via verific-
ation questions) about what it believes the user in-
tends. Consider the following exchange.
U: I want to go to Amsterdam.
S: So you want to go to Rotterdam?
As soon as the user hears the explicit verification
question of the system, it will be clear that his or
her last turn was misunderstood. The problem-
feature was labeled by two of the authors to
avoid labeling errors. Differences between the
two annotators were infrequent and could always
easily be resolved.
3.2 Baselines Of the 3739 user utterances
1564 gave rise to communication problems (an
error rate of 41.8%). The majority class is thus
formed by the unproblematic user utterances,
which form 58.2% of all user utterances. This
suggests that the baseline for predicting com-
munication problems is obtained by always
predicting that there are no communication prob-
lems. This strategy has an accuracy of 58.2%,
and a recall of 0% (all problems are missed). 
The precision is not defined, 	 and consequently
neither is the 


.  This baseline is misleading,
however, when we are interested in predicting
whether the previous user utterance gave rise to
communication problems. There are cases when
the dialogue system is itself clearly aware of
communication problems. This is in particular
the case when the system repeats the question
(labeled with the suffix R) or when it asks a meta-
question (M). In the corpus under investigation
here this happens 1024 times. It would not be

For definitions of accuracy, precision and recall see e.g.,
Manning and Schu?tze (1999:268-269).

Since 0 cases are selected, one would have to divide by
0 to determine precision for this baseline.

Throughout this paper we use the   measure (van
Rijsbergen 1979:174) to combine precision and recall in a
single measure. By setting  equal to 1, precision and recall
are given an equal weight, and the  measure simplifies to
ffPredicting User Reactions to System Error
Diane Litman and Julia Hirschberg
AT&T Labs?Research
Florham Park, NJ, 07932 USA
 
diane/julia  @research.att.com
Marc Swerts
IPO, Eindhoven, The Netherlands,
and CNTS, Antwerp, Belgium
m.g.j.swerts@tue.nl
Abstract
This paper focuses on the analysis and
prediction of so-called aware sites,
defined as turns where a user of a
spoken dialogue system first becomes
aware that the system has made a
speech recognition error. We describe
statistical comparisons of features of
these aware sites in a train timetable
spoken dialogue corpus, which re-
veal significant prosodic differences
between such turns, compared with
turns that ?correct? speech recogni-
tion errors as well as with ?normal?
turns that are neither aware sites nor
corrections. We then present machine
learning results in which we show how
prosodic features in combination with
other automatically available features
can predict whether or not a user turn
was a normal turn, a correction, and/or
an aware site.
1 Introduction
This paper describes new results in our continu-
ing investigation of prosodic information as a po-
tential resource for error recovery in interactions
between a user and a spoken dialogue system. In
human-human interaction, dialogue partners ap-
ply sophisticated strategies to detect and correct
communication failures so that errors of recog-
nition and understanding rarely lead to a com-
plete breakdown of the interaction (Clark and
Wilkes-Gibbs, 1986). In particular, various stud-
ies have shown that prosody is an important cue
in avoiding such breakdown, e.g. (Shimojima et
al., 1999). Human-machine interactions between
a user and a spoken dialogue system (SDS) ex-
hibit more frequent communication breakdowns,
due mainly to errors in the Automatic Speech Re-
cognition (ASR) component of these systems. In
such interactions, however, there is also evidence
showing prosodic information may be used as a
resource for error recovery. In previous work,
we identified new procedures to detect recogni-
tion errors. In particular, we found that pros-
odic features, in combination with other inform-
ation already available to the recognizer, can dis-
tinguish user turns that are misrecognized by the
system far better than traditional methods used in
ASR rejection (Litman et al, 2000; Hirschberg et
al., 2000). We also found that user corrections
of system misrecognitions exhibit certain typical
prosodic features, which can be used to identify
such turns (Swerts et al, 2000; Hirschberg et al,
2001). These findings are consistent with previ-
ous research showing that corrections tend to be
hyperarticulated ? higher, louder, longer . . . than
other turns (Wade et al, 1992; Oviatt et al, 1996;
Levow, 1998; Bell and Gustafson, 1999).
In the current study, we focus on another turn
category that is potentially useful in error hand-
ling. In particular, we examine what we term
aware sites ? turns where a user, while interact-
ing with a machine, first becomes aware that the
system has misrecognized a previous user turn.
Note that such aware sites may or may not also be
corrections (another type of post-misrecognition
turn), since a user may not immediately provide
correcting information. We will refer to turns
that are both aware sites and corrections as corr-
awares, to turns that are only corrections as corrs,
to turns that are only aware sites as awares, and to
turns that are neither aware sites nor corrections as
norm.
We believe that it would be useful for the
dialogue manager in an SDS to be able to de-
tect aware sites for several reasons. First, if
aware sites are detectable, they can function as
backward-looking error-signaling devices, mak-
ing it clear to the system that something has gone
wrong in the preceding context, so that, for ex-
ample, the system can reprompt for information.
In this way, they are similar to what others have
termed ?go-back? signals (Krahmer et al, 1999).
Second, aware sites can be used as forward-
looking signals, indicating upcoming corrections
or more drastic changes in user behavior, such
as complete restarts of the task. Given that, in
current systems, both corrections and restarts of-
ten lead to recognition error (Swerts et al, 2000),
aware sites may be useful in preparing systems to
deal with such problems.
In this paper, we investigate whether aware
sites share acoustic properties that set them apart
from normal turns, from corrections, and from
turns which are both aware sites and corrections.
We also want to test whether these different turn
categories can be distinguished automatically, via
their prosodic features or from other features
known to or automatically detectible by a spoken
dialogue system. Our domain is the TOOT spoken
dialogue corpus, which we describe in Section 2.
In Section 3, we present some descriptive findings
on different turn categories in TOOT. Section 4
presents results of our machine learning experi-
ments on distinguishing the different turn classes.
In Section 5 we summarize our conclusions.
2 Data
The TOOT corpus was collected using an experi-
mental SDS developed for the purpose of compar-
ing differences in dialogue strategy. It provides
access to train information over the phone and
is implemented using an internal platform com-
bining ASR, text-to-speech, a phone interface,
and modules for specifying a finite-state dialogue
manager, and application functions. Subjects per-
formed four tasks with versions of TOOT, which
varied confirmation type and locus of initiative
(system initiative with explicit system confirma-
tion, user initiative with no system confirmation
until the end of the task, mixed initiative with im-
plicit system confirmation), as well as whether
the user could change versions at will using voice
commands. Subjects were 39 students, 20 nat-
ive speakers of standard American English and
19 non-native speakers; 16 subjects were female
and 23 male. The exchanges were recorded and
the system and user behavior logged automatic-
ally. Dialogues were manually transcribed and
user turns automatically compared to the corres-
ponding ASR (one-best) recognized string to pro-
duce a word accuracy score (WA) for each turn.
Each turn?s concept accuracy (CA) was labeled
by the experimenters from the dialogue recordings
and the system log; if the recognizer correctly cap-
tured all the task-related information given in the
user?s original input (e.g. date, time, departure or
arrival cities), the turn was given a CA score of
1, indicating a semantically correct recognition.
Otherwise, the CA score reflected the percentage
of correctly recognized task concepts in the turn.
For the study described below, we examined 2328
user turns from 152 dialogues generated during
these experiments. 194 of the 2320 turns were re-
jected by the system.
To identify the different turn categories in the
corpus, two authors independently labeled each
turn as to whether or not it constituted a correction
of a prior system failure (a CA error or a rejection)
and what turn was being corrected, and whether
or not it represented an aware site for a prior fail-
ure, and, if so, the turn which the system had failed
on. Labeler disagreement was subsequently re-
solved by consensus. The fragment in Figure 1,
produced with a version of TOOT in which the
user has the initiative with no confirmation until
the end of the task, illustrates these labels. This
example illustrates cases of corraware, in which
both the user?s awareness and correction of a mis-
recognition occur in the same turn (e.g. turns
1159 and 1160, after system prompts for informa-
tion already given in turn 1158). It also illustrates
cases in which aware sites and corrections occur
in different turns. For example, after the immedi-
ate explicit system confirmation of turn 1162, the
user first becomes aware of the system errors (turn
1163), then separately corrects them (turn 1164);
turn 1163 is thus an aware turn and turn 1164 a
corr. When no immediate confirmation of an ut-
terance occurs (as with turn 1158), it may take sev-
eral turns before the user becomes aware of any
Turn Turn ID Aware of Corr of Type
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1158 NA NA NORM
S: Which city do you want to go to?
U: New York City 1159 1158 1158 CORRAWARE
S: Which city do you want to leave from?
U: Baltimore 1160 1158 1158 CORRAWARE
S: Do you want me to find the trains from
Baltimore to New York City today at anytime now?
U: No 1161 1158 NA AWARE
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1162 NA 1158 CORR
S: Do you want me to find the trains from
Baltimore to New York City today at anytime now?
U: No 1163 1162 NA AWARE
S: How may I help you?
U: Get me the train on Sunday at eight thirty pm 1164 NA 1162 CORR
Figure 1: Dialogue Fragment with Aware and Correction Labels.
misrecognition errors. For example, it is not un-
til turn 1161 that the user first becomes aware of
the error in date and time from 1158; the user then
corrects the error in 1162. So, 1161 is classified as
an aware and 1162 as a corr. Note that corr turns
represent 13% of the turns in our corpus, awares
represent 14%, corrawares account for 16%, and
norm turns represent 57% of the turns in the cor-
pus.
3 Descriptive Analysis and Results
We examined prosodic features for each user turn
which had previously been shown to be useful for
predicting misrecognized turns and corrections:
maximum and mean fundamental frequency val-
ues (F0 Max, F0 Mean), maximum and mean en-
ergy values (RMS Max, RMS Mean), total dur-
ation (Dur), length of pause preceding the turn
(Ppau), speaking rate (Tempo) and amount of si-
lence within the turn (%Sil). F0 and RMS val-
ues, representing measures of pitch excursion and
loudness, were calculated from the output of En-
tropic Research Laboratory?s pitch tracker, get f0,
with no post-correction. Timing variation was
represented by four features. Duration within and
length of pause between turns was computed from
the temporal labels associated with each turn?s be-

While the features were automatically computed, begin-
nings and endings were hand segmented from recordings of
the entire dialogue, as the turn-level speech files used as in-
put in the original recognition process created by TOOT were
unavailable.
ginning and end. Speaking rate was approximated
in terms of syllables in the recognized string per
second, while %Sil was defined as the percentage
of zero frames in the turn, i.e., roughly the per-
centage of time within the turn that the speaker
was silent.
To see whether the different turn categories
were prosodically distinct from one another, we
applied the following procedure. We first calcu-
lated mean values for each prosodic feature for
each of the four turn categories produced by each
individual speaker. So, for speaker A, we divided
all turns produced into four classes. For each
class, we then calculated mean F0 Max, mean F0
Mean, and so on. After this step had been repeated
for each speaker and for each feature, we then cre-
ated four vectors of speaker means for each indi-
vidual prosodic feature. Then, for each prosodic
feature, we ran a one-factor within subjects anova
on the means to learn whether there was an overall
effect of turn category.
Table 1 shows that, overall, the turn categor-
ies do indeed differ significantly with respect to
different prosodic features; there is a signific-
ant, overall effect of category on F0 Max, RMS
Max, RMS Mean, Duration, Tempo and %Sil. To
identify which pairs of turns were significantly
different where there was an overall significant ef-
fect, we performed posthoc paired t-tests using the
Bonferroni method to adjust the p-level to 0.008
(on the basis of the number of possible pairs that
Turn categories
Feature Normal Correction Aware Corraware  -stat
***F0 Max (Hz) 220.05 263.40 216.87 229.00 
	  =10.477
F0 Mean (Hz) 161.78 173.43 162.61 158.24 
	  =1.575
***RMS Max (dB) 1484.14 1833.62 1538.91 1925.38 
	  =7.548
*RMS Mean (dB) 372.47 379.65 425.96 464.16  
	  =3.190
***Dur (sec) 1.43 4.39 1.12 2.33 
	  =34.418
Ppau (sec) 0.60 0.93 0.87 0.80 
	  =1.325
**Tempo (syls/sec) 2.59 2.38 2.16 2.43  
	  =4.206
*%Sil (sec) 0.46 0.41 0.44 0.42  
	  =3.182
Significance level: *(p  .05), **(p  .01), ***(p  .001)
Table 1: Mean Values of Prosodic Features for Turn Categories.
Prosodic features
Classes F0 max F0 mean RMS max RMS mean Dur Ppau Tempo %Sil
norm/corr ? ? ? +
norm/aware +
norm/corraware ? ?
aware/corr ? ? ? ?
aware/corraware ? ? ?
corraware/corr ? ?
Table 2: Pairwise Comparisons of Different Turn Categories by Prosodic Feature.
can be drawn from an array of 4 means). Res-
ults are summarized in Table 2, where ? + ? or
? ? ? indicates that the feature value of the first cat-
egory is either significantly higher or lower than
the second. Note that, for each of the pairs, there
is at least one prosodic feature that distinguishes
the categories significantly, though it is clear that
some pairs, like aware vs. corr and norm vs. corr
appear to have more distinguishing features than
others, like norm vs. aware. It is also interesting to
see that the three types of post-error turns are in-
deed prosodically different: awares are less prom-
inent in terms of F0 and RMS maximum than cor-
rawares, which, in turn, are less prominent than
corrections, for example. In fact, awares, except
for duration, are prosodically similar to normal
turns.
4 Predictive Results
We next wanted to determine whether the pros-
odic features described above could, alone or
in combination with other automatically avail-
able features, be used to predict our turn categor-
ies automatically. This section describes experi-
ments using the machine learning program RIP-
PER (Cohen, 1996) to automatically induce pre-
diction models from our data. Like many learn-
ing programs, RIPPER takes as input the classes
to be learned, a set of feature names and possible
values, and training data specifying the class and
feature values for each training example. RIPPER
outputs a classification model for predicting the
class of future examples, expressed as an ordered
set of if-then rules. The main advantages of RIP-
PER for our experiments are that RIPPER supports
?set-valued? features (which allows us to repres-
ent the speech recognizer?s best hypothesis as a set
of words), and that rule output is an intuitive way
to gain insight into our data.
In the current experiments, we used 10-fold
cross-validation to estimate the accuracy of the
rulesets learned. Our predicted classes corres-
pond to the turn categories described in Section
2 and variations described below. We repres-
ent each user turn using the feature set shown in
Figure 2, which we previously found useful for
predicting corrections (Hirschberg et al, 2001).
A subset of the features includes the automatic-
ally computable raw prosodic features shown in
Table 1 (Raw), and normalized versions of these
features, where normalization was done by first
turn (Norm1) or by previous turn (Norm2) in a
dialogue. The set labeled ?ASR? contains stand-
ard input and output of the speech recognition pro-
cess, which grammar was used for the dialogue
state the system believed the user to be in (gram),
Raw: f0 max, f0 mean, rms max, rms mean, dur, ppau,
tempo, %sil;
Norm1: f0 max1, f0 mean1, rms max1, rms mean1, dur1,
ppau1, tempo1, %sil1;
Norm2: f0 max2, f0 mean2, rms max2, rms mean2, dur2,
ppau2, tempo2, %sil2;
ASR: gram, str, conf, ynstr, nofeat, canc, help, wordsstr,
syls, rejbool;
System Experimental: inittype, conftype, adapt, realstrat;
Dialogue Position: diadist;
PreTurn: features for preceding turn (e.g., pref0max);
PrepreTurn: features for preceding preceding turn (e.g.,
ppref0max);
Prior: for each boolean-valued feature (ynstr, nofeat,
canc, help, rejbool), the number/percentage of
prior turns exhibiting the feature (e.g., prioryn-
strnum/priorynstrpct);
PMean: for each continuous-valued feature, the mean of the
feature?s value over all prior turns (e.g., pmnf0max);
Figure 2: Feature Set.
the system?s best hypothesis for the user input
(str), and the acoustic confidence score produced
by the recognizer for the turn (conf). As subcases
of the str feature, we also included whether or not
the recognized string included the strings yes or no
(ynstr), some variant of no such as nope (nofeat),
cancel (canc), or help (help), as these lexical items
were often used to signal problems in our sys-
tem. We also derived features to approximate the
length of the user turn in words (wordsstr) and in
syllables (syls) from the str features. And we ad-
ded a boolean feature identifying whether or not
the turn had been rejected by the system (rejbool).
Next, we include a set of features representing
the system?s dialogue strategy when each turn was
produced. These include the system?s current ini-
tiative and confirmation strategies (inittype, conf-
type), whether users could adapt the system?s dia-
logue strategies (adapt), and the combined initiat-
ive/confirmation strategy in effect at the time of
the turn (realstrat). Finally, given that our previ-
ous studies showed that preceding dialogue con-
text may affect correction behavior (Swerts et al,
2000; Hirschberg et al, 2001), we included a fea-
ture (diadist) reflecting the distance of the current
turn from the beginning of the dialogue, and a set
of features summarizing aspects of the prior dia-
logue: for the latter features, we included both the
number of times prior turns exhibited certain char-
acteristics (e.g. priorcancnum) and the percent-
age of the prior dialogue containing one of these
features (e.g. priorcancpct). We also examined
means for all raw and normalized prosodic fea-
tures and some word-based features over the en-
tire dialogue preceding the turn to be predicted
(pmn ). Finally, we examined more local con-
texts, including all features of the preceding turn
(pre ) and for the turn preceding that (ppre ).
We provided all of the above features to the
learning algorithm first to predict the four-way
classification of turns into normal, aware, corr and
corraware. A baseline for this classification (al-
ways predicting norm, the majority class) has a
success rate of 57%. Compared to this, our fea-
tures improve classification accuracy to 74.23%
(+/? 0.96%). Figure 3 presents the rules learned
for this classification. Of the features that appear
in the ruleset, about half are features of current
turn and half features of the prior context. Only
once does a system feature appear, suggesting that
the rules generalize beyond the experimental con-
ditions of the data collection. Of the features spe-
cific to the current turn, prosodic features domin-
ate, and, overall, timing features (dur and tempo
especially) appear most frequently in the rules.
About half of the contextual features are prosodic
ones and half are ASR features, with ASR confid-
ence score appearing to be most useful. ASR fea-
tures of the current turn which appear most often
are string-based features and the grammar state
the system used for recognizing the turn. There
appear to be no differences in which type of fea-
tures are chosen to predict the different classes.
If we express the prediction results in terms of
precision and recall, we see how our classification
accuracy varies for the different turn categories
(Table 3). From Table 3, we see that the majority
class (normal) is most accurately classified. Pre-
dictions for the other three categories, which oc-
cur about equally often in our corpus, vary consid-
erably, with modest results for corr and corraware,
and rather poor results for aware. Table 4 shows a
confusion matrix for the four classes, produced by
if (gram=universal)  (dur2  7.31) then CORR
if (dur2  2.19)  (priornofeatpct  0.09)  (tempo  1.50)  (pmntempo  2.39) then CORR
if (dur2  1.53)  (pmnwordsstr  2.06)  (tempo1  1.07)  (predur  0.80)  (prenofeat=F)  (presyls  4) then CORR
if (predur1  0.26)  (dur  0.79)  (rmsmean2  1.51)  (f0mean  173.49) then CORR
if (dur2  1.41)  (prenofeat=T)  (str contains word ?eight?) then CORR
if (predur1  0.18)  (dur2  4.21)  (dur1  0.50)  (f0mean  276.43) then CORR
if (predur1  0.19)  (ppregram=cityname)  (rmsmax1  1.10)  (pmntempo2  1.64) then CORR
if (realstrat=SystemImplicit)  (gram=cityname)  (pmnf0mean1  0.96) then CORR
if (preconf  -2.66)  (dur2  0.31)  (pprenofeat=T)  (tempo2  0.61) then AWARE
if (preconf  -2.85)  (syls  2)  (predur  1.05)  (pref0max  4.82)  (tempo2  0.58)  (pmn%sil  0.53) then AWARE
if (preconf  -3.34)  (syls  2)  (ppau  0.57)  (conf  -3.07)  (preppau  0.72) then AWARE
if (dur  0.74)  (pmndur  2.57)  (preconf  -4.36)  (f0mean2  0.90) then CORRAWARE
if (preconf  -2.80)  (pretempo  2.16)  (preconf  -3.95)  (tempo1  4.67) then CORRAWARE
if (preconf  -2.80)  (dur  0.66)  (rmsmean  488.56) then CORRAWARE
if (preconf  -3.56)  (dur2  0.64)  (prerejbool=T) then CORRAWARE
if (pretempo  0.71)  (tempo  3.31) then CORRAWARE
if (preconf  -3.01)  (tempo2  0.78)  (pmndur  2.83)  (pmnf0mean  199.84) then CORRAWARE
if (pmnconf  -3.10)  (prestr contains the word ?help?)  (pmndur2  2.01)  (ppau  0.98) then CORRAWARE
if (pmnconf  -3.10)  (gram=universal)  (pregram=universal)  ( %sil  0.39) then CORRAWARE
else NORM
Figure 3: Rules for Predicting 4 Turn Categories.
Precision (%) Recall (%)
norm 80.09 89.39
corr 72.86 61.66
aware 61.01 39.79
corraware 61.76 61.72
Accuracy: 74.23% (  0.96%); baseline: 57%
Table 3: 4-way Classification Performance.
applying our best ruleset to the whole corpus. This
Classified as
norm corr aware corraware
norm 1263 14 11 38
corr 68 219 0 7
aware 149 1 130 47
corraware 53 5 8 315
Table 4: Confusion Matrix, 4-way Classification.
matrix clearly shows a tendency for the minority
classes, aware, corr and corraware, to be falsely
classified as normal. It also shows that aware and
corraware are more often confused than the other
categories.
These confusability results motivated us to col-
lapse the aware and corraware into one class,
which we will label isaware; this class thus rep-
resents all turns in which users become aware of
a problem. From a system perspective, such a
3-way classification would be useful in identify-
ing the existence of a prior system failure and in
further identifying those turns which simply rep-
resent corrections; such information might be as
useful, potentially, as the 4-way distinction, if we
could achieve it with greater accuracy.
Indeed, when we predict the three classes
(isaware, corr, and norm) instead of four, we
do improve in predictive power ? from 74.23%
to 81.14% (+/? 0.83%) classification success.
Again, this compares to the baseline (predicting
norm, which is still the majority class) of 57%. We
also get a corresponding improvement in terms of
precision and recall, as shown in Table 5, with
the isaware category considerably better distin-
guished than either aware or corraware in Table 3.
The ruleset for the 3-class predictions is given in
Precision (%) Recall(%)
norm 84.49 87.48
corr 72.07 67.38
isaware 80.52 77.07
Accuracy: 81.14% (  0.83%); baseline: 57%
Table 5: 3-way Classification Performance.
Figure 4. The distribution of features in this rule-
set is quite similar to that in Figure 3. However,
there appear to be clear differences in which fea-
tures best predict which classes. First, the features
used to predict corrections are balanced between
those from the current turn and features from the
preceding context, whereas isaware rules primar-
ily make use of features of the preceding context.
Second, the features appearing most often in the
rules predicting corrections are durational features
(dur2, predur1, dur), while duration is used only
if (gram=universal)  (dur2  7.31) then CORR
if (dur2  2.25)  (priornofeatpct  0.11)  (%sil  0.55)
 (wordsstr  4) then CORR
if (dur2  2.75)  (gram=universal)  (pre%sil1  1.17)
then CORR
if (predur1  0.24)  (dur  0.85)  (priornofeatnum  2)
 (pmnconf  -3.11)  (pmn%sil  0.45) then CORR
if (predur1  0.19)  (dur  1.21)  (pmnf0mean2  0.99)
 (predur2  0.90)  (%sil  0.70)  (tempo  3.25) then
CORR
if (predur1  0.20)  (ynstr=F)  (pregram=cityname) 
(ppref0mean  171.58) then CORR
if (dur2  0.75)  (gram=cityname)  (pmnsyls  3.67) 
(pmnconf  -3.23)  (%sil  0.41) then CORR
if (prenofeat=T)  (preconf  -0.72) then CORR
if (preconf  -4.07) then ISAWARE
if (preconf  -2.76)  (pmntempo  2.39)  (tempo2 
1.56)  (preynstr=F) then ISAWARE
if (preconf  -2.75)  (ppau  0.46)  (tempo  1.20) then
ISAWARE
if (pretempo  0.23) then ISAWARE
if (pmnconf  -3.10)  (ppregram=universal)  (ppre%sil 
0.34)  (tempo1  2.94) then ISAWARE
if (predur  1.27)  (pretempo  2.36)  (prermsmean 
229.33)  (tempo2  0.83) then ISAWARE
if (preconf  -2.80)  (nofeat=T)  (f0mean  205.56) then
ISAWARE
else NORM
Figure 4: Rules for Predicting 3 Turn Categories.
once in isaware rules. Instead, these rules make
considerable use of the ASR confidence score of
the preceding turn; in cases where aware turns im-
mediately follow a rejection or recognition error,
one would expect this to be true. Isaware rules
also appear distinct from correction rules in that
they make frequent use of the tempo feature. It
is also interesting to note that rules for predicting
isaware turns make only limited use of the nofeat
feature, i.e. whether or not a variant of the word
no appears in the turn. We might expect this lex-
ical item to be a more useful predictor, since in
the explicit confirmation condition, users should
become aware of errors while responding to a re-
quest for confirmation.
Note that corrections, now the minority class,
are more poorly distinguished than other classes in
our 3-way classification task (Table 5). In a third
set of experiments, we merged corrections with
normal turns to form a 2-way distinction over all
between aware turns and all others. Thus, we only
distinguish turns in which a user first becomes
aware of an ASR failure (our original isaware and
corraware categories) from those that are not (our
original corr and norm categories). Such a dis-
tinction could be useful in flagging a prior sys-
tem problem, even though it fails to target the ma-
terial intended to correct that problem. For this
new 2-way distinction, we obtain a higher de-
gree of classification accuracy than for the 3-way
classification ? 87.80% (+/? 0.61%) compared to
81.14%. Note, however, that the baseline (predict
majority class of !isaware) for this new classifica-
tion is 70%, considerably higher than the previous
baseline. Table 6 shows the improvement in terms
of accuracy, precision, and recall.
Precision (%) Recall (%)
!isaware 91.7 91.6
isaware 80.7 81.1
Accuracy: 87.80% (  0.61%); baseline: 70%
Table 6: 2-way Classification Performance.
The ruleset for the 2-way distinction is shown in
Figure 5. The features appearing most frequently
if (preconf  -4.06)  (pretempo  2.65)  (ppau  0.25)
then T
if (preconf  -3.59)  (prerejbool=T) then T
if (preconf  -2.85)  (predur  1.039)  (tempo2  1.04)
 (preppau  0.57)  (pretempo  2.18) then T
if (preconf  -3.78)  (pmnsyls  4.04) then T
if (preconf  -2.75)  (prestr contains the word ?help?) then
T
if (pregram=universal)  (pprewordsstr  2) then T
if (preconf  -2.60)  (predur  1.04)  (%sil1  1.06) 
(prermsmean  370.65) then T
if (pretempo  0.13) then T
if (predur  1.27)  (pretempo  2.36)  (prermsmean 
245.36) then T
if (pretempo  0.80)  (pmntempo  1.75)  (ppretempo2
 1.39) then T
then F
Figure 5: Rules for Predicting 2 Turn Categories:
ISAWARE (T) versus the rest (F).
in these rules are similar to those in the previous
two rulesets in some ways, but quite different in
others. Like the rules in Figures 3 and 4, they ap-
pear independent of system characteristics. Also,
of the contextual features appearing in the rules,
about half are prosodic features and half ASR-
related; and, of the current turn features, pros-
odic features dominate. And timing features again
(especially tempo) dominate the prosodic features
that appear in the rules. However, in contrast to
previous classification rulesets, very few features
of the current turn appear in the rules at all. So,
it would seem that, for the broader classification
task, contextual features are far more important
than for the more fine-grained distinctions.
5 Conclusion
Continuing our earlier research into the use of
prosodic information to identify system misrecog-
nitions and user corrections in a SDS, we have
studied aware sites, turns in which a user first no-
tices a system error. We find first that these sites
have prosodic properties which distinguish them
from other turns, such as corrections and normal
turns. Subsequent machine learning experiments
distinguishing aware sites from corrections and
from normal turns show that aware sites can be
classified as such automatically, with a consid-
erable degree of accuracy. In particular, in a 2-
way classification of aware sites vs. all other turns
we achieve an estimated success rate of 87.8%.
Such classification, we believe, will be especially
useful in error-handling for SDS. We have pre-
viously shown that misrecognitions can be clas-
sified with considerable accuracy, using prosodic
and other automatically available features. With
our new success in identifying aware sites, we
acquire another potentially powerful indicator of
prior error. Using these two indicators together,
we hope to target system errors considerably more
accurately than current SDS can do and to hypo-
thesize likely locations of user attempts to correct
these errors. Our future research will focus upon
combining these sources of information identify-
ing system errors and user corrections, and invest-
igating strategies to make use of this information,
including changes in dialogue strategy (e.g. from
user or mixed initiative to system initiative after
errors) and the use of specially trained acoustic
models to better recognize corrections.
References
L. Bell and J. Gustafson. 1999. Repetition and its
phonetic realizations: Investigating a Swedish data-
base of spontaneous computer-directed speech. In
Proceedings of ICPhS-99, San Francisco. Interna-
tional Congress of Phonetic Sciences.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1?39.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In 14th Conference of the American
Association of Artificial Intelligence, AAAI.
J. Hirschberg, D. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech recog-
nition errors. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing,
Beijing.
J. Hirschberg, D. Litman, and M. Swerts. 2001.
Identifying user corrections automatically in spoken
dialogue systems. In Proceedings of NAACL-2001,
Pittsburgh.
E. Krahmer, M. Swerts, M. Theune, and M. Weegels.
1999. Error spotting in human-machine interac-
tions. In Proceedings of EUROSPEECH-99.
G. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue.
In Proceedings of the 36th Annual Meeting of the
Association of Computational Linguistics, COL-
ING/ACL 98, pages 736?742.
D. Litman, J. Hirschberg, and M. Swerts. 2000. Pre-
dicting automatic speech recognition performance
using prosodic cues. In Proceedings of NAACL-00,
Seattle, May.
S. L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn.
1996. Modeling hyperarticulate speech during
human-computer error resolution. In Proceedings
of ICSLP-96, pages 801?804, Philadelphia.
A. Shimojima, K. Katagiri, H. Koiso, and M. Swerts.
1999. An experimental study on the informational
and grounding functions of prosodic features of Ja-
panese echoic responses. In Proceedings of the
ESCA Workshop on Dialogue and Prosody, pages
187?192, Veldhoven.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In Pro-
ceedings of the Sixth International Conference on
Spoken Language Processing, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User
behaviors affecting speech recognition. In Proceed-
ings of ICSLP-92, volume 2, pages 995?998, Banff.
Labeling Corrections and Aware Sites
in Spoken Dialogue Systems
Julia Hirschbergy and Marc Swertsz and Diane Litmany
y AT&T Labs{Research z IPO, Eindhoven, The Netherlands,
Florham Park, NJ, 07932 USA and CNTS, Antwerp, Belgium
fjulia/dianeg@research.att.com m.g.j.swerts@tue.nl
Abstract
This paper deals with user correc-
tions and aware sites of system er-
rors in the TOOT spoken dialogue
system. We rst describe our cor-
pus, and give details on our proced-
ure to label corrections and aware
sites. Then, we show that correc-
tions and aware sites exhibit some
prosodic and other properties which
set them apart from `normal' utter-
ances. It appears that some correc-
tion types, such as simple repeats,
are more likely to be correctly recog-
nized than other types, such as para-
phrases. We also present evidence
that system dialogue strategy aects
users' choice of correction type, sug-
gesting that strategy-specic meth-
ods of detecting or coaching users on
corrections may be useful. Aware
sites tend to be shorter than other
utterances, and are also more dif-
cult to recognize correctly for the
ASR system.
1 Introduction
Compared to many other systems, spoken
dialogue systems (SDS) tend to have more
diculties in correctly interpreting user in-
put. Whereas a car will normally go left if
the driver turns the steering wheel in that
direction or a vacuum cleaner will start work-
ing if one pushes the on-button, interactions
between a user and a spoken dialogue system
are often hampered by mismatches between
the action intended by the user and the action
executed by the system. Such mismatches
are mainly due to errors in the Automatic
Speech Recognition (ASR) and/or the Nat-
ural Language Understanding (NLU) com-
ponent of these systems. To solve these mis-
matches, users often have to put considerable
eort in trying to make it clear to the system
that there was a problem, and trying to cor-
rect it by re-entering misrecognized or misin-
terpreted information. Previous research has
already brought to light that it is not always
easy for users to determine whether their in-
tended actions were carried out correctly or
not, in particular when the dialogue system
does not give appropriate feedback about its
internal representation at the right moment.
In addition, users' corrections may miss their
goal, because corrections themselves are more
dicult for the system to recognize and in-
terpret correctly, which may lead to so-called
cyclic (or spiral) errors. That corrections
are dicult for ASR systems is generally ex-
plained by the fact that they tend to be hyper-
articulated | higher, louder, longer . . . than
other turns (Wade et al, 1992; Oviatt et al,
1996; Levow, 1998; Bell and Gustafson, 1999;
Shimojima et al, 1999), where ASR models
are not well adapted to handle this special
speaking style.
The current paper focuses on user correc-
tions, and looks at places where people rst
become aware of a system problem (\aware
sites"). In other papers (Swerts et al, 2000;
Hirschberg et al, 2001; Litman et al, 2001),
we have already given some descriptive stat-
istics on corrections and aware sites and we
have been looking at methods to automatic-
ally predict these two utterance categories.
One of our major ndings is that prosody,
which had already been shown to be a good
predictor of misrecognitions (Litman et al,
2000; Hirschberg et al, 2000), is also useful to
correctly classify corrections and aware sites.
In this paper, we will elaborate more on the
exact labeling scheme we used, and add fur-
ther descriptive statistics. More in particular,
we address the question whether there is much
variance in the way people react to system er-
rors, and if so, to what extent this variance
can be explained on the basis of particular
properties of the dialogue system. In the fol-
lowing section we rst provide details on the
TOOT corpus that we used for our analyses.
Then we give information on the labels for
corrections and aware sites, and on the actual
labeling procedure. The next section gives
the results of some descriptive statistics on
properties of corrections and aware sites and
on their distributions. We will end the paper
with a general discussion of our ndings.
2 The data
2.1 The TOOT corpus
Our corpus consists of dialogues between hu-
man subjects and TOOT, a spoken dialogue
system that allows access to train information
from the web via telephone. TOOT was col-
lected to study variations in dialogue strategy
and in user-adapted interaction (Litman and
Pan, 1999). It is implemented using an
IVR (interactive voice response) platform de-
veloped at AT&T, combining ASR and text-
to-speech with a phone interface (Kamm et
al., 1997). The system's speech recognizer is
a speaker-independent hidden Markov model
system with context-dependent phone models
for telephone speech and constrained gram-
mars dening vocabulary at any dialogue
state. The platform supports barge-in. Sub-
jects performed four tasks with one of several
versions of the system that diered in terms
of locus of initiative (system, user, or mixed),
conrmation strategy (explicit, implicit, or
none), and whether these conditions could
be changed by the user during the task (ad-
aptive vs. non-adaptive). TOOT's initiative
System Initiative, Explicit Conrmation
T: Which city do you want to go to?
U: Chicago.
S: Do you want to go to Chicago?
U: Yes.
User Initiative, No Conrmation
S: How may I help you?
U: I want to go to Chicago from Baltimore.
S: On which day of the week do you want
to leave?
U: I want a train at 8:00.
Mixed Initiative, Implicit Conrmation
S: How may I help you?
U: I want to go to Chicago.
S: I heard you say go to Chicago.
Which city do you want to leave from?
U: Baltimore.
Figure 1: Illustrations of various dialogue
strategies in TOOT
strategy species who has control of the dia-
logue, while TOOT's conrmation strategy
species how and whether TOOT lets the user
know what it just understood. The fragments
in Figure 1 provide some illustrations of how
dialogues vary with strategy. Subjects were
39 students; 20 native speakers and 19 non-
native, 16 female and 23 male. Dialogues
were recorded and system and user behavior
logged automatically. The concept accuracy
(CA) of each turn was manually labeled. If
the ASR correctly captured all task-related
information in the turn (e.g. time, departure
and arrival cities), the turn's CA score was
1 (semantically correct). Otherwise, the CA
score reected the percentage of correctly re-
cognized task information in the turn. The
dialogues were also transcribed and automat-
ically scored in comparison to the ASR re-
cognized string to produce a word error rate
(WER) for each turn. For the study described
below, we examined 2328 user turns (all user
input between two system inputs) from 152
dialogues.
2.2 Dening Corrections and Aware
Sites
To identify corrections
1
in the corpus two au-
thors independently labeled each turn as to
whether or not it constituted a correction of
a prior system failure (a rejection or CA er-
ror, which were the only system failure sub-
jects were aware of) and subsequently de-
cided upon a consensus label. Note that much
of the discrepancies between labels were due
to tiredness or incidental sloppiness of indi-
vidual annotators, rather than true disagree-
ment. Each turn labeled `correction' was fur-
ther classied as belonging to one of the fol-
lowing categories: REP (repetition, includ-
ing repetitions with dierences in pronunci-
ation or uency), PAR (paraphrase), ADD
(task-relevant content added, OMIT (content
omitted), and ADD/OMIT (content both ad-
ded and omitted). Repetitions were further
divided into repetitions with pronunciation
variation (PRON) (e.g. yes correcting yeah),
and repetitions where the correction was pro-
nounced using the same pronunciation as the
original turn, but this distinction was di-
cult to make and turned out not to be useful.
User turns which included both corrections
and other speech acts were so distinguished by
labeling them \2+". For user turns contain-
ing a correction plus one or more additional
dialogue acts, only the correction is used for
purposes of analysis below. We also labeled as
restarts user corrections which followed non-
initial system-initial prompts (e.g. \How may
I help you?" or \What city do you want to
go to?"); in such cases system and user es-
sentially started the dialogue over from the
beginning. Figure 2 shows examples of each
correction type and additional label for cor-
rections of system failures on I want to go
to Boston on Sunday. Note that the utter-
ance on the last line of this gure is labeled
2+PAR, given that this turn consist of two
speech acts: the goal of the no-part of this
1
The labels discussed in this section for corrections
and aware sites may well be related to more general
dialogue acts, like the ones proposed by (Allen and
Core, 1997), but this needs to be explored in more
detail in the future.
turn is to signal a problem, whereas the re-
mainder of this turn serves to correct a prior
error.
Corr Type Correction
REP I want to go to Boston on Sunday
PAR To Boston on Sunday
OMIT I want to go to Boston
ADD To Boston on Sunday at 8pm
ADD/ I want to arrive Sunday at 8pm
OMIT
2+PAR No, to Boston on Sunday
Figure 2: Examples of Correction Types
Each correction was also indexed with an
identier representing the closest prior turn
it was correcting, so that we could investigate
\chains" of corrections of a single failed turn,
by tracing back through subsequent correc-
tions of that turn. Figure 3 shows a fragment
of a TOOT dialogue with corrections labeled
as discussed above.
We also identied aware sites in our cor-
pus | turns where a user, while interact-
ing with a machine, rst becomes aware that
the system has misrecognized a previous user
turn. For our corpus, we tried to determ-
ine whether there was some evidence in the
user turn indicating that the user had be-
come aware of a mistake in the system's un-
derstanding of a previous user turn, and, if
so, which previous turn had occasioned that
error. Note that such aware sites may or
may not also be corrections (another type of
post-misrecognition turn), since a user may
not immediately provide correcting informa-
tion. Also, it may take a while before the
user is able to notice a system error. Fig-
ure 4 shows an example that illustrates cases
in which both the user's awareness and cor-
rection of a misrecognition occur in the same
turn (e.g. turns 1159 and 1160, after sys-
tem prompts for information already given in
turn 1158). It also illustrates cases in which
aware sites and corrections occur in dierent
turns. For example, after the immediate ex-
plicit system conrmation of turn 1162, the
user rst becomes aware of the system errors
Turn Turn ID Corr of Corr Type
S: On which day of the week do you want to leave?
S: Sorry, I didn't hear anything from you.
Please tell me your departure date.
U: Um tomorrow 776 NA NA
S: You can say the departure date. For example,
you can say today or tomorrow. Or you can say
Monday, Tuesday, Wednesday...
U: Tomorrow 777 776 REP
S: You can say the departure date. For example,
you can say today or tomorrow. Or you can say
Monday, Tuesday, Wednesday, Thursday, Friday...
U: Saturday 778 777 PAR
S: Do you want to leave on Saturday?
U: Yes 779 NA NA
S: What time do you want to leave?
U: Eight o'clock A M 780 NA NA
S: Do you want to leave around 10:00pm?
U: Eight o'clock 781 780 OMIT
S: Do you want to leave around 10:00pm?
U: No 782 NA NA
S: What time do you want to leave?
U: 8:00am 783 781 ADD
Figure 3: Toot Dialogue Fragment with Correction Labels
(turn 1163), then separately corrects them
(turn 1164). When no immediate conrm-
ation of an utterance occurs (as with turn
1158), it may take several turns before the
user becomes aware of any misrecognition er-
rors. For example, it is not until turn 1161
that the user rst becomes aware of the er-
ror in date and time from 1158; the user then
corrects the error in 1162. Of all turns in
our corpus, 13% represent cases of turns that
are only corrections, 14% are only aware sites,
and 16% are turns where aware sites and cor-
rections co-occur. Also, note that turns 1162
and 1164 in this dialogue fragment represent
cases of restarts after a system-initial, non-
initial prompt (\How may I help you?").
3 Characteristics of corrections
and aware sites in TOOT
3.1 Corrections
We identied 29% of the turns in the corpus
as corrections. These were a serious prob-
lem for the system, being misrecognized in
terms of transcription error about twice as
often (61%) as non-corrections (33%) and in
terms of concept error in 53% of cases for cor-
rections compared with 21% concept error for
non-corrections.
Corrections diered from non-corrections
in other ways as well, which are suggest-
ive of how they might be identied by an
SDS. We have previously discussed the pros-
odic and acoustic dierences between correc-
tions and non-correction (Swerts et al, 2000;
Hirschberg et al, 2001). In particular, in
line with previous studies, we found that cor-
rections more often tend to be hyperarticu-
lated (longer, louder, with great pitch excur-
sions,...) than other turns. More recently we
have also discovered dierences in other di-
mensions.
First, corrections appear to be longer than
non-corrections in numbers of words, when
we examine actual human transcriptions of
Turn Turn ID Aware of Corr of
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1158 NA NA
S: Which city do you want to go to?
U: New York City 1159 1158 1158
S: Which city do you want to leave from?
U: Baltimore 1160 1158 1158
S: Do you want me to nd the trains from
Baltimore to New York City today at anytime now?
U: No 1161 1158 NA
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1162 NA 1158
S: Do you want me to nd the trains from
Baltimore to New York City today at anytime now?
U: No 1163 1162 NA
S: How may I help you?
U: Get me the train on Sunday at eight thirty pm 1164 NA 1162
Figure 4: Dialogue Fragment with Aware and Correction Labels.
them, both in absolute terms (T=17.68;
df=2326; p=0) and also controlling for
speaker (T=5.32; df=38; p=0). Even the
ASR hypotheses show this dierence, with
hypotheses of corrections being longer in ab-
solute terms (T=13.72; df=2326; p=0) and
across speakers (T=5.18; df=38; p=0).
Of the correction types we labeled, the
largest number were REPs and OMITs, as
shown in Table 1, which shows over-all dis-
tribution of correction types, and distribu-
tions for each type of system failure corrected.
Table 1 shows that 39% of TOOT corrections
were simple repetitions of the previously mis-
recognized turn. While this strategy is often
suboptimal in correcting ASR errors (Levow,
1998), REPs (45% error) and OMITs (52% er-
ror) were better recognized than ADDs (90%
error) and PARs (72% error). Thus, over-
all, users tend to have a preference for correc-
tion types that are more likely to be succes-
ful. That REPs and OMITs are more often
correctly recognized can be linked to the ob-
servation that they tend to be realized with
prosody which is less marked than the pros-
ody on ADDs and PARs. Table 2 shows that
REPs and OMITs are closer to normal utter-
ances in terms of their prosodic features than
ADDs, which are considerably higher, longer
and slower. This is in line with our previous
observations that marked settings for these
prosodic features more often lead to recogni-
tion errors.
What the user was correcting also inu-
enced the type of correction chosen. Table
1 shows that corrections of misrecognitions
(Post-Mrec) were more likely to omit inform-
ation present in the original turn (OMITs),
while corrections of rejections (Post-Rej) were
more likely to be simple repetitions. The
latter nding is not surprising, since the re-
jection message for tasks was always a close
paraphrase of \Sorry, I can't understand
you. Can you please repeat your utterance?"
However, it does suggest the surprising power
of system directions, and how important it is
to craft prompts to favor the type of correc-
tion most easily recognized by the system.
Corrections following system restarts
diered in type somewhat from other correc-
tions, with more turns adding new material
to the correction and fewer of them repeating
ADD ADD/OMIT OMIT PAR REP
All 8% 2% 32% 19% 39%
% Mrec(WER) 90% 93% 52% 72% 45%
% Mrec(CA) 88% 71% 47% 65% 45%
Post-Mrec 7% 3% 40% 18% 32%
Post-Rej 6% 0% 7% 28% 59%
Table 1: Distribution of Correction Types
Feature Normal ADD ADD/OMIT OMIT PAR REP
F0max (Hz) 219.4 286.3 252.9 236.7 252.1 239.9
rmsmax 1495.0 1868.1 2646.3 1698.0 1852.4 2024.6
dur (s) 1.4 6.8 4.1 2.3 4.7 2.5
tempo (sylls/s) 2.5 1.7 1.6 2.9 2.1 2.3
Table 2: Averages for dierent prosodic features of dierent Correction Types
the original turn.
Dialogue strategy clearly aected the type
of correction users made. For example, users
more frequently repeat their misrecognized
utterance in the SystemExplicit condition,
than in the MixedImplicit or UserNoConrm;
the latter conditions have larger proportions
of OMITs and ADDs. This is an important
observation given that this suggests that some
dialogue strategies lead to correction types,
such as ADDs, which are more likely to be
misrecognized than correction types elicited
by other strategies.
As noted above, corrections in the TOOT
corpus often take the form of chains of correc-
tions of a single original error. Looking back
at Figure 3, for example, we see two chains
of corrections: In the rst, which begins with
the misrecognition of turn 776 (\Um, tomor-
row"), the user repeats the original phrase
and then provides a paraphrase (\Saturday"),
which is correctly recognized. In the second,
beginning with turn 780, the time of depar-
ture is misrecognized. The user omits some
information (\am") in turn 781, but without
success; an ADD correction follows, with the
previously omitted information restored, in
turn 783. Elsewhere (Swerts et al 2000),
we have shown that chain position has an in-
uence on correction behaviour in the sense
that more distant corrections tend to be mis-
recognized more often than corrections closer
to the original error.
3.2 Aware Sites
708 (30%) of the turns in our corpus were
labeled aware sites. The majority of these
turns (89%) immediately follow the system
failures they react to, unlike the more com-
plex cases in Figure 4 above. If a system
would be able to detect aware sites with a
reasonable accuracy, this would be useful,
given that the system would then be able to
correctly guess in the majority of the cases
that the problem occurred in the preceding
turn. Aware turns, like corrections, tend to
be misrecognized at a higher rate than other
turns; in terms of transcription accuracy, 50%
of awares are misrecognized vs. 35% of other
turns, and in terms of concept accuracy, 39%
of awares are misrecognized compared to 27%
of other turns. In other words, both types
of post-error utterances, i.e., corrections and
aware sites, share the fact that they tend to
lead to additional errors. But whereas we
have shown above that for corrections this is
probably caused by the fact that these utter-
ances are uttered in a hyperarticulated speak-
ing style, we do not nd dierences in hyper-
articulation between aware sites and `normal
utterances' (T= 0.9085; df=38; p=0.3693).
This could mean that these sites are real-
ized in a speaking style which is not per-
ceptibly dierent from normal speaking style
ADD ADD/OMIT OMIT PAR REP
MixedExplicit 1 0 4 1 4
MixedImplicit 16 8 58 44 64
MixedNoConrm 0 0 2 0 1
SystemExplicit 2 2 8 31 67
SystemImplicit 0 1 18 0 20
SystemNoConrm 0 0 5 0 4
UserExplicit 0 0 0 1 1
UserImplicit 1 0 4 3 6
UserNoConrm 31 3 116 47 98
Table 3: Number of Correction Types for dierent dialogue strategies
Single no Other Turns
Aware site 162 546
Not Aware site 122 1498
Table 4: Distribution of single no utterances
and other turns for aware sites versus other
utterances
when judged by human labelers, but which
is still suciently dierent to cause problems
for an ASR system.
In terms of distinguishing features which
might explain or help to identify these turns,
we have previously examined the acoustic
and prosodic features of aware sites (Lit-
man et al, 2001). Here we present some
additional features. Aware sites appear to
be signicantly shorter, in general, than
other turns, both in absolute terms and con-
trolling for speaker variation, and whether
we examine the ASR transcription (absolute:
T=4.86; df=2326; p=0; speaker-controlled:
T=5.37; df=38; p=0) or the human one (ab-
solute: T=3.45; df=2326; p<.0001; speaker-
controlled: T=4.69; df=38; p=0). A sizable
but not overwhelming number of aware sites
in fact consist of a simple negation (i.e., a vari-
ant of the word `no') (see Table 4). This at
the same time shows that a simple no-detector
will not be sucient as an indicator of aware
sites (see also (Krahmer et al, 1999; Krahmer
et al, to appear)), given that most aware sites
are more complex than that, such as turns
1159 and 1160 in the example of Figure 4.
More concretely, Table 4 shows that a single
no would correctly predict that the turn is an
aware site with a precision of only 57% and a
recall of only 23%.
4 Discussion
This paper has dealt with user corrections and
aware sites of system errors in the TOOT
spoken dialogue system. We have described
our corpus, and have given details on our pro-
cedure to label corrections and aware sites.
Then, we have shown that corrections and
aware sites exhibit some prosodic and other
properties which set them apart from `normal'
utterances. It appears that some correction
types, such as simple repeats, are more likely
to be correctly recognized than other types,
such as paraphrases. We have also presen-
ted evidence that system dialogue strategy
aects users' choice of correction type, sug-
gesting that strategy-specic methods of de-
tecting or coaching users on corrections may
be useful. Aware sites tend to be shorter than
other utterances, and are also more dicult
to recognize correctly for the ASR system.
In addition to the descriptive study presen-
ted in this paper, we have also tried to auto-
matically predict corrections and aware sites
using the machine learning program RIP-
PER (Cohen, 1996). These experiments show
that corrections and aware sites can be clas-
sied as such automatically, with a consider-
able degree of accuracy (Litman et al, 2001;
Hirschberg et al, 2001). Such classication,
we believe, will be especially useful in error-
handling for SDS. If aware sites are detect-
able, they can function as backward-looking
error-signaling devices, making it clear to the
system that something has gone wrong in
the preceding context, so that, for example,
the system can reprompt for information. In
this way, they are similar to what others
have termed `go-back' signals (Krahmer et
al., 1999). Aware sites can also be used as
forward-looking signals, indicating upcoming
corrections or more drastic changes in user be-
havior, such as complete restarts of the task.
Given that, in current systems, both correc-
tions and restarts often lead to recognition er-
ror (Swerts et al, 2000), aware sites may be
useful in preparing systems to deal with such
problems. An accurate detection of turns that
are corrections may trigger the use of specially
trained ASR models to better recognize cor-
rections, or can be used to change dialogue
strategy (e.g. from user or mixed initiative to
system initiative after errors).
References
J. Allen and M. Core. 1997. Dialogue markup in
several layers. Draft contribution for the Dis-
course Resource Initiative.
L. Bell and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating a
Swedish database of spontaneous computer-
directed speech. In Proceedings of ICPhS-99,
San Francisco. International Congress of Phon-
etic Sciences.
W. Cohen. 1996. Learning trees and rules with
set-valued features. In 14th Conference of the
American Association of Articial Intelligence,
AAAI.
J. Hirschberg, D. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech re-
cognition errors. In Proceedings of the Sixth
International Conference on Spoken Language
Processing, Beijing.
J. Hirschberg, D. Litman, and M. Swerts. 2001.
Identifying user corrections automatically in
spoken dialogue systems. In Proceedings of
NAACL-2001, Pittsburgh.
C. Kamm, S. Narayanan, D. Dutton, and R.
Ritenour. 1997. Evaluating spoken dialogue
systems for telecommunication services. In
Proc. EUROSPEECH-97, Rhodes.
E. Krahmer, M. Swerts, M. Theune, and M. Wee-
gels. 1999. Error spotting in human-
machine interactions. In Proceedings of
EUROSPEECH-99.
E. Krahmer, M. Swerts, M. Theune, and M. Wee-
gels. to appear. The dual of denial: Two uses
of disconrmations in dialogue and their pros-
odic correlates. Accepted for Speech Commu-
nication.
G. Levow. 1998. Characterizing and recogniz-
ing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meet-
ing of the Association of Computational Lin-
guistics, COLING/ACL 98, pages 736{742.
D. Litman, J. Hirschberg, and M. Swerts. 2000.
Predicting automatic speech recognition per-
formance using prosodic cues. In Proceedings
of NAACL-00, Seattle, May.
D. Litman, J. Hirschberg, and M. Swerts. 2001.
Predicting User Reactions to System Error. In
Proceedings of ACL-01, Toulouse, July.
D. Litman and S. Pan. 1999. Empirically eval-
uating an adaptable spoken dialogue system.
In Proceedings tth International conference on
User Modeling.
S. L. Oviatt, G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error resolu-
tion. In Proceedings of ICSLP-96, pages 801{
804, Philadelphia.
A. Shimojima, K. Katagiri, H. Koiso, and
M. Swerts. 1999. An experimental study on
the informational and grounding functions of
prosodic features of Japanese echoic responses.
In Proceedings of the ESCA Workshop on Dia-
logue and Prosody, pages 187{192, Veldhoven.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In Pro-
ceedings of the Sixth International Conference
on Spoken Language Processing, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992.
User behaviors aecting speech recognition. In
Proceedings of ICSLP-92, volume 2, pages 995{
998, Ban.
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 55?62,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Let?s lie together:
Co-presence effects on children?s deceptive skills
Marc Swerts
Tilburg University
School of Humanities
Tilburg center for Communication and Cognition (TiCC)
Tilburg, The Netherlands
m.g.j.swerts@uvt.nl
Abstract
A person?s expressive behavior is differ-
ent in situations where he or she is alone,
or where an additional person is present.
This study looks at the extent to which such
physical co-presence effects have an impact
on a child?s ability to deceive. Using an
experimental digitized puppet show, truth-
ful and deceptive utterances were elicited
from children who were interacting with
two story characters. The children were
sitting alone, or as a couple together with
another child. A first perception study in
which minimal pairs of truthful and decep-
tive utterances were shown (vision-only) to
adult observers revealed that the correct de-
tection of deceptive utterances is depen-
dent on whether the stimuli were produced
by a child alone or together with another
child (both being visible). A second per-
ception study presented participants with
videos from children of the couples condi-
tion that were edited so that only one child
was visible. The study revealed that the de-
ceptive utterances could more often be de-
tected correctly in the more talkative chil-
dren than in the more passive ones.
1 Introduction
Deceiving others is not always easy. Past re-
search has shown that various factors can have a
detrimental effect on a person?s deceptive skills,
as it may matter whom one tries to deceive,
what kind of lie is being produced, and un-
der what circumstances a lie is elicited (De-
Paulo, Lindsay, Malone, Muhlenbruck, Charlton,
& Cooper 2003). The current study wants to ex-
plore whether the behavior of a deceiver is in-
fluenced by co-presence effects: i.e., is there an
essential difference between a deceiver who is
solely responsible for the lie he or she is produc-
ing, and someone who shares the responsibility
for the deceit with another person who is phys-
ically present. We investigate such questions in
data produced by children around the age of 5,
and focus in particular on possible nonverbal cues
to deception. As such, the current investigation
fits with other studies on deceptive skills of chil-
dren, given that these skills may reveal important
aspects of a child?s cognitive development. In-
deed, telling a lie is often claimed to be mentally
more demanding than telling the truth, and also
presumes that one is able to understand and ma-
nipulate another person?s perspective on a given
state of affair. Given this, the study of lies has
been thought to be potentially useful as a means
to learn more about how growing children develop
their metacognitive skills (e.g. Talwar, Lee, Bala,
& Lindsay, 2004; Talwar, Murphy, & Lee, 2007).
Previous researchers have often explored some-
one?s deceptive skills by running perception ex-
periments in which independent observers have to
judge in recordings of speakers whether a person
is telling the truth or not. The current study ex-
plores whether the detection of a lie is different
when an observer has to judge the recording of
a person who is alone, or of a person who pro-
duces a lie together with another person. From
the literature, it is not immediately clear whether
co-presence effects are likely to maximize or di-
minish the perceived difference between truth and
deceit. On the one hand, one could hypothesize
that the presence of another person may make it
easier for an observer to detect whether some-
one is telling the truth or not. Such an expecta-
tion could be based on studies that suggest that
55
people contaminate each other?s expressive be-
haviour, such that their facial and other nonverbal
cues become more pronounced and more clearly
interpretable for observers as cues to deception.
In a study with game-playing children (Shahid,
Krahmer, & Swerts 2008), to give an example,
it was found that observers tend to find it eas-
ier to determine whether a child had won or lost
a card guessing game, when it was playing to-
gether with another child, compared to a situation
in which it was playing the game alone. That re-
sult is reminiscent of work on gesturing, where it
is often reported that speakers become more ex-
pressive when they are directly being observed by
someone else. Bavelas, Gerwing, Sutton, and Pre-
vost (2008), for instance, found that speakers ges-
ture more and with a larger amplitude if they are
engaged in a face-to-face interaction, compared
to a telephone conversation or in a setting where
they talk to an answering machine. Similar find-
ings were reported by Mol, Krahmer, Maes, and
Swerts (2009).
On the other hand, findings that indicate that
people become more expressive in the presence
of other people may not generalize to all situa-
tional contexts, and may sometimes even be op-
posite to what was described above. For instance,
Lee and Wagner (2002) analysed video record-
ings of women who were speaking about a pos-
itive or a negative experience either in the pres-
ence of an experimenter or alone. They found
that women were more expressive about posi-
tive emotions when another person was present,
whereas the negative emotions were less clearly
expressed when someone else was present. These
results show that social context can have differ-
ent kinds of effects on a person?s nonverbal be-
havior depending on a speaker?s specific state of
mind. This begs the question as to what happens
when people are trying to deceive another person,
and whether possible nonverbal correlates of their
deceptive behavior become more pronounced or
rather more diminished in contexts where they are
alone, or physically co-present with other people.
Moreover, from a perceptual perspective, it is not
clear whether an observer would profit from the
fact that he or she has to judge the truthfulness of
only one person or of more than one person simul-
taneously. It could be that the exposure to multi-
ple persons would make it easier for an observer
because of having access to more resources to de-
cide about truth or lie. But it could also be the case
that the mere fact that an observer would have
to judge multiple people at the same time would
make the task of detecting deception more chal-
lenging than in the case where only one person
is speaking, because it might be that subtle cor-
relates of deception would escape the observer?s
attention.
Given the overall aim to investigate the effect
of physical co-presence on a child?s deceptive be-
havior, this study also explores whether the child?s
specific role in a situational context is of impor-
tance for the correct detection of deception. It has
of course already been known for a few decades
that a person?s personality may matter, for in-
stance in that extraverts tend to show more cor-
relates of deception than introvert people (e.g.
Bradley & Janisse 1981). Also, previous work
suggests that more dominant people exhibit dif-
ferent kinds of nonverbal behaviour than follow-
ers (Tiedens & Fragale 2003). In line with this
observation, we will look at children who are pas-
sive or active in a setting, and see whether that
difference has repercussions for lie detection. On
the one hand, active children in being more in-
volved in the interaction may increase the likeli-
hood of showing nonverbal cues to deception. On
the other hand, it may that the more passive chil-
dren may reveal such cues more clearly, as a result
of their belief that the observer?s focus of attention
is directed towards the more active child, so that
they leak more cues to deception.
The current research consists of two perception
experiments. Experiment 1 investigates whether
correlates of a child?s deceptive behaviour are
different for situations in which the child is ei-
ther alone or co-present with another child. Ex-
periment 2 looks at differences between partici-
pants within an interaction, in particular compar-
ing children who are very active and talkative ver-
sus those who take less initiative. We only focus
on visual cues (from which auditory features are
removed), given that earlier work (Ecoff, Ekman,
Mage & Frank 2000) has shown that observers
can more accurately detect deception when they
only have to focus on one modality (compared to
tests with multimodal stimuli).
2 Interactive elicitation procedure
To obtain truthful and deceptive utterances from
children, a new elicitation procedure was used,
56
based on a computerized version of an animated
puppet show. In the set-up, child participants are
seated in front of a computer screen on which they
see a story that unfolds. While the story is actually
controlled by the experimenter (whom the child
cannot see), the child is given the impression that
some crucial actions of 2 main characters depend
on the input of the child participant. During the
interaction, the video and speech of the child are
being recorded with a camera that is positioned on
the top of the computer screen to which the child
is looking. In this way, the recordings capture the
faces and upper part of the chest (frontal view) of
the child participants.
Figure 1: A few visual materials of scenes used in
the interactive puppet show
The show starts with a longer part in which a
narrating voice introduces 2 main characters, a
prince (the good guy) and a dragon (the bad guy),
to the participating child, in a typical fairy-tale
plot. The narrator explains to the child that a bad
dragon has been terrorizing a far-away country.
Luckily, Prince Peter has come up with a plan to
capture the dragon, for which he needs the help
from the child. The narrator explains that the per-
son who catches the dragon, receives a reward
(a bag of gold) from the king. In order to in-
crease the child?s level of engagement, an actual
bag of gold (actually, chocolate coins wrapped in
goldish-looking paper) is clearly shown on a table
in the visual field of the participant. Then the in-
teractive part starts in which child utterances are
elicited from exchanges with the 2 main charac-
ters of the story, the prince and the dragon. The
interactive part contains 2 central scenes designed
to elicit minimal pairs of truthful and deceptive ut-
terances from children to be used in a perception
test later on. As will become clear below, decep-
tive utterances are elicited from a child?s interac-
tion with the dragon, and the truthful ones from
interactions with the prince.
First, the prince appears, and asks the child for
its name, mainly to ensure that the latter becomes
aware that it can interact with the story character.
After this, the prince tells the child that he wants
to capture the dragon, and needs the child?s help.
He tells the child that he will hide behind a tree
(shown on the left of the screen), and that, if the
dragon appears, the child needs to tell the dragon
that the prince has entered the castle (shown on
the right of the screen). Then he hides behind
the tree, after which the dragon appears on stage
and asks the child where the prince is. The child
typically replies with a deceptive phrase like ?in
the castle? (first deceptive response), after which
the dragon expresses some disbelief about this re-
sponse, and repeats the earlier question, so that
the child needs to repeat the earlier response (sec-
ond deceptive response). Then, the dragon leaves,
enters the castle, after which the prince appears
again. He tells the child he believes he has heard
the dragon, and asks where the dragon is, to which
a child typically responds with a truthful ?in the
castle? (first truthful response). The prince says
he cannot believe that response, so asks the child
to repeat its truthful utterance (second truthful re-
sponse). Given that both the deceptive and truth-
ful scene contain a repeat, we obtain 4 versions
from every participating child of the utterance
?in the castle? (or equivalent phrases like ?in the
tower?, or ?in the church?): first and second at-
57
tempts of truthful and deceptive utterances. Fig-
ure 1 depicts some representative scenes from the
story.
We obtained minimal pairs (truthful and de-
ceptive variants of the utterance ?in the castle?)
from 38 children (18 boys; 20 girls), who had vol-
unteered for the experiment with written consent
from their parents and/or primary caretakers. The
average age of these children was 5 years and 7
months (minimum: 4 years and 10 months; max-
imum: 6 and 4 months) in addition, we collected
recordings for 10 pairs of children who did the
same task as the singles, but sitting next to each
other and both facing the screen. Their average
age was 5 years and 5 months (minimum: 4 years
and 3 months; maximum: 6 and 9 months). Note
that the task given to the pairs of children was
the same as the one given to the children sitting
alone. It was interesting to note that there was es-
sentially no interaction between two participants
in the pairs condition, and that they basically only
responded to questions and instructions from the
story characters. We did observe, however, that
within these pairs, there tended to be a division of
labor, in that one of the children would typically
take the initiative and talk to the story characters,
while the other would be more passive.
3 Experiment 1: singles vs. couples
The first experiment explores whether there is a
difference in the extent to which lies can be de-
tected in children who are interacting alone with
some story characters, versus children who are
doing a similar task together with another child.
3.1 Method
3.1.1 Stimuli
The stimuli consisted of the children?s re-
sponses to either the prince (truthful) or the
dragon (deceptive), where some of the children
were interacting alone, and some were interact-
ing in couples. As said above, stimuli were pre-
sented as video-only materials, so with the sound
removed.
3.1.2 Participants
The data for the singles condition were col-
lected in an earlier study, and came from 20 ob-
servers (Swerts 2011). In addition, 121 partici-
pants took part in the couples condition of the ex-
periment, as partial fulfillment to get course cred-
its.
3.1.3 Procedure
Observers were presented with pairs of video
recordings, i.e., a truthful and a deceptive utter-
ance of either a single child, or similar clips in
which 2 children are visible who are sitting next
to each other. Pairs of recordings were either com-
paring the children?s first time they had responded
to a question from the prince or the dragon, or
pairs of utterances of their second responses to
those characters. Note that pairs of stimuli shown
to observers were always produced by the same
child. Stimuli were presented in a group exper-
iment, although each participant had to perform
the test individually (paper-and-pencil test). The
task given to observers was to guess by forced
choice which of the 2 clips they saw contained a
child?s deceptive utterance. The order of presenta-
tion of the truthful and deceptive utterance within
a pair, and of the pairs within the larger test was
fully randomized.
3.2 Results
The observer responses were analysed with a re-
peated measures ANOVA with the percentage
correct detection of deceptive utterances for all
stimulus pairs per observer as dependent variable,
and with attempt (2 levels: first attempt, second
attempt) and order (2 levels: deceptive utterance
shown first, deceptive utterance shown second)
as within-subject factors, and presence (2 levels:
alone, together) as between-subject factors. Ta-
ble 1 reveals that, while the main effects of pres-
ence and attempt are not significant, there is a
significant effect of presentation order on the ob-
servers? likelihood to correctly detect the decep-
tive utterance: deceptive utterances could more
easily be detected correctly if they were shown
after the truthful utterance, rather than the other
way around. In addition, we found a significant
2-way interaction between attempt and presence
(F(1,139) = 5.793, p < .05, 
2
p = .040), which
can be explained by the data shown in table 2.
As can be seen, for the alone condition, observers
tend to find it easier to detect the deceptive utter-
ance in pairs of second interactions with the story
characters, than in the first interactions. However,
for those stimuli taken from children being to-
gether, there appears to be no difference between
58
Table 1: Percentage correct detection of deception (mean, standard error, 95% intervals) and F-statistics for
different levels of experimental factors
Factor Level Correct detection F-stats
Presence Alone 58.0 (.24, 53.1 ? 62.8) F(1,139) = .991, p = n.s.,
Together 60.6 (.10, 58.7 ? 62.6) 2p = .007
Attempt First 57.4 (.18, 53.9 ? 61.0) F(1,139) = 2.556, p = n.s.,
Second 61.2 (.17, 57.8 ? 64.6) 2p = .018
Order Deception first 53.2 (.18, 49.6 ? 56.8) F(1,139) = 24.163, p < .001,
Deception second 65.4 (.18, 61.8 ? 68.9) 2p = .148
Table 2: Percentage correct detection of deception
(mean, standard error) for speakers in alone or couples
condition as a function of order of speaker attempt
Attempt
Presence First Second
Alone 52.3 (.33) 62.7 (.32)
Together 61.6 (.14) 59.7 (.13)
first and second attempts.
3.3 Discussion
While the experiment did not reveal a main ef-
fect of co-presence on the detection of deception,
that factor turned out to be important in a 2-way
interaction with attempt. This significant interac-
tion may be explained by ceiling effects that are
only true for the condition in which 2 children
were being observed, but appear to be absent in
the alone condition. That is, in the alone condi-
tion, the probability to correctly detect a lie ap-
pears to depend on whether observers were seeing
a first or second attempt of a child interacting with
the story characters. As table 2 reveals, during a
second attempt, a single child was more likely to
show correlates of deceptive behavior compared
to its first attempt. That effect may be due to the
fact that during a second attempt a child is more
consciously aware of the fact that it tries to de-
ceive which may have the ironic counter-effect
that more cues to deception are leaked, as it tries
harder than the first time (Swerts 2011; see also
Wardlow Lane et al 2006). However, in the to-
gether condition, it appears not to matter whether
the children were interacting for the first or sec-
ond time; rather, the results appears to be around
60% correct detection both during first and sec-
ond attempts. Compared with related studies in
this area of research (e.g. DePaulo, Lindsay, Mal-
one, Muhlenbruck, Charlton, & Cooper, 2003),
this percentage is high, so that some ceiling ef-
fects may come into play: the correct detection
for first attempts is already so high that it is hard
to get even better results during second attempts.
While experiment 1 has provided some evidence
that detection of deceit is affected by co-presence
effects, it remains unclear whether observers were
able to extract cues to deception from both chil-
dren in the together condition or whether they
were especially paying attention to certain types
of children. More specifically, informal observa-
tions of the video clips suggested that some chil-
dren were playing a more active role in the inter-
actions than other children.
4 Experiment 2: active vs. passive
children
Experiment 2 explores to what extent differences
between the child participants (talkative vs. silent
ones) may influence an observer?s ability to find a
deceptive utterance.
4.1 Method
4.1.1 Stimuli
The stimuli showed children from the couples
condition of experiment 1, except that the clips
only showed 1 child (zoomed in so that the other
child was not visible). As discussed above, when
two children were placed next to each other to in-
teract with the prince and the dragon in the story,
there tended to be one child who was more active
59
Table 3: Percentage correct detection of deception (mean, standard error, 95% intervals) and F-statistics for
different levels of experimental factors
Factor Level Correct detection F-stats
Speaker Passive 50.4 (.18, 46.8 ? 54.0) F(1,92) = 47.160, p < .001,
Active 62.0 (.15, 59.1 ? 64.9) 2p = .339
Order Deception first 47.3 (.18, 43.7 ? 50.9) F(1,92) = 23.889, p < .001,
Deception second 65.1 (.16, 61.9 ? 68.3) 2p = .206
than the other when addressing the story charac-
ters. For the purpose of the current experiment,
we distinguished between children who were la-
beled ?active? as those who had been speaking in
both the truthful and deceptive utterance, versus
the ?passive? ones as those who had been silent in
at least one of the two. In doing so, we obtained
13 active and 7 passive children. Also, given that
we were only interested in the effect of passive vs
active children and to reduce the time it took to
complete the experiment, we decided to only use
stimuli from the second attempts of the children
to produce a truthful or deceptive utterance.
4.1.2 Participants
In total, 93 participants took part in the exper-
iment, as partial fulfillment to get course credits.
None of them had participated in any of the per-
ception tests of experiment 1.
4.1.3 Procedure
The procedure of this experiment was exactly
the same as the one used for experiment 1.
4.2 Results
The data were again analysed with a repeated
measures ANOVA with the percentage correct de-
tection of deceptive utterances for all stimulus
pairs per observer as dependent variable, and with
order (2 levels: deceptive utterance shown first,
deceptive utterance shown second) and speaker
role (2 levels: active, passive) as independent
within-subject factors. As shown in table 3, both
speaker type and presentation order had a signif-
icant effect on correct detection of the deceptive
utterance, such that observers found it easier to
detect the lies in the more active speakers, and in
those pairs in which the deceptive utterance was
presented as the second one in a pair (see also
experiment 1). Interestingly, the interaction be-
Table 4: Percentage correct detection of deception
(mean, standard error) for passive and active speakers
as a function of order of deceptive utterance
Deceptive utterance
Speaker shown first shown second
Passive 39.8 (.27) 61.0 (.27)
Active 54.8 (.22) 69.2 (.18)
tween order and speaker role was not significant
(F(1,92) = 2.432, p = n.s., 
2
p = .026). As ta-
ble 4 reveals, the 2 effects of speaker role and or-
der are additive.
4.3 Discussion
Experiment 2 has shown that the likelihood of
correctly detecting whether a child is deceiving
or speaking the truth depends on how active it is
within a specific social context. That is, when it
takes the initiative of responding to the story char-
acters and is being relatively talkative, then this
level of engagement makes it easier for an ob-
server to decide whether or not the child is pro-
ducing a lie. Further research is needed to find out
why exactly it is that detection of deception is eas-
ier when people have to judge more active partic-
ipants. One reason could be that children who are
more active are also more expressive, which in-
creases the chances that specific cues to deception
are leaked to an observer. Such an explanation
would be compatible with earlier findings that the
accuracy with which lies can be detected correctly
varies for deceivers who have different personal-
ities. More specifically, it has been shown that,
when comparing introvert with extravert people,
it is generally easier to detect the lies in the latter
group (Bradley & Janisse, 1981).
In the current set-up of the experiment, the chil-
60
dren were not explicitly given any explicit roles in
the story, for instance, in that one of them would
be asked to be silent, while the other would be
given the instruction to take initiative with the
characters of the story. Rather, their level of en-
gagement within the interactive story occurred
spontaneously in the course of the interaction,
which was thought to guarantee that their interac-
tion was relatively natural. In future work, how-
ever, it could be worthwhile to make a partici-
pants? active or passive role within the discourse
more explicit to the child and also measure as-
pects of their personality. This would help to de-
cide whether the detection of deception is due to
the fact that some children are more active, or to
the fact that some children are more extravert, or
to a combination of these factors.
5 General discussion
The current study revealed that deceiving children
are affected by co-presence effects. Experiment 1,
in which minimal pairs of truthful and deceptive
utterances were shown (vision-only) to adult ob-
servers, brought to light that the correct detection
of deceptive utterances is dependent on whether
the stimuli were produced by a child alone or to-
gether with another child. This result reminds one
of some practices in typical investigations of a
committed crime, where it is general practice to
confront various suspects with each other. Usu-
ally, the goal of letting multiple suspects meet
is to confront them with each other?s statements
from earlier police interrogations during which
they were separately interviewed independently
from each other. If these earlier sessions has let
to inconsistencies between the statements of the
different suspects, it might be interesting to see
how suspects react when they are exposed to each
other?s claims in a face-to-face situation. Ideally,
such a confrontation might help to let one of them
confess, or admit that an earlier claim was false.
Obviously, the story paradigm used in the produc-
tion experiment to elicit truthful and deceptive ut-
terances is different from such a police case, but
it does show that presence effects may maximize
the differences between truth and deceit.
This result appears to be compatible with the
idea that the presence of another person increases
a liar?s social awareness, which in turn might have
a detrimental effect on that person?s deceptive
skills. Such an effect could be similar to the re-
ported effect of an increased mental load on de-
ceptive behaviour: lying is generally assumed to
be more cognitively demanding than truth telling
(e.g. DePaulo, Lindsay, Malone, Muhlenbruck,
Charlton, & Cooper 2003; Vrij, Fisher, Mann, &
Leal 2006), given that liars have to monitor more
tasks than truth telling people, such as inventing
facts and controlling their behaviour while inter-
acting with another person. Consequently, tech-
niques that increase cognitive load, e.g. asking
people to tell a story in reverse order (Vrij, Mann,
Fisher, Leal, Milne, & Bull, 2008) or instructing
them to maintain eye contact with an interviewer
(Vrij, Mann, Leal, & Fisher 2010), tend to lead
to the effect that deception becomes more easily
observable. Under conditions of such increased
cognitive load, deceivers supposedly have less re-
sources to monitor their behavior, so that they leak
cues that others may pick up as indicators of de-
ception. Similarly, an increased social awareness
because of the mere presence of another person
could possibly lead to leaking more cues to lies.
The second perception experiment presented
participants with videos from children of the cou-
ples condition that were edited so that only one
child was visible. The study revealed that the de-
ceptive utterances could more often be detected
correctly in the more talkative children compared
to the more passive ones. It remains to be seen
whether these results are due to the fact that the
higher probability of correctly detecting decep-
tion in the more active children is due to the fact
that their higher level of engagement makes them
more expressive and more likely to leak cues to
deception, or because these more active children
have a more extravert personality that has been
shown to show more cues to deception than more
introvert children (Bradley & Janisse, 1981).
And finally, we found an additional order ef-
fect, as deceptive utterances can more often be
detected correctly when they are presented as the
second in a pair, as opposed to when they are pre-
sented as the first ones. This effect, in line with
previous observations by O?Sullivan et al(1988),
could be related to what is known as the truth
bias in the literature on deception, which refers
to ?an a priori belief, expectation, or presumption
that reflects the oft-observed tendency to assume
communicators are truthful most of the time? (e.g.
Burgoon et al 2008, p. 575). Accordingly, this
could possibly lead to the effect that an initial ut-
61
terance is first processed as being truthful, and re-
vised if an utterance contains counter-evidence to
this effect. Therefore, given that the truthful utter-
ances are more in line with default expectations of
an observer, it would become more easy to detect
the deceptive utterance as the more marked and
deviant case, if it is presented after the truthful
one.
Acknowledgments
We thank the director, teachers, parents and
children of the elementary school ?The Palet? in
Hapert and ?de Oversteek? in Liempde (both in
The Netherlands) for their willingness to partic-
ipate in the interactive story experiment. Many
thanks also to Lennard van de Laar and Emiel
Krahmer for help with setting up the elicitation
procedure and perception experiment, to Mar-
lon van Dijk, Linda Dolmans and John van den
Broeck for help with collecting the stimuli and
the perception results, and to Marieke Hoetjes and
Lisette Mol for comments on an earlier version of
this document.
6 References
Bavelas, J., Gerwing, J., Sutton, C., & Prevost, D.
(2008). Gesturing on the telephone: Indepen-
dent effects of dialogue and visibility. Journal
of Memory and Language 58, 495?520.
Bradley, M. T. & Janisse, M.P. (1981) Extraver-
sion and the detection of deception Personality
and Individual Differences 2, 99?103
Burgoon, J.K., Blair, J.P. & Strom, R.E. (2008)
Cognitive Biases and Nonverbal Cue Availabil-
ity in Detecting Deception. Human Communi-
cation Research, 34 572?599.
DePaulo, B., Lindsay, J., Malone, B., Muhlen-
bruck, L., Charlton, K., & Cooper, H. (2003).
Cues to deception. Psychological Bulletin, 129,
74?118.
Ecoff, N.L., Ekman, P., Mage, J.J. & Frank, M.G.
(2000) Lie Detection And Language Compre-
hension. Nature, 405, 139.
Lee, V. & Wagner, H. (2002). The Effect of Social
Presence on the Facial and Verbal Expression
of Emotion and the Interrelationships Among
Emotion Components. Journal of Nonverbal
Behavior, 26, 3?25.
Mol, L., Krahmer, E., Maes, A.A., & Swerts,
M. (2009). The communicative import of ges-
tures: Evidence from a comparative analysis of
human-human and human-machine interactions
Gesture 9, 97?126.
O?Sullivan, M., Ekman, P. & Friesen, W.V.
(1988). The effect of comparisons on detecting
deceit Journal of Nonverbal Behavior 12, 203?
215.
Shahid, S., Krahmer, E.J., & Swerts, M. (2008).
Alone or together: Exploring the effect of phys-
ical co-presence on the emotional expressions
of game playing children across cultures. In: P.
Markopoulus et al (Eds.), Fun and Games (pp.
94-105). Springer. (Lecture Notes in Computer
Science LNCS, 5294)
Swerts, M. (2011). Correlates of social awareness
in the visual prosody of growing children. Lab-
oratory Phonology, 2(2), 381?402.
Talwar, V., Lee, K., Bala, N. & Lindsay, R.C.L.
(2004). Children?s Lie-Telling to Conceal Par-
ents? Transgressions: Legal Implications. Law
and Human Behavior, 28 411?435.
Talwar, V., Murphy S., & Lee, K. (2007) White
lie-telling in children for politeness purposes.
International Journal of Behavioral Develop-
ment, 31, 1?11.
Tiedens, L. & Fragale, A.R. (2003) Power Moves:
Complementarity in Dominant and Submissive
Nonverbal Behavior. Journal of Personality and
Social Psychology (84), 558?568.
Vrij, A., Fisher, R., Mann, S., & Leal (2006)
Detecting deception by manipulating cognitive
load Trends in cognitive sciences 10, 141?142.
Vrij, A., Mann, S., Fisher, R., Leal, S., Milne, R.,
& Bull, R. (2008) Increasing cognitive load to
facilitate lie detection: the benefit of recalling
am event in reverse order. Law and Human Be-
havior 32, 253?265.
Vrij, A., Mann, S., Leal, S. & Fisher, R. (2010)
?Look into my eyes?: can an instruction to
maintain eye contact facilitate lie detection?
Psychology, Crime & Law, 16 (4), 327?348
Wagner, H., & Lee, V. (1999). Facial behav-
ior alone and in the presence of others. In: P.
Philippott et al (eds.) The social context of
nonverbal behavior, pp. 262?286. Cambridge
University Press, New York.
Wardlow Lane, Liane, Michelle Groisman & Vic-
tor S. Ferreira. (2006). Don?t Talk About Pink
Elephants! Speakers? Control Over Leaking
Private Information During Language Produc-
tion. Psychological Science 17 ( 4), 273-277.
62
