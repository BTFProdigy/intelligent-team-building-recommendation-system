Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre
IXA NLP group
UBC
Donostia, Basque Country
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
oier.lopezdelacalle@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Andrea Marchetti
IIT
CNR
Pisa, Italy
andrea.marchetti@iit.cnr.it
Antonio Toral
ILC
CNR
Pisa, Italy
antonio.toral@ilc.cnr.it
Piek Vossen
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambiguation
systems present new challenges. The diffi-
culties found by supervised systems to adapt
might change the way we assess the strengths
and weaknesses of supervised and knowledge-
based WSD systems. Unfortunately, all ex-
isting evaluation datasets for specific domains
are lexical-sample corpora. With this paper
we want to motivate the creation of an all-
words test dataset for WSD on the environ-
ment domain in several languages, and present
the overall design of this SemEval task.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in the last Senseval and Semeval competitions (Kil-
garriff, 2001; Mihalcea et al, 2004; Pradhan et al,
2007). Specific domains pose fresh challenges to
WSD systems: the context in which the senses occur
might change, distributions and predominant senses
vary, some words tend to occur in fewer senses in
specific domains, and new senses and terms might
be involved. Both supervised and knowledge-based
systems are affected by these issues: while the first
suffer from different context and sense priors, the
later suffer from lack of coverage of domain-related
words and information.
Domain adaptation of supervised techniques is a
hot issue in Natural Language Processing, includ-
ing Word Sense Disambiguation. Supervised Word
Sense Disambiguation systems trained on general
corpora are known to perform worse when applied
to specific domains (Escudero et al, 2000; Mart??nez
and Agirre, 2000), and domain adaptation tech-
niques have been proposed as a solution to this prob-
lem with mixed results.
Current research on applying WSD to specific do-
mains has been evaluated on three available lexical-
sample datasets (Ng and Lee, 1996; Weeber et al,
2001; Koeling et al, 2005). This kind of dataset
contains hand-labeled examples for a handful of se-
lected target words. As the systems are evaluated on
a few words, the actual performance of the systems
over complete texts can not be measured. Differ-
ences in behavior of WSD systems when applied to
lexical-sample and all-words datasets have been ob-
served on previous Senseval and Semeval competi-
tions (Kilgarriff, 2001; Mihalcea et al, 2004; Prad-
han et al, 2007): supervised systems attain results
on the high 80?s and beat the most frequent base-
line by a large margin for lexical-sample datasets,
but results on the all-words datasets were much more
modest, on the low 70?s, and a few points above the
most frequent baseline.
Thus, the behaviour of WSD systems on domain-
specific texts is largely unknown. While some words
could be supposed to behave in similar ways, and
thus be amenable to be properly treated by a generic
123
WSD algorithm, other words have senses closely
linked to the domain, and might be disambiguated
using purpose-built domain adaptation strategies (cf.
Section 4). While it seems that domain-specific
WSD might be a tougher problem than generic
WSD, it might well be that domain-related words
are easier to disambiguate.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain, that of
environment-related texts. The paper is structured
as follows. The next section presents current lexi-
cal sample datasets for domain-specific WSD. Sec-
tion 3 presents some possible settings for domain
adaptation. Section 4 reviews the state-of-the art in
domain-specific WSD. Section 5 presents the design
of our task, and finally, Section 6 draws some con-
clusions.
2 Specific domain datasets available
We will briefly present the three existing datasets
for domain-related studies in WSD, which are all
lexical-sample.
The most commonly used dataset is the Defense
Science Organization (DSO) corpus (Ng and Lee,
1996), which comprises sentences from two differ-
ent corpora. The first is the Wall Street Journal
(WSJ), which belongs to the financial domain, and
the second is the Brown Corpus (BC) which is a bal-
anced corpora of English usage. 191 polysemous
words (nouns and verbs) of high frequency in WSJ
and BC were selected and a total of 192,800 occur-
rences of these words were tagged with WordNet 1.5
senses, more than 1,000 instances per word in aver-
age. The examples from BC comprise 78,080 oc-
currences of word senses, and examples from WSJ
consist on 114,794 occurrences. In domain adapta-
tion experiments, the Brown Corpus examples play
the role of general corpora, and the examples from
the WSJ play the role of domain-specific examples.
Koeling et al (2005) present a corpus were the
examples are drawn from the balanced BNC cor-
pus (Leech, 1992) and the SPORTS and FINANCES
sections of the newswire Reuters corpus (Rose et al,
2002), comprising around 300 examples (roughly
100 from each of those corpora) for each of the 41
nouns. The nouns were selected because they were
salient in either the SPORTS or FINANCES domains,
or because they had senses linked to those domains.
The occurrences were hand-tagged with the senses
from WordNet version 1.7.1 (Fellbaum, 1998). In
domain adaptation experiments the BNC examples
play the role of general corpora, and the FINANCES
and SPORTS examples the role of two specific do-
main corpora.
Finally, a dataset for biomedicine was developed
by Weeber et al (2001), and has been used as
a benchmark by many independent groups. The
UMLS Metathesaurus was used to provide a set of
possible meanings for terms in biomedical text. 50
ambiguous terms which occur frequently in MED-
LINE were chosen for inclusion in the test set. 100
instances of each term were selected from citations
added to the MEDLINE database in 1998 and man-
ually disambiguated by 11 annotators. Twelve terms
were flagged as ?problematic? due to substantial dis-
agreement between the annotators. In addition to the
meanings defined in UMLS, annotators had the op-
tion of assigning a special tag (?none?) when none
of the UMLS meanings seemed appropriate.
Although these three corpora are useful for WSD
research, it is difficult to infer which would be the
performance of a WSD system on full texts. The
corpus of Koeling et al, for instance, only includes
words which where salient for the target domains,
but the behavior of WSD systems on other words
cannot be explored. We would also like to note that
while the biomedicine corpus tackles scholarly text
of a very specific domain, the WSJ part of the DSO
includes texts from a financially oriented newspaper,
but also includes news of general interest which have
no strict relation to the finance domain.
3 Possible settings for domain adaptation
When performing supervised WSD on specific do-
mains the first setting is to train on a general domain
data set and to test on the specific domain (source
setting). If performance would be optimal, this
would be the ideal solution, as it would show that a
generic WSD system is robust enough to tackle texts
from new domains, and domain adaptation would
not be necessary.
The second setting (target setting) would be to
train the WSD systems only using examples from
124
the target domain. If this would be the optimal set-
ting, it would show that there is no cost-effective
method for domain adaptation. WSD systems would
need fresh examples every time they were deployed
in new domains, and examples from general do-
mains could be discarded.
In the third setting, the WSD system is trained
with examples coming from both the general domain
and the specific domain. Good results in this setting
would show that supervised domain adaptation is
working, and that generic WSD systems can be sup-
plemented with hand-tagged examples from the tar-
get domain.
There is an additional setting, where a generic
WSD system is supplemented with untagged exam-
ples from the domain. Good results in this setting
would show that semi-supervised domain adapta-
tion works, and that generic WSD systems can be
supplemented with untagged examples from the tar-
get domain in order to improve their results.
Most of current all-words generic supervised
WSD systems take SemCor (Miller et al, 1993) as
their source corpus, i.e. they are trained on SemCor
examples and then applied to new examples. Sem-
Cor is the largest publicly available annotated cor-
pus. It?s mainly a subset of the Brown Corpus, plus
the novel The Red Badge of Courage. The Brown
corpus is balanced, yet not from the general domain,
as it comprises 500 documents drawn from differ-
ent domains, each approximately 2000 words long.
Although the Brown corpus is balanced, SemCor is
not, as the documents were not chosen at random.
4 State-of-the-art in WSD for specific
domains
Initial work on domain adaptation for WSD sys-
tems showed that WSD systems were not able to
obtain better results on the source or adaptation set-
tings compared to the target settings (Escudero et
al., 2000), showing that a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would not be useful when moved to new do-
mains.
Escudero et al (2000) tested the supervised adap-
tation scenario on the DSO corpus, which had exam-
ples from the Brown Corpus and Wall Street Journal
corpus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice, and
concluding that hand tagging a large general corpus
would not guarantee robust broad-coverage WSD.
Agirre and Mart??nez (2000) used the same DSO cor-
pus and showed that training on the subset of the
source corpus that is topically related to the target
corpus does allow for domain adaptation, obtaining
better results than training on the target data alone.
In (Agirre and Lopez de Lacalle, 2008), the au-
thors also show that state-of-the-art WSD systems
are not able to adapt to the domains in the context
of the Koeling et al (2005) dataset. While WSD
systems trained on the target domain obtained 85.1
and 87.0 of precision on the sports and finances do-
mains, respectively, the same systems trained on the
BNC corpus (considered as a general domain cor-
pus) obtained 53.9 and 62.9 of precision on sports
and finances, respectively. Training on both source
and target was inferior that using the target examples
alone.
Supervised adaptation
Supervised adaptation for other NLP tasks has been
widely reported. For instance, (Daume? III, 2007)
shows that a simple feature augmentation method
for SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously explored
more sophisticated methods (Daume? III and Marcu,
2006; Chelba and Acero, 2004). In contrast, (Agirre
and Lopez de Lacalle, 2009) reimplemented this
method and showed that the improvement on WSD
in the (Koeling et al, 2005) data was marginal.
Better results have been obtained using purpose-
built adaptation methods. Chan and Ng (2007) per-
formed supervised domain adaptation on a manu-
ally selected subset of 21 nouns from the DSO cor-
pus. They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding just
30% of the target data to the source examples the
same precision as the full combination of target and
source data could be achieved. They also showed
that using the source corpus significantly improved
results when only 10%-30% of the target corpus
was used for training. In followup work (Zhong et
125
Projections for 2100 suggest that temperature in Europe will have risen by between 2 to 6.3 C above 1990
levels. The sea level is projected to rise, and a greater frequency and intensity of extreme weather events are
expected. Even if emissions of greenhouse gases stop today, these changes would continue for many decades
and in the case of sea level for centuries. This is due to the historical build up of the gases in the atmosphere
and time lags in the response of climatic and oceanic systems to changes in the atmospheric concentration
of the gases.
Figure 1: Sample text from the environment domain.
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation ex-
periment. They significantly reduced the effort of
hand-tagging, but only obtained positive domain-
adaptation results for smaller fractions of the target
corpus.
In (Agirre and Lopez de Lacalle, 2009) the au-
thors report successful adaptation on the (Koeling
et al, 2005) dataset on supervised setting. Their
method is based on the use of unlabeled data, re-
ducing the feature space with SVD, and combina-
tion of features using an ensemble of kernel meth-
ods. They report 22% error reduction when using
both source and target data compared to a classifier
trained on target the target data alone, even when the
full dataset is used.
Semi-supervised adaptation
There are less works on semi-supervised domain
adaptation in NLP tasks, and fewer in WSD task.
Blitzer et al (2006) used Structural Correspondence
Learning and unlabeled data to adapt a Part-of-
Speech tagger. They carefully select so-called pivot
features to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source and
target domains. Agirre and Lopez de Lacalle (2008)
show that methods based on SVD with unlabeled
data and combination of distinct feature spaces pro-
duce positive semi-supervised domain adaptation re-
sults for WSD.
Unsupervised adaptation
In this context, we take unsupervised to mean
Knowledge-Based methods which do not require
hand-tagged corpora. The predominant sense acqui-
sition method was succesfully applied to specific do-
mains in (Koeling et al, 2005). The methos has two
steps: In the first, a corpus of untagged text from the
target domain is used to construct a thesaurus of sim-
ilar words. In the second, each target word is disam-
biguated using pairwise WordNet-based similarity
measures, taking as pairs the target word and each of
the most related words according to the thesaurus up
to a certain threshold. This method aims to obtain,
for each target word, the sense which is the most
predominant for the target corpus. When a general
corpus is used, the most predominant sense in gen-
eral is obtained, and when a domain-specific corpus
is used, the most predominant sense for that corpus
is obtained (Koeling et al, 2005). The main motiva-
tion of the authors is that the most frequent sense is a
very powerful baseline, but it is one which requires
hand-tagging text, while their method yields simi-
lar information automatically. The results show that
they are able to obtain good results. In related work,
(Agirre et al, 2009) report improved results using
the same strategy but applying a graph-based WSD
method, and highlight the domain-adaptation poten-
tial of unsupervised knowledge-based WSD systems
compared to supervised WSD.
5 Design of the WSD-domain task
This task was designed in the context of Ky-
oto (Piek Vossen and VanGent, 2008)1, an Asian-
European project that develops a community plat-
form for modeling knowledge and finding facts
across languages and cultures. The platform op-
erates as a Wiki system with an ontological sup-
port that social communities can use to agree on the
meaning of terms in specific domains of their inter-
est. Kyoto will focus on the environmental domain
because it poses interesting challenges for informa-
tion sharing, but the techniques and platforms will
be independent of the application domain. Kyoto
1http://www.kyoto-project.eu/
126
will make use of semantic technologies based on
ontologies and WSD in order to extract and repre-
sent relevant information for the domain, and is thus
interested on measuring the performance of WSD
techniques on this domain.
The WSD-domain task will comprise comparable
all-words test corpora on the environment domain.
Texts from the European Center for Nature Con-
servation2 and Worldwide Wildlife Forum3 will be
used in order to build domain specific test corpora.
We will select documents that are written for a gen-
eral but interested public and that involve specific
terms from the domain. The document content will
be comparable across languages. Figure 1 shows an
example in English related to global warming.
The data will be available in a number of lan-
guages: English, Dutch, Italian and Chinese. The
sense inventories will be based on wordnets of the
respective languages, which will be updated to in-
clude new vocabulary and senses. The test data will
comprise three documents of around 2000 words
each for each language. The annotation procedure
will involve double-blind annotation plus adjudica-
tion, and inter-tagger agreement data will be pro-
vided. The formats and scoring software will fol-
low those of Senseval-34 and SemEval-20075 En-
glish all-words tasks.
There will not be training data available, but par-
ticipants are free to use existing hand-tagged cor-
pora and lexical resources (e.g. SemCor and pre-
vious Senseval and SemEval data). We plan to make
available a corpus of documents from the same do-
main as the selected documents, as well as wordnets
updated to include the terms and senses in the se-
lected documents.
6 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. Unfor-
tunately, all existing evaluation datasets for specific
2http://www.ecnc.org
3http://www.wwf.org
4http://www.senseval.org/senseval3
5http://nlp.cs.swarthmore.edu/semeval/
domains are lexical-sample corpora. With this paper
we have motivated the creation of an all-words test
dataset for WSD on the environment domain in sev-
eral languages, and presented the overall design of
this SemEval task.
Further details can be obtained from the Semeval-
20106 website, our task website7, and in our distri-
bution list8
7 Acknowledgments
The organization of the task is partially funded
by the European Commission (KYOTO FP7 ICT-
2007-211423) and the Spanish Research Depart-
ment (KNOW TIN2006-15049-C03-01).
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using SVD for word
sense disambiguation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 17?24, Manchester, UK, August.
Coling 2008 Organizing Committee.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Super-
vised domain adaptation for wsd. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-09).
E. Agirre, O. Lopez de Lacalle, and A. Soroa. 2009.
Knowledge-based WSD and specific domains: Per-
forming over supervised WSD. In Proceedings of IJ-
CAI, Pasadena, USA.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
49?56, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
6http://semeval2.fbk.eu/
7http://xmlgroup.iit.cnr.it/SemEval2010/
8http://groups.google.com/groups/wsd-domain
127
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Dependence
of Supervised Word Sense Disambiguation Systems.
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proceedings of the Second International
Workshop on evaluating Word Sense Disambiguation
Systems, Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense
acquisition. In Proceedings of the Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
HLT/EMNLP, pages 419?426, Ann Arbor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. Conference
on Empirical Method in Natural Language.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computationla Linguistics (ACL), pages 40?47.
Nicoletta Calzolari Christiane Fellbaum Shu-kai Hsieh
Chu-Ren Huang Hitoshi Isahara Kyoko Kanzaki An-
drea Marchetti Monica Monachini Federico Neri
Remo Raffaelli German Rigau Maurizio Tescon
Piek Vossen, Eneko Agirre and Joop VanGent. 2008.
Kyoto: a system for mining, structuring and distribut-
ing knowledge across languages and cultures. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 87?92, Prague, Czech
Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volumen 1: from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-2002),
pages 827?832, Las Palmas, Canary Islands.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In Proceedings of the
AMAI Symposium, pages 746?750, Washington, DC.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1002?1010, Honolulu, Hawaii, October.
Association for Computational Linguistics.
128
MEANING: a Roadmap to Knowledge Technologies 
 
German Rigau. TALP Research Center. UPC. Barcelona. rigau@lsi.upc.es 
Bernardo Magnini. ITC-IRST. Povo-Trento. magnini@itc.it 
Eneko Agirre. IXA group. EHU. Donostia. eneko@si.ehu.es  
Piek Vossen. Irion Technologies. Delft. Piek.Vossen@irion.nl  
John Carroll. COGS. U. Sussex. Brighton. johnca@cogs.susx.ac.uk 
 
Abstract  
Knowledge Technologies need to extract 
knowledge from existing texts, which 
calls for advanced Human Language 
Technologies (HLT). Progress is being 
made in Natural Language Processing but 
there is still a long way towards Natural 
Language Understanding. An important 
step towards this goal is the development 
of technologies and resources that deal 
with concepts rather than words. The 
MEANING project argues that we need to 
solve two complementary and 
intermediate tasks to enable the next 
generation of intelligent open domain 
HLT application systems: Word Sense 
Disambiguation and large-scale 
enrichment of Lexical Knowledge Bases. 
Innovations in this area will lead to HLT 
with deeper understanding of texts, and 
immediate progress in real applications of 
Knowledge Technologies. 
Introduction 
The field of Information Society Technologies 
(IST) is one of the main thematic priorities of 
the European Commission for the 6th Framework 
programme. In this field, Knowledge 
Technologies (KT) aim to provide meaning to 
the petabytes of information content our 
societies will generate in the near future. 
Information and knowledge management 
systems need to evolve accordingly, to enable 
the next generation of intelligent open domain 
Human Language Technologies (HLT) that will 
deal with the growing potential of the 
knowledge-rich and multilingual society. 
In order to develop a trustable semantic web 
infrastructure and a multilingual ontology 
framework to support knowledge management a 
wide range of techniques are required to 
progressively automate the knowledge lifecycle. 
In particular, this involves extracting high-level 
meaning from the large collections of content 
data and its representation and management in a 
common knowledge base. 
Even now, building large and rich knowledge 
bases takes a great deal of expensive manual 
effort; this has severely hampered Knowledge-
Technologies and HLT application development. 
For example, dozens of person-years have been 
invest into the development of wordnets1 for 
various languages, but the data in these 
resources is still not sufficiently rich to support 
advanced concept-based HLT applications 
directly. Furthermore, resources produced by 
introspection usually fail to register what really 
occurs in texts. Applications will not scale up to 
working in the open domain without more 
detailed and rich general-purpose, which should 
perhaps include domain-specific linguistic 
knowledge.  
The MEANING project identifies two 
complementary intermediate tasks which we 
think are crucial in order to enable the next 
generation of intelligent open domain HLT 
application systems: Word Sense 
Disambiguation (WSD) and large-scale 
enrichment of Lexical Knowledge Bases.  
                                                     
1 A wordnet is a conceptually structured knowledge 
base of word senses. The English WordNet (Miller 
90, Fellbaum 98) has been developed at Princeton 
University over the past 14 years. EuroWordNet 
(Vossen 1998) is a multilingual database with 
wordnets for several European languages (Dutch, 
Italian, Spanish, German, French, Czech and 
Estonian). Balkanet is building wordnets for the 
Balkan languages following the EuroWordNet 
design. 
The advance in these two areas will allow for 
large-scale extractions of shallow meaning from 
texts, in the form of relations among concepts. 
WSD provides the technology to convert 
relations between words into relations between 
concepts. Rich and large-scale Lexical 
Knowledge Bases will have be the repositories 
of extracted relations and other linguistic 
knowledge.  
However, progress is difficult due to the 
following interdependence: 
? In order to achieve accurate WSD, we need 
far more linguistic and semantic knowledge 
than is available in current lexical 
knowledge bases (e.g. current wordnets).  
? In order to enrich Lexical Knowledge Bases 
we need to acquire information from 
corpora, which have been accurately tagged 
with word senses.  
Providing innovative technology to solve this 
problem will be one of the main challenges to 
access KTs.  
Following this introduction section 1 presents 
the major research goals in HLT. Section 2 
presents the MEANING roadmap. Finally, 
section 4 draws the conclusions. 
1 Major research goals in HLT 
In order to extend the state-of-the-art in human 
language technologies (HLT) future research 
must devise: (1) innovative processes and tools 
for automatic acquisition of lexical knowledge 
from large-scale document collections; (2) novel 
techniques for accurately selecting the sense of 
open-class words in a large number of 
languages; (3) ways to enrich existing 
multilingual linguistic knowledge resources with 
new kinds of lexical information by 
automatically mapping information across 
languages. We present each one in turn. 
1.1 Dealing with knowledge acquisition 
The acquisition of linguistic knowledge from 
corpora has been a very successful line of 
research. Research in the acquisition of 
subcategorization information, selectional 
preferences, in thematic role assignments and 
diathesis alternations (Agirre and Mart?nez 
2001, 2002, McCarthy and Korhonen, 1998; 
Korhonen et al, 2000; McCarthy 2001), domain 
information (Magnini and Cavagli? 2000), topic 
signatures (Agirre et al 2001b), lexico-semantic 
relations between words (Agirre et al 2002) etc. 
has obtained encouraging results. The 
acquisition process usually involves large bodies 
of text, which have been previously processed 
with shallow language processors.  
Much of the use of the acquired knowledge 
has been hampered by the fact that the texts are 
not sense-disambiguated, and therefore, only 
knowledge for words can be acquired, that is, 
subcategorization for words, selectional 
preferences for words, etc. It is a well 
established fact that much of the linguistic 
behavior of words can be better explained if it is 
keyed to word senses.  
For instance, the subcategorization frames of 
verbs are highly dependent of the sense of the 
verb. Some senses of a given verb allow for a 
particular combination of complements, while 
others do not (McCarthy, 2001). The same is 
applicable to selectional preferences; traditional 
approaches that learn selectional preferences for 
a verb, tend to mix e.g. all subjects for differents 
senses, even if verbs can have different 
selectional preferences for each word sense 
(Agirre & Martinez, 2002). 
Having texts automatically sense-tagged with 
high accuracy will produce significantly better 
acquired knowledge at a sense level, including 
subcategorization frequencies, domain 
information, topic signatures, selectional 
preferences, specific lexico-semantic relations, 
thematic role assignments and diathesis 
alternations. It will also facilitate the 
investigation on automatic methods for dealing 
with new senses not present in current wordnets 
and clustering of word senses. Furthermore, 
linguistic information keyed to word senses that 
are linked to interlingual concepts (as proposed 
in the EuroWordNet model), can be easily 
integrated in a multilingual Lexical Knowledge 
Base (cf. section 2.3) 
2.2 Dealing with WSD 
Word Sense Disambiguation (WSD) is the task 
of assigning the appropriate meaning (sense) to a 
given word in a text or discourse. Ide and 
Veronis (1998) argue that word sense ambiguity 
is a central problem for many established HLT 
applications (for example Machine Translation, 
Information Extraction and Information 
Retrieval). This is also the case for associated 
sub-tasks (i.e. reference resolution and parsing). 
For this reason many international research 
groups are working on WSD, using a wide range 
of approaches. However, no large-scale broad-
coverage accurate WSD system has been built 
up to date2. With current state-of-the-art 
accuracy in the range 60-70%, WSD is one of 
the most important open problems in Natural 
Language Processing. 
A promising current line of research uses 
semantically annotated corpora to train Machine 
Learning (ML) algorithms to decide which word 
sense to choose in which contexts. The words in 
these annotated corpora are tagged manually 
with semantic classes taken from a particular 
lexical semantic resource (most commonly 
WordNet). Many standard ML techniques have 
been tried, such as Bayesian learning, Exemplar 
based learning, Decision Lists, and recently 
margin-based classifiers like Boosting and 
Support Vector Machines (Escudero et al, 
2000a, 2000b, 2000c, 2000d, 2001; Mart?nez 
and Agirre, 2000). These approaches are termed 
"supervised" because they learn from previously 
sense annotated data and therefore they require a 
large amount of human intervention to annotate 
the training data. 
Supervised WSD systems are data hungry. 
They suffer from the "knowledge acquisition 
bottleneck", it takes them mere seconds to digest 
all of the processed corpus contained in training 
materials that take months to annotate manually. 
So, although Machine Learning classifiers are 
undeniably effective, they are not feasible until 
we can obtain reliable unsupervised training 
data. Ng (1997) estimates that the manual 
annotation effort necessary to build a broad 
coverage word-sense annotated English corpus 
is about 16 person-years; and this effort would 
have to be replicated for each different language. 
Unfortunately, many people think that Ng?s 
estimate might fell short, as the annotated corpus 
thus produced is not guaranteed to enable high 
accuracy WSD.  
Some recent work is focusing on reducing 
the acquisition cost and the need for supervision 
                                                     
2 See the conclusions of the SENSEVAL-2 
competition: http://www.sle.sharp.co.uk/senseval2/ 
in corpus-based methods for WSD. Leacock et 
al. (1998) and Mihalcea and Moldovan (1999) 
automatically generate arbitrarily large corpora 
for unsupervised WSD training, using the 
synonyms or definitions of word senses 
provided in WordNet to formulate search engine 
queries over the Web. In another line of 
research, (Yarowsky, 1995) and (Blum and 
Mitchell, 1998) have shown that it is possible to 
reduce the need for supervision with the help of 
large amounts of unannotated data. Applying 
these ideas, (Agirre and Mart?nez, 2000) has 
developed knowledge-based prototypes for 
obtaining accurate examples from the web for 
specific WordNet synsets, as well as, large 
quantities of unannotated examples. 
But in order to make significant advances in 
WSD system accuracy, systems need to be able 
to use types of lexical knowledge that are not 
currently available in wide-coverage lexical 
knowledge bases: for example subcategorisation 
frequencies for predicates (particularly verbs) 
rely on word senses, selectional preferences of 
predicates for classes of arguments, amongst 
others (Carroll and McCarthy, 2000; McCarthy 
et al, 2001; Agirre and Mart?nez, 2002;).  
2.3 Dealing with multilingualism  
Language diversity is at the same time a 
valuable cultural heritage worth preserving, and 
an obstacle to achieving a more cohesive social 
and economic development. This situation has 
been further stressed as a major challenge in IST 
research lines. Improving language 
communication capabilities is a prerequisite for 
increasing industrial competitiveness, this way 
leading to a sound growth in key economic 
sectors.  
However, this obstacle can be helpful 
because all languages realize the meaning in 
different ways. We can benefit from this fact 
using a novel multilingual mapping process that 
exploits the EuroWordNet architecture. In 
EuroWordNet local wordnets are linked via an 
Inter-Lingual-Index (ILI) allowing the 
connection from words in one language to 
translation equivalent words in any of the other 
languages. In that way, technological advances 
in one language can help the other.  
For instance, for Basque, being an 
agglutinative language with very rich 
morphological-syntactic information, it is 
possible to extract semantic relations that would 
be more difficult to capture in other languages. 
Below we can see an example of the relation 
betwewen silversmith and silver, extracted from 
the Basque words zilargile ? zilar respectively. 
This relation has been disambiguated into the 
?maker_of? lexico-semantic relation (Agirre & 
Lersundi, 2000).  
On the contrary, Basque is not largely present 
in the web as the others. Using this approach it is 
possible to balance both gaps.  
Although the technology to provide 
compatibility across wordnets exits (Daud? et al 
1999, 2000, 2001), new research is needed for 
porting and uploading the various types of 
knowledge across languages, and new ways to 
test the validity of the ported knowledge in the 
target languages.  
3. The MEANING Roadmap 
The improvements mentioned above have been 
explored separately with relative success. In 
fact, no research group in isolation has tried to 
combine all this aforementioned factors. We 
designed the MEANING project3 convinced that 
only a combination of all relevant knowledge 
and resources will be able to produce significant 
advances in this crucial research area.  
MEANING will treat the web as a (huge) 
corpus to learn information from, since even the 
largest conventional corpora available (e.g. the 
Reuters corpus, the British National Corpus) are 
not large enough to be able to acquire reliable 
information in sufficient detail about language 
behaviour. Moreover, most languages do not 
have large or diverse enough corpora available. 
MEANING proposes an innovative 
bootstrapping process to deal with the inter-
dependency between WSD and knowledge 
acquisition: 
1. Train accurate WSD systems and apply 
them to very large corpora by coupling 
knowledge-based techniques on the existing 
EuroWordNet (e.g. to populate it with 
domain labels, to induce automatically 
                                                     
                                                     3 Started in March 2002, MEANING IST-2001-
34460 "Developing Multilingual Web-scale 
Language Technologies" is a three years research 
project funded by the EC. 
training examples) with ML techniques that 
combine very large amounts of labeled and 
unlabeled data. When ready, use also the 
knowledge acquired in 2. 
2. Use the obtained accurate WSD data in 
conjunction with shallow parsing techniques 
and domain tagging to extract new linguistic 
knowledge to incorporate into 
EuroWordNet. 
This method will be able to break this 
interdependency in a series of cycles thanks to 
the fact that the WSD system will be based on 
all domain information, sophisticated linguistic 
knowledge, large numbers of automatically 
tagged examples from the web, and a 
combination of annotated and unannotated data. 
The first WSD system will have weaker 
linguistic knowledge, but the sole combination 
of the rest of the factors will produce significant 
performance gains. Besides, some of the 
required linguistic knowledge can be acquired 
from unnanotated data, and can therefore be 
acquired without using any WSD system. Once 
acceptable WSD is available, the acquired 
knowledge will be of a higher quality, and will 
allow for better WSD performance. 
Multilingualism will be also helpful for 
MEANING. The idiosyncratic way the meaning 
is realised in a particular language will be 
captured and ported to the rest of languages 
involved in the project4 using EuroWordNet as a 
Multilingual Central Repository in three 
consecutive phases (see figure 1). 
For instance, selectional preferences acquired 
for verb senses based on the English corpora, 
can be uploaded into the Multilingual Central 
Repository. As the selectional prefenrece 
relation is keyed to concepts in the repository, 
this knowledge can be ported to the other 
languages. Of course, the ported knowledge 
needs to be checked in order to evaluate the 
validity of this approach.  
Below, we can see the selectional preference 
for the first sense of know from (Agirre & 
martinez, 2002). The first sense of know is 
univocally linked to <know, cognize,
cognise>, which in EuroWordNet is linked to 
4 MEANING will work with three major European 
languages (English, Spanish and Italian) and two 
minority languages (Catalan and Basque).  
w
S
a
B
s
0
0
0
0
0
4
W
s
m
p
m
c
e
Multilingual Central Repository 
EANING is going to constitute 
wledge resource for a number of 
sses that need large amounts of 
to be effective tools (e.g. web 
P tools and software of the next 
l benefit from the MEANING 
Multilingual
Central Repository
Italian
EWN
Basque
EWN
Spanish
EWN
English
EWN
Basque
Web Corpus
Italian
Web Corpus
English
Web Corpus
Catalan
EWN
Spanish
Web Corpus
Catalan
Web Corpus
ACQ
ACQACQ
ACQ
UPLOADUPLOAD
UPLOADUPLOAD
PORT
PORT
PORT
PORT
WSD
WSD
WSD
WSD
 access applications are based on 
NG will open the way for access 
gual web based on concepts, 
lications with capabilities that 
ceed those currently available. 
ill facilitate development of 
pen domain Internet applications 
tion/Answering, Cross Lingual 
etrieval, Summarisation, Text 
Event Tracking, Information 
achine Translation, etc.). 
EANING will supply a common 
cture to Internet documents, thus 
owledge management of web 
ommon conceptual structure is a ord senses conocer_1 and saber_1 in 
panish, con?ixer_1 and saber_1 in Catalan 
nd antzeman_1, jakin_2 and ezagutu_1 in 
asque.  
ense 1: know, cognize -- (be
cognizant or aware of a fact or a
specific piece of information;
possess knowledge or information
about;
,1128 <communication> 
,0615 <measure quantity amount quantum> 
,0535 <attribute> 
,0389 <object physical_object> 
,0307 <cognition knowledge> 
 Conclusions 
here the acquisition of knowledge  from large-
cale document collections will be  one of the 
ajor challenge for the next generation of text 
rocessing applications, MEANING emphasises 
ultilingual  content-based access to web 
ontent. Moreover, it can provide a keystone 
nabling technologies for the semantic web. In 
particular, the 
produced by M
the natural kno
semantic proce
linguistic data 
ontologies). NL
generation wil
outcomes.  
Figure 1: MEANING data flow. 
Current web
words; MEANI
to the multilin
providing app
significantly ex
MEANING w
concept-based o
(such as Ques
Information R
Categorisation, 
Extraction, M
Furthermore, M
conceptual stru
facilitating kn
content. This c
decisive enabling technology for allowing the 
semantic web. 
Acknowledgements 
The MEANING project is funded by the 
European Commission (IST-2001-34460). 
References 
Agirre E. and Lersundi M. Extracci?n de relaciones 
l?xico-sem?nticas a partir de palabras derivadas 
usando patrones de definici?n. Proceedings of the 
Annual SEPLN meeting. Spain, 2000. 
Agirre E., Lersundi M. and Mart?nez D. A 
Multilingual Approach to Disambiguate 
Prepositions and Case Suffixes. Proceeding of the 
Workshop ?Word Sense Disambiguation: Recent 
Successes and Future Directions? organized by 
ACL 2002.  
Agirre E. and Mart?nez D. Exploring automatic word 
sense disambiguation with decision lists and the 
Web.  Proceedings of the Workshop ?Semantic 
Annotation And Intelligent Annotation? organized 
by COLING 2000. Luxembourg. 2000.  
Agirre E. and Martinez D. Learning class-to-class 
selectional preferences. Proceedings of the 
Workshop "Computational Natural Language 
Learning" (CoNLL-2001). In conjunction with 
ACL'2001/EACL'2001. Toulouse. 2001. 
Agirre E., Ansa O., Mart?nez D. and Hovy E. 
Enriching WordNet concepts with topic signatures. 
Proceedings of the NAACL workshop on WordNet 
and Other lexical Resources: Applications, 
Extensions and Customizations. Pittsburg. 2001. 
Agirre E. and Martinez D. Integrating selectional 
preferences in WordNet. Proceedings of the first 
International WordNet Conference. Mysore, India, 
2002. 
Blum A. and Mitchel T. Combining labelled and 
unlabeled data with co-training. In Proceedings of 
the 11th Annual Conference on Computational 
Learning Theory. 1998. 
Carroll, J. and McCarthy, D. Word sense 
disambiguation using automatically acquired 
verbal preferences. Computers and the Humanities. 
Senseval Special Issue, Vol. 34, No 1-2. 2000. 
Daud? J., Padr? L. and Rigau G., Mapping 
Multilingual Hierarchies using Relaxation 
Labelling, Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora (EMNLP/VLC'99). Maryland, 
1999.  
Daud? J., Padr? L. and Rigau G., Mapping WordNets 
Using Structural Information , 38th Anual Meeting 
of the ACL. Hong Kong, 2000.  
Daud? J., Padr? L. and Rigau G., A Complete WN1.5 
to WN1.6 Mapping, Proceedings of NAACL 
Workshop "WordNet and Other Lexical Resources: 
Applications, Extensions and Customizations". 
Pittsburg, PA, 2001. 
Escudero G., M?rquez L. and Rigau G., Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 11th European Conference on 
Machine Learning. Barcelona. 2000.  
Escudero G., M?rquez L. and Rigau G., Naive Bayes 
and Exemplar-Based approaches to Word Sense 
Disambiguation Revisited.  Proceedings of the 14th 
European Conference on Artificial Intelligence, 
Berlin. 2000.  
Escudero G., M?rquez L. and Rigau G., A 
Comparison between Supervised Learning 
Algorithms for Word Sense Disambiguation. 
Proceedings of Fourth Computational Natural 
Language Learning Workshop. Lisbon. 2000.  
Escudero G., M?rquez L. and Rigau G., An Empirical 
Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems. Proceedings 
of Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. Hong Kong. 2000. 
Escudero G., M?rquez L. and Rigau G., Using 
LazyBoosting for Word Sense Disambiguation. 
Proceedings of 2nd International Workshop 
?Evaluating Word Sense Disambiguation 
Systems?, SENSEVAL-2. Toulouse. 2001. 
Fellbaum C. editor. WordNet An Electronic Lexical 
Database. The MIT Press. 1998. 
Ide, N. and V?ronis, J. Introduction to the special 
issue on word sense disambiguation: The state of 
the art. Computational Linguistics, 24 (1), 1998. 
Korhonen A., Gorrell, G. and McCarthy D. Statistical 
Filtering and Subcategorization Frame 
Acquisition. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
Hong Kong. 2000. 
Leacock, C. Chodorow, M. and Miller, G.A. Using 
Corpus Statistics and WordNet Relations for Sense 
Identication, Computational Linguistics, 24(1), 
1998. 
Magnini B. and Cavagli? G., Integrating subject field 
codes into WordNet. In Proceedings of the 2nd 
International Conference on Language Resources 
and Evaluation, Athens. 2000. 
Mart?nez D. and Agirre E. One Sense per Collocation 
and Genre/Topic Variations. Proceedings of the 
Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. Hong Kong, 2000. 
McCarthy, D. and Korhonen, A. Detecting verbal 
participation in diathesis alternations. Proceedings 
of the 17th International Conference on 
Computational Linguistics and 36th Annual 
Meeting of the Association for Computational 
Linguistics COLING-ACL'98. Montreal. 1998.  
McCarthy D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, 
Subcategorization Frames and Selectional 
Preferences. Ph.D. thesis, University of Sussex. 
2001. 
McCarthy D., Carroll J. and Preiss J. Disambiguating 
noun and verb senses using automatically acquired 
selectional preferences. Proceedings of the 
SENSEVAL-2 Workshop at ACL/EACL'01, 
Toulouse. 2001. 
Mihalcea R. and Moldovan D. An automatic method 
for generating sense tagged corpora. In 
Proceedings of American Association for Artificial 
Intelligence. 1999. 
Miller G. Five papers on WordNet, Special Issue of 
International Journal of Lexicogrphy 3(4). 1990. 
Ng. H. T. Getting Serious about Word Sense 
Disambiguation. In Proceedings of Workshop 
?Tagging Text with Lexical Semantics: Why, what 
and how??, Washington, 1997. 
Vossen P. EuroWordNet: A Multilingual Database 
with Lexical Semantic Networks, Kluwer Academic 
Publishers, Dordrecht. 1998. 
Yarowsky D., Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 1995. 
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 01: Evaluating WSD
on Cross-Language Information Retrieval
Eneko Agirre
IXA NLP group
University of the Basque Country
Donostia, Basque Counntry
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibloleo@ehu.es
German Rigau
IXA NLP group
University of the Basque Country
Donostia, Basque Country
german.rigau@ehu.es
Bernardo Magnini
ITC-IRST
Trento, Italy
magnini@itc.it
Arantxa Otegi
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibotusa@ehu.es
Piek Vossen
Irion Technologies
Delftechpark 26
2628XH Delft, Netherlands
Piek.Vossen@irion.nl
Abstract
This paper presents a first attempt of an
application-driven evaluation exercise of
WSD. We used a CLIR testbed from the
Cross Lingual Evaluation Forum. The ex-
pansion, indexing and retrieval strategies
where fixed by the organizers. The par-
ticipants had to return both the topics and
documents tagged with WordNet 1.6 word
senses. The organization provided training
data in the form of a pre-processed Semcor
which could be readily used by participants.
The task had two participants, and the orga-
nizer also provide an in-house WSD system
for comparison.
1 Introduction
Since the start of Senseval, the evaluation of Word
Sense Disambiguation (WSD) as a separate task is a
mature field, with both lexical-sample and all-words
tasks. In the first case the participants need to tag the
occurrences of a few words, for which hand-tagged
data has already been provided. In the all-words task
all the occurrences of open-class words occurring in
two or three documents (a few thousand words) need
to be disambiguated.
The community has long mentioned the neces-
sity of evaluating WSD in an application, in order
to check which WSD strategy is best, and more im-
portant, to try to show that WSD can make a differ-
ence in applications. The use of WSD in Machine
Translation has been the subject of some recent pa-
pers, but less attention has been paid to Information
Retrieval (IR).
With this proposal we want to make a first try to
define a task where WSD is evaluated with respect
to an Information Retrieval and Cross-Lingual Infor-
mation Retrieval (CLIR) exercise. From the WSD
perspective, this task will evaluate all-words WSD
systems indirectly on a real task. From the CLIR
perspective, this task will evaluate which WSD sys-
tems and strategies work best.
We are conscious that the number of possible con-
figurations for such an exercise is very large (in-
cluding sense inventory choice, using word sense in-
duction instead of disambiguation, query expansion,
WSD strategies, IR strategies, etc.), so this first edi-
tion focuses on the following:
? The IR/CLIR system is fixed.
? The expansion / translation strategy is fixed.
? The participants can choose the best WSD
strategy.
1
? The IR system is used as the upperbound for
the CLIR systems.
We think that it is important to start doing this
kind of application-driven evaluations, which might
shed light to the intricacies in the interaction be-
tween WSD and IR strategies. We see this as the
first of a series of exercises, and one outcome of this
task should be that both WSD and CLIR communi-
ties discuss together future evaluation possibilities.
This task has been organized in collabora-
tion with the Cross-Language Evaluation Forum
(CLEF1). The results will be analyzed in the CLEF-
2007 workshop, and a special track will be pro-
posed for CLEF-2008, where CLIR systems will
have the opportunity to use the annotated data
produced as a result of the Semeval-2007 task.
The task has a webpage with all the details at
http://ixa2.si.ehu.es/semeval-clir.
This paper is organized as follows. Section 2
describes the task with all the details regarding
datasets, expansion/translation, the IR/CLIR system
used, and steps for participation. Section 3 presents
the evaluation performed and the results obtained by
the participants. Finally, Section 4 draws the con-
clusions and mention the future work.
2 Description of the task
This is an application-driven task, where the appli-
cation is a fixed CLIR system. Participants disam-
biguate text by assigning WordNet 1.6 synsets and
the system will do the expansion to other languages,
index the expanded documents and run the retrieval
for all the languages in batch. The retrieval results
are taken as the measure for fitness of the disam-
biguation. The modules and rules for the expansion
and the retrieval will be exactly the same for all par-
ticipants.
We proposed two specific subtasks:
1. Participants disambiguate the corpus, the cor-
pus is expanded to synonyms/translations and
we measure the effects on IR/CLIR. Topics2 are
not processed.
1http://www.clef-campaign.org
2In IR topics are the short texts which are used by the sys-
tems to produce the queries. They usually provide extensive
information about the text to be searched, which can be used
both by the search engine and the human evaluators.
2. Participants disambiguate the topics per lan-
guage, we expand the queries to syn-
onyms/translations and we measure the effects
on IR/CLIR. Documents are not processed
The corpora and topics were obtained from the
ad-hoc CLEF tasks. The supported languages in the
topics are English and Spanish, but in order to limit
the scope of the exercise we decided to only use En-
glish documents. The participants only had to dis-
ambiguate the English topics and documents. Note
that most WSD systems only run on English text.
Due to these limitations, we had the following
evaluation settings:
IR with WSD of topics , where the participants
disambiguate the documents, the disam-
biguated documents are expanded to syn-
onyms, and the original topics are used for
querying. All documents and topics are in En-
glish.
IR with WSD of documents , where the partic-
ipants disambiguate the topics, the disam-
biguated topics are expanded and used for
querying the original documents. All docu-
ments and topics are in English.
CLIR with WSD of documents , where the partic-
ipants disambiguate the documents, the dis-
ambiguated documents are translated, and the
original topics in Spanish are used for query-
ing. The documents are in English and the top-
ics are in Spanish.
We decided to focus on CLIR for evaluation,
given the difficulty of improving IR. The IR results
are given as illustration, and as an upperbound of
the CLIR task. This use of IR results as a reference
for CLIR systems is customary in the CLIR commu-
nity (Harman, 2005).
2.1 Datasets
The English CLEF data from years 2000-2005 com-
prises corpora from ?Los Angeles Times? (year
1994) and ?Glasgow Herald? (year 1995) amounting
to 169,477 documents (579 MB of raw text, 4.8GB
in the XML format provided to participants, see Sec-
tion 2.3) and 300 topics in English and Spanish (the
topics are human translations of each other). The
relevance judgments were taken from CLEF. This
2
might have the disadvantage of having been pro-
duced by pooling the results of CLEF participants,
and might bias the results towards systems not using
WSD, specially for monolingual English retrieval.
We are considering the realization of a post-hoc
analysis of the participants results in order to ana-
lyze the effect on the lack of pooling.
Due to the size of the document collection, we de-
cided that the limited time available in the competi-
tion was too short to disambiguate the whole collec-
tion. We thus chose to take a sixth part of the corpus
at random, comprising 29,375 documents (874MB
in the XML format distributed to participants). Not
all topics had relevant documents in this 17% sam-
ple, and therefore only 201 topics were effectively
used for evaluation. All in all, we reused 21,797
relevance judgements that contained one of the doc-
uments in the 17% sample, from which 923 are pos-
itive3. For the future we would like to use the whole
collection.
2.2 Expansion and translation
For expansion and translation we used the publicly
available Multilingual Central Repository (MCR)
from the MEANING project (Atserias et al, 2004).
The MCR follows the EuroWordNet design, and
currently includes English, Spanish, Italian, Basque
and Catalan wordnets tightly connected through the
Interlingual Index (based on WordNet 1.6, but linked
to all other WordNet versions).
We only expanded (translated) the senses returned
by the WSD systems. That is, given a word like
?car?, it will be expanded to ?automobile? or ?railcar?
(and translated to ?auto? or ?vago?n? respectively) de-
pending on the sense in WN 1.6. If the systems re-
turns more than one sense, we choose the sense with
maximum weight. In case of ties, we expand (trans-
late) all. The participants could thus implicitly affect
the expansion results, for instance, when no sense
could be selected for a target noun, the participants
could either return nothing (or NOSENSE, which
would be equivalent), or all senses with 0 score. In
the first case no expansion would be performed, in
the second all senses would be expanded, which is
equivalent to full expansion. This fact will be men-
tioned again in Section 3.5.
3The overall figures are 125,556 relevance judgements for
the 300 topics, from which 5700 are positive
Note that in all cases we never delete any of the
words in the original text.
In addition to the expansion strategy used with the
participants, we tested other expansion strategies as
baselines:
noexp no expansion, original text
fullexp expansion (translation in the case of English
to Spanish expansion) to all synonyms of all
senses
wsd50 expansion to the best 50% senses as returned
by the WSD system. This expansion was tried
over the in-house WSD system of the organizer
only.
2.3 IR/CLIR system
The retrieval engine is an adaptation of the Twenty-
One search system (Hiemstra and Kraaij, 1998) that
was developed during the 90?s by the TNO research
institute at Delft (The Netherlands) getting good re-
sults on IR and CLIR exercises in TREC (Harman,
2005). It is now further developed by Irion technolo-
gies as a cross-lingual retrieval system (Vossen et al,
). For indexing, the TwentyOne system takes Noun
Phrases as an input. Noun Phases (NPs) are detected
using a chunker and a word form with POS lexicon.
Phrases outside the NPs are not indexed, as well as
non-content words (determiners, prepositions, etc.)
within the phrase.
The Irion TwentyOne system uses a two-stage re-
trieval process where relevant documents are first
extracted using a vector space matching and sec-
ondly phrases are matched with specific queries.
Likewise, the system is optimized for high-precision
phrase retrieval with short queries (1 up 5 words
with a phrasal structure as well). The system can be
stripped down to a basic vector space retrieval sys-
tem with an tf.idf metrics that returns documents for
topics up to a length of 30 words. The stripped-down
version was used for this task to make the retrieval
results compatible with the TREC/CLEF system.
The Irion system was also used for pre-
processing. The CLEF corpus and topics were con-
verted to the TwentyOne XML format, normalized,
and named-entities and phrasal structured detected.
Each of the target tokens was identified by an unique
identifier.
2.4 Participation
The participants were provided with the following:
3
1. the document collection in Irion XML format
2. the topics in Irion XML format
In addition, the organizers also provided some of
the widely used WSD features in a word-to-word
fashion4 (Agirre et al, 2006) in order to make partic-
ipation easier. These features were available for both
topics and documents as well as for all the words
with frequency above 10 in SemCor 1.6 (which can
be taken as the training data for supervised WSD
systems). The Semcor data is publicly available 5.
For the rest of the data, participants had to sign and
end user agreement.
The participants had to return the input files en-
riched with WordNet 1.6 sense tags in the required
XML format:
1. for all the documents in the collection
2. for all the topics
Scripts to produce the desired output from word-
to-word files and the input files were provided by
organizers, as well as DTD?s and software to check
that the results were conformant to the respective
DTD?s.
3 Evaluation and results
For each of the settings presented in Section 2 we
present the results of the participants, as well as
those of an in-house system presented by the orga-
nizers. Please refer to the system description papers
for a more complete description. We also provide
some baselines and alternative expansion (transla-
tion) strategies. All systems are evaluated accord-
ing to their Mean Average Precision 6 (MAP) as
computed by the trec eval software on the pre-
existing CLEF relevance-assessments.
3.1 Participants
The two systems that registered sent the results on
time.
PUTOP They extend on McCarthy?s predominant
sense method to create an unsupervised method
of word sense disambiguation that uses auto-
matically derived topics using Latent Dirichlet
4Each target word gets a file with all the occurrences, and
each occurrence gets the occurrence identifier, the sense tag (if
in training), and the list of features that apply to the occurrence.
5http://ixa2.si.ehu.es/semeval-clir/
6http://en.wikipedia.org/wiki/
Information retrieval
Allocation. Using topic-specific synset similar-
ity measures, they create predictions for each
word in each document using only word fre-
quency information. The disambiguation pro-
cess took aprox. 12 hours on a cluster of 48 ma-
chines (dual Xeons with 4GB of RAM). Note
that contrary to the specifications, this team
returned WordNet 2.1 senses, so we had to
map automatically to 1.6 senses (Daude et al,
2000).
UNIBA This team uses a a knowledge-based WSD
system that attempts to disambiguate all words
in a text by exploiting WordNet relations. The
main assumption is that a specific strategy for
each Part-Of-Speech (POS) is better than a sin-
gle strategy. Nouns are disambiguated basi-
cally using hypernymy links. Verbs are dis-
ambiguated according to the nouns surrounding
them, and adjectives and adverbs use glosses.
ORGANIZERS In addition to the regular partic-
ipants, and out of the competition, the orga-
nizers run a regular supervised WSD system
trained on Semcor. The system is based on
a single k-NN classifier using the features de-
scribed in (Agirre et al, 2006) and made avail-
able at the task website (cf. Section 2.4).
In addition to those we also present some com-
mon IR/CLIR baselines, baseline WSD systems, and
an alternative expansion:
noexp a non-expansion IR/CLIR baseline of the
documents or topics.
fullexp a full-expansion IR/CLIR baseline of the
documents or topics.
wsdrand a WSD baseline system which chooses a
sense at random. The usual expansion is ap-
plied.
1st a WSD baseline system which returns the sense
numbered as 1 in WordNet. The usual expan-
sion is applied.
wsd50 the organizer?s WSD system, where the 50%
senses of the word ranking according to the
WSD system are expanded. That is, instead of
expanding the single best sense, it expands the
best 50% senses.
3.2 IR Results
This section present the results obtained by the par-
ticipants and baselines in the two IR settings. The
4
IRtops IRdocs CLIR
no expansion 0.3599 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
UNIBA 0.3030 0.1521 0.1373
PUTOP 0.3036 0.1482 0.1734
wsdrand 0.2673 0.1482 0.2617
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
wsd50 0.2651 0.1479 0.2640
Table 1: Retrieval results given as MAP. IRtops
stands for English IR with topic expansion. IR-
docs stands for English IR with document expan-
sion. CLIR stands for CLIR results for translated
documents.
second and third columns of Table 1 present the re-
sults when disambiguating the topics and the docu-
ments respectively. Non of the expansion techniques
improves over the baseline (no expansion).
Note that due to the limitation of the search en-
gine, long queries were truncated at 50 words, which
might explain the very low results of the full expan-
sion.
3.3 CLIR results
The last column of Table 1 shows the CLIR results
when expanding (translating) the disambiguated
documents. None of the WSD systems attains the
performance of full expansion, which would be the
baseline CLIR system, but the WSD of the organizer
gets close.
3.4 WSD results
In addition to the IR and CLIR results we also pro-
vide the WSD performance of the participants on
the Senseval 2 and 3 all-words task. The documents
from those tasks were included alongside the CLEF
documents, in the same formats, so they are treated
as any other document. In order to evaluate, we had
to map automatically all WSD results to the respec-
tive WordNet version (using the mappings in (Daude
et al, 2000) which are publicly available).
The results are presented in Table 2, where we can
see that the best results are attained by the organizers
WSD system.
3.5 Discussion
First of all, we would like to mention that the WSD
and expansion strategy, which is very simplistic, de-
grades the IR performance. This was rather ex-
Senseval-2 all words
precision recall coverage
ORGANIZERS 0.584 0.577 93.61%
UNIBA 0.498 0.375 75.39%
PUTOP 0.388 0.240 61.92%
Senseval-3 all words
precision recall coverage
ORGANIZERS 0.591 0.566 95.76%
UNIBA 0.484 0.338 69.98%
PUTOP 0.334 0.186 55.68%
Table 2: English WSD results in the Senseval-2 and
Senseval-3 all-words datasets.
pected, as the IR experiments had an illustration
goal, and are used for comparison with the CLIR
experiments. In monolingual IR, expanding the top-
ics is much less harmful than expanding the docu-
ments. Unfortunately the limitation to 50 words in
the queries might have limited the expansion of the
topics, which make the results rather unreliable. We
plan to fix this for future evaluations.
Regarding CLIR results, even if none of the WSD
systems were able to beat the full-expansion base-
line, the organizers system was very close, which is
quite encouraging due to the very simplistic expan-
sion, indexing and retrieval strategies used.
In order to better interpret the results, Table 3
shows the amount of words after the expansion in
each case. This data is very important in order to un-
derstand the behavior of each of the systems. Note
that UNIBA returns 3 synsets at most, and therefore
the wsd50 strategy (select the 50% senses with best
score) leaves a single synset, which is the same as
taking the single best system (wsdbest). Regarding
PUTOP, this system returned a single synset, and
therefore the wsd50 figures are the same as the ws-
dbest figures.
Comparing the amount of words for the two par-
ticipant systems, we see that UNIBA has the least
words, closely followed by PUTOP. The organizers
WSD system gets far more expanded words. The
explanation is that when the synsets returned by a
WSD system all have 0 weights, the wsdbest expan-
sion strategy expands them all. This was not explicit
in the rules for participation, and might have affected
the results.
A cross analysis of the result tables and the num-
ber of words is interesting. For instance, in the IR
exercise, when we expand documents, the results in
5
English Spanish
No WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767
UNIBA
wsdbest 19,436,374 17,226,104
wsd50 19,436,374 17,226,104
PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485
Baseline 1st 24,842,800 20,261,081
WSD wsdrand 24,904,717 19,137,981
ORG. wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723
Table 3: Number of words in the document col-
lection after expansion for the WSD system and all
baselines. wsdbest stands for the expansion strategy
used with participants.
the third column of Table 1 show that the ranking for
the non-informed baselines is the following: best for
no expansion, second for random WSD, and third
for full expansion. These results can be explained
because of the amount of expansion: the more ex-
pansion the worst results. When more informed
WSD is performed, documents with more expansion
can get better results, and in fact the WSD system of
the organizers is the second best result from all sys-
tem and baselines, and has more words than the rest
(with exception of wsd50 and full expansion). Still,
the no expansion baseline is far from the WSD re-
sults.
Regarding the CLIR result, the situation is in-
verted, with the best results for the most productive
expansions (full expansion, random WSD and no ex-
pansion, in this order). For the more informed WSD
methods, the best results are again for the organizers
WSD system, which is very close to the full expan-
sion baseline. Even if wsd50 has more expanded
words wsdbest is more effective. Note the very high
results attained by random. These high results can
be explained by the fact that many senses get the
same translation, and thus for many words with few
translation, the random translation might be valid.
Still the wsdbest, 1st sense and wsd50 results get
better results.
4 Conclusions and future work
This paper presents the results of a preliminary at-
tempt of an application-driven evaluation exercise
of WSD in CLIR. The expansion, indexing and re-
trieval strategies proved too simplistic, and none of
the two participant systems and the organizers sys-
tem were able to beat the full-expansion baseline.
Due to efficiency reasons, the IRION system had
some of its features turned off. Still the results are
encouraging, as the organizers system was able to
get very close to the full expansion strategy with
much less expansion (translation).
For the future, a special track of CLEF-2008 will
leave the avenue open for more sophisticated CLIR
techniques. We plan to extend the WSD annotation
to all words in the CLEF English document collec-
tion, and we also plan to contact the best performing
systems of the SemEval all-words tasks to have bet-
ter quality annotations.
Acknowledgements
We wish to thank CLEF for allowing us to use their data, and the
CLEF coordinator, Carol Peters, for her help and collaboration.
This work has been partially funded by the Spanish education
ministry (project KNOW)
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2006.
Exploring feature set combinations for WSD. In Proc.
of the SEPLN.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The MEANING
Multilingual Central Repository. In Proceedings of the
2.nd Global WordNet Conference, GWC 2004, pages
23?30. Masaryk University, Brno, Czech Republic.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets Using Structural Information. In Proc. of ACL,
Hong Kong.
D. Harman. 2005. Beyond English. In E. M. Voorhees
and D. Harman, editors, TREC: Experiment and Eval-
uation in Information Retrieval, pages 153?181. MIT
press.
D. Hiemstra and W. Kraaij. 1998. Twenty-One in ad-hoc
and CLIR. In E.M. Voorhees and D. K. Harman, ed-
itors, Proc. of TREC-7, pages 500?540. NIST Special
Publication.
P. Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,
and M. Fuentes. Meaningful results for Information
Retrieval in the MEANING project. In Proc. of the
3rd Global Wordnet Conference.
6
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1691?1701,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Offspring from Reproduction Problems:
What Replication Failure Teaches Us
Antske Fokkens and Marieke van Erp
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
{a.s.fokkens,m.g.j.van.erp}@vu.nl
Marten Postma
Utrecht University
Utrecht, The Netherlands
martenp@gmail.com
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
Piek Vossen
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
piek.vossen@vu.nl
Nuno Freire
The European Library
The Hague, The Netherlands
nfreire@gmail.com
Abstract
Repeating experiments is an important in-
strument in the scientific toolbox to vali-
date previous work and build upon exist-
ing work. We present two concrete use
cases involving key techniques in the NLP
domain for which we show that reproduc-
ing results is still difficult. We show that
the deviation that can be found in repro-
duction efforts leads to questions about
how our results should be interpreted.
Moreover, investigating these deviations
provides new insights and a deeper under-
standing of the examined techniques. We
identify five aspects that can influence the
outcomes of experiments that are typically
not addressed in research papers. Our use
cases show that these aspects may change
the answer to research questions leading
us to conclude that more care should be
taken in interpreting our results and more
research involving systematic testing of
methods is required in our field.
1 Introduction
Research is a collaborative effort to increase
knowledge. While it includes validating previous
approaches, our experience is that most research
output in our field focuses on presenting new ap-
proaches, and to a somewhat lesser extent building
upon existing work.
In this paper, we argue that the value of research
that attempts to replicate previous approaches goes
beyond simply validating what is already known.
It is also an essential aspect for building upon
existing approaches. Especially when validation
fails or variations in results are found, systematic
testing helps to obtain a clearer picture of both the
approach itself and of the meaning of state-of-the-
art results leading to a better insight into the qual-
ity of new approaches in relation to previous work.
We support our claims by presenting two use
cases that aim to reproduce results of previous
work in two key NLP technologies: measuring
WordNet similarity and Named Entity Recogni-
tion (NER). Besides highlighting the difficulty of
repeating other researchers? work, new insights
about the approaches emerged that were not pre-
sented in the original papers. This last point shows
that reproducing results is not merely part of good
practice in science, but also an essential part in
gaining a better understanding of the methods we
use. Likewise, the problems we face in reproduc-
ing previous results are not merely frustrating in-
conveniences, but also pointers to research ques-
tions that deserve deeper investigation.
We investigated five aspects that cause exper-
imental variation that are not typically described
in publications: preprocessing (e.g. tokenisa-
tion), experimental setup (e.g. splitting data for
cross-validation), versioning (e.g. which version
of WordNet), system output (e.g. the exact fea-
tures used for individual tokens in NER), and sys-
tem variation (e.g. treatment of ties).
As such, reproduction provides a platform for
systematically testing individual aspects of an ap-
proach that contribute to a given result. What is
the influence of the size of the dataset, for exam-
ple? How does using a different dataset affect the
results? What is a reasonable divergence between
different runs of the same experiment? Finding
answers to these questions enables us to better in-
terpret our state-of-the-art results.
1691
Moreover, the experiments in this paper show
that even while strictly trying to replicate a pre-
vious experiment, results may vary up to a point
where they lead to different answers to the main
question addressed by the experiment. The Word-
Net similarity experiment use case compares the
performance of different similarity measures. We
will show that the answer as to which measure
works best changes depending on factors such as
the gold standard used, the strategy towards part-
of-speech or the ranking coefficient, all aspects
that are typically not addressed in the literature.
The main contributions of this paper are the
following:
1) An in-depth analysis of two reproduction use
cases in NLP
2) New insights into the state-of-the-art results
for WordNet similarities and NER, found because
of problems in reproducing prior research
3) A categorisation of aspects influencing
reproduction of experiments and suggestions on
testing their influence systematically
The code, data and experimental setup
for the WordNet experiments are avail-
able at http://github.com/antske/
WordNetSimilarity, and for the NER exper-
iments at http://github.com/Mvanerp/
NER. The experiments presented in this paper
have been repeated by colleagues not involved in
the development of the software using the code
included in these repositories. The remainder of
this paper is structured as follows. In Section 2,
previous work is discussed. Sections 3 and 4
describe our real-world use cases. In Section 5,
we present our observations, followed by a more
general discussion in Section 6. In Section 7, we
present our conclusions.
2 Background
This section provides a brief overview of recent
work addressing reproduction and benchmark re-
sults in computer science related studies and dis-
cusses how our research fits in the overall picture.
Most researchers agree that validating results
entails that a method should lead to the same over-
all conclusions rather than producing the exact
same numbers (Drummond, 2009; Dalle, 2012;
Buchert and Nussbaum, 2012, etc.). In other
words, we should strive to reproduce the same an-
swer to a research question by different means,
perhaps by re-implementing an algorithm or eval-
uating it on a new (in domain) data set. Replica-
tion has a somewhat more limited aim, and simply
involves running the exact same system under the
same conditions in order to get the exact same re-
sults as output.
According to Drummond (2009) replication is
not interesting, since it does not lead to new in-
sights. On this point we disagree with Drum-
mond (2009) as replication allows us to: 1) vali-
date prior research, 2) improve on prior research
without having to rebuild software from scratch,
and 3) compare results of reimplementations and
obtain the necessary insights to perform reproduc-
tion experiments. The outcome of our use cases
confirms the statement that deeper insights into an
approach can be obtained when all resources are
available, an observation also made by Ince et al
(2012).
Even if exact replication is not a goal many
strive for, Ince et al (2012) argue that insightful
reproduction can be an (almost) impossible un-
dertaking without the source code being available.
Moreover, it is not always clear where replication
stops and reproduction begins. Dalle (2012) dis-
tinguishes levels of reproducing results related to
how close they are to the original work and how
each contributes to research. In general, an in-
creasing awareness of the importance of reproduc-
tion research and open code and data can be ob-
served based on publications in high-profile jour-
nals (e.g. Nature (Ince et al, 2012)) and initiatives
such as myExperiment.1
Howison and Herbsleb (2013) point out that,
even though this is important, often not enough
(academic) credit is gained from making resources
available. What is worse, the same holds for re-
search that investigates existing methods rather
than introducing new ones, as illustrated by the
question that is found on many review forms ?how
novel is the presented approach??. On the other
hand, initiatives for journals addressing exactly
this issue (Neylon et al, 2012) and tracks focus-
ing on results verification at conferences such as
VLDB2 show that this opinion is not universal.
A handful of use cases on reproducing or repli-
cating results have been published. Louridas and
Gousios (2012) present a use case revealing that
source code alone is not enough for reproducing
1http://www.myexperiment.org
2http://www.vldb.org/2013/
1692
results, a point that is also made by Mende (2010)
who provides an overview of all information re-
quired to replicate results.
The experiments in this paper provide use cases
that confirm the points brought out in the litera-
ture mentioned above. This includes both obser-
vations that a detailed level of information is re-
quired for truly insightful reproduction research as
well as the claim that such research leads to better
understanding of our techniques. Furthermore, the
work in this paper relates to Bikel (2004)?s work.
He provides all information needed in addition to
Collins (1999) to replicate Collins? benchmark re-
sults. Our work is similar in that we also aim to fill
in the blanks needed to replicate results. It must
be noted, however, that the use cases in this paper
have a significantly smaller scale than Bikel?s.
Our research distinguishes itself from previous
work, because it links the challenges of reproduc-
tion to what they mean for reported results be-
yond validation. Ruml (2010) mentions variations
in outcome as a reason not to emphasise compar-
isons to benchmarks. Vanschoren et al (2012)
propose to use experimental databases to system-
atically test variations for machine learning, but
neither links the two issues together. Raeder et al
(2010) come closest to our work in a critical study
on the evaluation of machine learning. They show
that choices in the methodology, such as data sets,
evaluation metrics and type of cross-validation can
influence the conclusions of an experiment, as we
also find in our second use case. However, they
focus on the problem of evaluation and recom-
mendations on how to achieve consistent repro-
ducible results. Our contribution is to investigate
how much results vary. We cannot control how
fellow researchers carry out their evaluation, but
if we have an idea of the variations that typically
occur within a system, we can better compare ap-
proaches for which not all details are known.
3 WordNet Similarity Measures
Patwardhan and Pedersen (2006) and Pedersen
(2010) present studies where the output of a va-
riety of WordNet similarity and relatedness mea-
sures are compared. They rank Miller and Charles
(1991)?s set (henceforth ?mc-set?) of 30 word
pairs according to their semantic relatedness with
several WordNet similarity measures.
Each measure ranks the mc-set of word pairs
and these outputs are compared to Miller and
Charles (1991)?s gold standard based on human
rankings using the Spearman?s Correlation Coeffi-
cient (Spearman, 1904, ?). Pedersen (2010) also
ranks the original set of 65 word pairs ranked
by humans in an experiment by Rubenstein and
Goodenough (1965) (rg-set) which is a superset of
Miller and Charles?s set.
3.1 Replication Attempts
This research emerged from a project run-
ning a similar experiment for Dutch on Cor-
netto (Vossen et al, 2013). First, an attempt
was made to reproduce the results reported in
Patwardhan and Pedersen (2006) and Peder-
sen (2010) on the English WordNet using their
WordNet::Similarity web-interface.3 Results dif-
fered from those reported in the aforementioned
works, even when using the same versions as
the original, WordNet::Similarity-1.02 and Word-
Net 2.1 (Patwardhan and Pedersen, 2006) and
WordNet::Similarity-2.05 and WordNet 3.0 (Ped-
ersen, 2010), respectively.4
The fact that results of similarity measures on
WordNet can differ even while the same software
and same versions are used indicates that proper-
ties which are not addressed in the literature may
influence the output of similarity measures. We
therefore conducted a range of experiments that,
in addition to searching for the right settings to
replicate results of previous research, address the
following questions:
1) Which properties have an impact on the per-
formance of WordNet similarity measures?
2) How much does the performance of individ-
ual measures vary?
3) How do commonly used measures compare
when the variation of their performance are taken
into account?
3.2 Methodology and first observations
The questions above were addressed in two stages.
In the first stage, Fokkens, who was not involved
in the first replication attempt implemented a
script to calculate similarity measures using Word-
Net::Similarity. This included similarity mea-
sures introduced by Wu and Palmer (1994) (wup),
3Obtained from http://talisker.d.umn.edu/
cgi-bin/similarity/similarity.cgi, Word-
Net::Similarity version 2.05. This web interface has now
moved to http://maraca.d.umn.edu
4WordNet::Similarity were obtained http://
search.cpan.org/dist/WordNet-Similarity/.
1693
Leacock and Chodorow (1998) (lch), Resnik
(1995) (res), Jiang and Conrath (1997) (jcn),
Lin (1998) (lin), Banerjee and Pedersen (2003)
(lesk), Hirst and St-Onge (1998) (hso) and
Patwardhan and Pedersen (2006) (vector and
vpairs) respectively.
Consequently, settings and properties were
changed systematically and shared with Pedersen
who attempted to produce the new results with his
own implementations. First, we made sure that
the script implemented by Fokkens could produce
the same WordNet similarity scores for each in-
dividual word pair as those used to calculate the
ranking on the mc-set by Pedersen (2010). Finally,
the gold standard and exact implementation of the
Spearman ranking coefficient were compared.
Differences in results turned out to be related
to variations in the experimental setup. First,
we made different assumptions on the restriction
of part-of-speech tags (henceforth ?PoS-tag?) con-
sidered in the comparison. Miller and Charles
(1991) do not discuss how they deal with words
with more than one PoS-tag in their study. Ped-
ersen therefore included all senses with any PoS-
tag in his study. The first replication attempt had
restricted PoS-tags to nouns based on the idea
that most items are nouns and subjects would be
primed to primarily think of the noun senses. Both
assumptions are reasonable. Pos-tags were not re-
stricted in the second replication attempt, but be-
cause of a bug in the code only the first identified
PoS-tag (?noun? in all cases) was considered. We
therefore mistakenly assumed that PoS-tag restric-
tions did not matter until we compared individual
scores between Pedersen and the replication at-
tempts.
Second, there are two gold standards for the
Miller and Charles (1991) set: one has the scores
assigned during the original experiment run by
Rubenstein and Goodenough (1965), the other
has the scores assigned during Miller and Charles
(1991)?s own experiment. The ranking correlation
between the two sets is high, but they are not iden-
tical. Again, there is no reason why one gold stan-
dard would be a better choice than the other, but in
order to replicate results, it must be known which
of the two was used. Third, results changed be-
cause of differences in the treatment of ties while
calculating Spearman ?. The influence of the ex-
act gold standard and calculation of Spearman ?
could only be found because Pedersen could pro-
measure Spearman ? Kendall ? ranking
min max min max variation
path based similarity
path 0.70 0.78 0.55 0.62 1-8
wup 0.70 0.79 0.53 0.61 1-6
lch 0.70 0.78 0.55 0.62 1-7
path based information content
res 0.65 0.75 0.26 0.57 4-11
lin 0.49 0.73 0.36 0.53 6-10
jcn 0.46 0.73 0.32 0.55 5, 7-11
path based relatedness
hso 0.73 0.80 0.36 0.41 1-3,5-10
dictionary and corpus based relatedness
vpairs 0.40 0.70 0.26 0.50 7-11
vector 0.48 0.92 0.33 0.76 1,2,4,6-11
lesk 0.66 0.83 -0.02 0.61 1-8,11,12
Table 1: Variation WordNet measures? results
vide the output of the similarity measures he used
to calculate the coefficient. It is unlikely we would
have been able to replicate his results at all with-
out the output of this intermediate step. Finally,
results for lch, lesk and wup changed accord-
ing to measure specific configuration settings such
as including a PoS-tag specific root node or turn-
ing on normalisation.
In the second stage of this research, we ran ex-
periments that systematically manipulate the influ-
ential factors described above. In this experiment,
we included both the mc-set and the complete rg-
set. The implementation of Spearman ? used in
Pedersen (2010) assigned the lowest number in
ranking to ties rather than the mean, resulting in
an unjustified drop in results for scores that lead
to many ties. We therefore experimented with a
different correlation measure, Kendall tau coeffi-
cient (Kendall, 1938, ? ) rather than two versions
of Spearman ?.
3.3 Variation per measure
All measures varied in their performance.
The complete outcome of our experiments
(both the similarity measures assigned to
each pair as well as the output of the ranking
coefficients) are included in the data set pro-
vided at http://github.com/antske/
WordNetSimilarity. Table 1 presents an
overview of the main point we wish to make
through this experiment: the minimal and maxi-
mal results according to both ranking coefficients.
Results for similarity measures varied from 0.06-
0.42 points for Spearman ? and from 0.05-0.60
points for Kendall ? . The last column indi-
cates the variation of performance of a measure
1694
compared to the other measures, where 1 is the
best performing measure and 12 is the worst.5
For instance, path has been best performing
measure, second best, eighth best and all positions
in between, vector has ranked first, second and
fourth, but also occupied all positions from six to
eleven.
In principle, it is to be expected that num-
bers are not exactly the same while evaluating
against a different data set (the mc-set versus the
rg-set), taking a different set of synsets to evalu-
ate on (changing PoS-tag restrictions) or changing
configuration settings that influence the similarity
score. However, a variation of up to 0.44 points
in Spearman ? and 0.60 in Kendall ? 6 leads to
the question of how indicative these results really
are. A more serious problem is the fact that the
comparative performance of individual measure
changes. Which measure performs best depends
on the evaluation set, ranking coefficient, PoS-tag
restrictions and configuration settings. This means
that the answer to the question of which similarity
measure is best to mimic human similarity scores
depends on aspects that are often not even men-
tioned, let alne systematically compared.
3.4 Variation per category
For each influential category of experimental vari-
ation, we compared the variation in Spearman ?
and Kendall ? , while similarity measure and other
influential categories were kept stable. The cat-
egories we varied include WordNet and Word-
Net::Similarity version, the gold standard used to
evaluate, restrictions on PoS-tags, and measure
specific configurations. Table 2 presents the maxi-
mum variation found across measures for each cat-
egory. The last column indicates how often the
ranking of a specific measure changed as the cat-
egory changed, e.g. did the measure ranking third
using specific configurations, PoS-tag restrictions
and a specific gold standard using WordNet 2.1
still rank third when WordNet 3.0 was used in-
stead? The number in parentheses next to the ?dif-
ferent ranks? in the table presents the total num-
ber of scores investigated. Note that this num-
ber changes for each category, because we com-
5Some measures ranked differently as their individual
configuration settings changed. In these cases, the measure
was included in the overall ranking multiple times, which is
why there are more ranking positions than measures.
6Section 3.4 explains why the variation in Kendall is this
extreme and ? is more appropriate for this task.
Variation Maximum difference Different
Spearman ? Kendall ? rank (tot)
WN version 0.44 0.42 223 (252)
gold standard 0.24 0.21 359 (504)
PoS-tag 0.09 0.08 208 (504)
configuration 0.08 0.60 37 (90)
Table 2: Variations per category
pared two WordNet versions (WN version), three
gold standard and PoS-tag restriction variations
and configuration only for the subset of scores
where configuration matters.
There are no definite statements to make as to
which version (Patwardhan and Pedersen (2006)
vs Pedersen (2010)), PoS-tag restriction or con-
figuration gives the best results. Likewise, while
most measures do better on the smaller data set,
some achieve their highest results on the full set.
This is partially due to the fact that ranking coef-
ficients are sensitive to outliers. In several cases
where PoS-tag restrictions led to different results,
only one pair received a different score. For in-
stance, path assigns a relatively high score to
the pair chord-smile when verbs are included, be-
cause the hierarchy of verbs in WordNet is rela-
tively flat. This effect is not observed in wup and
lch which correct for the depth of the hierarchy.
On the other hand, res, lin and jcn score bet-
ter on the same set when verbs are considered, be-
cause they cannot detect any relatedness for the
pair crane-implement when restricted to nouns.
On top of the variations presented above, we no-
tice a discrepancy between the two coefficients.
Kendall ? generally leads to lower coefficiency
scores than Spearman ?. Moreover, they each
give different relative indications: where lesk
achieves its highest Spearman ?, it has an ex-
tremely low Kendall ? of 0.01. Spearman ? uses
the difference in rank as its basis to calculate a cor-
relation, where Kendall ? uses the number of items
with the correct rank. The low Kendall ? for lesk
is the result of three pairs receiving a score that is
too high. Other pairs that get a relatively accurate
score are pushed one place down in rank. Because
only items that receive the exact same rank help to
increase ? , such a shift can result in a drastic drop
in the coefficient. In our opinion, Spearman ? is
therefore preferable over Kendall ? . We included
? , because many authors do not mention the rank-
ing coefficient they use (cf. Budanitsky and Hirst
(2006), Resnik (1995)) and both ? and ? are com-
1695
monly used coefficients.
Except for WordNet, which Budanitsky and
Hirst (2006) hold accountable for minor variations
in a footnote, the influential categories we investi-
gated in this paper, to our knowledge, have not yet
been addressed in the literature. Cramer (2008)
points out that results from WordNet-Human sim-
ilarity correlations lead to scattered results report-
ing variations similar to ours, but she compares
studies using different measures, data and exper-
imental setup. This study shows that even if
the main properties are kept stable, results vary
enough to change the identity of the measure that
yields the best performance. Table 1 reveals a
wide variation in ranking relative to alternative ap-
proaches. Results in Table 2 show that it is com-
mon for the ranking of a score to change due to
variations that are not at the core of the method.
This study shows that it is far from clear how
different WordNet similarity measures relate to
each other. In fact, we do not know how we can
obtain the best results. This is particularly chal-
lenging, because the ?best results? may depend on
the intended use of the similarity scores (Meng
et al, 2013). This is also the reason why we
presented the maximum variation observed, rather
than the average or typical variation (mostly be-
low 0.10 points). The experiments presented in
this paper resulted in a vast amount of data. An
elaborate analysis of this data is needed to get a
better understanding of how measures work and
why results vary to such an extent. We leave this
investigation to future work. If there is one take-
home message from this experiment, it is that one
should experiment with parameters such as restric-
tions on PoS-tags or configurations and determine
which score to use depending on what it is used
for, rather than picking something that did best in
a study using different data for a different task and
may have used a different version of WordNet.
4 Reproducing a NER method
Freire et al (2012) describe an approach to clas-
sifying named entities in the cultural heritage do-
main. The approach is based on the assumption
that domain knowledge, encoded in complex fea-
tures, can aid a machine learning algorithm in
NER tasks when only little training data is avail-
able. These features include information about
person and organisation names, locations, as well
as PoS-tags. Additionally, some general features
are used such as a window of three preceding and
two following tokens, token length and capitalisa-
tion information. Experiments are run in a 10-fold
cross-validation setup using an open source ma-
chine learning toolkit (McCallum, 2002).
4.1 Reproducing NER Experiments
This experiment can be seen as a real-world case
of the sad tale of the Zigglebottom tagger (Peder-
sen, 2008). The (fictional) Zigglebottom tagger is
a tagger with spectacular results that looks like it
will solve some major problems in your system.
However, the code is not available and a new im-
plementation does not yield the same results. The
original authors cannot provide the necessary de-
tails to reproduce their results, because most of the
work has been done by a PhD student who has fin-
ished and moved on to something else. In the end,
the newly implemented Zigglebottom tagger is not
used, because it does not lead to the promised bet-
ter results and all effort went to waste.
Van Erp was interested in the NER approach
presented in Freire et al (2012). Unfortunately,
the code could not be made available, so she de-
cided to reimplement the approach. Despite feed-
back from Freire about particular details of the
system, results remained 20 points below those
reported in Freire et al (2012) in overall F-score
(Van Erp and Van der Meij, 2013).
The reimplementation process involved choices
about seemingly small details such as rounding
to how many decimals, how to tokenise or how
much data cleanup to perform (normalisation of
non-alphanumeric characters for example). Try-
ing different parameter combinations for feature
generation and the algorithm never yielded the ex-
act same results as Freire et al (2012). The results
of the best run in our first reproduction attempt,
together with the original results from Freire et al
(2012) are presented in Table 3. Van Erp and Van
der Meij (2013) provide an overview of the imple-
mentation efforts.
4.2 Following up from reproduction
Since the experiments in Van Erp and Van der Meij
(2013) introduce several new research questions
regarding the influence of data cleaning and the
limitations of the dataset, we performed some ad-
ditional experiments.
First, we varied the tokenisation, removing non-
alphanumeric characters from the data set. This
yielded a significantly smaller data set (10,442
1696
(Freire et al, 2012) results Van Erp and Van der Meij?s replication results
Precision Recall F?=1 Precision Recall F?=1
LOC (388) 92% 55% 69 77.80% 39.18% 52.05
ORG (157) 90% 57% 70 65.75% 30.57% 41.74
PER (614) 91% 56% 69 73.33% 37.62% 49.73
Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45
Table 3: Precision, recall and F?=1 scores for the original experiments from Freire et al 2012 and our
replication of their approach as presented in Van Erp and Van der Meij (2013)
tokens vs 12,510), and a 15 point drop in over-
all F-score. Then, we investigated whether vari-
ation in the cross-validation splits made any dif-
ference as we noticed that some NEs were only
present in particular fields in the data, which can
have a significant impact on a small dataset. We
inspected the difference between different cross-
validation folds by computing the standard devi-
ations of the scores and found deviations of up
to 25 points in F-score between the 10 splits. In
the general setup, database records were randomly
distributed over the folds and cut off to balance the
fold sizes. In a different approach to dividing the
data by distributing individual sentences from the
records over the folds, performance increases by
8.57 points in overall F-score to 58.02. This is not
what was done in the original Freire et al (2012)
paper, but shows that the results obtained with this
dataset are quite fragile.
As we worried about the complexity of the fea-
ture set relative to the size of the data set, we de-
viated somewhat from Freire et al (2012)?s exper-
iments in that we switched some features on and
off. Removal of complex features pertaining to the
window around the focus token improved our re-
sults by 3.84 points in overall F-score to 53.39.
The complex features based on VIAF,7 GeoN-
ames8 and WordNet do contribute to the classifica-
tion in the Mallet setup as removing them and only
using the focus token, window and generic fea-
tures causes a slight drop in overall F-score from
49.45 to 47.25.
When training the Stanford NER system (Finkel
et al, 2005) on just the tokens from the
Freire data set and the parameters from en-
glish.all.3class.distsim.prop (included in the Stan-
ford NER release, see also Van Erp and Van der
Meij (2013)), our F-scores come very close to
those reported by Freire et al (2012), but mostly
with a higher recall and lower precision. It is puz-
zling that the Stanford system obtains such high
7http://www.viaf.org
8http://www.geonames.org
results with only very simple features, whereas
for Mallet the complex features show improve-
ment over simpler features. This leads to ques-
tions about the differences between the CRF im-
plementations and the influence of their parame-
ters, which we hope to investigate in future work.
4.3 Reproduction difficulties explained
Several reasons may be the cause of why we fail to
reproduce results. As mentioned, not all resources
and data were available for this experiment, thus
causing us to navigate in the dark as we could not
reverse-engineer intermediate steps, but only com-
pare to the final precision, recall and F-scores.
The experiments follow a general machine
learning setup consisting roughly of four steps:
preprocess data, generate features, train model and
test model. The novelty and replication problems
lie in the first three steps. How the data was pre-
processed is a major factor here. The data set con-
sisted of XML files marked up with inline named
entity tags. In order to generate machine learn-
ing features, this data has to be tokenised, possi-
bly cleaned up and the named entity markup had
to be converted to a token-based scheme. Each of
these steps can be carried out in several ways, and
choices made here can have great influence on the
rest of the pipeline.
Similar choices have to be made for prepro-
cessing external resources. From the descriptions
in the original paper, it is unclear how records
in VIAF and GeoNames were preprocessed, or
even which versions of these resources were used.
Preprocessing and calculating occurrence statis-
tics over VIAF takes 30 hours for each run. It
is thus not feasible to identify the main potential
variations without the original data to verify this
prepatory step.
Numbers had to be rounded when generating
the features, leading to the question of how many
decimals are required to be discriminative with-
out creating an overly sparse dataset. Freire recalls
that encoding features as multi-value discrete fea-
1697
tures versus several boolean features can have sig-
nificant impact. These settings are not mentioned
in the paper, making reproduction very difficult.
As the project in which the original research
was performed has ended, and there is no cen-
tral repository where such information can be re-
trieved, we are left to wonder how to reuse this
approach in order to further domain-specific NER.
5 Observations
In this section, we generalise the observations
from our use cases to the main categories that can
influence reproduction.
Despite our efforts to describe our systems as
clearly as possible, details that can make a tremen-
dous difference are often omitted in papers. It will
be no surprise to researchers in the field that pre-
processing of data can make or break an experi-
ment.
The choice of which steps we perform, and how
each of these steps is carried out exactly are part
of our experimental setup. A major difference in
the results for the NER experiments was caused by
variations in the way in which we split the data for
cross-validation.
As we fine-tune our techniques, software gets
updated, data sets are extended or annotation bugs
are fixed. In the WordNet experiment, we found
that there were two different gold standard data
sets. There are also different versions of Word-
Net, and the WordNet::Similarity packages. Sim-
ilarly for the NER experiment, GeoNames, VIAF
and Mallet are updated regularly. It is therefore
critical to pay attention to versioning.
Our experiments often consist of several differ-
ent steps whose outputs may be difficult to retrace.
In order to check the output of a reproduction ex-
periment at every step of the way, system out-
put of experiments, including intermediate steps,
is vital. The WordNet replication was only pos-
sible, because Pedersen could provide the similar-
ity scores of each word pair. This enabled us to
compare the intermediate output and identify the
source of differences in output.
Lastly, there may be inherent system variations
in the techniques used. Machine learning algo-
rithms may for instance use coin flips in case of
a tie. This was not observed in our experiments,
but such variations may be determined by running
an experiment several times and taking the average
over the different runs (cf. Raeder et al (2010)).
All together, these observations show that shar-
ing data and software play a key role in gaining in-
sight into how our methods work. Vanschoren et
al. (2012) propose a setup that allows researchers
to provide their full experimental setup, which
should include exact steps followed in preprocess-
ing the data, documentation of the experimen-
tal setup, exact versions of the software and re-
sources used and experimental output. Having
access to such a setup allows other researchers
to validate research, but also tweak the approach
to investigate system variation, systematically test
the approach in order to learn its limitations and
strengths and ultimately improve on it.
6 Discussion
Many of the aspects addressed in the previous sec-
tion such as preprocessing are typically only men-
tioned in passing, or not at all. There is often not
enough space to capture all details, and they are
generally not the core of the research described.
Still, our use cases have shown that they can have a
tremendous impact on reproduction, and can even
lead to different conclusions. This leads to serious
questions on how we can interpret our results and
how we can compare the performance of different
methods. Is an improvement of a few per cent re-
ally due to the novelty of the approach if larger
variations are found when the data is split differ-
ently? Is a method that does not quite achieve the
highest reported state-of-the-art result truly less
good? What does a state-of-the-art result mean if
it is only tested on one data set?
If one really wants to know whether a result
is better or worse than the state-of-the-art, the
range of variation within the state-of-the-art must
be known. Systematic experiments such as the
ones we carried out for WordNet similarity and
NER, can help determine this range. For results
that fall within the range, it holds that they can
only be judged by evaluations going beyond com-
paring performance numbers, i.e. an evaluation of
how the approach achieves a given result and how
that relates to alternative approaches.
Naturally, our use cases do not represent the en-
tire gamut of research methodologies and prob-
lems in the NLP community. However, they do
represent two core technologies and our observa-
tions align with previous literature on replication
and reproduction.
Despite the systematic variation we employed
1698
in our experiments, they do not answer all ques-
tions that the problems in reproduction evoked.
For the WordNet experiments, deeper analysis is
required to gain full understanding of how indi-
vidual influential aspects interact with each mea-
surement. For the NER experiments, we are yet to
identify the cause of our failure to reproduce.
The considerable time investment required for
such experiments forms a challenge. Due to pres-
sure to publish or other time limitations, they can-
not be carried out for each evaluation. There-
fore, it is important to share our experiments, so
that other researchers (or students) can take this
up. This could be stimulated by instituting repro-
duction tracks in conferences, thus rewarding sys-
tematic investigation of research approaches. It
can also be aided by adopting initiatives that en-
able authors to easily include data, code and/or
workflows with their publications such as the
PLOS/figshare collaboration.9 We already do a
similar thing for our research problems by organ-
ising challenges or shared tasks, why not extend
this to systematic testing of our approaches?
7 Conclusion
We have presented two reproduction use cases for
the NLP domain. We show that repeating other
researchers? experiments can lead to new research
questions and provide new insights into and better
understanding of the investigated techniques.
Our WordNet experiments show that the perfor-
mance of similarity measures can be influenced by
the PoS-tags considered, measure specific varia-
tions, the rank coefficient and the gold standard
used for comparison. We not only find that such
variations lead to different numbers, but also dif-
ferent rankings of the individual measures, i.e.
these aspects lead to a different answer to the
question as to which measure performs best. We
did not succeed in reproducing the NER results
of Freire et al (2012), showing the complexity
of what seems a straightforward reproduction case
based on a system description and training data
only. Our analyses show that it is still an open
question whether additional complex features im-
prove domain specific NER and that this may par-
tially depend on the CRF implementation.
Some observations go beyond our use cases. In
particular, the fact that results vary significantly
9http://blogs.plos.org/plos/2013/01/
easier-access-to-plos-data/
because of details that are not made explicit in
our publications. Systematic testing can provide
an indication of this variation. We have classi-
fied relevant aspects in five categories occurring
across subdisciplines of NLP: preprocessing, ex-
perimental setup, versioning, system output,
and system variation.
We believe that knowing the influence of differ-
ent aspects in our experimental workflow can help
increase our understanding of the robustness of
the approach at hand and will help understand the
meaning of the state-of-the-art better. Some tech-
niques are reused so often (the papers introducing
WordNet similarity measures have around 1,000-
2,000 citations each as of February 2013, for ex-
ample) that knowing their strengths and weak-
nesses is essential for optimising their use.
As mentioned many times before, sharing is key
to facilitating reuse, even if the code is imper-
fect and contains hacks and possibly bugs. In the
end, the same holds for software as for documen-
tation: it is like sex: if it is good, it is very good
and if it is bad, it is better than nothing!10 But
most of all: when reproduction fails, regardless of
whether original code or a reimplementation was
used, valuable insights can emerge from investi-
gating the cause of this failure. So don?t let your
failing reimplementations of the Zigglebottom tag-
ger collect dusk on a shelf while others reimple-
ment their own failing Zigglebottoms. As a com-
munity, we need to know where our approaches
fail, as much ?if not more? as where they succeed.
Acknowledgments
We would like to thank the anonymous review-
ers for their eye to detail and useful comments
to make this a better paper. We furthermore
thank Ruben Izquierdo, Lourens van der Meij,
Christoph Zwirello, Rebecca Dridan and the Se-
mantic Web Group at VU University for their
help and useful feedback. The research leading to
this paper was supported by the European Union?s
7th Framework Programme via the NewsReader
Project (ICT-316404), the Agora project, by NWO
CATCH programme, grant 640.004.801, and the
BiographyNed project, a joint project with Huy-
gens/ING Institute of the Dutch Academy of Sci-
ences funded by the Netherlands eScience Center
(http://esciencecenter.nl/).
10The documentation variant of this quote is attributed to
Dick Brandon.
1699
References
Stanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805?
810, Acapulco, August.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Tomasz Buchert and Lucas Nussbaum. 2012. Lever-
aging business workflows in distributed systems re-
search for the orchestration of reproducible and scal-
able experiments. In Anne Etien, editor, 9e`me
e?dition de la confe?rence MAnifestation des JE-
unes Chercheurs en Sciences et Technologies de
l?Information et de la Communication - MajecSTIC
2012 (2012).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Phd dissertation,
University of Pennsylvania.
Irene Cramer. 2008. How well do semantic related-
ness measures perform? a meta-study. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1, pages 59?70.
Olivier Dalle. 2012. On reproducibility and trace-
ability of simulations. In WSC-Winter Simulation
Conference-2012.
Chris Drummond. 2009. Replicability is not repro-
ducibility: nor is it good science. In Proceedings of
the Twenty-Sixth International Conference on Ma-
chine Learning: Workshop on Evaluation Methods
for Machine Learning IV.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370, Ann Arbor, USA.
Nuno Freire, Jose? Borbinha, and Pa?vel Calado. 2012.
An approach for named entity recognition in poorly
structured data. In Proceedings of ESWC 2012.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detection
and correction of malapropisms. In C. Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages
305?332. MIT Press.
James Howison and James D. Herbsleb. 2013. Shar-
ing the spoils: incentives and collaboration in sci-
entific software development. In Proceedings of the
2013 conference on Computer Supported Coopera-
tive Work, pages 459?470.
Darrel C. Ince, Leslie Hatton, and John Graham-
Cumming. 2012. The case for open computer pro-
grams. Nature, 482(7386):485?488.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING X), pages 19?33, Taiwan.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30(1-2):81?93.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In C. Fellbaum, edi-
tor, WordNet: An electronic lexical database, pages
265?283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, Madison, USA.
Panos Louridas and Georgios Gousios. 2012. A note
on rigour and replicability. SIGSOFT Softw. Eng.
Notes, 37(5):1?4.
Andrew K. McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Thilo Mende. 2010. Replication of defect prediction
studies: problems, pitfalls and recommendations. In
Proceedings of the 6th International Conference on
Predictive Models in Software Engineering. ACM.
Lingling Meng, Runqing Huang, and Junzhong Gu.
2013. A review of semantic similarity measures in
wordnet. International Journal of Hybrid Informa-
tion Technology, 6(1):1?12.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Cameron Neylon, Jan Aerts, C Titus Brown, Si-
mon J Coles, Les Hatton, Daniel Lemire, K Jar-
rod Millman, Peter Murray-Rust, Fernando Perez,
Neil Saunders, Nigam Shah, Arfon Smith, Gae?l
Varoquaux, and Egon Willighagen. 2012. Chang-
ing computational research. the challenges ahead.
Source Code for Biology and Medicine, 7(2).
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing wordnet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 1?8, Trento, Italy.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465?470.
1700
Ted Pedersen. 2010. Information content measures
of semantic similarity perform better without sense-
tagged text. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2010), pages 329?332, Los Angeles, USA.
Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla.
2010. Consequences of variability in classifier per-
formance estimates. In Proceedings of ICDM?2010.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448?453,
Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Wheeler Ruml. 2010. The logic of benchmarking: A
case against state-of-the-art performance. In Pro-
ceedings of the Third Annual Symposium on Combi-
natorial Search (SOCS-10).
Charles Spearman. 1904. Proof and measurement of
association between two things. American Journal
of Psychology, 15:72?101.
Marieke Van Erp and Lourens Van der Meij. 2013.
Reusable research? a case study in named entity
recognition. CLTL 2013-01, Computational Lexi-
cology & Terminology Lab, VU University Amster-
dam.
Joaquin Vanschoren, Hendrik Blockeel, Bernhard
Pfahringer, and Geoffrey Holmes. 2012. Experi-
ment databases. Machine Learning, 87(2):127?158.
Piek Vossen, Isa Maks, Roxane Segers, Hennie van der
Vliet, Marie-Francine Moens, Katja Hofmann, Erik
Tjong Kim Sang, and Maarten de Rijke. 2013. Cor-
netto: a Combinatorial Lexical Semantic Database
for Dutch. In Peter Spyns and Jan Odijk, editors, Es-
sential Speech and Language Technology for Dutch
Results by the STEVIN-programme, number XVII in
Theory and Applications of Natural Language Pro-
cessing, chapter 10. Springer.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, Las Cruces,
USA.
1701
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75?80,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
{e.agirre,oier.lopezdelacalle}@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Shu-Kai Hsieh
Department of English
National Taiwan Normal University
Taipei, Taiwan
shukai@ntnu.edu.tw
Maurizio Tesconi
IIT
CNR
Pisa, Italy
maurizio.tesconi@iit.cnr.it
Monica Monachini
ILC
CNR
Pisa, Italy
monica.monachini@ilc.cnr.it
Piek Vossen, Roxanne Segers
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl,roxane.segers@gmail.com
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambigua-
tion systems present new challenges. The
difficulties found by supervised systems to
adapt might change the way we assess the
strengths and weaknesses of supervised
and knowledge-based WSD systems. Un-
fortunately, all existing evaluation datasets
for specific domains are lexical-sample
corpora. This task presented all-words
datasets on the environment domain for
WSD in four languages (Chinese, Dutch,
English, Italian). 11 teams participated,
with supervised and knowledge-based sys-
tems, mainly in the English dataset. The
results show that in all languages the par-
ticipants where able to beat the most fre-
quent sense heuristic as estimated from
general corpora. The most successful ap-
proaches used some sort of supervision in
the form of hand-tagged examples from
the domain.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in previous Senseval and SemEval competitions
(Kilgarriff, 2001; Mihalcea et al, 2004; Snyder
and Palmer, 2004; Pradhan et al, 2007). Spe-
cific domains pose fresh challenges to WSD sys-
tems: the context in which the senses occur might
change, different domains involve different sense
distributions and predominant senses, some words
tend to occur in fewer senses in specific domains,
the context of the senses might change, and new
senses and terms might be involved. Both super-
vised and knowledge-based systems are affected
by these issues: while the first suffer from differ-
ent context and sense priors, the later suffer from
lack of coverage of domain-related words and in-
formation.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain. All
datasets and related information are publicly avail-
able from the task websites
1
.
This task was designed in the context of Ky-
oto (Vossen et al, 2008)
2
, an Asian-European
project that develops a community platform for
modeling knowledge and finding facts across lan-
guages and cultures. The platform operates as a
Wiki system with an ontological support that so-
cial communities can use to agree on the mean-
ing of terms in specific domains of their interest.
Kyoto focuses on the environmental domain be-
cause it poses interesting challenges for informa-
tion sharing, but the techniques and platforms are
1
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
2
http://www.kyoto-project.eu/
75
independent of the application domain.
The paper is structured as follows. We first
present the preparation of the data. Section 3 re-
views participant systems and Section 4 the re-
sults. Finally, Section 5 presents the conclusions.
2 Data preparation
The data made available to the participants in-
cluded the test set proper, and background texts.
Participants had one week to work on the test set,
but the background texts where provided months
earlier.
2.1 Test datasets
The WSD-domain comprises comparable all-
words test corpora on the environment domain.
Three texts were compiled for each language by
the European Center for Nature Conservation
3
and
Worldwide Wildlife Forum
4
. They are documents
written for a general but interested public and in-
volve specific terms from the domain. The docu-
ment content is comparable across languages. Ta-
ble 1 shows the numbers for the datasets.
Although the original plan was to annotate mul-
tiword terms, and domain terminology, due to time
constraints we focused on single-word nouns and
verbs. The test set clearly marked which were
the words to be annotated. In the case of Dutch,
we also marked components of single-word com-
pounds. The format of the test set followed that of
previous all-word exercises, which we extended to
accommodate Dutch compounds. For further de-
tails check the datasets in the task website.
The sense inventory was based on publicly
available wordnets of the respective languages
(see task website for details). The annotation pro-
cedure involved double-blind annotation by ex-
perts plus adjudication, which allowed us to also
provide Inter Annotator Agreement (IAA) figures
for the dataset. The procedure was carried out us-
ing KAFnotator tool (Tesconi et al, 2010). Due
to limitations in resources and time, the English
dataset was annotated by a single expert annota-
tor. For the rest of languages, the agreement was
very good, as reported in Table 1.
Table 1 includes the results of the random base-
line, as an indication of the polysemy in each
dataset. Average polysemy is highest for English,
and lowest for Dutch.
3
http://www.ecnc.org
4
http://www.wwf.org
Total Noun Verb IAA Random
Chinese 3989 754 450 0.96 0.321
Dutch 8157 997 635 0.90 0.328
English 5342 1032 366 n/a 0.232
Italian 8560 1340 513 0.72 0.294
Table 1: Dataset numbers, including number of
tokens, nouns and verbs to be tagged, Inter-
Annotator Agreement (IAA) and precision of ran-
dom baseline.
Documents Words
Chinese 58 455359
Dutch 98 21089
English 113 2737202
Italian 27 240158
Table 2: Size of the background data.
2.2 Background data
In addition to the test datasets proper, we also pro-
vided additional documents on related subjects,
kindly provided by ECNC and WWF. Table 2
shows the number of documents and words made
available for each language. The full list with the
urls of the documents are available from the task
website, together with the background documents.
3 Participants
Eleven participants submitted more than thirty
runs (cf. Table 3). The authors classified their runs
into supervised (S in the tables, three runs), weakly
supervised (WS, four runs), unsupervised (no runs)
and knowledge-based (KB, the rest of runs)
5
. Only
one group used hand-tagged data from the domain,
which they produced on their own. We will briefly
review each of the participant groups, ordered fol-
lowing the rank obtained for English. They all par-
ticipated on the English task, with one exception
as noted below, so we report their rank in the En-
glish task. Please refer to their respective paper in
these proceedings for more details.
CFILT: They participated with a domain-
specific knowledge-based method based on Hop-
field networks (Khapra et al, 2010). They first
identify domain-dependant words using the back-
ground texts, use a graph based on hyponyms in
WordNet, and a breadth-first search to select the
most representative synsets within domain. In ad-
dition they added manually disambiguated around
one hundred examples from the domain as seeds.
5
Note that boundaries are slippery. We show the classifi-
cations as reported by the authors.
76
English
Rank Participant System ID Type P R R nouns R verbs
1 Anup Kulkarni CFILT-2 WS 0.570 0.555 ?0.024 0.594 ?0.028 0.445 ?0.047
2 Anup Kulkarni CFILT-1 WS 0.554 0.540 ?0.021 0.580 ?0.025 0.426 ?0.043
3 Siva Reddy IIITH1-d.l.ppr.05 WS 0.534 0.528 ?0.027 0.553 ?0.023 0.456 ?0.041
4 Abhilash Inumella IIITH2-d.r.l.ppr.05 WS 0.522 0.516 ?0.023 0.529 ?0.027 0.478 ?0.041
5 Ruben Izquierdo BLC20SemcorBackground S 0.513 0.513 ?0.022 0.534 ?0.026 0.454 ?0.044
- - Most Frequent Sense - 0.505 0.505 ?0.023 0.519 ?0.026 0.464 ?0.043
6 Ruben Izquierdo BLC20Semcor S 0.505 0.505 ?0.025 0.527 ?0.031 0.443 ?0.045
7 Anup Kulkarni CFILT-3 KB 0.512 0.495 ?0.023 0.516 ?0.027 0.434 ?0.048
8 Andrew Tran Treematch KB 0.506 0.493 ?0.021 0.516 ?0.028 0.426 ?0.046
9 Andrew Tran Treematch-2 KB 0.504 0.491 ?0.021 0.515 ?0.030 0.425 ?0.044
10 Aitor Soroa kyoto-2 KB 0.481 0.481 ?0.022 0.487 ?0.025 0.462 ?0.039
11 Andrew Tran Treematch-3 KB 0.492 0.479 ?0.022 0.494 ?0.028 0.434 ?0.039
12 Radu Ion RACAI-MFS KB 0.461 0.460 ?0.022 0.458 ?0.025 0.464 ?0.046
13 Hansen A. Schwartz UCF-WS KB 0.447 0.441 ?0.022 0.440 ?0.025 0.445 ?0.043
14 Yuhang Guo HIT-CIR-DMFS-1.ans KB 0.436 0.435 ?0.023 0.428 ?0.027 0.454 ?0.043
15 Hansen A. Schwartz UCF-WS-domain KB 0.440 0.434 ?0.024 0.434 ?0.029 0.434 ?0.044
16 Abhilash Inumella IIITH2-d.r.l.baseline.05 KB 0.496 0.433 ?0.024 0.452 ?0.023 0.390 ?0.044
17 Siva Reddy IIITH1-d.l.baseline.05 KB 0.498 0.432 ?0.021 0.463 ?0.026 0.344 ?0.038
18 Radu Ion RACAI-2MFS KB 0.433 0.431 ?0.022 0.434 ?0.027 0.399 ?0.049
19 Siva Reddy IIITH1-d.l.ppv.05 KB 0.426 0.425 ?0.026 0.434 ?0.028 0.399 ?0.043
20 Abhilash Inumella IIITH2-d.r.l.ppv.05 KB 0.424 0.422 ?0.023 0.456 ?0.025 0.325 ?0.044
21 Hansen A. Schwartz UCF-WS-domain.noPropers KB 0.437 0.392 ?0.025 0.377 ?0.025 0.434 ?0.043
22 Aitor Soroa kyoto-1 KB 0.384 0.384 ?0.022 0.382 ?0.024 0.391 ?0.047
23 Ruben Izquierdo BLC20Background S 0.380 0.380 ?0.022 0.385 ?0.026 0.366 ?0.037
24 Davide Buscaldi NLEL-WSD-PDB WS 0.381 0.356 ?0.022 0.357 ?0.027 0.352 ?0.049
25 Radu Ion RACAI-Lexical-Chains KB 0.351 0.350 ?0.015 0.344 ?0.017 0.368 ?0.030
26 Davide Buscaldi NLEL-WSD WS 0.370 0.345 ?0.022 0.352 ?0.027 0.328 ?0.037
27 Yoan Gutierrez Relevant Semantic Trees KB 0.328 0.322 ?0.022 0.335 ?0.026 0.284 ?0.044
28 Yoan Gutierrez Relevant Semantic Trees-2 KB 0.321 0.315 ?0.022 0.327 ?0.024 0.281 ?0.040
29 Yoan Gutierrez Relevant Cliques KB 0.312 0.303 ?0.021 0.304 ?0.024 0.301 ?0.041
- - Random baseline - 0.232 0.232 0.253 0.172
Chinese
Rank Participant System ID Type P R R nouns R verbs
- - Most Frequent Sense - 0.562 0.562 ?0.026 0.589 ?0.027 0.518 ?0.039
1 Meng-Hsien Shih HR KB 0.559 0.559 ?0.024 0.615 ?0.026 0.464 ?0.039
2 Meng-Hsien Shih GHR KB 0.517 0.517 ?0.024 0.533 ?0.035 0.491 ?0.038
- - Random baseline - 0.321 0.321 0.326 0.312
4 Aitor Soroa kyoto-3 KB 0.322 0.296 ?0.022 0.257 ?0.027 0.360 ?0.038
3 Aitor Soroa kyoto-2 KB 0.342 0.285 ?0.021 0.251 ?0.026 0.342 ?0.040
5 Aitor Soroa kyoto-1 KB 0.310 0.258 ?0.023 0.256 ?0.029 0.261 ?0.031
Dutch
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.526 0.526 ?0.022 0.575 ?0.029 0.450 ?0.034
2 Aitor Soroa kyoto-2 KB 0.519 0.519 ?0.022 0.561 ?0.027 0.454 ?0.034
- - Most Frequent Sense - 0.480 0.480 ?0.022 0.600 ?0.027 0.291 ?0.025
3 Aitor Soroa kyoto-1 KB 0.465 0.465 ?0.021 0.505 ?0.026 0.403 ?0.033
- - Random baseline - 0.328 0.328 0.350 0.293
Italian
Rank Participant System ID Type P R R nouns R verbs
1 Aitor Soroa kyoto-3 KB 0.529 0.529 ?0.021 0.530 ?0.024 0.528 ?0.038
2 Aitor Soroa kyoto-2 KB 0.521 0.521 ?0.018 0.522 ?0.023 0.519 ?0.035
3 Aitor Soroa kyoto-1 KB 0.496 0.496 ?0.019 0.507 ?0.020 0.468 ?0.037
- - Most Frequent Sense - 0.462 0.462 ?0.020 0.472 ?0.024 0.437 ?0.035
- - Random baseline - 0.294 0.294 0.308 0.257
Table 3: Overall results for the domain WSD datasets, ordered by recall.
This is the only group using hand-tagged data
from the target domain. Their best run ranked 1st.
IIITTH: They presented a personalized PageR-
ank algorithm over a graph constructed from
WordNet similar to (Agirre and Soroa, 2009),
with two variants. In the first (IIITH1), the vertices
of the graph are initialized following the rank-
ing scores obtained from predominant senses as in
(McCarthy et al, 2007). In the second (IIITH2),
the graph is initialized with keyness values as in
77
0.3 0.35 0.4 0.45 0.5 0.55
Rel. Cliques
Rel. Sem. Trees-2
Rel. Sem. Trees
NLEL-WSD
RACAI-Lexical-Chains
NLEL-WSD-PDB
BLC20BG
Kyoto-1
UCF-WS-domain.noPropers
IIITH2-d.r.l.ppv.05
IIITH1-d.l.ppv.05
RACAI-2MFS-BOW
IIITH1-d.l.baseline.05
IIITH2-d.r.l.baseline.05
UCF-WS-domain
HIT-CIR-DMFS
UCF-WS
RACAI-MFS
Treematch-3
Kyoto-2
Treematch-2
Treematch
CFILT-3
BLC20SC
BLC20SCBG
IIITH2-d.l.ppr.05
IIITH1-d.l.ppr.05
CFILT-1
CFILT-2
MFS
Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond
to one system (denoted in axis Y) according each recall and confidence interval (axis X ). Systems are
ordered depending on their rank.
(Rayson and Garside, 2000). Some of the runs
use sense statistics from SemCor, and have been
classified as weakly supervised. They submitted a
total of six runs, with the best run ranking 3rd.
BLC20(SC/BG/SCBG): This system is super-
vised. A Support Vector Machine was trained us-
ing the usual set of features extracted from con-
text and the most frequent class of the target word.
Semantic class-based classifiers were built from
SemCor (Izquierdo et al, 2009), where the classes
were automatically obtained exploiting the struc-
tural properties of WordNet. Their best run ranked
5th.
Treematch: This system uses a knowledge-
based disambiguation method that requires a dic-
tionary and untagged text as input. A previously
developed system (Chen et al, 2009) was adapted
to handle domain specific WSD. They built a
domain-specific corpus using words mined from
relevant web sites (e.g. WWF and ECNC) as
seeds. Once parsed the corpus, the used the de-
pendency knowledge to build a nodeset that was
used for WSD. The background documents pro-
vided by the organizers were only used to test how
exhaustive the initial seeds were. Their best run
ranked 8th.
Kyoto: This system participated in all four
languages, with a free reimplementation of
the domain-specific knowledge-based method for
WSD presented in (Agirre et al, 2009). It
uses a module to construct a distributional the-
saurus, which was run on the background text, and
a disambiguation module based on Personalized
PageRank over wordnet graphs. Different Word-
Net were used as the LKB depending on the lan-
guage. Their best run ranked 10th. Note that this
team includes some of the organizers of the task.
A strict separation was kept, in order to keep the
test dataset hidden from the actual developers of
the system.
RACAI: This participant submitted three differ-
ent knowledge-based systems. In the first, they use
the mapping to domains of WordNet (version 2.0)
in order to constraint the domains of the content
words of the test text. In the second, they choose
among senses using lexical chains (Ion and Ste-
fanescu, 2009). The third system combines the
previous two. Their best system ranked 12th.
HIT-CIR: They presented a knowledge-based
system which estimates predominant sense from
raw test. The predominant senses were calculated
with the frequency information in the provided
background text, and automatically constructed
78
thesauri from bilingual parallel corpora. The sys-
tem ranked 14.
UCFWS: This knowledge-based WSD system
was based on an algorithm originally described in
(Schwartz and Gomez, 2008), in which selectors
are acquired from the Web via searching with lo-
cal context of a given word. The sense is cho-
sen based on the similarity or relatedness between
the senses of the target word and various types
of selectors. In some runs they include predom-
inant senses(McCarthy et al, 2007). The best run
ranked 13th.
NLEL-WSD(-PDB): The system used for the
participation is based on an ensemble of different
methods using fuzzy-Borda voting. A similar sys-
tem was proposed in SemEval-2007 task-7 (Bus-
caldi and Rosso, 2007). In this case, the com-
ponent method used where the following ones:
1) Most Frequent Sense from SemCor; 2) Con-
ceptual Density ; 3) Supervised Domain Relative
Entropy classifier based on WordNet Domains;
4) Supervised Bayesian classifier based on Word-
Net Domains probabilities; and 5) Unsupervised
Knownet-20 classifiers. The best run ranked 24th.
UMCC-DLSI (Relevant): The team submitted
three different runs using a knowledge-based sys-
tem. The first two runs use domain vectors and
the third is based on cliques, which measure how
much a concept is correlated to the sentence by
obtaining Relevant Semantic Trees. Their best run
ranked 27th.
(G)HR: They presented a Knowledge-based
WSD system, which make use of two heuristic
rules (Li et al, 1995). The system enriched the
Chinese WordNet by adding semantic relations for
English domain specific words (e.g. ecology, en-
vironment). When in-domain senses are not avail-
able, the system relies on the first sense in the Chi-
nese WordNet. In addition, they also use sense
definitions. They only participated in the Chinese
task, with their best system ranking 1st.
4 Results
The evaluation has been carried out using the stan-
dard Senseval/SemEval scorer scorer2 as in-
cluded in the trial dataset, which computes preci-
sion and recall. Table 3 shows the results in each
dataset. Note that the main evaluation measure is
recall (R). In addition we also report precision (P)
and the recall for nouns and verbs. Recall mea-
sures are accompanied by a 95% confidence in-
terval calculated using bootstrap resampling pro-
cedure (Noreen, 1989). The difference between
two systems is deemed to be statistically signifi-
cant if there is no overlap between the confidence
intervals. We show graphically the results in Fig-
ure 1. For instance, the differences between the
highest scoring system and the following four sys-
tems are not statistically significant. Note that this
method of estimating statistical significance might
be more strict than other pairwise methods.
We also include the results of two baselines.
The random baseline was calculated analytically.
The first sense baseline for each language was
taken from each wordnet. The first sense baseline
in English and Chinese corresponds to the most
frequent sense, as estimated from out-of-domain
corpora. In Dutch and Italian, it followed the in-
tuitions of the lexicographer. Note that we don?t
have the most frequent sense baseline from the do-
main texts, which would surely show higher re-
sults (Koeling et al, 2005).
5 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. With
this paper we have motivated the creation of an
all-words test dataset for WSD on the environ-
ment domain in several languages, and presented
the overall design of this SemEval task.
One of the goals of the exercise was to show
that WSD systems could make use of unannotated
background corpora to adapt to the domain and
improve their results. Although it?s early to reach
hard conclusions, the results show that in each of
the datasets, knowledge-based systems are able to
improve their results using background text, and
in two datasets the adaptation of knowledge-based
systems leads to results over the MFS baseline.
The evidence of domain adaptation of supervised
systems is weaker, as only one team tried, and the
differences with respect to MFS are very small.
The best results for English are obtained by a sys-
tem that combines a knowledge-based system with
some targeted hand-tagging. Regarding the tech-
niques used, graph-based methods over WordNet
and distributional thesaurus acquisition methods
have been used by several teams.
79
All datasets and related information are publicly
available from the task websites
6
.
Acknowledgments
We thank the collaboration of Lawrence Jones-Walters, Amor
Torre-Marin (ECNC) and Karin de Boom (WWF), com-
piling the test and background documents. This work
task is partially funded by the European Commission (KY-
OTO ICT-2007-211423), the Spanish Research Department
(KNOW-2 TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings of the
12th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL09), pages 33?
41. Association for Computational Linguistics.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
Davide Buscaldi and Paolo Rosso. 2007. Upv-wsd : Com-
bining different wsd methods by means of fuzzy borda
voting. In Proceedings of the Fourth International Work-
shop on Semantic Evaluations (SemEval-2007), pages
434?437.
P. Chen, W. Ding, and D. Brown. 2009. A fully unsupervised
word sense disambiguation method and its evaluation on
coarse-grained all-words task. In Proceeding of the North
American Chapter of the Association for Computational
Linguistics (NAACL09).
Radu Ion and Dan Stefanescu. 2009. Unsupervised word
sense disambiguation with lexical chains and graph-based
context formalization. In Proceedings of the 4th Language
and Technology Conference: Human Language Technolo-
gies as a Challenge for Computer Science and Linguistics,
pages 190?194.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense dis-
ambiguation. In EACL ?09: Proceedings of the 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics, pages 389?397, Morristown,
NJ, USA. Association for Computational Linguistics.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense dis-
ambiguation combining corpus based and wordnet based
parameters. In Proceedings of the 5th International Con-
ference on Global Wordnet (GWC2010).
A. Kilgarriff. 2001. English Lexical Sample Task Descrip-
tion. In Proceedings of the Second International Work-
shop on evaluating Word Sense Disambiguation Systems,
Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense acqui-
sition. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
6
http://xmlgroup.iit.cnr.it/SemEval2010/
and http://semeval2.fbk.eu/
Natural Language Processing. HLT/EMNLP, pages 419?
426, Ann Arbor, Michigan.
Xiaobin Li, Stan Szpakowicz, and Stan Matwin. 1995. A
wordnet-based algorithm for word sense disambiguation.
In Proceedings of The 14th International Joint Conference
on Artificial Intelligence (IJCAI95).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2007. Unsupervised acquisition of predominant
word senses. Computational Linguistics, 33(4).
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004. The
Senseval-3 English lexical sample task. In Proceedings of
the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Eric W. Noreen. 1989. Computer-Intensive Methods for Test-
ing Hypotheses. John Wiley & Sons.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague, Czech Republic.
Paul Rayson and Roger Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the workshop
on Comparing corpora, pages 1?6.
Hansen A. Schwartz and Fernando Gomez. 2008. Acquir-
ing knowledge from the web to be used as selectors for
noun sense disambiguation. In Proceedings of the Twelfth
Conference on Computational Natural Language Learn-
ing (CONLL08).
B. Snyder and M. Palmer. 2004. The English all-words task.
In Proceedings of the 3rd ACL workshop on the Evalua-
tion of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
M. Tesconi, F. Ronzano, S. Minutoli, C. Aliprandi, and
A. Marchetti. 2010. Kafnotator: a multilingual seman-
tic text annotation tool. In In Proceedings of the Second
International Conference on Global Interoperability for
Language Resources.
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Christiane
Fellbaum, Shu kai Hsieh, Chu-Ren Huang, Hitoshi Isa-
hara, Kyoko Kanzaki, Andrea Marchetti, Monica Mona-
chini, Federico Neri, Remo Raffaelli, German Rigau,
Maurizio Tescon, and Joop VanGent. 2008. Kyoto: a
system for mining, structuring and distributing knowl-
edge across languages and cultures. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
80
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
Kyoto: An Integrated System for Specific Domain WSD
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle
University of the Basque Country
a.soroa@ehu.es
Monica Monachini
Istituto di Linguistica Computazionale
monica.monachini@ilc.cnr.it
Jessie Lo, Shu-Kai Hsieh
National Taiwan Normal University
shukai@ntnu.edu.tw
Wauter Bosma, Piek Vossen
Vrije Universiteit
{p.vossen,w.bosma}@let.vu.nl
Abstract
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
1 Introduction
In this paper we describe the participation of the
integrated Kyoto system on the ?SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain? task (Agirre et al,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project
1
. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al, 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
1
http://www.kyoto-project.eu
results. Finally, the conclusions are presented.
2 The Kyoto System for Domain Specific
WSD
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
2.1 UKB
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V,E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node?s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
417
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
2.2 Tybots
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion
2
. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al, 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
2.3 Lexical Knowledge bases
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al, 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie
3
. Cornetto com-
prises taxonomic relations and equivalence rela-
2
http://kyoto.let.vu.nl/svn/kyoto/trunk
3
http://www.inl.nl/nl/lexica/780
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al, 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public
4
(Tsai et al,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
3 Experimental setting
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
3.1 UKB parameters
We use UKB with the default parameters. In par-
ticular, we don?t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It?s also worth mentioning that in the
default setting parts of speech were not used.
4
http://cwn.ling.sinica.edu.tw
418
RANK RUN P R R-NOUN R-VERB
Chinese
- 1sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- 1sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- 1sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- 1sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
Table 2: Overall results of our runs, including pre-
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
3.2 Run1: UKB using context
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
3.3 Run2: UKB using related words
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al, 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. ?biodiversity, agri-
culture, ecosystem, nature, life, climate, . . .?). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al, 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 10
5
. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
3.4 Run3: UKB using related words and
bilingual graphs
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
4 Results
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
5
In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
419
our previous work on English (Agirre et al,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
?flat? structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don?t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
5 Conclusions
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
Acknowledgments
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33?41. Association for Computational Linguistics.
E. Agirre, O. L?opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
E. Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
?02: Proceedings of the 11th international conference on
WWW, pages 517?526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745?791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485?506.
420
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 1?10,
Beijing, August 2010
KYOTO: an open platform for mining facts
Piek Vossen
VU University Amsterdam
p.vossen@let.vu.nl
German Rigau
Eneko Agirre
Aitor Soroa
University of the Basque 
Country
german.rigau/e.a-
girre/a.soroa@ehu.es
Monica Monachini
Roberto Bartolini
Istituto di Linguistica 
Computazionale, CNR
monica.monachini/r
oberto.bartolin-
i@ilc.cnr.it
Abstract
This  document  describes  an  open 
text-mining  system  that  was  developed 
for the Asian-European project KYOTO. 
The  KYOTO system uses  an  open text 
representation format and a central onto-
logy to  enable  extraction  of  knowledge 
and facts  from large volumes of text  in 
many different languages. We implemen-
ted a semantic tagging approach that per-
forms off-line reasoning. Mining of facts 
and  knowledge  is  achieved  through  a 
flexible pattern matching module that can 
work in much the same way for different 
languages,  can  handle  efficiently  large 
volumes of documents and is not restric-
ted to a specific domain. We applied the 
system to an English database on estuar-
ies.
1 Introduction
Traditionally, Information Extraction (IE) is the 
task of filling template information from previ-
ously unseen text which belongs to a predefined 
domain (Peshkin & Pfeffer 2003). Most systems 
in  the  Message  Understanding  Conferences 
(MUC,  1987-1998)  and the  Automatic  Content 
Extraction  program  (ACE)1 use  a  pipeline  of 
tools to achieve this, ranging from sophisticated 
NLP tools (like deep parsing) to shallower text-
processing (e.g. FASTUS (Appelt 1995)).
Standard  IE  systems  are  based  on  lan-
guage-specific  pattern  matching  (Kaiser  & 
1http://www.itl.nist.gov/iad/mig//tests/ace  
Miksch 2005), where each pattern consists of a 
regular  expression  and  an  associated  mapping 
from syntactic to logical form. In general, the ap-
proaches can be categorized into two groups: (1) 
the Knowledge Engineering approach (Appelt et 
al.1995), and (2) the learning approach, such as 
AutoSlog  (Appelt  et  al.  1993),  SRV  (Freitag 
1998), or RAPIER (Califf & R. Mooney 1999). 
Another  important  system  is  GATE (Cunning-
ham et al2002), which is a platform for creating 
IE systems. It uses regular expressions, but it can 
also  use  ontologies  to  perform semantic  infer-
ences  to  constrain  linguistic  patterns  semantic-
ally. The use of ontologies in IE is an emerging 
field (Bontcheva & Wilks 2004): linking text in-
stances with elements belonging to the ontology, 
instead of consulting flat gazetteers.
The major disadvantage of traditional IE sys-
tems is that they focus on satisfying precise, nar-
row, pre-specified requests from small homogen-
eous corpora (e.g., extract information about ter-
rorist events). Likewise, they are not flexible, are 
limited to specific types of knowledge and need 
to be built by knowledge engineers for each spe-
cific application and language. In fact most text 
mining  systems are  developed for  a  single  do-
main and a single language, and are not able to 
handle  knowledge  expressed  in  different  lan-
guages  or  expressed  and conceptualized  differ-
ently across cultures.
In this paper we describe an open platform for 
text-mining  or  IE that  can  be applied  to many 
different  languages  in  the  same  way  using  an 
open text representation system and a central on-
1
tology that  is  shared across  languages.  Ontolo-
gical implications are inserted in the text through 
off-line  reasoning and ontological  tagging.  The 
events and facts are extracted from large amounts 
of text using a flexible pattern-matching module, 
as specified by profiles  which comprise  ontolo-
gical and shallow linguistic patterns. The system 
is  developed  in  the  Asian-European  project 
KYOTO2.
In the next section,  we describe the general 
architecture of the KYOTO system. In section 3, 
we specify the knowledge structure that is used. 
Section  4,  describes  the  off-line  reasoning  and 
ontological tagging. In section 5, we describe the 
module for mining knowledge from the text that 
is enriched with ontological  statements.  Finally 
in section 6, we describe the first results of ap-
plying the system to databases on Estuaries.
2 KYOTO overview
The  KYOTO  project  allows  communities  to 
model terms and concepts in their domain and to 
use this knowledge to apply text mining on docu-
ments. The knowledge cycle in the KYOTO sys-
tem starts  with a set  of  source  documents pro-
duced by the community, such as PDFs and web-
sites.  Linguistic  processors  apply  tokenization, 
segmentation, morpho-syntactic analysis and  se-
mantic  processing  to  the  text  in  different  lan-
guages. The semantic processing involves the de-
tection of named-entities (persons, organizations, 
places,  time-expressions)  and  determining  the 
meaning of  words  in  the  text  according to  the 
given wordnet.  
The  output  of  the  linguistic  processors is 
stored in an XML annotation format that  is the 
same for  all  the languages,  called  the KYOTO 
Annotation  Format  (KAF,  Bosma  et  al  2009). 
This format incorporates standardized proposals 
for the linguistic annotation of text and represents 
them in an easy-to-use layered structure, which is 
compatible with the Linguistic Annotation Frame-
work  (LAF,  Ide  and  Romary  2003).  In  KAF, 
words, terms, constituents and syntactic depend-
encies  are  stored  in  separate  layers  with  refer-
ences across the structures. This makes it easier 
to harmonize the output of  linguistic processors 
2 Http://www.kyoto-project.eu
for different languages and to add new semantic 
layers to the basic output, when needed (Bosma 
et al 2009, Vossen et al 2010). All modules in 
KYOTO draw their input from these structures. 
In fact, the word-sense disambiguation process is 
carried out to the same KAF annotation in differ-
ent languages and is therefore the same for all the 
languages (Agirre et al 2009). In the current sys-
tem,  there  are  processors  for  English,  Dutch, 
Italian, Spanish, Basque, Chinese and Japanese.
The KYOTO system proceeds in 2 cycles (see 
Figure 1). In the 1st cycle, the Tybot (Term Yield-
ing Robot) extracts the most relevant terms from 
the documents. The Tybot is another generic pro-
gram that  can  do  this  for  all  the  different  lan-
guages in much the same way. The terms are or-
ganized as a structured hierarchy and, wherever 
possible,  related  to  generic  semantic  databases, 
i.e. wordnets for each language. In the left part of 
Figure 1, we show those terms in the input docu-
ment and their classification in wordnet. Terms in 
italics are present in the original wordnet, while 
underlined terms correspond to terms which were 
not in the original wordnet but were automatic-
ally discovered and linked to wordnet by Tybots. 
Straight  terms  correspond  to hyperonyms  in 
wordnet that do not necessarily occur in the text 
but are linked to ontological classes. The result of 
this  1st cycle  is a domain wordnet  for the target 
language.
The 2nd cycle of the system involves the actu-
al extraction of factual knowledge from the docu-
ments by the Kybots  (Knowledge Yielding Ro-
bots). Kybots use a collection of profiles that rep-
resent patterns of information of interest. In the 
profile, conceptual relations are expressed using 
ontological  and morpho-syntactic linguistic pat-
terns. Since the semantics is defined through the 
ontology,  it  is  possible  to  detect  similar  data 
across documents in different languages, even if 
expressed differently. In Figure 1, we give an ex-
ample of a conceptual pattern that relates organ-
isms that live in habitats. The Kybot can combine 
morpho-syntactic and semantic patterns. When a 
match is detected, the instantiation of the pattern 
is saved in a formal representation, either in KAF 
or in RDF. Since the wordnets in different lan-
guages are mapped to the same ontology and the 
text in these languages is represented in the same 
KAF,  similar  patterns  can  easily  be  applied  to 
multiple languages.
2
3 Ontological  and  lexical  background 
knowledge
As a semantic background model, we defined a 
3-layered  knowledge  architecture  following the 
principle  of  the  division  of  labour  (Putnam 
1975). In this model, the ontology does not need 
to be the central hub for all terms in a domain in 
all  languages.  Following the division  of labour 
principle, we can state that a computer does not 
need  to  distinguish  between  instances  of  a 
European Tree Frog and a Glass Tree frog. We 
assume  that  rigid  concepts  (as  defined  by 
Guarino and Welty 2002) are known to the do-
main experts and do not need to be defined form-
ally in the ontology but can remain in the avail-
able  background  resources,  such  as   databases 
with millions of species.  Terms in the documents 
are mostly non-rigid, e.g.  endangered frogs,  in-
vasive  frogs.  Such  non-rigid  terms  refer  to  in-
stances  of  species  in  contextual  circumstances. 
The processes and states are the important pieces 
of  information  that  matter  to the users  and are 
useful for mining text. The model therefore dis-
tinguishes between background vocabularies, do-
main terms,  wordnets and the central  ontology. 
The  background  vocabularies  are  automatically 
aligned  to  wordnet,  where  we  assume  that 
hyponymy relations to rigid synsets in wordnet 
declare those subconcepts as rigid subtypes too, 
without the necessity to include them in the onto-
logy.  For  non-rigid  terms,  we  defined  a  set  of 
mapping relations to the ontology through which 
we express their non-rigid involvement in these 
processes and states. Likewise, the ontology has 
been extended with processes and states for the 
domain  and  verbs  and  adjectives  have  been 
mapped to be able to detect expressions in text.
The  3-layered  knowledge  model  combines  the 
efforts from 3 different communities:
1.Domain  experts  in  social  communities  that 
continuously build background vocabularies;
2.Wordnet  specialists  that  define  the  basic  se-
mantic model for general concepts for a lan-
guage
3.Semantic Web specialists that define top-level 
and domain-specific ontologies that capture 
formal definitions of concepts;
We formalized the relations between these repos-
itories so that they can developed separately but 
combined within KYOTO to form a coherent and 
formal model.
3.1 Ontology
The KYOTO ontology currently consists of 1149 
classes divided over three layers. The top layer is 
based  on  DOLCE  (DOLCE-Lite-Plus  version 
3.9.7,  Masolo  et  al  2003)  and  OntoWordNet. 
This layer of the ontology has been modified for 
our purposes (Herold et. al. 2009).  The second 
layer consists of so-called Base Concepts (BCs) 
derived  from various  wordnets  (Vossen  1998, 
Izquierdo  et  al. 2007).  Examples  of  BCs  are: 
building,  vehicle,  animal,  plant,  change,  move,  
size, weight. The BCs are those synsets in Word-
Net 3.0 that have the most relations with other 
synsets in the wordnet hierarchies and are selec-
ted in a way that ensures complete coverage of 
the nominal and verbal part of WordNet. This has 
been  completed  for  the  nouns  (about  500 
synsets).  The ontology has also been adapted to 
include important concepts in the domain. Spe-
cial attention has been paid to represents the pro-
cesses  (perdurants)  in  which  objects  (endur-
ants)  of  the domain are  involved and qualities 
they may have. This is typically the information 
that is found in documents on the environment. 
We thus added 40 new event classes for repres-
enting  important  verbs  (e.g.  pollute, absorb, 
damage, drain) and 115 new qualities and qual-
ity-regions for representing important adjectives 
(e.g. airborne, acid, (un)healthy, clear). The full 
Figure 1: Two Cycles of processing in KYOTO
3
ontology can be downloaded from the KYOTO 
website, free for use. A considerable set of gener-
al verbs and adjectives (relevant for for the do-
main)  have  then  been  mapped  to  ontological 
classes: 189  verbal  synsets  and  222  adjectival 
synsets.
The  500  nominal  BCs  are  connected  to  the 
complete  WordNet  hierarchy,  whereas  the  189 
verbs represent 5,978 more specific verbal syn-
sets and the 222 adjectives represent  1,081 ad-
jectival synsets through the wordnet relations.
This basic ontology and the mapping to Word-
Net  are  used  to  model  the  shared  and  lan-
guage-neutral  concepts  and  relations  in the do-
main. Instances are excluded from the ontology. 
Instances will be detected in the documents and 
will be mapped to the ontology through instance 
to ontology relations (see below).  Likewise, we 
make a clear separation between the ontological 
model and the instantiation of the model as de-
scribed in the text.
3.2 Wordnet to ontology mappings
In addition to the ontology, we have wordnets for 
each language in the domain. In addition to the 
regular synset to synset relations in the wordnet, 
we will have a specific set of relations for map-
ping the synsets  to the ontology,  which are  all 
prefixed with sc_ standing for synset-to-concept. 
We differentiate between rigid and non-rigid con-
cepts in the wordnets through the mapping rela-
tions:
? sc_equivalenceOf: the synset is fully equi-
valent to the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_  subclassOf: the synset is a proper sub-
class of the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_domainOf: the synset is not a proper sub-
class  of  the  ontology  Type  &  is  not  disjoint 
(therefore orthogonal) with other synsets that are 
mapped to the same Type either through sc_sub-
classOf or sc_domainOf; the synset is non-Rigid 
but still inherits all properties of the target onto-
logy Type;  the synset  is  also related to a Role 
with a sc_playRole relation
? sc_playRole:  the  synset  denotes  instances 
for  which  the  context  of  the  Role  applies  for 
some period of time but this is not essential for 
the existence of the instances, i.e. if the context 
ceases to exist then the instances may still exist 
(Mizoguchi et al 2007).3
? sc_participantOf:  instances of the concept 
(denoted by the synset) participate in some en-
durant, where the specific role relation is indic-
ated by the playRole mapping. 
? sc_hasState: instances of the concept are in 
a particular state which is not essential and can 
be changed. There is no need to represent the role 
for a stative perdurant.
This model  extends  existing  WordNet  to  onto-
logy mappings.  For  instance,  in  the  SUMO to 
Wordnet mapping (Niles and Pease 2003), only 
the  sc_equivalenceOf and  sc_subclassOf rela-
tions  are  used,  represented  by  the symbols  ?=? 
and ?+? respectively. The SUMO-Wordnet map-
ping likewise does not systematically distinguish 
rigid from non-rigid  synsets.  In our  model,  we 
separate the linguistically and culturally specific 
vocabularies from the shared ontology while us-
ing the ontology  to interface  the concepts used 
by the various communities.
Using these mapping relations, we can express 
that the synset for  duck (which has a hypernym 
relation to the synset  bird, which, in its turn, has 
an  equivalence  relation  to  the  ontology  class 
bird) is  thus  a  proper  subclassOf  the  ontology 
class bird:
wn:duck hypernym wn:bird
wn:bird  sc_equivalenceOf ont:bird
For a concept such as migratory bird, which is 
also  a  hyponym of  bird in  wordnet  but  not  a 
proper subclass as a non-rigid concept, we thus 
create the following mapping:
wn:migratory bird 
? sc_domainOf ont:bird
? sc_playRole ont:done-by
? sc_participantOf ont:migration
This mapping indicates that the synset is used to 
refer to instances of endurants (not subclasses!), 
where the domain is restricted to birds. Further-
more, these instances participate in the process of 
3 Some terms involve more than one role,  e.g.  gas-
powered-vehicle.  Secondary  participants  are  related 
through  sc_hasCoParticipant and sc_playCoRole 
mappings.
4
migration in the role of  done-by. The properties 
of  the  process  migration are  further  defined  in 
the  ontology,  which  indicates  that  it  is  a  act-
ive-change-of-location  done-by  some  endurant, 
going from a source, via a path to some destina-
tion. The mapping relations from the wordnet to 
the ontology, need to satisfy the constraints of the 
ontology, i.e. only roles can be expressed that are 
compatible with the role-schema of the process 
in which they participate.
For  implied  non-essential  states,  we  use  the 
sc_hasState relation to express that a synset such 
as wild dog refers to instances of dogs that life in 
the wild but can stop being wild:
wn:wild dog ? sc_domainOf ont:dog
wn:wild dog ? sc_hasState ont:wild
Ideally, all processes and states that can be ap-
plied to endurants should be defined in the onto-
logy. This may hold for most verbs and adject-
ives in languages, which do not tend to extend in 
specific  domains  and  are  part  of  the  general 
vocabulary  (e.g.  to  pollute,  to  reduce,  wild). 
However, domain specific text contain many new 
nominal terms that refer to domain-specific pro-
cesses and states, e.g. air pollution, nitrogen pol-
lution,  nitrogen  reduction.  These  terms  are 
equally relevant as their counter-parts that refer 
to endurants involved in similar  processes, e.g. 
polluted air, polluting nitrogen or reduced nitro-
gen. We therefore use the reverse participant and 
role mappings to be able to define such terms for 
processes  as  subclasses  of  more  general  pro-
cesses  involving  specific  participants  in  a  spe-
cified role:
wn:air pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:air
? sc_hasRole ont:patient
wn:nitrogen pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:nitrogen
? sc_hasRole ont:done-by
 
Further  mapping  relations  are  described  in  the 
documentation on the KYOTO website. Through 
the mapping relations, we can keep the ontology 
relatively small and compact whereas we can still 
define  the  richness  of  the  vocabularies  of  lan-
guages in a precise way. The classes in the onto-
logy can be defined using rich axioms that model 
precise implications for inferencing. The wordnet 
to synset mappings can be used to define rather 
basic relations relative to the given ontology that 
still  captures  the  semantics  of  the  terms. The 
term definitions capture both relevance and per-
spective  (those  relations  that  matter  from  the 
point of the view of the term), on the one hand, 
and some semantics with respect to the concepts 
that are involved and their (role) relation on the 
other  hand.  Likewise,  the  KYOTO  system can 
model the linguistic and cultural diversity of lan-
guages in a domain but at the same time keep a 
firm anchoring to a basic and compact ontology.
3.3 Domain wordnet
We selected 3 representative documents on estu-
aries to extract relevant terms for the domain us-
ing the Tybot module. The terms have been re-
lated  through  structural  relations,  e.g.  nitrogen 
pollution is a hyponym of pollution, and through 
WordNet synsets that are assigned through WSD 
of the text.  We extracted 3950 candidate  terms 
form the KAF representations of the documents. 
Most of these are nouns (2818 terms). The nom-
inal  terms matched for 40% with wordnet syn-
sets, the verbs and adjectives for 98% and 85% 
respectively. For the domain wordnet, we restric-
ted ourselves to the nouns. From the new nomin-
al  terms,  environmentalists selected  390  terms 
that they deem to be important. These terms are 
connected to parent terms, which ultimately are 
connected to wordnet synsets.  The final domain 
wordnet contains 659 synsets: 197 synsets from 
the generic wordnet and 462 new synsets connec-
ted to the former.  The domain wordnet synsets 
got 990 mappings to the ontology, using the rela-
tions described in the previous section. There are 
86 synsets that have a sc_domainOf mapping, in-
dicating  that  they  are  non-rigid.  Note  that 
hyponyms of these synsets are also non-rigid by 
definition. These non-rigid synsets have complex 
mappings to processes and states in which  they 
are involved. The domain wordnet can be down-
loaded from the KYOTO website, free for use.
5
4 Off-line reasoning and ontological tag-
ging 
The ontological tagging represents the last phase 
in the KYOTO Linguistic  Processor  annotation 
pipeline.  It  consists  of  a three-step module  de-
vised to enrich the KAF documents with know-
ledge derived from the ontology. For each synset 
connected to a term, the first step   adds the Base 
Concepts to which the synset is related through 
the wordnet taxonomical relations. Then, through 
the synset to ontology mapping, it  adds the cor-
responding ontology type with appropriate rela-
tions. Once each synset is specified as to its onto-
logy type,  the  last  ontotagging  step  inserts  the 
full  set  of  ontological  implications  that  follow 
from the explicit ontology. The explicit ontology 
is a new data  structure consisting of a table with 
all  ontology nodes and all  ontological  implica-
tions expressed. The main purpose is to optimize 
<term lemma="pollution" pos="N" tid="t13444" type="open">
  <externalReferences>
   <externalRef reference="eng-30-00191142-n" reftype="baseConcept" resource="wn30g"/>
   <externalRef reference="Kyoto#change-eng-3.0-00191142-n" reftype="sc_subClassOf" resource="ontology">
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#contamination_pollution"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#accomplishment" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#event" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#part" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#specific-constant-constituent" reference="DOLCE-Lite.owl#perdurant" 
status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-quality" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#spatio-temporal-particular" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#participant" reference="DOLCE-Lite.owl#endurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-location_q" status="im-
plied"/>
    <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#particular" status="implied"/>
    </externalRef>
  </externalReferences>
</term>
Figure 2: An example of an OntoTagged output
<kprofile>
 <variables>
<var name="x" type="term" pos="N"/>
  <var name="y" type="term" 
       lemma="produce | generate | release | ! create"/>
  <var name="z" type="term"
       reference="DOLCE-Lite.owl#contamination_pollution"
       reftype="SubClassOf"/>
 </variables>
 <relations>
  <root span="y"/>
  <rel span="x" pivot="y" direction="preceding"/>
  <rel span="z" pivot="y" direction="following"/>
 </relations>
 <events>
  <event target="$y/@tid" lemma="$y/@lemma" pos="$y/@pos"/>
  <role target="$x/@tid" rtype="agent" lemma="$x/@lemma"/>
  <role target="$z/@tid" rtype="patient"lemma="$z/@lemma"/>$
 </events>
</kprofile>
Figure 3: An example of a Kybot profile
<kybotOut>
 <doc name="11767.mw.wsd.ne.onto.kaf">
  <event eid="e1" lemma="generate" pos="V" target="t3504"/>
  <role rid="r1" lemma="industry" rtype="agent" target="t3493" pos="N" event="e1"/>
  <role rid="r2" lemma="pollution" rtype="patient" target="t3495" pos="N" event="e1"/>
 </doc>
 <doc name="16266.mw.wsd.ne.onto.kaf">
  <event eid="e2" lemma="release" pos="V" target="t97"/>
  <role rid="r3" lemma="fuel" rtype="agent" target="t96" pos="N" event="e2"/>
  <role rid="r4" lemma="exhaust_gas" rtype="patient" target="t101" pos="V" event="e2"/>
 </doc>
</kybotOut>
Figure 4: An example of a Kybot output
6
the performance of the mining module over large 
quantities of documents. The advantage for Ky-
bots from ontotagging are many. First of all, they 
are  able  to  run  and  apply  pattern-matching  to 
Base  Concepts  and  ontological  classes  rather 
than just to words or synsets. Moreover, by mak-
ing explicit  the  implicit  ontological  statements, 
Kybots are able to find the same relations hidden 
in  different  expressions  with  different  surface 
realizations:  fish migration,  migratory  fish,  mi-
gration of fish, fishes that migrate, that directly 
or indirectly express the same relations. With on-
totagging,  they  share  the  same ontological  im-
plications which will allow Kybots to apply the 
same patterns and perform the extraction of facts. 
The implications will be represented in the same 
way across different languages, thus facilitating 
cross-lingual extraction of facts. Lastly, ontotag-
ging is a kind of off-line ontological reasoning: 
without  doing reasoning over concepts,  Kybots 
substantially  improve their  performance.  Figure 
2 shows the result of onto-tagging for the term 
pollution.
5 Event and fact extraction
Kybots (Knowledge Yielding Robots) are  com-
puter  programs  that  use  the  mined 
concepts and the generic  concepts  already con-
nected to the language wordnets and the KYOTO 
ontology to extract actual concept instances and 
relations in KAF documents. Kybots incorporate 
technology  for  the  extraction  of  relationships, 
either eventual or not, relative to the general or 
domain concepts already captured by the Tybots. 
That is, the extraction of factual knowledge is be-
ing carried out by the Kybot server by processing 
Kybot profiles on the linguistically enriched doc-
uments.
Kybots  are  defined  following  a  declarative 
format,  the  so  called  Kybot  
profiles, which describe general morpho-syntact-
ic  and  semantic  conditions  on  sequences  of 
terms. Profiles are compiled to generate the Ky-
bots, which scan over KAF documents searching 
for the patterns and extract the relevant informa-
tion from each matching.
Linguistic  patterns  include morphologic  con-
straints and also semantic conditions the matched 
terms must hold.  Kybot are thus able to search 
for term lemmas or part-of-speech tags but also 
for terms linked to ontological process and states 
using  the  mappings  described  in  Section  3.2. 
Thus, it is possible to detect similar eventual in-
formation  across  documents  in  different  lan-
guages, even if expressed differently.
5.1 Example of a Kybot Profile
Kybot Profiles are described using XML syn-
tax.  Figure 3 presents an example of a profile. 
Kybot profiles consist of three main parts: 
?Variable  declaration (<variables> element): 
In this section the search entities are defined. The 
example  defines  three  variables:  x (denoting 
terms  whose  part-of-speech is  noun),  y (which 
are  terms whose lemma is ?release?, ?produce? 
or  ?generate?  but   not  ?create?)  and  z (terms 
linked to  the  ontological  endurant  ?DOLCE-L-
ite.owl#contamination_pollution?, meaning ``be-
ing contaminated with harmful  substances''). 
?Declarations  of  the  relations  among  variables 
(<rel> element): specify the relations among the 
previously  defined variables.  The example pro-
file specifies y  as the main pivot, and states that 
variable  x must  be  preceding  variable  y in  the 
same sentence, and that variable  z must be fol-
lowing variable  y.  Thus,  the Kybot will  search 
for patterns like 'x ? y ? z' in a sentence.
?Output template (<events> element): describes 
the output to be produced on every matching. In 
the example, each match generates a new event 
targeting term  y,  which becomes the main term 
of the event. It also fills two roles of the event, 
the 'agent' role filled by term x and 'patient' role, 
filled by z. 
Figure  4  presents  the  output  of  the  Kybot 
when applied against the benchmark documents.
The Kybot output follows the stand-off architec-
ture when producing new information, and it thus 
forms  a  new KAF layer  on  the  original  docu-
ments.
6 Experimental results
We applied the KYOTO system and resources to 
English documents on estuaries. We collected 50 
URLs for two English estuaries: the Humber Es-
tuary in Hull (UK) and the Chesapeake Bay estu-
ary in the US and for background documents on 
bird  migration,  sedimentation,  habitat  destruc-
tion,  and  climate  change.  In  addition  to  the 
webpages, we extracted 815 PDF files from the 
sites. In total, 4625 files have been extracted. All 
7
the documents have been processed by the lin-
guistic  processor  for  English,  which  generated 
KAF representations for all the documents. From 
this  database,  3  documents  were  selected  for 
benchmarking.
The  documents  were  processed  by  applying 
multiword  tagging,  word-sense-disambiguation, 
named-entity-recognition  and  the  ontological 
tagging to the 3 documents and to the complete 
database; This was done twice: once without the 
domain model and once with the domain model. 
We thus created 4 datasets:  3 benchmark docu-
ments  processed  with  and  without  the  domain 
model; the complete database processed with and 
without the domain model.
Furthermore, we created Kybot profiles based 
on the type of information represented in the do-
main model. We applied the Kybots to all 4 data 
sets. We generate the following data files through 
an WN-LMF export of the domain wordnet:
1. a set of domain multiwords for the multi-
word tagger
2. an extension of the lexicon and the graph 
of  concepts  that  is  used  by  the  WSD 
module
3. an extension of the wordnet-to-ontology 
mappings for the ontotagger
In addition, we constructed mapping lists for all 
WordNet 3.0 synsets to Base Concepts and to ad-
jective and verbs that are matched to the onto-
logy.  These mappings provide the generic  con-
ceptual model based on wordnet and on the onto-
logy. 
Table 1 shows the effects of using the domain 
model for the first 3 modules. We can see that the 
domain  model  has  a  clear  effect  on  the multi-
word  detection  in  the  3  evaluation  documents. 
Using the domain model,  600 multiwords have 
been detected, against 145 with just the generic 
wordnet. This is obvious since the terms are ex-
tracted  from  the  same  documents.  However, 
when applying it  to the complete  database,  we 
see that  still  over 2,300 more multiwords have 
been  detected  using  the domain wordnet.  Note 
that the domain wordnet has only 97 multiwords 
and the generic wordnet has 19,126 multiwords. 
So 0.5% of the multiwords in the domain word-
net add 1.5 times more multiword tokens in the 
database. The third row specifies the number of 
synsets that have been assigned. We can see that 
for the domain model almost 400 more synsets 
have been detected. In the case of the full estuary 
database, we see that relatively few more have 
been detected, almost 1,500 while the database is 
80 times as big. If we look more closely at the 
numbers of actual  domain synsets detected,  we 
see the following results. In the benchmark docu-
ments  637 (or 5%) of  the synsets  is  a  domain 
wordnet  synset,  whereas  5,353 synsets  are  do-
main synsets in the full estuary database, which 
is only 0.52%. Note that in KAF multiwords are 
represented both as a single terms and in terms of 
their elements. The WSD module assigns synsets 
to  both.  The  domain  model  can  thus  only  add 
synsets compared to the processing without the 
domain. 
Finally, if we look at the named-entity-recogni-
tion module, we see a slight negative effect for 
the detection of named-entities due to the domain 
model.  The  named-entity-recognition  module 
does not consider the elements of multiwords but 
just  the multiword terms as a whole. Grouping 
terms  as  multiwords  thus  leads  to  less  named-
entities being detected. This is not necessarily a 
bad things, since the detection heavily over-gen-
erates and could have now more precision.
Table 1: Statistics on processing the estuary documents with and without domain model
bench mark documents (3) estuary documents (4742)
No Domain Domain No Domain Domain
terms 22,204 22,204 2,419,839 2,419,839
multiwords 145 600 4,389 6,671
12,526 12,910 1,021,598 1,023,017
158 126 41,681 40,714
67 66 10,288 10,233
synsets
ne location
ne date
8
Table 2 shows the effect of inserting ontologic-
al  implications  into  the  text  representation.For 
the benchmark documents, we see that more than 
half a million ontological implications have been 
inserted.  Of  these, 82% are implied references, 
that are extracted from the explicit ontology on 
the  basis  of  a  direct  mapping to  the  ontology. 
About  8% of  the  mappings  are  synset-to-onto-
logy mappings (sc) and 9.5% are mappings rep-
resenting the subclass hierarchy. The differences 
between using the domain model and not-using 
the domain model are minimal. For the complete 
database, the implications are 80 times as much 
but the proportions are similar.
Table 3 shows the type of sc-relations that oc-
cur.  Obviously,  sc_subClassOf  and  sc_equival-
entOf  are  the  most  frequent.  Nevertheless,  we 
still  find  about  500  mappings  that  present  the 
participation in a process or state. 
 
     30  reftype="sc_playCoRole"
     32  reftype="sc_hasCoParticipant"
     42  reftype="sc_partOf"
     59  reftype="sc_stateOf"
     92  reftype="sc_playRole"
     94  reftype="sc_hasRole"
     97  reftype="sc_participantOf"
   105  reftype="sc_hasParticipant"
   128  reftype="sc_domainOf"
   169  reftype="sc_hasState"
   312  reftype="sc_hasPart"
 3637  reftype="sc_equivalentOf"
42048  reftype="sc_subClassOf"
Table 3: Type of relations for the wordnet to ontology  
mappings using the domain model
The table clearly shows the impact of role rela-
tions  that  are  encoded  in  the  domain  wordnet. 
When  we  extract  the  mappings  for  the  files 
without the domain model (ony using the map-
pings to the generic wordnet), we get only equi-
valence and subclass mappings.
Finally to complete the knowledge cycle, we cre-
ated a few Kybot profiles for extracting events 
from the  onto-tagged  documents.  As  an  initial 
test, 3 profiles have been created:
1. events of destruction
2. destructions of locations
3. destruction of objects
Using  these  profiles,  we  extracted  211  events 
from the 3 benchmark documents with 396 roles. 
The profiles are created to run over the ontolo-
gical  types  inserted  by  the  ontotagger,  e.g.  re-
stricted to events and change_of_integrity.  Des-
pite the generality of the profiles, we still see a 
clear signature of the domain in the output. This 
is a good indication that we will be able to ex-
tract valuable events from the data, even though 
the  ontotagger  generates  a  massive  amount  of 
implications.  Especially  events  that  combine 
multiple  roles  appear  to  give  rich  information. 
For example, the following sentence:
"One of the greatest challenges to restoration is con-
tinued population growth and development, which 
destroys forests, wetlands and other natural areas"
yielded the following output:
   <event target="t1471" lemma="destroy" pos="V" 
eid="e74"/>
   <role target="t1477" rtype="patient" lemma="area" 
pos="N" event="e74" rid="r138"/>
   <role target="t1472" rtype="patient" 
lemma="forest" pos="N" event="e74" rid="r151"/>
   <role target="t1469" rtype="actor" lemma="devel-
opment" pos="N" event="e74" rid="r180"/>
Running the full set of profiles on the complete data-
base with almost 60 million ontological statements 
took about 2 hours. This shows that our approach is 
scalable and efficient.
Table 2: Ontological implications for the four data sets
bench mark documents (3) estuary documents (4272
No Domain Domain Domain
ontology references 555,677 576,432 48,708,300
implied ontology references 457,332 82.30% 474,916 82.39% 40,523,452 83.20%
direct ontology references 53,178 9.57% 54,769 9.50% 4,377,814 8.99%
45,167 8.13% 46,747 8.11% 3,807,034 7.82%domain synset to ontology mappings
9
7 Conclusions
In this paper, we described an open platform for 
text-mining  using wordnets  and a central  onto-
logy.  The  system  can  be  used  across  different 
languages and can be tailored to mine any type of 
conceptual relations. It can handle semantic im-
plications that are expressed in very different lin-
guistic expressions and yield systematic output. 
As future work, we will carry out benchmarking 
and testing of the mining of events, both for Eng-
lish and for the other languages in the KYOTO 
project.
Acknowledgements
The KYOTO project is co-funded by EU - FP7 
ICT Work Programme 2007 under Challenge 4 - 
Digital  libraries  and  Content,  Objective  ICT-
2007.4.2  (ICT-2007.4.4):  Intelligent  Contsent 
and Semantics  (challenge 4.2).  The Asian part-
ners from Tapei and Kyoto are funded from na-
tional funds. This work has been also supported 
by  Spanish  project  KNOW-2 (TIN2009-14715-
C04-01).
References
Agirre, E., & Soroa, A. (2009) Personalizing PageR-
ank for Word Sense Disambiguation. Proceedings 
of the 12th EACL, 2009. Athens, Greece. 
Agirre, E., Lopez de Lacalle, O., & Soroa, A. (2009) 
Knowledge-based WSD and specific domains: per-
forming over supervised WSD. Proceedings of IJ-
CAI. Pasadena, USA. http://ixa.si.ehu.es/ukb
?lvez J., Atserias J., Carrera J., Climent S., Laparra 
E., Oliver A. and Rigau G. (2008) Complete and 
Consistent  Annotation of  WordNet  using the Top 
Concept Ontology. Proceedings of LREC'08, Mar-
rakesh, Morroco. 2008.
Appelt Douglas E., Jerry R. Hobbs, John Bear, David 
Israel, Megumi Kameyama, Andrew Kehler, David 
Martin,  Karen Myers and Mabry Tyson. Descrip-
tion of the FASTUS System Used for MUC-6. In 
Proceedings  of  MUC-6,  pages  237?248.  San 
Mateo, Morgan Kaufmann, 1995.
Auer A., C. Bizer, G. Kobilarov, J. Lehmann, R. Cy-
ganiak and Z. Ives. DBpedia: A Nucleus for a Web 
of  Open  Data.  In  Proceedings  of  the
International  Semantic  Web  Conference  (ISWC), 
volume 4825 of  Lecture Notes  in Computer Sci-
ence, pages 722-735. 2007.
Bosma, W., Vossen, P., Soroa, A. , Rigau, G., Tesconi, 
M., Marchetti, A., Monachini, M., & Apiprandi, C. 
(2009) KAF: a generic semantic annotation format. 
In Proceedings of the 5th International Conference 
on Generative Approaches to the Lexicon Sept 17-
19, 2009, Pisa, Italy.
Fellbaum,  C.  (Ed.)  (1998)  WordNet:  An  Electronic 
Lexical Database. Cambridge, MA: MIT Press.
Freitag, D. (1998) Information extraction from html: 
Application  of  a  general  machine  learning  ap-
proach.  In  Proceedings  of  the  Fifteenth  National 
Conference on Artificial Intelligence, 1998.
Gangemi  A.,  Guarino  N.,  Masolo  C.,  Oltramari  A., 
Schneider  L.  (2002)  Sweetening  Ontologies  with 
DOLCE. Proceedings of EKAW. 2002
Ide, N. and L. Romary. 2003. Outline of the inter- na-
tional standard Linguistic Annotation Framework. 
In Proceedings of ACL 2003 Workshop on Lin-
guistic Annotation: Getting the Model Right, pages 
1?5.
Izquierdo R., Su?rez A. & Rigau G. Exploring the 
Automatic Selection of Basic Level Concepts. Pro-
ceedings of RANLP'07, Borovetz, Bulgaria. 
September, 2007.
Masolo, C., Borgo, S., Gangemi, A.,  Guarino, N. & 
Oltramari, A. (2003) WonderWeb Deliverable D18: 
Ontology Library, ISTC-CNR, Trento, Italy.
Mizoguchi R., Sunagawa E., Kozaki K. & Kitamura 
Y. (2007 A Model of Roles within an Ontology De-
velopment  Tool:  Hozo.  Journal  of  Applied  Onto-
logy, Vol.2, No.2, 159-179.
Niles, I. & Pease, A. (2001) Formal Ontology in In-
formation Systems. Proceedings of the internation-
al Conference on Formal Ontology in Information 
Systems ? Vol. 2001 Ogunquit, Maine,  USA
Niles, I. and A. Pease. Linking lexicons and ontolo-
gies:  Mapping  WordNet  to  the  Suggested  Upper 
Merged Ontology. In Proc. IEEE IKE, pages 412?
416, 2003.
Vossen, P. (Ed.) (1998) EuroWordNet: a multilingual 
database  with  lexical  semantic  networks  for 
European Languages. Kluwer, Dordrecht.
Vossen P., W. Bosma, E. Agirre, G. Rigau, A. Soroa 
(2010) A full Knowledge Cycle for Semantic Inter-
operability.  Proceedings  of  the  5th  Joint  ISO-
ACL/SIGSEM Workshop on Interoperable Semant-
ic Annotation, (ICGL 2010) Hong Kong, 2010.
10
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 39?43,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Historical Event Extraction from Text 
 
Agata Cybulska Piek Vossen 
VU University Amsterdam VU University Amsterdam 
De Boelelaan 1105 De Boelelaan 1105 
1081 HV Amsterdam 1081 HV Amsterdam 
ak.cybulska@let.vu.nl p.vossen@let.vu.nl 
 
 
 
 
Abstract 
In this paper, we report on how historical 
events are extracted from text within the Se-
mantics of History research project. The project 
aims at the creation of resources for a historical 
information retrieval system that can handle the 
time-based dynamics and varying perspectives 
of Dutch historical archives. The historical 
event extraction module will be used for mu-
seum collections, allowing users to search for 
exhibits related to particular historical events or 
actors within time periods and geographic 
areas, extracted from accompanying text. We 
present here the methodology and tools used for 
the purpose of historical event extraction along-
side with the first evaluation results. 
1 Introduction 
The research project Semantics of History1 is con-
cerned with the development of a historical ontol-
ogy and a lexicon that will be used in a new type of 
information retrieval system. In historical texts the 
reality changes over time (Ide & Woolner, 2007). 
Furthermore, historical realities can be seen diffe-
rently depending on the subjective view of the 
writer. In the design of our search system, we will 
take into consideration the change of reality and 
the diverse attitudes of writers towards historical 
events so that they both can be used for the pur-
pose of historical information retrieval. 
In the first phase of the project we researched 
how descriptions of historical events are realized in 
different types of text and what the implications 
                                                        
1 The Semantics of History is funded by the Interfaculty Re-
search Institute CAMeRA at the Free University Amsterdam 
as a collaboration of the Faculties of Arts and Exact Science: 
http://www2.let.vu.nl/oz/cltl/semhis/index.html. 
are for historical information retrieval. Different 
historical perspectives of writers correspond with 
genre distinctions and correlate with variation in 
language use. Texts, written shortly after an event 
happened, use more specific and uniquely occur-
ring event descriptions than texts describing the 
same events but written from a longer time pers-
pective. Statistical analysis performed within the 
first phase of the project confirmed this hypothe-
sis2. To capture differences between event repre-
sentations and to identify relations between 
historical events, we defined a historical event 
model which consists of 4 slots: a location slot, 
time, participant and an action slot (see also Van 
Hage et al2011 for the formal SEM model). 
After arriving at an understanding of how to 
model historical events, we moved on to actually 
extracting events from text. In this paper we report 
on our approach into historical event extraction 
from textual data about the Srebrenica Massacre 
from July 19953. There are two problems that had 
to be tackled for the purpose of this task: 1) extrac-
tion of event actions with their participants, loca-
tions and time markers and 2) filtering of events 
lacking historical value from all events extracted 
by the system. We believe that event actions and 
their participants, locations and time markers can 
be extracted based on some syntactic clues, PoS, 
lemma and combinatory information together with 
semantic class definition and exclusion by means 
of Wordnet. Historical filtering can be performed 
through semantic classification of event actions. 
                                                        
2 For details see Cybulska, Vossen, LREC 2010. 
3 The Srebrenica corpus consists of 78 Dutch texts. For more 
information on the design of the corpus see Cybulska, Vossen 
(2010). 
39
We tested this hypothesis within the KYOTO 
framework4.   
2 Related Work 
Two other projects concerned with extraction of 
historical information are the FDR/Pearl Harbor 
project and the New Web Portal. The latter5 aimed 
at creation of a digital archive of historical news-
papers of the National Library of Finland6. Within 
the project a semantic search system for historical 
texts was created using a common ontology with 
semantically annotated cultural objects (Ahonen 
and Hyv?onen, 2009). Related content is being 
linked through semantic annotation of historical 
texts based on ontology labels which presupposes 
that only high level historical events from text 
were annotated. The Pearl Harbor project aimed at 
facilitating enhanced search and retrieval from a 
set of documents from the FDRL library by utiliz-
ing a series of multiple temporally contextualized 
snapshot ontologies determined by the occurrence 
of key historical events (Ide & Woolner, 2007). 
We did not manage to find evaluation results for 
any of the two projects. Traditional approaches to 
event extraction that do report evaluation results 
use models that severely restrict the relations. They 
achieve high precision but poorly represent the text 
as a whole. E.g., Xu et. al. (2006) report over 80% 
precision for prize award extraction and Tanev et. 
al. (2008) 74% precision for violent events and 
disasters. Our approach models more events in a 
text and events of a broader scope, more compara-
ble to Wunderwald (2011), who extracts partici-
pants and roles from news in general, reporting 50-
60% precision. Wunderwald uses a machine-
learning approach, while our method is know-
ledge-based. Furthermore, Wunderwald does not 
distinguish historical from non-historical events. 
3 Historical Event Extraction 
3.1 Generic Event Extraction by means of 
KYOTO 
KYOTO tools were specifically designed to extract 
events from text. This pipeline-architecture of lin-
                                                        
4 For more information about the KYOTO - project 
(www.kyoto-project.eu) see Vossen et al(2008a). 
5 The New Web Portal is part of the National Semantic Web 
2.0 (FinnONTO 2.0) project. 
6 http://digi.lib.helsinki.fi/sanomalehti/secure/main.html 
guistic processors generates a uniform semantic 
representation of text in the so-called Kyoto Anno-
tation Format (KAF)7. KAF is a stand-off format 
that distinguishes separate layers for text tokens, 
text terms, constituents and dependencies. It can be 
used to represent event actions with their partici-
pants, locations and time markers. For the purpose 
of this research, the Srebrenica corpus was 
processed by means of the KYOTO ? architecture. 
First, the corpus was tagged with PoS- informa-
tion; it was lemmatized and syntactically parsed by 
means of a dependency parser for Dutch - Alpino8. 
Next, word sense disambiguation was performed9 
and the corpus was semantically annotated with 
labels from the Dutch Wordnet10 and ontological 
classes. Generic event information stored in the 
KAF ? format can be extracted within KYOTO by 
means of Kybot-profiles which are stored in the 
XML format11. These profiles define patterns over 
different layers in KAF and create a semantic out-
put layer for matches over these layers. 
3.2 Semantic Tagging of Historical Events 
To extract historical events we developed ?histori-
cal? Kybot-profiles which define appropriate con-
structions and semantic classes of historical actions 
and their participants, locations and time markers. 
In these profiles, the semantic action classes are 
used to distinguish historical from non-historical 
events. The semantic type specification was de-
rived from manual tagging of historical event slots 
by means of the KAF-annotator12 in 5 development 
texts from the Srebrenica corpus 13 . Manually 
tagged historical event actions as well as partici-
pants, locations and time markers were automati-
cally mapped with corresponding Wordnet synsets. 
In case of multiple senses assigned per word the 
appropriate Wordnet ID was manually chosen. 
Historical event tagging with Wordnet ID?s re-
vealed a few problematic issues. For a number of 
                                                        
7 Kyoto Annotation Format is described in Bosma et al(2009). 
8 http://www.let.rug.nl/vannoord/alp/Alpino/ 
9 For word sense disambiguation the UKB system 
(http://ixa2.si.ehu.es/ukb/) was used. For more information the 
reader is referred to Agirre & Soroa (2009). 
10 For more information see Vossen et al(2008b). 
11 For more information see KYOTO deliverable 5.4 at 
http://www.kyoto-project.eu/. 
12 See tools at http://www.kyoto-project.eu/. 
13 The development set contains one Wikipedia entry, two 
educational texts and two newspaper articles written a few 
years after the Srebrenica massacre happened. 
40
locations, time markers, participants and actions 
there were no Wordnet synsets automatically as-
signed. No WN-concepts were found for geograph-
ical names as Srebrenica or Zagreb. Also person 
and organization names (Mladic, Dutchbat III, 
NIOD) and dates would not get any synsets as-
signed. The same applies to compounds (moslim-
mannen ?Muslim men?, VN-militairen ?UN sol-
diers?), pronoun participants and loanwords: (such 
as safe haven in a Dutch text). Furthermore there 
were some historical senses missing in the Dutch 
Wordnet (such as vredesoperatie ?peacekeeping 
operation?, oorlogspad ?warpath?). To be able to 
handle proper names we used a named entity rec-
ognition module. By means of NER we added 
dates and geographical names to KAF so that we 
could further use them for the extraction of time 
markers and locations. In the future, we will look 
into compound splitting and we are also going to 
add the missing historical senses to the Wordnet 
database. 
After identifying historical WN-synsets, we au-
tomatically determined the most informative 
hypernyms of the seed terms per historical label. 
Based on the chosen hypernyms (and their hypo-
nyms), we manually selected a number of semantic 
classes to be able to identify event locations, time 
markers, participants and historical actions in his-
torical texts. We defined six semantic classes de-
noting: human participants, time periods, moments 
in time, places, historical and motion actions. Fur-
thermore we specified six more action classes to 
filter out non historical and potential events: ac-
tions indicating modality, polarity, intention, sub-
jectivity, cognitive (also rarely of historical 
importance) and contentless actions. Next, we de-
rived a table that assigns one of the ontological 
classes to every synset in Wordnet on the basis of 
the relations to the labeled hypernyms. All KAF-
files were then annotated with the twelve semantic 
classes, on the basis of the Wordnet synsets as-
signed by the WSD module and this mapping ta-
ble. 
4 Kybot Profiles 
Kyoto-Kybot extracts events from KAF by means 
of Kybot profiles. Based on event descriptions 
from the development set 402 profiles were de-
fined, using semantic and constructional informa-
tion and specifically PoS, lemma, compositional 
and semantic restrictions with regards to locations, 
time expressions, event actions and participants. 
The current version of the system uses 22 pro-
files to extract historical actions, based on semantic 
tagging by means of Wordnet and the specification 
of some compositional properties. Historical ac-
tions are the most significant part of historical 
event extraction. They serve to distinguish histori-
cal actions from the non-historical ones and to 
identify parts of the same historical event. The pro-
files extract both, verbal actions (such as deport, 
murder, occupy) and nominal ones (such as fight, 
war and offensive) as well as actions with a syntac-
tic object (sign a treaty, start the offensive etc). 
Next to the semantic class of historical actions also 
motion actions (often occurring with a goal or re-
sult phrase as transport into a location) are ex-
tracted as potential historical event actions. The 
action profiles exclude from the output the non-
historical semantic action classes and by that the 
non historical events are filtered out. 
For the extraction of historical participants we 
now use 314 profiles. The variation within histori-
cal participant descriptions of the development set 
was, as expected, much higher than the diversity of 
formulations denoting other event parts. Participant 
profiles specify noun phrases (also proper names) 
organized around the semantic class of human par-
ticipants14. It is a relatively common phenomenon 
in historical event descriptions that geographical 
proper names are used for referral to participants. 
So we also created some profiles identifying coun-
try and city names occurring in the subject position 
of active sentences. 
To extract historical event time we specified 43 
temporal profiles. Thanks to the named entity rec-
ognition module of Kyoto we are able to retrieve 
dates and, based on Wordnet, the system can rec-
ognize temporal expressions which refer to week-
days or months and more general and relative time 
markers (such as now or two weeks later). 
Furthermore, 23 location profiles are utilized to 
extract geographical proper names and other loca-
tive expressions based on the Wordnet class of 
places (as street, city, country etc). 
                                                        
14 For now we focused on human animate participants and 
those referred to by personal pronouns. In the future we will 
also look into extracting participants indirectly named through 
word combinations consisting of geo adjectives preceding 
words denoting weapons and transportation vehicles (such as 
Serbian tanks). 
41
5 Evaluation 
For the evaluation purposes we used the KYOTO 
triplet representation of historical events, which is 
a generic event representation format. A triplet 
consists of a historical action, mapped with its 
nearby occurring participant, location or time ex-
pression together with a label indicating the event 
slot type. In the evaluation the gold standard trip-
lets will be compared with triplets generated by the 
system. A set of five texts from the Srebrenica cor-
pus, written some years after the massacre, was 
tagged manually with historical events by two in-
dependent annotators. We obtained a very high 
inter-annotator agreement of 94% (0.91 Kappa). 
As a baseline, we generated triplets from all 
constituent heads in a sentence. Each constituent 
head is once treated as an action while all the oth-
ers are seen as participants. Applying the default 
relation ? historical participant ? the baseline 
achieved an average of 66% recall and a (unders-
tandably) low precision of less than 0.01%. Tables 
1 and 2 present the performance of the system on 
the evaluation set. The abbreviations in the tables 
stand for: T. Nr ? Token Number, G. Trp ? Gold 
Triplets, S. Trp ? System Triplets, C.S. Trp ? Cor-
rect System Triplets, R ? Recall, P. ? Precision, F ? 
F-measure. 
 
         Counts   
File 
T. 
Nr 
G.  
Trp 
S.  
Trp 
C.S.  
Trp 
R. 
% 
P. 
% 
F 
File 1 243 5 4 1 20 25 0.22 
File 2 440 32 25 18 56 72 0.63 
File 3 647 58 68 32 55 47 0.51 
File 4 429 32 22 17 53 77 0.63 
File 5 209 19 19 12 63 63 0.63 
Micro Average - - - - 49 57 0.53 
 
Table 1. Evaluation results per file (micro average). 
 
     Counts     
Relation 
G. 
Trp 
S. 
Trp 
C.S. 
Trp 
R. 
% 
P. 
% 
F 
Participants 98 95 57 58 60 0.59 
Time 17 20 13 76 65 0.70 
Location 31 23 10 32 43 0.37 
 
Table 2. Evaluation results per relation (macro average) 
 
The system reached an overall recall of 49% and 
a precision of 57%. The low scores for file 1 can 
be explained by the fact that in this text some so 
called ?political events? were described such as 
responsibility issues and an investigation w. r. t. 
events in Srebrenica that was performed in the 
Netherlands few years after the massacre. Current-
ly the system is not prepared to handle any other 
events than the conflict related ones. 
Historical actions, evaluated in a separate non 
triplet evaluation cycle, were extracted with a re-
call of 67.94% and a precision of 51.96%. We ex-
tracted time expressions with the highest precision 
of 65% and also the highest recall of 76%. The 
lower recall and precision measures reached for the 
extraction of participants and especially locations 
can be explained by the type shift of the semantic 
class of locations used for referral to event partici-
pants. As mentioned before, so far we only are able 
to identify these if occurring in subject position; in 
the future we will add deeper syntactic dependency 
information into KAF and by that we will improve 
the recognition of locations used as participants. 
6 Conclusion and Future Work 
In this paper we showed that historical events can 
successfully be extracted from text, based on con-
structional clues and semantic type specification.  
To extract events we used a generic fact mining 
system KYOTO; we specified language structures 
and Wordnet concepts denoting event actions, par-
ticipants, locations and time markers and we iden-
tified the historical events through recognition of 
historical actions. The evaluation results confirm 
that historical events can be extracted from histori-
cal texts by means of this approach with a relative-
ly high recall of almost 50% and a precision of 
57%, (comparable to the results of Wunderwald, 
2011). In our future work we are going to increase 
the performance of the system by utilizing in the 
profiles more specific syntactic information and 
the grammatical tense. We will also look into other 
possibilities of distinguishing between historical 
events and events lacking historical value, also in 
non historical genres. In the next stage of the 
project we will make an attempt to automatically 
determine relations between historical events over 
textual data. We will also apply the system to other 
historical descriptions that are connected to mu-
seum collections. Because of the generic design of 
the extraction module, we expect that the extrac-
tion of conflict events can be applied to other pe-
riods and events with little adaptation. 
42
Acknowledgments 
This research was funded by the interfaculty re-
search institute CAMeRA (Center for Advanced 
Media Research) of the VU University of Amster-
dam: http://camera.vu.nl. 
References  
Agirre, Eneko and Aitor Soroa, 2009, ?Personalizing 
PageRank for Word Sense Disambiguation?, in: Pro-
ceedings of the 12th conference of the European 
chapter of the Association for Computational Lin-
guistics, (EACL-2009), Athens, Greece. 
Ahonen, Eeva and Eero Hyv?onen, 2009, ?Publishing 
Historical Texts on the Semantic Web -A Case 
Study? [online] available: 
http://www.seco.tkk.fi/publications/2009/ahonen-
hyvonen-historical-texts-2009.pdf 
Bosma, Wauter, Vossen, Piek, Soroa, Aitor, Rigau, 
German, Tesconi, Maurizio, Marchetti, Andrea, Mo-
nachini, Monica, and Carlo Aliprandi, 2009 ?KAF: a 
generic semantic annotation format.?, in Proceedings 
of the GL2009 Workshop on Semantic Annotation, 
Pisa, Italy, Sept 17-19, 2009. 
Bosma, Wauter and Piek Vossen, 2010, "Bootstrapping 
language neutral term extraction", in: Proceedings of 
the 7th international conference on Language Re-
sources and Evaluation, (LREC2010), Valletta, Mal-
ta, May 17-23, 2010. 
Cybulska, Agata and Piek Vossen, ?Event models for 
Historical Perspectives: Determining Relations be-
tween High and Low Level Events in Text, Based on 
the Classification of Time, Location and Partici-
pants?, in Proceedings of LREC 2010, Valletta, Mal-
ta, May 17-23, 2010 
Ide, Nancy and David Woolner, 2007, ?Historical On-
tologies?, in: Ahmad, Khurshid, Brewster, Christo-
pher, and Mark Stevenson (eds.), Words and 
Intelligence II: Essays in Honor of Yorick Wilks, 
Springer, 137-152. 
Tanev, Hristo, Piskorski, Jakub and Martin Atkinson, 
?Real-Time News Event Extraction for Global Crisis 
Monitoring?, in NLDB 2008: Kapetanios, Epami-
nondas, Sugumaran, Vijayan, Spiliopoulou, Myra 
(eds.) Proceedings of the 13th International Confe-
rence on Applications of Natural Language to Infor-
mation Systems, 2008, Springer: LNCS, vol. 5039, 
pp. 207-218.  
Van Hage, Willem, Malais?, Veronique, Segers, Rox-
ane, Hollink, Laura (fc), Design and use of the Sim-
ple Event Model (SEM), the Journal of Web Seman-
tics, Elsevier 
Vossen, Piek, Agirre, Eneko, Calzolari, Nicoletta, Fell-
baum, Christiane, Hsieh, Shu-kai, Huang, Chu-Ren, 
Isahara, Hitoshi, Kanzaki, Kyoko, Marchetti, Andrea, 
Monachini, Monica, Neri, Federico, Raffaelli, Remo, 
Rigau, German, Tescon, Maurizio, 2008a, "KYOTO: 
A system for Mining, Structuring and Distributing 
Knowledge Across Languages and Cultures", in: 
Proceedings of LREC 2008, Marrakech, Morocco, 
May 28-30, 2008. 
Vossen, Piek, Bosma, Wauter, Agirre, Eneko, Rigau, 
German and Aitor Soroa, 2010, "A full Knowledge 
Cycle for Semantic Interoperability", in: Proceedings 
of the 5th Joint ISO-ACL/SIGSEM Workshop on In-
teroperable Semantic Annotation, in conjunction with 
the Second International Conference on Global Inte-
roperability for Language Resources, (ICGL 2010), 
Hong Kong, January 15-17, 2010. 
Wunderwald, Martin, 2011, ?NewsX Event Extraction 
from News Articles?, diploma thesis, Dresden Uni-
versity of Technology, Dresden, Germany, URL: 
http://www.rn.inf.tu-
dres-
den.de/uploads/Studentische_Arbeiten/Diplomarbeit_
Wunderwald_Martin.pdf 
Xu, Feiyu, Uszkoreit, Hans, Li, Hong, 2006. ?Automat-
ic Event and Relation Detection with Seeds of Vary-
ing Complexity?, in: Proceedings of the AAAI 2006 
Workshop Event Extraction and Synthesis, Boston, 
491-498. 
43
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 10?18,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
A verb lexicon model for deep sentiment analysis and opinion mining  
applications 
 
Isa Maks Piek Vossen 
VU University, Faculty of Arts 
De Boelelaan 1105, 1081 HV Amsterdam, 
The Netherlands 
VU University, Faculty of Arts 
De Boelelaan 1105, 1081 HV Amsterdam, 
The Netherlands 
e.maks@let.vu.nl p.vossen@let.vu.nl 
 
 
 
 
 
Abstract 
This paper presents a lexicon model for subjec-
tivity description of Dutch verbs that offers a 
framework for the development of sentiment 
analysis and opinion mining applications based 
on a deep syntactic-semantic approach. The 
model aims to describe the detailed subjectivity 
relations that exist between the participants of 
the verbs, expressing multiple attitudes for each 
verb sense.  Validation is provided by an anno-
tation study that shows that these subtle subjec-
tivity relations are reliably identifiable by 
human annotators.  
 
1 Introduction 
This paper presents a lexicon model for the de-
scription of verbs to be used in applications like 
sentiment analysis and opinion mining. Verbs are 
considered as the core of the sentence as they name 
events or states with participants expressed by the 
other elements in the sentence. We consider the 
detailed and subtle subjectivity relations that exist 
between the different participants as part of the 
meaning of a verb that can be modelled  in a lex-
icon. 
Consider the following example: 
 
Ex. (1) ? Damilola?s killers were boasting about      
his murder... 
 
This sentence expresses a positive sentiment of the 
killers towards the fact they murdered Damilola 
and it expresses the negative attitude on behalf of 
the speaker/writer who has negative opinion of the 
the murderers of Damilola. Both attitudes are part 
of the semantic profile of the verb and should be 
modelled in a subjectivity lexicon.   
  As opinion mining and sentiment analysis appli-
cations tend to utilize more and more the composi-
tion of sentences (Moilanen (2007), Choi and 
Cardie (2008), Jia et al (2009)) and to use the val-
ue and properties of the verbs expressed by its 
dependency trees, there is a need for specialized 
lexicons where this information can be found. For 
the analysis of more complex opinionated text like 
news, political documents, and (online) debates the 
identification of the attitude holder and topic are of 
crucial importance. Applications that exploit the 
relations between the verb meaning and its argu-
ments can better determine sentiment at sentence-
level and trace emotions and opninions to their 
holders.   
  Our model seeks to combine the insights from a 
rather complex model like Framenet (Ruppenhofer 
et al (2010)) with operational models like Senti-
wordnet where simple polarity values (positive, 
negative, neutral) are applied to the entire lexicon.  
Subjectivity relations that exist between the differ-
ent participants are labeled with information con-
cerning both the identity of the attitude holder and 
the orientation (positive vs. negative) of the atti-
tude. The model accounts for the fact that verbs 
may express multiple attitudes. It includes a cate-
gorisation into semantic categories relevant to opi-
nion mining and sentiment analysis and provides 
means for the identification of the attitude holder 
and the polarity of the attitude and for the descrip-
tion of the emotions and sentiments of the different 
10
participants involved in the event. Attention is paid 
to the role of the speaker/writer of the event whose 
perspective is expressed and whose views on what 
is happening are conveyed in the text. 
  As we wish to provide a model for a lexicon that 
is operational and can be exploited by tools for 
deeper sentiment analysis and rich opinion mining, 
the model is validated by an annotation study of 
580 verb lexical units (cf. section 4). 
 
2 Related Work 
   Polarity and subjectivity lexicons are valuable 
resources for sentiment analysis and opinion min-
ing. For English, a couple of smaller and larger 
lexicons are available. 
      Widely used in sentiment analysis are auto-
matically derived or manually built polarity lexi-
cons. These lexicons are lists of words (for 
example, Hatzivassiloglou and McKeown (1997), 
Kamps et al (2004), Kim and Hovy (2004) or 
word senses (for example, Esuli and Sebastiani 
(2006), Wiebe and Mihalcea (2006), Su and Mar-
kert, (2008)) annotated for negative or positive 
polarity. As they attribute single polarity values 
(positive, negative, neutral) to words they are not 
able to account for more complex cases like boast 
(cf. example 1) which carry both negative and 
positive polarity dependening on who is the atti-
tude holder.  
  Strapparava and Valitutti (2004) developed 
Wordnet-Affect, an affective extension of Word-
net. It describes ?direct? affective words, i.e. words 
which denote emotions. Synsets are classified into 
categories like emotion, cognitive state, trait, be-
haviour, attitude and feeling. The resource is fur-
ther developed (Valittutti and Strapparava, 2010) 
by adding the descriptions of ?indirect? affective 
words according to a specific appraisal model of 
emotions (OCC). An indirect affective word indi-
rectly refers to emotion categories and can refer to 
different possible emotions according to the sub-
jects (actor, actee and observer) semantically con-
nected to it. For example, the word victory, if 
localized in the past, can be used for expressing 
pride (related to the actor or ?winner?), and disap-
pointment (related to the actee or ?loser?). If victo-
ry is a future event the expressed emotion is hope.  
Their model is similar to ours, as we both relate 
attitude to the participants of the event. However, 
their model focuses on a rich description of differ-
ent aspects and implications of emotions for each 
participant whereas we infer a single positive or 
negative attitude. Their model seems to focus on 
the cognitive aspects of emotion whereas we aim 
to also model the linguistic aspects by including 
specifically the attitude of the Speaker/Writer in 
our model. Moreover, our description is not at the 
level of the synset but at lexical unit level which 
enables us to differentiate gradations of the 
strength of emotions within the synsets. This 
enables us to relate the attitudes directly to the 
syntactic-semantic patterns of the lexical unit.   
  Also Framenet (Ruppenhofer et al (2010)) is 
used as a resource in opinion mining and sentiment 
analysis (Kim and Hovy (2006)). Framenet (FN) is 
an online lexical resource for English that contains 
more than 11,600 lexical units. The aim is to clas-
sify words into categories (frames) which give for 
each lexical unit the range of semantic and syntac-
tic combinatory possibilities. The semantic roles 
range from general ones like Agent, Patient and 
Theme to specific ones such as Speaker, Message 
and Addressee for Verbs of Communication. FN 
includes frames such as Communication, Judg-
ment, Opinion, Emotion_Directed and semantic 
roles such as Judge, Experiencer, Communicator 
which are highly relevant for opinion mining and 
sentiment analysis. However, subjectivity is not 
systematically and not (yet) exhaustively encoded 
in Framenet. For example, the verb gobble (eat 
hurriedly and noisily) belongs to the frame Inges-
tion (consumption of food, drink or smoke) and 
neither the frame nor the frame elements account 
for the negative connotation of gobble. Yet, we 
think that a resource like FN with rich and corpus 
based valency patterns is an ideal base/ starting 
point for subjectivity description. 
  None of these theories, models or resources is 
specifically tailored for the subjectivity description 
of verbs. Studies which focus on verbs for senti-
ment analysis, usually refer to smaller subclasssess 
like, for example, emotion verbs (Mathieu, 2005, 
Mathieu and Fellbaum, 2010) or quotation verbs 
(Chen 2005, 2007). 
 
3 Model  
The proposed model is built as an extension of an 
already existing lexical database for Dutch, i.e. 
11
Cornetto (Vossen et al 2008). Cornetto combines 
two resources with different semantic organisa-
tions: the Dutch Wordnet (DWN) which has, like 
the Princeton Wordnet, a synset organization and 
the Dutch Reference Lexicon (RBN) which is or-
ganised in form-meaning composites or lexical 
units. The description of the lexical units includes 
definitions, usage constraints, selectional restric-
tions, syntactic behaviors, illustrative contexts, etc. 
DWN and RBN are linked to each other as each 
synonym in a synset is linked to a corresponding 
lexical unit. The subjectivity information is mod-
elled as an extra layer related to the lexical units of 
Reference Lexicon thus providing a basis for the 
description of the verbs at word sense level.  
 
3.1 Semantic Classes 
For the identification of relevant semantic classes 
we adopt ? and broaden ? the definition of subjec-
tive language by Wiebe et al (2006). Subjective 
expressions are defined as words and phrases that 
are used to express private states like opinions, 
emotions, evaluations, speculations.  
Three main types are distinguished: 
 
Type I: 
Direct reference to private states (e.g. his alarm  
grew, he was boiling with anger). We include in 
this category emotion verbs (like feel, love and 
hate) and cognitive verbs (like defend, dare,realize 
etc.) ; 
 
Type II: 
Reference to speech or writing events that express 
private states (e.g. he condemns the president, they 
attack the speaker). According to our schema, this 
category  includes all speech and writing events 
and the annotation  schema points out if they are 
neutral (say, ask) or bear polarity (condemn, 
praise); 
 
Type III: 
Expressive subjective elements are expressions 
that indirectly express private states (e.g. superb, 
that doctor is a quack).  According to our annota-
tion  schema this category is not a separate one , 
but verbs senses which fall in this category are 
always also member of one of the other categories. 
For example, boast (cf. ex. 1) is both a Type II (i.e. 
speech act verb) verb and a Type III verb as it indi-
rectly expresses the negative attitude of the speak-
er/writer towards the speech event. By considering 
this category as combinational, it enables to make 
a clear distinction between Speaker/Writer subjec-
tivity and participant subjectivity. 
 
Moreover, we add a fourth category which in-
cludes verbs which implicitly refer to private 
states. If we consider the following examples: 
 
Ex. (2) the teacher used to beat the  students  
Ex. (3) C.A is arrested for public intoxication  by 
the police 
 
Neither beat nor arrest are included in one of the 
three mentioned categories as neither of them ex-
plicitly expresses a private state. However, in 
many contexts these verbs implicitly and indirectly 
refer to the private state of one of the participants. 
In ex. (2) the teacher and the students will have 
bad feelings towards each other and also in ex. (3) 
C.A. will have negative feelings about the situa-
tion. To be able to describe also these aspects of 
subjectivity we define the following additional 
category:  
 
Type IV: 
Indirect reference to a private state that is the 
source or the consequence of an event (action, state 
or process). The event is explicitly mentioned.   
 
Verb senses which are categorized as Type I, II or 
III are considered as subjective; verb senses cate-
gorized as Type IV are only subjective if one of the 
annotation categories (see below for more details) 
has a non-zero value; otherwise they are consi-
dered as objective. 
We assigned well-known semantic categories to 
each of the above mentioned Types (I, II and IV).  
Table 1 presents the resulting categories with ex-
amples for each category. The first column lists the 
potential subjectivity classes that can apply. 
 
  
12
 
 
 
 
 
 
Table 1 Semantic Categories
Type %ame Description Examples 
I (+III) EXPERIENCER Verbs that denote emotions. Included are both experiencer 
subject and experiencer object verbs. 
hate, love, enjoy, enter-
tain, frighten, upset, fru-
strate 
I(+III) ATTITUDE A cognitive action performed by one of the participants, in 
general the structural subject of the verb. The category is rele-
vant as these cognitive actions may imply attitudes between 
participants.  
defend, think, dare, ig-
nore, avoid, feign, pre-
tend, patronize, devote, 
dedicate 
II(+III) JUDGMENT A judgment (mostly positive or negative) that someone may 
have towards something or somebody. The verbs directly refer 
to the thinking or speech act of judgment. 
praise, admire, rebuke, 
criticize, scold, reproach, 
value, rate, estimate 
II(+III) COMM-S A speech act that denotes the transfer of a spoken or written 
message from the perspective of the sender or speaker (S) of 
the message. The sender or speaker is the structural subject of 
the verb. 
speak, say, write, grum-
ble, stammer, talk, email, 
cable, chitchat, nag, in-
form 
II(+III) COMM-R A speech act that denotes the transfer of a spoken or written 
message from the perspective of the receiver(R) of the mes-
sage. The receiver is the structural subject of the verb 
 read, hear, observe, 
record, watch, compre-
hend 
IV(+III) ACTION A physical action performed by one of the participants, in 
general the structural subject of the verb. The category is rele-
vant as in some cases participants express an attitude by per-
forming this action.    
run, ride, disappear, hit, 
strike, stagger, stumble 
IV(+III) PROCESS_STATE This is a broad and underspecified category of state and process 
verbs (non-action verbs) and may be considered as a rest cate-
gory as it includes all verbs which are not included in other 
categories.  
grow, disturb, drizzle, 
mizzle  
13
 
                    
 
3.2 Attitude and roles 
 
In our model, verb subjectivity is defined in terms 
of verb arguments carrying attitude towards each 
other, i.e. as experiencers holding attitudes towards 
targets or communicators expressing a judgment 
about an evaluee. The various participants or atti-
tude holders which are involved in the events ex-
pressed by the verbs all may have different 
attitudes towards the event and/or towards each 
other. We developed an annotation schema (see 
Table 2 below) which enables us to relate the atti-
tude holders, the orientation of the attitude (posi-
tive, negative or neutral) and the syntactic 
valencies of the verb to each other.  
  To be able to attribute the attitudes to the relevant 
participants we identify for each form-meaning 
unit the semantic-syntactic distribution of the ar-
guments, the associated Semantic Roles and some 
coarse grained selection restrictions. 
We make a distinction between participants 
which are part of the described situation, the so-
called event internal participants, and participants  
that are outside the described situation, the external 
participants.  
 
? Event internal attitude holders 
 
The event internal attitude holders are partici-
pants which are lexicalized by the structural sub-
ject (A1), direct object (A2 or A3) or 
indirect/prepositional object (A2 or A3). A2 and 
A3 both can be syntactically realized as an NP, a 
PP, that-clause or infinitive clause. Each partici-
pant is associated with coarse-grained selection 
restrictions: SMB (somebody +human), SMT 
(something -human) or SMB/SMT (some-
body/something + ? human).  
Attitude (positive, negative and neutral) is attri-
buted to the relations between participants A1 vs. 
A2 (A1A2) and A1 vs. A3 (A1A3) and/or the rela-
tion between the participants (A1, A2 and A3) and 
the event itself (A1EV, A2EV and A3EV, respec-
tively) as illustrated by the following examples.  
 
verdedigen  (defend: argue or speak in defense of) 
A1A2:  positive 
A1A3:  negative 
SMB (A1) SMB/SMT 
(A2) 
tegen SMB/SMT 
(A3) 
He(A1) defends his decision(A2) against 
 critique(A3) 
 
verliezen (lose: miss from one's possessions) 
A1EV: negative 
SMB(A1) SMB/SMT(A2) 
He (A1) loses his sunglasses (A2) like crazy  
 
? Event external attitude holders 
 
Event external attitude holders are participants who 
are not part of the event itself but who are outside 
observers. We distinguish two kind of perspec-
tives, i.e. that of the Speaker or Writer (SW) and a 
more general perspective (ALL) shared by a vast 
majority of people.  
 
? Speaker /Writer (SW) 
 
The Speaker/Writer (SW) expresses his attitude 
towards the described state of affairs by choosing 
words with overt affective connotation (cf. ex. 4) 
or by conveying his subjective interpretation of 
what happens (cf. ex. 5).  
 
Ex. 4: He gobbles down three hamburgers a day 
 
In (ex. 4) the SW not only describes the eating 
behavior of the ?he? but he also expresses his nega-
tive attitude towards this behavior by choosing the 
negative connotation word gobble.  
 
(Ex. 5) B. S. misleads district A voters 
 
In (ex. 5), the SW expresses his negative attitude 
towards the behavior of the subject of the sentence, 
by conceptualizing it in a negative way.  
 
? ALL 
 
Some concepts are considered as negative by a vast 
majority of people and therefore express a more 
general attitude shared by most people. For exam-
ple, to drown, will be considered negative by eve-
rybody, i.e. observers, participants to the event and 
listener to the speech event. These concepts are 
labeled with a positive or negative attitude label for 
ALL. The annotation model is illustrated in table 2. 
14
 
 
FORM SUMMARY SEMTYPE COMPLEME%TATIO% A1A2 A1A3 A1EV A2EV A3EV SW ALL 
vreten 
(devour, gobble) 
 eat immoderately 
and hurriedly 
ACTION SMT (A2) 2 0 0 0 0 -4 0 
afpakken 
(take away) 
take without the 
owner?s consent 
ACTION SMT(A2) van SMB (A3) 0 0 0 0 -3 0 0 
verliezen (lose) 
lose: fail to keep 
or to maintain 
PROCESS SMT (A2) 0 0 -3 0 0 0 0 
dwingen (force) 
 
urge a person to 
an action 
ATTITUDE SMB (A2) tot SMT (A3) -3 2 0 0 0 0 0 
opscheppen (boast) 
to speak with 
exaggeration and 
excessive pride 
COMM-S over SMB/SMT (A2) 3 0 0 0 0 -4 0 
helpen (help) 
give help or assis-
tance ; be of 
service 
ACTION SMB(A2) met SMT (A3) 2 1 0 0 0 0 0 
bekritiseren(criticize) 
express criticism 
of 
COMM-S SMB (A2)  -3 0 0 0 0 0 0 
zwartmaken (slander) 
charge falsely or 
with malicious 
intent 
COMM-S SMB (A2)  -3 0 0 0 0 -4 0 
verwaarlozen (neglect) fail to attend to ATTITUDE SMB (A2) -3 0 0 0 0 -4 0 
afleggen 
(lay out) 
prepare a dead 
body 
ACTION SMB (A2) 0 0 0 0 0 0 -1 
Explanation: 
A1A2   A1 has a positive (+) or negative(-) attitude towards A2 
A1A3  A1 has a positive (+) or negative(-) attitude towards A3 
A1EV  A1 has a positive or negative attitude towards the event 
A2EV  A2 has a positive or negative attitude towards the event 
A3EV  A3 has a positive or negative attitude towards the event 
SW  SW has a positive or negative attitude towards event or towards the structural subject of the event 
ALL   there is a general positive or negative attitude towards the event 
 
 
Table 2: Annotation Schema 
 
4 Intercoder Agreement Study 
 
To explore our hypothesis that different attitudes  
associated with the different attitude holders can be 
modelled in an operational lexicon and to explore 
how far we can stretch the description of subtle 
subjectivity relations, we performed an inter-
annotator agreement study to assess the reliability 
of the annotation schema.  
We are aware of the fact that it is a rather complex 
annotation schema and that high agreement rates 
are not likely to be achieved. The main goal of the 
annotation task is to determine what extent this 
kind of subjectivity information can be reliably 
identified, which parts of the annotation schema 
are more difficult than others and perhaps need to 
be redefined. This information is especially valua-
ble when ? in future- lexical acquisition tasks will 
be carried out to acquire automatically parts of the 
information specified by the annotation schema. .  
Annotation is performed by 2 linguists (i.e. both 
authors of this paper). We did a first annotation 
task for training and discussed the problems before 
the gold standard annotation task was carried out. 
The annotation is based upon the full description of 
 
15
the lexical units including glosses and illustrative 
examples. 
4.1 Agreement results 
All attitude holder categories were annotated as 
combined categories and will be evaluated together 
and as separate categories.   
? Semantic category polarity  
   Overall percent agreement for all 7 attitude hold-
er categories is 66% with a Cohen kappa (?) of 
0.62 (cf. table 3, first row). Table 3 shows that not 
all semantic classes are of equal difficulty.  
 
 Number 
of items 
Kappa 
Agreement 
Percent 
Agreement 
All  581 0.62 0.66 
Comm-s 57 0.75 0.77 
Comm-r 16 0.55 0.81 
Attitude 74 0.55 0.60 
Action 304 0.60 0.66 
StateProcess 83 0.47 0.55 
Judgment 25 0.82 0.84 
Experiencer 23 0.74 0.83 
Table 3: Agreement for semantic categories  
 
? Attitude Holder Polarity 
Table 4 shows that agreement rates for each sepa-
rate attitude holder differ. Although some catego-
ries are not reliable identifiable (cf. A1EV, A2EV, 
A3EV, ALL), the larger categories with many 
sentiment-laden items (cf. the third column which 
gives the coverage in percentage with regard to 
positive or negative annotations) are the ones with 
high agreement rates.  
 
 
 Kappa Percent 
agreement 
PosOrNeg 
A1-A2 0.73 0.89 25% 
A1-A3 0.73 0.98 2% 
A1EV 0.41 0.93 6% 
A2EV 0.56 0.94 7% 
A3EV 0.54 0.98 2% 
SW 0.76 0.91 23% 
ALL 0.37 0.87 10% 
  Table 4: Agreement rates for attitude holder categories  
 
? Attitude Holder Polarity 
Table 5 gives agreement figures for the most im-
portant attitude holder categories (A1A2 and SW) 
with respect to the different semantic categories. 
Low scores are found especially in categories (like 
State_Process) less relevant for Sentiment Analysis 
and opinion mining.  
 
 A1A2(
?)  
SW(?) 
Comm-s 0.83 0.93 
Comm-r 1.00 1.00  
Experiencer 0.82 0.84 
Action 0.61 0.78 
Judgment 0.92 0.63 
State-process 0.33 0.64 
Attitude 0.72 0.68 
Table 5: Kappa agreement  for SW and A1A2  
 
? Single Polarity  
One single polarity value for each item is derived 
by collapsing all attitude holder polarity values 
into one single value. If an item is tagged with 
different polarity values we apply them in the fol-
lowing order: SW, A1A2, A1A3, A1EV, A2EV, 
A3EV, ALL. As can be seen from table 6, ob-
served agreement is 84% and kappa=0.75. Separate 
polarity computation (positive, negative and neu-
tral) ? with one polarity value of interest and the 
other values combined into one non-relevant cate-
gory - shows that all polarity values are reliable 
identifiable.  
 
 Kappa Percent 
Agreement 
Single polarity 0.75 0.84 
Positive 0.70 0.91 
Negative 0.82 0.92 
Neutral 0.72 0.86 
Table 6: agreement rates for polarity categories 
 
4.2 Disagreement Analysis 
 
Overall agreement is 66% (K=0.62) which is a 
reasonable score, in particular for such a compli-
cated annotation schema. Moreover, scores are 
high for semantic categories such as Communica-
tion (0.75), Judgment (0.80), Experiencer (0.74) 
which are relevant for subjectivity analysis. 
   Table 4 shows that low performance is largely 
due to the attitude holder categories A1EV, A2EV, 
A3EV and ALL which have scores ranging from 
0.37 to 0.56 whereas the categories A1A2, A1A3 
and SW are reliably identifiable. As the last 3 cate-
gories are the largest ones with respect to senti-
16
ment bearing items, overall scores do not degrade 
much.    
   The low scores of A1EV, A2EV, A3EV and 
ALL are probably due to the fact that they are easi-
ly confused with each other.  For example, jagen 
(hunt), vallen (fall), klemmen (stick, jam) and 
flauwvallen (faint) all have negative polarity but 
the annotators do not agree about who is the atti-
tude holder:  ALL (i.e. ALL have a negative atti-
tude towards hunting, falling, being jammed, and 
fainting) or A1/2-RES (i.e. the person who falls, is 
jammed, is fainted or is hunted is the one who has 
the negative attitude).  Confusion is found also 
between A2EV and A1A2. For example, with re-
spect to misleiden (mislead), annotators agree 
about a negative attitude from A1 vs. A2 , but one 
annotator marks additionally a negative attitude on 
behalf of A2 (A2EV: negative) whereas the other 
does not. 
   Especially the category ALL seems not to be 
defined well as many items are marked positive or 
negative by one annotator and neutral by the other.  
Examples of disagreements of this kind are ploe-
gen (plough), ontwateren (drain), omvertrekken 
(pull over) and achternalopen (follow, pursue).  
Both annotators regard these items as objective 
expressions but they do not agree about whether 
some general positive or negative feelings are as-
sociated to them or not.  
    Disagreement occurs also where collocational 
information may lead one annotator to see subjec-
tivity in a sense and the other not. For example, 
houden (keep - conform one?s action or practice 
to) associated with collocations like to keep ap-
pointments and to keep one?s promises is consi-
dered positive (A1A2) by one annotator and 
neutral by the other.  This seems to apply to all 
frequent  light verbs with little semantic content 
like make, do and take. 
   With respect to the category SW disagreements 
do not arise from confusions with other categories 
but from judgments which differ between neutral 
vs. non-neutral. Consider for example, tevredens-
tellen (mollify) as in I mollified her (A2) by clean-
ing my room. Both annotators agree about the 
positive attitude between A1 and A2, but they dis-
agree (SW:positive vs. SW:neutral) about whether 
the SW conveys a positive attitude towards ?I? by 
describing her behavior or not. Other examples of 
this type are ignoreren (ignore), zich verzoenen 
(make up), redden (deal with), and dwingen 
(force).  
  Overall agreement for one polarity is rather high 
with ?=0.75. (cf. table 6). The scores are compar-
ible to agreement rates of other studies where verbs 
are marked for single polarity. For example, inter-
annotator agreement between 2 annotators who 
annotated 265 verb senses of the Micro-WNop 
corpus (Cerini et al (2007)) is 0.75 (?) as well.  It 
shows that a complicated and layered annotation 
does not hamper overall agreement and may also 
produce lexicons which are appropriate to use 
within applications that use single polarity only.   
   Summarizing, we conclude that overall agree-
ment is good, especially with regard to most se-
mantic categories relevant for subjectivity analysis 
and with respect to the most important attitude 
holder categories, SW and A1A2.  When defining 
an operational model the small and low scoring  
categories, i.e. A1/A2/A3EV and ALL, will be 
collapsed into one underspecified attitude holder 
category.  
5 Conclusions 
  In this paper we presented a lexicon model for the 
description of verbs to be used in applications like 
deeper sentiment analysis and opinion mining, 
describing the detailed and subtle subjectivity rela-
tions that exist between the different participants of 
a verb. The relations can be labeled with subjectiv-
ity information concerning the identity of the atti-
tude holder, the orientation (positive vs. negative) 
of the attitude and its target. Special attention is 
paid to the role of the speaker/writer of the event 
whose perspective is expressed and whose views 
on what is happening are conveyed in the text. 
  We measured the reliability of the annotation. 
The results show that when using all 7 attitude 
holder categories, 3 categories, SW, A1A2 and 
A1A3 are reliable and the other 4 are not. As these 
not reliable categories are also small, we think that 
the annotation schema is sufficiently validated. 
  An additional outcome to our study is that we 
created a gold standard of 580 verb senses. In the 
future we will use this gold standard  to test me-
thods for the automatic detection of subjectivity 
and polarity properties of word senses in order to 
build a rich subjectivity lexicon for Dutch verbs. 
 
17
6 Acknowledgments 
  This research has been carried out within the 
project  From Text To Political Positions (http: 
//www2.let.vu.nl/oz/cltl/t2pp/). It is funded by the 
VUA Interfaculty Research Institute CAMeRA.  
 
7 References 
Andreevskaia, A.  and S. Bergler (2006) Mining Word-
Net for Fuzzy Sentiment:Sentiment Tag Extraction 
fromWordNet Glosses. In: EACL-2006, Trento, Ita-
ly. 
Chen, L. (2005) Transitivity in Media Texts: negative 
verbal process sub-functions and narrator bias. In In-
ternational Review of Applied Linguistics in Teach-
ing, (IRAL-vol. 43) Mouton De Gruyter, The Hague, 
The Netherlands.   
Cerini, S., Compagnoni, V., Demontis, A., Formentelli, 
M., and Gandini, G. (2007). Language resources and 
linguistic theory: Typology, second language acquisi-
tion, English linguistics (Forthcoming), chapter Mi-
cro-WNOp: A gold standard for the evaluation of 
automatically compiled lexical resources for opinion 
mining. Milano, Italy. 
 
Choi Y. and C. Cardie (2008). Learning with Composi 
     tional Semantics as Structural Inference for subsen                
tential Sentiment Analysis.  Proceedings of Recent 
Advances in Natural Language Processing (RANLP), 
Hawaii.  
Esuli, Andrea and Fabrizio Sebastiani. (2006). Senti-
WordNet: A Publicly Available Lexical Resource for 
Opinion Mining. In: Proceedings of LREC-2006, 
Genova, Italy. 
Hatzivassiloglou, V., McKeown, K.B. (1997) Predicting 
the semantic orientation of adjectives. In Proceedings 
of ACL-97, Madrid, Spain. 
 
Jia, L., Yu, C.T., Meng, W. (2009) The effect of negation 
on sentiment analysis and retrieval effectiveness. In 
CIKM-2009, China. 
 
Kamps, J.,  R. J. Mokken, M. Marx, and M. de Rijke 
(2004). Using WordNet to measure semantic orienta-
tion of adjectives. In  Proceedings LREC-2004, Paris. 
 
Kim, S. and E. Hovy (2004) Determining the sentiment 
of opinions. In Proceedings of COLING, Geneva, 
Swtizerland. 
 
Kim, S. and E. Hovy (2006) Extracting Opinions Ex-
pressed in Online News Media Text with Opinion 
Holders and Topics.  In: Proceedings of the Workshop 
on Sentiment and Subjectivity in Text (SST-06). Syd-
ney, Australia. 
 
Maks, I.and P. Vossen (2010)  Modeling Attitude, Polar-
ity and Subjectivity in Wordnet. In Proceedings of 
Fifth Global Wordnet Conference, Mumbai, India.  
 
Mathieu, Y. Y. (2005). A Computational Semantic 
Lexicon of French Verbs of Emotion. In: Computing 
Attitude and Affect in Text: Theory and Applications 
J. Shanahan, Yan Qu, J.Wiebe (Eds.). Springer, Dor-
drecht, The Netherlands.  
Mathieu,Y.Y. and C. Felbaum (2010). Verbs of emotion 
in French and English. In: Proceedings of GWC-
2010, Mumbai, India. 
Moilanen K. and S. Pulman. (2007). Sentiment Compo-
sition. In Proceedings of Recent Advances in Natural 
Language Processing (RANLP), Bulgaria. 
Ruppenhofer, J. , M. Ellsworth, M. Petruck, C. Johnson, 
and J. Scheffzcyk (2010) Framenet II: Theory and 
Practice (e-book) http://framenet.icsi. berkeley.edu/ 
book/book.pdf. 
C. Strapparava and A. Valitutti (2004). WordNet-Affect: 
an affective extension of WordNet. In Proceedings 
LREC 2004, Lisbon, Portugal 
 
Su, F.and K. Markert (2008). Eliciting Subjectivity and 
Polarity Judgements on Word Senses. In Proceedings 
of Coling-2008, Manchester, UK. 
 
Valitutti, A. and C. Strapparava (2010). Interfacing 
Wordnet-Affect withj OCC model of emotions. In 
Proceedings of EMOTION-2010, Valletta, Malta.  
 
Wiebe, Janyce and Rada Micalcea.(2006) . Word Sense 
and Subjectivity. In Proceedings of ACL?06, Sydney, 
Australia.  
18
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 11?20,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
GAF: A Grounded Annotation Framework for Events
Antske Fokkens, Marieke van Erp, Piek Vossen
The Network Institute
VU University Amsterdam
antske.fokkens@vu.nl
marieke.van.erp@vu.nl
piek.vossen@vu.nl
Sara Tonelli
FBK
Trento, Italy
satonelli@fbk.eu
Willem Robert van Hage
SynerScope B.V.
Eindhoven, The Netherlands
willem.van.hage
@synerscope.com
Luciano Serafini, Rachele Sprugnoli
FBK
Trento, Italy
serafini@fbk.eu
sprugnoli@fbk.eu
Jesper Hoeksema
The Network Institute
VU University Amsterdam
j.e.hoeksema@vu.nl
Abstract
This paper introduces GAF, a grounded an-
notation framework to represent events in a
formal context that can represent information
from both textual and extra-textual sources.
GAF makes a clear distinction between men-
tions of events in text and their formal rep-
resentation as instances in a semantic layer.
Instances are represented by RDF compliant
URIs that are shared across different research
disciplines. This allows us to complete textual
information with external sources and facili-
tates reasoning. The semantic layer can inte-
grate any linguistic information and is com-
patible with previous event representations in
NLP. Through a use case on earthquakes in
Southeast Asia, we demonstrate GAF flexibil-
ity and ability to reason over events with the
aid of extra-linguistic resources.
1 Introduction
Events are not only described in textual documents,
they are also represented in many other non-textual
sources. These sources include videos, pictures,
sensors or evidence from data registration such as
mobile phone data, financial transactions and hos-
pital registrations. Nevertheless, many approaches
to textual event annotation consider events as text-
internal-affairs, possibly across multiple documents
but seldom across different modalities. It follows
from the above that event representation is not ex-
clusively a concern for the NLP community. It also
plays a major role in several other branches of in-
formation science such as knowledge representation
and the Semantic Web, which have created their own
models for representing events.
We propose a grounded annotation framework
(GAF) that allows us to interconnect different ways
of describing and registering events, including non-
linguistic sources. GAF representations can be used
to reason over the cumulated and linked sources of
knowledge and information to interpret the often in-
complete and fragmented information that is pro-
vided by each source. We make a clear distinction
between mentions of events in text or any other form
of registration and their formal representation as in-
stances in a semantic layer.
Mentions in text are annotated using the Terence
Annotation Format (Moens et al, 2011, TAF) on top
of which the semantic layer is realized using Seman-
tic Web technologies and standards. In this semantic
layer, instances are denoted with Uniform Resource
Identifiers (URIs). Attributes and relations are ex-
pressed according to the Simple Event Model (Van
Hage et al, 2011, SEM) and other established on-
tologies. Statements are grouped in named graphs
based on provenance and (temporal) validity, en-
abling the representation of conflicting information.
External knowledge can be related to instances from
a wide variety of sources such as those found in the
Linked Open Data Cloud (Bizer et al, 2009a).
Instances in the semantic layer can optionally be
linked to one or more mentions in text or to other
sources. Because linking instances is optional, our
11
representation offers a straightforward way to in-
clude information that can be inferred from text,
such as implied participants or whether an event is
part of a series that is not explicitly mentioned. Due
to the fact that each URI is unique, it is clear that
mentions connected to the same URI have a coref-
erential relation. Other relations between instances
(participants, subevents, temporal relations, etc.) are
represented explicitly in the semantic layer.
The remainder of this paper is structured as fol-
lows. In Section 2, we present related work and ex-
plain the motivation behind our approach. Section 3
describes the in-text annotation approach. Our se-
mantic annotation layer is presented in Section 4.
Sections 5-7 present GAF through a use case on
earthquakes in Indonesia. This is followed by our
conclusions and future work in section 8.
2 Motivation and Background
Annotation of events and of relations between them
has a long tradition in NLP. The MUC confer-
ences (Grishman and Sundheim, 1996) in the 90s
did not explicitly annotate events and coreference
relations, but the templates used for evaluating the
information extraction tasks indirectly can be seen
as annotation of events represented in newswires.
Such events are not ordered in time or further related
to each other. In response, Setzer and Gaizauskas
(2000) describe an annotation framework to create
coherent temporal orderings of events represented
in documents using closure rules. They suggest that
reasoning with text independent models, such as a
calendar, helps annotating textual representations.
More recently, generic corpora, such as Prop-
bank (Palmer et al, 2005) and the Framenet cor-
pus (Baker et al, 2003) have been built according to
linguistic principles. The annotations aim at prop-
erly representing verb structures within a sentence
context, focusing on verb arguments, semantic roles
and other elements. In ACE 2004 (Linguistic Data
Consortium, 2004b), event detection and linking is
included as a pilot task for the first time, inspired by
annotation schemes developed for named entities.
They distinguish between event mentions and the
trigger event, which is the mention that most clearly
expresses its occurrence (Linguistic Data Consor-
tium, 2004a). Typically, agreement on the trigger
event is low across annotators (around 55% (Moens
et al, 2011)). Timebank (Pustejovsky et al, 2006b)
is a more recent corpus for representing events and
time-expressions that includes temporal relations in
addition to plain coreference relations.
All these approaches have in common that they
consider the textual representation as a closed world
within which events need to be represented. This
means that mentions are linked to a trigger event
or to each other but not to an independent semantic
representation. More recently, researchers started to
annotate events across multiple documents, such as
the EventCorefBank (Bejan and Harabagiu, 2010).
Cross-document coreference is more challenging for
establishing the trigger event, but it is in essence not
different from annotating textual event coreference
within a single document. Descriptions of events
across documents may complement each other pro-
viding a more complete picture, but still textual de-
scriptions tend to be incomplete and sparse with re-
spect to time, place and participants. At the same
time, the comparison of events becomes more com-
plex. We thus expect even lower agreement in as-
signing trigger events across documents. Nothman
et al (2012) define the trigger as the first new ar-
ticle that mentions an event, which is easier than
to find the clearest description and still report inter-
annotator agreement of .48 and .73, respectively.
Recent approaches to automatically resolve event
coreference (cf. Chambers and Jurafsky (2011a),
Bejan and Harabagiu (2010)) use some background
data to establish coreference and other relations be-
tween events in text. Background information, in-
cluding resources, and models learned from textual
data do not represent mentions of events directly but
are useful to fill gaps of knowledge in the textual
descriptions. They do not alter the model for anno-
tation as such.
We aim to take these recent efforts one step fur-
ther and propose a grounded annotation framework
(GAF). Our main goal is to integrate information
from text analysis in a formal context shared with
researchers across domains. Furthermore, GAF is
flexible enough to contain contradictory informa-
tion. This is both important to represent sources
that (partially) contradict each other and to com-
bine alternative annotations or output of different
NLP tools. Because conflicting information may be
12
present, provenance of information is provided in
our framework, so that we may decide which source
to trust more or use it as a feature to decide which in-
terpretation to follow. Different models of event rep-
resentation exist that can contribute valuable infor-
mation. Therefore our model is compliant with prior
approaches regardless of whether they are manual or
automatic. Finally, GAF makes a clear distinction
between instances and instance mentions avoiding
the problem of determining a trigger event. Addi-
tionally, it facilitates the integration of information
from extra-textual sources and information that can
be inferred from texts, but is not explicitly men-
tioned. Sections 5 to 7 will explain how we can
achieve this with GAF.
3 The TERENCE annotation format
The TERENCE Annotation Format (TAF) is de-
fined within the TERENCE Project1 with the goal
to include event mentions, temporal expressions and
participant mentions in a single annotation proto-
col (Moens et al, 2011). TAF is based on ISO-
TimeML (Pustejovsky et al, 2010), but introduces
several adaptations in order to fit the domain of chil-
dren?s stories for which it was originally developed.
The format has been used to annotate around 30 chil-
dren stories in Italian and 10 in English.
We selected TAF as the basis for our in-text anno-
tation for three reasons. First, it incorporates the (in
our opinion crucial) distinction between instances
and instance mentions. Second, it adapts some con-
solidated paradigms for linguistic annotation such as
TimeML for events and temporal expressions and
ACE for participants and participant mentions (Lin-
guistic Data Consortium, 2005). It is thus compat-
ible with other annotation schemes. Third, it inte-
grates the annotation of event mentions, participants
and temporal expressions into a unified framework.
We will elaborate briefly on these properties below.
As mentioned, TAF makes a clear distinction be-
tween instances and instance mentions. Originally,
this distinction only applied to nominal and named
entities, similar to ACE (Linguistic Data Consor-
tium, 2005), because children?s stories can gener-
ally be treated as a closed world, usually present-
1ICT FP7 Programme, ICT-2010-25410, http://www.
terenceproject.eu/
ing a simple sequence of events that do not corefer.
Event coreference and linking to other sources was
thus not relevant for this domain. In GAF, we ex-
tend the distinction between instances and instance
mentions to events to model event coreference, link
them to other sources and create a consistent model
for all instances.
Children?s stories usually include a small set of
characters, event sequences (mostly in chronologi-
cal order), and a few generic temporal expressions.
In the TERENCE project, modeling characters in
the stories is necessary. This requires an extension
of TimeML to deal with event participants. Puste-
jovsky et al (2006a) address the need to include ar-
guments in TimeML annotations, but that proposal
did not include specific examples and details on how
to perform annotation (e.g., on the participants? at-
tributes). Such guidelines were created for TAF.
The TAF annotation of event mentions largely
follows TimeML in annotating tense, aspect, class,
mood, modality and polarity and temporal expres-
sions. However, there are several differences be-
tween TAF and TimeML. First, temporal expres-
sions are not normalized into the ISO-8601 form,
because most children?s stories are not fixed to a spe-
cific date. In GAF, the normalization of expressions
takes place in the semantic layer as these go beyond
the scope of the text. As a result, temporal vague-
ness of linguistic expressions in text do not need to
be normalized in the textual representation to actual
time points and remain underspecified.2
In TAF, events and participant mentions are linked
through a has participant relation, which is defined
as a directional, one-to-one relation from the event
to the participant mentions. Only mentions corre-
sponding to mandatory arguments of the events in
the story are annotated. Annotators look up each
verb in a reference dictionary providing information
on the predicate-argument structure of each verb.
This makes annotation easier and generally not con-
troversial. However, this kind of information can be
provided only by annotators having a good knowl-
edge of linguistics.
All annotations are performed with the Celct An-
2Note that we can still use existing tools for normalization
at the linguistic level: early normalizations can be integrated
in the semantic layer alongside normalizations carried out at a
later point.
13
sem:sub
EventOf
sem:Event sem:Actor sem:Place sem:Time
sem:hasTime
sem:hasActor
sem:hasPlace
sem:PlaceType
sem:placeType
sem:EventType
sem:eventType
sem:ActorType
sem:actorType
sem:TimeType
sem:Type
sem:timeType
sem:Core
sem:subTypeOf
C
o
r
e
 
C
l
a
s
s
e
s
(
F
o
r
e
i
g
n
)
T
y
p
e
 
S
y
s
t
e
m
Literal sem:hasTimeStamp
Literal sem:hasTimeStamp
Figure 1: The SEM ontology
notation Tool (Bartalesi Lenzi et al, 2012), an online
tool supporting TimeML that can easily be extended
to include participant information. The annotated
file can be exported to various XML formats and im-
ported into the semantic layer. The next section de-
scribes SEM, the event model used in our semantic
layer, and how it complements the TAF annotations.
4 The Simple Event Model
The Simple Event Model (SEM) is an RDF
schema (Carroll and Klyne, 2004; Guha and Brick-
ley, 2004) to express who did what, where, and
when. There are many RDF schemas and OWL on-
tologies (Motik et al, 2009) that describe events,
e.g., Shaw et al (2009), Crofts et al (2008) and
Scherp et al (2009). SEM is among the most
flexible and easiest to adapt to different domains.
SEM describes events and related instances such as
the place, time and participants (called Actors in
SEM) by representing the interactions between the
instances with RDF triples. SEM models are se-
mantic networks that include events, places, times,
participants and all related concepts, such as their
types.
An overview of all the classes in the SEM ontol-
ogy and the relations connecting them is shown in
Figure 1. Nodes can be identified by URIs, which
universally identify them across all RDF models. If
for example one uses the URI used by DBpedia3
(Bizer et al, 2009b) for the 2004 catastrophe in In-
3http://dbpedia.org
donesia, then one really means the same event as ev-
erybody else who uses that URI. SEM does not put
any constraints on the RDF vocabulary, so vocabu-
laries can easily be reused. Places and place types
can for example be imported from GeoNames4 and
event types from the RDF version of WordNet.
SEM supports two types of abstraction: gener-
alization with hierarchical relations from other on-
tologies, such as the subclass relation from RDFS,
and aggregation of events into superevents with the
sem:subEventOf relation, as exemplified in Fig-
ure 2. Other types of abstractions can be represented
using additional schemas or ontologies in combina-
tion with SEM. For instance, temporal aggregation
can be done with constructs from the OWL Time
ontology (Hobbs and Pan, 2004).
Relations between events and other instances,
which could be other events, places, actors, times,
or external concepts, can be modeled using the
sem:eventProperty relation. This relation can
be refined to represent specific relations, such as
specific participation, causality or simultaneity rela-
tions. The provenance of information in the SEM
graph is captured through assigning contexts to
statements using the PROV Data Model (Moreau et
al., 2012). In this manner, all statements derived
from a specific newspaper article are stored in a
named graph that represents that origin. Conflicting
statements can be stored in different named graphs,
and can thus coexist. This gives us the possibility
4http://www.geonames.org/ontology/
14
sem:Event
sem:Place
sem:EventType
sem:Time
dbpedia:2004_Indian_Ocean_
earthquake_and_ tsunami
rdf:type
"December 2004 
Earthquake and 
Tsunami"@en
rdfs:label
rdf:type
rdf:type
"3.316"^^xsd:decimal
"2004-12-26"^^xsd:date
"95.854"^^xsd:decimal
wgs84:long
wgs84:lat
owltime:inXSD
DateTime
sem:hasPlace sem:hasTime
naacl:INSTANCE_186
rdf:type
sem:subEventOf
wn30:synset-
earthquake-noun-1
sem:eventType
rdf:type
naacl:INSTANCE_188
rdf:type
sem:subEventOf
naacl:INSTANCE_198
sem:hasTime
naacl:TIMEX3_81 "2004"str:anchorOfnwr:denotedBy
naacl:INSTANCE_MENTION_118
nwr:denotedBy "temblor"@en
str:anchorOf
nwr:denotedBy
"tsunami"@en
naacl:INSTANCE_MENTION_120
str:anchorOf
naacl:INSTANCE_189
sem:subEventOf
naacl:INSTANCE_MENTION_121
nwr:denotedBy
"swept"@en
str:anchorOf
sem:hasPlace
naacl:INSTANCE_67
naacl:INSTANCE_MENTION_19nwr:denotedBy
"Indian Ocean"@en
str:anchorOf
taf:LOCATION
taf:NSUBJ
geonames:1545739
skos:exactMatch
gaf:G1
gaf:G2
gaf:G3
gaf:G4
gaf:G5
dbpedia:Bloomberg
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
sem:
derived
From
gaf:causes
Figure 2: Partial SEM representation of December 26th 2004 Earthquake
of delaying or ignoring the resolution of the conflict,
which enables use cases that require the analysis of
the conflict itself.
5 The GAF Annotation Framework
This section explains the basic idea behind GAF by
using texts on earthquakes in Indonesia. GAF pro-
vides a general model for event representation (in-
cluding textual and extra-textual mentions) as well
as exact representation of linguistic annotation or
output of NLP tools. Simply put, GAF is the combi-
nation of textual analyses and formal semantic rep-
resentations in RDF.
5.1 A SEM for earthquakes
We selected newspaper texts on the January 2009
West Papua earthquakes from Bejan and Harabagiu
(2010) to illustrate GAF. This choice was made be-
cause the topic ?earthquake? illustrates the advan-
tage of sharing URIs across domains. Gao and
Hunter (2011) propose a Linked Data model to cap-
ture major geological events such as earthquakes,
volcano activity and tsunamis. They combine infor-
mation from different seismological databases with
the intention to provide more complete information
to experts which may help to predict the occurrence
of such events. The information can also be used
in text interpretation. We can verify whether in-
terpretations by NLP tools correspond to the data
and relations defined by geologists or, through gen-
eralization, which interpretation is the most sensi-
ble given what we know about the events. General
information on events obtained from automatic text
processing, such as event templates (Chambers and
Jurafsky, 2011b) or typical event durations (Gusev
et al, 2010) can be integrated in SEM in a similar
manner. Provenance indications can be used to in-
dicate whether information is based on a model cre-
ated by an expert or an automatically derived model
obtained by a particular approach.
Figure 2 provides a fragment of a SEM represen-
tation for the earthquake and tsunami of December
26 2004.5 The model is partially inspired by Gao
and Hunter (2011)?s proposal. It combines infor-
mation extracted from texts with information from
DBpedia. The linking between the two can be es-
tablished either manually or automatically through
5The annotation and a larger representation including the
sentence it represents can be found on the GAF website http:
//wordpress.let.vu.nl/gaf.
15
an entity linking system.6 The combined event of
the earthquake and tsunami is represented by a DB-
pedia URI. The node labeled naacl:INSTANCE 186
represents the earthquake itself. The unambiguous
representation of the 2004 earthquake leads us to ad-
ditional information about it, for instance that the
earthquake is an event (sem:Event) and that the
sem:EventType is an earthquake, in this case
represented by a synset from WordNet, but also the
exact date it occurred and the exact location (cf
sem:hasTime, sem:hasPlace).
5.2 Integrating TAF representations into SEM
TAF annotations are converted to SEM relations.
For example, the TAF as participant relations
are translated to sem:hasActor relations, and
temporal relations are translated to sem:hasTime.
We use the relation nwr:denotedBy to link in-
stances to their mentions in the text which are repre-
sented by their unique identifiers in Figure 2.
Named graphs are used to model the source of
information as discussed in Section 4. The re-
lation sem:accordingTo indicates provenance
of information in the graph.7 For instance, the
mentions from the text in named graph gaf:G1
come from the source dbpedia:Bloomberg.
Relations between instances (e.g. between IN-
STANCE 189 and INSTANCE 188) are derived
from a specific grammatical relation in the text
(here, that tsunami is subject of swept) indicated
by the nwr:derivedFrom relation from gaf:G5
to gaf:G4. The grammatical relations included
in graph gaf:G5 come from a TAF annotation
(tag:annotation 2013 03 24).
6 GAF Earthquake Examples
This section takes a closer look at a few selected sen-
tences from the text that illustrate different aspects
of GAF. Figure 2 showed how a URI can provide a
formal context including important background in-
6Entity linking is the task of associating a mention to an
instance in a knowledge base. Several approaches and tools for
entity linking w.r.t. DBpedia and other data sets in the Linked
Open Data cloud are available and achieve good performances,
such as DBpedia Spotlight (Mendes et al, 2011); see (Rizzo
and Troncy, 2011) for a comparison of tools.
7The use of named graphs in this way to denote context is
compatible with the method used by Bozzato et al (2012).
formation on the event. Several texts in the corpus
refer to the tsunami of December 26, 2004, a 9.1
temblor in 2004 caused a tsunami and The catastro-
phe four years ago, among others. Compared to time
expressions such as 2004 and four years ago, time
indications extracted from external sources like DB-
pedia are not only more precise, but also permit us to
correctly establish the fact that these expressions re-
fer to the same event and thus indicate the same time.
The articles were published in January 2009: a direct
normalization of time indications would have placed
the catastrophe in 2005. The flexibility to combine
these seemingly conflicting time indications and de-
lay normalization can be used to correctly interpret
that four years ago early January 2009 refers to an
event taking place at the end of December 2004.
A fragment relating to one of the earthquakes of
January 2009: The quake struck off the coast [...] 75
kilometers (50 miles) west of [....] Manokwari pro-
vides a similar example. The expressions 75 kilo-
meters and 50 miles are clearly meant to express
the same distance, but not identical. The location
is most likely neither exactly 75 km nor 50 miles.
SEM can represent an underspecified location that
is included in the correct region. The exact location
of the earthquake can be found in external resources.
We can include both distances as expressions of the
location and decide whether they denote the general
location or include the normalized locations as alter-
natives to those from external resources.
Different sources may report different details.
Details may only be known later, or sources may
report from a different perspective. As provenance
information can be incorporated into the semantic
layer, we can represent different perspectives, and
choose which one to use when reasoning over the
information. For example, the following phrases
indicate the magnitude of the earthquakes that
struck Manokwari on January 4, 2009:
the 7.7 magnitude quake (source: Xinhuanet)
two quakes, measuring 7.6 and 7.4 (source: Bloomberg)
One 7.3-magnitude tremor (source: Jakartapost)
The first two magnitude indicators (7.7, 7.6)
are likely to pertain to the same earthquake, just as
the second two (7.4, 7.3) are. Trust indicators can
be found through the provenance trace of each men-
16
tion. Trust indicators can include the date on which
it was published, properties of the creation process,
the author, or publisher (Ceolin et al, 2010).
Furthermore, because the URIs are shared across
domains, we can link the information from the text
to information from seismological databases, which
may contain the exact measurement for the quake.
Similarly, external information obtained through
shared links can help us establish coreference. Con-
sider the sentences in Figure 3. There are several
ways to establish that the same event is meant in all
three sentences by using shared URIs and reasoning.
All sentences give us approximate time indications,
location of the affected area and casualties. Rea-
soning over these sentences combined with external
knowledge allows us to infer facts such as that un-
dersea [...] off [...] Aceh will be in the Indian Ocean,
or that the affected countries listed in the first sen-
tence are countries around the Indian Ocean, which
constitutes the Indian Ocean Community. The num-
ber of casualties in combination of the approximate
time indication or approximate location suffices to
identify the earthquake and tsunami in Indonesia on
December 26, 2004. The DBpedia representation
contains additional information such as the magni-
tude, exact location of the quake and a list of affected
countries, which can be used for additional verifica-
tion. This example illustrates how a formal context
using URIs that are shared across disciplines of in-
formation science can help to determine exact refer-
ents from limited or imprecise information.
7 Creating GAF
GAF entails integrating linguistic information
(e.g. TAF annotations) into RDF models (e.g. SEM).
The information in the model includes provenance
that points back to specific annotations. There are
two approaches to annotate text according to GAF.
The first approach is bottom-up. Mentions are
marked in the text as well as relations between them
(participants, time, causal relations, basically any-
thing except coreference). Consequently, these an-
notations are converted to SEM representations as
explained above. Coreference is established by link-
ing mentions to the same instance in SEM. The sec-
ond approach is top-down. Here, annotators mark
relations between instances (events, their partici-
pants, time relations, etc.) directly into SEM and
then link these to mentions in the text.
As mention in Section 2, inter-annotator agree-
ment on event annotation is generally low showing
that it is challenging. The task is somewhat simpli-
fied in GAF, since it removes the problem of identi-
fying an event trigger in the text. The GAF equiva-
lent of the event trigger in other linguistic annotation
approaches is an instance in SEM. However, other
challenges such as which mentions to select are in
principle not addressed by GAF, though differences
in inter-annotator agreement may be found depend-
ing on whether the bottom-up approach or the top-
down approach is selected. The formal context of
SEM may help frame annotations, especially for do-
mains such as earthquakes, where expert knowledge
was used to create basic event models. This may
help annotators while defining the correct relations
between events. On the other hand, the top-down
approach may lead to additional challenges, because
annotators are forced to link events to unambiguous
instances leading to hesitations as to when new in-
stances should be introduced.
Currently, we only use the bottom-up approach.
The main reason is the lack of an appropriate anno-
tation tool to directly annotate information in SEM.
We plan to perform comparative studies between the
two annotation approaches in future work.
8 Conclusion and Future Work
We presented GAF, an event annotation framework
in which textual mentions of events are grounded in
a semantic model that facilitates linking these events
to mentions in external (possibly non-textual) re-
sources and thereby reasoning. We illustrated how
GAF combines TAF and SEM through a use case
on earthquakes. We explained that we aim for a
representation that can combine textual and extra-
linguistic information, provides a clear distinction
between instances and instance mentions, is flexi-
ble enough to include conflicting information and
clearly marks the provenance of information.
GAF ticks all these boxes. All instances are rep-
resented by URIs in a semantic layer following stan-
dard RDF representations that are shared across re-
search disciplines. They are thus represented com-
pletely independent of the source and clearly distin-
17
There have been hundreds of earthquakes in Indonesia since a 9.1 temblor in 2004 caused a
tsunami that swept across the Indian Ocean, devastating coastal communities and leaving more
than 220,000 people dead in Indonesia, Sri Lanka, India, Thailand and other countries.
(Bloomberg, 2009-01-07 01:55 EST)
The catastrophe four years ago devastated Indian Ocean community and killed more than 230,000
people, over 170,000 of them in Aceh at northern tip of Sumatra Island of Indonesia.
(Xinhuanet, 2009-01-05 13:25:46 GMT)
In December 2004, a massive undersea quake off the western Indonesian province of Aceh
triggered a giant tsunami that left at least 230,000 people dead and missing in a dozen
countries facing the Indian Ocean. (Aljazeera, 2009-01-05 08:49 GMT)
Figure 3: Sample sentences mentioning the December 2004 Indonesian earthquake from sample texts
guished from mentions in text or mentions in other
sources. The Terence Annotation Format (TAF) pro-
vides a unified framework to annotate events, par-
ticipants and temporal expressions (and the corre-
sponding relations) by leaning on past, consolidated
annotation experiences such TimeML and ACE. We
will harmonize TAF, the Kyoto Annotation Format
(Bosma et al, 2009, KAF) and the NLP Interchange
Format (Hellmann et al, 2012, NIF) with respect
to the textual representation in the near future. The
NAF format includes the lessons learned from these
predecessors: layered standoff representations using
URI as identifiers and where possible standardized
data categories. The formal semantic model (SEM)
provides the flexibility to include conflicting infor-
mation as well as indications of the provenance of
this information. This allows us to use inferencing
and reasoning over the cumulated and aggregated
information, possibly exploiting the provenance of
the type of information source. This flexibility also
makes our representation compatible with all ap-
proaches dealing with event representation and de-
tections mentioned in Section 2. It can include au-
tomatically learned templates as well as specific re-
lations between events and time expressed in text.
Moreover, it may simultaneously contain output of
different NLP tools.
The proposed semantic layer may be simple, its
flexibility in importing external knowledge may in-
crease complexity in usage as it can model events in
every thinkable domain. To resolve this issue, it is
important to scope the domain by importing the ap-
propriate vocabularies, but no more. When keeping
this in mind, reasoning with SEM is shown to be rich
but still versatile (Van Hage et al, 2012).
While GAF provides us with the desired granu-
larity and flexibility for the event annotation tasks
we envision, a thorough evaluation still needs to be
carried out. This includes an evaluation of the anno-
tations created with GAF compared to other anno-
tation formats, as well as testing it within a greater
application. A comparative study of top-down and
bottom-up annotation will also be carried out. As al-
ready mentioned in Section 7, there is no appropriate
modeling tool for SEM yet. We are currently using
the CAT tool to create TAF annotations and convert
those to SEM, but will develop a tool to annotate the
semantic layer directly for this comparative study.
The most interesting effect of the GAF annota-
tions is that it provides us with relatively simple ac-
cess to a vast wealth of extra-linguistic information,
which we can utilize in a variety of NLP tasks; some
of the reasoning options that are made available by
the pairing up with Semantic Web technology may
for example aid us in identifying coreference rela-
tions between events. Investigating the implications
of this combination of NLP and Semantic Web tech-
nologies lies at the heart of our future work.
Acknowledgements
We thank Francesco Corcoglioniti for his helpful
comments and suggestions. The research lead-
ing to this paper was supported by the European
Union?s 7th Framework Programme via the News-
Reader Project (ICT-316404) and by the Biogra-
phyNed project, funded by the Netherlands eScience
Center (http://esciencecenter.nl/). Partners in Biog-
raphyNed are Huygens/ING Institute of the Dutch
Academy of Sciences and VU University Amster-
dam.
18
References
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography, 16(3):281?296.
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele
Sprugnoli. 2012. CAT: the CELCT Annotation Tool.
In Proceedings of LREC 2012.
Cosmin Bejan and Sandra Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412?1422.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009a. Linked data - the story so far. International
Journal on Semantic Web and Information Systems,
5(3):1?22.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009b. DBpedia - A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3):154
? 165.
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic se-
mantic annotation format. In Proceedings of the 5th
International Conference on Generative Approaches
to the Lexicon GL 2009, Pisa, Italy.
Loris Bozzato, Francesco Corcoglioniti, Martin Homola,
Mathew Joseph, and Luciano Serafini. 2012. Manag-
ing contextualized knowledge with the ckr (poster). In
Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC 2012), May 27-31.
Jeremy J. Carroll and Graham Klyne. 2004. Re-
source description framework (RDF): Concepts and
abstract syntax. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Davide Ceolin, Paul Groth, and Willem Robert Van Hage.
2010. Calculating the trust of event descriptions using
provenance. Proceedings Of The SWPM.
Nathanael Chambers and Dan Jurafsky. 2011a.
Template-based information extraction without the
templates. In Proceedings of ACL-2011.
Nathanael Chambers and Dan Jurafsky. 2011b.
Template-based information extraction without the
templates. In Proceedings of ACL-2011, Portland, OR.
Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead,
and Matthew Stiff. 2008. Definition of the CIDOC
Conceptual Reference Model. Technical report,
ICOM/CIDOC CRM Special Interest Group. version
4.2.5.
Lianli Gao and Jane Hunter. 2011. Publishing, link-
ing and annotating events via interactive timelines: an
earth sciences case study. In DeRiVE 2011 (Detec-
tion, Representation, and Exploitation of Events in the
Semantic Web) Workshop in conjunction with ISWC
2011, Bonn, Germany.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics (COLING?96), pages 466?471.
Ramanathan V. Guha and Dan Brickley. 2004.
RDF vocabulary description language 1.0: RDF
schema. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Jurafsky.
2010. Using query patterns to learn the duration of
events. In Proceedings of ISWC 2010.
Sebastian Hellmann, Jens Lehmann, and So?ren Auer.
2012. NIF: An ontology-based and linked-data-aware
NLP Interchange Format. Working Draft.
Jerry R Hobbs and Feng Pan. 2004. An ontology of time
for the semantic web. ACM Transactions on Asian
Language Information Processing (TALIP), 3(1):66?
85.
Linguistic Data Consortium. 2004a. Annotation
Guidelines for Event Detection and Characterization
(EDC). http://projects.ldc.upenn.edu/
ace/docs/EnglishEDCV2.0.pdf.
Linguistic Data Consortium. 2004b. The ACE 2004
Evaluation Plan. Technical report, LDC.
Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines for
entities. Version 6.6, July.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In Proceedings of the
7th International Conference on Semantic Systems, I-
Semantics ?11, pages 1?8.
Marie-Francine Moens, Oleksandr Kolomiyets,
Emanuele Pianta, Sara Tonelli, and Steven Bethard.
2011. D3.1: State-of-the-art and design of novel
annotation languages and technologies: Updated
version. Technical report, TERENCE project ? ICT
FP7 Programme ? ICT-2010-25410.
Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B?Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, Timo-
thy Lebo, Jim McCusker, Simon Miles, James Myers,
Satya Sahoo, and Curt Tilmes. 2012. PROV-DM: The
PROV Data Model. Technical report.
Boris Motik, Bijan Parsia, and Peter F. Patel-
Schneider. 2009. OWL 2 Web Ontology
19
Language structural specification and functional-
style syntax. W3C recommendation, W3C,
October. http://www.w3.org/TR/2009/
REC-owl2-syntax-20091027/.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R. Curran. 2012. Event linking: Ground-
ing event reference in a news archive. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 228?232, Jeju Island, Korea, July. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, 2013/03/12.
James Pustejovsky, Jessica Littman, and Roser Saur?`.
2006a. Argument Structure in TimeML. In Dagstuhl
Seminar Proceedings. Internationales Begegnungs-
und Forschungszentrum.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006b. Timebank 1.2 documentation.
Technical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An international
standard for semantic annotation. In Proceedings o
the Fifth International Workshop on Interoperable Se-
mantic Annotation.
Giuseppe Rizzo and Raphae?l Troncy. 2011. NERD:
A framework for evaluating named entity recognition
tools in the Web of data. In Workshop on Web Scale
Knowledge Extraction, colocated with ISWC 2011.
Ansgar Scherp, Thomas Franz, Carsten Saathoff, and
Steffen Staab. 2009. F?a model of events based on
the foundational ontology dolce+ dns ultralight. In
Proceedings of the fifth international conference on
Knowledge capture, pages 137?144. ACM.
Andrea Setzer and Robert J. Gaizauskas. 2000. Annotat-
ing events and temporal information in newswire texts.
In LREC. European Language Resources Association.
Ryan Shaw, Raphae?l Troncy, and Lynda Hardman. 2009.
LODE: Linking Open Descriptions of Events. In 4th
Annual Asian Semantic Web Conference (ASWC?09),
Shanghai, China.
Willem Robert Van Hage, Ve?ronique Malaise?, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011. De-
sign and use of the simple event model (SEM). Jour-
nal of Web Semantics.
Willem Robert Van Hage, Marieke Van Erp, and
Ve?ronique Malaise?. 2012. Linked open piracy: A
story about e-science, linked data, and statistics. Jour-
nal on Data Semantics, 1(3):187?201.
20
